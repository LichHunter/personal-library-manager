# POC-1b Complete Index

## üìã Documentation Files (Read in This Order)

### 1. **VISUAL_SUMMARY.txt** ‚Üê START HERE
   - ASCII art overview of entire project
   - 17 test files organized by phase
   - Key findings and metrics
   - Recommended configurations
   - **Time to read: 5 minutes**

### 2. **QUICK_REFERENCE.md** ‚Üê QUICK LOOKUP
   - TL;DR of all strategies
   - Strategy comparison table
   - Key findings summary
   - Recommended configurations
   - Common pitfalls to avoid
   - **Time to read: 10 minutes**

### 3. **ARCHITECTURE.md** ‚Üê MAIN REFERENCE
   - Complete system map
   - All 17 test files explained
   - Shared utilities and dependencies
   - Strategy dependency tree
   - Research directions and findings
   - Production recommendations
   - **Time to read: 30 minutes**

### 4. **DEPENDENCY_TREE.md** ‚Üê UNDERSTAND EVOLUTION
   - Visual dependency graph
   - Strategy evolution timeline
   - Phase-by-phase breakdown
   - Inheritance relationships
   - Implementation order
   - **Time to read: 20 minutes**

### 5. **RESULTS.md** ‚Üê EXPERIMENTAL FINDINGS
   - Executive summary
   - Detailed experiment results
   - Why small chunks work better
   - Understanding "hallucination"
   - Production configuration
   - **Time to read: 15 minutes**

### 6. **SPEC.md** ‚Üê DETAILED SPECIFICATION
   - Research questions
   - Background and motivation
   - Hypotheses
   - Experiment design
   - Implementation details
   - **Time to read: 25 minutes**

### 7. **README.md** ‚Üê PROJECT OVERVIEW
   - What, Why, How
   - Setup instructions
   - Usage commands
   - Key research findings
   - Files description
   - **Time to read: 10 minutes**

---

## üß™ Test Files (17 Total)

### Phase 1: Foundation (1 file)
- **test_single_chunk.py** (347 lines)
  - Strategies E-H: Instructor, Span Verify, Self-Consistency, Multi-Pass
  - Tests on POC-1 ground truth (45 chunks)
  - Baseline validation

### Phase 2: Hybrid Pattern+LLM (1 file)
- **test_pattern_plus_llm.py** (524 lines)
  - 198 K8s patterns + LLM expansion
  - Tests on POC-1 ground truth
  - Result: 75% P, 58% R

### Phase 3: Improved Recall (1 file)
- **test_improved_recall.py** (486 lines)
  - Exhaustive taxonomy + gleaning + categories
  - Tests on POC-1 ground truth
  - Result: 85%+ R

### Phase 4: Small Chunks ‚≠ê (1 file)
- **test_small_chunk_extraction.py** (647 lines)
  - 5 strategies on 10 small chunks (50-300 words)
  - Opus-generated ground truth
  - **PARADIGM SHIFT**: 92% recall vs 53-68% on full documents
  - Result: 92% R, 78.7% P

### Phase 5: Verification (3 files)
- **test_quote_verify_approach.py** (500+ lines)
  - Quote requirement + known terms vocabulary
  - Result: 92.6% P, 53.7% R, 7.4% H

- **test_zero_hallucination.py** (500+ lines)
  - Multi-stage verification (5 stages)
  - Result: <1% true H, 90%+ R

- **test_discrimination_approach.py** (500+ lines)
  - LLM discrimination (not generation)
  - Result: <5% H

### Phase 6: Ensemble & Voting (5 files)
- **test_high_recall_ensemble.py** (500+ lines)
  - Union of multiple extraction methods
  - Result: 90%+ R, <5% true H

- **test_combined_strategies.py** (500+ lines)
  - Voting + filtering approaches
  - Result: 75% R, 2.2% H

- **test_quote_extract_multipass.py** (500+ lines)
  - Multi-pass quote-extract with verification
  - Result: 90%+ P, 80%+ R

- **test_hybrid_ner.py** (500+ lines)
  - GLiNER + SpaCy + LLM approaches
  - Finding: NER doesn't work for K8s domain

- **test_advanced_strategies.py** (500+ lines)
  - Quote-then-extract + gleaning + cross-encoder
  - Result: Best balance

### Phase 7: Final Optimization (4 files)
- **test_hybrid_final.py** (500+ lines)
  - Ensemble + Sonnet verification
  - Result: 88.9% R, 10.7% H

- **test_fast_combined.py** (500+ lines)
  - Fast iteration on best strategies
  - Result: ensemble_verified best for recall

---

## üõ†Ô∏è Utility Files

### Core Utilities
- **utils/llm_provider.py**
  - Anthropic API wrapper
  - `call_llm(prompt, model, temperature, max_tokens)`
  - Supports: claude-haiku, claude-sonnet, claude-opus
  - Used by: ALL test files

- **utils/logger.py**
  - Logging utilities

### Analysis Scripts
- **audit_ground_truth.py**
  - Ground truth quality audit
  - Generates: artifacts/gt_audit.json

- **analyze_false_positives.py**
  - False positive analysis

- **run_experiment.py**
  - Main experiment runner

---

## üìä Ground Truth Files

### POC-1 Ground Truth
- **artifacts/phase-2-ground-truth.json**
  - 45 chunks from K8s documentation
  - Manually annotated terms
  - Used by: Phase 1-3, Phase 5, Phase 6a tests

### Small Chunk Ground Truth (NEW)
- **artifacts/small_chunk_ground_truth.json**
  - 10 semantic chunks (50-300 words each)
  - Opus-generated ground truth
  - Used by: Phase 4, Phase 6b-c, Phase 7 tests

### Audit Results
- **artifacts/gt_audit.json**
  - Ground truth quality metrics
  - 99.8% recoverable terms

---

## üìà Result Artifacts

- **artifacts/small_chunk_results.json**
  - Results from test_small_chunk_extraction.py

- **artifacts/advanced_strategies_results.json**
  - Results from test_advanced_strategies.py

- **artifacts/discrimination_results.json**
  - Results from test_discrimination_approach.py

- **artifacts/fast_combined_results.json**
  - Results from test_fast_combined.py

- **artifacts/hybrid_final_results.json**
  - Results from test_hybrid_final.py

- **artifacts/multipass_quote_extract_results.json**
  - Results from test_quote_extract_multipass.py

- **artifacts/quote_verify_results.json**
  - Results from test_quote_verify_approach.py

---

## üéØ Quick Navigation

### By Use Case

**I want to understand the project quickly**
1. Read VISUAL_SUMMARY.txt (5 min)
2. Read QUICK_REFERENCE.md (10 min)
3. Skim ARCHITECTURE.md (10 min)

**I want to implement a production system**
1. Read QUICK_REFERENCE.md (10 min)
2. Read ARCHITECTURE.md section 8 (5 min)
3. Study test_small_chunk_extraction.py (20 min)
4. Study test_high_recall_ensemble.py (20 min)

**I want to understand the research**
1. Read SPEC.md (25 min)
2. Read RESULTS.md (15 min)
3. Read ARCHITECTURE.md (30 min)
4. Study relevant test files (varies)

**I want to understand strategy evolution**
1. Read DEPENDENCY_TREE.md (20 min)
2. Follow the test files in order (varies)

**I want to find a specific strategy**
1. Use QUICK_REFERENCE.md strategy comparison table
2. Look up test file in ARCHITECTURE.md
3. Read that test file

---

## üìö Key Concepts

### Small Chunks
- 50-300 words per chunk
- Semantic boundaries (by heading/section)
- Achieves 92% recall vs 53-68% on full documents

### Span Verification
- Deterministic (no LLM)
- Checks if term exists in source text
- Achieves <1% true hallucination

### Ensemble Extraction
- Union of multiple strategies
- Achieves 92% recall
- Mostly valid terms (not true hallucinations)

### Quote Requirement
- Forces grounding before extraction
- Achieves 92.6% precision
- Reduces recall to 53.7%

### Discrimination Approach
- LLM classifies candidates (yes/no)
- Can't hallucinate new terms
- Achieves <5% hallucination

### Self-Consistency Voting
- N=5-10 samples with voting
- Improves precision
- Requires high temperature (0.8)

### Multi-Pass Extraction
- Multiple passes with different prompts
- "What did I miss?" follow-up
- Improves recall without increasing hallucination

---

## üîç Finding Information

### By Metric
- **Precision**: See QUICK_REFERENCE.md "By Precision"
- **Recall**: See QUICK_REFERENCE.md "By Recall"
- **Hallucination**: See QUICK_REFERENCE.md "By Hallucination"

### By Strategy
- **Quote-Extract**: test_quote_verify_approach.py, test_quote_extract_multipass.py
- **Ensemble**: test_high_recall_ensemble.py, test_fast_combined.py
- **Discrimination**: test_discrimination_approach.py
- **Zero-Hallucination**: test_zero_hallucination.py
- **Hybrid**: test_hybrid_final.py, test_hybrid_ner.py

### By Phase
- **Phase 1**: test_single_chunk.py
- **Phase 2**: test_pattern_plus_llm.py
- **Phase 3**: test_improved_recall.py
- **Phase 4**: test_small_chunk_extraction.py
- **Phase 5**: test_quote_verify_approach.py, test_zero_hallucination.py, test_discrimination_approach.py
- **Phase 6**: test_high_recall_ensemble.py, test_combined_strategies.py, test_quote_extract_multipass.py, test_hybrid_ner.py, test_advanced_strategies.py
- **Phase 7**: test_hybrid_final.py, test_fast_combined.py

---

## üìñ Reading Paths

### Path 1: Executive Summary (15 minutes)
1. VISUAL_SUMMARY.txt
2. QUICK_REFERENCE.md

### Path 2: Complete Understanding (90 minutes)
1. VISUAL_SUMMARY.txt (5 min)
2. QUICK_REFERENCE.md (10 min)
3. ARCHITECTURE.md (30 min)
4. DEPENDENCY_TREE.md (20 min)
5. RESULTS.md (15 min)
6. Skim test_small_chunk_extraction.py (10 min)

### Path 3: Implementation Guide (60 minutes)
1. QUICK_REFERENCE.md (10 min)
2. ARCHITECTURE.md section 8 (5 min)
3. test_small_chunk_extraction.py (20 min)
4. test_high_recall_ensemble.py (15 min)
5. QUICK_REFERENCE.md recommended configs (10 min)

### Path 4: Research Deep Dive (120 minutes)
1. SPEC.md (25 min)
2. RESULTS.md (15 min)
3. ARCHITECTURE.md (30 min)
4. DEPENDENCY_TREE.md (20 min)
5. Read 3-4 test files (30 min)

---

## ‚úÖ Checklist for Understanding

- [ ] Read VISUAL_SUMMARY.txt
- [ ] Read QUICK_REFERENCE.md
- [ ] Understand the 5 key findings
- [ ] Know the 3 recommended configurations
- [ ] Understand why small chunks matter
- [ ] Know the difference between "hallucination" and true hallucination
- [ ] Understand LLM as discriminator vs generator
- [ ] Know which strategy to use for your use case
- [ ] Read ARCHITECTURE.md for complete details
- [ ] Read DEPENDENCY_TREE.md for strategy evolution

---

## üöÄ Next Steps

1. **For Understanding**: Follow "Path 2: Complete Understanding"
2. **For Implementation**: Follow "Path 3: Implementation Guide"
3. **For Research**: Follow "Path 4: Research Deep Dive"
4. **For Quick Lookup**: Use QUICK_REFERENCE.md

---

## üìû Key Contacts

- **Project Lead**: POC-1b team
- **Ground Truth**: Opus-generated (small chunks), manually annotated (POC-1)
- **Models Used**: Claude Haiku, Claude Sonnet, Claude Opus
- **Status**: PARTIAL SUCCESS - 92% recall achieved, <5% true hallucination

---

**Last Updated**: 2026-02-05
**Total Documentation**: 7 files + 17 test files + utilities
**Total Lines of Code**: ~8,000+ lines across test files
**Total Documentation**: ~5,000+ lines across documentation files

