[16:01:06] INFO  | Loading config from /home/fujin/Code/personal-library-manager/poc/chunking_benchmark_v2/config_full_benchmark.yaml

================================================================================
[16:01:06] INFO  | COMPREHENSIVE RETRIEVAL BENCHMARK
================================================================================
[16:01:06] DEBUG | Timer 'total_benchmark' started

================================================================================
[16:01:06] INFO  | BENCHMARK CONFIGURATION
================================================================================
[16:01:06] METRIC | device=cuda
[16:01:06] METRIC | ollama_available=False
[16:01:06] METRIC | claude_available=True
[16:01:06] METRIC | llm_available=True
[16:01:06] INFO  | Device: cuda
[16:01:06] INFO  | Ollama available: False
[16:01:06] INFO  | Claude available: True
[16:01:06] INFO  | LLM available: True
[16:01:06] INFO  | Loading corpus...
[16:01:06] METRIC | documents=5
[16:01:06] METRIC | queries=20
[16:01:06] METRIC | total_facts=53
[16:01:06] METRIC | has_human_queries=True
[16:01:06] INFO  | Loaded 5 documents
[16:01:06] INFO  | Loaded 20 queries
[16:01:06] INFO  | Total key facts: 53
[16:01:06] INFO  | Human query variations: True
[16:01:06] INFO  | Enabled embedders: 1
[16:01:06] INFO  | Enabled rerankers: 0
[16:01:06] INFO  | Enabled LLMs: 0
[16:01:06] INFO  | Enabled chunking strategies: 1
[16:01:06] INFO  | Enabled retrieval strategies: 4
[16:01:06] METRIC | total_combinations=4
[16:01:06] METRIC | total_evaluations=4
[16:01:06] INFO  | Total strategy combinations: 4
[16:01:06] INFO  | Total evaluations (with k and metrics): 4

================================================================================
[16:01:06] INFO  | RETRIEVAL STRATEGY: bmx_pure
================================================================================
[16:01:06] INFO  | Loading embedder: BAAI/bge-base-en-v1.5
[16:01:09] PROG  | [  1/  4] █████░░░░░░░░░░░░░░░  25.0% | bmx_pure_bge-base-en-v1.5_fixed_512_0pct
[16:01:09] DEBUG | Chunking document 'api_reference': 1792 words, 16663 chars
[16:01:09] DEBUG |   Chunk 0: chars [0-3160] words=376 tokens≈500 preview='# CloudFlow API Reference Version 2.1.0 | Last Updated: January 2026 ## Overview The CloudFlow API i...'
[16:01:09] DEBUG |   Chunk 1: chars [3161-6111] words=379 tokens≈504 preview='## Rate Limiting To ensure fair usage and system stability, CloudFlow enforces rate limits on all AP...'
[16:01:09] DEBUG |   Chunk 2: chars [6112-8827] words=296 tokens≈393 preview='**Endpoint:** `POST /workflows` **Request Body:** ```json { "name": "Email Campaign Automation", "de...'
[16:01:09] DEBUG |   Chunk 3: chars [8828-11402] words=269 tokens≈357 preview='**Endpoint:** `GET /pipelines/{pipeline_id}/executions` **Query Parameters:** - `status` (optional):...'
[16:01:09] DEBUG |   Chunk 4: chars [11403-14151] words=365 tokens≈485 preview='### Error Response Format ```json { "error": { "code": "invalid_parameter", "message": "The 'limit' ...'
[16:01:09] DEBUG |   Chunk 5: chars [14152-15213] words=107 tokens≈142 preview='**Supported Events:** - `workflow.started` - `workflow.completed` - `workflow.failed` - `pipeline.co...'
[16:01:09] DEBUG | Created 6 chunks for document 'api_reference'
[16:01:09] DEBUG | Chunking document 'architecture_overview': 4607 words, 36722 chars
[16:01:09] DEBUG |   Chunk 0: chars [0-3230] words=370 tokens≈492 preview='# CloudFlow Platform - System Architecture Overview **Document Version:** 2.3.1 **Last Updated:** Ja...'
[16:01:09] DEBUG |   Chunk 1: chars [3231-5982] words=380 tokens≈505 preview='**Technology**: Node.js with Express.js framework **Replicas**: 12 pods (production), auto-scaling 8...'
[16:01:09] DEBUG |   Chunk 2: chars [5983-7875] words=248 tokens≈329 preview='**Technology**: Node.js with TypeScript, Bull queue library **Replicas**: 16 pods (production), auto...'
[16:01:09] DEBUG |   Chunk 3: chars [7876-9478] words=219 tokens≈291 preview='**Technology**: Go with distributed locking via Redis **Replicas**: 4 pods (production), active-pass...'
[16:01:09] DEBUG |   Chunk 4: chars [9479-11878] words=354 tokens≈470 preview='**Technology**: Node.js with worker pool pattern **Replicas**: 8 pods (production), auto-scaling 6-1...'
[16:01:09] DEBUG |   Chunk 5: chars [11879-14634] words=368 tokens≈489 preview='Success/failure events published back to Kafka ``` **Event Schema**: ```json { "event_id": "uuid-v4"...'
[16:01:09] DEBUG |   Chunk 6: chars [14635-17394] words=384 tokens≈510 preview='**Session Storage**: - Key pattern: `session:{user_id}` - TTL: 15 minutes (aligned with JWT expiry) ...'
[16:01:09] DEBUG |   Chunk 7: chars [17395-20141] words=381 tokens≈506 preview='records: 500 - Session timeout: 30 seconds --- ## Message Queue Patterns ### Pub/Sub Pattern Used fo...'
[16:01:09] DEBUG |   Chunk 8: chars [20142-22843] words=384 tokens≈510 preview='### Cache Invalidation Strategies **Time-Based Expiration (TTL)**: - Short-lived data: 5-15 minutes ...'
[16:01:09] DEBUG |   Chunk 9: chars [22844-25362] words=384 tokens≈510 preview='per API key (default: 1000 RPM) ### Secrets Management **HashiCorp Vault Integration**: - Dynamic da...'
[16:01:09] DEBUG |   Chunk 10: chars [25363-27842] words=384 tokens≈510 preview='- Execution start rate: 500 per second - Completion rate: 450 per second (average 2-second execution...'
[16:01:09] DEBUG |   Chunk 11: chars [27843-30286] words=375 tokens≈498 preview='Service (2) - PostgreSQL Primary - PostgreSQL Replica - PostgreSQL Replica - Redis Primary (2) - Red...'
[16:01:09] DEBUG |   Chunk 12: chars [30287-32834] words=376 tokens≈500 preview='Resume normal operations - Recovery time: 1-3 hours depending on data volume - Data loss: None if co...'
[16:01:09] DEBUG | Created 13 chunks for document 'architecture_overview'
[16:01:09] DEBUG | Chunking document 'deployment_guide': 2777 words, 25837 chars
[16:01:09] DEBUG |   Chunk 0: chars [0-3165] words=384 tokens≈510 preview='# CloudFlow Platform - Deployment and Operations Guide **Version:** 2.4.0 **Last Updated:** January ...'
[16:01:09] DEBUG |   Chunk 1: chars [3166-6560] words=384 tokens≈510 preview='create cluster -f cluster-config.yaml ``` ### Storage Configuration Install the EBS CSI driver for p...'
[16:01:09] DEBUG |   Chunk 2: chars [6561-8755] words=251 tokens≈333 preview='- secretName: cloudflow-tls hosts: - api.cloudflow.io healthCheck: enabled: true livenessProbe: path...'
[16:01:09] DEBUG |   Chunk 3: chars [8756-12138] words=384 tokens≈510 preview='Deploy PostgreSQL using the Bitnami Helm chart: ```yaml # postgres-values.yaml global: postgresql: a...'
[16:01:09] DEBUG |   Chunk 4: chars [12139-14139] words=204 tokens≈271 preview='--set prometheus.prometheusSpec.retention=30d \ --set prometheus.prometheusSpec.storageSpec.volumeCl...'
[16:01:09] DEBUG |   Chunk 5: chars [14140-17194] words=372 tokens≈494 preview='Configure log aggregation: ```bash # Install Fluent Bit for log forwarding helm install fluent-bit f...'
[16:01:09] DEBUG |   Chunk 6: chars [17195-20069] words=384 tokens≈510 preview='**Verify Service Health**: ```bash kubectl get pods -n cloudflow-prod curl https://api.cloudflow.io/...'
[16:01:09] DEBUG |   Chunk 7: chars [20070-23221] words=384 tokens≈510 preview='podSelector: matchLabels: app: cloudflow policyTypes: - Ingress - Egress ingress: - from: - namespac...'
[16:01:09] DEBUG |   Chunk 8: chars [23222-23448] words=30 tokens≈39 preview='UTC - **Secondary**: Wednesday 10:00-12:00 UTC All deployments should target these windows to minimi...'
[16:01:09] DEBUG | Created 9 chunks for document 'deployment_guide'
[16:01:09] DEBUG | Chunking document 'troubleshooting_guide': 4032 words, 32183 chars
[16:01:09] DEBUG |   Chunk 0: chars [0-2666] words=320 tokens≈425 preview='# CloudFlow Platform Troubleshooting Guide **Version:** 3.2.1 **Last Updated:** January 2026 **Audie...'
[16:01:09] DEBUG |   Chunk 1: chars [2667-5007] words=321 tokens≈426 preview='**Diagnosis:** ```bash # Check system time timedatectl status # Compare with NTP server ntpdate -q p...'
[16:01:09] DEBUG |   Chunk 2: chars [5008-8037] words=376 tokens≈500 preview='Review Query Execution Plans** ```sql -- Connect to CloudFlow database cloudflow db connect --readon...'
[16:01:09] DEBUG |   Chunk 3: chars [8038-10862] words=356 tokens≈473 preview='Workflow Context Accumulation** Large workflow executions may accumulate state in memory. **Solution...'
[16:01:09] DEBUG |   Chunk 4: chars [10863-13827] words=384 tokens≈510 preview='**Implement connection pooling optimization:** ```bash # Use PgBouncer for connection pooling kubect...'
[16:01:09] DEBUG |   Chunk 5: chars [13828-16401] words=334 tokens≈444 preview='exec_7h3j6k9m2n --show-bottlenecks ``` #### Solutions **1. Increase workflow timeout (if justified):...'
[16:01:09] DEBUG |   Chunk 6: chars [16402-19022] words=329 tokens≈437 preview='Data Validation Errors** ``` ValidationError: Field 'customer_id' is required but missing in 234 rec...'
[16:01:09] DEBUG |   Chunk 7: chars [19023-20595] words=194 tokens≈258 preview='Waiting {retry_after} seconds...") time.sleep(retry_after) continue remaining = int(response.headers...'
[16:01:09] DEBUG |   Chunk 8: chars [20596-23454] words=384 tokens≈510 preview='Leverage caching:** ```bash # Enable client-side caching export CLOUDFLOW_CACHE_ENABLED=true export ...'
[16:01:09] DEBUG |   Chunk 9: chars [23455-26239] words=384 tokens≈510 preview='in" # Find workflow failures by ID kubectl logs -n cloudflow deployment/cloudflow-workflow-engine | ...'
[16:01:09] DEBUG |   Chunk 10: chars [26240-29039] words=384 tokens≈510 preview='database nc -zv cloudflow-db.internal.company.com 5432 # Trace route traceroute api.cloudflow.io # C...'
[16:01:09] DEBUG |   Chunk 11: chars [29040-31206] words=266 tokens≈353 preview='outage" # Create war room cloudflow incident war-room create incident-2024012401 ``` **For SEV-2 (Hi...'
[16:01:09] DEBUG | Created 12 chunks for document 'troubleshooting_guide'
[16:01:09] DEBUG | Chunking document 'user_guide': 3735 words, 31582 chars
[16:01:09] DEBUG |   Chunk 0: chars [0-2617] words=375 tokens≈498 preview='# CloudFlow User Guide Welcome to CloudFlow, the modern workflow automation platform that helps you ...'
[16:01:09] DEBUG |   Chunk 1: chars [2618-5224] words=348 tokens≈462 preview='Open the Visual Editor by clicking **"Create Workflow"** or editing an existing workflow 2. Add a tr...'
[16:01:09] DEBUG |   Chunk 2: chars [5225-8372] words=381 tokens≈506 preview='### HTTP Requests Make HTTP requests to any API endpoint: **Configuration:** - **Method**: GET, POST...'
[16:01:09] DEBUG |   Chunk 3: chars [8373-11268] words=336 tokens≈446 preview='``` **Slack App Integration:** - Install the CloudFlow Slack app in your workspace - Authenticate on...'
[16:01:09] DEBUG |   Chunk 4: chars [11269-13371] words=375 tokens≈498 preview='### Cron Syntax Use standard cron expressions to define schedules: ``` * * * * * ┬ ┬ ┬ ┬ ┬ │ │ │ │ │...'
[16:01:09] DEBUG |   Chunk 5: chars [13372-16105] words=383 tokens≈509 preview='Save and activate **Testing Schedules:** Use the built-in schedule calculator to preview upcoming ex...'
[16:01:09] DEBUG |   Chunk 6: chars [16106-18686] words=363 tokens≈482 preview='Filter by error type, date range, or execution ID 3. Click an execution to view full details 4. Clic...'
[16:01:09] DEBUG |   Chunk 7: chars [18687-21448] words=364 tokens≈484 preview='Handle Errors Gracefully Always implement error handling for external API calls and database operati...'
[16:01:09] DEBUG |   Chunk 8: chars [21449-24677] words=384 tokens≈510 preview='Version Control For critical workflows: - Export YAML definitions regularly - Store in version contr...'
[16:01:09] DEBUG |   Chunk 9: chars [24678-27975] words=371 tokens≈493 preview='Requester: {{trigger.body.requester}} Description: {{trigger.body.description}} Approve: https://app...'
[16:01:09] DEBUG |   Chunk 10: chars [27976-28481] words=55 tokens≈73 preview='- **Documentation**: https://docs.cloudflow.io - **Community Forum**: https://community.cloudflow.io...'
[16:01:09] DEBUG | Created 11 chunks for document 'user_guide'
[16:01:09] INFO  |   Created 51 chunks
    [enriched] chunks=51 new=0 cache_hits=51 time=0.0s avg=0.0ms/chunk
[16:01:11] INFO  |   Indexed in 2.06s
[16:01:11] TRACE | === FACT COVERAGE ANALYSIS ===
[16:01:11] TRACE | Query: What is the API rate limit per minute?
[16:01:11] TRACE | Total key facts: 2
[16:01:11] TRACE | Found facts: 2/2
[16:01:11] TRACE |   FOUND[1] 100 requests per minute per authenticated user
[16:01:11] TRACE |   FOUND[2] 20 requests per minute for unauthenticated requests
[16:01:11] TRACE | === END FACT COVERAGE ===
[16:01:11] TRACE | === FACT COVERAGE ANALYSIS ===
[16:01:11] TRACE | Query: How many requests can I make per minute to the API?
[16:01:11] TRACE | Total key facts: 2
[16:01:11] TRACE | Found facts: 2/2
[16:01:11] TRACE |   FOUND[1] 100 requests per minute per authenticated user
[16:01:11] TRACE |   FOUND[2] 20 requests per minute for unauthenticated requests
[16:01:11] TRACE | === END FACT COVERAGE ===
[16:01:11] TRACE | === FACT COVERAGE ANALYSIS ===
[16:01:11] TRACE | Query: My API calls are getting blocked, what's the limit?
[16:01:11] TRACE | Total key facts: 2
[16:01:11] TRACE | Found facts: 2/2
[16:01:11] TRACE |   FOUND[1] 100 requests per minute per authenticated user
[16:01:11] TRACE |   FOUND[2] 20 requests per minute for unauthenticated requests
[16:01:11] TRACE | === END FACT COVERAGE ===
[16:01:11] TRACE | === FACT COVERAGE ANALYSIS ===
[16:01:11] TRACE | Query: api rate limit
[16:01:11] TRACE | Total key facts: 2
[16:01:11] TRACE | Found facts: 2/2
[16:01:11] TRACE |   FOUND[1] 100 requests per minute per authenticated user
[16:01:11] TRACE |   FOUND[2] 20 requests per minute for unauthenticated requests
[16:01:11] TRACE | === END FACT COVERAGE ===
[16:01:11] TRACE | === FACT COVERAGE ANALYSIS ===
[16:01:11] TRACE | Query: I'm building a batch job that calls CloudFlow API repeatedly, what rate limit should I expect?
[16:01:11] TRACE | Total key facts: 2
[16:01:11] TRACE | Found facts: 2/2
[16:01:11] TRACE |   FOUND[1] 100 requests per minute per authenticated user
[16:01:11] TRACE |   FOUND[2] 20 requests per minute for unauthenticated requests
[16:01:11] TRACE | === END FACT COVERAGE ===
[16:01:11] TRACE | === FACT COVERAGE ANALYSIS ===
[16:01:11] TRACE | Query: Why is the API rejecting my requests after 100 calls?
[16:01:11] TRACE | Total key facts: 2
[16:01:11] TRACE | Found facts: 2/2
[16:01:11] TRACE |   FOUND[1] 100 requests per minute per authenticated user
[16:01:11] TRACE |   FOUND[2] 20 requests per minute for unauthenticated requests
[16:01:11] TRACE | === END FACT COVERAGE ===
[16:01:11] TRACE | === FACT COVERAGE ANALYSIS ===
[16:01:11] TRACE | Query: How do I fix 429 Too Many Requests errors?
[16:01:11] TRACE | Total key facts: 5
[16:01:11] TRACE | Found facts: 3/5
[16:01:11] TRACE |   FOUND[1] 429 Too Many Requests
[16:01:11] TRACE |   FOUND[2] X-RateLimit-Remaining
[16:01:11] TRACE |   FOUND[3] Implement exponential backoff when receiving 429 responses
[16:01:11] TRACE | Missed facts: 2/5
[16:01:11] TRACE |   MISSED[1] Retry-After
[16:01:11] TRACE |     -> Found in chunk_id=troubleshooting_guide_fix_6 (rank not in top-5)
[16:01:11] TRACE |   MISSED[2] Monitor X-RateLimit-Remaining header values
[16:01:11] TRACE | === END FACT COVERAGE ===
[16:01:11] TRACE | === FACT COVERAGE ANALYSIS ===
[16:01:11] TRACE | Query: What should I do when I get rate limited?
[16:01:11] TRACE | Total key facts: 5
[16:01:11] TRACE | Found facts: 3/5
[16:01:11] TRACE |   FOUND[1] 429 Too Many Requests
[16:01:11] TRACE |   FOUND[2] X-RateLimit-Remaining
[16:01:11] TRACE |   FOUND[3] Implement exponential backoff when receiving 429 responses
[16:01:11] TRACE | Missed facts: 2/5
[16:01:11] TRACE |   MISSED[1] Retry-After
[16:01:11] TRACE |     -> Found in chunk_id=troubleshooting_guide_fix_6 (rank not in top-5)
[16:01:11] TRACE |   MISSED[2] Monitor X-RateLimit-Remaining header values
[16:01:11] TRACE | === END FACT COVERAGE ===
[16:01:11] TRACE | === FACT COVERAGE ANALYSIS ===
[16:01:11] TRACE | Query: My requests keep failing with 429 status code
[16:01:11] TRACE | Total key facts: 5
[16:01:11] TRACE | Found facts: 3/5
[16:01:11] TRACE |   FOUND[1] 429 Too Many Requests
[16:01:11] TRACE |   FOUND[2] X-RateLimit-Remaining
[16:01:11] TRACE |   FOUND[3] Implement exponential backoff when receiving 429 responses
[16:01:11] TRACE | Missed facts: 2/5
[16:01:11] TRACE |   MISSED[1] Retry-After
[16:01:11] TRACE |     -> Found in chunk_id=troubleshooting_guide_fix_6 (rank not in top-5)
[16:01:11] TRACE |   MISSED[2] Monitor X-RateLimit-Remaining header values
[16:01:11] TRACE | === END FACT COVERAGE ===
[16:01:11] TRACE | === FACT COVERAGE ANALYSIS ===
[16:01:11] TRACE | Query: 429 error fix
[16:01:11] TRACE | Total key facts: 5
[16:01:11] TRACE | Found facts: 3/5
[16:01:11] TRACE |   FOUND[1] 429 Too Many Requests
[16:01:11] TRACE |   FOUND[2] X-RateLimit-Remaining
[16:01:11] TRACE |   FOUND[3] Implement exponential backoff when receiving 429 responses
[16:01:11] TRACE | Missed facts: 2/5
[16:01:11] TRACE |   MISSED[1] Retry-After
[16:01:11] TRACE |     -> Found in chunk_id=troubleshooting_guide_fix_6 (rank not in top-5)
[16:01:11] TRACE |   MISSED[2] Monitor X-RateLimit-Remaining header values
[16:01:11] TRACE | === END FACT COVERAGE ===
[16:01:11] TRACE | === FACT COVERAGE ANALYSIS ===
[16:01:11] TRACE | Query: I'm getting throttled by CloudFlow API, how can I handle this in my application?
[16:01:11] TRACE | Total key facts: 5
[16:01:11] TRACE | Found facts: 3/5
[16:01:11] TRACE |   FOUND[1] 429 Too Many Requests
[16:01:11] TRACE |   FOUND[2] X-RateLimit-Remaining
[16:01:11] TRACE |   FOUND[3] Implement exponential backoff when receiving 429 responses
[16:01:11] TRACE | Missed facts: 2/5
[16:01:11] TRACE |   MISSED[1] Retry-After
[16:01:11] TRACE |     -> Found in chunk_id=troubleshooting_guide_fix_6 (rank not in top-5)
[16:01:11] TRACE |   MISSED[2] Monitor X-RateLimit-Remaining header values
[16:01:11] TRACE | === END FACT COVERAGE ===
[16:01:11] TRACE | === FACT COVERAGE ANALYSIS ===
[16:01:11] TRACE | Query: Why am I being blocked with rate limit errors?
[16:01:11] TRACE | Total key facts: 5
[16:01:11] TRACE | Found facts: 3/5
[16:01:11] TRACE |   FOUND[1] 429 Too Many Requests
[16:01:11] TRACE |   FOUND[2] X-RateLimit-Remaining
[16:01:11] TRACE |   FOUND[3] Implement exponential backoff when receiving 429 responses
[16:01:11] TRACE | Missed facts: 2/5
[16:01:11] TRACE |   MISSED[1] Retry-After
[16:01:11] TRACE |     -> Found in chunk_id=troubleshooting_guide_fix_6 (rank not in top-5)
[16:01:11] TRACE |   MISSED[2] Monitor X-RateLimit-Remaining header values
[16:01:11] TRACE | === END FACT COVERAGE ===
[16:01:11] TRACE | === FACT COVERAGE ANALYSIS ===
[16:01:11] TRACE | Query: What is the JWT token expiration time?
[16:01:11] TRACE | Total key facts: 3
[16:01:11] TRACE | Found facts: 2/3
[16:01:11] TRACE |   FOUND[1] 3600 seconds
[16:01:11] TRACE |   FOUND[2] All tokens expire after 3600 seconds
[16:01:11] TRACE | Missed facts: 1/3
[16:01:11] TRACE |   MISSED[1] max 3600 seconds from iat
[16:01:11] TRACE | === END FACT COVERAGE ===
[16:01:11] TRACE | === FACT COVERAGE ANALYSIS ===
[16:01:11] TRACE | Query: How long do access tokens last?
[16:01:11] TRACE | Total key facts: 3
[16:01:11] TRACE | Found facts: 2/3
[16:01:11] TRACE |   FOUND[1] 3600 seconds
[16:01:11] TRACE |   FOUND[2] All tokens expire after 3600 seconds
[16:01:11] TRACE | Missed facts: 1/3
[16:01:11] TRACE |   MISSED[1] max 3600 seconds from iat
[16:01:11] TRACE | === END FACT COVERAGE ===
[16:01:11] TRACE | === FACT COVERAGE ANALYSIS ===
[16:01:11] TRACE | Query: My authentication keeps expiring after an hour
[16:01:11] TRACE | Total key facts: 3
[16:01:11] TRACE | Found facts: 2/3
[16:01:11] TRACE |   FOUND[1] 3600 seconds
[16:01:11] TRACE |   FOUND[2] All tokens expire after 3600 seconds
[16:01:11] TRACE | Missed facts: 1/3
[16:01:11] TRACE |   MISSED[1] max 3600 seconds from iat
[16:01:11] TRACE | === END FACT COVERAGE ===
[16:01:11] TRACE | === FACT COVERAGE ANALYSIS ===
[16:01:11] TRACE | Query: token expiry time
[16:01:11] TRACE | Total key facts: 3
[16:01:11] TRACE | Found facts: 0/3
[16:01:11] TRACE | Missed facts: 3/3
[16:01:11] TRACE |   MISSED[1] 3600 seconds
[16:01:11] TRACE |     -> Found in chunk_id=api_reference_fix_0 (rank not in top-5)
[16:01:11] TRACE |   MISSED[2] max 3600 seconds from iat
[16:01:11] TRACE |   MISSED[3] All tokens expire after 3600 seconds
[16:01:11] TRACE |     -> Found in chunk_id=api_reference_fix_0 (rank not in top-5)
[16:01:11] TRACE | === END FACT COVERAGE ===
[16:01:11] TRACE | === FACT COVERAGE ANALYSIS ===
[16:01:11] TRACE | Query: I need to implement token refresh logic, when do JWT tokens expire?
[16:01:11] TRACE | Total key facts: 3
[16:01:11] TRACE | Found facts: 2/3
[16:01:11] TRACE |   FOUND[1] 3600 seconds
[16:01:11] TRACE |   FOUND[2] All tokens expire after 3600 seconds
[16:01:11] TRACE | Missed facts: 1/3
[16:01:11] TRACE |   MISSED[1] max 3600 seconds from iat
[16:01:11] TRACE | === END FACT COVERAGE ===
[16:01:11] TRACE | === FACT COVERAGE ANALYSIS ===
[16:01:11] TRACE | Query: Why does my token stop working after 3600 seconds?
[16:01:11] TRACE | Total key facts: 3
[16:01:11] TRACE | Found facts: 1/3
[16:01:11] TRACE |   FOUND[1] 3600 seconds
[16:01:11] TRACE | Missed facts: 2/3
[16:01:11] TRACE |   MISSED[1] max 3600 seconds from iat
[16:01:11] TRACE |   MISSED[2] All tokens expire after 3600 seconds
[16:01:11] TRACE |     -> Found in chunk_id=api_reference_fix_0 (rank not in top-5)
[16:01:11] TRACE | === END FACT COVERAGE ===
[16:01:11] TRACE | === FACT COVERAGE ANALYSIS ===
[16:01:11] TRACE | Query: What database technology does CloudFlow use?
[16:01:11] TRACE | Total key facts: 3
[16:01:11] TRACE | Found facts: 3/3
[16:01:11] TRACE |   FOUND[1] PostgreSQL 15.4
[16:01:11] TRACE |   FOUND[2] Redis 7.2
[16:01:11] TRACE |   FOUND[3] Apache Kafka 3.6
[16:01:11] TRACE | === END FACT COVERAGE ===
[16:01:11] TRACE | === FACT COVERAGE ANALYSIS ===
[16:01:11] TRACE | Query: Which database system powers CloudFlow?
[16:01:11] TRACE | Total key facts: 3
[16:01:11] TRACE | Found facts: 3/3
[16:01:11] TRACE |   FOUND[1] PostgreSQL 15.4
[16:01:11] TRACE |   FOUND[2] Redis 7.2
[16:01:11] TRACE |   FOUND[3] Apache Kafka 3.6
[16:01:11] TRACE | === END FACT COVERAGE ===
[16:01:11] TRACE | === FACT COVERAGE ANALYSIS ===
[16:01:11] TRACE | Query: I need to understand the data storage architecture
[16:01:11] TRACE | Total key facts: 3
[16:01:11] TRACE | Found facts: 3/3
[16:01:11] TRACE |   FOUND[1] PostgreSQL 15.4
[16:01:11] TRACE |   FOUND[2] Redis 7.2
[16:01:11] TRACE |   FOUND[3] Apache Kafka 3.6
[16:01:11] TRACE | === END FACT COVERAGE ===
[16:01:12] TRACE | === FACT COVERAGE ANALYSIS ===
[16:01:12] TRACE | Query: database stack
[16:01:12] TRACE | Total key facts: 3
[16:01:12] TRACE | Found facts: 0/3
[16:01:12] TRACE | Missed facts: 3/3
[16:01:12] TRACE |   MISSED[1] PostgreSQL 15.4
[16:01:12] TRACE |     -> Found in chunk_id=architecture_overview_fix_0 (rank not in top-5)
[16:01:12] TRACE |   MISSED[2] Redis 7.2
[16:01:12] TRACE |     -> Found in chunk_id=architecture_overview_fix_0 (rank not in top-5)
[16:01:12] TRACE |   MISSED[3] Apache Kafka 3.6
[16:01:12] TRACE |     -> Found in chunk_id=architecture_overview_fix_0 (rank not in top-5)
[16:01:12] TRACE | === END FACT COVERAGE ===
[16:01:12] TRACE | === FACT COVERAGE ANALYSIS ===
[16:01:12] TRACE | Query: For capacity planning, what databases does CloudFlow rely on?
[16:01:12] TRACE | Total key facts: 3
[16:01:12] TRACE | Found facts: 3/3
[16:01:12] TRACE |   FOUND[1] PostgreSQL 15.4
[16:01:12] TRACE |   FOUND[2] Redis 7.2
[16:01:12] TRACE |   FOUND[3] Apache Kafka 3.6
[16:01:12] TRACE | === END FACT COVERAGE ===
[16:01:12] TRACE | === FACT COVERAGE ANALYSIS ===
[16:01:12] TRACE | Query: Is CloudFlow using MySQL or something else?
[16:01:12] TRACE | Total key facts: 3
[16:01:12] TRACE | Found facts: 3/3
[16:01:12] TRACE |   FOUND[1] PostgreSQL 15.4
[16:01:12] TRACE |   FOUND[2] Redis 7.2
[16:01:12] TRACE |   FOUND[3] Apache Kafka 3.6
[16:01:12] TRACE | === END FACT COVERAGE ===
[16:01:12] TRACE | === FACT COVERAGE ANALYSIS ===
[16:01:12] TRACE | Query: What is the Kubernetes namespace for production deployment?
[16:01:12] TRACE | Total key facts: 1
[16:01:12] TRACE | Found facts: 1/1
[16:01:12] TRACE |   FOUND[1] cloudflow-prod
[16:01:12] TRACE | === END FACT COVERAGE ===
[16:01:12] TRACE | === FACT COVERAGE ANALYSIS ===
[16:01:12] TRACE | Query: Which namespace is used for CloudFlow production?
[16:01:12] TRACE | Total key facts: 1
[16:01:12] TRACE | Found facts: 1/1
[16:01:12] TRACE |   FOUND[1] cloudflow-prod
[16:01:12] TRACE | === END FACT COVERAGE ===
[16:01:12] TRACE | === FACT COVERAGE ANALYSIS ===
[16:01:12] TRACE | Query: I can't find the production pods
[16:01:12] TRACE | Total key facts: 1
[16:01:12] TRACE | Found facts: 0/1
[16:01:12] TRACE | Missed facts: 1/1
[16:01:12] TRACE |   MISSED[1] cloudflow-prod
[16:01:12] TRACE |     -> Found in chunk_id=deployment_guide_fix_0 (rank not in top-5)
[16:01:12] TRACE | === END FACT COVERAGE ===
[16:01:12] TRACE | === FACT COVERAGE ANALYSIS ===
[16:01:12] TRACE | Query: prod namespace
[16:01:12] TRACE | Total key facts: 1
[16:01:12] TRACE | Found facts: 1/1
[16:01:12] TRACE |   FOUND[1] cloudflow-prod
[16:01:12] TRACE | === END FACT COVERAGE ===
[16:01:12] TRACE | === FACT COVERAGE ANALYSIS ===
[16:01:12] TRACE | Query: I need to deploy to production, what namespace should I use?
[16:01:12] TRACE | Total key facts: 1
[16:01:12] TRACE | Found facts: 1/1
[16:01:12] TRACE |   FOUND[1] cloudflow-prod
[16:01:12] TRACE | === END FACT COVERAGE ===
[16:01:12] TRACE | === FACT COVERAGE ANALYSIS ===
[16:01:12] TRACE | Query: Why can't I see resources in the default namespace?
[16:01:12] TRACE | Total key facts: 1
[16:01:12] TRACE | Found facts: 1/1
[16:01:12] TRACE |   FOUND[1] cloudflow-prod
[16:01:12] TRACE | === END FACT COVERAGE ===
[16:01:12] TRACE | === FACT COVERAGE ANALYSIS ===
[16:01:12] TRACE | Query: What are the resource requirements for the API Gateway?
[16:01:12] TRACE | Total key facts: 1
[16:01:12] TRACE | Found facts: 0/1
[16:01:12] TRACE | Missed facts: 1/1
[16:01:12] TRACE |   MISSED[1] 2 vCPU, 4GB RAM per pod
[16:01:12] TRACE |     -> Found in chunk_id=architecture_overview_fix_1 (rank not in top-5)
[16:01:12] TRACE | === END FACT COVERAGE ===
[16:01:12] TRACE | === FACT COVERAGE ANALYSIS ===
[16:01:12] TRACE | Query: How much CPU and memory does the API Gateway need?
[16:01:12] TRACE | Total key facts: 1
[16:01:12] TRACE | Found facts: 1/1
[16:01:12] TRACE |   FOUND[1] 2 vCPU, 4GB RAM per pod
[16:01:12] TRACE | === END FACT COVERAGE ===
[16:01:12] TRACE | === FACT COVERAGE ANALYSIS ===
[16:01:12] TRACE | Query: The API Gateway pods keep getting OOMKilled
[16:01:12] TRACE | Total key facts: 1
[16:01:12] TRACE | Found facts: 1/1
[16:01:12] TRACE |   FOUND[1] 2 vCPU, 4GB RAM per pod
[16:01:12] TRACE | === END FACT COVERAGE ===
[16:01:12] TRACE | === FACT COVERAGE ANALYSIS ===
[16:01:12] TRACE | Query: api gateway resources
[16:01:12] TRACE | Total key facts: 1
[16:01:12] TRACE | Found facts: 0/1
[16:01:12] TRACE | Missed facts: 1/1
[16:01:12] TRACE |   MISSED[1] 2 vCPU, 4GB RAM per pod
[16:01:12] TRACE |     -> Found in chunk_id=architecture_overview_fix_1 (rank not in top-5)
[16:01:12] TRACE | === END FACT COVERAGE ===
[16:01:12] TRACE | === FACT COVERAGE ANALYSIS ===
[16:01:12] TRACE | Query: I'm provisioning infrastructure, what are the compute specs for API Gateway?
[16:01:12] TRACE | Total key facts: 1
[16:01:12] TRACE | Found facts: 0/1
[16:01:12] TRACE | Missed facts: 1/1
[16:01:12] TRACE |   MISSED[1] 2 vCPU, 4GB RAM per pod
[16:01:12] TRACE |     -> Found in chunk_id=architecture_overview_fix_1 (rank not in top-5)
[16:01:12] TRACE | === END FACT COVERAGE ===
[16:01:12] TRACE | === FACT COVERAGE ANALYSIS ===
[16:01:12] TRACE | Query: Why is 1GB RAM not enough for API Gateway?
[16:01:12] TRACE | Total key facts: 1
[16:01:12] TRACE | Found facts: 1/1
[16:01:12] TRACE |   FOUND[1] 2 vCPU, 4GB RAM per pod
[16:01:12] TRACE | === END FACT COVERAGE ===
[16:01:12] TRACE | === FACT COVERAGE ANALYSIS ===
[16:01:12] TRACE | Query: What are the health check endpoints?
[16:01:12] TRACE | Total key facts: 2
[16:01:12] TRACE | Found facts: 0/2
[16:01:12] TRACE | Missed facts: 2/2
[16:01:12] TRACE |   MISSED[1] /health
[16:01:12] TRACE |     -> Found in chunk_id=deployment_guide_fix_2 (rank not in top-5)
[16:01:12] TRACE |   MISSED[2] /ready
[16:01:12] TRACE |     -> Found in chunk_id=deployment_guide_fix_2 (rank not in top-5)
[16:01:12] TRACE | === END FACT COVERAGE ===
[16:01:12] TRACE | === FACT COVERAGE ANALYSIS ===
[16:01:12] TRACE | Query: Which URLs should I use for health monitoring?
[16:01:12] TRACE | Total key facts: 2
[16:01:12] TRACE | Found facts: 0/2
[16:01:12] TRACE | Missed facts: 2/2
[16:01:12] TRACE |   MISSED[1] /health
[16:01:12] TRACE |     -> Found in chunk_id=deployment_guide_fix_2 (rank not in top-5)
[16:01:12] TRACE |   MISSED[2] /ready
[16:01:12] TRACE |     -> Found in chunk_id=deployment_guide_fix_2 (rank not in top-5)
[16:01:12] TRACE | === END FACT COVERAGE ===
[16:01:12] TRACE | === FACT COVERAGE ANALYSIS ===
[16:01:12] TRACE | Query: My load balancer health checks are failing
[16:01:12] TRACE | Total key facts: 2
[16:01:12] TRACE | Found facts: 2/2
[16:01:12] TRACE |   FOUND[1] /health
[16:01:12] TRACE |   FOUND[2] /ready
[16:01:12] TRACE | === END FACT COVERAGE ===
[16:01:12] TRACE | === FACT COVERAGE ANALYSIS ===
[16:01:12] TRACE | Query: health endpoints
[16:01:12] TRACE | Total key facts: 2
[16:01:12] TRACE | Found facts: 0/2
[16:01:12] TRACE | Missed facts: 2/2
[16:01:12] TRACE |   MISSED[1] /health
[16:01:12] TRACE |     -> Found in chunk_id=deployment_guide_fix_2 (rank not in top-5)
[16:01:12] TRACE |   MISSED[2] /ready
[16:01:12] TRACE |     -> Found in chunk_id=deployment_guide_fix_2 (rank not in top-5)
[16:01:12] TRACE | === END FACT COVERAGE ===
[16:01:12] TRACE | === FACT COVERAGE ANALYSIS ===
[16:01:12] TRACE | Query: Setting up monitoring, what endpoints indicate service health?
[16:01:12] TRACE | Total key facts: 2
[16:01:12] TRACE | Found facts: 0/2
[16:01:12] TRACE | Missed facts: 2/2
[16:01:12] TRACE |   MISSED[1] /health
[16:01:12] TRACE |     -> Found in chunk_id=deployment_guide_fix_2 (rank not in top-5)
[16:01:12] TRACE |   MISSED[2] /ready
[16:01:12] TRACE |     -> Found in chunk_id=deployment_guide_fix_2 (rank not in top-5)
[16:01:12] TRACE | === END FACT COVERAGE ===
[16:01:12] TRACE | === FACT COVERAGE ANALYSIS ===
[16:01:12] TRACE | Query: Why doesn't /status return health information?
[16:01:12] TRACE | Total key facts: 2
[16:01:12] TRACE | Found facts: 2/2
[16:01:12] TRACE |   FOUND[1] /health
[16:01:12] TRACE |   FOUND[2] /ready
[16:01:12] TRACE | === END FACT COVERAGE ===
[16:01:12] TRACE | === FACT COVERAGE ANALYSIS ===
[16:01:12] TRACE | Query: What are the HPA scaling parameters?
[16:01:12] TRACE | Total key facts: 3
[16:01:12] TRACE | Found facts: 0/3
[16:01:12] TRACE | Missed facts: 3/3
[16:01:12] TRACE |   MISSED[1] minReplicas: 3
[16:01:12] TRACE |     -> Found in chunk_id=deployment_guide_fix_1 (rank not in top-5)
[16:01:12] TRACE |   MISSED[2] maxReplicas: 10
[16:01:12] TRACE |     -> Found in chunk_id=deployment_guide_fix_1 (rank not in top-5)
[16:01:12] TRACE |   MISSED[3] targetCPUUtilizationPercentage: 70
[16:01:12] TRACE |     -> Found in chunk_id=deployment_guide_fix_1 (rank not in top-5)
[16:01:12] TRACE | === END FACT COVERAGE ===
[16:01:12] TRACE | === FACT COVERAGE ANALYSIS ===
[16:01:12] TRACE | Query: How does horizontal pod autoscaling work?
[16:01:12] TRACE | Total key facts: 3
[16:01:12] TRACE | Found facts: 0/3
[16:01:12] TRACE | Missed facts: 3/3
[16:01:12] TRACE |   MISSED[1] minReplicas: 3
[16:01:12] TRACE |     -> Found in chunk_id=deployment_guide_fix_1 (rank not in top-5)
[16:01:12] TRACE |   MISSED[2] maxReplicas: 10
[16:01:12] TRACE |     -> Found in chunk_id=deployment_guide_fix_1 (rank not in top-5)
[16:01:12] TRACE |   MISSED[3] targetCPUUtilizationPercentage: 70
[16:01:12] TRACE |     -> Found in chunk_id=deployment_guide_fix_1 (rank not in top-5)
[16:01:12] TRACE | === END FACT COVERAGE ===
[16:01:12] TRACE | === FACT COVERAGE ANALYSIS ===
[16:01:12] TRACE | Query: Pods aren't scaling up during traffic spikes
[16:01:12] TRACE | Total key facts: 3
[16:01:12] TRACE | Found facts: 0/3
[16:01:12] TRACE | Missed facts: 3/3
[16:01:12] TRACE |   MISSED[1] minReplicas: 3
[16:01:12] TRACE |     -> Found in chunk_id=deployment_guide_fix_1 (rank not in top-5)
[16:01:12] TRACE |   MISSED[2] maxReplicas: 10
[16:01:12] TRACE |     -> Found in chunk_id=deployment_guide_fix_1 (rank not in top-5)
[16:01:12] TRACE |   MISSED[3] targetCPUUtilizationPercentage: 70
[16:01:12] TRACE |     -> Found in chunk_id=deployment_guide_fix_1 (rank not in top-5)
[16:01:12] TRACE | === END FACT COVERAGE ===
[16:01:12] TRACE | === FACT COVERAGE ANALYSIS ===
[16:01:12] TRACE | Query: autoscaling config
[16:01:12] TRACE | Total key facts: 3
[16:01:12] TRACE | Found facts: 3/3
[16:01:12] TRACE |   FOUND[1] minReplicas: 3
[16:01:12] TRACE |   FOUND[2] maxReplicas: 10
[16:01:12] TRACE |   FOUND[3] targetCPUUtilizationPercentage: 70
[16:01:12] TRACE | === END FACT COVERAGE ===
[16:01:12] TRACE | === FACT COVERAGE ANALYSIS ===
[16:01:12] TRACE | Query: I need to configure autoscaling, what are the min/max replicas and thresholds?
[16:01:12] TRACE | Total key facts: 3
[16:01:12] TRACE | Found facts: 0/3
[16:01:12] TRACE | Missed facts: 3/3
[16:01:12] TRACE |   MISSED[1] minReplicas: 3
[16:01:12] TRACE |     -> Found in chunk_id=deployment_guide_fix_1 (rank not in top-5)
[16:01:12] TRACE |   MISSED[2] maxReplicas: 10
[16:01:12] TRACE |     -> Found in chunk_id=deployment_guide_fix_1 (rank not in top-5)
[16:01:12] TRACE |   MISSED[3] targetCPUUtilizationPercentage: 70
[16:01:12] TRACE |     -> Found in chunk_id=deployment_guide_fix_1 (rank not in top-5)
[16:01:12] TRACE | === END FACT COVERAGE ===
[16:01:12] TRACE | === FACT COVERAGE ANALYSIS ===
[16:01:12] TRACE | Query: Why do we have 3 replicas even with low traffic?
[16:01:12] TRACE | Total key facts: 3
[16:01:12] TRACE | Found facts: 0/3
[16:01:12] TRACE | Missed facts: 3/3
[16:01:12] TRACE |   MISSED[1] minReplicas: 3
[16:01:12] TRACE |     -> Found in chunk_id=deployment_guide_fix_1 (rank not in top-5)
[16:01:12] TRACE |   MISSED[2] maxReplicas: 10
[16:01:12] TRACE |     -> Found in chunk_id=deployment_guide_fix_1 (rank not in top-5)
[16:01:12] TRACE |   MISSED[3] targetCPUUtilizationPercentage: 70
[16:01:12] TRACE |     -> Found in chunk_id=deployment_guide_fix_1 (rank not in top-5)
[16:01:12] TRACE | === END FACT COVERAGE ===
[16:01:12] TRACE | === FACT COVERAGE ANALYSIS ===
[16:01:12] TRACE | Query: What is the P99 latency target for API operations?
[16:01:12] TRACE | Total key facts: 2
[16:01:12] TRACE | Found facts: 2/2
[16:01:12] TRACE |   FOUND[1] P99 latency: < 200ms
[16:01:12] TRACE |   FOUND[2] average P99 latency of 180ms for API operations
[16:01:12] TRACE | === END FACT COVERAGE ===
[16:01:12] TRACE | === FACT COVERAGE ANALYSIS ===
[16:01:12] TRACE | Query: What's the 99th percentile response time target?
[16:01:12] TRACE | Total key facts: 2
[16:01:12] TRACE | Found facts: 1/2
[16:01:12] TRACE |   FOUND[1] P99 latency: < 200ms
[16:01:12] TRACE | Missed facts: 1/2
[16:01:12] TRACE |   MISSED[1] average P99 latency of 180ms for API operations
[16:01:12] TRACE |     -> Found in chunk_id=architecture_overview_fix_0 (rank not in top-5)
[16:01:12] TRACE | === END FACT COVERAGE ===
[16:01:12] TRACE | === FACT COVERAGE ANALYSIS ===
[16:01:12] TRACE | Query: Our API latency is 500ms, is that acceptable?
[16:01:12] TRACE | Total key facts: 2
[16:01:12] TRACE | Found facts: 0/2
[16:01:12] TRACE | Missed facts: 2/2
[16:01:12] TRACE |   MISSED[1] P99 latency: < 200ms
[16:01:12] TRACE |     -> Found in chunk_id=architecture_overview_fix_1 (rank not in top-5)
[16:01:12] TRACE |   MISSED[2] average P99 latency of 180ms for API operations
[16:01:12] TRACE |     -> Found in chunk_id=architecture_overview_fix_0 (rank not in top-5)
[16:01:12] TRACE | === END FACT COVERAGE ===
[16:01:12] TRACE | === FACT COVERAGE ANALYSIS ===
[16:01:12] TRACE | Query: api latency target
[16:01:12] TRACE | Total key facts: 2
[16:01:12] TRACE | Found facts: 1/2
[16:01:12] TRACE |   FOUND[1] P99 latency: < 200ms
[16:01:12] TRACE | Missed facts: 1/2
[16:01:12] TRACE |   MISSED[1] average P99 latency of 180ms for API operations
[16:01:12] TRACE |     -> Found in chunk_id=architecture_overview_fix_0 (rank not in top-5)
[16:01:12] TRACE | === END FACT COVERAGE ===
[16:01:12] TRACE | === FACT COVERAGE ANALYSIS ===
[16:01:12] TRACE | Query: Setting SLOs for our service, what P99 latency does CloudFlow guarantee?
[16:01:12] TRACE | Total key facts: 2
[16:01:12] TRACE | Found facts: 1/2
[16:01:12] TRACE |   FOUND[1] average P99 latency of 180ms for API operations
[16:01:12] TRACE | Missed facts: 1/2
[16:01:12] TRACE |   MISSED[1] P99 latency: < 200ms
[16:01:12] TRACE |     -> Found in chunk_id=architecture_overview_fix_1 (rank not in top-5)
[16:01:12] TRACE | === END FACT COVERAGE ===
[16:01:12] TRACE | === FACT COVERAGE ANALYSIS ===
[16:01:12] TRACE | Query: Why are we getting alerts at 200ms latency?
[16:01:12] TRACE | Total key facts: 2
[16:01:12] TRACE | Found facts: 0/2
[16:01:12] TRACE | Missed facts: 2/2
[16:01:12] TRACE |   MISSED[1] P99 latency: < 200ms
[16:01:12] TRACE |     -> Found in chunk_id=architecture_overview_fix_1 (rank not in top-5)
[16:01:12] TRACE |   MISSED[2] average P99 latency of 180ms for API operations
[16:01:12] TRACE |     -> Found in chunk_id=architecture_overview_fix_0 (rank not in top-5)
[16:01:12] TRACE | === END FACT COVERAGE ===
[16:01:12] TRACE | === FACT COVERAGE ANALYSIS ===
[16:01:12] TRACE | Query: What are the disaster recovery RPO and RTO values?
[16:01:12] TRACE | Total key facts: 2
[16:01:12] TRACE | Found facts: 0/2
[16:01:12] TRACE | Missed facts: 2/2
[16:01:12] TRACE |   MISSED[1] RPO (Recovery Point Objective): 1 hour
[16:01:12] TRACE |     -> Found in chunk_id=architecture_overview_fix_10 (rank not in top-5)
[16:01:12] TRACE |   MISSED[2] RTO (Recovery Time Objective): 4 hours
[16:01:12] TRACE |     -> Found in chunk_id=architecture_overview_fix_10 (rank not in top-5)
[16:01:12] TRACE | === END FACT COVERAGE ===
[16:01:12] TRACE | === FACT COVERAGE ANALYSIS ===
[16:01:12] TRACE | Query: What's the maximum data loss and recovery time?
[16:01:12] TRACE | Total key facts: 2
[16:01:12] TRACE | Found facts: 0/2
[16:01:12] TRACE | Missed facts: 2/2
[16:01:12] TRACE |   MISSED[1] RPO (Recovery Point Objective): 1 hour
[16:01:12] TRACE |     -> Found in chunk_id=architecture_overview_fix_10 (rank not in top-5)
[16:01:12] TRACE |   MISSED[2] RTO (Recovery Time Objective): 4 hours
[16:01:12] TRACE |     -> Found in chunk_id=architecture_overview_fix_10 (rank not in top-5)
[16:01:12] TRACE | === END FACT COVERAGE ===
[16:01:12] TRACE | === FACT COVERAGE ANALYSIS ===
[16:01:12] TRACE | Query: How long will it take to recover from a disaster?
[16:01:12] TRACE | Total key facts: 2
[16:01:12] TRACE | Found facts: 0/2
[16:01:12] TRACE | Missed facts: 2/2
[16:01:12] TRACE |   MISSED[1] RPO (Recovery Point Objective): 1 hour
[16:01:12] TRACE |     -> Found in chunk_id=architecture_overview_fix_10 (rank not in top-5)
[16:01:12] TRACE |   MISSED[2] RTO (Recovery Time Objective): 4 hours
[16:01:12] TRACE |     -> Found in chunk_id=architecture_overview_fix_10 (rank not in top-5)
[16:01:12] TRACE | === END FACT COVERAGE ===
[16:01:12] TRACE | === FACT COVERAGE ANALYSIS ===
[16:01:12] TRACE | Query: RPO RTO
[16:01:12] TRACE | Total key facts: 2
[16:01:12] TRACE | Found facts: 0/2
[16:01:12] TRACE | Missed facts: 2/2
[16:01:12] TRACE |   MISSED[1] RPO (Recovery Point Objective): 1 hour
[16:01:12] TRACE |     -> Found in chunk_id=architecture_overview_fix_10 (rank not in top-5)
[16:01:12] TRACE |   MISSED[2] RTO (Recovery Time Objective): 4 hours
[16:01:12] TRACE |     -> Found in chunk_id=architecture_overview_fix_10 (rank not in top-5)
[16:01:12] TRACE | === END FACT COVERAGE ===
[16:01:12] TRACE | === FACT COVERAGE ANALYSIS ===
[16:01:12] TRACE | Query: For our business continuity plan, what are CloudFlow's recovery objectives?
[16:01:12] TRACE | Total key facts: 2
[16:01:12] TRACE | Found facts: 0/2
[16:01:12] TRACE | Missed facts: 2/2
[16:01:12] TRACE |   MISSED[1] RPO (Recovery Point Objective): 1 hour
[16:01:12] TRACE |     -> Found in chunk_id=architecture_overview_fix_10 (rank not in top-5)
[16:01:12] TRACE |   MISSED[2] RTO (Recovery Time Objective): 4 hours
[16:01:12] TRACE |     -> Found in chunk_id=architecture_overview_fix_10 (rank not in top-5)
[16:01:12] TRACE | === END FACT COVERAGE ===
[16:01:12] TRACE | === FACT COVERAGE ANALYSIS ===
[16:01:12] TRACE | Query: Why can't we guarantee zero data loss?
[16:01:12] TRACE | Total key facts: 2
[16:01:12] TRACE | Found facts: 0/2
[16:01:12] TRACE | Missed facts: 2/2
[16:01:12] TRACE |   MISSED[1] RPO (Recovery Point Objective): 1 hour
[16:01:12] TRACE |     -> Found in chunk_id=architecture_overview_fix_10 (rank not in top-5)
[16:01:12] TRACE |   MISSED[2] RTO (Recovery Time Objective): 4 hours
[16:01:12] TRACE |     -> Found in chunk_id=architecture_overview_fix_10 (rank not in top-5)
[16:01:12] TRACE | === END FACT COVERAGE ===
[16:01:12] TRACE | === FACT COVERAGE ANALYSIS ===
[16:01:12] TRACE | Query: What is the maximum workflow execution timeout?
[16:01:12] TRACE | Total key facts: 2
[16:01:12] TRACE | Found facts: 1/2
[16:01:12] TRACE |   FOUND[1] 3600 seconds
[16:01:12] TRACE | Missed facts: 1/2
[16:01:12] TRACE |   MISSED[1] exceeded maximum execution time of 3600 seconds
[16:01:12] TRACE |     -> Found in chunk_id=troubleshooting_guide_fix_4 (rank not in top-5)
[16:01:12] TRACE | === END FACT COVERAGE ===
[16:01:12] TRACE | === FACT COVERAGE ANALYSIS ===
[16:01:12] TRACE | Query: How long can a workflow run before timing out?
[16:01:12] TRACE | Total key facts: 2
[16:01:12] TRACE | Found facts: 0/2
[16:01:12] TRACE | Missed facts: 2/2
[16:01:12] TRACE |   MISSED[1] 3600 seconds
[16:01:12] TRACE |     -> Found in chunk_id=api_reference_fix_0 (rank not in top-5)
[16:01:12] TRACE |   MISSED[2] exceeded maximum execution time of 3600 seconds
[16:01:12] TRACE |     -> Found in chunk_id=troubleshooting_guide_fix_4 (rank not in top-5)
[16:01:12] TRACE | === END FACT COVERAGE ===
[16:01:12] TRACE | === FACT COVERAGE ANALYSIS ===
[16:01:12] TRACE | Query: My workflow is being killed after an hour
[16:01:12] TRACE | Total key facts: 2
[16:01:12] TRACE | Found facts: 0/2
[16:01:12] TRACE | Missed facts: 2/2
[16:01:12] TRACE |   MISSED[1] 3600 seconds
[16:01:12] TRACE |     -> Found in chunk_id=api_reference_fix_0 (rank not in top-5)
[16:01:12] TRACE |   MISSED[2] exceeded maximum execution time of 3600 seconds
[16:01:12] TRACE |     -> Found in chunk_id=troubleshooting_guide_fix_4 (rank not in top-5)
[16:01:12] TRACE | === END FACT COVERAGE ===
[16:01:12] TRACE | === FACT COVERAGE ANALYSIS ===
[16:01:12] TRACE | Query: workflow timeout
[16:01:12] TRACE | Total key facts: 2
[16:01:12] TRACE | Found facts: 1/2
[16:01:12] TRACE |   FOUND[1] 3600 seconds
[16:01:12] TRACE | Missed facts: 1/2
[16:01:12] TRACE |   MISSED[1] exceeded maximum execution time of 3600 seconds
[16:01:12] TRACE |     -> Found in chunk_id=troubleshooting_guide_fix_4 (rank not in top-5)
[16:01:12] TRACE | === END FACT COVERAGE ===
[16:01:12] TRACE | === FACT COVERAGE ANALYSIS ===
[16:01:12] TRACE | Query: I have a long-running data processing workflow, what's the time limit?
[16:01:12] TRACE | Total key facts: 2
[16:01:12] TRACE | Found facts: 0/2
[16:01:12] TRACE | Missed facts: 2/2
[16:01:12] TRACE |   MISSED[1] 3600 seconds
[16:01:12] TRACE |     -> Found in chunk_id=api_reference_fix_0 (rank not in top-5)
[16:01:12] TRACE |   MISSED[2] exceeded maximum execution time of 3600 seconds
[16:01:12] TRACE |     -> Found in chunk_id=troubleshooting_guide_fix_4 (rank not in top-5)
[16:01:12] TRACE | === END FACT COVERAGE ===
[16:01:12] TRACE | === FACT COVERAGE ANALYSIS ===
[16:01:12] TRACE | Query: Why did my workflow fail after 3600 seconds?
[16:01:12] TRACE | Total key facts: 2
[16:01:12] TRACE | Found facts: 0/2
[16:01:12] TRACE | Missed facts: 2/2
[16:01:12] TRACE |   MISSED[1] 3600 seconds
[16:01:12] TRACE |     -> Found in chunk_id=api_reference_fix_0 (rank not in top-5)
[16:01:12] TRACE |   MISSED[2] exceeded maximum execution time of 3600 seconds
[16:01:12] TRACE |     -> Found in chunk_id=troubleshooting_guide_fix_4 (rank not in top-5)
[16:01:12] TRACE | === END FACT COVERAGE ===
[16:01:12] TRACE | === FACT COVERAGE ANALYSIS ===
[16:01:12] TRACE | Query: What JWT algorithm is used for token signing?
[16:01:12] TRACE | Total key facts: 3
[16:01:12] TRACE | Found facts: 3/3
[16:01:12] TRACE |   FOUND[1] RS256
[16:01:12] TRACE |   FOUND[2] RS256 algorithm
[16:01:12] TRACE |   FOUND[3] RS256 signing algorithm
[16:01:12] TRACE | === END FACT COVERAGE ===
[16:01:12] TRACE | === FACT COVERAGE ANALYSIS ===
[16:01:12] TRACE | Query: Which signing algorithm does CloudFlow use for JWTs?
[16:01:12] TRACE | Total key facts: 3
[16:01:12] TRACE | Found facts: 2/3
[16:01:12] TRACE |   FOUND[1] RS256
[16:01:12] TRACE |   FOUND[2] RS256 signing algorithm
[16:01:12] TRACE | Missed facts: 1/3
[16:01:12] TRACE |   MISSED[1] RS256 algorithm
[16:01:12] TRACE |     -> Found in chunk_id=architecture_overview_fix_1 (rank not in top-5)
[16:01:12] TRACE | === END FACT COVERAGE ===
[16:01:12] TRACE | === FACT COVERAGE ANALYSIS ===
[16:01:12] TRACE | Query: My JWT validation is failing with algorithm mismatch
[16:01:12] TRACE | Total key facts: 3
[16:01:12] TRACE | Found facts: 3/3
[16:01:12] TRACE |   FOUND[1] RS256
[16:01:12] TRACE |   FOUND[2] RS256 algorithm
[16:01:12] TRACE |   FOUND[3] RS256 signing algorithm
[16:01:12] TRACE | === END FACT COVERAGE ===
[16:01:12] TRACE | === FACT COVERAGE ANALYSIS ===
[16:01:12] TRACE | Query: jwt algorithm
[16:01:12] TRACE | Total key facts: 3
[16:01:12] TRACE | Found facts: 3/3
[16:01:12] TRACE |   FOUND[1] RS256
[16:01:12] TRACE |   FOUND[2] RS256 algorithm
[16:01:12] TRACE |   FOUND[3] RS256 signing algorithm
[16:01:12] TRACE | === END FACT COVERAGE ===
[16:01:12] TRACE | === FACT COVERAGE ANALYSIS ===
[16:01:12] TRACE | Query: I'm implementing JWT verification, what algorithm should I expect?
[16:01:12] TRACE | Total key facts: 3
[16:01:12] TRACE | Found facts: 3/3
[16:01:12] TRACE |   FOUND[1] RS256
[16:01:12] TRACE |   FOUND[2] RS256 algorithm
[16:01:12] TRACE |   FOUND[3] RS256 signing algorithm
[16:01:12] TRACE | === END FACT COVERAGE ===
[16:01:12] TRACE | === FACT COVERAGE ANALYSIS ===
[16:01:12] TRACE | Query: Why doesn't HS256 work for token validation?
[16:01:12] TRACE | Total key facts: 3
[16:01:12] TRACE | Found facts: 3/3
[16:01:12] TRACE |   FOUND[1] RS256
[16:01:12] TRACE |   FOUND[2] RS256 algorithm
[16:01:12] TRACE |   FOUND[3] RS256 signing algorithm
[16:01:12] TRACE | === END FACT COVERAGE ===
[16:01:12] TRACE | === FACT COVERAGE ANALYSIS ===
[16:01:12] TRACE | Query: What is the Redis cache TTL for workflow definitions?
[16:01:12] TRACE | Total key facts: 2
[16:01:12] TRACE | Found facts: 1/2
[16:01:12] TRACE |   FOUND[1] TTL: 1 hour
[16:01:12] TRACE | Missed facts: 1/2
[16:01:12] TRACE |   MISSED[1] Workflow Definitions: TTL: 1 hour
[16:01:12] TRACE | === END FACT COVERAGE ===
[16:01:12] TRACE | === FACT COVERAGE ANALYSIS ===
[16:01:12] TRACE | Query: How long are workflow definitions cached?
[16:01:12] TRACE | Total key facts: 2
[16:01:12] TRACE | Found facts: 0/2
[16:01:12] TRACE | Missed facts: 2/2
[16:01:12] TRACE |   MISSED[1] TTL: 1 hour
[16:01:12] TRACE |     -> Found in chunk_id=architecture_overview_fix_6 (rank not in top-5)
[16:01:12] TRACE |   MISSED[2] Workflow Definitions: TTL: 1 hour
[16:01:12] TRACE | === END FACT COVERAGE ===
[16:01:12] TRACE | === FACT COVERAGE ANALYSIS ===
[16:01:12] TRACE | Query: My workflow updates aren't reflecting immediately
[16:01:12] TRACE | Total key facts: 2
[16:01:12] TRACE | Found facts: 0/2
[16:01:12] TRACE | Missed facts: 2/2
[16:01:12] TRACE |   MISSED[1] TTL: 1 hour
[16:01:12] TRACE |     -> Found in chunk_id=architecture_overview_fix_6 (rank not in top-5)
[16:01:12] TRACE |   MISSED[2] Workflow Definitions: TTL: 1 hour
[16:01:12] TRACE | === END FACT COVERAGE ===
[16:01:12] TRACE | === FACT COVERAGE ANALYSIS ===
[16:01:12] TRACE | Query: cache ttl workflows
[16:01:12] TRACE | Total key facts: 2
[16:01:12] TRACE | Found facts: 1/2
[16:01:12] TRACE |   FOUND[1] TTL: 1 hour
[16:01:12] TRACE | Missed facts: 1/2
[16:01:12] TRACE |   MISSED[1] Workflow Definitions: TTL: 1 hour
[16:01:12] TRACE | === END FACT COVERAGE ===
[16:01:12] TRACE | === FACT COVERAGE ANALYSIS ===
[16:01:12] TRACE | Query: After updating a workflow, how long until the cache expires?
[16:01:12] TRACE | Total key facts: 2
[16:01:12] TRACE | Found facts: 0/2
[16:01:12] TRACE | Missed facts: 2/2
[16:01:12] TRACE |   MISSED[1] TTL: 1 hour
[16:01:12] TRACE |     -> Found in chunk_id=architecture_overview_fix_6 (rank not in top-5)
[16:01:12] TRACE |   MISSED[2] Workflow Definitions: TTL: 1 hour
[16:01:12] TRACE | === END FACT COVERAGE ===
[16:01:12] TRACE | === FACT COVERAGE ANALYSIS ===
[16:01:12] TRACE | Query: Why are changes taking an hour to appear?
[16:01:12] TRACE | Total key facts: 2
[16:01:12] TRACE | Found facts: 0/2
[16:01:12] TRACE | Missed facts: 2/2
[16:01:12] TRACE |   MISSED[1] TTL: 1 hour
[16:01:12] TRACE |     -> Found in chunk_id=architecture_overview_fix_6 (rank not in top-5)
[16:01:12] TRACE |   MISSED[2] Workflow Definitions: TTL: 1 hour
[16:01:12] TRACE | === END FACT COVERAGE ===
[16:01:12] TRACE | === FACT COVERAGE ANALYSIS ===
[16:01:12] TRACE | Query: What monitoring tools does CloudFlow use?
[16:01:12] TRACE | Total key facts: 3
[16:01:12] TRACE | Found facts: 3/3
[16:01:12] TRACE |   FOUND[1] Prometheus
[16:01:12] TRACE |   FOUND[2] Grafana
[16:01:12] TRACE |   FOUND[3] Jaeger for distributed tracing
[16:01:12] TRACE | === END FACT COVERAGE ===
[16:01:12] TRACE | === FACT COVERAGE ANALYSIS ===
[16:01:12] TRACE | Query: Which observability platform is integrated?
[16:01:12] TRACE | Total key facts: 3
[16:01:12] TRACE | Found facts: 3/3
[16:01:12] TRACE |   FOUND[1] Prometheus
[16:01:12] TRACE |   FOUND[2] Grafana
[16:01:12] TRACE |   FOUND[3] Jaeger for distributed tracing
[16:01:12] TRACE | === END FACT COVERAGE ===
[16:01:12] TRACE | === FACT COVERAGE ANALYSIS ===
[16:01:12] TRACE | Query: Where can I view CloudFlow metrics and logs?
[16:01:12] TRACE | Total key facts: 3
[16:01:12] TRACE | Found facts: 3/3
[16:01:12] TRACE |   FOUND[1] Prometheus
[16:01:12] TRACE |   FOUND[2] Grafana
[16:01:12] TRACE |   FOUND[3] Jaeger for distributed tracing
[16:01:12] TRACE | === END FACT COVERAGE ===
[16:01:12] TRACE | === FACT COVERAGE ANALYSIS ===
[16:01:12] TRACE | Query: monitoring stack
[16:01:12] TRACE | Total key facts: 3
[16:01:12] TRACE | Found facts: 0/3
[16:01:12] TRACE | Missed facts: 3/3
[16:01:12] TRACE |   MISSED[1] Prometheus
[16:01:12] TRACE |     -> Found in chunk_id=architecture_overview_fix_0 (rank not in top-5)
[16:01:12] TRACE |   MISSED[2] Grafana
[16:01:12] TRACE |     -> Found in chunk_id=architecture_overview_fix_0 (rank not in top-5)
[16:01:12] TRACE |   MISSED[3] Jaeger for distributed tracing
[16:01:12] TRACE |     -> Found in chunk_id=architecture_overview_fix_0 (rank not in top-5)
[16:01:12] TRACE | === END FACT COVERAGE ===
[16:01:12] TRACE | === FACT COVERAGE ANALYSIS ===
[16:01:12] TRACE | Query: I need to set up dashboards, what monitoring systems are available?
[16:01:12] TRACE | Total key facts: 3
[16:01:12] TRACE | Found facts: 0/3
[16:01:12] TRACE | Missed facts: 3/3
[16:01:12] TRACE |   MISSED[1] Prometheus
[16:01:12] TRACE |     -> Found in chunk_id=architecture_overview_fix_0 (rank not in top-5)
[16:01:12] TRACE |   MISSED[2] Grafana
[16:01:12] TRACE |     -> Found in chunk_id=architecture_overview_fix_0 (rank not in top-5)
[16:01:12] TRACE |   MISSED[3] Jaeger for distributed tracing
[16:01:12] TRACE |     -> Found in chunk_id=architecture_overview_fix_0 (rank not in top-5)
[16:01:12] TRACE | === END FACT COVERAGE ===
[16:01:12] TRACE | === FACT COVERAGE ANALYSIS ===
[16:01:12] TRACE | Query: Why can't I see metrics in Datadog?
[16:01:12] TRACE | Total key facts: 3
[16:01:12] TRACE | Found facts: 2/3
[16:01:12] TRACE |   FOUND[1] Prometheus
[16:01:12] TRACE |   FOUND[2] Grafana
[16:01:12] TRACE | Missed facts: 1/3
[16:01:12] TRACE |   MISSED[1] Jaeger for distributed tracing
[16:01:12] TRACE |     -> Found in chunk_id=architecture_overview_fix_0 (rank not in top-5)
[16:01:12] TRACE | === END FACT COVERAGE ===
[16:01:12] TRACE | === FACT COVERAGE ANALYSIS ===
[16:01:12] TRACE | Query: How do I diagnose database connection pool exhaustion?
[16:01:12] TRACE | Total key facts: 4
[16:01:12] TRACE | Found facts: 1/4
[16:01:12] TRACE |   FOUND[1] PgBouncer
[16:01:12] TRACE | Missed facts: 3/4
[16:01:12] TRACE |   MISSED[1] could not obtain connection from pool within 5000ms
[16:01:12] TRACE |     -> Found in chunk_id=troubleshooting_guide_fix_3 (rank not in top-5)
[16:01:12] TRACE |   MISSED[2] connection pool exhausted (100/100 connections in use)
[16:01:12] TRACE |     -> Found in chunk_id=troubleshooting_guide_fix_3 (rank not in top-5)
[16:01:12] TRACE |   MISSED[3] max_db_connections = 100
[16:01:12] TRACE |     -> Found in chunk_id=deployment_guide_fix_3 (rank not in top-5)
[16:01:12] TRACE | === END FACT COVERAGE ===
[16:01:12] TRACE | === FACT COVERAGE ANALYSIS ===
[16:01:12] TRACE | Query: What should I check when I run out of database connections?
[16:01:12] TRACE | Total key facts: 4
[16:01:12] TRACE | Found facts: 1/4
[16:01:12] TRACE |   FOUND[1] PgBouncer
[16:01:12] TRACE | Missed facts: 3/4
[16:01:12] TRACE |   MISSED[1] could not obtain connection from pool within 5000ms
[16:01:12] TRACE |     -> Found in chunk_id=troubleshooting_guide_fix_3 (rank not in top-5)
[16:01:12] TRACE |   MISSED[2] connection pool exhausted (100/100 connections in use)
[16:01:12] TRACE |     -> Found in chunk_id=troubleshooting_guide_fix_3 (rank not in top-5)
[16:01:12] TRACE |   MISSED[3] max_db_connections = 100
[16:01:12] TRACE |     -> Found in chunk_id=deployment_guide_fix_3 (rank not in top-5)
[16:01:12] TRACE | === END FACT COVERAGE ===
[16:01:12] TRACE | === FACT COVERAGE ANALYSIS ===
[16:01:12] TRACE | Query: Getting 'could not obtain connection from pool' errors
[16:01:12] TRACE | Total key facts: 4
[16:01:12] TRACE | Found facts: 1/4
[16:01:12] TRACE |   FOUND[1] PgBouncer
[16:01:12] TRACE | Missed facts: 3/4
[16:01:12] TRACE |   MISSED[1] could not obtain connection from pool within 5000ms
[16:01:12] TRACE |     -> Found in chunk_id=troubleshooting_guide_fix_3 (rank not in top-5)
[16:01:12] TRACE |   MISSED[2] connection pool exhausted (100/100 connections in use)
[16:01:12] TRACE |     -> Found in chunk_id=troubleshooting_guide_fix_3 (rank not in top-5)
[16:01:12] TRACE |   MISSED[3] max_db_connections = 100
[16:01:12] TRACE |     -> Found in chunk_id=deployment_guide_fix_3 (rank not in top-5)
[16:01:12] TRACE | === END FACT COVERAGE ===
[16:01:13] TRACE | === FACT COVERAGE ANALYSIS ===
[16:01:13] TRACE | Query: connection pool full
[16:01:13] TRACE | Total key facts: 4
[16:01:13] TRACE | Found facts: 1/4
[16:01:13] TRACE |   FOUND[1] PgBouncer
[16:01:13] TRACE | Missed facts: 3/4
[16:01:13] TRACE |   MISSED[1] could not obtain connection from pool within 5000ms
[16:01:13] TRACE |     -> Found in chunk_id=troubleshooting_guide_fix_3 (rank not in top-5)
[16:01:13] TRACE |   MISSED[2] connection pool exhausted (100/100 connections in use)
[16:01:13] TRACE |     -> Found in chunk_id=troubleshooting_guide_fix_3 (rank not in top-5)
[16:01:13] TRACE |   MISSED[3] max_db_connections = 100
[16:01:13] TRACE |     -> Found in chunk_id=deployment_guide_fix_3 (rank not in top-5)
[16:01:13] TRACE | === END FACT COVERAGE ===
[16:01:13] TRACE | === FACT COVERAGE ANALYSIS ===
[16:01:13] TRACE | Query: My app is failing with connection pool errors, how do I troubleshoot?
[16:01:13] TRACE | Total key facts: 4
[16:01:13] TRACE | Found facts: 1/4
[16:01:13] TRACE |   FOUND[1] PgBouncer
[16:01:13] TRACE | Missed facts: 3/4
[16:01:13] TRACE |   MISSED[1] could not obtain connection from pool within 5000ms
[16:01:13] TRACE |     -> Found in chunk_id=troubleshooting_guide_fix_3 (rank not in top-5)
[16:01:13] TRACE |   MISSED[2] connection pool exhausted (100/100 connections in use)
[16:01:13] TRACE |     -> Found in chunk_id=troubleshooting_guide_fix_3 (rank not in top-5)
[16:01:13] TRACE |   MISSED[3] max_db_connections = 100
[16:01:13] TRACE |     -> Found in chunk_id=deployment_guide_fix_3 (rank not in top-5)
[16:01:13] TRACE | === END FACT COVERAGE ===
[16:01:13] TRACE | === FACT COVERAGE ANALYSIS ===
[16:01:13] TRACE | Query: Why can't I get a database connection even though CPU is low?
[16:01:13] TRACE | Total key facts: 4
[16:01:13] TRACE | Found facts: 0/4
[16:01:13] TRACE | Missed facts: 4/4
[16:01:13] TRACE |   MISSED[1] could not obtain connection from pool within 5000ms
[16:01:13] TRACE |     -> Found in chunk_id=troubleshooting_guide_fix_3 (rank not in top-5)
[16:01:13] TRACE |   MISSED[2] connection pool exhausted (100/100 connections in use)
[16:01:13] TRACE |     -> Found in chunk_id=troubleshooting_guide_fix_3 (rank not in top-5)
[16:01:13] TRACE |   MISSED[3] PgBouncer
[16:01:13] TRACE |     -> Found in chunk_id=architecture_overview_fix_5 (rank not in top-5)
[16:01:13] TRACE |   MISSED[4] max_db_connections = 100
[16:01:13] TRACE |     -> Found in chunk_id=deployment_guide_fix_3 (rank not in top-5)
[16:01:13] TRACE | === END FACT COVERAGE ===
[16:01:13] TRACE | === FACT COVERAGE ANALYSIS ===
[16:01:13] TRACE | Query: How do I handle API authentication?
[16:01:13] TRACE | Total key facts: 3
[16:01:13] TRACE | Found facts: 3/3
[16:01:13] TRACE |   FOUND[1] OAuth 2.0
[16:01:13] TRACE |   FOUND[2] API keys
[16:01:13] TRACE |   FOUND[3] JWT tokens
[16:01:13] TRACE | === END FACT COVERAGE ===
[16:01:13] TRACE | === FACT COVERAGE ANALYSIS ===
[16:01:13] TRACE | Query: What authentication methods are supported?
[16:01:13] TRACE | Total key facts: 3
[16:01:13] TRACE | Found facts: 3/3
[16:01:13] TRACE |   FOUND[1] OAuth 2.0
[16:01:13] TRACE |   FOUND[2] API keys
[16:01:13] TRACE |   FOUND[3] JWT tokens
[16:01:13] TRACE | === END FACT COVERAGE ===
[16:01:13] TRACE | === FACT COVERAGE ANALYSIS ===
[16:01:13] TRACE | Query: My API requests are getting 401 errors
[16:01:13] TRACE | Total key facts: 3
[16:01:13] TRACE | Found facts: 3/3
[16:01:13] TRACE |   FOUND[1] OAuth 2.0
[16:01:13] TRACE |   FOUND[2] API keys
[16:01:13] TRACE |   FOUND[3] JWT tokens
[16:01:13] TRACE | === END FACT COVERAGE ===
[16:01:13] TRACE | === FACT COVERAGE ANALYSIS ===
[16:01:13] TRACE | Query: auth methods
[16:01:13] TRACE | Total key facts: 3
[16:01:13] TRACE | Found facts: 3/3
[16:01:13] TRACE |   FOUND[1] OAuth 2.0
[16:01:13] TRACE |   FOUND[2] API keys
[16:01:13] TRACE |   FOUND[3] JWT tokens
[16:01:13] TRACE | === END FACT COVERAGE ===
[16:01:13] TRACE | === FACT COVERAGE ANALYSIS ===
[16:01:13] TRACE | Query: I'm integrating with CloudFlow API, what authentication options do I have?
[16:01:13] TRACE | Total key facts: 3
[16:01:13] TRACE | Found facts: 3/3
[16:01:13] TRACE |   FOUND[1] OAuth 2.0
[16:01:13] TRACE |   FOUND[2] API keys
[16:01:13] TRACE |   FOUND[3] JWT tokens
[16:01:13] TRACE | === END FACT COVERAGE ===
[16:01:13] TRACE | === FACT COVERAGE ANALYSIS ===
[16:01:13] TRACE | Query: Why isn't basic auth working?
[16:01:13] TRACE | Total key facts: 3
[16:01:13] TRACE | Found facts: 3/3
[16:01:13] TRACE |   FOUND[1] OAuth 2.0
[16:01:13] TRACE |   FOUND[2] API keys
[16:01:13] TRACE |   FOUND[3] JWT tokens
[16:01:13] TRACE | === END FACT COVERAGE ===
[16:01:13] TRACE | === FACT COVERAGE ANALYSIS ===
[16:01:13] TRACE | Query: What is PgBouncer and why is it used in CloudFlow?
[16:01:13] TRACE | Total key facts: 5
[16:01:13] TRACE | Found facts: 5/5
[16:01:13] TRACE |   FOUND[1] PgBouncer
[16:01:13] TRACE |   FOUND[2] connection pooling
[16:01:13] TRACE |   FOUND[3] max_db_connections = 100
[16:01:13] TRACE |   FOUND[4] default_pool_size = 25
[16:01:13] TRACE |   FOUND[5] pool_mode = transaction
[16:01:13] TRACE | === END FACT COVERAGE ===
[16:01:13] TRACE | === FACT COVERAGE ANALYSIS ===
[16:01:13] TRACE | Query: What's the purpose of the connection pooler?
[16:01:13] TRACE | Total key facts: 5
[16:01:13] TRACE | Found facts: 0/5
[16:01:13] TRACE | Missed facts: 5/5
[16:01:13] TRACE |   MISSED[1] PgBouncer
[16:01:13] TRACE |     -> Found in chunk_id=architecture_overview_fix_5 (rank not in top-5)
[16:01:13] TRACE |   MISSED[2] connection pooling
[16:01:13] TRACE |     -> Found in chunk_id=architecture_overview_fix_5 (rank not in top-5)
[16:01:13] TRACE |   MISSED[3] max_db_connections = 100
[16:01:13] TRACE |     -> Found in chunk_id=deployment_guide_fix_3 (rank not in top-5)
[16:01:13] TRACE |   MISSED[4] default_pool_size = 25
[16:01:13] TRACE |     -> Found in chunk_id=deployment_guide_fix_3 (rank not in top-5)
[16:01:13] TRACE |   MISSED[5] pool_mode = transaction
[16:01:13] TRACE |     -> Found in chunk_id=deployment_guide_fix_3 (rank not in top-5)
[16:01:13] TRACE | === END FACT COVERAGE ===
[16:01:13] TRACE | === FACT COVERAGE ANALYSIS ===
[16:01:13] TRACE | Query: Should I connect directly to PostgreSQL or through PgBouncer?
[16:01:13] TRACE | Total key facts: 5
[16:01:13] TRACE | Found facts: 5/5
[16:01:13] TRACE |   FOUND[1] PgBouncer
[16:01:13] TRACE |   FOUND[2] connection pooling
[16:01:13] TRACE |   FOUND[3] max_db_connections = 100
[16:01:13] TRACE |   FOUND[4] default_pool_size = 25
[16:01:13] TRACE |   FOUND[5] pool_mode = transaction
[16:01:13] TRACE | === END FACT COVERAGE ===
[16:01:13] TRACE | === FACT COVERAGE ANALYSIS ===
[16:01:13] TRACE | Query: pgbouncer purpose
[16:01:13] TRACE | Total key facts: 5
[16:01:13] TRACE | Found facts: 5/5
[16:01:13] TRACE |   FOUND[1] PgBouncer
[16:01:13] TRACE |   FOUND[2] connection pooling
[16:01:13] TRACE |   FOUND[3] max_db_connections = 100
[16:01:13] TRACE |   FOUND[4] default_pool_size = 25
[16:01:13] TRACE |   FOUND[5] pool_mode = transaction
[16:01:13] TRACE | === END FACT COVERAGE ===
[16:01:13] TRACE | === FACT COVERAGE ANALYSIS ===
[16:01:13] TRACE | Query: Optimizing database connections, what role does PgBouncer play?
[16:01:13] TRACE | Total key facts: 5
[16:01:13] TRACE | Found facts: 5/5
[16:01:13] TRACE |   FOUND[1] PgBouncer
[16:01:13] TRACE |   FOUND[2] connection pooling
[16:01:13] TRACE |   FOUND[3] max_db_connections = 100
[16:01:13] TRACE |   FOUND[4] default_pool_size = 25
[16:01:13] TRACE |   FOUND[5] pool_mode = transaction
[16:01:13] TRACE | === END FACT COVERAGE ===
[16:01:13] TRACE | === FACT COVERAGE ANALYSIS ===
[16:01:13] TRACE | Query: Why can't I connect directly to the database?
[16:01:13] TRACE | Total key facts: 5
[16:01:13] TRACE | Found facts: 2/5
[16:01:13] TRACE |   FOUND[1] PgBouncer
[16:01:13] TRACE |   FOUND[2] connection pooling
[16:01:13] TRACE | Missed facts: 3/5
[16:01:13] TRACE |   MISSED[1] max_db_connections = 100
[16:01:13] TRACE |     -> Found in chunk_id=deployment_guide_fix_3 (rank not in top-5)
[16:01:13] TRACE |   MISSED[2] default_pool_size = 25
[16:01:13] TRACE |     -> Found in chunk_id=deployment_guide_fix_3 (rank not in top-5)
[16:01:13] TRACE |   MISSED[3] pool_mode = transaction
[16:01:13] TRACE |     -> Found in chunk_id=deployment_guide_fix_3 (rank not in top-5)
[16:01:13] TRACE | === END FACT COVERAGE ===
[16:01:13] TRACE | === FACT COVERAGE ANALYSIS ===
[16:01:13] TRACE | Query: How do I implement retry logic for failed workflow steps?
[16:01:13] TRACE | Total key facts: 4
[16:01:13] TRACE | Found facts: 0/4
[16:01:13] TRACE | Missed facts: 4/4
[16:01:13] TRACE |   MISSED[1] max_attempts: 3
[16:01:13] TRACE |     -> Found in chunk_id=user_guide_fix_5 (rank not in top-5)
[16:01:13] TRACE |   MISSED[2] backoff_type: exponential
[16:01:13] TRACE |     -> Found in chunk_id=user_guide_fix_7 (rank not in top-5)
[16:01:13] TRACE |   MISSED[3] initial_interval: 1000
[16:01:13] TRACE |     -> Found in chunk_id=user_guide_fix_5 (rank not in top-5)
[16:01:13] TRACE |   MISSED[4] max retries: 3
[16:01:13] TRACE |     -> Found in chunk_id=troubleshooting_guide_fix_5 (rank not in top-5)
[16:01:13] TRACE | === END FACT COVERAGE ===
[16:01:13] TRACE | === FACT COVERAGE ANALYSIS ===
[16:01:13] TRACE | Query: What's the retry strategy for transient failures?
[16:01:13] TRACE | Total key facts: 4
[16:01:13] TRACE | Found facts: 0/4
[16:01:13] TRACE | Missed facts: 4/4
[16:01:13] TRACE |   MISSED[1] max_attempts: 3
[16:01:13] TRACE |     -> Found in chunk_id=user_guide_fix_5 (rank not in top-5)
[16:01:13] TRACE |   MISSED[2] backoff_type: exponential
[16:01:13] TRACE |     -> Found in chunk_id=user_guide_fix_7 (rank not in top-5)
[16:01:13] TRACE |   MISSED[3] initial_interval: 1000
[16:01:13] TRACE |     -> Found in chunk_id=user_guide_fix_5 (rank not in top-5)
[16:01:13] TRACE |   MISSED[4] max retries: 3
[16:01:13] TRACE |     -> Found in chunk_id=troubleshooting_guide_fix_5 (rank not in top-5)
[16:01:13] TRACE | === END FACT COVERAGE ===
[16:01:13] TRACE | === FACT COVERAGE ANALYSIS ===
[16:01:13] TRACE | Query: My workflow fails on temporary network errors
[16:01:13] TRACE | Total key facts: 4
[16:01:13] TRACE | Found facts: 1/4
[16:01:13] TRACE |   FOUND[1] max retries: 3
[16:01:13] TRACE | Missed facts: 3/4
[16:01:13] TRACE |   MISSED[1] max_attempts: 3
[16:01:13] TRACE |     -> Found in chunk_id=user_guide_fix_5 (rank not in top-5)
[16:01:13] TRACE |   MISSED[2] backoff_type: exponential
[16:01:13] TRACE |     -> Found in chunk_id=user_guide_fix_7 (rank not in top-5)
[16:01:13] TRACE |   MISSED[3] initial_interval: 1000
[16:01:13] TRACE |     -> Found in chunk_id=user_guide_fix_5 (rank not in top-5)
[16:01:13] TRACE | === END FACT COVERAGE ===
[16:01:13] TRACE | === FACT COVERAGE ANALYSIS ===
[16:01:13] TRACE | Query: retry config
[16:01:13] TRACE | Total key facts: 4
[16:01:13] TRACE | Found facts: 2/4
[16:01:13] TRACE |   FOUND[1] max_attempts: 3
[16:01:13] TRACE |   FOUND[2] initial_interval: 1000
[16:01:13] TRACE | Missed facts: 2/4
[16:01:13] TRACE |   MISSED[1] backoff_type: exponential
[16:01:13] TRACE |     -> Found in chunk_id=user_guide_fix_7 (rank not in top-5)
[16:01:13] TRACE |   MISSED[2] max retries: 3
[16:01:13] TRACE |     -> Found in chunk_id=troubleshooting_guide_fix_5 (rank not in top-5)
[16:01:13] TRACE | === END FACT COVERAGE ===
[16:01:13] TRACE | === FACT COVERAGE ANALYSIS ===
[16:01:13] TRACE | Query: I want workflows to automatically retry on errors, what are the options?
[16:01:13] TRACE | Total key facts: 4
[16:01:13] TRACE | Found facts: 0/4
[16:01:13] TRACE | Missed facts: 4/4
[16:01:13] TRACE |   MISSED[1] max_attempts: 3
[16:01:13] TRACE |     -> Found in chunk_id=user_guide_fix_5 (rank not in top-5)
[16:01:13] TRACE |   MISSED[2] backoff_type: exponential
[16:01:13] TRACE |     -> Found in chunk_id=user_guide_fix_7 (rank not in top-5)
[16:01:13] TRACE |   MISSED[3] initial_interval: 1000
[16:01:13] TRACE |     -> Found in chunk_id=user_guide_fix_5 (rank not in top-5)
[16:01:13] TRACE |   MISSED[4] max retries: 3
[16:01:13] TRACE |     -> Found in chunk_id=troubleshooting_guide_fix_5 (rank not in top-5)
[16:01:13] TRACE | === END FACT COVERAGE ===
[16:01:13] TRACE | === FACT COVERAGE ANALYSIS ===
[16:01:13] TRACE | Query: Why doesn't my workflow retry after failing?
[16:01:13] TRACE | Total key facts: 4
[16:01:13] TRACE | Found facts: 0/4
[16:01:13] TRACE | Missed facts: 4/4
[16:01:13] TRACE |   MISSED[1] max_attempts: 3
[16:01:13] TRACE |     -> Found in chunk_id=user_guide_fix_5 (rank not in top-5)
[16:01:13] TRACE |   MISSED[2] backoff_type: exponential
[16:01:13] TRACE |     -> Found in chunk_id=user_guide_fix_7 (rank not in top-5)
[16:01:13] TRACE |   MISSED[3] initial_interval: 1000
[16:01:13] TRACE |     -> Found in chunk_id=user_guide_fix_5 (rank not in top-5)
[16:01:13] TRACE |   MISSED[4] max retries: 3
[16:01:13] TRACE |     -> Found in chunk_id=troubleshooting_guide_fix_5 (rank not in top-5)
[16:01:13] TRACE | === END FACT COVERAGE ===
[16:01:13] TRACE | === FACT COVERAGE ANALYSIS ===
[16:01:13] TRACE | Query: What Helm chart repository should I use for deployment?
[16:01:13] TRACE | Total key facts: 1
[16:01:13] TRACE | Found facts: 0/1
[16:01:13] TRACE | Missed facts: 1/1
[16:01:13] TRACE |   MISSED[1] helm repo add cloudflow https://charts.cloudflow.io
[16:01:13] TRACE |     -> Found in chunk_id=deployment_guide_fix_1 (rank not in top-5)
[16:01:13] TRACE | === END FACT COVERAGE ===
[16:01:13] TRACE | === FACT COVERAGE ANALYSIS ===
[16:01:13] TRACE | Query: Where is the CloudFlow Helm chart hosted?
[16:01:13] TRACE | Total key facts: 1
[16:01:13] TRACE | Found facts: 0/1
[16:01:13] TRACE | Missed facts: 1/1
[16:01:13] TRACE |   MISSED[1] helm repo add cloudflow https://charts.cloudflow.io
[16:01:13] TRACE |     -> Found in chunk_id=deployment_guide_fix_1 (rank not in top-5)
[16:01:13] TRACE | === END FACT COVERAGE ===
[16:01:13] TRACE | === FACT COVERAGE ANALYSIS ===
[16:01:13] TRACE | Query: helm repo add is failing, what's the correct URL?
[16:01:13] TRACE | Total key facts: 1
[16:01:13] TRACE | Found facts: 1/1
[16:01:13] TRACE |   FOUND[1] helm repo add cloudflow https://charts.cloudflow.io
[16:01:13] TRACE | === END FACT COVERAGE ===
[16:01:13] TRACE | === FACT COVERAGE ANALYSIS ===
[16:01:13] TRACE | Query: helm repo
[16:01:13] TRACE | Total key facts: 1
[16:01:13] TRACE | Found facts: 1/1
[16:01:13] TRACE |   FOUND[1] helm repo add cloudflow https://charts.cloudflow.io
[16:01:13] TRACE | === END FACT COVERAGE ===
[16:01:13] TRACE | === FACT COVERAGE ANALYSIS ===
[16:01:13] TRACE | Query: Setting up deployment pipeline, which Helm repository has CloudFlow charts?
[16:01:13] TRACE | Total key facts: 1
[16:01:13] TRACE | Found facts: 0/1
[16:01:13] TRACE | Missed facts: 1/1
[16:01:13] TRACE |   MISSED[1] helm repo add cloudflow https://charts.cloudflow.io
[16:01:13] TRACE |     -> Found in chunk_id=deployment_guide_fix_1 (rank not in top-5)
[16:01:13] TRACE | === END FACT COVERAGE ===
[16:01:13] TRACE | === FACT COVERAGE ANALYSIS ===
[16:01:13] TRACE | Query: Why can't I find CloudFlow in the official Helm hub?
[16:01:13] TRACE | Total key facts: 1
[16:01:13] TRACE | Found facts: 1/1
[16:01:13] TRACE |   FOUND[1] helm repo add cloudflow https://charts.cloudflow.io
[16:01:13] TRACE | === END FACT COVERAGE ===
[16:01:13] TRACE | === FACT COVERAGE ANALYSIS ===
[16:01:13] TRACE | Query: What is the minimum scheduling interval for workflows?
[16:01:13] TRACE | Total key facts: 2
[16:01:13] TRACE | Found facts: 0/2
[16:01:13] TRACE | Missed facts: 2/2
[16:01:13] TRACE |   MISSED[1] minimum scheduling interval is 1 minute
[16:01:13] TRACE |   MISSED[2] The minimum scheduling interval is **1 minute**
[16:01:13] TRACE |     -> Found in chunk_id=user_guide_fix_4 (rank not in top-5)
[16:01:13] TRACE | === END FACT COVERAGE ===
[16:01:13] TRACE | === FACT COVERAGE ANALYSIS ===
[16:01:13] TRACE | Query: How frequently can I schedule a workflow?
[16:01:13] TRACE | Total key facts: 2
[16:01:13] TRACE | Found facts: 0/2
[16:01:13] TRACE | Missed facts: 2/2
[16:01:13] TRACE |   MISSED[1] minimum scheduling interval is 1 minute
[16:01:13] TRACE |   MISSED[2] The minimum scheduling interval is **1 minute**
[16:01:13] TRACE |     -> Found in chunk_id=user_guide_fix_4 (rank not in top-5)
[16:01:13] TRACE | === END FACT COVERAGE ===
[16:01:13] TRACE | === FACT COVERAGE ANALYSIS ===
[16:01:13] TRACE | Query: My every-30-seconds schedule is being rejected
[16:01:13] TRACE | Total key facts: 2
[16:01:13] TRACE | Found facts: 0/2
[16:01:13] TRACE | Missed facts: 2/2
[16:01:13] TRACE |   MISSED[1] minimum scheduling interval is 1 minute
[16:01:13] TRACE |   MISSED[2] The minimum scheduling interval is **1 minute**
[16:01:13] TRACE |     -> Found in chunk_id=user_guide_fix_4 (rank not in top-5)
[16:01:13] TRACE | === END FACT COVERAGE ===
[16:01:13] TRACE | === FACT COVERAGE ANALYSIS ===
[16:01:13] TRACE | Query: min schedule interval
[16:01:13] TRACE | Total key facts: 2
[16:01:13] TRACE | Found facts: 0/2
[16:01:13] TRACE | Missed facts: 2/2
[16:01:13] TRACE |   MISSED[1] minimum scheduling interval is 1 minute
[16:01:13] TRACE |   MISSED[2] The minimum scheduling interval is **1 minute**
[16:01:13] TRACE |     -> Found in chunk_id=user_guide_fix_4 (rank not in top-5)
[16:01:13] TRACE | === END FACT COVERAGE ===
[16:01:13] TRACE | === FACT COVERAGE ANALYSIS ===
[16:01:13] TRACE | Query: I need near real-time execution, what's the fastest schedule I can set?
[16:01:13] TRACE | Total key facts: 2
[16:01:13] TRACE | Found facts: 0/2
[16:01:13] TRACE | Missed facts: 2/2
[16:01:13] TRACE |   MISSED[1] minimum scheduling interval is 1 minute
[16:01:13] TRACE |   MISSED[2] The minimum scheduling interval is **1 minute**
[16:01:13] TRACE |     -> Found in chunk_id=user_guide_fix_4 (rank not in top-5)
[16:01:13] TRACE | === END FACT COVERAGE ===
[16:01:13] TRACE | === FACT COVERAGE ANALYSIS ===
[16:01:13] TRACE | Query: Why can't I schedule workflows every 30 seconds?
[16:01:13] TRACE | Total key facts: 2
[16:01:13] TRACE | Found facts: 0/2
[16:01:13] TRACE | Missed facts: 2/2
[16:01:13] TRACE |   MISSED[1] minimum scheduling interval is 1 minute
[16:01:13] TRACE |   MISSED[2] The minimum scheduling interval is **1 minute**
[16:01:13] TRACE |     -> Found in chunk_id=user_guide_fix_4 (rank not in top-5)
[16:01:13] TRACE | === END FACT COVERAGE ===
[16:01:13] INFO  |   k=5 exact_match: 56.6% (30/53)
[16:01:13] INFO  |     synonym: 41.5%
[16:01:13] INFO  |     problem: 56.6%
[16:01:13] INFO  |     casual: 50.9%
[16:01:13] INFO  |     contextual: 45.3%
[16:01:13] INFO  |     negation: 45.3%

================================================================================
[16:01:13] INFO  | RETRIEVAL STRATEGY: bmx_wqa
================================================================================
[16:01:13] INFO  | Loading embedder: BAAI/bge-base-en-v1.5
[16:01:15] PROG  | [  2/  4] ██████████░░░░░░░░░░  50.0% | bmx_wqa_bge-base-en-v1.5_fixed_512_0pct
[16:01:15] DEBUG | Chunking document 'api_reference': 1792 words, 16663 chars
[16:01:15] DEBUG |   Chunk 0: chars [0-3160] words=376 tokens≈500 preview='# CloudFlow API Reference Version 2.1.0 | Last Updated: January 2026 ## Overview The CloudFlow API i...'
[16:01:15] DEBUG |   Chunk 1: chars [3161-6111] words=379 tokens≈504 preview='## Rate Limiting To ensure fair usage and system stability, CloudFlow enforces rate limits on all AP...'
[16:01:15] DEBUG |   Chunk 2: chars [6112-8827] words=296 tokens≈393 preview='**Endpoint:** `POST /workflows` **Request Body:** ```json { "name": "Email Campaign Automation", "de...'
[16:01:15] DEBUG |   Chunk 3: chars [8828-11402] words=269 tokens≈357 preview='**Endpoint:** `GET /pipelines/{pipeline_id}/executions` **Query Parameters:** - `status` (optional):...'
[16:01:15] DEBUG |   Chunk 4: chars [11403-14151] words=365 tokens≈485 preview='### Error Response Format ```json { "error": { "code": "invalid_parameter", "message": "The 'limit' ...'
[16:01:15] DEBUG |   Chunk 5: chars [14152-15213] words=107 tokens≈142 preview='**Supported Events:** - `workflow.started` - `workflow.completed` - `workflow.failed` - `pipeline.co...'
[16:01:15] DEBUG | Created 6 chunks for document 'api_reference'
[16:01:15] DEBUG | Chunking document 'architecture_overview': 4607 words, 36722 chars
[16:01:15] DEBUG |   Chunk 0: chars [0-3230] words=370 tokens≈492 preview='# CloudFlow Platform - System Architecture Overview **Document Version:** 2.3.1 **Last Updated:** Ja...'
[16:01:15] DEBUG |   Chunk 1: chars [3231-5982] words=380 tokens≈505 preview='**Technology**: Node.js with Express.js framework **Replicas**: 12 pods (production), auto-scaling 8...'
[16:01:15] DEBUG |   Chunk 2: chars [5983-7875] words=248 tokens≈329 preview='**Technology**: Node.js with TypeScript, Bull queue library **Replicas**: 16 pods (production), auto...'
[16:01:15] DEBUG |   Chunk 3: chars [7876-9478] words=219 tokens≈291 preview='**Technology**: Go with distributed locking via Redis **Replicas**: 4 pods (production), active-pass...'
[16:01:15] DEBUG |   Chunk 4: chars [9479-11878] words=354 tokens≈470 preview='**Technology**: Node.js with worker pool pattern **Replicas**: 8 pods (production), auto-scaling 6-1...'
[16:01:15] DEBUG |   Chunk 5: chars [11879-14634] words=368 tokens≈489 preview='Success/failure events published back to Kafka ``` **Event Schema**: ```json { "event_id": "uuid-v4"...'
[16:01:15] DEBUG |   Chunk 6: chars [14635-17394] words=384 tokens≈510 preview='**Session Storage**: - Key pattern: `session:{user_id}` - TTL: 15 minutes (aligned with JWT expiry) ...'
[16:01:15] DEBUG |   Chunk 7: chars [17395-20141] words=381 tokens≈506 preview='records: 500 - Session timeout: 30 seconds --- ## Message Queue Patterns ### Pub/Sub Pattern Used fo...'
[16:01:15] DEBUG |   Chunk 8: chars [20142-22843] words=384 tokens≈510 preview='### Cache Invalidation Strategies **Time-Based Expiration (TTL)**: - Short-lived data: 5-15 minutes ...'
[16:01:15] DEBUG |   Chunk 9: chars [22844-25362] words=384 tokens≈510 preview='per API key (default: 1000 RPM) ### Secrets Management **HashiCorp Vault Integration**: - Dynamic da...'
[16:01:15] DEBUG |   Chunk 10: chars [25363-27842] words=384 tokens≈510 preview='- Execution start rate: 500 per second - Completion rate: 450 per second (average 2-second execution...'
[16:01:15] DEBUG |   Chunk 11: chars [27843-30286] words=375 tokens≈498 preview='Service (2) - PostgreSQL Primary - PostgreSQL Replica - PostgreSQL Replica - Redis Primary (2) - Red...'
[16:01:15] DEBUG |   Chunk 12: chars [30287-32834] words=376 tokens≈500 preview='Resume normal operations - Recovery time: 1-3 hours depending on data volume - Data loss: None if co...'
[16:01:15] DEBUG | Created 13 chunks for document 'architecture_overview'
[16:01:15] DEBUG | Chunking document 'deployment_guide': 2777 words, 25837 chars
[16:01:15] DEBUG |   Chunk 0: chars [0-3165] words=384 tokens≈510 preview='# CloudFlow Platform - Deployment and Operations Guide **Version:** 2.4.0 **Last Updated:** January ...'
[16:01:15] DEBUG |   Chunk 1: chars [3166-6560] words=384 tokens≈510 preview='create cluster -f cluster-config.yaml ``` ### Storage Configuration Install the EBS CSI driver for p...'
[16:01:15] DEBUG |   Chunk 2: chars [6561-8755] words=251 tokens≈333 preview='- secretName: cloudflow-tls hosts: - api.cloudflow.io healthCheck: enabled: true livenessProbe: path...'
[16:01:15] DEBUG |   Chunk 3: chars [8756-12138] words=384 tokens≈510 preview='Deploy PostgreSQL using the Bitnami Helm chart: ```yaml # postgres-values.yaml global: postgresql: a...'
[16:01:15] DEBUG |   Chunk 4: chars [12139-14139] words=204 tokens≈271 preview='--set prometheus.prometheusSpec.retention=30d \ --set prometheus.prometheusSpec.storageSpec.volumeCl...'
[16:01:15] DEBUG |   Chunk 5: chars [14140-17194] words=372 tokens≈494 preview='Configure log aggregation: ```bash # Install Fluent Bit for log forwarding helm install fluent-bit f...'
[16:01:15] DEBUG |   Chunk 6: chars [17195-20069] words=384 tokens≈510 preview='**Verify Service Health**: ```bash kubectl get pods -n cloudflow-prod curl https://api.cloudflow.io/...'
[16:01:15] DEBUG |   Chunk 7: chars [20070-23221] words=384 tokens≈510 preview='podSelector: matchLabels: app: cloudflow policyTypes: - Ingress - Egress ingress: - from: - namespac...'
[16:01:15] DEBUG |   Chunk 8: chars [23222-23448] words=30 tokens≈39 preview='UTC - **Secondary**: Wednesday 10:00-12:00 UTC All deployments should target these windows to minimi...'
[16:01:15] DEBUG | Created 9 chunks for document 'deployment_guide'
[16:01:15] DEBUG | Chunking document 'troubleshooting_guide': 4032 words, 32183 chars
[16:01:15] DEBUG |   Chunk 0: chars [0-2666] words=320 tokens≈425 preview='# CloudFlow Platform Troubleshooting Guide **Version:** 3.2.1 **Last Updated:** January 2026 **Audie...'
[16:01:15] DEBUG |   Chunk 1: chars [2667-5007] words=321 tokens≈426 preview='**Diagnosis:** ```bash # Check system time timedatectl status # Compare with NTP server ntpdate -q p...'
[16:01:15] DEBUG |   Chunk 2: chars [5008-8037] words=376 tokens≈500 preview='Review Query Execution Plans** ```sql -- Connect to CloudFlow database cloudflow db connect --readon...'
[16:01:15] DEBUG |   Chunk 3: chars [8038-10862] words=356 tokens≈473 preview='Workflow Context Accumulation** Large workflow executions may accumulate state in memory. **Solution...'
[16:01:15] DEBUG |   Chunk 4: chars [10863-13827] words=384 tokens≈510 preview='**Implement connection pooling optimization:** ```bash # Use PgBouncer for connection pooling kubect...'
[16:01:15] DEBUG |   Chunk 5: chars [13828-16401] words=334 tokens≈444 preview='exec_7h3j6k9m2n --show-bottlenecks ``` #### Solutions **1. Increase workflow timeout (if justified):...'
[16:01:15] DEBUG |   Chunk 6: chars [16402-19022] words=329 tokens≈437 preview='Data Validation Errors** ``` ValidationError: Field 'customer_id' is required but missing in 234 rec...'
[16:01:15] DEBUG |   Chunk 7: chars [19023-20595] words=194 tokens≈258 preview='Waiting {retry_after} seconds...") time.sleep(retry_after) continue remaining = int(response.headers...'
[16:01:15] DEBUG |   Chunk 8: chars [20596-23454] words=384 tokens≈510 preview='Leverage caching:** ```bash # Enable client-side caching export CLOUDFLOW_CACHE_ENABLED=true export ...'
[16:01:15] DEBUG |   Chunk 9: chars [23455-26239] words=384 tokens≈510 preview='in" # Find workflow failures by ID kubectl logs -n cloudflow deployment/cloudflow-workflow-engine | ...'
[16:01:15] DEBUG |   Chunk 10: chars [26240-29039] words=384 tokens≈510 preview='database nc -zv cloudflow-db.internal.company.com 5432 # Trace route traceroute api.cloudflow.io # C...'
[16:01:15] DEBUG |   Chunk 11: chars [29040-31206] words=266 tokens≈353 preview='outage" # Create war room cloudflow incident war-room create incident-2024012401 ``` **For SEV-2 (Hi...'
[16:01:15] DEBUG | Created 12 chunks for document 'troubleshooting_guide'
[16:01:15] DEBUG | Chunking document 'user_guide': 3735 words, 31582 chars
[16:01:15] DEBUG |   Chunk 0: chars [0-2617] words=375 tokens≈498 preview='# CloudFlow User Guide Welcome to CloudFlow, the modern workflow automation platform that helps you ...'
[16:01:15] DEBUG |   Chunk 1: chars [2618-5224] words=348 tokens≈462 preview='Open the Visual Editor by clicking **"Create Workflow"** or editing an existing workflow 2. Add a tr...'
[16:01:15] DEBUG |   Chunk 2: chars [5225-8372] words=381 tokens≈506 preview='### HTTP Requests Make HTTP requests to any API endpoint: **Configuration:** - **Method**: GET, POST...'
[16:01:15] DEBUG |   Chunk 3: chars [8373-11268] words=336 tokens≈446 preview='``` **Slack App Integration:** - Install the CloudFlow Slack app in your workspace - Authenticate on...'
[16:01:15] DEBUG |   Chunk 4: chars [11269-13371] words=375 tokens≈498 preview='### Cron Syntax Use standard cron expressions to define schedules: ``` * * * * * ┬ ┬ ┬ ┬ ┬ │ │ │ │ │...'
[16:01:15] DEBUG |   Chunk 5: chars [13372-16105] words=383 tokens≈509 preview='Save and activate **Testing Schedules:** Use the built-in schedule calculator to preview upcoming ex...'
[16:01:15] DEBUG |   Chunk 6: chars [16106-18686] words=363 tokens≈482 preview='Filter by error type, date range, or execution ID 3. Click an execution to view full details 4. Clic...'
[16:01:15] DEBUG |   Chunk 7: chars [18687-21448] words=364 tokens≈484 preview='Handle Errors Gracefully Always implement error handling for external API calls and database operati...'
[16:01:15] DEBUG |   Chunk 8: chars [21449-24677] words=384 tokens≈510 preview='Version Control For critical workflows: - Export YAML definitions regularly - Store in version contr...'
[16:01:15] DEBUG |   Chunk 9: chars [24678-27975] words=371 tokens≈493 preview='Requester: {{trigger.body.requester}} Description: {{trigger.body.description}} Approve: https://app...'
[16:01:15] DEBUG |   Chunk 10: chars [27976-28481] words=55 tokens≈73 preview='- **Documentation**: https://docs.cloudflow.io - **Community Forum**: https://community.cloudflow.io...'
[16:01:15] DEBUG | Created 11 chunks for document 'user_guide'
[16:01:15] INFO  |   Created 51 chunks
    [enriched] chunks=51 new=0 cache_hits=51 time=0.0s avg=0.0ms/chunk
[16:01:17] INFO  |   Indexed in 1.74s
[16:01:17] DEBUG | [provider] Creating anthropic provider for model=claude-haiku
[16:01:17] DEBUG | [anthropic] Loading auth from /home/fujin/.local/share/opencode/auth.json
[16:01:17] DEBUG | [anthropic] Auth loaded, token expires in 128.7 minutes
[16:01:17] DEBUG | [anthropic] Model alias: claude-haiku -> claude-3-5-haiku-latest
[16:01:17] DEBUG | [anthropic] model=claude-3-5-haiku-latest prompt_len=837 timeout=5s
[16:01:17] DEBUG | [anthropic] PROMPT: You are a technical documentation search expert. Your task is to rewrite user questions as direct documentation lookup queries.

Guidelines:
1. Convert problem descriptions to feature/capability questions
2. Expand abbreviations and acronyms to full terms
3. Replace casual language with technical terminology
4. Align with documentation vocabulary and structure
5. Keep the rewritten query concise (one line)

Examples:
- "Why can't I schedule workflows every 30 seconds?" → "workflow scheduling min...
[16:01:17] DEBUG | [anthropic] REQUEST: POST https://api.anthropic.com/v1/messages
[16:01:17] DEBUG | [anthropic] REQUEST body: model=claude-3-5-haiku-latest max_tokens=1024
[16:01:18] DEBUG | [anthropic] RESPONSE status=200 elapsed=0.97s
[16:01:18] DEBUG | [anthropic] USAGE: input_tokens=195 output_tokens=13
[16:01:18] DEBUG | [anthropic] SUCCESS stop_reason=end_turn response_len=54
[16:01:18] DEBUG | [anthropic] RESPONSE: "api rate limit per minute maximum requests threshold"
[16:01:18] TRACE | === FACT COVERAGE ANALYSIS ===
[16:01:18] TRACE | Query: What is the API rate limit per minute?
[16:01:18] TRACE | Total key facts: 2
[16:01:18] TRACE | Found facts: 2/2
[16:01:18] TRACE |   FOUND[1] 100 requests per minute per authenticated user
[16:01:18] TRACE |   FOUND[2] 20 requests per minute for unauthenticated requests
[16:01:18] TRACE | === END FACT COVERAGE ===
[16:01:18] DEBUG | [anthropic] Model alias: claude-haiku -> claude-3-5-haiku-latest
[16:01:18] DEBUG | [anthropic] model=claude-3-5-haiku-latest prompt_len=850 timeout=5s
[16:01:18] DEBUG | [anthropic] PROMPT: You are a technical documentation search expert. Your task is to rewrite user questions as direct documentation lookup queries.

Guidelines:
1. Convert problem descriptions to feature/capability questions
2. Expand abbreviations and acronyms to full terms
3. Replace casual language with technical terminology
4. Align with documentation vocabulary and structure
5. Keep the rewritten query concise (one line)

Examples:
- "Why can't I schedule workflows every 30 seconds?" → "workflow scheduling min...
[16:01:18] DEBUG | [anthropic] REQUEST: POST https://api.anthropic.com/v1/messages
[16:01:18] DEBUG | [anthropic] REQUEST body: model=claude-3-5-haiku-latest max_tokens=1024
[16:01:20] DEBUG | [anthropic] RESPONSE status=200 elapsed=1.07s
[16:01:20] DEBUG | [anthropic] USAGE: input_tokens=198 output_tokens=10
[16:01:20] DEBUG | [anthropic] SUCCESS stop_reason=end_turn response_len=43
[16:01:20] DEBUG | [anthropic] RESPONSE: api rate limit request frequency per minute
[16:01:20] TRACE | === FACT COVERAGE ANALYSIS ===
[16:01:20] TRACE | Query: How many requests can I make per minute to the API?
[16:01:20] TRACE | Total key facts: 2
[16:01:20] TRACE | Found facts: 2/2
[16:01:20] TRACE |   FOUND[1] 100 requests per minute per authenticated user
[16:01:20] TRACE |   FOUND[2] 20 requests per minute for unauthenticated requests
[16:01:20] TRACE | === END FACT COVERAGE ===
[16:01:20] DEBUG | [anthropic] Model alias: claude-haiku -> claude-3-5-haiku-latest
[16:01:20] DEBUG | [anthropic] model=claude-3-5-haiku-latest prompt_len=850 timeout=5s
[16:01:20] DEBUG | [anthropic] PROMPT: You are a technical documentation search expert. Your task is to rewrite user questions as direct documentation lookup queries.

Guidelines:
1. Convert problem descriptions to feature/capability questions
2. Expand abbreviations and acronyms to full terms
3. Replace casual language with technical terminology
4. Align with documentation vocabulary and structure
5. Keep the rewritten query concise (one line)

Examples:
- "Why can't I schedule workflows every 30 seconds?" → "workflow scheduling min...
[16:01:20] DEBUG | [anthropic] REQUEST: POST https://api.anthropic.com/v1/messages
[16:01:20] DEBUG | [anthropic] REQUEST body: model=claude-3-5-haiku-latest max_tokens=1024
[16:01:21] DEBUG | [anthropic] RESPONSE status=200 elapsed=0.88s
[16:01:21] DEBUG | [anthropic] USAGE: input_tokens=198 output_tokens=10
[16:01:21] DEBUG | [anthropic] SUCCESS stop_reason=end_turn response_len=40
[16:01:21] DEBUG | [anthropic] RESPONSE: API request rate limit throttling policy
[16:01:21] TRACE | === FACT COVERAGE ANALYSIS ===
[16:01:21] TRACE | Query: My API calls are getting blocked, what's the limit?
[16:01:21] TRACE | Total key facts: 2
[16:01:21] TRACE | Found facts: 2/2
[16:01:21] TRACE |   FOUND[1] 100 requests per minute per authenticated user
[16:01:21] TRACE |   FOUND[2] 20 requests per minute for unauthenticated requests
[16:01:21] TRACE | === END FACT COVERAGE ===
[16:01:21] DEBUG | [anthropic] Model alias: claude-haiku -> claude-3-5-haiku-latest
[16:01:21] DEBUG | [anthropic] model=claude-3-5-haiku-latest prompt_len=813 timeout=5s
[16:01:21] DEBUG | [anthropic] PROMPT: You are a technical documentation search expert. Your task is to rewrite user questions as direct documentation lookup queries.

Guidelines:
1. Convert problem descriptions to feature/capability questions
2. Expand abbreviations and acronyms to full terms
3. Replace casual language with technical terminology
4. Align with documentation vocabulary and structure
5. Keep the rewritten query concise (one line)

Examples:
- "Why can't I schedule workflows every 30 seconds?" → "workflow scheduling min...
[16:01:21] DEBUG | [anthropic] REQUEST: POST https://api.anthropic.com/v1/messages
[16:01:21] DEBUG | [anthropic] REQUEST body: model=claude-3-5-haiku-latest max_tokens=1024
[16:01:22] DEBUG | [anthropic] RESPONSE status=200 elapsed=1.10s
[16:01:22] DEBUG | [anthropic] USAGE: input_tokens=189 output_tokens=12
[16:01:22] DEBUG | [anthropic] SUCCESS stop_reason=end_turn response_len=64
[16:01:22] DEBUG | [anthropic] RESPONSE: api rate limiting configuration maximum requests per time period
[16:01:23] TRACE | === FACT COVERAGE ANALYSIS ===
[16:01:23] TRACE | Query: api rate limit
[16:01:23] TRACE | Total key facts: 2
[16:01:23] TRACE | Found facts: 2/2
[16:01:23] TRACE |   FOUND[1] 100 requests per minute per authenticated user
[16:01:23] TRACE |   FOUND[2] 20 requests per minute for unauthenticated requests
[16:01:23] TRACE | === END FACT COVERAGE ===
[16:01:23] DEBUG | [anthropic] Model alias: claude-haiku -> claude-3-5-haiku-latest
[16:01:23] DEBUG | [anthropic] model=claude-3-5-haiku-latest prompt_len=893 timeout=5s
[16:01:23] DEBUG | [anthropic] PROMPT: You are a technical documentation search expert. Your task is to rewrite user questions as direct documentation lookup queries.

Guidelines:
1. Convert problem descriptions to feature/capability questions
2. Expand abbreviations and acronyms to full terms
3. Replace casual language with technical terminology
4. Align with documentation vocabulary and structure
5. Keep the rewritten query concise (one line)

Examples:
- "Why can't I schedule workflows every 30 seconds?" → "workflow scheduling min...
[16:01:23] DEBUG | [anthropic] REQUEST: POST https://api.anthropic.com/v1/messages
[16:01:23] DEBUG | [anthropic] REQUEST body: model=claude-3-5-haiku-latest max_tokens=1024
[16:01:24] DEBUG | [anthropic] RESPONSE status=200 elapsed=1.01s
[16:01:24] DEBUG | [anthropic] USAGE: input_tokens=206 output_tokens=13
[16:01:24] DEBUG | [anthropic] SUCCESS stop_reason=end_turn response_len=64
[16:01:24] DEBUG | [anthropic] RESPONSE: cloudflow api batch job rate limit request frequency constraints
[16:01:24] TRACE | === FACT COVERAGE ANALYSIS ===
[16:01:24] TRACE | Query: I'm building a batch job that calls CloudFlow API repeatedly, what rate limit should I expect?
[16:01:24] TRACE | Total key facts: 2
[16:01:24] TRACE | Found facts: 2/2
[16:01:24] TRACE |   FOUND[1] 100 requests per minute per authenticated user
[16:01:24] TRACE |   FOUND[2] 20 requests per minute for unauthenticated requests
[16:01:24] TRACE | === END FACT COVERAGE ===
[16:01:24] DEBUG | [anthropic] Model alias: claude-haiku -> claude-3-5-haiku-latest
[16:01:24] DEBUG | [anthropic] model=claude-3-5-haiku-latest prompt_len=852 timeout=5s
[16:01:24] DEBUG | [anthropic] PROMPT: You are a technical documentation search expert. Your task is to rewrite user questions as direct documentation lookup queries.

Guidelines:
1. Convert problem descriptions to feature/capability questions
2. Expand abbreviations and acronyms to full terms
3. Replace casual language with technical terminology
4. Align with documentation vocabulary and structure
5. Keep the rewritten query concise (one line)

Examples:
- "Why can't I schedule workflows every 30 seconds?" → "workflow scheduling min...
[16:01:24] DEBUG | [anthropic] REQUEST: POST https://api.anthropic.com/v1/messages
[16:01:24] DEBUG | [anthropic] REQUEST body: model=claude-3-5-haiku-latest max_tokens=1024
[16:01:25] DEBUG | [anthropic] RESPONSE status=200 elapsed=0.89s
[16:01:25] DEBUG | [anthropic] USAGE: input_tokens=200 output_tokens=10
[16:01:25] DEBUG | [anthropic] SUCCESS stop_reason=end_turn response_len=40
[16:01:25] DEBUG | [anthropic] RESPONSE: api request rate limit throttling policy
[16:01:25] TRACE | === FACT COVERAGE ANALYSIS ===
[16:01:25] TRACE | Query: Why is the API rejecting my requests after 100 calls?
[16:01:25] TRACE | Total key facts: 2
[16:01:25] TRACE | Found facts: 2/2
[16:01:25] TRACE |   FOUND[1] 100 requests per minute per authenticated user
[16:01:25] TRACE |   FOUND[2] 20 requests per minute for unauthenticated requests
[16:01:25] TRACE | === END FACT COVERAGE ===
[16:01:25] DEBUG | [anthropic] Model alias: claude-haiku -> claude-3-5-haiku-latest
[16:01:25] DEBUG | [anthropic] model=claude-3-5-haiku-latest prompt_len=841 timeout=5s
[16:01:25] DEBUG | [anthropic] PROMPT: You are a technical documentation search expert. Your task is to rewrite user questions as direct documentation lookup queries.

Guidelines:
1. Convert problem descriptions to feature/capability questions
2. Expand abbreviations and acronyms to full terms
3. Replace casual language with technical terminology
4. Align with documentation vocabulary and structure
5. Keep the rewritten query concise (one line)

Examples:
- "Why can't I schedule workflows every 30 seconds?" → "workflow scheduling min...
[16:01:25] DEBUG | [anthropic] REQUEST: POST https://api.anthropic.com/v1/messages
[16:01:25] DEBUG | [anthropic] REQUEST body: model=claude-3-5-haiku-latest max_tokens=1024
[16:01:26] DEBUG | [anthropic] RESPONSE status=200 elapsed=1.06s
[16:01:26] DEBUG | [anthropic] USAGE: input_tokens=199 output_tokens=17
[16:01:26] DEBUG | [anthropic] SUCCESS stop_reason=end_turn response_len=77
[16:01:26] DEBUG | [anthropic] RESPONSE: rate limiting api request throttling 429 error handling mitigation strategies
[16:01:27] TRACE | === FACT COVERAGE ANALYSIS ===
[16:01:27] TRACE | Query: How do I fix 429 Too Many Requests errors?
[16:01:27] TRACE | Total key facts: 5
[16:01:27] TRACE | Found facts: 3/5
[16:01:27] TRACE |   FOUND[1] 429 Too Many Requests
[16:01:27] TRACE |   FOUND[2] X-RateLimit-Remaining
[16:01:27] TRACE |   FOUND[3] Implement exponential backoff when receiving 429 responses
[16:01:27] TRACE | Missed facts: 2/5
[16:01:27] TRACE |   MISSED[1] Retry-After
[16:01:27] TRACE |     -> Found in chunk_id=troubleshooting_guide_fix_6 (rank not in top-5)
[16:01:27] TRACE |   MISSED[2] Monitor X-RateLimit-Remaining header values
[16:01:27] TRACE | === END FACT COVERAGE ===
[16:01:27] DEBUG | [anthropic] Model alias: claude-haiku -> claude-3-5-haiku-latest
[16:01:27] DEBUG | [anthropic] model=claude-3-5-haiku-latest prompt_len=840 timeout=5s
[16:01:27] DEBUG | [anthropic] PROMPT: You are a technical documentation search expert. Your task is to rewrite user questions as direct documentation lookup queries.

Guidelines:
1. Convert problem descriptions to feature/capability questions
2. Expand abbreviations and acronyms to full terms
3. Replace casual language with technical terminology
4. Align with documentation vocabulary and structure
5. Keep the rewritten query concise (one line)

Examples:
- "Why can't I schedule workflows every 30 seconds?" → "workflow scheduling min...
[16:01:27] DEBUG | [anthropic] REQUEST: POST https://api.anthropic.com/v1/messages
[16:01:27] DEBUG | [anthropic] REQUEST body: model=claude-3-5-haiku-latest max_tokens=1024
[16:01:28] DEBUG | [anthropic] RESPONSE status=200 elapsed=0.99s
[16:01:28] DEBUG | [anthropic] USAGE: input_tokens=196 output_tokens=11
[16:01:28] DEBUG | [anthropic] SUCCESS stop_reason=end_turn response_len=56
[16:01:28] DEBUG | [anthropic] RESPONSE: rate limit handling error response mitigation strategies
[16:01:28] TRACE | === FACT COVERAGE ANALYSIS ===
[16:01:28] TRACE | Query: What should I do when I get rate limited?
[16:01:28] TRACE | Total key facts: 5
[16:01:28] TRACE | Found facts: 3/5
[16:01:28] TRACE |   FOUND[1] 429 Too Many Requests
[16:01:28] TRACE |   FOUND[2] X-RateLimit-Remaining
[16:01:28] TRACE |   FOUND[3] Implement exponential backoff when receiving 429 responses
[16:01:28] TRACE | Missed facts: 2/5
[16:01:28] TRACE |   MISSED[1] Retry-After
[16:01:28] TRACE |     -> Found in chunk_id=troubleshooting_guide_fix_6 (rank not in top-5)
[16:01:28] TRACE |   MISSED[2] Monitor X-RateLimit-Remaining header values
[16:01:28] TRACE | === END FACT COVERAGE ===
[16:01:28] DEBUG | [anthropic] Model alias: claude-haiku -> claude-3-5-haiku-latest
[16:01:28] DEBUG | [anthropic] model=claude-3-5-haiku-latest prompt_len=844 timeout=5s
[16:01:28] DEBUG | [anthropic] PROMPT: You are a technical documentation search expert. Your task is to rewrite user questions as direct documentation lookup queries.

Guidelines:
1. Convert problem descriptions to feature/capability questions
2. Expand abbreviations and acronyms to full terms
3. Replace casual language with technical terminology
4. Align with documentation vocabulary and structure
5. Keep the rewritten query concise (one line)

Examples:
- "Why can't I schedule workflows every 30 seconds?" → "workflow scheduling min...
[16:01:28] DEBUG | [anthropic] REQUEST: POST https://api.anthropic.com/v1/messages
[16:01:28] DEBUG | [anthropic] REQUEST body: model=claude-3-5-haiku-latest max_tokens=1024
[16:01:29] DEBUG | [anthropic] RESPONSE status=200 elapsed=0.95s
[16:01:29] DEBUG | [anthropic] USAGE: input_tokens=196 output_tokens=16
[16:01:29] DEBUG | [anthropic] SUCCESS stop_reason=end_turn response_len=67
[16:01:29] DEBUG | [anthropic] RESPONSE: rate limit exceeded api request throttling 429 status code handling
[16:01:30] TRACE | === FACT COVERAGE ANALYSIS ===
[16:01:30] TRACE | Query: My requests keep failing with 429 status code
[16:01:30] TRACE | Total key facts: 5
[16:01:30] TRACE | Found facts: 3/5
[16:01:30] TRACE |   FOUND[1] 429 Too Many Requests
[16:01:30] TRACE |   FOUND[2] X-RateLimit-Remaining
[16:01:30] TRACE |   FOUND[3] Implement exponential backoff when receiving 429 responses
[16:01:30] TRACE | Missed facts: 2/5
[16:01:30] TRACE |   MISSED[1] Retry-After
[16:01:30] TRACE |     -> Found in chunk_id=troubleshooting_guide_fix_6 (rank not in top-5)
[16:01:30] TRACE |   MISSED[2] Monitor X-RateLimit-Remaining header values
[16:01:30] TRACE | === END FACT COVERAGE ===
[16:01:30] DEBUG | [anthropic] Model alias: claude-haiku -> claude-3-5-haiku-latest
[16:01:30] DEBUG | [anthropic] model=claude-3-5-haiku-latest prompt_len=812 timeout=5s
[16:01:30] DEBUG | [anthropic] PROMPT: You are a technical documentation search expert. Your task is to rewrite user questions as direct documentation lookup queries.

Guidelines:
1. Convert problem descriptions to feature/capability questions
2. Expand abbreviations and acronyms to full terms
3. Replace casual language with technical terminology
4. Align with documentation vocabulary and structure
5. Keep the rewritten query concise (one line)

Examples:
- "Why can't I schedule workflows every 30 seconds?" → "workflow scheduling min...
[16:01:30] DEBUG | [anthropic] REQUEST: POST https://api.anthropic.com/v1/messages
[16:01:30] DEBUG | [anthropic] REQUEST body: model=claude-3-5-haiku-latest max_tokens=1024
[16:01:31] DEBUG | [anthropic] RESPONSE status=200 elapsed=0.96s
[16:01:31] DEBUG | [anthropic] USAGE: input_tokens=191 output_tokens=15
[16:01:31] DEBUG | [anthropic] SUCCESS stop_reason=end_turn response_len=71
[16:01:31] DEBUG | [anthropic] RESPONSE: rate limiting error handling http status code 429 resolution strategies
[16:01:31] TRACE | === FACT COVERAGE ANALYSIS ===
[16:01:31] TRACE | Query: 429 error fix
[16:01:31] TRACE | Total key facts: 5
[16:01:31] TRACE | Found facts: 3/5
[16:01:31] TRACE |   FOUND[1] 429 Too Many Requests
[16:01:31] TRACE |   FOUND[2] X-RateLimit-Remaining
[16:01:31] TRACE |   FOUND[3] Implement exponential backoff when receiving 429 responses
[16:01:31] TRACE | Missed facts: 2/5
[16:01:31] TRACE |   MISSED[1] Retry-After
[16:01:31] TRACE |     -> Found in chunk_id=troubleshooting_guide_fix_6 (rank not in top-5)
[16:01:31] TRACE |   MISSED[2] Monitor X-RateLimit-Remaining header values
[16:01:31] TRACE | === END FACT COVERAGE ===
[16:01:31] DEBUG | [anthropic] Model alias: claude-haiku -> claude-3-5-haiku-latest
[16:01:31] DEBUG | [anthropic] model=claude-3-5-haiku-latest prompt_len=879 timeout=5s
[16:01:31] DEBUG | [anthropic] PROMPT: You are a technical documentation search expert. Your task is to rewrite user questions as direct documentation lookup queries.

Guidelines:
1. Convert problem descriptions to feature/capability questions
2. Expand abbreviations and acronyms to full terms
3. Replace casual language with technical terminology
4. Align with documentation vocabulary and structure
5. Keep the rewritten query concise (one line)

Examples:
- "Why can't I schedule workflows every 30 seconds?" → "workflow scheduling min...
[16:01:31] DEBUG | [anthropic] REQUEST: POST https://api.anthropic.com/v1/messages
[16:01:31] DEBUG | [anthropic] REQUEST body: model=claude-3-5-haiku-latest max_tokens=1024
[16:01:32] DEBUG | [anthropic] RESPONSE status=200 elapsed=0.94s
[16:01:32] DEBUG | [anthropic] USAGE: input_tokens=205 output_tokens=14
[16:01:32] DEBUG | [anthropic] SUCCESS stop_reason=end_turn response_len=70
[16:01:32] DEBUG | [anthropic] RESPONSE: cloudflow api rate limit error handling retry mechanism best practices
[16:01:32] TRACE | === FACT COVERAGE ANALYSIS ===
[16:01:32] TRACE | Query: I'm getting throttled by CloudFlow API, how can I handle this in my application?
[16:01:32] TRACE | Total key facts: 5
[16:01:32] TRACE | Found facts: 3/5
[16:01:32] TRACE |   FOUND[1] 429 Too Many Requests
[16:01:32] TRACE |   FOUND[2] X-RateLimit-Remaining
[16:01:32] TRACE |   FOUND[3] Implement exponential backoff when receiving 429 responses
[16:01:32] TRACE | Missed facts: 2/5
[16:01:32] TRACE |   MISSED[1] Retry-After
[16:01:32] TRACE |     -> Found in chunk_id=troubleshooting_guide_fix_6 (rank not in top-5)
[16:01:32] TRACE |   MISSED[2] Monitor X-RateLimit-Remaining header values
[16:01:32] TRACE | === END FACT COVERAGE ===
[16:01:32] DEBUG | [anthropic] Model alias: claude-haiku -> claude-3-5-haiku-latest
[16:01:32] DEBUG | [anthropic] model=claude-3-5-haiku-latest prompt_len=845 timeout=5s
[16:01:32] DEBUG | [anthropic] PROMPT: You are a technical documentation search expert. Your task is to rewrite user questions as direct documentation lookup queries.

Guidelines:
1. Convert problem descriptions to feature/capability questions
2. Expand abbreviations and acronyms to full terms
3. Replace casual language with technical terminology
4. Align with documentation vocabulary and structure
5. Keep the rewritten query concise (one line)

Examples:
- "Why can't I schedule workflows every 30 seconds?" → "workflow scheduling min...
[16:01:32] DEBUG | [anthropic] REQUEST: POST https://api.anthropic.com/v1/messages
[16:01:32] DEBUG | [anthropic] REQUEST body: model=claude-3-5-haiku-latest max_tokens=1024
[16:01:34] DEBUG | [anthropic] RESPONSE status=200 elapsed=1.38s
[16:01:34] DEBUG | [anthropic] USAGE: input_tokens=196 output_tokens=11
[16:01:34] DEBUG | [anthropic] SUCCESS stop_reason=end_turn response_len=61
[16:01:34] DEBUG | [anthropic] RESPONSE: api rate limit error handling blocking threshold restrictions
[16:01:34] TRACE | === FACT COVERAGE ANALYSIS ===
[16:01:34] TRACE | Query: Why am I being blocked with rate limit errors?
[16:01:34] TRACE | Total key facts: 5
[16:01:34] TRACE | Found facts: 3/5
[16:01:34] TRACE |   FOUND[1] 429 Too Many Requests
[16:01:34] TRACE |   FOUND[2] X-RateLimit-Remaining
[16:01:34] TRACE |   FOUND[3] Implement exponential backoff when receiving 429 responses
[16:01:34] TRACE | Missed facts: 2/5
[16:01:34] TRACE |   MISSED[1] Retry-After
[16:01:34] TRACE |     -> Found in chunk_id=troubleshooting_guide_fix_6 (rank not in top-5)
[16:01:34] TRACE |   MISSED[2] Monitor X-RateLimit-Remaining header values
[16:01:34] TRACE | === END FACT COVERAGE ===
[16:01:34] DEBUG | [anthropic] Model alias: claude-haiku -> claude-3-5-haiku-latest
[16:01:34] DEBUG | [anthropic] model=claude-3-5-haiku-latest prompt_len=837 timeout=5s
[16:01:34] DEBUG | [anthropic] PROMPT: You are a technical documentation search expert. Your task is to rewrite user questions as direct documentation lookup queries.

Guidelines:
1. Convert problem descriptions to feature/capability questions
2. Expand abbreviations and acronyms to full terms
3. Replace casual language with technical terminology
4. Align with documentation vocabulary and structure
5. Keep the rewritten query concise (one line)

Examples:
- "Why can't I schedule workflows every 30 seconds?" → "workflow scheduling min...
[16:01:34] DEBUG | [anthropic] REQUEST: POST https://api.anthropic.com/v1/messages
[16:01:34] DEBUG | [anthropic] REQUEST body: model=claude-3-5-haiku-latest max_tokens=1024
[16:01:35] DEBUG | [anthropic] RESPONSE status=200 elapsed=0.87s
[16:01:35] DEBUG | [anthropic] USAGE: input_tokens=195 output_tokens=10
[16:01:35] DEBUG | [anthropic] SUCCESS stop_reason=end_turn response_len=53
[16:01:35] DEBUG | [anthropic] RESPONSE: jwt token lifetime authentication expiration duration
[16:01:35] TRACE | === FACT COVERAGE ANALYSIS ===
[16:01:35] TRACE | Query: What is the JWT token expiration time?
[16:01:35] TRACE | Total key facts: 3
[16:01:35] TRACE | Found facts: 2/3
[16:01:35] TRACE |   FOUND[1] 3600 seconds
[16:01:35] TRACE |   FOUND[2] All tokens expire after 3600 seconds
[16:01:35] TRACE | Missed facts: 1/3
[16:01:35] TRACE |   MISSED[1] max 3600 seconds from iat
[16:01:35] TRACE | === END FACT COVERAGE ===
[16:01:35] DEBUG | [anthropic] Model alias: claude-haiku -> claude-3-5-haiku-latest
[16:01:35] DEBUG | [anthropic] model=claude-3-5-haiku-latest prompt_len=830 timeout=5s
[16:01:35] DEBUG | [anthropic] PROMPT: You are a technical documentation search expert. Your task is to rewrite user questions as direct documentation lookup queries.

Guidelines:
1. Convert problem descriptions to feature/capability questions
2. Expand abbreviations and acronyms to full terms
3. Replace casual language with technical terminology
4. Align with documentation vocabulary and structure
5. Keep the rewritten query concise (one line)

Examples:
- "Why can't I schedule workflows every 30 seconds?" → "workflow scheduling min...
[16:01:35] DEBUG | [anthropic] REQUEST: POST https://api.anthropic.com/v1/messages
[16:01:35] DEBUG | [anthropic] REQUEST body: model=claude-3-5-haiku-latest max_tokens=1024
[16:01:37] DEBUG | [anthropic] RESPONSE status=200 elapsed=1.90s
[16:01:37] DEBUG | [anthropic] USAGE: input_tokens=193 output_tokens=13
[16:01:37] DEBUG | [anthropic] SUCCESS stop_reason=end_turn response_len=78
[16:01:37] DEBUG | [anthropic] RESPONSE: access token duration lifetime expiration authentication token validity period
[16:01:38] TRACE | === FACT COVERAGE ANALYSIS ===
[16:01:38] TRACE | Query: How long do access tokens last?
[16:01:38] TRACE | Total key facts: 3
[16:01:38] TRACE | Found facts: 2/3
[16:01:38] TRACE |   FOUND[1] 3600 seconds
[16:01:38] TRACE |   FOUND[2] All tokens expire after 3600 seconds
[16:01:38] TRACE | Missed facts: 1/3
[16:01:38] TRACE |   MISSED[1] max 3600 seconds from iat
[16:01:38] TRACE | === END FACT COVERAGE ===
[16:01:38] DEBUG | [anthropic] Model alias: claude-haiku -> claude-3-5-haiku-latest
[16:01:38] DEBUG | [anthropic] model=claude-3-5-haiku-latest prompt_len=845 timeout=5s
[16:01:38] DEBUG | [anthropic] PROMPT: You are a technical documentation search expert. Your task is to rewrite user questions as direct documentation lookup queries.

Guidelines:
1. Convert problem descriptions to feature/capability questions
2. Expand abbreviations and acronyms to full terms
3. Replace casual language with technical terminology
4. Align with documentation vocabulary and structure
5. Keep the rewritten query concise (one line)

Examples:
- "Why can't I schedule workflows every 30 seconds?" → "workflow scheduling min...
[16:01:38] DEBUG | [anthropic] REQUEST: POST https://api.anthropic.com/v1/messages
[16:01:38] DEBUG | [anthropic] REQUEST body: model=claude-3-5-haiku-latest max_tokens=1024
[16:01:39] DEBUG | [anthropic] RESPONSE status=200 elapsed=0.90s
[16:01:39] DEBUG | [anthropic] USAGE: input_tokens=194 output_tokens=11
[16:01:39] DEBUG | [anthropic] SUCCESS stop_reason=end_turn response_len=63
[16:01:39] DEBUG | [anthropic] RESPONSE: authentication token session timeout expiration policy duration
[16:01:39] TRACE | === FACT COVERAGE ANALYSIS ===
[16:01:39] TRACE | Query: My authentication keeps expiring after an hour
[16:01:39] TRACE | Total key facts: 3
[16:01:39] TRACE | Found facts: 2/3
[16:01:39] TRACE |   FOUND[1] 3600 seconds
[16:01:39] TRACE |   FOUND[2] All tokens expire after 3600 seconds
[16:01:39] TRACE | Missed facts: 1/3
[16:01:39] TRACE |   MISSED[1] max 3600 seconds from iat
[16:01:39] TRACE | === END FACT COVERAGE ===
[16:01:39] DEBUG | [anthropic] Model alias: claude-haiku -> claude-3-5-haiku-latest
[16:01:39] DEBUG | [anthropic] model=claude-3-5-haiku-latest prompt_len=816 timeout=5s
[16:01:39] DEBUG | [anthropic] PROMPT: You are a technical documentation search expert. Your task is to rewrite user questions as direct documentation lookup queries.

Guidelines:
1. Convert problem descriptions to feature/capability questions
2. Expand abbreviations and acronyms to full terms
3. Replace casual language with technical terminology
4. Align with documentation vocabulary and structure
5. Keep the rewritten query concise (one line)

Examples:
- "Why can't I schedule workflows every 30 seconds?" → "workflow scheduling min...
[16:01:39] DEBUG | [anthropic] REQUEST: POST https://api.anthropic.com/v1/messages
[16:01:39] DEBUG | [anthropic] REQUEST body: model=claude-3-5-haiku-latest max_tokens=1024
[16:01:40] DEBUG | [anthropic] RESPONSE status=200 elapsed=0.94s
[16:01:40] DEBUG | [anthropic] USAGE: input_tokens=190 output_tokens=10
[16:01:40] DEBUG | [anthropic] SUCCESS stop_reason=end_turn response_len=63
[16:01:40] DEBUG | [anthropic] RESPONSE: token authentication expiration duration lifetime configuration
[16:01:40] TRACE | === FACT COVERAGE ANALYSIS ===
[16:01:40] TRACE | Query: token expiry time
[16:01:40] TRACE | Total key facts: 3
[16:01:40] TRACE | Found facts: 0/3
[16:01:40] TRACE | Missed facts: 3/3
[16:01:40] TRACE |   MISSED[1] 3600 seconds
[16:01:40] TRACE |     -> Found in chunk_id=api_reference_fix_0 (rank not in top-5)
[16:01:40] TRACE |   MISSED[2] max 3600 seconds from iat
[16:01:40] TRACE |   MISSED[3] All tokens expire after 3600 seconds
[16:01:40] TRACE |     -> Found in chunk_id=api_reference_fix_0 (rank not in top-5)
[16:01:40] TRACE | === END FACT COVERAGE ===
[16:01:40] DEBUG | [anthropic] Model alias: claude-haiku -> claude-3-5-haiku-latest
[16:01:40] DEBUG | [anthropic] model=claude-3-5-haiku-latest prompt_len=866 timeout=5s
[16:01:40] DEBUG | [anthropic] PROMPT: You are a technical documentation search expert. Your task is to rewrite user questions as direct documentation lookup queries.

Guidelines:
1. Convert problem descriptions to feature/capability questions
2. Expand abbreviations and acronyms to full terms
3. Replace casual language with technical terminology
4. Align with documentation vocabulary and structure
5. Keep the rewritten query concise (one line)

Examples:
- "Why can't I schedule workflows every 30 seconds?" → "workflow scheduling min...
[16:01:40] DEBUG | [anthropic] REQUEST: POST https://api.anthropic.com/v1/messages
[16:01:40] DEBUG | [anthropic] REQUEST body: model=claude-3-5-haiku-latest max_tokens=1024
[16:01:41] DEBUG | [anthropic] RESPONSE status=200 elapsed=0.89s
[16:01:41] DEBUG | [anthropic] USAGE: input_tokens=200 output_tokens=11
[16:01:41] DEBUG | [anthropic] SUCCESS stop_reason=end_turn response_len=62
[16:01:41] DEBUG | [anthropic] RESPONSE: jwt token authentication expiration lifetime refresh mechanism
[16:01:42] TRACE | === FACT COVERAGE ANALYSIS ===
[16:01:42] TRACE | Query: I need to implement token refresh logic, when do JWT tokens expire?
[16:01:42] TRACE | Total key facts: 3
[16:01:42] TRACE | Found facts: 2/3
[16:01:42] TRACE |   FOUND[1] 3600 seconds
[16:01:42] TRACE |   FOUND[2] All tokens expire after 3600 seconds
[16:01:42] TRACE | Missed facts: 1/3
[16:01:42] TRACE |   MISSED[1] max 3600 seconds from iat
[16:01:42] TRACE | === END FACT COVERAGE ===
[16:01:42] DEBUG | [anthropic] Model alias: claude-haiku -> claude-3-5-haiku-latest
[16:01:42] DEBUG | [anthropic] model=claude-3-5-haiku-latest prompt_len=849 timeout=5s
[16:01:42] DEBUG | [anthropic] PROMPT: You are a technical documentation search expert. Your task is to rewrite user questions as direct documentation lookup queries.

Guidelines:
1. Convert problem descriptions to feature/capability questions
2. Expand abbreviations and acronyms to full terms
3. Replace casual language with technical terminology
4. Align with documentation vocabulary and structure
5. Keep the rewritten query concise (one line)

Examples:
- "Why can't I schedule workflows every 30 seconds?" → "workflow scheduling min...
[16:01:42] DEBUG | [anthropic] REQUEST: POST https://api.anthropic.com/v1/messages
[16:01:42] DEBUG | [anthropic] REQUEST body: model=claude-3-5-haiku-latest max_tokens=1024
[16:01:42] DEBUG | [anthropic] RESPONSE status=200 elapsed=0.87s
[16:01:42] DEBUG | [anthropic] USAGE: input_tokens=199 output_tokens=10
[16:01:42] DEBUG | [anthropic] SUCCESS stop_reason=end_turn response_len=62
[16:01:42] DEBUG | [anthropic] RESPONSE: token authentication expiration lifetime default configuration
[16:01:43] TRACE | === FACT COVERAGE ANALYSIS ===
[16:01:43] TRACE | Query: Why does my token stop working after 3600 seconds?
[16:01:43] TRACE | Total key facts: 3
[16:01:43] TRACE | Found facts: 1/3
[16:01:43] TRACE |   FOUND[1] 3600 seconds
[16:01:43] TRACE | Missed facts: 2/3
[16:01:43] TRACE |   MISSED[1] max 3600 seconds from iat
[16:01:43] TRACE |   MISSED[2] All tokens expire after 3600 seconds
[16:01:43] TRACE |     -> Found in chunk_id=api_reference_fix_0 (rank not in top-5)
[16:01:43] TRACE | === END FACT COVERAGE ===
[16:01:43] DEBUG | [anthropic] Model alias: claude-haiku -> claude-3-5-haiku-latest
[16:01:43] DEBUG | [anthropic] model=claude-3-5-haiku-latest prompt_len=843 timeout=5s
[16:01:43] DEBUG | [anthropic] PROMPT: You are a technical documentation search expert. Your task is to rewrite user questions as direct documentation lookup queries.

Guidelines:
1. Convert problem descriptions to feature/capability questions
2. Expand abbreviations and acronyms to full terms
3. Replace casual language with technical terminology
4. Align with documentation vocabulary and structure
5. Keep the rewritten query concise (one line)

Examples:
- "Why can't I schedule workflows every 30 seconds?" → "workflow scheduling min...
[16:01:43] DEBUG | [anthropic] REQUEST: POST https://api.anthropic.com/v1/messages
[16:01:43] DEBUG | [anthropic] REQUEST body: model=claude-3-5-haiku-latest max_tokens=1024
[16:01:44] DEBUG | [anthropic] RESPONSE status=200 elapsed=0.89s
[16:01:44] DEBUG | [anthropic] USAGE: input_tokens=194 output_tokens=11
[16:01:44] DEBUG | [anthropic] SUCCESS stop_reason=end_turn response_len=65
[16:01:44] DEBUG | [anthropic] RESPONSE: cloudflow database technology storage backend supported platforms
[16:01:44] TRACE | === FACT COVERAGE ANALYSIS ===
[16:01:44] TRACE | Query: What database technology does CloudFlow use?
[16:01:44] TRACE | Total key facts: 3
[16:01:44] TRACE | Found facts: 3/3
[16:01:44] TRACE |   FOUND[1] PostgreSQL 15.4
[16:01:44] TRACE |   FOUND[2] Redis 7.2
[16:01:44] TRACE |   FOUND[3] Apache Kafka 3.6
[16:01:44] TRACE | === END FACT COVERAGE ===
[16:01:44] DEBUG | [anthropic] Model alias: claude-haiku -> claude-3-5-haiku-latest
[16:01:44] DEBUG | [anthropic] model=claude-3-5-haiku-latest prompt_len=838 timeout=5s
[16:01:44] DEBUG | [anthropic] PROMPT: You are a technical documentation search expert. Your task is to rewrite user questions as direct documentation lookup queries.

Guidelines:
1. Convert problem descriptions to feature/capability questions
2. Expand abbreviations and acronyms to full terms
3. Replace casual language with technical terminology
4. Align with documentation vocabulary and structure
5. Keep the rewritten query concise (one line)

Examples:
- "Why can't I schedule workflows every 30 seconds?" → "workflow scheduling min...
[16:01:44] DEBUG | [anthropic] REQUEST: POST https://api.anthropic.com/v1/messages
[16:01:44] DEBUG | [anthropic] REQUEST body: model=claude-3-5-haiku-latest max_tokens=1024
[16:01:45] DEBUG | [anthropic] RESPONSE status=200 elapsed=0.96s
[16:01:45] DEBUG | [anthropic] USAGE: input_tokens=193 output_tokens=9
[16:01:45] DEBUG | [anthropic] SUCCESS stop_reason=end_turn response_len=46
[16:01:45] DEBUG | [anthropic] RESPONSE: CloudFlow database backend system architecture
[16:01:45] TRACE | === FACT COVERAGE ANALYSIS ===
[16:01:45] TRACE | Query: Which database system powers CloudFlow?
[16:01:45] TRACE | Total key facts: 3
[16:01:45] TRACE | Found facts: 3/3
[16:01:45] TRACE |   FOUND[1] PostgreSQL 15.4
[16:01:45] TRACE |   FOUND[2] Redis 7.2
[16:01:45] TRACE |   FOUND[3] Apache Kafka 3.6
[16:01:45] TRACE | === END FACT COVERAGE ===
[16:01:45] DEBUG | [anthropic] Model alias: claude-haiku -> claude-3-5-haiku-latest
[16:01:45] DEBUG | [anthropic] model=claude-3-5-haiku-latest prompt_len=849 timeout=5s
[16:01:45] DEBUG | [anthropic] PROMPT: You are a technical documentation search expert. Your task is to rewrite user questions as direct documentation lookup queries.

Guidelines:
1. Convert problem descriptions to feature/capability questions
2. Expand abbreviations and acronyms to full terms
3. Replace casual language with technical terminology
4. Align with documentation vocabulary and structure
5. Keep the rewritten query concise (one line)

Examples:
- "Why can't I schedule workflows every 30 seconds?" → "workflow scheduling min...
[16:01:45] DEBUG | [anthropic] REQUEST: POST https://api.anthropic.com/v1/messages
[16:01:45] DEBUG | [anthropic] REQUEST body: model=claude-3-5-haiku-latest max_tokens=1024
[16:01:46] DEBUG | [anthropic] RESPONSE status=200 elapsed=0.88s
[16:01:46] DEBUG | [anthropic] USAGE: input_tokens=194 output_tokens=10
[16:01:46] DEBUG | [anthropic] SUCCESS stop_reason=end_turn response_len=60
[16:01:46] DEBUG | [anthropic] RESPONSE: data storage system architecture design principles reference
[16:01:47] TRACE | === FACT COVERAGE ANALYSIS ===
[16:01:47] TRACE | Query: I need to understand the data storage architecture
[16:01:47] TRACE | Total key facts: 3
[16:01:47] TRACE | Found facts: 3/3
[16:01:47] TRACE |   FOUND[1] PostgreSQL 15.4
[16:01:47] TRACE |   FOUND[2] Redis 7.2
[16:01:47] TRACE |   FOUND[3] Apache Kafka 3.6
[16:01:47] TRACE | === END FACT COVERAGE ===
[16:01:47] DEBUG | [anthropic] Model alias: claude-haiku -> claude-3-5-haiku-latest
[16:01:47] DEBUG | [anthropic] model=claude-3-5-haiku-latest prompt_len=813 timeout=5s
[16:01:47] DEBUG | [anthropic] PROMPT: You are a technical documentation search expert. Your task is to rewrite user questions as direct documentation lookup queries.

Guidelines:
1. Convert problem descriptions to feature/capability questions
2. Expand abbreviations and acronyms to full terms
3. Replace casual language with technical terminology
4. Align with documentation vocabulary and structure
5. Keep the rewritten query concise (one line)

Examples:
- "Why can't I schedule workflows every 30 seconds?" → "workflow scheduling min...
[16:01:47] DEBUG | [anthropic] REQUEST: POST https://api.anthropic.com/v1/messages
[16:01:47] DEBUG | [anthropic] REQUEST body: model=claude-3-5-haiku-latest max_tokens=1024
[16:01:48] DEBUG | [anthropic] RESPONSE status=200 elapsed=0.88s
[16:01:48] DEBUG | [anthropic] USAGE: input_tokens=188 output_tokens=8
[16:01:48] DEBUG | [anthropic] SUCCESS stop_reason=end_turn response_len=52
[16:01:48] DEBUG | [anthropic] RESPONSE: database technology stack configuration architecture
[16:01:48] TRACE | === FACT COVERAGE ANALYSIS ===
[16:01:48] TRACE | Query: database stack
[16:01:48] TRACE | Total key facts: 3
[16:01:48] TRACE | Found facts: 0/3
[16:01:48] TRACE | Missed facts: 3/3
[16:01:48] TRACE |   MISSED[1] PostgreSQL 15.4
[16:01:48] TRACE |     -> Found in chunk_id=architecture_overview_fix_0 (rank not in top-5)
[16:01:48] TRACE |   MISSED[2] Redis 7.2
[16:01:48] TRACE |     -> Found in chunk_id=architecture_overview_fix_0 (rank not in top-5)
[16:01:48] TRACE |   MISSED[3] Apache Kafka 3.6
[16:01:48] TRACE |     -> Found in chunk_id=architecture_overview_fix_0 (rank not in top-5)
[16:01:48] TRACE | === END FACT COVERAGE ===
[16:01:48] DEBUG | [anthropic] Model alias: claude-haiku -> claude-3-5-haiku-latest
[16:01:48] DEBUG | [anthropic] model=claude-3-5-haiku-latest prompt_len=860 timeout=5s
[16:01:48] DEBUG | [anthropic] PROMPT: You are a technical documentation search expert. Your task is to rewrite user questions as direct documentation lookup queries.

Guidelines:
1. Convert problem descriptions to feature/capability questions
2. Expand abbreviations and acronyms to full terms
3. Replace casual language with technical terminology
4. Align with documentation vocabulary and structure
5. Keep the rewritten query concise (one line)

Examples:
- "Why can't I schedule workflows every 30 seconds?" → "workflow scheduling min...
[16:01:48] DEBUG | [anthropic] REQUEST: POST https://api.anthropic.com/v1/messages
[16:01:48] DEBUG | [anthropic] REQUEST body: model=claude-3-5-haiku-latest max_tokens=1024
[16:01:49] DEBUG | [anthropic] RESPONSE status=200 elapsed=1.01s
[16:01:49] DEBUG | [anthropic] USAGE: input_tokens=198 output_tokens=13
[16:01:49] DEBUG | [anthropic] SUCCESS stop_reason=end_turn response_len=78
[16:01:49] DEBUG | [anthropic] RESPONSE: cloudflow supported database systems capacity planning backend storage options
[16:01:49] TRACE | === FACT COVERAGE ANALYSIS ===
[16:01:49] TRACE | Query: For capacity planning, what databases does CloudFlow rely on?
[16:01:49] TRACE | Total key facts: 3
[16:01:49] TRACE | Found facts: 3/3
[16:01:49] TRACE |   FOUND[1] PostgreSQL 15.4
[16:01:49] TRACE |   FOUND[2] Redis 7.2
[16:01:49] TRACE |   FOUND[3] Apache Kafka 3.6
[16:01:49] TRACE | === END FACT COVERAGE ===
[16:01:49] DEBUG | [anthropic] Model alias: claude-haiku -> claude-3-5-haiku-latest
[16:01:49] DEBUG | [anthropic] model=claude-3-5-haiku-latest prompt_len=842 timeout=5s
[16:01:49] DEBUG | [anthropic] PROMPT: You are a technical documentation search expert. Your task is to rewrite user questions as direct documentation lookup queries.

Guidelines:
1. Convert problem descriptions to feature/capability questions
2. Expand abbreviations and acronyms to full terms
3. Replace casual language with technical terminology
4. Align with documentation vocabulary and structure
5. Keep the rewritten query concise (one line)

Examples:
- "Why can't I schedule workflows every 30 seconds?" → "workflow scheduling min...
[16:01:49] DEBUG | [anthropic] REQUEST: POST https://api.anthropic.com/v1/messages
[16:01:49] DEBUG | [anthropic] REQUEST body: model=claude-3-5-haiku-latest max_tokens=1024
[16:01:50] DEBUG | [anthropic] RESPONSE status=200 elapsed=1.05s
[16:01:50] DEBUG | [anthropic] USAGE: input_tokens=195 output_tokens=10
[16:01:50] DEBUG | [anthropic] SUCCESS stop_reason=end_turn response_len=46
[16:01:50] DEBUG | [anthropic] RESPONSE: cloudflow database storage engine backend type
[16:01:51] TRACE | === FACT COVERAGE ANALYSIS ===
[16:01:51] TRACE | Query: Is CloudFlow using MySQL or something else?
[16:01:51] TRACE | Total key facts: 3
[16:01:51] TRACE | Found facts: 3/3
[16:01:51] TRACE |   FOUND[1] PostgreSQL 15.4
[16:01:51] TRACE |   FOUND[2] Redis 7.2
[16:01:51] TRACE |   FOUND[3] Apache Kafka 3.6
[16:01:51] TRACE | === END FACT COVERAGE ===
[16:01:51] DEBUG | [anthropic] Model alias: claude-haiku -> claude-3-5-haiku-latest
[16:01:51] DEBUG | [anthropic] model=claude-3-5-haiku-latest prompt_len=858 timeout=5s
[16:01:51] DEBUG | [anthropic] PROMPT: You are a technical documentation search expert. Your task is to rewrite user questions as direct documentation lookup queries.

Guidelines:
1. Convert problem descriptions to feature/capability questions
2. Expand abbreviations and acronyms to full terms
3. Replace casual language with technical terminology
4. Align with documentation vocabulary and structure
5. Keep the rewritten query concise (one line)

Examples:
- "Why can't I schedule workflows every 30 seconds?" → "workflow scheduling min...
[16:01:51] DEBUG | [anthropic] REQUEST: POST https://api.anthropic.com/v1/messages
[16:01:51] DEBUG | [anthropic] REQUEST body: model=claude-3-5-haiku-latest max_tokens=1024
[16:01:52] DEBUG | [anthropic] RESPONSE status=200 elapsed=0.90s
[16:01:52] DEBUG | [anthropic] USAGE: input_tokens=196 output_tokens=10
[16:01:52] DEBUG | [anthropic] SUCCESS stop_reason=end_turn response_len=72
[16:01:52] DEBUG | [anthropic] RESPONSE: kubernetes namespace best practices production environment configuration
[16:01:52] TRACE | === FACT COVERAGE ANALYSIS ===
[16:01:52] TRACE | Query: What is the Kubernetes namespace for production deployment?
[16:01:52] TRACE | Total key facts: 1
[16:01:52] TRACE | Found facts: 1/1
[16:01:52] TRACE |   FOUND[1] cloudflow-prod
[16:01:52] TRACE | === END FACT COVERAGE ===
[16:01:52] DEBUG | [anthropic] Model alias: claude-haiku -> claude-3-5-haiku-latest
[16:01:52] DEBUG | [anthropic] model=claude-3-5-haiku-latest prompt_len=848 timeout=5s
[16:01:52] DEBUG | [anthropic] PROMPT: You are a technical documentation search expert. Your task is to rewrite user questions as direct documentation lookup queries.

Guidelines:
1. Convert problem descriptions to feature/capability questions
2. Expand abbreviations and acronyms to full terms
3. Replace casual language with technical terminology
4. Align with documentation vocabulary and structure
5. Keep the rewritten query concise (one line)

Examples:
- "Why can't I schedule workflows every 30 seconds?" → "workflow scheduling min...
[16:01:52] DEBUG | [anthropic] REQUEST: POST https://api.anthropic.com/v1/messages
[16:01:52] DEBUG | [anthropic] REQUEST body: model=claude-3-5-haiku-latest max_tokens=1024
[16:01:53] DEBUG | [anthropic] RESPONSE status=200 elapsed=0.82s
[16:01:53] DEBUG | [anthropic] USAGE: input_tokens=195 output_tokens=9
[16:01:53] DEBUG | [anthropic] SUCCESS stop_reason=end_turn response_len=52
[16:01:53] DEBUG | [anthropic] RESPONSE: cloudflow production default namespace configuration
[16:01:53] TRACE | === FACT COVERAGE ANALYSIS ===
[16:01:53] TRACE | Query: Which namespace is used for CloudFlow production?
[16:01:53] TRACE | Total key facts: 1
[16:01:53] TRACE | Found facts: 1/1
[16:01:53] TRACE |   FOUND[1] cloudflow-prod
[16:01:53] TRACE | === END FACT COVERAGE ===
[16:01:53] DEBUG | [anthropic] Model alias: claude-haiku -> claude-3-5-haiku-latest
[16:01:53] DEBUG | [anthropic] model=claude-3-5-haiku-latest prompt_len=831 timeout=5s
[16:01:53] DEBUG | [anthropic] PROMPT: You are a technical documentation search expert. Your task is to rewrite user questions as direct documentation lookup queries.

Guidelines:
1. Convert problem descriptions to feature/capability questions
2. Expand abbreviations and acronyms to full terms
3. Replace casual language with technical terminology
4. Align with documentation vocabulary and structure
5. Keep the rewritten query concise (one line)

Examples:
- "Why can't I schedule workflows every 30 seconds?" → "workflow scheduling min...
[16:01:53] DEBUG | [anthropic] REQUEST: POST https://api.anthropic.com/v1/messages
[16:01:53] DEBUG | [anthropic] REQUEST body: model=claude-3-5-haiku-latest max_tokens=1024
[16:01:54] DEBUG | [anthropic] RESPONSE status=200 elapsed=1.06s
[16:01:54] DEBUG | [anthropic] USAGE: input_tokens=193 output_tokens=12
[16:01:54] DEBUG | [anthropic] SUCCESS stop_reason=end_turn response_len=61
[16:01:54] DEBUG | [anthropic] RESPONSE: kubernetes production namespace pod discovery troubleshooting
[16:01:55] TRACE | === FACT COVERAGE ANALYSIS ===
[16:01:55] TRACE | Query: I can't find the production pods
[16:01:55] TRACE | Total key facts: 1
[16:01:55] TRACE | Found facts: 0/1
[16:01:55] TRACE | Missed facts: 1/1
[16:01:55] TRACE |   MISSED[1] cloudflow-prod
[16:01:55] TRACE |     -> Found in chunk_id=deployment_guide_fix_0 (rank not in top-5)
[16:01:55] TRACE | === END FACT COVERAGE ===
[16:01:55] DEBUG | [anthropic] Model alias: claude-haiku -> claude-3-5-haiku-latest
[16:01:55] DEBUG | [anthropic] model=claude-3-5-haiku-latest prompt_len=813 timeout=5s
[16:01:55] DEBUG | [anthropic] PROMPT: You are a technical documentation search expert. Your task is to rewrite user questions as direct documentation lookup queries.

Guidelines:
1. Convert problem descriptions to feature/capability questions
2. Expand abbreviations and acronyms to full terms
3. Replace casual language with technical terminology
4. Align with documentation vocabulary and structure
5. Keep the rewritten query concise (one line)

Examples:
- "Why can't I schedule workflows every 30 seconds?" → "workflow scheduling min...
[16:01:55] DEBUG | [anthropic] REQUEST: POST https://api.anthropic.com/v1/messages
[16:01:55] DEBUG | [anthropic] REQUEST body: model=claude-3-5-haiku-latest max_tokens=1024
[16:01:56] DEBUG | [anthropic] RESPONSE status=200 elapsed=0.95s
[16:01:56] DEBUG | [anthropic] USAGE: input_tokens=188 output_tokens=11
[16:01:56] DEBUG | [anthropic] SUCCESS stop_reason=end_turn response_len=62
[16:01:56] DEBUG | [anthropic] RESPONSE: "kubernetes production namespace configuration best practices"
[16:01:56] TRACE | === FACT COVERAGE ANALYSIS ===
[16:01:56] TRACE | Query: prod namespace
[16:01:56] TRACE | Total key facts: 1
[16:01:56] TRACE | Found facts: 1/1
[16:01:56] TRACE |   FOUND[1] cloudflow-prod
[16:01:56] TRACE | === END FACT COVERAGE ===
[16:01:56] DEBUG | [anthropic] Model alias: claude-haiku -> claude-3-5-haiku-latest
[16:01:56] DEBUG | [anthropic] model=claude-3-5-haiku-latest prompt_len=859 timeout=5s
[16:01:56] DEBUG | [anthropic] PROMPT: You are a technical documentation search expert. Your task is to rewrite user questions as direct documentation lookup queries.

Guidelines:
1. Convert problem descriptions to feature/capability questions
2. Expand abbreviations and acronyms to full terms
3. Replace casual language with technical terminology
4. Align with documentation vocabulary and structure
5. Keep the rewritten query concise (one line)

Examples:
- "Why can't I schedule workflows every 30 seconds?" → "workflow scheduling min...
[16:01:56] DEBUG | [anthropic] REQUEST: POST https://api.anthropic.com/v1/messages
[16:01:56] DEBUG | [anthropic] REQUEST body: model=claude-3-5-haiku-latest max_tokens=1024
[16:01:57] DEBUG | [anthropic] RESPONSE status=200 elapsed=0.83s
[16:01:57] DEBUG | [anthropic] USAGE: input_tokens=199 output_tokens=10
[16:01:57] DEBUG | [anthropic] SUCCESS stop_reason=end_turn response_len=65
[16:01:57] DEBUG | [anthropic] RESPONSE: kubernetes production deployment default namespace best practices
[16:01:57] TRACE | === FACT COVERAGE ANALYSIS ===
[16:01:57] TRACE | Query: I need to deploy to production, what namespace should I use?
[16:01:57] TRACE | Total key facts: 1
[16:01:57] TRACE | Found facts: 1/1
[16:01:57] TRACE |   FOUND[1] cloudflow-prod
[16:01:57] TRACE | === END FACT COVERAGE ===
[16:01:57] DEBUG | [anthropic] Model alias: claude-haiku -> claude-3-5-haiku-latest
[16:01:57] DEBUG | [anthropic] model=claude-3-5-haiku-latest prompt_len=850 timeout=5s
[16:01:57] DEBUG | [anthropic] PROMPT: You are a technical documentation search expert. Your task is to rewrite user questions as direct documentation lookup queries.

Guidelines:
1. Convert problem descriptions to feature/capability questions
2. Expand abbreviations and acronyms to full terms
3. Replace casual language with technical terminology
4. Align with documentation vocabulary and structure
5. Keep the rewritten query concise (one line)

Examples:
- "Why can't I schedule workflows every 30 seconds?" → "workflow scheduling min...
[16:01:57] DEBUG | [anthropic] REQUEST: POST https://api.anthropic.com/v1/messages
[16:01:57] DEBUG | [anthropic] REQUEST body: model=claude-3-5-haiku-latest max_tokens=1024
[16:01:59] DEBUG | [anthropic] RESPONSE status=200 elapsed=1.21s
[16:01:59] DEBUG | [anthropic] USAGE: input_tokens=197 output_tokens=10
[16:01:59] DEBUG | [anthropic] SUCCESS stop_reason=end_turn response_len=67
[16:01:59] DEBUG | [anthropic] RESPONSE: kubernetes default namespace resource visibility access permissions
[16:01:59] TRACE | === FACT COVERAGE ANALYSIS ===
[16:01:59] TRACE | Query: Why can't I see resources in the default namespace?
[16:01:59] TRACE | Total key facts: 1
[16:01:59] TRACE | Found facts: 1/1
[16:01:59] TRACE |   FOUND[1] cloudflow-prod
[16:01:59] TRACE | === END FACT COVERAGE ===
[16:01:59] DEBUG | [anthropic] Model alias: claude-haiku -> claude-3-5-haiku-latest
[16:01:59] DEBUG | [anthropic] model=claude-3-5-haiku-latest prompt_len=854 timeout=5s
[16:01:59] DEBUG | [anthropic] PROMPT: You are a technical documentation search expert. Your task is to rewrite user questions as direct documentation lookup queries.

Guidelines:
1. Convert problem descriptions to feature/capability questions
2. Expand abbreviations and acronyms to full terms
3. Replace casual language with technical terminology
4. Align with documentation vocabulary and structure
5. Keep the rewritten query concise (one line)

Examples:
- "Why can't I schedule workflows every 30 seconds?" → "workflow scheduling min...
[16:01:59] DEBUG | [anthropic] REQUEST: POST https://api.anthropic.com/v1/messages
[16:01:59] DEBUG | [anthropic] REQUEST body: model=claude-3-5-haiku-latest max_tokens=1024
[16:02:00] DEBUG | [anthropic] RESPONSE status=200 elapsed=1.04s
[16:02:00] DEBUG | [anthropic] USAGE: input_tokens=196 output_tokens=11
[16:02:00] DEBUG | [anthropic] SUCCESS stop_reason=end_turn response_len=75
[16:02:00] DEBUG | [anthropic] RESPONSE: api gateway resource allocation sizing requirements hardware specifications
[16:02:01] TRACE | === FACT COVERAGE ANALYSIS ===
[16:02:01] TRACE | Query: What are the resource requirements for the API Gateway?
[16:02:01] TRACE | Total key facts: 1
[16:02:01] TRACE | Found facts: 0/1
[16:02:01] TRACE | Missed facts: 1/1
[16:02:01] TRACE |   MISSED[1] 2 vCPU, 4GB RAM per pod
[16:02:01] TRACE |     -> Found in chunk_id=architecture_overview_fix_1 (rank not in top-5)
[16:02:01] TRACE | === END FACT COVERAGE ===
[16:02:01] DEBUG | [anthropic] Model alias: claude-haiku -> claude-3-5-haiku-latest
[16:02:01] DEBUG | [anthropic] model=claude-3-5-haiku-latest prompt_len=849 timeout=5s
[16:02:01] DEBUG | [anthropic] PROMPT: You are a technical documentation search expert. Your task is to rewrite user questions as direct documentation lookup queries.

Guidelines:
1. Convert problem descriptions to feature/capability questions
2. Expand abbreviations and acronyms to full terms
3. Replace casual language with technical terminology
4. Align with documentation vocabulary and structure
5. Keep the rewritten query concise (one line)

Examples:
- "Why can't I schedule workflows every 30 seconds?" → "workflow scheduling min...
[16:02:01] DEBUG | [anthropic] REQUEST: POST https://api.anthropic.com/v1/messages
[16:02:01] DEBUG | [anthropic] REQUEST body: model=claude-3-5-haiku-latest max_tokens=1024
[16:02:02] DEBUG | [anthropic] RESPONSE status=200 elapsed=1.22s
[16:02:02] DEBUG | [anthropic] USAGE: input_tokens=197 output_tokens=13
[16:02:02] DEBUG | [anthropic] SUCCESS stop_reason=end_turn response_len=85
[16:02:02] DEBUG | [anthropic] RESPONSE: api gateway system requirements cpu memory resource allocation sizing recommendations
[16:02:02] TRACE | === FACT COVERAGE ANALYSIS ===
[16:02:02] TRACE | Query: How much CPU and memory does the API Gateway need?
[16:02:02] TRACE | Total key facts: 1
[16:02:02] TRACE | Found facts: 1/1
[16:02:02] TRACE |   FOUND[1] 2 vCPU, 4GB RAM per pod
[16:02:02] TRACE | === END FACT COVERAGE ===
[16:02:02] DEBUG | [anthropic] Model alias: claude-haiku -> claude-3-5-haiku-latest
[16:02:02] DEBUG | [anthropic] model=claude-3-5-haiku-latest prompt_len=842 timeout=5s
[16:02:02] DEBUG | [anthropic] PROMPT: You are a technical documentation search expert. Your task is to rewrite user questions as direct documentation lookup queries.

Guidelines:
1. Convert problem descriptions to feature/capability questions
2. Expand abbreviations and acronyms to full terms
3. Replace casual language with technical terminology
4. Align with documentation vocabulary and structure
5. Keep the rewritten query concise (one line)

Examples:
- "Why can't I schedule workflows every 30 seconds?" → "workflow scheduling min...
[16:02:02] DEBUG | [anthropic] REQUEST: POST https://api.anthropic.com/v1/messages
[16:02:02] DEBUG | [anthropic] REQUEST body: model=claude-3-5-haiku-latest max_tokens=1024
[16:02:04] DEBUG | [anthropic] RESPONSE status=200 elapsed=1.47s
[16:02:04] DEBUG | [anthropic] USAGE: input_tokens=196 output_tokens=20
[16:02:04] DEBUG | [anthropic] SUCCESS stop_reason=end_turn response_len=82
[16:02:04] DEBUG | [anthropic] RESPONSE: kubernetes api gateway pod out of memory oomkilled troubleshooting resource limits
[16:02:04] TRACE | === FACT COVERAGE ANALYSIS ===
[16:02:04] TRACE | Query: The API Gateway pods keep getting OOMKilled
[16:02:04] TRACE | Total key facts: 1
[16:02:04] TRACE | Found facts: 1/1
[16:02:04] TRACE |   FOUND[1] 2 vCPU, 4GB RAM per pod
[16:02:04] TRACE | === END FACT COVERAGE ===
[16:02:04] DEBUG | [anthropic] Model alias: claude-haiku -> claude-3-5-haiku-latest
[16:02:04] DEBUG | [anthropic] model=claude-3-5-haiku-latest prompt_len=820 timeout=5s
[16:02:04] DEBUG | [anthropic] PROMPT: You are a technical documentation search expert. Your task is to rewrite user questions as direct documentation lookup queries.

Guidelines:
1. Convert problem descriptions to feature/capability questions
2. Expand abbreviations and acronyms to full terms
3. Replace casual language with technical terminology
4. Align with documentation vocabulary and structure
5. Keep the rewritten query concise (one line)

Examples:
- "Why can't I schedule workflows every 30 seconds?" → "workflow scheduling min...
[16:02:04] DEBUG | [anthropic] REQUEST: POST https://api.anthropic.com/v1/messages
[16:02:04] DEBUG | [anthropic] REQUEST body: model=claude-3-5-haiku-latest max_tokens=1024
[16:02:05] DEBUG | [anthropic] RESPONSE status=200 elapsed=0.92s
[16:02:05] DEBUG | [anthropic] USAGE: input_tokens=189 output_tokens=9
[16:02:05] DEBUG | [anthropic] SUCCESS stop_reason=end_turn response_len=55
[16:02:05] DEBUG | [anthropic] RESPONSE: api gateway resource configuration management endpoints
[16:02:05] TRACE | === FACT COVERAGE ANALYSIS ===
[16:02:05] TRACE | Query: api gateway resources
[16:02:05] TRACE | Total key facts: 1
[16:02:05] TRACE | Found facts: 0/1
[16:02:05] TRACE | Missed facts: 1/1
[16:02:05] TRACE |   MISSED[1] 2 vCPU, 4GB RAM per pod
[16:02:05] TRACE |     -> Found in chunk_id=architecture_overview_fix_1 (rank not in top-5)
[16:02:05] TRACE | === END FACT COVERAGE ===
[16:02:05] DEBUG | [anthropic] Model alias: claude-haiku -> claude-3-5-haiku-latest
[16:02:05] DEBUG | [anthropic] model=claude-3-5-haiku-latest prompt_len=875 timeout=5s
[16:02:05] DEBUG | [anthropic] PROMPT: You are a technical documentation search expert. Your task is to rewrite user questions as direct documentation lookup queries.

Guidelines:
1. Convert problem descriptions to feature/capability questions
2. Expand abbreviations and acronyms to full terms
3. Replace casual language with technical terminology
4. Align with documentation vocabulary and structure
5. Keep the rewritten query concise (one line)

Examples:
- "Why can't I schedule workflows every 30 seconds?" → "workflow scheduling min...
[16:02:05] DEBUG | [anthropic] REQUEST: POST https://api.anthropic.com/v1/messages
[16:02:05] DEBUG | [anthropic] REQUEST body: model=claude-3-5-haiku-latest max_tokens=1024
[16:02:06] DEBUG | [anthropic] RESPONSE status=200 elapsed=0.91s
[16:02:06] DEBUG | [anthropic] USAGE: input_tokens=201 output_tokens=10
[16:02:06] DEBUG | [anthropic] SUCCESS stop_reason=end_turn response_len=62
[16:02:06] DEBUG | [anthropic] RESPONSE: api gateway compute resource specifications performance limits
[16:02:07] TRACE | === FACT COVERAGE ANALYSIS ===
[16:02:07] TRACE | Query: I'm provisioning infrastructure, what are the compute specs for API Gateway?
[16:02:07] TRACE | Total key facts: 1
[16:02:07] TRACE | Found facts: 0/1
[16:02:07] TRACE | Missed facts: 1/1
[16:02:07] TRACE |   MISSED[1] 2 vCPU, 4GB RAM per pod
[16:02:07] TRACE |     -> Found in chunk_id=architecture_overview_fix_1 (rank not in top-5)
[16:02:07] TRACE | === END FACT COVERAGE ===
[16:02:07] DEBUG | [anthropic] Model alias: claude-haiku -> claude-3-5-haiku-latest
[16:02:07] DEBUG | [anthropic] model=claude-3-5-haiku-latest prompt_len=841 timeout=5s
[16:02:07] DEBUG | [anthropic] PROMPT: You are a technical documentation search expert. Your task is to rewrite user questions as direct documentation lookup queries.

Guidelines:
1. Convert problem descriptions to feature/capability questions
2. Expand abbreviations and acronyms to full terms
3. Replace casual language with technical terminology
4. Align with documentation vocabulary and structure
5. Keep the rewritten query concise (one line)

Examples:
- "Why can't I schedule workflows every 30 seconds?" → "workflow scheduling min...
[16:02:07] DEBUG | [anthropic] REQUEST: POST https://api.anthropic.com/v1/messages
[16:02:07] DEBUG | [anthropic] REQUEST body: model=claude-3-5-haiku-latest max_tokens=1024
[16:02:08] DEBUG | [anthropic] RESPONSE status=200 elapsed=1.26s
[16:02:08] DEBUG | [anthropic] USAGE: input_tokens=198 output_tokens=12
[16:02:08] DEBUG | [anthropic] SUCCESS stop_reason=end_turn response_len=75
[16:02:08] DEBUG | [anthropic] RESPONSE: api gateway minimum memory requirements performance scalability constraints
[16:02:08] TRACE | === FACT COVERAGE ANALYSIS ===
[16:02:08] TRACE | Query: Why is 1GB RAM not enough for API Gateway?
[16:02:08] TRACE | Total key facts: 1
[16:02:08] TRACE | Found facts: 1/1
[16:02:08] TRACE |   FOUND[1] 2 vCPU, 4GB RAM per pod
[16:02:08] TRACE | === END FACT COVERAGE ===
[16:02:08] DEBUG | [anthropic] Model alias: claude-haiku -> claude-3-5-haiku-latest
[16:02:08] DEBUG | [anthropic] model=claude-3-5-haiku-latest prompt_len=835 timeout=5s
[16:02:08] DEBUG | [anthropic] PROMPT: You are a technical documentation search expert. Your task is to rewrite user questions as direct documentation lookup queries.

Guidelines:
1. Convert problem descriptions to feature/capability questions
2. Expand abbreviations and acronyms to full terms
3. Replace casual language with technical terminology
4. Align with documentation vocabulary and structure
5. Keep the rewritten query concise (one line)

Examples:
- "Why can't I schedule workflows every 30 seconds?" → "workflow scheduling min...
[16:02:08] DEBUG | [anthropic] REQUEST: POST https://api.anthropic.com/v1/messages
[16:02:08] DEBUG | [anthropic] REQUEST body: model=claude-3-5-haiku-latest max_tokens=1024
[16:02:10] DEBUG | [anthropic] RESPONSE status=200 elapsed=1.20s
[16:02:10] DEBUG | [anthropic] USAGE: input_tokens=193 output_tokens=11
[16:02:10] DEBUG | [anthropic] SUCCESS stop_reason=end_turn response_len=67
[16:02:10] DEBUG | [anthropic] RESPONSE: health check endpoint api availability monitoring diagnostic routes
[16:02:10] TRACE | === FACT COVERAGE ANALYSIS ===
[16:02:10] TRACE | Query: What are the health check endpoints?
[16:02:10] TRACE | Total key facts: 2
[16:02:10] TRACE | Found facts: 0/2
[16:02:10] TRACE | Missed facts: 2/2
[16:02:10] TRACE |   MISSED[1] /health
[16:02:10] TRACE |     -> Found in chunk_id=deployment_guide_fix_2 (rank not in top-5)
[16:02:10] TRACE |   MISSED[2] /ready
[16:02:10] TRACE |     -> Found in chunk_id=deployment_guide_fix_2 (rank not in top-5)
[16:02:10] TRACE | === END FACT COVERAGE ===
[16:02:10] DEBUG | [anthropic] Model alias: claude-haiku -> claude-3-5-haiku-latest
[16:02:10] DEBUG | [anthropic] model=claude-3-5-haiku-latest prompt_len=845 timeout=5s
[16:02:10] DEBUG | [anthropic] PROMPT: You are a technical documentation search expert. Your task is to rewrite user questions as direct documentation lookup queries.

Guidelines:
1. Convert problem descriptions to feature/capability questions
2. Expand abbreviations and acronyms to full terms
3. Replace casual language with technical terminology
4. Align with documentation vocabulary and structure
5. Keep the rewritten query concise (one line)

Examples:
- "Why can't I schedule workflows every 30 seconds?" → "workflow scheduling min...
[16:02:10] DEBUG | [anthropic] REQUEST: POST https://api.anthropic.com/v1/messages
[16:02:10] DEBUG | [anthropic] REQUEST body: model=claude-3-5-haiku-latest max_tokens=1024
[16:02:11] DEBUG | [anthropic] RESPONSE status=200 elapsed=0.78s
[16:02:11] DEBUG | [anthropic] USAGE: input_tokens=195 output_tokens=10
[16:02:11] DEBUG | [anthropic] SUCCESS stop_reason=end_turn response_len=56
[16:02:11] DEBUG | [anthropic] RESPONSE: health monitoring endpoint base urls configuration guide
[16:02:11] TRACE | === FACT COVERAGE ANALYSIS ===
[16:02:11] TRACE | Query: Which URLs should I use for health monitoring?
[16:02:11] TRACE | Total key facts: 2
[16:02:11] TRACE | Found facts: 0/2
[16:02:11] TRACE | Missed facts: 2/2
[16:02:11] TRACE |   MISSED[1] /health
[16:02:11] TRACE |     -> Found in chunk_id=deployment_guide_fix_2 (rank not in top-5)
[16:02:11] TRACE |   MISSED[2] /ready
[16:02:11] TRACE |     -> Found in chunk_id=deployment_guide_fix_2 (rank not in top-5)
[16:02:11] TRACE | === END FACT COVERAGE ===
[16:02:11] DEBUG | [anthropic] Model alias: claude-haiku -> claude-3-5-haiku-latest
[16:02:11] DEBUG | [anthropic] model=claude-3-5-haiku-latest prompt_len=841 timeout=5s
[16:02:11] DEBUG | [anthropic] PROMPT: You are a technical documentation search expert. Your task is to rewrite user questions as direct documentation lookup queries.

Guidelines:
1. Convert problem descriptions to feature/capability questions
2. Expand abbreviations and acronyms to full terms
3. Replace casual language with technical terminology
4. Align with documentation vocabulary and structure
5. Keep the rewritten query concise (one line)

Examples:
- "Why can't I schedule workflows every 30 seconds?" → "workflow scheduling min...
[16:02:11] DEBUG | [anthropic] REQUEST: POST https://api.anthropic.com/v1/messages
[16:02:11] DEBUG | [anthropic] REQUEST body: model=claude-3-5-haiku-latest max_tokens=1024
[16:02:12] DEBUG | [anthropic] RESPONSE status=200 elapsed=1.05s
[16:02:12] DEBUG | [anthropic] USAGE: input_tokens=194 output_tokens=15
[16:02:12] DEBUG | [anthropic] SUCCESS stop_reason=end_turn response_len=73
[16:02:12] DEBUG | [anthropic] RESPONSE: load balancer health check configuration failure troubleshooting criteria
[16:02:13] TRACE | === FACT COVERAGE ANALYSIS ===
[16:02:13] TRACE | Query: My load balancer health checks are failing
[16:02:13] TRACE | Total key facts: 2
[16:02:13] TRACE | Found facts: 2/2
[16:02:13] TRACE |   FOUND[1] /health
[16:02:13] TRACE |   FOUND[2] /ready
[16:02:13] TRACE | === END FACT COVERAGE ===
[16:02:13] DEBUG | [anthropic] Model alias: claude-haiku -> claude-3-5-haiku-latest
[16:02:13] DEBUG | [anthropic] model=claude-3-5-haiku-latest prompt_len=815 timeout=5s
[16:02:13] DEBUG | [anthropic] PROMPT: You are a technical documentation search expert. Your task is to rewrite user questions as direct documentation lookup queries.

Guidelines:
1. Convert problem descriptions to feature/capability questions
2. Expand abbreviations and acronyms to full terms
3. Replace casual language with technical terminology
4. Align with documentation vocabulary and structure
5. Keep the rewritten query concise (one line)

Examples:
- "Why can't I schedule workflows every 30 seconds?" → "workflow scheduling min...
[16:02:13] DEBUG | [anthropic] REQUEST: POST https://api.anthropic.com/v1/messages
[16:02:13] DEBUG | [anthropic] REQUEST body: model=claude-3-5-haiku-latest max_tokens=1024
[16:02:14] DEBUG | [anthropic] RESPONSE status=200 elapsed=1.18s
[16:02:14] DEBUG | [anthropic] USAGE: input_tokens=188 output_tokens=10
[16:02:14] DEBUG | [anthropic] SUCCESS stop_reason=end_turn response_len=54
[16:02:14] DEBUG | [anthropic] RESPONSE: API health check endpoint monitoring status diagnostic
[16:02:14] TRACE | === FACT COVERAGE ANALYSIS ===
[16:02:14] TRACE | Query: health endpoints
[16:02:14] TRACE | Total key facts: 2
[16:02:14] TRACE | Found facts: 0/2
[16:02:14] TRACE | Missed facts: 2/2
[16:02:14] TRACE |   MISSED[1] /health
[16:02:14] TRACE |     -> Found in chunk_id=deployment_guide_fix_2 (rank not in top-5)
[16:02:14] TRACE |   MISSED[2] /ready
[16:02:14] TRACE |     -> Found in chunk_id=deployment_guide_fix_2 (rank not in top-5)
[16:02:14] TRACE | === END FACT COVERAGE ===
[16:02:14] DEBUG | [anthropic] Model alias: claude-haiku -> claude-3-5-haiku-latest
[16:02:14] DEBUG | [anthropic] model=claude-3-5-haiku-latest prompt_len=861 timeout=5s
[16:02:14] DEBUG | [anthropic] PROMPT: You are a technical documentation search expert. Your task is to rewrite user questions as direct documentation lookup queries.

Guidelines:
1. Convert problem descriptions to feature/capability questions
2. Expand abbreviations and acronyms to full terms
3. Replace casual language with technical terminology
4. Align with documentation vocabulary and structure
5. Keep the rewritten query concise (one line)

Examples:
- "Why can't I schedule workflows every 30 seconds?" → "workflow scheduling min...
[16:02:14] DEBUG | [anthropic] REQUEST: POST https://api.anthropic.com/v1/messages
[16:02:14] DEBUG | [anthropic] REQUEST body: model=claude-3-5-haiku-latest max_tokens=1024
[16:02:15] DEBUG | [anthropic] RESPONSE status=200 elapsed=0.87s
[16:02:15] DEBUG | [anthropic] USAGE: input_tokens=196 output_tokens=12
[16:02:15] DEBUG | [anthropic] SUCCESS stop_reason=end_turn response_len=69
[16:02:15] DEBUG | [anthropic] RESPONSE: service health monitoring endpoints availability probes status checks
[16:02:15] TRACE | === FACT COVERAGE ANALYSIS ===
[16:02:15] TRACE | Query: Setting up monitoring, what endpoints indicate service health?
[16:02:15] TRACE | Total key facts: 2
[16:02:15] TRACE | Found facts: 0/2
[16:02:15] TRACE | Missed facts: 2/2
[16:02:15] TRACE |   MISSED[1] /health
[16:02:15] TRACE |     -> Found in chunk_id=deployment_guide_fix_2 (rank not in top-5)
[16:02:15] TRACE |   MISSED[2] /ready
[16:02:15] TRACE |     -> Found in chunk_id=deployment_guide_fix_2 (rank not in top-5)
[16:02:15] TRACE | === END FACT COVERAGE ===
[16:02:15] DEBUG | [anthropic] Model alias: claude-haiku -> claude-3-5-haiku-latest
[16:02:15] DEBUG | [anthropic] model=claude-3-5-haiku-latest prompt_len=845 timeout=5s
[16:02:15] DEBUG | [anthropic] PROMPT: You are a technical documentation search expert. Your task is to rewrite user questions as direct documentation lookup queries.

Guidelines:
1. Convert problem descriptions to feature/capability questions
2. Expand abbreviations and acronyms to full terms
3. Replace casual language with technical terminology
4. Align with documentation vocabulary and structure
5. Keep the rewritten query concise (one line)

Examples:
- "Why can't I schedule workflows every 30 seconds?" → "workflow scheduling min...
[16:02:15] DEBUG | [anthropic] REQUEST: POST https://api.anthropic.com/v1/messages
[16:02:15] DEBUG | [anthropic] REQUEST body: model=claude-3-5-haiku-latest max_tokens=1024
[16:02:16] DEBUG | [anthropic] RESPONSE status=200 elapsed=1.03s
[16:02:16] DEBUG | [anthropic] USAGE: input_tokens=195 output_tokens=14
[16:02:16] DEBUG | [anthropic] SUCCESS stop_reason=end_turn response_len=63
[16:02:16] DEBUG | [anthropic] RESPONSE: "endpoint /status health check response details API monitoring"
[16:02:17] TRACE | === FACT COVERAGE ANALYSIS ===
[16:02:17] TRACE | Query: Why doesn't /status return health information?
[16:02:17] TRACE | Total key facts: 2
[16:02:17] TRACE | Found facts: 2/2
[16:02:17] TRACE |   FOUND[1] /health
[16:02:17] TRACE |   FOUND[2] /ready
[16:02:17] TRACE | === END FACT COVERAGE ===
[16:02:17] DEBUG | [anthropic] Model alias: claude-haiku -> claude-3-5-haiku-latest
[16:02:17] DEBUG | [anthropic] model=claude-3-5-haiku-latest prompt_len=835 timeout=5s
[16:02:17] DEBUG | [anthropic] PROMPT: You are a technical documentation search expert. Your task is to rewrite user questions as direct documentation lookup queries.

Guidelines:
1. Convert problem descriptions to feature/capability questions
2. Expand abbreviations and acronyms to full terms
3. Replace casual language with technical terminology
4. Align with documentation vocabulary and structure
5. Keep the rewritten query concise (one line)

Examples:
- "Why can't I schedule workflows every 30 seconds?" → "workflow scheduling min...
[16:02:17] DEBUG | [anthropic] REQUEST: POST https://api.anthropic.com/v1/messages
[16:02:17] DEBUG | [anthropic] REQUEST body: model=claude-3-5-haiku-latest max_tokens=1024
[16:02:18] DEBUG | [anthropic] RESPONSE status=200 elapsed=0.93s
[16:02:18] DEBUG | [anthropic] USAGE: input_tokens=194 output_tokens=12
[16:02:18] DEBUG | [anthropic] SUCCESS stop_reason=end_turn response_len=69
[16:02:18] DEBUG | [anthropic] RESPONSE: horizontal pod autoscaler kubernetes scaling parameters configuration
[16:02:18] TRACE | === FACT COVERAGE ANALYSIS ===
[16:02:18] TRACE | Query: What are the HPA scaling parameters?
[16:02:18] TRACE | Total key facts: 3
[16:02:18] TRACE | Found facts: 0/3
[16:02:18] TRACE | Missed facts: 3/3
[16:02:18] TRACE |   MISSED[1] minReplicas: 3
[16:02:18] TRACE |     -> Found in chunk_id=deployment_guide_fix_1 (rank not in top-5)
[16:02:18] TRACE |   MISSED[2] maxReplicas: 10
[16:02:18] TRACE |     -> Found in chunk_id=deployment_guide_fix_1 (rank not in top-5)
[16:02:18] TRACE |   MISSED[3] targetCPUUtilizationPercentage: 70
[16:02:18] TRACE |     -> Found in chunk_id=deployment_guide_fix_1 (rank not in top-5)
[16:02:18] TRACE | === END FACT COVERAGE ===
[16:02:18] DEBUG | [anthropic] Model alias: claude-haiku -> claude-3-5-haiku-latest
[16:02:18] DEBUG | [anthropic] model=claude-3-5-haiku-latest prompt_len=840 timeout=5s
[16:02:18] DEBUG | [anthropic] PROMPT: You are a technical documentation search expert. Your task is to rewrite user questions as direct documentation lookup queries.

Guidelines:
1. Convert problem descriptions to feature/capability questions
2. Expand abbreviations and acronyms to full terms
3. Replace casual language with technical terminology
4. Align with documentation vocabulary and structure
5. Keep the rewritten query concise (one line)

Examples:
- "Why can't I schedule workflows every 30 seconds?" → "workflow scheduling min...
[16:02:18] DEBUG | [anthropic] REQUEST: POST https://api.anthropic.com/v1/messages
[16:02:18] DEBUG | [anthropic] REQUEST body: model=claude-3-5-haiku-latest max_tokens=1024
[16:02:19] DEBUG | [anthropic] RESPONSE status=200 elapsed=1.17s
[16:02:19] DEBUG | [anthropic] USAGE: input_tokens=195 output_tokens=17
[16:02:19] DEBUG | [anthropic] SUCCESS stop_reason=end_turn response_len=97
[16:02:19] DEBUG | [anthropic] RESPONSE: horizontal pod autoscaler kubernetes scaling mechanism metrics resource utilization replica count
[16:02:20] TRACE | === FACT COVERAGE ANALYSIS ===
[16:02:20] TRACE | Query: How does horizontal pod autoscaling work?
[16:02:20] TRACE | Total key facts: 3
[16:02:20] TRACE | Found facts: 0/3
[16:02:20] TRACE | Missed facts: 3/3
[16:02:20] TRACE |   MISSED[1] minReplicas: 3
[16:02:20] TRACE |     -> Found in chunk_id=deployment_guide_fix_1 (rank not in top-5)
[16:02:20] TRACE |   MISSED[2] maxReplicas: 10
[16:02:20] TRACE |     -> Found in chunk_id=deployment_guide_fix_1 (rank not in top-5)
[16:02:20] TRACE |   MISSED[3] targetCPUUtilizationPercentage: 70
[16:02:20] TRACE |     -> Found in chunk_id=deployment_guide_fix_1 (rank not in top-5)
[16:02:20] TRACE | === END FACT COVERAGE ===
[16:02:20] DEBUG | [anthropic] Model alias: claude-haiku -> claude-3-5-haiku-latest
[16:02:20] DEBUG | [anthropic] model=claude-3-5-haiku-latest prompt_len=843 timeout=5s
[16:02:20] DEBUG | [anthropic] PROMPT: You are a technical documentation search expert. Your task is to rewrite user questions as direct documentation lookup queries.

Guidelines:
1. Convert problem descriptions to feature/capability questions
2. Expand abbreviations and acronyms to full terms
3. Replace casual language with technical terminology
4. Align with documentation vocabulary and structure
5. Keep the rewritten query concise (one line)

Examples:
- "Why can't I schedule workflows every 30 seconds?" → "workflow scheduling min...
[16:02:20] DEBUG | [anthropic] REQUEST: POST https://api.anthropic.com/v1/messages
[16:02:20] DEBUG | [anthropic] REQUEST body: model=claude-3-5-haiku-latest max_tokens=1024
[16:02:21] DEBUG | [anthropic] RESPONSE status=200 elapsed=0.98s
[16:02:21] DEBUG | [anthropic] USAGE: input_tokens=196 output_tokens=13
[16:02:21] DEBUG | [anthropic] SUCCESS stop_reason=end_turn response_len=77
[16:02:21] DEBUG | [anthropic] RESPONSE: kubernetes horizontal pod autoscaler traffic scaling configuration parameters
[16:02:21] TRACE | === FACT COVERAGE ANALYSIS ===
[16:02:21] TRACE | Query: Pods aren't scaling up during traffic spikes
[16:02:21] TRACE | Total key facts: 3
[16:02:21] TRACE | Found facts: 2/3
[16:02:21] TRACE |   FOUND[1] minReplicas: 3
[16:02:21] TRACE |   FOUND[2] maxReplicas: 10
[16:02:21] TRACE | Missed facts: 1/3
[16:02:21] TRACE |   MISSED[1] targetCPUUtilizationPercentage: 70
[16:02:21] TRACE |     -> Found in chunk_id=deployment_guide_fix_1 (rank not in top-5)
[16:02:21] TRACE | === END FACT COVERAGE ===
[16:02:21] DEBUG | [anthropic] Model alias: claude-haiku -> claude-3-5-haiku-latest
[16:02:21] DEBUG | [anthropic] model=claude-3-5-haiku-latest prompt_len=817 timeout=5s
[16:02:21] DEBUG | [anthropic] PROMPT: You are a technical documentation search expert. Your task is to rewrite user questions as direct documentation lookup queries.

Guidelines:
1. Convert problem descriptions to feature/capability questions
2. Expand abbreviations and acronyms to full terms
3. Replace casual language with technical terminology
4. Align with documentation vocabulary and structure
5. Keep the rewritten query concise (one line)

Examples:
- "Why can't I schedule workflows every 30 seconds?" → "workflow scheduling min...
[16:02:21] DEBUG | [anthropic] REQUEST: POST https://api.anthropic.com/v1/messages
[16:02:21] DEBUG | [anthropic] REQUEST body: model=claude-3-5-haiku-latest max_tokens=1024
[16:02:22] DEBUG | [anthropic] RESPONSE status=200 elapsed=1.01s
[16:02:22] DEBUG | [anthropic] USAGE: input_tokens=190 output_tokens=11
[16:02:22] DEBUG | [anthropic] SUCCESS stop_reason=end_turn response_len=58
[16:02:22] DEBUG | [anthropic] RESPONSE: autoscaling configuration scaling policy parameters limits
[16:02:23] TRACE | === FACT COVERAGE ANALYSIS ===
[16:02:23] TRACE | Query: autoscaling config
[16:02:23] TRACE | Total key facts: 3
[16:02:23] TRACE | Found facts: 3/3
[16:02:23] TRACE |   FOUND[1] minReplicas: 3
[16:02:23] TRACE |   FOUND[2] maxReplicas: 10
[16:02:23] TRACE |   FOUND[3] targetCPUUtilizationPercentage: 70
[16:02:23] TRACE | === END FACT COVERAGE ===
[16:02:23] DEBUG | [anthropic] Model alias: claude-haiku -> claude-3-5-haiku-latest
[16:02:23] DEBUG | [anthropic] model=claude-3-5-haiku-latest prompt_len=877 timeout=5s
[16:02:23] DEBUG | [anthropic] PROMPT: You are a technical documentation search expert. Your task is to rewrite user questions as direct documentation lookup queries.

Guidelines:
1. Convert problem descriptions to feature/capability questions
2. Expand abbreviations and acronyms to full terms
3. Replace casual language with technical terminology
4. Align with documentation vocabulary and structure
5. Keep the rewritten query concise (one line)

Examples:
- "Why can't I schedule workflows every 30 seconds?" → "workflow scheduling min...
[16:02:23] DEBUG | [anthropic] REQUEST: POST https://api.anthropic.com/v1/messages
[16:02:23] DEBUG | [anthropic] REQUEST body: model=claude-3-5-haiku-latest max_tokens=1024
[16:02:24] DEBUG | [anthropic] RESPONSE status=200 elapsed=1.01s
[16:02:24] DEBUG | [anthropic] USAGE: input_tokens=207 output_tokens=16
[16:02:24] DEBUG | [anthropic] SUCCESS stop_reason=end_turn response_len=81
[16:02:24] DEBUG | [anthropic] RESPONSE: autoscaling configuration minimum maximum replica count scaling thresholds policy
[16:02:24] TRACE | === FACT COVERAGE ANALYSIS ===
[16:02:24] TRACE | Query: I need to configure autoscaling, what are the min/max replicas and thresholds?
[16:02:24] TRACE | Total key facts: 3
[16:02:24] TRACE | Found facts: 0/3
[16:02:24] TRACE | Missed facts: 3/3
[16:02:24] TRACE |   MISSED[1] minReplicas: 3
[16:02:24] TRACE |     -> Found in chunk_id=deployment_guide_fix_1 (rank not in top-5)
[16:02:24] TRACE |   MISSED[2] maxReplicas: 10
[16:02:24] TRACE |     -> Found in chunk_id=deployment_guide_fix_1 (rank not in top-5)
[16:02:24] TRACE |   MISSED[3] targetCPUUtilizationPercentage: 70
[16:02:24] TRACE |     -> Found in chunk_id=deployment_guide_fix_1 (rank not in top-5)
[16:02:24] TRACE | === END FACT COVERAGE ===
[16:02:24] DEBUG | [anthropic] Model alias: claude-haiku -> claude-3-5-haiku-latest
[16:02:24] DEBUG | [anthropic] model=claude-3-5-haiku-latest prompt_len=847 timeout=5s
[16:02:24] DEBUG | [anthropic] PROMPT: You are a technical documentation search expert. Your task is to rewrite user questions as direct documentation lookup queries.

Guidelines:
1. Convert problem descriptions to feature/capability questions
2. Expand abbreviations and acronyms to full terms
3. Replace casual language with technical terminology
4. Align with documentation vocabulary and structure
5. Keep the rewritten query concise (one line)

Examples:
- "Why can't I schedule workflows every 30 seconds?" → "workflow scheduling min...
[16:02:24] DEBUG | [anthropic] REQUEST: POST https://api.anthropic.com/v1/messages
[16:02:24] DEBUG | [anthropic] REQUEST body: model=claude-3-5-haiku-latest max_tokens=1024
[16:02:25] DEBUG | [anthropic] RESPONSE status=200 elapsed=1.08s
[16:02:25] DEBUG | [anthropic] USAGE: input_tokens=200 output_tokens=13
[16:02:25] DEBUG | [anthropic] SUCCESS stop_reason=end_turn response_len=78
[16:02:25] DEBUG | [anthropic] RESPONSE: database replication factor minimum replica count traffic volume configuration
[16:02:25] TRACE | === FACT COVERAGE ANALYSIS ===
[16:02:25] TRACE | Query: Why do we have 3 replicas even with low traffic?
[16:02:25] TRACE | Total key facts: 3
[16:02:25] TRACE | Found facts: 0/3
[16:02:25] TRACE | Missed facts: 3/3
[16:02:25] TRACE |   MISSED[1] minReplicas: 3
[16:02:25] TRACE |     -> Found in chunk_id=deployment_guide_fix_1 (rank not in top-5)
[16:02:25] TRACE |   MISSED[2] maxReplicas: 10
[16:02:25] TRACE |     -> Found in chunk_id=deployment_guide_fix_1 (rank not in top-5)
[16:02:25] TRACE |   MISSED[3] targetCPUUtilizationPercentage: 70
[16:02:25] TRACE |     -> Found in chunk_id=deployment_guide_fix_1 (rank not in top-5)
[16:02:25] TRACE | === END FACT COVERAGE ===
[16:02:25] DEBUG | [anthropic] Model alias: claude-haiku -> claude-3-5-haiku-latest
[16:02:25] DEBUG | [anthropic] model=claude-3-5-haiku-latest prompt_len=849 timeout=5s
[16:02:25] DEBUG | [anthropic] PROMPT: You are a technical documentation search expert. Your task is to rewrite user questions as direct documentation lookup queries.

Guidelines:
1. Convert problem descriptions to feature/capability questions
2. Expand abbreviations and acronyms to full terms
3. Replace casual language with technical terminology
4. Align with documentation vocabulary and structure
5. Keep the rewritten query concise (one line)

Examples:
- "Why can't I schedule workflows every 30 seconds?" → "workflow scheduling min...
[16:02:25] DEBUG | [anthropic] REQUEST: POST https://api.anthropic.com/v1/messages
[16:02:25] DEBUG | [anthropic] REQUEST body: model=claude-3-5-haiku-latest max_tokens=1024
[16:02:27] DEBUG | [anthropic] RESPONSE status=200 elapsed=1.09s
[16:02:27] DEBUG | [anthropic] USAGE: input_tokens=199 output_tokens=14
[16:02:27] DEBUG | [anthropic] SUCCESS stop_reason=end_turn response_len=58
[16:02:27] DEBUG | [anthropic] RESPONSE: api performance p99 latency service level objective target
[16:02:27] TRACE | === FACT COVERAGE ANALYSIS ===
[16:02:27] TRACE | Query: What is the P99 latency target for API operations?
[16:02:27] TRACE | Total key facts: 2
[16:02:27] TRACE | Found facts: 2/2
[16:02:27] TRACE |   FOUND[1] P99 latency: < 200ms
[16:02:27] TRACE |   FOUND[2] average P99 latency of 180ms for API operations
[16:02:27] TRACE | === END FACT COVERAGE ===
[16:02:27] DEBUG | [anthropic] Model alias: claude-haiku -> claude-3-5-haiku-latest
[16:02:27] DEBUG | [anthropic] model=claude-3-5-haiku-latest prompt_len=847 timeout=5s
[16:02:27] DEBUG | [anthropic] PROMPT: You are a technical documentation search expert. Your task is to rewrite user questions as direct documentation lookup queries.

Guidelines:
1. Convert problem descriptions to feature/capability questions
2. Expand abbreviations and acronyms to full terms
3. Replace casual language with technical terminology
4. Align with documentation vocabulary and structure
5. Keep the rewritten query concise (one line)

Examples:
- "Why can't I schedule workflows every 30 seconds?" → "workflow scheduling min...
[16:02:27] DEBUG | [anthropic] REQUEST: POST https://api.anthropic.com/v1/messages
[16:02:27] DEBUG | [anthropic] REQUEST body: model=claude-3-5-haiku-latest max_tokens=1024
[16:02:28] DEBUG | [anthropic] RESPONSE status=200 elapsed=1.10s
[16:02:28] DEBUG | [anthropic] USAGE: input_tokens=198 output_tokens=15
[16:02:28] DEBUG | [anthropic] SUCCESS stop_reason=end_turn response_len=73
[16:02:28] DEBUG | [anthropic] RESPONSE: service level objective performance metrics 99th percentile response time
[16:02:28] TRACE | === FACT COVERAGE ANALYSIS ===
[16:02:28] TRACE | Query: What's the 99th percentile response time target?
[16:02:28] TRACE | Total key facts: 2
[16:02:28] TRACE | Found facts: 1/2
[16:02:28] TRACE |   FOUND[1] P99 latency: < 200ms
[16:02:28] TRACE | Missed facts: 1/2
[16:02:28] TRACE |   MISSED[1] average P99 latency of 180ms for API operations
[16:02:28] TRACE |     -> Found in chunk_id=architecture_overview_fix_0 (rank not in top-5)
[16:02:28] TRACE | === END FACT COVERAGE ===
[16:02:28] DEBUG | [anthropic] Model alias: claude-haiku -> claude-3-5-haiku-latest
[16:02:28] DEBUG | [anthropic] model=claude-3-5-haiku-latest prompt_len=844 timeout=5s
[16:02:28] DEBUG | [anthropic] PROMPT: You are a technical documentation search expert. Your task is to rewrite user questions as direct documentation lookup queries.

Guidelines:
1. Convert problem descriptions to feature/capability questions
2. Expand abbreviations and acronyms to full terms
3. Replace casual language with technical terminology
4. Align with documentation vocabulary and structure
5. Keep the rewritten query concise (one line)

Examples:
- "Why can't I schedule workflows every 30 seconds?" → "workflow scheduling min...
[16:02:28] DEBUG | [anthropic] REQUEST: POST https://api.anthropic.com/v1/messages
[16:02:28] DEBUG | [anthropic] REQUEST body: model=claude-3-5-haiku-latest max_tokens=1024
[16:02:29] DEBUG | [anthropic] RESPONSE status=200 elapsed=0.87s
[16:02:29] DEBUG | [anthropic] USAGE: input_tokens=199 output_tokens=10
[16:02:29] DEBUG | [anthropic] SUCCESS stop_reason=end_turn response_len=54
[16:02:29] DEBUG | [anthropic] RESPONSE: api performance latency acceptable threshold benchmark
[16:02:30] TRACE | === FACT COVERAGE ANALYSIS ===
[16:02:30] TRACE | Query: Our API latency is 500ms, is that acceptable?
[16:02:30] TRACE | Total key facts: 2
[16:02:30] TRACE | Found facts: 0/2
[16:02:30] TRACE | Missed facts: 2/2
[16:02:30] TRACE |   MISSED[1] P99 latency: < 200ms
[16:02:30] TRACE |     -> Found in chunk_id=architecture_overview_fix_1 (rank not in top-5)
[16:02:30] TRACE |   MISSED[2] average P99 latency of 180ms for API operations
[16:02:30] TRACE |     -> Found in chunk_id=architecture_overview_fix_0 (rank not in top-5)
[16:02:30] TRACE | === END FACT COVERAGE ===
[16:02:30] DEBUG | [anthropic] Model alias: claude-haiku -> claude-3-5-haiku-latest
[16:02:30] DEBUG | [anthropic] model=claude-3-5-haiku-latest prompt_len=817 timeout=5s
[16:02:30] DEBUG | [anthropic] PROMPT: You are a technical documentation search expert. Your task is to rewrite user questions as direct documentation lookup queries.

Guidelines:
1. Convert problem descriptions to feature/capability questions
2. Expand abbreviations and acronyms to full terms
3. Replace casual language with technical terminology
4. Align with documentation vocabulary and structure
5. Keep the rewritten query concise (one line)

Examples:
- "Why can't I schedule workflows every 30 seconds?" → "workflow scheduling min...
[16:02:30] DEBUG | [anthropic] REQUEST: POST https://api.anthropic.com/v1/messages
[16:02:30] DEBUG | [anthropic] REQUEST body: model=claude-3-5-haiku-latest max_tokens=1024
[16:02:31] DEBUG | [anthropic] RESPONSE status=200 elapsed=0.95s
[16:02:31] DEBUG | [anthropic] USAGE: input_tokens=190 output_tokens=14
[16:02:31] DEBUG | [anthropic] SUCCESS stop_reason=end_turn response_len=76
[16:02:31] DEBUG | [anthropic] RESPONSE: api performance service level objective latency target maximum response time
[16:02:31] TRACE | === FACT COVERAGE ANALYSIS ===
[16:02:31] TRACE | Query: api latency target
[16:02:31] TRACE | Total key facts: 2
[16:02:31] TRACE | Found facts: 1/2
[16:02:31] TRACE |   FOUND[1] P99 latency: < 200ms
[16:02:31] TRACE | Missed facts: 1/2
[16:02:31] TRACE |   MISSED[1] average P99 latency of 180ms for API operations
[16:02:31] TRACE |     -> Found in chunk_id=architecture_overview_fix_0 (rank not in top-5)
[16:02:31] TRACE | === END FACT COVERAGE ===
[16:02:31] DEBUG | [anthropic] Model alias: claude-haiku -> claude-3-5-haiku-latest
[16:02:31] DEBUG | [anthropic] model=claude-3-5-haiku-latest prompt_len=871 timeout=5s
[16:02:31] DEBUG | [anthropic] PROMPT: You are a technical documentation search expert. Your task is to rewrite user questions as direct documentation lookup queries.

Guidelines:
1. Convert problem descriptions to feature/capability questions
2. Expand abbreviations and acronyms to full terms
3. Replace casual language with technical terminology
4. Align with documentation vocabulary and structure
5. Keep the rewritten query concise (one line)

Examples:
- "Why can't I schedule workflows every 30 seconds?" → "workflow scheduling min...
[16:02:31] DEBUG | [anthropic] REQUEST: POST https://api.anthropic.com/v1/messages
[16:02:31] DEBUG | [anthropic] REQUEST body: model=claude-3-5-haiku-latest max_tokens=1024
[16:02:32] DEBUG | [anthropic] RESPONSE status=200 elapsed=0.92s
[16:02:32] DEBUG | [anthropic] USAGE: input_tokens=205 output_tokens=15
[16:02:32] DEBUG | [anthropic] SUCCESS stop_reason=end_turn response_len=67
[16:02:32] DEBUG | [anthropic] RESPONSE: cloudflow service level objective p99 latency performance guarantee
[16:02:32] TRACE | === FACT COVERAGE ANALYSIS ===
[16:02:32] TRACE | Query: Setting SLOs for our service, what P99 latency does CloudFlow guarantee?
[16:02:32] TRACE | Total key facts: 2
[16:02:32] TRACE | Found facts: 1/2
[16:02:32] TRACE |   FOUND[1] average P99 latency of 180ms for API operations
[16:02:32] TRACE | Missed facts: 1/2
[16:02:32] TRACE |   MISSED[1] P99 latency: < 200ms
[16:02:32] TRACE |     -> Found in chunk_id=architecture_overview_fix_1 (rank not in top-5)
[16:02:32] TRACE | === END FACT COVERAGE ===
[16:02:32] DEBUG | [anthropic] Model alias: claude-haiku -> claude-3-5-haiku-latest
[16:02:32] DEBUG | [anthropic] model=claude-3-5-haiku-latest prompt_len=842 timeout=5s
[16:02:32] DEBUG | [anthropic] PROMPT: You are a technical documentation search expert. Your task is to rewrite user questions as direct documentation lookup queries.

Guidelines:
1. Convert problem descriptions to feature/capability questions
2. Expand abbreviations and acronyms to full terms
3. Replace casual language with technical terminology
4. Align with documentation vocabulary and structure
5. Keep the rewritten query concise (one line)

Examples:
- "Why can't I schedule workflows every 30 seconds?" → "workflow scheduling min...
[16:02:32] DEBUG | [anthropic] REQUEST: POST https://api.anthropic.com/v1/messages
[16:02:32] DEBUG | [anthropic] REQUEST body: model=claude-3-5-haiku-latest max_tokens=1024
[16:02:33] DEBUG | [anthropic] RESPONSE status=200 elapsed=0.95s
[16:02:33] DEBUG | [anthropic] USAGE: input_tokens=198 output_tokens=10
[16:02:33] DEBUG | [anthropic] SUCCESS stop_reason=end_turn response_len=60
[16:02:33] DEBUG | [anthropic] RESPONSE: performance monitoring latency threshold alert configuration
[16:02:34] TRACE | === FACT COVERAGE ANALYSIS ===
[16:02:34] TRACE | Query: Why are we getting alerts at 200ms latency?
[16:02:34] TRACE | Total key facts: 2
[16:02:34] TRACE | Found facts: 0/2
[16:02:34] TRACE | Missed facts: 2/2
[16:02:34] TRACE |   MISSED[1] P99 latency: < 200ms
[16:02:34] TRACE |     -> Found in chunk_id=architecture_overview_fix_1 (rank not in top-5)
[16:02:34] TRACE |   MISSED[2] average P99 latency of 180ms for API operations
[16:02:34] TRACE |     -> Found in chunk_id=architecture_overview_fix_0 (rank not in top-5)
[16:02:34] TRACE | === END FACT COVERAGE ===
[16:02:34] DEBUG | [anthropic] Model alias: claude-haiku -> claude-3-5-haiku-latest
[16:02:34] DEBUG | [anthropic] model=claude-3-5-haiku-latest prompt_len=849 timeout=5s
[16:02:34] DEBUG | [anthropic] PROMPT: You are a technical documentation search expert. Your task is to rewrite user questions as direct documentation lookup queries.

Guidelines:
1. Convert problem descriptions to feature/capability questions
2. Expand abbreviations and acronyms to full terms
3. Replace casual language with technical terminology
4. Align with documentation vocabulary and structure
5. Keep the rewritten query concise (one line)

Examples:
- "Why can't I schedule workflows every 30 seconds?" → "workflow scheduling min...
[16:02:34] DEBUG | [anthropic] REQUEST: POST https://api.anthropic.com/v1/messages
[16:02:34] DEBUG | [anthropic] REQUEST body: model=claude-3-5-haiku-latest max_tokens=1024
[16:02:35] DEBUG | [anthropic] RESPONSE status=200 elapsed=0.99s
[16:02:35] DEBUG | [anthropic] USAGE: input_tokens=198 output_tokens=13
[16:02:35] DEBUG | [anthropic] SUCCESS stop_reason=end_turn response_len=87
[16:02:35] DEBUG | [anthropic] RESPONSE: disaster recovery recovery point objective recovery time objective configuration values
[16:02:35] TRACE | === FACT COVERAGE ANALYSIS ===
[16:02:35] TRACE | Query: What are the disaster recovery RPO and RTO values?
[16:02:35] TRACE | Total key facts: 2
[16:02:35] TRACE | Found facts: 0/2
[16:02:35] TRACE | Missed facts: 2/2
[16:02:35] TRACE |   MISSED[1] RPO (Recovery Point Objective): 1 hour
[16:02:35] TRACE |     -> Found in chunk_id=architecture_overview_fix_10 (rank not in top-5)
[16:02:35] TRACE |   MISSED[2] RTO (Recovery Time Objective): 4 hours
[16:02:35] TRACE |     -> Found in chunk_id=architecture_overview_fix_10 (rank not in top-5)
[16:02:35] TRACE | === END FACT COVERAGE ===
[16:02:35] DEBUG | [anthropic] Model alias: claude-haiku -> claude-3-5-haiku-latest
[16:02:35] DEBUG | [anthropic] model=claude-3-5-haiku-latest prompt_len=846 timeout=5s
[16:02:35] DEBUG | [anthropic] PROMPT: You are a technical documentation search expert. Your task is to rewrite user questions as direct documentation lookup queries.

Guidelines:
1. Convert problem descriptions to feature/capability questions
2. Expand abbreviations and acronyms to full terms
3. Replace casual language with technical terminology
4. Align with documentation vocabulary and structure
5. Keep the rewritten query concise (one line)

Examples:
- "Why can't I schedule workflows every 30 seconds?" → "workflow scheduling min...
[16:02:35] DEBUG | [anthropic] REQUEST: POST https://api.anthropic.com/v1/messages
[16:02:35] DEBUG | [anthropic] REQUEST body: model=claude-3-5-haiku-latest max_tokens=1024
[16:02:36] DEBUG | [anthropic] RESPONSE status=200 elapsed=0.91s
[16:02:36] DEBUG | [anthropic] USAGE: input_tokens=196 output_tokens=16
[16:02:36] DEBUG | [anthropic] SUCCESS stop_reason=end_turn response_len=91
[16:02:36] DEBUG | [anthropic] RESPONSE: recovery point objective recovery time objective maximum data loss and recovery time limits
[16:02:36] TRACE | === FACT COVERAGE ANALYSIS ===
[16:02:36] TRACE | Query: What's the maximum data loss and recovery time?
[16:02:36] TRACE | Total key facts: 2
[16:02:36] TRACE | Found facts: 0/2
[16:02:36] TRACE | Missed facts: 2/2
[16:02:36] TRACE |   MISSED[1] RPO (Recovery Point Objective): 1 hour
[16:02:36] TRACE |     -> Found in chunk_id=architecture_overview_fix_10 (rank not in top-5)
[16:02:36] TRACE |   MISSED[2] RTO (Recovery Time Objective): 4 hours
[16:02:36] TRACE |     -> Found in chunk_id=architecture_overview_fix_10 (rank not in top-5)
[16:02:36] TRACE | === END FACT COVERAGE ===
[16:02:36] DEBUG | [anthropic] Model alias: claude-haiku -> claude-3-5-haiku-latest
[16:02:36] DEBUG | [anthropic] model=claude-3-5-haiku-latest prompt_len=848 timeout=5s
[16:02:36] DEBUG | [anthropic] PROMPT: You are a technical documentation search expert. Your task is to rewrite user questions as direct documentation lookup queries.

Guidelines:
1. Convert problem descriptions to feature/capability questions
2. Expand abbreviations and acronyms to full terms
3. Replace casual language with technical terminology
4. Align with documentation vocabulary and structure
5. Keep the rewritten query concise (one line)

Examples:
- "Why can't I schedule workflows every 30 seconds?" → "workflow scheduling min...
[16:02:36] DEBUG | [anthropic] REQUEST: POST https://api.anthropic.com/v1/messages
[16:02:36] DEBUG | [anthropic] REQUEST body: model=claude-3-5-haiku-latest max_tokens=1024
[16:02:37] DEBUG | [anthropic] RESPONSE status=200 elapsed=0.90s
[16:02:37] DEBUG | [anthropic] USAGE: input_tokens=197 output_tokens=14
[16:02:37] DEBUG | [anthropic] SUCCESS stop_reason=end_turn response_len=95
[16:02:37] DEBUG | [anthropic] RESPONSE: recovery point objective recovery time objective disaster recovery planning performance metrics
[16:02:38] TRACE | === FACT COVERAGE ANALYSIS ===
[16:02:38] TRACE | Query: How long will it take to recover from a disaster?
[16:02:38] TRACE | Total key facts: 2
[16:02:38] TRACE | Found facts: 0/2
[16:02:38] TRACE | Missed facts: 2/2
[16:02:38] TRACE |   MISSED[1] RPO (Recovery Point Objective): 1 hour
[16:02:38] TRACE |     -> Found in chunk_id=architecture_overview_fix_10 (rank not in top-5)
[16:02:38] TRACE |   MISSED[2] RTO (Recovery Time Objective): 4 hours
[16:02:38] TRACE |     -> Found in chunk_id=architecture_overview_fix_10 (rank not in top-5)
[16:02:38] TRACE | === END FACT COVERAGE ===
[16:02:38] DEBUG | [anthropic] Model alias: claude-haiku -> claude-3-5-haiku-latest
[16:02:38] DEBUG | [anthropic] model=claude-3-5-haiku-latest prompt_len=806 timeout=5s
[16:02:38] DEBUG | [anthropic] PROMPT: You are a technical documentation search expert. Your task is to rewrite user questions as direct documentation lookup queries.

Guidelines:
1. Convert problem descriptions to feature/capability questions
2. Expand abbreviations and acronyms to full terms
3. Replace casual language with technical terminology
4. Align with documentation vocabulary and structure
5. Keep the rewritten query concise (one line)

Examples:
- "Why can't I schedule workflows every 30 seconds?" → "workflow scheduling min...
[16:02:38] DEBUG | [anthropic] REQUEST: POST https://api.anthropic.com/v1/messages
[16:02:38] DEBUG | [anthropic] REQUEST body: model=claude-3-5-haiku-latest max_tokens=1024
[16:02:39] DEBUG | [anthropic] RESPONSE status=200 elapsed=0.98s
[16:02:39] DEBUG | [anthropic] USAGE: input_tokens=190 output_tokens=16
[16:02:39] DEBUG | [anthropic] SUCCESS stop_reason=end_turn response_len=103
[16:02:39] DEBUG | [anthropic] RESPONSE: recovery point objective recovery time objective disaster recovery metrics business continuity planning
[16:02:39] TRACE | === FACT COVERAGE ANALYSIS ===
[16:02:39] TRACE | Query: RPO RTO
[16:02:39] TRACE | Total key facts: 2
[16:02:39] TRACE | Found facts: 0/2
[16:02:39] TRACE | Missed facts: 2/2
[16:02:39] TRACE |   MISSED[1] RPO (Recovery Point Objective): 1 hour
[16:02:39] TRACE |     -> Found in chunk_id=architecture_overview_fix_10 (rank not in top-5)
[16:02:39] TRACE |   MISSED[2] RTO (Recovery Time Objective): 4 hours
[16:02:39] TRACE |     -> Found in chunk_id=architecture_overview_fix_10 (rank not in top-5)
[16:02:39] TRACE | === END FACT COVERAGE ===
[16:02:39] DEBUG | [anthropic] Model alias: claude-haiku -> claude-3-5-haiku-latest
[16:02:39] DEBUG | [anthropic] model=claude-3-5-haiku-latest prompt_len=874 timeout=5s
[16:02:39] DEBUG | [anthropic] PROMPT: You are a technical documentation search expert. Your task is to rewrite user questions as direct documentation lookup queries.

Guidelines:
1. Convert problem descriptions to feature/capability questions
2. Expand abbreviations and acronyms to full terms
3. Replace casual language with technical terminology
4. Align with documentation vocabulary and structure
5. Keep the rewritten query concise (one line)

Examples:
- "Why can't I schedule workflows every 30 seconds?" → "workflow scheduling min...
[16:02:39] DEBUG | [anthropic] REQUEST: POST https://api.anthropic.com/v1/messages
[16:02:39] DEBUG | [anthropic] REQUEST body: model=claude-3-5-haiku-latest max_tokens=1024
[16:02:40] DEBUG | [anthropic] RESPONSE status=200 elapsed=0.95s
[16:02:40] DEBUG | [anthropic] USAGE: input_tokens=201 output_tokens=21
[16:02:40] DEBUG | [anthropic] SUCCESS stop_reason=end_turn response_len=118
[16:02:40] DEBUG | [anthropic] RESPONSE: cloudflow business continuity recovery point objective recovery time objective rpo rto disaster recovery configuration
[16:02:40] TRACE | === FACT COVERAGE ANALYSIS ===
[16:02:40] TRACE | Query: For our business continuity plan, what are CloudFlow's recovery objectives?
[16:02:40] TRACE | Total key facts: 2
[16:02:40] TRACE | Found facts: 0/2
[16:02:40] TRACE | Missed facts: 2/2
[16:02:40] TRACE |   MISSED[1] RPO (Recovery Point Objective): 1 hour
[16:02:40] TRACE |     -> Found in chunk_id=architecture_overview_fix_10 (rank not in top-5)
[16:02:40] TRACE |   MISSED[2] RTO (Recovery Time Objective): 4 hours
[16:02:40] TRACE |     -> Found in chunk_id=architecture_overview_fix_10 (rank not in top-5)
[16:02:40] TRACE | === END FACT COVERAGE ===
[16:02:40] DEBUG | [anthropic] Model alias: claude-haiku -> claude-3-5-haiku-latest
[16:02:40] DEBUG | [anthropic] model=claude-3-5-haiku-latest prompt_len=837 timeout=5s
[16:02:40] DEBUG | [anthropic] PROMPT: You are a technical documentation search expert. Your task is to rewrite user questions as direct documentation lookup queries.

Guidelines:
1. Convert problem descriptions to feature/capability questions
2. Expand abbreviations and acronyms to full terms
3. Replace casual language with technical terminology
4. Align with documentation vocabulary and structure
5. Keep the rewritten query concise (one line)

Examples:
- "Why can't I schedule workflows every 30 seconds?" → "workflow scheduling min...
[16:02:40] DEBUG | [anthropic] REQUEST: POST https://api.anthropic.com/v1/messages
[16:02:40] DEBUG | [anthropic] REQUEST body: model=claude-3-5-haiku-latest max_tokens=1024
[16:02:41] DEBUG | [anthropic] RESPONSE status=200 elapsed=0.95s
[16:02:41] DEBUG | [anthropic] USAGE: input_tokens=195 output_tokens=15
[16:02:41] DEBUG | [anthropic] SUCCESS stop_reason=end_turn response_len=79
[16:02:41] DEBUG | [anthropic] RESPONSE: "data replication consistency zero data loss guarantee reliability constraints"
[16:02:42] TRACE | === FACT COVERAGE ANALYSIS ===
[16:02:42] TRACE | Query: Why can't we guarantee zero data loss?
[16:02:42] TRACE | Total key facts: 2
[16:02:42] TRACE | Found facts: 0/2
[16:02:42] TRACE | Missed facts: 2/2
[16:02:42] TRACE |   MISSED[1] RPO (Recovery Point Objective): 1 hour
[16:02:42] TRACE |     -> Found in chunk_id=architecture_overview_fix_10 (rank not in top-5)
[16:02:42] TRACE |   MISSED[2] RTO (Recovery Time Objective): 4 hours
[16:02:42] TRACE |     -> Found in chunk_id=architecture_overview_fix_10 (rank not in top-5)
[16:02:42] TRACE | === END FACT COVERAGE ===
[16:02:42] DEBUG | [anthropic] Model alias: claude-haiku -> claude-3-5-haiku-latest
[16:02:42] DEBUG | [anthropic] model=claude-3-5-haiku-latest prompt_len=846 timeout=5s
[16:02:42] DEBUG | [anthropic] PROMPT: You are a technical documentation search expert. Your task is to rewrite user questions as direct documentation lookup queries.

Guidelines:
1. Convert problem descriptions to feature/capability questions
2. Expand abbreviations and acronyms to full terms
3. Replace casual language with technical terminology
4. Align with documentation vocabulary and structure
5. Keep the rewritten query concise (one line)

Examples:
- "Why can't I schedule workflows every 30 seconds?" → "workflow scheduling min...
[16:02:42] DEBUG | [anthropic] REQUEST: POST https://api.anthropic.com/v1/messages
[16:02:42] DEBUG | [anthropic] REQUEST body: model=claude-3-5-haiku-latest max_tokens=1024
[16:02:43] DEBUG | [anthropic] RESPONSE status=200 elapsed=0.82s
[16:02:43] DEBUG | [anthropic] USAGE: input_tokens=194 output_tokens=9
[16:02:43] DEBUG | [anthropic] SUCCESS stop_reason=end_turn response_len=54
[16:02:43] DEBUG | [anthropic] RESPONSE: workflow execution maximum timeout configuration limit
[16:02:43] TRACE | === FACT COVERAGE ANALYSIS ===
[16:02:43] TRACE | Query: What is the maximum workflow execution timeout?
[16:02:43] TRACE | Total key facts: 2
[16:02:43] TRACE | Found facts: 1/2
[16:02:43] TRACE |   FOUND[1] 3600 seconds
[16:02:43] TRACE | Missed facts: 1/2
[16:02:43] TRACE |   MISSED[1] exceeded maximum execution time of 3600 seconds
[16:02:43] TRACE |     -> Found in chunk_id=troubleshooting_guide_fix_4 (rank not in top-5)
[16:02:43] TRACE | === END FACT COVERAGE ===
[16:02:43] DEBUG | [anthropic] Model alias: claude-haiku -> claude-3-5-haiku-latest
[16:02:43] DEBUG | [anthropic] model=claude-3-5-haiku-latest prompt_len=845 timeout=5s
[16:02:43] DEBUG | [anthropic] PROMPT: You are a technical documentation search expert. Your task is to rewrite user questions as direct documentation lookup queries.

Guidelines:
1. Convert problem descriptions to feature/capability questions
2. Expand abbreviations and acronyms to full terms
3. Replace casual language with technical terminology
4. Align with documentation vocabulary and structure
5. Keep the rewritten query concise (one line)

Examples:
- "Why can't I schedule workflows every 30 seconds?" → "workflow scheduling min...
[16:02:43] DEBUG | [anthropic] REQUEST: POST https://api.anthropic.com/v1/messages
[16:02:43] DEBUG | [anthropic] REQUEST body: model=claude-3-5-haiku-latest max_tokens=1024
[16:02:44] DEBUG | [anthropic] RESPONSE status=200 elapsed=0.87s
[16:02:44] DEBUG | [anthropic] USAGE: input_tokens=196 output_tokens=9
[16:02:44] DEBUG | [anthropic] SUCCESS stop_reason=end_turn response_len=49
[16:02:44] DEBUG | [anthropic] RESPONSE: workflow maximum execution duration timeout limit
[16:02:44] TRACE | === FACT COVERAGE ANALYSIS ===
[16:02:44] TRACE | Query: How long can a workflow run before timing out?
[16:02:44] TRACE | Total key facts: 2
[16:02:44] TRACE | Found facts: 0/2
[16:02:44] TRACE | Missed facts: 2/2
[16:02:44] TRACE |   MISSED[1] 3600 seconds
[16:02:44] TRACE |     -> Found in chunk_id=api_reference_fix_0 (rank not in top-5)
[16:02:44] TRACE |   MISSED[2] exceeded maximum execution time of 3600 seconds
[16:02:44] TRACE |     -> Found in chunk_id=troubleshooting_guide_fix_4 (rank not in top-5)
[16:02:44] TRACE | === END FACT COVERAGE ===
[16:02:44] DEBUG | [anthropic] Model alias: claude-haiku -> claude-3-5-haiku-latest
[16:02:44] DEBUG | [anthropic] model=claude-3-5-haiku-latest prompt_len=840 timeout=5s
[16:02:44] DEBUG | [anthropic] PROMPT: You are a technical documentation search expert. Your task is to rewrite user questions as direct documentation lookup queries.

Guidelines:
1. Convert problem descriptions to feature/capability questions
2. Expand abbreviations and acronyms to full terms
3. Replace casual language with technical terminology
4. Align with documentation vocabulary and structure
5. Keep the rewritten query concise (one line)

Examples:
- "Why can't I schedule workflows every 30 seconds?" → "workflow scheduling min...
[16:02:44] DEBUG | [anthropic] REQUEST: POST https://api.anthropic.com/v1/messages
[16:02:44] DEBUG | [anthropic] REQUEST body: model=claude-3-5-haiku-latest max_tokens=1024
[16:02:45] DEBUG | [anthropic] RESPONSE status=200 elapsed=1.04s
[16:02:45] DEBUG | [anthropic] USAGE: input_tokens=194 output_tokens=10
[16:02:45] DEBUG | [anthropic] SUCCESS stop_reason=end_turn response_len=55
[16:02:45] DEBUG | [anthropic] RESPONSE: workflow execution maximum runtime timeout limit policy
[16:02:46] TRACE | === FACT COVERAGE ANALYSIS ===
[16:02:46] TRACE | Query: My workflow is being killed after an hour
[16:02:46] TRACE | Total key facts: 2
[16:02:46] TRACE | Found facts: 0/2
[16:02:46] TRACE | Missed facts: 2/2
[16:02:46] TRACE |   MISSED[1] 3600 seconds
[16:02:46] TRACE |     -> Found in chunk_id=api_reference_fix_0 (rank not in top-5)
[16:02:46] TRACE |   MISSED[2] exceeded maximum execution time of 3600 seconds
[16:02:46] TRACE |     -> Found in chunk_id=troubleshooting_guide_fix_4 (rank not in top-5)
[16:02:46] TRACE | === END FACT COVERAGE ===
[16:02:46] DEBUG | [anthropic] Model alias: claude-haiku -> claude-3-5-haiku-latest
[16:02:46] DEBUG | [anthropic] model=claude-3-5-haiku-latest prompt_len=815 timeout=5s
[16:02:46] DEBUG | [anthropic] PROMPT: You are a technical documentation search expert. Your task is to rewrite user questions as direct documentation lookup queries.

Guidelines:
1. Convert problem descriptions to feature/capability questions
2. Expand abbreviations and acronyms to full terms
3. Replace casual language with technical terminology
4. Align with documentation vocabulary and structure
5. Keep the rewritten query concise (one line)

Examples:
- "Why can't I schedule workflows every 30 seconds?" → "workflow scheduling min...
[16:02:46] DEBUG | [anthropic] REQUEST: POST https://api.anthropic.com/v1/messages
[16:02:46] DEBUG | [anthropic] REQUEST body: model=claude-3-5-haiku-latest max_tokens=1024
[16:02:47] DEBUG | [anthropic] RESPONSE status=200 elapsed=1.26s
[16:02:47] DEBUG | [anthropic] USAGE: input_tokens=188 output_tokens=11
[16:02:47] DEBUG | [anthropic] SUCCESS stop_reason=end_turn response_len=69
[16:02:47] DEBUG | [anthropic] RESPONSE: workflow timeout configuration duration maximum execution time limits
[16:02:47] TRACE | === FACT COVERAGE ANALYSIS ===
[16:02:47] TRACE | Query: workflow timeout
[16:02:47] TRACE | Total key facts: 2
[16:02:47] TRACE | Found facts: 1/2
[16:02:47] TRACE |   FOUND[1] 3600 seconds
[16:02:47] TRACE | Missed facts: 1/2
[16:02:47] TRACE |   MISSED[1] exceeded maximum execution time of 3600 seconds
[16:02:47] TRACE |     -> Found in chunk_id=troubleshooting_guide_fix_4 (rank not in top-5)
[16:02:47] TRACE | === END FACT COVERAGE ===
[16:02:47] DEBUG | [anthropic] Model alias: claude-haiku -> claude-3-5-haiku-latest
[16:02:47] DEBUG | [anthropic] model=claude-3-5-haiku-latest prompt_len=869 timeout=5s
[16:02:47] DEBUG | [anthropic] PROMPT: You are a technical documentation search expert. Your task is to rewrite user questions as direct documentation lookup queries.

Guidelines:
1. Convert problem descriptions to feature/capability questions
2. Expand abbreviations and acronyms to full terms
3. Replace casual language with technical terminology
4. Align with documentation vocabulary and structure
5. Keep the rewritten query concise (one line)

Examples:
- "Why can't I schedule workflows every 30 seconds?" → "workflow scheduling min...
[16:02:47] DEBUG | [anthropic] REQUEST: POST https://api.anthropic.com/v1/messages
[16:02:47] DEBUG | [anthropic] REQUEST body: model=claude-3-5-haiku-latest max_tokens=1024
[16:02:48] DEBUG | [anthropic] RESPONSE status=200 elapsed=1.00s
[16:02:48] DEBUG | [anthropic] USAGE: input_tokens=202 output_tokens=14
[16:02:48] DEBUG | [anthropic] SUCCESS stop_reason=end_turn response_len=71
[16:02:48] DEBUG | [anthropic] RESPONSE: workflow long-running job maximum execution time limit timeout duration
[16:02:49] TRACE | === FACT COVERAGE ANALYSIS ===
[16:02:49] TRACE | Query: I have a long-running data processing workflow, what's the time limit?
[16:02:49] TRACE | Total key facts: 2
[16:02:49] TRACE | Found facts: 0/2
[16:02:49] TRACE | Missed facts: 2/2
[16:02:49] TRACE |   MISSED[1] 3600 seconds
[16:02:49] TRACE |     -> Found in chunk_id=api_reference_fix_0 (rank not in top-5)
[16:02:49] TRACE |   MISSED[2] exceeded maximum execution time of 3600 seconds
[16:02:49] TRACE |     -> Found in chunk_id=troubleshooting_guide_fix_4 (rank not in top-5)
[16:02:49] TRACE | === END FACT COVERAGE ===
[16:02:49] DEBUG | [anthropic] Model alias: claude-haiku -> claude-3-5-haiku-latest
[16:02:49] DEBUG | [anthropic] model=claude-3-5-haiku-latest prompt_len=843 timeout=5s
[16:02:49] DEBUG | [anthropic] PROMPT: You are a technical documentation search expert. Your task is to rewrite user questions as direct documentation lookup queries.

Guidelines:
1. Convert problem descriptions to feature/capability questions
2. Expand abbreviations and acronyms to full terms
3. Replace casual language with technical terminology
4. Align with documentation vocabulary and structure
5. Keep the rewritten query concise (one line)

Examples:
- "Why can't I schedule workflows every 30 seconds?" → "workflow scheduling min...
[16:02:49] DEBUG | [anthropic] REQUEST: POST https://api.anthropic.com/v1/messages
[16:02:49] DEBUG | [anthropic] REQUEST body: model=claude-3-5-haiku-latest max_tokens=1024
[16:02:50] DEBUG | [anthropic] RESPONSE status=200 elapsed=0.98s
[16:02:50] DEBUG | [anthropic] USAGE: input_tokens=198 output_tokens=11
[16:02:50] DEBUG | [anthropic] SUCCESS stop_reason=end_turn response_len=64
[16:02:50] DEBUG | [anthropic] RESPONSE: workflow execution timeout maximum runtime limit failure reasons
[16:02:50] TRACE | === FACT COVERAGE ANALYSIS ===
[16:02:50] TRACE | Query: Why did my workflow fail after 3600 seconds?
[16:02:50] TRACE | Total key facts: 2
[16:02:50] TRACE | Found facts: 0/2
[16:02:50] TRACE | Missed facts: 2/2
[16:02:50] TRACE |   MISSED[1] 3600 seconds
[16:02:50] TRACE |     -> Found in chunk_id=api_reference_fix_0 (rank not in top-5)
[16:02:50] TRACE |   MISSED[2] exceeded maximum execution time of 3600 seconds
[16:02:50] TRACE |     -> Found in chunk_id=troubleshooting_guide_fix_4 (rank not in top-5)
[16:02:50] TRACE | === END FACT COVERAGE ===
[16:02:50] DEBUG | [anthropic] Model alias: claude-haiku -> claude-3-5-haiku-latest
[16:02:50] DEBUG | [anthropic] model=claude-3-5-haiku-latest prompt_len=844 timeout=5s
[16:02:50] DEBUG | [anthropic] PROMPT: You are a technical documentation search expert. Your task is to rewrite user questions as direct documentation lookup queries.

Guidelines:
1. Convert problem descriptions to feature/capability questions
2. Expand abbreviations and acronyms to full terms
3. Replace casual language with technical terminology
4. Align with documentation vocabulary and structure
5. Keep the rewritten query concise (one line)

Examples:
- "Why can't I schedule workflows every 30 seconds?" → "workflow scheduling min...
[16:02:50] DEBUG | [anthropic] REQUEST: POST https://api.anthropic.com/v1/messages
[16:02:50] DEBUG | [anthropic] REQUEST body: model=claude-3-5-haiku-latest max_tokens=1024
[16:02:51] DEBUG | [anthropic] RESPONSE status=200 elapsed=0.87s
[16:02:51] DEBUG | [anthropic] USAGE: input_tokens=195 output_tokens=10
[16:02:51] DEBUG | [anthropic] SUCCESS stop_reason=end_turn response_len=60
[16:02:51] DEBUG | [anthropic] RESPONSE: jwt token signing algorithm supported methods authentication
[16:02:51] TRACE | === FACT COVERAGE ANALYSIS ===
[16:02:51] TRACE | Query: What JWT algorithm is used for token signing?
[16:02:51] TRACE | Total key facts: 3
[16:02:51] TRACE | Found facts: 3/3
[16:02:51] TRACE |   FOUND[1] RS256
[16:02:51] TRACE |   FOUND[2] RS256 algorithm
[16:02:51] TRACE |   FOUND[3] RS256 signing algorithm
[16:02:51] TRACE | === END FACT COVERAGE ===
[16:02:51] DEBUG | [anthropic] Model alias: claude-haiku -> claude-3-5-haiku-latest
[16:02:51] DEBUG | [anthropic] model=claude-3-5-haiku-latest prompt_len=851 timeout=5s
[16:02:51] DEBUG | [anthropic] PROMPT: You are a technical documentation search expert. Your task is to rewrite user questions as direct documentation lookup queries.

Guidelines:
1. Convert problem descriptions to feature/capability questions
2. Expand abbreviations and acronyms to full terms
3. Replace casual language with technical terminology
4. Align with documentation vocabulary and structure
5. Keep the rewritten query concise (one line)

Examples:
- "Why can't I schedule workflows every 30 seconds?" → "workflow scheduling min...
[16:02:51] DEBUG | [anthropic] REQUEST: POST https://api.anthropic.com/v1/messages
[16:02:51] DEBUG | [anthropic] REQUEST body: model=claude-3-5-haiku-latest max_tokens=1024
[16:02:53] DEBUG | [anthropic] RESPONSE status=200 elapsed=1.14s
[16:02:53] DEBUG | [anthropic] USAGE: input_tokens=198 output_tokens=11
[16:02:53] DEBUG | [anthropic] SUCCESS stop_reason=end_turn response_len=63
[16:02:53] DEBUG | [anthropic] RESPONSE: jwt signing algorithm cloudflow authentication token generation
[16:02:53] TRACE | === FACT COVERAGE ANALYSIS ===
[16:02:53] TRACE | Query: Which signing algorithm does CloudFlow use for JWTs?
[16:02:53] TRACE | Total key facts: 3
[16:02:53] TRACE | Found facts: 2/3
[16:02:53] TRACE |   FOUND[1] RS256
[16:02:53] TRACE |   FOUND[2] RS256 signing algorithm
[16:02:53] TRACE | Missed facts: 1/3
[16:02:53] TRACE |   MISSED[1] RS256 algorithm
[16:02:53] TRACE |     -> Found in chunk_id=architecture_overview_fix_1 (rank not in top-5)
[16:02:53] TRACE | === END FACT COVERAGE ===
[16:02:53] DEBUG | [anthropic] Model alias: claude-haiku -> claude-3-5-haiku-latest
[16:02:53] DEBUG | [anthropic] model=claude-3-5-haiku-latest prompt_len=851 timeout=5s
[16:02:53] DEBUG | [anthropic] PROMPT: You are a technical documentation search expert. Your task is to rewrite user questions as direct documentation lookup queries.

Guidelines:
1. Convert problem descriptions to feature/capability questions
2. Expand abbreviations and acronyms to full terms
3. Replace casual language with technical terminology
4. Align with documentation vocabulary and structure
5. Keep the rewritten query concise (one line)

Examples:
- "Why can't I schedule workflows every 30 seconds?" → "workflow scheduling min...
[16:02:53] DEBUG | [anthropic] REQUEST: POST https://api.anthropic.com/v1/messages
[16:02:53] DEBUG | [anthropic] REQUEST body: model=claude-3-5-haiku-latest max_tokens=1024
[16:02:54] DEBUG | [anthropic] RESPONSE status=200 elapsed=0.82s
[16:02:54] DEBUG | [anthropic] USAGE: input_tokens=196 output_tokens=10
[16:02:54] DEBUG | [anthropic] SUCCESS stop_reason=end_turn response_len=62
[16:02:54] DEBUG | [anthropic] RESPONSE: jwt validation algorithm verification error handling mechanism
[16:02:54] TRACE | === FACT COVERAGE ANALYSIS ===
[16:02:54] TRACE | Query: My JWT validation is failing with algorithm mismatch
[16:02:54] TRACE | Total key facts: 3
[16:02:54] TRACE | Found facts: 3/3
[16:02:54] TRACE |   FOUND[1] RS256
[16:02:54] TRACE |   FOUND[2] RS256 algorithm
[16:02:54] TRACE |   FOUND[3] RS256 signing algorithm
[16:02:54] TRACE | === END FACT COVERAGE ===
[16:02:54] DEBUG | [anthropic] Model alias: claude-haiku -> claude-3-5-haiku-latest
[16:02:54] DEBUG | [anthropic] model=claude-3-5-haiku-latest prompt_len=812 timeout=5s
[16:02:54] DEBUG | [anthropic] PROMPT: You are a technical documentation search expert. Your task is to rewrite user questions as direct documentation lookup queries.

Guidelines:
1. Convert problem descriptions to feature/capability questions
2. Expand abbreviations and acronyms to full terms
3. Replace casual language with technical terminology
4. Align with documentation vocabulary and structure
5. Keep the rewritten query concise (one line)

Examples:
- "Why can't I schedule workflows every 30 seconds?" → "workflow scheduling min...
[16:02:54] DEBUG | [anthropic] REQUEST: POST https://api.anthropic.com/v1/messages
[16:02:54] DEBUG | [anthropic] REQUEST body: model=claude-3-5-haiku-latest max_tokens=1024
[16:02:55] DEBUG | [anthropic] RESPONSE status=200 elapsed=1.00s
[16:02:55] DEBUG | [anthropic] USAGE: input_tokens=188 output_tokens=10
[16:02:55] DEBUG | [anthropic] SUCCESS stop_reason=end_turn response_len=55
[16:02:55] DEBUG | [anthropic] RESPONSE: jwt signature algorithm supported cryptographic methods
[16:02:56] TRACE | === FACT COVERAGE ANALYSIS ===
[16:02:56] TRACE | Query: jwt algorithm
[16:02:56] TRACE | Total key facts: 3
[16:02:56] TRACE | Found facts: 3/3
[16:02:56] TRACE |   FOUND[1] RS256
[16:02:56] TRACE |   FOUND[2] RS256 algorithm
[16:02:56] TRACE |   FOUND[3] RS256 signing algorithm
[16:02:56] TRACE | === END FACT COVERAGE ===
[16:02:56] DEBUG | [anthropic] Model alias: claude-haiku -> claude-3-5-haiku-latest
[16:02:56] DEBUG | [anthropic] model=claude-3-5-haiku-latest prompt_len=865 timeout=5s
[16:02:56] DEBUG | [anthropic] PROMPT: You are a technical documentation search expert. Your task is to rewrite user questions as direct documentation lookup queries.

Guidelines:
1. Convert problem descriptions to feature/capability questions
2. Expand abbreviations and acronyms to full terms
3. Replace casual language with technical terminology
4. Align with documentation vocabulary and structure
5. Keep the rewritten query concise (one line)

Examples:
- "Why can't I schedule workflows every 30 seconds?" → "workflow scheduling min...
[16:02:56] DEBUG | [anthropic] REQUEST: POST https://api.anthropic.com/v1/messages
[16:02:56] DEBUG | [anthropic] REQUEST body: model=claude-3-5-haiku-latest max_tokens=1024
[16:02:57] DEBUG | [anthropic] RESPONSE status=200 elapsed=0.92s
[16:02:57] DEBUG | [anthropic] USAGE: input_tokens=198 output_tokens=11
[16:02:57] DEBUG | [anthropic] SUCCESS stop_reason=end_turn response_len=74
[16:02:57] DEBUG | [anthropic] RESPONSE: jwt verification supported algorithms authentication token signing methods
[16:02:57] TRACE | === FACT COVERAGE ANALYSIS ===
[16:02:57] TRACE | Query: I'm implementing JWT verification, what algorithm should I expect?
[16:02:57] TRACE | Total key facts: 3
[16:02:57] TRACE | Found facts: 3/3
[16:02:57] TRACE |   FOUND[1] RS256
[16:02:57] TRACE |   FOUND[2] RS256 algorithm
[16:02:57] TRACE |   FOUND[3] RS256 signing algorithm
[16:02:57] TRACE | === END FACT COVERAGE ===
[16:02:57] DEBUG | [anthropic] Model alias: claude-haiku -> claude-3-5-haiku-latest
[16:02:57] DEBUG | [anthropic] model=claude-3-5-haiku-latest prompt_len=843 timeout=5s
[16:02:57] DEBUG | [anthropic] PROMPT: You are a technical documentation search expert. Your task is to rewrite user questions as direct documentation lookup queries.

Guidelines:
1. Convert problem descriptions to feature/capability questions
2. Expand abbreviations and acronyms to full terms
3. Replace casual language with technical terminology
4. Align with documentation vocabulary and structure
5. Keep the rewritten query concise (one line)

Examples:
- "Why can't I schedule workflows every 30 seconds?" → "workflow scheduling min...
[16:02:57] DEBUG | [anthropic] REQUEST: POST https://api.anthropic.com/v1/messages
[16:02:57] DEBUG | [anthropic] REQUEST body: model=claude-3-5-haiku-latest max_tokens=1024
[16:02:58] DEBUG | [anthropic] RESPONSE status=200 elapsed=1.16s
[16:02:58] DEBUG | [anthropic] USAGE: input_tokens=198 output_tokens=20
[16:02:58] DEBUG | [anthropic] SUCCESS stop_reason=end_turn response_len=103
[16:02:58] DEBUG | [anthropic] RESPONSE: HMAC SHA-256 JWT token validation failure authentication mechanism cryptographic signature verification
[16:02:59] TRACE | === FACT COVERAGE ANALYSIS ===
[16:02:59] TRACE | Query: Why doesn't HS256 work for token validation?
[16:02:59] TRACE | Total key facts: 3
[16:02:59] TRACE | Found facts: 3/3
[16:02:59] TRACE |   FOUND[1] RS256
[16:02:59] TRACE |   FOUND[2] RS256 algorithm
[16:02:59] TRACE |   FOUND[3] RS256 signing algorithm
[16:02:59] TRACE | === END FACT COVERAGE ===
[16:02:59] DEBUG | [anthropic] Model alias: claude-haiku -> claude-3-5-haiku-latest
[16:02:59] DEBUG | [anthropic] model=claude-3-5-haiku-latest prompt_len=852 timeout=5s
[16:02:59] DEBUG | [anthropic] PROMPT: You are a technical documentation search expert. Your task is to rewrite user questions as direct documentation lookup queries.

Guidelines:
1. Convert problem descriptions to feature/capability questions
2. Expand abbreviations and acronyms to full terms
3. Replace casual language with technical terminology
4. Align with documentation vocabulary and structure
5. Keep the rewritten query concise (one line)

Examples:
- "Why can't I schedule workflows every 30 seconds?" → "workflow scheduling min...
[16:02:59] DEBUG | [anthropic] REQUEST: POST https://api.anthropic.com/v1/messages
[16:02:59] DEBUG | [anthropic] REQUEST body: model=claude-3-5-haiku-latest max_tokens=1024
[16:03:00] DEBUG | [anthropic] RESPONSE status=200 elapsed=1.06s
[16:03:00] DEBUG | [anthropic] USAGE: input_tokens=197 output_tokens=14
[16:03:00] DEBUG | [anthropic] SUCCESS stop_reason=end_turn response_len=69
[16:03:00] DEBUG | [anthropic] RESPONSE: workflow definition redis cache time-to-live configuration parameters
[16:03:00] TRACE | === FACT COVERAGE ANALYSIS ===
[16:03:00] TRACE | Query: What is the Redis cache TTL for workflow definitions?
[16:03:00] TRACE | Total key facts: 2
[16:03:00] TRACE | Found facts: 1/2
[16:03:00] TRACE |   FOUND[1] TTL: 1 hour
[16:03:00] TRACE | Missed facts: 1/2
[16:03:00] TRACE |   MISSED[1] Workflow Definitions: TTL: 1 hour
[16:03:00] TRACE | === END FACT COVERAGE ===
[16:03:00] DEBUG | [anthropic] Model alias: claude-haiku -> claude-3-5-haiku-latest
[16:03:00] DEBUG | [anthropic] model=claude-3-5-haiku-latest prompt_len=840 timeout=5s
[16:03:00] DEBUG | [anthropic] PROMPT: You are a technical documentation search expert. Your task is to rewrite user questions as direct documentation lookup queries.

Guidelines:
1. Convert problem descriptions to feature/capability questions
2. Expand abbreviations and acronyms to full terms
3. Replace casual language with technical terminology
4. Align with documentation vocabulary and structure
5. Keep the rewritten query concise (one line)

Examples:
- "Why can't I schedule workflows every 30 seconds?" → "workflow scheduling min...
[16:03:00] DEBUG | [anthropic] REQUEST: POST https://api.anthropic.com/v1/messages
[16:03:00] DEBUG | [anthropic] REQUEST body: model=claude-3-5-haiku-latest max_tokens=1024
[16:03:01] DEBUG | [anthropic] RESPONSE status=200 elapsed=0.96s
[16:03:01] DEBUG | [anthropic] USAGE: input_tokens=193 output_tokens=9
[16:03:01] DEBUG | [anthropic] SUCCESS stop_reason=end_turn response_len=46
[16:03:01] DEBUG | [anthropic] RESPONSE: workflow definition caching duration lifecycle
[16:03:01] TRACE | === FACT COVERAGE ANALYSIS ===
[16:03:01] TRACE | Query: How long are workflow definitions cached?
[16:03:01] TRACE | Total key facts: 2
[16:03:01] TRACE | Found facts: 0/2
[16:03:01] TRACE | Missed facts: 2/2
[16:03:01] TRACE |   MISSED[1] TTL: 1 hour
[16:03:01] TRACE |     -> Found in chunk_id=architecture_overview_fix_6 (rank not in top-5)
[16:03:01] TRACE |   MISSED[2] Workflow Definitions: TTL: 1 hour
[16:03:01] TRACE | === END FACT COVERAGE ===
[16:03:01] DEBUG | [anthropic] Model alias: claude-haiku -> claude-3-5-haiku-latest
[16:03:01] DEBUG | [anthropic] model=claude-3-5-haiku-latest prompt_len=848 timeout=5s
[16:03:01] DEBUG | [anthropic] PROMPT: You are a technical documentation search expert. Your task is to rewrite user questions as direct documentation lookup queries.

Guidelines:
1. Convert problem descriptions to feature/capability questions
2. Expand abbreviations and acronyms to full terms
3. Replace casual language with technical terminology
4. Align with documentation vocabulary and structure
5. Keep the rewritten query concise (one line)

Examples:
- "Why can't I schedule workflows every 30 seconds?" → "workflow scheduling min...
[16:03:01] DEBUG | [anthropic] REQUEST: POST https://api.anthropic.com/v1/messages
[16:03:01] DEBUG | [anthropic] REQUEST body: model=claude-3-5-haiku-latest max_tokens=1024
[16:03:02] DEBUG | [anthropic] RESPONSE status=200 elapsed=0.97s
[16:03:02] DEBUG | [anthropic] USAGE: input_tokens=193 output_tokens=13
[16:03:02] DEBUG | [anthropic] SUCCESS stop_reason=end_turn response_len=59
[16:03:02] DEBUG | [anthropic] RESPONSE: workflow real-time synchronization update propagation delay
[16:03:03] TRACE | === FACT COVERAGE ANALYSIS ===
[16:03:03] TRACE | Query: My workflow updates aren't reflecting immediately
[16:03:03] TRACE | Total key facts: 2
[16:03:03] TRACE | Found facts: 0/2
[16:03:03] TRACE | Missed facts: 2/2
[16:03:03] TRACE |   MISSED[1] TTL: 1 hour
[16:03:03] TRACE |     -> Found in chunk_id=architecture_overview_fix_6 (rank not in top-5)
[16:03:03] TRACE |   MISSED[2] Workflow Definitions: TTL: 1 hour
[16:03:03] TRACE | === END FACT COVERAGE ===
[16:03:03] DEBUG | [anthropic] Model alias: claude-haiku -> claude-3-5-haiku-latest
[16:03:03] DEBUG | [anthropic] model=claude-3-5-haiku-latest prompt_len=818 timeout=5s
[16:03:03] DEBUG | [anthropic] PROMPT: You are a technical documentation search expert. Your task is to rewrite user questions as direct documentation lookup queries.

Guidelines:
1. Convert problem descriptions to feature/capability questions
2. Expand abbreviations and acronyms to full terms
3. Replace casual language with technical terminology
4. Align with documentation vocabulary and structure
5. Keep the rewritten query concise (one line)

Examples:
- "Why can't I schedule workflows every 30 seconds?" → "workflow scheduling min...
[16:03:03] DEBUG | [anthropic] REQUEST: POST https://api.anthropic.com/v1/messages
[16:03:03] DEBUG | [anthropic] REQUEST body: model=claude-3-5-haiku-latest max_tokens=1024
[16:03:04] DEBUG | [anthropic] RESPONSE status=200 elapsed=0.93s
[16:03:04] DEBUG | [anthropic] USAGE: input_tokens=190 output_tokens=13
[16:03:04] DEBUG | [anthropic] SUCCESS stop_reason=end_turn response_len=54
[16:03:04] DEBUG | [anthropic] RESPONSE: workflow caching time-to-live configuration parameters
[16:03:04] TRACE | === FACT COVERAGE ANALYSIS ===
[16:03:04] TRACE | Query: cache ttl workflows
[16:03:04] TRACE | Total key facts: 2
[16:03:04] TRACE | Found facts: 1/2
[16:03:04] TRACE |   FOUND[1] TTL: 1 hour
[16:03:04] TRACE | Missed facts: 1/2
[16:03:04] TRACE |   MISSED[1] Workflow Definitions: TTL: 1 hour
[16:03:04] TRACE | === END FACT COVERAGE ===
[16:03:04] DEBUG | [anthropic] Model alias: claude-haiku -> claude-3-5-haiku-latest
[16:03:04] DEBUG | [anthropic] model=claude-3-5-haiku-latest prompt_len=859 timeout=5s
[16:03:04] DEBUG | [anthropic] PROMPT: You are a technical documentation search expert. Your task is to rewrite user questions as direct documentation lookup queries.

Guidelines:
1. Convert problem descriptions to feature/capability questions
2. Expand abbreviations and acronyms to full terms
3. Replace casual language with technical terminology
4. Align with documentation vocabulary and structure
5. Keep the rewritten query concise (one line)

Examples:
- "Why can't I schedule workflows every 30 seconds?" → "workflow scheduling min...
[16:03:04] DEBUG | [anthropic] REQUEST: POST https://api.anthropic.com/v1/messages
[16:03:04] DEBUG | [anthropic] REQUEST body: model=claude-3-5-haiku-latest max_tokens=1024
[16:03:05] DEBUG | [anthropic] RESPONSE status=200 elapsed=1.38s
[16:03:05] DEBUG | [anthropic] USAGE: input_tokens=198 output_tokens=11
[16:03:05] DEBUG | [anthropic] SUCCESS stop_reason=end_turn response_len=60
[16:03:05] DEBUG | [anthropic] RESPONSE: workflow cache invalidation update refresh interval duration
[16:03:06] TRACE | === FACT COVERAGE ANALYSIS ===
[16:03:06] TRACE | Query: After updating a workflow, how long until the cache expires?
[16:03:06] TRACE | Total key facts: 2
[16:03:06] TRACE | Found facts: 0/2
[16:03:06] TRACE | Missed facts: 2/2
[16:03:06] TRACE |   MISSED[1] TTL: 1 hour
[16:03:06] TRACE |     -> Found in chunk_id=architecture_overview_fix_6 (rank not in top-5)
[16:03:06] TRACE |   MISSED[2] Workflow Definitions: TTL: 1 hour
[16:03:06] TRACE | === END FACT COVERAGE ===
[16:03:06] DEBUG | [anthropic] Model alias: claude-haiku -> claude-3-5-haiku-latest
[16:03:06] DEBUG | [anthropic] model=claude-3-5-haiku-latest prompt_len=840 timeout=5s
[16:03:06] DEBUG | [anthropic] PROMPT: You are a technical documentation search expert. Your task is to rewrite user questions as direct documentation lookup queries.

Guidelines:
1. Convert problem descriptions to feature/capability questions
2. Expand abbreviations and acronyms to full terms
3. Replace casual language with technical terminology
4. Align with documentation vocabulary and structure
5. Keep the rewritten query concise (one line)

Examples:
- "Why can't I schedule workflows every 30 seconds?" → "workflow scheduling min...
[16:03:06] DEBUG | [anthropic] REQUEST: POST https://api.anthropic.com/v1/messages
[16:03:06] DEBUG | [anthropic] REQUEST body: model=claude-3-5-haiku-latest max_tokens=1024
[16:03:07] DEBUG | [anthropic] RESPONSE status=200 elapsed=0.93s
[16:03:07] DEBUG | [anthropic] USAGE: input_tokens=195 output_tokens=13
[16:03:07] DEBUG | [anthropic] SUCCESS stop_reason=end_turn response_len=66
[16:03:07] DEBUG | [anthropic] RESPONSE: changes synchronization latency propagation delay data consistency
[16:03:07] TRACE | === FACT COVERAGE ANALYSIS ===
[16:03:07] TRACE | Query: Why are changes taking an hour to appear?
[16:03:07] TRACE | Total key facts: 2
[16:03:07] TRACE | Found facts: 0/2
[16:03:07] TRACE | Missed facts: 2/2
[16:03:07] TRACE |   MISSED[1] TTL: 1 hour
[16:03:07] TRACE |     -> Found in chunk_id=architecture_overview_fix_6 (rank not in top-5)
[16:03:07] TRACE |   MISSED[2] Workflow Definitions: TTL: 1 hour
[16:03:07] TRACE | === END FACT COVERAGE ===
[16:03:07] DEBUG | [anthropic] Model alias: claude-haiku -> claude-3-5-haiku-latest
[16:03:07] DEBUG | [anthropic] model=claude-3-5-haiku-latest prompt_len=840 timeout=5s
[16:03:07] DEBUG | [anthropic] PROMPT: You are a technical documentation search expert. Your task is to rewrite user questions as direct documentation lookup queries.

Guidelines:
1. Convert problem descriptions to feature/capability questions
2. Expand abbreviations and acronyms to full terms
3. Replace casual language with technical terminology
4. Align with documentation vocabulary and structure
5. Keep the rewritten query concise (one line)

Examples:
- "Why can't I schedule workflows every 30 seconds?" → "workflow scheduling min...
[16:03:07] DEBUG | [anthropic] REQUEST: POST https://api.anthropic.com/v1/messages
[16:03:07] DEBUG | [anthropic] REQUEST body: model=claude-3-5-haiku-latest max_tokens=1024
[16:03:08] DEBUG | [anthropic] RESPONSE status=200 elapsed=0.95s
[16:03:08] DEBUG | [anthropic] USAGE: input_tokens=194 output_tokens=12
[16:03:08] DEBUG | [anthropic] SUCCESS stop_reason=end_turn response_len=63
[16:03:08] DEBUG | [anthropic] RESPONSE: cloudflow monitoring tools observability platforms integrations
[16:03:09] TRACE | === FACT COVERAGE ANALYSIS ===
[16:03:09] TRACE | Query: What monitoring tools does CloudFlow use?
[16:03:09] TRACE | Total key facts: 3
[16:03:09] TRACE | Found facts: 3/3
[16:03:09] TRACE |   FOUND[1] Prometheus
[16:03:09] TRACE |   FOUND[2] Grafana
[16:03:09] TRACE |   FOUND[3] Jaeger for distributed tracing
[16:03:09] TRACE | === END FACT COVERAGE ===
[16:03:09] DEBUG | [anthropic] Model alias: claude-haiku -> claude-3-5-haiku-latest
[16:03:09] DEBUG | [anthropic] model=claude-3-5-haiku-latest prompt_len=842 timeout=5s
[16:03:09] DEBUG | [anthropic] PROMPT: You are a technical documentation search expert. Your task is to rewrite user questions as direct documentation lookup queries.

Guidelines:
1. Convert problem descriptions to feature/capability questions
2. Expand abbreviations and acronyms to full terms
3. Replace casual language with technical terminology
4. Align with documentation vocabulary and structure
5. Keep the rewritten query concise (one line)

Examples:
- "Why can't I schedule workflows every 30 seconds?" → "workflow scheduling min...
[16:03:09] DEBUG | [anthropic] REQUEST: POST https://api.anthropic.com/v1/messages
[16:03:09] DEBUG | [anthropic] REQUEST body: model=claude-3-5-haiku-latest max_tokens=1024
[16:03:09] DEBUG | [anthropic] RESPONSE status=200 elapsed=0.80s
[16:03:09] DEBUG | [anthropic] USAGE: input_tokens=193 output_tokens=10
[16:03:09] DEBUG | [anthropic] SUCCESS stop_reason=end_turn response_len=61
[16:03:09] DEBUG | [anthropic] RESPONSE: observability platform integration supported monitoring tools
[16:03:10] TRACE | === FACT COVERAGE ANALYSIS ===
[16:03:10] TRACE | Query: Which observability platform is integrated?
[16:03:10] TRACE | Total key facts: 3
[16:03:10] TRACE | Found facts: 3/3
[16:03:10] TRACE |   FOUND[1] Prometheus
[16:03:10] TRACE |   FOUND[2] Grafana
[16:03:10] TRACE |   FOUND[3] Jaeger for distributed tracing
[16:03:10] TRACE | === END FACT COVERAGE ===
[16:03:10] DEBUG | [anthropic] Model alias: claude-haiku -> claude-3-5-haiku-latest
[16:03:10] DEBUG | [anthropic] model=claude-3-5-haiku-latest prompt_len=843 timeout=5s
[16:03:10] DEBUG | [anthropic] PROMPT: You are a technical documentation search expert. Your task is to rewrite user questions as direct documentation lookup queries.

Guidelines:
1. Convert problem descriptions to feature/capability questions
2. Expand abbreviations and acronyms to full terms
3. Replace casual language with technical terminology
4. Align with documentation vocabulary and structure
5. Keep the rewritten query concise (one line)

Examples:
- "Why can't I schedule workflows every 30 seconds?" → "workflow scheduling min...
[16:03:10] DEBUG | [anthropic] REQUEST: POST https://api.anthropic.com/v1/messages
[16:03:10] DEBUG | [anthropic] REQUEST body: model=claude-3-5-haiku-latest max_tokens=1024
[16:03:11] DEBUG | [anthropic] RESPONSE status=200 elapsed=0.91s
[16:03:11] DEBUG | [anthropic] USAGE: input_tokens=196 output_tokens=11
[16:03:11] DEBUG | [anthropic] SUCCESS stop_reason=end_turn response_len=60
[16:03:11] DEBUG | [anthropic] RESPONSE: cloudflow observability metrics logging monitoring dashboard
[16:03:11] TRACE | === FACT COVERAGE ANALYSIS ===
[16:03:11] TRACE | Query: Where can I view CloudFlow metrics and logs?
[16:03:11] TRACE | Total key facts: 3
[16:03:11] TRACE | Found facts: 3/3
[16:03:11] TRACE |   FOUND[1] Prometheus
[16:03:11] TRACE |   FOUND[2] Grafana
[16:03:11] TRACE |   FOUND[3] Jaeger for distributed tracing
[16:03:11] TRACE | === END FACT COVERAGE ===
[16:03:11] DEBUG | [anthropic] Model alias: claude-haiku -> claude-3-5-haiku-latest
[16:03:11] DEBUG | [anthropic] model=claude-3-5-haiku-latest prompt_len=815 timeout=5s
[16:03:11] DEBUG | [anthropic] PROMPT: You are a technical documentation search expert. Your task is to rewrite user questions as direct documentation lookup queries.

Guidelines:
1. Convert problem descriptions to feature/capability questions
2. Expand abbreviations and acronyms to full terms
3. Replace casual language with technical terminology
4. Align with documentation vocabulary and structure
5. Keep the rewritten query concise (one line)

Examples:
- "Why can't I schedule workflows every 30 seconds?" → "workflow scheduling min...
[16:03:11] DEBUG | [anthropic] REQUEST: POST https://api.anthropic.com/v1/messages
[16:03:11] DEBUG | [anthropic] REQUEST body: model=claude-3-5-haiku-latest max_tokens=1024
[16:03:12] DEBUG | [anthropic] RESPONSE status=200 elapsed=1.02s
[16:03:12] DEBUG | [anthropic] USAGE: input_tokens=188 output_tokens=11
[16:03:12] DEBUG | [anthropic] SUCCESS stop_reason=end_turn response_len=75
[16:03:12] DEBUG | [anthropic] RESPONSE: monitoring infrastructure observability stack configuration and integration
[16:03:12] TRACE | === FACT COVERAGE ANALYSIS ===
[16:03:12] TRACE | Query: monitoring stack
[16:03:12] TRACE | Total key facts: 3
[16:03:12] TRACE | Found facts: 0/3
[16:03:12] TRACE | Missed facts: 3/3
[16:03:12] TRACE |   MISSED[1] Prometheus
[16:03:12] TRACE |     -> Found in chunk_id=architecture_overview_fix_0 (rank not in top-5)
[16:03:12] TRACE |   MISSED[2] Grafana
[16:03:12] TRACE |     -> Found in chunk_id=architecture_overview_fix_0 (rank not in top-5)
[16:03:12] TRACE |   MISSED[3] Jaeger for distributed tracing
[16:03:12] TRACE |     -> Found in chunk_id=architecture_overview_fix_0 (rank not in top-5)
[16:03:12] TRACE | === END FACT COVERAGE ===
[16:03:12] DEBUG | [anthropic] Model alias: claude-haiku -> claude-3-5-haiku-latest
[16:03:12] DEBUG | [anthropic] model=claude-3-5-haiku-latest prompt_len=866 timeout=5s
[16:03:12] DEBUG | [anthropic] PROMPT: You are a technical documentation search expert. Your task is to rewrite user questions as direct documentation lookup queries.

Guidelines:
1. Convert problem descriptions to feature/capability questions
2. Expand abbreviations and acronyms to full terms
3. Replace casual language with technical terminology
4. Align with documentation vocabulary and structure
5. Keep the rewritten query concise (one line)

Examples:
- "Why can't I schedule workflows every 30 seconds?" → "workflow scheduling min...
[16:03:12] DEBUG | [anthropic] REQUEST: POST https://api.anthropic.com/v1/messages
[16:03:12] DEBUG | [anthropic] REQUEST body: model=claude-3-5-haiku-latest max_tokens=1024
[16:03:14] DEBUG | [anthropic] RESPONSE status=200 elapsed=1.06s
[16:03:14] DEBUG | [anthropic] USAGE: input_tokens=200 output_tokens=9
[16:03:14] DEBUG | [anthropic] SUCCESS stop_reason=end_turn response_len=65
[16:03:14] DEBUG | [anthropic] RESPONSE: monitoring dashboard integration supported platforms capabilities
[16:03:14] TRACE | === FACT COVERAGE ANALYSIS ===
[16:03:14] TRACE | Query: I need to set up dashboards, what monitoring systems are available?
[16:03:14] TRACE | Total key facts: 3
[16:03:14] TRACE | Found facts: 0/3
[16:03:14] TRACE | Missed facts: 3/3
[16:03:14] TRACE |   MISSED[1] Prometheus
[16:03:14] TRACE |     -> Found in chunk_id=architecture_overview_fix_0 (rank not in top-5)
[16:03:14] TRACE |   MISSED[2] Grafana
[16:03:14] TRACE |     -> Found in chunk_id=architecture_overview_fix_0 (rank not in top-5)
[16:03:14] TRACE |   MISSED[3] Jaeger for distributed tracing
[16:03:14] TRACE |     -> Found in chunk_id=architecture_overview_fix_0 (rank not in top-5)
[16:03:14] TRACE | === END FACT COVERAGE ===
[16:03:14] DEBUG | [anthropic] Model alias: claude-haiku -> claude-3-5-haiku-latest
[16:03:14] DEBUG | [anthropic] model=claude-3-5-haiku-latest prompt_len=834 timeout=5s
[16:03:14] DEBUG | [anthropic] PROMPT: You are a technical documentation search expert. Your task is to rewrite user questions as direct documentation lookup queries.

Guidelines:
1. Convert problem descriptions to feature/capability questions
2. Expand abbreviations and acronyms to full terms
3. Replace casual language with technical terminology
4. Align with documentation vocabulary and structure
5. Keep the rewritten query concise (one line)

Examples:
- "Why can't I schedule workflows every 30 seconds?" → "workflow scheduling min...
[16:03:14] DEBUG | [anthropic] REQUEST: POST https://api.anthropic.com/v1/messages
[16:03:14] DEBUG | [anthropic] REQUEST body: model=claude-3-5-haiku-latest max_tokens=1024
[16:03:15] DEBUG | [anthropic] RESPONSE status=200 elapsed=0.95s
[16:03:15] DEBUG | [anthropic] USAGE: input_tokens=197 output_tokens=15
[16:03:15] DEBUG | [anthropic] SUCCESS stop_reason=end_turn response_len=86
[16:03:15] DEBUG | [anthropic] RESPONSE: datadog metrics visibility configuration troubleshooting agent integration permissions
[16:03:15] TRACE | === FACT COVERAGE ANALYSIS ===
[16:03:15] TRACE | Query: Why can't I see metrics in Datadog?
[16:03:15] TRACE | Total key facts: 3
[16:03:15] TRACE | Found facts: 2/3
[16:03:15] TRACE |   FOUND[1] Prometheus
[16:03:15] TRACE |   FOUND[2] Grafana
[16:03:15] TRACE | Missed facts: 1/3
[16:03:15] TRACE |   MISSED[1] Jaeger for distributed tracing
[16:03:15] TRACE |     -> Found in chunk_id=architecture_overview_fix_0 (rank not in top-5)
[16:03:15] TRACE | === END FACT COVERAGE ===
[16:03:15] DEBUG | [anthropic] Model alias: claude-haiku -> claude-3-5-haiku-latest
[16:03:15] DEBUG | [anthropic] model=claude-3-5-haiku-latest prompt_len=853 timeout=5s
[16:03:15] DEBUG | [anthropic] PROMPT: You are a technical documentation search expert. Your task is to rewrite user questions as direct documentation lookup queries.

Guidelines:
1. Convert problem descriptions to feature/capability questions
2. Expand abbreviations and acronyms to full terms
3. Replace casual language with technical terminology
4. Align with documentation vocabulary and structure
5. Keep the rewritten query concise (one line)

Examples:
- "Why can't I schedule workflows every 30 seconds?" → "workflow scheduling min...
[16:03:15] DEBUG | [anthropic] REQUEST: POST https://api.anthropic.com/v1/messages
[16:03:15] DEBUG | [anthropic] REQUEST body: model=claude-3-5-haiku-latest max_tokens=1024
[16:03:16] DEBUG | [anthropic] RESPONSE status=200 elapsed=1.01s
[16:03:16] DEBUG | [anthropic] USAGE: input_tokens=197 output_tokens=17
[16:03:16] DEBUG | [anthropic] SUCCESS stop_reason=end_turn response_len=94
[16:03:16] DEBUG | [anthropic] RESPONSE: database connection pool monitoring troubleshooting diagnostics performance metrics exhaustion
[16:03:17] TRACE | === FACT COVERAGE ANALYSIS ===
[16:03:17] TRACE | Query: How do I diagnose database connection pool exhaustion?
[16:03:17] TRACE | Total key facts: 4
[16:03:17] TRACE | Found facts: 1/4
[16:03:17] TRACE |   FOUND[1] PgBouncer
[16:03:17] TRACE | Missed facts: 3/4
[16:03:17] TRACE |   MISSED[1] could not obtain connection from pool within 5000ms
[16:03:17] TRACE |     -> Found in chunk_id=troubleshooting_guide_fix_3 (rank not in top-5)
[16:03:17] TRACE |   MISSED[2] connection pool exhausted (100/100 connections in use)
[16:03:17] TRACE |     -> Found in chunk_id=troubleshooting_guide_fix_3 (rank not in top-5)
[16:03:17] TRACE |   MISSED[3] max_db_connections = 100
[16:03:17] TRACE |     -> Found in chunk_id=deployment_guide_fix_3 (rank not in top-5)
[16:03:17] TRACE | === END FACT COVERAGE ===
[16:03:17] DEBUG | [anthropic] Model alias: claude-haiku -> claude-3-5-haiku-latest
[16:03:17] DEBUG | [anthropic] model=claude-3-5-haiku-latest prompt_len=858 timeout=5s
[16:03:17] DEBUG | [anthropic] PROMPT: You are a technical documentation search expert. Your task is to rewrite user questions as direct documentation lookup queries.

Guidelines:
1. Convert problem descriptions to feature/capability questions
2. Expand abbreviations and acronyms to full terms
3. Replace casual language with technical terminology
4. Align with documentation vocabulary and structure
5. Keep the rewritten query concise (one line)

Examples:
- "Why can't I schedule workflows every 30 seconds?" → "workflow scheduling min...
[16:03:17] DEBUG | [anthropic] REQUEST: POST https://api.anthropic.com/v1/messages
[16:03:17] DEBUG | [anthropic] REQUEST body: model=claude-3-5-haiku-latest max_tokens=1024
[16:03:18] DEBUG | [anthropic] RESPONSE status=200 elapsed=1.50s
[16:03:18] DEBUG | [anthropic] USAGE: input_tokens=198 output_tokens=17
[16:03:18] DEBUG | [anthropic] SUCCESS stop_reason=end_turn response_len=89
[16:03:18] DEBUG | [anthropic] RESPONSE: database connection pool exhaustion troubleshooting maximum connections limit diagnostics
[16:03:19] TRACE | === FACT COVERAGE ANALYSIS ===
[16:03:19] TRACE | Query: What should I check when I run out of database connections?
[16:03:19] TRACE | Total key facts: 4
[16:03:19] TRACE | Found facts: 1/4
[16:03:19] TRACE |   FOUND[1] PgBouncer
[16:03:19] TRACE | Missed facts: 3/4
[16:03:19] TRACE |   MISSED[1] could not obtain connection from pool within 5000ms
[16:03:19] TRACE |     -> Found in chunk_id=troubleshooting_guide_fix_3 (rank not in top-5)
[16:03:19] TRACE |   MISSED[2] connection pool exhausted (100/100 connections in use)
[16:03:19] TRACE |     -> Found in chunk_id=troubleshooting_guide_fix_3 (rank not in top-5)
[16:03:19] TRACE |   MISSED[3] max_db_connections = 100
[16:03:19] TRACE |     -> Found in chunk_id=deployment_guide_fix_3 (rank not in top-5)
[16:03:19] TRACE | === END FACT COVERAGE ===
[16:03:19] DEBUG | [anthropic] Model alias: claude-haiku -> claude-3-5-haiku-latest
[16:03:19] DEBUG | [anthropic] model=claude-3-5-haiku-latest prompt_len=853 timeout=5s
[16:03:19] DEBUG | [anthropic] PROMPT: You are a technical documentation search expert. Your task is to rewrite user questions as direct documentation lookup queries.

Guidelines:
1. Convert problem descriptions to feature/capability questions
2. Expand abbreviations and acronyms to full terms
3. Replace casual language with technical terminology
4. Align with documentation vocabulary and structure
5. Keep the rewritten query concise (one line)

Examples:
- "Why can't I schedule workflows every 30 seconds?" → "workflow scheduling min...
[16:03:19] DEBUG | [anthropic] REQUEST: POST https://api.anthropic.com/v1/messages
[16:03:19] DEBUG | [anthropic] REQUEST body: model=claude-3-5-haiku-latest max_tokens=1024
[16:03:20] DEBUG | [anthropic] RESPONSE status=200 elapsed=1.25s
[16:03:20] DEBUG | [anthropic] USAGE: input_tokens=197 output_tokens=11
[16:03:20] DEBUG | [anthropic] SUCCESS stop_reason=end_turn response_len=74
[16:03:20] DEBUG | [anthropic] RESPONSE: database connection pool availability timeout configuration error handling
[16:03:20] TRACE | === FACT COVERAGE ANALYSIS ===
[16:03:20] TRACE | Query: Getting 'could not obtain connection from pool' errors
[16:03:20] TRACE | Total key facts: 4
[16:03:20] TRACE | Found facts: 1/4
[16:03:20] TRACE |   FOUND[1] PgBouncer
[16:03:20] TRACE | Missed facts: 3/4
[16:03:20] TRACE |   MISSED[1] could not obtain connection from pool within 5000ms
[16:03:20] TRACE |     -> Found in chunk_id=troubleshooting_guide_fix_3 (rank not in top-5)
[16:03:20] TRACE |   MISSED[2] connection pool exhausted (100/100 connections in use)
[16:03:20] TRACE |     -> Found in chunk_id=troubleshooting_guide_fix_3 (rank not in top-5)
[16:03:20] TRACE |   MISSED[3] max_db_connections = 100
[16:03:20] TRACE |     -> Found in chunk_id=deployment_guide_fix_3 (rank not in top-5)
[16:03:20] TRACE | === END FACT COVERAGE ===
[16:03:20] DEBUG | [anthropic] Model alias: claude-haiku -> claude-3-5-haiku-latest
[16:03:20] DEBUG | [anthropic] model=claude-3-5-haiku-latest prompt_len=819 timeout=5s
[16:03:20] DEBUG | [anthropic] PROMPT: You are a technical documentation search expert. Your task is to rewrite user questions as direct documentation lookup queries.

Guidelines:
1. Convert problem descriptions to feature/capability questions
2. Expand abbreviations and acronyms to full terms
3. Replace casual language with technical terminology
4. Align with documentation vocabulary and structure
5. Keep the rewritten query concise (one line)

Examples:
- "Why can't I schedule workflows every 30 seconds?" → "workflow scheduling min...
[16:03:20] DEBUG | [anthropic] REQUEST: POST https://api.anthropic.com/v1/messages
[16:03:20] DEBUG | [anthropic] REQUEST body: model=claude-3-5-haiku-latest max_tokens=1024
[16:03:21] DEBUG | [anthropic] RESPONSE status=200 elapsed=0.90s
[16:03:21] DEBUG | [anthropic] USAGE: input_tokens=189 output_tokens=11
[16:03:21] DEBUG | [anthropic] SUCCESS stop_reason=end_turn response_len=58
[16:03:21] DEBUG | [anthropic] RESPONSE: database connection pool maximum limit exhaustion handling
[16:03:22] TRACE | === FACT COVERAGE ANALYSIS ===
[16:03:22] TRACE | Query: connection pool full
[16:03:22] TRACE | Total key facts: 4
[16:03:22] TRACE | Found facts: 1/4
[16:03:22] TRACE |   FOUND[1] PgBouncer
[16:03:22] TRACE | Missed facts: 3/4
[16:03:22] TRACE |   MISSED[1] could not obtain connection from pool within 5000ms
[16:03:22] TRACE |     -> Found in chunk_id=troubleshooting_guide_fix_3 (rank not in top-5)
[16:03:22] TRACE |   MISSED[2] connection pool exhausted (100/100 connections in use)
[16:03:22] TRACE |     -> Found in chunk_id=troubleshooting_guide_fix_3 (rank not in top-5)
[16:03:22] TRACE |   MISSED[3] max_db_connections = 100
[16:03:22] TRACE |     -> Found in chunk_id=deployment_guide_fix_3 (rank not in top-5)
[16:03:22] TRACE | === END FACT COVERAGE ===
[16:03:22] DEBUG | [anthropic] Model alias: claude-haiku -> claude-3-5-haiku-latest
[16:03:22] DEBUG | [anthropic] model=claude-3-5-haiku-latest prompt_len=868 timeout=5s
[16:03:22] DEBUG | [anthropic] PROMPT: You are a technical documentation search expert. Your task is to rewrite user questions as direct documentation lookup queries.

Guidelines:
1. Convert problem descriptions to feature/capability questions
2. Expand abbreviations and acronyms to full terms
3. Replace casual language with technical terminology
4. Align with documentation vocabulary and structure
5. Keep the rewritten query concise (one line)

Examples:
- "Why can't I schedule workflows every 30 seconds?" → "workflow scheduling min...
[16:03:22] DEBUG | [anthropic] REQUEST: POST https://api.anthropic.com/v1/messages
[16:03:22] DEBUG | [anthropic] REQUEST body: model=claude-3-5-haiku-latest max_tokens=1024
[16:03:23] DEBUG | [anthropic] RESPONSE status=200 elapsed=1.09s
[16:03:23] DEBUG | [anthropic] USAGE: input_tokens=202 output_tokens=16
[16:03:23] DEBUG | [anthropic] SUCCESS stop_reason=end_turn response_len=98
[16:03:23] DEBUG | [anthropic] RESPONSE: connection pool configuration troubleshooting database connectivity error handling max connections
[16:03:23] TRACE | === FACT COVERAGE ANALYSIS ===
[16:03:23] TRACE | Query: My app is failing with connection pool errors, how do I troubleshoot?
[16:03:23] TRACE | Total key facts: 4
[16:03:23] TRACE | Found facts: 1/4
[16:03:23] TRACE |   FOUND[1] PgBouncer
[16:03:23] TRACE | Missed facts: 3/4
[16:03:23] TRACE |   MISSED[1] could not obtain connection from pool within 5000ms
[16:03:23] TRACE |     -> Found in chunk_id=troubleshooting_guide_fix_3 (rank not in top-5)
[16:03:23] TRACE |   MISSED[2] connection pool exhausted (100/100 connections in use)
[16:03:23] TRACE |     -> Found in chunk_id=troubleshooting_guide_fix_3 (rank not in top-5)
[16:03:23] TRACE |   MISSED[3] max_db_connections = 100
[16:03:23] TRACE |     -> Found in chunk_id=deployment_guide_fix_3 (rank not in top-5)
[16:03:23] TRACE | === END FACT COVERAGE ===
[16:03:23] DEBUG | [anthropic] Model alias: claude-haiku -> claude-3-5-haiku-latest
[16:03:23] DEBUG | [anthropic] model=claude-3-5-haiku-latest prompt_len=860 timeout=5s
[16:03:23] DEBUG | [anthropic] PROMPT: You are a technical documentation search expert. Your task is to rewrite user questions as direct documentation lookup queries.

Guidelines:
1. Convert problem descriptions to feature/capability questions
2. Expand abbreviations and acronyms to full terms
3. Replace casual language with technical terminology
4. Align with documentation vocabulary and structure
5. Keep the rewritten query concise (one line)

Examples:
- "Why can't I schedule workflows every 30 seconds?" → "workflow scheduling min...
[16:03:23] DEBUG | [anthropic] REQUEST: POST https://api.anthropic.com/v1/messages
[16:03:23] DEBUG | [anthropic] REQUEST body: model=claude-3-5-haiku-latest max_tokens=1024
[16:03:24] DEBUG | [anthropic] RESPONSE status=200 elapsed=1.09s
[16:03:24] DEBUG | [anthropic] USAGE: input_tokens=200 output_tokens=17
[16:03:24] DEBUG | [anthropic] SUCCESS stop_reason=end_turn response_len=105
[16:03:24] DEBUG | [anthropic] RESPONSE: database connection failure troubleshooting connection pool configuration max connections resource limits
[16:03:25] TRACE | === FACT COVERAGE ANALYSIS ===
[16:03:25] TRACE | Query: Why can't I get a database connection even though CPU is low?
[16:03:25] TRACE | Total key facts: 4
[16:03:25] TRACE | Found facts: 0/4
[16:03:25] TRACE | Missed facts: 4/4
[16:03:25] TRACE |   MISSED[1] could not obtain connection from pool within 5000ms
[16:03:25] TRACE |     -> Found in chunk_id=troubleshooting_guide_fix_3 (rank not in top-5)
[16:03:25] TRACE |   MISSED[2] connection pool exhausted (100/100 connections in use)
[16:03:25] TRACE |     -> Found in chunk_id=troubleshooting_guide_fix_3 (rank not in top-5)
[16:03:25] TRACE |   MISSED[3] PgBouncer
[16:03:25] TRACE |     -> Found in chunk_id=architecture_overview_fix_5 (rank not in top-5)
[16:03:25] TRACE |   MISSED[4] max_db_connections = 100
[16:03:25] TRACE |     -> Found in chunk_id=deployment_guide_fix_3 (rank not in top-5)
[16:03:25] TRACE | === END FACT COVERAGE ===
[16:03:25] DEBUG | [anthropic] Model alias: claude-haiku -> claude-3-5-haiku-latest
[16:03:25] DEBUG | [anthropic] model=claude-3-5-haiku-latest prompt_len=834 timeout=5s
[16:03:25] DEBUG | [anthropic] PROMPT: You are a technical documentation search expert. Your task is to rewrite user questions as direct documentation lookup queries.

Guidelines:
1. Convert problem descriptions to feature/capability questions
2. Expand abbreviations and acronyms to full terms
3. Replace casual language with technical terminology
4. Align with documentation vocabulary and structure
5. Keep the rewritten query concise (one line)

Examples:
- "Why can't I schedule workflows every 30 seconds?" → "workflow scheduling min...
[16:03:25] DEBUG | [anthropic] REQUEST: POST https://api.anthropic.com/v1/messages
[16:03:25] DEBUG | [anthropic] REQUEST body: model=claude-3-5-haiku-latest max_tokens=1024
[16:03:25] DEBUG | [anthropic] RESPONSE status=200 elapsed=0.95s
[16:03:25] DEBUG | [anthropic] USAGE: input_tokens=193 output_tokens=14
[16:03:25] DEBUG | [anthropic] SUCCESS stop_reason=end_turn response_len=77
[16:03:25] DEBUG | [anthropic] RESPONSE: "api authentication methods token-based authorization credentials validation"
[16:03:26] TRACE | === FACT COVERAGE ANALYSIS ===
[16:03:26] TRACE | Query: How do I handle API authentication?
[16:03:26] TRACE | Total key facts: 3
[16:03:26] TRACE | Found facts: 3/3
[16:03:26] TRACE |   FOUND[1] OAuth 2.0
[16:03:26] TRACE |   FOUND[2] API keys
[16:03:26] TRACE |   FOUND[3] JWT tokens
[16:03:26] TRACE | === END FACT COVERAGE ===
[16:03:26] DEBUG | [anthropic] Model alias: claude-haiku -> claude-3-5-haiku-latest
[16:03:26] DEBUG | [anthropic] model=claude-3-5-haiku-latest prompt_len=841 timeout=5s
[16:03:26] DEBUG | [anthropic] PROMPT: You are a technical documentation search expert. Your task is to rewrite user questions as direct documentation lookup queries.

Guidelines:
1. Convert problem descriptions to feature/capability questions
2. Expand abbreviations and acronyms to full terms
3. Replace casual language with technical terminology
4. Align with documentation vocabulary and structure
5. Keep the rewritten query concise (one line)

Examples:
- "Why can't I schedule workflows every 30 seconds?" → "workflow scheduling min...
[16:03:26] DEBUG | [anthropic] REQUEST: POST https://api.anthropic.com/v1/messages
[16:03:26] DEBUG | [anthropic] REQUEST body: model=claude-3-5-haiku-latest max_tokens=1024
[16:03:27] DEBUG | [anthropic] RESPONSE status=200 elapsed=0.91s
[16:03:27] DEBUG | [anthropic] USAGE: input_tokens=192 output_tokens=8
[16:03:27] DEBUG | [anthropic] SUCCESS stop_reason=end_turn response_len=46
[16:03:27] DEBUG | [anthropic] RESPONSE: authentication supported methods and protocols
[16:03:27] TRACE | === FACT COVERAGE ANALYSIS ===
[16:03:27] TRACE | Query: What authentication methods are supported?
[16:03:27] TRACE | Total key facts: 3
[16:03:27] TRACE | Found facts: 3/3
[16:03:27] TRACE |   FOUND[1] OAuth 2.0
[16:03:27] TRACE |   FOUND[2] API keys
[16:03:27] TRACE |   FOUND[3] JWT tokens
[16:03:27] TRACE | === END FACT COVERAGE ===
[16:03:27] DEBUG | [anthropic] Model alias: claude-haiku -> claude-3-5-haiku-latest
[16:03:27] DEBUG | [anthropic] model=claude-3-5-haiku-latest prompt_len=837 timeout=5s
[16:03:27] DEBUG | [anthropic] PROMPT: You are a technical documentation search expert. Your task is to rewrite user questions as direct documentation lookup queries.

Guidelines:
1. Convert problem descriptions to feature/capability questions
2. Expand abbreviations and acronyms to full terms
3. Replace casual language with technical terminology
4. Align with documentation vocabulary and structure
5. Keep the rewritten query concise (one line)

Examples:
- "Why can't I schedule workflows every 30 seconds?" → "workflow scheduling min...
[16:03:27] DEBUG | [anthropic] REQUEST: POST https://api.anthropic.com/v1/messages
[16:03:27] DEBUG | [anthropic] REQUEST body: model=claude-3-5-haiku-latest max_tokens=1024
[16:03:28] DEBUG | [anthropic] RESPONSE status=200 elapsed=0.95s
[16:03:28] DEBUG | [anthropic] USAGE: input_tokens=195 output_tokens=14
[16:03:28] DEBUG | [anthropic] SUCCESS stop_reason=end_turn response_len=57
[16:03:28] DEBUG | [anthropic] RESPONSE: api authentication unauthorized 401 error troubleshooting
[16:03:29] TRACE | === FACT COVERAGE ANALYSIS ===
[16:03:29] TRACE | Query: My API requests are getting 401 errors
[16:03:29] TRACE | Total key facts: 3
[16:03:29] TRACE | Found facts: 3/3
[16:03:29] TRACE |   FOUND[1] OAuth 2.0
[16:03:29] TRACE |   FOUND[2] API keys
[16:03:29] TRACE |   FOUND[3] JWT tokens
[16:03:29] TRACE | === END FACT COVERAGE ===
[16:03:29] DEBUG | [anthropic] Model alias: claude-haiku -> claude-3-5-haiku-latest
[16:03:29] DEBUG | [anthropic] model=claude-3-5-haiku-latest prompt_len=811 timeout=5s
[16:03:29] DEBUG | [anthropic] PROMPT: You are a technical documentation search expert. Your task is to rewrite user questions as direct documentation lookup queries.

Guidelines:
1. Convert problem descriptions to feature/capability questions
2. Expand abbreviations and acronyms to full terms
3. Replace casual language with technical terminology
4. Align with documentation vocabulary and structure
5. Keep the rewritten query concise (one line)

Examples:
- "Why can't I schedule workflows every 30 seconds?" → "workflow scheduling min...
[16:03:29] DEBUG | [anthropic] REQUEST: POST https://api.anthropic.com/v1/messages
[16:03:29] DEBUG | [anthropic] REQUEST body: model=claude-3-5-haiku-latest max_tokens=1024
[16:03:29] DEBUG | [anthropic] RESPONSE status=200 elapsed=0.87s
[16:03:29] DEBUG | [anthropic] USAGE: input_tokens=188 output_tokens=9
[16:03:29] DEBUG | [anthropic] SUCCESS stop_reason=end_turn response_len=67
[16:03:29] DEBUG | [anthropic] RESPONSE: authentication methods authorization techniques supported protocols
[16:03:30] TRACE | === FACT COVERAGE ANALYSIS ===
[16:03:30] TRACE | Query: auth methods
[16:03:30] TRACE | Total key facts: 3
[16:03:30] TRACE | Found facts: 3/3
[16:03:30] TRACE |   FOUND[1] OAuth 2.0
[16:03:30] TRACE |   FOUND[2] API keys
[16:03:30] TRACE |   FOUND[3] JWT tokens
[16:03:30] TRACE | === END FACT COVERAGE ===
[16:03:30] DEBUG | [anthropic] Model alias: claude-haiku -> claude-3-5-haiku-latest
[16:03:30] DEBUG | [anthropic] model=claude-3-5-haiku-latest prompt_len=873 timeout=5s
[16:03:30] DEBUG | [anthropic] PROMPT: You are a technical documentation search expert. Your task is to rewrite user questions as direct documentation lookup queries.

Guidelines:
1. Convert problem descriptions to feature/capability questions
2. Expand abbreviations and acronyms to full terms
3. Replace casual language with technical terminology
4. Align with documentation vocabulary and structure
5. Keep the rewritten query concise (one line)

Examples:
- "Why can't I schedule workflows every 30 seconds?" → "workflow scheduling min...
[16:03:30] DEBUG | [anthropic] REQUEST: POST https://api.anthropic.com/v1/messages
[16:03:30] DEBUG | [anthropic] REQUEST body: model=claude-3-5-haiku-latest max_tokens=1024
[16:03:31] DEBUG | [anthropic] RESPONSE status=200 elapsed=0.88s
[16:03:31] DEBUG | [anthropic] USAGE: input_tokens=202 output_tokens=11
[16:03:31] DEBUG | [anthropic] SUCCESS stop_reason=end_turn response_len=71
[16:03:31] DEBUG | [anthropic] RESPONSE: CloudFlow API authentication methods authentication options credentials
[16:03:31] TRACE | === FACT COVERAGE ANALYSIS ===
[16:03:31] TRACE | Query: I'm integrating with CloudFlow API, what authentication options do I have?
[16:03:31] TRACE | Total key facts: 3
[16:03:31] TRACE | Found facts: 3/3
[16:03:31] TRACE |   FOUND[1] OAuth 2.0
[16:03:31] TRACE |   FOUND[2] API keys
[16:03:31] TRACE |   FOUND[3] JWT tokens
[16:03:31] TRACE | === END FACT COVERAGE ===
[16:03:31] DEBUG | [anthropic] Model alias: claude-haiku -> claude-3-5-haiku-latest
[16:03:31] DEBUG | [anthropic] model=claude-3-5-haiku-latest prompt_len=828 timeout=5s
[16:03:31] DEBUG | [anthropic] PROMPT: You are a technical documentation search expert. Your task is to rewrite user questions as direct documentation lookup queries.

Guidelines:
1. Convert problem descriptions to feature/capability questions
2. Expand abbreviations and acronyms to full terms
3. Replace casual language with technical terminology
4. Align with documentation vocabulary and structure
5. Keep the rewritten query concise (one line)

Examples:
- "Why can't I schedule workflows every 30 seconds?" → "workflow scheduling min...
[16:03:31] DEBUG | [anthropic] REQUEST: POST https://api.anthropic.com/v1/messages
[16:03:31] DEBUG | [anthropic] REQUEST body: model=claude-3-5-haiku-latest max_tokens=1024
[16:03:32] DEBUG | [anthropic] RESPONSE status=200 elapsed=0.93s
[16:03:32] DEBUG | [anthropic] USAGE: input_tokens=193 output_tokens=13
[16:03:32] DEBUG | [anthropic] SUCCESS stop_reason=end_turn response_len=81
[16:03:32] DEBUG | [anthropic] RESPONSE: basic authentication configuration troubleshooting method validation requirements
[16:03:32] TRACE | === FACT COVERAGE ANALYSIS ===
[16:03:32] TRACE | Query: Why isn't basic auth working?
[16:03:32] TRACE | Total key facts: 3
[16:03:32] TRACE | Found facts: 3/3
[16:03:32] TRACE |   FOUND[1] OAuth 2.0
[16:03:32] TRACE |   FOUND[2] API keys
[16:03:32] TRACE |   FOUND[3] JWT tokens
[16:03:32] TRACE | === END FACT COVERAGE ===
[16:03:32] DEBUG | [anthropic] Model alias: claude-haiku -> claude-3-5-haiku-latest
[16:03:32] DEBUG | [anthropic] model=claude-3-5-haiku-latest prompt_len=849 timeout=5s
[16:03:32] DEBUG | [anthropic] PROMPT: You are a technical documentation search expert. Your task is to rewrite user questions as direct documentation lookup queries.

Guidelines:
1. Convert problem descriptions to feature/capability questions
2. Expand abbreviations and acronyms to full terms
3. Replace casual language with technical terminology
4. Align with documentation vocabulary and structure
5. Keep the rewritten query concise (one line)

Examples:
- "Why can't I schedule workflows every 30 seconds?" → "workflow scheduling min...
[16:03:32] DEBUG | [anthropic] REQUEST: POST https://api.anthropic.com/v1/messages
[16:03:32] DEBUG | [anthropic] REQUEST body: model=claude-3-5-haiku-latest max_tokens=1024
[16:03:33] DEBUG | [anthropic] RESPONSE status=200 elapsed=0.97s
[16:03:33] DEBUG | [anthropic] USAGE: input_tokens=202 output_tokens=13
[16:03:33] DEBUG | [anthropic] SUCCESS stop_reason=end_turn response_len=76
[16:03:33] DEBUG | [anthropic] RESPONSE: PostgreSQL connection pooling CloudFlow integration performance optimization
[16:03:34] TRACE | === FACT COVERAGE ANALYSIS ===
[16:03:34] TRACE | Query: What is PgBouncer and why is it used in CloudFlow?
[16:03:34] TRACE | Total key facts: 5
[16:03:34] TRACE | Found facts: 5/5
[16:03:34] TRACE |   FOUND[1] PgBouncer
[16:03:34] TRACE |   FOUND[2] connection pooling
[16:03:34] TRACE |   FOUND[3] max_db_connections = 100
[16:03:34] TRACE |   FOUND[4] default_pool_size = 25
[16:03:34] TRACE |   FOUND[5] pool_mode = transaction
[16:03:34] TRACE | === END FACT COVERAGE ===
[16:03:34] DEBUG | [anthropic] Model alias: claude-haiku -> claude-3-5-haiku-latest
[16:03:34] DEBUG | [anthropic] model=claude-3-5-haiku-latest prompt_len=843 timeout=5s
[16:03:34] DEBUG | [anthropic] PROMPT: You are a technical documentation search expert. Your task is to rewrite user questions as direct documentation lookup queries.

Guidelines:
1. Convert problem descriptions to feature/capability questions
2. Expand abbreviations and acronyms to full terms
3. Replace casual language with technical terminology
4. Align with documentation vocabulary and structure
5. Keep the rewritten query concise (one line)

Examples:
- "Why can't I schedule workflows every 30 seconds?" → "workflow scheduling min...
[16:03:34] DEBUG | [anthropic] REQUEST: POST https://api.anthropic.com/v1/messages
[16:03:34] DEBUG | [anthropic] REQUEST body: model=claude-3-5-haiku-latest max_tokens=1024
[16:03:35] DEBUG | [anthropic] RESPONSE status=200 elapsed=1.34s
[16:03:35] DEBUG | [anthropic] USAGE: input_tokens=196 output_tokens=11
[16:03:35] DEBUG | [anthropic] SUCCESS stop_reason=end_turn response_len=73
[16:03:35] DEBUG | [anthropic] RESPONSE: connection pooler database connection management performance optimization
[16:03:36] TRACE | === FACT COVERAGE ANALYSIS ===
[16:03:36] TRACE | Query: What's the purpose of the connection pooler?
[16:03:36] TRACE | Total key facts: 5
[16:03:36] TRACE | Found facts: 0/5
[16:03:36] TRACE | Missed facts: 5/5
[16:03:36] TRACE |   MISSED[1] PgBouncer
[16:03:36] TRACE |     -> Found in chunk_id=architecture_overview_fix_5 (rank not in top-5)
[16:03:36] TRACE |   MISSED[2] connection pooling
[16:03:36] TRACE |     -> Found in chunk_id=architecture_overview_fix_5 (rank not in top-5)
[16:03:36] TRACE |   MISSED[3] max_db_connections = 100
[16:03:36] TRACE |     -> Found in chunk_id=deployment_guide_fix_3 (rank not in top-5)
[16:03:36] TRACE |   MISSED[4] default_pool_size = 25
[16:03:36] TRACE |     -> Found in chunk_id=deployment_guide_fix_3 (rank not in top-5)
[16:03:36] TRACE |   MISSED[5] pool_mode = transaction
[16:03:36] TRACE |     -> Found in chunk_id=deployment_guide_fix_3 (rank not in top-5)
[16:03:36] TRACE | === END FACT COVERAGE ===
[16:03:36] DEBUG | [anthropic] Model alias: claude-haiku -> claude-3-5-haiku-latest
[16:03:36] DEBUG | [anthropic] model=claude-3-5-haiku-latest prompt_len=860 timeout=5s
[16:03:36] DEBUG | [anthropic] PROMPT: You are a technical documentation search expert. Your task is to rewrite user questions as direct documentation lookup queries.

Guidelines:
1. Convert problem descriptions to feature/capability questions
2. Expand abbreviations and acronyms to full terms
3. Replace casual language with technical terminology
4. Align with documentation vocabulary and structure
5. Keep the rewritten query concise (one line)

Examples:
- "Why can't I schedule workflows every 30 seconds?" → "workflow scheduling min...
[16:03:36] DEBUG | [anthropic] REQUEST: POST https://api.anthropic.com/v1/messages
[16:03:36] DEBUG | [anthropic] REQUEST body: model=claude-3-5-haiku-latest max_tokens=1024
[16:03:37] DEBUG | [anthropic] RESPONSE status=200 elapsed=1.03s
[16:03:37] DEBUG | [anthropic] USAGE: input_tokens=201 output_tokens=20
[16:03:37] DEBUG | [anthropic] SUCCESS stop_reason=end_turn response_len=80
[16:03:37] DEBUG | [anthropic] RESPONSE: "PostgreSQL connection pooling PgBouncer performance scalability best practices"
[16:03:37] TRACE | === FACT COVERAGE ANALYSIS ===
[16:03:37] TRACE | Query: Should I connect directly to PostgreSQL or through PgBouncer?
[16:03:37] TRACE | Total key facts: 5
[16:03:37] TRACE | Found facts: 5/5
[16:03:37] TRACE |   FOUND[1] PgBouncer
[16:03:37] TRACE |   FOUND[2] connection pooling
[16:03:37] TRACE |   FOUND[3] max_db_connections = 100
[16:03:37] TRACE |   FOUND[4] default_pool_size = 25
[16:03:37] TRACE |   FOUND[5] pool_mode = transaction
[16:03:37] TRACE | === END FACT COVERAGE ===
[16:03:37] DEBUG | [anthropic] Model alias: claude-haiku -> claude-3-5-haiku-latest
[16:03:37] DEBUG | [anthropic] model=claude-3-5-haiku-latest prompt_len=816 timeout=5s
[16:03:37] DEBUG | [anthropic] PROMPT: You are a technical documentation search expert. Your task is to rewrite user questions as direct documentation lookup queries.

Guidelines:
1. Convert problem descriptions to feature/capability questions
2. Expand abbreviations and acronyms to full terms
3. Replace casual language with technical terminology
4. Align with documentation vocabulary and structure
5. Keep the rewritten query concise (one line)

Examples:
- "Why can't I schedule workflows every 30 seconds?" → "workflow scheduling min...
[16:03:37] DEBUG | [anthropic] REQUEST: POST https://api.anthropic.com/v1/messages
[16:03:37] DEBUG | [anthropic] REQUEST body: model=claude-3-5-haiku-latest max_tokens=1024
[16:03:38] DEBUG | [anthropic] RESPONSE status=200 elapsed=0.79s
[16:03:38] DEBUG | [anthropic] USAGE: input_tokens=191 output_tokens=13
[16:03:38] DEBUG | [anthropic] SUCCESS stop_reason=end_turn response_len=57
[16:03:38] DEBUG | [anthropic] RESPONSE: pgbouncer connection pooling database proxy configuration
[16:03:38] TRACE | === FACT COVERAGE ANALYSIS ===
[16:03:38] TRACE | Query: pgbouncer purpose
[16:03:38] TRACE | Total key facts: 5
[16:03:38] TRACE | Found facts: 5/5
[16:03:38] TRACE |   FOUND[1] PgBouncer
[16:03:38] TRACE |   FOUND[2] connection pooling
[16:03:38] TRACE |   FOUND[3] max_db_connections = 100
[16:03:38] TRACE |   FOUND[4] default_pool_size = 25
[16:03:38] TRACE |   FOUND[5] pool_mode = transaction
[16:03:38] TRACE | === END FACT COVERAGE ===
[16:03:38] DEBUG | [anthropic] Model alias: claude-haiku -> claude-3-5-haiku-latest
[16:03:38] DEBUG | [anthropic] model=claude-3-5-haiku-latest prompt_len=862 timeout=5s
[16:03:38] DEBUG | [anthropic] PROMPT: You are a technical documentation search expert. Your task is to rewrite user questions as direct documentation lookup queries.

Guidelines:
1. Convert problem descriptions to feature/capability questions
2. Expand abbreviations and acronyms to full terms
3. Replace casual language with technical terminology
4. Align with documentation vocabulary and structure
5. Keep the rewritten query concise (one line)

Examples:
- "Why can't I schedule workflows every 30 seconds?" → "workflow scheduling min...
[16:03:38] DEBUG | [anthropic] REQUEST: POST https://api.anthropic.com/v1/messages
[16:03:38] DEBUG | [anthropic] REQUEST body: model=claude-3-5-haiku-latest max_tokens=1024
[16:03:39] DEBUG | [anthropic] RESPONSE status=200 elapsed=0.95s
[16:03:39] DEBUG | [anthropic] USAGE: input_tokens=202 output_tokens=16
[16:03:39] DEBUG | [anthropic] SUCCESS stop_reason=end_turn response_len=92
[16:03:39] DEBUG | [anthropic] RESPONSE: connection pooling postgresql pgbouncer database connection optimization performance scaling
[16:03:40] TRACE | === FACT COVERAGE ANALYSIS ===
[16:03:40] TRACE | Query: Optimizing database connections, what role does PgBouncer play?
[16:03:40] TRACE | Total key facts: 5
[16:03:40] TRACE | Found facts: 5/5
[16:03:40] TRACE |   FOUND[1] PgBouncer
[16:03:40] TRACE |   FOUND[2] connection pooling
[16:03:40] TRACE |   FOUND[3] max_db_connections = 100
[16:03:40] TRACE |   FOUND[4] default_pool_size = 25
[16:03:40] TRACE |   FOUND[5] pool_mode = transaction
[16:03:40] TRACE | === END FACT COVERAGE ===
[16:03:40] DEBUG | [anthropic] Model alias: claude-haiku -> claude-3-5-haiku-latest
[16:03:40] DEBUG | [anthropic] model=claude-3-5-haiku-latest prompt_len=844 timeout=5s
[16:03:40] DEBUG | [anthropic] PROMPT: You are a technical documentation search expert. Your task is to rewrite user questions as direct documentation lookup queries.

Guidelines:
1. Convert problem descriptions to feature/capability questions
2. Expand abbreviations and acronyms to full terms
3. Replace casual language with technical terminology
4. Align with documentation vocabulary and structure
5. Keep the rewritten query concise (one line)

Examples:
- "Why can't I schedule workflows every 30 seconds?" → "workflow scheduling min...
[16:03:40] DEBUG | [anthropic] REQUEST: POST https://api.anthropic.com/v1/messages
[16:03:40] DEBUG | [anthropic] REQUEST body: model=claude-3-5-haiku-latest max_tokens=1024
[16:03:41] DEBUG | [anthropic] RESPONSE status=200 elapsed=1.14s
[16:03:41] DEBUG | [anthropic] USAGE: input_tokens=196 output_tokens=13
[16:03:41] DEBUG | [anthropic] SUCCESS stop_reason=end_turn response_len=102
[16:03:41] DEBUG | [anthropic] RESPONSE: database connection authentication network security access restrictions direct connection requirements
[16:03:41] TRACE | === FACT COVERAGE ANALYSIS ===
[16:03:41] TRACE | Query: Why can't I connect directly to the database?
[16:03:41] TRACE | Total key facts: 5
[16:03:41] TRACE | Found facts: 2/5
[16:03:41] TRACE |   FOUND[1] PgBouncer
[16:03:41] TRACE |   FOUND[2] connection pooling
[16:03:41] TRACE | Missed facts: 3/5
[16:03:41] TRACE |   MISSED[1] max_db_connections = 100
[16:03:41] TRACE |     -> Found in chunk_id=deployment_guide_fix_3 (rank not in top-5)
[16:03:41] TRACE |   MISSED[2] default_pool_size = 25
[16:03:41] TRACE |     -> Found in chunk_id=deployment_guide_fix_3 (rank not in top-5)
[16:03:41] TRACE |   MISSED[3] pool_mode = transaction
[16:03:41] TRACE |     -> Found in chunk_id=deployment_guide_fix_3 (rank not in top-5)
[16:03:41] TRACE | === END FACT COVERAGE ===
[16:03:41] DEBUG | [anthropic] Model alias: claude-haiku -> claude-3-5-haiku-latest
[16:03:41] DEBUG | [anthropic] model=claude-3-5-haiku-latest prompt_len=856 timeout=5s
[16:03:41] DEBUG | [anthropic] PROMPT: You are a technical documentation search expert. Your task is to rewrite user questions as direct documentation lookup queries.

Guidelines:
1. Convert problem descriptions to feature/capability questions
2. Expand abbreviations and acronyms to full terms
3. Replace casual language with technical terminology
4. Align with documentation vocabulary and structure
5. Keep the rewritten query concise (one line)

Examples:
- "Why can't I schedule workflows every 30 seconds?" → "workflow scheduling min...
[16:03:41] DEBUG | [anthropic] REQUEST: POST https://api.anthropic.com/v1/messages
[16:03:41] DEBUG | [anthropic] REQUEST body: model=claude-3-5-haiku-latest max_tokens=1024
[16:03:42] DEBUG | [anthropic] RESPONSE status=200 elapsed=0.92s
[16:03:42] DEBUG | [anthropic] USAGE: input_tokens=197 output_tokens=10
[16:03:42] DEBUG | [anthropic] SUCCESS stop_reason=end_turn response_len=60
[16:03:42] DEBUG | [anthropic] RESPONSE: workflow step failure retry mechanism implementation pattern
[16:03:42] TRACE | === FACT COVERAGE ANALYSIS ===
[16:03:42] TRACE | Query: How do I implement retry logic for failed workflow steps?
[16:03:42] TRACE | Total key facts: 4
[16:03:42] TRACE | Found facts: 0/4
[16:03:42] TRACE | Missed facts: 4/4
[16:03:42] TRACE |   MISSED[1] max_attempts: 3
[16:03:42] TRACE |     -> Found in chunk_id=user_guide_fix_5 (rank not in top-5)
[16:03:42] TRACE |   MISSED[2] backoff_type: exponential
[16:03:42] TRACE |     -> Found in chunk_id=user_guide_fix_7 (rank not in top-5)
[16:03:42] TRACE |   MISSED[3] initial_interval: 1000
[16:03:42] TRACE |     -> Found in chunk_id=user_guide_fix_5 (rank not in top-5)
[16:03:42] TRACE |   MISSED[4] max retries: 3
[16:03:42] TRACE |     -> Found in chunk_id=troubleshooting_guide_fix_5 (rank not in top-5)
[16:03:42] TRACE | === END FACT COVERAGE ===
[16:03:42] DEBUG | [anthropic] Model alias: claude-haiku -> claude-3-5-haiku-latest
[16:03:42] DEBUG | [anthropic] model=claude-3-5-haiku-latest prompt_len=848 timeout=5s
[16:03:42] DEBUG | [anthropic] PROMPT: You are a technical documentation search expert. Your task is to rewrite user questions as direct documentation lookup queries.

Guidelines:
1. Convert problem descriptions to feature/capability questions
2. Expand abbreviations and acronyms to full terms
3. Replace casual language with technical terminology
4. Align with documentation vocabulary and structure
5. Keep the rewritten query concise (one line)

Examples:
- "Why can't I schedule workflows every 30 seconds?" → "workflow scheduling min...
[16:03:42] DEBUG | [anthropic] REQUEST: POST https://api.anthropic.com/v1/messages
[16:03:42] DEBUG | [anthropic] REQUEST body: model=claude-3-5-haiku-latest max_tokens=1024
[16:03:43] DEBUG | [anthropic] RESPONSE status=200 elapsed=0.92s
[16:03:43] DEBUG | [anthropic] USAGE: input_tokens=196 output_tokens=15
[16:03:43] DEBUG | [anthropic] SUCCESS stop_reason=end_turn response_len=77
[16:03:43] DEBUG | [anthropic] RESPONSE: transient failure retry mechanism exponential backoff strategy error handling
[16:03:44] TRACE | === FACT COVERAGE ANALYSIS ===
[16:03:44] TRACE | Query: What's the retry strategy for transient failures?
[16:03:44] TRACE | Total key facts: 4
[16:03:44] TRACE | Found facts: 0/4
[16:03:44] TRACE | Missed facts: 4/4
[16:03:44] TRACE |   MISSED[1] max_attempts: 3
[16:03:44] TRACE |     -> Found in chunk_id=user_guide_fix_5 (rank not in top-5)
[16:03:44] TRACE |   MISSED[2] backoff_type: exponential
[16:03:44] TRACE |     -> Found in chunk_id=user_guide_fix_7 (rank not in top-5)
[16:03:44] TRACE |   MISSED[3] initial_interval: 1000
[16:03:44] TRACE |     -> Found in chunk_id=user_guide_fix_5 (rank not in top-5)
[16:03:44] TRACE |   MISSED[4] max retries: 3
[16:03:44] TRACE |     -> Found in chunk_id=troubleshooting_guide_fix_5 (rank not in top-5)
[16:03:44] TRACE | === END FACT COVERAGE ===
[16:03:44] DEBUG | [anthropic] Model alias: claude-haiku -> claude-3-5-haiku-latest
[16:03:44] DEBUG | [anthropic] model=claude-3-5-haiku-latest prompt_len=844 timeout=5s
[16:03:44] DEBUG | [anthropic] PROMPT: You are a technical documentation search expert. Your task is to rewrite user questions as direct documentation lookup queries.

Guidelines:
1. Convert problem descriptions to feature/capability questions
2. Expand abbreviations and acronyms to full terms
3. Replace casual language with technical terminology
4. Align with documentation vocabulary and structure
5. Keep the rewritten query concise (one line)

Examples:
- "Why can't I schedule workflows every 30 seconds?" → "workflow scheduling min...
[16:03:44] DEBUG | [anthropic] REQUEST: POST https://api.anthropic.com/v1/messages
[16:03:44] DEBUG | [anthropic] REQUEST body: model=claude-3-5-haiku-latest max_tokens=1024
[16:03:45] DEBUG | [anthropic] RESPONSE status=200 elapsed=0.97s
[16:03:45] DEBUG | [anthropic] USAGE: input_tokens=193 output_tokens=13
[16:03:45] DEBUG | [anthropic] SUCCESS stop_reason=end_turn response_len=73
[16:03:45] DEBUG | [anthropic] RESPONSE: workflow transient network error handling fault tolerance retry mechanism
[16:03:45] TRACE | === FACT COVERAGE ANALYSIS ===
[16:03:45] TRACE | Query: My workflow fails on temporary network errors
[16:03:45] TRACE | Total key facts: 4
[16:03:45] TRACE | Found facts: 1/4
[16:03:45] TRACE |   FOUND[1] max retries: 3
[16:03:45] TRACE | Missed facts: 3/4
[16:03:45] TRACE |   MISSED[1] max_attempts: 3
[16:03:45] TRACE |     -> Found in chunk_id=user_guide_fix_5 (rank not in top-5)
[16:03:45] TRACE |   MISSED[2] backoff_type: exponential
[16:03:45] TRACE |     -> Found in chunk_id=user_guide_fix_7 (rank not in top-5)
[16:03:45] TRACE |   MISSED[3] initial_interval: 1000
[16:03:45] TRACE |     -> Found in chunk_id=user_guide_fix_5 (rank not in top-5)
[16:03:45] TRACE | === END FACT COVERAGE ===
[16:03:45] DEBUG | [anthropic] Model alias: claude-haiku -> claude-3-5-haiku-latest
[16:03:45] DEBUG | [anthropic] model=claude-3-5-haiku-latest prompt_len=811 timeout=5s
[16:03:45] DEBUG | [anthropic] PROMPT: You are a technical documentation search expert. Your task is to rewrite user questions as direct documentation lookup queries.

Guidelines:
1. Convert problem descriptions to feature/capability questions
2. Expand abbreviations and acronyms to full terms
3. Replace casual language with technical terminology
4. Align with documentation vocabulary and structure
5. Keep the rewritten query concise (one line)

Examples:
- "Why can't I schedule workflows every 30 seconds?" → "workflow scheduling min...
[16:03:45] DEBUG | [anthropic] REQUEST: POST https://api.anthropic.com/v1/messages
[16:03:45] DEBUG | [anthropic] REQUEST body: model=claude-3-5-haiku-latest max_tokens=1024
[16:03:46] DEBUG | [anthropic] RESPONSE status=200 elapsed=0.83s
[16:03:46] DEBUG | [anthropic] USAGE: input_tokens=188 output_tokens=8
[16:03:46] DEBUG | [anthropic] SUCCESS stop_reason=end_turn response_len=44
[16:03:46] DEBUG | [anthropic] RESPONSE: retry mechanism configuration options policy
[16:03:46] TRACE | === FACT COVERAGE ANALYSIS ===
[16:03:46] TRACE | Query: retry config
[16:03:46] TRACE | Total key facts: 4
[16:03:46] TRACE | Found facts: 0/4
[16:03:46] TRACE | Missed facts: 4/4
[16:03:46] TRACE |   MISSED[1] max_attempts: 3
[16:03:46] TRACE |     -> Found in chunk_id=user_guide_fix_5 (rank not in top-5)
[16:03:46] TRACE |   MISSED[2] backoff_type: exponential
[16:03:46] TRACE |     -> Found in chunk_id=user_guide_fix_7 (rank not in top-5)
[16:03:46] TRACE |   MISSED[3] initial_interval: 1000
[16:03:46] TRACE |     -> Found in chunk_id=user_guide_fix_5 (rank not in top-5)
[16:03:46] TRACE |   MISSED[4] max retries: 3
[16:03:46] TRACE |     -> Found in chunk_id=troubleshooting_guide_fix_5 (rank not in top-5)
[16:03:46] TRACE | === END FACT COVERAGE ===
[16:03:46] DEBUG | [anthropic] Model alias: claude-haiku -> claude-3-5-haiku-latest
[16:03:46] DEBUG | [anthropic] model=claude-3-5-haiku-latest prompt_len=871 timeout=5s
[16:03:46] DEBUG | [anthropic] PROMPT: You are a technical documentation search expert. Your task is to rewrite user questions as direct documentation lookup queries.

Guidelines:
1. Convert problem descriptions to feature/capability questions
2. Expand abbreviations and acronyms to full terms
3. Replace casual language with technical terminology
4. Align with documentation vocabulary and structure
5. Keep the rewritten query concise (one line)

Examples:
- "Why can't I schedule workflows every 30 seconds?" → "workflow scheduling min...
[16:03:46] DEBUG | [anthropic] REQUEST: POST https://api.anthropic.com/v1/messages
[16:03:46] DEBUG | [anthropic] REQUEST body: model=claude-3-5-haiku-latest max_tokens=1024
[16:03:47] DEBUG | [anthropic] RESPONSE status=200 elapsed=0.94s
[16:03:47] DEBUG | [anthropic] USAGE: input_tokens=200 output_tokens=10
[16:03:47] DEBUG | [anthropic] SUCCESS stop_reason=end_turn response_len=59
[16:03:47] DEBUG | [anthropic] RESPONSE: workflow automatic error retry configuration policy options
[16:03:48] TRACE | === FACT COVERAGE ANALYSIS ===
[16:03:48] TRACE | Query: I want workflows to automatically retry on errors, what are the options?
[16:03:48] TRACE | Total key facts: 4
[16:03:48] TRACE | Found facts: 0/4
[16:03:48] TRACE | Missed facts: 4/4
[16:03:48] TRACE |   MISSED[1] max_attempts: 3
[16:03:48] TRACE |     -> Found in chunk_id=user_guide_fix_5 (rank not in top-5)
[16:03:48] TRACE |   MISSED[2] backoff_type: exponential
[16:03:48] TRACE |     -> Found in chunk_id=user_guide_fix_7 (rank not in top-5)
[16:03:48] TRACE |   MISSED[3] initial_interval: 1000
[16:03:48] TRACE |     -> Found in chunk_id=user_guide_fix_5 (rank not in top-5)
[16:03:48] TRACE |   MISSED[4] max retries: 3
[16:03:48] TRACE |     -> Found in chunk_id=troubleshooting_guide_fix_5 (rank not in top-5)
[16:03:48] TRACE | === END FACT COVERAGE ===
[16:03:48] DEBUG | [anthropic] Model alias: claude-haiku -> claude-3-5-haiku-latest
[16:03:48] DEBUG | [anthropic] model=claude-3-5-haiku-latest prompt_len=843 timeout=5s
[16:03:48] DEBUG | [anthropic] PROMPT: You are a technical documentation search expert. Your task is to rewrite user questions as direct documentation lookup queries.

Guidelines:
1. Convert problem descriptions to feature/capability questions
2. Expand abbreviations and acronyms to full terms
3. Replace casual language with technical terminology
4. Align with documentation vocabulary and structure
5. Keep the rewritten query concise (one line)

Examples:
- "Why can't I schedule workflows every 30 seconds?" → "workflow scheduling min...
[16:03:48] DEBUG | [anthropic] REQUEST: POST https://api.anthropic.com/v1/messages
[16:03:48] DEBUG | [anthropic] REQUEST body: model=claude-3-5-haiku-latest max_tokens=1024
[16:03:49] DEBUG | [anthropic] RESPONSE status=200 elapsed=1.49s
[16:03:49] DEBUG | [anthropic] USAGE: input_tokens=195 output_tokens=11
[16:03:49] DEBUG | [anthropic] SUCCESS stop_reason=end_turn response_len=67
[16:03:49] DEBUG | [anthropic] RESPONSE: workflow retry mechanism failure conditions error handling policies
[16:03:50] TRACE | === FACT COVERAGE ANALYSIS ===
[16:03:50] TRACE | Query: Why doesn't my workflow retry after failing?
[16:03:50] TRACE | Total key facts: 4
[16:03:50] TRACE | Found facts: 0/4
[16:03:50] TRACE | Missed facts: 4/4
[16:03:50] TRACE |   MISSED[1] max_attempts: 3
[16:03:50] TRACE |     -> Found in chunk_id=user_guide_fix_5 (rank not in top-5)
[16:03:50] TRACE |   MISSED[2] backoff_type: exponential
[16:03:50] TRACE |     -> Found in chunk_id=user_guide_fix_7 (rank not in top-5)
[16:03:50] TRACE |   MISSED[3] initial_interval: 1000
[16:03:50] TRACE |     -> Found in chunk_id=user_guide_fix_5 (rank not in top-5)
[16:03:50] TRACE |   MISSED[4] max retries: 3
[16:03:50] TRACE |     -> Found in chunk_id=troubleshooting_guide_fix_5 (rank not in top-5)
[16:03:50] TRACE | === END FACT COVERAGE ===
[16:03:50] DEBUG | [anthropic] Model alias: claude-haiku -> claude-3-5-haiku-latest
[16:03:50] DEBUG | [anthropic] model=claude-3-5-haiku-latest prompt_len=854 timeout=5s
[16:03:50] DEBUG | [anthropic] PROMPT: You are a technical documentation search expert. Your task is to rewrite user questions as direct documentation lookup queries.

Guidelines:
1. Convert problem descriptions to feature/capability questions
2. Expand abbreviations and acronyms to full terms
3. Replace casual language with technical terminology
4. Align with documentation vocabulary and structure
5. Keep the rewritten query concise (one line)

Examples:
- "Why can't I schedule workflows every 30 seconds?" → "workflow scheduling min...
[16:03:50] DEBUG | [anthropic] REQUEST: POST https://api.anthropic.com/v1/messages
[16:03:50] DEBUG | [anthropic] REQUEST body: model=claude-3-5-haiku-latest max_tokens=1024
[16:03:51] DEBUG | [anthropic] RESPONSE status=200 elapsed=1.00s
[16:03:51] DEBUG | [anthropic] USAGE: input_tokens=197 output_tokens=10
[16:03:51] DEBUG | [anthropic] SUCCESS stop_reason=end_turn response_len=60
[16:03:51] DEBUG | [anthropic] RESPONSE: helm chart repository official kubernetes deployment sources
[16:03:51] TRACE | === FACT COVERAGE ANALYSIS ===
[16:03:51] TRACE | Query: What Helm chart repository should I use for deployment?
[16:03:51] TRACE | Total key facts: 1
[16:03:51] TRACE | Found facts: 0/1
[16:03:51] TRACE | Missed facts: 1/1
[16:03:51] TRACE |   MISSED[1] helm repo add cloudflow https://charts.cloudflow.io
[16:03:51] TRACE |     -> Found in chunk_id=deployment_guide_fix_1 (rank not in top-5)
[16:03:51] TRACE | === END FACT COVERAGE ===
[16:03:51] DEBUG | [anthropic] Model alias: claude-haiku -> claude-3-5-haiku-latest
[16:03:51] DEBUG | [anthropic] model=claude-3-5-haiku-latest prompt_len=840 timeout=5s
[16:03:51] DEBUG | [anthropic] PROMPT: You are a technical documentation search expert. Your task is to rewrite user questions as direct documentation lookup queries.

Guidelines:
1. Convert problem descriptions to feature/capability questions
2. Expand abbreviations and acronyms to full terms
3. Replace casual language with technical terminology
4. Align with documentation vocabulary and structure
5. Keep the rewritten query concise (one line)

Examples:
- "Why can't I schedule workflows every 30 seconds?" → "workflow scheduling min...
[16:03:51] DEBUG | [anthropic] REQUEST: POST https://api.anthropic.com/v1/messages
[16:03:51] DEBUG | [anthropic] REQUEST body: model=claude-3-5-haiku-latest max_tokens=1024
[16:03:52] DEBUG | [anthropic] RESPONSE status=200 elapsed=0.87s
[16:03:52] DEBUG | [anthropic] USAGE: input_tokens=196 output_tokens=13
[16:03:52] DEBUG | [anthropic] SUCCESS stop_reason=end_turn response_len=49
[16:03:52] DEBUG | [anthropic] RESPONSE: "CloudFlow Helm chart repository source location"
[16:03:52] TRACE | === FACT COVERAGE ANALYSIS ===
[16:03:52] TRACE | Query: Where is the CloudFlow Helm chart hosted?
[16:03:52] TRACE | Total key facts: 1
[16:03:52] TRACE | Found facts: 0/1
[16:03:52] TRACE | Missed facts: 1/1
[16:03:52] TRACE |   MISSED[1] helm repo add cloudflow https://charts.cloudflow.io
[16:03:52] TRACE |     -> Found in chunk_id=deployment_guide_fix_1 (rank not in top-5)
[16:03:52] TRACE | === END FACT COVERAGE ===
[16:03:52] DEBUG | [anthropic] Model alias: claude-haiku -> claude-3-5-haiku-latest
[16:03:52] DEBUG | [anthropic] model=claude-3-5-haiku-latest prompt_len=848 timeout=5s
[16:03:52] DEBUG | [anthropic] PROMPT: You are a technical documentation search expert. Your task is to rewrite user questions as direct documentation lookup queries.

Guidelines:
1. Convert problem descriptions to feature/capability questions
2. Expand abbreviations and acronyms to full terms
3. Replace casual language with technical terminology
4. Align with documentation vocabulary and structure
5. Keep the rewritten query concise (one line)

Examples:
- "Why can't I schedule workflows every 30 seconds?" → "workflow scheduling min...
[16:03:52] DEBUG | [anthropic] REQUEST: POST https://api.anthropic.com/v1/messages
[16:03:52] DEBUG | [anthropic] REQUEST body: model=claude-3-5-haiku-latest max_tokens=1024
[16:03:54] DEBUG | [anthropic] RESPONSE status=200 elapsed=1.43s
[16:03:54] DEBUG | [anthropic] USAGE: input_tokens=198 output_tokens=11
[16:03:54] DEBUG | [anthropic] SUCCESS stop_reason=end_turn response_len=71
[16:03:54] DEBUG | [anthropic] RESPONSE: helm repository remote source url validation configuration requirements
[16:03:54] TRACE | === FACT COVERAGE ANALYSIS ===
[16:03:54] TRACE | Query: helm repo add is failing, what's the correct URL?
[16:03:54] TRACE | Total key facts: 1
[16:03:54] TRACE | Found facts: 1/1
[16:03:54] TRACE |   FOUND[1] helm repo add cloudflow https://charts.cloudflow.io
[16:03:54] TRACE | === END FACT COVERAGE ===
[16:03:54] DEBUG | [anthropic] Model alias: claude-haiku -> claude-3-5-haiku-latest
[16:03:54] DEBUG | [anthropic] model=claude-3-5-haiku-latest prompt_len=808 timeout=5s
[16:03:54] DEBUG | [anthropic] PROMPT: You are a technical documentation search expert. Your task is to rewrite user questions as direct documentation lookup queries.

Guidelines:
1. Convert problem descriptions to feature/capability questions
2. Expand abbreviations and acronyms to full terms
3. Replace casual language with technical terminology
4. Align with documentation vocabulary and structure
5. Keep the rewritten query concise (one line)

Examples:
- "Why can't I schedule workflows every 30 seconds?" → "workflow scheduling min...
[16:03:54] DEBUG | [anthropic] REQUEST: POST https://api.anthropic.com/v1/messages
[16:03:54] DEBUG | [anthropic] REQUEST body: model=claude-3-5-haiku-latest max_tokens=1024
[16:03:55] DEBUG | [anthropic] RESPONSE status=200 elapsed=0.91s
[16:03:55] DEBUG | [anthropic] USAGE: input_tokens=188 output_tokens=8
[16:03:55] DEBUG | [anthropic] SUCCESS stop_reason=end_turn response_len=49
[16:03:55] DEBUG | [anthropic] RESPONSE: helm repository management commands configuration
[16:03:55] TRACE | === FACT COVERAGE ANALYSIS ===
[16:03:55] TRACE | Query: helm repo
[16:03:55] TRACE | Total key facts: 1
[16:03:55] TRACE | Found facts: 1/1
[16:03:55] TRACE |   FOUND[1] helm repo add cloudflow https://charts.cloudflow.io
[16:03:55] TRACE | === END FACT COVERAGE ===
[16:03:55] DEBUG | [anthropic] Model alias: claude-haiku -> claude-3-5-haiku-latest
[16:03:55] DEBUG | [anthropic] model=claude-3-5-haiku-latest prompt_len=874 timeout=5s
[16:03:55] DEBUG | [anthropic] PROMPT: You are a technical documentation search expert. Your task is to rewrite user questions as direct documentation lookup queries.

Guidelines:
1. Convert problem descriptions to feature/capability questions
2. Expand abbreviations and acronyms to full terms
3. Replace casual language with technical terminology
4. Align with documentation vocabulary and structure
5. Keep the rewritten query concise (one line)

Examples:
- "Why can't I schedule workflows every 30 seconds?" → "workflow scheduling min...
[16:03:55] DEBUG | [anthropic] REQUEST: POST https://api.anthropic.com/v1/messages
[16:03:55] DEBUG | [anthropic] REQUEST body: model=claude-3-5-haiku-latest max_tokens=1024
[16:03:56] DEBUG | [anthropic] RESPONSE status=200 elapsed=0.93s
[16:03:56] DEBUG | [anthropic] USAGE: input_tokens=200 output_tokens=11
[16:03:56] DEBUG | [anthropic] SUCCESS stop_reason=end_turn response_len=65
[16:03:56] DEBUG | [anthropic] RESPONSE: cloudflow helm chart repository configuration deployment pipeline
[16:03:57] TRACE | === FACT COVERAGE ANALYSIS ===
[16:03:57] TRACE | Query: Setting up deployment pipeline, which Helm repository has CloudFlow charts?
[16:03:57] TRACE | Total key facts: 1
[16:03:57] TRACE | Found facts: 0/1
[16:03:57] TRACE | Missed facts: 1/1
[16:03:57] TRACE |   MISSED[1] helm repo add cloudflow https://charts.cloudflow.io
[16:03:57] TRACE |     -> Found in chunk_id=deployment_guide_fix_1 (rank not in top-5)
[16:03:57] TRACE | === END FACT COVERAGE ===
[16:03:57] DEBUG | [anthropic] Model alias: claude-haiku -> claude-3-5-haiku-latest
[16:03:57] DEBUG | [anthropic] model=claude-3-5-haiku-latest prompt_len=851 timeout=5s
[16:03:57] DEBUG | [anthropic] PROMPT: You are a technical documentation search expert. Your task is to rewrite user questions as direct documentation lookup queries.

Guidelines:
1. Convert problem descriptions to feature/capability questions
2. Expand abbreviations and acronyms to full terms
3. Replace casual language with technical terminology
4. Align with documentation vocabulary and structure
5. Keep the rewritten query concise (one line)

Examples:
- "Why can't I schedule workflows every 30 seconds?" → "workflow scheduling min...
[16:03:57] DEBUG | [anthropic] REQUEST: POST https://api.anthropic.com/v1/messages
[16:03:57] DEBUG | [anthropic] REQUEST body: model=claude-3-5-haiku-latest max_tokens=1024
[16:03:58] DEBUG | [anthropic] RESPONSE status=200 elapsed=0.97s
[16:03:58] DEBUG | [anthropic] USAGE: input_tokens=200 output_tokens=12
[16:03:58] DEBUG | [anthropic] SUCCESS stop_reason=end_turn response_len=73
[16:03:58] DEBUG | [anthropic] RESPONSE: cloudflow kubernetes helm chart repository availability package discovery
[16:03:58] TRACE | === FACT COVERAGE ANALYSIS ===
[16:03:58] TRACE | Query: Why can't I find CloudFlow in the official Helm hub?
[16:03:58] TRACE | Total key facts: 1
[16:03:58] TRACE | Found facts: 1/1
[16:03:58] TRACE |   FOUND[1] helm repo add cloudflow https://charts.cloudflow.io
[16:03:58] TRACE | === END FACT COVERAGE ===
[16:03:58] DEBUG | [anthropic] Model alias: claude-haiku -> claude-3-5-haiku-latest
[16:03:58] DEBUG | [anthropic] model=claude-3-5-haiku-latest prompt_len=853 timeout=5s
[16:03:58] DEBUG | [anthropic] PROMPT: You are a technical documentation search expert. Your task is to rewrite user questions as direct documentation lookup queries.

Guidelines:
1. Convert problem descriptions to feature/capability questions
2. Expand abbreviations and acronyms to full terms
3. Replace casual language with technical terminology
4. Align with documentation vocabulary and structure
5. Keep the rewritten query concise (one line)

Examples:
- "Why can't I schedule workflows every 30 seconds?" → "workflow scheduling min...
[16:03:58] DEBUG | [anthropic] REQUEST: POST https://api.anthropic.com/v1/messages
[16:03:58] DEBUG | [anthropic] REQUEST body: model=claude-3-5-haiku-latest max_tokens=1024
[16:03:59] DEBUG | [anthropic] RESPONSE status=200 elapsed=0.70s
[16:03:59] DEBUG | [anthropic] USAGE: input_tokens=195 output_tokens=9
[16:03:59] DEBUG | [anthropic] SUCCESS stop_reason=end_turn response_len=57
[16:03:59] DEBUG | [anthropic] RESPONSE: workflow scheduling minimum interval configuration limits
[16:03:59] TRACE | === FACT COVERAGE ANALYSIS ===
[16:03:59] TRACE | Query: What is the minimum scheduling interval for workflows?
[16:03:59] TRACE | Total key facts: 2
[16:03:59] TRACE | Found facts: 0/2
[16:03:59] TRACE | Missed facts: 2/2
[16:03:59] TRACE |   MISSED[1] minimum scheduling interval is 1 minute
[16:03:59] TRACE |   MISSED[2] The minimum scheduling interval is **1 minute**
[16:03:59] TRACE |     -> Found in chunk_id=user_guide_fix_4 (rank not in top-5)
[16:03:59] TRACE | === END FACT COVERAGE ===
[16:03:59] DEBUG | [anthropic] Model alias: claude-haiku -> claude-3-5-haiku-latest
[16:03:59] DEBUG | [anthropic] model=claude-3-5-haiku-latest prompt_len=840 timeout=5s
[16:03:59] DEBUG | [anthropic] PROMPT: You are a technical documentation search expert. Your task is to rewrite user questions as direct documentation lookup queries.

Guidelines:
1. Convert problem descriptions to feature/capability questions
2. Expand abbreviations and acronyms to full terms
3. Replace casual language with technical terminology
4. Align with documentation vocabulary and structure
5. Keep the rewritten query concise (one line)

Examples:
- "Why can't I schedule workflows every 30 seconds?" → "workflow scheduling min...
[16:03:59] DEBUG | [anthropic] REQUEST: POST https://api.anthropic.com/v1/messages
[16:03:59] DEBUG | [anthropic] REQUEST body: model=claude-3-5-haiku-latest max_tokens=1024
[16:04:00] DEBUG | [anthropic] RESPONSE status=200 elapsed=0.85s
[16:04:00] DEBUG | [anthropic] USAGE: input_tokens=194 output_tokens=9
[16:04:00] DEBUG | [anthropic] SUCCESS stop_reason=end_turn response_len=59
[16:04:00] DEBUG | [anthropic] RESPONSE: workflow scheduling interval frequency limits configuration
[16:04:00] TRACE | === FACT COVERAGE ANALYSIS ===
[16:04:00] TRACE | Query: How frequently can I schedule a workflow?
[16:04:00] TRACE | Total key facts: 2
[16:04:00] TRACE | Found facts: 0/2
[16:04:00] TRACE | Missed facts: 2/2
[16:04:00] TRACE |   MISSED[1] minimum scheduling interval is 1 minute
[16:04:00] TRACE |   MISSED[2] The minimum scheduling interval is **1 minute**
[16:04:00] TRACE |     -> Found in chunk_id=user_guide_fix_4 (rank not in top-5)
[16:04:00] TRACE | === END FACT COVERAGE ===
[16:04:00] DEBUG | [anthropic] Model alias: claude-haiku -> claude-3-5-haiku-latest
[16:04:00] DEBUG | [anthropic] model=claude-3-5-haiku-latest prompt_len=845 timeout=5s
[16:04:00] DEBUG | [anthropic] PROMPT: You are a technical documentation search expert. Your task is to rewrite user questions as direct documentation lookup queries.

Guidelines:
1. Convert problem descriptions to feature/capability questions
2. Expand abbreviations and acronyms to full terms
3. Replace casual language with technical terminology
4. Align with documentation vocabulary and structure
5. Keep the rewritten query concise (one line)

Examples:
- "Why can't I schedule workflows every 30 seconds?" → "workflow scheduling min...
[16:04:00] DEBUG | [anthropic] REQUEST: POST https://api.anthropic.com/v1/messages
[16:04:00] DEBUG | [anthropic] REQUEST body: model=claude-3-5-haiku-latest max_tokens=1024
[16:04:01] DEBUG | [anthropic] RESPONSE status=200 elapsed=0.90s
[16:04:01] DEBUG | [anthropic] USAGE: input_tokens=196 output_tokens=11
[16:04:01] DEBUG | [anthropic] SUCCESS stop_reason=end_turn response_len=72
[16:04:01] DEBUG | [anthropic] RESPONSE: workflow scheduling minimum interval frequency constraints policy limits
[16:04:02] TRACE | === FACT COVERAGE ANALYSIS ===
[16:04:02] TRACE | Query: My every-30-seconds schedule is being rejected
[16:04:02] TRACE | Total key facts: 2
[16:04:02] TRACE | Found facts: 0/2
[16:04:02] TRACE | Missed facts: 2/2
[16:04:02] TRACE |   MISSED[1] minimum scheduling interval is 1 minute
[16:04:02] TRACE |   MISSED[2] The minimum scheduling interval is **1 minute**
[16:04:02] TRACE |     -> Found in chunk_id=user_guide_fix_4 (rank not in top-5)
[16:04:02] TRACE | === END FACT COVERAGE ===
[16:04:02] DEBUG | [anthropic] Model alias: claude-haiku -> claude-3-5-haiku-latest
[16:04:02] DEBUG | [anthropic] model=claude-3-5-haiku-latest prompt_len=820 timeout=5s
[16:04:02] DEBUG | [anthropic] PROMPT: You are a technical documentation search expert. Your task is to rewrite user questions as direct documentation lookup queries.

Guidelines:
1. Convert problem descriptions to feature/capability questions
2. Expand abbreviations and acronyms to full terms
3. Replace casual language with technical terminology
4. Align with documentation vocabulary and structure
5. Keep the rewritten query concise (one line)

Examples:
- "Why can't I schedule workflows every 30 seconds?" → "workflow scheduling min...
[16:04:02] DEBUG | [anthropic] REQUEST: POST https://api.anthropic.com/v1/messages
[16:04:02] DEBUG | [anthropic] REQUEST body: model=claude-3-5-haiku-latest max_tokens=1024
[16:04:03] DEBUG | [anthropic] RESPONSE status=200 elapsed=0.85s
[16:04:03] DEBUG | [anthropic] USAGE: input_tokens=189 output_tokens=8
[16:04:03] DEBUG | [anthropic] SUCCESS stop_reason=end_turn response_len=50
[16:04:03] DEBUG | [anthropic] RESPONSE: workflow scheduling minimum interval configuration
[16:04:03] TRACE | === FACT COVERAGE ANALYSIS ===
[16:04:03] TRACE | Query: min schedule interval
[16:04:03] TRACE | Total key facts: 2
[16:04:03] TRACE | Found facts: 0/2
[16:04:03] TRACE | Missed facts: 2/2
[16:04:03] TRACE |   MISSED[1] minimum scheduling interval is 1 minute
[16:04:03] TRACE |   MISSED[2] The minimum scheduling interval is **1 minute**
[16:04:03] TRACE |     -> Found in chunk_id=user_guide_fix_4 (rank not in top-5)
[16:04:03] TRACE | === END FACT COVERAGE ===
[16:04:03] DEBUG | [anthropic] Model alias: claude-haiku -> claude-3-5-haiku-latest
[16:04:03] DEBUG | [anthropic] model=claude-3-5-haiku-latest prompt_len=870 timeout=5s
[16:04:03] DEBUG | [anthropic] PROMPT: You are a technical documentation search expert. Your task is to rewrite user questions as direct documentation lookup queries.

Guidelines:
1. Convert problem descriptions to feature/capability questions
2. Expand abbreviations and acronyms to full terms
3. Replace casual language with technical terminology
4. Align with documentation vocabulary and structure
5. Keep the rewritten query concise (one line)

Examples:
- "Why can't I schedule workflows every 30 seconds?" → "workflow scheduling min...
[16:04:03] DEBUG | [anthropic] REQUEST: POST https://api.anthropic.com/v1/messages
[16:04:03] DEBUG | [anthropic] REQUEST body: model=claude-3-5-haiku-latest max_tokens=1024
[16:04:04] DEBUG | [anthropic] RESPONSE status=200 elapsed=0.89s
[16:04:04] DEBUG | [anthropic] USAGE: input_tokens=203 output_tokens=12
[16:04:04] DEBUG | [anthropic] SUCCESS stop_reason=end_turn response_len=74
[16:04:04] DEBUG | [anthropic] RESPONSE: workflow scheduling minimum execution interval realtime performance limits
[16:04:04] TRACE | === FACT COVERAGE ANALYSIS ===
[16:04:04] TRACE | Query: I need near real-time execution, what's the fastest schedule I can set?
[16:04:04] TRACE | Total key facts: 2
[16:04:04] TRACE | Found facts: 0/2
[16:04:04] TRACE | Missed facts: 2/2
[16:04:04] TRACE |   MISSED[1] minimum scheduling interval is 1 minute
[16:04:04] TRACE |   MISSED[2] The minimum scheduling interval is **1 minute**
[16:04:04] TRACE |     -> Found in chunk_id=user_guide_fix_4 (rank not in top-5)
[16:04:04] TRACE | === END FACT COVERAGE ===
[16:04:04] DEBUG | [anthropic] Model alias: claude-haiku -> claude-3-5-haiku-latest
[16:04:04] DEBUG | [anthropic] model=claude-3-5-haiku-latest prompt_len=847 timeout=5s
[16:04:04] DEBUG | [anthropic] PROMPT: You are a technical documentation search expert. Your task is to rewrite user questions as direct documentation lookup queries.

Guidelines:
1. Convert problem descriptions to feature/capability questions
2. Expand abbreviations and acronyms to full terms
3. Replace casual language with technical terminology
4. Align with documentation vocabulary and structure
5. Keep the rewritten query concise (one line)

Examples:
- "Why can't I schedule workflows every 30 seconds?" → "workflow scheduling min...
[16:04:04] DEBUG | [anthropic] REQUEST: POST https://api.anthropic.com/v1/messages
[16:04:04] DEBUG | [anthropic] REQUEST body: model=claude-3-5-haiku-latest max_tokens=1024
[16:04:05] DEBUG | [anthropic] RESPONSE status=200 elapsed=0.73s
[16:04:05] DEBUG | [anthropic] USAGE: input_tokens=198 output_tokens=9
[16:04:05] DEBUG | [anthropic] SUCCESS stop_reason=end_turn response_len=58
[16:04:05] DEBUG | [anthropic] RESPONSE: workflow scheduling minimum interval frequency constraints
[16:04:05] TRACE | === FACT COVERAGE ANALYSIS ===
[16:04:05] TRACE | Query: Why can't I schedule workflows every 30 seconds?
[16:04:05] TRACE | Total key facts: 2
[16:04:05] TRACE | Found facts: 0/2
[16:04:05] TRACE | Missed facts: 2/2
[16:04:05] TRACE |   MISSED[1] minimum scheduling interval is 1 minute
[16:04:05] TRACE |   MISSED[2] The minimum scheduling interval is **1 minute**
[16:04:05] TRACE |     -> Found in chunk_id=user_guide_fix_4 (rank not in top-5)
[16:04:05] TRACE | === END FACT COVERAGE ===
[16:04:05] INFO  |   k=5 exact_match: 56.6% (30/53)
[16:04:05] INFO  |     synonym: 41.5%
[16:04:05] INFO  |     problem: 60.4%
[16:04:05] INFO  |     casual: 47.2%
[16:04:05] INFO  |     contextual: 45.3%
[16:04:05] INFO  |     negation: 45.3%

================================================================================
[16:04:05] INFO  | RETRIEVAL STRATEGY: bmx_semantic
================================================================================
[16:04:06] INFO  | Loading embedder: BAAI/bge-base-en-v1.5
[16:04:08] PROG  | [  3/  4] ███████████████░░░░░  75.0% | bmx_semantic_bge-base-en-v1.5_fixed_512_0pct
[16:04:08] DEBUG | Chunking document 'api_reference': 1792 words, 16663 chars
[16:04:08] DEBUG |   Chunk 0: chars [0-3160] words=376 tokens≈500 preview='# CloudFlow API Reference Version 2.1.0 | Last Updated: January 2026 ## Overview The CloudFlow API i...'
[16:04:08] DEBUG |   Chunk 1: chars [3161-6111] words=379 tokens≈504 preview='## Rate Limiting To ensure fair usage and system stability, CloudFlow enforces rate limits on all AP...'
[16:04:08] DEBUG |   Chunk 2: chars [6112-8827] words=296 tokens≈393 preview='**Endpoint:** `POST /workflows` **Request Body:** ```json { "name": "Email Campaign Automation", "de...'
[16:04:08] DEBUG |   Chunk 3: chars [8828-11402] words=269 tokens≈357 preview='**Endpoint:** `GET /pipelines/{pipeline_id}/executions` **Query Parameters:** - `status` (optional):...'
[16:04:08] DEBUG |   Chunk 4: chars [11403-14151] words=365 tokens≈485 preview='### Error Response Format ```json { "error": { "code": "invalid_parameter", "message": "The 'limit' ...'
[16:04:08] DEBUG |   Chunk 5: chars [14152-15213] words=107 tokens≈142 preview='**Supported Events:** - `workflow.started` - `workflow.completed` - `workflow.failed` - `pipeline.co...'
[16:04:08] DEBUG | Created 6 chunks for document 'api_reference'
[16:04:08] DEBUG | Chunking document 'architecture_overview': 4607 words, 36722 chars
[16:04:08] DEBUG |   Chunk 0: chars [0-3230] words=370 tokens≈492 preview='# CloudFlow Platform - System Architecture Overview **Document Version:** 2.3.1 **Last Updated:** Ja...'
[16:04:08] DEBUG |   Chunk 1: chars [3231-5982] words=380 tokens≈505 preview='**Technology**: Node.js with Express.js framework **Replicas**: 12 pods (production), auto-scaling 8...'
[16:04:08] DEBUG |   Chunk 2: chars [5983-7875] words=248 tokens≈329 preview='**Technology**: Node.js with TypeScript, Bull queue library **Replicas**: 16 pods (production), auto...'
[16:04:08] DEBUG |   Chunk 3: chars [7876-9478] words=219 tokens≈291 preview='**Technology**: Go with distributed locking via Redis **Replicas**: 4 pods (production), active-pass...'
[16:04:08] DEBUG |   Chunk 4: chars [9479-11878] words=354 tokens≈470 preview='**Technology**: Node.js with worker pool pattern **Replicas**: 8 pods (production), auto-scaling 6-1...'
[16:04:08] DEBUG |   Chunk 5: chars [11879-14634] words=368 tokens≈489 preview='Success/failure events published back to Kafka ``` **Event Schema**: ```json { "event_id": "uuid-v4"...'
[16:04:08] DEBUG |   Chunk 6: chars [14635-17394] words=384 tokens≈510 preview='**Session Storage**: - Key pattern: `session:{user_id}` - TTL: 15 minutes (aligned with JWT expiry) ...'
[16:04:08] DEBUG |   Chunk 7: chars [17395-20141] words=381 tokens≈506 preview='records: 500 - Session timeout: 30 seconds --- ## Message Queue Patterns ### Pub/Sub Pattern Used fo...'
[16:04:08] DEBUG |   Chunk 8: chars [20142-22843] words=384 tokens≈510 preview='### Cache Invalidation Strategies **Time-Based Expiration (TTL)**: - Short-lived data: 5-15 minutes ...'
[16:04:08] DEBUG |   Chunk 9: chars [22844-25362] words=384 tokens≈510 preview='per API key (default: 1000 RPM) ### Secrets Management **HashiCorp Vault Integration**: - Dynamic da...'
[16:04:08] DEBUG |   Chunk 10: chars [25363-27842] words=384 tokens≈510 preview='- Execution start rate: 500 per second - Completion rate: 450 per second (average 2-second execution...'
[16:04:08] DEBUG |   Chunk 11: chars [27843-30286] words=375 tokens≈498 preview='Service (2) - PostgreSQL Primary - PostgreSQL Replica - PostgreSQL Replica - Redis Primary (2) - Red...'
[16:04:08] DEBUG |   Chunk 12: chars [30287-32834] words=376 tokens≈500 preview='Resume normal operations - Recovery time: 1-3 hours depending on data volume - Data loss: None if co...'
[16:04:08] DEBUG | Created 13 chunks for document 'architecture_overview'
[16:04:08] DEBUG | Chunking document 'deployment_guide': 2777 words, 25837 chars
[16:04:08] DEBUG |   Chunk 0: chars [0-3165] words=384 tokens≈510 preview='# CloudFlow Platform - Deployment and Operations Guide **Version:** 2.4.0 **Last Updated:** January ...'
[16:04:08] DEBUG |   Chunk 1: chars [3166-6560] words=384 tokens≈510 preview='create cluster -f cluster-config.yaml ``` ### Storage Configuration Install the EBS CSI driver for p...'
[16:04:08] DEBUG |   Chunk 2: chars [6561-8755] words=251 tokens≈333 preview='- secretName: cloudflow-tls hosts: - api.cloudflow.io healthCheck: enabled: true livenessProbe: path...'
[16:04:08] DEBUG |   Chunk 3: chars [8756-12138] words=384 tokens≈510 preview='Deploy PostgreSQL using the Bitnami Helm chart: ```yaml # postgres-values.yaml global: postgresql: a...'
[16:04:08] DEBUG |   Chunk 4: chars [12139-14139] words=204 tokens≈271 preview='--set prometheus.prometheusSpec.retention=30d \ --set prometheus.prometheusSpec.storageSpec.volumeCl...'
[16:04:08] DEBUG |   Chunk 5: chars [14140-17194] words=372 tokens≈494 preview='Configure log aggregation: ```bash # Install Fluent Bit for log forwarding helm install fluent-bit f...'
[16:04:08] DEBUG |   Chunk 6: chars [17195-20069] words=384 tokens≈510 preview='**Verify Service Health**: ```bash kubectl get pods -n cloudflow-prod curl https://api.cloudflow.io/...'
[16:04:08] DEBUG |   Chunk 7: chars [20070-23221] words=384 tokens≈510 preview='podSelector: matchLabels: app: cloudflow policyTypes: - Ingress - Egress ingress: - from: - namespac...'
[16:04:08] DEBUG |   Chunk 8: chars [23222-23448] words=30 tokens≈39 preview='UTC - **Secondary**: Wednesday 10:00-12:00 UTC All deployments should target these windows to minimi...'
[16:04:08] DEBUG | Created 9 chunks for document 'deployment_guide'
[16:04:08] DEBUG | Chunking document 'troubleshooting_guide': 4032 words, 32183 chars
[16:04:08] DEBUG |   Chunk 0: chars [0-2666] words=320 tokens≈425 preview='# CloudFlow Platform Troubleshooting Guide **Version:** 3.2.1 **Last Updated:** January 2026 **Audie...'
[16:04:08] DEBUG |   Chunk 1: chars [2667-5007] words=321 tokens≈426 preview='**Diagnosis:** ```bash # Check system time timedatectl status # Compare with NTP server ntpdate -q p...'
[16:04:08] DEBUG |   Chunk 2: chars [5008-8037] words=376 tokens≈500 preview='Review Query Execution Plans** ```sql -- Connect to CloudFlow database cloudflow db connect --readon...'
[16:04:08] DEBUG |   Chunk 3: chars [8038-10862] words=356 tokens≈473 preview='Workflow Context Accumulation** Large workflow executions may accumulate state in memory. **Solution...'
[16:04:08] DEBUG |   Chunk 4: chars [10863-13827] words=384 tokens≈510 preview='**Implement connection pooling optimization:** ```bash # Use PgBouncer for connection pooling kubect...'
[16:04:08] DEBUG |   Chunk 5: chars [13828-16401] words=334 tokens≈444 preview='exec_7h3j6k9m2n --show-bottlenecks ``` #### Solutions **1. Increase workflow timeout (if justified):...'
[16:04:08] DEBUG |   Chunk 6: chars [16402-19022] words=329 tokens≈437 preview='Data Validation Errors** ``` ValidationError: Field 'customer_id' is required but missing in 234 rec...'
[16:04:08] DEBUG |   Chunk 7: chars [19023-20595] words=194 tokens≈258 preview='Waiting {retry_after} seconds...") time.sleep(retry_after) continue remaining = int(response.headers...'
[16:04:08] DEBUG |   Chunk 8: chars [20596-23454] words=384 tokens≈510 preview='Leverage caching:** ```bash # Enable client-side caching export CLOUDFLOW_CACHE_ENABLED=true export ...'
[16:04:08] DEBUG |   Chunk 9: chars [23455-26239] words=384 tokens≈510 preview='in" # Find workflow failures by ID kubectl logs -n cloudflow deployment/cloudflow-workflow-engine | ...'
[16:04:08] DEBUG |   Chunk 10: chars [26240-29039] words=384 tokens≈510 preview='database nc -zv cloudflow-db.internal.company.com 5432 # Trace route traceroute api.cloudflow.io # C...'
[16:04:08] DEBUG |   Chunk 11: chars [29040-31206] words=266 tokens≈353 preview='outage" # Create war room cloudflow incident war-room create incident-2024012401 ``` **For SEV-2 (Hi...'
[16:04:08] DEBUG | Created 12 chunks for document 'troubleshooting_guide'
[16:04:08] DEBUG | Chunking document 'user_guide': 3735 words, 31582 chars
[16:04:08] DEBUG |   Chunk 0: chars [0-2617] words=375 tokens≈498 preview='# CloudFlow User Guide Welcome to CloudFlow, the modern workflow automation platform that helps you ...'
[16:04:08] DEBUG |   Chunk 1: chars [2618-5224] words=348 tokens≈462 preview='Open the Visual Editor by clicking **"Create Workflow"** or editing an existing workflow 2. Add a tr...'
[16:04:08] DEBUG |   Chunk 2: chars [5225-8372] words=381 tokens≈506 preview='### HTTP Requests Make HTTP requests to any API endpoint: **Configuration:** - **Method**: GET, POST...'
[16:04:08] DEBUG |   Chunk 3: chars [8373-11268] words=336 tokens≈446 preview='``` **Slack App Integration:** - Install the CloudFlow Slack app in your workspace - Authenticate on...'
[16:04:08] DEBUG |   Chunk 4: chars [11269-13371] words=375 tokens≈498 preview='### Cron Syntax Use standard cron expressions to define schedules: ``` * * * * * ┬ ┬ ┬ ┬ ┬ │ │ │ │ │...'
[16:04:08] DEBUG |   Chunk 5: chars [13372-16105] words=383 tokens≈509 preview='Save and activate **Testing Schedules:** Use the built-in schedule calculator to preview upcoming ex...'
[16:04:08] DEBUG |   Chunk 6: chars [16106-18686] words=363 tokens≈482 preview='Filter by error type, date range, or execution ID 3. Click an execution to view full details 4. Clic...'
[16:04:08] DEBUG |   Chunk 7: chars [18687-21448] words=364 tokens≈484 preview='Handle Errors Gracefully Always implement error handling for external API calls and database operati...'
[16:04:08] DEBUG |   Chunk 8: chars [21449-24677] words=384 tokens≈510 preview='Version Control For critical workflows: - Export YAML definitions regularly - Store in version contr...'
[16:04:08] DEBUG |   Chunk 9: chars [24678-27975] words=371 tokens≈493 preview='Requester: {{trigger.body.requester}} Description: {{trigger.body.description}} Approve: https://app...'
[16:04:08] DEBUG |   Chunk 10: chars [27976-28481] words=55 tokens≈73 preview='- **Documentation**: https://docs.cloudflow.io - **Community Forum**: https://community.cloudflow.io...'
[16:04:08] DEBUG | Created 11 chunks for document 'user_guide'
[16:04:08] INFO  |   Created 51 chunks
    [bmx-semantic] chunks=51 new=0 cache_hits=51 time=0.0s avg=0.0ms/chunk
[16:04:09] INFO  |   Indexed in 1.75s
[16:04:10] TRACE | === FACT COVERAGE ANALYSIS ===
[16:04:10] TRACE | Query: What is the API rate limit per minute?
[16:04:10] TRACE | Total key facts: 2
[16:04:10] TRACE | Found facts: 2/2
[16:04:10] TRACE |   FOUND[1] 100 requests per minute per authenticated user
[16:04:10] TRACE |   FOUND[2] 20 requests per minute for unauthenticated requests
[16:04:10] TRACE | === END FACT COVERAGE ===
[16:04:10] TRACE | === FACT COVERAGE ANALYSIS ===
[16:04:10] TRACE | Query: How many requests can I make per minute to the API?
[16:04:10] TRACE | Total key facts: 2
[16:04:10] TRACE | Found facts: 2/2
[16:04:10] TRACE |   FOUND[1] 100 requests per minute per authenticated user
[16:04:10] TRACE |   FOUND[2] 20 requests per minute for unauthenticated requests
[16:04:10] TRACE | === END FACT COVERAGE ===
[16:04:10] TRACE | === FACT COVERAGE ANALYSIS ===
[16:04:10] TRACE | Query: My API calls are getting blocked, what's the limit?
[16:04:10] TRACE | Total key facts: 2
[16:04:10] TRACE | Found facts: 2/2
[16:04:10] TRACE |   FOUND[1] 100 requests per minute per authenticated user
[16:04:10] TRACE |   FOUND[2] 20 requests per minute for unauthenticated requests
[16:04:10] TRACE | === END FACT COVERAGE ===
[16:04:10] TRACE | === FACT COVERAGE ANALYSIS ===
[16:04:10] TRACE | Query: api rate limit
[16:04:10] TRACE | Total key facts: 2
[16:04:10] TRACE | Found facts: 2/2
[16:04:10] TRACE |   FOUND[1] 100 requests per minute per authenticated user
[16:04:10] TRACE |   FOUND[2] 20 requests per minute for unauthenticated requests
[16:04:10] TRACE | === END FACT COVERAGE ===
[16:04:10] TRACE | === FACT COVERAGE ANALYSIS ===
[16:04:10] TRACE | Query: I'm building a batch job that calls CloudFlow API repeatedly, what rate limit should I expect?
[16:04:10] TRACE | Total key facts: 2
[16:04:10] TRACE | Found facts: 2/2
[16:04:10] TRACE |   FOUND[1] 100 requests per minute per authenticated user
[16:04:10] TRACE |   FOUND[2] 20 requests per minute for unauthenticated requests
[16:04:10] TRACE | === END FACT COVERAGE ===
[16:04:10] TRACE | === FACT COVERAGE ANALYSIS ===
[16:04:10] TRACE | Query: Why is the API rejecting my requests after 100 calls?
[16:04:10] TRACE | Total key facts: 2
[16:04:10] TRACE | Found facts: 2/2
[16:04:10] TRACE |   FOUND[1] 100 requests per minute per authenticated user
[16:04:10] TRACE |   FOUND[2] 20 requests per minute for unauthenticated requests
[16:04:10] TRACE | === END FACT COVERAGE ===
[16:04:10] TRACE | === FACT COVERAGE ANALYSIS ===
[16:04:10] TRACE | Query: How do I fix 429 Too Many Requests errors?
[16:04:10] TRACE | Total key facts: 5
[16:04:10] TRACE | Found facts: 3/5
[16:04:10] TRACE |   FOUND[1] 429 Too Many Requests
[16:04:10] TRACE |   FOUND[2] X-RateLimit-Remaining
[16:04:10] TRACE |   FOUND[3] Implement exponential backoff when receiving 429 responses
[16:04:10] TRACE | Missed facts: 2/5
[16:04:10] TRACE |   MISSED[1] Retry-After
[16:04:10] TRACE |     -> Found in chunk_id=troubleshooting_guide_fix_6 (rank not in top-5)
[16:04:10] TRACE |   MISSED[2] Monitor X-RateLimit-Remaining header values
[16:04:10] TRACE | === END FACT COVERAGE ===
[16:04:10] TRACE | === FACT COVERAGE ANALYSIS ===
[16:04:10] TRACE | Query: What should I do when I get rate limited?
[16:04:10] TRACE | Total key facts: 5
[16:04:10] TRACE | Found facts: 3/5
[16:04:10] TRACE |   FOUND[1] 429 Too Many Requests
[16:04:10] TRACE |   FOUND[2] X-RateLimit-Remaining
[16:04:10] TRACE |   FOUND[3] Implement exponential backoff when receiving 429 responses
[16:04:10] TRACE | Missed facts: 2/5
[16:04:10] TRACE |   MISSED[1] Retry-After
[16:04:10] TRACE |     -> Found in chunk_id=troubleshooting_guide_fix_6 (rank not in top-5)
[16:04:10] TRACE |   MISSED[2] Monitor X-RateLimit-Remaining header values
[16:04:10] TRACE | === END FACT COVERAGE ===
[16:04:10] TRACE | === FACT COVERAGE ANALYSIS ===
[16:04:10] TRACE | Query: My requests keep failing with 429 status code
[16:04:10] TRACE | Total key facts: 5
[16:04:10] TRACE | Found facts: 3/5
[16:04:10] TRACE |   FOUND[1] 429 Too Many Requests
[16:04:10] TRACE |   FOUND[2] X-RateLimit-Remaining
[16:04:10] TRACE |   FOUND[3] Implement exponential backoff when receiving 429 responses
[16:04:10] TRACE | Missed facts: 2/5
[16:04:10] TRACE |   MISSED[1] Retry-After
[16:04:10] TRACE |     -> Found in chunk_id=troubleshooting_guide_fix_6 (rank not in top-5)
[16:04:10] TRACE |   MISSED[2] Monitor X-RateLimit-Remaining header values
[16:04:10] TRACE | === END FACT COVERAGE ===
[16:04:10] TRACE | === FACT COVERAGE ANALYSIS ===
[16:04:10] TRACE | Query: 429 error fix
[16:04:10] TRACE | Total key facts: 5
[16:04:10] TRACE | Found facts: 3/5
[16:04:10] TRACE |   FOUND[1] 429 Too Many Requests
[16:04:10] TRACE |   FOUND[2] X-RateLimit-Remaining
[16:04:10] TRACE |   FOUND[3] Implement exponential backoff when receiving 429 responses
[16:04:10] TRACE | Missed facts: 2/5
[16:04:10] TRACE |   MISSED[1] Retry-After
[16:04:10] TRACE |     -> Found in chunk_id=troubleshooting_guide_fix_6 (rank not in top-5)
[16:04:10] TRACE |   MISSED[2] Monitor X-RateLimit-Remaining header values
[16:04:10] TRACE | === END FACT COVERAGE ===
[16:04:10] TRACE | === FACT COVERAGE ANALYSIS ===
[16:04:10] TRACE | Query: I'm getting throttled by CloudFlow API, how can I handle this in my application?
[16:04:10] TRACE | Total key facts: 5
[16:04:10] TRACE | Found facts: 3/5
[16:04:10] TRACE |   FOUND[1] 429 Too Many Requests
[16:04:10] TRACE |   FOUND[2] X-RateLimit-Remaining
[16:04:10] TRACE |   FOUND[3] Implement exponential backoff when receiving 429 responses
[16:04:10] TRACE | Missed facts: 2/5
[16:04:10] TRACE |   MISSED[1] Retry-After
[16:04:10] TRACE |     -> Found in chunk_id=troubleshooting_guide_fix_6 (rank not in top-5)
[16:04:10] TRACE |   MISSED[2] Monitor X-RateLimit-Remaining header values
[16:04:10] TRACE | === END FACT COVERAGE ===
[16:04:10] TRACE | === FACT COVERAGE ANALYSIS ===
[16:04:10] TRACE | Query: Why am I being blocked with rate limit errors?
[16:04:10] TRACE | Total key facts: 5
[16:04:10] TRACE | Found facts: 3/5
[16:04:10] TRACE |   FOUND[1] 429 Too Many Requests
[16:04:10] TRACE |   FOUND[2] X-RateLimit-Remaining
[16:04:10] TRACE |   FOUND[3] Implement exponential backoff when receiving 429 responses
[16:04:10] TRACE | Missed facts: 2/5
[16:04:10] TRACE |   MISSED[1] Retry-After
[16:04:10] TRACE |     -> Found in chunk_id=troubleshooting_guide_fix_6 (rank not in top-5)
[16:04:10] TRACE |   MISSED[2] Monitor X-RateLimit-Remaining header values
[16:04:10] TRACE | === END FACT COVERAGE ===
[16:04:10] TRACE | === FACT COVERAGE ANALYSIS ===
[16:04:10] TRACE | Query: What is the JWT token expiration time?
[16:04:10] TRACE | Total key facts: 3
[16:04:10] TRACE | Found facts: 2/3
[16:04:10] TRACE |   FOUND[1] 3600 seconds
[16:04:10] TRACE |   FOUND[2] All tokens expire after 3600 seconds
[16:04:10] TRACE | Missed facts: 1/3
[16:04:10] TRACE |   MISSED[1] max 3600 seconds from iat
[16:04:10] TRACE | === END FACT COVERAGE ===
[16:04:10] TRACE | === FACT COVERAGE ANALYSIS ===
[16:04:10] TRACE | Query: How long do access tokens last?
[16:04:10] TRACE | Total key facts: 3
[16:04:10] TRACE | Found facts: 2/3
[16:04:10] TRACE |   FOUND[1] 3600 seconds
[16:04:10] TRACE |   FOUND[2] All tokens expire after 3600 seconds
[16:04:10] TRACE | Missed facts: 1/3
[16:04:10] TRACE |   MISSED[1] max 3600 seconds from iat
[16:04:10] TRACE | === END FACT COVERAGE ===
[16:04:10] TRACE | === FACT COVERAGE ANALYSIS ===
[16:04:10] TRACE | Query: My authentication keeps expiring after an hour
[16:04:10] TRACE | Total key facts: 3
[16:04:10] TRACE | Found facts: 2/3
[16:04:10] TRACE |   FOUND[1] 3600 seconds
[16:04:10] TRACE |   FOUND[2] All tokens expire after 3600 seconds
[16:04:10] TRACE | Missed facts: 1/3
[16:04:10] TRACE |   MISSED[1] max 3600 seconds from iat
[16:04:10] TRACE | === END FACT COVERAGE ===
[16:04:10] TRACE | === FACT COVERAGE ANALYSIS ===
[16:04:10] TRACE | Query: token expiry time
[16:04:10] TRACE | Total key facts: 3
[16:04:10] TRACE | Found facts: 0/3
[16:04:10] TRACE | Missed facts: 3/3
[16:04:10] TRACE |   MISSED[1] 3600 seconds
[16:04:10] TRACE |     -> Found in chunk_id=api_reference_fix_0 (rank not in top-5)
[16:04:10] TRACE |   MISSED[2] max 3600 seconds from iat
[16:04:10] TRACE |   MISSED[3] All tokens expire after 3600 seconds
[16:04:10] TRACE |     -> Found in chunk_id=api_reference_fix_0 (rank not in top-5)
[16:04:10] TRACE | === END FACT COVERAGE ===
[16:04:10] TRACE | === FACT COVERAGE ANALYSIS ===
[16:04:10] TRACE | Query: I need to implement token refresh logic, when do JWT tokens expire?
[16:04:10] TRACE | Total key facts: 3
[16:04:10] TRACE | Found facts: 2/3
[16:04:10] TRACE |   FOUND[1] 3600 seconds
[16:04:10] TRACE |   FOUND[2] All tokens expire after 3600 seconds
[16:04:10] TRACE | Missed facts: 1/3
[16:04:10] TRACE |   MISSED[1] max 3600 seconds from iat
[16:04:10] TRACE | === END FACT COVERAGE ===
[16:04:10] TRACE | === FACT COVERAGE ANALYSIS ===
[16:04:10] TRACE | Query: Why does my token stop working after 3600 seconds?
[16:04:10] TRACE | Total key facts: 3
[16:04:10] TRACE | Found facts: 1/3
[16:04:10] TRACE |   FOUND[1] 3600 seconds
[16:04:10] TRACE | Missed facts: 2/3
[16:04:10] TRACE |   MISSED[1] max 3600 seconds from iat
[16:04:10] TRACE |   MISSED[2] All tokens expire after 3600 seconds
[16:04:10] TRACE |     -> Found in chunk_id=api_reference_fix_0 (rank not in top-5)
[16:04:10] TRACE | === END FACT COVERAGE ===
[16:04:10] TRACE | === FACT COVERAGE ANALYSIS ===
[16:04:10] TRACE | Query: What database technology does CloudFlow use?
[16:04:10] TRACE | Total key facts: 3
[16:04:10] TRACE | Found facts: 3/3
[16:04:10] TRACE |   FOUND[1] PostgreSQL 15.4
[16:04:10] TRACE |   FOUND[2] Redis 7.2
[16:04:10] TRACE |   FOUND[3] Apache Kafka 3.6
[16:04:10] TRACE | === END FACT COVERAGE ===
[16:04:10] TRACE | === FACT COVERAGE ANALYSIS ===
[16:04:10] TRACE | Query: Which database system powers CloudFlow?
[16:04:10] TRACE | Total key facts: 3
[16:04:10] TRACE | Found facts: 3/3
[16:04:10] TRACE |   FOUND[1] PostgreSQL 15.4
[16:04:10] TRACE |   FOUND[2] Redis 7.2
[16:04:10] TRACE |   FOUND[3] Apache Kafka 3.6
[16:04:10] TRACE | === END FACT COVERAGE ===
[16:04:10] TRACE | === FACT COVERAGE ANALYSIS ===
[16:04:10] TRACE | Query: I need to understand the data storage architecture
[16:04:10] TRACE | Total key facts: 3
[16:04:10] TRACE | Found facts: 3/3
[16:04:10] TRACE |   FOUND[1] PostgreSQL 15.4
[16:04:10] TRACE |   FOUND[2] Redis 7.2
[16:04:10] TRACE |   FOUND[3] Apache Kafka 3.6
[16:04:10] TRACE | === END FACT COVERAGE ===
[16:04:10] TRACE | === FACT COVERAGE ANALYSIS ===
[16:04:10] TRACE | Query: database stack
[16:04:10] TRACE | Total key facts: 3
[16:04:10] TRACE | Found facts: 0/3
[16:04:10] TRACE | Missed facts: 3/3
[16:04:10] TRACE |   MISSED[1] PostgreSQL 15.4
[16:04:10] TRACE |     -> Found in chunk_id=architecture_overview_fix_0 (rank not in top-5)
[16:04:10] TRACE |   MISSED[2] Redis 7.2
[16:04:10] TRACE |     -> Found in chunk_id=architecture_overview_fix_0 (rank not in top-5)
[16:04:10] TRACE |   MISSED[3] Apache Kafka 3.6
[16:04:10] TRACE |     -> Found in chunk_id=architecture_overview_fix_0 (rank not in top-5)
[16:04:10] TRACE | === END FACT COVERAGE ===
[16:04:10] TRACE | === FACT COVERAGE ANALYSIS ===
[16:04:10] TRACE | Query: For capacity planning, what databases does CloudFlow rely on?
[16:04:10] TRACE | Total key facts: 3
[16:04:10] TRACE | Found facts: 3/3
[16:04:10] TRACE |   FOUND[1] PostgreSQL 15.4
[16:04:10] TRACE |   FOUND[2] Redis 7.2
[16:04:10] TRACE |   FOUND[3] Apache Kafka 3.6
[16:04:10] TRACE | === END FACT COVERAGE ===
[16:04:10] TRACE | === FACT COVERAGE ANALYSIS ===
[16:04:10] TRACE | Query: Is CloudFlow using MySQL or something else?
[16:04:10] TRACE | Total key facts: 3
[16:04:10] TRACE | Found facts: 3/3
[16:04:10] TRACE |   FOUND[1] PostgreSQL 15.4
[16:04:10] TRACE |   FOUND[2] Redis 7.2
[16:04:10] TRACE |   FOUND[3] Apache Kafka 3.6
[16:04:10] TRACE | === END FACT COVERAGE ===
[16:04:10] TRACE | === FACT COVERAGE ANALYSIS ===
[16:04:10] TRACE | Query: What is the Kubernetes namespace for production deployment?
[16:04:10] TRACE | Total key facts: 1
[16:04:10] TRACE | Found facts: 1/1
[16:04:10] TRACE |   FOUND[1] cloudflow-prod
[16:04:10] TRACE | === END FACT COVERAGE ===
[16:04:10] TRACE | === FACT COVERAGE ANALYSIS ===
[16:04:10] TRACE | Query: Which namespace is used for CloudFlow production?
[16:04:10] TRACE | Total key facts: 1
[16:04:10] TRACE | Found facts: 1/1
[16:04:10] TRACE |   FOUND[1] cloudflow-prod
[16:04:10] TRACE | === END FACT COVERAGE ===
[16:04:10] TRACE | === FACT COVERAGE ANALYSIS ===
[16:04:10] TRACE | Query: I can't find the production pods
[16:04:10] TRACE | Total key facts: 1
[16:04:10] TRACE | Found facts: 0/1
[16:04:10] TRACE | Missed facts: 1/1
[16:04:10] TRACE |   MISSED[1] cloudflow-prod
[16:04:10] TRACE |     -> Found in chunk_id=deployment_guide_fix_0 (rank not in top-5)
[16:04:10] TRACE | === END FACT COVERAGE ===
[16:04:10] TRACE | === FACT COVERAGE ANALYSIS ===
[16:04:10] TRACE | Query: prod namespace
[16:04:10] TRACE | Total key facts: 1
[16:04:10] TRACE | Found facts: 1/1
[16:04:10] TRACE |   FOUND[1] cloudflow-prod
[16:04:10] TRACE | === END FACT COVERAGE ===
[16:04:10] TRACE | === FACT COVERAGE ANALYSIS ===
[16:04:10] TRACE | Query: I need to deploy to production, what namespace should I use?
[16:04:10] TRACE | Total key facts: 1
[16:04:10] TRACE | Found facts: 1/1
[16:04:10] TRACE |   FOUND[1] cloudflow-prod
[16:04:10] TRACE | === END FACT COVERAGE ===
[16:04:10] TRACE | === FACT COVERAGE ANALYSIS ===
[16:04:10] TRACE | Query: Why can't I see resources in the default namespace?
[16:04:10] TRACE | Total key facts: 1
[16:04:10] TRACE | Found facts: 1/1
[16:04:10] TRACE |   FOUND[1] cloudflow-prod
[16:04:10] TRACE | === END FACT COVERAGE ===
[16:04:10] TRACE | === FACT COVERAGE ANALYSIS ===
[16:04:10] TRACE | Query: What are the resource requirements for the API Gateway?
[16:04:10] TRACE | Total key facts: 1
[16:04:10] TRACE | Found facts: 0/1
[16:04:10] TRACE | Missed facts: 1/1
[16:04:10] TRACE |   MISSED[1] 2 vCPU, 4GB RAM per pod
[16:04:10] TRACE |     -> Found in chunk_id=architecture_overview_fix_1 (rank not in top-5)
[16:04:10] TRACE | === END FACT COVERAGE ===
[16:04:10] TRACE | === FACT COVERAGE ANALYSIS ===
[16:04:10] TRACE | Query: How much CPU and memory does the API Gateway need?
[16:04:10] TRACE | Total key facts: 1
[16:04:10] TRACE | Found facts: 1/1
[16:04:10] TRACE |   FOUND[1] 2 vCPU, 4GB RAM per pod
[16:04:10] TRACE | === END FACT COVERAGE ===
[16:04:10] TRACE | === FACT COVERAGE ANALYSIS ===
[16:04:10] TRACE | Query: The API Gateway pods keep getting OOMKilled
[16:04:10] TRACE | Total key facts: 1
[16:04:10] TRACE | Found facts: 1/1
[16:04:10] TRACE |   FOUND[1] 2 vCPU, 4GB RAM per pod
[16:04:10] TRACE | === END FACT COVERAGE ===
[16:04:10] TRACE | === FACT COVERAGE ANALYSIS ===
[16:04:10] TRACE | Query: api gateway resources
[16:04:10] TRACE | Total key facts: 1
[16:04:10] TRACE | Found facts: 0/1
[16:04:10] TRACE | Missed facts: 1/1
[16:04:10] TRACE |   MISSED[1] 2 vCPU, 4GB RAM per pod
[16:04:10] TRACE |     -> Found in chunk_id=architecture_overview_fix_1 (rank not in top-5)
[16:04:10] TRACE | === END FACT COVERAGE ===
[16:04:10] TRACE | === FACT COVERAGE ANALYSIS ===
[16:04:10] TRACE | Query: I'm provisioning infrastructure, what are the compute specs for API Gateway?
[16:04:10] TRACE | Total key facts: 1
[16:04:10] TRACE | Found facts: 0/1
[16:04:10] TRACE | Missed facts: 1/1
[16:04:10] TRACE |   MISSED[1] 2 vCPU, 4GB RAM per pod
[16:04:10] TRACE |     -> Found in chunk_id=architecture_overview_fix_1 (rank not in top-5)
[16:04:10] TRACE | === END FACT COVERAGE ===
[16:04:10] TRACE | === FACT COVERAGE ANALYSIS ===
[16:04:10] TRACE | Query: Why is 1GB RAM not enough for API Gateway?
[16:04:10] TRACE | Total key facts: 1
[16:04:10] TRACE | Found facts: 1/1
[16:04:10] TRACE |   FOUND[1] 2 vCPU, 4GB RAM per pod
[16:04:10] TRACE | === END FACT COVERAGE ===
[16:04:10] TRACE | === FACT COVERAGE ANALYSIS ===
[16:04:10] TRACE | Query: What are the health check endpoints?
[16:04:10] TRACE | Total key facts: 2
[16:04:10] TRACE | Found facts: 0/2
[16:04:10] TRACE | Missed facts: 2/2
[16:04:10] TRACE |   MISSED[1] /health
[16:04:10] TRACE |     -> Found in chunk_id=deployment_guide_fix_2 (rank not in top-5)
[16:04:10] TRACE |   MISSED[2] /ready
[16:04:10] TRACE |     -> Found in chunk_id=deployment_guide_fix_2 (rank not in top-5)
[16:04:10] TRACE | === END FACT COVERAGE ===
[16:04:10] TRACE | === FACT COVERAGE ANALYSIS ===
[16:04:10] TRACE | Query: Which URLs should I use for health monitoring?
[16:04:10] TRACE | Total key facts: 2
[16:04:10] TRACE | Found facts: 0/2
[16:04:10] TRACE | Missed facts: 2/2
[16:04:10] TRACE |   MISSED[1] /health
[16:04:10] TRACE |     -> Found in chunk_id=deployment_guide_fix_2 (rank not in top-5)
[16:04:10] TRACE |   MISSED[2] /ready
[16:04:10] TRACE |     -> Found in chunk_id=deployment_guide_fix_2 (rank not in top-5)
[16:04:10] TRACE | === END FACT COVERAGE ===
[16:04:10] TRACE | === FACT COVERAGE ANALYSIS ===
[16:04:10] TRACE | Query: My load balancer health checks are failing
[16:04:10] TRACE | Total key facts: 2
[16:04:10] TRACE | Found facts: 2/2
[16:04:10] TRACE |   FOUND[1] /health
[16:04:10] TRACE |   FOUND[2] /ready
[16:04:10] TRACE | === END FACT COVERAGE ===
[16:04:10] TRACE | === FACT COVERAGE ANALYSIS ===
[16:04:10] TRACE | Query: health endpoints
[16:04:10] TRACE | Total key facts: 2
[16:04:10] TRACE | Found facts: 0/2
[16:04:10] TRACE | Missed facts: 2/2
[16:04:10] TRACE |   MISSED[1] /health
[16:04:10] TRACE |     -> Found in chunk_id=deployment_guide_fix_2 (rank not in top-5)
[16:04:10] TRACE |   MISSED[2] /ready
[16:04:10] TRACE |     -> Found in chunk_id=deployment_guide_fix_2 (rank not in top-5)
[16:04:10] TRACE | === END FACT COVERAGE ===
[16:04:10] TRACE | === FACT COVERAGE ANALYSIS ===
[16:04:10] TRACE | Query: Setting up monitoring, what endpoints indicate service health?
[16:04:10] TRACE | Total key facts: 2
[16:04:10] TRACE | Found facts: 0/2
[16:04:10] TRACE | Missed facts: 2/2
[16:04:10] TRACE |   MISSED[1] /health
[16:04:10] TRACE |     -> Found in chunk_id=deployment_guide_fix_2 (rank not in top-5)
[16:04:10] TRACE |   MISSED[2] /ready
[16:04:10] TRACE |     -> Found in chunk_id=deployment_guide_fix_2 (rank not in top-5)
[16:04:10] TRACE | === END FACT COVERAGE ===
[16:04:10] TRACE | === FACT COVERAGE ANALYSIS ===
[16:04:10] TRACE | Query: Why doesn't /status return health information?
[16:04:10] TRACE | Total key facts: 2
[16:04:10] TRACE | Found facts: 2/2
[16:04:10] TRACE |   FOUND[1] /health
[16:04:10] TRACE |   FOUND[2] /ready
[16:04:10] TRACE | === END FACT COVERAGE ===
[16:04:10] TRACE | === FACT COVERAGE ANALYSIS ===
[16:04:10] TRACE | Query: What are the HPA scaling parameters?
[16:04:10] TRACE | Total key facts: 3
[16:04:10] TRACE | Found facts: 0/3
[16:04:10] TRACE | Missed facts: 3/3
[16:04:10] TRACE |   MISSED[1] minReplicas: 3
[16:04:10] TRACE |     -> Found in chunk_id=deployment_guide_fix_1 (rank not in top-5)
[16:04:10] TRACE |   MISSED[2] maxReplicas: 10
[16:04:10] TRACE |     -> Found in chunk_id=deployment_guide_fix_1 (rank not in top-5)
[16:04:10] TRACE |   MISSED[3] targetCPUUtilizationPercentage: 70
[16:04:10] TRACE |     -> Found in chunk_id=deployment_guide_fix_1 (rank not in top-5)
[16:04:10] TRACE | === END FACT COVERAGE ===
[16:04:10] TRACE | === FACT COVERAGE ANALYSIS ===
[16:04:10] TRACE | Query: How does horizontal pod autoscaling work?
[16:04:10] TRACE | Total key facts: 3
[16:04:10] TRACE | Found facts: 0/3
[16:04:10] TRACE | Missed facts: 3/3
[16:04:10] TRACE |   MISSED[1] minReplicas: 3
[16:04:10] TRACE |     -> Found in chunk_id=deployment_guide_fix_1 (rank not in top-5)
[16:04:10] TRACE |   MISSED[2] maxReplicas: 10
[16:04:10] TRACE |     -> Found in chunk_id=deployment_guide_fix_1 (rank not in top-5)
[16:04:10] TRACE |   MISSED[3] targetCPUUtilizationPercentage: 70
[16:04:10] TRACE |     -> Found in chunk_id=deployment_guide_fix_1 (rank not in top-5)
[16:04:10] TRACE | === END FACT COVERAGE ===
[16:04:10] TRACE | === FACT COVERAGE ANALYSIS ===
[16:04:10] TRACE | Query: Pods aren't scaling up during traffic spikes
[16:04:10] TRACE | Total key facts: 3
[16:04:10] TRACE | Found facts: 0/3
[16:04:10] TRACE | Missed facts: 3/3
[16:04:10] TRACE |   MISSED[1] minReplicas: 3
[16:04:10] TRACE |     -> Found in chunk_id=deployment_guide_fix_1 (rank not in top-5)
[16:04:10] TRACE |   MISSED[2] maxReplicas: 10
[16:04:10] TRACE |     -> Found in chunk_id=deployment_guide_fix_1 (rank not in top-5)
[16:04:10] TRACE |   MISSED[3] targetCPUUtilizationPercentage: 70
[16:04:10] TRACE |     -> Found in chunk_id=deployment_guide_fix_1 (rank not in top-5)
[16:04:10] TRACE | === END FACT COVERAGE ===
[16:04:10] TRACE | === FACT COVERAGE ANALYSIS ===
[16:04:10] TRACE | Query: autoscaling config
[16:04:10] TRACE | Total key facts: 3
[16:04:10] TRACE | Found facts: 3/3
[16:04:10] TRACE |   FOUND[1] minReplicas: 3
[16:04:10] TRACE |   FOUND[2] maxReplicas: 10
[16:04:10] TRACE |   FOUND[3] targetCPUUtilizationPercentage: 70
[16:04:10] TRACE | === END FACT COVERAGE ===
[16:04:10] TRACE | === FACT COVERAGE ANALYSIS ===
[16:04:10] TRACE | Query: I need to configure autoscaling, what are the min/max replicas and thresholds?
[16:04:10] TRACE | Total key facts: 3
[16:04:10] TRACE | Found facts: 0/3
[16:04:10] TRACE | Missed facts: 3/3
[16:04:10] TRACE |   MISSED[1] minReplicas: 3
[16:04:10] TRACE |     -> Found in chunk_id=deployment_guide_fix_1 (rank not in top-5)
[16:04:10] TRACE |   MISSED[2] maxReplicas: 10
[16:04:10] TRACE |     -> Found in chunk_id=deployment_guide_fix_1 (rank not in top-5)
[16:04:10] TRACE |   MISSED[3] targetCPUUtilizationPercentage: 70
[16:04:10] TRACE |     -> Found in chunk_id=deployment_guide_fix_1 (rank not in top-5)
[16:04:10] TRACE | === END FACT COVERAGE ===
[16:04:10] TRACE | === FACT COVERAGE ANALYSIS ===
[16:04:10] TRACE | Query: Why do we have 3 replicas even with low traffic?
[16:04:10] TRACE | Total key facts: 3
[16:04:10] TRACE | Found facts: 0/3
[16:04:10] TRACE | Missed facts: 3/3
[16:04:10] TRACE |   MISSED[1] minReplicas: 3
[16:04:10] TRACE |     -> Found in chunk_id=deployment_guide_fix_1 (rank not in top-5)
[16:04:10] TRACE |   MISSED[2] maxReplicas: 10
[16:04:10] TRACE |     -> Found in chunk_id=deployment_guide_fix_1 (rank not in top-5)
[16:04:10] TRACE |   MISSED[3] targetCPUUtilizationPercentage: 70
[16:04:10] TRACE |     -> Found in chunk_id=deployment_guide_fix_1 (rank not in top-5)
[16:04:10] TRACE | === END FACT COVERAGE ===
[16:04:10] TRACE | === FACT COVERAGE ANALYSIS ===
[16:04:10] TRACE | Query: What is the P99 latency target for API operations?
[16:04:10] TRACE | Total key facts: 2
[16:04:10] TRACE | Found facts: 2/2
[16:04:10] TRACE |   FOUND[1] P99 latency: < 200ms
[16:04:10] TRACE |   FOUND[2] average P99 latency of 180ms for API operations
[16:04:10] TRACE | === END FACT COVERAGE ===
[16:04:10] TRACE | === FACT COVERAGE ANALYSIS ===
[16:04:10] TRACE | Query: What's the 99th percentile response time target?
[16:04:10] TRACE | Total key facts: 2
[16:04:10] TRACE | Found facts: 1/2
[16:04:10] TRACE |   FOUND[1] P99 latency: < 200ms
[16:04:10] TRACE | Missed facts: 1/2
[16:04:10] TRACE |   MISSED[1] average P99 latency of 180ms for API operations
[16:04:10] TRACE |     -> Found in chunk_id=architecture_overview_fix_0 (rank not in top-5)
[16:04:10] TRACE | === END FACT COVERAGE ===
[16:04:10] TRACE | === FACT COVERAGE ANALYSIS ===
[16:04:10] TRACE | Query: Our API latency is 500ms, is that acceptable?
[16:04:10] TRACE | Total key facts: 2
[16:04:10] TRACE | Found facts: 0/2
[16:04:10] TRACE | Missed facts: 2/2
[16:04:10] TRACE |   MISSED[1] P99 latency: < 200ms
[16:04:10] TRACE |     -> Found in chunk_id=architecture_overview_fix_1 (rank not in top-5)
[16:04:10] TRACE |   MISSED[2] average P99 latency of 180ms for API operations
[16:04:10] TRACE |     -> Found in chunk_id=architecture_overview_fix_0 (rank not in top-5)
[16:04:10] TRACE | === END FACT COVERAGE ===
[16:04:10] TRACE | === FACT COVERAGE ANALYSIS ===
[16:04:10] TRACE | Query: api latency target
[16:04:10] TRACE | Total key facts: 2
[16:04:10] TRACE | Found facts: 1/2
[16:04:10] TRACE |   FOUND[1] P99 latency: < 200ms
[16:04:10] TRACE | Missed facts: 1/2
[16:04:10] TRACE |   MISSED[1] average P99 latency of 180ms for API operations
[16:04:10] TRACE |     -> Found in chunk_id=architecture_overview_fix_0 (rank not in top-5)
[16:04:10] TRACE | === END FACT COVERAGE ===
[16:04:10] TRACE | === FACT COVERAGE ANALYSIS ===
[16:04:10] TRACE | Query: Setting SLOs for our service, what P99 latency does CloudFlow guarantee?
[16:04:10] TRACE | Total key facts: 2
[16:04:10] TRACE | Found facts: 1/2
[16:04:10] TRACE |   FOUND[1] average P99 latency of 180ms for API operations
[16:04:10] TRACE | Missed facts: 1/2
[16:04:10] TRACE |   MISSED[1] P99 latency: < 200ms
[16:04:10] TRACE |     -> Found in chunk_id=architecture_overview_fix_1 (rank not in top-5)
[16:04:10] TRACE | === END FACT COVERAGE ===
[16:04:10] TRACE | === FACT COVERAGE ANALYSIS ===
[16:04:10] TRACE | Query: Why are we getting alerts at 200ms latency?
[16:04:10] TRACE | Total key facts: 2
[16:04:10] TRACE | Found facts: 0/2
[16:04:10] TRACE | Missed facts: 2/2
[16:04:10] TRACE |   MISSED[1] P99 latency: < 200ms
[16:04:10] TRACE |     -> Found in chunk_id=architecture_overview_fix_1 (rank not in top-5)
[16:04:10] TRACE |   MISSED[2] average P99 latency of 180ms for API operations
[16:04:10] TRACE |     -> Found in chunk_id=architecture_overview_fix_0 (rank not in top-5)
[16:04:10] TRACE | === END FACT COVERAGE ===
[16:04:10] TRACE | === FACT COVERAGE ANALYSIS ===
[16:04:10] TRACE | Query: What are the disaster recovery RPO and RTO values?
[16:04:10] TRACE | Total key facts: 2
[16:04:10] TRACE | Found facts: 0/2
[16:04:10] TRACE | Missed facts: 2/2
[16:04:10] TRACE |   MISSED[1] RPO (Recovery Point Objective): 1 hour
[16:04:10] TRACE |     -> Found in chunk_id=architecture_overview_fix_10 (rank not in top-5)
[16:04:10] TRACE |   MISSED[2] RTO (Recovery Time Objective): 4 hours
[16:04:10] TRACE |     -> Found in chunk_id=architecture_overview_fix_10 (rank not in top-5)
[16:04:10] TRACE | === END FACT COVERAGE ===
[16:04:10] TRACE | === FACT COVERAGE ANALYSIS ===
[16:04:10] TRACE | Query: What's the maximum data loss and recovery time?
[16:04:10] TRACE | Total key facts: 2
[16:04:10] TRACE | Found facts: 0/2
[16:04:10] TRACE | Missed facts: 2/2
[16:04:10] TRACE |   MISSED[1] RPO (Recovery Point Objective): 1 hour
[16:04:10] TRACE |     -> Found in chunk_id=architecture_overview_fix_10 (rank not in top-5)
[16:04:10] TRACE |   MISSED[2] RTO (Recovery Time Objective): 4 hours
[16:04:10] TRACE |     -> Found in chunk_id=architecture_overview_fix_10 (rank not in top-5)
[16:04:10] TRACE | === END FACT COVERAGE ===
[16:04:10] TRACE | === FACT COVERAGE ANALYSIS ===
[16:04:10] TRACE | Query: How long will it take to recover from a disaster?
[16:04:10] TRACE | Total key facts: 2
[16:04:10] TRACE | Found facts: 0/2
[16:04:10] TRACE | Missed facts: 2/2
[16:04:10] TRACE |   MISSED[1] RPO (Recovery Point Objective): 1 hour
[16:04:10] TRACE |     -> Found in chunk_id=architecture_overview_fix_10 (rank not in top-5)
[16:04:10] TRACE |   MISSED[2] RTO (Recovery Time Objective): 4 hours
[16:04:10] TRACE |     -> Found in chunk_id=architecture_overview_fix_10 (rank not in top-5)
[16:04:10] TRACE | === END FACT COVERAGE ===
[16:04:10] TRACE | === FACT COVERAGE ANALYSIS ===
[16:04:10] TRACE | Query: RPO RTO
[16:04:10] TRACE | Total key facts: 2
[16:04:10] TRACE | Found facts: 0/2
[16:04:10] TRACE | Missed facts: 2/2
[16:04:10] TRACE |   MISSED[1] RPO (Recovery Point Objective): 1 hour
[16:04:10] TRACE |     -> Found in chunk_id=architecture_overview_fix_10 (rank not in top-5)
[16:04:10] TRACE |   MISSED[2] RTO (Recovery Time Objective): 4 hours
[16:04:10] TRACE |     -> Found in chunk_id=architecture_overview_fix_10 (rank not in top-5)
[16:04:10] TRACE | === END FACT COVERAGE ===
[16:04:10] TRACE | === FACT COVERAGE ANALYSIS ===
[16:04:10] TRACE | Query: For our business continuity plan, what are CloudFlow's recovery objectives?
[16:04:10] TRACE | Total key facts: 2
[16:04:10] TRACE | Found facts: 0/2
[16:04:10] TRACE | Missed facts: 2/2
[16:04:10] TRACE |   MISSED[1] RPO (Recovery Point Objective): 1 hour
[16:04:10] TRACE |     -> Found in chunk_id=architecture_overview_fix_10 (rank not in top-5)
[16:04:10] TRACE |   MISSED[2] RTO (Recovery Time Objective): 4 hours
[16:04:10] TRACE |     -> Found in chunk_id=architecture_overview_fix_10 (rank not in top-5)
[16:04:10] TRACE | === END FACT COVERAGE ===
[16:04:10] TRACE | === FACT COVERAGE ANALYSIS ===
[16:04:10] TRACE | Query: Why can't we guarantee zero data loss?
[16:04:10] TRACE | Total key facts: 2
[16:04:10] TRACE | Found facts: 0/2
[16:04:10] TRACE | Missed facts: 2/2
[16:04:10] TRACE |   MISSED[1] RPO (Recovery Point Objective): 1 hour
[16:04:10] TRACE |     -> Found in chunk_id=architecture_overview_fix_10 (rank not in top-5)
[16:04:10] TRACE |   MISSED[2] RTO (Recovery Time Objective): 4 hours
[16:04:10] TRACE |     -> Found in chunk_id=architecture_overview_fix_10 (rank not in top-5)
[16:04:10] TRACE | === END FACT COVERAGE ===
[16:04:10] TRACE | === FACT COVERAGE ANALYSIS ===
[16:04:10] TRACE | Query: What is the maximum workflow execution timeout?
[16:04:10] TRACE | Total key facts: 2
[16:04:10] TRACE | Found facts: 1/2
[16:04:10] TRACE |   FOUND[1] 3600 seconds
[16:04:10] TRACE | Missed facts: 1/2
[16:04:10] TRACE |   MISSED[1] exceeded maximum execution time of 3600 seconds
[16:04:10] TRACE |     -> Found in chunk_id=troubleshooting_guide_fix_4 (rank not in top-5)
[16:04:10] TRACE | === END FACT COVERAGE ===
[16:04:10] TRACE | === FACT COVERAGE ANALYSIS ===
[16:04:10] TRACE | Query: How long can a workflow run before timing out?
[16:04:10] TRACE | Total key facts: 2
[16:04:10] TRACE | Found facts: 0/2
[16:04:10] TRACE | Missed facts: 2/2
[16:04:10] TRACE |   MISSED[1] 3600 seconds
[16:04:10] TRACE |     -> Found in chunk_id=api_reference_fix_0 (rank not in top-5)
[16:04:10] TRACE |   MISSED[2] exceeded maximum execution time of 3600 seconds
[16:04:10] TRACE |     -> Found in chunk_id=troubleshooting_guide_fix_4 (rank not in top-5)
[16:04:10] TRACE | === END FACT COVERAGE ===
[16:04:10] TRACE | === FACT COVERAGE ANALYSIS ===
[16:04:10] TRACE | Query: My workflow is being killed after an hour
[16:04:10] TRACE | Total key facts: 2
[16:04:10] TRACE | Found facts: 0/2
[16:04:10] TRACE | Missed facts: 2/2
[16:04:10] TRACE |   MISSED[1] 3600 seconds
[16:04:10] TRACE |     -> Found in chunk_id=api_reference_fix_0 (rank not in top-5)
[16:04:10] TRACE |   MISSED[2] exceeded maximum execution time of 3600 seconds
[16:04:10] TRACE |     -> Found in chunk_id=troubleshooting_guide_fix_4 (rank not in top-5)
[16:04:10] TRACE | === END FACT COVERAGE ===
[16:04:10] TRACE | === FACT COVERAGE ANALYSIS ===
[16:04:10] TRACE | Query: workflow timeout
[16:04:10] TRACE | Total key facts: 2
[16:04:10] TRACE | Found facts: 1/2
[16:04:10] TRACE |   FOUND[1] 3600 seconds
[16:04:10] TRACE | Missed facts: 1/2
[16:04:10] TRACE |   MISSED[1] exceeded maximum execution time of 3600 seconds
[16:04:10] TRACE |     -> Found in chunk_id=troubleshooting_guide_fix_4 (rank not in top-5)
[16:04:10] TRACE | === END FACT COVERAGE ===
[16:04:10] TRACE | === FACT COVERAGE ANALYSIS ===
[16:04:10] TRACE | Query: I have a long-running data processing workflow, what's the time limit?
[16:04:10] TRACE | Total key facts: 2
[16:04:10] TRACE | Found facts: 0/2
[16:04:10] TRACE | Missed facts: 2/2
[16:04:10] TRACE |   MISSED[1] 3600 seconds
[16:04:10] TRACE |     -> Found in chunk_id=api_reference_fix_0 (rank not in top-5)
[16:04:10] TRACE |   MISSED[2] exceeded maximum execution time of 3600 seconds
[16:04:10] TRACE |     -> Found in chunk_id=troubleshooting_guide_fix_4 (rank not in top-5)
[16:04:10] TRACE | === END FACT COVERAGE ===
[16:04:10] TRACE | === FACT COVERAGE ANALYSIS ===
[16:04:10] TRACE | Query: Why did my workflow fail after 3600 seconds?
[16:04:10] TRACE | Total key facts: 2
[16:04:10] TRACE | Found facts: 0/2
[16:04:10] TRACE | Missed facts: 2/2
[16:04:10] TRACE |   MISSED[1] 3600 seconds
[16:04:10] TRACE |     -> Found in chunk_id=api_reference_fix_0 (rank not in top-5)
[16:04:10] TRACE |   MISSED[2] exceeded maximum execution time of 3600 seconds
[16:04:10] TRACE |     -> Found in chunk_id=troubleshooting_guide_fix_4 (rank not in top-5)
[16:04:10] TRACE | === END FACT COVERAGE ===
[16:04:10] TRACE | === FACT COVERAGE ANALYSIS ===
[16:04:10] TRACE | Query: What JWT algorithm is used for token signing?
[16:04:10] TRACE | Total key facts: 3
[16:04:10] TRACE | Found facts: 3/3
[16:04:10] TRACE |   FOUND[1] RS256
[16:04:10] TRACE |   FOUND[2] RS256 algorithm
[16:04:10] TRACE |   FOUND[3] RS256 signing algorithm
[16:04:10] TRACE | === END FACT COVERAGE ===
[16:04:11] TRACE | === FACT COVERAGE ANALYSIS ===
[16:04:11] TRACE | Query: Which signing algorithm does CloudFlow use for JWTs?
[16:04:11] TRACE | Total key facts: 3
[16:04:11] TRACE | Found facts: 2/3
[16:04:11] TRACE |   FOUND[1] RS256
[16:04:11] TRACE |   FOUND[2] RS256 signing algorithm
[16:04:11] TRACE | Missed facts: 1/3
[16:04:11] TRACE |   MISSED[1] RS256 algorithm
[16:04:11] TRACE |     -> Found in chunk_id=architecture_overview_fix_1 (rank not in top-5)
[16:04:11] TRACE | === END FACT COVERAGE ===
[16:04:11] TRACE | === FACT COVERAGE ANALYSIS ===
[16:04:11] TRACE | Query: My JWT validation is failing with algorithm mismatch
[16:04:11] TRACE | Total key facts: 3
[16:04:11] TRACE | Found facts: 3/3
[16:04:11] TRACE |   FOUND[1] RS256
[16:04:11] TRACE |   FOUND[2] RS256 algorithm
[16:04:11] TRACE |   FOUND[3] RS256 signing algorithm
[16:04:11] TRACE | === END FACT COVERAGE ===
[16:04:11] TRACE | === FACT COVERAGE ANALYSIS ===
[16:04:11] TRACE | Query: jwt algorithm
[16:04:11] TRACE | Total key facts: 3
[16:04:11] TRACE | Found facts: 3/3
[16:04:11] TRACE |   FOUND[1] RS256
[16:04:11] TRACE |   FOUND[2] RS256 algorithm
[16:04:11] TRACE |   FOUND[3] RS256 signing algorithm
[16:04:11] TRACE | === END FACT COVERAGE ===
[16:04:11] TRACE | === FACT COVERAGE ANALYSIS ===
[16:04:11] TRACE | Query: I'm implementing JWT verification, what algorithm should I expect?
[16:04:11] TRACE | Total key facts: 3
[16:04:11] TRACE | Found facts: 3/3
[16:04:11] TRACE |   FOUND[1] RS256
[16:04:11] TRACE |   FOUND[2] RS256 algorithm
[16:04:11] TRACE |   FOUND[3] RS256 signing algorithm
[16:04:11] TRACE | === END FACT COVERAGE ===
[16:04:11] TRACE | === FACT COVERAGE ANALYSIS ===
[16:04:11] TRACE | Query: Why doesn't HS256 work for token validation?
[16:04:11] TRACE | Total key facts: 3
[16:04:11] TRACE | Found facts: 3/3
[16:04:11] TRACE |   FOUND[1] RS256
[16:04:11] TRACE |   FOUND[2] RS256 algorithm
[16:04:11] TRACE |   FOUND[3] RS256 signing algorithm
[16:04:11] TRACE | === END FACT COVERAGE ===
[16:04:11] TRACE | === FACT COVERAGE ANALYSIS ===
[16:04:11] TRACE | Query: What is the Redis cache TTL for workflow definitions?
[16:04:11] TRACE | Total key facts: 2
[16:04:11] TRACE | Found facts: 1/2
[16:04:11] TRACE |   FOUND[1] TTL: 1 hour
[16:04:11] TRACE | Missed facts: 1/2
[16:04:11] TRACE |   MISSED[1] Workflow Definitions: TTL: 1 hour
[16:04:11] TRACE | === END FACT COVERAGE ===
[16:04:11] TRACE | === FACT COVERAGE ANALYSIS ===
[16:04:11] TRACE | Query: How long are workflow definitions cached?
[16:04:11] TRACE | Total key facts: 2
[16:04:11] TRACE | Found facts: 0/2
[16:04:11] TRACE | Missed facts: 2/2
[16:04:11] TRACE |   MISSED[1] TTL: 1 hour
[16:04:11] TRACE |     -> Found in chunk_id=architecture_overview_fix_6 (rank not in top-5)
[16:04:11] TRACE |   MISSED[2] Workflow Definitions: TTL: 1 hour
[16:04:11] TRACE | === END FACT COVERAGE ===
[16:04:11] TRACE | === FACT COVERAGE ANALYSIS ===
[16:04:11] TRACE | Query: My workflow updates aren't reflecting immediately
[16:04:11] TRACE | Total key facts: 2
[16:04:11] TRACE | Found facts: 0/2
[16:04:11] TRACE | Missed facts: 2/2
[16:04:11] TRACE |   MISSED[1] TTL: 1 hour
[16:04:11] TRACE |     -> Found in chunk_id=architecture_overview_fix_6 (rank not in top-5)
[16:04:11] TRACE |   MISSED[2] Workflow Definitions: TTL: 1 hour
[16:04:11] TRACE | === END FACT COVERAGE ===
[16:04:11] TRACE | === FACT COVERAGE ANALYSIS ===
[16:04:11] TRACE | Query: cache ttl workflows
[16:04:11] TRACE | Total key facts: 2
[16:04:11] TRACE | Found facts: 1/2
[16:04:11] TRACE |   FOUND[1] TTL: 1 hour
[16:04:11] TRACE | Missed facts: 1/2
[16:04:11] TRACE |   MISSED[1] Workflow Definitions: TTL: 1 hour
[16:04:11] TRACE | === END FACT COVERAGE ===
[16:04:11] TRACE | === FACT COVERAGE ANALYSIS ===
[16:04:11] TRACE | Query: After updating a workflow, how long until the cache expires?
[16:04:11] TRACE | Total key facts: 2
[16:04:11] TRACE | Found facts: 0/2
[16:04:11] TRACE | Missed facts: 2/2
[16:04:11] TRACE |   MISSED[1] TTL: 1 hour
[16:04:11] TRACE |     -> Found in chunk_id=architecture_overview_fix_6 (rank not in top-5)
[16:04:11] TRACE |   MISSED[2] Workflow Definitions: TTL: 1 hour
[16:04:11] TRACE | === END FACT COVERAGE ===
[16:04:11] TRACE | === FACT COVERAGE ANALYSIS ===
[16:04:11] TRACE | Query: Why are changes taking an hour to appear?
[16:04:11] TRACE | Total key facts: 2
[16:04:11] TRACE | Found facts: 0/2
[16:04:11] TRACE | Missed facts: 2/2
[16:04:11] TRACE |   MISSED[1] TTL: 1 hour
[16:04:11] TRACE |     -> Found in chunk_id=architecture_overview_fix_6 (rank not in top-5)
[16:04:11] TRACE |   MISSED[2] Workflow Definitions: TTL: 1 hour
[16:04:11] TRACE | === END FACT COVERAGE ===
[16:04:11] TRACE | === FACT COVERAGE ANALYSIS ===
[16:04:11] TRACE | Query: What monitoring tools does CloudFlow use?
[16:04:11] TRACE | Total key facts: 3
[16:04:11] TRACE | Found facts: 3/3
[16:04:11] TRACE |   FOUND[1] Prometheus
[16:04:11] TRACE |   FOUND[2] Grafana
[16:04:11] TRACE |   FOUND[3] Jaeger for distributed tracing
[16:04:11] TRACE | === END FACT COVERAGE ===
[16:04:11] TRACE | === FACT COVERAGE ANALYSIS ===
[16:04:11] TRACE | Query: Which observability platform is integrated?
[16:04:11] TRACE | Total key facts: 3
[16:04:11] TRACE | Found facts: 3/3
[16:04:11] TRACE |   FOUND[1] Prometheus
[16:04:11] TRACE |   FOUND[2] Grafana
[16:04:11] TRACE |   FOUND[3] Jaeger for distributed tracing
[16:04:11] TRACE | === END FACT COVERAGE ===
[16:04:11] TRACE | === FACT COVERAGE ANALYSIS ===
[16:04:11] TRACE | Query: Where can I view CloudFlow metrics and logs?
[16:04:11] TRACE | Total key facts: 3
[16:04:11] TRACE | Found facts: 3/3
[16:04:11] TRACE |   FOUND[1] Prometheus
[16:04:11] TRACE |   FOUND[2] Grafana
[16:04:11] TRACE |   FOUND[3] Jaeger for distributed tracing
[16:04:11] TRACE | === END FACT COVERAGE ===
[16:04:11] TRACE | === FACT COVERAGE ANALYSIS ===
[16:04:11] TRACE | Query: monitoring stack
[16:04:11] TRACE | Total key facts: 3
[16:04:11] TRACE | Found facts: 0/3
[16:04:11] TRACE | Missed facts: 3/3
[16:04:11] TRACE |   MISSED[1] Prometheus
[16:04:11] TRACE |     -> Found in chunk_id=architecture_overview_fix_0 (rank not in top-5)
[16:04:11] TRACE |   MISSED[2] Grafana
[16:04:11] TRACE |     -> Found in chunk_id=architecture_overview_fix_0 (rank not in top-5)
[16:04:11] TRACE |   MISSED[3] Jaeger for distributed tracing
[16:04:11] TRACE |     -> Found in chunk_id=architecture_overview_fix_0 (rank not in top-5)
[16:04:11] TRACE | === END FACT COVERAGE ===
[16:04:11] TRACE | === FACT COVERAGE ANALYSIS ===
[16:04:11] TRACE | Query: I need to set up dashboards, what monitoring systems are available?
[16:04:11] TRACE | Total key facts: 3
[16:04:11] TRACE | Found facts: 0/3
[16:04:11] TRACE | Missed facts: 3/3
[16:04:11] TRACE |   MISSED[1] Prometheus
[16:04:11] TRACE |     -> Found in chunk_id=architecture_overview_fix_0 (rank not in top-5)
[16:04:11] TRACE |   MISSED[2] Grafana
[16:04:11] TRACE |     -> Found in chunk_id=architecture_overview_fix_0 (rank not in top-5)
[16:04:11] TRACE |   MISSED[3] Jaeger for distributed tracing
[16:04:11] TRACE |     -> Found in chunk_id=architecture_overview_fix_0 (rank not in top-5)
[16:04:11] TRACE | === END FACT COVERAGE ===
[16:04:11] TRACE | === FACT COVERAGE ANALYSIS ===
[16:04:11] TRACE | Query: Why can't I see metrics in Datadog?
[16:04:11] TRACE | Total key facts: 3
[16:04:11] TRACE | Found facts: 2/3
[16:04:11] TRACE |   FOUND[1] Prometheus
[16:04:11] TRACE |   FOUND[2] Grafana
[16:04:11] TRACE | Missed facts: 1/3
[16:04:11] TRACE |   MISSED[1] Jaeger for distributed tracing
[16:04:11] TRACE |     -> Found in chunk_id=architecture_overview_fix_0 (rank not in top-5)
[16:04:11] TRACE | === END FACT COVERAGE ===
[16:04:11] TRACE | === FACT COVERAGE ANALYSIS ===
[16:04:11] TRACE | Query: How do I diagnose database connection pool exhaustion?
[16:04:11] TRACE | Total key facts: 4
[16:04:11] TRACE | Found facts: 1/4
[16:04:11] TRACE |   FOUND[1] PgBouncer
[16:04:11] TRACE | Missed facts: 3/4
[16:04:11] TRACE |   MISSED[1] could not obtain connection from pool within 5000ms
[16:04:11] TRACE |     -> Found in chunk_id=troubleshooting_guide_fix_3 (rank not in top-5)
[16:04:11] TRACE |   MISSED[2] connection pool exhausted (100/100 connections in use)
[16:04:11] TRACE |     -> Found in chunk_id=troubleshooting_guide_fix_3 (rank not in top-5)
[16:04:11] TRACE |   MISSED[3] max_db_connections = 100
[16:04:11] TRACE |     -> Found in chunk_id=deployment_guide_fix_3 (rank not in top-5)
[16:04:11] TRACE | === END FACT COVERAGE ===
[16:04:11] TRACE | === FACT COVERAGE ANALYSIS ===
[16:04:11] TRACE | Query: What should I check when I run out of database connections?
[16:04:11] TRACE | Total key facts: 4
[16:04:11] TRACE | Found facts: 1/4
[16:04:11] TRACE |   FOUND[1] PgBouncer
[16:04:11] TRACE | Missed facts: 3/4
[16:04:11] TRACE |   MISSED[1] could not obtain connection from pool within 5000ms
[16:04:11] TRACE |     -> Found in chunk_id=troubleshooting_guide_fix_3 (rank not in top-5)
[16:04:11] TRACE |   MISSED[2] connection pool exhausted (100/100 connections in use)
[16:04:11] TRACE |     -> Found in chunk_id=troubleshooting_guide_fix_3 (rank not in top-5)
[16:04:11] TRACE |   MISSED[3] max_db_connections = 100
[16:04:11] TRACE |     -> Found in chunk_id=deployment_guide_fix_3 (rank not in top-5)
[16:04:11] TRACE | === END FACT COVERAGE ===
[16:04:11] TRACE | === FACT COVERAGE ANALYSIS ===
[16:04:11] TRACE | Query: Getting 'could not obtain connection from pool' errors
[16:04:11] TRACE | Total key facts: 4
[16:04:11] TRACE | Found facts: 1/4
[16:04:11] TRACE |   FOUND[1] PgBouncer
[16:04:11] TRACE | Missed facts: 3/4
[16:04:11] TRACE |   MISSED[1] could not obtain connection from pool within 5000ms
[16:04:11] TRACE |     -> Found in chunk_id=troubleshooting_guide_fix_3 (rank not in top-5)
[16:04:11] TRACE |   MISSED[2] connection pool exhausted (100/100 connections in use)
[16:04:11] TRACE |     -> Found in chunk_id=troubleshooting_guide_fix_3 (rank not in top-5)
[16:04:11] TRACE |   MISSED[3] max_db_connections = 100
[16:04:11] TRACE |     -> Found in chunk_id=deployment_guide_fix_3 (rank not in top-5)
[16:04:11] TRACE | === END FACT COVERAGE ===
[16:04:11] TRACE | === FACT COVERAGE ANALYSIS ===
[16:04:11] TRACE | Query: connection pool full
[16:04:11] TRACE | Total key facts: 4
[16:04:11] TRACE | Found facts: 1/4
[16:04:11] TRACE |   FOUND[1] PgBouncer
[16:04:11] TRACE | Missed facts: 3/4
[16:04:11] TRACE |   MISSED[1] could not obtain connection from pool within 5000ms
[16:04:11] TRACE |     -> Found in chunk_id=troubleshooting_guide_fix_3 (rank not in top-5)
[16:04:11] TRACE |   MISSED[2] connection pool exhausted (100/100 connections in use)
[16:04:11] TRACE |     -> Found in chunk_id=troubleshooting_guide_fix_3 (rank not in top-5)
[16:04:11] TRACE |   MISSED[3] max_db_connections = 100
[16:04:11] TRACE |     -> Found in chunk_id=deployment_guide_fix_3 (rank not in top-5)
[16:04:11] TRACE | === END FACT COVERAGE ===
[16:04:11] TRACE | === FACT COVERAGE ANALYSIS ===
[16:04:11] TRACE | Query: My app is failing with connection pool errors, how do I troubleshoot?
[16:04:11] TRACE | Total key facts: 4
[16:04:11] TRACE | Found facts: 1/4
[16:04:11] TRACE |   FOUND[1] PgBouncer
[16:04:11] TRACE | Missed facts: 3/4
[16:04:11] TRACE |   MISSED[1] could not obtain connection from pool within 5000ms
[16:04:11] TRACE |     -> Found in chunk_id=troubleshooting_guide_fix_3 (rank not in top-5)
[16:04:11] TRACE |   MISSED[2] connection pool exhausted (100/100 connections in use)
[16:04:11] TRACE |     -> Found in chunk_id=troubleshooting_guide_fix_3 (rank not in top-5)
[16:04:11] TRACE |   MISSED[3] max_db_connections = 100
[16:04:11] TRACE |     -> Found in chunk_id=deployment_guide_fix_3 (rank not in top-5)
[16:04:11] TRACE | === END FACT COVERAGE ===
[16:04:11] TRACE | === FACT COVERAGE ANALYSIS ===
[16:04:11] TRACE | Query: Why can't I get a database connection even though CPU is low?
[16:04:11] TRACE | Total key facts: 4
[16:04:11] TRACE | Found facts: 0/4
[16:04:11] TRACE | Missed facts: 4/4
[16:04:11] TRACE |   MISSED[1] could not obtain connection from pool within 5000ms
[16:04:11] TRACE |     -> Found in chunk_id=troubleshooting_guide_fix_3 (rank not in top-5)
[16:04:11] TRACE |   MISSED[2] connection pool exhausted (100/100 connections in use)
[16:04:11] TRACE |     -> Found in chunk_id=troubleshooting_guide_fix_3 (rank not in top-5)
[16:04:11] TRACE |   MISSED[3] PgBouncer
[16:04:11] TRACE |     -> Found in chunk_id=architecture_overview_fix_5 (rank not in top-5)
[16:04:11] TRACE |   MISSED[4] max_db_connections = 100
[16:04:11] TRACE |     -> Found in chunk_id=deployment_guide_fix_3 (rank not in top-5)
[16:04:11] TRACE | === END FACT COVERAGE ===
[16:04:11] TRACE | === FACT COVERAGE ANALYSIS ===
[16:04:11] TRACE | Query: How do I handle API authentication?
[16:04:11] TRACE | Total key facts: 3
[16:04:11] TRACE | Found facts: 3/3
[16:04:11] TRACE |   FOUND[1] OAuth 2.0
[16:04:11] TRACE |   FOUND[2] API keys
[16:04:11] TRACE |   FOUND[3] JWT tokens
[16:04:11] TRACE | === END FACT COVERAGE ===
[16:04:11] TRACE | === FACT COVERAGE ANALYSIS ===
[16:04:11] TRACE | Query: What authentication methods are supported?
[16:04:11] TRACE | Total key facts: 3
[16:04:11] TRACE | Found facts: 3/3
[16:04:11] TRACE |   FOUND[1] OAuth 2.0
[16:04:11] TRACE |   FOUND[2] API keys
[16:04:11] TRACE |   FOUND[3] JWT tokens
[16:04:11] TRACE | === END FACT COVERAGE ===
[16:04:11] TRACE | === FACT COVERAGE ANALYSIS ===
[16:04:11] TRACE | Query: My API requests are getting 401 errors
[16:04:11] TRACE | Total key facts: 3
[16:04:11] TRACE | Found facts: 3/3
[16:04:11] TRACE |   FOUND[1] OAuth 2.0
[16:04:11] TRACE |   FOUND[2] API keys
[16:04:11] TRACE |   FOUND[3] JWT tokens
[16:04:11] TRACE | === END FACT COVERAGE ===
[16:04:11] TRACE | === FACT COVERAGE ANALYSIS ===
[16:04:11] TRACE | Query: auth methods
[16:04:11] TRACE | Total key facts: 3
[16:04:11] TRACE | Found facts: 3/3
[16:04:11] TRACE |   FOUND[1] OAuth 2.0
[16:04:11] TRACE |   FOUND[2] API keys
[16:04:11] TRACE |   FOUND[3] JWT tokens
[16:04:11] TRACE | === END FACT COVERAGE ===
[16:04:11] TRACE | === FACT COVERAGE ANALYSIS ===
[16:04:11] TRACE | Query: I'm integrating with CloudFlow API, what authentication options do I have?
[16:04:11] TRACE | Total key facts: 3
[16:04:11] TRACE | Found facts: 3/3
[16:04:11] TRACE |   FOUND[1] OAuth 2.0
[16:04:11] TRACE |   FOUND[2] API keys
[16:04:11] TRACE |   FOUND[3] JWT tokens
[16:04:11] TRACE | === END FACT COVERAGE ===
[16:04:11] TRACE | === FACT COVERAGE ANALYSIS ===
[16:04:11] TRACE | Query: Why isn't basic auth working?
[16:04:11] TRACE | Total key facts: 3
[16:04:11] TRACE | Found facts: 3/3
[16:04:11] TRACE |   FOUND[1] OAuth 2.0
[16:04:11] TRACE |   FOUND[2] API keys
[16:04:11] TRACE |   FOUND[3] JWT tokens
[16:04:11] TRACE | === END FACT COVERAGE ===
[16:04:11] TRACE | === FACT COVERAGE ANALYSIS ===
[16:04:11] TRACE | Query: What is PgBouncer and why is it used in CloudFlow?
[16:04:11] TRACE | Total key facts: 5
[16:04:11] TRACE | Found facts: 5/5
[16:04:11] TRACE |   FOUND[1] PgBouncer
[16:04:11] TRACE |   FOUND[2] connection pooling
[16:04:11] TRACE |   FOUND[3] max_db_connections = 100
[16:04:11] TRACE |   FOUND[4] default_pool_size = 25
[16:04:11] TRACE |   FOUND[5] pool_mode = transaction
[16:04:11] TRACE | === END FACT COVERAGE ===
[16:04:11] TRACE | === FACT COVERAGE ANALYSIS ===
[16:04:11] TRACE | Query: What's the purpose of the connection pooler?
[16:04:11] TRACE | Total key facts: 5
[16:04:11] TRACE | Found facts: 0/5
[16:04:11] TRACE | Missed facts: 5/5
[16:04:11] TRACE |   MISSED[1] PgBouncer
[16:04:11] TRACE |     -> Found in chunk_id=architecture_overview_fix_5 (rank not in top-5)
[16:04:11] TRACE |   MISSED[2] connection pooling
[16:04:11] TRACE |     -> Found in chunk_id=architecture_overview_fix_5 (rank not in top-5)
[16:04:11] TRACE |   MISSED[3] max_db_connections = 100
[16:04:11] TRACE |     -> Found in chunk_id=deployment_guide_fix_3 (rank not in top-5)
[16:04:11] TRACE |   MISSED[4] default_pool_size = 25
[16:04:11] TRACE |     -> Found in chunk_id=deployment_guide_fix_3 (rank not in top-5)
[16:04:11] TRACE |   MISSED[5] pool_mode = transaction
[16:04:11] TRACE |     -> Found in chunk_id=deployment_guide_fix_3 (rank not in top-5)
[16:04:11] TRACE | === END FACT COVERAGE ===
[16:04:11] TRACE | === FACT COVERAGE ANALYSIS ===
[16:04:11] TRACE | Query: Should I connect directly to PostgreSQL or through PgBouncer?
[16:04:11] TRACE | Total key facts: 5
[16:04:11] TRACE | Found facts: 5/5
[16:04:11] TRACE |   FOUND[1] PgBouncer
[16:04:11] TRACE |   FOUND[2] connection pooling
[16:04:11] TRACE |   FOUND[3] max_db_connections = 100
[16:04:11] TRACE |   FOUND[4] default_pool_size = 25
[16:04:11] TRACE |   FOUND[5] pool_mode = transaction
[16:04:11] TRACE | === END FACT COVERAGE ===
[16:04:11] TRACE | === FACT COVERAGE ANALYSIS ===
[16:04:11] TRACE | Query: pgbouncer purpose
[16:04:11] TRACE | Total key facts: 5
[16:04:11] TRACE | Found facts: 5/5
[16:04:11] TRACE |   FOUND[1] PgBouncer
[16:04:11] TRACE |   FOUND[2] connection pooling
[16:04:11] TRACE |   FOUND[3] max_db_connections = 100
[16:04:11] TRACE |   FOUND[4] default_pool_size = 25
[16:04:11] TRACE |   FOUND[5] pool_mode = transaction
[16:04:11] TRACE | === END FACT COVERAGE ===
[16:04:11] TRACE | === FACT COVERAGE ANALYSIS ===
[16:04:11] TRACE | Query: Optimizing database connections, what role does PgBouncer play?
[16:04:11] TRACE | Total key facts: 5
[16:04:11] TRACE | Found facts: 5/5
[16:04:11] TRACE |   FOUND[1] PgBouncer
[16:04:11] TRACE |   FOUND[2] connection pooling
[16:04:11] TRACE |   FOUND[3] max_db_connections = 100
[16:04:11] TRACE |   FOUND[4] default_pool_size = 25
[16:04:11] TRACE |   FOUND[5] pool_mode = transaction
[16:04:11] TRACE | === END FACT COVERAGE ===
[16:04:11] TRACE | === FACT COVERAGE ANALYSIS ===
[16:04:11] TRACE | Query: Why can't I connect directly to the database?
[16:04:11] TRACE | Total key facts: 5
[16:04:11] TRACE | Found facts: 2/5
[16:04:11] TRACE |   FOUND[1] PgBouncer
[16:04:11] TRACE |   FOUND[2] connection pooling
[16:04:11] TRACE | Missed facts: 3/5
[16:04:11] TRACE |   MISSED[1] max_db_connections = 100
[16:04:11] TRACE |     -> Found in chunk_id=deployment_guide_fix_3 (rank not in top-5)
[16:04:11] TRACE |   MISSED[2] default_pool_size = 25
[16:04:11] TRACE |     -> Found in chunk_id=deployment_guide_fix_3 (rank not in top-5)
[16:04:11] TRACE |   MISSED[3] pool_mode = transaction
[16:04:11] TRACE |     -> Found in chunk_id=deployment_guide_fix_3 (rank not in top-5)
[16:04:11] TRACE | === END FACT COVERAGE ===
[16:04:11] TRACE | === FACT COVERAGE ANALYSIS ===
[16:04:11] TRACE | Query: How do I implement retry logic for failed workflow steps?
[16:04:11] TRACE | Total key facts: 4
[16:04:11] TRACE | Found facts: 0/4
[16:04:11] TRACE | Missed facts: 4/4
[16:04:11] TRACE |   MISSED[1] max_attempts: 3
[16:04:11] TRACE |     -> Found in chunk_id=user_guide_fix_5 (rank not in top-5)
[16:04:11] TRACE |   MISSED[2] backoff_type: exponential
[16:04:11] TRACE |     -> Found in chunk_id=user_guide_fix_7 (rank not in top-5)
[16:04:11] TRACE |   MISSED[3] initial_interval: 1000
[16:04:11] TRACE |     -> Found in chunk_id=user_guide_fix_5 (rank not in top-5)
[16:04:11] TRACE |   MISSED[4] max retries: 3
[16:04:11] TRACE |     -> Found in chunk_id=troubleshooting_guide_fix_5 (rank not in top-5)
[16:04:11] TRACE | === END FACT COVERAGE ===
[16:04:11] TRACE | === FACT COVERAGE ANALYSIS ===
[16:04:11] TRACE | Query: What's the retry strategy for transient failures?
[16:04:11] TRACE | Total key facts: 4
[16:04:11] TRACE | Found facts: 0/4
[16:04:11] TRACE | Missed facts: 4/4
[16:04:11] TRACE |   MISSED[1] max_attempts: 3
[16:04:11] TRACE |     -> Found in chunk_id=user_guide_fix_5 (rank not in top-5)
[16:04:11] TRACE |   MISSED[2] backoff_type: exponential
[16:04:11] TRACE |     -> Found in chunk_id=user_guide_fix_7 (rank not in top-5)
[16:04:11] TRACE |   MISSED[3] initial_interval: 1000
[16:04:11] TRACE |     -> Found in chunk_id=user_guide_fix_5 (rank not in top-5)
[16:04:11] TRACE |   MISSED[4] max retries: 3
[16:04:11] TRACE |     -> Found in chunk_id=troubleshooting_guide_fix_5 (rank not in top-5)
[16:04:11] TRACE | === END FACT COVERAGE ===
[16:04:11] TRACE | === FACT COVERAGE ANALYSIS ===
[16:04:11] TRACE | Query: My workflow fails on temporary network errors
[16:04:11] TRACE | Total key facts: 4
[16:04:11] TRACE | Found facts: 1/4
[16:04:11] TRACE |   FOUND[1] max retries: 3
[16:04:11] TRACE | Missed facts: 3/4
[16:04:11] TRACE |   MISSED[1] max_attempts: 3
[16:04:11] TRACE |     -> Found in chunk_id=user_guide_fix_5 (rank not in top-5)
[16:04:11] TRACE |   MISSED[2] backoff_type: exponential
[16:04:11] TRACE |     -> Found in chunk_id=user_guide_fix_7 (rank not in top-5)
[16:04:11] TRACE |   MISSED[3] initial_interval: 1000
[16:04:11] TRACE |     -> Found in chunk_id=user_guide_fix_5 (rank not in top-5)
[16:04:11] TRACE | === END FACT COVERAGE ===
[16:04:11] TRACE | === FACT COVERAGE ANALYSIS ===
[16:04:11] TRACE | Query: retry config
[16:04:11] TRACE | Total key facts: 4
[16:04:11] TRACE | Found facts: 2/4
[16:04:11] TRACE |   FOUND[1] max_attempts: 3
[16:04:11] TRACE |   FOUND[2] initial_interval: 1000
[16:04:11] TRACE | Missed facts: 2/4
[16:04:11] TRACE |   MISSED[1] backoff_type: exponential
[16:04:11] TRACE |     -> Found in chunk_id=user_guide_fix_7 (rank not in top-5)
[16:04:11] TRACE |   MISSED[2] max retries: 3
[16:04:11] TRACE |     -> Found in chunk_id=troubleshooting_guide_fix_5 (rank not in top-5)
[16:04:11] TRACE | === END FACT COVERAGE ===
[16:04:11] TRACE | === FACT COVERAGE ANALYSIS ===
[16:04:11] TRACE | Query: I want workflows to automatically retry on errors, what are the options?
[16:04:11] TRACE | Total key facts: 4
[16:04:11] TRACE | Found facts: 0/4
[16:04:11] TRACE | Missed facts: 4/4
[16:04:11] TRACE |   MISSED[1] max_attempts: 3
[16:04:11] TRACE |     -> Found in chunk_id=user_guide_fix_5 (rank not in top-5)
[16:04:11] TRACE |   MISSED[2] backoff_type: exponential
[16:04:11] TRACE |     -> Found in chunk_id=user_guide_fix_7 (rank not in top-5)
[16:04:11] TRACE |   MISSED[3] initial_interval: 1000
[16:04:11] TRACE |     -> Found in chunk_id=user_guide_fix_5 (rank not in top-5)
[16:04:11] TRACE |   MISSED[4] max retries: 3
[16:04:11] TRACE |     -> Found in chunk_id=troubleshooting_guide_fix_5 (rank not in top-5)
[16:04:11] TRACE | === END FACT COVERAGE ===
[16:04:11] TRACE | === FACT COVERAGE ANALYSIS ===
[16:04:11] TRACE | Query: Why doesn't my workflow retry after failing?
[16:04:11] TRACE | Total key facts: 4
[16:04:11] TRACE | Found facts: 0/4
[16:04:11] TRACE | Missed facts: 4/4
[16:04:11] TRACE |   MISSED[1] max_attempts: 3
[16:04:11] TRACE |     -> Found in chunk_id=user_guide_fix_5 (rank not in top-5)
[16:04:11] TRACE |   MISSED[2] backoff_type: exponential
[16:04:11] TRACE |     -> Found in chunk_id=user_guide_fix_7 (rank not in top-5)
[16:04:11] TRACE |   MISSED[3] initial_interval: 1000
[16:04:11] TRACE |     -> Found in chunk_id=user_guide_fix_5 (rank not in top-5)
[16:04:11] TRACE |   MISSED[4] max retries: 3
[16:04:11] TRACE |     -> Found in chunk_id=troubleshooting_guide_fix_5 (rank not in top-5)
[16:04:11] TRACE | === END FACT COVERAGE ===
[16:04:11] TRACE | === FACT COVERAGE ANALYSIS ===
[16:04:11] TRACE | Query: What Helm chart repository should I use for deployment?
[16:04:11] TRACE | Total key facts: 1
[16:04:11] TRACE | Found facts: 0/1
[16:04:11] TRACE | Missed facts: 1/1
[16:04:11] TRACE |   MISSED[1] helm repo add cloudflow https://charts.cloudflow.io
[16:04:11] TRACE |     -> Found in chunk_id=deployment_guide_fix_1 (rank not in top-5)
[16:04:11] TRACE | === END FACT COVERAGE ===
[16:04:11] TRACE | === FACT COVERAGE ANALYSIS ===
[16:04:11] TRACE | Query: Where is the CloudFlow Helm chart hosted?
[16:04:11] TRACE | Total key facts: 1
[16:04:11] TRACE | Found facts: 0/1
[16:04:11] TRACE | Missed facts: 1/1
[16:04:11] TRACE |   MISSED[1] helm repo add cloudflow https://charts.cloudflow.io
[16:04:11] TRACE |     -> Found in chunk_id=deployment_guide_fix_1 (rank not in top-5)
[16:04:11] TRACE | === END FACT COVERAGE ===
[16:04:11] TRACE | === FACT COVERAGE ANALYSIS ===
[16:04:11] TRACE | Query: helm repo add is failing, what's the correct URL?
[16:04:11] TRACE | Total key facts: 1
[16:04:11] TRACE | Found facts: 1/1
[16:04:11] TRACE |   FOUND[1] helm repo add cloudflow https://charts.cloudflow.io
[16:04:11] TRACE | === END FACT COVERAGE ===
[16:04:11] TRACE | === FACT COVERAGE ANALYSIS ===
[16:04:11] TRACE | Query: helm repo
[16:04:11] TRACE | Total key facts: 1
[16:04:11] TRACE | Found facts: 1/1
[16:04:11] TRACE |   FOUND[1] helm repo add cloudflow https://charts.cloudflow.io
[16:04:11] TRACE | === END FACT COVERAGE ===
[16:04:11] TRACE | === FACT COVERAGE ANALYSIS ===
[16:04:11] TRACE | Query: Setting up deployment pipeline, which Helm repository has CloudFlow charts?
[16:04:11] TRACE | Total key facts: 1
[16:04:11] TRACE | Found facts: 0/1
[16:04:11] TRACE | Missed facts: 1/1
[16:04:11] TRACE |   MISSED[1] helm repo add cloudflow https://charts.cloudflow.io
[16:04:11] TRACE |     -> Found in chunk_id=deployment_guide_fix_1 (rank not in top-5)
[16:04:11] TRACE | === END FACT COVERAGE ===
[16:04:11] TRACE | === FACT COVERAGE ANALYSIS ===
[16:04:11] TRACE | Query: Why can't I find CloudFlow in the official Helm hub?
[16:04:11] TRACE | Total key facts: 1
[16:04:11] TRACE | Found facts: 1/1
[16:04:11] TRACE |   FOUND[1] helm repo add cloudflow https://charts.cloudflow.io
[16:04:11] TRACE | === END FACT COVERAGE ===
[16:04:11] TRACE | === FACT COVERAGE ANALYSIS ===
[16:04:11] TRACE | Query: What is the minimum scheduling interval for workflows?
[16:04:11] TRACE | Total key facts: 2
[16:04:11] TRACE | Found facts: 0/2
[16:04:11] TRACE | Missed facts: 2/2
[16:04:11] TRACE |   MISSED[1] minimum scheduling interval is 1 minute
[16:04:11] TRACE |   MISSED[2] The minimum scheduling interval is **1 minute**
[16:04:11] TRACE |     -> Found in chunk_id=user_guide_fix_4 (rank not in top-5)
[16:04:11] TRACE | === END FACT COVERAGE ===
[16:04:11] TRACE | === FACT COVERAGE ANALYSIS ===
[16:04:11] TRACE | Query: How frequently can I schedule a workflow?
[16:04:11] TRACE | Total key facts: 2
[16:04:11] TRACE | Found facts: 0/2
[16:04:11] TRACE | Missed facts: 2/2
[16:04:11] TRACE |   MISSED[1] minimum scheduling interval is 1 minute
[16:04:11] TRACE |   MISSED[2] The minimum scheduling interval is **1 minute**
[16:04:11] TRACE |     -> Found in chunk_id=user_guide_fix_4 (rank not in top-5)
[16:04:11] TRACE | === END FACT COVERAGE ===
[16:04:11] TRACE | === FACT COVERAGE ANALYSIS ===
[16:04:11] TRACE | Query: My every-30-seconds schedule is being rejected
[16:04:11] TRACE | Total key facts: 2
[16:04:11] TRACE | Found facts: 0/2
[16:04:11] TRACE | Missed facts: 2/2
[16:04:11] TRACE |   MISSED[1] minimum scheduling interval is 1 minute
[16:04:11] TRACE |   MISSED[2] The minimum scheduling interval is **1 minute**
[16:04:11] TRACE |     -> Found in chunk_id=user_guide_fix_4 (rank not in top-5)
[16:04:11] TRACE | === END FACT COVERAGE ===
[16:04:11] TRACE | === FACT COVERAGE ANALYSIS ===
[16:04:11] TRACE | Query: min schedule interval
[16:04:11] TRACE | Total key facts: 2
[16:04:11] TRACE | Found facts: 0/2
[16:04:11] TRACE | Missed facts: 2/2
[16:04:11] TRACE |   MISSED[1] minimum scheduling interval is 1 minute
[16:04:11] TRACE |   MISSED[2] The minimum scheduling interval is **1 minute**
[16:04:11] TRACE |     -> Found in chunk_id=user_guide_fix_4 (rank not in top-5)
[16:04:11] TRACE | === END FACT COVERAGE ===
[16:04:11] TRACE | === FACT COVERAGE ANALYSIS ===
[16:04:11] TRACE | Query: I need near real-time execution, what's the fastest schedule I can set?
[16:04:11] TRACE | Total key facts: 2
[16:04:11] TRACE | Found facts: 0/2
[16:04:11] TRACE | Missed facts: 2/2
[16:04:11] TRACE |   MISSED[1] minimum scheduling interval is 1 minute
[16:04:11] TRACE |   MISSED[2] The minimum scheduling interval is **1 minute**
[16:04:11] TRACE |     -> Found in chunk_id=user_guide_fix_4 (rank not in top-5)
[16:04:11] TRACE | === END FACT COVERAGE ===
[16:04:11] TRACE | === FACT COVERAGE ANALYSIS ===
[16:04:11] TRACE | Query: Why can't I schedule workflows every 30 seconds?
[16:04:11] TRACE | Total key facts: 2
[16:04:11] TRACE | Found facts: 0/2
[16:04:11] TRACE | Missed facts: 2/2
[16:04:11] TRACE |   MISSED[1] minimum scheduling interval is 1 minute
[16:04:11] TRACE |   MISSED[2] The minimum scheduling interval is **1 minute**
[16:04:11] TRACE |     -> Found in chunk_id=user_guide_fix_4 (rank not in top-5)
[16:04:11] TRACE | === END FACT COVERAGE ===
[16:04:11] INFO  |   k=5 exact_match: 56.6% (30/53)
[16:04:11] INFO  |     synonym: 41.5%
[16:04:11] INFO  |     problem: 56.6%
[16:04:11] INFO  |     casual: 50.9%
[16:04:11] INFO  |     contextual: 45.3%
[16:04:11] INFO  |     negation: 45.3%

================================================================================
[16:04:11] INFO  | RETRIEVAL STRATEGY: enriched_hybrid
================================================================================
[16:04:11] INFO  | Loading embedder: BAAI/bge-base-en-v1.5
[16:04:14] PROG  | [  4/  4] ████████████████████ 100.0% | enriched_hybrid_bge-base-en-v1.5_fixed_512_0pct
[16:04:14] DEBUG | Chunking document 'api_reference': 1792 words, 16663 chars
[16:04:14] DEBUG |   Chunk 0: chars [0-3160] words=376 tokens≈500 preview='# CloudFlow API Reference Version 2.1.0 | Last Updated: January 2026 ## Overview The CloudFlow API i...'
[16:04:14] DEBUG |   Chunk 1: chars [3161-6111] words=379 tokens≈504 preview='## Rate Limiting To ensure fair usage and system stability, CloudFlow enforces rate limits on all AP...'
[16:04:14] DEBUG |   Chunk 2: chars [6112-8827] words=296 tokens≈393 preview='**Endpoint:** `POST /workflows` **Request Body:** ```json { "name": "Email Campaign Automation", "de...'
[16:04:14] DEBUG |   Chunk 3: chars [8828-11402] words=269 tokens≈357 preview='**Endpoint:** `GET /pipelines/{pipeline_id}/executions` **Query Parameters:** - `status` (optional):...'
[16:04:14] DEBUG |   Chunk 4: chars [11403-14151] words=365 tokens≈485 preview='### Error Response Format ```json { "error": { "code": "invalid_parameter", "message": "The 'limit' ...'
[16:04:14] DEBUG |   Chunk 5: chars [14152-15213] words=107 tokens≈142 preview='**Supported Events:** - `workflow.started` - `workflow.completed` - `workflow.failed` - `pipeline.co...'
[16:04:14] DEBUG | Created 6 chunks for document 'api_reference'
[16:04:14] DEBUG | Chunking document 'architecture_overview': 4607 words, 36722 chars
[16:04:14] DEBUG |   Chunk 0: chars [0-3230] words=370 tokens≈492 preview='# CloudFlow Platform - System Architecture Overview **Document Version:** 2.3.1 **Last Updated:** Ja...'
[16:04:14] DEBUG |   Chunk 1: chars [3231-5982] words=380 tokens≈505 preview='**Technology**: Node.js with Express.js framework **Replicas**: 12 pods (production), auto-scaling 8...'
[16:04:14] DEBUG |   Chunk 2: chars [5983-7875] words=248 tokens≈329 preview='**Technology**: Node.js with TypeScript, Bull queue library **Replicas**: 16 pods (production), auto...'
[16:04:14] DEBUG |   Chunk 3: chars [7876-9478] words=219 tokens≈291 preview='**Technology**: Go with distributed locking via Redis **Replicas**: 4 pods (production), active-pass...'
[16:04:14] DEBUG |   Chunk 4: chars [9479-11878] words=354 tokens≈470 preview='**Technology**: Node.js with worker pool pattern **Replicas**: 8 pods (production), auto-scaling 6-1...'
[16:04:14] DEBUG |   Chunk 5: chars [11879-14634] words=368 tokens≈489 preview='Success/failure events published back to Kafka ``` **Event Schema**: ```json { "event_id": "uuid-v4"...'
[16:04:14] DEBUG |   Chunk 6: chars [14635-17394] words=384 tokens≈510 preview='**Session Storage**: - Key pattern: `session:{user_id}` - TTL: 15 minutes (aligned with JWT expiry) ...'
[16:04:14] DEBUG |   Chunk 7: chars [17395-20141] words=381 tokens≈506 preview='records: 500 - Session timeout: 30 seconds --- ## Message Queue Patterns ### Pub/Sub Pattern Used fo...'
[16:04:14] DEBUG |   Chunk 8: chars [20142-22843] words=384 tokens≈510 preview='### Cache Invalidation Strategies **Time-Based Expiration (TTL)**: - Short-lived data: 5-15 minutes ...'
[16:04:14] DEBUG |   Chunk 9: chars [22844-25362] words=384 tokens≈510 preview='per API key (default: 1000 RPM) ### Secrets Management **HashiCorp Vault Integration**: - Dynamic da...'
[16:04:14] DEBUG |   Chunk 10: chars [25363-27842] words=384 tokens≈510 preview='- Execution start rate: 500 per second - Completion rate: 450 per second (average 2-second execution...'
[16:04:14] DEBUG |   Chunk 11: chars [27843-30286] words=375 tokens≈498 preview='Service (2) - PostgreSQL Primary - PostgreSQL Replica - PostgreSQL Replica - Redis Primary (2) - Red...'
[16:04:14] DEBUG |   Chunk 12: chars [30287-32834] words=376 tokens≈500 preview='Resume normal operations - Recovery time: 1-3 hours depending on data volume - Data loss: None if co...'
[16:04:14] DEBUG | Created 13 chunks for document 'architecture_overview'
[16:04:14] DEBUG | Chunking document 'deployment_guide': 2777 words, 25837 chars
[16:04:14] DEBUG |   Chunk 0: chars [0-3165] words=384 tokens≈510 preview='# CloudFlow Platform - Deployment and Operations Guide **Version:** 2.4.0 **Last Updated:** January ...'
[16:04:14] DEBUG |   Chunk 1: chars [3166-6560] words=384 tokens≈510 preview='create cluster -f cluster-config.yaml ``` ### Storage Configuration Install the EBS CSI driver for p...'
[16:04:14] DEBUG |   Chunk 2: chars [6561-8755] words=251 tokens≈333 preview='- secretName: cloudflow-tls hosts: - api.cloudflow.io healthCheck: enabled: true livenessProbe: path...'
[16:04:14] DEBUG |   Chunk 3: chars [8756-12138] words=384 tokens≈510 preview='Deploy PostgreSQL using the Bitnami Helm chart: ```yaml # postgres-values.yaml global: postgresql: a...'
[16:04:14] DEBUG |   Chunk 4: chars [12139-14139] words=204 tokens≈271 preview='--set prometheus.prometheusSpec.retention=30d \ --set prometheus.prometheusSpec.storageSpec.volumeCl...'
[16:04:14] DEBUG |   Chunk 5: chars [14140-17194] words=372 tokens≈494 preview='Configure log aggregation: ```bash # Install Fluent Bit for log forwarding helm install fluent-bit f...'
[16:04:14] DEBUG |   Chunk 6: chars [17195-20069] words=384 tokens≈510 preview='**Verify Service Health**: ```bash kubectl get pods -n cloudflow-prod curl https://api.cloudflow.io/...'
[16:04:14] DEBUG |   Chunk 7: chars [20070-23221] words=384 tokens≈510 preview='podSelector: matchLabels: app: cloudflow policyTypes: - Ingress - Egress ingress: - from: - namespac...'
[16:04:14] DEBUG |   Chunk 8: chars [23222-23448] words=30 tokens≈39 preview='UTC - **Secondary**: Wednesday 10:00-12:00 UTC All deployments should target these windows to minimi...'
[16:04:14] DEBUG | Created 9 chunks for document 'deployment_guide'
[16:04:14] DEBUG | Chunking document 'troubleshooting_guide': 4032 words, 32183 chars
[16:04:14] DEBUG |   Chunk 0: chars [0-2666] words=320 tokens≈425 preview='# CloudFlow Platform Troubleshooting Guide **Version:** 3.2.1 **Last Updated:** January 2026 **Audie...'
[16:04:14] DEBUG |   Chunk 1: chars [2667-5007] words=321 tokens≈426 preview='**Diagnosis:** ```bash # Check system time timedatectl status # Compare with NTP server ntpdate -q p...'
[16:04:14] DEBUG |   Chunk 2: chars [5008-8037] words=376 tokens≈500 preview='Review Query Execution Plans** ```sql -- Connect to CloudFlow database cloudflow db connect --readon...'
[16:04:14] DEBUG |   Chunk 3: chars [8038-10862] words=356 tokens≈473 preview='Workflow Context Accumulation** Large workflow executions may accumulate state in memory. **Solution...'
[16:04:14] DEBUG |   Chunk 4: chars [10863-13827] words=384 tokens≈510 preview='**Implement connection pooling optimization:** ```bash # Use PgBouncer for connection pooling kubect...'
[16:04:14] DEBUG |   Chunk 5: chars [13828-16401] words=334 tokens≈444 preview='exec_7h3j6k9m2n --show-bottlenecks ``` #### Solutions **1. Increase workflow timeout (if justified):...'
[16:04:14] DEBUG |   Chunk 6: chars [16402-19022] words=329 tokens≈437 preview='Data Validation Errors** ``` ValidationError: Field 'customer_id' is required but missing in 234 rec...'
[16:04:14] DEBUG |   Chunk 7: chars [19023-20595] words=194 tokens≈258 preview='Waiting {retry_after} seconds...") time.sleep(retry_after) continue remaining = int(response.headers...'
[16:04:14] DEBUG |   Chunk 8: chars [20596-23454] words=384 tokens≈510 preview='Leverage caching:** ```bash # Enable client-side caching export CLOUDFLOW_CACHE_ENABLED=true export ...'
[16:04:14] DEBUG |   Chunk 9: chars [23455-26239] words=384 tokens≈510 preview='in" # Find workflow failures by ID kubectl logs -n cloudflow deployment/cloudflow-workflow-engine | ...'
[16:04:14] DEBUG |   Chunk 10: chars [26240-29039] words=384 tokens≈510 preview='database nc -zv cloudflow-db.internal.company.com 5432 # Trace route traceroute api.cloudflow.io # C...'
[16:04:14] DEBUG |   Chunk 11: chars [29040-31206] words=266 tokens≈353 preview='outage" # Create war room cloudflow incident war-room create incident-2024012401 ``` **For SEV-2 (Hi...'
[16:04:14] DEBUG | Created 12 chunks for document 'troubleshooting_guide'
[16:04:14] DEBUG | Chunking document 'user_guide': 3735 words, 31582 chars
[16:04:14] DEBUG |   Chunk 0: chars [0-2617] words=375 tokens≈498 preview='# CloudFlow User Guide Welcome to CloudFlow, the modern workflow automation platform that helps you ...'
[16:04:14] DEBUG |   Chunk 1: chars [2618-5224] words=348 tokens≈462 preview='Open the Visual Editor by clicking **"Create Workflow"** or editing an existing workflow 2. Add a tr...'
[16:04:14] DEBUG |   Chunk 2: chars [5225-8372] words=381 tokens≈506 preview='### HTTP Requests Make HTTP requests to any API endpoint: **Configuration:** - **Method**: GET, POST...'
[16:04:14] DEBUG |   Chunk 3: chars [8373-11268] words=336 tokens≈446 preview='``` **Slack App Integration:** - Install the CloudFlow Slack app in your workspace - Authenticate on...'
[16:04:14] DEBUG |   Chunk 4: chars [11269-13371] words=375 tokens≈498 preview='### Cron Syntax Use standard cron expressions to define schedules: ``` * * * * * ┬ ┬ ┬ ┬ ┬ │ │ │ │ │...'
[16:04:14] DEBUG |   Chunk 5: chars [13372-16105] words=383 tokens≈509 preview='Save and activate **Testing Schedules:** Use the built-in schedule calculator to preview upcoming ex...'
[16:04:14] DEBUG |   Chunk 6: chars [16106-18686] words=363 tokens≈482 preview='Filter by error type, date range, or execution ID 3. Click an execution to view full details 4. Clic...'
[16:04:14] DEBUG |   Chunk 7: chars [18687-21448] words=364 tokens≈484 preview='Handle Errors Gracefully Always implement error handling for external API calls and database operati...'
[16:04:14] DEBUG |   Chunk 8: chars [21449-24677] words=384 tokens≈510 preview='Version Control For critical workflows: - Export YAML definitions regularly - Store in version contr...'
[16:04:14] DEBUG |   Chunk 9: chars [24678-27975] words=371 tokens≈493 preview='Requester: {{trigger.body.requester}} Description: {{trigger.body.description}} Approve: https://app...'
[16:04:14] DEBUG |   Chunk 10: chars [27976-28481] words=55 tokens≈73 preview='- **Documentation**: https://docs.cloudflow.io - **Community Forum**: https://community.cloudflow.io...'
[16:04:14] DEBUG | Created 11 chunks for document 'user_guide'
[16:04:14] INFO  |   Created 51 chunks
    [enriched] chunks=51 new=0 cache_hits=51 time=0.0s avg=0.0ms/chunk
[16:04:15] INFO  |   Indexed in 0.97s
[16:04:15] TRACE | === FACT COVERAGE ANALYSIS ===
[16:04:15] TRACE | Query: What is the API rate limit per minute?
[16:04:15] TRACE | Total key facts: 2
[16:04:15] TRACE | Found facts: 2/2
[16:04:15] TRACE |   FOUND[1] 100 requests per minute per authenticated user
[16:04:15] TRACE |   FOUND[2] 20 requests per minute for unauthenticated requests
[16:04:15] TRACE | === END FACT COVERAGE ===
[16:04:15] TRACE | === FACT COVERAGE ANALYSIS ===
[16:04:15] TRACE | Query: How many requests can I make per minute to the API?
[16:04:15] TRACE | Total key facts: 2
[16:04:15] TRACE | Found facts: 2/2
[16:04:15] TRACE |   FOUND[1] 100 requests per minute per authenticated user
[16:04:15] TRACE |   FOUND[2] 20 requests per minute for unauthenticated requests
[16:04:15] TRACE | === END FACT COVERAGE ===
[16:04:15] TRACE | === FACT COVERAGE ANALYSIS ===
[16:04:15] TRACE | Query: My API calls are getting blocked, what's the limit?
[16:04:15] TRACE | Total key facts: 2
[16:04:15] TRACE | Found facts: 2/2
[16:04:15] TRACE |   FOUND[1] 100 requests per minute per authenticated user
[16:04:15] TRACE |   FOUND[2] 20 requests per minute for unauthenticated requests
[16:04:15] TRACE | === END FACT COVERAGE ===
[16:04:15] TRACE | === FACT COVERAGE ANALYSIS ===
[16:04:15] TRACE | Query: api rate limit
[16:04:15] TRACE | Total key facts: 2
[16:04:15] TRACE | Found facts: 2/2
[16:04:15] TRACE |   FOUND[1] 100 requests per minute per authenticated user
[16:04:15] TRACE |   FOUND[2] 20 requests per minute for unauthenticated requests
[16:04:15] TRACE | === END FACT COVERAGE ===
[16:04:15] TRACE | === FACT COVERAGE ANALYSIS ===
[16:04:15] TRACE | Query: I'm building a batch job that calls CloudFlow API repeatedly, what rate limit should I expect?
[16:04:15] TRACE | Total key facts: 2
[16:04:15] TRACE | Found facts: 2/2
[16:04:15] TRACE |   FOUND[1] 100 requests per minute per authenticated user
[16:04:15] TRACE |   FOUND[2] 20 requests per minute for unauthenticated requests
[16:04:15] TRACE | === END FACT COVERAGE ===
[16:04:15] TRACE | === FACT COVERAGE ANALYSIS ===
[16:04:15] TRACE | Query: Why is the API rejecting my requests after 100 calls?
[16:04:15] TRACE | Total key facts: 2
[16:04:15] TRACE | Found facts: 2/2
[16:04:15] TRACE |   FOUND[1] 100 requests per minute per authenticated user
[16:04:15] TRACE |   FOUND[2] 20 requests per minute for unauthenticated requests
[16:04:15] TRACE | === END FACT COVERAGE ===
[16:04:15] TRACE | === FACT COVERAGE ANALYSIS ===
[16:04:15] TRACE | Query: How do I fix 429 Too Many Requests errors?
[16:04:15] TRACE | Total key facts: 5
[16:04:15] TRACE | Found facts: 4/5
[16:04:15] TRACE |   FOUND[1] 429 Too Many Requests
[16:04:15] TRACE |   FOUND[2] X-RateLimit-Remaining
[16:04:15] TRACE |   FOUND[3] Retry-After
[16:04:15] TRACE |   FOUND[4] Implement exponential backoff when receiving 429 responses
[16:04:15] TRACE | Missed facts: 1/5
[16:04:15] TRACE |   MISSED[1] Monitor X-RateLimit-Remaining header values
[16:04:15] TRACE | === END FACT COVERAGE ===
[16:04:15] TRACE | === FACT COVERAGE ANALYSIS ===
[16:04:15] TRACE | Query: What should I do when I get rate limited?
[16:04:15] TRACE | Total key facts: 5
[16:04:15] TRACE | Found facts: 3/5
[16:04:15] TRACE |   FOUND[1] 429 Too Many Requests
[16:04:15] TRACE |   FOUND[2] X-RateLimit-Remaining
[16:04:15] TRACE |   FOUND[3] Implement exponential backoff when receiving 429 responses
[16:04:15] TRACE | Missed facts: 2/5
[16:04:15] TRACE |   MISSED[1] Retry-After
[16:04:15] TRACE |     -> Found in chunk_id=troubleshooting_guide_fix_6 (rank not in top-5)
[16:04:15] TRACE |   MISSED[2] Monitor X-RateLimit-Remaining header values
[16:04:15] TRACE | === END FACT COVERAGE ===
[16:04:15] TRACE | === FACT COVERAGE ANALYSIS ===
[16:04:15] TRACE | Query: My requests keep failing with 429 status code
[16:04:15] TRACE | Total key facts: 5
[16:04:15] TRACE | Found facts: 4/5
[16:04:15] TRACE |   FOUND[1] 429 Too Many Requests
[16:04:15] TRACE |   FOUND[2] X-RateLimit-Remaining
[16:04:15] TRACE |   FOUND[3] Retry-After
[16:04:15] TRACE |   FOUND[4] Implement exponential backoff when receiving 429 responses
[16:04:15] TRACE | Missed facts: 1/5
[16:04:15] TRACE |   MISSED[1] Monitor X-RateLimit-Remaining header values
[16:04:15] TRACE | === END FACT COVERAGE ===
[16:04:15] TRACE | === FACT COVERAGE ANALYSIS ===
[16:04:15] TRACE | Query: 429 error fix
[16:04:15] TRACE | Total key facts: 5
[16:04:15] TRACE | Found facts: 3/5
[16:04:15] TRACE |   FOUND[1] 429 Too Many Requests
[16:04:15] TRACE |   FOUND[2] X-RateLimit-Remaining
[16:04:15] TRACE |   FOUND[3] Retry-After
[16:04:15] TRACE | Missed facts: 2/5
[16:04:15] TRACE |   MISSED[1] Monitor X-RateLimit-Remaining header values
[16:04:15] TRACE |   MISSED[2] Implement exponential backoff when receiving 429 responses
[16:04:15] TRACE |     -> Found in chunk_id=api_reference_fix_1 (rank not in top-5)
[16:04:15] TRACE | === END FACT COVERAGE ===
[16:04:15] TRACE | === FACT COVERAGE ANALYSIS ===
[16:04:15] TRACE | Query: I'm getting throttled by CloudFlow API, how can I handle this in my application?
[16:04:15] TRACE | Total key facts: 5
[16:04:15] TRACE | Found facts: 3/5
[16:04:15] TRACE |   FOUND[1] 429 Too Many Requests
[16:04:15] TRACE |   FOUND[2] X-RateLimit-Remaining
[16:04:15] TRACE |   FOUND[3] Implement exponential backoff when receiving 429 responses
[16:04:15] TRACE | Missed facts: 2/5
[16:04:15] TRACE |   MISSED[1] Retry-After
[16:04:15] TRACE |     -> Found in chunk_id=troubleshooting_guide_fix_6 (rank not in top-5)
[16:04:15] TRACE |   MISSED[2] Monitor X-RateLimit-Remaining header values
[16:04:15] TRACE | === END FACT COVERAGE ===
[16:04:15] TRACE | === FACT COVERAGE ANALYSIS ===
[16:04:15] TRACE | Query: Why am I being blocked with rate limit errors?
[16:04:15] TRACE | Total key facts: 5
[16:04:15] TRACE | Found facts: 4/5
[16:04:15] TRACE |   FOUND[1] 429 Too Many Requests
[16:04:15] TRACE |   FOUND[2] X-RateLimit-Remaining
[16:04:15] TRACE |   FOUND[3] Retry-After
[16:04:15] TRACE |   FOUND[4] Implement exponential backoff when receiving 429 responses
[16:04:15] TRACE | Missed facts: 1/5
[16:04:15] TRACE |   MISSED[1] Monitor X-RateLimit-Remaining header values
[16:04:15] TRACE | === END FACT COVERAGE ===
[16:04:15] TRACE | === FACT COVERAGE ANALYSIS ===
[16:04:15] TRACE | Query: What is the JWT token expiration time?
[16:04:15] TRACE | Total key facts: 3
[16:04:15] TRACE | Found facts: 2/3
[16:04:15] TRACE |   FOUND[1] 3600 seconds
[16:04:15] TRACE |   FOUND[2] All tokens expire after 3600 seconds
[16:04:15] TRACE | Missed facts: 1/3
[16:04:15] TRACE |   MISSED[1] max 3600 seconds from iat
[16:04:15] TRACE | === END FACT COVERAGE ===
[16:04:15] TRACE | === FACT COVERAGE ANALYSIS ===
[16:04:15] TRACE | Query: How long do access tokens last?
[16:04:15] TRACE | Total key facts: 3
[16:04:15] TRACE | Found facts: 2/3
[16:04:15] TRACE |   FOUND[1] 3600 seconds
[16:04:15] TRACE |   FOUND[2] All tokens expire after 3600 seconds
[16:04:15] TRACE | Missed facts: 1/3
[16:04:15] TRACE |   MISSED[1] max 3600 seconds from iat
[16:04:15] TRACE | === END FACT COVERAGE ===
[16:04:15] TRACE | === FACT COVERAGE ANALYSIS ===
[16:04:15] TRACE | Query: My authentication keeps expiring after an hour
[16:04:15] TRACE | Total key facts: 3
[16:04:15] TRACE | Found facts: 1/3
[16:04:15] TRACE |   FOUND[1] 3600 seconds
[16:04:15] TRACE | Missed facts: 2/3
[16:04:15] TRACE |   MISSED[1] max 3600 seconds from iat
[16:04:15] TRACE |   MISSED[2] All tokens expire after 3600 seconds
[16:04:15] TRACE |     -> Found in chunk_id=api_reference_fix_0 (rank not in top-5)
[16:04:15] TRACE | === END FACT COVERAGE ===
[16:04:15] TRACE | === FACT COVERAGE ANALYSIS ===
[16:04:15] TRACE | Query: token expiry time
[16:04:15] TRACE | Total key facts: 3
[16:04:15] TRACE | Found facts: 2/3
[16:04:15] TRACE |   FOUND[1] 3600 seconds
[16:04:15] TRACE |   FOUND[2] All tokens expire after 3600 seconds
[16:04:15] TRACE | Missed facts: 1/3
[16:04:15] TRACE |   MISSED[1] max 3600 seconds from iat
[16:04:15] TRACE | === END FACT COVERAGE ===
[16:04:15] TRACE | === FACT COVERAGE ANALYSIS ===
[16:04:15] TRACE | Query: I need to implement token refresh logic, when do JWT tokens expire?
[16:04:15] TRACE | Total key facts: 3
[16:04:15] TRACE | Found facts: 2/3
[16:04:15] TRACE |   FOUND[1] 3600 seconds
[16:04:15] TRACE |   FOUND[2] All tokens expire after 3600 seconds
[16:04:15] TRACE | Missed facts: 1/3
[16:04:15] TRACE |   MISSED[1] max 3600 seconds from iat
[16:04:15] TRACE | === END FACT COVERAGE ===
[16:04:15] TRACE | === FACT COVERAGE ANALYSIS ===
[16:04:15] TRACE | Query: Why does my token stop working after 3600 seconds?
[16:04:15] TRACE | Total key facts: 3
[16:04:15] TRACE | Found facts: 2/3
[16:04:15] TRACE |   FOUND[1] 3600 seconds
[16:04:15] TRACE |   FOUND[2] All tokens expire after 3600 seconds
[16:04:15] TRACE | Missed facts: 1/3
[16:04:15] TRACE |   MISSED[1] max 3600 seconds from iat
[16:04:15] TRACE | === END FACT COVERAGE ===
[16:04:15] TRACE | === FACT COVERAGE ANALYSIS ===
[16:04:15] TRACE | Query: What database technology does CloudFlow use?
[16:04:15] TRACE | Total key facts: 3
[16:04:15] TRACE | Found facts: 3/3
[16:04:15] TRACE |   FOUND[1] PostgreSQL 15.4
[16:04:15] TRACE |   FOUND[2] Redis 7.2
[16:04:15] TRACE |   FOUND[3] Apache Kafka 3.6
[16:04:15] TRACE | === END FACT COVERAGE ===
[16:04:15] TRACE | === FACT COVERAGE ANALYSIS ===
[16:04:15] TRACE | Query: Which database system powers CloudFlow?
[16:04:15] TRACE | Total key facts: 3
[16:04:15] TRACE | Found facts: 3/3
[16:04:15] TRACE |   FOUND[1] PostgreSQL 15.4
[16:04:15] TRACE |   FOUND[2] Redis 7.2
[16:04:15] TRACE |   FOUND[3] Apache Kafka 3.6
[16:04:15] TRACE | === END FACT COVERAGE ===
[16:04:15] TRACE | === FACT COVERAGE ANALYSIS ===
[16:04:15] TRACE | Query: I need to understand the data storage architecture
[16:04:15] TRACE | Total key facts: 3
[16:04:15] TRACE | Found facts: 3/3
[16:04:15] TRACE |   FOUND[1] PostgreSQL 15.4
[16:04:15] TRACE |   FOUND[2] Redis 7.2
[16:04:15] TRACE |   FOUND[3] Apache Kafka 3.6
[16:04:15] TRACE | === END FACT COVERAGE ===
[16:04:15] TRACE | === FACT COVERAGE ANALYSIS ===
[16:04:15] TRACE | Query: database stack
[16:04:15] TRACE | Total key facts: 3
[16:04:15] TRACE | Found facts: 3/3
[16:04:15] TRACE |   FOUND[1] PostgreSQL 15.4
[16:04:15] TRACE |   FOUND[2] Redis 7.2
[16:04:15] TRACE |   FOUND[3] Apache Kafka 3.6
[16:04:15] TRACE | === END FACT COVERAGE ===
[16:04:15] TRACE | === FACT COVERAGE ANALYSIS ===
[16:04:15] TRACE | Query: For capacity planning, what databases does CloudFlow rely on?
[16:04:15] TRACE | Total key facts: 3
[16:04:15] TRACE | Found facts: 3/3
[16:04:15] TRACE |   FOUND[1] PostgreSQL 15.4
[16:04:15] TRACE |   FOUND[2] Redis 7.2
[16:04:15] TRACE |   FOUND[3] Apache Kafka 3.6
[16:04:15] TRACE | === END FACT COVERAGE ===
[16:04:15] TRACE | === FACT COVERAGE ANALYSIS ===
[16:04:15] TRACE | Query: Is CloudFlow using MySQL or something else?
[16:04:15] TRACE | Total key facts: 3
[16:04:15] TRACE | Found facts: 0/3
[16:04:15] TRACE | Missed facts: 3/3
[16:04:15] TRACE |   MISSED[1] PostgreSQL 15.4
[16:04:15] TRACE |     -> Found in chunk_id=architecture_overview_fix_0 (rank not in top-5)
[16:04:15] TRACE |   MISSED[2] Redis 7.2
[16:04:15] TRACE |     -> Found in chunk_id=architecture_overview_fix_0 (rank not in top-5)
[16:04:15] TRACE |   MISSED[3] Apache Kafka 3.6
[16:04:15] TRACE |     -> Found in chunk_id=architecture_overview_fix_0 (rank not in top-5)
[16:04:15] TRACE | === END FACT COVERAGE ===
[16:04:15] TRACE | === FACT COVERAGE ANALYSIS ===
[16:04:15] TRACE | Query: What is the Kubernetes namespace for production deployment?
[16:04:15] TRACE | Total key facts: 1
[16:04:15] TRACE | Found facts: 1/1
[16:04:15] TRACE |   FOUND[1] cloudflow-prod
[16:04:15] TRACE | === END FACT COVERAGE ===
[16:04:15] TRACE | === FACT COVERAGE ANALYSIS ===
[16:04:15] TRACE | Query: Which namespace is used for CloudFlow production?
[16:04:15] TRACE | Total key facts: 1
[16:04:15] TRACE | Found facts: 1/1
[16:04:15] TRACE |   FOUND[1] cloudflow-prod
[16:04:15] TRACE | === END FACT COVERAGE ===
[16:04:15] TRACE | === FACT COVERAGE ANALYSIS ===
[16:04:15] TRACE | Query: I can't find the production pods
[16:04:15] TRACE | Total key facts: 1
[16:04:15] TRACE | Found facts: 1/1
[16:04:15] TRACE |   FOUND[1] cloudflow-prod
[16:04:15] TRACE | === END FACT COVERAGE ===
[16:04:15] TRACE | === FACT COVERAGE ANALYSIS ===
[16:04:15] TRACE | Query: prod namespace
[16:04:15] TRACE | Total key facts: 1
[16:04:15] TRACE | Found facts: 1/1
[16:04:15] TRACE |   FOUND[1] cloudflow-prod
[16:04:15] TRACE | === END FACT COVERAGE ===
[16:04:15] TRACE | === FACT COVERAGE ANALYSIS ===
[16:04:15] TRACE | Query: I need to deploy to production, what namespace should I use?
[16:04:15] TRACE | Total key facts: 1
[16:04:15] TRACE | Found facts: 1/1
[16:04:15] TRACE |   FOUND[1] cloudflow-prod
[16:04:15] TRACE | === END FACT COVERAGE ===
[16:04:15] TRACE | === FACT COVERAGE ANALYSIS ===
[16:04:15] TRACE | Query: Why can't I see resources in the default namespace?
[16:04:15] TRACE | Total key facts: 1
[16:04:15] TRACE | Found facts: 1/1
[16:04:15] TRACE |   FOUND[1] cloudflow-prod
[16:04:15] TRACE | === END FACT COVERAGE ===
[16:04:15] TRACE | === FACT COVERAGE ANALYSIS ===
[16:04:15] TRACE | Query: What are the resource requirements for the API Gateway?
[16:04:15] TRACE | Total key facts: 1
[16:04:15] TRACE | Found facts: 1/1
[16:04:15] TRACE |   FOUND[1] 2 vCPU, 4GB RAM per pod
[16:04:15] TRACE | === END FACT COVERAGE ===
[16:04:15] TRACE | === FACT COVERAGE ANALYSIS ===
[16:04:15] TRACE | Query: How much CPU and memory does the API Gateway need?
[16:04:15] TRACE | Total key facts: 1
[16:04:15] TRACE | Found facts: 1/1
[16:04:15] TRACE |   FOUND[1] 2 vCPU, 4GB RAM per pod
[16:04:15] TRACE | === END FACT COVERAGE ===
[16:04:15] TRACE | === FACT COVERAGE ANALYSIS ===
[16:04:15] TRACE | Query: The API Gateway pods keep getting OOMKilled
[16:04:15] TRACE | Total key facts: 1
[16:04:15] TRACE | Found facts: 1/1
[16:04:15] TRACE |   FOUND[1] 2 vCPU, 4GB RAM per pod
[16:04:15] TRACE | === END FACT COVERAGE ===
[16:04:15] TRACE | === FACT COVERAGE ANALYSIS ===
[16:04:15] TRACE | Query: api gateway resources
[16:04:15] TRACE | Total key facts: 1
[16:04:15] TRACE | Found facts: 0/1
[16:04:15] TRACE | Missed facts: 1/1
[16:04:15] TRACE |   MISSED[1] 2 vCPU, 4GB RAM per pod
[16:04:15] TRACE |     -> Found in chunk_id=architecture_overview_fix_1 (rank not in top-5)
[16:04:15] TRACE | === END FACT COVERAGE ===
[16:04:15] TRACE | === FACT COVERAGE ANALYSIS ===
[16:04:15] TRACE | Query: I'm provisioning infrastructure, what are the compute specs for API Gateway?
[16:04:15] TRACE | Total key facts: 1
[16:04:15] TRACE | Found facts: 0/1
[16:04:15] TRACE | Missed facts: 1/1
[16:04:15] TRACE |   MISSED[1] 2 vCPU, 4GB RAM per pod
[16:04:15] TRACE |     -> Found in chunk_id=architecture_overview_fix_1 (rank not in top-5)
[16:04:15] TRACE | === END FACT COVERAGE ===
[16:04:15] TRACE | === FACT COVERAGE ANALYSIS ===
[16:04:15] TRACE | Query: Why is 1GB RAM not enough for API Gateway?
[16:04:15] TRACE | Total key facts: 1
[16:04:15] TRACE | Found facts: 1/1
[16:04:15] TRACE |   FOUND[1] 2 vCPU, 4GB RAM per pod
[16:04:15] TRACE | === END FACT COVERAGE ===
[16:04:15] TRACE | === FACT COVERAGE ANALYSIS ===
[16:04:15] TRACE | Query: What are the health check endpoints?
[16:04:15] TRACE | Total key facts: 2
[16:04:15] TRACE | Found facts: 2/2
[16:04:15] TRACE |   FOUND[1] /health
[16:04:15] TRACE |   FOUND[2] /ready
[16:04:15] TRACE | === END FACT COVERAGE ===
[16:04:15] TRACE | === FACT COVERAGE ANALYSIS ===
[16:04:15] TRACE | Query: Which URLs should I use for health monitoring?
[16:04:15] TRACE | Total key facts: 2
[16:04:15] TRACE | Found facts: 0/2
[16:04:15] TRACE | Missed facts: 2/2
[16:04:15] TRACE |   MISSED[1] /health
[16:04:15] TRACE |     -> Found in chunk_id=deployment_guide_fix_2 (rank not in top-5)
[16:04:15] TRACE |   MISSED[2] /ready
[16:04:15] TRACE |     -> Found in chunk_id=deployment_guide_fix_2 (rank not in top-5)
[16:04:15] TRACE | === END FACT COVERAGE ===
[16:04:15] TRACE | === FACT COVERAGE ANALYSIS ===
[16:04:15] TRACE | Query: My load balancer health checks are failing
[16:04:15] TRACE | Total key facts: 2
[16:04:15] TRACE | Found facts: 2/2
[16:04:15] TRACE |   FOUND[1] /health
[16:04:15] TRACE |   FOUND[2] /ready
[16:04:15] TRACE | === END FACT COVERAGE ===
[16:04:15] TRACE | === FACT COVERAGE ANALYSIS ===
[16:04:15] TRACE | Query: health endpoints
[16:04:15] TRACE | Total key facts: 2
[16:04:15] TRACE | Found facts: 2/2
[16:04:15] TRACE |   FOUND[1] /health
[16:04:15] TRACE |   FOUND[2] /ready
[16:04:15] TRACE | === END FACT COVERAGE ===
[16:04:15] TRACE | === FACT COVERAGE ANALYSIS ===
[16:04:15] TRACE | Query: Setting up monitoring, what endpoints indicate service health?
[16:04:15] TRACE | Total key facts: 2
[16:04:15] TRACE | Found facts: 0/2
[16:04:15] TRACE | Missed facts: 2/2
[16:04:15] TRACE |   MISSED[1] /health
[16:04:15] TRACE |     -> Found in chunk_id=deployment_guide_fix_2 (rank not in top-5)
[16:04:15] TRACE |   MISSED[2] /ready
[16:04:15] TRACE |     -> Found in chunk_id=deployment_guide_fix_2 (rank not in top-5)
[16:04:15] TRACE | === END FACT COVERAGE ===
[16:04:15] TRACE | === FACT COVERAGE ANALYSIS ===
[16:04:15] TRACE | Query: Why doesn't /status return health information?
[16:04:15] TRACE | Total key facts: 2
[16:04:15] TRACE | Found facts: 2/2
[16:04:15] TRACE |   FOUND[1] /health
[16:04:15] TRACE |   FOUND[2] /ready
[16:04:15] TRACE | === END FACT COVERAGE ===
[16:04:15] TRACE | === FACT COVERAGE ANALYSIS ===
[16:04:15] TRACE | Query: What are the HPA scaling parameters?
[16:04:15] TRACE | Total key facts: 3
[16:04:15] TRACE | Found facts: 2/3
[16:04:15] TRACE |   FOUND[1] minReplicas: 3
[16:04:15] TRACE |   FOUND[2] maxReplicas: 10
[16:04:15] TRACE | Missed facts: 1/3
[16:04:15] TRACE |   MISSED[1] targetCPUUtilizationPercentage: 70
[16:04:15] TRACE |     -> Found in chunk_id=deployment_guide_fix_1 (rank not in top-5)
[16:04:15] TRACE | === END FACT COVERAGE ===
[16:04:15] TRACE | === FACT COVERAGE ANALYSIS ===
[16:04:15] TRACE | Query: How does horizontal pod autoscaling work?
[16:04:15] TRACE | Total key facts: 3
[16:04:15] TRACE | Found facts: 2/3
[16:04:15] TRACE |   FOUND[1] minReplicas: 3
[16:04:15] TRACE |   FOUND[2] maxReplicas: 10
[16:04:15] TRACE | Missed facts: 1/3
[16:04:15] TRACE |   MISSED[1] targetCPUUtilizationPercentage: 70
[16:04:15] TRACE |     -> Found in chunk_id=deployment_guide_fix_1 (rank not in top-5)
[16:04:15] TRACE | === END FACT COVERAGE ===
[16:04:15] TRACE | === FACT COVERAGE ANALYSIS ===
[16:04:15] TRACE | Query: Pods aren't scaling up during traffic spikes
[16:04:15] TRACE | Total key facts: 3
[16:04:15] TRACE | Found facts: 2/3
[16:04:15] TRACE |   FOUND[1] minReplicas: 3
[16:04:15] TRACE |   FOUND[2] maxReplicas: 10
[16:04:15] TRACE | Missed facts: 1/3
[16:04:15] TRACE |   MISSED[1] targetCPUUtilizationPercentage: 70
[16:04:15] TRACE |     -> Found in chunk_id=deployment_guide_fix_1 (rank not in top-5)
[16:04:15] TRACE | === END FACT COVERAGE ===
[16:04:15] TRACE | === FACT COVERAGE ANALYSIS ===
[16:04:15] TRACE | Query: autoscaling config
[16:04:15] TRACE | Total key facts: 3
[16:04:15] TRACE | Found facts: 2/3
[16:04:15] TRACE |   FOUND[1] minReplicas: 3
[16:04:15] TRACE |   FOUND[2] maxReplicas: 10
[16:04:15] TRACE | Missed facts: 1/3
[16:04:15] TRACE |   MISSED[1] targetCPUUtilizationPercentage: 70
[16:04:15] TRACE |     -> Found in chunk_id=deployment_guide_fix_1 (rank not in top-5)
[16:04:15] TRACE | === END FACT COVERAGE ===
[16:04:15] TRACE | === FACT COVERAGE ANALYSIS ===
[16:04:15] TRACE | Query: I need to configure autoscaling, what are the min/max replicas and thresholds?
[16:04:15] TRACE | Total key facts: 3
[16:04:15] TRACE | Found facts: 2/3
[16:04:15] TRACE |   FOUND[1] minReplicas: 3
[16:04:15] TRACE |   FOUND[2] maxReplicas: 10
[16:04:15] TRACE | Missed facts: 1/3
[16:04:15] TRACE |   MISSED[1] targetCPUUtilizationPercentage: 70
[16:04:15] TRACE |     -> Found in chunk_id=deployment_guide_fix_1 (rank not in top-5)
[16:04:15] TRACE | === END FACT COVERAGE ===
[16:04:15] TRACE | === FACT COVERAGE ANALYSIS ===
[16:04:15] TRACE | Query: Why do we have 3 replicas even with low traffic?
[16:04:15] TRACE | Total key facts: 3
[16:04:15] TRACE | Found facts: 0/3
[16:04:15] TRACE | Missed facts: 3/3
[16:04:15] TRACE |   MISSED[1] minReplicas: 3
[16:04:15] TRACE |     -> Found in chunk_id=deployment_guide_fix_1 (rank not in top-5)
[16:04:15] TRACE |   MISSED[2] maxReplicas: 10
[16:04:15] TRACE |     -> Found in chunk_id=deployment_guide_fix_1 (rank not in top-5)
[16:04:15] TRACE |   MISSED[3] targetCPUUtilizationPercentage: 70
[16:04:15] TRACE |     -> Found in chunk_id=deployment_guide_fix_1 (rank not in top-5)
[16:04:15] TRACE | === END FACT COVERAGE ===
[16:04:15] TRACE | === FACT COVERAGE ANALYSIS ===
[16:04:15] TRACE | Query: What is the P99 latency target for API operations?
[16:04:15] TRACE | Total key facts: 2
[16:04:15] TRACE | Found facts: 1/2
[16:04:15] TRACE |   FOUND[1] average P99 latency of 180ms for API operations
[16:04:15] TRACE | Missed facts: 1/2
[16:04:15] TRACE |   MISSED[1] P99 latency: < 200ms
[16:04:15] TRACE |     -> Found in chunk_id=architecture_overview_fix_1 (rank not in top-5)
[16:04:15] TRACE | === END FACT COVERAGE ===
[16:04:15] TRACE | === FACT COVERAGE ANALYSIS ===
[16:04:15] TRACE | Query: What's the 99th percentile response time target?
[16:04:15] TRACE | Total key facts: 2
[16:04:15] TRACE | Found facts: 0/2
[16:04:15] TRACE | Missed facts: 2/2
[16:04:15] TRACE |   MISSED[1] P99 latency: < 200ms
[16:04:15] TRACE |     -> Found in chunk_id=architecture_overview_fix_1 (rank not in top-5)
[16:04:15] TRACE |   MISSED[2] average P99 latency of 180ms for API operations
[16:04:15] TRACE |     -> Found in chunk_id=architecture_overview_fix_0 (rank not in top-5)
[16:04:15] TRACE | === END FACT COVERAGE ===
[16:04:15] TRACE | === FACT COVERAGE ANALYSIS ===
[16:04:15] TRACE | Query: Our API latency is 500ms, is that acceptable?
[16:04:15] TRACE | Total key facts: 2
[16:04:15] TRACE | Found facts: 0/2
[16:04:15] TRACE | Missed facts: 2/2
[16:04:15] TRACE |   MISSED[1] P99 latency: < 200ms
[16:04:15] TRACE |     -> Found in chunk_id=architecture_overview_fix_1 (rank not in top-5)
[16:04:15] TRACE |   MISSED[2] average P99 latency of 180ms for API operations
[16:04:15] TRACE |     -> Found in chunk_id=architecture_overview_fix_0 (rank not in top-5)
[16:04:15] TRACE | === END FACT COVERAGE ===
[16:04:15] TRACE | === FACT COVERAGE ANALYSIS ===
[16:04:15] TRACE | Query: api latency target
[16:04:15] TRACE | Total key facts: 2
[16:04:15] TRACE | Found facts: 0/2
[16:04:15] TRACE | Missed facts: 2/2
[16:04:15] TRACE |   MISSED[1] P99 latency: < 200ms
[16:04:15] TRACE |     -> Found in chunk_id=architecture_overview_fix_1 (rank not in top-5)
[16:04:15] TRACE |   MISSED[2] average P99 latency of 180ms for API operations
[16:04:15] TRACE |     -> Found in chunk_id=architecture_overview_fix_0 (rank not in top-5)
[16:04:15] TRACE | === END FACT COVERAGE ===
[16:04:15] TRACE | === FACT COVERAGE ANALYSIS ===
[16:04:15] TRACE | Query: Setting SLOs for our service, what P99 latency does CloudFlow guarantee?
[16:04:15] TRACE | Total key facts: 2
[16:04:15] TRACE | Found facts: 1/2
[16:04:15] TRACE |   FOUND[1] average P99 latency of 180ms for API operations
[16:04:15] TRACE | Missed facts: 1/2
[16:04:15] TRACE |   MISSED[1] P99 latency: < 200ms
[16:04:15] TRACE |     -> Found in chunk_id=architecture_overview_fix_1 (rank not in top-5)
[16:04:15] TRACE | === END FACT COVERAGE ===
[16:04:15] TRACE | === FACT COVERAGE ANALYSIS ===
[16:04:15] TRACE | Query: Why are we getting alerts at 200ms latency?
[16:04:15] TRACE | Total key facts: 2
[16:04:15] TRACE | Found facts: 0/2
[16:04:15] TRACE | Missed facts: 2/2
[16:04:15] TRACE |   MISSED[1] P99 latency: < 200ms
[16:04:15] TRACE |     -> Found in chunk_id=architecture_overview_fix_1 (rank not in top-5)
[16:04:15] TRACE |   MISSED[2] average P99 latency of 180ms for API operations
[16:04:15] TRACE |     -> Found in chunk_id=architecture_overview_fix_0 (rank not in top-5)
[16:04:15] TRACE | === END FACT COVERAGE ===
[16:04:15] TRACE | === FACT COVERAGE ANALYSIS ===
[16:04:15] TRACE | Query: What are the disaster recovery RPO and RTO values?
[16:04:15] TRACE | Total key facts: 2
[16:04:15] TRACE | Found facts: 2/2
[16:04:15] TRACE |   FOUND[1] RPO (Recovery Point Objective): 1 hour
[16:04:15] TRACE |   FOUND[2] RTO (Recovery Time Objective): 4 hours
[16:04:15] TRACE | === END FACT COVERAGE ===
[16:04:15] TRACE | === FACT COVERAGE ANALYSIS ===
[16:04:15] TRACE | Query: What's the maximum data loss and recovery time?
[16:04:15] TRACE | Total key facts: 2
[16:04:15] TRACE | Found facts: 2/2
[16:04:15] TRACE |   FOUND[1] RPO (Recovery Point Objective): 1 hour
[16:04:15] TRACE |   FOUND[2] RTO (Recovery Time Objective): 4 hours
[16:04:15] TRACE | === END FACT COVERAGE ===
[16:04:15] TRACE | === FACT COVERAGE ANALYSIS ===
[16:04:15] TRACE | Query: How long will it take to recover from a disaster?
[16:04:15] TRACE | Total key facts: 2
[16:04:15] TRACE | Found facts: 0/2
[16:04:15] TRACE | Missed facts: 2/2
[16:04:15] TRACE |   MISSED[1] RPO (Recovery Point Objective): 1 hour
[16:04:15] TRACE |     -> Found in chunk_id=architecture_overview_fix_10 (rank not in top-5)
[16:04:15] TRACE |   MISSED[2] RTO (Recovery Time Objective): 4 hours
[16:04:15] TRACE |     -> Found in chunk_id=architecture_overview_fix_10 (rank not in top-5)
[16:04:15] TRACE | === END FACT COVERAGE ===
[16:04:15] TRACE | === FACT COVERAGE ANALYSIS ===
[16:04:15] TRACE | Query: RPO RTO
[16:04:15] TRACE | Total key facts: 2
[16:04:15] TRACE | Found facts: 2/2
[16:04:15] TRACE |   FOUND[1] RPO (Recovery Point Objective): 1 hour
[16:04:15] TRACE |   FOUND[2] RTO (Recovery Time Objective): 4 hours
[16:04:15] TRACE | === END FACT COVERAGE ===
[16:04:15] TRACE | === FACT COVERAGE ANALYSIS ===
[16:04:15] TRACE | Query: For our business continuity plan, what are CloudFlow's recovery objectives?
[16:04:15] TRACE | Total key facts: 2
[16:04:15] TRACE | Found facts: 0/2
[16:04:15] TRACE | Missed facts: 2/2
[16:04:15] TRACE |   MISSED[1] RPO (Recovery Point Objective): 1 hour
[16:04:15] TRACE |     -> Found in chunk_id=architecture_overview_fix_10 (rank not in top-5)
[16:04:15] TRACE |   MISSED[2] RTO (Recovery Time Objective): 4 hours
[16:04:15] TRACE |     -> Found in chunk_id=architecture_overview_fix_10 (rank not in top-5)
[16:04:15] TRACE | === END FACT COVERAGE ===
[16:04:15] TRACE | === FACT COVERAGE ANALYSIS ===
[16:04:15] TRACE | Query: Why can't we guarantee zero data loss?
[16:04:15] TRACE | Total key facts: 2
[16:04:15] TRACE | Found facts: 0/2
[16:04:15] TRACE | Missed facts: 2/2
[16:04:15] TRACE |   MISSED[1] RPO (Recovery Point Objective): 1 hour
[16:04:15] TRACE |     -> Found in chunk_id=architecture_overview_fix_10 (rank not in top-5)
[16:04:15] TRACE |   MISSED[2] RTO (Recovery Time Objective): 4 hours
[16:04:15] TRACE |     -> Found in chunk_id=architecture_overview_fix_10 (rank not in top-5)
[16:04:15] TRACE | === END FACT COVERAGE ===
[16:04:15] TRACE | === FACT COVERAGE ANALYSIS ===
[16:04:15] TRACE | Query: What is the maximum workflow execution timeout?
[16:04:15] TRACE | Total key facts: 2
[16:04:15] TRACE | Found facts: 2/2
[16:04:15] TRACE |   FOUND[1] 3600 seconds
[16:04:15] TRACE |   FOUND[2] exceeded maximum execution time of 3600 seconds
[16:04:15] TRACE | === END FACT COVERAGE ===
[16:04:15] TRACE | === FACT COVERAGE ANALYSIS ===
[16:04:15] TRACE | Query: How long can a workflow run before timing out?
[16:04:15] TRACE | Total key facts: 2
[16:04:15] TRACE | Found facts: 2/2
[16:04:15] TRACE |   FOUND[1] 3600 seconds
[16:04:15] TRACE |   FOUND[2] exceeded maximum execution time of 3600 seconds
[16:04:15] TRACE | === END FACT COVERAGE ===
[16:04:16] TRACE | === FACT COVERAGE ANALYSIS ===
[16:04:16] TRACE | Query: My workflow is being killed after an hour
[16:04:16] TRACE | Total key facts: 2
[16:04:16] TRACE | Found facts: 2/2
[16:04:16] TRACE |   FOUND[1] 3600 seconds
[16:04:16] TRACE |   FOUND[2] exceeded maximum execution time of 3600 seconds
[16:04:16] TRACE | === END FACT COVERAGE ===
[16:04:16] TRACE | === FACT COVERAGE ANALYSIS ===
[16:04:16] TRACE | Query: workflow timeout
[16:04:16] TRACE | Total key facts: 2
[16:04:16] TRACE | Found facts: 2/2
[16:04:16] TRACE |   FOUND[1] 3600 seconds
[16:04:16] TRACE |   FOUND[2] exceeded maximum execution time of 3600 seconds
[16:04:16] TRACE | === END FACT COVERAGE ===
[16:04:16] TRACE | === FACT COVERAGE ANALYSIS ===
[16:04:16] TRACE | Query: I have a long-running data processing workflow, what's the time limit?
[16:04:16] TRACE | Total key facts: 2
[16:04:16] TRACE | Found facts: 1/2
[16:04:16] TRACE |   FOUND[1] 3600 seconds
[16:04:16] TRACE | Missed facts: 1/2
[16:04:16] TRACE |   MISSED[1] exceeded maximum execution time of 3600 seconds
[16:04:16] TRACE |     -> Found in chunk_id=troubleshooting_guide_fix_4 (rank not in top-5)
[16:04:16] TRACE | === END FACT COVERAGE ===
[16:04:16] TRACE | === FACT COVERAGE ANALYSIS ===
[16:04:16] TRACE | Query: Why did my workflow fail after 3600 seconds?
[16:04:16] TRACE | Total key facts: 2
[16:04:16] TRACE | Found facts: 2/2
[16:04:16] TRACE |   FOUND[1] 3600 seconds
[16:04:16] TRACE |   FOUND[2] exceeded maximum execution time of 3600 seconds
[16:04:16] TRACE | === END FACT COVERAGE ===
[16:04:16] TRACE | === FACT COVERAGE ANALYSIS ===
[16:04:16] TRACE | Query: What JWT algorithm is used for token signing?
[16:04:16] TRACE | Total key facts: 3
[16:04:16] TRACE | Found facts: 3/3
[16:04:16] TRACE |   FOUND[1] RS256
[16:04:16] TRACE |   FOUND[2] RS256 algorithm
[16:04:16] TRACE |   FOUND[3] RS256 signing algorithm
[16:04:16] TRACE | === END FACT COVERAGE ===
[16:04:16] TRACE | === FACT COVERAGE ANALYSIS ===
[16:04:16] TRACE | Query: Which signing algorithm does CloudFlow use for JWTs?
[16:04:16] TRACE | Total key facts: 3
[16:04:16] TRACE | Found facts: 3/3
[16:04:16] TRACE |   FOUND[1] RS256
[16:04:16] TRACE |   FOUND[2] RS256 algorithm
[16:04:16] TRACE |   FOUND[3] RS256 signing algorithm
[16:04:16] TRACE | === END FACT COVERAGE ===
[16:04:16] TRACE | === FACT COVERAGE ANALYSIS ===
[16:04:16] TRACE | Query: My JWT validation is failing with algorithm mismatch
[16:04:16] TRACE | Total key facts: 3
[16:04:16] TRACE | Found facts: 3/3
[16:04:16] TRACE |   FOUND[1] RS256
[16:04:16] TRACE |   FOUND[2] RS256 algorithm
[16:04:16] TRACE |   FOUND[3] RS256 signing algorithm
[16:04:16] TRACE | === END FACT COVERAGE ===
[16:04:16] TRACE | === FACT COVERAGE ANALYSIS ===
[16:04:16] TRACE | Query: jwt algorithm
[16:04:16] TRACE | Total key facts: 3
[16:04:16] TRACE | Found facts: 3/3
[16:04:16] TRACE |   FOUND[1] RS256
[16:04:16] TRACE |   FOUND[2] RS256 algorithm
[16:04:16] TRACE |   FOUND[3] RS256 signing algorithm
[16:04:16] TRACE | === END FACT COVERAGE ===
[16:04:16] TRACE | === FACT COVERAGE ANALYSIS ===
[16:04:16] TRACE | Query: I'm implementing JWT verification, what algorithm should I expect?
[16:04:16] TRACE | Total key facts: 3
[16:04:16] TRACE | Found facts: 3/3
[16:04:16] TRACE |   FOUND[1] RS256
[16:04:16] TRACE |   FOUND[2] RS256 algorithm
[16:04:16] TRACE |   FOUND[3] RS256 signing algorithm
[16:04:16] TRACE | === END FACT COVERAGE ===
[16:04:16] TRACE | === FACT COVERAGE ANALYSIS ===
[16:04:16] TRACE | Query: Why doesn't HS256 work for token validation?
[16:04:16] TRACE | Total key facts: 3
[16:04:16] TRACE | Found facts: 3/3
[16:04:16] TRACE |   FOUND[1] RS256
[16:04:16] TRACE |   FOUND[2] RS256 algorithm
[16:04:16] TRACE |   FOUND[3] RS256 signing algorithm
[16:04:16] TRACE | === END FACT COVERAGE ===
[16:04:16] TRACE | === FACT COVERAGE ANALYSIS ===
[16:04:16] TRACE | Query: What is the Redis cache TTL for workflow definitions?
[16:04:16] TRACE | Total key facts: 2
[16:04:16] TRACE | Found facts: 0/2
[16:04:16] TRACE | Missed facts: 2/2
[16:04:16] TRACE |   MISSED[1] TTL: 1 hour
[16:04:16] TRACE |     -> Found in chunk_id=architecture_overview_fix_6 (rank not in top-5)
[16:04:16] TRACE |   MISSED[2] Workflow Definitions: TTL: 1 hour
[16:04:16] TRACE | === END FACT COVERAGE ===
[16:04:16] TRACE | === FACT COVERAGE ANALYSIS ===
[16:04:16] TRACE | Query: How long are workflow definitions cached?
[16:04:16] TRACE | Total key facts: 2
[16:04:16] TRACE | Found facts: 0/2
[16:04:16] TRACE | Missed facts: 2/2
[16:04:16] TRACE |   MISSED[1] TTL: 1 hour
[16:04:16] TRACE |     -> Found in chunk_id=architecture_overview_fix_6 (rank not in top-5)
[16:04:16] TRACE |   MISSED[2] Workflow Definitions: TTL: 1 hour
[16:04:16] TRACE | === END FACT COVERAGE ===
[16:04:16] TRACE | === FACT COVERAGE ANALYSIS ===
[16:04:16] TRACE | Query: My workflow updates aren't reflecting immediately
[16:04:16] TRACE | Total key facts: 2
[16:04:16] TRACE | Found facts: 0/2
[16:04:16] TRACE | Missed facts: 2/2
[16:04:16] TRACE |   MISSED[1] TTL: 1 hour
[16:04:16] TRACE |     -> Found in chunk_id=architecture_overview_fix_6 (rank not in top-5)
[16:04:16] TRACE |   MISSED[2] Workflow Definitions: TTL: 1 hour
[16:04:16] TRACE | === END FACT COVERAGE ===
[16:04:16] TRACE | === FACT COVERAGE ANALYSIS ===
[16:04:16] TRACE | Query: cache ttl workflows
[16:04:16] TRACE | Total key facts: 2
[16:04:16] TRACE | Found facts: 0/2
[16:04:16] TRACE | Missed facts: 2/2
[16:04:16] TRACE |   MISSED[1] TTL: 1 hour
[16:04:16] TRACE |     -> Found in chunk_id=architecture_overview_fix_6 (rank not in top-5)
[16:04:16] TRACE |   MISSED[2] Workflow Definitions: TTL: 1 hour
[16:04:16] TRACE | === END FACT COVERAGE ===
[16:04:16] TRACE | === FACT COVERAGE ANALYSIS ===
[16:04:16] TRACE | Query: After updating a workflow, how long until the cache expires?
[16:04:16] TRACE | Total key facts: 2
[16:04:16] TRACE | Found facts: 0/2
[16:04:16] TRACE | Missed facts: 2/2
[16:04:16] TRACE |   MISSED[1] TTL: 1 hour
[16:04:16] TRACE |     -> Found in chunk_id=architecture_overview_fix_6 (rank not in top-5)
[16:04:16] TRACE |   MISSED[2] Workflow Definitions: TTL: 1 hour
[16:04:16] TRACE | === END FACT COVERAGE ===
[16:04:16] TRACE | === FACT COVERAGE ANALYSIS ===
[16:04:16] TRACE | Query: Why are changes taking an hour to appear?
[16:04:16] TRACE | Total key facts: 2
[16:04:16] TRACE | Found facts: 0/2
[16:04:16] TRACE | Missed facts: 2/2
[16:04:16] TRACE |   MISSED[1] TTL: 1 hour
[16:04:16] TRACE |     -> Found in chunk_id=architecture_overview_fix_6 (rank not in top-5)
[16:04:16] TRACE |   MISSED[2] Workflow Definitions: TTL: 1 hour
[16:04:16] TRACE | === END FACT COVERAGE ===
[16:04:16] TRACE | === FACT COVERAGE ANALYSIS ===
[16:04:16] TRACE | Query: What monitoring tools does CloudFlow use?
[16:04:16] TRACE | Total key facts: 3
[16:04:16] TRACE | Found facts: 3/3
[16:04:16] TRACE |   FOUND[1] Prometheus
[16:04:16] TRACE |   FOUND[2] Grafana
[16:04:16] TRACE |   FOUND[3] Jaeger for distributed tracing
[16:04:16] TRACE | === END FACT COVERAGE ===
[16:04:16] TRACE | === FACT COVERAGE ANALYSIS ===
[16:04:16] TRACE | Query: Which observability platform is integrated?
[16:04:16] TRACE | Total key facts: 3
[16:04:16] TRACE | Found facts: 3/3
[16:04:16] TRACE |   FOUND[1] Prometheus
[16:04:16] TRACE |   FOUND[2] Grafana
[16:04:16] TRACE |   FOUND[3] Jaeger for distributed tracing
[16:04:16] TRACE | === END FACT COVERAGE ===
[16:04:16] TRACE | === FACT COVERAGE ANALYSIS ===
[16:04:16] TRACE | Query: Where can I view CloudFlow metrics and logs?
[16:04:16] TRACE | Total key facts: 3
[16:04:16] TRACE | Found facts: 2/3
[16:04:16] TRACE |   FOUND[1] Prometheus
[16:04:16] TRACE |   FOUND[2] Grafana
[16:04:16] TRACE | Missed facts: 1/3
[16:04:16] TRACE |   MISSED[1] Jaeger for distributed tracing
[16:04:16] TRACE |     -> Found in chunk_id=architecture_overview_fix_0 (rank not in top-5)
[16:04:16] TRACE | === END FACT COVERAGE ===
[16:04:16] TRACE | === FACT COVERAGE ANALYSIS ===
[16:04:16] TRACE | Query: monitoring stack
[16:04:16] TRACE | Total key facts: 3
[16:04:16] TRACE | Found facts: 3/3
[16:04:16] TRACE |   FOUND[1] Prometheus
[16:04:16] TRACE |   FOUND[2] Grafana
[16:04:16] TRACE |   FOUND[3] Jaeger for distributed tracing
[16:04:16] TRACE | === END FACT COVERAGE ===
[16:04:16] TRACE | === FACT COVERAGE ANALYSIS ===
[16:04:16] TRACE | Query: I need to set up dashboards, what monitoring systems are available?
[16:04:16] TRACE | Total key facts: 3
[16:04:16] TRACE | Found facts: 3/3
[16:04:16] TRACE |   FOUND[1] Prometheus
[16:04:16] TRACE |   FOUND[2] Grafana
[16:04:16] TRACE |   FOUND[3] Jaeger for distributed tracing
[16:04:16] TRACE | === END FACT COVERAGE ===
[16:04:16] TRACE | === FACT COVERAGE ANALYSIS ===
[16:04:16] TRACE | Query: Why can't I see metrics in Datadog?
[16:04:16] TRACE | Total key facts: 3
[16:04:16] TRACE | Found facts: 2/3
[16:04:16] TRACE |   FOUND[1] Prometheus
[16:04:16] TRACE |   FOUND[2] Grafana
[16:04:16] TRACE | Missed facts: 1/3
[16:04:16] TRACE |   MISSED[1] Jaeger for distributed tracing
[16:04:16] TRACE |     -> Found in chunk_id=architecture_overview_fix_0 (rank not in top-5)
[16:04:16] TRACE | === END FACT COVERAGE ===
[16:04:16] TRACE | === FACT COVERAGE ANALYSIS ===
[16:04:16] TRACE | Query: How do I diagnose database connection pool exhaustion?
[16:04:16] TRACE | Total key facts: 4
[16:04:16] TRACE | Found facts: 3/4
[16:04:16] TRACE |   FOUND[1] could not obtain connection from pool within 5000ms
[16:04:16] TRACE |   FOUND[2] connection pool exhausted (100/100 connections in use)
[16:04:16] TRACE |   FOUND[3] PgBouncer
[16:04:16] TRACE | Missed facts: 1/4
[16:04:16] TRACE |   MISSED[1] max_db_connections = 100
[16:04:16] TRACE |     -> Found in chunk_id=deployment_guide_fix_3 (rank not in top-5)
[16:04:16] TRACE | === END FACT COVERAGE ===
[16:04:16] TRACE | === FACT COVERAGE ANALYSIS ===
[16:04:16] TRACE | Query: What should I check when I run out of database connections?
[16:04:16] TRACE | Total key facts: 4
[16:04:16] TRACE | Found facts: 1/4
[16:04:16] TRACE |   FOUND[1] PgBouncer
[16:04:16] TRACE | Missed facts: 3/4
[16:04:16] TRACE |   MISSED[1] could not obtain connection from pool within 5000ms
[16:04:16] TRACE |     -> Found in chunk_id=troubleshooting_guide_fix_3 (rank not in top-5)
[16:04:16] TRACE |   MISSED[2] connection pool exhausted (100/100 connections in use)
[16:04:16] TRACE |     -> Found in chunk_id=troubleshooting_guide_fix_3 (rank not in top-5)
[16:04:16] TRACE |   MISSED[3] max_db_connections = 100
[16:04:16] TRACE |     -> Found in chunk_id=deployment_guide_fix_3 (rank not in top-5)
[16:04:16] TRACE | === END FACT COVERAGE ===
[16:04:16] TRACE | === FACT COVERAGE ANALYSIS ===
[16:04:16] TRACE | Query: Getting 'could not obtain connection from pool' errors
[16:04:16] TRACE | Total key facts: 4
[16:04:16] TRACE | Found facts: 3/4
[16:04:16] TRACE |   FOUND[1] could not obtain connection from pool within 5000ms
[16:04:16] TRACE |   FOUND[2] connection pool exhausted (100/100 connections in use)
[16:04:16] TRACE |   FOUND[3] PgBouncer
[16:04:16] TRACE | Missed facts: 1/4
[16:04:16] TRACE |   MISSED[1] max_db_connections = 100
[16:04:16] TRACE |     -> Found in chunk_id=deployment_guide_fix_3 (rank not in top-5)
[16:04:16] TRACE | === END FACT COVERAGE ===
[16:04:16] TRACE | === FACT COVERAGE ANALYSIS ===
[16:04:16] TRACE | Query: connection pool full
[16:04:16] TRACE | Total key facts: 4
[16:04:16] TRACE | Found facts: 3/4
[16:04:16] TRACE |   FOUND[1] could not obtain connection from pool within 5000ms
[16:04:16] TRACE |   FOUND[2] connection pool exhausted (100/100 connections in use)
[16:04:16] TRACE |   FOUND[3] PgBouncer
[16:04:16] TRACE | Missed facts: 1/4
[16:04:16] TRACE |   MISSED[1] max_db_connections = 100
[16:04:16] TRACE |     -> Found in chunk_id=deployment_guide_fix_3 (rank not in top-5)
[16:04:16] TRACE | === END FACT COVERAGE ===
[16:04:16] TRACE | === FACT COVERAGE ANALYSIS ===
[16:04:16] TRACE | Query: My app is failing with connection pool errors, how do I troubleshoot?
[16:04:16] TRACE | Total key facts: 4
[16:04:16] TRACE | Found facts: 3/4
[16:04:16] TRACE |   FOUND[1] could not obtain connection from pool within 5000ms
[16:04:16] TRACE |   FOUND[2] connection pool exhausted (100/100 connections in use)
[16:04:16] TRACE |   FOUND[3] PgBouncer
[16:04:16] TRACE | Missed facts: 1/4
[16:04:16] TRACE |   MISSED[1] max_db_connections = 100
[16:04:16] TRACE |     -> Found in chunk_id=deployment_guide_fix_3 (rank not in top-5)
[16:04:16] TRACE | === END FACT COVERAGE ===
[16:04:16] TRACE | === FACT COVERAGE ANALYSIS ===
[16:04:16] TRACE | Query: Why can't I get a database connection even though CPU is low?
[16:04:16] TRACE | Total key facts: 4
[16:04:16] TRACE | Found facts: 1/4
[16:04:16] TRACE |   FOUND[1] PgBouncer
[16:04:16] TRACE | Missed facts: 3/4
[16:04:16] TRACE |   MISSED[1] could not obtain connection from pool within 5000ms
[16:04:16] TRACE |     -> Found in chunk_id=troubleshooting_guide_fix_3 (rank not in top-5)
[16:04:16] TRACE |   MISSED[2] connection pool exhausted (100/100 connections in use)
[16:04:16] TRACE |     -> Found in chunk_id=troubleshooting_guide_fix_3 (rank not in top-5)
[16:04:16] TRACE |   MISSED[3] max_db_connections = 100
[16:04:16] TRACE |     -> Found in chunk_id=deployment_guide_fix_3 (rank not in top-5)
[16:04:16] TRACE | === END FACT COVERAGE ===
[16:04:16] TRACE | === FACT COVERAGE ANALYSIS ===
[16:04:16] TRACE | Query: How do I handle API authentication?
[16:04:16] TRACE | Total key facts: 3
[16:04:16] TRACE | Found facts: 3/3
[16:04:16] TRACE |   FOUND[1] OAuth 2.0
[16:04:16] TRACE |   FOUND[2] API keys
[16:04:16] TRACE |   FOUND[3] JWT tokens
[16:04:16] TRACE | === END FACT COVERAGE ===
[16:04:16] TRACE | === FACT COVERAGE ANALYSIS ===
[16:04:16] TRACE | Query: What authentication methods are supported?
[16:04:16] TRACE | Total key facts: 3
[16:04:16] TRACE | Found facts: 3/3
[16:04:16] TRACE |   FOUND[1] OAuth 2.0
[16:04:16] TRACE |   FOUND[2] API keys
[16:04:16] TRACE |   FOUND[3] JWT tokens
[16:04:16] TRACE | === END FACT COVERAGE ===
[16:04:16] TRACE | === FACT COVERAGE ANALYSIS ===
[16:04:16] TRACE | Query: My API requests are getting 401 errors
[16:04:16] TRACE | Total key facts: 3
[16:04:16] TRACE | Found facts: 1/3
[16:04:16] TRACE |   FOUND[1] API keys
[16:04:16] TRACE | Missed facts: 2/3
[16:04:16] TRACE |   MISSED[1] OAuth 2.0
[16:04:16] TRACE |     -> Found in chunk_id=api_reference_fix_0 (rank not in top-5)
[16:04:16] TRACE |   MISSED[2] JWT tokens
[16:04:16] TRACE |     -> Found in chunk_id=api_reference_fix_0 (rank not in top-5)
[16:04:16] TRACE | === END FACT COVERAGE ===
[16:04:16] TRACE | === FACT COVERAGE ANALYSIS ===
[16:04:16] TRACE | Query: auth methods
[16:04:16] TRACE | Total key facts: 3
[16:04:16] TRACE | Found facts: 3/3
[16:04:16] TRACE |   FOUND[1] OAuth 2.0
[16:04:16] TRACE |   FOUND[2] API keys
[16:04:16] TRACE |   FOUND[3] JWT tokens
[16:04:16] TRACE | === END FACT COVERAGE ===
[16:04:16] TRACE | === FACT COVERAGE ANALYSIS ===
[16:04:16] TRACE | Query: I'm integrating with CloudFlow API, what authentication options do I have?
[16:04:16] TRACE | Total key facts: 3
[16:04:16] TRACE | Found facts: 3/3
[16:04:16] TRACE |   FOUND[1] OAuth 2.0
[16:04:16] TRACE |   FOUND[2] API keys
[16:04:16] TRACE |   FOUND[3] JWT tokens
[16:04:16] TRACE | === END FACT COVERAGE ===
[16:04:16] TRACE | === FACT COVERAGE ANALYSIS ===
[16:04:16] TRACE | Query: Why isn't basic auth working?
[16:04:16] TRACE | Total key facts: 3
[16:04:16] TRACE | Found facts: 1/3
[16:04:16] TRACE |   FOUND[1] OAuth 2.0
[16:04:16] TRACE | Missed facts: 2/3
[16:04:16] TRACE |   MISSED[1] API keys
[16:04:16] TRACE |     -> Found in chunk_id=api_reference_fix_0 (rank not in top-5)
[16:04:16] TRACE |   MISSED[2] JWT tokens
[16:04:16] TRACE |     -> Found in chunk_id=api_reference_fix_0 (rank not in top-5)
[16:04:16] TRACE | === END FACT COVERAGE ===
[16:04:16] TRACE | === FACT COVERAGE ANALYSIS ===
[16:04:16] TRACE | Query: What is PgBouncer and why is it used in CloudFlow?
[16:04:16] TRACE | Total key facts: 5
[16:04:16] TRACE | Found facts: 5/5
[16:04:16] TRACE |   FOUND[1] PgBouncer
[16:04:16] TRACE |   FOUND[2] connection pooling
[16:04:16] TRACE |   FOUND[3] max_db_connections = 100
[16:04:16] TRACE |   FOUND[4] default_pool_size = 25
[16:04:16] TRACE |   FOUND[5] pool_mode = transaction
[16:04:16] TRACE | === END FACT COVERAGE ===
[16:04:16] TRACE | === FACT COVERAGE ANALYSIS ===
[16:04:16] TRACE | Query: What's the purpose of the connection pooler?
[16:04:16] TRACE | Total key facts: 5
[16:04:16] TRACE | Found facts: 2/5
[16:04:16] TRACE |   FOUND[1] PgBouncer
[16:04:16] TRACE |   FOUND[2] connection pooling
[16:04:16] TRACE | Missed facts: 3/5
[16:04:16] TRACE |   MISSED[1] max_db_connections = 100
[16:04:16] TRACE |     -> Found in chunk_id=deployment_guide_fix_3 (rank not in top-5)
[16:04:16] TRACE |   MISSED[2] default_pool_size = 25
[16:04:16] TRACE |     -> Found in chunk_id=deployment_guide_fix_3 (rank not in top-5)
[16:04:16] TRACE |   MISSED[3] pool_mode = transaction
[16:04:16] TRACE |     -> Found in chunk_id=deployment_guide_fix_3 (rank not in top-5)
[16:04:16] TRACE | === END FACT COVERAGE ===
[16:04:16] TRACE | === FACT COVERAGE ANALYSIS ===
[16:04:16] TRACE | Query: Should I connect directly to PostgreSQL or through PgBouncer?
[16:04:16] TRACE | Total key facts: 5
[16:04:16] TRACE | Found facts: 5/5
[16:04:16] TRACE |   FOUND[1] PgBouncer
[16:04:16] TRACE |   FOUND[2] connection pooling
[16:04:16] TRACE |   FOUND[3] max_db_connections = 100
[16:04:16] TRACE |   FOUND[4] default_pool_size = 25
[16:04:16] TRACE |   FOUND[5] pool_mode = transaction
[16:04:16] TRACE | === END FACT COVERAGE ===
[16:04:16] TRACE | === FACT COVERAGE ANALYSIS ===
[16:04:16] TRACE | Query: pgbouncer purpose
[16:04:16] TRACE | Total key facts: 5
[16:04:16] TRACE | Found facts: 2/5
[16:04:16] TRACE |   FOUND[1] PgBouncer
[16:04:16] TRACE |   FOUND[2] connection pooling
[16:04:16] TRACE | Missed facts: 3/5
[16:04:16] TRACE |   MISSED[1] max_db_connections = 100
[16:04:16] TRACE |     -> Found in chunk_id=deployment_guide_fix_3 (rank not in top-5)
[16:04:16] TRACE |   MISSED[2] default_pool_size = 25
[16:04:16] TRACE |     -> Found in chunk_id=deployment_guide_fix_3 (rank not in top-5)
[16:04:16] TRACE |   MISSED[3] pool_mode = transaction
[16:04:16] TRACE |     -> Found in chunk_id=deployment_guide_fix_3 (rank not in top-5)
[16:04:16] TRACE | === END FACT COVERAGE ===
[16:04:16] TRACE | === FACT COVERAGE ANALYSIS ===
[16:04:16] TRACE | Query: Optimizing database connections, what role does PgBouncer play?
[16:04:16] TRACE | Total key facts: 5
[16:04:16] TRACE | Found facts: 5/5
[16:04:16] TRACE |   FOUND[1] PgBouncer
[16:04:16] TRACE |   FOUND[2] connection pooling
[16:04:16] TRACE |   FOUND[3] max_db_connections = 100
[16:04:16] TRACE |   FOUND[4] default_pool_size = 25
[16:04:16] TRACE |   FOUND[5] pool_mode = transaction
[16:04:16] TRACE | === END FACT COVERAGE ===
[16:04:16] TRACE | === FACT COVERAGE ANALYSIS ===
[16:04:16] TRACE | Query: Why can't I connect directly to the database?
[16:04:16] TRACE | Total key facts: 5
[16:04:16] TRACE | Found facts: 2/5
[16:04:16] TRACE |   FOUND[1] PgBouncer
[16:04:16] TRACE |   FOUND[2] connection pooling
[16:04:16] TRACE | Missed facts: 3/5
[16:04:16] TRACE |   MISSED[1] max_db_connections = 100
[16:04:16] TRACE |     -> Found in chunk_id=deployment_guide_fix_3 (rank not in top-5)
[16:04:16] TRACE |   MISSED[2] default_pool_size = 25
[16:04:16] TRACE |     -> Found in chunk_id=deployment_guide_fix_3 (rank not in top-5)
[16:04:16] TRACE |   MISSED[3] pool_mode = transaction
[16:04:16] TRACE |     -> Found in chunk_id=deployment_guide_fix_3 (rank not in top-5)
[16:04:16] TRACE | === END FACT COVERAGE ===
[16:04:16] TRACE | === FACT COVERAGE ANALYSIS ===
[16:04:16] TRACE | Query: How do I implement retry logic for failed workflow steps?
[16:04:16] TRACE | Total key facts: 4
[16:04:16] TRACE | Found facts: 4/4
[16:04:16] TRACE |   FOUND[1] max_attempts: 3
[16:04:16] TRACE |   FOUND[2] backoff_type: exponential
[16:04:16] TRACE |   FOUND[3] initial_interval: 1000
[16:04:16] TRACE |   FOUND[4] max retries: 3
[16:04:16] TRACE | === END FACT COVERAGE ===
[16:04:16] TRACE | === FACT COVERAGE ANALYSIS ===
[16:04:16] TRACE | Query: What's the retry strategy for transient failures?
[16:04:16] TRACE | Total key facts: 4
[16:04:16] TRACE | Found facts: 3/4
[16:04:16] TRACE |   FOUND[1] max_attempts: 3
[16:04:16] TRACE |   FOUND[2] initial_interval: 1000
[16:04:16] TRACE |   FOUND[3] max retries: 3
[16:04:16] TRACE | Missed facts: 1/4
[16:04:16] TRACE |   MISSED[1] backoff_type: exponential
[16:04:16] TRACE |     -> Found in chunk_id=user_guide_fix_7 (rank not in top-5)
[16:04:16] TRACE | === END FACT COVERAGE ===
[16:04:16] TRACE | === FACT COVERAGE ANALYSIS ===
[16:04:16] TRACE | Query: My workflow fails on temporary network errors
[16:04:16] TRACE | Total key facts: 4
[16:04:16] TRACE | Found facts: 0/4
[16:04:16] TRACE | Missed facts: 4/4
[16:04:16] TRACE |   MISSED[1] max_attempts: 3
[16:04:16] TRACE |     -> Found in chunk_id=user_guide_fix_5 (rank not in top-5)
[16:04:16] TRACE |   MISSED[2] backoff_type: exponential
[16:04:16] TRACE |     -> Found in chunk_id=user_guide_fix_7 (rank not in top-5)
[16:04:16] TRACE |   MISSED[3] initial_interval: 1000
[16:04:16] TRACE |     -> Found in chunk_id=user_guide_fix_5 (rank not in top-5)
[16:04:16] TRACE |   MISSED[4] max retries: 3
[16:04:16] TRACE |     -> Found in chunk_id=troubleshooting_guide_fix_5 (rank not in top-5)
[16:04:16] TRACE | === END FACT COVERAGE ===
[16:04:16] TRACE | === FACT COVERAGE ANALYSIS ===
[16:04:16] TRACE | Query: retry config
[16:04:16] TRACE | Total key facts: 4
[16:04:16] TRACE | Found facts: 3/4
[16:04:16] TRACE |   FOUND[1] max_attempts: 3
[16:04:16] TRACE |   FOUND[2] initial_interval: 1000
[16:04:16] TRACE |   FOUND[3] max retries: 3
[16:04:16] TRACE | Missed facts: 1/4
[16:04:16] TRACE |   MISSED[1] backoff_type: exponential
[16:04:16] TRACE |     -> Found in chunk_id=user_guide_fix_7 (rank not in top-5)
[16:04:16] TRACE | === END FACT COVERAGE ===
[16:04:16] TRACE | === FACT COVERAGE ANALYSIS ===
[16:04:16] TRACE | Query: I want workflows to automatically retry on errors, what are the options?
[16:04:16] TRACE | Total key facts: 4
[16:04:16] TRACE | Found facts: 3/4
[16:04:16] TRACE |   FOUND[1] max_attempts: 3
[16:04:16] TRACE |   FOUND[2] initial_interval: 1000
[16:04:16] TRACE |   FOUND[3] max retries: 3
[16:04:16] TRACE | Missed facts: 1/4
[16:04:16] TRACE |   MISSED[1] backoff_type: exponential
[16:04:16] TRACE |     -> Found in chunk_id=user_guide_fix_7 (rank not in top-5)
[16:04:16] TRACE | === END FACT COVERAGE ===
[16:04:16] TRACE | === FACT COVERAGE ANALYSIS ===
[16:04:16] TRACE | Query: Why doesn't my workflow retry after failing?
[16:04:16] TRACE | Total key facts: 4
[16:04:16] TRACE | Found facts: 3/4
[16:04:16] TRACE |   FOUND[1] max_attempts: 3
[16:04:16] TRACE |   FOUND[2] initial_interval: 1000
[16:04:16] TRACE |   FOUND[3] max retries: 3
[16:04:16] TRACE | Missed facts: 1/4
[16:04:16] TRACE |   MISSED[1] backoff_type: exponential
[16:04:16] TRACE |     -> Found in chunk_id=user_guide_fix_7 (rank not in top-5)
[16:04:16] TRACE | === END FACT COVERAGE ===
[16:04:16] TRACE | === FACT COVERAGE ANALYSIS ===
[16:04:16] TRACE | Query: What Helm chart repository should I use for deployment?
[16:04:16] TRACE | Total key facts: 1
[16:04:16] TRACE | Found facts: 1/1
[16:04:16] TRACE |   FOUND[1] helm repo add cloudflow https://charts.cloudflow.io
[16:04:16] TRACE | === END FACT COVERAGE ===
[16:04:16] TRACE | === FACT COVERAGE ANALYSIS ===
[16:04:16] TRACE | Query: Where is the CloudFlow Helm chart hosted?
[16:04:16] TRACE | Total key facts: 1
[16:04:16] TRACE | Found facts: 1/1
[16:04:16] TRACE |   FOUND[1] helm repo add cloudflow https://charts.cloudflow.io
[16:04:16] TRACE | === END FACT COVERAGE ===
[16:04:16] TRACE | === FACT COVERAGE ANALYSIS ===
[16:04:16] TRACE | Query: helm repo add is failing, what's the correct URL?
[16:04:16] TRACE | Total key facts: 1
[16:04:16] TRACE | Found facts: 1/1
[16:04:16] TRACE |   FOUND[1] helm repo add cloudflow https://charts.cloudflow.io
[16:04:16] TRACE | === END FACT COVERAGE ===
[16:04:16] TRACE | === FACT COVERAGE ANALYSIS ===
[16:04:16] TRACE | Query: helm repo
[16:04:16] TRACE | Total key facts: 1
[16:04:16] TRACE | Found facts: 1/1
[16:04:16] TRACE |   FOUND[1] helm repo add cloudflow https://charts.cloudflow.io
[16:04:16] TRACE | === END FACT COVERAGE ===
[16:04:16] TRACE | === FACT COVERAGE ANALYSIS ===
[16:04:16] TRACE | Query: Setting up deployment pipeline, which Helm repository has CloudFlow charts?
[16:04:16] TRACE | Total key facts: 1
[16:04:16] TRACE | Found facts: 1/1
[16:04:16] TRACE |   FOUND[1] helm repo add cloudflow https://charts.cloudflow.io
[16:04:16] TRACE | === END FACT COVERAGE ===
[16:04:16] TRACE | === FACT COVERAGE ANALYSIS ===
[16:04:16] TRACE | Query: Why can't I find CloudFlow in the official Helm hub?
[16:04:16] TRACE | Total key facts: 1
[16:04:16] TRACE | Found facts: 1/1
[16:04:16] TRACE |   FOUND[1] helm repo add cloudflow https://charts.cloudflow.io
[16:04:16] TRACE | === END FACT COVERAGE ===
[16:04:16] TRACE | === FACT COVERAGE ANALYSIS ===
[16:04:16] TRACE | Query: What is the minimum scheduling interval for workflows?
[16:04:16] TRACE | Total key facts: 2
[16:04:16] TRACE | Found facts: 0/2
[16:04:16] TRACE | Missed facts: 2/2
[16:04:16] TRACE |   MISSED[1] minimum scheduling interval is 1 minute
[16:04:16] TRACE |   MISSED[2] The minimum scheduling interval is **1 minute**
[16:04:16] TRACE |     -> Found in chunk_id=user_guide_fix_4 (rank not in top-5)
[16:04:16] TRACE | === END FACT COVERAGE ===
[16:04:16] TRACE | === FACT COVERAGE ANALYSIS ===
[16:04:16] TRACE | Query: How frequently can I schedule a workflow?
[16:04:16] TRACE | Total key facts: 2
[16:04:16] TRACE | Found facts: 1/2
[16:04:16] TRACE |   FOUND[1] The minimum scheduling interval is **1 minute**
[16:04:16] TRACE | Missed facts: 1/2
[16:04:16] TRACE |   MISSED[1] minimum scheduling interval is 1 minute
[16:04:16] TRACE | === END FACT COVERAGE ===
[16:04:16] TRACE | === FACT COVERAGE ANALYSIS ===
[16:04:16] TRACE | Query: My every-30-seconds schedule is being rejected
[16:04:16] TRACE | Total key facts: 2
[16:04:16] TRACE | Found facts: 1/2
[16:04:16] TRACE |   FOUND[1] The minimum scheduling interval is **1 minute**
[16:04:16] TRACE | Missed facts: 1/2
[16:04:16] TRACE |   MISSED[1] minimum scheduling interval is 1 minute
[16:04:16] TRACE | === END FACT COVERAGE ===
[16:04:16] TRACE | === FACT COVERAGE ANALYSIS ===
[16:04:16] TRACE | Query: min schedule interval
[16:04:16] TRACE | Total key facts: 2
[16:04:16] TRACE | Found facts: 1/2
[16:04:16] TRACE |   FOUND[1] The minimum scheduling interval is **1 minute**
[16:04:16] TRACE | Missed facts: 1/2
[16:04:16] TRACE |   MISSED[1] minimum scheduling interval is 1 minute
[16:04:16] TRACE | === END FACT COVERAGE ===
[16:04:16] TRACE | === FACT COVERAGE ANALYSIS ===
[16:04:16] TRACE | Query: I need near real-time execution, what's the fastest schedule I can set?
[16:04:16] TRACE | Total key facts: 2
[16:04:16] TRACE | Found facts: 1/2
[16:04:16] TRACE |   FOUND[1] The minimum scheduling interval is **1 minute**
[16:04:16] TRACE | Missed facts: 1/2
[16:04:16] TRACE |   MISSED[1] minimum scheduling interval is 1 minute
[16:04:16] TRACE | === END FACT COVERAGE ===
[16:04:16] TRACE | === FACT COVERAGE ANALYSIS ===
[16:04:16] TRACE | Query: Why can't I schedule workflows every 30 seconds?
[16:04:16] TRACE | Total key facts: 2
[16:04:16] TRACE | Found facts: 1/2
[16:04:16] TRACE |   FOUND[1] The minimum scheduling interval is **1 minute**
[16:04:16] TRACE | Missed facts: 1/2
[16:04:16] TRACE |   MISSED[1] minimum scheduling interval is 1 minute
[16:04:16] TRACE | === END FACT COVERAGE ===
[16:04:16] INFO  |   k=5 exact_match: 83.0% (44/53)
[16:04:16] INFO  |     synonym: 66.0%
[16:04:16] INFO  |     problem: 64.2%
[16:04:16] INFO  |     casual: 71.7%
[16:04:16] INFO  |     contextual: 69.8%
[16:04:16] INFO  |     negation: 52.8%
[16:04:16] METRIC | total_benchmark_time=190.805s

================================================================================
[16:04:16] INFO  | SAVING RESULTS
================================================================================
[16:04:17] INFO  | Saved full results to /home/fujin/Code/personal-library-manager/poc/chunking_benchmark_v2/results/2026-01-25_160106/benchmark_results.json
[16:04:17] INFO  | Saved summary to /home/fujin/Code/personal-library-manager/poc/chunking_benchmark_v2/results/2026-01-25_160106/summary.json

================================================================================
[16:04:17] INFO  | BENCHMARK COMPLETE
================================================================================
[16:04:17] INFO  | Total evaluations: 4

Coverage by Query Dimension
Dimension  | Coverage | Delta 
-----------+----------+-------
casual     | 55.2%    | -8.0% 
contextual | 51.4%    | -11.8%
negation   | 47.2%    | -16.0%
original   | 63.2%    | -     
problem    | 59.4%    | -3.8% 
synonym    | 47.6%    | -15.6%

================================================================================
[16:04:17] INFO  | RUN SUMMARY
================================================================================
[16:04:17] INFO  | Total runtime: 190.89s

------------------------------------------------------------
[16:04:17] INFO  | Timing Breakdown
------------------------------------------------------------
Phase           | Duration
----------------+---------
total_benchmark | 190.81s 
[16:04:17] INFO  | Log saved to: /home/fujin/Code/personal-library-manager/poc/chunking_benchmark_v2/results/2026-01-25_160106/benchmark.log
