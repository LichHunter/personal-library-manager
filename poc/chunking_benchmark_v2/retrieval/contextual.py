"""Contextual Retrieval strategy.

LLM-based contextual enrichment where each chunk is prepended with a 1-2 sentence
context summary generated by Claude Haiku. Uses hybrid retrieval (BM25 + semantic)
on enriched chunks.

Target queries: tmp_004, imp_001, cmp_003 (ambiguous queries needing context)
"""

import json
from pathlib import Path
from typing import Optional

import numpy as np
from rank_bm25 import BM25Okapi

from strategies import Chunk, Document
from .base import RetrievalStrategy, EmbedderMixin, StructuredDocument
from .gem_utils import reciprocal_rank_fusion
from enrichment.provider import call_llm


CONTEXT_PROMPT = """Summarize this chunk's topic and key facts in 1-2 sentences.
Document: {doc_title}
Chunk: {chunk_content}
Context:"""

TOKENS_PER_CHUNK = 200
COST_PER_MILLION_TOKENS = 0.25


class ContextualRetrieval(RetrievalStrategy, EmbedderMixin):
    """Contextual retrieval with LLM-generated context enrichment.

    Each chunk is prepended with a 1-2 sentence context summary generated
    by Claude Haiku. Retrieval uses hybrid BM25 + semantic on enriched chunks.
    """

    def __init__(
        self,
        name: str = "contextual",
        cache_dir: str = "contextual_index",
        **kwargs,
    ):
        super().__init__(name, **kwargs)
        self.chunks: Optional[list[Chunk]] = None
        self.embeddings: Optional[np.ndarray] = None
        self.bm25: Optional[BM25Okapi] = None

        self.cache_dir = Path(cache_dir)
        self.cache_file = self.cache_dir / "context_cache.json"
        self.context_cache: dict[str, str] = {}

        self.llm_calls = 0
        self.cache_hits = 0

        self.doc_lookup: dict[str, Document] = {}

        self._load_cache()

    def _load_cache(self) -> None:
        if self.cache_file.exists():
            try:
                with open(self.cache_file, "r") as f:
                    self.context_cache = json.load(f)
            except (json.JSONDecodeError, IOError) as e:
                print(f"Warning: Failed to load context cache: {e}")
                self.context_cache = {}

    def _save_cache(self) -> None:
        self.cache_dir.mkdir(parents=True, exist_ok=True)
        try:
            with open(self.cache_file, "w") as f:
                json.dump(self.context_cache, f, indent=2)
        except IOError as e:
            print(f"Warning: Failed to save context cache: {e}")

    def _get_context(self, chunk: Chunk, documents: list[Document]) -> str:
        if chunk.id in self.context_cache:
            self.cache_hits += 1
            return self.context_cache[chunk.id]

        doc_title = "Unknown"
        if chunk.doc_id in self.doc_lookup:
            doc_title = self.doc_lookup[chunk.doc_id].title

        prompt = CONTEXT_PROMPT.format(
            doc_title=doc_title,
            chunk_content=chunk.content[:1500],
        )

        try:
            context = call_llm(prompt, model="claude-haiku", timeout=10)
            context = context.strip()
            self.llm_calls += 1
        except Exception as e:
            print(f"Warning: LLM context generation failed for {chunk.id}: {e}")
            context = f"From document: {doc_title}"

        self.context_cache[chunk.id] = context
        return context

    def index(
        self,
        chunks: list[Chunk],
        documents: Optional[list[Document]] = None,
        structured_docs: Optional[list[StructuredDocument]] = None,
    ) -> None:
        if self.embedder is None:
            raise ValueError("Embedder not set. Call set_embedder() first.")

        if documents:
            self.doc_lookup = {doc.id: doc for doc in documents}

        self.llm_calls = 0
        self.cache_hits = 0

        enriched_chunks = []
        for chunk in chunks:
            context = self._get_context(chunk, documents or [])
            enriched_content = f"{context}\n\n{chunk.content}"
            enriched_chunk = Chunk(
                id=chunk.id,
                doc_id=chunk.doc_id,
                content=enriched_content,
                start_char=chunk.start_char,
                end_char=chunk.end_char,
                heading=chunk.heading,
                heading_path=chunk.heading_path,
                level=chunk.level,
                parent_id=chunk.parent_id,
                children_ids=chunk.children_ids,
                metadata={**chunk.metadata, "has_context": True},
            )
            enriched_chunks.append(enriched_chunk)

        self._save_cache()

        self.chunks = enriched_chunks

        texts = [chunk.content for chunk in enriched_chunks]
        self.embeddings = self.encode_texts(texts)

        tokenized = [text.lower().split() for text in texts]
        self.bm25 = BM25Okapi(tokenized)

    def retrieve(self, query: str, k: int = 5) -> list[Chunk]:
        if self.chunks is None or self.embeddings is None or self.bm25 is None:
            return []

        bm25_scores = self.bm25.get_scores(query.lower().split())
        bm25_ranks = np.argsort(bm25_scores)[::-1][:20]
        bm25_results = [self.chunks[i] for i in bm25_ranks]

        q_emb = self.encode_query(query)
        sem_scores = np.dot(self.embeddings, q_emb)
        sem_ranks = np.argsort(sem_scores)[::-1][:20]
        sem_results = [self.chunks[i] for i in sem_ranks]

        fused = reciprocal_rank_fusion([bm25_results, sem_results], k=60)

        return fused[:k]

    def get_index_stats(self) -> dict:
        estimated_cost = (
            self.llm_calls * TOKENS_PER_CHUNK * COST_PER_MILLION_TOKENS
        ) / 1_000_000

        return {
            "num_chunks": len(self.chunks) if self.chunks else 0,
            "embedding_dim": self.embeddings.shape[1]
            if self.embeddings is not None
            else 0,
            "bm25_avg_doc_len": self.bm25.avgdl if self.bm25 else 0,
            "context_cache_size": len(self.context_cache),
            "llm_calls": self.llm_calls,
            "cache_hits": self.cache_hits,
            "estimated_cost_usd": round(estimated_cost, 4),
        }
