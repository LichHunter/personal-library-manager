{
  "description": "Human-like query variations for RAG benchmark evaluation",
  "version": "1.0.1",
  "generated_at": "2026-01-24T12:43:46.516254",
  "schema_file": "human_queries_schema.json",
  "statistics": {
    "total_queries": 53,
    "total_variations": 265,
    "variations_per_query": 5,
    "dimensions": [
      "synonym",
      "problem",
      "casual",
      "contextual",
      "negation"
    ],
    "categories": {
      "simple_lookup": {
        "queries": 10,
        "variations": 50
      },
      "cross_document": {
        "queries": 5,
        "variations": 25
      },
      "how_to": {
        "queries": 7,
        "variations": 35
      },
      "comparison": {
        "queries": 4,
        "variations": 20
      },
      "troubleshooting": {
        "queries": 6,
        "variations": 30
      },
      "architecture": {
        "queries": 5,
        "variations": 25
      },
      "api_reference": {
        "queries": 8,
        "variations": 40
      },
      "release_notes": {
        "queries": 4,
        "variations": 20
      },
      "complex_multi_hop": {
        "queries": 4,
        "variations": 20
      }
    }
  },
  "queries": [
    {
      "id": "simple_001",
      "category": "simple_lookup",
      "original_query": "What is the default rate limit for API requests?",
      "human_queries": [
        {
          "dimension": "synonym",
          "query": "what's the API throttling limit per user"
        },
        {
          "dimension": "problem",
          "query": "my requests keep getting rejected with 429 too many requests"
        },
        {
          "dimension": "casual",
          "query": "api rate limit"
        },
        {
          "dimension": "contextual",
          "query": "building a batch sync tool that hits the API, need to know the request cap"
        },
        {
          "dimension": "negation",
          "query": "why is the API blocking my requests after a while"
        }
      ],
      "key_facts": [
        "100 requests per minute",
        "rate limit"
      ],
      "key_facts_semantic": [
        "100 req/min",
        "100 rpm",
        "hundred requests per minute",
        "throttled at 100",
        "100 per min",
        "rate limited to 100"
      ],
      "expected_docs": [
        "api_workflows",
        "arch_api_gateway"
      ],
      "ground_truth_answer": "The default rate limit is 100 requests per minute per user.",
      "metadata": {
        "concepts": [
          "rate-limiting",
          "api-throttling",
          "request-quotas"
        ],
        "related_terms": [
          "throttle",
          "quota",
          "cap",
          "429",
          "too many requests",
          "sliding window",
          "redis-backed"
        ],
        "user_intent": "understand_api_limits",
        "difficulty": "easy",
        "requires_inference": false,
        "domain": "api"
      }
    },
    {
      "id": "simple_002",
      "category": "simple_lookup",
      "original_query": "What database does CloudFlow use?",
      "human_queries": [
        {
          "dimension": "synonym",
          "query": "what's the primary data store in CloudFlow"
        },
        {
          "dimension": "problem",
          "query": "getting database-specific errors from CloudFlow, what DB does it use"
        },
        {
          "dimension": "casual",
          "query": "cloudflow db?"
        },
        {
          "dimension": "contextual",
          "query": "need to write raw queries against CloudFlow's backend, what DB engine is it"
        },
        {
          "dimension": "negation",
          "query": "is CloudFlow not using a NoSQL database"
        }
      ],
      "key_facts": [
        "PostgreSQL",
        "JSONB"
      ],
      "key_facts_semantic": [
        "Postgres",
        "pg",
        "psql",
        "JSON binary",
        "relational database",
        "SQL database",
        "PostgreSQL with JSONB"
      ],
      "expected_docs": [
        "adr_001",
        "config_database"
      ],
      "ground_truth_answer": "CloudFlow uses PostgreSQL as the primary database with JSONB for flexible data.",
      "metadata": {
        "concepts": [
          "database",
          "data-storage",
          "relational-db"
        ],
        "related_terms": [
          "SQL",
          "relational",
          "data store",
          "persistence",
          "PgBouncer",
          "connection pool"
        ],
        "user_intent": "identify_technology_stack",
        "difficulty": "easy",
        "requires_inference": false,
        "domain": "database"
      }
    },
    {
      "id": "simple_003",
      "category": "simple_lookup",
      "original_query": "How long do access tokens last?",
      "expected_docs": [
        "adr_003",
        "arch_authentication_service"
      ],
      "human_queries": [
        {
          "dimension": "synonym",
          "query": "what's the JWT token expiration time"
        },
        {
          "dimension": "problem",
          "query": "users are getting logged out after about an hour, is that expected"
        },
        {
          "dimension": "casual",
          "query": "token expiry"
        },
        {
          "dimension": "contextual",
          "query": "implementing token refresh logic, need to know when access tokens expire"
        },
        {
          "dimension": "negation",
          "query": "why does my token stop working after a while"
        }
      ],
      "key_facts": [
        "1 hour",
        "7 days",
        "refresh tokens"
      ],
      "key_facts_semantic": [
        "60 minutes",
        "one hour",
        "1h",
        "7d",
        "seven days",
        "week-long refresh",
        "access token 1 hour",
        "refresh token 7 days"
      ],
      "ground_truth_answer": "Access tokens expire after 1 hour. Refresh tokens last 7 days.",
      "metadata": {
        "concepts": [
          "authentication",
          "token-management",
          "session-lifetime"
        ],
        "related_terms": [
          "JWT",
          "expiration",
          "expiry",
          "token lifetime",
          "refresh",
          "OAuth",
          "RS256"
        ],
        "user_intent": "understand_token_lifecycle",
        "difficulty": "easy",
        "requires_inference": false,
        "domain": "security"
      }
    },
    {
      "id": "simple_004",
      "category": "simple_lookup",
      "original_query": "What is the maximum workflow execution time?",
      "expected_docs": [
        "config_workflow_engine"
      ],
      "human_queries": [
        {
          "dimension": "synonym",
          "query": "what's the workflow timeout limit"
        },
        {
          "dimension": "problem",
          "query": "my long-running workflow keeps getting killed before it finishes"
        },
        {
          "dimension": "casual",
          "query": "workflow timeout"
        },
        {
          "dimension": "contextual",
          "query": "planning a data migration workflow, need to know the max execution duration"
        },
        {
          "dimension": "negation",
          "query": "why does my workflow fail after running for a long time"
        }
      ],
      "key_facts": [
        "3600",
        "WORKFLOW_TIMEOUT_SECONDS"
      ],
      "key_facts_semantic": [
        "3600 seconds",
        "1 hour",
        "60 minutes",
        "one hour timeout",
        "1h max",
        "3600s",
        "workflow times out at 3600"
      ],
      "ground_truth_answer": "The maximum workflow execution time is 3600 seconds (1 hour).",
      "metadata": {
        "concepts": [
          "workflow-execution",
          "timeout",
          "limits"
        ],
        "related_terms": [
          "execution time",
          "max duration",
          "timeout",
          "kill",
          "terminate",
          "limit"
        ],
        "user_intent": "understand_execution_limits",
        "difficulty": "easy",
        "requires_inference": false,
        "domain": "workflow"
      }
    },
    {
      "id": "simple_005",
      "category": "simple_lookup",
      "original_query": "What container orchestration does CloudFlow use?",
      "expected_docs": [
        "adr_002"
      ],
      "human_queries": [
        {
          "dimension": "synonym",
          "query": "what platform manages CloudFlow's containers"
        },
        {
          "dimension": "problem",
          "query": "seeing kubectl commands in the docs, so it's running on k8s?"
        },
        {
          "dimension": "casual",
          "query": "cloudflow k8s or ecs?"
        },
        {
          "dimension": "contextual",
          "query": "need to understand the deployment platform for capacity planning"
        },
        {
          "dimension": "negation",
          "query": "CloudFlow isn't running on serverless is it"
        }
      ],
      "key_facts": [
        "Kubernetes",
        "EKS",
        "Helm"
      ],
      "key_facts_semantic": [
        "k8s",
        "K8s on AWS",
        "Kubernetes on EKS",
        "k8s with Helm",
        "EKS cluster",
        "Helm charts",
        "container orchestrator is Kubernetes"
      ],
      "ground_truth_answer": "CloudFlow uses Kubernetes on EKS with Helm for deployments.",
      "metadata": {
        "concepts": [
          "container-orchestration",
          "deployment-platform",
          "infrastructure"
        ],
        "related_terms": [
          "k8s",
          "Docker",
          "containers",
          "pods",
          "ECS",
          "Lambda",
          "serverless",
          "ArgoCD",
          "GitOps"
        ],
        "user_intent": "identify_infrastructure_platform",
        "difficulty": "easy",
        "requires_inference": false,
        "domain": "devops"
      }
    },
    {
      "id": "simple_006",
      "category": "simple_lookup",
      "original_query": "What is the cache TTL?",
      "expected_docs": [
        "config_cache"
      ],
      "human_queries": [
        {
          "dimension": "synonym",
          "query": "how long does cached data stay valid"
        },
        {
          "dimension": "problem",
          "query": "changes aren't showing up immediately, probably cached - how long til it refreshes"
        },
        {
          "dimension": "casual",
          "query": "cache expiry time"
        },
        {
          "dimension": "contextual",
          "query": "debugging stale data issues, need to know the default cache duration"
        },
        {
          "dimension": "negation",
          "query": "why isn't my data updating right away"
        }
      ],
      "key_facts": [
        "300",
        "CACHE_TTL_SECONDS"
      ],
      "key_facts_semantic": [
        "300 seconds",
        "5 minutes",
        "5 min",
        "five minutes",
        "300s",
        "cache expires in 300",
        "ttl is 300"
      ],
      "ground_truth_answer": "The default cache TTL is 300 seconds.",
      "metadata": {
        "concepts": [
          "caching",
          "ttl",
          "data-freshness"
        ],
        "related_terms": [
          "time to live",
          "expiration",
          "stale data",
          "Redis",
          "invalidation",
          "refresh"
        ],
        "user_intent": "understand_cache_behavior",
        "difficulty": "easy",
        "requires_inference": false,
        "domain": "caching"
      }
    },
    {
      "id": "simple_007",
      "category": "simple_lookup",
      "original_query": "What encryption is used for data at rest?",
      "expected_docs": [
        "howto_set_up_data_encryption"
      ],
      "human_queries": [
        {
          "dimension": "synonym",
          "query": "what cipher does CloudFlow use for stored data"
        },
        {
          "dimension": "problem",
          "query": "compliance team asking about our data encryption standard"
        },
        {
          "dimension": "casual",
          "query": "encryption at rest algo"
        },
        {
          "dimension": "contextual",
          "query": "filling out security questionnaire, need to know the encryption standard for stored data"
        },
        {
          "dimension": "negation",
          "query": "is our stored data actually encrypted or not"
        }
      ],
      "key_facts": [
        "AES-256",
        "encryption at rest"
      ],
      "key_facts_semantic": [
        "AES 256",
        "AES-256-bit",
        "256-bit AES",
        "AES256",
        "Advanced Encryption Standard 256",
        "encrypted with AES-256"
      ],
      "ground_truth_answer": "All data is encrypted at rest using AES-256.",
      "metadata": {
        "concepts": [
          "encryption",
          "data-security",
          "compliance"
        ],
        "related_terms": [
          "cipher",
          "cryptography",
          "KMS",
          "at rest",
          "stored data",
          "security"
        ],
        "user_intent": "verify_security_compliance",
        "difficulty": "easy",
        "requires_inference": false,
        "domain": "security"
      }
    },
    {
      "id": "simple_008",
      "category": "simple_lookup",
      "original_query": "What is the maximum number of steps in a workflow?",
      "expected_docs": [
        "config_workflow_engine"
      ],
      "human_queries": [
        {
          "dimension": "synonym",
          "query": "workflow step count limit"
        },
        {
          "dimension": "problem",
          "query": "workflow creation failing with 'too many steps' error"
        },
        {
          "dimension": "casual",
          "query": "max steps per workflow"
        },
        {
          "dimension": "contextual",
          "query": "designing a complex ETL workflow, need to know if there's a step limit"
        },
        {
          "dimension": "negation",
          "query": "why can't I add more steps to my workflow"
        }
      ],
      "key_facts": [
        "100",
        "WORKFLOW_MAX_STEPS"
      ],
      "key_facts_semantic": [
        "100 steps",
        "hundred steps",
        "max 100",
        "limit of 100 steps",
        "capped at 100",
        "100 step maximum"
      ],
      "ground_truth_answer": "The maximum number of steps per workflow is 100.",
      "metadata": {
        "concepts": [
          "workflow-design",
          "limits",
          "configuration"
        ],
        "related_terms": [
          "steps",
          "tasks",
          "nodes",
          "limit",
          "cap",
          "maximum"
        ],
        "user_intent": "understand_workflow_constraints",
        "difficulty": "easy",
        "requires_inference": false,
        "domain": "workflow"
      }
    },
    {
      "id": "simple_009",
      "category": "simple_lookup",
      "original_query": "What is the API gateway timeout?",
      "expected_docs": [
        "config_api_gateway"
      ],
      "human_queries": [
        {
          "dimension": "synonym",
          "query": "what's the request timeout at the gateway level"
        },
        {
          "dimension": "problem",
          "query": "long-running API calls timing out with 504 gateway timeout"
        },
        {
          "dimension": "casual",
          "query": "gateway timeout config"
        },
        {
          "dimension": "contextual",
          "query": "have a slow endpoint that takes 45 seconds, will it hit the gateway timeout"
        },
        {
          "dimension": "negation",
          "query": "why are my slow requests getting cut off"
        }
      ],
      "key_facts": [
        "30000",
        "GATEWAY_TIMEOUT_MS"
      ],
      "key_facts_semantic": [
        "30000ms",
        "30 seconds",
        "30s",
        "30000 milliseconds",
        "thirty seconds",
        "30 sec timeout",
        "gateway times out at 30s"
      ],
      "ground_truth_answer": "The API gateway timeout is 30000 milliseconds.",
      "metadata": {
        "concepts": [
          "api-gateway",
          "timeout",
          "request-handling"
        ],
        "related_terms": [
          "request timeout",
          "504",
          "gateway timeout",
          "Kong",
          "connection timeout"
        ],
        "user_intent": "understand_timeout_behavior",
        "difficulty": "easy",
        "requires_inference": false,
        "domain": "api"
      }
    },
    {
      "id": "simple_010",
      "category": "simple_lookup",
      "original_query": "What message broker is used for workflows?",
      "expected_docs": [
        "adr_005"
      ],
      "human_queries": [
        {
          "dimension": "synonym",
          "query": "what messaging system handles workflow events"
        },
        {
          "dimension": "problem",
          "query": "seeing consumer lag alerts, what message queue does CloudFlow use"
        },
        {
          "dimension": "casual",
          "query": "workflow message broker"
        },
        {
          "dimension": "contextual",
          "query": "need to monitor workflow throughput, what queue system should I look at"
        },
        {
          "dimension": "negation",
          "query": "workflows aren't using synchronous calls right"
        }
      ],
      "key_facts": [
        "Kafka",
        "event-driven"
      ],
      "key_facts_semantic": [
        "Apache Kafka",
        "Kafka messaging",
        "Kafka broker",
        "event-driven with Kafka",
        "pub-sub with Kafka",
        "Kafka for events"
      ],
      "ground_truth_answer": "CloudFlow uses Kafka for event-driven workflow execution.",
      "metadata": {
        "concepts": [
          "messaging",
          "event-driven-architecture",
          "async-processing"
        ],
        "related_terms": [
          "message queue",
          "pub-sub",
          "events",
          "consumer",
          "producer",
          "topic",
          "dead letter queue"
        ],
        "user_intent": "identify_messaging_infrastructure",
        "difficulty": "easy",
        "requires_inference": false,
        "domain": "messaging"
      }
    },
    {
      "id": "cross_011",
      "category": "cross_document",
      "original_query": "How is authentication implemented across the system?",
      "human_queries": [
        {
          "dimension": "synonym",
          "query": "what's the login and authorization setup in cloudflow",
          "notes": "Replaced 'authentication' with 'login and authorization', 'implemented' with 'setup'"
        },
        {
          "dimension": "problem",
          "query": "users are getting 401 unauthorized on API calls even with tokens that were working yesterday",
          "notes": "Describes authentication failure symptoms without asking for solution"
        },
        {
          "dimension": "casual",
          "query": "auth flow details",
          "notes": "Terse Slack-style query, minimal words"
        },
        {
          "dimension": "contextual",
          "query": "building a third-party integration that needs to call cloudflow APIs on behalf of users, need to understand the full auth chain from credentials to token validation",
          "notes": "Real use case: integrating with CloudFlow, needs full auth picture"
        },
        {
          "dimension": "negation",
          "query": "why isn't there a simpler auth mechanism instead of all these JWT and OAuth layers",
          "notes": "Expresses skepticism about complexity, seeking justification for auth design"
        }
      ],
      "key_facts": [
        "OAuth 2.0",
        "JWT",
        "bcrypt",
        "RS256",
        "cost factor 12"
      ],
      "key_facts_semantic": [
        "uses OAuth 2.0 protocol for authorization",
        "tokens are JSON Web Tokens",
        "passwords hashed with bcrypt algorithm",
        "JWT signed using RS256 asymmetric algorithm",
        "bcrypt uses cost factor of 12 for password hashing",
        "industry standard token-based authentication",
        "stateless token validation"
      ],
      "expected_docs": [
        "adr_003",
        "arch_authentication_service",
        "arch_api_gateway"
      ],
      "ground_truth_answer": "Authentication uses OAuth 2.0 with JWT tokens signed with RS256. Passwords are hashed with bcrypt cost factor 12.",
      "metadata": {
        "concepts": [
          "authentication",
          "authorization",
          "OAuth",
          "JWT",
          "password hashing",
          "token validation"
        ],
        "related_terms": [
          "login",
          "sign-in",
          "credentials",
          "access control",
          "identity",
          "session",
          "bearer token"
        ],
        "user_intent": "understand_system_behavior",
        "difficulty": "medium",
        "requires_inference": true,
        "domain": "security"
      }
    },
    {
      "id": "cross_012",
      "category": "cross_document",
      "original_query": "What monitoring and observability tools does CloudFlow use?",
      "human_queries": [
        {
          "dimension": "synonym",
          "query": "what's the metrics and logging infrastructure in cloudflow",
          "notes": "Replaced 'monitoring and observability' with 'metrics and logging infrastructure'"
        },
        {
          "dimension": "problem",
          "query": "can't find any dashboards or logs when trying to debug a production issue",
          "notes": "Describes symptom of not knowing where to look for observability data"
        },
        {
          "dimension": "casual",
          "query": "monitoring stack",
          "notes": "Terse query typical of searching internal docs"
        },
        {
          "dimension": "contextual",
          "query": "setting up alerts for our team's workflows, need to know what monitoring tools are available and how they connect",
          "notes": "Real scenario: SRE setting up alerting, needs full observability picture"
        },
        {
          "dimension": "negation",
          "query": "why are there so many different monitoring tools instead of a unified platform",
          "notes": "Questions the architecture decision, seeks understanding of tool diversity"
        }
      ],
      "key_facts": [
        "Prometheus",
        "Thanos",
        "Loki",
        "Grafana",
        "Jaeger",
        "OpenTelemetry"
      ],
      "key_facts_semantic": [
        "Prometheus handles time-series metrics collection",
        "Thanos provides long-term metric storage",
        "Loki used for log aggregation",
        "Grafana for dashboards and visualization",
        "Jaeger for distributed tracing",
        "OpenTelemetry for trace instrumentation",
        "metrics retention 15 days local, 1 year long-term"
      ],
      "expected_docs": [
        "arch_monitoring_stack"
      ],
      "ground_truth_answer": "CloudFlow uses Prometheus with Thanos for metrics, Loki with Grafana for logs, and Jaeger with OpenTelemetry for tracing.",
      "metadata": {
        "concepts": [
          "monitoring",
          "observability",
          "metrics",
          "logging",
          "tracing",
          "alerting"
        ],
        "related_terms": [
          "dashboards",
          "visualization",
          "telemetry",
          "SRE",
          "debugging",
          "performance"
        ],
        "user_intent": "see_full_picture",
        "difficulty": "medium",
        "requires_inference": false,
        "domain": "devops"
      }
    },
    {
      "id": "cross_013",
      "category": "cross_document",
      "original_query": "What caching strategy is used across the system?",
      "human_queries": [
        {
          "dimension": "synonym",
          "query": "how does cloudflow handle data caching and what layers are involved",
          "notes": "Replaced 'caching strategy' with 'handle data caching', added 'layers' for multi-tier aspect"
        },
        {
          "dimension": "problem",
          "query": "response times are slow even though we should be hitting cache, not sure where the bottleneck is",
          "notes": "Performance symptom that requires understanding caching architecture"
        },
        {
          "dimension": "casual",
          "query": "caching layers explained",
          "notes": "Short query mentioning specific known technology"
        },
        {
          "dimension": "contextual",
          "query": "optimizing our API response times, need to understand all the caching layers from local to CDN and their TTLs",
          "notes": "Performance optimization scenario requiring full cache architecture knowledge"
        },
        {
          "dimension": "negation",
          "query": "why do we have both local and distributed cache instead of just using redis for everything",
          "notes": "Questions multi-tier design, seeks justification for complexity"
        }
      ],
      "key_facts": [
        "Redis Cluster",
        "Ristretto",
        "CloudFront",
        "cache-aside"
      ],
      "key_facts_semantic": [
        "uses Redis Cluster for distributed caching",
        "Ristretto provides in-process local cache",
        "CloudFront CDN for edge caching",
        "implements cache-aside pattern",
        "multi-tier caching with local and distributed layers",
        "default cache TTL is 300 seconds",
        "6-node Redis cluster for high availability"
      ],
      "expected_docs": [
        "arch_caching_layer",
        "adr_004",
        "config_cache"
      ],
      "ground_truth_answer": "Multi-tier caching with Ristretto local cache, Redis Cluster distributed cache, and CloudFront CDN.",
      "metadata": {
        "concepts": [
          "caching",
          "performance",
          "Redis",
          "CDN",
          "distributed systems"
        ],
        "related_terms": [
          "TTL",
          "cache invalidation",
          "cache hit",
          "latency",
          "throughput",
          "memory"
        ],
        "user_intent": "trace_data_flow",
        "difficulty": "medium",
        "requires_inference": true,
        "domain": "infrastructure"
      }
    },
    {
      "id": "cross_014",
      "category": "cross_document",
      "original_query": "How does CloudFlow handle data storage?",
      "human_queries": [
        {
          "dimension": "synonym",
          "query": "what databases and storage systems does cloudflow use",
          "notes": "Replaced 'handle data storage' with 'databases and storage systems'"
        },
        {
          "dimension": "problem",
          "query": "running out of storage space and need to understand where all our data actually lives",
          "notes": "Storage capacity issue requiring understanding of data distribution"
        },
        {
          "dimension": "casual",
          "query": "storage architecture",
          "notes": "Brief two-word query"
        },
        {
          "dimension": "contextual",
          "query": "planning data retention policies for compliance, need to map out all the storage tiers from transactional DB to data lake",
          "notes": "Compliance use case requiring comprehensive storage understanding"
        },
        {
          "dimension": "negation",
          "query": "why do we need multiple storage solutions instead of consolidating into one",
          "notes": "Questions multiple storage systems, seeks architectural justification"
        }
      ],
      "key_facts": [
        "PostgreSQL",
        "S3",
        "Iceberg",
        "Parquet"
      ],
      "key_facts_semantic": [
        "PostgreSQL for transactional OLTP data",
        "S3 for object and artifact storage",
        "Apache Iceberg tables for data lake",
        "Parquet format for analytical data",
        "JSONB support in PostgreSQL for flexible schemas",
        "intelligent tiering on S3 for cost optimization"
      ],
      "expected_docs": [
        "adr_001",
        "adr_008",
        "arch_data_pipeline"
      ],
      "ground_truth_answer": "PostgreSQL for transactional data, S3 for object storage, and Iceberg tables for data lake.",
      "metadata": {
        "concepts": [
          "data storage",
          "database",
          "object storage",
          "data lake",
          "persistence"
        ],
        "related_terms": [
          "OLTP",
          "analytics",
          "blob storage",
          "files",
          "artifacts",
          "durability"
        ],
        "user_intent": "understand_system_behavior",
        "difficulty": "medium",
        "requires_inference": true,
        "domain": "database"
      }
    },
    {
      "id": "cross_015",
      "category": "cross_document",
      "original_query": "What are all the API authentication methods?",
      "human_queries": [
        {
          "dimension": "synonym",
          "query": "what credentials and tokens can I use to call cloudflow APIs",
          "notes": "Replaced 'authentication methods' with 'credentials and tokens'"
        },
        {
          "dimension": "problem",
          "query": "getting 403 forbidden when calling the API with what I thought was a valid API key",
          "notes": "Auth failure symptom prompting need to understand valid auth methods"
        },
        {
          "dimension": "casual",
          "query": "api auth options",
          "notes": "Three-word casual query"
        },
        {
          "dimension": "contextual",
          "query": "writing a CLI tool that needs to authenticate users and also support service accounts, what auth methods should I implement",
          "notes": "Real development scenario requiring knowledge of all auth options"
        },
        {
          "dimension": "negation",
          "query": "why can't I just use a simple API key for everything instead of dealing with OAuth flows",
          "notes": "Frustration with complexity, seeks understanding of auth method trade-offs"
        }
      ],
      "key_facts": [
        "Bearer",
        "API key",
        "OAuth 2.0",
        "Authorization"
      ],
      "key_facts_semantic": [
        "Bearer tokens in Authorization header",
        "API keys for machine-to-machine auth",
        "OAuth 2.0 for user authentication flows",
        "Authorization header required for all requests",
        "supports both user tokens and service account keys",
        "JWT tokens for stateless validation"
      ],
      "expected_docs": [
        "api_users",
        "api_integrations",
        "adr_003"
      ],
      "ground_truth_answer": "API authentication supports Bearer tokens (JWT), API keys, and OAuth 2.0.",
      "metadata": {
        "concepts": [
          "API authentication",
          "authorization",
          "tokens",
          "API keys",
          "OAuth"
        ],
        "related_terms": [
          "credentials",
          "header",
          "bearer",
          "service account",
          "machine-to-machine"
        ],
        "user_intent": "understand_system_behavior",
        "difficulty": "medium",
        "requires_inference": true,
        "domain": "api"
      }
    },
    {
      "id": "howto_016",
      "category": "how_to",
      "original_query": "How do I set up CI/CD for CloudFlow?",
      "human_queries": [
        {
          "dimension": "synonym",
          "query": "How do I configure continuous deployment pipelines for CloudFlow?",
          "notes": "Replaced CI/CD with continuous deployment pipelines"
        },
        {
          "dimension": "problem",
          "query": "I want to automatically deploy my CloudFlow workflows when I push to GitHub but don't know where to start",
          "notes": "Goal-driven framing expressing desire to accomplish automated deployment"
        },
        {
          "dimension": "casual",
          "query": "cloudflow ci/cd setup",
          "notes": "Terse search-style query with key terms"
        },
        {
          "dimension": "contextual",
          "query": "setting up automated deployments for my team's CloudFlow project, need to integrate with our GitHub repo",
          "notes": "Real-world scenario with team context and GitHub integration goal"
        },
        {
          "dimension": "negation",
          "query": "why aren't my CloudFlow deployments happening automatically when I push code?",
          "notes": "Framed as confusion about missing automation"
        }
      ],
      "key_facts": [
        "GitHub Actions",
        "cloudflow/deploy-action",
        "CLOUDFLOW_API_KEY"
      ],
      "key_facts_semantic": [
        "GitHub Actions workflow for deployment",
        "official CloudFlow deploy action version 2",
        "API key stored as GitHub secret",
        "cloudflow.yml workflow file",
        "automated deployment on push to main"
      ],
      "expected_docs": [
        "howto_set_up_ci/cd_pipeline"
      ],
      "ground_truth_answer": "Create GitHub Actions workflow, add CLOUDFLOW_API_KEY secret, use cloudflow/deploy-action@v2.",
      "metadata": {
        "concepts": [
          "CI/CD",
          "continuous deployment",
          "GitHub Actions",
          "automation",
          "DevOps"
        ],
        "related_terms": [
          "pipeline",
          "workflow",
          "deploy",
          "automated deployment",
          "GitHub secrets",
          "build automation"
        ],
        "user_intent": "set_up_automation",
        "difficulty": "medium",
        "requires_inference": false,
        "domain": "devops"
      }
    },
    {
      "id": "howto_017",
      "category": "how_to",
      "original_query": "How do I configure a custom domain?",
      "human_queries": [
        {
          "dimension": "synonym",
          "query": "How do I set up my own domain name for CloudFlow?",
          "notes": "Replaced custom domain with own domain name"
        },
        {
          "dimension": "problem",
          "query": "I want to use app.mycompany.com instead of the default CloudFlow URL",
          "notes": "Goal-driven with specific example of desired outcome"
        },
        {
          "dimension": "casual",
          "query": "custom domain dns setup cloudflow",
          "notes": "Terse keyword-based search"
        },
        {
          "dimension": "contextual",
          "query": "launching our app publicly and need to point our company domain to CloudFlow with SSL",
          "notes": "Real-world scenario with SSL requirement mentioned"
        },
        {
          "dimension": "negation",
          "query": "why can't I access my CloudFlow app through my own domain?",
          "notes": "Framed as troubleshooting why domain isn't working"
        }
      ],
      "key_facts": [
        "CNAME",
        "ingress.cloudflow.io",
        "Let's Encrypt"
      ],
      "key_facts_semantic": [
        "DNS CNAME record pointing to CloudFlow",
        "ingress endpoint for custom domains",
        "automatic SSL certificate provisioning",
        "Let's Encrypt for free SSL certificates",
        "domain verification via TXT record"
      ],
      "expected_docs": [
        "howto_configure_custom_domain"
      ],
      "ground_truth_answer": "Add CNAME pointing to ingress.cloudflow.io, SSL is automatically provisioned via Let's Encrypt.",
      "metadata": {
        "concepts": [
          "DNS configuration",
          "custom domains",
          "SSL certificates",
          "domain routing"
        ],
        "related_terms": [
          "CNAME record",
          "DNS",
          "SSL",
          "TLS",
          "certificate",
          "domain name",
          "ingress"
        ],
        "user_intent": "configure_domain",
        "difficulty": "easy",
        "requires_inference": false,
        "domain": "infrastructure"
      }
    },
    {
      "id": "howto_018",
      "category": "how_to",
      "original_query": "How do I implement retry logic?",
      "human_queries": [
        {
          "dimension": "synonym",
          "query": "How do I add automatic retries to my workflow steps?",
          "notes": "Replaced retry logic with automatic retries"
        },
        {
          "dimension": "problem",
          "query": "my workflow steps sometimes fail due to transient errors and I want them to automatically try again",
          "notes": "Problem-driven describing the symptom and desired behavior"
        },
        {
          "dimension": "casual",
          "query": "workflow retry backoff config",
          "notes": "Terse search with key technical terms"
        },
        {
          "dimension": "contextual",
          "query": "building a workflow that calls an external API which sometimes times out, need to handle temporary failures gracefully",
          "notes": "Real-world scenario with external API and timeout context"
        },
        {
          "dimension": "negation",
          "query": "why does my workflow fail permanently instead of retrying when a step errors?",
          "notes": "Framed as confusion about lack of retry behavior"
        }
      ],
      "key_facts": [
        "retry",
        "exponential backoff",
        "on_failure"
      ],
      "key_facts_semantic": [
        "configurable retry policy per step",
        "exponential backoff with base delay",
        "maximum 5 retry attempts",
        "on_failure handler for exhausted retries",
        "timeout configuration to prevent infinite loops"
      ],
      "expected_docs": [
        "howto_implement_retry_logic"
      ],
      "ground_truth_answer": "Define retry policy with exponential backoff. Configure on_failure handler for exhausted retries.",
      "metadata": {
        "concepts": [
          "error handling",
          "retry patterns",
          "fault tolerance",
          "resilience"
        ],
        "related_terms": [
          "backoff",
          "retry",
          "failure handling",
          "timeout",
          "transient errors",
          "idempotency"
        ],
        "user_intent": "implement_resilience",
        "difficulty": "medium",
        "requires_inference": false,
        "domain": "workflow"
      }
    },
    {
      "id": "howto_019",
      "category": "how_to",
      "original_query": "How do I set up database connection?",
      "human_queries": [
        {
          "dimension": "synonym",
          "query": "How do I connect CloudFlow to my database?",
          "notes": "Made more specific with PostgreSQL as the common database"
        },
        {
          "dimension": "problem",
          "query": "I need my CloudFlow workflows to read and write data from my existing database",
          "notes": "Goal-driven describing the data access need"
        },
        {
          "dimension": "casual",
          "query": "cloudflow database connection string setup",
          "notes": "Terse search with connection string mention"
        },
        {
          "dimension": "contextual",
          "query": "migrating our app to CloudFlow and need to connect to our production database securely",
          "notes": "Migration scenario with security concern"
        },
        {
          "dimension": "negation",
          "query": "why can't my workflow access my database even though I have the credentials?",
          "notes": "Framed as troubleshooting connection issues"
        }
      ],
      "key_facts": [
        "DATABASE_URL",
        "cloudflow secrets set",
        "require-ssl"
      ],
      "key_facts_semantic": [
        "store connection string as secret",
        "DATABASE_URL environment variable",
        "connection pool configuration",
        "SSL required for production",
        "cloudflow secrets command for secure storage"
      ],
      "expected_docs": [
        "howto_set_up_database_connecti"
      ],
      "ground_truth_answer": "Store DATABASE_URL as secret, configure pool size, enable SSL for production.",
      "metadata": {
        "concepts": [
          "database connectivity",
          "secrets management",
          "connection pooling",
          "secure connections"
        ],
        "related_terms": [
          "connection string",
          "DATABASE_URL",
          "PostgreSQL",
          "SSL",
          "pool size",
          "credentials"
        ],
        "user_intent": "connect_database",
        "difficulty": "easy",
        "requires_inference": false,
        "domain": "database"
      }
    },
    {
      "id": "howto_020",
      "category": "how_to",
      "original_query": "How do I create a scheduled workflow?",
      "human_queries": [
        {
          "dimension": "synonym",
          "query": "How do I run a workflow automatically on a recurring schedule?",
          "notes": "Replaced scheduled with recurring schedule concept"
        },
        {
          "dimension": "problem",
          "query": "I want my workflow to run every weekday morning at 9 AM automatically",
          "notes": "Specific goal with example schedule matching docs"
        },
        {
          "dimension": "casual",
          "query": "cron schedule workflow cloudflow",
          "notes": "Terse search with cron mention"
        },
        {
          "dimension": "contextual",
          "query": "building a daily report generation workflow that needs to run at a specific time each day",
          "notes": "Real-world reporting scenario with time requirement"
        },
        {
          "dimension": "negation",
          "query": "why isn't my workflow running at the scheduled time I configured?",
          "notes": "Framed as troubleshooting missed schedule"
        }
      ],
      "key_facts": [
        "cron",
        "0 9 * * MON-FRI",
        "timezone"
      ],
      "key_facts_semantic": [
        "cron expression for scheduling",
        "weekday schedule example 9 AM",
        "IANA timezone specification",
        "overlap handling options: skip, queue, cancel",
        "schedule failure notifications"
      ],
      "expected_docs": [
        "howto_create_scheduled_workflo"
      ],
      "ground_truth_answer": "Use cron expression like '0 9 * * MON-FRI' for weekdays at 9 AM, specify timezone.",
      "metadata": {
        "concepts": [
          "scheduling",
          "cron jobs",
          "automation",
          "time-based triggers"
        ],
        "related_terms": [
          "cron",
          "schedule",
          "recurring",
          "timer",
          "timezone",
          "UTC"
        ],
        "user_intent": "schedule_automation",
        "difficulty": "easy",
        "requires_inference": false,
        "domain": "workflow"
      }
    },
    {
      "id": "howto_021",
      "category": "how_to",
      "original_query": "How do I configure SSO?",
      "human_queries": [
        {
          "dimension": "synonym",
          "query": "How do I set up single sign-on authentication for CloudFlow?",
          "notes": "Expanded SSO acronym to single sign-on"
        },
        {
          "dimension": "problem",
          "query": "I want my team to log into CloudFlow using our company's Okta/Azure AD credentials",
          "notes": "Goal-driven with common IdP examples"
        },
        {
          "dimension": "casual",
          "query": "cloudflow sso setup",
          "notes": "Terse search with SAML protocol mention"
        },
        {
          "dimension": "contextual",
          "query": "enterprise security requires us to integrate CloudFlow with our identity provider for centralized authentication",
          "notes": "Enterprise compliance scenario with IdP integration"
        },
        {
          "dimension": "negation",
          "query": "why can't my users log in with their corporate credentials?",
          "notes": "Framed as troubleshooting SSO login issues"
        }
      ],
      "key_facts": [
        "SAML 2.0",
        "OpenID Connect",
        "ACS URL"
      ],
      "key_facts_semantic": [
        "SAML 2.0 protocol support",
        "OpenID Connect (OIDC) support",
        "Assertion Consumer Service URL callback",
        "https://auth.cloudflow.io/saml/callback",
        "IdP metadata upload required"
      ],
      "expected_docs": [
        "howto_configure_single_sign-on"
      ],
      "ground_truth_answer": "CloudFlow supports SAML 2.0 and OpenID Connect. ACS URL is https://auth.cloudflow.io/saml/callback.",
      "metadata": {
        "concepts": [
          "single sign-on",
          "identity federation",
          "enterprise authentication",
          "SAML"
        ],
        "related_terms": [
          "SSO",
          "SAML",
          "OIDC",
          "OpenID Connect",
          "identity provider",
          "IdP",
          "Okta",
          "Azure AD"
        ],
        "user_intent": "configure_authentication",
        "difficulty": "medium",
        "requires_inference": false,
        "domain": "security"
      }
    },
    {
      "id": "howto_022",
      "category": "how_to",
      "original_query": "How do I debug a failed execution?",
      "human_queries": [
        {
          "dimension": "synonym",
          "query": "How do I troubleshoot a workflow that errored out?",
          "notes": "Replaced debug with troubleshoot, failed execution with errored out"
        },
        {
          "dimension": "problem",
          "query": "my workflow failed and I need to figure out which step caused the error and why",
          "notes": "Problem-driven describing the investigation need"
        },
        {
          "dimension": "casual",
          "query": "cloudflow execution logs debug failed",
          "notes": "Terse search with key debugging terms"
        },
        {
          "dimension": "contextual",
          "query": "production workflow started failing last night and I need to investigate what went wrong",
          "notes": "Incident response scenario with urgency"
        },
        {
          "dimension": "negation",
          "query": "why did my workflow fail and where can I see what happened?",
          "notes": "Framed as seeking understanding of failure"
        }
      ],
      "key_facts": [
        "cloudflow executions get",
        "cloudflow executions logs",
        "--debug"
      ],
      "key_facts_semantic": [
        "get execution details with step outputs",
        "view logs for specific failed step",
        "inspect input/output between steps",
        "debug flag for verbose logging",
        "list recent failures with status filter"
      ],
      "expected_docs": [
        "howto_debug_failed_execution"
      ],
      "ground_truth_answer": "Use cloudflow executions get for details, logs for output, and --debug flag for verbose mode.",
      "metadata": {
        "concepts": [
          "debugging",
          "troubleshooting",
          "error investigation",
          "logging"
        ],
        "related_terms": [
          "debug",
          "logs",
          "failed",
          "error",
          "execution",
          "trace",
          "inspect"
        ],
        "user_intent": "diagnose_failure",
        "difficulty": "easy",
        "requires_inference": false,
        "domain": "workflow"
      }
    },
    {
      "id": "compare_023",
      "category": "comparison",
      "original_query": "What database options were considered and why was PostgreSQL chosen?",
      "human_queries": [
        {
          "dimension": "synonym",
          "query": "What data stores did we evaluate before picking Postgres?",
          "notes": "Replaced 'database' with 'data stores', 'considered' with 'evaluate', 'PostgreSQL' with 'Postgres'"
        },
        {
          "dimension": "problem",
          "query": "We're reviewing our db choice and need to document the alternatives we rejected",
          "notes": "Frames as a documentation need, describes the underlying problem of needing to justify a past decision"
        },
        {
          "dimension": "casual",
          "query": "postgres vs mongodb vs cockroach decision",
          "notes": "Terse search-style query listing the specific options, no question words"
        },
        {
          "dimension": "contextual",
          "query": "New team member is asking why we use PostgreSQL instead of MongoDB - what's the rationale?",
          "notes": "Real scenario of onboarding someone who needs to understand architectural decisions"
        },
        {
          "dimension": "negation",
          "query": "Why didn't we go with MongoDB or CockroachDB?",
          "notes": "Frames as questioning the rejection of alternatives rather than the selection"
        }
      ],
      "key_facts": [
        "PostgreSQL",
        "MongoDB",
        "CockroachDB",
        "JSONB",
        "Team expertise"
      ],
      "key_facts_semantic": [
        "PostgreSQL was selected as the primary database",
        "MongoDB was considered but rejected due to weaker consistency",
        "CockroachDB was considered but rejected due to higher latency and cost",
        "JSONB support was a key factor in choosing PostgreSQL",
        "Team already had PostgreSQL expertise",
        "Postgres chosen for flexible workflow definitions",
        "Relational database with JSON query support"
      ],
      "expected_docs": [
        "adr_001"
      ],
      "ground_truth_answer": "PostgreSQL, MongoDB, and CockroachDB were considered. PostgreSQL chosen for JSONB support and team expertise.",
      "metadata": {
        "concepts": [
          "database selection",
          "architectural decision",
          "technology evaluation",
          "tradeoff analysis"
        ],
        "related_terms": [
          "SQL",
          "NoSQL",
          "document database",
          "relational database",
          "sharding",
          "ACID",
          "consistency",
          "data store"
        ],
        "user_intent": "understand_tradeoffs",
        "difficulty": "medium",
        "requires_inference": false,
        "domain": "database"
      }
    },
    {
      "id": "compare_024",
      "category": "comparison",
      "original_query": "Compare Kubernetes vs ECS for container orchestration",
      "human_queries": [
        {
          "dimension": "synonym",
          "query": "K8s versus AWS container service - what's the difference?",
          "notes": "Used common abbreviation 'K8s', 'AWS container service' instead of ECS, casual phrasing"
        },
        {
          "dimension": "problem",
          "query": "Trying to decide on our container platform and need to understand the vendor lock-in tradeoffs",
          "notes": "Describes the decision-making problem, mentions a key concern (lock-in)"
        },
        {
          "dimension": "casual",
          "query": "k8s vs ecs pros cons",
          "notes": "Very terse search query with abbreviations, no grammar"
        },
        {
          "dimension": "contextual",
          "query": "Infra team is proposing we move to ECS for simpler ops - what did we consider when we chose Kubernetes?",
          "notes": "Real scenario where someone is questioning the current choice"
        },
        {
          "dimension": "negation",
          "query": "Why did we pick Kubernetes over ECS despite the complexity?",
          "notes": "Questions the decision given a known downside (complexity)"
        }
      ],
      "key_facts": [
        "Kubernetes",
        "ECS",
        "industry standard",
        "AWS lock-in",
        "ecosystem"
      ],
      "key_facts_semantic": [
        "Kubernetes is the industry standard for container orchestration",
        "ECS would cause AWS vendor lock-in",
        "Kubernetes has a richer ecosystem than ECS",
        "ECS is simpler to operate than Kubernetes",
        "Kubernetes runs on EKS with Helm for deployments",
        "Lambda was also considered as serverless alternative",
        "Kubernetes provides cloud-agnostic portability"
      ],
      "expected_docs": [
        "adr_002"
      ],
      "ground_truth_answer": "Kubernetes offers industry standard and rich ecosystem. ECS has AWS lock-in but simpler operations.",
      "metadata": {
        "concepts": [
          "container orchestration",
          "infrastructure decision",
          "vendor lock-in",
          "DevOps platform"
        ],
        "related_terms": [
          "K8s",
          "EKS",
          "Docker",
          "containers",
          "pods",
          "Helm",
          "Lambda",
          "serverless",
          "microservices"
        ],
        "user_intent": "evaluate_options",
        "difficulty": "medium",
        "requires_inference": false,
        "domain": "devops"
      }
    },
    {
      "id": "compare_025",
      "category": "comparison",
      "original_query": "What authentication options were evaluated?",
      "human_queries": [
        {
          "dimension": "synonym",
          "query": "What auth approaches did we consider for the login system?",
          "notes": "Replaced 'authentication' with 'auth' and 'login system', 'evaluated' with 'consider'"
        },
        {
          "dimension": "problem",
          "query": "Security audit wants to know what alternatives we rejected for our auth implementation",
          "notes": "Frames as compliance/audit requirement, common real-world scenario"
        },
        {
          "dimension": "casual",
          "query": "oauth vs session vs api key auth comparison",
          "notes": "Lists the specific options being compared, search-style query"
        },
        {
          "dimension": "contextual",
          "query": "Building a new service that needs auth - what authentication strategy does CloudFlow use and why?",
          "notes": "Developer starting new work needs to understand and align with existing decisions"
        },
        {
          "dimension": "negation",
          "query": "Why didn't we just use simple session-based authentication?",
          "notes": "Questions why a simpler approach wasn't chosen"
        }
      ],
      "key_facts": [
        "OAuth 2.0",
        "Session-based",
        "API keys",
        "stateless validation"
      ],
      "key_facts_semantic": [
        "OAuth 2.0 with JWT was selected for authentication",
        "Session-based auth was rejected due to scaling challenges",
        "API keys alone were rejected due to no user context",
        "JWT enables stateless token validation",
        "Access tokens expire after 1 hour",
        "Refresh tokens last 7 days",
        "System supports both JWT and API keys for different use cases"
      ],
      "expected_docs": [
        "adr_003"
      ],
      "ground_truth_answer": "OAuth 2.0 with JWT, session-based auth, and API keys only were evaluated. JWT chosen for stateless validation.",
      "metadata": {
        "concepts": [
          "authentication strategy",
          "security architecture",
          "token-based auth",
          "identity management"
        ],
        "related_terms": [
          "JWT",
          "tokens",
          "login",
          "credentials",
          "SSO",
          "MFA",
          "bearer token",
          "authorization"
        ],
        "user_intent": "understand_tradeoffs",
        "difficulty": "medium",
        "requires_inference": false,
        "domain": "security"
      }
    },
    {
      "id": "compare_026",
      "category": "comparison",
      "original_query": "Compare caching options: Redis vs Memcached",
      "human_queries": [
        {
          "dimension": "synonym",
          "query": "What's the difference between Redis and Memcached for our cache layer?",
          "notes": "Natural question phrasing, added context about cache layer"
        },
        {
          "dimension": "problem",
          "query": "Cache is getting expensive - should we switch from Redis to Memcached to save on memory?",
          "notes": "Frames as a cost optimization problem, real operational concern"
        },
        {
          "dimension": "casual",
          "query": "redis vs memcached decision",
          "notes": "Minimal search query for the ADR"
        },
        {
          "dimension": "contextual",
          "query": "Evaluating cache solutions for a new microservice - why did we standardize on Redis over Memcached?",
          "notes": "New development needs to understand and follow existing patterns"
        },
        {
          "dimension": "negation",
          "query": "Why isn't Memcached good enough if it's simpler to operate?",
          "notes": "Challenges the decision by highlighting a benefit of the rejected option"
        }
      ],
      "key_facts": [
        "Redis Cluster",
        "Memcached",
        "data structures",
        "persistence"
      ],
      "key_facts_semantic": [
        "Redis Cluster was selected for caching",
        "Redis provides rich data structures beyond simple key-value",
        "Redis supports persistence which Memcached lacks",
        "Memcached is simpler but has no clustering",
        "Redis supports pub/sub functionality",
        "6-node Redis cluster for high availability",
        "Redis has sub-millisecond latency"
      ],
      "expected_docs": [
        "adr_004"
      ],
      "ground_truth_answer": "Redis offers rich data structures and persistence. Memcached is simpler but lacks clustering.",
      "metadata": {
        "concepts": [
          "caching strategy",
          "distributed cache",
          "in-memory data store",
          "performance optimization"
        ],
        "related_terms": [
          "cache",
          "TTL",
          "key-value store",
          "session storage",
          "rate limiting",
          "in-memory",
          "cache-aside",
          "cache stampede"
        ],
        "user_intent": "evaluate_options",
        "difficulty": "medium",
        "requires_inference": false,
        "domain": "infrastructure"
      }
    },
    {
      "id": "troubleshoot_027",
      "category": "troubleshooting",
      "original_query": "How do I diagnose high CPU usage?",
      "human_queries": [
        {
          "dimension": "synonym",
          "query": "How do I troubleshoot elevated processor utilization in the cluster?",
          "notes": "Replaced 'CPU' with 'processor', 'diagnose' with 'troubleshoot', added 'cluster' context"
        },
        {
          "dimension": "problem",
          "query": "kubectl top shows pods at 95% CPU and response times are spiking to 5+ seconds",
          "notes": "Specific symptoms with metrics - CPU percentage and latency numbers from runbook context"
        },
        {
          "dimension": "casual",
          "query": "pods maxing out cpu, what now",
          "notes": "Terse Slack-style query, lowercase, minimal words"
        },
        {
          "dimension": "contextual",
          "query": "getting paged about high CPU in production, autoscaler is already at max replicas, need to find the root cause",
          "notes": "Real-world incident scenario with urgency and constraint mentioned"
        },
        {
          "dimension": "negation",
          "query": "why is CPU stuck at 80% even after I scaled up the deployment?",
          "notes": "Frustration about attempted fix not working, uses 'why' question format"
        }
      ],
      "key_facts": [
        "kubectl top pods",
        "cloudflow debug profile",
        "rollout restart"
      ],
      "key_facts_semantic": [
        "check pod CPU metrics with kubectl top",
        "profile application using cloudflow debug",
        "restart deployment with rollout restart",
        "scale up replicas if traffic-related",
        "roll back if regression from recent deployment"
      ],
      "expected_docs": [
        "runbook_high_cpu_usage"
      ],
      "ground_truth_answer": "Check CPU with kubectl top pods, profile with cloudflow debug profile, restart or scale as needed.",
      "metadata": {
        "concepts": [
          "CPU utilization",
          "performance troubleshooting",
          "Kubernetes pods",
          "autoscaling",
          "profiling"
        ],
        "related_terms": [
          "processor",
          "compute",
          "resource exhaustion",
          "latency",
          "throttling",
          "hotspots"
        ],
        "user_intent": "diagnose_performance_issue",
        "difficulty": "medium",
        "requires_inference": false,
        "domain": "devops"
      }
    },
    {
      "id": "troubleshoot_028",
      "category": "troubleshooting",
      "original_query": "What to do when database connections are exhausted?",
      "human_queries": [
        {
          "dimension": "synonym",
          "query": "How do I handle DB connection pool depletion?",
          "notes": "Replaced 'database' with 'DB', 'exhausted' with 'pool depletion'"
        },
        {
          "dimension": "problem",
          "query": "seeing 'too many connections' errors in logs and queries are timing out after 30 seconds",
          "notes": "Specific error message from runbook symptoms plus timeout detail"
        },
        {
          "dimension": "casual",
          "query": "db connection pool maxed out",
          "notes": "Terse, uses 'pg' abbreviation for PostgreSQL"
        },
        {
          "dimension": "contextual",
          "query": "just deployed a new feature and now the database is rejecting new connections, users are seeing errors",
          "notes": "Incident scenario with cause hypothesis and user impact"
        },
        {
          "dimension": "negation",
          "query": "why can't my app connect to the database anymore, it was working fine yesterday",
          "notes": "Confusion and frustration about sudden change"
        }
      ],
      "key_facts": [
        "pg_stat_activity",
        "SHOW POOLS",
        "pg_terminate_backend"
      ],
      "key_facts_semantic": [
        "check active connections with pg_stat_activity",
        "view PgBouncer connection pools with SHOW POOLS",
        "terminate long-running queries with pg_terminate_backend",
        "identify queries running longer than expected",
        "increase connection pool size if needed"
      ],
      "expected_docs": [
        "runbook_database_connection_ex"
      ],
      "ground_truth_answer": "Check connections with pg_stat_activity, verify PgBouncer with SHOW POOLS, kill long queries.",
      "metadata": {
        "concepts": [
          "database connections",
          "connection pooling",
          "PostgreSQL",
          "PgBouncer",
          "query performance"
        ],
        "related_terms": [
          "connection pool",
          "pg_stat_activity",
          "connection limit",
          "too many connections",
          "query timeout"
        ],
        "user_intent": "resolve_database_issue",
        "difficulty": "medium",
        "requires_inference": false,
        "domain": "database"
      }
    },
    {
      "id": "troubleshoot_029",
      "category": "troubleshooting",
      "original_query": "How to handle Kafka consumer lag?",
      "human_queries": [
        {
          "dimension": "synonym",
          "query": "How do I fix message queue processing delays in Kafka?",
          "notes": "Replaced 'consumer lag' with 'message queue processing delays'"
        },
        {
          "dimension": "problem",
          "query": "kafka-consumer-groups shows 50000 messages behind and workflows are delayed by 10+ minutes",
          "notes": "Specific lag number and real-world impact on workflow execution"
        },
        {
          "dimension": "casual",
          "query": "kafka lag through the roof, consumers cant keep up",
          "notes": "Informal language, no punctuation, describes the symptom casually"
        },
        {
          "dimension": "contextual",
          "query": "we had a traffic spike and now Kafka consumers are way behind, need to catch up before the backlog gets worse",
          "notes": "Incident context with cause and urgency"
        },
        {
          "dimension": "negation",
          "query": "why aren't my Kafka consumers processing messages fast enough?",
          "notes": "Questioning why expected behavior isn't happening"
        }
      ],
      "key_facts": [
        "kafka-consumer-groups.sh",
        "consumer lag",
        "reset-offsets"
      ],
      "key_facts_semantic": [
        "check consumer group lag with kafka-consumer-groups.sh",
        "monitor consumer lag metrics",
        "reset consumer offsets to latest if messages are stale",
        "scale up consumer replicas for higher throughput",
        "increase partition count for more parallelism"
      ],
      "expected_docs": [
        "runbook_kafka_consumer_lag"
      ],
      "ground_truth_answer": "Check lag with kafka-consumer-groups.sh, scale consumers or reset offsets if needed.",
      "metadata": {
        "concepts": [
          "Kafka",
          "consumer groups",
          "message lag",
          "event processing",
          "throughput"
        ],
        "related_terms": [
          "message queue",
          "consumer offset",
          "partition",
          "backpressure",
          "processing delay",
          "message backlog"
        ],
        "user_intent": "fix_message_processing",
        "difficulty": "medium",
        "requires_inference": false,
        "domain": "messaging"
      }
    },
    {
      "id": "troubleshoot_030",
      "category": "troubleshooting",
      "original_query": "What to do when SSL certificate is expiring?",
      "human_queries": [
        {
          "dimension": "synonym",
          "query": "How do I renew TLS certs before they expire?",
          "notes": "Replaced 'SSL' with 'TLS', 'certificate' with 'certs', proactive framing"
        },
        {
          "dimension": "problem",
          "query": "users are getting browser security warnings and openssl shows cert expires in 3 days",
          "notes": "User-facing symptom plus diagnostic output showing urgency"
        },
        {
          "dimension": "casual",
          "query": "ssl cert about to expire, how to renew",
          "notes": "Terse, lowercase, direct question"
        },
        {
          "dimension": "contextual",
          "query": "got an alert that our production SSL certificate expires next week, need to renew with Let's Encrypt",
          "notes": "Alert-driven scenario with specific CA mentioned"
        },
        {
          "dimension": "negation",
          "query": "why didn't cert-manager auto-renew our certificate?",
          "notes": "Frustration that automation didn't work as expected"
        }
      ],
      "key_facts": [
        "openssl s_client",
        "cert-manager",
        "Let's Encrypt"
      ],
      "key_facts_semantic": [
        "check certificate expiry date with openssl s_client",
        "verify cert-manager status in Kubernetes",
        "use Let's Encrypt for automatic SSL certificates",
        "trigger manual certificate renewal if needed",
        "check Let's Encrypt rate limits"
      ],
      "expected_docs": [
        "runbook_ssl_certificate_expir"
      ],
      "ground_truth_answer": "Check expiry with openssl, verify cert-manager status, trigger renewal or manually upload.",
      "metadata": {
        "concepts": [
          "SSL/TLS certificates",
          "certificate renewal",
          "cert-manager",
          "Let's Encrypt",
          "HTTPS"
        ],
        "related_terms": [
          "TLS",
          "HTTPS",
          "certificate expiry",
          "SSL handshake",
          "browser warning",
          "X.509"
        ],
        "user_intent": "renew_certificate",
        "difficulty": "easy",
        "requires_inference": false,
        "domain": "security"
      }
    },
    {
      "id": "troubleshoot_031",
      "category": "troubleshooting",
      "original_query": "How to fix OOM kills?",
      "human_queries": [
        {
          "dimension": "synonym",
          "query": "How do I resolve out-of-memory container terminations?",
          "notes": "Replaced 'OOM kills' with 'out-of-memory container terminations'"
        },
        {
          "dimension": "problem",
          "query": "pods keep restarting with OOMKilled status and kubectl describe shows memory at limit",
          "notes": "Specific Kubernetes status and diagnostic output"
        },
        {
          "dimension": "casual",
          "query": "pods crashing oom, need more memory?",
          "notes": "Terse, uses abbreviation, includes hypothesis as question"
        },
        {
          "dimension": "contextual",
          "query": "our Java app keeps getting OOM killed after processing large files, need to figure out if it's a leak or just undersized",
          "notes": "Real scenario with specific trigger and diagnostic question"
        },
        {
          "dimension": "negation",
          "query": "why do my containers keep dying from memory issues even with 4GB limit?",
          "notes": "Frustration that setting memory limits didn't help, includes specific limit value"
        }
      ],
      "key_facts": [
        "OOMKilled",
        "resources.limits.memory",
        "HeapDumpOnOutOfMemoryError"
      ],
      "key_facts_semantic": [
        "pods restarting with OOMKilled status",
        "increase memory limits in resources.limits.memory",
        "enable heap dumps with HeapDumpOnOutOfMemoryError JVM flag",
        "analyze heap dumps to find memory leaks",
        "consider horizontal scaling instead of vertical"
      ],
      "expected_docs": [
        "runbook_out_of_memory_oom_kill"
      ],
      "ground_truth_answer": "Check OOM events, increase memory limits, enable heap dumps for leak investigation.",
      "metadata": {
        "concepts": [
          "OOM kills",
          "memory limits",
          "container resources",
          "memory leaks",
          "JVM heap"
        ],
        "related_terms": [
          "out of memory",
          "memory exhaustion",
          "container restart",
          "heap dump",
          "memory leak",
          "garbage collection"
        ],
        "user_intent": "fix_memory_issue",
        "difficulty": "medium",
        "requires_inference": false,
        "domain": "devops"
      }
    },
    {
      "id": "troubleshoot_032",
      "category": "troubleshooting",
      "original_query": "How to troubleshoot API Gateway 5xx errors?",
      "human_queries": [
        {
          "dimension": "synonym",
          "query": "How do I debug server errors from the ingress layer?",
          "notes": "Replaced 'API Gateway' with 'Kong ingress', '5xx' with 'server errors'"
        },
        {
          "dimension": "problem",
          "query": "error rate jumped to 5% and gateway logs show 502 Bad Gateway errors from backend services",
          "notes": "Specific error rate and HTTP status code with component attribution"
        },
        {
          "dimension": "casual",
          "query": "gateway throwing 500s, backends look fine though",
          "notes": "Casual description with observation that complicates diagnosis"
        },
        {
          "dimension": "contextual",
          "query": "customers are reporting intermittent errors, monitoring shows 5xx spike from API gateway after we scaled down backends",
          "notes": "User-reported issue with suspected cause from recent action"
        },
        {
          "dimension": "negation",
          "query": "why is the API gateway returning 503 when all my backend pods are running?",
          "notes": "Confusion about why healthy pods still cause gateway errors"
        }
      ],
      "key_facts": [
        "kubectl logs",
        "circuit breaker",
        "kubectl get endpoints"
      ],
      "key_facts_semantic": [
        "check Kong error logs with kubectl logs",
        "enable circuit breaker plugin for cascading failures",
        "verify service endpoints with kubectl get endpoints",
        "test backend health directly from gateway pod",
        "restart unhealthy backend deployments"
      ],
      "expected_docs": [
        "runbook_api_gateway_5xx_error"
      ],
      "ground_truth_answer": "Check Kong logs, verify backend health, enable circuit breaker if cascading failures.",
      "metadata": {
        "concepts": [
          "API Gateway",
          "Kong",
          "5xx errors",
          "backend health",
          "circuit breaker"
        ],
        "related_terms": [
          "server error",
          "502",
          "503",
          "504",
          "Bad Gateway",
          "Service Unavailable",
          "Gateway Timeout",
          "ingress"
        ],
        "user_intent": "fix_gateway_errors",
        "difficulty": "medium",
        "requires_inference": false,
        "domain": "api"
      }
    },
    {
      "id": "arch_033",
      "category": "architecture",
      "original_query": "Explain the Workflow Engine architecture",
      "human_queries": [
        {
          "dimension": "synonym",
          "query": "What is the design of the workflow orchestration system?",
          "notes": "Replaced 'Workflow Engine' with 'workflow orchestration system' and 'architecture' with 'design'"
        },
        {
          "dimension": "problem",
          "query": "I'm seeing bottlenecks in workflow execution - need to understand how the scheduler and executor interact",
          "notes": "Frames as performance investigation requiring architectural understanding"
        },
        {
          "dimension": "casual",
          "query": "workflow engine internals",
          "notes": "Terse search-style query"
        },
        {
          "dimension": "contextual",
          "query": "Onboarding to the team and trying to understand how workflows get scheduled, executed, and where state is stored",
          "notes": "Frames as new team member onboarding scenario"
        },
        {
          "dimension": "negation",
          "query": "I don't understand how the workflow engine handles parallel execution and retries",
          "notes": "Expresses confusion about specific architectural features"
        }
      ],
      "key_facts": [
        "Scheduler",
        "Executor",
        "State Store",
        "PostgreSQL with JSONB",
        "5000 executions/minute"
      ],
      "key_facts_semantic": [
        "workflow scheduling component",
        "execution runner using Kubernetes Jobs",
        "persistent state storage",
        "Postgres database with JSON support",
        "throughput of five thousand runs per minute",
        "job queue and worker pattern",
        "optimistic locking for concurrency"
      ],
      "expected_docs": [
        "arch_workflow_engine"
      ],
      "ground_truth_answer": "Workflow Engine has Scheduler, Executor (K8s Jobs), and State Store (PostgreSQL). Throughput: 5000 executions/minute.",
      "metadata": {
        "concepts": [
          "workflow orchestration",
          "job scheduling",
          "state management",
          "distributed systems",
          "event-driven architecture"
        ],
        "related_terms": [
          "scheduler",
          "executor",
          "state store",
          "idempotency",
          "Kubernetes jobs",
          "PostgreSQL",
          "JSONB",
          "cron",
          "triggers"
        ],
        "user_intent": "understand_design",
        "difficulty": "medium",
        "requires_inference": false,
        "domain": "workflow"
      }
    },
    {
      "id": "arch_034",
      "category": "architecture",
      "original_query": "How does the API Gateway work?",
      "human_queries": [
        {
          "dimension": "synonym",
          "query": "What is the design of the ingress layer and request routing?",
          "notes": "Replaced 'API Gateway' with 'ingress layer' and framed functionally"
        },
        {
          "dimension": "problem",
          "query": "Getting 429 errors and want to understand how rate limiting and auth validation happen at the gateway",
          "notes": "Problem-based framing that requires understanding gateway architecture"
        },
        {
          "dimension": "casual",
          "query": "api gateway setup cloudflow",
          "notes": "Casual search combining the tech (Kong) with product name"
        },
        {
          "dimension": "contextual",
          "query": "Doing a security review and need to understand how requests flow through authentication and rate limiting before hitting backend services",
          "notes": "Security audit context requiring data flow understanding"
        },
        {
          "dimension": "negation",
          "query": "I don't get how TLS termination and token validation work in the API gateway",
          "notes": "Confusion about specific gateway security features"
        }
      ],
      "key_facts": [
        "Kong",
        "TLS termination",
        "50000 requests/second",
        "5ms"
      ],
      "key_facts_semantic": [
        "Kong-based gateway",
        "SSL/TLS handling at the edge",
        "fifty thousand requests per second throughput",
        "sub-5-millisecond token validation",
        "request routing and ingress",
        "Zero Trust security model",
        "rate limiting with Redis"
      ],
      "expected_docs": [
        "arch_api_gateway"
      ],
      "ground_truth_answer": "Kong Gateway handles TLS, auth, rate limiting. 50000 req/s throughput, token validation under 5ms.",
      "metadata": {
        "concepts": [
          "API gateway",
          "ingress controller",
          "rate limiting",
          "authentication",
          "TLS termination"
        ],
        "related_terms": [
          "Kong",
          "TLS",
          "JWT",
          "OAuth2",
          "rate limiter",
          "Redis",
          "Zero Trust",
          "request routing"
        ],
        "user_intent": "understand_design",
        "difficulty": "medium",
        "requires_inference": false,
        "domain": "api"
      }
    },
    {
      "id": "arch_035",
      "category": "architecture",
      "original_query": "Describe the Data Pipeline architecture",
      "human_queries": [
        {
          "dimension": "synonym",
          "query": "How is the ETL and stream processing system designed?",
          "notes": "Replaced 'Data Pipeline' with 'ETL and stream processing system'"
        },
        {
          "dimension": "problem",
          "query": "Seeing data processing delays and need to understand the batch vs streaming components and their throughput limits",
          "notes": "Latency investigation requiring understanding of pipeline components"
        },
        {
          "dimension": "casual",
          "query": "data pipeline architecture",
          "notes": "Listing key technologies as search terms"
        },
        {
          "dimension": "contextual",
          "query": "Planning a high-volume data integration and need to understand how CloudFlow handles streaming and batch processing at scale",
          "notes": "Capacity planning context"
        },
        {
          "dimension": "negation",
          "query": "I'm confused about how data moves between stream processor, batch processor, and data lake",
          "notes": "Confusion about component interactions"
        }
      ],
      "key_facts": [
        "Kafka",
        "Flink",
        "Spark",
        "100000 messages per second",
        "Iceberg"
      ],
      "key_facts_semantic": [
        "Apache Kafka message broker",
        "Apache Flink stream processing",
        "Apache Spark batch jobs",
        "hundred thousand messages per second throughput",
        "Iceberg table format for data lake",
        "S3 object storage",
        "exactly-once processing semantics"
      ],
      "expected_docs": [
        "arch_data_pipeline"
      ],
      "ground_truth_answer": "Stream processing with Kafka/Flink (100k msg/s), batch with Spark, storage in S3 with Iceberg.",
      "metadata": {
        "concepts": [
          "data pipeline",
          "stream processing",
          "batch processing",
          "data lake",
          "ETL"
        ],
        "related_terms": [
          "Kafka",
          "Flink",
          "Spark",
          "Iceberg",
          "S3",
          "Parquet",
          "streaming",
          "batch",
          "exactly-once"
        ],
        "user_intent": "understand_design",
        "difficulty": "medium",
        "requires_inference": false,
        "domain": "data"
      }
    },
    {
      "id": "arch_036",
      "category": "architecture",
      "original_query": "How does the Search Service work?",
      "human_queries": [
        {
          "dimension": "synonym",
          "query": "What is the full-text search system design and how does indexing work?",
          "notes": "Replaced 'Search Service' with 'full-text search system' and added indexing aspect"
        },
        {
          "dimension": "problem",
          "query": "Search results are taking too long - need to understand the query engine and indexing latency",
          "notes": "Performance issue requiring understanding of search architecture"
        },
        {
          "dimension": "casual",
          "query": "search service architecture",
          "notes": "Abbreviated tech-focused search"
        },
        {
          "dimension": "contextual",
          "query": "Building an autocomplete feature and need to understand how CloudFlow's suggestion service works and its response time",
          "notes": "Feature development context focusing on specific component"
        },
        {
          "dimension": "negation",
          "query": "I don't understand how documents get indexed and how autocomplete suggestions are generated",
          "notes": "Confusion about indexing and suggestion flow"
        }
      ],
      "key_facts": [
        "Elasticsearch",
        "500ms",
        "autocomplete",
        "20ms"
      ],
      "key_facts_semantic": [
        "Elasticsearch-based search",
        "500 millisecond indexing latency",
        "autocomplete suggestions",
        "20 millisecond suggestion response time",
        "full-text and semantic search",
        "custom analyzers for technical content",
        "completion suggester"
      ],
      "expected_docs": [
        "arch_search_service"
      ],
      "ground_truth_answer": "Elasticsearch-based with 500ms indexing latency. Autocomplete suggestions within 20ms.",
      "metadata": {
        "concepts": [
          "search service",
          "full-text search",
          "indexing",
          "autocomplete",
          "query engine"
        ],
        "related_terms": [
          "Elasticsearch",
          "indexer",
          "query engine",
          "autocomplete",
          "suggestions",
          "facets",
          "relevance ranking",
          "analyzers"
        ],
        "user_intent": "understand_design",
        "difficulty": "medium",
        "requires_inference": false,
        "domain": "search"
      }
    },
    {
      "id": "arch_037",
      "category": "architecture",
      "original_query": "Explain the Authentication Service",
      "human_queries": [
        {
          "dimension": "synonym",
          "query": "How is the identity management and token issuance system designed?",
          "notes": "Replaced 'Authentication Service' with 'identity management and token issuance system'"
        },
        {
          "dimension": "problem",
          "query": "Users report getting logged out unexpectedly - need to understand token lifecycle and refresh mechanism",
          "notes": "User-reported issue requiring understanding of token architecture"
        },
        {
          "dimension": "casual",
          "query": "auth service design",
          "notes": "Key technology terms as search"
        },
        {
          "dimension": "contextual",
          "query": "Integrating with our corporate SSO and need to understand how CloudFlow handles OAuth, SAML, and password hashing",
          "notes": "SSO integration context"
        },
        {
          "dimension": "negation",
          "query": "I don't get how password hashing and JWT token signing work in the auth service",
          "notes": "Confusion about security implementation details"
        }
      ],
      "key_facts": [
        "bcrypt",
        "cost factor 12",
        "RS256",
        "refresh tokens"
      ],
      "key_facts_semantic": [
        "bcrypt password hashing",
        "cost factor of 12 for bcrypt",
        "RS256 JWT signing algorithm",
        "refresh token rotation",
        "1-hour access tokens",
        "7-day refresh tokens",
        "OAuth 2.0 and SAML support",
        "zero knowledge password storage"
      ],
      "expected_docs": [
        "arch_authentication_service"
      ],
      "ground_truth_answer": "Password hashing with bcrypt cost factor 12, JWT with RS256, 1-hour access tokens, 7-day refresh tokens.",
      "metadata": {
        "concepts": [
          "authentication",
          "identity management",
          "token management",
          "password security",
          "SSO"
        ],
        "related_terms": [
          "bcrypt",
          "JWT",
          "RS256",
          "OAuth 2.0",
          "SAML",
          "refresh tokens",
          "access tokens",
          "SSO",
          "Dex",
          "OIDC"
        ],
        "user_intent": "understand_design",
        "difficulty": "medium",
        "requires_inference": false,
        "domain": "security"
      }
    },
    {
      "id": "api_038",
      "category": "api_reference",
      "original_query": "How do I list workflows via API?",
      "human_queries": [
        {
          "dimension": "synonym",
          "query": "I need to fetch all my workflows through the REST endpoint",
          "notes": "Uses 'fetch' instead of 'list', 'REST endpoint' instead of 'API'"
        },
        {
          "dimension": "problem",
          "query": "my GET request to /workflows returns empty array even though I have workflows",
          "notes": "Describes a symptom - getting empty results when listing"
        },
        {
          "dimension": "casual",
          "query": "list workflows endpoint",
          "notes": "Terse lookup format"
        },
        {
          "dimension": "contextual",
          "query": "building a dashboard that shows all user workflows, need the API call to retrieve them",
          "notes": "Real-world scenario of building a dashboard"
        },
        {
          "dimension": "negation",
          "query": "why can't I see my workflows when calling the API",
          "notes": "Expresses confusion about not seeing workflows"
        }
      ],
      "key_facts": [
        "GET /workflows",
        "limit",
        "status"
      ],
      "key_facts_semantic": [
        "retrieve workflows using GET request to /workflows endpoint",
        "workflows can be filtered by limit parameter",
        "workflows can be filtered by status (active, paused, archived)"
      ],
      "expected_docs": [
        "api_workflows"
      ],
      "ground_truth_answer": "GET /workflows with optional limit and status parameters.",
      "metadata": {
        "concepts": [
          "REST API",
          "workflow listing",
          "HTTP GET",
          "pagination"
        ],
        "related_terms": [
          "fetch workflows",
          "retrieve workflows",
          "get all workflows",
          "query workflows",
          "workflow list endpoint"
        ],
        "user_intent": "find_endpoint",
        "difficulty": "easy",
        "requires_inference": false,
        "domain": "api"
      }
    },
    {
      "id": "api_039",
      "category": "api_reference",
      "original_query": "How do I create a workflow via API?",
      "human_queries": [
        {
          "dimension": "synonym",
          "query": "I need to programmatically add a new workflow using the REST interface",
          "notes": "Uses 'programmatically add' and 'REST interface' instead of 'create via API'"
        },
        {
          "dimension": "problem",
          "query": "getting 400 error when trying to POST a new workflow, what fields are required",
          "notes": "Describes symptom of creation failure"
        },
        {
          "dimension": "casual",
          "query": "POST workflows endpoint params",
          "notes": "Terse lookup for POST endpoint parameters"
        },
        {
          "dimension": "contextual",
          "query": "automating workflow provisioning for new customers, need to know how to create workflows via API",
          "notes": "Customer onboarding automation scenario"
        },
        {
          "dimension": "negation",
          "query": "why isn't my workflow creation request working",
          "notes": "Frustration with failed workflow creation"
        }
      ],
      "key_facts": [
        "POST /workflows",
        "201",
        "name",
        "definition"
      ],
      "key_facts_semantic": [
        "create workflow using POST request to /workflows",
        "successful creation returns 201 status code",
        "name field is required when creating workflow",
        "definition field contains workflow configuration in JSON"
      ],
      "expected_docs": [
        "api_workflows"
      ],
      "ground_truth_answer": "POST /workflows with name and definition. Returns 201 on success.",
      "metadata": {
        "concepts": [
          "REST API",
          "workflow creation",
          "HTTP POST",
          "resource creation"
        ],
        "related_terms": [
          "add workflow",
          "new workflow",
          "workflow provisioning",
          "submit workflow",
          "workflow endpoint"
        ],
        "user_intent": "use_api",
        "difficulty": "easy",
        "requires_inference": false,
        "domain": "api"
      }
    },
    {
      "id": "api_040",
      "category": "api_reference",
      "original_query": "How do I cancel a running execution?",
      "human_queries": [
        {
          "dimension": "synonym",
          "query": "I need to stop an in-progress workflow execution through the API",
          "notes": "Uses 'stop' instead of 'cancel', 'in-progress' instead of 'running'"
        },
        {
          "dimension": "problem",
          "query": "workflow stuck running for hours, how do I terminate it via API",
          "notes": "Describes problem of stuck execution"
        },
        {
          "dimension": "casual",
          "query": "cancel execution endpoint",
          "notes": "Minimal lookup query"
        },
        {
          "dimension": "contextual",
          "query": "building a kill switch for runaway workflows, what's the API to abort executions",
          "notes": "Building safeguards for runaway workflows"
        },
        {
          "dimension": "negation",
          "query": "why can't I stop this execution that's taking forever",
          "notes": "Frustration with long-running execution"
        }
      ],
      "key_facts": [
        "POST /executions/{id}/cancel",
        "reason"
      ],
      "key_facts_semantic": [
        "cancel execution using POST to /executions/{id}/cancel",
        "optional reason parameter to document why cancellation occurred"
      ],
      "expected_docs": [
        "api_executions"
      ],
      "ground_truth_answer": "POST /executions/{id}/cancel with optional reason parameter.",
      "metadata": {
        "concepts": [
          "REST API",
          "execution management",
          "workflow control",
          "HTTP POST"
        ],
        "related_terms": [
          "stop execution",
          "abort workflow",
          "terminate execution",
          "kill workflow",
          "halt execution"
        ],
        "user_intent": "use_api",
        "difficulty": "easy",
        "requires_inference": false,
        "domain": "api"
      }
    },
    {
      "id": "api_041",
      "category": "api_reference",
      "original_query": "How do I get current user profile?",
      "human_queries": [
        {
          "dimension": "synonym",
          "query": "I need to retrieve the authenticated user's info from the API",
          "notes": "Uses 'retrieve' and 'authenticated user's info' instead of 'get current user profile'"
        },
        {
          "dimension": "problem",
          "query": "getting 401 when calling users endpoint, how do I fetch my own profile",
          "notes": "Describes auth issue when fetching profile"
        },
        {
          "dimension": "casual",
          "query": "users me endpoint",
          "notes": "Terse lookup for /users/me"
        },
        {
          "dimension": "contextual",
          "query": "building a profile page, need to pull the logged-in user's data from the API",
          "notes": "Building UI that needs user data"
        },
        {
          "dimension": "negation",
          "query": "why doesn't the API return my user details",
          "notes": "Confusion about not getting user data"
        }
      ],
      "key_facts": [
        "GET /users/me",
        "email",
        "role"
      ],
      "key_facts_semantic": [
        "get current user profile using GET /users/me",
        "response includes user's email address",
        "response includes user's role (admin, member, etc.)"
      ],
      "expected_docs": [
        "api_users"
      ],
      "ground_truth_answer": "GET /users/me returns user object with id, email, and role.",
      "metadata": {
        "concepts": [
          "REST API",
          "user profile",
          "authentication",
          "HTTP GET"
        ],
        "related_terms": [
          "fetch user",
          "current user",
          "my profile",
          "whoami",
          "user info"
        ],
        "user_intent": "find_endpoint",
        "difficulty": "easy",
        "requires_inference": false,
        "domain": "api"
      }
    },
    {
      "id": "api_042",
      "category": "api_reference",
      "original_query": "How do I add a team member?",
      "human_queries": [
        {
          "dimension": "synonym",
          "query": "I need to invite a user to a team via the API",
          "notes": "Uses 'invite' instead of 'add', common alternative"
        },
        {
          "dimension": "problem",
          "query": "getting 400 when trying to add someone to my team, what parameters does the endpoint need",
          "notes": "Describes error when adding member"
        },
        {
          "dimension": "casual",
          "query": "team members POST endpoint",
          "notes": "Minimal lookup"
        },
        {
          "dimension": "contextual",
          "query": "automating team onboarding, need the API call to add new members to a team",
          "notes": "Team onboarding automation use case"
        },
        {
          "dimension": "negation",
          "query": "why can't I add users to my team through the API",
          "notes": "Frustration with adding team members"
        }
      ],
      "key_facts": [
        "POST /teams/{id}/members",
        "user_id",
        "role"
      ],
      "key_facts_semantic": [
        "add team member using POST to /teams/{id}/members",
        "user_id parameter specifies which user to add",
        "role parameter sets permission level (owner, admin, member, viewer)"
      ],
      "expected_docs": [
        "api_teams"
      ],
      "ground_truth_answer": "POST /teams/{id}/members with user_id and role (owner, admin, member, viewer).",
      "metadata": {
        "concepts": [
          "REST API",
          "team management",
          "user permissions",
          "HTTP POST"
        ],
        "related_terms": [
          "invite team member",
          "add user to team",
          "team invitation",
          "assign team role",
          "team membership"
        ],
        "user_intent": "use_api",
        "difficulty": "easy",
        "requires_inference": false,
        "domain": "api"
      }
    },
    {
      "id": "api_043",
      "category": "api_reference",
      "original_query": "How do I create a webhook?",
      "human_queries": [
        {
          "dimension": "synonym",
          "query": "I need to register a callback URL for workflow events via the API",
          "notes": "Uses 'register callback URL' instead of 'create webhook'"
        },
        {
          "dimension": "problem",
          "query": "webhook creation failing with invalid URL error, what format does it expect",
          "notes": "Describes URL validation error"
        },
        {
          "dimension": "casual",
          "query": "webhooks POST endpoint",
          "notes": "Terse endpoint lookup"
        },
        {
          "dimension": "contextual",
          "query": "integrating with Slack to notify on workflow completion, need to set up a webhook endpoint",
          "notes": "Slack integration scenario"
        },
        {
          "dimension": "negation",
          "query": "why isn't my webhook getting created, what am I missing",
          "notes": "Confusion about webhook creation failure"
        }
      ],
      "key_facts": [
        "POST /webhooks",
        "url",
        "events",
        "secret"
      ],
      "key_facts_semantic": [
        "create webhook using POST to /webhooks",
        "url parameter must be HTTPS",
        "events array specifies which events to subscribe to",
        "secret is used for signature verification"
      ],
      "expected_docs": [
        "api_webhooks"
      ],
      "ground_truth_answer": "POST /webhooks with HTTPS url, events array, and signing secret.",
      "metadata": {
        "concepts": [
          "REST API",
          "webhooks",
          "event subscriptions",
          "HTTP POST"
        ],
        "related_terms": [
          "register webhook",
          "callback URL",
          "event notifications",
          "webhook endpoint",
          "webhook subscription"
        ],
        "user_intent": "use_api",
        "difficulty": "easy",
        "requires_inference": false,
        "domain": "api"
      }
    },
    {
      "id": "api_044",
      "category": "api_reference",
      "original_query": "How do I get billing usage?",
      "human_queries": [
        {
          "dimension": "synonym",
          "query": "I need to pull our consumption metrics from the billing API",
          "notes": "Uses 'pull consumption metrics' instead of 'get billing usage'"
        },
        {
          "dimension": "problem",
          "query": "need to track our API costs, where can I find usage data via API",
          "notes": "Cost tracking problem"
        },
        {
          "dimension": "casual",
          "query": "billing usage endpoint",
          "notes": "Minimal lookup query"
        },
        {
          "dimension": "contextual",
          "query": "building an internal cost dashboard, need to fetch monthly usage via API",
          "notes": "Internal tooling scenario"
        },
        {
          "dimension": "negation",
          "query": "why can't I find our usage metrics in the API response",
          "notes": "Confusion about missing usage data"
        }
      ],
      "key_facts": [
        "GET /billing/usage",
        "executions",
        "compute_minutes"
      ],
      "key_facts_semantic": [
        "get billing usage using GET /billing/usage",
        "response includes execution count",
        "response includes compute_minutes consumed",
        "response also includes storage_gb"
      ],
      "expected_docs": [
        "api_billing"
      ],
      "ground_truth_answer": "GET /billing/usage returns executions, compute_minutes, and storage_gb.",
      "metadata": {
        "concepts": [
          "REST API",
          "billing",
          "usage metrics",
          "HTTP GET"
        ],
        "related_terms": [
          "consumption data",
          "usage statistics",
          "cost tracking",
          "billing metrics",
          "resource usage"
        ],
        "user_intent": "find_endpoint",
        "difficulty": "easy",
        "requires_inference": false,
        "domain": "api"
      }
    },
    {
      "id": "api_045",
      "category": "api_reference",
      "original_query": "How do I manage secrets?",
      "human_queries": [
        {
          "dimension": "synonym",
          "query": "I need to store and update environment variables via the secrets API",
          "notes": "Uses 'environment variables' instead of 'secrets'"
        },
        {
          "dimension": "problem",
          "query": "need to rotate database credentials, what's the API for updating secrets",
          "notes": "Credential rotation use case"
        },
        {
          "dimension": "casual",
          "query": "secrets CRUD endpoints",
          "notes": "Developer shorthand for create/read/update/delete"
        },
        {
          "dimension": "contextual",
          "query": "automating secret rotation in CI/CD, need to PUT and DELETE secrets via API",
          "notes": "CI/CD secret management scenario"
        },
        {
          "dimension": "negation",
          "query": "why can't I delete old secrets from the API",
          "notes": "Trouble removing secrets"
        }
      ],
      "key_facts": [
        "PUT /secrets/{name}",
        "DELETE /secrets/{name}",
        "encrypted at rest"
      ],
      "key_facts_semantic": [
        "create or update secret using PUT /secrets/{name}",
        "remove secret using DELETE /secrets/{name}",
        "all secret values are encrypted at rest for security"
      ],
      "expected_docs": [
        "api_secrets"
      ],
      "ground_truth_answer": "PUT to create/update, DELETE to remove. Values encrypted at rest.",
      "metadata": {
        "concepts": [
          "REST API",
          "secrets management",
          "security",
          "HTTP PUT",
          "HTTP DELETE"
        ],
        "related_terms": [
          "environment variables",
          "credentials",
          "secret rotation",
          "secure storage",
          "config secrets"
        ],
        "user_intent": "use_api",
        "difficulty": "easy",
        "requires_inference": false,
        "domain": "api"
      }
    },
    {
      "id": "release_046",
      "category": "release_notes",
      "original_query": "What's new in version 3.2.0?",
      "human_queries": [
        {
          "dimension": "synonym",
          "query": "What changes and updates are included in the v3.2.0 changelog?",
          "notes": "Used 'changelog', 'changes', 'updates' as synonyms for 'release notes' and 'what's new'"
        },
        {
          "dimension": "problem",
          "query": "I upgraded to 3.2.0 and now the API v1 calls are showing deprecation warnings, what happened?",
          "notes": "Describes symptom of upgrading - discovering deprecation warnings related to v3.2.0 changes"
        },
        {
          "dimension": "casual",
          "query": "3.2.0 new features",
          "notes": "Terse search query style, just version and 'new features'"
        },
        {
          "dimension": "contextual",
          "query": "We're evaluating upgrading from 3.1 to 3.2.0, what new capabilities did they add like workflow versioning or SDK improvements?",
          "notes": "Upgrade planning context with specific feature mentions"
        },
        {
          "dimension": "negation",
          "query": "Why doesn't version 3.1 have workflow rollback support?",
          "notes": "Asks about missing feature in older version, answer points to 3.2.0 where it was added"
        }
      ],
      "key_facts": [
        "Workflow versioning",
        "Python SDK",
        "async support",
        "v4.0"
      ],
      "key_facts_semantic": [
        "Workflow version control with rollback capability",
        "New Python library with asynchronous support",
        "async/await support in Python SDK",
        "API v1 deprecated, removed in version 4.0"
      ],
      "expected_docs": [
        "release_3_2_0"
      ],
      "ground_truth_answer": "Workflow versioning with rollback, new Python SDK with async support. API v1 deprecated.",
      "metadata": {
        "concepts": [
          "release notes",
          "versioning",
          "SDK",
          "API deprecation"
        ],
        "related_terms": [
          "changelog",
          "updates",
          "new features",
          "version history",
          "release",
          "upgrade"
        ],
        "user_intent": "track_changes",
        "difficulty": "easy",
        "requires_inference": false,
        "domain": "devops"
      }
    },
    {
      "id": "release_047",
      "category": "release_notes",
      "original_query": "What security fixes were in v3.0.0?",
      "human_queries": [
        {
          "dimension": "synonym",
          "query": "What security patches and vulnerability fixes were shipped in release 3.0.0?",
          "notes": "Used 'patches', 'vulnerability fixes', 'shipped' as synonyms"
        },
        {
          "dimension": "problem",
          "query": "Our security team flagged a token validation issue, was this addressed in any recent updates?",
          "notes": "Describes the security concern as a problem that needs resolution"
        },
        {
          "dimension": "casual",
          "query": "v3.0.0 security fixes",
          "notes": "Minimal search-style query"
        },
        {
          "dimension": "contextual",
          "query": "Running a security audit and need to verify if the token validation vulnerability was fixed, which version has that patch?",
          "notes": "Security audit context asking about specific vulnerability fix"
        },
        {
          "dimension": "negation",
          "query": "Is there a known security vulnerability in token validation that hasn't been patched?",
          "notes": "Framed as verification whether a vulnerability exists (answer: it was fixed in v3.0.0)"
        }
      ],
      "key_facts": [
        "critical security issue",
        "token validation"
      ],
      "key_facts_semantic": [
        "Critical security vulnerability was patched",
        "Token validation security fix",
        "Authentication token security issue resolved"
      ],
      "expected_docs": [
        "release_3_0_0"
      ],
      "ground_truth_answer": "Fixed critical security issue in token validation.",
      "metadata": {
        "concepts": [
          "security",
          "vulnerabilities",
          "patches",
          "token validation"
        ],
        "related_terms": [
          "security fix",
          "vulnerability",
          "CVE",
          "patch",
          "hotfix",
          "security update"
        ],
        "user_intent": "find_version_info",
        "difficulty": "easy",
        "requires_inference": false,
        "domain": "security"
      }
    },
    {
      "id": "release_048",
      "category": "release_notes",
      "original_query": "What breaking changes are in v3.0.0?",
      "human_queries": [
        {
          "dimension": "synonym",
          "query": "What backwards-incompatible changes and migration requirements are in version 3.0.0?",
          "notes": "Used 'backwards-incompatible', 'migration requirements' as synonyms for 'breaking changes'"
        },
        {
          "dimension": "problem",
          "query": "After upgrading to 3.0.0 our webhooks stopped working and auth is broken, what changed?",
          "notes": "Describes symptoms of breaking changes - webhooks and auth failures post-upgrade"
        },
        {
          "dimension": "casual",
          "query": "v3.0 breaking changes nodejs",
          "notes": "Casual search with abbreviated version and specific concern about Node.js"
        },
        {
          "dimension": "contextual",
          "query": "Planning to upgrade from 2.x to 3.0.0, what will break and do I need to update our Node.js version?",
          "notes": "Upgrade planning context with specific concern about Node.js requirements"
        },
        {
          "dimension": "negation",
          "query": "Why did the legacy webhook format stop working after updating?",
          "notes": "Expresses confusion about unexpected behavior (webhook breakage)"
        }
      ],
      "key_facts": [
        "Authentication API",
        "Legacy webhook format",
        "Node.js version now 18"
      ],
      "key_facts_semantic": [
        "Auth API was completely rewritten and requires migration",
        "Old webhook format is no longer supported",
        "Minimum Node.js requirement increased to version 18"
      ],
      "expected_docs": [
        "release_3_0_0"
      ],
      "ground_truth_answer": "Authentication API rewritten, legacy webhook format removed, minimum Node.js 18.",
      "metadata": {
        "concepts": [
          "breaking changes",
          "migration",
          "API compatibility",
          "version requirements"
        ],
        "related_terms": [
          "backwards incompatible",
          "upgrade guide",
          "migration path",
          "deprecated",
          "removed features"
        ],
        "user_intent": "understand_migration",
        "difficulty": "medium",
        "requires_inference": false,
        "domain": "devops"
      }
    },
    {
      "id": "release_049",
      "category": "release_notes",
      "original_query": "When was GraphQL API released?",
      "human_queries": [
        {
          "dimension": "synonym",
          "query": "What version introduced the GraphQL endpoint and when was it launched?",
          "notes": "Used 'introduced', 'endpoint', 'launched' as synonyms"
        },
        {
          "dimension": "problem",
          "query": "I can't find the GraphQL API in my current version, when was it added?",
          "notes": "Problem framing - user can't find feature, needs to know when/where it appeared"
        },
        {
          "dimension": "casual",
          "query": "graphql api release date cloudflow",
          "notes": "Search-style terse query for GraphQL release timing"
        },
        {
          "dimension": "contextual",
          "query": "We want to migrate from REST to GraphQL, which CloudFlow version do we need to be on to use the GraphQL API?",
          "notes": "Migration context asking about version requirements for GraphQL"
        },
        {
          "dimension": "negation",
          "query": "Why doesn't v3.0.0 have GraphQL support?",
          "notes": "Asks about absence in earlier version (GraphQL was added in v3.1.0)"
        }
      ],
      "key_facts": [
        "GraphQL API beta",
        "2023-12-01"
      ],
      "key_facts_semantic": [
        "GraphQL API was released as a beta feature",
        "GraphQL became available on December 1st, 2023",
        "Version 3.1.0 introduced GraphQL support"
      ],
      "expected_docs": [
        "release_3_1_0"
      ],
      "ground_truth_answer": "GraphQL API was released in beta in v3.1.0 on 2023-12-01.",
      "metadata": {
        "concepts": [
          "release dates",
          "GraphQL",
          "API",
          "feature availability"
        ],
        "related_terms": [
          "launch date",
          "release version",
          "when added",
          "introduced",
          "shipped"
        ],
        "user_intent": "find_version_info",
        "difficulty": "easy",
        "requires_inference": false,
        "domain": "api"
      }
    },
    {
      "id": "complex_050",
      "category": "complex_multi_hop",
      "original_query": "What are all the technologies used for data storage and caching?",
      "human_queries": [
        {
          "dimension": "synonym",
          "query": "what data stores, databases, and cache solutions does cloudflow use for persistence and memory caching?",
          "notes": "Replaced 'technologies' with 'data stores, databases, cache solutions'; 'storage' with 'persistence'; 'caching' with 'memory caching'"
        },
        {
          "dimension": "problem",
          "query": "we're hitting slow queries and cache misses everywhere - what's our current data layer stack and where is data actually stored?",
          "notes": "Describes performance symptoms (slow queries, cache misses) that lead to investigating the storage/caching stack"
        },
        {
          "dimension": "casual",
          "query": "cloudflow storage and caching stack overview",
          "notes": "Terse query mentioning expected technologies as examples, typical dev shorthand"
        },
        {
          "dimension": "contextual",
          "query": "planning our disaster recovery strategy - need a complete inventory of all storage and caching technologies so we know what to back up and replicate",
          "notes": "Frames within DR planning context, explains why complete inventory is needed"
        },
        {
          "dimension": "negation",
          "query": "do we even have a proper caching strategy? what's actually persisting data vs just caching it in memory?",
          "notes": "Expresses doubt about caching architecture, frames as verification request"
        }
      ],
      "key_facts": [
        "PostgreSQL",
        "Redis Cluster",
        "S3",
        "Ristretto",
        "CloudFront",
        "Iceberg"
      ],
      "key_facts_semantic": [
        "PostgreSQL serves as the primary relational database for OLTP workloads",
        "Redis Cluster provides distributed caching with a 6-node high availability setup",
        "Amazon S3 handles object storage for artifacts and uploads with intelligent tiering",
        "Ristretto provides in-memory local caching at the application level",
        "CloudFront CDN caches static assets and API responses at edge locations",
        "Iceberg tables on S3 serve as the data lake for long-term storage and analytics"
      ],
      "expected_docs": [
        "adr_001",
        "adr_004",
        "adr_008",
        "arch_caching_layer",
        "arch_data_pipeline"
      ],
      "ground_truth_answer": "PostgreSQL for OLTP, Redis Cluster for caching, S3 for objects, Ristretto for local cache, CloudFront CDN, Iceberg for data lake.",
      "metadata": {
        "concepts": [
          "data storage",
          "caching architecture",
          "database selection",
          "object storage",
          "CDN",
          "data lake"
        ],
        "related_terms": [
          "persistence layer",
          "data tier",
          "cache invalidation",
          "storage backend",
          "in-memory cache",
          "distributed cache",
          "blob storage"
        ],
        "user_intent": "synthesize_storage_architecture",
        "difficulty": "hard",
        "requires_inference": true,
        "domain": "infrastructure"
      }
    },
    {
      "id": "complex_051",
      "category": "complex_multi_hop",
      "original_query": "How does CloudFlow ensure security across the stack?",
      "human_queries": [
        {
          "dimension": "synonym",
          "query": "what security controls and protection mechanisms are implemented throughout the cloudflow architecture?",
          "notes": "Replaced 'ensure security' with 'security controls and protection mechanisms'; 'across the stack' with 'throughout the architecture'"
        },
        {
          "dimension": "problem",
          "query": "our security audit is asking about auth, encryption, and data protection - where do I find what security measures are in place at each layer?",
          "notes": "Describes audit scenario as the trigger, mentions specific security concerns (auth, encryption, data protection)"
        },
        {
          "dimension": "casual",
          "query": "cloudflow security - auth encryption tls etc full stack overview",
          "notes": "Abbreviated query listing security topics, asks for full stack view"
        },
        {
          "dimension": "contextual",
          "query": "preparing for SOC 2 compliance review and need to document our security posture - what authentication, encryption, and network security is implemented across gateway, services, and storage?",
          "notes": "Real compliance scenario (SOC 2), explicitly mentions the layers (gateway, services, storage)"
        },
        {
          "dimension": "negation",
          "query": "is our data even encrypted? and what about authentication - is it properly secured end-to-end or are there gaps?",
          "notes": "Expresses doubt and concern about security gaps, frames as verification"
        }
      ],
      "key_facts": [
        "OAuth 2.0",
        "JWT",
        "bcrypt",
        "AES-256",
        "TLS",
        "Zero Trust"
      ],
      "key_facts_semantic": [
        "OAuth 2.0 with JWT tokens provides stateless authentication with 1-hour access tokens",
        "bcrypt with cost factor 12 securely hashes user passwords",
        "AES-256 encryption protects all data at rest",
        "TLS termination at the Kong gateway secures data in transit",
        "Zero Trust architecture means every request is authenticated and authorized at the gateway",
        "RS256 algorithm signs JWT tokens for cryptographic verification"
      ],
      "expected_docs": [
        "adr_003",
        "arch_authentication_service",
        "arch_api_gateway",
        "howto_set_up_data_encryption"
      ],
      "ground_truth_answer": "OAuth 2.0 with JWT auth, bcrypt password hashing, AES-256 encryption at rest, TLS in transit, Zero Trust at gateway.",
      "metadata": {
        "concepts": [
          "security architecture",
          "authentication",
          "authorization",
          "encryption",
          "zero trust",
          "defense in depth"
        ],
        "related_terms": [
          "auth",
          "crypto",
          "secure by default",
          "data protection",
          "identity management",
          "access control",
          "secrets management"
        ],
        "user_intent": "understand_security_posture",
        "difficulty": "hard",
        "requires_inference": true,
        "domain": "security"
      }
    },
    {
      "id": "complex_052",
      "category": "complex_multi_hop",
      "original_query": "What configuration is needed for production deployment?",
      "human_queries": [
        {
          "dimension": "synonym",
          "query": "what env vars and settings are required to deploy cloudflow to a prod environment?",
          "notes": "Replaced 'configuration' with 'env vars and settings'; 'production deployment' with 'deploy to prod environment'"
        },
        {
          "dimension": "problem",
          "query": "deploying to prod but getting connection failures and ssl errors - what are all the required config settings I might be missing?",
          "notes": "Describes deployment failure symptoms, asks for required configs they might have missed"
        },
        {
          "dimension": "casual",
          "query": "prod config checklist - db redis gateway ssl required settings",
          "notes": "Abbreviated checklist request mentioning key components"
        },
        {
          "dimension": "contextual",
          "query": "setting up a new production cluster and need to configure database, cache, and gateway - what are the essential environment variables and security settings?",
          "notes": "Real deployment scenario, explicitly mentions components to configure"
        },
        {
          "dimension": "negation",
          "query": "why does prod keep failing to connect? what production-only configs am I forgetting compared to dev?",
          "notes": "Expresses frustration with prod failures, frames as difference from dev environment"
        }
      ],
      "key_facts": [
        "DATABASE_URL",
        "REDIS_URL",
        "GATEWAY_SSL_ENABLED",
        "DATABASE_SSL_MODE"
      ],
      "key_facts_semantic": [
        "DATABASE_URL is required to connect to PostgreSQL in production",
        "REDIS_URL is required to connect to the Redis cache cluster",
        "GATEWAY_SSL_ENABLED must be true for production TLS termination",
        "DATABASE_SSL_MODE should be set to 'require' for encrypted database connections"
      ],
      "expected_docs": [
        "config_workflow_engine",
        "config_api_gateway",
        "config_database",
        "config_cache"
      ],
      "ground_truth_answer": "Set DATABASE_URL, REDIS_URL, enable GATEWAY_SSL_ENABLED and DATABASE_SSL_MODE=require.",
      "metadata": {
        "concepts": [
          "production configuration",
          "environment variables",
          "deployment checklist",
          "security configuration"
        ],
        "related_terms": [
          "env vars",
          "config settings",
          "deployment settings",
          "production setup",
          "ssl config",
          "connection strings"
        ],
        "user_intent": "configure_production_deployment",
        "difficulty": "hard",
        "requires_inference": true,
        "domain": "devops"
      }
    },
    {
      "id": "complex_053",
      "category": "complex_multi_hop",
      "original_query": "How does CloudFlow handle failures and recovery?",
      "human_queries": [
        {
          "dimension": "synonym",
          "query": "what fault tolerance and error recovery mechanisms does cloudflow implement for workflow execution?",
          "notes": "Replaced 'handle failures' with 'fault tolerance and error recovery mechanisms'; added context about workflow execution"
        },
        {
          "dimension": "problem",
          "query": "our workflows keep failing and we're seeing duplicate executions - how does the retry and recovery system work and are there dead letter queues?",
          "notes": "Describes specific failure symptoms (failures, duplicates), asks about retry system and DLQs"
        },
        {
          "dimension": "casual",
          "query": "cloudflow failure handling - retries backoff dlq idempotency",
          "notes": "Abbreviated listing key failure handling concepts as keywords"
        },
        {
          "dimension": "contextual",
          "query": "designing a critical financial workflow that must handle failures gracefully - how do retries, idempotency, and dead letter queues work together in cloudflow?",
          "notes": "Real use case (financial workflow), asks about integration of failure handling components"
        },
        {
          "dimension": "negation",
          "query": "why do failed workflows sometimes process twice? is there proper idempotency and what happens when all retries are exhausted?",
          "notes": "Expresses concern about duplicates, questions idempotency, asks about retry exhaustion"
        }
      ],
      "key_facts": [
        "Idempotency",
        "retry",
        "exponential backoff",
        "dead letter queues"
      ],
      "key_facts_semantic": [
        "Idempotent operations ensure workflows can be safely retried without duplicate side effects",
        "Configurable retry logic allows failed steps to be automatically retried",
        "Exponential backoff with formula delay = base * 2^attempt prevents thundering herd on failures",
        "Dead letter queues capture messages after all retries are exhausted for investigation"
      ],
      "expected_docs": [
        "arch_workflow_engine",
        "adr_005",
        "howto_implement_retry_logic"
      ],
      "ground_truth_answer": "Idempotent operations, configurable retry with exponential backoff, dead letter queues for failed messages.",
      "metadata": {
        "concepts": [
          "fault tolerance",
          "error handling",
          "retry logic",
          "message queues",
          "idempotency",
          "failure recovery"
        ],
        "related_terms": [
          "resilience",
          "retry policy",
          "backoff strategy",
          "DLQ",
          "at-least-once delivery",
          "exactly-once processing",
          "failure modes"
        ],
        "user_intent": "understand_failure_handling",
        "difficulty": "hard",
        "requires_inference": true,
        "domain": "reliability"
      }
    }
  ]
}