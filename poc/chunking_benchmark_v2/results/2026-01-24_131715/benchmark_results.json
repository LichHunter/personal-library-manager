{
  "metadata": {
    "timestamp": "2026-01-24_131715",
    "num_documents": 52,
    "num_queries": 53,
    "total_facts": 180,
    "has_human_queries": true
  },
  "evaluations": [
    {
      "strategy": "semantic",
      "chunking": "fixed_512_0pct",
      "embedder": "all-MiniLM-L6-v2",
      "reranker": null,
      "llm": null,
      "k": 5,
      "metric": "exact_match",
      "num_chunks": 52,
      "index_time_s": 0.2843022369997925,
      "aggregate": {
        "original": {
          "coverage": 0.9611111111111111,
          "found": 173,
          "total": 180,
          "avg_latency_ms": 3.9234358300745953,
          "p95_latency_ms": 4.572059799465932
        },
        "synonym": {
          "coverage": 0.9388888888888889,
          "found": 169,
          "total": 180,
          "avg_latency_ms": 3.818626358310521,
          "p95_latency_ms": 4.597525800636504
        },
        "problem": {
          "coverage": 0.8888888888888888,
          "found": 160,
          "total": 180,
          "avg_latency_ms": 3.831511792531325,
          "p95_latency_ms": 4.863152000689297
        },
        "casual": {
          "coverage": 0.9666666666666667,
          "found": 174,
          "total": 180,
          "avg_latency_ms": 3.7450460566208865,
          "p95_latency_ms": 4.529593601182569
        },
        "contextual": {
          "coverage": 0.95,
          "found": 171,
          "total": 180,
          "avg_latency_ms": 3.8675848302104123,
          "p95_latency_ms": 4.453612200450152
        },
        "negation": {
          "coverage": 0.8888888888888888,
          "found": 160,
          "total": 180,
          "avg_latency_ms": 3.8346459810816427,
          "p95_latency_ms": 4.881752799701644
        }
      },
      "per_query": [
        {
          "key_facts": [
            "100 requests per minute",
            "rate limit"
          ],
          "found_facts": [
            "100 requests per minute",
            "rate limit"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "api_workflows",
            "arch_api_gateway"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "adr_007",
              "chunk_id": "adr_007_fix_0",
              "content": "# ADR-007: Implement Rate Limiting at API Gateway **Status:** Accepted **Date:** 2024-09-03 ## Context We need to protect our services from abuse and ensure fair resource usage across customers. ## Options Considered ### Option 1: Sliding window at gateway Rate limit at API gateway level. **Pros:** - Centralized enforcement - Low latency - Easy configuration **Cons:** - Gateway becomes bottleneck - Coarse granularity - Redis dependency ### Option 2: Per-service limiting Each service implements its own limits. **Pros:** - Fine-grained control - Service autonomy - No single point of failure **Cons:** - Inconsistent implementation - Higher complexity - Distributed state ### Option 3: Token bucket Token-based rate limiting. **Pros:** - Allows bursts - Smooth rate limiting - Well understood **Cons:** - More complex - Harder to explain to users - State management ## Decision We will implement sliding window rate limiting at the Kong API Gateway using Redis. ## Consequences - Default limit: 100 requests per minute - Implement per-plan limits - Return rate limit headers - Add retry-after on 429",
              "score": null
            },
            {
              "doc_id": "arch_api_gateway",
              "chunk_id": "arch_api_gateway_fix_0",
              "content": "# API Gateway Architecture ## Overview The API Gateway handles all incoming HTTP requests, providing authentication, rate limiting, and request routing. Built on Kong with custom plugins for CloudFlow-specific functionality. ## Design Principles - **Zero Trust**: Every request is authenticated and authorized - **Defense in Depth**: Multiple layers of security validation ## Components ### Kong Gateway Primary ingress controller. Handles TLS termination, request routing, and plugin execution. **Technology:** Kong 3.4 on Kubernetes **Scaling:** Horizontal with HPA ### Auth Plugin Validates JWT tokens, API keys, and OAuth2 flows. Token validation latency under 5ms. **Technology:** Custom Lua plugin **Scaling:** Stateless ### Rate Limiter Enforces per-user and per-IP rate limits. Default limit is 100 requests per minute. **Technology:** Redis-backed sliding window **Scaling:** Shared Redis cluster ## Data Flow Client \u2192 TLS \u2192 Kong \u2192 Auth \u2192 Rate Limit \u2192 Backend Services ## Performance Characteristics - **Latency P50:** 12ms - **Latency P99:** 45ms - **Throughput:** 50000 requests/second",
              "score": null
            },
            {
              "doc_id": "config_api_gateway",
              "chunk_id": "config_api_gateway_fix_0",
              "content": "# API Gateway Configuration Reference ## Overview This document describes all configuration options for API Gateway. ## Environment Variables | Variable | Type | Default | Description | |----------|------|---------|-------------| | `GATEWAY_PORT` | integer | 8000 | Gateway listen port | | `GATEWAY_RATE_LIMIT` | integer | 100 | Requests per minute per user | | `GATEWAY_TIMEOUT_MS` | integer | 30000 | Request timeout in milliseconds | | `GATEWAY_MAX_BODY_SIZE` | string | 10MB | Maximum request body size | | `GATEWAY_CORS_ORIGINS` | string | * | Allowed CORS origins | | `GATEWAY_SSL_ENABLED` | boolean | true | Enable TLS termination | ## Configuration File Configuration can also be provided via `api gateway.yaml`: ```yaml gateway_port: 8000 gateway_rate_limit: 100 gateway_timeout_ms: 30000 gateway_max_body_size: 10MB gateway_cors_origins: * ``` ## Validation Configuration is validated at startup. Invalid configuration will prevent the service from starting.",
              "score": null
            },
            {
              "doc_id": "api_metrics",
              "chunk_id": "api_metrics_fix_0",
              "content": "# Metrics API Reference ## Overview The Metrics API provides programmatic access to metrics functionality. Base URL: `https://api.cloudflow.io/v1/metrics` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /metrics/workflows Get workflow performance metrics. **Parameters:** - `period` (string): Time period: hour, day, week, month - `workflow_id` (string): Filter by workflow ID **Response:** - Success: `200` - Returns metrics data - Error: `400` - Bad Request **Example:** ```json {\"success_rate\": 0.985, \"avg_duration_ms\": 1250, \"p99_duration_ms\": 5000, \"total_executions\": 10000} ```",
              "score": null
            },
            {
              "doc_id": "api_workflows",
              "chunk_id": "api_workflows_fix_0",
              "content": "# Workflows API Reference ## Overview The Workflows API provides programmatic access to workflows functionality. Base URL: `https://api.cloudflow.io/v1/workflows` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /workflows List all workflows for the authenticated user. **Parameters:** - `limit` (integer): Maximum number of results (default: 50, max: 200) - `status` (string): Filter by status: active, paused, archived **Response:** - Success: `200` - Returns array of workflow objects - Error: `400` - Bad Request **Example:** ```json {\"workflows\": [{\"id\": \"wf_123\", \"name\": \"Daily Report\", \"status\": \"active\"}]} ``` ### POST /workflows Create a new workflow. **Parameters:** - `name` (string): Workflow name (required) - `definition` (object): Workflow definition in JSON format **Response:** - Success: `201` - Workflow created successfully - Error: `400` - Bad Request **Example:** ```json {\"id\": \"wf_456\", \"name\": \"New Workflow\", \"created_at\": \"2024-01-15T10:00:00Z\"} ``` ### DELETE /workflows/{id} Delete a workflow permanently. **Parameters:** - `id` (string): Workflow ID (required) **Response:** - Success: `204` - Workflow deleted - Error: `404` - Workflow not found **Example:** ```json {} ```",
              "score": null
            }
          ],
          "latency_ms": 13.688480999917374,
          "query_id": "simple_001",
          "dimension": "original",
          "query": "What is the default rate limit for API requests?"
        },
        {
          "key_facts": [
            "100 requests per minute",
            "rate limit"
          ],
          "found_facts": [
            "100 requests per minute",
            "rate limit"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "api_workflows",
            "arch_api_gateway"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "adr_007",
              "chunk_id": "adr_007_fix_0",
              "content": "# ADR-007: Implement Rate Limiting at API Gateway **Status:** Accepted **Date:** 2024-09-03 ## Context We need to protect our services from abuse and ensure fair resource usage across customers. ## Options Considered ### Option 1: Sliding window at gateway Rate limit at API gateway level. **Pros:** - Centralized enforcement - Low latency - Easy configuration **Cons:** - Gateway becomes bottleneck - Coarse granularity - Redis dependency ### Option 2: Per-service limiting Each service implements its own limits. **Pros:** - Fine-grained control - Service autonomy - No single point of failure **Cons:** - Inconsistent implementation - Higher complexity - Distributed state ### Option 3: Token bucket Token-based rate limiting. **Pros:** - Allows bursts - Smooth rate limiting - Well understood **Cons:** - More complex - Harder to explain to users - State management ## Decision We will implement sliding window rate limiting at the Kong API Gateway using Redis. ## Consequences - Default limit: 100 requests per minute - Implement per-plan limits - Return rate limit headers - Add retry-after on 429",
              "score": null
            },
            {
              "doc_id": "arch_api_gateway",
              "chunk_id": "arch_api_gateway_fix_0",
              "content": "# API Gateway Architecture ## Overview The API Gateway handles all incoming HTTP requests, providing authentication, rate limiting, and request routing. Built on Kong with custom plugins for CloudFlow-specific functionality. ## Design Principles - **Zero Trust**: Every request is authenticated and authorized - **Defense in Depth**: Multiple layers of security validation ## Components ### Kong Gateway Primary ingress controller. Handles TLS termination, request routing, and plugin execution. **Technology:** Kong 3.4 on Kubernetes **Scaling:** Horizontal with HPA ### Auth Plugin Validates JWT tokens, API keys, and OAuth2 flows. Token validation latency under 5ms. **Technology:** Custom Lua plugin **Scaling:** Stateless ### Rate Limiter Enforces per-user and per-IP rate limits. Default limit is 100 requests per minute. **Technology:** Redis-backed sliding window **Scaling:** Shared Redis cluster ## Data Flow Client \u2192 TLS \u2192 Kong \u2192 Auth \u2192 Rate Limit \u2192 Backend Services ## Performance Characteristics - **Latency P50:** 12ms - **Latency P99:** 45ms - **Throughput:** 50000 requests/second",
              "score": null
            },
            {
              "doc_id": "config_api_gateway",
              "chunk_id": "config_api_gateway_fix_0",
              "content": "# API Gateway Configuration Reference ## Overview This document describes all configuration options for API Gateway. ## Environment Variables | Variable | Type | Default | Description | |----------|------|---------|-------------| | `GATEWAY_PORT` | integer | 8000 | Gateway listen port | | `GATEWAY_RATE_LIMIT` | integer | 100 | Requests per minute per user | | `GATEWAY_TIMEOUT_MS` | integer | 30000 | Request timeout in milliseconds | | `GATEWAY_MAX_BODY_SIZE` | string | 10MB | Maximum request body size | | `GATEWAY_CORS_ORIGINS` | string | * | Allowed CORS origins | | `GATEWAY_SSL_ENABLED` | boolean | true | Enable TLS termination | ## Configuration File Configuration can also be provided via `api gateway.yaml`: ```yaml gateway_port: 8000 gateway_rate_limit: 100 gateway_timeout_ms: 30000 gateway_max_body_size: 10MB gateway_cors_origins: * ``` ## Validation Configuration is validated at startup. Invalid configuration will prevent the service from starting.",
              "score": null
            },
            {
              "doc_id": "howto_create_api_integration",
              "chunk_id": "howto_create_api_integration_fix_0",
              "content": "# How to Create API Integration ## Prerequisites - CloudFlow account ## Steps ### Step 1: Generate API key Create an API key for programmatic access. ```bash cloudflow apikeys create --name 'CI Integration' --scopes workflows:read,workflows:execute ``` ### Step 2: Store key securely Never commit API keys to source control. Use environment variables or secret management. > **Note:** Rotate keys every 90 days ### Step 3: Test authentication Verify the API key works correctly. ```bash curl -H 'Authorization: Bearer cf_...' https://api.cloudflow.io/v1/me ``` ### Step 4: Implement rate limiting Handle rate limit responses (HTTP 429) with exponential backoff. > **Note:** Default limit is 100 requests per minute ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            },
            {
              "doc_id": "api_metrics",
              "chunk_id": "api_metrics_fix_0",
              "content": "# Metrics API Reference ## Overview The Metrics API provides programmatic access to metrics functionality. Base URL: `https://api.cloudflow.io/v1/metrics` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /metrics/workflows Get workflow performance metrics. **Parameters:** - `period` (string): Time period: hour, day, week, month - `workflow_id` (string): Filter by workflow ID **Response:** - Success: `200` - Returns metrics data - Error: `400` - Bad Request **Example:** ```json {\"success_rate\": 0.985, \"avg_duration_ms\": 1250, \"p99_duration_ms\": 5000, \"total_executions\": 10000} ```",
              "score": null
            }
          ],
          "latency_ms": 4.656201001125737,
          "query_id": "simple_001",
          "dimension": "synonym",
          "query": "what's the API throttling limit per user"
        },
        {
          "key_facts": [
            "100 requests per minute",
            "rate limit"
          ],
          "found_facts": [
            "100 requests per minute",
            "rate limit"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "api_workflows",
            "arch_api_gateway"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "api_workflows",
              "chunk_id": "api_workflows_fix_0",
              "content": "# Workflows API Reference ## Overview The Workflows API provides programmatic access to workflows functionality. Base URL: `https://api.cloudflow.io/v1/workflows` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /workflows List all workflows for the authenticated user. **Parameters:** - `limit` (integer): Maximum number of results (default: 50, max: 200) - `status` (string): Filter by status: active, paused, archived **Response:** - Success: `200` - Returns array of workflow objects - Error: `400` - Bad Request **Example:** ```json {\"workflows\": [{\"id\": \"wf_123\", \"name\": \"Daily Report\", \"status\": \"active\"}]} ``` ### POST /workflows Create a new workflow. **Parameters:** - `name` (string): Workflow name (required) - `definition` (object): Workflow definition in JSON format **Response:** - Success: `201` - Workflow created successfully - Error: `400` - Bad Request **Example:** ```json {\"id\": \"wf_456\", \"name\": \"New Workflow\", \"created_at\": \"2024-01-15T10:00:00Z\"} ``` ### DELETE /workflows/{id} Delete a workflow permanently. **Parameters:** - `id` (string): Workflow ID (required) **Response:** - Success: `204` - Workflow deleted - Error: `404` - Workflow not found **Example:** ```json {} ```",
              "score": null
            },
            {
              "doc_id": "adr_007",
              "chunk_id": "adr_007_fix_0",
              "content": "# ADR-007: Implement Rate Limiting at API Gateway **Status:** Accepted **Date:** 2024-09-03 ## Context We need to protect our services from abuse and ensure fair resource usage across customers. ## Options Considered ### Option 1: Sliding window at gateway Rate limit at API gateway level. **Pros:** - Centralized enforcement - Low latency - Easy configuration **Cons:** - Gateway becomes bottleneck - Coarse granularity - Redis dependency ### Option 2: Per-service limiting Each service implements its own limits. **Pros:** - Fine-grained control - Service autonomy - No single point of failure **Cons:** - Inconsistent implementation - Higher complexity - Distributed state ### Option 3: Token bucket Token-based rate limiting. **Pros:** - Allows bursts - Smooth rate limiting - Well understood **Cons:** - More complex - Harder to explain to users - State management ## Decision We will implement sliding window rate limiting at the Kong API Gateway using Redis. ## Consequences - Default limit: 100 requests per minute - Implement per-plan limits - Return rate limit headers - Add retry-after on 429",
              "score": null
            },
            {
              "doc_id": "arch_api_gateway",
              "chunk_id": "arch_api_gateway_fix_0",
              "content": "# API Gateway Architecture ## Overview The API Gateway handles all incoming HTTP requests, providing authentication, rate limiting, and request routing. Built on Kong with custom plugins for CloudFlow-specific functionality. ## Design Principles - **Zero Trust**: Every request is authenticated and authorized - **Defense in Depth**: Multiple layers of security validation ## Components ### Kong Gateway Primary ingress controller. Handles TLS termination, request routing, and plugin execution. **Technology:** Kong 3.4 on Kubernetes **Scaling:** Horizontal with HPA ### Auth Plugin Validates JWT tokens, API keys, and OAuth2 flows. Token validation latency under 5ms. **Technology:** Custom Lua plugin **Scaling:** Stateless ### Rate Limiter Enforces per-user and per-IP rate limits. Default limit is 100 requests per minute. **Technology:** Redis-backed sliding window **Scaling:** Shared Redis cluster ## Data Flow Client \u2192 TLS \u2192 Kong \u2192 Auth \u2192 Rate Limit \u2192 Backend Services ## Performance Characteristics - **Latency P50:** 12ms - **Latency P99:** 45ms - **Throughput:** 50000 requests/second",
              "score": null
            },
            {
              "doc_id": "api_webhooks",
              "chunk_id": "api_webhooks_fix_0",
              "content": "# Webhooks API Reference ## Overview The Webhooks API provides programmatic access to webhooks functionality. Base URL: `https://api.cloudflow.io/v1/webhooks` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /webhooks List configured webhooks. **Parameters:** - `active` (boolean): Filter by active status **Response:** - Success: `200` - Returns array of webhook objects - Error: `400` - Bad Request **Example:** ```json {\"webhooks\": [{\"id\": \"wh_01\", \"url\": \"https://example.com/hook\", \"events\": [\"workflow.completed\"]}]} ``` ### POST /webhooks Create a new webhook endpoint. **Parameters:** - `url` (string): Webhook URL (must be HTTPS) - `events` (array): Events to subscribe to - `secret` (string): Signing secret for verification **Response:** - Success: `201` - Webhook created - Error: `400` - Bad Request **Example:** ```json {\"id\": \"wh_02\", \"url\": \"https://example.com/new-hook\", \"secret\": \"whsec_...\"} ```",
              "score": null
            },
            {
              "doc_id": "config_api_gateway",
              "chunk_id": "config_api_gateway_fix_0",
              "content": "# API Gateway Configuration Reference ## Overview This document describes all configuration options for API Gateway. ## Environment Variables | Variable | Type | Default | Description | |----------|------|---------|-------------| | `GATEWAY_PORT` | integer | 8000 | Gateway listen port | | `GATEWAY_RATE_LIMIT` | integer | 100 | Requests per minute per user | | `GATEWAY_TIMEOUT_MS` | integer | 30000 | Request timeout in milliseconds | | `GATEWAY_MAX_BODY_SIZE` | string | 10MB | Maximum request body size | | `GATEWAY_CORS_ORIGINS` | string | * | Allowed CORS origins | | `GATEWAY_SSL_ENABLED` | boolean | true | Enable TLS termination | ## Configuration File Configuration can also be provided via `api gateway.yaml`: ```yaml gateway_port: 8000 gateway_rate_limit: 100 gateway_timeout_ms: 30000 gateway_max_body_size: 10MB gateway_cors_origins: * ``` ## Validation Configuration is validated at startup. Invalid configuration will prevent the service from starting.",
              "score": null
            }
          ],
          "latency_ms": 3.567845000361558,
          "query_id": "simple_001",
          "dimension": "problem",
          "query": "my requests keep getting rejected with 429 too many requests"
        },
        {
          "key_facts": [
            "100 requests per minute",
            "rate limit"
          ],
          "found_facts": [
            "100 requests per minute",
            "rate limit"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "api_workflows",
            "arch_api_gateway"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "adr_007",
              "chunk_id": "adr_007_fix_0",
              "content": "# ADR-007: Implement Rate Limiting at API Gateway **Status:** Accepted **Date:** 2024-09-03 ## Context We need to protect our services from abuse and ensure fair resource usage across customers. ## Options Considered ### Option 1: Sliding window at gateway Rate limit at API gateway level. **Pros:** - Centralized enforcement - Low latency - Easy configuration **Cons:** - Gateway becomes bottleneck - Coarse granularity - Redis dependency ### Option 2: Per-service limiting Each service implements its own limits. **Pros:** - Fine-grained control - Service autonomy - No single point of failure **Cons:** - Inconsistent implementation - Higher complexity - Distributed state ### Option 3: Token bucket Token-based rate limiting. **Pros:** - Allows bursts - Smooth rate limiting - Well understood **Cons:** - More complex - Harder to explain to users - State management ## Decision We will implement sliding window rate limiting at the Kong API Gateway using Redis. ## Consequences - Default limit: 100 requests per minute - Implement per-plan limits - Return rate limit headers - Add retry-after on 429",
              "score": null
            },
            {
              "doc_id": "arch_api_gateway",
              "chunk_id": "arch_api_gateway_fix_0",
              "content": "# API Gateway Architecture ## Overview The API Gateway handles all incoming HTTP requests, providing authentication, rate limiting, and request routing. Built on Kong with custom plugins for CloudFlow-specific functionality. ## Design Principles - **Zero Trust**: Every request is authenticated and authorized - **Defense in Depth**: Multiple layers of security validation ## Components ### Kong Gateway Primary ingress controller. Handles TLS termination, request routing, and plugin execution. **Technology:** Kong 3.4 on Kubernetes **Scaling:** Horizontal with HPA ### Auth Plugin Validates JWT tokens, API keys, and OAuth2 flows. Token validation latency under 5ms. **Technology:** Custom Lua plugin **Scaling:** Stateless ### Rate Limiter Enforces per-user and per-IP rate limits. Default limit is 100 requests per minute. **Technology:** Redis-backed sliding window **Scaling:** Shared Redis cluster ## Data Flow Client \u2192 TLS \u2192 Kong \u2192 Auth \u2192 Rate Limit \u2192 Backend Services ## Performance Characteristics - **Latency P50:** 12ms - **Latency P99:** 45ms - **Throughput:** 50000 requests/second",
              "score": null
            },
            {
              "doc_id": "config_api_gateway",
              "chunk_id": "config_api_gateway_fix_0",
              "content": "# API Gateway Configuration Reference ## Overview This document describes all configuration options for API Gateway. ## Environment Variables | Variable | Type | Default | Description | |----------|------|---------|-------------| | `GATEWAY_PORT` | integer | 8000 | Gateway listen port | | `GATEWAY_RATE_LIMIT` | integer | 100 | Requests per minute per user | | `GATEWAY_TIMEOUT_MS` | integer | 30000 | Request timeout in milliseconds | | `GATEWAY_MAX_BODY_SIZE` | string | 10MB | Maximum request body size | | `GATEWAY_CORS_ORIGINS` | string | * | Allowed CORS origins | | `GATEWAY_SSL_ENABLED` | boolean | true | Enable TLS termination | ## Configuration File Configuration can also be provided via `api gateway.yaml`: ```yaml gateway_port: 8000 gateway_rate_limit: 100 gateway_timeout_ms: 30000 gateway_max_body_size: 10MB gateway_cors_origins: * ``` ## Validation Configuration is validated at startup. Invalid configuration will prevent the service from starting.",
              "score": null
            },
            {
              "doc_id": "howto_create_api_integration",
              "chunk_id": "howto_create_api_integration_fix_0",
              "content": "# How to Create API Integration ## Prerequisites - CloudFlow account ## Steps ### Step 1: Generate API key Create an API key for programmatic access. ```bash cloudflow apikeys create --name 'CI Integration' --scopes workflows:read,workflows:execute ``` ### Step 2: Store key securely Never commit API keys to source control. Use environment variables or secret management. > **Note:** Rotate keys every 90 days ### Step 3: Test authentication Verify the API key works correctly. ```bash curl -H 'Authorization: Bearer cf_...' https://api.cloudflow.io/v1/me ``` ### Step 4: Implement rate limiting Handle rate limit responses (HTTP 429) with exponential backoff. > **Note:** Default limit is 100 requests per minute ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            },
            {
              "doc_id": "api_metrics",
              "chunk_id": "api_metrics_fix_0",
              "content": "# Metrics API Reference ## Overview The Metrics API provides programmatic access to metrics functionality. Base URL: `https://api.cloudflow.io/v1/metrics` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /metrics/workflows Get workflow performance metrics. **Parameters:** - `period` (string): Time period: hour, day, week, month - `workflow_id` (string): Filter by workflow ID **Response:** - Success: `200` - Returns metrics data - Error: `400` - Bad Request **Example:** ```json {\"success_rate\": 0.985, \"avg_duration_ms\": 1250, \"p99_duration_ms\": 5000, \"total_executions\": 10000} ```",
              "score": null
            }
          ],
          "latency_ms": 3.6917749985150294,
          "query_id": "simple_001",
          "dimension": "casual",
          "query": "api rate limit"
        },
        {
          "key_facts": [
            "100 requests per minute",
            "rate limit"
          ],
          "found_facts": [
            "100 requests per minute",
            "rate limit"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "api_workflows",
            "arch_api_gateway"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "api_metrics",
              "chunk_id": "api_metrics_fix_0",
              "content": "# Metrics API Reference ## Overview The Metrics API provides programmatic access to metrics functionality. Base URL: `https://api.cloudflow.io/v1/metrics` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /metrics/workflows Get workflow performance metrics. **Parameters:** - `period` (string): Time period: hour, day, week, month - `workflow_id` (string): Filter by workflow ID **Response:** - Success: `200` - Returns metrics data - Error: `400` - Bad Request **Example:** ```json {\"success_rate\": 0.985, \"avg_duration_ms\": 1250, \"p99_duration_ms\": 5000, \"total_executions\": 10000} ```",
              "score": null
            },
            {
              "doc_id": "api_workflows",
              "chunk_id": "api_workflows_fix_0",
              "content": "# Workflows API Reference ## Overview The Workflows API provides programmatic access to workflows functionality. Base URL: `https://api.cloudflow.io/v1/workflows` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /workflows List all workflows for the authenticated user. **Parameters:** - `limit` (integer): Maximum number of results (default: 50, max: 200) - `status` (string): Filter by status: active, paused, archived **Response:** - Success: `200` - Returns array of workflow objects - Error: `400` - Bad Request **Example:** ```json {\"workflows\": [{\"id\": \"wf_123\", \"name\": \"Daily Report\", \"status\": \"active\"}]} ``` ### POST /workflows Create a new workflow. **Parameters:** - `name` (string): Workflow name (required) - `definition` (object): Workflow definition in JSON format **Response:** - Success: `201` - Workflow created successfully - Error: `400` - Bad Request **Example:** ```json {\"id\": \"wf_456\", \"name\": \"New Workflow\", \"created_at\": \"2024-01-15T10:00:00Z\"} ``` ### DELETE /workflows/{id} Delete a workflow permanently. **Parameters:** - `id` (string): Workflow ID (required) **Response:** - Success: `204` - Workflow deleted - Error: `404` - Workflow not found **Example:** ```json {} ```",
              "score": null
            },
            {
              "doc_id": "api_executions",
              "chunk_id": "api_executions_fix_0",
              "content": "# Executions API Reference ## Overview The Executions API provides programmatic access to executions functionality. Base URL: `https://api.cloudflow.io/v1/executions` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /executions List workflow executions. **Parameters:** - `workflow_id` (string): Filter by workflow ID - `since` (datetime): Filter executions after this timestamp **Response:** - Success: `200` - Returns array of execution objects - Error: `400` - Bad Request **Example:** ```json {\"executions\": [{\"id\": \"ex_789\", \"status\": \"completed\", \"duration_ms\": 1523}]} ``` ### POST /executions/{id}/cancel Cancel a running execution. **Parameters:** - `id` (string): Execution ID - `reason` (string): Cancellation reason (optional) **Response:** - Success: `200` - Execution cancelled - Error: `400` - Bad Request **Example:** ```json {\"id\": \"ex_789\", \"status\": \"cancelled\", \"cancelled_at\": \"2024-01-15T10:05:00Z\"} ```",
              "score": null
            },
            {
              "doc_id": "howto_create_api_integration",
              "chunk_id": "howto_create_api_integration_fix_0",
              "content": "# How to Create API Integration ## Prerequisites - CloudFlow account ## Steps ### Step 1: Generate API key Create an API key for programmatic access. ```bash cloudflow apikeys create --name 'CI Integration' --scopes workflows:read,workflows:execute ``` ### Step 2: Store key securely Never commit API keys to source control. Use environment variables or secret management. > **Note:** Rotate keys every 90 days ### Step 3: Test authentication Verify the API key works correctly. ```bash curl -H 'Authorization: Bearer cf_...' https://api.cloudflow.io/v1/me ``` ### Step 4: Implement rate limiting Handle rate limit responses (HTTP 429) with exponential backoff. > **Note:** Default limit is 100 requests per minute ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            },
            {
              "doc_id": "api_audit",
              "chunk_id": "api_audit_fix_0",
              "content": "# Audit API Reference ## Overview The Audit API provides programmatic access to audit functionality. Base URL: `https://api.cloudflow.io/v1/audit` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /audit/logs Retrieve audit logs for compliance. **Parameters:** - `start_date` (date): Start date for log range - `end_date` (date): End date for log range - `actor` (string): Filter by user ID **Response:** - Success: `200` - Returns array of audit log entries - Error: `400` - Bad Request **Example:** ```json {\"logs\": [{\"timestamp\": \"2024-01-15T10:00:00Z\", \"actor\": \"usr_001\", \"action\": \"workflow.created\", \"resource\": \"wf_123\"}]} ```",
              "score": null
            }
          ],
          "latency_ms": 3.8497490004374413,
          "query_id": "simple_001",
          "dimension": "contextual",
          "query": "building a batch sync tool that hits the API, need to know the request cap"
        },
        {
          "key_facts": [
            "100 requests per minute",
            "rate limit"
          ],
          "found_facts": [],
          "missed_facts": [
            "100 requests per minute",
            "rate limit"
          ],
          "coverage": 0.0,
          "expected_docs": [
            "api_workflows",
            "arch_api_gateway"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "api_workflows",
              "chunk_id": "api_workflows_fix_0",
              "content": "# Workflows API Reference ## Overview The Workflows API provides programmatic access to workflows functionality. Base URL: `https://api.cloudflow.io/v1/workflows` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /workflows List all workflows for the authenticated user. **Parameters:** - `limit` (integer): Maximum number of results (default: 50, max: 200) - `status` (string): Filter by status: active, paused, archived **Response:** - Success: `200` - Returns array of workflow objects - Error: `400` - Bad Request **Example:** ```json {\"workflows\": [{\"id\": \"wf_123\", \"name\": \"Daily Report\", \"status\": \"active\"}]} ``` ### POST /workflows Create a new workflow. **Parameters:** - `name` (string): Workflow name (required) - `definition` (object): Workflow definition in JSON format **Response:** - Success: `201` - Workflow created successfully - Error: `400` - Bad Request **Example:** ```json {\"id\": \"wf_456\", \"name\": \"New Workflow\", \"created_at\": \"2024-01-15T10:00:00Z\"} ``` ### DELETE /workflows/{id} Delete a workflow permanently. **Parameters:** - `id` (string): Workflow ID (required) **Response:** - Success: `204` - Workflow deleted - Error: `404` - Workflow not found **Example:** ```json {} ```",
              "score": null
            },
            {
              "doc_id": "api_webhooks",
              "chunk_id": "api_webhooks_fix_0",
              "content": "# Webhooks API Reference ## Overview The Webhooks API provides programmatic access to webhooks functionality. Base URL: `https://api.cloudflow.io/v1/webhooks` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /webhooks List configured webhooks. **Parameters:** - `active` (boolean): Filter by active status **Response:** - Success: `200` - Returns array of webhook objects - Error: `400` - Bad Request **Example:** ```json {\"webhooks\": [{\"id\": \"wh_01\", \"url\": \"https://example.com/hook\", \"events\": [\"workflow.completed\"]}]} ``` ### POST /webhooks Create a new webhook endpoint. **Parameters:** - `url` (string): Webhook URL (must be HTTPS) - `events` (array): Events to subscribe to - `secret` (string): Signing secret for verification **Response:** - Success: `201` - Webhook created - Error: `400` - Bad Request **Example:** ```json {\"id\": \"wh_02\", \"url\": \"https://example.com/new-hook\", \"secret\": \"whsec_...\"} ```",
              "score": null
            },
            {
              "doc_id": "api_executions",
              "chunk_id": "api_executions_fix_0",
              "content": "# Executions API Reference ## Overview The Executions API provides programmatic access to executions functionality. Base URL: `https://api.cloudflow.io/v1/executions` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /executions List workflow executions. **Parameters:** - `workflow_id` (string): Filter by workflow ID - `since` (datetime): Filter executions after this timestamp **Response:** - Success: `200` - Returns array of execution objects - Error: `400` - Bad Request **Example:** ```json {\"executions\": [{\"id\": \"ex_789\", \"status\": \"completed\", \"duration_ms\": 1523}]} ``` ### POST /executions/{id}/cancel Cancel a running execution. **Parameters:** - `id` (string): Execution ID - `reason` (string): Cancellation reason (optional) **Response:** - Success: `200` - Execution cancelled - Error: `400` - Bad Request **Example:** ```json {\"id\": \"ex_789\", \"status\": \"cancelled\", \"cancelled_at\": \"2024-01-15T10:05:00Z\"} ```",
              "score": null
            },
            {
              "doc_id": "api_integrations",
              "chunk_id": "api_integrations_fix_0",
              "content": "# Integrations API Reference ## Overview The Integrations API provides programmatic access to integrations functionality. Base URL: `https://api.cloudflow.io/v1/integrations` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /integrations List available integrations. **Parameters:** - `category` (string): Filter by category: storage, notification, database **Response:** - Success: `200` - Returns array of integration objects - Error: `400` - Bad Request **Example:** ```json {\"integrations\": [{\"id\": \"int_s3\", \"name\": \"Amazon S3\", \"category\": \"storage\", \"status\": \"connected\"}]} ``` ### POST /integrations/{id}/connect Connect an integration. **Parameters:** - `credentials` (object): Integration-specific credentials **Response:** - Success: `200` - Integration connected - Error: `400` - Bad Request **Example:** ```json {\"id\": \"int_s3\", \"status\": \"connected\", \"connected_at\": \"2024-01-15T10:00:00Z\"} ```",
              "score": null
            },
            {
              "doc_id": "config_api_gateway",
              "chunk_id": "config_api_gateway_fix_0",
              "content": "# API Gateway Configuration Reference ## Overview This document describes all configuration options for API Gateway. ## Environment Variables | Variable | Type | Default | Description | |----------|------|---------|-------------| | `GATEWAY_PORT` | integer | 8000 | Gateway listen port | | `GATEWAY_RATE_LIMIT` | integer | 100 | Requests per minute per user | | `GATEWAY_TIMEOUT_MS` | integer | 30000 | Request timeout in milliseconds | | `GATEWAY_MAX_BODY_SIZE` | string | 10MB | Maximum request body size | | `GATEWAY_CORS_ORIGINS` | string | * | Allowed CORS origins | | `GATEWAY_SSL_ENABLED` | boolean | true | Enable TLS termination | ## Configuration File Configuration can also be provided via `api gateway.yaml`: ```yaml gateway_port: 8000 gateway_rate_limit: 100 gateway_timeout_ms: 30000 gateway_max_body_size: 10MB gateway_cors_origins: * ``` ## Validation Configuration is validated at startup. Invalid configuration will prevent the service from starting.",
              "score": null
            }
          ],
          "latency_ms": 3.906363999703899,
          "query_id": "simple_001",
          "dimension": "negation",
          "query": "why is the API blocking my requests after a while"
        },
        {
          "key_facts": [
            "PostgreSQL",
            "JSONB"
          ],
          "found_facts": [
            "PostgreSQL"
          ],
          "missed_facts": [
            "JSONB"
          ],
          "coverage": 0.5,
          "expected_docs": [
            "adr_001",
            "config_database"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "howto_set_up_database_connection",
              "chunk_id": "howto_set_up_database_connection_fix_0",
              "content": "# How to Set Up Database Connection ## Prerequisites - Database with network access to CloudFlow - Connection credentials ## Steps ### Step 1: Add connection secret Store your database connection string as a secret. ```bash cloudflow secrets set DATABASE_URL postgresql://user:pass@host:5432/db ``` ### Step 2: Configure connection pool Set pool size based on your plan limits. Free tier allows 5 connections, Pro allows 20. > **Note:** Pool exhaustion will queue requests ### Step 3: Test connection Verify the database is accessible from CloudFlow. ```bash cloudflow db test --secret DATABASE_URL ``` ### Step 4: Enable SSL For production, always require SSL connections. ```bash cloudflow db configure --require-ssl ``` ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            },
            {
              "doc_id": "api_secrets",
              "chunk_id": "api_secrets_fix_0",
              "content": "# Secrets API Reference ## Overview The Secrets API provides programmatic access to secrets functionality. Base URL: `https://api.cloudflow.io/v1/secrets` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /secrets List secret names (values not returned). **Parameters:** **Response:** - Success: `200` - Returns array of secret metadata - Error: `400` - Bad Request **Example:** ```json {\"secrets\": [{\"name\": \"DATABASE_URL\", \"created_at\": \"2024-01-01\", \"updated_at\": \"2024-01-10\"}]} ``` ### PUT /secrets/{name} Create or update a secret. **Parameters:** - `value` (string): Secret value (encrypted at rest) **Response:** - Success: `200` - Secret saved - Error: `400` - Bad Request **Example:** ```json {\"name\": \"DATABASE_URL\", \"updated_at\": \"2024-01-15T10:00:00Z\"} ``` ### DELETE /secrets/{name} Delete a secret. **Parameters:** **Response:** - Success: `204` - Secret deleted - Error: `400` - Bad Request **Example:** ```json {} ```",
              "score": null
            },
            {
              "doc_id": "arch_data_pipeline",
              "chunk_id": "arch_data_pipeline_fix_0",
              "content": "# Data Pipeline Architecture ## Overview The Data Pipeline processes and transforms data between workflow steps. It supports batch processing up to 10GB and streaming for real-time use cases. ## Design Principles - **Exactly-Once Processing**: Each record is processed exactly once - **Backpressure Handling**: System gracefully handles varying load ## Components ### Stream Processor Handles real-time data streams. Maximum throughput of 100000 messages per second. **Technology:** Apache Kafka with Flink **Scaling:** Partition-based scaling ### Batch Processor Processes large batch jobs. Supports data formats: JSON, CSV, Parquet, Avro. **Technology:** Apache Spark on Kubernetes **Scaling:** Dynamic resource allocation ### Data Lake Long-term data storage with time-travel queries. Retention configurable up to 7 years. **Technology:** S3 with Iceberg tables **Scaling:** Unlimited with tiered storage ## Data Flow Input \u2192 Validation \u2192 Transform \u2192 Enrich \u2192 Output ## Performance Characteristics - **Latency P50:** 100ms (streaming), 5min (batch) - **Latency P99:** 500ms (streaming), 30min (batch) - **Throughput:** 100000 messages/second (streaming)",
              "score": null
            },
            {
              "doc_id": "howto_configure_custom_domain",
              "chunk_id": "howto_configure_custom_domain_fix_0",
              "content": "# How to Configure Custom Domain ## Prerequisites - Domain with DNS access - CloudFlow Pro plan or higher ## Steps ### Step 1: Verify domain ownership Add a TXT record to verify domain ownership. ```bash cloudflow domains verify --domain app.example.com ``` ### Step 2: Configure DNS Add a CNAME record pointing to your CloudFlow endpoint. > **Note:** CNAME should point to ingress.cloudflow.io ### Step 3: Enable SSL CloudFlow automatically provisions SSL certificates via Let's Encrypt. ```bash cloudflow domains ssl --domain app.example.com ``` ### Step 4: Set as primary Make this the primary domain for your application. ```bash cloudflow domains set-primary --domain app.example.com ``` ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            },
            {
              "doc_id": "release_3_0_0",
              "chunk_id": "release_3_0_0_fix_0",
              "content": "# Release Notes - v3.0.0 **Release Date:** 2023-10-15 ## New Features - Complete UI redesign - New workflow editor with visual builder - Real-time execution monitoring - Team workspaces ## Bug Fixes - Fixed critical security issue in token validation - Resolved data loss bug in failed migrations ## Breaking Changes - **BREAKING:** Authentication API completely rewritten - **BREAKING:** Legacy webhook format no longer supported - **BREAKING:** Minimum Node.js version now 18 ## Upgrade Instructions ```bash cloudflow upgrade --version 3.0.0 ``` ## Known Issues None at this time.",
              "score": null
            }
          ],
          "latency_ms": 3.672539000035613,
          "query_id": "simple_002",
          "dimension": "original",
          "query": "What database does CloudFlow use?"
        },
        {
          "key_facts": [
            "PostgreSQL",
            "JSONB"
          ],
          "found_facts": [
            "PostgreSQL"
          ],
          "missed_facts": [
            "JSONB"
          ],
          "coverage": 0.5,
          "expected_docs": [
            "adr_001",
            "config_database"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "adr_008",
              "chunk_id": "adr_008_fix_0",
              "content": "# ADR-008: Use S3 for Object Storage **Status:** Accepted **Date:** 2024-10-14 ## Context We need storage for workflow artifacts, execution outputs, and user uploads. ## Options Considered ### Option 1: Amazon S3 Object storage service. **Pros:** - 99.999999999% durability - Lifecycle policies - Event notifications - CDN integration **Cons:** - AWS lock-in - Eventual consistency for overwrites - Cost at scale ### Option 2: Self-hosted MinIO S3-compatible object storage. **Pros:** - No vendor lock-in - S3 compatible - On-premises option **Cons:** - Operational burden - Lower durability guarantees - Scaling complexity ### Option 3: Google Cloud Storage GCP object storage. **Pros:** - Strong consistency - Good performance - Unified billing **Cons:** - GCP lock-in - Fewer features than S3 - Smaller ecosystem ## Decision We will use Amazon S3 for object storage with intelligent tiering for cost optimization. ## Consequences - Enable versioning for critical buckets - Implement lifecycle policies - Use pre-signed URLs for uploads - Enable server-side encryption",
              "score": null
            },
            {
              "doc_id": "arch_data_pipeline",
              "chunk_id": "arch_data_pipeline_fix_0",
              "content": "# Data Pipeline Architecture ## Overview The Data Pipeline processes and transforms data between workflow steps. It supports batch processing up to 10GB and streaming for real-time use cases. ## Design Principles - **Exactly-Once Processing**: Each record is processed exactly once - **Backpressure Handling**: System gracefully handles varying load ## Components ### Stream Processor Handles real-time data streams. Maximum throughput of 100000 messages per second. **Technology:** Apache Kafka with Flink **Scaling:** Partition-based scaling ### Batch Processor Processes large batch jobs. Supports data formats: JSON, CSV, Parquet, Avro. **Technology:** Apache Spark on Kubernetes **Scaling:** Dynamic resource allocation ### Data Lake Long-term data storage with time-travel queries. Retention configurable up to 7 years. **Technology:** S3 with Iceberg tables **Scaling:** Unlimited with tiered storage ## Data Flow Input \u2192 Validation \u2192 Transform \u2192 Enrich \u2192 Output ## Performance Characteristics - **Latency P50:** 100ms (streaming), 5min (batch) - **Latency P99:** 500ms (streaming), 30min (batch) - **Throughput:** 100000 messages/second (streaming)",
              "score": null
            },
            {
              "doc_id": "howto_configure_custom_domain",
              "chunk_id": "howto_configure_custom_domain_fix_0",
              "content": "# How to Configure Custom Domain ## Prerequisites - Domain with DNS access - CloudFlow Pro plan or higher ## Steps ### Step 1: Verify domain ownership Add a TXT record to verify domain ownership. ```bash cloudflow domains verify --domain app.example.com ``` ### Step 2: Configure DNS Add a CNAME record pointing to your CloudFlow endpoint. > **Note:** CNAME should point to ingress.cloudflow.io ### Step 3: Enable SSL CloudFlow automatically provisions SSL certificates via Let's Encrypt. ```bash cloudflow domains ssl --domain app.example.com ``` ### Step 4: Set as primary Make this the primary domain for your application. ```bash cloudflow domains set-primary --domain app.example.com ``` ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            },
            {
              "doc_id": "howto_set_up_database_connection",
              "chunk_id": "howto_set_up_database_connection_fix_0",
              "content": "# How to Set Up Database Connection ## Prerequisites - Database with network access to CloudFlow - Connection credentials ## Steps ### Step 1: Add connection secret Store your database connection string as a secret. ```bash cloudflow secrets set DATABASE_URL postgresql://user:pass@host:5432/db ``` ### Step 2: Configure connection pool Set pool size based on your plan limits. Free tier allows 5 connections, Pro allows 20. > **Note:** Pool exhaustion will queue requests ### Step 3: Test connection Verify the database is accessible from CloudFlow. ```bash cloudflow db test --secret DATABASE_URL ``` ### Step 4: Enable SSL For production, always require SSL connections. ```bash cloudflow db configure --require-ssl ``` ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            },
            {
              "doc_id": "arch_api_gateway",
              "chunk_id": "arch_api_gateway_fix_0",
              "content": "# API Gateway Architecture ## Overview The API Gateway handles all incoming HTTP requests, providing authentication, rate limiting, and request routing. Built on Kong with custom plugins for CloudFlow-specific functionality. ## Design Principles - **Zero Trust**: Every request is authenticated and authorized - **Defense in Depth**: Multiple layers of security validation ## Components ### Kong Gateway Primary ingress controller. Handles TLS termination, request routing, and plugin execution. **Technology:** Kong 3.4 on Kubernetes **Scaling:** Horizontal with HPA ### Auth Plugin Validates JWT tokens, API keys, and OAuth2 flows. Token validation latency under 5ms. **Technology:** Custom Lua plugin **Scaling:** Stateless ### Rate Limiter Enforces per-user and per-IP rate limits. Default limit is 100 requests per minute. **Technology:** Redis-backed sliding window **Scaling:** Shared Redis cluster ## Data Flow Client \u2192 TLS \u2192 Kong \u2192 Auth \u2192 Rate Limit \u2192 Backend Services ## Performance Characteristics - **Latency P50:** 12ms - **Latency P99:** 45ms - **Throughput:** 50000 requests/second",
              "score": null
            }
          ],
          "latency_ms": 3.4780269998009317,
          "query_id": "simple_002",
          "dimension": "synonym",
          "query": "what's the primary data store in CloudFlow"
        },
        {
          "key_facts": [
            "PostgreSQL",
            "JSONB"
          ],
          "found_facts": [
            "PostgreSQL"
          ],
          "missed_facts": [
            "JSONB"
          ],
          "coverage": 0.5,
          "expected_docs": [
            "adr_001",
            "config_database"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "howto_debug_failed_execution",
              "chunk_id": "howto_debug_failed_execution_fix_0",
              "content": "# How to Debug Failed Execution ## Prerequisites - Failed execution ID ## Steps ### Step 1: Get execution details Retrieve the full execution record including step outputs. ```bash cloudflow executions get ex_123 --include-steps ``` ### Step 2: View step logs Check logs for the failed step. ```bash cloudflow executions logs ex_123 --step 3 ``` ### Step 3: Inspect input/output Review the data passed between steps. ```bash cloudflow executions inspect ex_123 --step 3 ``` ### Step 4: Enable debug mode Re-run with verbose logging enabled. ```bash cloudflow workflows execute wf_123 --debug ``` ### Step 5: Check error patterns Look for similar failures in recent executions. ```bash cloudflow executions list --status failed --since 24h ``` ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            },
            {
              "doc_id": "api_secrets",
              "chunk_id": "api_secrets_fix_0",
              "content": "# Secrets API Reference ## Overview The Secrets API provides programmatic access to secrets functionality. Base URL: `https://api.cloudflow.io/v1/secrets` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /secrets List secret names (values not returned). **Parameters:** **Response:** - Success: `200` - Returns array of secret metadata - Error: `400` - Bad Request **Example:** ```json {\"secrets\": [{\"name\": \"DATABASE_URL\", \"created_at\": \"2024-01-01\", \"updated_at\": \"2024-01-10\"}]} ``` ### PUT /secrets/{name} Create or update a secret. **Parameters:** - `value` (string): Secret value (encrypted at rest) **Response:** - Success: `200` - Secret saved - Error: `400` - Bad Request **Example:** ```json {\"name\": \"DATABASE_URL\", \"updated_at\": \"2024-01-15T10:00:00Z\"} ``` ### DELETE /secrets/{name} Delete a secret. **Parameters:** **Response:** - Success: `204` - Secret deleted - Error: `400` - Bad Request **Example:** ```json {} ```",
              "score": null
            },
            {
              "doc_id": "howto_set_up_database_connection",
              "chunk_id": "howto_set_up_database_connection_fix_0",
              "content": "# How to Set Up Database Connection ## Prerequisites - Database with network access to CloudFlow - Connection credentials ## Steps ### Step 1: Add connection secret Store your database connection string as a secret. ```bash cloudflow secrets set DATABASE_URL postgresql://user:pass@host:5432/db ``` ### Step 2: Configure connection pool Set pool size based on your plan limits. Free tier allows 5 connections, Pro allows 20. > **Note:** Pool exhaustion will queue requests ### Step 3: Test connection Verify the database is accessible from CloudFlow. ```bash cloudflow db test --secret DATABASE_URL ``` ### Step 4: Enable SSL For production, always require SSL connections. ```bash cloudflow db configure --require-ssl ``` ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            },
            {
              "doc_id": "api_integrations",
              "chunk_id": "api_integrations_fix_0",
              "content": "# Integrations API Reference ## Overview The Integrations API provides programmatic access to integrations functionality. Base URL: `https://api.cloudflow.io/v1/integrations` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /integrations List available integrations. **Parameters:** - `category` (string): Filter by category: storage, notification, database **Response:** - Success: `200` - Returns array of integration objects - Error: `400` - Bad Request **Example:** ```json {\"integrations\": [{\"id\": \"int_s3\", \"name\": \"Amazon S3\", \"category\": \"storage\", \"status\": \"connected\"}]} ``` ### POST /integrations/{id}/connect Connect an integration. **Parameters:** - `credentials` (object): Integration-specific credentials **Response:** - Success: `200` - Integration connected - Error: `400` - Bad Request **Example:** ```json {\"id\": \"int_s3\", \"status\": \"connected\", \"connected_at\": \"2024-01-15T10:00:00Z\"} ```",
              "score": null
            },
            {
              "doc_id": "release_3_0_0",
              "chunk_id": "release_3_0_0_fix_0",
              "content": "# Release Notes - v3.0.0 **Release Date:** 2023-10-15 ## New Features - Complete UI redesign - New workflow editor with visual builder - Real-time execution monitoring - Team workspaces ## Bug Fixes - Fixed critical security issue in token validation - Resolved data loss bug in failed migrations ## Breaking Changes - **BREAKING:** Authentication API completely rewritten - **BREAKING:** Legacy webhook format no longer supported - **BREAKING:** Minimum Node.js version now 18 ## Upgrade Instructions ```bash cloudflow upgrade --version 3.0.0 ``` ## Known Issues None at this time.",
              "score": null
            }
          ],
          "latency_ms": 4.325415000494104,
          "query_id": "simple_002",
          "dimension": "problem",
          "query": "getting database-specific errors from CloudFlow, what DB does it use"
        },
        {
          "key_facts": [
            "PostgreSQL",
            "JSONB"
          ],
          "found_facts": [
            "PostgreSQL",
            "JSONB"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "adr_001",
            "config_database"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "howto_set_up_database_connection",
              "chunk_id": "howto_set_up_database_connection_fix_0",
              "content": "# How to Set Up Database Connection ## Prerequisites - Database with network access to CloudFlow - Connection credentials ## Steps ### Step 1: Add connection secret Store your database connection string as a secret. ```bash cloudflow secrets set DATABASE_URL postgresql://user:pass@host:5432/db ``` ### Step 2: Configure connection pool Set pool size based on your plan limits. Free tier allows 5 connections, Pro allows 20. > **Note:** Pool exhaustion will queue requests ### Step 3: Test connection Verify the database is accessible from CloudFlow. ```bash cloudflow db test --secret DATABASE_URL ``` ### Step 4: Enable SSL For production, always require SSL connections. ```bash cloudflow db configure --require-ssl ``` ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            },
            {
              "doc_id": "arch_data_pipeline",
              "chunk_id": "arch_data_pipeline_fix_0",
              "content": "# Data Pipeline Architecture ## Overview The Data Pipeline processes and transforms data between workflow steps. It supports batch processing up to 10GB and streaming for real-time use cases. ## Design Principles - **Exactly-Once Processing**: Each record is processed exactly once - **Backpressure Handling**: System gracefully handles varying load ## Components ### Stream Processor Handles real-time data streams. Maximum throughput of 100000 messages per second. **Technology:** Apache Kafka with Flink **Scaling:** Partition-based scaling ### Batch Processor Processes large batch jobs. Supports data formats: JSON, CSV, Parquet, Avro. **Technology:** Apache Spark on Kubernetes **Scaling:** Dynamic resource allocation ### Data Lake Long-term data storage with time-travel queries. Retention configurable up to 7 years. **Technology:** S3 with Iceberg tables **Scaling:** Unlimited with tiered storage ## Data Flow Input \u2192 Validation \u2192 Transform \u2192 Enrich \u2192 Output ## Performance Characteristics - **Latency P50:** 100ms (streaming), 5min (batch) - **Latency P99:** 500ms (streaming), 30min (batch) - **Throughput:** 100000 messages/second (streaming)",
              "score": null
            },
            {
              "doc_id": "howto_create_api_integration",
              "chunk_id": "howto_create_api_integration_fix_0",
              "content": "# How to Create API Integration ## Prerequisites - CloudFlow account ## Steps ### Step 1: Generate API key Create an API key for programmatic access. ```bash cloudflow apikeys create --name 'CI Integration' --scopes workflows:read,workflows:execute ``` ### Step 2: Store key securely Never commit API keys to source control. Use environment variables or secret management. > **Note:** Rotate keys every 90 days ### Step 3: Test authentication Verify the API key works correctly. ```bash curl -H 'Authorization: Bearer cf_...' https://api.cloudflow.io/v1/me ``` ### Step 4: Implement rate limiting Handle rate limit responses (HTTP 429) with exponential backoff. > **Note:** Default limit is 100 requests per minute ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            },
            {
              "doc_id": "adr_001",
              "chunk_id": "adr_001_fix_0",
              "content": "# ADR-001: Use PostgreSQL for Primary Database **Status:** Accepted **Date:** 2024-11-04 ## Context We need a database to store workflow definitions, execution state, and user data. The database must support ACID transactions, JSON queries, and scale to millions of records. ## Options Considered ### Option 1: PostgreSQL Mature relational database with JSONB support. **Pros:** - ACID compliance - Rich JSONB queries - Team expertise - Proven at scale **Cons:** - Manual sharding for extreme scale - Requires connection pooling ### Option 2: MongoDB Document database with native JSON. **Pros:** - Flexible schema - Built-in sharding - Good for documents **Cons:** - Weaker consistency guarantees - Complex transactions - Higher operational cost ### Option 3: CockroachDB Distributed SQL database. **Pros:** - Automatic sharding - Strong consistency - PostgreSQL compatible **Cons:** - Higher latency - More expensive - Less mature ecosystem ## Decision We will use PostgreSQL as our primary database with JSONB for flexible workflow definitions. ## Consequences - Need PgBouncer for connection pooling - Must plan for sharding at 100M+ records - Requires regular VACUUM maintenance",
              "score": null
            },
            {
              "doc_id": "howto_configure_custom_domain",
              "chunk_id": "howto_configure_custom_domain_fix_0",
              "content": "# How to Configure Custom Domain ## Prerequisites - Domain with DNS access - CloudFlow Pro plan or higher ## Steps ### Step 1: Verify domain ownership Add a TXT record to verify domain ownership. ```bash cloudflow domains verify --domain app.example.com ``` ### Step 2: Configure DNS Add a CNAME record pointing to your CloudFlow endpoint. > **Note:** CNAME should point to ingress.cloudflow.io ### Step 3: Enable SSL CloudFlow automatically provisions SSL certificates via Let's Encrypt. ```bash cloudflow domains ssl --domain app.example.com ``` ### Step 4: Set as primary Make this the primary domain for your application. ```bash cloudflow domains set-primary --domain app.example.com ``` ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            }
          ],
          "latency_ms": 3.414408998651197,
          "query_id": "simple_002",
          "dimension": "casual",
          "query": "cloudflow db?"
        },
        {
          "key_facts": [
            "PostgreSQL",
            "JSONB"
          ],
          "found_facts": [
            "PostgreSQL"
          ],
          "missed_facts": [
            "JSONB"
          ],
          "coverage": 0.5,
          "expected_docs": [
            "adr_001",
            "config_database"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "howto_set_up_database_connection",
              "chunk_id": "howto_set_up_database_connection_fix_0",
              "content": "# How to Set Up Database Connection ## Prerequisites - Database with network access to CloudFlow - Connection credentials ## Steps ### Step 1: Add connection secret Store your database connection string as a secret. ```bash cloudflow secrets set DATABASE_URL postgresql://user:pass@host:5432/db ``` ### Step 2: Configure connection pool Set pool size based on your plan limits. Free tier allows 5 connections, Pro allows 20. > **Note:** Pool exhaustion will queue requests ### Step 3: Test connection Verify the database is accessible from CloudFlow. ```bash cloudflow db test --secret DATABASE_URL ``` ### Step 4: Enable SSL For production, always require SSL connections. ```bash cloudflow db configure --require-ssl ``` ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            },
            {
              "doc_id": "api_secrets",
              "chunk_id": "api_secrets_fix_0",
              "content": "# Secrets API Reference ## Overview The Secrets API provides programmatic access to secrets functionality. Base URL: `https://api.cloudflow.io/v1/secrets` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /secrets List secret names (values not returned). **Parameters:** **Response:** - Success: `200` - Returns array of secret metadata - Error: `400` - Bad Request **Example:** ```json {\"secrets\": [{\"name\": \"DATABASE_URL\", \"created_at\": \"2024-01-01\", \"updated_at\": \"2024-01-10\"}]} ``` ### PUT /secrets/{name} Create or update a secret. **Parameters:** - `value` (string): Secret value (encrypted at rest) **Response:** - Success: `200` - Secret saved - Error: `400` - Bad Request **Example:** ```json {\"name\": \"DATABASE_URL\", \"updated_at\": \"2024-01-15T10:00:00Z\"} ``` ### DELETE /secrets/{name} Delete a secret. **Parameters:** **Response:** - Success: `204` - Secret deleted - Error: `400` - Bad Request **Example:** ```json {} ```",
              "score": null
            },
            {
              "doc_id": "arch_data_pipeline",
              "chunk_id": "arch_data_pipeline_fix_0",
              "content": "# Data Pipeline Architecture ## Overview The Data Pipeline processes and transforms data between workflow steps. It supports batch processing up to 10GB and streaming for real-time use cases. ## Design Principles - **Exactly-Once Processing**: Each record is processed exactly once - **Backpressure Handling**: System gracefully handles varying load ## Components ### Stream Processor Handles real-time data streams. Maximum throughput of 100000 messages per second. **Technology:** Apache Kafka with Flink **Scaling:** Partition-based scaling ### Batch Processor Processes large batch jobs. Supports data formats: JSON, CSV, Parquet, Avro. **Technology:** Apache Spark on Kubernetes **Scaling:** Dynamic resource allocation ### Data Lake Long-term data storage with time-travel queries. Retention configurable up to 7 years. **Technology:** S3 with Iceberg tables **Scaling:** Unlimited with tiered storage ## Data Flow Input \u2192 Validation \u2192 Transform \u2192 Enrich \u2192 Output ## Performance Characteristics - **Latency P50:** 100ms (streaming), 5min (batch) - **Latency P99:** 500ms (streaming), 30min (batch) - **Throughput:** 100000 messages/second (streaming)",
              "score": null
            },
            {
              "doc_id": "howto_create_api_integration",
              "chunk_id": "howto_create_api_integration_fix_0",
              "content": "# How to Create API Integration ## Prerequisites - CloudFlow account ## Steps ### Step 1: Generate API key Create an API key for programmatic access. ```bash cloudflow apikeys create --name 'CI Integration' --scopes workflows:read,workflows:execute ``` ### Step 2: Store key securely Never commit API keys to source control. Use environment variables or secret management. > **Note:** Rotate keys every 90 days ### Step 3: Test authentication Verify the API key works correctly. ```bash curl -H 'Authorization: Bearer cf_...' https://api.cloudflow.io/v1/me ``` ### Step 4: Implement rate limiting Handle rate limit responses (HTTP 429) with exponential backoff. > **Note:** Default limit is 100 requests per minute ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            },
            {
              "doc_id": "api_metrics",
              "chunk_id": "api_metrics_fix_0",
              "content": "# Metrics API Reference ## Overview The Metrics API provides programmatic access to metrics functionality. Base URL: `https://api.cloudflow.io/v1/metrics` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /metrics/workflows Get workflow performance metrics. **Parameters:** - `period` (string): Time period: hour, day, week, month - `workflow_id` (string): Filter by workflow ID **Response:** - Success: `200` - Returns metrics data - Error: `400` - Bad Request **Example:** ```json {\"success_rate\": 0.985, \"avg_duration_ms\": 1250, \"p99_duration_ms\": 5000, \"total_executions\": 10000} ```",
              "score": null
            }
          ],
          "latency_ms": 4.2905999998765765,
          "query_id": "simple_002",
          "dimension": "contextual",
          "query": "need to write raw queries against CloudFlow's backend, what DB engine is it"
        },
        {
          "key_facts": [
            "PostgreSQL",
            "JSONB"
          ],
          "found_facts": [
            "PostgreSQL"
          ],
          "missed_facts": [
            "JSONB"
          ],
          "coverage": 0.5,
          "expected_docs": [
            "adr_001",
            "config_database"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "howto_set_up_database_connection",
              "chunk_id": "howto_set_up_database_connection_fix_0",
              "content": "# How to Set Up Database Connection ## Prerequisites - Database with network access to CloudFlow - Connection credentials ## Steps ### Step 1: Add connection secret Store your database connection string as a secret. ```bash cloudflow secrets set DATABASE_URL postgresql://user:pass@host:5432/db ``` ### Step 2: Configure connection pool Set pool size based on your plan limits. Free tier allows 5 connections, Pro allows 20. > **Note:** Pool exhaustion will queue requests ### Step 3: Test connection Verify the database is accessible from CloudFlow. ```bash cloudflow db test --secret DATABASE_URL ``` ### Step 4: Enable SSL For production, always require SSL connections. ```bash cloudflow db configure --require-ssl ``` ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            },
            {
              "doc_id": "api_secrets",
              "chunk_id": "api_secrets_fix_0",
              "content": "# Secrets API Reference ## Overview The Secrets API provides programmatic access to secrets functionality. Base URL: `https://api.cloudflow.io/v1/secrets` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /secrets List secret names (values not returned). **Parameters:** **Response:** - Success: `200` - Returns array of secret metadata - Error: `400` - Bad Request **Example:** ```json {\"secrets\": [{\"name\": \"DATABASE_URL\", \"created_at\": \"2024-01-01\", \"updated_at\": \"2024-01-10\"}]} ``` ### PUT /secrets/{name} Create or update a secret. **Parameters:** - `value` (string): Secret value (encrypted at rest) **Response:** - Success: `200` - Secret saved - Error: `400` - Bad Request **Example:** ```json {\"name\": \"DATABASE_URL\", \"updated_at\": \"2024-01-15T10:00:00Z\"} ``` ### DELETE /secrets/{name} Delete a secret. **Parameters:** **Response:** - Success: `204` - Secret deleted - Error: `400` - Bad Request **Example:** ```json {} ```",
              "score": null
            },
            {
              "doc_id": "howto_configure_custom_domain",
              "chunk_id": "howto_configure_custom_domain_fix_0",
              "content": "# How to Configure Custom Domain ## Prerequisites - Domain with DNS access - CloudFlow Pro plan or higher ## Steps ### Step 1: Verify domain ownership Add a TXT record to verify domain ownership. ```bash cloudflow domains verify --domain app.example.com ``` ### Step 2: Configure DNS Add a CNAME record pointing to your CloudFlow endpoint. > **Note:** CNAME should point to ingress.cloudflow.io ### Step 3: Enable SSL CloudFlow automatically provisions SSL certificates via Let's Encrypt. ```bash cloudflow domains ssl --domain app.example.com ``` ### Step 4: Set as primary Make this the primary domain for your application. ```bash cloudflow domains set-primary --domain app.example.com ``` ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            },
            {
              "doc_id": "release_3_0_0",
              "chunk_id": "release_3_0_0_fix_0",
              "content": "# Release Notes - v3.0.0 **Release Date:** 2023-10-15 ## New Features - Complete UI redesign - New workflow editor with visual builder - Real-time execution monitoring - Team workspaces ## Bug Fixes - Fixed critical security issue in token validation - Resolved data loss bug in failed migrations ## Breaking Changes - **BREAKING:** Authentication API completely rewritten - **BREAKING:** Legacy webhook format no longer supported - **BREAKING:** Minimum Node.js version now 18 ## Upgrade Instructions ```bash cloudflow upgrade --version 3.0.0 ``` ## Known Issues None at this time.",
              "score": null
            },
            {
              "doc_id": "release_3_1_0",
              "chunk_id": "release_3_1_0_fix_0",
              "content": "# Release Notes - v3.1.0 **Release Date:** 2023-12-01 ## New Features - GraphQL API beta release - Workflow templates marketplace - Enhanced audit logging - SSO support for Okta ## Bug Fixes - Fixed webhook delivery retry logic - Resolved issue with large payload handling - Fixed dashboard loading performance ## Upgrade Instructions ```bash cloudflow upgrade --version 3.1.0 ``` ## Known Issues None at this time.",
              "score": null
            }
          ],
          "latency_ms": 3.6758260011993116,
          "query_id": "simple_002",
          "dimension": "negation",
          "query": "is CloudFlow not using a NoSQL database"
        },
        {
          "key_facts": [
            "1 hour",
            "7 days",
            "refresh tokens"
          ],
          "found_facts": [
            "1 hour",
            "7 days",
            "refresh tokens"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "adr_003",
            "arch_authentication_service"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "arch_authentication_service",
              "chunk_id": "arch_authentication_service_fix_0",
              "content": "# Authentication Service Architecture ## Overview The Authentication Service handles user identity, session management, and token issuance. Supports OAuth 2.0, SAML, and API key authentication. ## Design Principles - **Zero Knowledge**: Passwords never stored in plaintext - **Token Rotation**: Refresh tokens rotate on each use ## Components ### Identity Provider Manages user accounts and credentials. Password hashing uses bcrypt with cost factor 12. **Technology:** Custom Go service **Scaling:** Horizontal stateless ### Token Service Issues and validates access tokens. Token lifetime is 1 hour with 7-day refresh tokens. **Technology:** JWT with RS256 **Scaling:** Horizontal stateless ### SSO Gateway Integrates with external identity providers. Supports Google, Microsoft, Okta, and custom SAML. **Technology:** Dex OIDC proxy **Scaling:** Active-passive ## Data Flow Credentials \u2192 Validation \u2192 Token Issue \u2192 Token Refresh \u2192 Logout ## Performance Characteristics - **Latency P50:** 50ms - **Latency P99:** 200ms - **Throughput:** 5000 auth/second",
              "score": null
            },
            {
              "doc_id": "adr_003",
              "chunk_id": "adr_003_fix_0",
              "content": "# ADR-003: Implement OAuth 2.0 with JWT for Authentication **Status:** Accepted **Date:** 2024-05-08 ## Context We need a secure authentication system that supports API keys, user sessions, and third-party integrations. ## Options Considered ### Option 1: OAuth 2.0 with JWT Industry standard token-based auth. **Pros:** - Industry standard - Stateless validation - Supports refresh tokens - SSO compatible **Cons:** - Token size overhead - Cannot revoke individual tokens - Complexity ### Option 2: Session-based auth Traditional server-side sessions. **Pros:** - Simple implementation - Easy revocation - Smaller payloads **Cons:** - Requires session store - Not suitable for APIs - Scaling challenges ### Option 3: API keys only Simple key-based authentication. **Pros:** - Very simple - Low overhead - Easy to implement **Cons:** - No user context - Hard to rotate - No SSO support ## Decision We will implement OAuth 2.0 with JWT tokens. Access tokens expire after 1 hour, refresh tokens after 7 days. ## Consequences - Implement token rotation on refresh - Support API keys for machine-to-machine auth - Add MFA for admin accounts",
              "score": null
            },
            {
              "doc_id": "arch_api_gateway",
              "chunk_id": "arch_api_gateway_fix_0",
              "content": "# API Gateway Architecture ## Overview The API Gateway handles all incoming HTTP requests, providing authentication, rate limiting, and request routing. Built on Kong with custom plugins for CloudFlow-specific functionality. ## Design Principles - **Zero Trust**: Every request is authenticated and authorized - **Defense in Depth**: Multiple layers of security validation ## Components ### Kong Gateway Primary ingress controller. Handles TLS termination, request routing, and plugin execution. **Technology:** Kong 3.4 on Kubernetes **Scaling:** Horizontal with HPA ### Auth Plugin Validates JWT tokens, API keys, and OAuth2 flows. Token validation latency under 5ms. **Technology:** Custom Lua plugin **Scaling:** Stateless ### Rate Limiter Enforces per-user and per-IP rate limits. Default limit is 100 requests per minute. **Technology:** Redis-backed sliding window **Scaling:** Shared Redis cluster ## Data Flow Client \u2192 TLS \u2192 Kong \u2192 Auth \u2192 Rate Limit \u2192 Backend Services ## Performance Characteristics - **Latency P50:** 12ms - **Latency P99:** 45ms - **Throughput:** 50000 requests/second",
              "score": null
            },
            {
              "doc_id": "adr_007",
              "chunk_id": "adr_007_fix_0",
              "content": "# ADR-007: Implement Rate Limiting at API Gateway **Status:** Accepted **Date:** 2024-09-03 ## Context We need to protect our services from abuse and ensure fair resource usage across customers. ## Options Considered ### Option 1: Sliding window at gateway Rate limit at API gateway level. **Pros:** - Centralized enforcement - Low latency - Easy configuration **Cons:** - Gateway becomes bottleneck - Coarse granularity - Redis dependency ### Option 2: Per-service limiting Each service implements its own limits. **Pros:** - Fine-grained control - Service autonomy - No single point of failure **Cons:** - Inconsistent implementation - Higher complexity - Distributed state ### Option 3: Token bucket Token-based rate limiting. **Pros:** - Allows bursts - Smooth rate limiting - Well understood **Cons:** - More complex - Harder to explain to users - State management ## Decision We will implement sliding window rate limiting at the Kong API Gateway using Redis. ## Consequences - Default limit: 100 requests per minute - Implement per-plan limits - Return rate limit headers - Add retry-after on 429",
              "score": null
            },
            {
              "doc_id": "arch_monitoring_stack",
              "chunk_id": "arch_monitoring_stack_fix_0",
              "content": "# Monitoring Stack Architecture ## Overview The Monitoring Stack provides observability into system health, performance, and errors. Includes metrics, logs, traces, and alerting. ## Design Principles - **Observability**: Every component emits metrics and traces - **Actionable Alerts**: Alerts include runbook links ## Components ### Metrics Collects time-series metrics. Retention is 15 days local, 1 year in object storage. **Technology:** Prometheus with Thanos **Scaling:** Federated with long-term storage ### Logging Centralized log aggregation. Supports structured JSON logs with label indexing. **Technology:** Loki with Grafana **Scaling:** Distributed with sharding ### Tracing Distributed request tracing. Trace retention is 7 days. **Technology:** Jaeger with OpenTelemetry **Scaling:** Sampling at 10% ### Alerting Routes alerts to on-call engineers. Supports escalation policies and silencing. **Technology:** Alertmanager with PagerDuty **Scaling:** HA pair ## Data Flow Application \u2192 Collector \u2192 Storage \u2192 Query \u2192 Dashboard/Alert ## Performance Characteristics - **Latency P50:** N/A - **Latency P99:** N/A - **Throughput:** 1M metrics/second, 100GB logs/day",
              "score": null
            }
          ],
          "latency_ms": 3.6257219999242807,
          "query_id": "simple_003",
          "dimension": "original",
          "query": "How long do access tokens last?"
        },
        {
          "key_facts": [
            "1 hour",
            "7 days",
            "refresh tokens"
          ],
          "found_facts": [
            "1 hour",
            "7 days",
            "refresh tokens"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "adr_003",
            "arch_authentication_service"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "arch_authentication_service",
              "chunk_id": "arch_authentication_service_fix_0",
              "content": "# Authentication Service Architecture ## Overview The Authentication Service handles user identity, session management, and token issuance. Supports OAuth 2.0, SAML, and API key authentication. ## Design Principles - **Zero Knowledge**: Passwords never stored in plaintext - **Token Rotation**: Refresh tokens rotate on each use ## Components ### Identity Provider Manages user accounts and credentials. Password hashing uses bcrypt with cost factor 12. **Technology:** Custom Go service **Scaling:** Horizontal stateless ### Token Service Issues and validates access tokens. Token lifetime is 1 hour with 7-day refresh tokens. **Technology:** JWT with RS256 **Scaling:** Horizontal stateless ### SSO Gateway Integrates with external identity providers. Supports Google, Microsoft, Okta, and custom SAML. **Technology:** Dex OIDC proxy **Scaling:** Active-passive ## Data Flow Credentials \u2192 Validation \u2192 Token Issue \u2192 Token Refresh \u2192 Logout ## Performance Characteristics - **Latency P50:** 50ms - **Latency P99:** 200ms - **Throughput:** 5000 auth/second",
              "score": null
            },
            {
              "doc_id": "adr_003",
              "chunk_id": "adr_003_fix_0",
              "content": "# ADR-003: Implement OAuth 2.0 with JWT for Authentication **Status:** Accepted **Date:** 2024-05-08 ## Context We need a secure authentication system that supports API keys, user sessions, and third-party integrations. ## Options Considered ### Option 1: OAuth 2.0 with JWT Industry standard token-based auth. **Pros:** - Industry standard - Stateless validation - Supports refresh tokens - SSO compatible **Cons:** - Token size overhead - Cannot revoke individual tokens - Complexity ### Option 2: Session-based auth Traditional server-side sessions. **Pros:** - Simple implementation - Easy revocation - Smaller payloads **Cons:** - Requires session store - Not suitable for APIs - Scaling challenges ### Option 3: API keys only Simple key-based authentication. **Pros:** - Very simple - Low overhead - Easy to implement **Cons:** - No user context - Hard to rotate - No SSO support ## Decision We will implement OAuth 2.0 with JWT tokens. Access tokens expire after 1 hour, refresh tokens after 7 days. ## Consequences - Implement token rotation on refresh - Support API keys for machine-to-machine auth - Add MFA for admin accounts",
              "score": null
            },
            {
              "doc_id": "arch_api_gateway",
              "chunk_id": "arch_api_gateway_fix_0",
              "content": "# API Gateway Architecture ## Overview The API Gateway handles all incoming HTTP requests, providing authentication, rate limiting, and request routing. Built on Kong with custom plugins for CloudFlow-specific functionality. ## Design Principles - **Zero Trust**: Every request is authenticated and authorized - **Defense in Depth**: Multiple layers of security validation ## Components ### Kong Gateway Primary ingress controller. Handles TLS termination, request routing, and plugin execution. **Technology:** Kong 3.4 on Kubernetes **Scaling:** Horizontal with HPA ### Auth Plugin Validates JWT tokens, API keys, and OAuth2 flows. Token validation latency under 5ms. **Technology:** Custom Lua plugin **Scaling:** Stateless ### Rate Limiter Enforces per-user and per-IP rate limits. Default limit is 100 requests per minute. **Technology:** Redis-backed sliding window **Scaling:** Shared Redis cluster ## Data Flow Client \u2192 TLS \u2192 Kong \u2192 Auth \u2192 Rate Limit \u2192 Backend Services ## Performance Characteristics - **Latency P50:** 12ms - **Latency P99:** 45ms - **Throughput:** 50000 requests/second",
              "score": null
            },
            {
              "doc_id": "adr_007",
              "chunk_id": "adr_007_fix_0",
              "content": "# ADR-007: Implement Rate Limiting at API Gateway **Status:** Accepted **Date:** 2024-09-03 ## Context We need to protect our services from abuse and ensure fair resource usage across customers. ## Options Considered ### Option 1: Sliding window at gateway Rate limit at API gateway level. **Pros:** - Centralized enforcement - Low latency - Easy configuration **Cons:** - Gateway becomes bottleneck - Coarse granularity - Redis dependency ### Option 2: Per-service limiting Each service implements its own limits. **Pros:** - Fine-grained control - Service autonomy - No single point of failure **Cons:** - Inconsistent implementation - Higher complexity - Distributed state ### Option 3: Token bucket Token-based rate limiting. **Pros:** - Allows bursts - Smooth rate limiting - Well understood **Cons:** - More complex - Harder to explain to users - State management ## Decision We will implement sliding window rate limiting at the Kong API Gateway using Redis. ## Consequences - Default limit: 100 requests per minute - Implement per-plan limits - Return rate limit headers - Add retry-after on 429",
              "score": null
            },
            {
              "doc_id": "runbook_ssl_certificate_expiry",
              "chunk_id": "runbook_ssl_certificate_expiry_fix_0",
              "content": "# Runbook: SSL Certificate Expiry ## Severity **Level:** P3 - Medium **Response Time:** 15 minutes ## Symptoms - Certificate expiring within 7 days - SSL handshake failures - Browser security warnings ## Diagnosis ### Step 1 Check certificate expiry: `echo | openssl s_client -connect api.cloudflow.io:443 2>/dev/null | openssl x509 -noout -dates` ### Step 2 Verify cert-manager status: `kubectl get certificates -n production` ### Step 3 Check cert-manager logs: `kubectl logs -l app=cert-manager -n cert-manager --tail=50` ### Step 4 Verify DNS is correct: `dig +short api.cloudflow.io` ## Resolution ### Step 1 Trigger manual renewal: `kubectl delete certificate api-cert -n production && kubectl apply -f certificate.yaml` ### Step 2 Check rate limits: Let's Encrypt has 50 certs/domain/week limit ### Step 3 Use staging issuer for testing: switch to letsencrypt-staging ClusterIssuer ### Step 4 If Let's Encrypt fails, manually upload certificate ## Escalation If certificate cannot be renewed, escalate to security team immediately. Consider using backup certificate. ## Post-Incident - Document timeline in incident log - Schedule post-mortem within 48 hours - Update runbook if new patterns discovered",
              "score": null
            }
          ],
          "latency_ms": 3.327116000946262,
          "query_id": "simple_003",
          "dimension": "synonym",
          "query": "what's the JWT token expiration time"
        },
        {
          "key_facts": [
            "1 hour",
            "7 days",
            "refresh tokens"
          ],
          "found_facts": [
            "1 hour",
            "7 days",
            "refresh tokens"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "adr_003",
            "arch_authentication_service"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "arch_authentication_service",
              "chunk_id": "arch_authentication_service_fix_0",
              "content": "# Authentication Service Architecture ## Overview The Authentication Service handles user identity, session management, and token issuance. Supports OAuth 2.0, SAML, and API key authentication. ## Design Principles - **Zero Knowledge**: Passwords never stored in plaintext - **Token Rotation**: Refresh tokens rotate on each use ## Components ### Identity Provider Manages user accounts and credentials. Password hashing uses bcrypt with cost factor 12. **Technology:** Custom Go service **Scaling:** Horizontal stateless ### Token Service Issues and validates access tokens. Token lifetime is 1 hour with 7-day refresh tokens. **Technology:** JWT with RS256 **Scaling:** Horizontal stateless ### SSO Gateway Integrates with external identity providers. Supports Google, Microsoft, Okta, and custom SAML. **Technology:** Dex OIDC proxy **Scaling:** Active-passive ## Data Flow Credentials \u2192 Validation \u2192 Token Issue \u2192 Token Refresh \u2192 Logout ## Performance Characteristics - **Latency P50:** 50ms - **Latency P99:** 200ms - **Throughput:** 5000 auth/second",
              "score": null
            },
            {
              "doc_id": "runbook_database_connection_exhaustion",
              "chunk_id": "runbook_database_connection_exhaustion_fix_0",
              "content": "# Runbook: Database Connection Exhaustion ## Severity **Level:** P1 - Critical **Response Time:** 15 minutes ## Symptoms - Connection pool at 100% - Queries timing out - 'too many connections' errors in logs ## Diagnosis ### Step 1 Check active connections: `SELECT count(*) FROM pg_stat_activity` ### Step 2 Identify long-running queries: `SELECT pid, now() - pg_stat_activity.query_start AS duration, query FROM pg_stat_activity WHERE state != 'idle' ORDER BY duration DESC` ### Step 3 Check PgBouncer stats: `SHOW POOLS` ### Step 4 Verify connection limits: `SHOW max_connections` ## Resolution ### Step 1 Kill long-running queries: `SELECT pg_terminate_backend(pid) FROM pg_stat_activity WHERE duration > interval '5 minutes'` ### Step 2 Increase PgBouncer pool size temporarily: update pgbouncer.ini ### Step 3 Restart affected services to release connections: `kubectl rollout restart deployment/api` ### Step 4 Scale down non-critical services to free connections ## Escalation If connections remain exhausted, escalate to DBA. Consider enabling connection queueing. ## Post-Incident - Document timeline in incident log - Schedule post-mortem within 48 hours - Update runbook if new patterns discovered",
              "score": null
            },
            {
              "doc_id": "adr_003",
              "chunk_id": "adr_003_fix_0",
              "content": "# ADR-003: Implement OAuth 2.0 with JWT for Authentication **Status:** Accepted **Date:** 2024-05-08 ## Context We need a secure authentication system that supports API keys, user sessions, and third-party integrations. ## Options Considered ### Option 1: OAuth 2.0 with JWT Industry standard token-based auth. **Pros:** - Industry standard - Stateless validation - Supports refresh tokens - SSO compatible **Cons:** - Token size overhead - Cannot revoke individual tokens - Complexity ### Option 2: Session-based auth Traditional server-side sessions. **Pros:** - Simple implementation - Easy revocation - Smaller payloads **Cons:** - Requires session store - Not suitable for APIs - Scaling challenges ### Option 3: API keys only Simple key-based authentication. **Pros:** - Very simple - Low overhead - Easy to implement **Cons:** - No user context - Hard to rotate - No SSO support ## Decision We will implement OAuth 2.0 with JWT tokens. Access tokens expire after 1 hour, refresh tokens after 7 days. ## Consequences - Implement token rotation on refresh - Support API keys for machine-to-machine auth - Add MFA for admin accounts",
              "score": null
            },
            {
              "doc_id": "arch_monitoring_stack",
              "chunk_id": "arch_monitoring_stack_fix_0",
              "content": "# Monitoring Stack Architecture ## Overview The Monitoring Stack provides observability into system health, performance, and errors. Includes metrics, logs, traces, and alerting. ## Design Principles - **Observability**: Every component emits metrics and traces - **Actionable Alerts**: Alerts include runbook links ## Components ### Metrics Collects time-series metrics. Retention is 15 days local, 1 year in object storage. **Technology:** Prometheus with Thanos **Scaling:** Federated with long-term storage ### Logging Centralized log aggregation. Supports structured JSON logs with label indexing. **Technology:** Loki with Grafana **Scaling:** Distributed with sharding ### Tracing Distributed request tracing. Trace retention is 7 days. **Technology:** Jaeger with OpenTelemetry **Scaling:** Sampling at 10% ### Alerting Routes alerts to on-call engineers. Supports escalation policies and silencing. **Technology:** Alertmanager with PagerDuty **Scaling:** HA pair ## Data Flow Application \u2192 Collector \u2192 Storage \u2192 Query \u2192 Dashboard/Alert ## Performance Characteristics - **Latency P50:** N/A - **Latency P99:** N/A - **Throughput:** 1M metrics/second, 100GB logs/day",
              "score": null
            },
            {
              "doc_id": "api_users",
              "chunk_id": "api_users_fix_0",
              "content": "# Users API Reference ## Overview The Users API provides programmatic access to users functionality. Base URL: `https://api.cloudflow.io/v1/users` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /users/me Get current user profile. **Parameters:** **Response:** - Success: `200` - Returns user object - Error: `400` - Bad Request **Example:** ```json {\"id\": \"usr_001\", \"email\": \"user@example.com\", \"role\": \"admin\", \"created_at\": \"2023-06-01\"} ``` ### PATCH /users/me Update current user profile. **Parameters:** - `name` (string): Display name - `timezone` (string): Timezone (e.g., America/New_York) **Response:** - Success: `200` - User updated - Error: `400` - Bad Request **Example:** ```json {\"id\": \"usr_001\", \"name\": \"John Doe\", \"timezone\": \"America/New_York\"} ```",
              "score": null
            }
          ],
          "latency_ms": 3.5701880005944986,
          "query_id": "simple_003",
          "dimension": "problem",
          "query": "users are getting logged out after about an hour, is that expected"
        },
        {
          "key_facts": [
            "1 hour",
            "7 days",
            "refresh tokens"
          ],
          "found_facts": [
            "1 hour",
            "7 days",
            "refresh tokens"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "adr_003",
            "arch_authentication_service"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "arch_authentication_service",
              "chunk_id": "arch_authentication_service_fix_0",
              "content": "# Authentication Service Architecture ## Overview The Authentication Service handles user identity, session management, and token issuance. Supports OAuth 2.0, SAML, and API key authentication. ## Design Principles - **Zero Knowledge**: Passwords never stored in plaintext - **Token Rotation**: Refresh tokens rotate on each use ## Components ### Identity Provider Manages user accounts and credentials. Password hashing uses bcrypt with cost factor 12. **Technology:** Custom Go service **Scaling:** Horizontal stateless ### Token Service Issues and validates access tokens. Token lifetime is 1 hour with 7-day refresh tokens. **Technology:** JWT with RS256 **Scaling:** Horizontal stateless ### SSO Gateway Integrates with external identity providers. Supports Google, Microsoft, Okta, and custom SAML. **Technology:** Dex OIDC proxy **Scaling:** Active-passive ## Data Flow Credentials \u2192 Validation \u2192 Token Issue \u2192 Token Refresh \u2192 Logout ## Performance Characteristics - **Latency P50:** 50ms - **Latency P99:** 200ms - **Throughput:** 5000 auth/second",
              "score": null
            },
            {
              "doc_id": "adr_003",
              "chunk_id": "adr_003_fix_0",
              "content": "# ADR-003: Implement OAuth 2.0 with JWT for Authentication **Status:** Accepted **Date:** 2024-05-08 ## Context We need a secure authentication system that supports API keys, user sessions, and third-party integrations. ## Options Considered ### Option 1: OAuth 2.0 with JWT Industry standard token-based auth. **Pros:** - Industry standard - Stateless validation - Supports refresh tokens - SSO compatible **Cons:** - Token size overhead - Cannot revoke individual tokens - Complexity ### Option 2: Session-based auth Traditional server-side sessions. **Pros:** - Simple implementation - Easy revocation - Smaller payloads **Cons:** - Requires session store - Not suitable for APIs - Scaling challenges ### Option 3: API keys only Simple key-based authentication. **Pros:** - Very simple - Low overhead - Easy to implement **Cons:** - No user context - Hard to rotate - No SSO support ## Decision We will implement OAuth 2.0 with JWT tokens. Access tokens expire after 1 hour, refresh tokens after 7 days. ## Consequences - Implement token rotation on refresh - Support API keys for machine-to-machine auth - Add MFA for admin accounts",
              "score": null
            },
            {
              "doc_id": "arch_api_gateway",
              "chunk_id": "arch_api_gateway_fix_0",
              "content": "# API Gateway Architecture ## Overview The API Gateway handles all incoming HTTP requests, providing authentication, rate limiting, and request routing. Built on Kong with custom plugins for CloudFlow-specific functionality. ## Design Principles - **Zero Trust**: Every request is authenticated and authorized - **Defense in Depth**: Multiple layers of security validation ## Components ### Kong Gateway Primary ingress controller. Handles TLS termination, request routing, and plugin execution. **Technology:** Kong 3.4 on Kubernetes **Scaling:** Horizontal with HPA ### Auth Plugin Validates JWT tokens, API keys, and OAuth2 flows. Token validation latency under 5ms. **Technology:** Custom Lua plugin **Scaling:** Stateless ### Rate Limiter Enforces per-user and per-IP rate limits. Default limit is 100 requests per minute. **Technology:** Redis-backed sliding window **Scaling:** Shared Redis cluster ## Data Flow Client \u2192 TLS \u2192 Kong \u2192 Auth \u2192 Rate Limit \u2192 Backend Services ## Performance Characteristics - **Latency P50:** 12ms - **Latency P99:** 45ms - **Throughput:** 50000 requests/second",
              "score": null
            },
            {
              "doc_id": "adr_007",
              "chunk_id": "adr_007_fix_0",
              "content": "# ADR-007: Implement Rate Limiting at API Gateway **Status:** Accepted **Date:** 2024-09-03 ## Context We need to protect our services from abuse and ensure fair resource usage across customers. ## Options Considered ### Option 1: Sliding window at gateway Rate limit at API gateway level. **Pros:** - Centralized enforcement - Low latency - Easy configuration **Cons:** - Gateway becomes bottleneck - Coarse granularity - Redis dependency ### Option 2: Per-service limiting Each service implements its own limits. **Pros:** - Fine-grained control - Service autonomy - No single point of failure **Cons:** - Inconsistent implementation - Higher complexity - Distributed state ### Option 3: Token bucket Token-based rate limiting. **Pros:** - Allows bursts - Smooth rate limiting - Well understood **Cons:** - More complex - Harder to explain to users - State management ## Decision We will implement sliding window rate limiting at the Kong API Gateway using Redis. ## Consequences - Default limit: 100 requests per minute - Implement per-plan limits - Return rate limit headers - Add retry-after on 429",
              "score": null
            },
            {
              "doc_id": "runbook_ssl_certificate_expiry",
              "chunk_id": "runbook_ssl_certificate_expiry_fix_0",
              "content": "# Runbook: SSL Certificate Expiry ## Severity **Level:** P3 - Medium **Response Time:** 15 minutes ## Symptoms - Certificate expiring within 7 days - SSL handshake failures - Browser security warnings ## Diagnosis ### Step 1 Check certificate expiry: `echo | openssl s_client -connect api.cloudflow.io:443 2>/dev/null | openssl x509 -noout -dates` ### Step 2 Verify cert-manager status: `kubectl get certificates -n production` ### Step 3 Check cert-manager logs: `kubectl logs -l app=cert-manager -n cert-manager --tail=50` ### Step 4 Verify DNS is correct: `dig +short api.cloudflow.io` ## Resolution ### Step 1 Trigger manual renewal: `kubectl delete certificate api-cert -n production && kubectl apply -f certificate.yaml` ### Step 2 Check rate limits: Let's Encrypt has 50 certs/domain/week limit ### Step 3 Use staging issuer for testing: switch to letsencrypt-staging ClusterIssuer ### Step 4 If Let's Encrypt fails, manually upload certificate ## Escalation If certificate cannot be renewed, escalate to security team immediately. Consider using backup certificate. ## Post-Incident - Document timeline in incident log - Schedule post-mortem within 48 hours - Update runbook if new patterns discovered",
              "score": null
            }
          ],
          "latency_ms": 3.652601000794675,
          "query_id": "simple_003",
          "dimension": "casual",
          "query": "token expiry"
        },
        {
          "key_facts": [
            "1 hour",
            "7 days",
            "refresh tokens"
          ],
          "found_facts": [
            "1 hour",
            "7 days",
            "refresh tokens"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "adr_003",
            "arch_authentication_service"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "arch_authentication_service",
              "chunk_id": "arch_authentication_service_fix_0",
              "content": "# Authentication Service Architecture ## Overview The Authentication Service handles user identity, session management, and token issuance. Supports OAuth 2.0, SAML, and API key authentication. ## Design Principles - **Zero Knowledge**: Passwords never stored in plaintext - **Token Rotation**: Refresh tokens rotate on each use ## Components ### Identity Provider Manages user accounts and credentials. Password hashing uses bcrypt with cost factor 12. **Technology:** Custom Go service **Scaling:** Horizontal stateless ### Token Service Issues and validates access tokens. Token lifetime is 1 hour with 7-day refresh tokens. **Technology:** JWT with RS256 **Scaling:** Horizontal stateless ### SSO Gateway Integrates with external identity providers. Supports Google, Microsoft, Okta, and custom SAML. **Technology:** Dex OIDC proxy **Scaling:** Active-passive ## Data Flow Credentials \u2192 Validation \u2192 Token Issue \u2192 Token Refresh \u2192 Logout ## Performance Characteristics - **Latency P50:** 50ms - **Latency P99:** 200ms - **Throughput:** 5000 auth/second",
              "score": null
            },
            {
              "doc_id": "adr_003",
              "chunk_id": "adr_003_fix_0",
              "content": "# ADR-003: Implement OAuth 2.0 with JWT for Authentication **Status:** Accepted **Date:** 2024-05-08 ## Context We need a secure authentication system that supports API keys, user sessions, and third-party integrations. ## Options Considered ### Option 1: OAuth 2.0 with JWT Industry standard token-based auth. **Pros:** - Industry standard - Stateless validation - Supports refresh tokens - SSO compatible **Cons:** - Token size overhead - Cannot revoke individual tokens - Complexity ### Option 2: Session-based auth Traditional server-side sessions. **Pros:** - Simple implementation - Easy revocation - Smaller payloads **Cons:** - Requires session store - Not suitable for APIs - Scaling challenges ### Option 3: API keys only Simple key-based authentication. **Pros:** - Very simple - Low overhead - Easy to implement **Cons:** - No user context - Hard to rotate - No SSO support ## Decision We will implement OAuth 2.0 with JWT tokens. Access tokens expire after 1 hour, refresh tokens after 7 days. ## Consequences - Implement token rotation on refresh - Support API keys for machine-to-machine auth - Add MFA for admin accounts",
              "score": null
            },
            {
              "doc_id": "adr_007",
              "chunk_id": "adr_007_fix_0",
              "content": "# ADR-007: Implement Rate Limiting at API Gateway **Status:** Accepted **Date:** 2024-09-03 ## Context We need to protect our services from abuse and ensure fair resource usage across customers. ## Options Considered ### Option 1: Sliding window at gateway Rate limit at API gateway level. **Pros:** - Centralized enforcement - Low latency - Easy configuration **Cons:** - Gateway becomes bottleneck - Coarse granularity - Redis dependency ### Option 2: Per-service limiting Each service implements its own limits. **Pros:** - Fine-grained control - Service autonomy - No single point of failure **Cons:** - Inconsistent implementation - Higher complexity - Distributed state ### Option 3: Token bucket Token-based rate limiting. **Pros:** - Allows bursts - Smooth rate limiting - Well understood **Cons:** - More complex - Harder to explain to users - State management ## Decision We will implement sliding window rate limiting at the Kong API Gateway using Redis. ## Consequences - Default limit: 100 requests per minute - Implement per-plan limits - Return rate limit headers - Add retry-after on 429",
              "score": null
            },
            {
              "doc_id": "arch_api_gateway",
              "chunk_id": "arch_api_gateway_fix_0",
              "content": "# API Gateway Architecture ## Overview The API Gateway handles all incoming HTTP requests, providing authentication, rate limiting, and request routing. Built on Kong with custom plugins for CloudFlow-specific functionality. ## Design Principles - **Zero Trust**: Every request is authenticated and authorized - **Defense in Depth**: Multiple layers of security validation ## Components ### Kong Gateway Primary ingress controller. Handles TLS termination, request routing, and plugin execution. **Technology:** Kong 3.4 on Kubernetes **Scaling:** Horizontal with HPA ### Auth Plugin Validates JWT tokens, API keys, and OAuth2 flows. Token validation latency under 5ms. **Technology:** Custom Lua plugin **Scaling:** Stateless ### Rate Limiter Enforces per-user and per-IP rate limits. Default limit is 100 requests per minute. **Technology:** Redis-backed sliding window **Scaling:** Shared Redis cluster ## Data Flow Client \u2192 TLS \u2192 Kong \u2192 Auth \u2192 Rate Limit \u2192 Backend Services ## Performance Characteristics - **Latency P50:** 12ms - **Latency P99:** 45ms - **Throughput:** 50000 requests/second",
              "score": null
            },
            {
              "doc_id": "adr_004",
              "chunk_id": "adr_004_fix_0",
              "content": "# ADR-004: Use Redis for Caching and Session Storage **Status:** Accepted **Date:** 2024-04-05 ## Context We need a fast data store for caching API responses, session data, and rate limiting counters. ## Options Considered ### Option 1: Redis Cluster In-memory data structure store. **Pros:** - Sub-millisecond latency - Rich data structures - Pub/sub support - Clustering **Cons:** - Memory cost - Persistence complexity - Cluster management ### Option 2: Memcached Simple key-value cache. **Pros:** - Simple operation - Multi-threaded - Lower memory overhead **Cons:** - No persistence - Limited data types - No clustering ### Option 3: Application-level cache In-process caching only. **Pros:** - No external dependency - Fastest possible - Simple **Cons:** - Not shared across pods - Memory pressure - Cold starts ## Decision We will use Redis Cluster for caching with a 6-node cluster for high availability. ## Consequences - Implement cache-aside pattern - Set appropriate TTLs - Monitor memory usage - Plan for cache stampede prevention",
              "score": null
            }
          ],
          "latency_ms": 4.297163000956061,
          "query_id": "simple_003",
          "dimension": "contextual",
          "query": "implementing token refresh logic, need to know when access tokens expire"
        },
        {
          "key_facts": [
            "1 hour",
            "7 days",
            "refresh tokens"
          ],
          "found_facts": [
            "1 hour",
            "7 days",
            "refresh tokens"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "adr_003",
            "arch_authentication_service"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "arch_authentication_service",
              "chunk_id": "arch_authentication_service_fix_0",
              "content": "# Authentication Service Architecture ## Overview The Authentication Service handles user identity, session management, and token issuance. Supports OAuth 2.0, SAML, and API key authentication. ## Design Principles - **Zero Knowledge**: Passwords never stored in plaintext - **Token Rotation**: Refresh tokens rotate on each use ## Components ### Identity Provider Manages user accounts and credentials. Password hashing uses bcrypt with cost factor 12. **Technology:** Custom Go service **Scaling:** Horizontal stateless ### Token Service Issues and validates access tokens. Token lifetime is 1 hour with 7-day refresh tokens. **Technology:** JWT with RS256 **Scaling:** Horizontal stateless ### SSO Gateway Integrates with external identity providers. Supports Google, Microsoft, Okta, and custom SAML. **Technology:** Dex OIDC proxy **Scaling:** Active-passive ## Data Flow Credentials \u2192 Validation \u2192 Token Issue \u2192 Token Refresh \u2192 Logout ## Performance Characteristics - **Latency P50:** 50ms - **Latency P99:** 200ms - **Throughput:** 5000 auth/second",
              "score": null
            },
            {
              "doc_id": "runbook_api_gateway_5xx_errors",
              "chunk_id": "runbook_api_gateway_5xx_errors_fix_0",
              "content": "# Runbook: API Gateway 5xx Errors ## Severity **Level:** P3 - Medium **Response Time:** 1 hour ## Symptoms - Error rate > 1% - 5xx responses in access logs - Downstream service failures ## Diagnosis ### Step 1 Check Kong error logs: `kubectl logs -l app=kong -n kong --tail=100 | grep -E '5[0-9]{2}'` ### Step 2 Verify backend health: `kubectl get pods -n production -o wide` ### Step 3 Check service endpoints: `kubectl get endpoints -n production` ### Step 4 Test backend directly: `kubectl exec -it kong-xxx -n kong -- curl http://api.production.svc:8080/health` ## Resolution ### Step 1 Restart unhealthy backends: `kubectl rollout restart deployment/api -n production` ### Step 2 Enable circuit breaker if cascading: update Kong circuit-breaker plugin ### Step 3 Scale up backends: `kubectl scale deployment/api --replicas=5 -n production` ### Step 4 Check for resource exhaustion on backend pods ### Step 5 Verify network policies allow traffic ## Escalation If 5xx persists > 15 minutes, page on-call. May indicate infrastructure issue. ## Post-Incident - Document timeline in incident log - Schedule post-mortem within 48 hours - Update runbook if new patterns discovered",
              "score": null
            },
            {
              "doc_id": "runbook_ssl_certificate_expiry",
              "chunk_id": "runbook_ssl_certificate_expiry_fix_0",
              "content": "# Runbook: SSL Certificate Expiry ## Severity **Level:** P3 - Medium **Response Time:** 15 minutes ## Symptoms - Certificate expiring within 7 days - SSL handshake failures - Browser security warnings ## Diagnosis ### Step 1 Check certificate expiry: `echo | openssl s_client -connect api.cloudflow.io:443 2>/dev/null | openssl x509 -noout -dates` ### Step 2 Verify cert-manager status: `kubectl get certificates -n production` ### Step 3 Check cert-manager logs: `kubectl logs -l app=cert-manager -n cert-manager --tail=50` ### Step 4 Verify DNS is correct: `dig +short api.cloudflow.io` ## Resolution ### Step 1 Trigger manual renewal: `kubectl delete certificate api-cert -n production && kubectl apply -f certificate.yaml` ### Step 2 Check rate limits: Let's Encrypt has 50 certs/domain/week limit ### Step 3 Use staging issuer for testing: switch to letsencrypt-staging ClusterIssuer ### Step 4 If Let's Encrypt fails, manually upload certificate ## Escalation If certificate cannot be renewed, escalate to security team immediately. Consider using backup certificate. ## Post-Incident - Document timeline in incident log - Schedule post-mortem within 48 hours - Update runbook if new patterns discovered",
              "score": null
            },
            {
              "doc_id": "runbook_database_connection_exhaustion",
              "chunk_id": "runbook_database_connection_exhaustion_fix_0",
              "content": "# Runbook: Database Connection Exhaustion ## Severity **Level:** P1 - Critical **Response Time:** 15 minutes ## Symptoms - Connection pool at 100% - Queries timing out - 'too many connections' errors in logs ## Diagnosis ### Step 1 Check active connections: `SELECT count(*) FROM pg_stat_activity` ### Step 2 Identify long-running queries: `SELECT pid, now() - pg_stat_activity.query_start AS duration, query FROM pg_stat_activity WHERE state != 'idle' ORDER BY duration DESC` ### Step 3 Check PgBouncer stats: `SHOW POOLS` ### Step 4 Verify connection limits: `SHOW max_connections` ## Resolution ### Step 1 Kill long-running queries: `SELECT pg_terminate_backend(pid) FROM pg_stat_activity WHERE duration > interval '5 minutes'` ### Step 2 Increase PgBouncer pool size temporarily: update pgbouncer.ini ### Step 3 Restart affected services to release connections: `kubectl rollout restart deployment/api` ### Step 4 Scale down non-critical services to free connections ## Escalation If connections remain exhausted, escalate to DBA. Consider enabling connection queueing. ## Post-Incident - Document timeline in incident log - Schedule post-mortem within 48 hours - Update runbook if new patterns discovered",
              "score": null
            },
            {
              "doc_id": "arch_api_gateway",
              "chunk_id": "arch_api_gateway_fix_0",
              "content": "# API Gateway Architecture ## Overview The API Gateway handles all incoming HTTP requests, providing authentication, rate limiting, and request routing. Built on Kong with custom plugins for CloudFlow-specific functionality. ## Design Principles - **Zero Trust**: Every request is authenticated and authorized - **Defense in Depth**: Multiple layers of security validation ## Components ### Kong Gateway Primary ingress controller. Handles TLS termination, request routing, and plugin execution. **Technology:** Kong 3.4 on Kubernetes **Scaling:** Horizontal with HPA ### Auth Plugin Validates JWT tokens, API keys, and OAuth2 flows. Token validation latency under 5ms. **Technology:** Custom Lua plugin **Scaling:** Stateless ### Rate Limiter Enforces per-user and per-IP rate limits. Default limit is 100 requests per minute. **Technology:** Redis-backed sliding window **Scaling:** Shared Redis cluster ## Data Flow Client \u2192 TLS \u2192 Kong \u2192 Auth \u2192 Rate Limit \u2192 Backend Services ## Performance Characteristics - **Latency P50:** 12ms - **Latency P99:** 45ms - **Throughput:** 50000 requests/second",
              "score": null
            }
          ],
          "latency_ms": 3.619490000346559,
          "query_id": "simple_003",
          "dimension": "negation",
          "query": "why does my token stop working after a while"
        },
        {
          "key_facts": [
            "3600",
            "WORKFLOW_TIMEOUT_SECONDS"
          ],
          "found_facts": [
            "3600",
            "WORKFLOW_TIMEOUT_SECONDS"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "config_workflow_engine"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "config_workflow_engine",
              "chunk_id": "config_workflow_engine_fix_0",
              "content": "# Workflow Engine Configuration Reference ## Overview This document describes all configuration options for Workflow Engine. ## Environment Variables | Variable | Type | Default | Description | |----------|------|---------|-------------| | `WORKFLOW_MAX_STEPS` | integer | 100 | Maximum steps per workflow | | `WORKFLOW_TIMEOUT_SECONDS` | integer | 3600 | Maximum workflow execution time | | `WORKFLOW_RETRY_LIMIT` | integer | 3 | Default retry attempts for failed steps | | `WORKFLOW_CONCURRENT_LIMIT` | integer | 10 | Maximum concurrent executions per workflow | | `EXECUTOR_POOL_SIZE` | integer | 50 | Number of executor workers | | `EXECUTOR_MEMORY_LIMIT` | string | 512Mi | Memory limit per executor | ## Configuration File Configuration can also be provided via `workflow engine.yaml`: ```yaml workflow_max_steps: 100 workflow_timeout_seconds: 3600 workflow_retry_limit: 3 workflow_concurrent_limit: 10 executor_pool_size: 50 ``` ## Validation Configuration is validated at startup. Invalid configuration will prevent the service from starting.",
              "score": null
            },
            {
              "doc_id": "arch_workflow_engine",
              "chunk_id": "arch_workflow_engine_fix_0",
              "content": "# Workflow Engine Architecture ## Overview The Workflow Engine is the core component responsible for executing user-defined workflows. It uses an event-driven architecture with support for parallel execution, conditional branching, and automatic retries. ## Design Principles - **Idempotency**: All operations can be safely retried without side effects - **Eventual Consistency**: State converges to consistent view within 5 seconds - **Fault Tolerance**: System continues operating despite component failures ## Components ### Scheduler Manages workflow scheduling and triggers. Supports cron expressions, webhooks, and event-based triggers. **Technology:** Custom Go service **Scaling:** Horizontal with leader election ### Executor Runs individual workflow steps in isolated containers. Maximum execution time is 30 minutes per step. **Technology:** Kubernetes Jobs **Scaling:** Auto-scaling based on queue depth ### State Store Persists workflow state, execution history, and step results. Uses optimistic locking for concurrent updates. **Technology:** PostgreSQL with JSONB **Scaling:** Primary-replica with read replicas ## Data Flow Triggers \u2192 Scheduler \u2192 Job Queue \u2192 Executor \u2192 State Store \u2192 Webhooks/Notifications ## Performance Characteristics - **Latency P50:** 45ms - **Latency P99:** 250ms - **Throughput:** 5000 executions/minute",
              "score": null
            },
            {
              "doc_id": "api_metrics",
              "chunk_id": "api_metrics_fix_0",
              "content": "# Metrics API Reference ## Overview The Metrics API provides programmatic access to metrics functionality. Base URL: `https://api.cloudflow.io/v1/metrics` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /metrics/workflows Get workflow performance metrics. **Parameters:** - `period` (string): Time period: hour, day, week, month - `workflow_id` (string): Filter by workflow ID **Response:** - Success: `200` - Returns metrics data - Error: `400` - Bad Request **Example:** ```json {\"success_rate\": 0.985, \"avg_duration_ms\": 1250, \"p99_duration_ms\": 5000, \"total_executions\": 10000} ```",
              "score": null
            },
            {
              "doc_id": "api_workflows",
              "chunk_id": "api_workflows_fix_0",
              "content": "# Workflows API Reference ## Overview The Workflows API provides programmatic access to workflows functionality. Base URL: `https://api.cloudflow.io/v1/workflows` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /workflows List all workflows for the authenticated user. **Parameters:** - `limit` (integer): Maximum number of results (default: 50, max: 200) - `status` (string): Filter by status: active, paused, archived **Response:** - Success: `200` - Returns array of workflow objects - Error: `400` - Bad Request **Example:** ```json {\"workflows\": [{\"id\": \"wf_123\", \"name\": \"Daily Report\", \"status\": \"active\"}]} ``` ### POST /workflows Create a new workflow. **Parameters:** - `name` (string): Workflow name (required) - `definition` (object): Workflow definition in JSON format **Response:** - Success: `201` - Workflow created successfully - Error: `400` - Bad Request **Example:** ```json {\"id\": \"wf_456\", \"name\": \"New Workflow\", \"created_at\": \"2024-01-15T10:00:00Z\"} ``` ### DELETE /workflows/{id} Delete a workflow permanently. **Parameters:** - `id` (string): Workflow ID (required) **Response:** - Success: `204` - Workflow deleted - Error: `404` - Workflow not found **Example:** ```json {} ```",
              "score": null
            },
            {
              "doc_id": "api_executions",
              "chunk_id": "api_executions_fix_0",
              "content": "# Executions API Reference ## Overview The Executions API provides programmatic access to executions functionality. Base URL: `https://api.cloudflow.io/v1/executions` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /executions List workflow executions. **Parameters:** - `workflow_id` (string): Filter by workflow ID - `since` (datetime): Filter executions after this timestamp **Response:** - Success: `200` - Returns array of execution objects - Error: `400` - Bad Request **Example:** ```json {\"executions\": [{\"id\": \"ex_789\", \"status\": \"completed\", \"duration_ms\": 1523}]} ``` ### POST /executions/{id}/cancel Cancel a running execution. **Parameters:** - `id` (string): Execution ID - `reason` (string): Cancellation reason (optional) **Response:** - Success: `200` - Execution cancelled - Error: `400` - Bad Request **Example:** ```json {\"id\": \"ex_789\", \"status\": \"cancelled\", \"cancelled_at\": \"2024-01-15T10:05:00Z\"} ```",
              "score": null
            }
          ],
          "latency_ms": 3.4803010003088275,
          "query_id": "simple_004",
          "dimension": "original",
          "query": "What is the maximum workflow execution time?"
        },
        {
          "key_facts": [
            "3600",
            "WORKFLOW_TIMEOUT_SECONDS"
          ],
          "found_facts": [
            "3600",
            "WORKFLOW_TIMEOUT_SECONDS"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "config_workflow_engine"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "howto_implement_retry_logic",
              "chunk_id": "howto_implement_retry_logic_fix_0",
              "content": "# How to Implement Retry Logic ## Prerequisites - Existing workflow ## Steps ### Step 1: Define retry policy Add retry configuration to your workflow step. > **Note:** Maximum 5 retry attempts allowed ### Step 2: Configure backoff Set exponential backoff with base delay of 1 second. The formula is: delay = base * 2^attempt. ### Step 3: Add timeout Set maximum execution time to prevent infinite loops. Default timeout is 30 seconds. ### Step 4: Handle final failure Configure on_failure handler to notify or escalate when all retries are exhausted. ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            },
            {
              "doc_id": "config_workflow_engine",
              "chunk_id": "config_workflow_engine_fix_0",
              "content": "# Workflow Engine Configuration Reference ## Overview This document describes all configuration options for Workflow Engine. ## Environment Variables | Variable | Type | Default | Description | |----------|------|---------|-------------| | `WORKFLOW_MAX_STEPS` | integer | 100 | Maximum steps per workflow | | `WORKFLOW_TIMEOUT_SECONDS` | integer | 3600 | Maximum workflow execution time | | `WORKFLOW_RETRY_LIMIT` | integer | 3 | Default retry attempts for failed steps | | `WORKFLOW_CONCURRENT_LIMIT` | integer | 10 | Maximum concurrent executions per workflow | | `EXECUTOR_POOL_SIZE` | integer | 50 | Number of executor workers | | `EXECUTOR_MEMORY_LIMIT` | string | 512Mi | Memory limit per executor | ## Configuration File Configuration can also be provided via `workflow engine.yaml`: ```yaml workflow_max_steps: 100 workflow_timeout_seconds: 3600 workflow_retry_limit: 3 workflow_concurrent_limit: 10 executor_pool_size: 50 ``` ## Validation Configuration is validated at startup. Invalid configuration will prevent the service from starting.",
              "score": null
            },
            {
              "doc_id": "arch_workflow_engine",
              "chunk_id": "arch_workflow_engine_fix_0",
              "content": "# Workflow Engine Architecture ## Overview The Workflow Engine is the core component responsible for executing user-defined workflows. It uses an event-driven architecture with support for parallel execution, conditional branching, and automatic retries. ## Design Principles - **Idempotency**: All operations can be safely retried without side effects - **Eventual Consistency**: State converges to consistent view within 5 seconds - **Fault Tolerance**: System continues operating despite component failures ## Components ### Scheduler Manages workflow scheduling and triggers. Supports cron expressions, webhooks, and event-based triggers. **Technology:** Custom Go service **Scaling:** Horizontal with leader election ### Executor Runs individual workflow steps in isolated containers. Maximum execution time is 30 minutes per step. **Technology:** Kubernetes Jobs **Scaling:** Auto-scaling based on queue depth ### State Store Persists workflow state, execution history, and step results. Uses optimistic locking for concurrent updates. **Technology:** PostgreSQL with JSONB **Scaling:** Primary-replica with read replicas ## Data Flow Triggers \u2192 Scheduler \u2192 Job Queue \u2192 Executor \u2192 State Store \u2192 Webhooks/Notifications ## Performance Characteristics - **Latency P50:** 45ms - **Latency P99:** 250ms - **Throughput:** 5000 executions/minute",
              "score": null
            },
            {
              "doc_id": "api_workflows",
              "chunk_id": "api_workflows_fix_0",
              "content": "# Workflows API Reference ## Overview The Workflows API provides programmatic access to workflows functionality. Base URL: `https://api.cloudflow.io/v1/workflows` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /workflows List all workflows for the authenticated user. **Parameters:** - `limit` (integer): Maximum number of results (default: 50, max: 200) - `status` (string): Filter by status: active, paused, archived **Response:** - Success: `200` - Returns array of workflow objects - Error: `400` - Bad Request **Example:** ```json {\"workflows\": [{\"id\": \"wf_123\", \"name\": \"Daily Report\", \"status\": \"active\"}]} ``` ### POST /workflows Create a new workflow. **Parameters:** - `name` (string): Workflow name (required) - `definition` (object): Workflow definition in JSON format **Response:** - Success: `201` - Workflow created successfully - Error: `400` - Bad Request **Example:** ```json {\"id\": \"wf_456\", \"name\": \"New Workflow\", \"created_at\": \"2024-01-15T10:00:00Z\"} ``` ### DELETE /workflows/{id} Delete a workflow permanently. **Parameters:** - `id` (string): Workflow ID (required) **Response:** - Success: `204` - Workflow deleted - Error: `404` - Workflow not found **Example:** ```json {} ```",
              "score": null
            },
            {
              "doc_id": "release_3_2_0",
              "chunk_id": "release_3_2_0_fix_0",
              "content": "# Release Notes - v3.2.0 **Release Date:** 2024-01-15 ## New Features - Workflow versioning with rollback support - New Python SDK with async support - Bulk operations API for workflows - Custom retry policies per step ## Bug Fixes - Fixed race condition in concurrent execution handler - Resolved memory leak in long-running workflows - Fixed timezone handling in scheduled triggers ## Breaking Changes - **BREAKING:** API v1 endpoints deprecated, will be removed in v4.0 - **BREAKING:** Minimum Python version now 3.9 ## Upgrade Instructions ```bash cloudflow upgrade --version 3.2.0 ``` ## Known Issues None at this time.",
              "score": null
            }
          ],
          "latency_ms": 3.3193419985764194,
          "query_id": "simple_004",
          "dimension": "synonym",
          "query": "what's the workflow timeout limit"
        },
        {
          "key_facts": [
            "3600",
            "WORKFLOW_TIMEOUT_SECONDS"
          ],
          "found_facts": [],
          "missed_facts": [
            "3600",
            "WORKFLOW_TIMEOUT_SECONDS"
          ],
          "coverage": 0.0,
          "expected_docs": [
            "config_workflow_engine"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "release_3_2_0",
              "chunk_id": "release_3_2_0_fix_0",
              "content": "# Release Notes - v3.2.0 **Release Date:** 2024-01-15 ## New Features - Workflow versioning with rollback support - New Python SDK with async support - Bulk operations API for workflows - Custom retry policies per step ## Bug Fixes - Fixed race condition in concurrent execution handler - Resolved memory leak in long-running workflows - Fixed timezone handling in scheduled triggers ## Breaking Changes - **BREAKING:** API v1 endpoints deprecated, will be removed in v4.0 - **BREAKING:** Minimum Python version now 3.9 ## Upgrade Instructions ```bash cloudflow upgrade --version 3.2.0 ``` ## Known Issues None at this time.",
              "score": null
            },
            {
              "doc_id": "howto_debug_failed_execution",
              "chunk_id": "howto_debug_failed_execution_fix_0",
              "content": "# How to Debug Failed Execution ## Prerequisites - Failed execution ID ## Steps ### Step 1: Get execution details Retrieve the full execution record including step outputs. ```bash cloudflow executions get ex_123 --include-steps ``` ### Step 2: View step logs Check logs for the failed step. ```bash cloudflow executions logs ex_123 --step 3 ``` ### Step 3: Inspect input/output Review the data passed between steps. ```bash cloudflow executions inspect ex_123 --step 3 ``` ### Step 4: Enable debug mode Re-run with verbose logging enabled. ```bash cloudflow workflows execute wf_123 --debug ``` ### Step 5: Check error patterns Look for similar failures in recent executions. ```bash cloudflow executions list --status failed --since 24h ``` ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            },
            {
              "doc_id": "howto_implement_retry_logic",
              "chunk_id": "howto_implement_retry_logic_fix_0",
              "content": "# How to Implement Retry Logic ## Prerequisites - Existing workflow ## Steps ### Step 1: Define retry policy Add retry configuration to your workflow step. > **Note:** Maximum 5 retry attempts allowed ### Step 2: Configure backoff Set exponential backoff with base delay of 1 second. The formula is: delay = base * 2^attempt. ### Step 3: Add timeout Set maximum execution time to prevent infinite loops. Default timeout is 30 seconds. ### Step 4: Handle final failure Configure on_failure handler to notify or escalate when all retries are exhausted. ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            },
            {
              "doc_id": "api_executions",
              "chunk_id": "api_executions_fix_0",
              "content": "# Executions API Reference ## Overview The Executions API provides programmatic access to executions functionality. Base URL: `https://api.cloudflow.io/v1/executions` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /executions List workflow executions. **Parameters:** - `workflow_id` (string): Filter by workflow ID - `since` (datetime): Filter executions after this timestamp **Response:** - Success: `200` - Returns array of execution objects - Error: `400` - Bad Request **Example:** ```json {\"executions\": [{\"id\": \"ex_789\", \"status\": \"completed\", \"duration_ms\": 1523}]} ``` ### POST /executions/{id}/cancel Cancel a running execution. **Parameters:** - `id` (string): Execution ID - `reason` (string): Cancellation reason (optional) **Response:** - Success: `200` - Execution cancelled - Error: `400` - Bad Request **Example:** ```json {\"id\": \"ex_789\", \"status\": \"cancelled\", \"cancelled_at\": \"2024-01-15T10:05:00Z\"} ```",
              "score": null
            },
            {
              "doc_id": "arch_workflow_engine",
              "chunk_id": "arch_workflow_engine_fix_0",
              "content": "# Workflow Engine Architecture ## Overview The Workflow Engine is the core component responsible for executing user-defined workflows. It uses an event-driven architecture with support for parallel execution, conditional branching, and automatic retries. ## Design Principles - **Idempotency**: All operations can be safely retried without side effects - **Eventual Consistency**: State converges to consistent view within 5 seconds - **Fault Tolerance**: System continues operating despite component failures ## Components ### Scheduler Manages workflow scheduling and triggers. Supports cron expressions, webhooks, and event-based triggers. **Technology:** Custom Go service **Scaling:** Horizontal with leader election ### Executor Runs individual workflow steps in isolated containers. Maximum execution time is 30 minutes per step. **Technology:** Kubernetes Jobs **Scaling:** Auto-scaling based on queue depth ### State Store Persists workflow state, execution history, and step results. Uses optimistic locking for concurrent updates. **Technology:** PostgreSQL with JSONB **Scaling:** Primary-replica with read replicas ## Data Flow Triggers \u2192 Scheduler \u2192 Job Queue \u2192 Executor \u2192 State Store \u2192 Webhooks/Notifications ## Performance Characteristics - **Latency P50:** 45ms - **Latency P99:** 250ms - **Throughput:** 5000 executions/minute",
              "score": null
            }
          ],
          "latency_ms": 3.398078000827809,
          "query_id": "simple_004",
          "dimension": "problem",
          "query": "my long-running workflow keeps getting killed before it finishes"
        },
        {
          "key_facts": [
            "3600",
            "WORKFLOW_TIMEOUT_SECONDS"
          ],
          "found_facts": [],
          "missed_facts": [
            "3600",
            "WORKFLOW_TIMEOUT_SECONDS"
          ],
          "coverage": 0.0,
          "expected_docs": [
            "config_workflow_engine"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "howto_implement_retry_logic",
              "chunk_id": "howto_implement_retry_logic_fix_0",
              "content": "# How to Implement Retry Logic ## Prerequisites - Existing workflow ## Steps ### Step 1: Define retry policy Add retry configuration to your workflow step. > **Note:** Maximum 5 retry attempts allowed ### Step 2: Configure backoff Set exponential backoff with base delay of 1 second. The formula is: delay = base * 2^attempt. ### Step 3: Add timeout Set maximum execution time to prevent infinite loops. Default timeout is 30 seconds. ### Step 4: Handle final failure Configure on_failure handler to notify or escalate when all retries are exhausted. ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            },
            {
              "doc_id": "release_3_2_0",
              "chunk_id": "release_3_2_0_fix_0",
              "content": "# Release Notes - v3.2.0 **Release Date:** 2024-01-15 ## New Features - Workflow versioning with rollback support - New Python SDK with async support - Bulk operations API for workflows - Custom retry policies per step ## Bug Fixes - Fixed race condition in concurrent execution handler - Resolved memory leak in long-running workflows - Fixed timezone handling in scheduled triggers ## Breaking Changes - **BREAKING:** API v1 endpoints deprecated, will be removed in v4.0 - **BREAKING:** Minimum Python version now 3.9 ## Upgrade Instructions ```bash cloudflow upgrade --version 3.2.0 ``` ## Known Issues None at this time.",
              "score": null
            },
            {
              "doc_id": "api_workflows",
              "chunk_id": "api_workflows_fix_0",
              "content": "# Workflows API Reference ## Overview The Workflows API provides programmatic access to workflows functionality. Base URL: `https://api.cloudflow.io/v1/workflows` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /workflows List all workflows for the authenticated user. **Parameters:** - `limit` (integer): Maximum number of results (default: 50, max: 200) - `status` (string): Filter by status: active, paused, archived **Response:** - Success: `200` - Returns array of workflow objects - Error: `400` - Bad Request **Example:** ```json {\"workflows\": [{\"id\": \"wf_123\", \"name\": \"Daily Report\", \"status\": \"active\"}]} ``` ### POST /workflows Create a new workflow. **Parameters:** - `name` (string): Workflow name (required) - `definition` (object): Workflow definition in JSON format **Response:** - Success: `201` - Workflow created successfully - Error: `400` - Bad Request **Example:** ```json {\"id\": \"wf_456\", \"name\": \"New Workflow\", \"created_at\": \"2024-01-15T10:00:00Z\"} ``` ### DELETE /workflows/{id} Delete a workflow permanently. **Parameters:** - `id` (string): Workflow ID (required) **Response:** - Success: `204` - Workflow deleted - Error: `404` - Workflow not found **Example:** ```json {} ```",
              "score": null
            },
            {
              "doc_id": "runbook_kafka_consumer_lag",
              "chunk_id": "runbook_kafka_consumer_lag_fix_0",
              "content": "# Runbook: Kafka Consumer Lag ## Severity **Level:** P1 - Critical **Response Time:** 1 hour ## Symptoms - Consumer lag > 10000 messages - Processing delays reported - Alerts from lag monitor ## Diagnosis ### Step 1 Check consumer group lag: `kafka-consumer-groups.sh --bootstrap-server kafka:9092 --describe --group workflow-executor` ### Step 2 Verify consumer health: `kubectl get pods -l app=workflow-executor -n production` ### Step 3 Check for processing errors: `kubectl logs -l app=workflow-executor --tail=100 -n production | grep ERROR` ### Step 4 Monitor throughput: `cloudflow metrics get kafka.consumer.records_per_second` ## Resolution ### Step 1 Scale up consumers: `kubectl scale deployment/workflow-executor --replicas=10 -n production` ### Step 2 Reset offset if messages are stale: `kafka-consumer-groups.sh --bootstrap-server kafka:9092 --group workflow-executor --reset-offsets --to-latest --execute --topic workflows` ### Step 3 Pause producer if backpressure needed: `cloudflow workflows pause-triggers` ### Step 4 Increase partition count for parallelism (requires coordination) ## Escalation If lag persists > 30 minutes, page platform team. May need Kafka cluster scaling. ## Post-Incident - Document timeline in incident log - Schedule post-mortem within 48 hours - Update runbook if new patterns discovered",
              "score": null
            },
            {
              "doc_id": "release_2_9_0",
              "chunk_id": "release_2_9_0_fix_0",
              "content": "# Release Notes - v2.9.0 **Release Date:** 2023-08-01 ## New Features - Parallel step execution - Conditional branching in workflows - Slack integration improvements - Performance dashboard ## Bug Fixes - Fixed issue with special characters in secrets - Resolved timeout in large file uploads ## Upgrade Instructions ```bash cloudflow upgrade --version 2.9.0 ``` ## Known Issues None at this time.",
              "score": null
            }
          ],
          "latency_ms": 4.673582998293568,
          "query_id": "simple_004",
          "dimension": "casual",
          "query": "workflow timeout"
        },
        {
          "key_facts": [
            "3600",
            "WORKFLOW_TIMEOUT_SECONDS"
          ],
          "found_facts": [
            "3600",
            "WORKFLOW_TIMEOUT_SECONDS"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "config_workflow_engine"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "config_workflow_engine",
              "chunk_id": "config_workflow_engine_fix_0",
              "content": "# Workflow Engine Configuration Reference ## Overview This document describes all configuration options for Workflow Engine. ## Environment Variables | Variable | Type | Default | Description | |----------|------|---------|-------------| | `WORKFLOW_MAX_STEPS` | integer | 100 | Maximum steps per workflow | | `WORKFLOW_TIMEOUT_SECONDS` | integer | 3600 | Maximum workflow execution time | | `WORKFLOW_RETRY_LIMIT` | integer | 3 | Default retry attempts for failed steps | | `WORKFLOW_CONCURRENT_LIMIT` | integer | 10 | Maximum concurrent executions per workflow | | `EXECUTOR_POOL_SIZE` | integer | 50 | Number of executor workers | | `EXECUTOR_MEMORY_LIMIT` | string | 512Mi | Memory limit per executor | ## Configuration File Configuration can also be provided via `workflow engine.yaml`: ```yaml workflow_max_steps: 100 workflow_timeout_seconds: 3600 workflow_retry_limit: 3 workflow_concurrent_limit: 10 executor_pool_size: 50 ``` ## Validation Configuration is validated at startup. Invalid configuration will prevent the service from starting.",
              "score": null
            },
            {
              "doc_id": "arch_workflow_engine",
              "chunk_id": "arch_workflow_engine_fix_0",
              "content": "# Workflow Engine Architecture ## Overview The Workflow Engine is the core component responsible for executing user-defined workflows. It uses an event-driven architecture with support for parallel execution, conditional branching, and automatic retries. ## Design Principles - **Idempotency**: All operations can be safely retried without side effects - **Eventual Consistency**: State converges to consistent view within 5 seconds - **Fault Tolerance**: System continues operating despite component failures ## Components ### Scheduler Manages workflow scheduling and triggers. Supports cron expressions, webhooks, and event-based triggers. **Technology:** Custom Go service **Scaling:** Horizontal with leader election ### Executor Runs individual workflow steps in isolated containers. Maximum execution time is 30 minutes per step. **Technology:** Kubernetes Jobs **Scaling:** Auto-scaling based on queue depth ### State Store Persists workflow state, execution history, and step results. Uses optimistic locking for concurrent updates. **Technology:** PostgreSQL with JSONB **Scaling:** Primary-replica with read replicas ## Data Flow Triggers \u2192 Scheduler \u2192 Job Queue \u2192 Executor \u2192 State Store \u2192 Webhooks/Notifications ## Performance Characteristics - **Latency P50:** 45ms - **Latency P99:** 250ms - **Throughput:** 5000 executions/minute",
              "score": null
            },
            {
              "doc_id": "arch_data_pipeline",
              "chunk_id": "arch_data_pipeline_fix_0",
              "content": "# Data Pipeline Architecture ## Overview The Data Pipeline processes and transforms data between workflow steps. It supports batch processing up to 10GB and streaming for real-time use cases. ## Design Principles - **Exactly-Once Processing**: Each record is processed exactly once - **Backpressure Handling**: System gracefully handles varying load ## Components ### Stream Processor Handles real-time data streams. Maximum throughput of 100000 messages per second. **Technology:** Apache Kafka with Flink **Scaling:** Partition-based scaling ### Batch Processor Processes large batch jobs. Supports data formats: JSON, CSV, Parquet, Avro. **Technology:** Apache Spark on Kubernetes **Scaling:** Dynamic resource allocation ### Data Lake Long-term data storage with time-travel queries. Retention configurable up to 7 years. **Technology:** S3 with Iceberg tables **Scaling:** Unlimited with tiered storage ## Data Flow Input \u2192 Validation \u2192 Transform \u2192 Enrich \u2192 Output ## Performance Characteristics - **Latency P50:** 100ms (streaming), 5min (batch) - **Latency P99:** 500ms (streaming), 30min (batch) - **Throughput:** 100000 messages/second (streaming)",
              "score": null
            },
            {
              "doc_id": "runbook_database_connection_exhaustion",
              "chunk_id": "runbook_database_connection_exhaustion_fix_0",
              "content": "# Runbook: Database Connection Exhaustion ## Severity **Level:** P1 - Critical **Response Time:** 15 minutes ## Symptoms - Connection pool at 100% - Queries timing out - 'too many connections' errors in logs ## Diagnosis ### Step 1 Check active connections: `SELECT count(*) FROM pg_stat_activity` ### Step 2 Identify long-running queries: `SELECT pid, now() - pg_stat_activity.query_start AS duration, query FROM pg_stat_activity WHERE state != 'idle' ORDER BY duration DESC` ### Step 3 Check PgBouncer stats: `SHOW POOLS` ### Step 4 Verify connection limits: `SHOW max_connections` ## Resolution ### Step 1 Kill long-running queries: `SELECT pg_terminate_backend(pid) FROM pg_stat_activity WHERE duration > interval '5 minutes'` ### Step 2 Increase PgBouncer pool size temporarily: update pgbouncer.ini ### Step 3 Restart affected services to release connections: `kubectl rollout restart deployment/api` ### Step 4 Scale down non-critical services to free connections ## Escalation If connections remain exhausted, escalate to DBA. Consider enabling connection queueing. ## Post-Incident - Document timeline in incident log - Schedule post-mortem within 48 hours - Update runbook if new patterns discovered",
              "score": null
            },
            {
              "doc_id": "howto_implement_retry_logic",
              "chunk_id": "howto_implement_retry_logic_fix_0",
              "content": "# How to Implement Retry Logic ## Prerequisites - Existing workflow ## Steps ### Step 1: Define retry policy Add retry configuration to your workflow step. > **Note:** Maximum 5 retry attempts allowed ### Step 2: Configure backoff Set exponential backoff with base delay of 1 second. The formula is: delay = base * 2^attempt. ### Step 3: Add timeout Set maximum execution time to prevent infinite loops. Default timeout is 30 seconds. ### Step 4: Handle final failure Configure on_failure handler to notify or escalate when all retries are exhausted. ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            }
          ],
          "latency_ms": 4.988549000700004,
          "query_id": "simple_004",
          "dimension": "contextual",
          "query": "planning a data migration workflow, need to know the max execution duration"
        },
        {
          "key_facts": [
            "3600",
            "WORKFLOW_TIMEOUT_SECONDS"
          ],
          "found_facts": [],
          "missed_facts": [
            "3600",
            "WORKFLOW_TIMEOUT_SECONDS"
          ],
          "coverage": 0.0,
          "expected_docs": [
            "config_workflow_engine"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "release_3_2_0",
              "chunk_id": "release_3_2_0_fix_0",
              "content": "# Release Notes - v3.2.0 **Release Date:** 2024-01-15 ## New Features - Workflow versioning with rollback support - New Python SDK with async support - Bulk operations API for workflows - Custom retry policies per step ## Bug Fixes - Fixed race condition in concurrent execution handler - Resolved memory leak in long-running workflows - Fixed timezone handling in scheduled triggers ## Breaking Changes - **BREAKING:** API v1 endpoints deprecated, will be removed in v4.0 - **BREAKING:** Minimum Python version now 3.9 ## Upgrade Instructions ```bash cloudflow upgrade --version 3.2.0 ``` ## Known Issues None at this time.",
              "score": null
            },
            {
              "doc_id": "howto_implement_retry_logic",
              "chunk_id": "howto_implement_retry_logic_fix_0",
              "content": "# How to Implement Retry Logic ## Prerequisites - Existing workflow ## Steps ### Step 1: Define retry policy Add retry configuration to your workflow step. > **Note:** Maximum 5 retry attempts allowed ### Step 2: Configure backoff Set exponential backoff with base delay of 1 second. The formula is: delay = base * 2^attempt. ### Step 3: Add timeout Set maximum execution time to prevent infinite loops. Default timeout is 30 seconds. ### Step 4: Handle final failure Configure on_failure handler to notify or escalate when all retries are exhausted. ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            },
            {
              "doc_id": "howto_debug_failed_execution",
              "chunk_id": "howto_debug_failed_execution_fix_0",
              "content": "# How to Debug Failed Execution ## Prerequisites - Failed execution ID ## Steps ### Step 1: Get execution details Retrieve the full execution record including step outputs. ```bash cloudflow executions get ex_123 --include-steps ``` ### Step 2: View step logs Check logs for the failed step. ```bash cloudflow executions logs ex_123 --step 3 ``` ### Step 3: Inspect input/output Review the data passed between steps. ```bash cloudflow executions inspect ex_123 --step 3 ``` ### Step 4: Enable debug mode Re-run with verbose logging enabled. ```bash cloudflow workflows execute wf_123 --debug ``` ### Step 5: Check error patterns Look for similar failures in recent executions. ```bash cloudflow executions list --status failed --since 24h ``` ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            },
            {
              "doc_id": "api_workflows",
              "chunk_id": "api_workflows_fix_0",
              "content": "# Workflows API Reference ## Overview The Workflows API provides programmatic access to workflows functionality. Base URL: `https://api.cloudflow.io/v1/workflows` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /workflows List all workflows for the authenticated user. **Parameters:** - `limit` (integer): Maximum number of results (default: 50, max: 200) - `status` (string): Filter by status: active, paused, archived **Response:** - Success: `200` - Returns array of workflow objects - Error: `400` - Bad Request **Example:** ```json {\"workflows\": [{\"id\": \"wf_123\", \"name\": \"Daily Report\", \"status\": \"active\"}]} ``` ### POST /workflows Create a new workflow. **Parameters:** - `name` (string): Workflow name (required) - `definition` (object): Workflow definition in JSON format **Response:** - Success: `201` - Workflow created successfully - Error: `400` - Bad Request **Example:** ```json {\"id\": \"wf_456\", \"name\": \"New Workflow\", \"created_at\": \"2024-01-15T10:00:00Z\"} ``` ### DELETE /workflows/{id} Delete a workflow permanently. **Parameters:** - `id` (string): Workflow ID (required) **Response:** - Success: `204` - Workflow deleted - Error: `404` - Workflow not found **Example:** ```json {} ```",
              "score": null
            },
            {
              "doc_id": "release_3_0_0",
              "chunk_id": "release_3_0_0_fix_0",
              "content": "# Release Notes - v3.0.0 **Release Date:** 2023-10-15 ## New Features - Complete UI redesign - New workflow editor with visual builder - Real-time execution monitoring - Team workspaces ## Bug Fixes - Fixed critical security issue in token validation - Resolved data loss bug in failed migrations ## Breaking Changes - **BREAKING:** Authentication API completely rewritten - **BREAKING:** Legacy webhook format no longer supported - **BREAKING:** Minimum Node.js version now 18 ## Upgrade Instructions ```bash cloudflow upgrade --version 3.0.0 ``` ## Known Issues None at this time.",
              "score": null
            }
          ],
          "latency_ms": 3.392917999008205,
          "query_id": "simple_004",
          "dimension": "negation",
          "query": "why does my workflow fail after running for a long time"
        },
        {
          "key_facts": [
            "Kubernetes",
            "EKS",
            "Helm"
          ],
          "found_facts": [
            "Kubernetes",
            "EKS",
            "Helm"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "adr_002"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "arch_workflow_engine",
              "chunk_id": "arch_workflow_engine_fix_0",
              "content": "# Workflow Engine Architecture ## Overview The Workflow Engine is the core component responsible for executing user-defined workflows. It uses an event-driven architecture with support for parallel execution, conditional branching, and automatic retries. ## Design Principles - **Idempotency**: All operations can be safely retried without side effects - **Eventual Consistency**: State converges to consistent view within 5 seconds - **Fault Tolerance**: System continues operating despite component failures ## Components ### Scheduler Manages workflow scheduling and triggers. Supports cron expressions, webhooks, and event-based triggers. **Technology:** Custom Go service **Scaling:** Horizontal with leader election ### Executor Runs individual workflow steps in isolated containers. Maximum execution time is 30 minutes per step. **Technology:** Kubernetes Jobs **Scaling:** Auto-scaling based on queue depth ### State Store Persists workflow state, execution history, and step results. Uses optimistic locking for concurrent updates. **Technology:** PostgreSQL with JSONB **Scaling:** Primary-replica with read replicas ## Data Flow Triggers \u2192 Scheduler \u2192 Job Queue \u2192 Executor \u2192 State Store \u2192 Webhooks/Notifications ## Performance Characteristics - **Latency P50:** 45ms - **Latency P99:** 250ms - **Throughput:** 5000 executions/minute",
              "score": null
            },
            {
              "doc_id": "adr_002",
              "chunk_id": "adr_002_fix_0",
              "content": "# ADR-002: Adopt Kubernetes for Container Orchestration **Status:** Accepted **Date:** 2024-01-24 ## Context We need a platform to deploy and scale our microservices. The platform must support auto-scaling, rolling deployments, and multi-region deployment. ## Options Considered ### Option 1: Kubernetes (EKS) Industry-standard container orchestration. **Pros:** - Industry standard - Rich ecosystem - Cloud agnostic - Auto-scaling **Cons:** - Operational complexity - Learning curve - Resource overhead ### Option 2: AWS ECS Amazon's native container service. **Pros:** - Simple operation - Native AWS integration - Lower overhead **Cons:** - AWS lock-in - Less flexible - Smaller community ### Option 3: Serverless (Lambda) Function-as-a-service approach. **Pros:** - Zero infrastructure - Pay per use - Auto-scaling **Cons:** - Cold starts - 15-minute limit - Vendor lock-in ## Decision We will use Kubernetes on EKS for container orchestration with Helm for deployments. ## Consequences - Need platform team expertise - Implement GitOps with ArgoCD - Use managed node groups for easier operations",
              "score": null
            },
            {
              "doc_id": "arch_api_gateway",
              "chunk_id": "arch_api_gateway_fix_0",
              "content": "# API Gateway Architecture ## Overview The API Gateway handles all incoming HTTP requests, providing authentication, rate limiting, and request routing. Built on Kong with custom plugins for CloudFlow-specific functionality. ## Design Principles - **Zero Trust**: Every request is authenticated and authorized - **Defense in Depth**: Multiple layers of security validation ## Components ### Kong Gateway Primary ingress controller. Handles TLS termination, request routing, and plugin execution. **Technology:** Kong 3.4 on Kubernetes **Scaling:** Horizontal with HPA ### Auth Plugin Validates JWT tokens, API keys, and OAuth2 flows. Token validation latency under 5ms. **Technology:** Custom Lua plugin **Scaling:** Stateless ### Rate Limiter Enforces per-user and per-IP rate limits. Default limit is 100 requests per minute. **Technology:** Redis-backed sliding window **Scaling:** Shared Redis cluster ## Data Flow Client \u2192 TLS \u2192 Kong \u2192 Auth \u2192 Rate Limit \u2192 Backend Services ## Performance Characteristics - **Latency P50:** 12ms - **Latency P99:** 45ms - **Throughput:** 50000 requests/second",
              "score": null
            },
            {
              "doc_id": "release_3_0_0",
              "chunk_id": "release_3_0_0_fix_0",
              "content": "# Release Notes - v3.0.0 **Release Date:** 2023-10-15 ## New Features - Complete UI redesign - New workflow editor with visual builder - Real-time execution monitoring - Team workspaces ## Bug Fixes - Fixed critical security issue in token validation - Resolved data loss bug in failed migrations ## Breaking Changes - **BREAKING:** Authentication API completely rewritten - **BREAKING:** Legacy webhook format no longer supported - **BREAKING:** Minimum Node.js version now 18 ## Upgrade Instructions ```bash cloudflow upgrade --version 3.0.0 ``` ## Known Issues None at this time.",
              "score": null
            },
            {
              "doc_id": "release_3_2_0",
              "chunk_id": "release_3_2_0_fix_0",
              "content": "# Release Notes - v3.2.0 **Release Date:** 2024-01-15 ## New Features - Workflow versioning with rollback support - New Python SDK with async support - Bulk operations API for workflows - Custom retry policies per step ## Bug Fixes - Fixed race condition in concurrent execution handler - Resolved memory leak in long-running workflows - Fixed timezone handling in scheduled triggers ## Breaking Changes - **BREAKING:** API v1 endpoints deprecated, will be removed in v4.0 - **BREAKING:** Minimum Python version now 3.9 ## Upgrade Instructions ```bash cloudflow upgrade --version 3.2.0 ``` ## Known Issues None at this time.",
              "score": null
            }
          ],
          "latency_ms": 3.3385870010533836,
          "query_id": "simple_005",
          "dimension": "original",
          "query": "What container orchestration does CloudFlow use?"
        },
        {
          "key_facts": [
            "Kubernetes",
            "EKS",
            "Helm"
          ],
          "found_facts": [
            "Kubernetes",
            "EKS",
            "Helm"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "adr_002"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "arch_workflow_engine",
              "chunk_id": "arch_workflow_engine_fix_0",
              "content": "# Workflow Engine Architecture ## Overview The Workflow Engine is the core component responsible for executing user-defined workflows. It uses an event-driven architecture with support for parallel execution, conditional branching, and automatic retries. ## Design Principles - **Idempotency**: All operations can be safely retried without side effects - **Eventual Consistency**: State converges to consistent view within 5 seconds - **Fault Tolerance**: System continues operating despite component failures ## Components ### Scheduler Manages workflow scheduling and triggers. Supports cron expressions, webhooks, and event-based triggers. **Technology:** Custom Go service **Scaling:** Horizontal with leader election ### Executor Runs individual workflow steps in isolated containers. Maximum execution time is 30 minutes per step. **Technology:** Kubernetes Jobs **Scaling:** Auto-scaling based on queue depth ### State Store Persists workflow state, execution history, and step results. Uses optimistic locking for concurrent updates. **Technology:** PostgreSQL with JSONB **Scaling:** Primary-replica with read replicas ## Data Flow Triggers \u2192 Scheduler \u2192 Job Queue \u2192 Executor \u2192 State Store \u2192 Webhooks/Notifications ## Performance Characteristics - **Latency P50:** 45ms - **Latency P99:** 250ms - **Throughput:** 5000 executions/minute",
              "score": null
            },
            {
              "doc_id": "release_3_0_0",
              "chunk_id": "release_3_0_0_fix_0",
              "content": "# Release Notes - v3.0.0 **Release Date:** 2023-10-15 ## New Features - Complete UI redesign - New workflow editor with visual builder - Real-time execution monitoring - Team workspaces ## Bug Fixes - Fixed critical security issue in token validation - Resolved data loss bug in failed migrations ## Breaking Changes - **BREAKING:** Authentication API completely rewritten - **BREAKING:** Legacy webhook format no longer supported - **BREAKING:** Minimum Node.js version now 18 ## Upgrade Instructions ```bash cloudflow upgrade --version 3.0.0 ``` ## Known Issues None at this time.",
              "score": null
            },
            {
              "doc_id": "release_3_2_0",
              "chunk_id": "release_3_2_0_fix_0",
              "content": "# Release Notes - v3.2.0 **Release Date:** 2024-01-15 ## New Features - Workflow versioning with rollback support - New Python SDK with async support - Bulk operations API for workflows - Custom retry policies per step ## Bug Fixes - Fixed race condition in concurrent execution handler - Resolved memory leak in long-running workflows - Fixed timezone handling in scheduled triggers ## Breaking Changes - **BREAKING:** API v1 endpoints deprecated, will be removed in v4.0 - **BREAKING:** Minimum Python version now 3.9 ## Upgrade Instructions ```bash cloudflow upgrade --version 3.2.0 ``` ## Known Issues None at this time.",
              "score": null
            },
            {
              "doc_id": "adr_002",
              "chunk_id": "adr_002_fix_0",
              "content": "# ADR-002: Adopt Kubernetes for Container Orchestration **Status:** Accepted **Date:** 2024-01-24 ## Context We need a platform to deploy and scale our microservices. The platform must support auto-scaling, rolling deployments, and multi-region deployment. ## Options Considered ### Option 1: Kubernetes (EKS) Industry-standard container orchestration. **Pros:** - Industry standard - Rich ecosystem - Cloud agnostic - Auto-scaling **Cons:** - Operational complexity - Learning curve - Resource overhead ### Option 2: AWS ECS Amazon's native container service. **Pros:** - Simple operation - Native AWS integration - Lower overhead **Cons:** - AWS lock-in - Less flexible - Smaller community ### Option 3: Serverless (Lambda) Function-as-a-service approach. **Pros:** - Zero infrastructure - Pay per use - Auto-scaling **Cons:** - Cold starts - 15-minute limit - Vendor lock-in ## Decision We will use Kubernetes on EKS for container orchestration with Helm for deployments. ## Consequences - Need platform team expertise - Implement GitOps with ArgoCD - Use managed node groups for easier operations",
              "score": null
            },
            {
              "doc_id": "howto_configure_custom_domain",
              "chunk_id": "howto_configure_custom_domain_fix_0",
              "content": "# How to Configure Custom Domain ## Prerequisites - Domain with DNS access - CloudFlow Pro plan or higher ## Steps ### Step 1: Verify domain ownership Add a TXT record to verify domain ownership. ```bash cloudflow domains verify --domain app.example.com ``` ### Step 2: Configure DNS Add a CNAME record pointing to your CloudFlow endpoint. > **Note:** CNAME should point to ingress.cloudflow.io ### Step 3: Enable SSL CloudFlow automatically provisions SSL certificates via Let's Encrypt. ```bash cloudflow domains ssl --domain app.example.com ``` ### Step 4: Set as primary Make this the primary domain for your application. ```bash cloudflow domains set-primary --domain app.example.com ``` ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            }
          ],
          "latency_ms": 4.057645999637316,
          "query_id": "simple_005",
          "dimension": "synonym",
          "query": "what platform manages CloudFlow's containers"
        },
        {
          "key_facts": [
            "Kubernetes",
            "EKS",
            "Helm"
          ],
          "found_facts": [],
          "missed_facts": [
            "Kubernetes",
            "EKS",
            "Helm"
          ],
          "coverage": 0.0,
          "expected_docs": [
            "adr_002"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "runbook_api_gateway_5xx_errors",
              "chunk_id": "runbook_api_gateway_5xx_errors_fix_0",
              "content": "# Runbook: API Gateway 5xx Errors ## Severity **Level:** P3 - Medium **Response Time:** 1 hour ## Symptoms - Error rate > 1% - 5xx responses in access logs - Downstream service failures ## Diagnosis ### Step 1 Check Kong error logs: `kubectl logs -l app=kong -n kong --tail=100 | grep -E '5[0-9]{2}'` ### Step 2 Verify backend health: `kubectl get pods -n production -o wide` ### Step 3 Check service endpoints: `kubectl get endpoints -n production` ### Step 4 Test backend directly: `kubectl exec -it kong-xxx -n kong -- curl http://api.production.svc:8080/health` ## Resolution ### Step 1 Restart unhealthy backends: `kubectl rollout restart deployment/api -n production` ### Step 2 Enable circuit breaker if cascading: update Kong circuit-breaker plugin ### Step 3 Scale up backends: `kubectl scale deployment/api --replicas=5 -n production` ### Step 4 Check for resource exhaustion on backend pods ### Step 5 Verify network policies allow traffic ## Escalation If 5xx persists > 15 minutes, page on-call. May indicate infrastructure issue. ## Post-Incident - Document timeline in incident log - Schedule post-mortem within 48 hours - Update runbook if new patterns discovered",
              "score": null
            },
            {
              "doc_id": "runbook_high_cpu_usage",
              "chunk_id": "runbook_high_cpu_usage_fix_0",
              "content": "# Runbook: High CPU Usage ## Severity **Level:** P1 - Critical **Response Time:** 15 minutes ## Symptoms - CPU utilization > 80% sustained - Increased response latency - Autoscaler at maximum replicas ## Diagnosis ### Step 1 Check current CPU usage: `kubectl top pods -n production` ### Step 2 Identify hot pods: `kubectl top pods --sort-by=cpu -n production | head -10` ### Step 3 Check for recent deployments: `kubectl rollout history deployment/api -n production` ### Step 4 Profile the application: `cloudflow debug profile --pod api-xyz --duration 60s` ## Resolution ### Step 1 Scale up if traffic-related: `kubectl scale deployment/api --replicas=10 -n production` ### Step 2 Restart pods if memory leak suspected: `kubectl rollout restart deployment/api -n production` ### Step 3 Roll back recent deployment if regression: `kubectl rollout undo deployment/api -n production` ### Step 4 Add resource limits if unbounded: update deployment with CPU limits ## Escalation If CPU remains high after scaling, escalate to platform team. Page on-call if affecting users. ## Post-Incident - Document timeline in incident log - Schedule post-mortem within 48 hours - Update runbook if new patterns discovered",
              "score": null
            },
            {
              "doc_id": "runbook_out_of_memory_oom_kills",
              "chunk_id": "runbook_out_of_memory_oom_kills_fix_0",
              "content": "# Runbook: Out of Memory (OOM) Kills ## Severity **Level:** P3 - Medium **Response Time:** 15 minutes ## Symptoms - Pods restarting with OOMKilled status - Memory usage at limit - Application slowdown before restart ## Diagnosis ### Step 1 Check OOM events: `kubectl get events -n production --field-selector reason=OOMKilled` ### Step 2 Review memory usage: `kubectl top pods -n production --sort-by=memory` ### Step 3 Check container limits: `kubectl describe pod <pod-name> -n production | grep -A5 Limits` ### Step 4 Analyze heap dumps if available: `cloudflow debug heapdump --pod <pod-name>` ## Resolution ### Step 1 Increase memory limit: update deployment resources.limits.memory ### Step 2 Add memory request equal to limit to guarantee allocation ### Step 3 Enable JVM heap dump on OOM: add -XX:+HeapDumpOnOutOfMemoryError ### Step 4 Investigate memory leaks with profiling tools ### Step 5 Consider horizontal scaling instead of vertical ## Escalation If OOMs continue after limit increase, escalate to development team for memory leak investigation. ## Post-Incident - Document timeline in incident log - Schedule post-mortem within 48 hours - Update runbook if new patterns discovered",
              "score": null
            },
            {
              "doc_id": "runbook_kafka_consumer_lag",
              "chunk_id": "runbook_kafka_consumer_lag_fix_0",
              "content": "# Runbook: Kafka Consumer Lag ## Severity **Level:** P1 - Critical **Response Time:** 1 hour ## Symptoms - Consumer lag > 10000 messages - Processing delays reported - Alerts from lag monitor ## Diagnosis ### Step 1 Check consumer group lag: `kafka-consumer-groups.sh --bootstrap-server kafka:9092 --describe --group workflow-executor` ### Step 2 Verify consumer health: `kubectl get pods -l app=workflow-executor -n production` ### Step 3 Check for processing errors: `kubectl logs -l app=workflow-executor --tail=100 -n production | grep ERROR` ### Step 4 Monitor throughput: `cloudflow metrics get kafka.consumer.records_per_second` ## Resolution ### Step 1 Scale up consumers: `kubectl scale deployment/workflow-executor --replicas=10 -n production` ### Step 2 Reset offset if messages are stale: `kafka-consumer-groups.sh --bootstrap-server kafka:9092 --group workflow-executor --reset-offsets --to-latest --execute --topic workflows` ### Step 3 Pause producer if backpressure needed: `cloudflow workflows pause-triggers` ### Step 4 Increase partition count for parallelism (requires coordination) ## Escalation If lag persists > 30 minutes, page platform team. May need Kafka cluster scaling. ## Post-Incident - Document timeline in incident log - Schedule post-mortem within 48 hours - Update runbook if new patterns discovered",
              "score": null
            },
            {
              "doc_id": "runbook_ssl_certificate_expiry",
              "chunk_id": "runbook_ssl_certificate_expiry_fix_0",
              "content": "# Runbook: SSL Certificate Expiry ## Severity **Level:** P3 - Medium **Response Time:** 15 minutes ## Symptoms - Certificate expiring within 7 days - SSL handshake failures - Browser security warnings ## Diagnosis ### Step 1 Check certificate expiry: `echo | openssl s_client -connect api.cloudflow.io:443 2>/dev/null | openssl x509 -noout -dates` ### Step 2 Verify cert-manager status: `kubectl get certificates -n production` ### Step 3 Check cert-manager logs: `kubectl logs -l app=cert-manager -n cert-manager --tail=50` ### Step 4 Verify DNS is correct: `dig +short api.cloudflow.io` ## Resolution ### Step 1 Trigger manual renewal: `kubectl delete certificate api-cert -n production && kubectl apply -f certificate.yaml` ### Step 2 Check rate limits: Let's Encrypt has 50 certs/domain/week limit ### Step 3 Use staging issuer for testing: switch to letsencrypt-staging ClusterIssuer ### Step 4 If Let's Encrypt fails, manually upload certificate ## Escalation If certificate cannot be renewed, escalate to security team immediately. Consider using backup certificate. ## Post-Incident - Document timeline in incident log - Schedule post-mortem within 48 hours - Update runbook if new patterns discovered",
              "score": null
            }
          ],
          "latency_ms": 3.9268029995582765,
          "query_id": "simple_005",
          "dimension": "problem",
          "query": "seeing kubectl commands in the docs, so it's running on k8s?"
        },
        {
          "key_facts": [
            "Kubernetes",
            "EKS",
            "Helm"
          ],
          "found_facts": [
            "Kubernetes",
            "EKS",
            "Helm"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "adr_002"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "adr_002",
              "chunk_id": "adr_002_fix_0",
              "content": "# ADR-002: Adopt Kubernetes for Container Orchestration **Status:** Accepted **Date:** 2024-01-24 ## Context We need a platform to deploy and scale our microservices. The platform must support auto-scaling, rolling deployments, and multi-region deployment. ## Options Considered ### Option 1: Kubernetes (EKS) Industry-standard container orchestration. **Pros:** - Industry standard - Rich ecosystem - Cloud agnostic - Auto-scaling **Cons:** - Operational complexity - Learning curve - Resource overhead ### Option 2: AWS ECS Amazon's native container service. **Pros:** - Simple operation - Native AWS integration - Lower overhead **Cons:** - AWS lock-in - Less flexible - Smaller community ### Option 3: Serverless (Lambda) Function-as-a-service approach. **Pros:** - Zero infrastructure - Pay per use - Auto-scaling **Cons:** - Cold starts - 15-minute limit - Vendor lock-in ## Decision We will use Kubernetes on EKS for container orchestration with Helm for deployments. ## Consequences - Need platform team expertise - Implement GitOps with ArgoCD - Use managed node groups for easier operations",
              "score": null
            },
            {
              "doc_id": "arch_api_gateway",
              "chunk_id": "arch_api_gateway_fix_0",
              "content": "# API Gateway Architecture ## Overview The API Gateway handles all incoming HTTP requests, providing authentication, rate limiting, and request routing. Built on Kong with custom plugins for CloudFlow-specific functionality. ## Design Principles - **Zero Trust**: Every request is authenticated and authorized - **Defense in Depth**: Multiple layers of security validation ## Components ### Kong Gateway Primary ingress controller. Handles TLS termination, request routing, and plugin execution. **Technology:** Kong 3.4 on Kubernetes **Scaling:** Horizontal with HPA ### Auth Plugin Validates JWT tokens, API keys, and OAuth2 flows. Token validation latency under 5ms. **Technology:** Custom Lua plugin **Scaling:** Stateless ### Rate Limiter Enforces per-user and per-IP rate limits. Default limit is 100 requests per minute. **Technology:** Redis-backed sliding window **Scaling:** Shared Redis cluster ## Data Flow Client \u2192 TLS \u2192 Kong \u2192 Auth \u2192 Rate Limit \u2192 Backend Services ## Performance Characteristics - **Latency P50:** 12ms - **Latency P99:** 45ms - **Throughput:** 50000 requests/second",
              "score": null
            },
            {
              "doc_id": "release_3_0_0",
              "chunk_id": "release_3_0_0_fix_0",
              "content": "# Release Notes - v3.0.0 **Release Date:** 2023-10-15 ## New Features - Complete UI redesign - New workflow editor with visual builder - Real-time execution monitoring - Team workspaces ## Bug Fixes - Fixed critical security issue in token validation - Resolved data loss bug in failed migrations ## Breaking Changes - **BREAKING:** Authentication API completely rewritten - **BREAKING:** Legacy webhook format no longer supported - **BREAKING:** Minimum Node.js version now 18 ## Upgrade Instructions ```bash cloudflow upgrade --version 3.0.0 ``` ## Known Issues None at this time.",
              "score": null
            },
            {
              "doc_id": "arch_data_pipeline",
              "chunk_id": "arch_data_pipeline_fix_0",
              "content": "# Data Pipeline Architecture ## Overview The Data Pipeline processes and transforms data between workflow steps. It supports batch processing up to 10GB and streaming for real-time use cases. ## Design Principles - **Exactly-Once Processing**: Each record is processed exactly once - **Backpressure Handling**: System gracefully handles varying load ## Components ### Stream Processor Handles real-time data streams. Maximum throughput of 100000 messages per second. **Technology:** Apache Kafka with Flink **Scaling:** Partition-based scaling ### Batch Processor Processes large batch jobs. Supports data formats: JSON, CSV, Parquet, Avro. **Technology:** Apache Spark on Kubernetes **Scaling:** Dynamic resource allocation ### Data Lake Long-term data storage with time-travel queries. Retention configurable up to 7 years. **Technology:** S3 with Iceberg tables **Scaling:** Unlimited with tiered storage ## Data Flow Input \u2192 Validation \u2192 Transform \u2192 Enrich \u2192 Output ## Performance Characteristics - **Latency P50:** 100ms (streaming), 5min (batch) - **Latency P99:** 500ms (streaming), 30min (batch) - **Throughput:** 100000 messages/second (streaming)",
              "score": null
            },
            {
              "doc_id": "adr_008",
              "chunk_id": "adr_008_fix_0",
              "content": "# ADR-008: Use S3 for Object Storage **Status:** Accepted **Date:** 2024-10-14 ## Context We need storage for workflow artifacts, execution outputs, and user uploads. ## Options Considered ### Option 1: Amazon S3 Object storage service. **Pros:** - 99.999999999% durability - Lifecycle policies - Event notifications - CDN integration **Cons:** - AWS lock-in - Eventual consistency for overwrites - Cost at scale ### Option 2: Self-hosted MinIO S3-compatible object storage. **Pros:** - No vendor lock-in - S3 compatible - On-premises option **Cons:** - Operational burden - Lower durability guarantees - Scaling complexity ### Option 3: Google Cloud Storage GCP object storage. **Pros:** - Strong consistency - Good performance - Unified billing **Cons:** - GCP lock-in - Fewer features than S3 - Smaller ecosystem ## Decision We will use Amazon S3 for object storage with intelligent tiering for cost optimization. ## Consequences - Enable versioning for critical buckets - Implement lifecycle policies - Use pre-signed URLs for uploads - Enable server-side encryption",
              "score": null
            }
          ],
          "latency_ms": 3.35822400120378,
          "query_id": "simple_005",
          "dimension": "casual",
          "query": "cloudflow k8s or ecs?"
        },
        {
          "key_facts": [
            "Kubernetes",
            "EKS",
            "Helm"
          ],
          "found_facts": [
            "Kubernetes",
            "EKS",
            "Helm"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "adr_002"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "adr_002",
              "chunk_id": "adr_002_fix_0",
              "content": "# ADR-002: Adopt Kubernetes for Container Orchestration **Status:** Accepted **Date:** 2024-01-24 ## Context We need a platform to deploy and scale our microservices. The platform must support auto-scaling, rolling deployments, and multi-region deployment. ## Options Considered ### Option 1: Kubernetes (EKS) Industry-standard container orchestration. **Pros:** - Industry standard - Rich ecosystem - Cloud agnostic - Auto-scaling **Cons:** - Operational complexity - Learning curve - Resource overhead ### Option 2: AWS ECS Amazon's native container service. **Pros:** - Simple operation - Native AWS integration - Lower overhead **Cons:** - AWS lock-in - Less flexible - Smaller community ### Option 3: Serverless (Lambda) Function-as-a-service approach. **Pros:** - Zero infrastructure - Pay per use - Auto-scaling **Cons:** - Cold starts - 15-minute limit - Vendor lock-in ## Decision We will use Kubernetes on EKS for container orchestration with Helm for deployments. ## Consequences - Need platform team expertise - Implement GitOps with ArgoCD - Use managed node groups for easier operations",
              "score": null
            },
            {
              "doc_id": "arch_data_pipeline",
              "chunk_id": "arch_data_pipeline_fix_0",
              "content": "# Data Pipeline Architecture ## Overview The Data Pipeline processes and transforms data between workflow steps. It supports batch processing up to 10GB and streaming for real-time use cases. ## Design Principles - **Exactly-Once Processing**: Each record is processed exactly once - **Backpressure Handling**: System gracefully handles varying load ## Components ### Stream Processor Handles real-time data streams. Maximum throughput of 100000 messages per second. **Technology:** Apache Kafka with Flink **Scaling:** Partition-based scaling ### Batch Processor Processes large batch jobs. Supports data formats: JSON, CSV, Parquet, Avro. **Technology:** Apache Spark on Kubernetes **Scaling:** Dynamic resource allocation ### Data Lake Long-term data storage with time-travel queries. Retention configurable up to 7 years. **Technology:** S3 with Iceberg tables **Scaling:** Unlimited with tiered storage ## Data Flow Input \u2192 Validation \u2192 Transform \u2192 Enrich \u2192 Output ## Performance Characteristics - **Latency P50:** 100ms (streaming), 5min (batch) - **Latency P99:** 500ms (streaming), 30min (batch) - **Throughput:** 100000 messages/second (streaming)",
              "score": null
            },
            {
              "doc_id": "adr_006",
              "chunk_id": "adr_006_fix_0",
              "content": "# ADR-006: Use Terraform for Infrastructure as Code **Status:** Accepted **Date:** 2024-11-24 ## Context We need a way to manage cloud infrastructure consistently across environments. ## Options Considered ### Option 1: Terraform Multi-cloud IaC tool. **Pros:** - Multi-cloud support - Large community - State management - Plan before apply **Cons:** - State file management - Learning curve - HCL limitations ### Option 2: AWS CloudFormation AWS-native IaC. **Pros:** - Native AWS support - No state file - Drift detection **Cons:** - AWS only - Verbose YAML/JSON - Slower development ### Option 3: Pulumi IaC with real programming languages. **Pros:** - Real languages - Type safety - Reusable components **Cons:** - Smaller community - Vendor lock-in - Commercial features ## Decision We will use Terraform with S3 backend for state and DynamoDB for locking. ## Consequences - Implement module structure - Use workspaces for environments - Automate with Atlantis",
              "score": null
            },
            {
              "doc_id": "arch_workflow_engine",
              "chunk_id": "arch_workflow_engine_fix_0",
              "content": "# Workflow Engine Architecture ## Overview The Workflow Engine is the core component responsible for executing user-defined workflows. It uses an event-driven architecture with support for parallel execution, conditional branching, and automatic retries. ## Design Principles - **Idempotency**: All operations can be safely retried without side effects - **Eventual Consistency**: State converges to consistent view within 5 seconds - **Fault Tolerance**: System continues operating despite component failures ## Components ### Scheduler Manages workflow scheduling and triggers. Supports cron expressions, webhooks, and event-based triggers. **Technology:** Custom Go service **Scaling:** Horizontal with leader election ### Executor Runs individual workflow steps in isolated containers. Maximum execution time is 30 minutes per step. **Technology:** Kubernetes Jobs **Scaling:** Auto-scaling based on queue depth ### State Store Persists workflow state, execution history, and step results. Uses optimistic locking for concurrent updates. **Technology:** PostgreSQL with JSONB **Scaling:** Primary-replica with read replicas ## Data Flow Triggers \u2192 Scheduler \u2192 Job Queue \u2192 Executor \u2192 State Store \u2192 Webhooks/Notifications ## Performance Characteristics - **Latency P50:** 45ms - **Latency P99:** 250ms - **Throughput:** 5000 executions/minute",
              "score": null
            },
            {
              "doc_id": "adr_008",
              "chunk_id": "adr_008_fix_0",
              "content": "# ADR-008: Use S3 for Object Storage **Status:** Accepted **Date:** 2024-10-14 ## Context We need storage for workflow artifacts, execution outputs, and user uploads. ## Options Considered ### Option 1: Amazon S3 Object storage service. **Pros:** - 99.999999999% durability - Lifecycle policies - Event notifications - CDN integration **Cons:** - AWS lock-in - Eventual consistency for overwrites - Cost at scale ### Option 2: Self-hosted MinIO S3-compatible object storage. **Pros:** - No vendor lock-in - S3 compatible - On-premises option **Cons:** - Operational burden - Lower durability guarantees - Scaling complexity ### Option 3: Google Cloud Storage GCP object storage. **Pros:** - Strong consistency - Good performance - Unified billing **Cons:** - GCP lock-in - Fewer features than S3 - Smaller ecosystem ## Decision We will use Amazon S3 for object storage with intelligent tiering for cost optimization. ## Consequences - Enable versioning for critical buckets - Implement lifecycle policies - Use pre-signed URLs for uploads - Enable server-side encryption",
              "score": null
            }
          ],
          "latency_ms": 3.3173069987242343,
          "query_id": "simple_005",
          "dimension": "contextual",
          "query": "need to understand the deployment platform for capacity planning"
        },
        {
          "key_facts": [
            "Kubernetes",
            "EKS",
            "Helm"
          ],
          "found_facts": [],
          "missed_facts": [
            "Kubernetes",
            "EKS",
            "Helm"
          ],
          "coverage": 0.0,
          "expected_docs": [
            "adr_002"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "howto_debug_failed_execution",
              "chunk_id": "howto_debug_failed_execution_fix_0",
              "content": "# How to Debug Failed Execution ## Prerequisites - Failed execution ID ## Steps ### Step 1: Get execution details Retrieve the full execution record including step outputs. ```bash cloudflow executions get ex_123 --include-steps ``` ### Step 2: View step logs Check logs for the failed step. ```bash cloudflow executions logs ex_123 --step 3 ``` ### Step 3: Inspect input/output Review the data passed between steps. ```bash cloudflow executions inspect ex_123 --step 3 ``` ### Step 4: Enable debug mode Re-run with verbose logging enabled. ```bash cloudflow workflows execute wf_123 --debug ``` ### Step 5: Check error patterns Look for similar failures in recent executions. ```bash cloudflow executions list --status failed --since 24h ``` ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            },
            {
              "doc_id": "release_3_0_0",
              "chunk_id": "release_3_0_0_fix_0",
              "content": "# Release Notes - v3.0.0 **Release Date:** 2023-10-15 ## New Features - Complete UI redesign - New workflow editor with visual builder - Real-time execution monitoring - Team workspaces ## Bug Fixes - Fixed critical security issue in token validation - Resolved data loss bug in failed migrations ## Breaking Changes - **BREAKING:** Authentication API completely rewritten - **BREAKING:** Legacy webhook format no longer supported - **BREAKING:** Minimum Node.js version now 18 ## Upgrade Instructions ```bash cloudflow upgrade --version 3.0.0 ``` ## Known Issues None at this time.",
              "score": null
            },
            {
              "doc_id": "howto_set_up_database_connection",
              "chunk_id": "howto_set_up_database_connection_fix_0",
              "content": "# How to Set Up Database Connection ## Prerequisites - Database with network access to CloudFlow - Connection credentials ## Steps ### Step 1: Add connection secret Store your database connection string as a secret. ```bash cloudflow secrets set DATABASE_URL postgresql://user:pass@host:5432/db ``` ### Step 2: Configure connection pool Set pool size based on your plan limits. Free tier allows 5 connections, Pro allows 20. > **Note:** Pool exhaustion will queue requests ### Step 3: Test connection Verify the database is accessible from CloudFlow. ```bash cloudflow db test --secret DATABASE_URL ``` ### Step 4: Enable SSL For production, always require SSL connections. ```bash cloudflow db configure --require-ssl ``` ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            },
            {
              "doc_id": "howto_configure_custom_domain",
              "chunk_id": "howto_configure_custom_domain_fix_0",
              "content": "# How to Configure Custom Domain ## Prerequisites - Domain with DNS access - CloudFlow Pro plan or higher ## Steps ### Step 1: Verify domain ownership Add a TXT record to verify domain ownership. ```bash cloudflow domains verify --domain app.example.com ``` ### Step 2: Configure DNS Add a CNAME record pointing to your CloudFlow endpoint. > **Note:** CNAME should point to ingress.cloudflow.io ### Step 3: Enable SSL CloudFlow automatically provisions SSL certificates via Let's Encrypt. ```bash cloudflow domains ssl --domain app.example.com ``` ### Step 4: Set as primary Make this the primary domain for your application. ```bash cloudflow domains set-primary --domain app.example.com ``` ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            },
            {
              "doc_id": "howto_configure_single_sign_on",
              "chunk_id": "howto_configure_single_sign_on_fix_0",
              "content": "# How to Configure Single Sign-On ## Prerequisites - Admin access - Enterprise plan - Identity provider access ## Steps ### Step 1: Choose provider CloudFlow supports SAML 2.0, OpenID Connect, and OAuth 2.0 providers. ### Step 2: Get metadata Download CloudFlow's SAML metadata for your IdP configuration. ```bash cloudflow sso metadata --format xml > cloudflow-metadata.xml ``` ### Step 3: Configure IdP Add CloudFlow as a service provider in your identity provider. > **Note:** ACS URL: https://auth.cloudflow.io/saml/callback ### Step 4: Upload IdP metadata Provide your IdP's metadata to CloudFlow. ```bash cloudflow sso configure --idp-metadata ./idp-metadata.xml ``` ### Step 5: Test login Verify SSO login works correctly. ```bash cloudflow sso test ``` ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            }
          ],
          "latency_ms": 3.9694820006843656,
          "query_id": "simple_005",
          "dimension": "negation",
          "query": "CloudFlow isn't running on serverless is it"
        },
        {
          "key_facts": [
            "300",
            "CACHE_TTL_SECONDS"
          ],
          "found_facts": [
            "300",
            "CACHE_TTL_SECONDS"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "config_cache"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "arch_caching_layer",
              "chunk_id": "arch_caching_layer_fix_0",
              "content": "# Caching Layer Architecture ## Overview The Caching Layer reduces latency and database load by caching frequently accessed data. Uses a multi-tier strategy with local and distributed caches. ## Design Principles - **Cache Invalidation**: Explicit invalidation on writes - **Graceful Degradation**: System works without cache ## Components ### Local Cache First-level cache with 100ms TTL. Reduces Redis round-trips for hot data. **Technology:** Ristretto in-memory cache **Scaling:** Per-pod memory allocation ### Distributed Cache Second-level cache with configurable TTL. Maximum key size 512MB. **Technology:** Redis Cluster 7.x **Scaling:** 6-node cluster with sharding ### CDN Caches static assets and API responses. Cache hit ratio target 95%. **Technology:** CloudFront with Lambda@Edge **Scaling:** Global edge locations ## Data Flow Request \u2192 Local Cache \u2192 Redis \u2192 Database \u2192 Cache Population ## Performance Characteristics - **Latency P50:** 1ms (hit), 50ms (miss) - **Latency P99:** 5ms (hit), 200ms (miss) - **Throughput:** 100000 reads/second",
              "score": null
            },
            {
              "doc_id": "config_cache",
              "chunk_id": "config_cache_fix_0",
              "content": "# Cache Configuration Reference ## Overview This document describes all configuration options for Cache. ## Environment Variables | Variable | Type | Default | Description | |----------|------|---------|-------------| | `REDIS_URL` | string | required | Redis connection string | | `REDIS_POOL_SIZE` | integer | 10 | Connection pool size | | `CACHE_TTL_SECONDS` | integer | 300 | Default cache TTL | | `CACHE_PREFIX` | string | cf: | Key prefix for namespacing | | `CACHE_COMPRESSION` | boolean | true | Enable value compression | | `CACHE_MAX_SIZE_MB` | integer | 100 | Maximum cache size per pod | ## Configuration File Configuration can also be provided via `cache.yaml`: ```yaml redis_url: required redis_pool_size: 10 cache_ttl_seconds: 300 cache_prefix: cf: cache_compression: true ``` ## Validation Configuration is validated at startup. Invalid configuration will prevent the service from starting.",
              "score": null
            },
            {
              "doc_id": "adr_004",
              "chunk_id": "adr_004_fix_0",
              "content": "# ADR-004: Use Redis for Caching and Session Storage **Status:** Accepted **Date:** 2024-04-05 ## Context We need a fast data store for caching API responses, session data, and rate limiting counters. ## Options Considered ### Option 1: Redis Cluster In-memory data structure store. **Pros:** - Sub-millisecond latency - Rich data structures - Pub/sub support - Clustering **Cons:** - Memory cost - Persistence complexity - Cluster management ### Option 2: Memcached Simple key-value cache. **Pros:** - Simple operation - Multi-threaded - Lower memory overhead **Cons:** - No persistence - Limited data types - No clustering ### Option 3: Application-level cache In-process caching only. **Pros:** - No external dependency - Fastest possible - Simple **Cons:** - Not shared across pods - Memory pressure - Cold starts ## Decision We will use Redis Cluster for caching with a 6-node cluster for high availability. ## Consequences - Implement cache-aside pattern - Set appropriate TTLs - Monitor memory usage - Plan for cache stampede prevention",
              "score": null
            },
            {
              "doc_id": "arch_monitoring_stack",
              "chunk_id": "arch_monitoring_stack_fix_0",
              "content": "# Monitoring Stack Architecture ## Overview The Monitoring Stack provides observability into system health, performance, and errors. Includes metrics, logs, traces, and alerting. ## Design Principles - **Observability**: Every component emits metrics and traces - **Actionable Alerts**: Alerts include runbook links ## Components ### Metrics Collects time-series metrics. Retention is 15 days local, 1 year in object storage. **Technology:** Prometheus with Thanos **Scaling:** Federated with long-term storage ### Logging Centralized log aggregation. Supports structured JSON logs with label indexing. **Technology:** Loki with Grafana **Scaling:** Distributed with sharding ### Tracing Distributed request tracing. Trace retention is 7 days. **Technology:** Jaeger with OpenTelemetry **Scaling:** Sampling at 10% ### Alerting Routes alerts to on-call engineers. Supports escalation policies and silencing. **Technology:** Alertmanager with PagerDuty **Scaling:** HA pair ## Data Flow Application \u2192 Collector \u2192 Storage \u2192 Query \u2192 Dashboard/Alert ## Performance Characteristics - **Latency P50:** N/A - **Latency P99:** N/A - **Throughput:** 1M metrics/second, 100GB logs/day",
              "score": null
            },
            {
              "doc_id": "arch_data_pipeline",
              "chunk_id": "arch_data_pipeline_fix_0",
              "content": "# Data Pipeline Architecture ## Overview The Data Pipeline processes and transforms data between workflow steps. It supports batch processing up to 10GB and streaming for real-time use cases. ## Design Principles - **Exactly-Once Processing**: Each record is processed exactly once - **Backpressure Handling**: System gracefully handles varying load ## Components ### Stream Processor Handles real-time data streams. Maximum throughput of 100000 messages per second. **Technology:** Apache Kafka with Flink **Scaling:** Partition-based scaling ### Batch Processor Processes large batch jobs. Supports data formats: JSON, CSV, Parquet, Avro. **Technology:** Apache Spark on Kubernetes **Scaling:** Dynamic resource allocation ### Data Lake Long-term data storage with time-travel queries. Retention configurable up to 7 years. **Technology:** S3 with Iceberg tables **Scaling:** Unlimited with tiered storage ## Data Flow Input \u2192 Validation \u2192 Transform \u2192 Enrich \u2192 Output ## Performance Characteristics - **Latency P50:** 100ms (streaming), 5min (batch) - **Latency P99:** 500ms (streaming), 30min (batch) - **Throughput:** 100000 messages/second (streaming)",
              "score": null
            }
          ],
          "latency_ms": 3.7203780011623167,
          "query_id": "simple_006",
          "dimension": "original",
          "query": "What is the cache TTL?"
        },
        {
          "key_facts": [
            "300",
            "CACHE_TTL_SECONDS"
          ],
          "found_facts": [
            "300",
            "CACHE_TTL_SECONDS"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "config_cache"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "adr_004",
              "chunk_id": "adr_004_fix_0",
              "content": "# ADR-004: Use Redis for Caching and Session Storage **Status:** Accepted **Date:** 2024-04-05 ## Context We need a fast data store for caching API responses, session data, and rate limiting counters. ## Options Considered ### Option 1: Redis Cluster In-memory data structure store. **Pros:** - Sub-millisecond latency - Rich data structures - Pub/sub support - Clustering **Cons:** - Memory cost - Persistence complexity - Cluster management ### Option 2: Memcached Simple key-value cache. **Pros:** - Simple operation - Multi-threaded - Lower memory overhead **Cons:** - No persistence - Limited data types - No clustering ### Option 3: Application-level cache In-process caching only. **Pros:** - No external dependency - Fastest possible - Simple **Cons:** - Not shared across pods - Memory pressure - Cold starts ## Decision We will use Redis Cluster for caching with a 6-node cluster for high availability. ## Consequences - Implement cache-aside pattern - Set appropriate TTLs - Monitor memory usage - Plan for cache stampede prevention",
              "score": null
            },
            {
              "doc_id": "arch_caching_layer",
              "chunk_id": "arch_caching_layer_fix_0",
              "content": "# Caching Layer Architecture ## Overview The Caching Layer reduces latency and database load by caching frequently accessed data. Uses a multi-tier strategy with local and distributed caches. ## Design Principles - **Cache Invalidation**: Explicit invalidation on writes - **Graceful Degradation**: System works without cache ## Components ### Local Cache First-level cache with 100ms TTL. Reduces Redis round-trips for hot data. **Technology:** Ristretto in-memory cache **Scaling:** Per-pod memory allocation ### Distributed Cache Second-level cache with configurable TTL. Maximum key size 512MB. **Technology:** Redis Cluster 7.x **Scaling:** 6-node cluster with sharding ### CDN Caches static assets and API responses. Cache hit ratio target 95%. **Technology:** CloudFront with Lambda@Edge **Scaling:** Global edge locations ## Data Flow Request \u2192 Local Cache \u2192 Redis \u2192 Database \u2192 Cache Population ## Performance Characteristics - **Latency P50:** 1ms (hit), 50ms (miss) - **Latency P99:** 5ms (hit), 200ms (miss) - **Throughput:** 100000 reads/second",
              "score": null
            },
            {
              "doc_id": "config_cache",
              "chunk_id": "config_cache_fix_0",
              "content": "# Cache Configuration Reference ## Overview This document describes all configuration options for Cache. ## Environment Variables | Variable | Type | Default | Description | |----------|------|---------|-------------| | `REDIS_URL` | string | required | Redis connection string | | `REDIS_POOL_SIZE` | integer | 10 | Connection pool size | | `CACHE_TTL_SECONDS` | integer | 300 | Default cache TTL | | `CACHE_PREFIX` | string | cf: | Key prefix for namespacing | | `CACHE_COMPRESSION` | boolean | true | Enable value compression | | `CACHE_MAX_SIZE_MB` | integer | 100 | Maximum cache size per pod | ## Configuration File Configuration can also be provided via `cache.yaml`: ```yaml redis_url: required redis_pool_size: 10 cache_ttl_seconds: 300 cache_prefix: cf: cache_compression: true ``` ## Validation Configuration is validated at startup. Invalid configuration will prevent the service from starting.",
              "score": null
            },
            {
              "doc_id": "arch_data_pipeline",
              "chunk_id": "arch_data_pipeline_fix_0",
              "content": "# Data Pipeline Architecture ## Overview The Data Pipeline processes and transforms data between workflow steps. It supports batch processing up to 10GB and streaming for real-time use cases. ## Design Principles - **Exactly-Once Processing**: Each record is processed exactly once - **Backpressure Handling**: System gracefully handles varying load ## Components ### Stream Processor Handles real-time data streams. Maximum throughput of 100000 messages per second. **Technology:** Apache Kafka with Flink **Scaling:** Partition-based scaling ### Batch Processor Processes large batch jobs. Supports data formats: JSON, CSV, Parquet, Avro. **Technology:** Apache Spark on Kubernetes **Scaling:** Dynamic resource allocation ### Data Lake Long-term data storage with time-travel queries. Retention configurable up to 7 years. **Technology:** S3 with Iceberg tables **Scaling:** Unlimited with tiered storage ## Data Flow Input \u2192 Validation \u2192 Transform \u2192 Enrich \u2192 Output ## Performance Characteristics - **Latency P50:** 100ms (streaming), 5min (batch) - **Latency P99:** 500ms (streaming), 30min (batch) - **Throughput:** 100000 messages/second (streaming)",
              "score": null
            },
            {
              "doc_id": "adr_008",
              "chunk_id": "adr_008_fix_0",
              "content": "# ADR-008: Use S3 for Object Storage **Status:** Accepted **Date:** 2024-10-14 ## Context We need storage for workflow artifacts, execution outputs, and user uploads. ## Options Considered ### Option 1: Amazon S3 Object storage service. **Pros:** - 99.999999999% durability - Lifecycle policies - Event notifications - CDN integration **Cons:** - AWS lock-in - Eventual consistency for overwrites - Cost at scale ### Option 2: Self-hosted MinIO S3-compatible object storage. **Pros:** - No vendor lock-in - S3 compatible - On-premises option **Cons:** - Operational burden - Lower durability guarantees - Scaling complexity ### Option 3: Google Cloud Storage GCP object storage. **Pros:** - Strong consistency - Good performance - Unified billing **Cons:** - GCP lock-in - Fewer features than S3 - Smaller ecosystem ## Decision We will use Amazon S3 for object storage with intelligent tiering for cost optimization. ## Consequences - Enable versioning for critical buckets - Implement lifecycle policies - Use pre-signed URLs for uploads - Enable server-side encryption",
              "score": null
            }
          ],
          "latency_ms": 3.814953999608406,
          "query_id": "simple_006",
          "dimension": "synonym",
          "query": "how long does cached data stay valid"
        },
        {
          "key_facts": [
            "300",
            "CACHE_TTL_SECONDS"
          ],
          "found_facts": [
            "300",
            "CACHE_TTL_SECONDS"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "config_cache"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "arch_caching_layer",
              "chunk_id": "arch_caching_layer_fix_0",
              "content": "# Caching Layer Architecture ## Overview The Caching Layer reduces latency and database load by caching frequently accessed data. Uses a multi-tier strategy with local and distributed caches. ## Design Principles - **Cache Invalidation**: Explicit invalidation on writes - **Graceful Degradation**: System works without cache ## Components ### Local Cache First-level cache with 100ms TTL. Reduces Redis round-trips for hot data. **Technology:** Ristretto in-memory cache **Scaling:** Per-pod memory allocation ### Distributed Cache Second-level cache with configurable TTL. Maximum key size 512MB. **Technology:** Redis Cluster 7.x **Scaling:** 6-node cluster with sharding ### CDN Caches static assets and API responses. Cache hit ratio target 95%. **Technology:** CloudFront with Lambda@Edge **Scaling:** Global edge locations ## Data Flow Request \u2192 Local Cache \u2192 Redis \u2192 Database \u2192 Cache Population ## Performance Characteristics - **Latency P50:** 1ms (hit), 50ms (miss) - **Latency P99:** 5ms (hit), 200ms (miss) - **Throughput:** 100000 reads/second",
              "score": null
            },
            {
              "doc_id": "config_cache",
              "chunk_id": "config_cache_fix_0",
              "content": "# Cache Configuration Reference ## Overview This document describes all configuration options for Cache. ## Environment Variables | Variable | Type | Default | Description | |----------|------|---------|-------------| | `REDIS_URL` | string | required | Redis connection string | | `REDIS_POOL_SIZE` | integer | 10 | Connection pool size | | `CACHE_TTL_SECONDS` | integer | 300 | Default cache TTL | | `CACHE_PREFIX` | string | cf: | Key prefix for namespacing | | `CACHE_COMPRESSION` | boolean | true | Enable value compression | | `CACHE_MAX_SIZE_MB` | integer | 100 | Maximum cache size per pod | ## Configuration File Configuration can also be provided via `cache.yaml`: ```yaml redis_url: required redis_pool_size: 10 cache_ttl_seconds: 300 cache_prefix: cf: cache_compression: true ``` ## Validation Configuration is validated at startup. Invalid configuration will prevent the service from starting.",
              "score": null
            },
            {
              "doc_id": "adr_004",
              "chunk_id": "adr_004_fix_0",
              "content": "# ADR-004: Use Redis for Caching and Session Storage **Status:** Accepted **Date:** 2024-04-05 ## Context We need a fast data store for caching API responses, session data, and rate limiting counters. ## Options Considered ### Option 1: Redis Cluster In-memory data structure store. **Pros:** - Sub-millisecond latency - Rich data structures - Pub/sub support - Clustering **Cons:** - Memory cost - Persistence complexity - Cluster management ### Option 2: Memcached Simple key-value cache. **Pros:** - Simple operation - Multi-threaded - Lower memory overhead **Cons:** - No persistence - Limited data types - No clustering ### Option 3: Application-level cache In-process caching only. **Pros:** - No external dependency - Fastest possible - Simple **Cons:** - Not shared across pods - Memory pressure - Cold starts ## Decision We will use Redis Cluster for caching with a 6-node cluster for high availability. ## Consequences - Implement cache-aside pattern - Set appropriate TTLs - Monitor memory usage - Plan for cache stampede prevention",
              "score": null
            },
            {
              "doc_id": "release_2_9_0",
              "chunk_id": "release_2_9_0_fix_0",
              "content": "# Release Notes - v2.9.0 **Release Date:** 2023-08-01 ## New Features - Parallel step execution - Conditional branching in workflows - Slack integration improvements - Performance dashboard ## Bug Fixes - Fixed issue with special characters in secrets - Resolved timeout in large file uploads ## Upgrade Instructions ```bash cloudflow upgrade --version 2.9.0 ``` ## Known Issues None at this time.",
              "score": null
            },
            {
              "doc_id": "howto_implement_workflow_versioning",
              "chunk_id": "howto_implement_workflow_versioning_fix_0",
              "content": "# How to Implement Workflow Versioning ## Prerequisites - Existing workflow ## Steps ### Step 1: Enable versioning Turn on versioning for your workflow. This is required for the feature. ```bash cloudflow workflows update wf_123 --enable-versioning ``` ### Step 2: Create new version Save changes as a new version instead of overwriting. ```bash cloudflow workflows publish wf_123 --version 2.0.0 ``` ### Step 3: Set active version Choose which version handles new executions. ```bash cloudflow workflows activate wf_123 --version 2.0.0 ``` ### Step 4: Roll back if needed Quickly revert to a previous version. ```bash cloudflow workflows rollback wf_123 --to-version 1.0.0 ``` ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            }
          ],
          "latency_ms": 3.620662999310298,
          "query_id": "simple_006",
          "dimension": "problem",
          "query": "changes aren't showing up immediately, probably cached - how long til it refreshes"
        },
        {
          "key_facts": [
            "300",
            "CACHE_TTL_SECONDS"
          ],
          "found_facts": [
            "300",
            "CACHE_TTL_SECONDS"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "config_cache"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "adr_004",
              "chunk_id": "adr_004_fix_0",
              "content": "# ADR-004: Use Redis for Caching and Session Storage **Status:** Accepted **Date:** 2024-04-05 ## Context We need a fast data store for caching API responses, session data, and rate limiting counters. ## Options Considered ### Option 1: Redis Cluster In-memory data structure store. **Pros:** - Sub-millisecond latency - Rich data structures - Pub/sub support - Clustering **Cons:** - Memory cost - Persistence complexity - Cluster management ### Option 2: Memcached Simple key-value cache. **Pros:** - Simple operation - Multi-threaded - Lower memory overhead **Cons:** - No persistence - Limited data types - No clustering ### Option 3: Application-level cache In-process caching only. **Pros:** - No external dependency - Fastest possible - Simple **Cons:** - Not shared across pods - Memory pressure - Cold starts ## Decision We will use Redis Cluster for caching with a 6-node cluster for high availability. ## Consequences - Implement cache-aside pattern - Set appropriate TTLs - Monitor memory usage - Plan for cache stampede prevention",
              "score": null
            },
            {
              "doc_id": "arch_caching_layer",
              "chunk_id": "arch_caching_layer_fix_0",
              "content": "# Caching Layer Architecture ## Overview The Caching Layer reduces latency and database load by caching frequently accessed data. Uses a multi-tier strategy with local and distributed caches. ## Design Principles - **Cache Invalidation**: Explicit invalidation on writes - **Graceful Degradation**: System works without cache ## Components ### Local Cache First-level cache with 100ms TTL. Reduces Redis round-trips for hot data. **Technology:** Ristretto in-memory cache **Scaling:** Per-pod memory allocation ### Distributed Cache Second-level cache with configurable TTL. Maximum key size 512MB. **Technology:** Redis Cluster 7.x **Scaling:** 6-node cluster with sharding ### CDN Caches static assets and API responses. Cache hit ratio target 95%. **Technology:** CloudFront with Lambda@Edge **Scaling:** Global edge locations ## Data Flow Request \u2192 Local Cache \u2192 Redis \u2192 Database \u2192 Cache Population ## Performance Characteristics - **Latency P50:** 1ms (hit), 50ms (miss) - **Latency P99:** 5ms (hit), 200ms (miss) - **Throughput:** 100000 reads/second",
              "score": null
            },
            {
              "doc_id": "config_cache",
              "chunk_id": "config_cache_fix_0",
              "content": "# Cache Configuration Reference ## Overview This document describes all configuration options for Cache. ## Environment Variables | Variable | Type | Default | Description | |----------|------|---------|-------------| | `REDIS_URL` | string | required | Redis connection string | | `REDIS_POOL_SIZE` | integer | 10 | Connection pool size | | `CACHE_TTL_SECONDS` | integer | 300 | Default cache TTL | | `CACHE_PREFIX` | string | cf: | Key prefix for namespacing | | `CACHE_COMPRESSION` | boolean | true | Enable value compression | | `CACHE_MAX_SIZE_MB` | integer | 100 | Maximum cache size per pod | ## Configuration File Configuration can also be provided via `cache.yaml`: ```yaml redis_url: required redis_pool_size: 10 cache_ttl_seconds: 300 cache_prefix: cf: cache_compression: true ``` ## Validation Configuration is validated at startup. Invalid configuration will prevent the service from starting.",
              "score": null
            },
            {
              "doc_id": "arch_authentication_service",
              "chunk_id": "arch_authentication_service_fix_0",
              "content": "# Authentication Service Architecture ## Overview The Authentication Service handles user identity, session management, and token issuance. Supports OAuth 2.0, SAML, and API key authentication. ## Design Principles - **Zero Knowledge**: Passwords never stored in plaintext - **Token Rotation**: Refresh tokens rotate on each use ## Components ### Identity Provider Manages user accounts and credentials. Password hashing uses bcrypt with cost factor 12. **Technology:** Custom Go service **Scaling:** Horizontal stateless ### Token Service Issues and validates access tokens. Token lifetime is 1 hour with 7-day refresh tokens. **Technology:** JWT with RS256 **Scaling:** Horizontal stateless ### SSO Gateway Integrates with external identity providers. Supports Google, Microsoft, Okta, and custom SAML. **Technology:** Dex OIDC proxy **Scaling:** Active-passive ## Data Flow Credentials \u2192 Validation \u2192 Token Issue \u2192 Token Refresh \u2192 Logout ## Performance Characteristics - **Latency P50:** 50ms - **Latency P99:** 200ms - **Throughput:** 5000 auth/second",
              "score": null
            },
            {
              "doc_id": "arch_data_pipeline",
              "chunk_id": "arch_data_pipeline_fix_0",
              "content": "# Data Pipeline Architecture ## Overview The Data Pipeline processes and transforms data between workflow steps. It supports batch processing up to 10GB and streaming for real-time use cases. ## Design Principles - **Exactly-Once Processing**: Each record is processed exactly once - **Backpressure Handling**: System gracefully handles varying load ## Components ### Stream Processor Handles real-time data streams. Maximum throughput of 100000 messages per second. **Technology:** Apache Kafka with Flink **Scaling:** Partition-based scaling ### Batch Processor Processes large batch jobs. Supports data formats: JSON, CSV, Parquet, Avro. **Technology:** Apache Spark on Kubernetes **Scaling:** Dynamic resource allocation ### Data Lake Long-term data storage with time-travel queries. Retention configurable up to 7 years. **Technology:** S3 with Iceberg tables **Scaling:** Unlimited with tiered storage ## Data Flow Input \u2192 Validation \u2192 Transform \u2192 Enrich \u2192 Output ## Performance Characteristics - **Latency P50:** 100ms (streaming), 5min (batch) - **Latency P99:** 500ms (streaming), 30min (batch) - **Throughput:** 100000 messages/second (streaming)",
              "score": null
            }
          ],
          "latency_ms": 3.5706199996639043,
          "query_id": "simple_006",
          "dimension": "casual",
          "query": "cache expiry time"
        },
        {
          "key_facts": [
            "300",
            "CACHE_TTL_SECONDS"
          ],
          "found_facts": [
            "300",
            "CACHE_TTL_SECONDS"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "config_cache"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "adr_004",
              "chunk_id": "adr_004_fix_0",
              "content": "# ADR-004: Use Redis for Caching and Session Storage **Status:** Accepted **Date:** 2024-04-05 ## Context We need a fast data store for caching API responses, session data, and rate limiting counters. ## Options Considered ### Option 1: Redis Cluster In-memory data structure store. **Pros:** - Sub-millisecond latency - Rich data structures - Pub/sub support - Clustering **Cons:** - Memory cost - Persistence complexity - Cluster management ### Option 2: Memcached Simple key-value cache. **Pros:** - Simple operation - Multi-threaded - Lower memory overhead **Cons:** - No persistence - Limited data types - No clustering ### Option 3: Application-level cache In-process caching only. **Pros:** - No external dependency - Fastest possible - Simple **Cons:** - Not shared across pods - Memory pressure - Cold starts ## Decision We will use Redis Cluster for caching with a 6-node cluster for high availability. ## Consequences - Implement cache-aside pattern - Set appropriate TTLs - Monitor memory usage - Plan for cache stampede prevention",
              "score": null
            },
            {
              "doc_id": "arch_caching_layer",
              "chunk_id": "arch_caching_layer_fix_0",
              "content": "# Caching Layer Architecture ## Overview The Caching Layer reduces latency and database load by caching frequently accessed data. Uses a multi-tier strategy with local and distributed caches. ## Design Principles - **Cache Invalidation**: Explicit invalidation on writes - **Graceful Degradation**: System works without cache ## Components ### Local Cache First-level cache with 100ms TTL. Reduces Redis round-trips for hot data. **Technology:** Ristretto in-memory cache **Scaling:** Per-pod memory allocation ### Distributed Cache Second-level cache with configurable TTL. Maximum key size 512MB. **Technology:** Redis Cluster 7.x **Scaling:** 6-node cluster with sharding ### CDN Caches static assets and API responses. Cache hit ratio target 95%. **Technology:** CloudFront with Lambda@Edge **Scaling:** Global edge locations ## Data Flow Request \u2192 Local Cache \u2192 Redis \u2192 Database \u2192 Cache Population ## Performance Characteristics - **Latency P50:** 1ms (hit), 50ms (miss) - **Latency P99:** 5ms (hit), 200ms (miss) - **Throughput:** 100000 reads/second",
              "score": null
            },
            {
              "doc_id": "config_cache",
              "chunk_id": "config_cache_fix_0",
              "content": "# Cache Configuration Reference ## Overview This document describes all configuration options for Cache. ## Environment Variables | Variable | Type | Default | Description | |----------|------|---------|-------------| | `REDIS_URL` | string | required | Redis connection string | | `REDIS_POOL_SIZE` | integer | 10 | Connection pool size | | `CACHE_TTL_SECONDS` | integer | 300 | Default cache TTL | | `CACHE_PREFIX` | string | cf: | Key prefix for namespacing | | `CACHE_COMPRESSION` | boolean | true | Enable value compression | | `CACHE_MAX_SIZE_MB` | integer | 100 | Maximum cache size per pod | ## Configuration File Configuration can also be provided via `cache.yaml`: ```yaml redis_url: required redis_pool_size: 10 cache_ttl_seconds: 300 cache_prefix: cf: cache_compression: true ``` ## Validation Configuration is validated at startup. Invalid configuration will prevent the service from starting.",
              "score": null
            },
            {
              "doc_id": "runbook_database_connection_exhaustion",
              "chunk_id": "runbook_database_connection_exhaustion_fix_0",
              "content": "# Runbook: Database Connection Exhaustion ## Severity **Level:** P1 - Critical **Response Time:** 15 minutes ## Symptoms - Connection pool at 100% - Queries timing out - 'too many connections' errors in logs ## Diagnosis ### Step 1 Check active connections: `SELECT count(*) FROM pg_stat_activity` ### Step 2 Identify long-running queries: `SELECT pid, now() - pg_stat_activity.query_start AS duration, query FROM pg_stat_activity WHERE state != 'idle' ORDER BY duration DESC` ### Step 3 Check PgBouncer stats: `SHOW POOLS` ### Step 4 Verify connection limits: `SHOW max_connections` ## Resolution ### Step 1 Kill long-running queries: `SELECT pg_terminate_backend(pid) FROM pg_stat_activity WHERE duration > interval '5 minutes'` ### Step 2 Increase PgBouncer pool size temporarily: update pgbouncer.ini ### Step 3 Restart affected services to release connections: `kubectl rollout restart deployment/api` ### Step 4 Scale down non-critical services to free connections ## Escalation If connections remain exhausted, escalate to DBA. Consider enabling connection queueing. ## Post-Incident - Document timeline in incident log - Schedule post-mortem within 48 hours - Update runbook if new patterns discovered",
              "score": null
            },
            {
              "doc_id": "runbook_kafka_consumer_lag",
              "chunk_id": "runbook_kafka_consumer_lag_fix_0",
              "content": "# Runbook: Kafka Consumer Lag ## Severity **Level:** P1 - Critical **Response Time:** 1 hour ## Symptoms - Consumer lag > 10000 messages - Processing delays reported - Alerts from lag monitor ## Diagnosis ### Step 1 Check consumer group lag: `kafka-consumer-groups.sh --bootstrap-server kafka:9092 --describe --group workflow-executor` ### Step 2 Verify consumer health: `kubectl get pods -l app=workflow-executor -n production` ### Step 3 Check for processing errors: `kubectl logs -l app=workflow-executor --tail=100 -n production | grep ERROR` ### Step 4 Monitor throughput: `cloudflow metrics get kafka.consumer.records_per_second` ## Resolution ### Step 1 Scale up consumers: `kubectl scale deployment/workflow-executor --replicas=10 -n production` ### Step 2 Reset offset if messages are stale: `kafka-consumer-groups.sh --bootstrap-server kafka:9092 --group workflow-executor --reset-offsets --to-latest --execute --topic workflows` ### Step 3 Pause producer if backpressure needed: `cloudflow workflows pause-triggers` ### Step 4 Increase partition count for parallelism (requires coordination) ## Escalation If lag persists > 30 minutes, page platform team. May need Kafka cluster scaling. ## Post-Incident - Document timeline in incident log - Schedule post-mortem within 48 hours - Update runbook if new patterns discovered",
              "score": null
            }
          ],
          "latency_ms": 3.5638370009110076,
          "query_id": "simple_006",
          "dimension": "contextual",
          "query": "debugging stale data issues, need to know the default cache duration"
        },
        {
          "key_facts": [
            "300",
            "CACHE_TTL_SECONDS"
          ],
          "found_facts": [],
          "missed_facts": [
            "300",
            "CACHE_TTL_SECONDS"
          ],
          "coverage": 0.0,
          "expected_docs": [
            "config_cache"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "runbook_database_connection_exhaustion",
              "chunk_id": "runbook_database_connection_exhaustion_fix_0",
              "content": "# Runbook: Database Connection Exhaustion ## Severity **Level:** P1 - Critical **Response Time:** 15 minutes ## Symptoms - Connection pool at 100% - Queries timing out - 'too many connections' errors in logs ## Diagnosis ### Step 1 Check active connections: `SELECT count(*) FROM pg_stat_activity` ### Step 2 Identify long-running queries: `SELECT pid, now() - pg_stat_activity.query_start AS duration, query FROM pg_stat_activity WHERE state != 'idle' ORDER BY duration DESC` ### Step 3 Check PgBouncer stats: `SHOW POOLS` ### Step 4 Verify connection limits: `SHOW max_connections` ## Resolution ### Step 1 Kill long-running queries: `SELECT pg_terminate_backend(pid) FROM pg_stat_activity WHERE duration > interval '5 minutes'` ### Step 2 Increase PgBouncer pool size temporarily: update pgbouncer.ini ### Step 3 Restart affected services to release connections: `kubectl rollout restart deployment/api` ### Step 4 Scale down non-critical services to free connections ## Escalation If connections remain exhausted, escalate to DBA. Consider enabling connection queueing. ## Post-Incident - Document timeline in incident log - Schedule post-mortem within 48 hours - Update runbook if new patterns discovered",
              "score": null
            },
            {
              "doc_id": "api_users",
              "chunk_id": "api_users_fix_0",
              "content": "# Users API Reference ## Overview The Users API provides programmatic access to users functionality. Base URL: `https://api.cloudflow.io/v1/users` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /users/me Get current user profile. **Parameters:** **Response:** - Success: `200` - Returns user object - Error: `400` - Bad Request **Example:** ```json {\"id\": \"usr_001\", \"email\": \"user@example.com\", \"role\": \"admin\", \"created_at\": \"2023-06-01\"} ``` ### PATCH /users/me Update current user profile. **Parameters:** - `name` (string): Display name - `timezone` (string): Timezone (e.g., America/New_York) **Response:** - Success: `200` - User updated - Error: `400` - Bad Request **Example:** ```json {\"id\": \"usr_001\", \"name\": \"John Doe\", \"timezone\": \"America/New_York\"} ```",
              "score": null
            },
            {
              "doc_id": "runbook_kafka_consumer_lag",
              "chunk_id": "runbook_kafka_consumer_lag_fix_0",
              "content": "# Runbook: Kafka Consumer Lag ## Severity **Level:** P1 - Critical **Response Time:** 1 hour ## Symptoms - Consumer lag > 10000 messages - Processing delays reported - Alerts from lag monitor ## Diagnosis ### Step 1 Check consumer group lag: `kafka-consumer-groups.sh --bootstrap-server kafka:9092 --describe --group workflow-executor` ### Step 2 Verify consumer health: `kubectl get pods -l app=workflow-executor -n production` ### Step 3 Check for processing errors: `kubectl logs -l app=workflow-executor --tail=100 -n production | grep ERROR` ### Step 4 Monitor throughput: `cloudflow metrics get kafka.consumer.records_per_second` ## Resolution ### Step 1 Scale up consumers: `kubectl scale deployment/workflow-executor --replicas=10 -n production` ### Step 2 Reset offset if messages are stale: `kafka-consumer-groups.sh --bootstrap-server kafka:9092 --group workflow-executor --reset-offsets --to-latest --execute --topic workflows` ### Step 3 Pause producer if backpressure needed: `cloudflow workflows pause-triggers` ### Step 4 Increase partition count for parallelism (requires coordination) ## Escalation If lag persists > 30 minutes, page platform team. May need Kafka cluster scaling. ## Post-Incident - Document timeline in incident log - Schedule post-mortem within 48 hours - Update runbook if new patterns discovered",
              "score": null
            },
            {
              "doc_id": "runbook_high_cpu_usage",
              "chunk_id": "runbook_high_cpu_usage_fix_0",
              "content": "# Runbook: High CPU Usage ## Severity **Level:** P1 - Critical **Response Time:** 15 minutes ## Symptoms - CPU utilization > 80% sustained - Increased response latency - Autoscaler at maximum replicas ## Diagnosis ### Step 1 Check current CPU usage: `kubectl top pods -n production` ### Step 2 Identify hot pods: `kubectl top pods --sort-by=cpu -n production | head -10` ### Step 3 Check for recent deployments: `kubectl rollout history deployment/api -n production` ### Step 4 Profile the application: `cloudflow debug profile --pod api-xyz --duration 60s` ## Resolution ### Step 1 Scale up if traffic-related: `kubectl scale deployment/api --replicas=10 -n production` ### Step 2 Restart pods if memory leak suspected: `kubectl rollout restart deployment/api -n production` ### Step 3 Roll back recent deployment if regression: `kubectl rollout undo deployment/api -n production` ### Step 4 Add resource limits if unbounded: update deployment with CPU limits ## Escalation If CPU remains high after scaling, escalate to platform team. Page on-call if affecting users. ## Post-Incident - Document timeline in incident log - Schedule post-mortem within 48 hours - Update runbook if new patterns discovered",
              "score": null
            },
            {
              "doc_id": "arch_caching_layer",
              "chunk_id": "arch_caching_layer_fix_0",
              "content": "# Caching Layer Architecture ## Overview The Caching Layer reduces latency and database load by caching frequently accessed data. Uses a multi-tier strategy with local and distributed caches. ## Design Principles - **Cache Invalidation**: Explicit invalidation on writes - **Graceful Degradation**: System works without cache ## Components ### Local Cache First-level cache with 100ms TTL. Reduces Redis round-trips for hot data. **Technology:** Ristretto in-memory cache **Scaling:** Per-pod memory allocation ### Distributed Cache Second-level cache with configurable TTL. Maximum key size 512MB. **Technology:** Redis Cluster 7.x **Scaling:** 6-node cluster with sharding ### CDN Caches static assets and API responses. Cache hit ratio target 95%. **Technology:** CloudFront with Lambda@Edge **Scaling:** Global edge locations ## Data Flow Request \u2192 Local Cache \u2192 Redis \u2192 Database \u2192 Cache Population ## Performance Characteristics - **Latency P50:** 1ms (hit), 50ms (miss) - **Latency P99:** 5ms (hit), 200ms (miss) - **Throughput:** 100000 reads/second",
              "score": null
            }
          ],
          "latency_ms": 3.286389999630046,
          "query_id": "simple_006",
          "dimension": "negation",
          "query": "why isn't my data updating right away"
        },
        {
          "key_facts": [
            "AES-256",
            "encryption at rest"
          ],
          "found_facts": [
            "AES-256",
            "encryption at rest"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "howto_set_up_data_encryption"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "howto_set_up_data_encryption",
              "chunk_id": "howto_set_up_data_encryption_fix_0",
              "content": "# How to Set Up Data Encryption ## Prerequisites - CloudFlow Enterprise plan ## Steps ### Step 1: Enable encryption at rest All data is encrypted at rest using AES-256 by default. > **Note:** Encryption is automatic for all plans ### Step 2: Configure KMS Bring your own encryption keys using AWS KMS or GCP KMS. ```bash cloudflow encryption configure --kms-key arn:aws:kms:... ``` ### Step 3: Enable field-level encryption Encrypt sensitive fields within your workflow data. ```bash cloudflow encryption add-field --path $.user.ssn --key-alias sensitive-data ``` ### Step 4: Verify encryption Confirm encryption is active for your workspace. ```bash cloudflow encryption status ``` ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            },
            {
              "doc_id": "api_secrets",
              "chunk_id": "api_secrets_fix_0",
              "content": "# Secrets API Reference ## Overview The Secrets API provides programmatic access to secrets functionality. Base URL: `https://api.cloudflow.io/v1/secrets` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /secrets List secret names (values not returned). **Parameters:** **Response:** - Success: `200` - Returns array of secret metadata - Error: `400` - Bad Request **Example:** ```json {\"secrets\": [{\"name\": \"DATABASE_URL\", \"created_at\": \"2024-01-01\", \"updated_at\": \"2024-01-10\"}]} ``` ### PUT /secrets/{name} Create or update a secret. **Parameters:** - `value` (string): Secret value (encrypted at rest) **Response:** - Success: `200` - Secret saved - Error: `400` - Bad Request **Example:** ```json {\"name\": \"DATABASE_URL\", \"updated_at\": \"2024-01-15T10:00:00Z\"} ``` ### DELETE /secrets/{name} Delete a secret. **Parameters:** **Response:** - Success: `204` - Secret deleted - Error: `400` - Bad Request **Example:** ```json {} ```",
              "score": null
            },
            {
              "doc_id": "arch_data_pipeline",
              "chunk_id": "arch_data_pipeline_fix_0",
              "content": "# Data Pipeline Architecture ## Overview The Data Pipeline processes and transforms data between workflow steps. It supports batch processing up to 10GB and streaming for real-time use cases. ## Design Principles - **Exactly-Once Processing**: Each record is processed exactly once - **Backpressure Handling**: System gracefully handles varying load ## Components ### Stream Processor Handles real-time data streams. Maximum throughput of 100000 messages per second. **Technology:** Apache Kafka with Flink **Scaling:** Partition-based scaling ### Batch Processor Processes large batch jobs. Supports data formats: JSON, CSV, Parquet, Avro. **Technology:** Apache Spark on Kubernetes **Scaling:** Dynamic resource allocation ### Data Lake Long-term data storage with time-travel queries. Retention configurable up to 7 years. **Technology:** S3 with Iceberg tables **Scaling:** Unlimited with tiered storage ## Data Flow Input \u2192 Validation \u2192 Transform \u2192 Enrich \u2192 Output ## Performance Characteristics - **Latency P50:** 100ms (streaming), 5min (batch) - **Latency P99:** 500ms (streaming), 30min (batch) - **Throughput:** 100000 messages/second (streaming)",
              "score": null
            },
            {
              "doc_id": "arch_authentication_service",
              "chunk_id": "arch_authentication_service_fix_0",
              "content": "# Authentication Service Architecture ## Overview The Authentication Service handles user identity, session management, and token issuance. Supports OAuth 2.0, SAML, and API key authentication. ## Design Principles - **Zero Knowledge**: Passwords never stored in plaintext - **Token Rotation**: Refresh tokens rotate on each use ## Components ### Identity Provider Manages user accounts and credentials. Password hashing uses bcrypt with cost factor 12. **Technology:** Custom Go service **Scaling:** Horizontal stateless ### Token Service Issues and validates access tokens. Token lifetime is 1 hour with 7-day refresh tokens. **Technology:** JWT with RS256 **Scaling:** Horizontal stateless ### SSO Gateway Integrates with external identity providers. Supports Google, Microsoft, Okta, and custom SAML. **Technology:** Dex OIDC proxy **Scaling:** Active-passive ## Data Flow Credentials \u2192 Validation \u2192 Token Issue \u2192 Token Refresh \u2192 Logout ## Performance Characteristics - **Latency P50:** 50ms - **Latency P99:** 200ms - **Throughput:** 5000 auth/second",
              "score": null
            },
            {
              "doc_id": "api_workflows",
              "chunk_id": "api_workflows_fix_0",
              "content": "# Workflows API Reference ## Overview The Workflows API provides programmatic access to workflows functionality. Base URL: `https://api.cloudflow.io/v1/workflows` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /workflows List all workflows for the authenticated user. **Parameters:** - `limit` (integer): Maximum number of results (default: 50, max: 200) - `status` (string): Filter by status: active, paused, archived **Response:** - Success: `200` - Returns array of workflow objects - Error: `400` - Bad Request **Example:** ```json {\"workflows\": [{\"id\": \"wf_123\", \"name\": \"Daily Report\", \"status\": \"active\"}]} ``` ### POST /workflows Create a new workflow. **Parameters:** - `name` (string): Workflow name (required) - `definition` (object): Workflow definition in JSON format **Response:** - Success: `201` - Workflow created successfully - Error: `400` - Bad Request **Example:** ```json {\"id\": \"wf_456\", \"name\": \"New Workflow\", \"created_at\": \"2024-01-15T10:00:00Z\"} ``` ### DELETE /workflows/{id} Delete a workflow permanently. **Parameters:** - `id` (string): Workflow ID (required) **Response:** - Success: `204` - Workflow deleted - Error: `404` - Workflow not found **Example:** ```json {} ```",
              "score": null
            }
          ],
          "latency_ms": 3.58374400093453,
          "query_id": "simple_007",
          "dimension": "original",
          "query": "What encryption is used for data at rest?"
        },
        {
          "key_facts": [
            "AES-256",
            "encryption at rest"
          ],
          "found_facts": [
            "AES-256",
            "encryption at rest"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "howto_set_up_data_encryption"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "howto_set_up_data_encryption",
              "chunk_id": "howto_set_up_data_encryption_fix_0",
              "content": "# How to Set Up Data Encryption ## Prerequisites - CloudFlow Enterprise plan ## Steps ### Step 1: Enable encryption at rest All data is encrypted at rest using AES-256 by default. > **Note:** Encryption is automatic for all plans ### Step 2: Configure KMS Bring your own encryption keys using AWS KMS or GCP KMS. ```bash cloudflow encryption configure --kms-key arn:aws:kms:... ``` ### Step 3: Enable field-level encryption Encrypt sensitive fields within your workflow data. ```bash cloudflow encryption add-field --path $.user.ssn --key-alias sensitive-data ``` ### Step 4: Verify encryption Confirm encryption is active for your workspace. ```bash cloudflow encryption status ``` ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            },
            {
              "doc_id": "api_secrets",
              "chunk_id": "api_secrets_fix_0",
              "content": "# Secrets API Reference ## Overview The Secrets API provides programmatic access to secrets functionality. Base URL: `https://api.cloudflow.io/v1/secrets` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /secrets List secret names (values not returned). **Parameters:** **Response:** - Success: `200` - Returns array of secret metadata - Error: `400` - Bad Request **Example:** ```json {\"secrets\": [{\"name\": \"DATABASE_URL\", \"created_at\": \"2024-01-01\", \"updated_at\": \"2024-01-10\"}]} ``` ### PUT /secrets/{name} Create or update a secret. **Parameters:** - `value` (string): Secret value (encrypted at rest) **Response:** - Success: `200` - Secret saved - Error: `400` - Bad Request **Example:** ```json {\"name\": \"DATABASE_URL\", \"updated_at\": \"2024-01-15T10:00:00Z\"} ``` ### DELETE /secrets/{name} Delete a secret. **Parameters:** **Response:** - Success: `204` - Secret deleted - Error: `400` - Bad Request **Example:** ```json {} ```",
              "score": null
            },
            {
              "doc_id": "howto_configure_custom_domain",
              "chunk_id": "howto_configure_custom_domain_fix_0",
              "content": "# How to Configure Custom Domain ## Prerequisites - Domain with DNS access - CloudFlow Pro plan or higher ## Steps ### Step 1: Verify domain ownership Add a TXT record to verify domain ownership. ```bash cloudflow domains verify --domain app.example.com ``` ### Step 2: Configure DNS Add a CNAME record pointing to your CloudFlow endpoint. > **Note:** CNAME should point to ingress.cloudflow.io ### Step 3: Enable SSL CloudFlow automatically provisions SSL certificates via Let's Encrypt. ```bash cloudflow domains ssl --domain app.example.com ``` ### Step 4: Set as primary Make this the primary domain for your application. ```bash cloudflow domains set-primary --domain app.example.com ``` ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            },
            {
              "doc_id": "howto_create_api_integration",
              "chunk_id": "howto_create_api_integration_fix_0",
              "content": "# How to Create API Integration ## Prerequisites - CloudFlow account ## Steps ### Step 1: Generate API key Create an API key for programmatic access. ```bash cloudflow apikeys create --name 'CI Integration' --scopes workflows:read,workflows:execute ``` ### Step 2: Store key securely Never commit API keys to source control. Use environment variables or secret management. > **Note:** Rotate keys every 90 days ### Step 3: Test authentication Verify the API key works correctly. ```bash curl -H 'Authorization: Bearer cf_...' https://api.cloudflow.io/v1/me ``` ### Step 4: Implement rate limiting Handle rate limit responses (HTTP 429) with exponential backoff. > **Note:** Default limit is 100 requests per minute ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            },
            {
              "doc_id": "howto_set_up_database_connection",
              "chunk_id": "howto_set_up_database_connection_fix_0",
              "content": "# How to Set Up Database Connection ## Prerequisites - Database with network access to CloudFlow - Connection credentials ## Steps ### Step 1: Add connection secret Store your database connection string as a secret. ```bash cloudflow secrets set DATABASE_URL postgresql://user:pass@host:5432/db ``` ### Step 2: Configure connection pool Set pool size based on your plan limits. Free tier allows 5 connections, Pro allows 20. > **Note:** Pool exhaustion will queue requests ### Step 3: Test connection Verify the database is accessible from CloudFlow. ```bash cloudflow db test --secret DATABASE_URL ``` ### Step 4: Enable SSL For production, always require SSL connections. ```bash cloudflow db configure --require-ssl ``` ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            }
          ],
          "latency_ms": 4.053158001624979,
          "query_id": "simple_007",
          "dimension": "synonym",
          "query": "what cipher does CloudFlow use for stored data"
        },
        {
          "key_facts": [
            "AES-256",
            "encryption at rest"
          ],
          "found_facts": [
            "AES-256",
            "encryption at rest"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "howto_set_up_data_encryption"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "howto_set_up_data_encryption",
              "chunk_id": "howto_set_up_data_encryption_fix_0",
              "content": "# How to Set Up Data Encryption ## Prerequisites - CloudFlow Enterprise plan ## Steps ### Step 1: Enable encryption at rest All data is encrypted at rest using AES-256 by default. > **Note:** Encryption is automatic for all plans ### Step 2: Configure KMS Bring your own encryption keys using AWS KMS or GCP KMS. ```bash cloudflow encryption configure --kms-key arn:aws:kms:... ``` ### Step 3: Enable field-level encryption Encrypt sensitive fields within your workflow data. ```bash cloudflow encryption add-field --path $.user.ssn --key-alias sensitive-data ``` ### Step 4: Verify encryption Confirm encryption is active for your workspace. ```bash cloudflow encryption status ``` ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            },
            {
              "doc_id": "adr_008",
              "chunk_id": "adr_008_fix_0",
              "content": "# ADR-008: Use S3 for Object Storage **Status:** Accepted **Date:** 2024-10-14 ## Context We need storage for workflow artifacts, execution outputs, and user uploads. ## Options Considered ### Option 1: Amazon S3 Object storage service. **Pros:** - 99.999999999% durability - Lifecycle policies - Event notifications - CDN integration **Cons:** - AWS lock-in - Eventual consistency for overwrites - Cost at scale ### Option 2: Self-hosted MinIO S3-compatible object storage. **Pros:** - No vendor lock-in - S3 compatible - On-premises option **Cons:** - Operational burden - Lower durability guarantees - Scaling complexity ### Option 3: Google Cloud Storage GCP object storage. **Pros:** - Strong consistency - Good performance - Unified billing **Cons:** - GCP lock-in - Fewer features than S3 - Smaller ecosystem ## Decision We will use Amazon S3 for object storage with intelligent tiering for cost optimization. ## Consequences - Enable versioning for critical buckets - Implement lifecycle policies - Use pre-signed URLs for uploads - Enable server-side encryption",
              "score": null
            },
            {
              "doc_id": "adr_006",
              "chunk_id": "adr_006_fix_0",
              "content": "# ADR-006: Use Terraform for Infrastructure as Code **Status:** Accepted **Date:** 2024-11-24 ## Context We need a way to manage cloud infrastructure consistently across environments. ## Options Considered ### Option 1: Terraform Multi-cloud IaC tool. **Pros:** - Multi-cloud support - Large community - State management - Plan before apply **Cons:** - State file management - Learning curve - HCL limitations ### Option 2: AWS CloudFormation AWS-native IaC. **Pros:** - Native AWS support - No state file - Drift detection **Cons:** - AWS only - Verbose YAML/JSON - Slower development ### Option 3: Pulumi IaC with real programming languages. **Pros:** - Real languages - Type safety - Reusable components **Cons:** - Smaller community - Vendor lock-in - Commercial features ## Decision We will use Terraform with S3 backend for state and DynamoDB for locking. ## Consequences - Implement module structure - Use workspaces for environments - Automate with Atlantis",
              "score": null
            },
            {
              "doc_id": "api_secrets",
              "chunk_id": "api_secrets_fix_0",
              "content": "# Secrets API Reference ## Overview The Secrets API provides programmatic access to secrets functionality. Base URL: `https://api.cloudflow.io/v1/secrets` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /secrets List secret names (values not returned). **Parameters:** **Response:** - Success: `200` - Returns array of secret metadata - Error: `400` - Bad Request **Example:** ```json {\"secrets\": [{\"name\": \"DATABASE_URL\", \"created_at\": \"2024-01-01\", \"updated_at\": \"2024-01-10\"}]} ``` ### PUT /secrets/{name} Create or update a secret. **Parameters:** - `value` (string): Secret value (encrypted at rest) **Response:** - Success: `200` - Secret saved - Error: `400` - Bad Request **Example:** ```json {\"name\": \"DATABASE_URL\", \"updated_at\": \"2024-01-15T10:00:00Z\"} ``` ### DELETE /secrets/{name} Delete a secret. **Parameters:** **Response:** - Success: `204` - Secret deleted - Error: `400` - Bad Request **Example:** ```json {} ```",
              "score": null
            },
            {
              "doc_id": "adr_001",
              "chunk_id": "adr_001_fix_0",
              "content": "# ADR-001: Use PostgreSQL for Primary Database **Status:** Accepted **Date:** 2024-11-04 ## Context We need a database to store workflow definitions, execution state, and user data. The database must support ACID transactions, JSON queries, and scale to millions of records. ## Options Considered ### Option 1: PostgreSQL Mature relational database with JSONB support. **Pros:** - ACID compliance - Rich JSONB queries - Team expertise - Proven at scale **Cons:** - Manual sharding for extreme scale - Requires connection pooling ### Option 2: MongoDB Document database with native JSON. **Pros:** - Flexible schema - Built-in sharding - Good for documents **Cons:** - Weaker consistency guarantees - Complex transactions - Higher operational cost ### Option 3: CockroachDB Distributed SQL database. **Pros:** - Automatic sharding - Strong consistency - PostgreSQL compatible **Cons:** - Higher latency - More expensive - Less mature ecosystem ## Decision We will use PostgreSQL as our primary database with JSONB for flexible workflow definitions. ## Consequences - Need PgBouncer for connection pooling - Must plan for sharding at 100M+ records - Requires regular VACUUM maintenance",
              "score": null
            }
          ],
          "latency_ms": 3.6149419993307674,
          "query_id": "simple_007",
          "dimension": "problem",
          "query": "compliance team asking about our data encryption standard"
        },
        {
          "key_facts": [
            "AES-256",
            "encryption at rest"
          ],
          "found_facts": [
            "AES-256",
            "encryption at rest"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "howto_set_up_data_encryption"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "howto_set_up_data_encryption",
              "chunk_id": "howto_set_up_data_encryption_fix_0",
              "content": "# How to Set Up Data Encryption ## Prerequisites - CloudFlow Enterprise plan ## Steps ### Step 1: Enable encryption at rest All data is encrypted at rest using AES-256 by default. > **Note:** Encryption is automatic for all plans ### Step 2: Configure KMS Bring your own encryption keys using AWS KMS or GCP KMS. ```bash cloudflow encryption configure --kms-key arn:aws:kms:... ``` ### Step 3: Enable field-level encryption Encrypt sensitive fields within your workflow data. ```bash cloudflow encryption add-field --path $.user.ssn --key-alias sensitive-data ``` ### Step 4: Verify encryption Confirm encryption is active for your workspace. ```bash cloudflow encryption status ``` ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            },
            {
              "doc_id": "api_secrets",
              "chunk_id": "api_secrets_fix_0",
              "content": "# Secrets API Reference ## Overview The Secrets API provides programmatic access to secrets functionality. Base URL: `https://api.cloudflow.io/v1/secrets` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /secrets List secret names (values not returned). **Parameters:** **Response:** - Success: `200` - Returns array of secret metadata - Error: `400` - Bad Request **Example:** ```json {\"secrets\": [{\"name\": \"DATABASE_URL\", \"created_at\": \"2024-01-01\", \"updated_at\": \"2024-01-10\"}]} ``` ### PUT /secrets/{name} Create or update a secret. **Parameters:** - `value` (string): Secret value (encrypted at rest) **Response:** - Success: `200` - Secret saved - Error: `400` - Bad Request **Example:** ```json {\"name\": \"DATABASE_URL\", \"updated_at\": \"2024-01-15T10:00:00Z\"} ``` ### DELETE /secrets/{name} Delete a secret. **Parameters:** **Response:** - Success: `204` - Secret deleted - Error: `400` - Bad Request **Example:** ```json {} ```",
              "score": null
            },
            {
              "doc_id": "config_api_gateway",
              "chunk_id": "config_api_gateway_fix_0",
              "content": "# API Gateway Configuration Reference ## Overview This document describes all configuration options for API Gateway. ## Environment Variables | Variable | Type | Default | Description | |----------|------|---------|-------------| | `GATEWAY_PORT` | integer | 8000 | Gateway listen port | | `GATEWAY_RATE_LIMIT` | integer | 100 | Requests per minute per user | | `GATEWAY_TIMEOUT_MS` | integer | 30000 | Request timeout in milliseconds | | `GATEWAY_MAX_BODY_SIZE` | string | 10MB | Maximum request body size | | `GATEWAY_CORS_ORIGINS` | string | * | Allowed CORS origins | | `GATEWAY_SSL_ENABLED` | boolean | true | Enable TLS termination | ## Configuration File Configuration can also be provided via `api gateway.yaml`: ```yaml gateway_port: 8000 gateway_rate_limit: 100 gateway_timeout_ms: 30000 gateway_max_body_size: 10MB gateway_cors_origins: * ``` ## Validation Configuration is validated at startup. Invalid configuration will prevent the service from starting.",
              "score": null
            },
            {
              "doc_id": "arch_authentication_service",
              "chunk_id": "arch_authentication_service_fix_0",
              "content": "# Authentication Service Architecture ## Overview The Authentication Service handles user identity, session management, and token issuance. Supports OAuth 2.0, SAML, and API key authentication. ## Design Principles - **Zero Knowledge**: Passwords never stored in plaintext - **Token Rotation**: Refresh tokens rotate on each use ## Components ### Identity Provider Manages user accounts and credentials. Password hashing uses bcrypt with cost factor 12. **Technology:** Custom Go service **Scaling:** Horizontal stateless ### Token Service Issues and validates access tokens. Token lifetime is 1 hour with 7-day refresh tokens. **Technology:** JWT with RS256 **Scaling:** Horizontal stateless ### SSO Gateway Integrates with external identity providers. Supports Google, Microsoft, Okta, and custom SAML. **Technology:** Dex OIDC proxy **Scaling:** Active-passive ## Data Flow Credentials \u2192 Validation \u2192 Token Issue \u2192 Token Refresh \u2192 Logout ## Performance Characteristics - **Latency P50:** 50ms - **Latency P99:** 200ms - **Throughput:** 5000 auth/second",
              "score": null
            },
            {
              "doc_id": "adr_001",
              "chunk_id": "adr_001_fix_0",
              "content": "# ADR-001: Use PostgreSQL for Primary Database **Status:** Accepted **Date:** 2024-11-04 ## Context We need a database to store workflow definitions, execution state, and user data. The database must support ACID transactions, JSON queries, and scale to millions of records. ## Options Considered ### Option 1: PostgreSQL Mature relational database with JSONB support. **Pros:** - ACID compliance - Rich JSONB queries - Team expertise - Proven at scale **Cons:** - Manual sharding for extreme scale - Requires connection pooling ### Option 2: MongoDB Document database with native JSON. **Pros:** - Flexible schema - Built-in sharding - Good for documents **Cons:** - Weaker consistency guarantees - Complex transactions - Higher operational cost ### Option 3: CockroachDB Distributed SQL database. **Pros:** - Automatic sharding - Strong consistency - PostgreSQL compatible **Cons:** - Higher latency - More expensive - Less mature ecosystem ## Decision We will use PostgreSQL as our primary database with JSONB for flexible workflow definitions. ## Consequences - Need PgBouncer for connection pooling - Must plan for sharding at 100M+ records - Requires regular VACUUM maintenance",
              "score": null
            }
          ],
          "latency_ms": 3.578674000891624,
          "query_id": "simple_007",
          "dimension": "casual",
          "query": "encryption at rest algo"
        },
        {
          "key_facts": [
            "AES-256",
            "encryption at rest"
          ],
          "found_facts": [
            "AES-256",
            "encryption at rest"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "howto_set_up_data_encryption"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "howto_set_up_data_encryption",
              "chunk_id": "howto_set_up_data_encryption_fix_0",
              "content": "# How to Set Up Data Encryption ## Prerequisites - CloudFlow Enterprise plan ## Steps ### Step 1: Enable encryption at rest All data is encrypted at rest using AES-256 by default. > **Note:** Encryption is automatic for all plans ### Step 2: Configure KMS Bring your own encryption keys using AWS KMS or GCP KMS. ```bash cloudflow encryption configure --kms-key arn:aws:kms:... ``` ### Step 3: Enable field-level encryption Encrypt sensitive fields within your workflow data. ```bash cloudflow encryption add-field --path $.user.ssn --key-alias sensitive-data ``` ### Step 4: Verify encryption Confirm encryption is active for your workspace. ```bash cloudflow encryption status ``` ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            },
            {
              "doc_id": "adr_001",
              "chunk_id": "adr_001_fix_0",
              "content": "# ADR-001: Use PostgreSQL for Primary Database **Status:** Accepted **Date:** 2024-11-04 ## Context We need a database to store workflow definitions, execution state, and user data. The database must support ACID transactions, JSON queries, and scale to millions of records. ## Options Considered ### Option 1: PostgreSQL Mature relational database with JSONB support. **Pros:** - ACID compliance - Rich JSONB queries - Team expertise - Proven at scale **Cons:** - Manual sharding for extreme scale - Requires connection pooling ### Option 2: MongoDB Document database with native JSON. **Pros:** - Flexible schema - Built-in sharding - Good for documents **Cons:** - Weaker consistency guarantees - Complex transactions - Higher operational cost ### Option 3: CockroachDB Distributed SQL database. **Pros:** - Automatic sharding - Strong consistency - PostgreSQL compatible **Cons:** - Higher latency - More expensive - Less mature ecosystem ## Decision We will use PostgreSQL as our primary database with JSONB for flexible workflow definitions. ## Consequences - Need PgBouncer for connection pooling - Must plan for sharding at 100M+ records - Requires regular VACUUM maintenance",
              "score": null
            },
            {
              "doc_id": "config_database",
              "chunk_id": "config_database_fix_0",
              "content": "# Database Configuration Reference ## Overview This document describes all configuration options for Database. ## Environment Variables | Variable | Type | Default | Description | |----------|------|---------|-------------| | `DATABASE_URL` | string | required | PostgreSQL connection string | | `DATABASE_POOL_MIN` | integer | 5 | Minimum connection pool size | | `DATABASE_POOL_MAX` | integer | 20 | Maximum connection pool size | | `DATABASE_STATEMENT_TIMEOUT` | integer | 30000 | Query timeout in milliseconds | | `DATABASE_SSL_MODE` | string | require | SSL mode: disable, require, verify-full | | `DATABASE_MIGRATION_AUTO` | boolean | false | Run migrations on startup | ## Configuration File Configuration can also be provided via `database.yaml`: ```yaml database_url: required database_pool_min: 5 database_pool_max: 20 database_statement_timeout: 30000 database_ssl_mode: require ``` ## Validation Configuration is validated at startup. Invalid configuration will prevent the service from starting.",
              "score": null
            },
            {
              "doc_id": "arch_authentication_service",
              "chunk_id": "arch_authentication_service_fix_0",
              "content": "# Authentication Service Architecture ## Overview The Authentication Service handles user identity, session management, and token issuance. Supports OAuth 2.0, SAML, and API key authentication. ## Design Principles - **Zero Knowledge**: Passwords never stored in plaintext - **Token Rotation**: Refresh tokens rotate on each use ## Components ### Identity Provider Manages user accounts and credentials. Password hashing uses bcrypt with cost factor 12. **Technology:** Custom Go service **Scaling:** Horizontal stateless ### Token Service Issues and validates access tokens. Token lifetime is 1 hour with 7-day refresh tokens. **Technology:** JWT with RS256 **Scaling:** Horizontal stateless ### SSO Gateway Integrates with external identity providers. Supports Google, Microsoft, Okta, and custom SAML. **Technology:** Dex OIDC proxy **Scaling:** Active-passive ## Data Flow Credentials \u2192 Validation \u2192 Token Issue \u2192 Token Refresh \u2192 Logout ## Performance Characteristics - **Latency P50:** 50ms - **Latency P99:** 200ms - **Throughput:** 5000 auth/second",
              "score": null
            },
            {
              "doc_id": "api_secrets",
              "chunk_id": "api_secrets_fix_0",
              "content": "# Secrets API Reference ## Overview The Secrets API provides programmatic access to secrets functionality. Base URL: `https://api.cloudflow.io/v1/secrets` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /secrets List secret names (values not returned). **Parameters:** **Response:** - Success: `200` - Returns array of secret metadata - Error: `400` - Bad Request **Example:** ```json {\"secrets\": [{\"name\": \"DATABASE_URL\", \"created_at\": \"2024-01-01\", \"updated_at\": \"2024-01-10\"}]} ``` ### PUT /secrets/{name} Create or update a secret. **Parameters:** - `value` (string): Secret value (encrypted at rest) **Response:** - Success: `200` - Secret saved - Error: `400` - Bad Request **Example:** ```json {\"name\": \"DATABASE_URL\", \"updated_at\": \"2024-01-15T10:00:00Z\"} ``` ### DELETE /secrets/{name} Delete a secret. **Parameters:** **Response:** - Success: `204` - Secret deleted - Error: `400` - Bad Request **Example:** ```json {} ```",
              "score": null
            }
          ],
          "latency_ms": 3.7963599988870556,
          "query_id": "simple_007",
          "dimension": "contextual",
          "query": "filling out security questionnaire, need to know the encryption standard for stored data"
        },
        {
          "key_facts": [
            "AES-256",
            "encryption at rest"
          ],
          "found_facts": [
            "AES-256",
            "encryption at rest"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "howto_set_up_data_encryption"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "howto_set_up_data_encryption",
              "chunk_id": "howto_set_up_data_encryption_fix_0",
              "content": "# How to Set Up Data Encryption ## Prerequisites - CloudFlow Enterprise plan ## Steps ### Step 1: Enable encryption at rest All data is encrypted at rest using AES-256 by default. > **Note:** Encryption is automatic for all plans ### Step 2: Configure KMS Bring your own encryption keys using AWS KMS or GCP KMS. ```bash cloudflow encryption configure --kms-key arn:aws:kms:... ``` ### Step 3: Enable field-level encryption Encrypt sensitive fields within your workflow data. ```bash cloudflow encryption add-field --path $.user.ssn --key-alias sensitive-data ``` ### Step 4: Verify encryption Confirm encryption is active for your workspace. ```bash cloudflow encryption status ``` ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            },
            {
              "doc_id": "api_secrets",
              "chunk_id": "api_secrets_fix_0",
              "content": "# Secrets API Reference ## Overview The Secrets API provides programmatic access to secrets functionality. Base URL: `https://api.cloudflow.io/v1/secrets` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /secrets List secret names (values not returned). **Parameters:** **Response:** - Success: `200` - Returns array of secret metadata - Error: `400` - Bad Request **Example:** ```json {\"secrets\": [{\"name\": \"DATABASE_URL\", \"created_at\": \"2024-01-01\", \"updated_at\": \"2024-01-10\"}]} ``` ### PUT /secrets/{name} Create or update a secret. **Parameters:** - `value` (string): Secret value (encrypted at rest) **Response:** - Success: `200` - Secret saved - Error: `400` - Bad Request **Example:** ```json {\"name\": \"DATABASE_URL\", \"updated_at\": \"2024-01-15T10:00:00Z\"} ``` ### DELETE /secrets/{name} Delete a secret. **Parameters:** **Response:** - Success: `204` - Secret deleted - Error: `400` - Bad Request **Example:** ```json {} ```",
              "score": null
            },
            {
              "doc_id": "arch_data_pipeline",
              "chunk_id": "arch_data_pipeline_fix_0",
              "content": "# Data Pipeline Architecture ## Overview The Data Pipeline processes and transforms data between workflow steps. It supports batch processing up to 10GB and streaming for real-time use cases. ## Design Principles - **Exactly-Once Processing**: Each record is processed exactly once - **Backpressure Handling**: System gracefully handles varying load ## Components ### Stream Processor Handles real-time data streams. Maximum throughput of 100000 messages per second. **Technology:** Apache Kafka with Flink **Scaling:** Partition-based scaling ### Batch Processor Processes large batch jobs. Supports data formats: JSON, CSV, Parquet, Avro. **Technology:** Apache Spark on Kubernetes **Scaling:** Dynamic resource allocation ### Data Lake Long-term data storage with time-travel queries. Retention configurable up to 7 years. **Technology:** S3 with Iceberg tables **Scaling:** Unlimited with tiered storage ## Data Flow Input \u2192 Validation \u2192 Transform \u2192 Enrich \u2192 Output ## Performance Characteristics - **Latency P50:** 100ms (streaming), 5min (batch) - **Latency P99:** 500ms (streaming), 30min (batch) - **Throughput:** 100000 messages/second (streaming)",
              "score": null
            },
            {
              "doc_id": "howto_configure_custom_domain",
              "chunk_id": "howto_configure_custom_domain_fix_0",
              "content": "# How to Configure Custom Domain ## Prerequisites - Domain with DNS access - CloudFlow Pro plan or higher ## Steps ### Step 1: Verify domain ownership Add a TXT record to verify domain ownership. ```bash cloudflow domains verify --domain app.example.com ``` ### Step 2: Configure DNS Add a CNAME record pointing to your CloudFlow endpoint. > **Note:** CNAME should point to ingress.cloudflow.io ### Step 3: Enable SSL CloudFlow automatically provisions SSL certificates via Let's Encrypt. ```bash cloudflow domains ssl --domain app.example.com ``` ### Step 4: Set as primary Make this the primary domain for your application. ```bash cloudflow domains set-primary --domain app.example.com ``` ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            },
            {
              "doc_id": "adr_008",
              "chunk_id": "adr_008_fix_0",
              "content": "# ADR-008: Use S3 for Object Storage **Status:** Accepted **Date:** 2024-10-14 ## Context We need storage for workflow artifacts, execution outputs, and user uploads. ## Options Considered ### Option 1: Amazon S3 Object storage service. **Pros:** - 99.999999999% durability - Lifecycle policies - Event notifications - CDN integration **Cons:** - AWS lock-in - Eventual consistency for overwrites - Cost at scale ### Option 2: Self-hosted MinIO S3-compatible object storage. **Pros:** - No vendor lock-in - S3 compatible - On-premises option **Cons:** - Operational burden - Lower durability guarantees - Scaling complexity ### Option 3: Google Cloud Storage GCP object storage. **Pros:** - Strong consistency - Good performance - Unified billing **Cons:** - GCP lock-in - Fewer features than S3 - Smaller ecosystem ## Decision We will use Amazon S3 for object storage with intelligent tiering for cost optimization. ## Consequences - Enable versioning for critical buckets - Implement lifecycle policies - Use pre-signed URLs for uploads - Enable server-side encryption",
              "score": null
            }
          ],
          "latency_ms": 3.39155599976948,
          "query_id": "simple_007",
          "dimension": "negation",
          "query": "is our stored data actually encrypted or not"
        },
        {
          "key_facts": [
            "100",
            "WORKFLOW_MAX_STEPS"
          ],
          "found_facts": [
            "100",
            "WORKFLOW_MAX_STEPS"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "config_workflow_engine"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "config_workflow_engine",
              "chunk_id": "config_workflow_engine_fix_0",
              "content": "# Workflow Engine Configuration Reference ## Overview This document describes all configuration options for Workflow Engine. ## Environment Variables | Variable | Type | Default | Description | |----------|------|---------|-------------| | `WORKFLOW_MAX_STEPS` | integer | 100 | Maximum steps per workflow | | `WORKFLOW_TIMEOUT_SECONDS` | integer | 3600 | Maximum workflow execution time | | `WORKFLOW_RETRY_LIMIT` | integer | 3 | Default retry attempts for failed steps | | `WORKFLOW_CONCURRENT_LIMIT` | integer | 10 | Maximum concurrent executions per workflow | | `EXECUTOR_POOL_SIZE` | integer | 50 | Number of executor workers | | `EXECUTOR_MEMORY_LIMIT` | string | 512Mi | Memory limit per executor | ## Configuration File Configuration can also be provided via `workflow engine.yaml`: ```yaml workflow_max_steps: 100 workflow_timeout_seconds: 3600 workflow_retry_limit: 3 workflow_concurrent_limit: 10 executor_pool_size: 50 ``` ## Validation Configuration is validated at startup. Invalid configuration will prevent the service from starting.",
              "score": null
            },
            {
              "doc_id": "arch_workflow_engine",
              "chunk_id": "arch_workflow_engine_fix_0",
              "content": "# Workflow Engine Architecture ## Overview The Workflow Engine is the core component responsible for executing user-defined workflows. It uses an event-driven architecture with support for parallel execution, conditional branching, and automatic retries. ## Design Principles - **Idempotency**: All operations can be safely retried without side effects - **Eventual Consistency**: State converges to consistent view within 5 seconds - **Fault Tolerance**: System continues operating despite component failures ## Components ### Scheduler Manages workflow scheduling and triggers. Supports cron expressions, webhooks, and event-based triggers. **Technology:** Custom Go service **Scaling:** Horizontal with leader election ### Executor Runs individual workflow steps in isolated containers. Maximum execution time is 30 minutes per step. **Technology:** Kubernetes Jobs **Scaling:** Auto-scaling based on queue depth ### State Store Persists workflow state, execution history, and step results. Uses optimistic locking for concurrent updates. **Technology:** PostgreSQL with JSONB **Scaling:** Primary-replica with read replicas ## Data Flow Triggers \u2192 Scheduler \u2192 Job Queue \u2192 Executor \u2192 State Store \u2192 Webhooks/Notifications ## Performance Characteristics - **Latency P50:** 45ms - **Latency P99:** 250ms - **Throughput:** 5000 executions/minute",
              "score": null
            },
            {
              "doc_id": "howto_implement_retry_logic",
              "chunk_id": "howto_implement_retry_logic_fix_0",
              "content": "# How to Implement Retry Logic ## Prerequisites - Existing workflow ## Steps ### Step 1: Define retry policy Add retry configuration to your workflow step. > **Note:** Maximum 5 retry attempts allowed ### Step 2: Configure backoff Set exponential backoff with base delay of 1 second. The formula is: delay = base * 2^attempt. ### Step 3: Add timeout Set maximum execution time to prevent infinite loops. Default timeout is 30 seconds. ### Step 4: Handle final failure Configure on_failure handler to notify or escalate when all retries are exhausted. ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            },
            {
              "doc_id": "api_workflows",
              "chunk_id": "api_workflows_fix_0",
              "content": "# Workflows API Reference ## Overview The Workflows API provides programmatic access to workflows functionality. Base URL: `https://api.cloudflow.io/v1/workflows` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /workflows List all workflows for the authenticated user. **Parameters:** - `limit` (integer): Maximum number of results (default: 50, max: 200) - `status` (string): Filter by status: active, paused, archived **Response:** - Success: `200` - Returns array of workflow objects - Error: `400` - Bad Request **Example:** ```json {\"workflows\": [{\"id\": \"wf_123\", \"name\": \"Daily Report\", \"status\": \"active\"}]} ``` ### POST /workflows Create a new workflow. **Parameters:** - `name` (string): Workflow name (required) - `definition` (object): Workflow definition in JSON format **Response:** - Success: `201` - Workflow created successfully - Error: `400` - Bad Request **Example:** ```json {\"id\": \"wf_456\", \"name\": \"New Workflow\", \"created_at\": \"2024-01-15T10:00:00Z\"} ``` ### DELETE /workflows/{id} Delete a workflow permanently. **Parameters:** - `id` (string): Workflow ID (required) **Response:** - Success: `204` - Workflow deleted - Error: `404` - Workflow not found **Example:** ```json {} ```",
              "score": null
            },
            {
              "doc_id": "release_3_2_0",
              "chunk_id": "release_3_2_0_fix_0",
              "content": "# Release Notes - v3.2.0 **Release Date:** 2024-01-15 ## New Features - Workflow versioning with rollback support - New Python SDK with async support - Bulk operations API for workflows - Custom retry policies per step ## Bug Fixes - Fixed race condition in concurrent execution handler - Resolved memory leak in long-running workflows - Fixed timezone handling in scheduled triggers ## Breaking Changes - **BREAKING:** API v1 endpoints deprecated, will be removed in v4.0 - **BREAKING:** Minimum Python version now 3.9 ## Upgrade Instructions ```bash cloudflow upgrade --version 3.2.0 ``` ## Known Issues None at this time.",
              "score": null
            }
          ],
          "latency_ms": 3.95485499939241,
          "query_id": "simple_008",
          "dimension": "original",
          "query": "What is the maximum number of steps in a workflow?"
        },
        {
          "key_facts": [
            "100",
            "WORKFLOW_MAX_STEPS"
          ],
          "found_facts": [
            "100",
            "WORKFLOW_MAX_STEPS"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "config_workflow_engine"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "config_workflow_engine",
              "chunk_id": "config_workflow_engine_fix_0",
              "content": "# Workflow Engine Configuration Reference ## Overview This document describes all configuration options for Workflow Engine. ## Environment Variables | Variable | Type | Default | Description | |----------|------|---------|-------------| | `WORKFLOW_MAX_STEPS` | integer | 100 | Maximum steps per workflow | | `WORKFLOW_TIMEOUT_SECONDS` | integer | 3600 | Maximum workflow execution time | | `WORKFLOW_RETRY_LIMIT` | integer | 3 | Default retry attempts for failed steps | | `WORKFLOW_CONCURRENT_LIMIT` | integer | 10 | Maximum concurrent executions per workflow | | `EXECUTOR_POOL_SIZE` | integer | 50 | Number of executor workers | | `EXECUTOR_MEMORY_LIMIT` | string | 512Mi | Memory limit per executor | ## Configuration File Configuration can also be provided via `workflow engine.yaml`: ```yaml workflow_max_steps: 100 workflow_timeout_seconds: 3600 workflow_retry_limit: 3 workflow_concurrent_limit: 10 executor_pool_size: 50 ``` ## Validation Configuration is validated at startup. Invalid configuration will prevent the service from starting.",
              "score": null
            },
            {
              "doc_id": "howto_implement_retry_logic",
              "chunk_id": "howto_implement_retry_logic_fix_0",
              "content": "# How to Implement Retry Logic ## Prerequisites - Existing workflow ## Steps ### Step 1: Define retry policy Add retry configuration to your workflow step. > **Note:** Maximum 5 retry attempts allowed ### Step 2: Configure backoff Set exponential backoff with base delay of 1 second. The formula is: delay = base * 2^attempt. ### Step 3: Add timeout Set maximum execution time to prevent infinite loops. Default timeout is 30 seconds. ### Step 4: Handle final failure Configure on_failure handler to notify or escalate when all retries are exhausted. ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            },
            {
              "doc_id": "api_workflows",
              "chunk_id": "api_workflows_fix_0",
              "content": "# Workflows API Reference ## Overview The Workflows API provides programmatic access to workflows functionality. Base URL: `https://api.cloudflow.io/v1/workflows` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /workflows List all workflows for the authenticated user. **Parameters:** - `limit` (integer): Maximum number of results (default: 50, max: 200) - `status` (string): Filter by status: active, paused, archived **Response:** - Success: `200` - Returns array of workflow objects - Error: `400` - Bad Request **Example:** ```json {\"workflows\": [{\"id\": \"wf_123\", \"name\": \"Daily Report\", \"status\": \"active\"}]} ``` ### POST /workflows Create a new workflow. **Parameters:** - `name` (string): Workflow name (required) - `definition` (object): Workflow definition in JSON format **Response:** - Success: `201` - Workflow created successfully - Error: `400` - Bad Request **Example:** ```json {\"id\": \"wf_456\", \"name\": \"New Workflow\", \"created_at\": \"2024-01-15T10:00:00Z\"} ``` ### DELETE /workflows/{id} Delete a workflow permanently. **Parameters:** - `id` (string): Workflow ID (required) **Response:** - Success: `204` - Workflow deleted - Error: `404` - Workflow not found **Example:** ```json {} ```",
              "score": null
            },
            {
              "doc_id": "arch_workflow_engine",
              "chunk_id": "arch_workflow_engine_fix_0",
              "content": "# Workflow Engine Architecture ## Overview The Workflow Engine is the core component responsible for executing user-defined workflows. It uses an event-driven architecture with support for parallel execution, conditional branching, and automatic retries. ## Design Principles - **Idempotency**: All operations can be safely retried without side effects - **Eventual Consistency**: State converges to consistent view within 5 seconds - **Fault Tolerance**: System continues operating despite component failures ## Components ### Scheduler Manages workflow scheduling and triggers. Supports cron expressions, webhooks, and event-based triggers. **Technology:** Custom Go service **Scaling:** Horizontal with leader election ### Executor Runs individual workflow steps in isolated containers. Maximum execution time is 30 minutes per step. **Technology:** Kubernetes Jobs **Scaling:** Auto-scaling based on queue depth ### State Store Persists workflow state, execution history, and step results. Uses optimistic locking for concurrent updates. **Technology:** PostgreSQL with JSONB **Scaling:** Primary-replica with read replicas ## Data Flow Triggers \u2192 Scheduler \u2192 Job Queue \u2192 Executor \u2192 State Store \u2192 Webhooks/Notifications ## Performance Characteristics - **Latency P50:** 45ms - **Latency P99:** 250ms - **Throughput:** 5000 executions/minute",
              "score": null
            },
            {
              "doc_id": "api_metrics",
              "chunk_id": "api_metrics_fix_0",
              "content": "# Metrics API Reference ## Overview The Metrics API provides programmatic access to metrics functionality. Base URL: `https://api.cloudflow.io/v1/metrics` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /metrics/workflows Get workflow performance metrics. **Parameters:** - `period` (string): Time period: hour, day, week, month - `workflow_id` (string): Filter by workflow ID **Response:** - Success: `200` - Returns metrics data - Error: `400` - Bad Request **Example:** ```json {\"success_rate\": 0.985, \"avg_duration_ms\": 1250, \"p99_duration_ms\": 5000, \"total_executions\": 10000} ```",
              "score": null
            }
          ],
          "latency_ms": 4.1571820001991,
          "query_id": "simple_008",
          "dimension": "synonym",
          "query": "workflow step count limit"
        },
        {
          "key_facts": [
            "100",
            "WORKFLOW_MAX_STEPS"
          ],
          "found_facts": [
            "100"
          ],
          "missed_facts": [
            "WORKFLOW_MAX_STEPS"
          ],
          "coverage": 0.5,
          "expected_docs": [
            "config_workflow_engine"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "release_3_2_0",
              "chunk_id": "release_3_2_0_fix_0",
              "content": "# Release Notes - v3.2.0 **Release Date:** 2024-01-15 ## New Features - Workflow versioning with rollback support - New Python SDK with async support - Bulk operations API for workflows - Custom retry policies per step ## Bug Fixes - Fixed race condition in concurrent execution handler - Resolved memory leak in long-running workflows - Fixed timezone handling in scheduled triggers ## Breaking Changes - **BREAKING:** API v1 endpoints deprecated, will be removed in v4.0 - **BREAKING:** Minimum Python version now 3.9 ## Upgrade Instructions ```bash cloudflow upgrade --version 3.2.0 ``` ## Known Issues None at this time.",
              "score": null
            },
            {
              "doc_id": "howto_debug_failed_execution",
              "chunk_id": "howto_debug_failed_execution_fix_0",
              "content": "# How to Debug Failed Execution ## Prerequisites - Failed execution ID ## Steps ### Step 1: Get execution details Retrieve the full execution record including step outputs. ```bash cloudflow executions get ex_123 --include-steps ``` ### Step 2: View step logs Check logs for the failed step. ```bash cloudflow executions logs ex_123 --step 3 ``` ### Step 3: Inspect input/output Review the data passed between steps. ```bash cloudflow executions inspect ex_123 --step 3 ``` ### Step 4: Enable debug mode Re-run with verbose logging enabled. ```bash cloudflow workflows execute wf_123 --debug ``` ### Step 5: Check error patterns Look for similar failures in recent executions. ```bash cloudflow executions list --status failed --since 24h ``` ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            },
            {
              "doc_id": "howto_create_api_integration",
              "chunk_id": "howto_create_api_integration_fix_0",
              "content": "# How to Create API Integration ## Prerequisites - CloudFlow account ## Steps ### Step 1: Generate API key Create an API key for programmatic access. ```bash cloudflow apikeys create --name 'CI Integration' --scopes workflows:read,workflows:execute ``` ### Step 2: Store key securely Never commit API keys to source control. Use environment variables or secret management. > **Note:** Rotate keys every 90 days ### Step 3: Test authentication Verify the API key works correctly. ```bash curl -H 'Authorization: Bearer cf_...' https://api.cloudflow.io/v1/me ``` ### Step 4: Implement rate limiting Handle rate limit responses (HTTP 429) with exponential backoff. > **Note:** Default limit is 100 requests per minute ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            },
            {
              "doc_id": "api_workflows",
              "chunk_id": "api_workflows_fix_0",
              "content": "# Workflows API Reference ## Overview The Workflows API provides programmatic access to workflows functionality. Base URL: `https://api.cloudflow.io/v1/workflows` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /workflows List all workflows for the authenticated user. **Parameters:** - `limit` (integer): Maximum number of results (default: 50, max: 200) - `status` (string): Filter by status: active, paused, archived **Response:** - Success: `200` - Returns array of workflow objects - Error: `400` - Bad Request **Example:** ```json {\"workflows\": [{\"id\": \"wf_123\", \"name\": \"Daily Report\", \"status\": \"active\"}]} ``` ### POST /workflows Create a new workflow. **Parameters:** - `name` (string): Workflow name (required) - `definition` (object): Workflow definition in JSON format **Response:** - Success: `201` - Workflow created successfully - Error: `400` - Bad Request **Example:** ```json {\"id\": \"wf_456\", \"name\": \"New Workflow\", \"created_at\": \"2024-01-15T10:00:00Z\"} ``` ### DELETE /workflows/{id} Delete a workflow permanently. **Parameters:** - `id` (string): Workflow ID (required) **Response:** - Success: `204` - Workflow deleted - Error: `404` - Workflow not found **Example:** ```json {} ```",
              "score": null
            },
            {
              "doc_id": "howto_implement_retry_logic",
              "chunk_id": "howto_implement_retry_logic_fix_0",
              "content": "# How to Implement Retry Logic ## Prerequisites - Existing workflow ## Steps ### Step 1: Define retry policy Add retry configuration to your workflow step. > **Note:** Maximum 5 retry attempts allowed ### Step 2: Configure backoff Set exponential backoff with base delay of 1 second. The formula is: delay = base * 2^attempt. ### Step 3: Add timeout Set maximum execution time to prevent infinite loops. Default timeout is 30 seconds. ### Step 4: Handle final failure Configure on_failure handler to notify or escalate when all retries are exhausted. ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            }
          ],
          "latency_ms": 3.632895000919234,
          "query_id": "simple_008",
          "dimension": "problem",
          "query": "workflow creation failing with 'too many steps' error"
        },
        {
          "key_facts": [
            "100",
            "WORKFLOW_MAX_STEPS"
          ],
          "found_facts": [
            "100",
            "WORKFLOW_MAX_STEPS"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "config_workflow_engine"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "config_workflow_engine",
              "chunk_id": "config_workflow_engine_fix_0",
              "content": "# Workflow Engine Configuration Reference ## Overview This document describes all configuration options for Workflow Engine. ## Environment Variables | Variable | Type | Default | Description | |----------|------|---------|-------------| | `WORKFLOW_MAX_STEPS` | integer | 100 | Maximum steps per workflow | | `WORKFLOW_TIMEOUT_SECONDS` | integer | 3600 | Maximum workflow execution time | | `WORKFLOW_RETRY_LIMIT` | integer | 3 | Default retry attempts for failed steps | | `WORKFLOW_CONCURRENT_LIMIT` | integer | 10 | Maximum concurrent executions per workflow | | `EXECUTOR_POOL_SIZE` | integer | 50 | Number of executor workers | | `EXECUTOR_MEMORY_LIMIT` | string | 512Mi | Memory limit per executor | ## Configuration File Configuration can also be provided via `workflow engine.yaml`: ```yaml workflow_max_steps: 100 workflow_timeout_seconds: 3600 workflow_retry_limit: 3 workflow_concurrent_limit: 10 executor_pool_size: 50 ``` ## Validation Configuration is validated at startup. Invalid configuration will prevent the service from starting.",
              "score": null
            },
            {
              "doc_id": "arch_workflow_engine",
              "chunk_id": "arch_workflow_engine_fix_0",
              "content": "# Workflow Engine Architecture ## Overview The Workflow Engine is the core component responsible for executing user-defined workflows. It uses an event-driven architecture with support for parallel execution, conditional branching, and automatic retries. ## Design Principles - **Idempotency**: All operations can be safely retried without side effects - **Eventual Consistency**: State converges to consistent view within 5 seconds - **Fault Tolerance**: System continues operating despite component failures ## Components ### Scheduler Manages workflow scheduling and triggers. Supports cron expressions, webhooks, and event-based triggers. **Technology:** Custom Go service **Scaling:** Horizontal with leader election ### Executor Runs individual workflow steps in isolated containers. Maximum execution time is 30 minutes per step. **Technology:** Kubernetes Jobs **Scaling:** Auto-scaling based on queue depth ### State Store Persists workflow state, execution history, and step results. Uses optimistic locking for concurrent updates. **Technology:** PostgreSQL with JSONB **Scaling:** Primary-replica with read replicas ## Data Flow Triggers \u2192 Scheduler \u2192 Job Queue \u2192 Executor \u2192 State Store \u2192 Webhooks/Notifications ## Performance Characteristics - **Latency P50:** 45ms - **Latency P99:** 250ms - **Throughput:** 5000 executions/minute",
              "score": null
            },
            {
              "doc_id": "api_workflows",
              "chunk_id": "api_workflows_fix_0",
              "content": "# Workflows API Reference ## Overview The Workflows API provides programmatic access to workflows functionality. Base URL: `https://api.cloudflow.io/v1/workflows` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /workflows List all workflows for the authenticated user. **Parameters:** - `limit` (integer): Maximum number of results (default: 50, max: 200) - `status` (string): Filter by status: active, paused, archived **Response:** - Success: `200` - Returns array of workflow objects - Error: `400` - Bad Request **Example:** ```json {\"workflows\": [{\"id\": \"wf_123\", \"name\": \"Daily Report\", \"status\": \"active\"}]} ``` ### POST /workflows Create a new workflow. **Parameters:** - `name` (string): Workflow name (required) - `definition` (object): Workflow definition in JSON format **Response:** - Success: `201` - Workflow created successfully - Error: `400` - Bad Request **Example:** ```json {\"id\": \"wf_456\", \"name\": \"New Workflow\", \"created_at\": \"2024-01-15T10:00:00Z\"} ``` ### DELETE /workflows/{id} Delete a workflow permanently. **Parameters:** - `id` (string): Workflow ID (required) **Response:** - Success: `204` - Workflow deleted - Error: `404` - Workflow not found **Example:** ```json {} ```",
              "score": null
            },
            {
              "doc_id": "howto_implement_retry_logic",
              "chunk_id": "howto_implement_retry_logic_fix_0",
              "content": "# How to Implement Retry Logic ## Prerequisites - Existing workflow ## Steps ### Step 1: Define retry policy Add retry configuration to your workflow step. > **Note:** Maximum 5 retry attempts allowed ### Step 2: Configure backoff Set exponential backoff with base delay of 1 second. The formula is: delay = base * 2^attempt. ### Step 3: Add timeout Set maximum execution time to prevent infinite loops. Default timeout is 30 seconds. ### Step 4: Handle final failure Configure on_failure handler to notify or escalate when all retries are exhausted. ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            },
            {
              "doc_id": "api_metrics",
              "chunk_id": "api_metrics_fix_0",
              "content": "# Metrics API Reference ## Overview The Metrics API provides programmatic access to metrics functionality. Base URL: `https://api.cloudflow.io/v1/metrics` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /metrics/workflows Get workflow performance metrics. **Parameters:** - `period` (string): Time period: hour, day, week, month - `workflow_id` (string): Filter by workflow ID **Response:** - Success: `200` - Returns metrics data - Error: `400` - Bad Request **Example:** ```json {\"success_rate\": 0.985, \"avg_duration_ms\": 1250, \"p99_duration_ms\": 5000, \"total_executions\": 10000} ```",
              "score": null
            }
          ],
          "latency_ms": 3.357994000907638,
          "query_id": "simple_008",
          "dimension": "casual",
          "query": "max steps per workflow"
        },
        {
          "key_facts": [
            "100",
            "WORKFLOW_MAX_STEPS"
          ],
          "found_facts": [
            "100",
            "WORKFLOW_MAX_STEPS"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "config_workflow_engine"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "config_workflow_engine",
              "chunk_id": "config_workflow_engine_fix_0",
              "content": "# Workflow Engine Configuration Reference ## Overview This document describes all configuration options for Workflow Engine. ## Environment Variables | Variable | Type | Default | Description | |----------|------|---------|-------------| | `WORKFLOW_MAX_STEPS` | integer | 100 | Maximum steps per workflow | | `WORKFLOW_TIMEOUT_SECONDS` | integer | 3600 | Maximum workflow execution time | | `WORKFLOW_RETRY_LIMIT` | integer | 3 | Default retry attempts for failed steps | | `WORKFLOW_CONCURRENT_LIMIT` | integer | 10 | Maximum concurrent executions per workflow | | `EXECUTOR_POOL_SIZE` | integer | 50 | Number of executor workers | | `EXECUTOR_MEMORY_LIMIT` | string | 512Mi | Memory limit per executor | ## Configuration File Configuration can also be provided via `workflow engine.yaml`: ```yaml workflow_max_steps: 100 workflow_timeout_seconds: 3600 workflow_retry_limit: 3 workflow_concurrent_limit: 10 executor_pool_size: 50 ``` ## Validation Configuration is validated at startup. Invalid configuration will prevent the service from starting.",
              "score": null
            },
            {
              "doc_id": "arch_workflow_engine",
              "chunk_id": "arch_workflow_engine_fix_0",
              "content": "# Workflow Engine Architecture ## Overview The Workflow Engine is the core component responsible for executing user-defined workflows. It uses an event-driven architecture with support for parallel execution, conditional branching, and automatic retries. ## Design Principles - **Idempotency**: All operations can be safely retried without side effects - **Eventual Consistency**: State converges to consistent view within 5 seconds - **Fault Tolerance**: System continues operating despite component failures ## Components ### Scheduler Manages workflow scheduling and triggers. Supports cron expressions, webhooks, and event-based triggers. **Technology:** Custom Go service **Scaling:** Horizontal with leader election ### Executor Runs individual workflow steps in isolated containers. Maximum execution time is 30 minutes per step. **Technology:** Kubernetes Jobs **Scaling:** Auto-scaling based on queue depth ### State Store Persists workflow state, execution history, and step results. Uses optimistic locking for concurrent updates. **Technology:** PostgreSQL with JSONB **Scaling:** Primary-replica with read replicas ## Data Flow Triggers \u2192 Scheduler \u2192 Job Queue \u2192 Executor \u2192 State Store \u2192 Webhooks/Notifications ## Performance Characteristics - **Latency P50:** 45ms - **Latency P99:** 250ms - **Throughput:** 5000 executions/minute",
              "score": null
            },
            {
              "doc_id": "adr_007",
              "chunk_id": "adr_007_fix_0",
              "content": "# ADR-007: Implement Rate Limiting at API Gateway **Status:** Accepted **Date:** 2024-09-03 ## Context We need to protect our services from abuse and ensure fair resource usage across customers. ## Options Considered ### Option 1: Sliding window at gateway Rate limit at API gateway level. **Pros:** - Centralized enforcement - Low latency - Easy configuration **Cons:** - Gateway becomes bottleneck - Coarse granularity - Redis dependency ### Option 2: Per-service limiting Each service implements its own limits. **Pros:** - Fine-grained control - Service autonomy - No single point of failure **Cons:** - Inconsistent implementation - Higher complexity - Distributed state ### Option 3: Token bucket Token-based rate limiting. **Pros:** - Allows bursts - Smooth rate limiting - Well understood **Cons:** - More complex - Harder to explain to users - State management ## Decision We will implement sliding window rate limiting at the Kong API Gateway using Redis. ## Consequences - Default limit: 100 requests per minute - Implement per-plan limits - Return rate limit headers - Add retry-after on 429",
              "score": null
            },
            {
              "doc_id": "howto_create_api_integration",
              "chunk_id": "howto_create_api_integration_fix_0",
              "content": "# How to Create API Integration ## Prerequisites - CloudFlow account ## Steps ### Step 1: Generate API key Create an API key for programmatic access. ```bash cloudflow apikeys create --name 'CI Integration' --scopes workflows:read,workflows:execute ``` ### Step 2: Store key securely Never commit API keys to source control. Use environment variables or secret management. > **Note:** Rotate keys every 90 days ### Step 3: Test authentication Verify the API key works correctly. ```bash curl -H 'Authorization: Bearer cf_...' https://api.cloudflow.io/v1/me ``` ### Step 4: Implement rate limiting Handle rate limit responses (HTTP 429) with exponential backoff. > **Note:** Default limit is 100 requests per minute ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            },
            {
              "doc_id": "howto_implement_retry_logic",
              "chunk_id": "howto_implement_retry_logic_fix_0",
              "content": "# How to Implement Retry Logic ## Prerequisites - Existing workflow ## Steps ### Step 1: Define retry policy Add retry configuration to your workflow step. > **Note:** Maximum 5 retry attempts allowed ### Step 2: Configure backoff Set exponential backoff with base delay of 1 second. The formula is: delay = base * 2^attempt. ### Step 3: Add timeout Set maximum execution time to prevent infinite loops. Default timeout is 30 seconds. ### Step 4: Handle final failure Configure on_failure handler to notify or escalate when all retries are exhausted. ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            }
          ],
          "latency_ms": 3.4880059993156465,
          "query_id": "simple_008",
          "dimension": "contextual",
          "query": "designing a complex ETL workflow, need to know if there's a step limit"
        },
        {
          "key_facts": [
            "100",
            "WORKFLOW_MAX_STEPS"
          ],
          "found_facts": [
            "100",
            "WORKFLOW_MAX_STEPS"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "config_workflow_engine"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "config_workflow_engine",
              "chunk_id": "config_workflow_engine_fix_0",
              "content": "# Workflow Engine Configuration Reference ## Overview This document describes all configuration options for Workflow Engine. ## Environment Variables | Variable | Type | Default | Description | |----------|------|---------|-------------| | `WORKFLOW_MAX_STEPS` | integer | 100 | Maximum steps per workflow | | `WORKFLOW_TIMEOUT_SECONDS` | integer | 3600 | Maximum workflow execution time | | `WORKFLOW_RETRY_LIMIT` | integer | 3 | Default retry attempts for failed steps | | `WORKFLOW_CONCURRENT_LIMIT` | integer | 10 | Maximum concurrent executions per workflow | | `EXECUTOR_POOL_SIZE` | integer | 50 | Number of executor workers | | `EXECUTOR_MEMORY_LIMIT` | string | 512Mi | Memory limit per executor | ## Configuration File Configuration can also be provided via `workflow engine.yaml`: ```yaml workflow_max_steps: 100 workflow_timeout_seconds: 3600 workflow_retry_limit: 3 workflow_concurrent_limit: 10 executor_pool_size: 50 ``` ## Validation Configuration is validated at startup. Invalid configuration will prevent the service from starting.",
              "score": null
            },
            {
              "doc_id": "howto_debug_failed_execution",
              "chunk_id": "howto_debug_failed_execution_fix_0",
              "content": "# How to Debug Failed Execution ## Prerequisites - Failed execution ID ## Steps ### Step 1: Get execution details Retrieve the full execution record including step outputs. ```bash cloudflow executions get ex_123 --include-steps ``` ### Step 2: View step logs Check logs for the failed step. ```bash cloudflow executions logs ex_123 --step 3 ``` ### Step 3: Inspect input/output Review the data passed between steps. ```bash cloudflow executions inspect ex_123 --step 3 ``` ### Step 4: Enable debug mode Re-run with verbose logging enabled. ```bash cloudflow workflows execute wf_123 --debug ``` ### Step 5: Check error patterns Look for similar failures in recent executions. ```bash cloudflow executions list --status failed --since 24h ``` ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            },
            {
              "doc_id": "release_3_2_0",
              "chunk_id": "release_3_2_0_fix_0",
              "content": "# Release Notes - v3.2.0 **Release Date:** 2024-01-15 ## New Features - Workflow versioning with rollback support - New Python SDK with async support - Bulk operations API for workflows - Custom retry policies per step ## Bug Fixes - Fixed race condition in concurrent execution handler - Resolved memory leak in long-running workflows - Fixed timezone handling in scheduled triggers ## Breaking Changes - **BREAKING:** API v1 endpoints deprecated, will be removed in v4.0 - **BREAKING:** Minimum Python version now 3.9 ## Upgrade Instructions ```bash cloudflow upgrade --version 3.2.0 ``` ## Known Issues None at this time.",
              "score": null
            },
            {
              "doc_id": "howto_implement_workflow_versioning",
              "chunk_id": "howto_implement_workflow_versioning_fix_0",
              "content": "# How to Implement Workflow Versioning ## Prerequisites - Existing workflow ## Steps ### Step 1: Enable versioning Turn on versioning for your workflow. This is required for the feature. ```bash cloudflow workflows update wf_123 --enable-versioning ``` ### Step 2: Create new version Save changes as a new version instead of overwriting. ```bash cloudflow workflows publish wf_123 --version 2.0.0 ``` ### Step 3: Set active version Choose which version handles new executions. ```bash cloudflow workflows activate wf_123 --version 2.0.0 ``` ### Step 4: Roll back if needed Quickly revert to a previous version. ```bash cloudflow workflows rollback wf_123 --to-version 1.0.0 ``` ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            },
            {
              "doc_id": "howto_implement_retry_logic",
              "chunk_id": "howto_implement_retry_logic_fix_0",
              "content": "# How to Implement Retry Logic ## Prerequisites - Existing workflow ## Steps ### Step 1: Define retry policy Add retry configuration to your workflow step. > **Note:** Maximum 5 retry attempts allowed ### Step 2: Configure backoff Set exponential backoff with base delay of 1 second. The formula is: delay = base * 2^attempt. ### Step 3: Add timeout Set maximum execution time to prevent infinite loops. Default timeout is 30 seconds. ### Step 4: Handle final failure Configure on_failure handler to notify or escalate when all retries are exhausted. ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            }
          ],
          "latency_ms": 4.435741000634152,
          "query_id": "simple_008",
          "dimension": "negation",
          "query": "why can't I add more steps to my workflow"
        },
        {
          "key_facts": [
            "30000",
            "GATEWAY_TIMEOUT_MS"
          ],
          "found_facts": [
            "30000",
            "GATEWAY_TIMEOUT_MS"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "config_api_gateway"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "config_api_gateway",
              "chunk_id": "config_api_gateway_fix_0",
              "content": "# API Gateway Configuration Reference ## Overview This document describes all configuration options for API Gateway. ## Environment Variables | Variable | Type | Default | Description | |----------|------|---------|-------------| | `GATEWAY_PORT` | integer | 8000 | Gateway listen port | | `GATEWAY_RATE_LIMIT` | integer | 100 | Requests per minute per user | | `GATEWAY_TIMEOUT_MS` | integer | 30000 | Request timeout in milliseconds | | `GATEWAY_MAX_BODY_SIZE` | string | 10MB | Maximum request body size | | `GATEWAY_CORS_ORIGINS` | string | * | Allowed CORS origins | | `GATEWAY_SSL_ENABLED` | boolean | true | Enable TLS termination | ## Configuration File Configuration can also be provided via `api gateway.yaml`: ```yaml gateway_port: 8000 gateway_rate_limit: 100 gateway_timeout_ms: 30000 gateway_max_body_size: 10MB gateway_cors_origins: * ``` ## Validation Configuration is validated at startup. Invalid configuration will prevent the service from starting.",
              "score": null
            },
            {
              "doc_id": "arch_api_gateway",
              "chunk_id": "arch_api_gateway_fix_0",
              "content": "# API Gateway Architecture ## Overview The API Gateway handles all incoming HTTP requests, providing authentication, rate limiting, and request routing. Built on Kong with custom plugins for CloudFlow-specific functionality. ## Design Principles - **Zero Trust**: Every request is authenticated and authorized - **Defense in Depth**: Multiple layers of security validation ## Components ### Kong Gateway Primary ingress controller. Handles TLS termination, request routing, and plugin execution. **Technology:** Kong 3.4 on Kubernetes **Scaling:** Horizontal with HPA ### Auth Plugin Validates JWT tokens, API keys, and OAuth2 flows. Token validation latency under 5ms. **Technology:** Custom Lua plugin **Scaling:** Stateless ### Rate Limiter Enforces per-user and per-IP rate limits. Default limit is 100 requests per minute. **Technology:** Redis-backed sliding window **Scaling:** Shared Redis cluster ## Data Flow Client \u2192 TLS \u2192 Kong \u2192 Auth \u2192 Rate Limit \u2192 Backend Services ## Performance Characteristics - **Latency P50:** 12ms - **Latency P99:** 45ms - **Throughput:** 50000 requests/second",
              "score": null
            },
            {
              "doc_id": "adr_007",
              "chunk_id": "adr_007_fix_0",
              "content": "# ADR-007: Implement Rate Limiting at API Gateway **Status:** Accepted **Date:** 2024-09-03 ## Context We need to protect our services from abuse and ensure fair resource usage across customers. ## Options Considered ### Option 1: Sliding window at gateway Rate limit at API gateway level. **Pros:** - Centralized enforcement - Low latency - Easy configuration **Cons:** - Gateway becomes bottleneck - Coarse granularity - Redis dependency ### Option 2: Per-service limiting Each service implements its own limits. **Pros:** - Fine-grained control - Service autonomy - No single point of failure **Cons:** - Inconsistent implementation - Higher complexity - Distributed state ### Option 3: Token bucket Token-based rate limiting. **Pros:** - Allows bursts - Smooth rate limiting - Well understood **Cons:** - More complex - Harder to explain to users - State management ## Decision We will implement sliding window rate limiting at the Kong API Gateway using Redis. ## Consequences - Default limit: 100 requests per minute - Implement per-plan limits - Return rate limit headers - Add retry-after on 429",
              "score": null
            },
            {
              "doc_id": "arch_authentication_service",
              "chunk_id": "arch_authentication_service_fix_0",
              "content": "# Authentication Service Architecture ## Overview The Authentication Service handles user identity, session management, and token issuance. Supports OAuth 2.0, SAML, and API key authentication. ## Design Principles - **Zero Knowledge**: Passwords never stored in plaintext - **Token Rotation**: Refresh tokens rotate on each use ## Components ### Identity Provider Manages user accounts and credentials. Password hashing uses bcrypt with cost factor 12. **Technology:** Custom Go service **Scaling:** Horizontal stateless ### Token Service Issues and validates access tokens. Token lifetime is 1 hour with 7-day refresh tokens. **Technology:** JWT with RS256 **Scaling:** Horizontal stateless ### SSO Gateway Integrates with external identity providers. Supports Google, Microsoft, Okta, and custom SAML. **Technology:** Dex OIDC proxy **Scaling:** Active-passive ## Data Flow Credentials \u2192 Validation \u2192 Token Issue \u2192 Token Refresh \u2192 Logout ## Performance Characteristics - **Latency P50:** 50ms - **Latency P99:** 200ms - **Throughput:** 5000 auth/second",
              "score": null
            },
            {
              "doc_id": "api_metrics",
              "chunk_id": "api_metrics_fix_0",
              "content": "# Metrics API Reference ## Overview The Metrics API provides programmatic access to metrics functionality. Base URL: `https://api.cloudflow.io/v1/metrics` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /metrics/workflows Get workflow performance metrics. **Parameters:** - `period` (string): Time period: hour, day, week, month - `workflow_id` (string): Filter by workflow ID **Response:** - Success: `200` - Returns metrics data - Error: `400` - Bad Request **Example:** ```json {\"success_rate\": 0.985, \"avg_duration_ms\": 1250, \"p99_duration_ms\": 5000, \"total_executions\": 10000} ```",
              "score": null
            }
          ],
          "latency_ms": 4.160487000262947,
          "query_id": "simple_009",
          "dimension": "original",
          "query": "What is the API gateway timeout?"
        },
        {
          "key_facts": [
            "30000",
            "GATEWAY_TIMEOUT_MS"
          ],
          "found_facts": [
            "30000",
            "GATEWAY_TIMEOUT_MS"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "config_api_gateway"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "arch_api_gateway",
              "chunk_id": "arch_api_gateway_fix_0",
              "content": "# API Gateway Architecture ## Overview The API Gateway handles all incoming HTTP requests, providing authentication, rate limiting, and request routing. Built on Kong with custom plugins for CloudFlow-specific functionality. ## Design Principles - **Zero Trust**: Every request is authenticated and authorized - **Defense in Depth**: Multiple layers of security validation ## Components ### Kong Gateway Primary ingress controller. Handles TLS termination, request routing, and plugin execution. **Technology:** Kong 3.4 on Kubernetes **Scaling:** Horizontal with HPA ### Auth Plugin Validates JWT tokens, API keys, and OAuth2 flows. Token validation latency under 5ms. **Technology:** Custom Lua plugin **Scaling:** Stateless ### Rate Limiter Enforces per-user and per-IP rate limits. Default limit is 100 requests per minute. **Technology:** Redis-backed sliding window **Scaling:** Shared Redis cluster ## Data Flow Client \u2192 TLS \u2192 Kong \u2192 Auth \u2192 Rate Limit \u2192 Backend Services ## Performance Characteristics - **Latency P50:** 12ms - **Latency P99:** 45ms - **Throughput:** 50000 requests/second",
              "score": null
            },
            {
              "doc_id": "config_api_gateway",
              "chunk_id": "config_api_gateway_fix_0",
              "content": "# API Gateway Configuration Reference ## Overview This document describes all configuration options for API Gateway. ## Environment Variables | Variable | Type | Default | Description | |----------|------|---------|-------------| | `GATEWAY_PORT` | integer | 8000 | Gateway listen port | | `GATEWAY_RATE_LIMIT` | integer | 100 | Requests per minute per user | | `GATEWAY_TIMEOUT_MS` | integer | 30000 | Request timeout in milliseconds | | `GATEWAY_MAX_BODY_SIZE` | string | 10MB | Maximum request body size | | `GATEWAY_CORS_ORIGINS` | string | * | Allowed CORS origins | | `GATEWAY_SSL_ENABLED` | boolean | true | Enable TLS termination | ## Configuration File Configuration can also be provided via `api gateway.yaml`: ```yaml gateway_port: 8000 gateway_rate_limit: 100 gateway_timeout_ms: 30000 gateway_max_body_size: 10MB gateway_cors_origins: * ``` ## Validation Configuration is validated at startup. Invalid configuration will prevent the service from starting.",
              "score": null
            },
            {
              "doc_id": "adr_007",
              "chunk_id": "adr_007_fix_0",
              "content": "# ADR-007: Implement Rate Limiting at API Gateway **Status:** Accepted **Date:** 2024-09-03 ## Context We need to protect our services from abuse and ensure fair resource usage across customers. ## Options Considered ### Option 1: Sliding window at gateway Rate limit at API gateway level. **Pros:** - Centralized enforcement - Low latency - Easy configuration **Cons:** - Gateway becomes bottleneck - Coarse granularity - Redis dependency ### Option 2: Per-service limiting Each service implements its own limits. **Pros:** - Fine-grained control - Service autonomy - No single point of failure **Cons:** - Inconsistent implementation - Higher complexity - Distributed state ### Option 3: Token bucket Token-based rate limiting. **Pros:** - Allows bursts - Smooth rate limiting - Well understood **Cons:** - More complex - Harder to explain to users - State management ## Decision We will implement sliding window rate limiting at the Kong API Gateway using Redis. ## Consequences - Default limit: 100 requests per minute - Implement per-plan limits - Return rate limit headers - Add retry-after on 429",
              "score": null
            },
            {
              "doc_id": "howto_implement_retry_logic",
              "chunk_id": "howto_implement_retry_logic_fix_0",
              "content": "# How to Implement Retry Logic ## Prerequisites - Existing workflow ## Steps ### Step 1: Define retry policy Add retry configuration to your workflow step. > **Note:** Maximum 5 retry attempts allowed ### Step 2: Configure backoff Set exponential backoff with base delay of 1 second. The formula is: delay = base * 2^attempt. ### Step 3: Add timeout Set maximum execution time to prevent infinite loops. Default timeout is 30 seconds. ### Step 4: Handle final failure Configure on_failure handler to notify or escalate when all retries are exhausted. ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            },
            {
              "doc_id": "arch_authentication_service",
              "chunk_id": "arch_authentication_service_fix_0",
              "content": "# Authentication Service Architecture ## Overview The Authentication Service handles user identity, session management, and token issuance. Supports OAuth 2.0, SAML, and API key authentication. ## Design Principles - **Zero Knowledge**: Passwords never stored in plaintext - **Token Rotation**: Refresh tokens rotate on each use ## Components ### Identity Provider Manages user accounts and credentials. Password hashing uses bcrypt with cost factor 12. **Technology:** Custom Go service **Scaling:** Horizontal stateless ### Token Service Issues and validates access tokens. Token lifetime is 1 hour with 7-day refresh tokens. **Technology:** JWT with RS256 **Scaling:** Horizontal stateless ### SSO Gateway Integrates with external identity providers. Supports Google, Microsoft, Okta, and custom SAML. **Technology:** Dex OIDC proxy **Scaling:** Active-passive ## Data Flow Credentials \u2192 Validation \u2192 Token Issue \u2192 Token Refresh \u2192 Logout ## Performance Characteristics - **Latency P50:** 50ms - **Latency P99:** 200ms - **Throughput:** 5000 auth/second",
              "score": null
            }
          ],
          "latency_ms": 3.4043100004055304,
          "query_id": "simple_009",
          "dimension": "synonym",
          "query": "what's the request timeout at the gateway level"
        },
        {
          "key_facts": [
            "30000",
            "GATEWAY_TIMEOUT_MS"
          ],
          "found_facts": [
            "30000",
            "GATEWAY_TIMEOUT_MS"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "config_api_gateway"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "config_api_gateway",
              "chunk_id": "config_api_gateway_fix_0",
              "content": "# API Gateway Configuration Reference ## Overview This document describes all configuration options for API Gateway. ## Environment Variables | Variable | Type | Default | Description | |----------|------|---------|-------------| | `GATEWAY_PORT` | integer | 8000 | Gateway listen port | | `GATEWAY_RATE_LIMIT` | integer | 100 | Requests per minute per user | | `GATEWAY_TIMEOUT_MS` | integer | 30000 | Request timeout in milliseconds | | `GATEWAY_MAX_BODY_SIZE` | string | 10MB | Maximum request body size | | `GATEWAY_CORS_ORIGINS` | string | * | Allowed CORS origins | | `GATEWAY_SSL_ENABLED` | boolean | true | Enable TLS termination | ## Configuration File Configuration can also be provided via `api gateway.yaml`: ```yaml gateway_port: 8000 gateway_rate_limit: 100 gateway_timeout_ms: 30000 gateway_max_body_size: 10MB gateway_cors_origins: * ``` ## Validation Configuration is validated at startup. Invalid configuration will prevent the service from starting.",
              "score": null
            },
            {
              "doc_id": "arch_api_gateway",
              "chunk_id": "arch_api_gateway_fix_0",
              "content": "# API Gateway Architecture ## Overview The API Gateway handles all incoming HTTP requests, providing authentication, rate limiting, and request routing. Built on Kong with custom plugins for CloudFlow-specific functionality. ## Design Principles - **Zero Trust**: Every request is authenticated and authorized - **Defense in Depth**: Multiple layers of security validation ## Components ### Kong Gateway Primary ingress controller. Handles TLS termination, request routing, and plugin execution. **Technology:** Kong 3.4 on Kubernetes **Scaling:** Horizontal with HPA ### Auth Plugin Validates JWT tokens, API keys, and OAuth2 flows. Token validation latency under 5ms. **Technology:** Custom Lua plugin **Scaling:** Stateless ### Rate Limiter Enforces per-user and per-IP rate limits. Default limit is 100 requests per minute. **Technology:** Redis-backed sliding window **Scaling:** Shared Redis cluster ## Data Flow Client \u2192 TLS \u2192 Kong \u2192 Auth \u2192 Rate Limit \u2192 Backend Services ## Performance Characteristics - **Latency P50:** 12ms - **Latency P99:** 45ms - **Throughput:** 50000 requests/second",
              "score": null
            },
            {
              "doc_id": "adr_007",
              "chunk_id": "adr_007_fix_0",
              "content": "# ADR-007: Implement Rate Limiting at API Gateway **Status:** Accepted **Date:** 2024-09-03 ## Context We need to protect our services from abuse and ensure fair resource usage across customers. ## Options Considered ### Option 1: Sliding window at gateway Rate limit at API gateway level. **Pros:** - Centralized enforcement - Low latency - Easy configuration **Cons:** - Gateway becomes bottleneck - Coarse granularity - Redis dependency ### Option 2: Per-service limiting Each service implements its own limits. **Pros:** - Fine-grained control - Service autonomy - No single point of failure **Cons:** - Inconsistent implementation - Higher complexity - Distributed state ### Option 3: Token bucket Token-based rate limiting. **Pros:** - Allows bursts - Smooth rate limiting - Well understood **Cons:** - More complex - Harder to explain to users - State management ## Decision We will implement sliding window rate limiting at the Kong API Gateway using Redis. ## Consequences - Default limit: 100 requests per minute - Implement per-plan limits - Return rate limit headers - Add retry-after on 429",
              "score": null
            },
            {
              "doc_id": "api_executions",
              "chunk_id": "api_executions_fix_0",
              "content": "# Executions API Reference ## Overview The Executions API provides programmatic access to executions functionality. Base URL: `https://api.cloudflow.io/v1/executions` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /executions List workflow executions. **Parameters:** - `workflow_id` (string): Filter by workflow ID - `since` (datetime): Filter executions after this timestamp **Response:** - Success: `200` - Returns array of execution objects - Error: `400` - Bad Request **Example:** ```json {\"executions\": [{\"id\": \"ex_789\", \"status\": \"completed\", \"duration_ms\": 1523}]} ``` ### POST /executions/{id}/cancel Cancel a running execution. **Parameters:** - `id` (string): Execution ID - `reason` (string): Cancellation reason (optional) **Response:** - Success: `200` - Execution cancelled - Error: `400` - Bad Request **Example:** ```json {\"id\": \"ex_789\", \"status\": \"cancelled\", \"cancelled_at\": \"2024-01-15T10:05:00Z\"} ```",
              "score": null
            },
            {
              "doc_id": "runbook_api_gateway_5xx_errors",
              "chunk_id": "runbook_api_gateway_5xx_errors_fix_0",
              "content": "# Runbook: API Gateway 5xx Errors ## Severity **Level:** P3 - Medium **Response Time:** 1 hour ## Symptoms - Error rate > 1% - 5xx responses in access logs - Downstream service failures ## Diagnosis ### Step 1 Check Kong error logs: `kubectl logs -l app=kong -n kong --tail=100 | grep -E '5[0-9]{2}'` ### Step 2 Verify backend health: `kubectl get pods -n production -o wide` ### Step 3 Check service endpoints: `kubectl get endpoints -n production` ### Step 4 Test backend directly: `kubectl exec -it kong-xxx -n kong -- curl http://api.production.svc:8080/health` ## Resolution ### Step 1 Restart unhealthy backends: `kubectl rollout restart deployment/api -n production` ### Step 2 Enable circuit breaker if cascading: update Kong circuit-breaker plugin ### Step 3 Scale up backends: `kubectl scale deployment/api --replicas=5 -n production` ### Step 4 Check for resource exhaustion on backend pods ### Step 5 Verify network policies allow traffic ## Escalation If 5xx persists > 15 minutes, page on-call. May indicate infrastructure issue. ## Post-Incident - Document timeline in incident log - Schedule post-mortem within 48 hours - Update runbook if new patterns discovered",
              "score": null
            }
          ],
          "latency_ms": 3.392237000298337,
          "query_id": "simple_009",
          "dimension": "problem",
          "query": "long-running API calls timing out with 504 gateway timeout"
        },
        {
          "key_facts": [
            "30000",
            "GATEWAY_TIMEOUT_MS"
          ],
          "found_facts": [
            "30000",
            "GATEWAY_TIMEOUT_MS"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "config_api_gateway"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "config_api_gateway",
              "chunk_id": "config_api_gateway_fix_0",
              "content": "# API Gateway Configuration Reference ## Overview This document describes all configuration options for API Gateway. ## Environment Variables | Variable | Type | Default | Description | |----------|------|---------|-------------| | `GATEWAY_PORT` | integer | 8000 | Gateway listen port | | `GATEWAY_RATE_LIMIT` | integer | 100 | Requests per minute per user | | `GATEWAY_TIMEOUT_MS` | integer | 30000 | Request timeout in milliseconds | | `GATEWAY_MAX_BODY_SIZE` | string | 10MB | Maximum request body size | | `GATEWAY_CORS_ORIGINS` | string | * | Allowed CORS origins | | `GATEWAY_SSL_ENABLED` | boolean | true | Enable TLS termination | ## Configuration File Configuration can also be provided via `api gateway.yaml`: ```yaml gateway_port: 8000 gateway_rate_limit: 100 gateway_timeout_ms: 30000 gateway_max_body_size: 10MB gateway_cors_origins: * ``` ## Validation Configuration is validated at startup. Invalid configuration will prevent the service from starting.",
              "score": null
            },
            {
              "doc_id": "arch_api_gateway",
              "chunk_id": "arch_api_gateway_fix_0",
              "content": "# API Gateway Architecture ## Overview The API Gateway handles all incoming HTTP requests, providing authentication, rate limiting, and request routing. Built on Kong with custom plugins for CloudFlow-specific functionality. ## Design Principles - **Zero Trust**: Every request is authenticated and authorized - **Defense in Depth**: Multiple layers of security validation ## Components ### Kong Gateway Primary ingress controller. Handles TLS termination, request routing, and plugin execution. **Technology:** Kong 3.4 on Kubernetes **Scaling:** Horizontal with HPA ### Auth Plugin Validates JWT tokens, API keys, and OAuth2 flows. Token validation latency under 5ms. **Technology:** Custom Lua plugin **Scaling:** Stateless ### Rate Limiter Enforces per-user and per-IP rate limits. Default limit is 100 requests per minute. **Technology:** Redis-backed sliding window **Scaling:** Shared Redis cluster ## Data Flow Client \u2192 TLS \u2192 Kong \u2192 Auth \u2192 Rate Limit \u2192 Backend Services ## Performance Characteristics - **Latency P50:** 12ms - **Latency P99:** 45ms - **Throughput:** 50000 requests/second",
              "score": null
            },
            {
              "doc_id": "howto_implement_retry_logic",
              "chunk_id": "howto_implement_retry_logic_fix_0",
              "content": "# How to Implement Retry Logic ## Prerequisites - Existing workflow ## Steps ### Step 1: Define retry policy Add retry configuration to your workflow step. > **Note:** Maximum 5 retry attempts allowed ### Step 2: Configure backoff Set exponential backoff with base delay of 1 second. The formula is: delay = base * 2^attempt. ### Step 3: Add timeout Set maximum execution time to prevent infinite loops. Default timeout is 30 seconds. ### Step 4: Handle final failure Configure on_failure handler to notify or escalate when all retries are exhausted. ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            },
            {
              "doc_id": "runbook_database_connection_exhaustion",
              "chunk_id": "runbook_database_connection_exhaustion_fix_0",
              "content": "# Runbook: Database Connection Exhaustion ## Severity **Level:** P1 - Critical **Response Time:** 15 minutes ## Symptoms - Connection pool at 100% - Queries timing out - 'too many connections' errors in logs ## Diagnosis ### Step 1 Check active connections: `SELECT count(*) FROM pg_stat_activity` ### Step 2 Identify long-running queries: `SELECT pid, now() - pg_stat_activity.query_start AS duration, query FROM pg_stat_activity WHERE state != 'idle' ORDER BY duration DESC` ### Step 3 Check PgBouncer stats: `SHOW POOLS` ### Step 4 Verify connection limits: `SHOW max_connections` ## Resolution ### Step 1 Kill long-running queries: `SELECT pg_terminate_backend(pid) FROM pg_stat_activity WHERE duration > interval '5 minutes'` ### Step 2 Increase PgBouncer pool size temporarily: update pgbouncer.ini ### Step 3 Restart affected services to release connections: `kubectl rollout restart deployment/api` ### Step 4 Scale down non-critical services to free connections ## Escalation If connections remain exhausted, escalate to DBA. Consider enabling connection queueing. ## Post-Incident - Document timeline in incident log - Schedule post-mortem within 48 hours - Update runbook if new patterns discovered",
              "score": null
            },
            {
              "doc_id": "adr_007",
              "chunk_id": "adr_007_fix_0",
              "content": "# ADR-007: Implement Rate Limiting at API Gateway **Status:** Accepted **Date:** 2024-09-03 ## Context We need to protect our services from abuse and ensure fair resource usage across customers. ## Options Considered ### Option 1: Sliding window at gateway Rate limit at API gateway level. **Pros:** - Centralized enforcement - Low latency - Easy configuration **Cons:** - Gateway becomes bottleneck - Coarse granularity - Redis dependency ### Option 2: Per-service limiting Each service implements its own limits. **Pros:** - Fine-grained control - Service autonomy - No single point of failure **Cons:** - Inconsistent implementation - Higher complexity - Distributed state ### Option 3: Token bucket Token-based rate limiting. **Pros:** - Allows bursts - Smooth rate limiting - Well understood **Cons:** - More complex - Harder to explain to users - State management ## Decision We will implement sliding window rate limiting at the Kong API Gateway using Redis. ## Consequences - Default limit: 100 requests per minute - Implement per-plan limits - Return rate limit headers - Add retry-after on 429",
              "score": null
            }
          ],
          "latency_ms": 3.6694939990411513,
          "query_id": "simple_009",
          "dimension": "casual",
          "query": "gateway timeout config"
        },
        {
          "key_facts": [
            "30000",
            "GATEWAY_TIMEOUT_MS"
          ],
          "found_facts": [
            "30000",
            "GATEWAY_TIMEOUT_MS"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "config_api_gateway"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "arch_api_gateway",
              "chunk_id": "arch_api_gateway_fix_0",
              "content": "# API Gateway Architecture ## Overview The API Gateway handles all incoming HTTP requests, providing authentication, rate limiting, and request routing. Built on Kong with custom plugins for CloudFlow-specific functionality. ## Design Principles - **Zero Trust**: Every request is authenticated and authorized - **Defense in Depth**: Multiple layers of security validation ## Components ### Kong Gateway Primary ingress controller. Handles TLS termination, request routing, and plugin execution. **Technology:** Kong 3.4 on Kubernetes **Scaling:** Horizontal with HPA ### Auth Plugin Validates JWT tokens, API keys, and OAuth2 flows. Token validation latency under 5ms. **Technology:** Custom Lua plugin **Scaling:** Stateless ### Rate Limiter Enforces per-user and per-IP rate limits. Default limit is 100 requests per minute. **Technology:** Redis-backed sliding window **Scaling:** Shared Redis cluster ## Data Flow Client \u2192 TLS \u2192 Kong \u2192 Auth \u2192 Rate Limit \u2192 Backend Services ## Performance Characteristics - **Latency P50:** 12ms - **Latency P99:** 45ms - **Throughput:** 50000 requests/second",
              "score": null
            },
            {
              "doc_id": "adr_007",
              "chunk_id": "adr_007_fix_0",
              "content": "# ADR-007: Implement Rate Limiting at API Gateway **Status:** Accepted **Date:** 2024-09-03 ## Context We need to protect our services from abuse and ensure fair resource usage across customers. ## Options Considered ### Option 1: Sliding window at gateway Rate limit at API gateway level. **Pros:** - Centralized enforcement - Low latency - Easy configuration **Cons:** - Gateway becomes bottleneck - Coarse granularity - Redis dependency ### Option 2: Per-service limiting Each service implements its own limits. **Pros:** - Fine-grained control - Service autonomy - No single point of failure **Cons:** - Inconsistent implementation - Higher complexity - Distributed state ### Option 3: Token bucket Token-based rate limiting. **Pros:** - Allows bursts - Smooth rate limiting - Well understood **Cons:** - More complex - Harder to explain to users - State management ## Decision We will implement sliding window rate limiting at the Kong API Gateway using Redis. ## Consequences - Default limit: 100 requests per minute - Implement per-plan limits - Return rate limit headers - Add retry-after on 429",
              "score": null
            },
            {
              "doc_id": "config_api_gateway",
              "chunk_id": "config_api_gateway_fix_0",
              "content": "# API Gateway Configuration Reference ## Overview This document describes all configuration options for API Gateway. ## Environment Variables | Variable | Type | Default | Description | |----------|------|---------|-------------| | `GATEWAY_PORT` | integer | 8000 | Gateway listen port | | `GATEWAY_RATE_LIMIT` | integer | 100 | Requests per minute per user | | `GATEWAY_TIMEOUT_MS` | integer | 30000 | Request timeout in milliseconds | | `GATEWAY_MAX_BODY_SIZE` | string | 10MB | Maximum request body size | | `GATEWAY_CORS_ORIGINS` | string | * | Allowed CORS origins | | `GATEWAY_SSL_ENABLED` | boolean | true | Enable TLS termination | ## Configuration File Configuration can also be provided via `api gateway.yaml`: ```yaml gateway_port: 8000 gateway_rate_limit: 100 gateway_timeout_ms: 30000 gateway_max_body_size: 10MB gateway_cors_origins: * ``` ## Validation Configuration is validated at startup. Invalid configuration will prevent the service from starting.",
              "score": null
            },
            {
              "doc_id": "runbook_database_connection_exhaustion",
              "chunk_id": "runbook_database_connection_exhaustion_fix_0",
              "content": "# Runbook: Database Connection Exhaustion ## Severity **Level:** P1 - Critical **Response Time:** 15 minutes ## Symptoms - Connection pool at 100% - Queries timing out - 'too many connections' errors in logs ## Diagnosis ### Step 1 Check active connections: `SELECT count(*) FROM pg_stat_activity` ### Step 2 Identify long-running queries: `SELECT pid, now() - pg_stat_activity.query_start AS duration, query FROM pg_stat_activity WHERE state != 'idle' ORDER BY duration DESC` ### Step 3 Check PgBouncer stats: `SHOW POOLS` ### Step 4 Verify connection limits: `SHOW max_connections` ## Resolution ### Step 1 Kill long-running queries: `SELECT pg_terminate_backend(pid) FROM pg_stat_activity WHERE duration > interval '5 minutes'` ### Step 2 Increase PgBouncer pool size temporarily: update pgbouncer.ini ### Step 3 Restart affected services to release connections: `kubectl rollout restart deployment/api` ### Step 4 Scale down non-critical services to free connections ## Escalation If connections remain exhausted, escalate to DBA. Consider enabling connection queueing. ## Post-Incident - Document timeline in incident log - Schedule post-mortem within 48 hours - Update runbook if new patterns discovered",
              "score": null
            },
            {
              "doc_id": "howto_implement_retry_logic",
              "chunk_id": "howto_implement_retry_logic_fix_0",
              "content": "# How to Implement Retry Logic ## Prerequisites - Existing workflow ## Steps ### Step 1: Define retry policy Add retry configuration to your workflow step. > **Note:** Maximum 5 retry attempts allowed ### Step 2: Configure backoff Set exponential backoff with base delay of 1 second. The formula is: delay = base * 2^attempt. ### Step 3: Add timeout Set maximum execution time to prevent infinite loops. Default timeout is 30 seconds. ### Step 4: Handle final failure Configure on_failure handler to notify or escalate when all retries are exhausted. ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            }
          ],
          "latency_ms": 4.262096999809728,
          "query_id": "simple_009",
          "dimension": "contextual",
          "query": "have a slow endpoint that takes 45 seconds, will it hit the gateway timeout"
        },
        {
          "key_facts": [
            "30000",
            "GATEWAY_TIMEOUT_MS"
          ],
          "found_facts": [],
          "missed_facts": [
            "30000",
            "GATEWAY_TIMEOUT_MS"
          ],
          "coverage": 0.0,
          "expected_docs": [
            "config_api_gateway"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "adr_007",
              "chunk_id": "adr_007_fix_0",
              "content": "# ADR-007: Implement Rate Limiting at API Gateway **Status:** Accepted **Date:** 2024-09-03 ## Context We need to protect our services from abuse and ensure fair resource usage across customers. ## Options Considered ### Option 1: Sliding window at gateway Rate limit at API gateway level. **Pros:** - Centralized enforcement - Low latency - Easy configuration **Cons:** - Gateway becomes bottleneck - Coarse granularity - Redis dependency ### Option 2: Per-service limiting Each service implements its own limits. **Pros:** - Fine-grained control - Service autonomy - No single point of failure **Cons:** - Inconsistent implementation - Higher complexity - Distributed state ### Option 3: Token bucket Token-based rate limiting. **Pros:** - Allows bursts - Smooth rate limiting - Well understood **Cons:** - More complex - Harder to explain to users - State management ## Decision We will implement sliding window rate limiting at the Kong API Gateway using Redis. ## Consequences - Default limit: 100 requests per minute - Implement per-plan limits - Return rate limit headers - Add retry-after on 429",
              "score": null
            },
            {
              "doc_id": "runbook_database_connection_exhaustion",
              "chunk_id": "runbook_database_connection_exhaustion_fix_0",
              "content": "# Runbook: Database Connection Exhaustion ## Severity **Level:** P1 - Critical **Response Time:** 15 minutes ## Symptoms - Connection pool at 100% - Queries timing out - 'too many connections' errors in logs ## Diagnosis ### Step 1 Check active connections: `SELECT count(*) FROM pg_stat_activity` ### Step 2 Identify long-running queries: `SELECT pid, now() - pg_stat_activity.query_start AS duration, query FROM pg_stat_activity WHERE state != 'idle' ORDER BY duration DESC` ### Step 3 Check PgBouncer stats: `SHOW POOLS` ### Step 4 Verify connection limits: `SHOW max_connections` ## Resolution ### Step 1 Kill long-running queries: `SELECT pg_terminate_backend(pid) FROM pg_stat_activity WHERE duration > interval '5 minutes'` ### Step 2 Increase PgBouncer pool size temporarily: update pgbouncer.ini ### Step 3 Restart affected services to release connections: `kubectl rollout restart deployment/api` ### Step 4 Scale down non-critical services to free connections ## Escalation If connections remain exhausted, escalate to DBA. Consider enabling connection queueing. ## Post-Incident - Document timeline in incident log - Schedule post-mortem within 48 hours - Update runbook if new patterns discovered",
              "score": null
            },
            {
              "doc_id": "arch_caching_layer",
              "chunk_id": "arch_caching_layer_fix_0",
              "content": "# Caching Layer Architecture ## Overview The Caching Layer reduces latency and database load by caching frequently accessed data. Uses a multi-tier strategy with local and distributed caches. ## Design Principles - **Cache Invalidation**: Explicit invalidation on writes - **Graceful Degradation**: System works without cache ## Components ### Local Cache First-level cache with 100ms TTL. Reduces Redis round-trips for hot data. **Technology:** Ristretto in-memory cache **Scaling:** Per-pod memory allocation ### Distributed Cache Second-level cache with configurable TTL. Maximum key size 512MB. **Technology:** Redis Cluster 7.x **Scaling:** 6-node cluster with sharding ### CDN Caches static assets and API responses. Cache hit ratio target 95%. **Technology:** CloudFront with Lambda@Edge **Scaling:** Global edge locations ## Data Flow Request \u2192 Local Cache \u2192 Redis \u2192 Database \u2192 Cache Population ## Performance Characteristics - **Latency P50:** 1ms (hit), 50ms (miss) - **Latency P99:** 5ms (hit), 200ms (miss) - **Throughput:** 100000 reads/second",
              "score": null
            },
            {
              "doc_id": "arch_search_service",
              "chunk_id": "arch_search_service_fix_0",
              "content": "# Search Service Architecture ## Overview The Search Service provides full-text and semantic search across workflows, executions, and documentation. Built on Elasticsearch with custom analyzers for technical content. ## Design Principles - **Relevance First**: Results ranked by relevance, not recency - **Typo Tolerance**: Handles misspellings and variations ## Components ### Indexer Indexes documents in near real-time. Typical indexing latency is 500ms. **Technology:** Custom Go service **Scaling:** Queue-based with backpressure ### Query Engine Executes search queries. Supports filters, facets, and aggregations. **Technology:** Elasticsearch 8.x **Scaling:** 3-node cluster with replicas ### Suggestion Service Provides autocomplete suggestions. Returns suggestions within 20ms. **Technology:** Completion suggester **Scaling:** In-memory FST ## Data Flow Document \u2192 Analyzer \u2192 Index \u2192 Query \u2192 Rank \u2192 Results ## Performance Characteristics - **Latency P50:** 25ms - **Latency P99:** 100ms - **Throughput:** 2000 queries/second",
              "score": null
            },
            {
              "doc_id": "arch_api_gateway",
              "chunk_id": "arch_api_gateway_fix_0",
              "content": "# API Gateway Architecture ## Overview The API Gateway handles all incoming HTTP requests, providing authentication, rate limiting, and request routing. Built on Kong with custom plugins for CloudFlow-specific functionality. ## Design Principles - **Zero Trust**: Every request is authenticated and authorized - **Defense in Depth**: Multiple layers of security validation ## Components ### Kong Gateway Primary ingress controller. Handles TLS termination, request routing, and plugin execution. **Technology:** Kong 3.4 on Kubernetes **Scaling:** Horizontal with HPA ### Auth Plugin Validates JWT tokens, API keys, and OAuth2 flows. Token validation latency under 5ms. **Technology:** Custom Lua plugin **Scaling:** Stateless ### Rate Limiter Enforces per-user and per-IP rate limits. Default limit is 100 requests per minute. **Technology:** Redis-backed sliding window **Scaling:** Shared Redis cluster ## Data Flow Client \u2192 TLS \u2192 Kong \u2192 Auth \u2192 Rate Limit \u2192 Backend Services ## Performance Characteristics - **Latency P50:** 12ms - **Latency P99:** 45ms - **Throughput:** 50000 requests/second",
              "score": null
            }
          ],
          "latency_ms": 3.9430629985872656,
          "query_id": "simple_009",
          "dimension": "negation",
          "query": "why are my slow requests getting cut off"
        },
        {
          "key_facts": [
            "Kafka",
            "event-driven"
          ],
          "found_facts": [
            "Kafka",
            "event-driven"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "adr_005"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "adr_005",
              "chunk_id": "adr_005_fix_0",
              "content": "# ADR-005: Adopt Event-Driven Architecture for Workflow Execution **Status:** Accepted **Date:** 2024-12-04 ## Context We need an architecture that supports asynchronous workflow execution, step dependencies, and failure recovery. ## Options Considered ### Option 1: Event-driven with Kafka Publish-subscribe messaging. **Pros:** - Loose coupling - Scalability - Event replay - Audit trail **Cons:** - Eventual consistency - Operational complexity - Message ordering challenges ### Option 2: Request-response Synchronous API calls. **Pros:** - Simple mental model - Immediate feedback - Easy debugging **Cons:** - Tight coupling - Cascading failures - Timeout issues ### Option 3: Polling-based Workers poll for tasks. **Pros:** - Simple implementation - Easy rate limiting - No message broker **Cons:** - Polling overhead - Higher latency - Inefficient at scale ## Decision We will use an event-driven architecture with Kafka for workflow step execution. ## Consequences - Implement idempotent consumers - Use dead letter queues for failed messages - Monitor consumer lag",
              "score": null
            },
            {
              "doc_id": "arch_notification_service",
              "chunk_id": "arch_notification_service_fix_0",
              "content": "# Notification Service Architecture ## Overview The Notification Service delivers alerts and updates through multiple channels including email, Slack, webhooks, and mobile push notifications. ## Design Principles - **Delivery Guarantee**: At-least-once delivery with deduplication - **Channel Abstraction**: Unified API regardless of delivery channel ## Components ### Dispatcher Routes notifications to appropriate channel handlers. Supports priority queues for urgent notifications. **Technology:** Go microservice **Scaling:** Horizontal with queue partitioning ### Email Provider Sends transactional emails. Maximum 10000 emails per hour per account. **Technology:** SendGrid integration **Scaling:** API rate limited ### Slack Integration Posts messages to Slack channels. Supports message formatting and interactive components. **Technology:** Slack Bolt SDK **Scaling:** Per-workspace rate limits ## Data Flow Event \u2192 Template \u2192 Dispatcher \u2192 Channel \u2192 Delivery Confirmation ## Performance Characteristics - **Latency P50:** 200ms - **Latency P99:** 2000ms - **Throughput:** 10000 notifications/minute",
              "score": null
            },
            {
              "doc_id": "arch_workflow_engine",
              "chunk_id": "arch_workflow_engine_fix_0",
              "content": "# Workflow Engine Architecture ## Overview The Workflow Engine is the core component responsible for executing user-defined workflows. It uses an event-driven architecture with support for parallel execution, conditional branching, and automatic retries. ## Design Principles - **Idempotency**: All operations can be safely retried without side effects - **Eventual Consistency**: State converges to consistent view within 5 seconds - **Fault Tolerance**: System continues operating despite component failures ## Components ### Scheduler Manages workflow scheduling and triggers. Supports cron expressions, webhooks, and event-based triggers. **Technology:** Custom Go service **Scaling:** Horizontal with leader election ### Executor Runs individual workflow steps in isolated containers. Maximum execution time is 30 minutes per step. **Technology:** Kubernetes Jobs **Scaling:** Auto-scaling based on queue depth ### State Store Persists workflow state, execution history, and step results. Uses optimistic locking for concurrent updates. **Technology:** PostgreSQL with JSONB **Scaling:** Primary-replica with read replicas ## Data Flow Triggers \u2192 Scheduler \u2192 Job Queue \u2192 Executor \u2192 State Store \u2192 Webhooks/Notifications ## Performance Characteristics - **Latency P50:** 45ms - **Latency P99:** 250ms - **Throughput:** 5000 executions/minute",
              "score": null
            },
            {
              "doc_id": "arch_monitoring_stack",
              "chunk_id": "arch_monitoring_stack_fix_0",
              "content": "# Monitoring Stack Architecture ## Overview The Monitoring Stack provides observability into system health, performance, and errors. Includes metrics, logs, traces, and alerting. ## Design Principles - **Observability**: Every component emits metrics and traces - **Actionable Alerts**: Alerts include runbook links ## Components ### Metrics Collects time-series metrics. Retention is 15 days local, 1 year in object storage. **Technology:** Prometheus with Thanos **Scaling:** Federated with long-term storage ### Logging Centralized log aggregation. Supports structured JSON logs with label indexing. **Technology:** Loki with Grafana **Scaling:** Distributed with sharding ### Tracing Distributed request tracing. Trace retention is 7 days. **Technology:** Jaeger with OpenTelemetry **Scaling:** Sampling at 10% ### Alerting Routes alerts to on-call engineers. Supports escalation policies and silencing. **Technology:** Alertmanager with PagerDuty **Scaling:** HA pair ## Data Flow Application \u2192 Collector \u2192 Storage \u2192 Query \u2192 Dashboard/Alert ## Performance Characteristics - **Latency P50:** N/A - **Latency P99:** N/A - **Throughput:** 1M metrics/second, 100GB logs/day",
              "score": null
            },
            {
              "doc_id": "release_3_1_0",
              "chunk_id": "release_3_1_0_fix_0",
              "content": "# Release Notes - v3.1.0 **Release Date:** 2023-12-01 ## New Features - GraphQL API beta release - Workflow templates marketplace - Enhanced audit logging - SSO support for Okta ## Bug Fixes - Fixed webhook delivery retry logic - Resolved issue with large payload handling - Fixed dashboard loading performance ## Upgrade Instructions ```bash cloudflow upgrade --version 3.1.0 ``` ## Known Issues None at this time.",
              "score": null
            }
          ],
          "latency_ms": 3.348366000864189,
          "query_id": "simple_010",
          "dimension": "original",
          "query": "What message broker is used for workflows?"
        },
        {
          "key_facts": [
            "Kafka",
            "event-driven"
          ],
          "found_facts": [
            "Kafka",
            "event-driven"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "adr_005"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "adr_005",
              "chunk_id": "adr_005_fix_0",
              "content": "# ADR-005: Adopt Event-Driven Architecture for Workflow Execution **Status:** Accepted **Date:** 2024-12-04 ## Context We need an architecture that supports asynchronous workflow execution, step dependencies, and failure recovery. ## Options Considered ### Option 1: Event-driven with Kafka Publish-subscribe messaging. **Pros:** - Loose coupling - Scalability - Event replay - Audit trail **Cons:** - Eventual consistency - Operational complexity - Message ordering challenges ### Option 2: Request-response Synchronous API calls. **Pros:** - Simple mental model - Immediate feedback - Easy debugging **Cons:** - Tight coupling - Cascading failures - Timeout issues ### Option 3: Polling-based Workers poll for tasks. **Pros:** - Simple implementation - Easy rate limiting - No message broker **Cons:** - Polling overhead - Higher latency - Inefficient at scale ## Decision We will use an event-driven architecture with Kafka for workflow step execution. ## Consequences - Implement idempotent consumers - Use dead letter queues for failed messages - Monitor consumer lag",
              "score": null
            },
            {
              "doc_id": "arch_notification_service",
              "chunk_id": "arch_notification_service_fix_0",
              "content": "# Notification Service Architecture ## Overview The Notification Service delivers alerts and updates through multiple channels including email, Slack, webhooks, and mobile push notifications. ## Design Principles - **Delivery Guarantee**: At-least-once delivery with deduplication - **Channel Abstraction**: Unified API regardless of delivery channel ## Components ### Dispatcher Routes notifications to appropriate channel handlers. Supports priority queues for urgent notifications. **Technology:** Go microservice **Scaling:** Horizontal with queue partitioning ### Email Provider Sends transactional emails. Maximum 10000 emails per hour per account. **Technology:** SendGrid integration **Scaling:** API rate limited ### Slack Integration Posts messages to Slack channels. Supports message formatting and interactive components. **Technology:** Slack Bolt SDK **Scaling:** Per-workspace rate limits ## Data Flow Event \u2192 Template \u2192 Dispatcher \u2192 Channel \u2192 Delivery Confirmation ## Performance Characteristics - **Latency P50:** 200ms - **Latency P99:** 2000ms - **Throughput:** 10000 notifications/minute",
              "score": null
            },
            {
              "doc_id": "arch_workflow_engine",
              "chunk_id": "arch_workflow_engine_fix_0",
              "content": "# Workflow Engine Architecture ## Overview The Workflow Engine is the core component responsible for executing user-defined workflows. It uses an event-driven architecture with support for parallel execution, conditional branching, and automatic retries. ## Design Principles - **Idempotency**: All operations can be safely retried without side effects - **Eventual Consistency**: State converges to consistent view within 5 seconds - **Fault Tolerance**: System continues operating despite component failures ## Components ### Scheduler Manages workflow scheduling and triggers. Supports cron expressions, webhooks, and event-based triggers. **Technology:** Custom Go service **Scaling:** Horizontal with leader election ### Executor Runs individual workflow steps in isolated containers. Maximum execution time is 30 minutes per step. **Technology:** Kubernetes Jobs **Scaling:** Auto-scaling based on queue depth ### State Store Persists workflow state, execution history, and step results. Uses optimistic locking for concurrent updates. **Technology:** PostgreSQL with JSONB **Scaling:** Primary-replica with read replicas ## Data Flow Triggers \u2192 Scheduler \u2192 Job Queue \u2192 Executor \u2192 State Store \u2192 Webhooks/Notifications ## Performance Characteristics - **Latency P50:** 45ms - **Latency P99:** 250ms - **Throughput:** 5000 executions/minute",
              "score": null
            },
            {
              "doc_id": "arch_monitoring_stack",
              "chunk_id": "arch_monitoring_stack_fix_0",
              "content": "# Monitoring Stack Architecture ## Overview The Monitoring Stack provides observability into system health, performance, and errors. Includes metrics, logs, traces, and alerting. ## Design Principles - **Observability**: Every component emits metrics and traces - **Actionable Alerts**: Alerts include runbook links ## Components ### Metrics Collects time-series metrics. Retention is 15 days local, 1 year in object storage. **Technology:** Prometheus with Thanos **Scaling:** Federated with long-term storage ### Logging Centralized log aggregation. Supports structured JSON logs with label indexing. **Technology:** Loki with Grafana **Scaling:** Distributed with sharding ### Tracing Distributed request tracing. Trace retention is 7 days. **Technology:** Jaeger with OpenTelemetry **Scaling:** Sampling at 10% ### Alerting Routes alerts to on-call engineers. Supports escalation policies and silencing. **Technology:** Alertmanager with PagerDuty **Scaling:** HA pair ## Data Flow Application \u2192 Collector \u2192 Storage \u2192 Query \u2192 Dashboard/Alert ## Performance Characteristics - **Latency P50:** N/A - **Latency P99:** N/A - **Throughput:** 1M metrics/second, 100GB logs/day",
              "score": null
            },
            {
              "doc_id": "howto_create_scheduled_workflow",
              "chunk_id": "howto_create_scheduled_workflow_fix_0",
              "content": "# How to Create Scheduled Workflow ## Prerequisites - Workflow definition ## Steps ### Step 1: Define schedule Use cron expression to set the schedule. Example: '0 9 * * MON-FRI' runs at 9 AM on weekdays. ### Step 2: Set timezone Specify timezone for the schedule. Default is UTC. > **Note:** Use IANA timezone names like America/New_York ### Step 3: Configure overlap handling Choose behavior when previous execution is still running: skip, queue, or cancel. ### Step 4: Enable notifications Get notified on schedule failures. ```bash cloudflow workflows notify --on-failure ``` ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            }
          ],
          "latency_ms": 3.471233998425305,
          "query_id": "simple_010",
          "dimension": "synonym",
          "query": "what messaging system handles workflow events"
        },
        {
          "key_facts": [
            "Kafka",
            "event-driven"
          ],
          "found_facts": [
            "Kafka"
          ],
          "missed_facts": [
            "event-driven"
          ],
          "coverage": 0.5,
          "expected_docs": [
            "adr_005"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "runbook_kafka_consumer_lag",
              "chunk_id": "runbook_kafka_consumer_lag_fix_0",
              "content": "# Runbook: Kafka Consumer Lag ## Severity **Level:** P1 - Critical **Response Time:** 1 hour ## Symptoms - Consumer lag > 10000 messages - Processing delays reported - Alerts from lag monitor ## Diagnosis ### Step 1 Check consumer group lag: `kafka-consumer-groups.sh --bootstrap-server kafka:9092 --describe --group workflow-executor` ### Step 2 Verify consumer health: `kubectl get pods -l app=workflow-executor -n production` ### Step 3 Check for processing errors: `kubectl logs -l app=workflow-executor --tail=100 -n production | grep ERROR` ### Step 4 Monitor throughput: `cloudflow metrics get kafka.consumer.records_per_second` ## Resolution ### Step 1 Scale up consumers: `kubectl scale deployment/workflow-executor --replicas=10 -n production` ### Step 2 Reset offset if messages are stale: `kafka-consumer-groups.sh --bootstrap-server kafka:9092 --group workflow-executor --reset-offsets --to-latest --execute --topic workflows` ### Step 3 Pause producer if backpressure needed: `cloudflow workflows pause-triggers` ### Step 4 Increase partition count for parallelism (requires coordination) ## Escalation If lag persists > 30 minutes, page platform team. May need Kafka cluster scaling. ## Post-Incident - Document timeline in incident log - Schedule post-mortem within 48 hours - Update runbook if new patterns discovered",
              "score": null
            },
            {
              "doc_id": "release_3_1_0",
              "chunk_id": "release_3_1_0_fix_0",
              "content": "# Release Notes - v3.1.0 **Release Date:** 2023-12-01 ## New Features - GraphQL API beta release - Workflow templates marketplace - Enhanced audit logging - SSO support for Okta ## Bug Fixes - Fixed webhook delivery retry logic - Resolved issue with large payload handling - Fixed dashboard loading performance ## Upgrade Instructions ```bash cloudflow upgrade --version 3.1.0 ``` ## Known Issues None at this time.",
              "score": null
            },
            {
              "doc_id": "arch_api_gateway",
              "chunk_id": "arch_api_gateway_fix_0",
              "content": "# API Gateway Architecture ## Overview The API Gateway handles all incoming HTTP requests, providing authentication, rate limiting, and request routing. Built on Kong with custom plugins for CloudFlow-specific functionality. ## Design Principles - **Zero Trust**: Every request is authenticated and authorized - **Defense in Depth**: Multiple layers of security validation ## Components ### Kong Gateway Primary ingress controller. Handles TLS termination, request routing, and plugin execution. **Technology:** Kong 3.4 on Kubernetes **Scaling:** Horizontal with HPA ### Auth Plugin Validates JWT tokens, API keys, and OAuth2 flows. Token validation latency under 5ms. **Technology:** Custom Lua plugin **Scaling:** Stateless ### Rate Limiter Enforces per-user and per-IP rate limits. Default limit is 100 requests per minute. **Technology:** Redis-backed sliding window **Scaling:** Shared Redis cluster ## Data Flow Client \u2192 TLS \u2192 Kong \u2192 Auth \u2192 Rate Limit \u2192 Backend Services ## Performance Characteristics - **Latency P50:** 12ms - **Latency P99:** 45ms - **Throughput:** 50000 requests/second",
              "score": null
            },
            {
              "doc_id": "howto_set_up_monitoring_alerts",
              "chunk_id": "howto_set_up_monitoring_alerts_fix_0",
              "content": "# How to Set Up Monitoring Alerts ## Prerequisites - CloudFlow Pro plan or higher ## Steps ### Step 1: Define alert rules Create alert conditions based on metrics thresholds. ```bash cloudflow alerts create --name 'High Error Rate' --condition 'error_rate > 0.05' ``` ### Step 2: Configure channels Set up notification channels: email, Slack, or PagerDuty. ```bash cloudflow alerts channel add --type slack --webhook https://hooks.slack.com/... ``` ### Step 3: Set severity levels Assign severity to each alert: critical, warning, or info. > **Note:** Critical alerts bypass quiet hours ### Step 4: Test alerts Trigger a test alert to verify routing. ```bash cloudflow alerts test --name 'High Error Rate' ``` ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            },
            {
              "doc_id": "arch_notification_service",
              "chunk_id": "arch_notification_service_fix_0",
              "content": "# Notification Service Architecture ## Overview The Notification Service delivers alerts and updates through multiple channels including email, Slack, webhooks, and mobile push notifications. ## Design Principles - **Delivery Guarantee**: At-least-once delivery with deduplication - **Channel Abstraction**: Unified API regardless of delivery channel ## Components ### Dispatcher Routes notifications to appropriate channel handlers. Supports priority queues for urgent notifications. **Technology:** Go microservice **Scaling:** Horizontal with queue partitioning ### Email Provider Sends transactional emails. Maximum 10000 emails per hour per account. **Technology:** SendGrid integration **Scaling:** API rate limited ### Slack Integration Posts messages to Slack channels. Supports message formatting and interactive components. **Technology:** Slack Bolt SDK **Scaling:** Per-workspace rate limits ## Data Flow Event \u2192 Template \u2192 Dispatcher \u2192 Channel \u2192 Delivery Confirmation ## Performance Characteristics - **Latency P50:** 200ms - **Latency P99:** 2000ms - **Throughput:** 10000 notifications/minute",
              "score": null
            }
          ],
          "latency_ms": 3.893660999892745,
          "query_id": "simple_010",
          "dimension": "problem",
          "query": "seeing consumer lag alerts, what message queue does CloudFlow use"
        },
        {
          "key_facts": [
            "Kafka",
            "event-driven"
          ],
          "found_facts": [
            "Kafka",
            "event-driven"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "adr_005"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "adr_005",
              "chunk_id": "adr_005_fix_0",
              "content": "# ADR-005: Adopt Event-Driven Architecture for Workflow Execution **Status:** Accepted **Date:** 2024-12-04 ## Context We need an architecture that supports asynchronous workflow execution, step dependencies, and failure recovery. ## Options Considered ### Option 1: Event-driven with Kafka Publish-subscribe messaging. **Pros:** - Loose coupling - Scalability - Event replay - Audit trail **Cons:** - Eventual consistency - Operational complexity - Message ordering challenges ### Option 2: Request-response Synchronous API calls. **Pros:** - Simple mental model - Immediate feedback - Easy debugging **Cons:** - Tight coupling - Cascading failures - Timeout issues ### Option 3: Polling-based Workers poll for tasks. **Pros:** - Simple implementation - Easy rate limiting - No message broker **Cons:** - Polling overhead - Higher latency - Inefficient at scale ## Decision We will use an event-driven architecture with Kafka for workflow step execution. ## Consequences - Implement idempotent consumers - Use dead letter queues for failed messages - Monitor consumer lag",
              "score": null
            },
            {
              "doc_id": "arch_notification_service",
              "chunk_id": "arch_notification_service_fix_0",
              "content": "# Notification Service Architecture ## Overview The Notification Service delivers alerts and updates through multiple channels including email, Slack, webhooks, and mobile push notifications. ## Design Principles - **Delivery Guarantee**: At-least-once delivery with deduplication - **Channel Abstraction**: Unified API regardless of delivery channel ## Components ### Dispatcher Routes notifications to appropriate channel handlers. Supports priority queues for urgent notifications. **Technology:** Go microservice **Scaling:** Horizontal with queue partitioning ### Email Provider Sends transactional emails. Maximum 10000 emails per hour per account. **Technology:** SendGrid integration **Scaling:** API rate limited ### Slack Integration Posts messages to Slack channels. Supports message formatting and interactive components. **Technology:** Slack Bolt SDK **Scaling:** Per-workspace rate limits ## Data Flow Event \u2192 Template \u2192 Dispatcher \u2192 Channel \u2192 Delivery Confirmation ## Performance Characteristics - **Latency P50:** 200ms - **Latency P99:** 2000ms - **Throughput:** 10000 notifications/minute",
              "score": null
            },
            {
              "doc_id": "arch_workflow_engine",
              "chunk_id": "arch_workflow_engine_fix_0",
              "content": "# Workflow Engine Architecture ## Overview The Workflow Engine is the core component responsible for executing user-defined workflows. It uses an event-driven architecture with support for parallel execution, conditional branching, and automatic retries. ## Design Principles - **Idempotency**: All operations can be safely retried without side effects - **Eventual Consistency**: State converges to consistent view within 5 seconds - **Fault Tolerance**: System continues operating despite component failures ## Components ### Scheduler Manages workflow scheduling and triggers. Supports cron expressions, webhooks, and event-based triggers. **Technology:** Custom Go service **Scaling:** Horizontal with leader election ### Executor Runs individual workflow steps in isolated containers. Maximum execution time is 30 minutes per step. **Technology:** Kubernetes Jobs **Scaling:** Auto-scaling based on queue depth ### State Store Persists workflow state, execution history, and step results. Uses optimistic locking for concurrent updates. **Technology:** PostgreSQL with JSONB **Scaling:** Primary-replica with read replicas ## Data Flow Triggers \u2192 Scheduler \u2192 Job Queue \u2192 Executor \u2192 State Store \u2192 Webhooks/Notifications ## Performance Characteristics - **Latency P50:** 45ms - **Latency P99:** 250ms - **Throughput:** 5000 executions/minute",
              "score": null
            },
            {
              "doc_id": "arch_monitoring_stack",
              "chunk_id": "arch_monitoring_stack_fix_0",
              "content": "# Monitoring Stack Architecture ## Overview The Monitoring Stack provides observability into system health, performance, and errors. Includes metrics, logs, traces, and alerting. ## Design Principles - **Observability**: Every component emits metrics and traces - **Actionable Alerts**: Alerts include runbook links ## Components ### Metrics Collects time-series metrics. Retention is 15 days local, 1 year in object storage. **Technology:** Prometheus with Thanos **Scaling:** Federated with long-term storage ### Logging Centralized log aggregation. Supports structured JSON logs with label indexing. **Technology:** Loki with Grafana **Scaling:** Distributed with sharding ### Tracing Distributed request tracing. Trace retention is 7 days. **Technology:** Jaeger with OpenTelemetry **Scaling:** Sampling at 10% ### Alerting Routes alerts to on-call engineers. Supports escalation policies and silencing. **Technology:** Alertmanager with PagerDuty **Scaling:** HA pair ## Data Flow Application \u2192 Collector \u2192 Storage \u2192 Query \u2192 Dashboard/Alert ## Performance Characteristics - **Latency P50:** N/A - **Latency P99:** N/A - **Throughput:** 1M metrics/second, 100GB logs/day",
              "score": null
            },
            {
              "doc_id": "release_3_1_0",
              "chunk_id": "release_3_1_0_fix_0",
              "content": "# Release Notes - v3.1.0 **Release Date:** 2023-12-01 ## New Features - GraphQL API beta release - Workflow templates marketplace - Enhanced audit logging - SSO support for Okta ## Bug Fixes - Fixed webhook delivery retry logic - Resolved issue with large payload handling - Fixed dashboard loading performance ## Upgrade Instructions ```bash cloudflow upgrade --version 3.1.0 ``` ## Known Issues None at this time.",
              "score": null
            }
          ],
          "latency_ms": 3.6723789999086875,
          "query_id": "simple_010",
          "dimension": "casual",
          "query": "workflow message broker"
        },
        {
          "key_facts": [
            "Kafka",
            "event-driven"
          ],
          "found_facts": [
            "Kafka",
            "event-driven"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "adr_005"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "arch_workflow_engine",
              "chunk_id": "arch_workflow_engine_fix_0",
              "content": "# Workflow Engine Architecture ## Overview The Workflow Engine is the core component responsible for executing user-defined workflows. It uses an event-driven architecture with support for parallel execution, conditional branching, and automatic retries. ## Design Principles - **Idempotency**: All operations can be safely retried without side effects - **Eventual Consistency**: State converges to consistent view within 5 seconds - **Fault Tolerance**: System continues operating despite component failures ## Components ### Scheduler Manages workflow scheduling and triggers. Supports cron expressions, webhooks, and event-based triggers. **Technology:** Custom Go service **Scaling:** Horizontal with leader election ### Executor Runs individual workflow steps in isolated containers. Maximum execution time is 30 minutes per step. **Technology:** Kubernetes Jobs **Scaling:** Auto-scaling based on queue depth ### State Store Persists workflow state, execution history, and step results. Uses optimistic locking for concurrent updates. **Technology:** PostgreSQL with JSONB **Scaling:** Primary-replica with read replicas ## Data Flow Triggers \u2192 Scheduler \u2192 Job Queue \u2192 Executor \u2192 State Store \u2192 Webhooks/Notifications ## Performance Characteristics - **Latency P50:** 45ms - **Latency P99:** 250ms - **Throughput:** 5000 executions/minute",
              "score": null
            },
            {
              "doc_id": "adr_005",
              "chunk_id": "adr_005_fix_0",
              "content": "# ADR-005: Adopt Event-Driven Architecture for Workflow Execution **Status:** Accepted **Date:** 2024-12-04 ## Context We need an architecture that supports asynchronous workflow execution, step dependencies, and failure recovery. ## Options Considered ### Option 1: Event-driven with Kafka Publish-subscribe messaging. **Pros:** - Loose coupling - Scalability - Event replay - Audit trail **Cons:** - Eventual consistency - Operational complexity - Message ordering challenges ### Option 2: Request-response Synchronous API calls. **Pros:** - Simple mental model - Immediate feedback - Easy debugging **Cons:** - Tight coupling - Cascading failures - Timeout issues ### Option 3: Polling-based Workers poll for tasks. **Pros:** - Simple implementation - Easy rate limiting - No message broker **Cons:** - Polling overhead - Higher latency - Inefficient at scale ## Decision We will use an event-driven architecture with Kafka for workflow step execution. ## Consequences - Implement idempotent consumers - Use dead letter queues for failed messages - Monitor consumer lag",
              "score": null
            },
            {
              "doc_id": "arch_data_pipeline",
              "chunk_id": "arch_data_pipeline_fix_0",
              "content": "# Data Pipeline Architecture ## Overview The Data Pipeline processes and transforms data between workflow steps. It supports batch processing up to 10GB and streaming for real-time use cases. ## Design Principles - **Exactly-Once Processing**: Each record is processed exactly once - **Backpressure Handling**: System gracefully handles varying load ## Components ### Stream Processor Handles real-time data streams. Maximum throughput of 100000 messages per second. **Technology:** Apache Kafka with Flink **Scaling:** Partition-based scaling ### Batch Processor Processes large batch jobs. Supports data formats: JSON, CSV, Parquet, Avro. **Technology:** Apache Spark on Kubernetes **Scaling:** Dynamic resource allocation ### Data Lake Long-term data storage with time-travel queries. Retention configurable up to 7 years. **Technology:** S3 with Iceberg tables **Scaling:** Unlimited with tiered storage ## Data Flow Input \u2192 Validation \u2192 Transform \u2192 Enrich \u2192 Output ## Performance Characteristics - **Latency P50:** 100ms (streaming), 5min (batch) - **Latency P99:** 500ms (streaming), 30min (batch) - **Throughput:** 100000 messages/second (streaming)",
              "score": null
            },
            {
              "doc_id": "api_metrics",
              "chunk_id": "api_metrics_fix_0",
              "content": "# Metrics API Reference ## Overview The Metrics API provides programmatic access to metrics functionality. Base URL: `https://api.cloudflow.io/v1/metrics` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /metrics/workflows Get workflow performance metrics. **Parameters:** - `period` (string): Time period: hour, day, week, month - `workflow_id` (string): Filter by workflow ID **Response:** - Success: `200` - Returns metrics data - Error: `400` - Bad Request **Example:** ```json {\"success_rate\": 0.985, \"avg_duration_ms\": 1250, \"p99_duration_ms\": 5000, \"total_executions\": 10000} ```",
              "score": null
            },
            {
              "doc_id": "runbook_kafka_consumer_lag",
              "chunk_id": "runbook_kafka_consumer_lag_fix_0",
              "content": "# Runbook: Kafka Consumer Lag ## Severity **Level:** P1 - Critical **Response Time:** 1 hour ## Symptoms - Consumer lag > 10000 messages - Processing delays reported - Alerts from lag monitor ## Diagnosis ### Step 1 Check consumer group lag: `kafka-consumer-groups.sh --bootstrap-server kafka:9092 --describe --group workflow-executor` ### Step 2 Verify consumer health: `kubectl get pods -l app=workflow-executor -n production` ### Step 3 Check for processing errors: `kubectl logs -l app=workflow-executor --tail=100 -n production | grep ERROR` ### Step 4 Monitor throughput: `cloudflow metrics get kafka.consumer.records_per_second` ## Resolution ### Step 1 Scale up consumers: `kubectl scale deployment/workflow-executor --replicas=10 -n production` ### Step 2 Reset offset if messages are stale: `kafka-consumer-groups.sh --bootstrap-server kafka:9092 --group workflow-executor --reset-offsets --to-latest --execute --topic workflows` ### Step 3 Pause producer if backpressure needed: `cloudflow workflows pause-triggers` ### Step 4 Increase partition count for parallelism (requires coordination) ## Escalation If lag persists > 30 minutes, page platform team. May need Kafka cluster scaling. ## Post-Incident - Document timeline in incident log - Schedule post-mortem within 48 hours - Update runbook if new patterns discovered",
              "score": null
            }
          ],
          "latency_ms": 4.196975998638663,
          "query_id": "simple_010",
          "dimension": "contextual",
          "query": "need to monitor workflow throughput, what queue system should I look at"
        },
        {
          "key_facts": [
            "Kafka",
            "event-driven"
          ],
          "found_facts": [
            "Kafka",
            "event-driven"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "adr_005"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "release_3_2_0",
              "chunk_id": "release_3_2_0_fix_0",
              "content": "# Release Notes - v3.2.0 **Release Date:** 2024-01-15 ## New Features - Workflow versioning with rollback support - New Python SDK with async support - Bulk operations API for workflows - Custom retry policies per step ## Bug Fixes - Fixed race condition in concurrent execution handler - Resolved memory leak in long-running workflows - Fixed timezone handling in scheduled triggers ## Breaking Changes - **BREAKING:** API v1 endpoints deprecated, will be removed in v4.0 - **BREAKING:** Minimum Python version now 3.9 ## Upgrade Instructions ```bash cloudflow upgrade --version 3.2.0 ``` ## Known Issues None at this time.",
              "score": null
            },
            {
              "doc_id": "api_executions",
              "chunk_id": "api_executions_fix_0",
              "content": "# Executions API Reference ## Overview The Executions API provides programmatic access to executions functionality. Base URL: `https://api.cloudflow.io/v1/executions` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /executions List workflow executions. **Parameters:** - `workflow_id` (string): Filter by workflow ID - `since` (datetime): Filter executions after this timestamp **Response:** - Success: `200` - Returns array of execution objects - Error: `400` - Bad Request **Example:** ```json {\"executions\": [{\"id\": \"ex_789\", \"status\": \"completed\", \"duration_ms\": 1523}]} ``` ### POST /executions/{id}/cancel Cancel a running execution. **Parameters:** - `id` (string): Execution ID - `reason` (string): Cancellation reason (optional) **Response:** - Success: `200` - Execution cancelled - Error: `400` - Bad Request **Example:** ```json {\"id\": \"ex_789\", \"status\": \"cancelled\", \"cancelled_at\": \"2024-01-15T10:05:00Z\"} ```",
              "score": null
            },
            {
              "doc_id": "api_workflows",
              "chunk_id": "api_workflows_fix_0",
              "content": "# Workflows API Reference ## Overview The Workflows API provides programmatic access to workflows functionality. Base URL: `https://api.cloudflow.io/v1/workflows` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /workflows List all workflows for the authenticated user. **Parameters:** - `limit` (integer): Maximum number of results (default: 50, max: 200) - `status` (string): Filter by status: active, paused, archived **Response:** - Success: `200` - Returns array of workflow objects - Error: `400` - Bad Request **Example:** ```json {\"workflows\": [{\"id\": \"wf_123\", \"name\": \"Daily Report\", \"status\": \"active\"}]} ``` ### POST /workflows Create a new workflow. **Parameters:** - `name` (string): Workflow name (required) - `definition` (object): Workflow definition in JSON format **Response:** - Success: `201` - Workflow created successfully - Error: `400` - Bad Request **Example:** ```json {\"id\": \"wf_456\", \"name\": \"New Workflow\", \"created_at\": \"2024-01-15T10:00:00Z\"} ``` ### DELETE /workflows/{id} Delete a workflow permanently. **Parameters:** - `id` (string): Workflow ID (required) **Response:** - Success: `204` - Workflow deleted - Error: `404` - Workflow not found **Example:** ```json {} ```",
              "score": null
            },
            {
              "doc_id": "howto_create_scheduled_workflow",
              "chunk_id": "howto_create_scheduled_workflow_fix_0",
              "content": "# How to Create Scheduled Workflow ## Prerequisites - Workflow definition ## Steps ### Step 1: Define schedule Use cron expression to set the schedule. Example: '0 9 * * MON-FRI' runs at 9 AM on weekdays. ### Step 2: Set timezone Specify timezone for the schedule. Default is UTC. > **Note:** Use IANA timezone names like America/New_York ### Step 3: Configure overlap handling Choose behavior when previous execution is still running: skip, queue, or cancel. ### Step 4: Enable notifications Get notified on schedule failures. ```bash cloudflow workflows notify --on-failure ``` ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            },
            {
              "doc_id": "adr_005",
              "chunk_id": "adr_005_fix_0",
              "content": "# ADR-005: Adopt Event-Driven Architecture for Workflow Execution **Status:** Accepted **Date:** 2024-12-04 ## Context We need an architecture that supports asynchronous workflow execution, step dependencies, and failure recovery. ## Options Considered ### Option 1: Event-driven with Kafka Publish-subscribe messaging. **Pros:** - Loose coupling - Scalability - Event replay - Audit trail **Cons:** - Eventual consistency - Operational complexity - Message ordering challenges ### Option 2: Request-response Synchronous API calls. **Pros:** - Simple mental model - Immediate feedback - Easy debugging **Cons:** - Tight coupling - Cascading failures - Timeout issues ### Option 3: Polling-based Workers poll for tasks. **Pros:** - Simple implementation - Easy rate limiting - No message broker **Cons:** - Polling overhead - Higher latency - Inefficient at scale ## Decision We will use an event-driven architecture with Kafka for workflow step execution. ## Consequences - Implement idempotent consumers - Use dead letter queues for failed messages - Monitor consumer lag",
              "score": null
            }
          ],
          "latency_ms": 3.829281000435003,
          "query_id": "simple_010",
          "dimension": "negation",
          "query": "workflows aren't using synchronous calls right"
        },
        {
          "key_facts": [
            "OAuth 2.0",
            "JWT",
            "bcrypt",
            "RS256",
            "cost factor 12"
          ],
          "found_facts": [
            "OAuth 2.0",
            "JWT",
            "bcrypt",
            "RS256",
            "cost factor 12"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "adr_003",
            "arch_authentication_service",
            "arch_api_gateway"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "arch_authentication_service",
              "chunk_id": "arch_authentication_service_fix_0",
              "content": "# Authentication Service Architecture ## Overview The Authentication Service handles user identity, session management, and token issuance. Supports OAuth 2.0, SAML, and API key authentication. ## Design Principles - **Zero Knowledge**: Passwords never stored in plaintext - **Token Rotation**: Refresh tokens rotate on each use ## Components ### Identity Provider Manages user accounts and credentials. Password hashing uses bcrypt with cost factor 12. **Technology:** Custom Go service **Scaling:** Horizontal stateless ### Token Service Issues and validates access tokens. Token lifetime is 1 hour with 7-day refresh tokens. **Technology:** JWT with RS256 **Scaling:** Horizontal stateless ### SSO Gateway Integrates with external identity providers. Supports Google, Microsoft, Okta, and custom SAML. **Technology:** Dex OIDC proxy **Scaling:** Active-passive ## Data Flow Credentials \u2192 Validation \u2192 Token Issue \u2192 Token Refresh \u2192 Logout ## Performance Characteristics - **Latency P50:** 50ms - **Latency P99:** 200ms - **Throughput:** 5000 auth/second",
              "score": null
            },
            {
              "doc_id": "adr_003",
              "chunk_id": "adr_003_fix_0",
              "content": "# ADR-003: Implement OAuth 2.0 with JWT for Authentication **Status:** Accepted **Date:** 2024-05-08 ## Context We need a secure authentication system that supports API keys, user sessions, and third-party integrations. ## Options Considered ### Option 1: OAuth 2.0 with JWT Industry standard token-based auth. **Pros:** - Industry standard - Stateless validation - Supports refresh tokens - SSO compatible **Cons:** - Token size overhead - Cannot revoke individual tokens - Complexity ### Option 2: Session-based auth Traditional server-side sessions. **Pros:** - Simple implementation - Easy revocation - Smaller payloads **Cons:** - Requires session store - Not suitable for APIs - Scaling challenges ### Option 3: API keys only Simple key-based authentication. **Pros:** - Very simple - Low overhead - Easy to implement **Cons:** - No user context - Hard to rotate - No SSO support ## Decision We will implement OAuth 2.0 with JWT tokens. Access tokens expire after 1 hour, refresh tokens after 7 days. ## Consequences - Implement token rotation on refresh - Support API keys for machine-to-machine auth - Add MFA for admin accounts",
              "score": null
            },
            {
              "doc_id": "howto_configure_single_sign_on",
              "chunk_id": "howto_configure_single_sign_on_fix_0",
              "content": "# How to Configure Single Sign-On ## Prerequisites - Admin access - Enterprise plan - Identity provider access ## Steps ### Step 1: Choose provider CloudFlow supports SAML 2.0, OpenID Connect, and OAuth 2.0 providers. ### Step 2: Get metadata Download CloudFlow's SAML metadata for your IdP configuration. ```bash cloudflow sso metadata --format xml > cloudflow-metadata.xml ``` ### Step 3: Configure IdP Add CloudFlow as a service provider in your identity provider. > **Note:** ACS URL: https://auth.cloudflow.io/saml/callback ### Step 4: Upload IdP metadata Provide your IdP's metadata to CloudFlow. ```bash cloudflow sso configure --idp-metadata ./idp-metadata.xml ``` ### Step 5: Test login Verify SSO login works correctly. ```bash cloudflow sso test ``` ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            },
            {
              "doc_id": "arch_api_gateway",
              "chunk_id": "arch_api_gateway_fix_0",
              "content": "# API Gateway Architecture ## Overview The API Gateway handles all incoming HTTP requests, providing authentication, rate limiting, and request routing. Built on Kong with custom plugins for CloudFlow-specific functionality. ## Design Principles - **Zero Trust**: Every request is authenticated and authorized - **Defense in Depth**: Multiple layers of security validation ## Components ### Kong Gateway Primary ingress controller. Handles TLS termination, request routing, and plugin execution. **Technology:** Kong 3.4 on Kubernetes **Scaling:** Horizontal with HPA ### Auth Plugin Validates JWT tokens, API keys, and OAuth2 flows. Token validation latency under 5ms. **Technology:** Custom Lua plugin **Scaling:** Stateless ### Rate Limiter Enforces per-user and per-IP rate limits. Default limit is 100 requests per minute. **Technology:** Redis-backed sliding window **Scaling:** Shared Redis cluster ## Data Flow Client \u2192 TLS \u2192 Kong \u2192 Auth \u2192 Rate Limit \u2192 Backend Services ## Performance Characteristics - **Latency P50:** 12ms - **Latency P99:** 45ms - **Throughput:** 50000 requests/second",
              "score": null
            },
            {
              "doc_id": "arch_workflow_engine",
              "chunk_id": "arch_workflow_engine_fix_0",
              "content": "# Workflow Engine Architecture ## Overview The Workflow Engine is the core component responsible for executing user-defined workflows. It uses an event-driven architecture with support for parallel execution, conditional branching, and automatic retries. ## Design Principles - **Idempotency**: All operations can be safely retried without side effects - **Eventual Consistency**: State converges to consistent view within 5 seconds - **Fault Tolerance**: System continues operating despite component failures ## Components ### Scheduler Manages workflow scheduling and triggers. Supports cron expressions, webhooks, and event-based triggers. **Technology:** Custom Go service **Scaling:** Horizontal with leader election ### Executor Runs individual workflow steps in isolated containers. Maximum execution time is 30 minutes per step. **Technology:** Kubernetes Jobs **Scaling:** Auto-scaling based on queue depth ### State Store Persists workflow state, execution history, and step results. Uses optimistic locking for concurrent updates. **Technology:** PostgreSQL with JSONB **Scaling:** Primary-replica with read replicas ## Data Flow Triggers \u2192 Scheduler \u2192 Job Queue \u2192 Executor \u2192 State Store \u2192 Webhooks/Notifications ## Performance Characteristics - **Latency P50:** 45ms - **Latency P99:** 250ms - **Throughput:** 5000 executions/minute",
              "score": null
            }
          ],
          "latency_ms": 3.726220000316971,
          "query_id": "cross_011",
          "dimension": "original",
          "query": "How is authentication implemented across the system?"
        },
        {
          "key_facts": [
            "OAuth 2.0",
            "JWT",
            "bcrypt",
            "RS256",
            "cost factor 12"
          ],
          "found_facts": [
            "OAuth 2.0",
            "JWT"
          ],
          "missed_facts": [
            "bcrypt",
            "RS256",
            "cost factor 12"
          ],
          "coverage": 0.4,
          "expected_docs": [
            "adr_003",
            "arch_authentication_service",
            "arch_api_gateway"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "howto_configure_single_sign_on",
              "chunk_id": "howto_configure_single_sign_on_fix_0",
              "content": "# How to Configure Single Sign-On ## Prerequisites - Admin access - Enterprise plan - Identity provider access ## Steps ### Step 1: Choose provider CloudFlow supports SAML 2.0, OpenID Connect, and OAuth 2.0 providers. ### Step 2: Get metadata Download CloudFlow's SAML metadata for your IdP configuration. ```bash cloudflow sso metadata --format xml > cloudflow-metadata.xml ``` ### Step 3: Configure IdP Add CloudFlow as a service provider in your identity provider. > **Note:** ACS URL: https://auth.cloudflow.io/saml/callback ### Step 4: Upload IdP metadata Provide your IdP's metadata to CloudFlow. ```bash cloudflow sso configure --idp-metadata ./idp-metadata.xml ``` ### Step 5: Test login Verify SSO login works correctly. ```bash cloudflow sso test ``` ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            },
            {
              "doc_id": "howto_create_api_integration",
              "chunk_id": "howto_create_api_integration_fix_0",
              "content": "# How to Create API Integration ## Prerequisites - CloudFlow account ## Steps ### Step 1: Generate API key Create an API key for programmatic access. ```bash cloudflow apikeys create --name 'CI Integration' --scopes workflows:read,workflows:execute ``` ### Step 2: Store key securely Never commit API keys to source control. Use environment variables or secret management. > **Note:** Rotate keys every 90 days ### Step 3: Test authentication Verify the API key works correctly. ```bash curl -H 'Authorization: Bearer cf_...' https://api.cloudflow.io/v1/me ``` ### Step 4: Implement rate limiting Handle rate limit responses (HTTP 429) with exponential backoff. > **Note:** Default limit is 100 requests per minute ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            },
            {
              "doc_id": "howto_set_up_data_encryption",
              "chunk_id": "howto_set_up_data_encryption_fix_0",
              "content": "# How to Set Up Data Encryption ## Prerequisites - CloudFlow Enterprise plan ## Steps ### Step 1: Enable encryption at rest All data is encrypted at rest using AES-256 by default. > **Note:** Encryption is automatic for all plans ### Step 2: Configure KMS Bring your own encryption keys using AWS KMS or GCP KMS. ```bash cloudflow encryption configure --kms-key arn:aws:kms:... ``` ### Step 3: Enable field-level encryption Encrypt sensitive fields within your workflow data. ```bash cloudflow encryption add-field --path $.user.ssn --key-alias sensitive-data ``` ### Step 4: Verify encryption Confirm encryption is active for your workspace. ```bash cloudflow encryption status ``` ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            },
            {
              "doc_id": "arch_api_gateway",
              "chunk_id": "arch_api_gateway_fix_0",
              "content": "# API Gateway Architecture ## Overview The API Gateway handles all incoming HTTP requests, providing authentication, rate limiting, and request routing. Built on Kong with custom plugins for CloudFlow-specific functionality. ## Design Principles - **Zero Trust**: Every request is authenticated and authorized - **Defense in Depth**: Multiple layers of security validation ## Components ### Kong Gateway Primary ingress controller. Handles TLS termination, request routing, and plugin execution. **Technology:** Kong 3.4 on Kubernetes **Scaling:** Horizontal with HPA ### Auth Plugin Validates JWT tokens, API keys, and OAuth2 flows. Token validation latency under 5ms. **Technology:** Custom Lua plugin **Scaling:** Stateless ### Rate Limiter Enforces per-user and per-IP rate limits. Default limit is 100 requests per minute. **Technology:** Redis-backed sliding window **Scaling:** Shared Redis cluster ## Data Flow Client \u2192 TLS \u2192 Kong \u2192 Auth \u2192 Rate Limit \u2192 Backend Services ## Performance Characteristics - **Latency P50:** 12ms - **Latency P99:** 45ms - **Throughput:** 50000 requests/second",
              "score": null
            },
            {
              "doc_id": "howto_configure_custom_domain",
              "chunk_id": "howto_configure_custom_domain_fix_0",
              "content": "# How to Configure Custom Domain ## Prerequisites - Domain with DNS access - CloudFlow Pro plan or higher ## Steps ### Step 1: Verify domain ownership Add a TXT record to verify domain ownership. ```bash cloudflow domains verify --domain app.example.com ``` ### Step 2: Configure DNS Add a CNAME record pointing to your CloudFlow endpoint. > **Note:** CNAME should point to ingress.cloudflow.io ### Step 3: Enable SSL CloudFlow automatically provisions SSL certificates via Let's Encrypt. ```bash cloudflow domains ssl --domain app.example.com ``` ### Step 4: Set as primary Make this the primary domain for your application. ```bash cloudflow domains set-primary --domain app.example.com ``` ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            }
          ],
          "latency_ms": 4.242780998538365,
          "query_id": "cross_011",
          "dimension": "synonym",
          "query": "what's the login and authorization setup in cloudflow"
        },
        {
          "key_facts": [
            "OAuth 2.0",
            "JWT",
            "bcrypt",
            "RS256",
            "cost factor 12"
          ],
          "found_facts": [
            "OAuth 2.0",
            "JWT",
            "bcrypt",
            "RS256",
            "cost factor 12"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "adr_003",
            "arch_authentication_service",
            "arch_api_gateway"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "api_workflows",
              "chunk_id": "api_workflows_fix_0",
              "content": "# Workflows API Reference ## Overview The Workflows API provides programmatic access to workflows functionality. Base URL: `https://api.cloudflow.io/v1/workflows` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /workflows List all workflows for the authenticated user. **Parameters:** - `limit` (integer): Maximum number of results (default: 50, max: 200) - `status` (string): Filter by status: active, paused, archived **Response:** - Success: `200` - Returns array of workflow objects - Error: `400` - Bad Request **Example:** ```json {\"workflows\": [{\"id\": \"wf_123\", \"name\": \"Daily Report\", \"status\": \"active\"}]} ``` ### POST /workflows Create a new workflow. **Parameters:** - `name` (string): Workflow name (required) - `definition` (object): Workflow definition in JSON format **Response:** - Success: `201` - Workflow created successfully - Error: `400` - Bad Request **Example:** ```json {\"id\": \"wf_456\", \"name\": \"New Workflow\", \"created_at\": \"2024-01-15T10:00:00Z\"} ``` ### DELETE /workflows/{id} Delete a workflow permanently. **Parameters:** - `id` (string): Workflow ID (required) **Response:** - Success: `204` - Workflow deleted - Error: `404` - Workflow not found **Example:** ```json {} ```",
              "score": null
            },
            {
              "doc_id": "api_webhooks",
              "chunk_id": "api_webhooks_fix_0",
              "content": "# Webhooks API Reference ## Overview The Webhooks API provides programmatic access to webhooks functionality. Base URL: `https://api.cloudflow.io/v1/webhooks` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /webhooks List configured webhooks. **Parameters:** - `active` (boolean): Filter by active status **Response:** - Success: `200` - Returns array of webhook objects - Error: `400` - Bad Request **Example:** ```json {\"webhooks\": [{\"id\": \"wh_01\", \"url\": \"https://example.com/hook\", \"events\": [\"workflow.completed\"]}]} ``` ### POST /webhooks Create a new webhook endpoint. **Parameters:** - `url` (string): Webhook URL (must be HTTPS) - `events` (array): Events to subscribe to - `secret` (string): Signing secret for verification **Response:** - Success: `201` - Webhook created - Error: `400` - Bad Request **Example:** ```json {\"id\": \"wh_02\", \"url\": \"https://example.com/new-hook\", \"secret\": \"whsec_...\"} ```",
              "score": null
            },
            {
              "doc_id": "api_users",
              "chunk_id": "api_users_fix_0",
              "content": "# Users API Reference ## Overview The Users API provides programmatic access to users functionality. Base URL: `https://api.cloudflow.io/v1/users` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /users/me Get current user profile. **Parameters:** **Response:** - Success: `200` - Returns user object - Error: `400` - Bad Request **Example:** ```json {\"id\": \"usr_001\", \"email\": \"user@example.com\", \"role\": \"admin\", \"created_at\": \"2023-06-01\"} ``` ### PATCH /users/me Update current user profile. **Parameters:** - `name` (string): Display name - `timezone` (string): Timezone (e.g., America/New_York) **Response:** - Success: `200` - User updated - Error: `400` - Bad Request **Example:** ```json {\"id\": \"usr_001\", \"name\": \"John Doe\", \"timezone\": \"America/New_York\"} ```",
              "score": null
            },
            {
              "doc_id": "arch_authentication_service",
              "chunk_id": "arch_authentication_service_fix_0",
              "content": "# Authentication Service Architecture ## Overview The Authentication Service handles user identity, session management, and token issuance. Supports OAuth 2.0, SAML, and API key authentication. ## Design Principles - **Zero Knowledge**: Passwords never stored in plaintext - **Token Rotation**: Refresh tokens rotate on each use ## Components ### Identity Provider Manages user accounts and credentials. Password hashing uses bcrypt with cost factor 12. **Technology:** Custom Go service **Scaling:** Horizontal stateless ### Token Service Issues and validates access tokens. Token lifetime is 1 hour with 7-day refresh tokens. **Technology:** JWT with RS256 **Scaling:** Horizontal stateless ### SSO Gateway Integrates with external identity providers. Supports Google, Microsoft, Okta, and custom SAML. **Technology:** Dex OIDC proxy **Scaling:** Active-passive ## Data Flow Credentials \u2192 Validation \u2192 Token Issue \u2192 Token Refresh \u2192 Logout ## Performance Characteristics - **Latency P50:** 50ms - **Latency P99:** 200ms - **Throughput:** 5000 auth/second",
              "score": null
            },
            {
              "doc_id": "api_executions",
              "chunk_id": "api_executions_fix_0",
              "content": "# Executions API Reference ## Overview The Executions API provides programmatic access to executions functionality. Base URL: `https://api.cloudflow.io/v1/executions` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /executions List workflow executions. **Parameters:** - `workflow_id` (string): Filter by workflow ID - `since` (datetime): Filter executions after this timestamp **Response:** - Success: `200` - Returns array of execution objects - Error: `400` - Bad Request **Example:** ```json {\"executions\": [{\"id\": \"ex_789\", \"status\": \"completed\", \"duration_ms\": 1523}]} ``` ### POST /executions/{id}/cancel Cancel a running execution. **Parameters:** - `id` (string): Execution ID - `reason` (string): Cancellation reason (optional) **Response:** - Success: `200` - Execution cancelled - Error: `400` - Bad Request **Example:** ```json {\"id\": \"ex_789\", \"status\": \"cancelled\", \"cancelled_at\": \"2024-01-15T10:05:00Z\"} ```",
              "score": null
            }
          ],
          "latency_ms": 4.265974001100403,
          "query_id": "cross_011",
          "dimension": "problem",
          "query": "users are getting 401 unauthorized on API calls even with tokens that were working yesterday"
        },
        {
          "key_facts": [
            "OAuth 2.0",
            "JWT",
            "bcrypt",
            "RS256",
            "cost factor 12"
          ],
          "found_facts": [
            "OAuth 2.0",
            "JWT",
            "bcrypt",
            "RS256",
            "cost factor 12"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "adr_003",
            "arch_authentication_service",
            "arch_api_gateway"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "arch_authentication_service",
              "chunk_id": "arch_authentication_service_fix_0",
              "content": "# Authentication Service Architecture ## Overview The Authentication Service handles user identity, session management, and token issuance. Supports OAuth 2.0, SAML, and API key authentication. ## Design Principles - **Zero Knowledge**: Passwords never stored in plaintext - **Token Rotation**: Refresh tokens rotate on each use ## Components ### Identity Provider Manages user accounts and credentials. Password hashing uses bcrypt with cost factor 12. **Technology:** Custom Go service **Scaling:** Horizontal stateless ### Token Service Issues and validates access tokens. Token lifetime is 1 hour with 7-day refresh tokens. **Technology:** JWT with RS256 **Scaling:** Horizontal stateless ### SSO Gateway Integrates with external identity providers. Supports Google, Microsoft, Okta, and custom SAML. **Technology:** Dex OIDC proxy **Scaling:** Active-passive ## Data Flow Credentials \u2192 Validation \u2192 Token Issue \u2192 Token Refresh \u2192 Logout ## Performance Characteristics - **Latency P50:** 50ms - **Latency P99:** 200ms - **Throughput:** 5000 auth/second",
              "score": null
            },
            {
              "doc_id": "howto_configure_single_sign_on",
              "chunk_id": "howto_configure_single_sign_on_fix_0",
              "content": "# How to Configure Single Sign-On ## Prerequisites - Admin access - Enterprise plan - Identity provider access ## Steps ### Step 1: Choose provider CloudFlow supports SAML 2.0, OpenID Connect, and OAuth 2.0 providers. ### Step 2: Get metadata Download CloudFlow's SAML metadata for your IdP configuration. ```bash cloudflow sso metadata --format xml > cloudflow-metadata.xml ``` ### Step 3: Configure IdP Add CloudFlow as a service provider in your identity provider. > **Note:** ACS URL: https://auth.cloudflow.io/saml/callback ### Step 4: Upload IdP metadata Provide your IdP's metadata to CloudFlow. ```bash cloudflow sso configure --idp-metadata ./idp-metadata.xml ``` ### Step 5: Test login Verify SSO login works correctly. ```bash cloudflow sso test ``` ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            },
            {
              "doc_id": "arch_api_gateway",
              "chunk_id": "arch_api_gateway_fix_0",
              "content": "# API Gateway Architecture ## Overview The API Gateway handles all incoming HTTP requests, providing authentication, rate limiting, and request routing. Built on Kong with custom plugins for CloudFlow-specific functionality. ## Design Principles - **Zero Trust**: Every request is authenticated and authorized - **Defense in Depth**: Multiple layers of security validation ## Components ### Kong Gateway Primary ingress controller. Handles TLS termination, request routing, and plugin execution. **Technology:** Kong 3.4 on Kubernetes **Scaling:** Horizontal with HPA ### Auth Plugin Validates JWT tokens, API keys, and OAuth2 flows. Token validation latency under 5ms. **Technology:** Custom Lua plugin **Scaling:** Stateless ### Rate Limiter Enforces per-user and per-IP rate limits. Default limit is 100 requests per minute. **Technology:** Redis-backed sliding window **Scaling:** Shared Redis cluster ## Data Flow Client \u2192 TLS \u2192 Kong \u2192 Auth \u2192 Rate Limit \u2192 Backend Services ## Performance Characteristics - **Latency P50:** 12ms - **Latency P99:** 45ms - **Throughput:** 50000 requests/second",
              "score": null
            },
            {
              "doc_id": "adr_003",
              "chunk_id": "adr_003_fix_0",
              "content": "# ADR-003: Implement OAuth 2.0 with JWT for Authentication **Status:** Accepted **Date:** 2024-05-08 ## Context We need a secure authentication system that supports API keys, user sessions, and third-party integrations. ## Options Considered ### Option 1: OAuth 2.0 with JWT Industry standard token-based auth. **Pros:** - Industry standard - Stateless validation - Supports refresh tokens - SSO compatible **Cons:** - Token size overhead - Cannot revoke individual tokens - Complexity ### Option 2: Session-based auth Traditional server-side sessions. **Pros:** - Simple implementation - Easy revocation - Smaller payloads **Cons:** - Requires session store - Not suitable for APIs - Scaling challenges ### Option 3: API keys only Simple key-based authentication. **Pros:** - Very simple - Low overhead - Easy to implement **Cons:** - No user context - Hard to rotate - No SSO support ## Decision We will implement OAuth 2.0 with JWT tokens. Access tokens expire after 1 hour, refresh tokens after 7 days. ## Consequences - Implement token rotation on refresh - Support API keys for machine-to-machine auth - Add MFA for admin accounts",
              "score": null
            },
            {
              "doc_id": "api_secrets",
              "chunk_id": "api_secrets_fix_0",
              "content": "# Secrets API Reference ## Overview The Secrets API provides programmatic access to secrets functionality. Base URL: `https://api.cloudflow.io/v1/secrets` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /secrets List secret names (values not returned). **Parameters:** **Response:** - Success: `200` - Returns array of secret metadata - Error: `400` - Bad Request **Example:** ```json {\"secrets\": [{\"name\": \"DATABASE_URL\", \"created_at\": \"2024-01-01\", \"updated_at\": \"2024-01-10\"}]} ``` ### PUT /secrets/{name} Create or update a secret. **Parameters:** - `value` (string): Secret value (encrypted at rest) **Response:** - Success: `200` - Secret saved - Error: `400` - Bad Request **Example:** ```json {\"name\": \"DATABASE_URL\", \"updated_at\": \"2024-01-15T10:00:00Z\"} ``` ### DELETE /secrets/{name} Delete a secret. **Parameters:** **Response:** - Success: `204` - Secret deleted - Error: `400` - Bad Request **Example:** ```json {} ```",
              "score": null
            }
          ],
          "latency_ms": 3.755513998839888,
          "query_id": "cross_011",
          "dimension": "casual",
          "query": "auth flow details"
        },
        {
          "key_facts": [
            "OAuth 2.0",
            "JWT",
            "bcrypt",
            "RS256",
            "cost factor 12"
          ],
          "found_facts": [
            "OAuth 2.0",
            "JWT"
          ],
          "missed_facts": [
            "bcrypt",
            "RS256",
            "cost factor 12"
          ],
          "coverage": 0.4,
          "expected_docs": [
            "adr_003",
            "arch_authentication_service",
            "arch_api_gateway"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "howto_create_api_integration",
              "chunk_id": "howto_create_api_integration_fix_0",
              "content": "# How to Create API Integration ## Prerequisites - CloudFlow account ## Steps ### Step 1: Generate API key Create an API key for programmatic access. ```bash cloudflow apikeys create --name 'CI Integration' --scopes workflows:read,workflows:execute ``` ### Step 2: Store key securely Never commit API keys to source control. Use environment variables or secret management. > **Note:** Rotate keys every 90 days ### Step 3: Test authentication Verify the API key works correctly. ```bash curl -H 'Authorization: Bearer cf_...' https://api.cloudflow.io/v1/me ``` ### Step 4: Implement rate limiting Handle rate limit responses (HTTP 429) with exponential backoff. > **Note:** Default limit is 100 requests per minute ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            },
            {
              "doc_id": "arch_api_gateway",
              "chunk_id": "arch_api_gateway_fix_0",
              "content": "# API Gateway Architecture ## Overview The API Gateway handles all incoming HTTP requests, providing authentication, rate limiting, and request routing. Built on Kong with custom plugins for CloudFlow-specific functionality. ## Design Principles - **Zero Trust**: Every request is authenticated and authorized - **Defense in Depth**: Multiple layers of security validation ## Components ### Kong Gateway Primary ingress controller. Handles TLS termination, request routing, and plugin execution. **Technology:** Kong 3.4 on Kubernetes **Scaling:** Horizontal with HPA ### Auth Plugin Validates JWT tokens, API keys, and OAuth2 flows. Token validation latency under 5ms. **Technology:** Custom Lua plugin **Scaling:** Stateless ### Rate Limiter Enforces per-user and per-IP rate limits. Default limit is 100 requests per minute. **Technology:** Redis-backed sliding window **Scaling:** Shared Redis cluster ## Data Flow Client \u2192 TLS \u2192 Kong \u2192 Auth \u2192 Rate Limit \u2192 Backend Services ## Performance Characteristics - **Latency P50:** 12ms - **Latency P99:** 45ms - **Throughput:** 50000 requests/second",
              "score": null
            },
            {
              "doc_id": "howto_configure_single_sign_on",
              "chunk_id": "howto_configure_single_sign_on_fix_0",
              "content": "# How to Configure Single Sign-On ## Prerequisites - Admin access - Enterprise plan - Identity provider access ## Steps ### Step 1: Choose provider CloudFlow supports SAML 2.0, OpenID Connect, and OAuth 2.0 providers. ### Step 2: Get metadata Download CloudFlow's SAML metadata for your IdP configuration. ```bash cloudflow sso metadata --format xml > cloudflow-metadata.xml ``` ### Step 3: Configure IdP Add CloudFlow as a service provider in your identity provider. > **Note:** ACS URL: https://auth.cloudflow.io/saml/callback ### Step 4: Upload IdP metadata Provide your IdP's metadata to CloudFlow. ```bash cloudflow sso configure --idp-metadata ./idp-metadata.xml ``` ### Step 5: Test login Verify SSO login works correctly. ```bash cloudflow sso test ``` ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            },
            {
              "doc_id": "howto_migrate_from_v1_to_v2_api",
              "chunk_id": "howto_migrate_from_v1_to_v2_api_fix_0",
              "content": "# How to Migrate from v1 to v2 API ## Prerequisites - Existing v1 integration - v2 API access enabled ## Steps ### Step 1: Review breaking changes The v2 API changes authentication from API keys to OAuth 2.0. Review the migration guide. ### Step 2: Update client library Upgrade to the latest SDK version. ```bash npm install @cloudflow/sdk@latest ``` ### Step 3: Update authentication Replace API key authentication with OAuth 2.0 client credentials flow. > **Note:** Generate new credentials in the dashboard ### Step 4: Test in staging Run your integration tests against the v2 staging endpoint. ```bash CLOUDFLOW_API_URL=https://api-v2-staging.cloudflow.io npm test ``` ### Step 5: Switch production Update production configuration to use v2 API. ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            },
            {
              "doc_id": "api_integrations",
              "chunk_id": "api_integrations_fix_0",
              "content": "# Integrations API Reference ## Overview The Integrations API provides programmatic access to integrations functionality. Base URL: `https://api.cloudflow.io/v1/integrations` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /integrations List available integrations. **Parameters:** - `category` (string): Filter by category: storage, notification, database **Response:** - Success: `200` - Returns array of integration objects - Error: `400` - Bad Request **Example:** ```json {\"integrations\": [{\"id\": \"int_s3\", \"name\": \"Amazon S3\", \"category\": \"storage\", \"status\": \"connected\"}]} ``` ### POST /integrations/{id}/connect Connect an integration. **Parameters:** - `credentials` (object): Integration-specific credentials **Response:** - Success: `200` - Integration connected - Error: `400` - Bad Request **Example:** ```json {\"id\": \"int_s3\", \"status\": \"connected\", \"connected_at\": \"2024-01-15T10:00:00Z\"} ```",
              "score": null
            }
          ],
          "latency_ms": 3.8824999992357334,
          "query_id": "cross_011",
          "dimension": "contextual",
          "query": "building a third-party integration that needs to call cloudflow APIs on behalf of users, need to understand the full auth chain from credentials to token validation"
        },
        {
          "key_facts": [
            "OAuth 2.0",
            "JWT",
            "bcrypt",
            "RS256",
            "cost factor 12"
          ],
          "found_facts": [
            "OAuth 2.0",
            "JWT",
            "bcrypt",
            "RS256",
            "cost factor 12"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "adr_003",
            "arch_authentication_service",
            "arch_api_gateway"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "adr_003",
              "chunk_id": "adr_003_fix_0",
              "content": "# ADR-003: Implement OAuth 2.0 with JWT for Authentication **Status:** Accepted **Date:** 2024-05-08 ## Context We need a secure authentication system that supports API keys, user sessions, and third-party integrations. ## Options Considered ### Option 1: OAuth 2.0 with JWT Industry standard token-based auth. **Pros:** - Industry standard - Stateless validation - Supports refresh tokens - SSO compatible **Cons:** - Token size overhead - Cannot revoke individual tokens - Complexity ### Option 2: Session-based auth Traditional server-side sessions. **Pros:** - Simple implementation - Easy revocation - Smaller payloads **Cons:** - Requires session store - Not suitable for APIs - Scaling challenges ### Option 3: API keys only Simple key-based authentication. **Pros:** - Very simple - Low overhead - Easy to implement **Cons:** - No user context - Hard to rotate - No SSO support ## Decision We will implement OAuth 2.0 with JWT tokens. Access tokens expire after 1 hour, refresh tokens after 7 days. ## Consequences - Implement token rotation on refresh - Support API keys for machine-to-machine auth - Add MFA for admin accounts",
              "score": null
            },
            {
              "doc_id": "arch_authentication_service",
              "chunk_id": "arch_authentication_service_fix_0",
              "content": "# Authentication Service Architecture ## Overview The Authentication Service handles user identity, session management, and token issuance. Supports OAuth 2.0, SAML, and API key authentication. ## Design Principles - **Zero Knowledge**: Passwords never stored in plaintext - **Token Rotation**: Refresh tokens rotate on each use ## Components ### Identity Provider Manages user accounts and credentials. Password hashing uses bcrypt with cost factor 12. **Technology:** Custom Go service **Scaling:** Horizontal stateless ### Token Service Issues and validates access tokens. Token lifetime is 1 hour with 7-day refresh tokens. **Technology:** JWT with RS256 **Scaling:** Horizontal stateless ### SSO Gateway Integrates with external identity providers. Supports Google, Microsoft, Okta, and custom SAML. **Technology:** Dex OIDC proxy **Scaling:** Active-passive ## Data Flow Credentials \u2192 Validation \u2192 Token Issue \u2192 Token Refresh \u2192 Logout ## Performance Characteristics - **Latency P50:** 50ms - **Latency P99:** 200ms - **Throughput:** 5000 auth/second",
              "score": null
            },
            {
              "doc_id": "arch_api_gateway",
              "chunk_id": "arch_api_gateway_fix_0",
              "content": "# API Gateway Architecture ## Overview The API Gateway handles all incoming HTTP requests, providing authentication, rate limiting, and request routing. Built on Kong with custom plugins for CloudFlow-specific functionality. ## Design Principles - **Zero Trust**: Every request is authenticated and authorized - **Defense in Depth**: Multiple layers of security validation ## Components ### Kong Gateway Primary ingress controller. Handles TLS termination, request routing, and plugin execution. **Technology:** Kong 3.4 on Kubernetes **Scaling:** Horizontal with HPA ### Auth Plugin Validates JWT tokens, API keys, and OAuth2 flows. Token validation latency under 5ms. **Technology:** Custom Lua plugin **Scaling:** Stateless ### Rate Limiter Enforces per-user and per-IP rate limits. Default limit is 100 requests per minute. **Technology:** Redis-backed sliding window **Scaling:** Shared Redis cluster ## Data Flow Client \u2192 TLS \u2192 Kong \u2192 Auth \u2192 Rate Limit \u2192 Backend Services ## Performance Characteristics - **Latency P50:** 12ms - **Latency P99:** 45ms - **Throughput:** 50000 requests/second",
              "score": null
            },
            {
              "doc_id": "howto_configure_single_sign_on",
              "chunk_id": "howto_configure_single_sign_on_fix_0",
              "content": "# How to Configure Single Sign-On ## Prerequisites - Admin access - Enterprise plan - Identity provider access ## Steps ### Step 1: Choose provider CloudFlow supports SAML 2.0, OpenID Connect, and OAuth 2.0 providers. ### Step 2: Get metadata Download CloudFlow's SAML metadata for your IdP configuration. ```bash cloudflow sso metadata --format xml > cloudflow-metadata.xml ``` ### Step 3: Configure IdP Add CloudFlow as a service provider in your identity provider. > **Note:** ACS URL: https://auth.cloudflow.io/saml/callback ### Step 4: Upload IdP metadata Provide your IdP's metadata to CloudFlow. ```bash cloudflow sso configure --idp-metadata ./idp-metadata.xml ``` ### Step 5: Test login Verify SSO login works correctly. ```bash cloudflow sso test ``` ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            },
            {
              "doc_id": "api_secrets",
              "chunk_id": "api_secrets_fix_0",
              "content": "# Secrets API Reference ## Overview The Secrets API provides programmatic access to secrets functionality. Base URL: `https://api.cloudflow.io/v1/secrets` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /secrets List secret names (values not returned). **Parameters:** **Response:** - Success: `200` - Returns array of secret metadata - Error: `400` - Bad Request **Example:** ```json {\"secrets\": [{\"name\": \"DATABASE_URL\", \"created_at\": \"2024-01-01\", \"updated_at\": \"2024-01-10\"}]} ``` ### PUT /secrets/{name} Create or update a secret. **Parameters:** - `value` (string): Secret value (encrypted at rest) **Response:** - Success: `200` - Secret saved - Error: `400` - Bad Request **Example:** ```json {\"name\": \"DATABASE_URL\", \"updated_at\": \"2024-01-15T10:00:00Z\"} ``` ### DELETE /secrets/{name} Delete a secret. **Parameters:** **Response:** - Success: `204` - Secret deleted - Error: `400` - Bad Request **Example:** ```json {} ```",
              "score": null
            }
          ],
          "latency_ms": 3.478328000710462,
          "query_id": "cross_011",
          "dimension": "negation",
          "query": "why isn't there a simpler auth mechanism instead of all these JWT and OAuth layers"
        },
        {
          "key_facts": [
            "Prometheus",
            "Thanos",
            "Loki",
            "Grafana",
            "Jaeger",
            "OpenTelemetry"
          ],
          "found_facts": [
            "Prometheus",
            "Thanos",
            "Loki",
            "Grafana",
            "Jaeger",
            "OpenTelemetry"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "arch_monitoring_stack"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "release_3_0_0",
              "chunk_id": "release_3_0_0_fix_0",
              "content": "# Release Notes - v3.0.0 **Release Date:** 2023-10-15 ## New Features - Complete UI redesign - New workflow editor with visual builder - Real-time execution monitoring - Team workspaces ## Bug Fixes - Fixed critical security issue in token validation - Resolved data loss bug in failed migrations ## Breaking Changes - **BREAKING:** Authentication API completely rewritten - **BREAKING:** Legacy webhook format no longer supported - **BREAKING:** Minimum Node.js version now 18 ## Upgrade Instructions ```bash cloudflow upgrade --version 3.0.0 ``` ## Known Issues None at this time.",
              "score": null
            },
            {
              "doc_id": "api_metrics",
              "chunk_id": "api_metrics_fix_0",
              "content": "# Metrics API Reference ## Overview The Metrics API provides programmatic access to metrics functionality. Base URL: `https://api.cloudflow.io/v1/metrics` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /metrics/workflows Get workflow performance metrics. **Parameters:** - `period` (string): Time period: hour, day, week, month - `workflow_id` (string): Filter by workflow ID **Response:** - Success: `200` - Returns metrics data - Error: `400` - Bad Request **Example:** ```json {\"success_rate\": 0.985, \"avg_duration_ms\": 1250, \"p99_duration_ms\": 5000, \"total_executions\": 10000} ```",
              "score": null
            },
            {
              "doc_id": "release_3_1_0",
              "chunk_id": "release_3_1_0_fix_0",
              "content": "# Release Notes - v3.1.0 **Release Date:** 2023-12-01 ## New Features - GraphQL API beta release - Workflow templates marketplace - Enhanced audit logging - SSO support for Okta ## Bug Fixes - Fixed webhook delivery retry logic - Resolved issue with large payload handling - Fixed dashboard loading performance ## Upgrade Instructions ```bash cloudflow upgrade --version 3.1.0 ``` ## Known Issues None at this time.",
              "score": null
            },
            {
              "doc_id": "arch_monitoring_stack",
              "chunk_id": "arch_monitoring_stack_fix_0",
              "content": "# Monitoring Stack Architecture ## Overview The Monitoring Stack provides observability into system health, performance, and errors. Includes metrics, logs, traces, and alerting. ## Design Principles - **Observability**: Every component emits metrics and traces - **Actionable Alerts**: Alerts include runbook links ## Components ### Metrics Collects time-series metrics. Retention is 15 days local, 1 year in object storage. **Technology:** Prometheus with Thanos **Scaling:** Federated with long-term storage ### Logging Centralized log aggregation. Supports structured JSON logs with label indexing. **Technology:** Loki with Grafana **Scaling:** Distributed with sharding ### Tracing Distributed request tracing. Trace retention is 7 days. **Technology:** Jaeger with OpenTelemetry **Scaling:** Sampling at 10% ### Alerting Routes alerts to on-call engineers. Supports escalation policies and silencing. **Technology:** Alertmanager with PagerDuty **Scaling:** HA pair ## Data Flow Application \u2192 Collector \u2192 Storage \u2192 Query \u2192 Dashboard/Alert ## Performance Characteristics - **Latency P50:** N/A - **Latency P99:** N/A - **Throughput:** 1M metrics/second, 100GB logs/day",
              "score": null
            },
            {
              "doc_id": "arch_api_gateway",
              "chunk_id": "arch_api_gateway_fix_0",
              "content": "# API Gateway Architecture ## Overview The API Gateway handles all incoming HTTP requests, providing authentication, rate limiting, and request routing. Built on Kong with custom plugins for CloudFlow-specific functionality. ## Design Principles - **Zero Trust**: Every request is authenticated and authorized - **Defense in Depth**: Multiple layers of security validation ## Components ### Kong Gateway Primary ingress controller. Handles TLS termination, request routing, and plugin execution. **Technology:** Kong 3.4 on Kubernetes **Scaling:** Horizontal with HPA ### Auth Plugin Validates JWT tokens, API keys, and OAuth2 flows. Token validation latency under 5ms. **Technology:** Custom Lua plugin **Scaling:** Stateless ### Rate Limiter Enforces per-user and per-IP rate limits. Default limit is 100 requests per minute. **Technology:** Redis-backed sliding window **Scaling:** Shared Redis cluster ## Data Flow Client \u2192 TLS \u2192 Kong \u2192 Auth \u2192 Rate Limit \u2192 Backend Services ## Performance Characteristics - **Latency P50:** 12ms - **Latency P99:** 45ms - **Throughput:** 50000 requests/second",
              "score": null
            }
          ],
          "latency_ms": 3.561621999324416,
          "query_id": "cross_012",
          "dimension": "original",
          "query": "What monitoring and observability tools does CloudFlow use?"
        },
        {
          "key_facts": [
            "Prometheus",
            "Thanos",
            "Loki",
            "Grafana",
            "Jaeger",
            "OpenTelemetry"
          ],
          "found_facts": [
            "Prometheus",
            "Thanos",
            "Loki",
            "Grafana",
            "Jaeger",
            "OpenTelemetry"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "arch_monitoring_stack"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "api_metrics",
              "chunk_id": "api_metrics_fix_0",
              "content": "# Metrics API Reference ## Overview The Metrics API provides programmatic access to metrics functionality. Base URL: `https://api.cloudflow.io/v1/metrics` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /metrics/workflows Get workflow performance metrics. **Parameters:** - `period` (string): Time period: hour, day, week, month - `workflow_id` (string): Filter by workflow ID **Response:** - Success: `200` - Returns metrics data - Error: `400` - Bad Request **Example:** ```json {\"success_rate\": 0.985, \"avg_duration_ms\": 1250, \"p99_duration_ms\": 5000, \"total_executions\": 10000} ```",
              "score": null
            },
            {
              "doc_id": "arch_monitoring_stack",
              "chunk_id": "arch_monitoring_stack_fix_0",
              "content": "# Monitoring Stack Architecture ## Overview The Monitoring Stack provides observability into system health, performance, and errors. Includes metrics, logs, traces, and alerting. ## Design Principles - **Observability**: Every component emits metrics and traces - **Actionable Alerts**: Alerts include runbook links ## Components ### Metrics Collects time-series metrics. Retention is 15 days local, 1 year in object storage. **Technology:** Prometheus with Thanos **Scaling:** Federated with long-term storage ### Logging Centralized log aggregation. Supports structured JSON logs with label indexing. **Technology:** Loki with Grafana **Scaling:** Distributed with sharding ### Tracing Distributed request tracing. Trace retention is 7 days. **Technology:** Jaeger with OpenTelemetry **Scaling:** Sampling at 10% ### Alerting Routes alerts to on-call engineers. Supports escalation policies and silencing. **Technology:** Alertmanager with PagerDuty **Scaling:** HA pair ## Data Flow Application \u2192 Collector \u2192 Storage \u2192 Query \u2192 Dashboard/Alert ## Performance Characteristics - **Latency P50:** N/A - **Latency P99:** N/A - **Throughput:** 1M metrics/second, 100GB logs/day",
              "score": null
            },
            {
              "doc_id": "arch_api_gateway",
              "chunk_id": "arch_api_gateway_fix_0",
              "content": "# API Gateway Architecture ## Overview The API Gateway handles all incoming HTTP requests, providing authentication, rate limiting, and request routing. Built on Kong with custom plugins for CloudFlow-specific functionality. ## Design Principles - **Zero Trust**: Every request is authenticated and authorized - **Defense in Depth**: Multiple layers of security validation ## Components ### Kong Gateway Primary ingress controller. Handles TLS termination, request routing, and plugin execution. **Technology:** Kong 3.4 on Kubernetes **Scaling:** Horizontal with HPA ### Auth Plugin Validates JWT tokens, API keys, and OAuth2 flows. Token validation latency under 5ms. **Technology:** Custom Lua plugin **Scaling:** Stateless ### Rate Limiter Enforces per-user and per-IP rate limits. Default limit is 100 requests per minute. **Technology:** Redis-backed sliding window **Scaling:** Shared Redis cluster ## Data Flow Client \u2192 TLS \u2192 Kong \u2192 Auth \u2192 Rate Limit \u2192 Backend Services ## Performance Characteristics - **Latency P50:** 12ms - **Latency P99:** 45ms - **Throughput:** 50000 requests/second",
              "score": null
            },
            {
              "doc_id": "api_audit",
              "chunk_id": "api_audit_fix_0",
              "content": "# Audit API Reference ## Overview The Audit API provides programmatic access to audit functionality. Base URL: `https://api.cloudflow.io/v1/audit` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /audit/logs Retrieve audit logs for compliance. **Parameters:** - `start_date` (date): Start date for log range - `end_date` (date): End date for log range - `actor` (string): Filter by user ID **Response:** - Success: `200` - Returns array of audit log entries - Error: `400` - Bad Request **Example:** ```json {\"logs\": [{\"timestamp\": \"2024-01-15T10:00:00Z\", \"actor\": \"usr_001\", \"action\": \"workflow.created\", \"resource\": \"wf_123\"}]} ```",
              "score": null
            },
            {
              "doc_id": "release_3_1_0",
              "chunk_id": "release_3_1_0_fix_0",
              "content": "# Release Notes - v3.1.0 **Release Date:** 2023-12-01 ## New Features - GraphQL API beta release - Workflow templates marketplace - Enhanced audit logging - SSO support for Okta ## Bug Fixes - Fixed webhook delivery retry logic - Resolved issue with large payload handling - Fixed dashboard loading performance ## Upgrade Instructions ```bash cloudflow upgrade --version 3.1.0 ``` ## Known Issues None at this time.",
              "score": null
            }
          ],
          "latency_ms": 3.446188000452821,
          "query_id": "cross_012",
          "dimension": "synonym",
          "query": "what's the metrics and logging infrastructure in cloudflow"
        },
        {
          "key_facts": [
            "Prometheus",
            "Thanos",
            "Loki",
            "Grafana",
            "Jaeger",
            "OpenTelemetry"
          ],
          "found_facts": [],
          "missed_facts": [
            "Prometheus",
            "Thanos",
            "Loki",
            "Grafana",
            "Jaeger",
            "OpenTelemetry"
          ],
          "coverage": 0.0,
          "expected_docs": [
            "arch_monitoring_stack"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "howto_debug_failed_execution",
              "chunk_id": "howto_debug_failed_execution_fix_0",
              "content": "# How to Debug Failed Execution ## Prerequisites - Failed execution ID ## Steps ### Step 1: Get execution details Retrieve the full execution record including step outputs. ```bash cloudflow executions get ex_123 --include-steps ``` ### Step 2: View step logs Check logs for the failed step. ```bash cloudflow executions logs ex_123 --step 3 ``` ### Step 3: Inspect input/output Review the data passed between steps. ```bash cloudflow executions inspect ex_123 --step 3 ``` ### Step 4: Enable debug mode Re-run with verbose logging enabled. ```bash cloudflow workflows execute wf_123 --debug ``` ### Step 5: Check error patterns Look for similar failures in recent executions. ```bash cloudflow executions list --status failed --since 24h ``` ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            },
            {
              "doc_id": "runbook_api_gateway_5xx_errors",
              "chunk_id": "runbook_api_gateway_5xx_errors_fix_0",
              "content": "# Runbook: API Gateway 5xx Errors ## Severity **Level:** P3 - Medium **Response Time:** 1 hour ## Symptoms - Error rate > 1% - 5xx responses in access logs - Downstream service failures ## Diagnosis ### Step 1 Check Kong error logs: `kubectl logs -l app=kong -n kong --tail=100 | grep -E '5[0-9]{2}'` ### Step 2 Verify backend health: `kubectl get pods -n production -o wide` ### Step 3 Check service endpoints: `kubectl get endpoints -n production` ### Step 4 Test backend directly: `kubectl exec -it kong-xxx -n kong -- curl http://api.production.svc:8080/health` ## Resolution ### Step 1 Restart unhealthy backends: `kubectl rollout restart deployment/api -n production` ### Step 2 Enable circuit breaker if cascading: update Kong circuit-breaker plugin ### Step 3 Scale up backends: `kubectl scale deployment/api --replicas=5 -n production` ### Step 4 Check for resource exhaustion on backend pods ### Step 5 Verify network policies allow traffic ## Escalation If 5xx persists > 15 minutes, page on-call. May indicate infrastructure issue. ## Post-Incident - Document timeline in incident log - Schedule post-mortem within 48 hours - Update runbook if new patterns discovered",
              "score": null
            },
            {
              "doc_id": "runbook_high_cpu_usage",
              "chunk_id": "runbook_high_cpu_usage_fix_0",
              "content": "# Runbook: High CPU Usage ## Severity **Level:** P1 - Critical **Response Time:** 15 minutes ## Symptoms - CPU utilization > 80% sustained - Increased response latency - Autoscaler at maximum replicas ## Diagnosis ### Step 1 Check current CPU usage: `kubectl top pods -n production` ### Step 2 Identify hot pods: `kubectl top pods --sort-by=cpu -n production | head -10` ### Step 3 Check for recent deployments: `kubectl rollout history deployment/api -n production` ### Step 4 Profile the application: `cloudflow debug profile --pod api-xyz --duration 60s` ## Resolution ### Step 1 Scale up if traffic-related: `kubectl scale deployment/api --replicas=10 -n production` ### Step 2 Restart pods if memory leak suspected: `kubectl rollout restart deployment/api -n production` ### Step 3 Roll back recent deployment if regression: `kubectl rollout undo deployment/api -n production` ### Step 4 Add resource limits if unbounded: update deployment with CPU limits ## Escalation If CPU remains high after scaling, escalate to platform team. Page on-call if affecting users. ## Post-Incident - Document timeline in incident log - Schedule post-mortem within 48 hours - Update runbook if new patterns discovered",
              "score": null
            },
            {
              "doc_id": "release_3_1_0",
              "chunk_id": "release_3_1_0_fix_0",
              "content": "# Release Notes - v3.1.0 **Release Date:** 2023-12-01 ## New Features - GraphQL API beta release - Workflow templates marketplace - Enhanced audit logging - SSO support for Okta ## Bug Fixes - Fixed webhook delivery retry logic - Resolved issue with large payload handling - Fixed dashboard loading performance ## Upgrade Instructions ```bash cloudflow upgrade --version 3.1.0 ``` ## Known Issues None at this time.",
              "score": null
            },
            {
              "doc_id": "runbook_out_of_memory_oom_kills",
              "chunk_id": "runbook_out_of_memory_oom_kills_fix_0",
              "content": "# Runbook: Out of Memory (OOM) Kills ## Severity **Level:** P3 - Medium **Response Time:** 15 minutes ## Symptoms - Pods restarting with OOMKilled status - Memory usage at limit - Application slowdown before restart ## Diagnosis ### Step 1 Check OOM events: `kubectl get events -n production --field-selector reason=OOMKilled` ### Step 2 Review memory usage: `kubectl top pods -n production --sort-by=memory` ### Step 3 Check container limits: `kubectl describe pod <pod-name> -n production | grep -A5 Limits` ### Step 4 Analyze heap dumps if available: `cloudflow debug heapdump --pod <pod-name>` ## Resolution ### Step 1 Increase memory limit: update deployment resources.limits.memory ### Step 2 Add memory request equal to limit to guarantee allocation ### Step 3 Enable JVM heap dump on OOM: add -XX:+HeapDumpOnOutOfMemoryError ### Step 4 Investigate memory leaks with profiling tools ### Step 5 Consider horizontal scaling instead of vertical ## Escalation If OOMs continue after limit increase, escalate to development team for memory leak investigation. ## Post-Incident - Document timeline in incident log - Schedule post-mortem within 48 hours - Update runbook if new patterns discovered",
              "score": null
            }
          ],
          "latency_ms": 4.350491999502992,
          "query_id": "cross_012",
          "dimension": "problem",
          "query": "can't find any dashboards or logs when trying to debug a production issue"
        },
        {
          "key_facts": [
            "Prometheus",
            "Thanos",
            "Loki",
            "Grafana",
            "Jaeger",
            "OpenTelemetry"
          ],
          "found_facts": [
            "Prometheus",
            "Thanos",
            "Loki",
            "Grafana",
            "Jaeger",
            "OpenTelemetry"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "arch_monitoring_stack"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "arch_monitoring_stack",
              "chunk_id": "arch_monitoring_stack_fix_0",
              "content": "# Monitoring Stack Architecture ## Overview The Monitoring Stack provides observability into system health, performance, and errors. Includes metrics, logs, traces, and alerting. ## Design Principles - **Observability**: Every component emits metrics and traces - **Actionable Alerts**: Alerts include runbook links ## Components ### Metrics Collects time-series metrics. Retention is 15 days local, 1 year in object storage. **Technology:** Prometheus with Thanos **Scaling:** Federated with long-term storage ### Logging Centralized log aggregation. Supports structured JSON logs with label indexing. **Technology:** Loki with Grafana **Scaling:** Distributed with sharding ### Tracing Distributed request tracing. Trace retention is 7 days. **Technology:** Jaeger with OpenTelemetry **Scaling:** Sampling at 10% ### Alerting Routes alerts to on-call engineers. Supports escalation policies and silencing. **Technology:** Alertmanager with PagerDuty **Scaling:** HA pair ## Data Flow Application \u2192 Collector \u2192 Storage \u2192 Query \u2192 Dashboard/Alert ## Performance Characteristics - **Latency P50:** N/A - **Latency P99:** N/A - **Throughput:** 1M metrics/second, 100GB logs/day",
              "score": null
            },
            {
              "doc_id": "adr_005",
              "chunk_id": "adr_005_fix_0",
              "content": "# ADR-005: Adopt Event-Driven Architecture for Workflow Execution **Status:** Accepted **Date:** 2024-12-04 ## Context We need an architecture that supports asynchronous workflow execution, step dependencies, and failure recovery. ## Options Considered ### Option 1: Event-driven with Kafka Publish-subscribe messaging. **Pros:** - Loose coupling - Scalability - Event replay - Audit trail **Cons:** - Eventual consistency - Operational complexity - Message ordering challenges ### Option 2: Request-response Synchronous API calls. **Pros:** - Simple mental model - Immediate feedback - Easy debugging **Cons:** - Tight coupling - Cascading failures - Timeout issues ### Option 3: Polling-based Workers poll for tasks. **Pros:** - Simple implementation - Easy rate limiting - No message broker **Cons:** - Polling overhead - Higher latency - Inefficient at scale ## Decision We will use an event-driven architecture with Kafka for workflow step execution. ## Consequences - Implement idempotent consumers - Use dead letter queues for failed messages - Monitor consumer lag",
              "score": null
            },
            {
              "doc_id": "runbook_high_cpu_usage",
              "chunk_id": "runbook_high_cpu_usage_fix_0",
              "content": "# Runbook: High CPU Usage ## Severity **Level:** P1 - Critical **Response Time:** 15 minutes ## Symptoms - CPU utilization > 80% sustained - Increased response latency - Autoscaler at maximum replicas ## Diagnosis ### Step 1 Check current CPU usage: `kubectl top pods -n production` ### Step 2 Identify hot pods: `kubectl top pods --sort-by=cpu -n production | head -10` ### Step 3 Check for recent deployments: `kubectl rollout history deployment/api -n production` ### Step 4 Profile the application: `cloudflow debug profile --pod api-xyz --duration 60s` ## Resolution ### Step 1 Scale up if traffic-related: `kubectl scale deployment/api --replicas=10 -n production` ### Step 2 Restart pods if memory leak suspected: `kubectl rollout restart deployment/api -n production` ### Step 3 Roll back recent deployment if regression: `kubectl rollout undo deployment/api -n production` ### Step 4 Add resource limits if unbounded: update deployment with CPU limits ## Escalation If CPU remains high after scaling, escalate to platform team. Page on-call if affecting users. ## Post-Incident - Document timeline in incident log - Schedule post-mortem within 48 hours - Update runbook if new patterns discovered",
              "score": null
            },
            {
              "doc_id": "arch_notification_service",
              "chunk_id": "arch_notification_service_fix_0",
              "content": "# Notification Service Architecture ## Overview The Notification Service delivers alerts and updates through multiple channels including email, Slack, webhooks, and mobile push notifications. ## Design Principles - **Delivery Guarantee**: At-least-once delivery with deduplication - **Channel Abstraction**: Unified API regardless of delivery channel ## Components ### Dispatcher Routes notifications to appropriate channel handlers. Supports priority queues for urgent notifications. **Technology:** Go microservice **Scaling:** Horizontal with queue partitioning ### Email Provider Sends transactional emails. Maximum 10000 emails per hour per account. **Technology:** SendGrid integration **Scaling:** API rate limited ### Slack Integration Posts messages to Slack channels. Supports message formatting and interactive components. **Technology:** Slack Bolt SDK **Scaling:** Per-workspace rate limits ## Data Flow Event \u2192 Template \u2192 Dispatcher \u2192 Channel \u2192 Delivery Confirmation ## Performance Characteristics - **Latency P50:** 200ms - **Latency P99:** 2000ms - **Throughput:** 10000 notifications/minute",
              "score": null
            },
            {
              "doc_id": "howto_set_up_monitoring_alerts",
              "chunk_id": "howto_set_up_monitoring_alerts_fix_0",
              "content": "# How to Set Up Monitoring Alerts ## Prerequisites - CloudFlow Pro plan or higher ## Steps ### Step 1: Define alert rules Create alert conditions based on metrics thresholds. ```bash cloudflow alerts create --name 'High Error Rate' --condition 'error_rate > 0.05' ``` ### Step 2: Configure channels Set up notification channels: email, Slack, or PagerDuty. ```bash cloudflow alerts channel add --type slack --webhook https://hooks.slack.com/... ``` ### Step 3: Set severity levels Assign severity to each alert: critical, warning, or info. > **Note:** Critical alerts bypass quiet hours ### Step 4: Test alerts Trigger a test alert to verify routing. ```bash cloudflow alerts test --name 'High Error Rate' ``` ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            }
          ],
          "latency_ms": 4.392721000840538,
          "query_id": "cross_012",
          "dimension": "casual",
          "query": "monitoring stack"
        },
        {
          "key_facts": [
            "Prometheus",
            "Thanos",
            "Loki",
            "Grafana",
            "Jaeger",
            "OpenTelemetry"
          ],
          "found_facts": [
            "Prometheus",
            "Thanos",
            "Loki",
            "Grafana",
            "Jaeger",
            "OpenTelemetry"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "arch_monitoring_stack"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "arch_monitoring_stack",
              "chunk_id": "arch_monitoring_stack_fix_0",
              "content": "# Monitoring Stack Architecture ## Overview The Monitoring Stack provides observability into system health, performance, and errors. Includes metrics, logs, traces, and alerting. ## Design Principles - **Observability**: Every component emits metrics and traces - **Actionable Alerts**: Alerts include runbook links ## Components ### Metrics Collects time-series metrics. Retention is 15 days local, 1 year in object storage. **Technology:** Prometheus with Thanos **Scaling:** Federated with long-term storage ### Logging Centralized log aggregation. Supports structured JSON logs with label indexing. **Technology:** Loki with Grafana **Scaling:** Distributed with sharding ### Tracing Distributed request tracing. Trace retention is 7 days. **Technology:** Jaeger with OpenTelemetry **Scaling:** Sampling at 10% ### Alerting Routes alerts to on-call engineers. Supports escalation policies and silencing. **Technology:** Alertmanager with PagerDuty **Scaling:** HA pair ## Data Flow Application \u2192 Collector \u2192 Storage \u2192 Query \u2192 Dashboard/Alert ## Performance Characteristics - **Latency P50:** N/A - **Latency P99:** N/A - **Throughput:** 1M metrics/second, 100GB logs/day",
              "score": null
            },
            {
              "doc_id": "howto_set_up_monitoring_alerts",
              "chunk_id": "howto_set_up_monitoring_alerts_fix_0",
              "content": "# How to Set Up Monitoring Alerts ## Prerequisites - CloudFlow Pro plan or higher ## Steps ### Step 1: Define alert rules Create alert conditions based on metrics thresholds. ```bash cloudflow alerts create --name 'High Error Rate' --condition 'error_rate > 0.05' ``` ### Step 2: Configure channels Set up notification channels: email, Slack, or PagerDuty. ```bash cloudflow alerts channel add --type slack --webhook https://hooks.slack.com/... ``` ### Step 3: Set severity levels Assign severity to each alert: critical, warning, or info. > **Note:** Critical alerts bypass quiet hours ### Step 4: Test alerts Trigger a test alert to verify routing. ```bash cloudflow alerts test --name 'High Error Rate' ``` ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            },
            {
              "doc_id": "arch_notification_service",
              "chunk_id": "arch_notification_service_fix_0",
              "content": "# Notification Service Architecture ## Overview The Notification Service delivers alerts and updates through multiple channels including email, Slack, webhooks, and mobile push notifications. ## Design Principles - **Delivery Guarantee**: At-least-once delivery with deduplication - **Channel Abstraction**: Unified API regardless of delivery channel ## Components ### Dispatcher Routes notifications to appropriate channel handlers. Supports priority queues for urgent notifications. **Technology:** Go microservice **Scaling:** Horizontal with queue partitioning ### Email Provider Sends transactional emails. Maximum 10000 emails per hour per account. **Technology:** SendGrid integration **Scaling:** API rate limited ### Slack Integration Posts messages to Slack channels. Supports message formatting and interactive components. **Technology:** Slack Bolt SDK **Scaling:** Per-workspace rate limits ## Data Flow Event \u2192 Template \u2192 Dispatcher \u2192 Channel \u2192 Delivery Confirmation ## Performance Characteristics - **Latency P50:** 200ms - **Latency P99:** 2000ms - **Throughput:** 10000 notifications/minute",
              "score": null
            },
            {
              "doc_id": "adr_005",
              "chunk_id": "adr_005_fix_0",
              "content": "# ADR-005: Adopt Event-Driven Architecture for Workflow Execution **Status:** Accepted **Date:** 2024-12-04 ## Context We need an architecture that supports asynchronous workflow execution, step dependencies, and failure recovery. ## Options Considered ### Option 1: Event-driven with Kafka Publish-subscribe messaging. **Pros:** - Loose coupling - Scalability - Event replay - Audit trail **Cons:** - Eventual consistency - Operational complexity - Message ordering challenges ### Option 2: Request-response Synchronous API calls. **Pros:** - Simple mental model - Immediate feedback - Easy debugging **Cons:** - Tight coupling - Cascading failures - Timeout issues ### Option 3: Polling-based Workers poll for tasks. **Pros:** - Simple implementation - Easy rate limiting - No message broker **Cons:** - Polling overhead - Higher latency - Inefficient at scale ## Decision We will use an event-driven architecture with Kafka for workflow step execution. ## Consequences - Implement idempotent consumers - Use dead letter queues for failed messages - Monitor consumer lag",
              "score": null
            },
            {
              "doc_id": "arch_workflow_engine",
              "chunk_id": "arch_workflow_engine_fix_0",
              "content": "# Workflow Engine Architecture ## Overview The Workflow Engine is the core component responsible for executing user-defined workflows. It uses an event-driven architecture with support for parallel execution, conditional branching, and automatic retries. ## Design Principles - **Idempotency**: All operations can be safely retried without side effects - **Eventual Consistency**: State converges to consistent view within 5 seconds - **Fault Tolerance**: System continues operating despite component failures ## Components ### Scheduler Manages workflow scheduling and triggers. Supports cron expressions, webhooks, and event-based triggers. **Technology:** Custom Go service **Scaling:** Horizontal with leader election ### Executor Runs individual workflow steps in isolated containers. Maximum execution time is 30 minutes per step. **Technology:** Kubernetes Jobs **Scaling:** Auto-scaling based on queue depth ### State Store Persists workflow state, execution history, and step results. Uses optimistic locking for concurrent updates. **Technology:** PostgreSQL with JSONB **Scaling:** Primary-replica with read replicas ## Data Flow Triggers \u2192 Scheduler \u2192 Job Queue \u2192 Executor \u2192 State Store \u2192 Webhooks/Notifications ## Performance Characteristics - **Latency P50:** 45ms - **Latency P99:** 250ms - **Throughput:** 5000 executions/minute",
              "score": null
            }
          ],
          "latency_ms": 3.6355600004753796,
          "query_id": "cross_012",
          "dimension": "contextual",
          "query": "setting up alerts for our team's workflows, need to know what monitoring tools are available and how they connect"
        },
        {
          "key_facts": [
            "Prometheus",
            "Thanos",
            "Loki",
            "Grafana",
            "Jaeger",
            "OpenTelemetry"
          ],
          "found_facts": [
            "Prometheus",
            "Thanos",
            "Loki",
            "Grafana",
            "Jaeger",
            "OpenTelemetry"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "arch_monitoring_stack"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "arch_monitoring_stack",
              "chunk_id": "arch_monitoring_stack_fix_0",
              "content": "# Monitoring Stack Architecture ## Overview The Monitoring Stack provides observability into system health, performance, and errors. Includes metrics, logs, traces, and alerting. ## Design Principles - **Observability**: Every component emits metrics and traces - **Actionable Alerts**: Alerts include runbook links ## Components ### Metrics Collects time-series metrics. Retention is 15 days local, 1 year in object storage. **Technology:** Prometheus with Thanos **Scaling:** Federated with long-term storage ### Logging Centralized log aggregation. Supports structured JSON logs with label indexing. **Technology:** Loki with Grafana **Scaling:** Distributed with sharding ### Tracing Distributed request tracing. Trace retention is 7 days. **Technology:** Jaeger with OpenTelemetry **Scaling:** Sampling at 10% ### Alerting Routes alerts to on-call engineers. Supports escalation policies and silencing. **Technology:** Alertmanager with PagerDuty **Scaling:** HA pair ## Data Flow Application \u2192 Collector \u2192 Storage \u2192 Query \u2192 Dashboard/Alert ## Performance Characteristics - **Latency P50:** N/A - **Latency P99:** N/A - **Throughput:** 1M metrics/second, 100GB logs/day",
              "score": null
            },
            {
              "doc_id": "adr_005",
              "chunk_id": "adr_005_fix_0",
              "content": "# ADR-005: Adopt Event-Driven Architecture for Workflow Execution **Status:** Accepted **Date:** 2024-12-04 ## Context We need an architecture that supports asynchronous workflow execution, step dependencies, and failure recovery. ## Options Considered ### Option 1: Event-driven with Kafka Publish-subscribe messaging. **Pros:** - Loose coupling - Scalability - Event replay - Audit trail **Cons:** - Eventual consistency - Operational complexity - Message ordering challenges ### Option 2: Request-response Synchronous API calls. **Pros:** - Simple mental model - Immediate feedback - Easy debugging **Cons:** - Tight coupling - Cascading failures - Timeout issues ### Option 3: Polling-based Workers poll for tasks. **Pros:** - Simple implementation - Easy rate limiting - No message broker **Cons:** - Polling overhead - Higher latency - Inefficient at scale ## Decision We will use an event-driven architecture with Kafka for workflow step execution. ## Consequences - Implement idempotent consumers - Use dead letter queues for failed messages - Monitor consumer lag",
              "score": null
            },
            {
              "doc_id": "adr_006",
              "chunk_id": "adr_006_fix_0",
              "content": "# ADR-006: Use Terraform for Infrastructure as Code **Status:** Accepted **Date:** 2024-11-24 ## Context We need a way to manage cloud infrastructure consistently across environments. ## Options Considered ### Option 1: Terraform Multi-cloud IaC tool. **Pros:** - Multi-cloud support - Large community - State management - Plan before apply **Cons:** - State file management - Learning curve - HCL limitations ### Option 2: AWS CloudFormation AWS-native IaC. **Pros:** - Native AWS support - No state file - Drift detection **Cons:** - AWS only - Verbose YAML/JSON - Slower development ### Option 3: Pulumi IaC with real programming languages. **Pros:** - Real languages - Type safety - Reusable components **Cons:** - Smaller community - Vendor lock-in - Commercial features ## Decision We will use Terraform with S3 backend for state and DynamoDB for locking. ## Consequences - Implement module structure - Use workspaces for environments - Automate with Atlantis",
              "score": null
            },
            {
              "doc_id": "arch_workflow_engine",
              "chunk_id": "arch_workflow_engine_fix_0",
              "content": "# Workflow Engine Architecture ## Overview The Workflow Engine is the core component responsible for executing user-defined workflows. It uses an event-driven architecture with support for parallel execution, conditional branching, and automatic retries. ## Design Principles - **Idempotency**: All operations can be safely retried without side effects - **Eventual Consistency**: State converges to consistent view within 5 seconds - **Fault Tolerance**: System continues operating despite component failures ## Components ### Scheduler Manages workflow scheduling and triggers. Supports cron expressions, webhooks, and event-based triggers. **Technology:** Custom Go service **Scaling:** Horizontal with leader election ### Executor Runs individual workflow steps in isolated containers. Maximum execution time is 30 minutes per step. **Technology:** Kubernetes Jobs **Scaling:** Auto-scaling based on queue depth ### State Store Persists workflow state, execution history, and step results. Uses optimistic locking for concurrent updates. **Technology:** PostgreSQL with JSONB **Scaling:** Primary-replica with read replicas ## Data Flow Triggers \u2192 Scheduler \u2192 Job Queue \u2192 Executor \u2192 State Store \u2192 Webhooks/Notifications ## Performance Characteristics - **Latency P50:** 45ms - **Latency P99:** 250ms - **Throughput:** 5000 executions/minute",
              "score": null
            },
            {
              "doc_id": "arch_notification_service",
              "chunk_id": "arch_notification_service_fix_0",
              "content": "# Notification Service Architecture ## Overview The Notification Service delivers alerts and updates through multiple channels including email, Slack, webhooks, and mobile push notifications. ## Design Principles - **Delivery Guarantee**: At-least-once delivery with deduplication - **Channel Abstraction**: Unified API regardless of delivery channel ## Components ### Dispatcher Routes notifications to appropriate channel handlers. Supports priority queues for urgent notifications. **Technology:** Go microservice **Scaling:** Horizontal with queue partitioning ### Email Provider Sends transactional emails. Maximum 10000 emails per hour per account. **Technology:** SendGrid integration **Scaling:** API rate limited ### Slack Integration Posts messages to Slack channels. Supports message formatting and interactive components. **Technology:** Slack Bolt SDK **Scaling:** Per-workspace rate limits ## Data Flow Event \u2192 Template \u2192 Dispatcher \u2192 Channel \u2192 Delivery Confirmation ## Performance Characteristics - **Latency P50:** 200ms - **Latency P99:** 2000ms - **Throughput:** 10000 notifications/minute",
              "score": null
            }
          ],
          "latency_ms": 3.474290000667679,
          "query_id": "cross_012",
          "dimension": "negation",
          "query": "why are there so many different monitoring tools instead of a unified platform"
        },
        {
          "key_facts": [
            "Redis Cluster",
            "Ristretto",
            "CloudFront",
            "cache-aside"
          ],
          "found_facts": [
            "Redis Cluster",
            "Ristretto",
            "CloudFront",
            "cache-aside"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "arch_caching_layer",
            "adr_004",
            "config_cache"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "arch_caching_layer",
              "chunk_id": "arch_caching_layer_fix_0",
              "content": "# Caching Layer Architecture ## Overview The Caching Layer reduces latency and database load by caching frequently accessed data. Uses a multi-tier strategy with local and distributed caches. ## Design Principles - **Cache Invalidation**: Explicit invalidation on writes - **Graceful Degradation**: System works without cache ## Components ### Local Cache First-level cache with 100ms TTL. Reduces Redis round-trips for hot data. **Technology:** Ristretto in-memory cache **Scaling:** Per-pod memory allocation ### Distributed Cache Second-level cache with configurable TTL. Maximum key size 512MB. **Technology:** Redis Cluster 7.x **Scaling:** 6-node cluster with sharding ### CDN Caches static assets and API responses. Cache hit ratio target 95%. **Technology:** CloudFront with Lambda@Edge **Scaling:** Global edge locations ## Data Flow Request \u2192 Local Cache \u2192 Redis \u2192 Database \u2192 Cache Population ## Performance Characteristics - **Latency P50:** 1ms (hit), 50ms (miss) - **Latency P99:** 5ms (hit), 200ms (miss) - **Throughput:** 100000 reads/second",
              "score": null
            },
            {
              "doc_id": "adr_004",
              "chunk_id": "adr_004_fix_0",
              "content": "# ADR-004: Use Redis for Caching and Session Storage **Status:** Accepted **Date:** 2024-04-05 ## Context We need a fast data store for caching API responses, session data, and rate limiting counters. ## Options Considered ### Option 1: Redis Cluster In-memory data structure store. **Pros:** - Sub-millisecond latency - Rich data structures - Pub/sub support - Clustering **Cons:** - Memory cost - Persistence complexity - Cluster management ### Option 2: Memcached Simple key-value cache. **Pros:** - Simple operation - Multi-threaded - Lower memory overhead **Cons:** - No persistence - Limited data types - No clustering ### Option 3: Application-level cache In-process caching only. **Pros:** - No external dependency - Fastest possible - Simple **Cons:** - Not shared across pods - Memory pressure - Cold starts ## Decision We will use Redis Cluster for caching with a 6-node cluster for high availability. ## Consequences - Implement cache-aside pattern - Set appropriate TTLs - Monitor memory usage - Plan for cache stampede prevention",
              "score": null
            },
            {
              "doc_id": "config_cache",
              "chunk_id": "config_cache_fix_0",
              "content": "# Cache Configuration Reference ## Overview This document describes all configuration options for Cache. ## Environment Variables | Variable | Type | Default | Description | |----------|------|---------|-------------| | `REDIS_URL` | string | required | Redis connection string | | `REDIS_POOL_SIZE` | integer | 10 | Connection pool size | | `CACHE_TTL_SECONDS` | integer | 300 | Default cache TTL | | `CACHE_PREFIX` | string | cf: | Key prefix for namespacing | | `CACHE_COMPRESSION` | boolean | true | Enable value compression | | `CACHE_MAX_SIZE_MB` | integer | 100 | Maximum cache size per pod | ## Configuration File Configuration can also be provided via `cache.yaml`: ```yaml redis_url: required redis_pool_size: 10 cache_ttl_seconds: 300 cache_prefix: cf: cache_compression: true ``` ## Validation Configuration is validated at startup. Invalid configuration will prevent the service from starting.",
              "score": null
            },
            {
              "doc_id": "arch_data_pipeline",
              "chunk_id": "arch_data_pipeline_fix_0",
              "content": "# Data Pipeline Architecture ## Overview The Data Pipeline processes and transforms data between workflow steps. It supports batch processing up to 10GB and streaming for real-time use cases. ## Design Principles - **Exactly-Once Processing**: Each record is processed exactly once - **Backpressure Handling**: System gracefully handles varying load ## Components ### Stream Processor Handles real-time data streams. Maximum throughput of 100000 messages per second. **Technology:** Apache Kafka with Flink **Scaling:** Partition-based scaling ### Batch Processor Processes large batch jobs. Supports data formats: JSON, CSV, Parquet, Avro. **Technology:** Apache Spark on Kubernetes **Scaling:** Dynamic resource allocation ### Data Lake Long-term data storage with time-travel queries. Retention configurable up to 7 years. **Technology:** S3 with Iceberg tables **Scaling:** Unlimited with tiered storage ## Data Flow Input \u2192 Validation \u2192 Transform \u2192 Enrich \u2192 Output ## Performance Characteristics - **Latency P50:** 100ms (streaming), 5min (batch) - **Latency P99:** 500ms (streaming), 30min (batch) - **Throughput:** 100000 messages/second (streaming)",
              "score": null
            },
            {
              "doc_id": "arch_workflow_engine",
              "chunk_id": "arch_workflow_engine_fix_0",
              "content": "# Workflow Engine Architecture ## Overview The Workflow Engine is the core component responsible for executing user-defined workflows. It uses an event-driven architecture with support for parallel execution, conditional branching, and automatic retries. ## Design Principles - **Idempotency**: All operations can be safely retried without side effects - **Eventual Consistency**: State converges to consistent view within 5 seconds - **Fault Tolerance**: System continues operating despite component failures ## Components ### Scheduler Manages workflow scheduling and triggers. Supports cron expressions, webhooks, and event-based triggers. **Technology:** Custom Go service **Scaling:** Horizontal with leader election ### Executor Runs individual workflow steps in isolated containers. Maximum execution time is 30 minutes per step. **Technology:** Kubernetes Jobs **Scaling:** Auto-scaling based on queue depth ### State Store Persists workflow state, execution history, and step results. Uses optimistic locking for concurrent updates. **Technology:** PostgreSQL with JSONB **Scaling:** Primary-replica with read replicas ## Data Flow Triggers \u2192 Scheduler \u2192 Job Queue \u2192 Executor \u2192 State Store \u2192 Webhooks/Notifications ## Performance Characteristics - **Latency P50:** 45ms - **Latency P99:** 250ms - **Throughput:** 5000 executions/minute",
              "score": null
            }
          ],
          "latency_ms": 3.6027589994773734,
          "query_id": "cross_013",
          "dimension": "original",
          "query": "What caching strategy is used across the system?"
        },
        {
          "key_facts": [
            "Redis Cluster",
            "Ristretto",
            "CloudFront",
            "cache-aside"
          ],
          "found_facts": [
            "Redis Cluster",
            "Ristretto",
            "CloudFront"
          ],
          "missed_facts": [
            "cache-aside"
          ],
          "coverage": 0.75,
          "expected_docs": [
            "arch_caching_layer",
            "adr_004",
            "config_cache"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "arch_data_pipeline",
              "chunk_id": "arch_data_pipeline_fix_0",
              "content": "# Data Pipeline Architecture ## Overview The Data Pipeline processes and transforms data between workflow steps. It supports batch processing up to 10GB and streaming for real-time use cases. ## Design Principles - **Exactly-Once Processing**: Each record is processed exactly once - **Backpressure Handling**: System gracefully handles varying load ## Components ### Stream Processor Handles real-time data streams. Maximum throughput of 100000 messages per second. **Technology:** Apache Kafka with Flink **Scaling:** Partition-based scaling ### Batch Processor Processes large batch jobs. Supports data formats: JSON, CSV, Parquet, Avro. **Technology:** Apache Spark on Kubernetes **Scaling:** Dynamic resource allocation ### Data Lake Long-term data storage with time-travel queries. Retention configurable up to 7 years. **Technology:** S3 with Iceberg tables **Scaling:** Unlimited with tiered storage ## Data Flow Input \u2192 Validation \u2192 Transform \u2192 Enrich \u2192 Output ## Performance Characteristics - **Latency P50:** 100ms (streaming), 5min (batch) - **Latency P99:** 500ms (streaming), 30min (batch) - **Throughput:** 100000 messages/second (streaming)",
              "score": null
            },
            {
              "doc_id": "arch_caching_layer",
              "chunk_id": "arch_caching_layer_fix_0",
              "content": "# Caching Layer Architecture ## Overview The Caching Layer reduces latency and database load by caching frequently accessed data. Uses a multi-tier strategy with local and distributed caches. ## Design Principles - **Cache Invalidation**: Explicit invalidation on writes - **Graceful Degradation**: System works without cache ## Components ### Local Cache First-level cache with 100ms TTL. Reduces Redis round-trips for hot data. **Technology:** Ristretto in-memory cache **Scaling:** Per-pod memory allocation ### Distributed Cache Second-level cache with configurable TTL. Maximum key size 512MB. **Technology:** Redis Cluster 7.x **Scaling:** 6-node cluster with sharding ### CDN Caches static assets and API responses. Cache hit ratio target 95%. **Technology:** CloudFront with Lambda@Edge **Scaling:** Global edge locations ## Data Flow Request \u2192 Local Cache \u2192 Redis \u2192 Database \u2192 Cache Population ## Performance Characteristics - **Latency P50:** 1ms (hit), 50ms (miss) - **Latency P99:** 5ms (hit), 200ms (miss) - **Throughput:** 100000 reads/second",
              "score": null
            },
            {
              "doc_id": "arch_workflow_engine",
              "chunk_id": "arch_workflow_engine_fix_0",
              "content": "# Workflow Engine Architecture ## Overview The Workflow Engine is the core component responsible for executing user-defined workflows. It uses an event-driven architecture with support for parallel execution, conditional branching, and automatic retries. ## Design Principles - **Idempotency**: All operations can be safely retried without side effects - **Eventual Consistency**: State converges to consistent view within 5 seconds - **Fault Tolerance**: System continues operating despite component failures ## Components ### Scheduler Manages workflow scheduling and triggers. Supports cron expressions, webhooks, and event-based triggers. **Technology:** Custom Go service **Scaling:** Horizontal with leader election ### Executor Runs individual workflow steps in isolated containers. Maximum execution time is 30 minutes per step. **Technology:** Kubernetes Jobs **Scaling:** Auto-scaling based on queue depth ### State Store Persists workflow state, execution history, and step results. Uses optimistic locking for concurrent updates. **Technology:** PostgreSQL with JSONB **Scaling:** Primary-replica with read replicas ## Data Flow Triggers \u2192 Scheduler \u2192 Job Queue \u2192 Executor \u2192 State Store \u2192 Webhooks/Notifications ## Performance Characteristics - **Latency P50:** 45ms - **Latency P99:** 250ms - **Throughput:** 5000 executions/minute",
              "score": null
            },
            {
              "doc_id": "howto_set_up_data_encryption",
              "chunk_id": "howto_set_up_data_encryption_fix_0",
              "content": "# How to Set Up Data Encryption ## Prerequisites - CloudFlow Enterprise plan ## Steps ### Step 1: Enable encryption at rest All data is encrypted at rest using AES-256 by default. > **Note:** Encryption is automatic for all plans ### Step 2: Configure KMS Bring your own encryption keys using AWS KMS or GCP KMS. ```bash cloudflow encryption configure --kms-key arn:aws:kms:... ``` ### Step 3: Enable field-level encryption Encrypt sensitive fields within your workflow data. ```bash cloudflow encryption add-field --path $.user.ssn --key-alias sensitive-data ``` ### Step 4: Verify encryption Confirm encryption is active for your workspace. ```bash cloudflow encryption status ``` ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            },
            {
              "doc_id": "api_secrets",
              "chunk_id": "api_secrets_fix_0",
              "content": "# Secrets API Reference ## Overview The Secrets API provides programmatic access to secrets functionality. Base URL: `https://api.cloudflow.io/v1/secrets` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /secrets List secret names (values not returned). **Parameters:** **Response:** - Success: `200` - Returns array of secret metadata - Error: `400` - Bad Request **Example:** ```json {\"secrets\": [{\"name\": \"DATABASE_URL\", \"created_at\": \"2024-01-01\", \"updated_at\": \"2024-01-10\"}]} ``` ### PUT /secrets/{name} Create or update a secret. **Parameters:** - `value` (string): Secret value (encrypted at rest) **Response:** - Success: `200` - Secret saved - Error: `400` - Bad Request **Example:** ```json {\"name\": \"DATABASE_URL\", \"updated_at\": \"2024-01-15T10:00:00Z\"} ``` ### DELETE /secrets/{name} Delete a secret. **Parameters:** **Response:** - Success: `204` - Secret deleted - Error: `400` - Bad Request **Example:** ```json {} ```",
              "score": null
            }
          ],
          "latency_ms": 4.778137999892351,
          "query_id": "cross_013",
          "dimension": "synonym",
          "query": "how does cloudflow handle data caching and what layers are involved"
        },
        {
          "key_facts": [
            "Redis Cluster",
            "Ristretto",
            "CloudFront",
            "cache-aside"
          ],
          "found_facts": [
            "Redis Cluster",
            "Ristretto",
            "CloudFront",
            "cache-aside"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "arch_caching_layer",
            "adr_004",
            "config_cache"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "arch_caching_layer",
              "chunk_id": "arch_caching_layer_fix_0",
              "content": "# Caching Layer Architecture ## Overview The Caching Layer reduces latency and database load by caching frequently accessed data. Uses a multi-tier strategy with local and distributed caches. ## Design Principles - **Cache Invalidation**: Explicit invalidation on writes - **Graceful Degradation**: System works without cache ## Components ### Local Cache First-level cache with 100ms TTL. Reduces Redis round-trips for hot data. **Technology:** Ristretto in-memory cache **Scaling:** Per-pod memory allocation ### Distributed Cache Second-level cache with configurable TTL. Maximum key size 512MB. **Technology:** Redis Cluster 7.x **Scaling:** 6-node cluster with sharding ### CDN Caches static assets and API responses. Cache hit ratio target 95%. **Technology:** CloudFront with Lambda@Edge **Scaling:** Global edge locations ## Data Flow Request \u2192 Local Cache \u2192 Redis \u2192 Database \u2192 Cache Population ## Performance Characteristics - **Latency P50:** 1ms (hit), 50ms (miss) - **Latency P99:** 5ms (hit), 200ms (miss) - **Throughput:** 100000 reads/second",
              "score": null
            },
            {
              "doc_id": "adr_004",
              "chunk_id": "adr_004_fix_0",
              "content": "# ADR-004: Use Redis for Caching and Session Storage **Status:** Accepted **Date:** 2024-04-05 ## Context We need a fast data store for caching API responses, session data, and rate limiting counters. ## Options Considered ### Option 1: Redis Cluster In-memory data structure store. **Pros:** - Sub-millisecond latency - Rich data structures - Pub/sub support - Clustering **Cons:** - Memory cost - Persistence complexity - Cluster management ### Option 2: Memcached Simple key-value cache. **Pros:** - Simple operation - Multi-threaded - Lower memory overhead **Cons:** - No persistence - Limited data types - No clustering ### Option 3: Application-level cache In-process caching only. **Pros:** - No external dependency - Fastest possible - Simple **Cons:** - Not shared across pods - Memory pressure - Cold starts ## Decision We will use Redis Cluster for caching with a 6-node cluster for high availability. ## Consequences - Implement cache-aside pattern - Set appropriate TTLs - Monitor memory usage - Plan for cache stampede prevention",
              "score": null
            },
            {
              "doc_id": "arch_data_pipeline",
              "chunk_id": "arch_data_pipeline_fix_0",
              "content": "# Data Pipeline Architecture ## Overview The Data Pipeline processes and transforms data between workflow steps. It supports batch processing up to 10GB and streaming for real-time use cases. ## Design Principles - **Exactly-Once Processing**: Each record is processed exactly once - **Backpressure Handling**: System gracefully handles varying load ## Components ### Stream Processor Handles real-time data streams. Maximum throughput of 100000 messages per second. **Technology:** Apache Kafka with Flink **Scaling:** Partition-based scaling ### Batch Processor Processes large batch jobs. Supports data formats: JSON, CSV, Parquet, Avro. **Technology:** Apache Spark on Kubernetes **Scaling:** Dynamic resource allocation ### Data Lake Long-term data storage with time-travel queries. Retention configurable up to 7 years. **Technology:** S3 with Iceberg tables **Scaling:** Unlimited with tiered storage ## Data Flow Input \u2192 Validation \u2192 Transform \u2192 Enrich \u2192 Output ## Performance Characteristics - **Latency P50:** 100ms (streaming), 5min (batch) - **Latency P99:** 500ms (streaming), 30min (batch) - **Throughput:** 100000 messages/second (streaming)",
              "score": null
            },
            {
              "doc_id": "config_cache",
              "chunk_id": "config_cache_fix_0",
              "content": "# Cache Configuration Reference ## Overview This document describes all configuration options for Cache. ## Environment Variables | Variable | Type | Default | Description | |----------|------|---------|-------------| | `REDIS_URL` | string | required | Redis connection string | | `REDIS_POOL_SIZE` | integer | 10 | Connection pool size | | `CACHE_TTL_SECONDS` | integer | 300 | Default cache TTL | | `CACHE_PREFIX` | string | cf: | Key prefix for namespacing | | `CACHE_COMPRESSION` | boolean | true | Enable value compression | | `CACHE_MAX_SIZE_MB` | integer | 100 | Maximum cache size per pod | ## Configuration File Configuration can also be provided via `cache.yaml`: ```yaml redis_url: required redis_pool_size: 10 cache_ttl_seconds: 300 cache_prefix: cf: cache_compression: true ``` ## Validation Configuration is validated at startup. Invalid configuration will prevent the service from starting.",
              "score": null
            },
            {
              "doc_id": "runbook_kafka_consumer_lag",
              "chunk_id": "runbook_kafka_consumer_lag_fix_0",
              "content": "# Runbook: Kafka Consumer Lag ## Severity **Level:** P1 - Critical **Response Time:** 1 hour ## Symptoms - Consumer lag > 10000 messages - Processing delays reported - Alerts from lag monitor ## Diagnosis ### Step 1 Check consumer group lag: `kafka-consumer-groups.sh --bootstrap-server kafka:9092 --describe --group workflow-executor` ### Step 2 Verify consumer health: `kubectl get pods -l app=workflow-executor -n production` ### Step 3 Check for processing errors: `kubectl logs -l app=workflow-executor --tail=100 -n production | grep ERROR` ### Step 4 Monitor throughput: `cloudflow metrics get kafka.consumer.records_per_second` ## Resolution ### Step 1 Scale up consumers: `kubectl scale deployment/workflow-executor --replicas=10 -n production` ### Step 2 Reset offset if messages are stale: `kafka-consumer-groups.sh --bootstrap-server kafka:9092 --group workflow-executor --reset-offsets --to-latest --execute --topic workflows` ### Step 3 Pause producer if backpressure needed: `cloudflow workflows pause-triggers` ### Step 4 Increase partition count for parallelism (requires coordination) ## Escalation If lag persists > 30 minutes, page platform team. May need Kafka cluster scaling. ## Post-Incident - Document timeline in incident log - Schedule post-mortem within 48 hours - Update runbook if new patterns discovered",
              "score": null
            }
          ],
          "latency_ms": 3.7399749999167398,
          "query_id": "cross_013",
          "dimension": "problem",
          "query": "response times are slow even though we should be hitting cache, not sure where the bottleneck is"
        },
        {
          "key_facts": [
            "Redis Cluster",
            "Ristretto",
            "CloudFront",
            "cache-aside"
          ],
          "found_facts": [
            "Redis Cluster",
            "Ristretto",
            "CloudFront",
            "cache-aside"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "arch_caching_layer",
            "adr_004",
            "config_cache"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "arch_caching_layer",
              "chunk_id": "arch_caching_layer_fix_0",
              "content": "# Caching Layer Architecture ## Overview The Caching Layer reduces latency and database load by caching frequently accessed data. Uses a multi-tier strategy with local and distributed caches. ## Design Principles - **Cache Invalidation**: Explicit invalidation on writes - **Graceful Degradation**: System works without cache ## Components ### Local Cache First-level cache with 100ms TTL. Reduces Redis round-trips for hot data. **Technology:** Ristretto in-memory cache **Scaling:** Per-pod memory allocation ### Distributed Cache Second-level cache with configurable TTL. Maximum key size 512MB. **Technology:** Redis Cluster 7.x **Scaling:** 6-node cluster with sharding ### CDN Caches static assets and API responses. Cache hit ratio target 95%. **Technology:** CloudFront with Lambda@Edge **Scaling:** Global edge locations ## Data Flow Request \u2192 Local Cache \u2192 Redis \u2192 Database \u2192 Cache Population ## Performance Characteristics - **Latency P50:** 1ms (hit), 50ms (miss) - **Latency P99:** 5ms (hit), 200ms (miss) - **Throughput:** 100000 reads/second",
              "score": null
            },
            {
              "doc_id": "adr_004",
              "chunk_id": "adr_004_fix_0",
              "content": "# ADR-004: Use Redis for Caching and Session Storage **Status:** Accepted **Date:** 2024-04-05 ## Context We need a fast data store for caching API responses, session data, and rate limiting counters. ## Options Considered ### Option 1: Redis Cluster In-memory data structure store. **Pros:** - Sub-millisecond latency - Rich data structures - Pub/sub support - Clustering **Cons:** - Memory cost - Persistence complexity - Cluster management ### Option 2: Memcached Simple key-value cache. **Pros:** - Simple operation - Multi-threaded - Lower memory overhead **Cons:** - No persistence - Limited data types - No clustering ### Option 3: Application-level cache In-process caching only. **Pros:** - No external dependency - Fastest possible - Simple **Cons:** - Not shared across pods - Memory pressure - Cold starts ## Decision We will use Redis Cluster for caching with a 6-node cluster for high availability. ## Consequences - Implement cache-aside pattern - Set appropriate TTLs - Monitor memory usage - Plan for cache stampede prevention",
              "score": null
            },
            {
              "doc_id": "config_cache",
              "chunk_id": "config_cache_fix_0",
              "content": "# Cache Configuration Reference ## Overview This document describes all configuration options for Cache. ## Environment Variables | Variable | Type | Default | Description | |----------|------|---------|-------------| | `REDIS_URL` | string | required | Redis connection string | | `REDIS_POOL_SIZE` | integer | 10 | Connection pool size | | `CACHE_TTL_SECONDS` | integer | 300 | Default cache TTL | | `CACHE_PREFIX` | string | cf: | Key prefix for namespacing | | `CACHE_COMPRESSION` | boolean | true | Enable value compression | | `CACHE_MAX_SIZE_MB` | integer | 100 | Maximum cache size per pod | ## Configuration File Configuration can also be provided via `cache.yaml`: ```yaml redis_url: required redis_pool_size: 10 cache_ttl_seconds: 300 cache_prefix: cf: cache_compression: true ``` ## Validation Configuration is validated at startup. Invalid configuration will prevent the service from starting.",
              "score": null
            },
            {
              "doc_id": "arch_data_pipeline",
              "chunk_id": "arch_data_pipeline_fix_0",
              "content": "# Data Pipeline Architecture ## Overview The Data Pipeline processes and transforms data between workflow steps. It supports batch processing up to 10GB and streaming for real-time use cases. ## Design Principles - **Exactly-Once Processing**: Each record is processed exactly once - **Backpressure Handling**: System gracefully handles varying load ## Components ### Stream Processor Handles real-time data streams. Maximum throughput of 100000 messages per second. **Technology:** Apache Kafka with Flink **Scaling:** Partition-based scaling ### Batch Processor Processes large batch jobs. Supports data formats: JSON, CSV, Parquet, Avro. **Technology:** Apache Spark on Kubernetes **Scaling:** Dynamic resource allocation ### Data Lake Long-term data storage with time-travel queries. Retention configurable up to 7 years. **Technology:** S3 with Iceberg tables **Scaling:** Unlimited with tiered storage ## Data Flow Input \u2192 Validation \u2192 Transform \u2192 Enrich \u2192 Output ## Performance Characteristics - **Latency P50:** 100ms (streaming), 5min (batch) - **Latency P99:** 500ms (streaming), 30min (batch) - **Throughput:** 100000 messages/second (streaming)",
              "score": null
            },
            {
              "doc_id": "adr_008",
              "chunk_id": "adr_008_fix_0",
              "content": "# ADR-008: Use S3 for Object Storage **Status:** Accepted **Date:** 2024-10-14 ## Context We need storage for workflow artifacts, execution outputs, and user uploads. ## Options Considered ### Option 1: Amazon S3 Object storage service. **Pros:** - 99.999999999% durability - Lifecycle policies - Event notifications - CDN integration **Cons:** - AWS lock-in - Eventual consistency for overwrites - Cost at scale ### Option 2: Self-hosted MinIO S3-compatible object storage. **Pros:** - No vendor lock-in - S3 compatible - On-premises option **Cons:** - Operational burden - Lower durability guarantees - Scaling complexity ### Option 3: Google Cloud Storage GCP object storage. **Pros:** - Strong consistency - Good performance - Unified billing **Cons:** - GCP lock-in - Fewer features than S3 - Smaller ecosystem ## Decision We will use Amazon S3 for object storage with intelligent tiering for cost optimization. ## Consequences - Enable versioning for critical buckets - Implement lifecycle policies - Use pre-signed URLs for uploads - Enable server-side encryption",
              "score": null
            }
          ],
          "latency_ms": 3.661589000330423,
          "query_id": "cross_013",
          "dimension": "casual",
          "query": "caching layers explained"
        },
        {
          "key_facts": [
            "Redis Cluster",
            "Ristretto",
            "CloudFront",
            "cache-aside"
          ],
          "found_facts": [
            "Redis Cluster",
            "Ristretto",
            "CloudFront",
            "cache-aside"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "arch_caching_layer",
            "adr_004",
            "config_cache"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "arch_caching_layer",
              "chunk_id": "arch_caching_layer_fix_0",
              "content": "# Caching Layer Architecture ## Overview The Caching Layer reduces latency and database load by caching frequently accessed data. Uses a multi-tier strategy with local and distributed caches. ## Design Principles - **Cache Invalidation**: Explicit invalidation on writes - **Graceful Degradation**: System works without cache ## Components ### Local Cache First-level cache with 100ms TTL. Reduces Redis round-trips for hot data. **Technology:** Ristretto in-memory cache **Scaling:** Per-pod memory allocation ### Distributed Cache Second-level cache with configurable TTL. Maximum key size 512MB. **Technology:** Redis Cluster 7.x **Scaling:** 6-node cluster with sharding ### CDN Caches static assets and API responses. Cache hit ratio target 95%. **Technology:** CloudFront with Lambda@Edge **Scaling:** Global edge locations ## Data Flow Request \u2192 Local Cache \u2192 Redis \u2192 Database \u2192 Cache Population ## Performance Characteristics - **Latency P50:** 1ms (hit), 50ms (miss) - **Latency P99:** 5ms (hit), 200ms (miss) - **Throughput:** 100000 reads/second",
              "score": null
            },
            {
              "doc_id": "adr_004",
              "chunk_id": "adr_004_fix_0",
              "content": "# ADR-004: Use Redis for Caching and Session Storage **Status:** Accepted **Date:** 2024-04-05 ## Context We need a fast data store for caching API responses, session data, and rate limiting counters. ## Options Considered ### Option 1: Redis Cluster In-memory data structure store. **Pros:** - Sub-millisecond latency - Rich data structures - Pub/sub support - Clustering **Cons:** - Memory cost - Persistence complexity - Cluster management ### Option 2: Memcached Simple key-value cache. **Pros:** - Simple operation - Multi-threaded - Lower memory overhead **Cons:** - No persistence - Limited data types - No clustering ### Option 3: Application-level cache In-process caching only. **Pros:** - No external dependency - Fastest possible - Simple **Cons:** - Not shared across pods - Memory pressure - Cold starts ## Decision We will use Redis Cluster for caching with a 6-node cluster for high availability. ## Consequences - Implement cache-aside pattern - Set appropriate TTLs - Monitor memory usage - Plan for cache stampede prevention",
              "score": null
            },
            {
              "doc_id": "adr_007",
              "chunk_id": "adr_007_fix_0",
              "content": "# ADR-007: Implement Rate Limiting at API Gateway **Status:** Accepted **Date:** 2024-09-03 ## Context We need to protect our services from abuse and ensure fair resource usage across customers. ## Options Considered ### Option 1: Sliding window at gateway Rate limit at API gateway level. **Pros:** - Centralized enforcement - Low latency - Easy configuration **Cons:** - Gateway becomes bottleneck - Coarse granularity - Redis dependency ### Option 2: Per-service limiting Each service implements its own limits. **Pros:** - Fine-grained control - Service autonomy - No single point of failure **Cons:** - Inconsistent implementation - Higher complexity - Distributed state ### Option 3: Token bucket Token-based rate limiting. **Pros:** - Allows bursts - Smooth rate limiting - Well understood **Cons:** - More complex - Harder to explain to users - State management ## Decision We will implement sliding window rate limiting at the Kong API Gateway using Redis. ## Consequences - Default limit: 100 requests per minute - Implement per-plan limits - Return rate limit headers - Add retry-after on 429",
              "score": null
            },
            {
              "doc_id": "config_cache",
              "chunk_id": "config_cache_fix_0",
              "content": "# Cache Configuration Reference ## Overview This document describes all configuration options for Cache. ## Environment Variables | Variable | Type | Default | Description | |----------|------|---------|-------------| | `REDIS_URL` | string | required | Redis connection string | | `REDIS_POOL_SIZE` | integer | 10 | Connection pool size | | `CACHE_TTL_SECONDS` | integer | 300 | Default cache TTL | | `CACHE_PREFIX` | string | cf: | Key prefix for namespacing | | `CACHE_COMPRESSION` | boolean | true | Enable value compression | | `CACHE_MAX_SIZE_MB` | integer | 100 | Maximum cache size per pod | ## Configuration File Configuration can also be provided via `cache.yaml`: ```yaml redis_url: required redis_pool_size: 10 cache_ttl_seconds: 300 cache_prefix: cf: cache_compression: true ``` ## Validation Configuration is validated at startup. Invalid configuration will prevent the service from starting.",
              "score": null
            },
            {
              "doc_id": "arch_api_gateway",
              "chunk_id": "arch_api_gateway_fix_0",
              "content": "# API Gateway Architecture ## Overview The API Gateway handles all incoming HTTP requests, providing authentication, rate limiting, and request routing. Built on Kong with custom plugins for CloudFlow-specific functionality. ## Design Principles - **Zero Trust**: Every request is authenticated and authorized - **Defense in Depth**: Multiple layers of security validation ## Components ### Kong Gateway Primary ingress controller. Handles TLS termination, request routing, and plugin execution. **Technology:** Kong 3.4 on Kubernetes **Scaling:** Horizontal with HPA ### Auth Plugin Validates JWT tokens, API keys, and OAuth2 flows. Token validation latency under 5ms. **Technology:** Custom Lua plugin **Scaling:** Stateless ### Rate Limiter Enforces per-user and per-IP rate limits. Default limit is 100 requests per minute. **Technology:** Redis-backed sliding window **Scaling:** Shared Redis cluster ## Data Flow Client \u2192 TLS \u2192 Kong \u2192 Auth \u2192 Rate Limit \u2192 Backend Services ## Performance Characteristics - **Latency P50:** 12ms - **Latency P99:** 45ms - **Throughput:** 50000 requests/second",
              "score": null
            }
          ],
          "latency_ms": 3.58736999987741,
          "query_id": "cross_013",
          "dimension": "contextual",
          "query": "optimizing our API response times, need to understand all the caching layers from local to CDN and their TTLs"
        },
        {
          "key_facts": [
            "Redis Cluster",
            "Ristretto",
            "CloudFront",
            "cache-aside"
          ],
          "found_facts": [
            "Redis Cluster",
            "Ristretto",
            "CloudFront",
            "cache-aside"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "arch_caching_layer",
            "adr_004",
            "config_cache"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "arch_caching_layer",
              "chunk_id": "arch_caching_layer_fix_0",
              "content": "# Caching Layer Architecture ## Overview The Caching Layer reduces latency and database load by caching frequently accessed data. Uses a multi-tier strategy with local and distributed caches. ## Design Principles - **Cache Invalidation**: Explicit invalidation on writes - **Graceful Degradation**: System works without cache ## Components ### Local Cache First-level cache with 100ms TTL. Reduces Redis round-trips for hot data. **Technology:** Ristretto in-memory cache **Scaling:** Per-pod memory allocation ### Distributed Cache Second-level cache with configurable TTL. Maximum key size 512MB. **Technology:** Redis Cluster 7.x **Scaling:** 6-node cluster with sharding ### CDN Caches static assets and API responses. Cache hit ratio target 95%. **Technology:** CloudFront with Lambda@Edge **Scaling:** Global edge locations ## Data Flow Request \u2192 Local Cache \u2192 Redis \u2192 Database \u2192 Cache Population ## Performance Characteristics - **Latency P50:** 1ms (hit), 50ms (miss) - **Latency P99:** 5ms (hit), 200ms (miss) - **Throughput:** 100000 reads/second",
              "score": null
            },
            {
              "doc_id": "adr_004",
              "chunk_id": "adr_004_fix_0",
              "content": "# ADR-004: Use Redis for Caching and Session Storage **Status:** Accepted **Date:** 2024-04-05 ## Context We need a fast data store for caching API responses, session data, and rate limiting counters. ## Options Considered ### Option 1: Redis Cluster In-memory data structure store. **Pros:** - Sub-millisecond latency - Rich data structures - Pub/sub support - Clustering **Cons:** - Memory cost - Persistence complexity - Cluster management ### Option 2: Memcached Simple key-value cache. **Pros:** - Simple operation - Multi-threaded - Lower memory overhead **Cons:** - No persistence - Limited data types - No clustering ### Option 3: Application-level cache In-process caching only. **Pros:** - No external dependency - Fastest possible - Simple **Cons:** - Not shared across pods - Memory pressure - Cold starts ## Decision We will use Redis Cluster for caching with a 6-node cluster for high availability. ## Consequences - Implement cache-aside pattern - Set appropriate TTLs - Monitor memory usage - Plan for cache stampede prevention",
              "score": null
            },
            {
              "doc_id": "config_cache",
              "chunk_id": "config_cache_fix_0",
              "content": "# Cache Configuration Reference ## Overview This document describes all configuration options for Cache. ## Environment Variables | Variable | Type | Default | Description | |----------|------|---------|-------------| | `REDIS_URL` | string | required | Redis connection string | | `REDIS_POOL_SIZE` | integer | 10 | Connection pool size | | `CACHE_TTL_SECONDS` | integer | 300 | Default cache TTL | | `CACHE_PREFIX` | string | cf: | Key prefix for namespacing | | `CACHE_COMPRESSION` | boolean | true | Enable value compression | | `CACHE_MAX_SIZE_MB` | integer | 100 | Maximum cache size per pod | ## Configuration File Configuration can also be provided via `cache.yaml`: ```yaml redis_url: required redis_pool_size: 10 cache_ttl_seconds: 300 cache_prefix: cf: cache_compression: true ``` ## Validation Configuration is validated at startup. Invalid configuration will prevent the service from starting.",
              "score": null
            },
            {
              "doc_id": "adr_007",
              "chunk_id": "adr_007_fix_0",
              "content": "# ADR-007: Implement Rate Limiting at API Gateway **Status:** Accepted **Date:** 2024-09-03 ## Context We need to protect our services from abuse and ensure fair resource usage across customers. ## Options Considered ### Option 1: Sliding window at gateway Rate limit at API gateway level. **Pros:** - Centralized enforcement - Low latency - Easy configuration **Cons:** - Gateway becomes bottleneck - Coarse granularity - Redis dependency ### Option 2: Per-service limiting Each service implements its own limits. **Pros:** - Fine-grained control - Service autonomy - No single point of failure **Cons:** - Inconsistent implementation - Higher complexity - Distributed state ### Option 3: Token bucket Token-based rate limiting. **Pros:** - Allows bursts - Smooth rate limiting - Well understood **Cons:** - More complex - Harder to explain to users - State management ## Decision We will implement sliding window rate limiting at the Kong API Gateway using Redis. ## Consequences - Default limit: 100 requests per minute - Implement per-plan limits - Return rate limit headers - Add retry-after on 429",
              "score": null
            },
            {
              "doc_id": "adr_008",
              "chunk_id": "adr_008_fix_0",
              "content": "# ADR-008: Use S3 for Object Storage **Status:** Accepted **Date:** 2024-10-14 ## Context We need storage for workflow artifacts, execution outputs, and user uploads. ## Options Considered ### Option 1: Amazon S3 Object storage service. **Pros:** - 99.999999999% durability - Lifecycle policies - Event notifications - CDN integration **Cons:** - AWS lock-in - Eventual consistency for overwrites - Cost at scale ### Option 2: Self-hosted MinIO S3-compatible object storage. **Pros:** - No vendor lock-in - S3 compatible - On-premises option **Cons:** - Operational burden - Lower durability guarantees - Scaling complexity ### Option 3: Google Cloud Storage GCP object storage. **Pros:** - Strong consistency - Good performance - Unified billing **Cons:** - GCP lock-in - Fewer features than S3 - Smaller ecosystem ## Decision We will use Amazon S3 for object storage with intelligent tiering for cost optimization. ## Consequences - Enable versioning for critical buckets - Implement lifecycle policies - Use pre-signed URLs for uploads - Enable server-side encryption",
              "score": null
            }
          ],
          "latency_ms": 4.042538001158391,
          "query_id": "cross_013",
          "dimension": "negation",
          "query": "why do we have both local and distributed cache instead of just using redis for everything"
        },
        {
          "key_facts": [
            "PostgreSQL",
            "S3",
            "Iceberg",
            "Parquet"
          ],
          "found_facts": [
            "PostgreSQL",
            "S3",
            "Iceberg",
            "Parquet"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "adr_001",
            "adr_008",
            "arch_data_pipeline"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "arch_data_pipeline",
              "chunk_id": "arch_data_pipeline_fix_0",
              "content": "# Data Pipeline Architecture ## Overview The Data Pipeline processes and transforms data between workflow steps. It supports batch processing up to 10GB and streaming for real-time use cases. ## Design Principles - **Exactly-Once Processing**: Each record is processed exactly once - **Backpressure Handling**: System gracefully handles varying load ## Components ### Stream Processor Handles real-time data streams. Maximum throughput of 100000 messages per second. **Technology:** Apache Kafka with Flink **Scaling:** Partition-based scaling ### Batch Processor Processes large batch jobs. Supports data formats: JSON, CSV, Parquet, Avro. **Technology:** Apache Spark on Kubernetes **Scaling:** Dynamic resource allocation ### Data Lake Long-term data storage with time-travel queries. Retention configurable up to 7 years. **Technology:** S3 with Iceberg tables **Scaling:** Unlimited with tiered storage ## Data Flow Input \u2192 Validation \u2192 Transform \u2192 Enrich \u2192 Output ## Performance Characteristics - **Latency P50:** 100ms (streaming), 5min (batch) - **Latency P99:** 500ms (streaming), 30min (batch) - **Throughput:** 100000 messages/second (streaming)",
              "score": null
            },
            {
              "doc_id": "adr_008",
              "chunk_id": "adr_008_fix_0",
              "content": "# ADR-008: Use S3 for Object Storage **Status:** Accepted **Date:** 2024-10-14 ## Context We need storage for workflow artifacts, execution outputs, and user uploads. ## Options Considered ### Option 1: Amazon S3 Object storage service. **Pros:** - 99.999999999% durability - Lifecycle policies - Event notifications - CDN integration **Cons:** - AWS lock-in - Eventual consistency for overwrites - Cost at scale ### Option 2: Self-hosted MinIO S3-compatible object storage. **Pros:** - No vendor lock-in - S3 compatible - On-premises option **Cons:** - Operational burden - Lower durability guarantees - Scaling complexity ### Option 3: Google Cloud Storage GCP object storage. **Pros:** - Strong consistency - Good performance - Unified billing **Cons:** - GCP lock-in - Fewer features than S3 - Smaller ecosystem ## Decision We will use Amazon S3 for object storage with intelligent tiering for cost optimization. ## Consequences - Enable versioning for critical buckets - Implement lifecycle policies - Use pre-signed URLs for uploads - Enable server-side encryption",
              "score": null
            },
            {
              "doc_id": "howto_set_up_data_encryption",
              "chunk_id": "howto_set_up_data_encryption_fix_0",
              "content": "# How to Set Up Data Encryption ## Prerequisites - CloudFlow Enterprise plan ## Steps ### Step 1: Enable encryption at rest All data is encrypted at rest using AES-256 by default. > **Note:** Encryption is automatic for all plans ### Step 2: Configure KMS Bring your own encryption keys using AWS KMS or GCP KMS. ```bash cloudflow encryption configure --kms-key arn:aws:kms:... ``` ### Step 3: Enable field-level encryption Encrypt sensitive fields within your workflow data. ```bash cloudflow encryption add-field --path $.user.ssn --key-alias sensitive-data ``` ### Step 4: Verify encryption Confirm encryption is active for your workspace. ```bash cloudflow encryption status ``` ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            },
            {
              "doc_id": "howto_set_up_database_connection",
              "chunk_id": "howto_set_up_database_connection_fix_0",
              "content": "# How to Set Up Database Connection ## Prerequisites - Database with network access to CloudFlow - Connection credentials ## Steps ### Step 1: Add connection secret Store your database connection string as a secret. ```bash cloudflow secrets set DATABASE_URL postgresql://user:pass@host:5432/db ``` ### Step 2: Configure connection pool Set pool size based on your plan limits. Free tier allows 5 connections, Pro allows 20. > **Note:** Pool exhaustion will queue requests ### Step 3: Test connection Verify the database is accessible from CloudFlow. ```bash cloudflow db test --secret DATABASE_URL ``` ### Step 4: Enable SSL For production, always require SSL connections. ```bash cloudflow db configure --require-ssl ``` ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            },
            {
              "doc_id": "howto_configure_custom_domain",
              "chunk_id": "howto_configure_custom_domain_fix_0",
              "content": "# How to Configure Custom Domain ## Prerequisites - Domain with DNS access - CloudFlow Pro plan or higher ## Steps ### Step 1: Verify domain ownership Add a TXT record to verify domain ownership. ```bash cloudflow domains verify --domain app.example.com ``` ### Step 2: Configure DNS Add a CNAME record pointing to your CloudFlow endpoint. > **Note:** CNAME should point to ingress.cloudflow.io ### Step 3: Enable SSL CloudFlow automatically provisions SSL certificates via Let's Encrypt. ```bash cloudflow domains ssl --domain app.example.com ``` ### Step 4: Set as primary Make this the primary domain for your application. ```bash cloudflow domains set-primary --domain app.example.com ``` ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            }
          ],
          "latency_ms": 4.222984000080032,
          "query_id": "cross_014",
          "dimension": "original",
          "query": "How does CloudFlow handle data storage?"
        },
        {
          "key_facts": [
            "PostgreSQL",
            "S3",
            "Iceberg",
            "Parquet"
          ],
          "found_facts": [
            "PostgreSQL",
            "S3",
            "Iceberg",
            "Parquet"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "adr_001",
            "adr_008",
            "arch_data_pipeline"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "arch_data_pipeline",
              "chunk_id": "arch_data_pipeline_fix_0",
              "content": "# Data Pipeline Architecture ## Overview The Data Pipeline processes and transforms data between workflow steps. It supports batch processing up to 10GB and streaming for real-time use cases. ## Design Principles - **Exactly-Once Processing**: Each record is processed exactly once - **Backpressure Handling**: System gracefully handles varying load ## Components ### Stream Processor Handles real-time data streams. Maximum throughput of 100000 messages per second. **Technology:** Apache Kafka with Flink **Scaling:** Partition-based scaling ### Batch Processor Processes large batch jobs. Supports data formats: JSON, CSV, Parquet, Avro. **Technology:** Apache Spark on Kubernetes **Scaling:** Dynamic resource allocation ### Data Lake Long-term data storage with time-travel queries. Retention configurable up to 7 years. **Technology:** S3 with Iceberg tables **Scaling:** Unlimited with tiered storage ## Data Flow Input \u2192 Validation \u2192 Transform \u2192 Enrich \u2192 Output ## Performance Characteristics - **Latency P50:** 100ms (streaming), 5min (batch) - **Latency P99:** 500ms (streaming), 30min (batch) - **Throughput:** 100000 messages/second (streaming)",
              "score": null
            },
            {
              "doc_id": "adr_008",
              "chunk_id": "adr_008_fix_0",
              "content": "# ADR-008: Use S3 for Object Storage **Status:** Accepted **Date:** 2024-10-14 ## Context We need storage for workflow artifacts, execution outputs, and user uploads. ## Options Considered ### Option 1: Amazon S3 Object storage service. **Pros:** - 99.999999999% durability - Lifecycle policies - Event notifications - CDN integration **Cons:** - AWS lock-in - Eventual consistency for overwrites - Cost at scale ### Option 2: Self-hosted MinIO S3-compatible object storage. **Pros:** - No vendor lock-in - S3 compatible - On-premises option **Cons:** - Operational burden - Lower durability guarantees - Scaling complexity ### Option 3: Google Cloud Storage GCP object storage. **Pros:** - Strong consistency - Good performance - Unified billing **Cons:** - GCP lock-in - Fewer features than S3 - Smaller ecosystem ## Decision We will use Amazon S3 for object storage with intelligent tiering for cost optimization. ## Consequences - Enable versioning for critical buckets - Implement lifecycle policies - Use pre-signed URLs for uploads - Enable server-side encryption",
              "score": null
            },
            {
              "doc_id": "howto_set_up_database_connection",
              "chunk_id": "howto_set_up_database_connection_fix_0",
              "content": "# How to Set Up Database Connection ## Prerequisites - Database with network access to CloudFlow - Connection credentials ## Steps ### Step 1: Add connection secret Store your database connection string as a secret. ```bash cloudflow secrets set DATABASE_URL postgresql://user:pass@host:5432/db ``` ### Step 2: Configure connection pool Set pool size based on your plan limits. Free tier allows 5 connections, Pro allows 20. > **Note:** Pool exhaustion will queue requests ### Step 3: Test connection Verify the database is accessible from CloudFlow. ```bash cloudflow db test --secret DATABASE_URL ``` ### Step 4: Enable SSL For production, always require SSL connections. ```bash cloudflow db configure --require-ssl ``` ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            },
            {
              "doc_id": "arch_workflow_engine",
              "chunk_id": "arch_workflow_engine_fix_0",
              "content": "# Workflow Engine Architecture ## Overview The Workflow Engine is the core component responsible for executing user-defined workflows. It uses an event-driven architecture with support for parallel execution, conditional branching, and automatic retries. ## Design Principles - **Idempotency**: All operations can be safely retried without side effects - **Eventual Consistency**: State converges to consistent view within 5 seconds - **Fault Tolerance**: System continues operating despite component failures ## Components ### Scheduler Manages workflow scheduling and triggers. Supports cron expressions, webhooks, and event-based triggers. **Technology:** Custom Go service **Scaling:** Horizontal with leader election ### Executor Runs individual workflow steps in isolated containers. Maximum execution time is 30 minutes per step. **Technology:** Kubernetes Jobs **Scaling:** Auto-scaling based on queue depth ### State Store Persists workflow state, execution history, and step results. Uses optimistic locking for concurrent updates. **Technology:** PostgreSQL with JSONB **Scaling:** Primary-replica with read replicas ## Data Flow Triggers \u2192 Scheduler \u2192 Job Queue \u2192 Executor \u2192 State Store \u2192 Webhooks/Notifications ## Performance Characteristics - **Latency P50:** 45ms - **Latency P99:** 250ms - **Throughput:** 5000 executions/minute",
              "score": null
            },
            {
              "doc_id": "howto_set_up_data_encryption",
              "chunk_id": "howto_set_up_data_encryption_fix_0",
              "content": "# How to Set Up Data Encryption ## Prerequisites - CloudFlow Enterprise plan ## Steps ### Step 1: Enable encryption at rest All data is encrypted at rest using AES-256 by default. > **Note:** Encryption is automatic for all plans ### Step 2: Configure KMS Bring your own encryption keys using AWS KMS or GCP KMS. ```bash cloudflow encryption configure --kms-key arn:aws:kms:... ``` ### Step 3: Enable field-level encryption Encrypt sensitive fields within your workflow data. ```bash cloudflow encryption add-field --path $.user.ssn --key-alias sensitive-data ``` ### Step 4: Verify encryption Confirm encryption is active for your workspace. ```bash cloudflow encryption status ``` ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            }
          ],
          "latency_ms": 3.607467999245273,
          "query_id": "cross_014",
          "dimension": "synonym",
          "query": "what databases and storage systems does cloudflow use"
        },
        {
          "key_facts": [
            "PostgreSQL",
            "S3",
            "Iceberg",
            "Parquet"
          ],
          "found_facts": [
            "S3",
            "Iceberg",
            "Parquet"
          ],
          "missed_facts": [
            "PostgreSQL"
          ],
          "coverage": 0.75,
          "expected_docs": [
            "adr_001",
            "adr_008",
            "arch_data_pipeline"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "arch_data_pipeline",
              "chunk_id": "arch_data_pipeline_fix_0",
              "content": "# Data Pipeline Architecture ## Overview The Data Pipeline processes and transforms data between workflow steps. It supports batch processing up to 10GB and streaming for real-time use cases. ## Design Principles - **Exactly-Once Processing**: Each record is processed exactly once - **Backpressure Handling**: System gracefully handles varying load ## Components ### Stream Processor Handles real-time data streams. Maximum throughput of 100000 messages per second. **Technology:** Apache Kafka with Flink **Scaling:** Partition-based scaling ### Batch Processor Processes large batch jobs. Supports data formats: JSON, CSV, Parquet, Avro. **Technology:** Apache Spark on Kubernetes **Scaling:** Dynamic resource allocation ### Data Lake Long-term data storage with time-travel queries. Retention configurable up to 7 years. **Technology:** S3 with Iceberg tables **Scaling:** Unlimited with tiered storage ## Data Flow Input \u2192 Validation \u2192 Transform \u2192 Enrich \u2192 Output ## Performance Characteristics - **Latency P50:** 100ms (streaming), 5min (batch) - **Latency P99:** 500ms (streaming), 30min (batch) - **Throughput:** 100000 messages/second (streaming)",
              "score": null
            },
            {
              "doc_id": "adr_008",
              "chunk_id": "adr_008_fix_0",
              "content": "# ADR-008: Use S3 for Object Storage **Status:** Accepted **Date:** 2024-10-14 ## Context We need storage for workflow artifacts, execution outputs, and user uploads. ## Options Considered ### Option 1: Amazon S3 Object storage service. **Pros:** - 99.999999999% durability - Lifecycle policies - Event notifications - CDN integration **Cons:** - AWS lock-in - Eventual consistency for overwrites - Cost at scale ### Option 2: Self-hosted MinIO S3-compatible object storage. **Pros:** - No vendor lock-in - S3 compatible - On-premises option **Cons:** - Operational burden - Lower durability guarantees - Scaling complexity ### Option 3: Google Cloud Storage GCP object storage. **Pros:** - Strong consistency - Good performance - Unified billing **Cons:** - GCP lock-in - Fewer features than S3 - Smaller ecosystem ## Decision We will use Amazon S3 for object storage with intelligent tiering for cost optimization. ## Consequences - Enable versioning for critical buckets - Implement lifecycle policies - Use pre-signed URLs for uploads - Enable server-side encryption",
              "score": null
            },
            {
              "doc_id": "adr_004",
              "chunk_id": "adr_004_fix_0",
              "content": "# ADR-004: Use Redis for Caching and Session Storage **Status:** Accepted **Date:** 2024-04-05 ## Context We need a fast data store for caching API responses, session data, and rate limiting counters. ## Options Considered ### Option 1: Redis Cluster In-memory data structure store. **Pros:** - Sub-millisecond latency - Rich data structures - Pub/sub support - Clustering **Cons:** - Memory cost - Persistence complexity - Cluster management ### Option 2: Memcached Simple key-value cache. **Pros:** - Simple operation - Multi-threaded - Lower memory overhead **Cons:** - No persistence - Limited data types - No clustering ### Option 3: Application-level cache In-process caching only. **Pros:** - No external dependency - Fastest possible - Simple **Cons:** - Not shared across pods - Memory pressure - Cold starts ## Decision We will use Redis Cluster for caching with a 6-node cluster for high availability. ## Consequences - Implement cache-aside pattern - Set appropriate TTLs - Monitor memory usage - Plan for cache stampede prevention",
              "score": null
            },
            {
              "doc_id": "arch_monitoring_stack",
              "chunk_id": "arch_monitoring_stack_fix_0",
              "content": "# Monitoring Stack Architecture ## Overview The Monitoring Stack provides observability into system health, performance, and errors. Includes metrics, logs, traces, and alerting. ## Design Principles - **Observability**: Every component emits metrics and traces - **Actionable Alerts**: Alerts include runbook links ## Components ### Metrics Collects time-series metrics. Retention is 15 days local, 1 year in object storage. **Technology:** Prometheus with Thanos **Scaling:** Federated with long-term storage ### Logging Centralized log aggregation. Supports structured JSON logs with label indexing. **Technology:** Loki with Grafana **Scaling:** Distributed with sharding ### Tracing Distributed request tracing. Trace retention is 7 days. **Technology:** Jaeger with OpenTelemetry **Scaling:** Sampling at 10% ### Alerting Routes alerts to on-call engineers. Supports escalation policies and silencing. **Technology:** Alertmanager with PagerDuty **Scaling:** HA pair ## Data Flow Application \u2192 Collector \u2192 Storage \u2192 Query \u2192 Dashboard/Alert ## Performance Characteristics - **Latency P50:** N/A - **Latency P99:** N/A - **Throughput:** 1M metrics/second, 100GB logs/day",
              "score": null
            },
            {
              "doc_id": "arch_caching_layer",
              "chunk_id": "arch_caching_layer_fix_0",
              "content": "# Caching Layer Architecture ## Overview The Caching Layer reduces latency and database load by caching frequently accessed data. Uses a multi-tier strategy with local and distributed caches. ## Design Principles - **Cache Invalidation**: Explicit invalidation on writes - **Graceful Degradation**: System works without cache ## Components ### Local Cache First-level cache with 100ms TTL. Reduces Redis round-trips for hot data. **Technology:** Ristretto in-memory cache **Scaling:** Per-pod memory allocation ### Distributed Cache Second-level cache with configurable TTL. Maximum key size 512MB. **Technology:** Redis Cluster 7.x **Scaling:** 6-node cluster with sharding ### CDN Caches static assets and API responses. Cache hit ratio target 95%. **Technology:** CloudFront with Lambda@Edge **Scaling:** Global edge locations ## Data Flow Request \u2192 Local Cache \u2192 Redis \u2192 Database \u2192 Cache Population ## Performance Characteristics - **Latency P50:** 1ms (hit), 50ms (miss) - **Latency P99:** 5ms (hit), 200ms (miss) - **Throughput:** 100000 reads/second",
              "score": null
            }
          ],
          "latency_ms": 3.4569979998195777,
          "query_id": "cross_014",
          "dimension": "problem",
          "query": "running out of storage space and need to understand where all our data actually lives"
        },
        {
          "key_facts": [
            "PostgreSQL",
            "S3",
            "Iceberg",
            "Parquet"
          ],
          "found_facts": [
            "PostgreSQL",
            "S3",
            "Iceberg",
            "Parquet"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "adr_001",
            "adr_008",
            "arch_data_pipeline"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "adr_004",
              "chunk_id": "adr_004_fix_0",
              "content": "# ADR-004: Use Redis for Caching and Session Storage **Status:** Accepted **Date:** 2024-04-05 ## Context We need a fast data store for caching API responses, session data, and rate limiting counters. ## Options Considered ### Option 1: Redis Cluster In-memory data structure store. **Pros:** - Sub-millisecond latency - Rich data structures - Pub/sub support - Clustering **Cons:** - Memory cost - Persistence complexity - Cluster management ### Option 2: Memcached Simple key-value cache. **Pros:** - Simple operation - Multi-threaded - Lower memory overhead **Cons:** - No persistence - Limited data types - No clustering ### Option 3: Application-level cache In-process caching only. **Pros:** - No external dependency - Fastest possible - Simple **Cons:** - Not shared across pods - Memory pressure - Cold starts ## Decision We will use Redis Cluster for caching with a 6-node cluster for high availability. ## Consequences - Implement cache-aside pattern - Set appropriate TTLs - Monitor memory usage - Plan for cache stampede prevention",
              "score": null
            },
            {
              "doc_id": "adr_008",
              "chunk_id": "adr_008_fix_0",
              "content": "# ADR-008: Use S3 for Object Storage **Status:** Accepted **Date:** 2024-10-14 ## Context We need storage for workflow artifacts, execution outputs, and user uploads. ## Options Considered ### Option 1: Amazon S3 Object storage service. **Pros:** - 99.999999999% durability - Lifecycle policies - Event notifications - CDN integration **Cons:** - AWS lock-in - Eventual consistency for overwrites - Cost at scale ### Option 2: Self-hosted MinIO S3-compatible object storage. **Pros:** - No vendor lock-in - S3 compatible - On-premises option **Cons:** - Operational burden - Lower durability guarantees - Scaling complexity ### Option 3: Google Cloud Storage GCP object storage. **Pros:** - Strong consistency - Good performance - Unified billing **Cons:** - GCP lock-in - Fewer features than S3 - Smaller ecosystem ## Decision We will use Amazon S3 for object storage with intelligent tiering for cost optimization. ## Consequences - Enable versioning for critical buckets - Implement lifecycle policies - Use pre-signed URLs for uploads - Enable server-side encryption",
              "score": null
            },
            {
              "doc_id": "arch_data_pipeline",
              "chunk_id": "arch_data_pipeline_fix_0",
              "content": "# Data Pipeline Architecture ## Overview The Data Pipeline processes and transforms data between workflow steps. It supports batch processing up to 10GB and streaming for real-time use cases. ## Design Principles - **Exactly-Once Processing**: Each record is processed exactly once - **Backpressure Handling**: System gracefully handles varying load ## Components ### Stream Processor Handles real-time data streams. Maximum throughput of 100000 messages per second. **Technology:** Apache Kafka with Flink **Scaling:** Partition-based scaling ### Batch Processor Processes large batch jobs. Supports data formats: JSON, CSV, Parquet, Avro. **Technology:** Apache Spark on Kubernetes **Scaling:** Dynamic resource allocation ### Data Lake Long-term data storage with time-travel queries. Retention configurable up to 7 years. **Technology:** S3 with Iceberg tables **Scaling:** Unlimited with tiered storage ## Data Flow Input \u2192 Validation \u2192 Transform \u2192 Enrich \u2192 Output ## Performance Characteristics - **Latency P50:** 100ms (streaming), 5min (batch) - **Latency P99:** 500ms (streaming), 30min (batch) - **Throughput:** 100000 messages/second (streaming)",
              "score": null
            },
            {
              "doc_id": "arch_caching_layer",
              "chunk_id": "arch_caching_layer_fix_0",
              "content": "# Caching Layer Architecture ## Overview The Caching Layer reduces latency and database load by caching frequently accessed data. Uses a multi-tier strategy with local and distributed caches. ## Design Principles - **Cache Invalidation**: Explicit invalidation on writes - **Graceful Degradation**: System works without cache ## Components ### Local Cache First-level cache with 100ms TTL. Reduces Redis round-trips for hot data. **Technology:** Ristretto in-memory cache **Scaling:** Per-pod memory allocation ### Distributed Cache Second-level cache with configurable TTL. Maximum key size 512MB. **Technology:** Redis Cluster 7.x **Scaling:** 6-node cluster with sharding ### CDN Caches static assets and API responses. Cache hit ratio target 95%. **Technology:** CloudFront with Lambda@Edge **Scaling:** Global edge locations ## Data Flow Request \u2192 Local Cache \u2192 Redis \u2192 Database \u2192 Cache Population ## Performance Characteristics - **Latency P50:** 1ms (hit), 50ms (miss) - **Latency P99:** 5ms (hit), 200ms (miss) - **Throughput:** 100000 reads/second",
              "score": null
            },
            {
              "doc_id": "adr_001",
              "chunk_id": "adr_001_fix_0",
              "content": "# ADR-001: Use PostgreSQL for Primary Database **Status:** Accepted **Date:** 2024-11-04 ## Context We need a database to store workflow definitions, execution state, and user data. The database must support ACID transactions, JSON queries, and scale to millions of records. ## Options Considered ### Option 1: PostgreSQL Mature relational database with JSONB support. **Pros:** - ACID compliance - Rich JSONB queries - Team expertise - Proven at scale **Cons:** - Manual sharding for extreme scale - Requires connection pooling ### Option 2: MongoDB Document database with native JSON. **Pros:** - Flexible schema - Built-in sharding - Good for documents **Cons:** - Weaker consistency guarantees - Complex transactions - Higher operational cost ### Option 3: CockroachDB Distributed SQL database. **Pros:** - Automatic sharding - Strong consistency - PostgreSQL compatible **Cons:** - Higher latency - More expensive - Less mature ecosystem ## Decision We will use PostgreSQL as our primary database with JSONB for flexible workflow definitions. ## Consequences - Need PgBouncer for connection pooling - Must plan for sharding at 100M+ records - Requires regular VACUUM maintenance",
              "score": null
            }
          ],
          "latency_ms": 3.5978699997940566,
          "query_id": "cross_014",
          "dimension": "casual",
          "query": "storage architecture"
        },
        {
          "key_facts": [
            "PostgreSQL",
            "S3",
            "Iceberg",
            "Parquet"
          ],
          "found_facts": [
            "PostgreSQL",
            "S3",
            "Iceberg",
            "Parquet"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "adr_001",
            "adr_008",
            "arch_data_pipeline"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "arch_data_pipeline",
              "chunk_id": "arch_data_pipeline_fix_0",
              "content": "# Data Pipeline Architecture ## Overview The Data Pipeline processes and transforms data between workflow steps. It supports batch processing up to 10GB and streaming for real-time use cases. ## Design Principles - **Exactly-Once Processing**: Each record is processed exactly once - **Backpressure Handling**: System gracefully handles varying load ## Components ### Stream Processor Handles real-time data streams. Maximum throughput of 100000 messages per second. **Technology:** Apache Kafka with Flink **Scaling:** Partition-based scaling ### Batch Processor Processes large batch jobs. Supports data formats: JSON, CSV, Parquet, Avro. **Technology:** Apache Spark on Kubernetes **Scaling:** Dynamic resource allocation ### Data Lake Long-term data storage with time-travel queries. Retention configurable up to 7 years. **Technology:** S3 with Iceberg tables **Scaling:** Unlimited with tiered storage ## Data Flow Input \u2192 Validation \u2192 Transform \u2192 Enrich \u2192 Output ## Performance Characteristics - **Latency P50:** 100ms (streaming), 5min (batch) - **Latency P99:** 500ms (streaming), 30min (batch) - **Throughput:** 100000 messages/second (streaming)",
              "score": null
            },
            {
              "doc_id": "adr_001",
              "chunk_id": "adr_001_fix_0",
              "content": "# ADR-001: Use PostgreSQL for Primary Database **Status:** Accepted **Date:** 2024-11-04 ## Context We need a database to store workflow definitions, execution state, and user data. The database must support ACID transactions, JSON queries, and scale to millions of records. ## Options Considered ### Option 1: PostgreSQL Mature relational database with JSONB support. **Pros:** - ACID compliance - Rich JSONB queries - Team expertise - Proven at scale **Cons:** - Manual sharding for extreme scale - Requires connection pooling ### Option 2: MongoDB Document database with native JSON. **Pros:** - Flexible schema - Built-in sharding - Good for documents **Cons:** - Weaker consistency guarantees - Complex transactions - Higher operational cost ### Option 3: CockroachDB Distributed SQL database. **Pros:** - Automatic sharding - Strong consistency - PostgreSQL compatible **Cons:** - Higher latency - More expensive - Less mature ecosystem ## Decision We will use PostgreSQL as our primary database with JSONB for flexible workflow definitions. ## Consequences - Need PgBouncer for connection pooling - Must plan for sharding at 100M+ records - Requires regular VACUUM maintenance",
              "score": null
            },
            {
              "doc_id": "adr_008",
              "chunk_id": "adr_008_fix_0",
              "content": "# ADR-008: Use S3 for Object Storage **Status:** Accepted **Date:** 2024-10-14 ## Context We need storage for workflow artifacts, execution outputs, and user uploads. ## Options Considered ### Option 1: Amazon S3 Object storage service. **Pros:** - 99.999999999% durability - Lifecycle policies - Event notifications - CDN integration **Cons:** - AWS lock-in - Eventual consistency for overwrites - Cost at scale ### Option 2: Self-hosted MinIO S3-compatible object storage. **Pros:** - No vendor lock-in - S3 compatible - On-premises option **Cons:** - Operational burden - Lower durability guarantees - Scaling complexity ### Option 3: Google Cloud Storage GCP object storage. **Pros:** - Strong consistency - Good performance - Unified billing **Cons:** - GCP lock-in - Fewer features than S3 - Smaller ecosystem ## Decision We will use Amazon S3 for object storage with intelligent tiering for cost optimization. ## Consequences - Enable versioning for critical buckets - Implement lifecycle policies - Use pre-signed URLs for uploads - Enable server-side encryption",
              "score": null
            },
            {
              "doc_id": "adr_004",
              "chunk_id": "adr_004_fix_0",
              "content": "# ADR-004: Use Redis for Caching and Session Storage **Status:** Accepted **Date:** 2024-04-05 ## Context We need a fast data store for caching API responses, session data, and rate limiting counters. ## Options Considered ### Option 1: Redis Cluster In-memory data structure store. **Pros:** - Sub-millisecond latency - Rich data structures - Pub/sub support - Clustering **Cons:** - Memory cost - Persistence complexity - Cluster management ### Option 2: Memcached Simple key-value cache. **Pros:** - Simple operation - Multi-threaded - Lower memory overhead **Cons:** - No persistence - Limited data types - No clustering ### Option 3: Application-level cache In-process caching only. **Pros:** - No external dependency - Fastest possible - Simple **Cons:** - Not shared across pods - Memory pressure - Cold starts ## Decision We will use Redis Cluster for caching with a 6-node cluster for high availability. ## Consequences - Implement cache-aside pattern - Set appropriate TTLs - Monitor memory usage - Plan for cache stampede prevention",
              "score": null
            },
            {
              "doc_id": "arch_monitoring_stack",
              "chunk_id": "arch_monitoring_stack_fix_0",
              "content": "# Monitoring Stack Architecture ## Overview The Monitoring Stack provides observability into system health, performance, and errors. Includes metrics, logs, traces, and alerting. ## Design Principles - **Observability**: Every component emits metrics and traces - **Actionable Alerts**: Alerts include runbook links ## Components ### Metrics Collects time-series metrics. Retention is 15 days local, 1 year in object storage. **Technology:** Prometheus with Thanos **Scaling:** Federated with long-term storage ### Logging Centralized log aggregation. Supports structured JSON logs with label indexing. **Technology:** Loki with Grafana **Scaling:** Distributed with sharding ### Tracing Distributed request tracing. Trace retention is 7 days. **Technology:** Jaeger with OpenTelemetry **Scaling:** Sampling at 10% ### Alerting Routes alerts to on-call engineers. Supports escalation policies and silencing. **Technology:** Alertmanager with PagerDuty **Scaling:** HA pair ## Data Flow Application \u2192 Collector \u2192 Storage \u2192 Query \u2192 Dashboard/Alert ## Performance Characteristics - **Latency P50:** N/A - **Latency P99:** N/A - **Throughput:** 1M metrics/second, 100GB logs/day",
              "score": null
            }
          ],
          "latency_ms": 3.8901249990885844,
          "query_id": "cross_014",
          "dimension": "contextual",
          "query": "planning data retention policies for compliance, need to map out all the storage tiers from transactional DB to data lake"
        },
        {
          "key_facts": [
            "PostgreSQL",
            "S3",
            "Iceberg",
            "Parquet"
          ],
          "found_facts": [
            "PostgreSQL",
            "S3",
            "Iceberg",
            "Parquet"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "adr_001",
            "adr_008",
            "arch_data_pipeline"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "adr_008",
              "chunk_id": "adr_008_fix_0",
              "content": "# ADR-008: Use S3 for Object Storage **Status:** Accepted **Date:** 2024-10-14 ## Context We need storage for workflow artifacts, execution outputs, and user uploads. ## Options Considered ### Option 1: Amazon S3 Object storage service. **Pros:** - 99.999999999% durability - Lifecycle policies - Event notifications - CDN integration **Cons:** - AWS lock-in - Eventual consistency for overwrites - Cost at scale ### Option 2: Self-hosted MinIO S3-compatible object storage. **Pros:** - No vendor lock-in - S3 compatible - On-premises option **Cons:** - Operational burden - Lower durability guarantees - Scaling complexity ### Option 3: Google Cloud Storage GCP object storage. **Pros:** - Strong consistency - Good performance - Unified billing **Cons:** - GCP lock-in - Fewer features than S3 - Smaller ecosystem ## Decision We will use Amazon S3 for object storage with intelligent tiering for cost optimization. ## Consequences - Enable versioning for critical buckets - Implement lifecycle policies - Use pre-signed URLs for uploads - Enable server-side encryption",
              "score": null
            },
            {
              "doc_id": "adr_004",
              "chunk_id": "adr_004_fix_0",
              "content": "# ADR-004: Use Redis for Caching and Session Storage **Status:** Accepted **Date:** 2024-04-05 ## Context We need a fast data store for caching API responses, session data, and rate limiting counters. ## Options Considered ### Option 1: Redis Cluster In-memory data structure store. **Pros:** - Sub-millisecond latency - Rich data structures - Pub/sub support - Clustering **Cons:** - Memory cost - Persistence complexity - Cluster management ### Option 2: Memcached Simple key-value cache. **Pros:** - Simple operation - Multi-threaded - Lower memory overhead **Cons:** - No persistence - Limited data types - No clustering ### Option 3: Application-level cache In-process caching only. **Pros:** - No external dependency - Fastest possible - Simple **Cons:** - Not shared across pods - Memory pressure - Cold starts ## Decision We will use Redis Cluster for caching with a 6-node cluster for high availability. ## Consequences - Implement cache-aside pattern - Set appropriate TTLs - Monitor memory usage - Plan for cache stampede prevention",
              "score": null
            },
            {
              "doc_id": "arch_data_pipeline",
              "chunk_id": "arch_data_pipeline_fix_0",
              "content": "# Data Pipeline Architecture ## Overview The Data Pipeline processes and transforms data between workflow steps. It supports batch processing up to 10GB and streaming for real-time use cases. ## Design Principles - **Exactly-Once Processing**: Each record is processed exactly once - **Backpressure Handling**: System gracefully handles varying load ## Components ### Stream Processor Handles real-time data streams. Maximum throughput of 100000 messages per second. **Technology:** Apache Kafka with Flink **Scaling:** Partition-based scaling ### Batch Processor Processes large batch jobs. Supports data formats: JSON, CSV, Parquet, Avro. **Technology:** Apache Spark on Kubernetes **Scaling:** Dynamic resource allocation ### Data Lake Long-term data storage with time-travel queries. Retention configurable up to 7 years. **Technology:** S3 with Iceberg tables **Scaling:** Unlimited with tiered storage ## Data Flow Input \u2192 Validation \u2192 Transform \u2192 Enrich \u2192 Output ## Performance Characteristics - **Latency P50:** 100ms (streaming), 5min (batch) - **Latency P99:** 500ms (streaming), 30min (batch) - **Throughput:** 100000 messages/second (streaming)",
              "score": null
            },
            {
              "doc_id": "adr_006",
              "chunk_id": "adr_006_fix_0",
              "content": "# ADR-006: Use Terraform for Infrastructure as Code **Status:** Accepted **Date:** 2024-11-24 ## Context We need a way to manage cloud infrastructure consistently across environments. ## Options Considered ### Option 1: Terraform Multi-cloud IaC tool. **Pros:** - Multi-cloud support - Large community - State management - Plan before apply **Cons:** - State file management - Learning curve - HCL limitations ### Option 2: AWS CloudFormation AWS-native IaC. **Pros:** - Native AWS support - No state file - Drift detection **Cons:** - AWS only - Verbose YAML/JSON - Slower development ### Option 3: Pulumi IaC with real programming languages. **Pros:** - Real languages - Type safety - Reusable components **Cons:** - Smaller community - Vendor lock-in - Commercial features ## Decision We will use Terraform with S3 backend for state and DynamoDB for locking. ## Consequences - Implement module structure - Use workspaces for environments - Automate with Atlantis",
              "score": null
            },
            {
              "doc_id": "adr_001",
              "chunk_id": "adr_001_fix_0",
              "content": "# ADR-001: Use PostgreSQL for Primary Database **Status:** Accepted **Date:** 2024-11-04 ## Context We need a database to store workflow definitions, execution state, and user data. The database must support ACID transactions, JSON queries, and scale to millions of records. ## Options Considered ### Option 1: PostgreSQL Mature relational database with JSONB support. **Pros:** - ACID compliance - Rich JSONB queries - Team expertise - Proven at scale **Cons:** - Manual sharding for extreme scale - Requires connection pooling ### Option 2: MongoDB Document database with native JSON. **Pros:** - Flexible schema - Built-in sharding - Good for documents **Cons:** - Weaker consistency guarantees - Complex transactions - Higher operational cost ### Option 3: CockroachDB Distributed SQL database. **Pros:** - Automatic sharding - Strong consistency - PostgreSQL compatible **Cons:** - Higher latency - More expensive - Less mature ecosystem ## Decision We will use PostgreSQL as our primary database with JSONB for flexible workflow definitions. ## Consequences - Need PgBouncer for connection pooling - Must plan for sharding at 100M+ records - Requires regular VACUUM maintenance",
              "score": null
            }
          ],
          "latency_ms": 4.354969998530578,
          "query_id": "cross_014",
          "dimension": "negation",
          "query": "why do we need multiple storage solutions instead of consolidating into one"
        },
        {
          "key_facts": [
            "Bearer",
            "API key",
            "OAuth 2.0",
            "Authorization"
          ],
          "found_facts": [
            "Bearer",
            "API key",
            "OAuth 2.0",
            "Authorization"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "api_users",
            "api_integrations",
            "adr_003"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "arch_authentication_service",
              "chunk_id": "arch_authentication_service_fix_0",
              "content": "# Authentication Service Architecture ## Overview The Authentication Service handles user identity, session management, and token issuance. Supports OAuth 2.0, SAML, and API key authentication. ## Design Principles - **Zero Knowledge**: Passwords never stored in plaintext - **Token Rotation**: Refresh tokens rotate on each use ## Components ### Identity Provider Manages user accounts and credentials. Password hashing uses bcrypt with cost factor 12. **Technology:** Custom Go service **Scaling:** Horizontal stateless ### Token Service Issues and validates access tokens. Token lifetime is 1 hour with 7-day refresh tokens. **Technology:** JWT with RS256 **Scaling:** Horizontal stateless ### SSO Gateway Integrates with external identity providers. Supports Google, Microsoft, Okta, and custom SAML. **Technology:** Dex OIDC proxy **Scaling:** Active-passive ## Data Flow Credentials \u2192 Validation \u2192 Token Issue \u2192 Token Refresh \u2192 Logout ## Performance Characteristics - **Latency P50:** 50ms - **Latency P99:** 200ms - **Throughput:** 5000 auth/second",
              "score": null
            },
            {
              "doc_id": "adr_003",
              "chunk_id": "adr_003_fix_0",
              "content": "# ADR-003: Implement OAuth 2.0 with JWT for Authentication **Status:** Accepted **Date:** 2024-05-08 ## Context We need a secure authentication system that supports API keys, user sessions, and third-party integrations. ## Options Considered ### Option 1: OAuth 2.0 with JWT Industry standard token-based auth. **Pros:** - Industry standard - Stateless validation - Supports refresh tokens - SSO compatible **Cons:** - Token size overhead - Cannot revoke individual tokens - Complexity ### Option 2: Session-based auth Traditional server-side sessions. **Pros:** - Simple implementation - Easy revocation - Smaller payloads **Cons:** - Requires session store - Not suitable for APIs - Scaling challenges ### Option 3: API keys only Simple key-based authentication. **Pros:** - Very simple - Low overhead - Easy to implement **Cons:** - No user context - Hard to rotate - No SSO support ## Decision We will implement OAuth 2.0 with JWT tokens. Access tokens expire after 1 hour, refresh tokens after 7 days. ## Consequences - Implement token rotation on refresh - Support API keys for machine-to-machine auth - Add MFA for admin accounts",
              "score": null
            },
            {
              "doc_id": "arch_api_gateway",
              "chunk_id": "arch_api_gateway_fix_0",
              "content": "# API Gateway Architecture ## Overview The API Gateway handles all incoming HTTP requests, providing authentication, rate limiting, and request routing. Built on Kong with custom plugins for CloudFlow-specific functionality. ## Design Principles - **Zero Trust**: Every request is authenticated and authorized - **Defense in Depth**: Multiple layers of security validation ## Components ### Kong Gateway Primary ingress controller. Handles TLS termination, request routing, and plugin execution. **Technology:** Kong 3.4 on Kubernetes **Scaling:** Horizontal with HPA ### Auth Plugin Validates JWT tokens, API keys, and OAuth2 flows. Token validation latency under 5ms. **Technology:** Custom Lua plugin **Scaling:** Stateless ### Rate Limiter Enforces per-user and per-IP rate limits. Default limit is 100 requests per minute. **Technology:** Redis-backed sliding window **Scaling:** Shared Redis cluster ## Data Flow Client \u2192 TLS \u2192 Kong \u2192 Auth \u2192 Rate Limit \u2192 Backend Services ## Performance Characteristics - **Latency P50:** 12ms - **Latency P99:** 45ms - **Throughput:** 50000 requests/second",
              "score": null
            },
            {
              "doc_id": "config_api_gateway",
              "chunk_id": "config_api_gateway_fix_0",
              "content": "# API Gateway Configuration Reference ## Overview This document describes all configuration options for API Gateway. ## Environment Variables | Variable | Type | Default | Description | |----------|------|---------|-------------| | `GATEWAY_PORT` | integer | 8000 | Gateway listen port | | `GATEWAY_RATE_LIMIT` | integer | 100 | Requests per minute per user | | `GATEWAY_TIMEOUT_MS` | integer | 30000 | Request timeout in milliseconds | | `GATEWAY_MAX_BODY_SIZE` | string | 10MB | Maximum request body size | | `GATEWAY_CORS_ORIGINS` | string | * | Allowed CORS origins | | `GATEWAY_SSL_ENABLED` | boolean | true | Enable TLS termination | ## Configuration File Configuration can also be provided via `api gateway.yaml`: ```yaml gateway_port: 8000 gateway_rate_limit: 100 gateway_timeout_ms: 30000 gateway_max_body_size: 10MB gateway_cors_origins: * ``` ## Validation Configuration is validated at startup. Invalid configuration will prevent the service from starting.",
              "score": null
            },
            {
              "doc_id": "api_workflows",
              "chunk_id": "api_workflows_fix_0",
              "content": "# Workflows API Reference ## Overview The Workflows API provides programmatic access to workflows functionality. Base URL: `https://api.cloudflow.io/v1/workflows` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /workflows List all workflows for the authenticated user. **Parameters:** - `limit` (integer): Maximum number of results (default: 50, max: 200) - `status` (string): Filter by status: active, paused, archived **Response:** - Success: `200` - Returns array of workflow objects - Error: `400` - Bad Request **Example:** ```json {\"workflows\": [{\"id\": \"wf_123\", \"name\": \"Daily Report\", \"status\": \"active\"}]} ``` ### POST /workflows Create a new workflow. **Parameters:** - `name` (string): Workflow name (required) - `definition` (object): Workflow definition in JSON format **Response:** - Success: `201` - Workflow created successfully - Error: `400` - Bad Request **Example:** ```json {\"id\": \"wf_456\", \"name\": \"New Workflow\", \"created_at\": \"2024-01-15T10:00:00Z\"} ``` ### DELETE /workflows/{id} Delete a workflow permanently. **Parameters:** - `id` (string): Workflow ID (required) **Response:** - Success: `204` - Workflow deleted - Error: `404` - Workflow not found **Example:** ```json {} ```",
              "score": null
            }
          ],
          "latency_ms": 3.4171039987995755,
          "query_id": "cross_015",
          "dimension": "original",
          "query": "What are all the API authentication methods?"
        },
        {
          "key_facts": [
            "Bearer",
            "API key",
            "OAuth 2.0",
            "Authorization"
          ],
          "found_facts": [
            "Bearer",
            "API key",
            "OAuth 2.0",
            "Authorization"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "api_users",
            "api_integrations",
            "adr_003"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "howto_create_api_integration",
              "chunk_id": "howto_create_api_integration_fix_0",
              "content": "# How to Create API Integration ## Prerequisites - CloudFlow account ## Steps ### Step 1: Generate API key Create an API key for programmatic access. ```bash cloudflow apikeys create --name 'CI Integration' --scopes workflows:read,workflows:execute ``` ### Step 2: Store key securely Never commit API keys to source control. Use environment variables or secret management. > **Note:** Rotate keys every 90 days ### Step 3: Test authentication Verify the API key works correctly. ```bash curl -H 'Authorization: Bearer cf_...' https://api.cloudflow.io/v1/me ``` ### Step 4: Implement rate limiting Handle rate limit responses (HTTP 429) with exponential backoff. > **Note:** Default limit is 100 requests per minute ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            },
            {
              "doc_id": "api_secrets",
              "chunk_id": "api_secrets_fix_0",
              "content": "# Secrets API Reference ## Overview The Secrets API provides programmatic access to secrets functionality. Base URL: `https://api.cloudflow.io/v1/secrets` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /secrets List secret names (values not returned). **Parameters:** **Response:** - Success: `200` - Returns array of secret metadata - Error: `400` - Bad Request **Example:** ```json {\"secrets\": [{\"name\": \"DATABASE_URL\", \"created_at\": \"2024-01-01\", \"updated_at\": \"2024-01-10\"}]} ``` ### PUT /secrets/{name} Create or update a secret. **Parameters:** - `value` (string): Secret value (encrypted at rest) **Response:** - Success: `200` - Secret saved - Error: `400` - Bad Request **Example:** ```json {\"name\": \"DATABASE_URL\", \"updated_at\": \"2024-01-15T10:00:00Z\"} ``` ### DELETE /secrets/{name} Delete a secret. **Parameters:** **Response:** - Success: `204` - Secret deleted - Error: `400` - Bad Request **Example:** ```json {} ```",
              "score": null
            },
            {
              "doc_id": "api_integrations",
              "chunk_id": "api_integrations_fix_0",
              "content": "# Integrations API Reference ## Overview The Integrations API provides programmatic access to integrations functionality. Base URL: `https://api.cloudflow.io/v1/integrations` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /integrations List available integrations. **Parameters:** - `category` (string): Filter by category: storage, notification, database **Response:** - Success: `200` - Returns array of integration objects - Error: `400` - Bad Request **Example:** ```json {\"integrations\": [{\"id\": \"int_s3\", \"name\": \"Amazon S3\", \"category\": \"storage\", \"status\": \"connected\"}]} ``` ### POST /integrations/{id}/connect Connect an integration. **Parameters:** - `credentials` (object): Integration-specific credentials **Response:** - Success: `200` - Integration connected - Error: `400` - Bad Request **Example:** ```json {\"id\": \"int_s3\", \"status\": \"connected\", \"connected_at\": \"2024-01-15T10:00:00Z\"} ```",
              "score": null
            },
            {
              "doc_id": "howto_migrate_from_v1_to_v2_api",
              "chunk_id": "howto_migrate_from_v1_to_v2_api_fix_0",
              "content": "# How to Migrate from v1 to v2 API ## Prerequisites - Existing v1 integration - v2 API access enabled ## Steps ### Step 1: Review breaking changes The v2 API changes authentication from API keys to OAuth 2.0. Review the migration guide. ### Step 2: Update client library Upgrade to the latest SDK version. ```bash npm install @cloudflow/sdk@latest ``` ### Step 3: Update authentication Replace API key authentication with OAuth 2.0 client credentials flow. > **Note:** Generate new credentials in the dashboard ### Step 4: Test in staging Run your integration tests against the v2 staging endpoint. ```bash CLOUDFLOW_API_URL=https://api-v2-staging.cloudflow.io npm test ``` ### Step 5: Switch production Update production configuration to use v2 API. ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            },
            {
              "doc_id": "api_users",
              "chunk_id": "api_users_fix_0",
              "content": "# Users API Reference ## Overview The Users API provides programmatic access to users functionality. Base URL: `https://api.cloudflow.io/v1/users` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /users/me Get current user profile. **Parameters:** **Response:** - Success: `200` - Returns user object - Error: `400` - Bad Request **Example:** ```json {\"id\": \"usr_001\", \"email\": \"user@example.com\", \"role\": \"admin\", \"created_at\": \"2023-06-01\"} ``` ### PATCH /users/me Update current user profile. **Parameters:** - `name` (string): Display name - `timezone` (string): Timezone (e.g., America/New_York) **Response:** - Success: `200` - User updated - Error: `400` - Bad Request **Example:** ```json {\"id\": \"usr_001\", \"name\": \"John Doe\", \"timezone\": \"America/New_York\"} ```",
              "score": null
            }
          ],
          "latency_ms": 3.713645999596338,
          "query_id": "cross_015",
          "dimension": "synonym",
          "query": "what credentials and tokens can I use to call cloudflow APIs"
        },
        {
          "key_facts": [
            "Bearer",
            "API key",
            "OAuth 2.0",
            "Authorization"
          ],
          "found_facts": [
            "Bearer",
            "API key",
            "Authorization"
          ],
          "missed_facts": [
            "OAuth 2.0"
          ],
          "coverage": 0.75,
          "expected_docs": [
            "api_users",
            "api_integrations",
            "adr_003"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "api_secrets",
              "chunk_id": "api_secrets_fix_0",
              "content": "# Secrets API Reference ## Overview The Secrets API provides programmatic access to secrets functionality. Base URL: `https://api.cloudflow.io/v1/secrets` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /secrets List secret names (values not returned). **Parameters:** **Response:** - Success: `200` - Returns array of secret metadata - Error: `400` - Bad Request **Example:** ```json {\"secrets\": [{\"name\": \"DATABASE_URL\", \"created_at\": \"2024-01-01\", \"updated_at\": \"2024-01-10\"}]} ``` ### PUT /secrets/{name} Create or update a secret. **Parameters:** - `value` (string): Secret value (encrypted at rest) **Response:** - Success: `200` - Secret saved - Error: `400` - Bad Request **Example:** ```json {\"name\": \"DATABASE_URL\", \"updated_at\": \"2024-01-15T10:00:00Z\"} ``` ### DELETE /secrets/{name} Delete a secret. **Parameters:** **Response:** - Success: `204` - Secret deleted - Error: `400` - Bad Request **Example:** ```json {} ```",
              "score": null
            },
            {
              "doc_id": "api_webhooks",
              "chunk_id": "api_webhooks_fix_0",
              "content": "# Webhooks API Reference ## Overview The Webhooks API provides programmatic access to webhooks functionality. Base URL: `https://api.cloudflow.io/v1/webhooks` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /webhooks List configured webhooks. **Parameters:** - `active` (boolean): Filter by active status **Response:** - Success: `200` - Returns array of webhook objects - Error: `400` - Bad Request **Example:** ```json {\"webhooks\": [{\"id\": \"wh_01\", \"url\": \"https://example.com/hook\", \"events\": [\"workflow.completed\"]}]} ``` ### POST /webhooks Create a new webhook endpoint. **Parameters:** - `url` (string): Webhook URL (must be HTTPS) - `events` (array): Events to subscribe to - `secret` (string): Signing secret for verification **Response:** - Success: `201` - Webhook created - Error: `400` - Bad Request **Example:** ```json {\"id\": \"wh_02\", \"url\": \"https://example.com/new-hook\", \"secret\": \"whsec_...\"} ```",
              "score": null
            },
            {
              "doc_id": "api_teams",
              "chunk_id": "api_teams_fix_0",
              "content": "# Teams API Reference ## Overview The Teams API provides programmatic access to teams functionality. Base URL: `https://api.cloudflow.io/v1/teams` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /teams List teams the user belongs to. **Parameters:** **Response:** - Success: `200` - Returns array of team objects - Error: `400` - Bad Request **Example:** ```json {\"teams\": [{\"id\": \"team_01\", \"name\": \"Engineering\", \"member_count\": 15}]} ``` ### POST /teams/{id}/members Add a member to a team. **Parameters:** - `user_id` (string): User ID to add - `role` (string): Role: owner, admin, member, viewer **Response:** - Success: `201` - Member added - Error: `400` - Bad Request **Example:** ```json {\"team_id\": \"team_01\", \"user_id\": \"usr_002\", \"role\": \"member\"} ```",
              "score": null
            },
            {
              "doc_id": "api_integrations",
              "chunk_id": "api_integrations_fix_0",
              "content": "# Integrations API Reference ## Overview The Integrations API provides programmatic access to integrations functionality. Base URL: `https://api.cloudflow.io/v1/integrations` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /integrations List available integrations. **Parameters:** - `category` (string): Filter by category: storage, notification, database **Response:** - Success: `200` - Returns array of integration objects - Error: `400` - Bad Request **Example:** ```json {\"integrations\": [{\"id\": \"int_s3\", \"name\": \"Amazon S3\", \"category\": \"storage\", \"status\": \"connected\"}]} ``` ### POST /integrations/{id}/connect Connect an integration. **Parameters:** - `credentials` (object): Integration-specific credentials **Response:** - Success: `200` - Integration connected - Error: `400` - Bad Request **Example:** ```json {\"id\": \"int_s3\", \"status\": \"connected\", \"connected_at\": \"2024-01-15T10:00:00Z\"} ```",
              "score": null
            },
            {
              "doc_id": "api_workflows",
              "chunk_id": "api_workflows_fix_0",
              "content": "# Workflows API Reference ## Overview The Workflows API provides programmatic access to workflows functionality. Base URL: `https://api.cloudflow.io/v1/workflows` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /workflows List all workflows for the authenticated user. **Parameters:** - `limit` (integer): Maximum number of results (default: 50, max: 200) - `status` (string): Filter by status: active, paused, archived **Response:** - Success: `200` - Returns array of workflow objects - Error: `400` - Bad Request **Example:** ```json {\"workflows\": [{\"id\": \"wf_123\", \"name\": \"Daily Report\", \"status\": \"active\"}]} ``` ### POST /workflows Create a new workflow. **Parameters:** - `name` (string): Workflow name (required) - `definition` (object): Workflow definition in JSON format **Response:** - Success: `201` - Workflow created successfully - Error: `400` - Bad Request **Example:** ```json {\"id\": \"wf_456\", \"name\": \"New Workflow\", \"created_at\": \"2024-01-15T10:00:00Z\"} ``` ### DELETE /workflows/{id} Delete a workflow permanently. **Parameters:** - `id` (string): Workflow ID (required) **Response:** - Success: `204` - Workflow deleted - Error: `404` - Workflow not found **Example:** ```json {} ```",
              "score": null
            }
          ],
          "latency_ms": 3.995180000856635,
          "query_id": "cross_015",
          "dimension": "problem",
          "query": "getting 403 forbidden when calling the API with what I thought was a valid API key"
        },
        {
          "key_facts": [
            "Bearer",
            "API key",
            "OAuth 2.0",
            "Authorization"
          ],
          "found_facts": [
            "Bearer",
            "API key",
            "OAuth 2.0",
            "Authorization"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "api_users",
            "api_integrations",
            "adr_003"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "adr_003",
              "chunk_id": "adr_003_fix_0",
              "content": "# ADR-003: Implement OAuth 2.0 with JWT for Authentication **Status:** Accepted **Date:** 2024-05-08 ## Context We need a secure authentication system that supports API keys, user sessions, and third-party integrations. ## Options Considered ### Option 1: OAuth 2.0 with JWT Industry standard token-based auth. **Pros:** - Industry standard - Stateless validation - Supports refresh tokens - SSO compatible **Cons:** - Token size overhead - Cannot revoke individual tokens - Complexity ### Option 2: Session-based auth Traditional server-side sessions. **Pros:** - Simple implementation - Easy revocation - Smaller payloads **Cons:** - Requires session store - Not suitable for APIs - Scaling challenges ### Option 3: API keys only Simple key-based authentication. **Pros:** - Very simple - Low overhead - Easy to implement **Cons:** - No user context - Hard to rotate - No SSO support ## Decision We will implement OAuth 2.0 with JWT tokens. Access tokens expire after 1 hour, refresh tokens after 7 days. ## Consequences - Implement token rotation on refresh - Support API keys for machine-to-machine auth - Add MFA for admin accounts",
              "score": null
            },
            {
              "doc_id": "arch_authentication_service",
              "chunk_id": "arch_authentication_service_fix_0",
              "content": "# Authentication Service Architecture ## Overview The Authentication Service handles user identity, session management, and token issuance. Supports OAuth 2.0, SAML, and API key authentication. ## Design Principles - **Zero Knowledge**: Passwords never stored in plaintext - **Token Rotation**: Refresh tokens rotate on each use ## Components ### Identity Provider Manages user accounts and credentials. Password hashing uses bcrypt with cost factor 12. **Technology:** Custom Go service **Scaling:** Horizontal stateless ### Token Service Issues and validates access tokens. Token lifetime is 1 hour with 7-day refresh tokens. **Technology:** JWT with RS256 **Scaling:** Horizontal stateless ### SSO Gateway Integrates with external identity providers. Supports Google, Microsoft, Okta, and custom SAML. **Technology:** Dex OIDC proxy **Scaling:** Active-passive ## Data Flow Credentials \u2192 Validation \u2192 Token Issue \u2192 Token Refresh \u2192 Logout ## Performance Characteristics - **Latency P50:** 50ms - **Latency P99:** 200ms - **Throughput:** 5000 auth/second",
              "score": null
            },
            {
              "doc_id": "howto_create_api_integration",
              "chunk_id": "howto_create_api_integration_fix_0",
              "content": "# How to Create API Integration ## Prerequisites - CloudFlow account ## Steps ### Step 1: Generate API key Create an API key for programmatic access. ```bash cloudflow apikeys create --name 'CI Integration' --scopes workflows:read,workflows:execute ``` ### Step 2: Store key securely Never commit API keys to source control. Use environment variables or secret management. > **Note:** Rotate keys every 90 days ### Step 3: Test authentication Verify the API key works correctly. ```bash curl -H 'Authorization: Bearer cf_...' https://api.cloudflow.io/v1/me ``` ### Step 4: Implement rate limiting Handle rate limit responses (HTTP 429) with exponential backoff. > **Note:** Default limit is 100 requests per minute ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            },
            {
              "doc_id": "api_teams",
              "chunk_id": "api_teams_fix_0",
              "content": "# Teams API Reference ## Overview The Teams API provides programmatic access to teams functionality. Base URL: `https://api.cloudflow.io/v1/teams` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /teams List teams the user belongs to. **Parameters:** **Response:** - Success: `200` - Returns array of team objects - Error: `400` - Bad Request **Example:** ```json {\"teams\": [{\"id\": \"team_01\", \"name\": \"Engineering\", \"member_count\": 15}]} ``` ### POST /teams/{id}/members Add a member to a team. **Parameters:** - `user_id` (string): User ID to add - `role` (string): Role: owner, admin, member, viewer **Response:** - Success: `201` - Member added - Error: `400` - Bad Request **Example:** ```json {\"team_id\": \"team_01\", \"user_id\": \"usr_002\", \"role\": \"member\"} ```",
              "score": null
            },
            {
              "doc_id": "api_secrets",
              "chunk_id": "api_secrets_fix_0",
              "content": "# Secrets API Reference ## Overview The Secrets API provides programmatic access to secrets functionality. Base URL: `https://api.cloudflow.io/v1/secrets` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /secrets List secret names (values not returned). **Parameters:** **Response:** - Success: `200` - Returns array of secret metadata - Error: `400` - Bad Request **Example:** ```json {\"secrets\": [{\"name\": \"DATABASE_URL\", \"created_at\": \"2024-01-01\", \"updated_at\": \"2024-01-10\"}]} ``` ### PUT /secrets/{name} Create or update a secret. **Parameters:** - `value` (string): Secret value (encrypted at rest) **Response:** - Success: `200` - Secret saved - Error: `400` - Bad Request **Example:** ```json {\"name\": \"DATABASE_URL\", \"updated_at\": \"2024-01-15T10:00:00Z\"} ``` ### DELETE /secrets/{name} Delete a secret. **Parameters:** **Response:** - Success: `204` - Secret deleted - Error: `400` - Bad Request **Example:** ```json {} ```",
              "score": null
            }
          ],
          "latency_ms": 3.3914649993675994,
          "query_id": "cross_015",
          "dimension": "casual",
          "query": "api auth options"
        },
        {
          "key_facts": [
            "Bearer",
            "API key",
            "OAuth 2.0",
            "Authorization"
          ],
          "found_facts": [
            "Bearer",
            "API key",
            "OAuth 2.0",
            "Authorization"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "api_users",
            "api_integrations",
            "adr_003"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "arch_authentication_service",
              "chunk_id": "arch_authentication_service_fix_0",
              "content": "# Authentication Service Architecture ## Overview The Authentication Service handles user identity, session management, and token issuance. Supports OAuth 2.0, SAML, and API key authentication. ## Design Principles - **Zero Knowledge**: Passwords never stored in plaintext - **Token Rotation**: Refresh tokens rotate on each use ## Components ### Identity Provider Manages user accounts and credentials. Password hashing uses bcrypt with cost factor 12. **Technology:** Custom Go service **Scaling:** Horizontal stateless ### Token Service Issues and validates access tokens. Token lifetime is 1 hour with 7-day refresh tokens. **Technology:** JWT with RS256 **Scaling:** Horizontal stateless ### SSO Gateway Integrates with external identity providers. Supports Google, Microsoft, Okta, and custom SAML. **Technology:** Dex OIDC proxy **Scaling:** Active-passive ## Data Flow Credentials \u2192 Validation \u2192 Token Issue \u2192 Token Refresh \u2192 Logout ## Performance Characteristics - **Latency P50:** 50ms - **Latency P99:** 200ms - **Throughput:** 5000 auth/second",
              "score": null
            },
            {
              "doc_id": "adr_003",
              "chunk_id": "adr_003_fix_0",
              "content": "# ADR-003: Implement OAuth 2.0 with JWT for Authentication **Status:** Accepted **Date:** 2024-05-08 ## Context We need a secure authentication system that supports API keys, user sessions, and third-party integrations. ## Options Considered ### Option 1: OAuth 2.0 with JWT Industry standard token-based auth. **Pros:** - Industry standard - Stateless validation - Supports refresh tokens - SSO compatible **Cons:** - Token size overhead - Cannot revoke individual tokens - Complexity ### Option 2: Session-based auth Traditional server-side sessions. **Pros:** - Simple implementation - Easy revocation - Smaller payloads **Cons:** - Requires session store - Not suitable for APIs - Scaling challenges ### Option 3: API keys only Simple key-based authentication. **Pros:** - Very simple - Low overhead - Easy to implement **Cons:** - No user context - Hard to rotate - No SSO support ## Decision We will implement OAuth 2.0 with JWT tokens. Access tokens expire after 1 hour, refresh tokens after 7 days. ## Consequences - Implement token rotation on refresh - Support API keys for machine-to-machine auth - Add MFA for admin accounts",
              "score": null
            },
            {
              "doc_id": "howto_configure_single_sign_on",
              "chunk_id": "howto_configure_single_sign_on_fix_0",
              "content": "# How to Configure Single Sign-On ## Prerequisites - Admin access - Enterprise plan - Identity provider access ## Steps ### Step 1: Choose provider CloudFlow supports SAML 2.0, OpenID Connect, and OAuth 2.0 providers. ### Step 2: Get metadata Download CloudFlow's SAML metadata for your IdP configuration. ```bash cloudflow sso metadata --format xml > cloudflow-metadata.xml ``` ### Step 3: Configure IdP Add CloudFlow as a service provider in your identity provider. > **Note:** ACS URL: https://auth.cloudflow.io/saml/callback ### Step 4: Upload IdP metadata Provide your IdP's metadata to CloudFlow. ```bash cloudflow sso configure --idp-metadata ./idp-metadata.xml ``` ### Step 5: Test login Verify SSO login works correctly. ```bash cloudflow sso test ``` ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            },
            {
              "doc_id": "howto_create_api_integration",
              "chunk_id": "howto_create_api_integration_fix_0",
              "content": "# How to Create API Integration ## Prerequisites - CloudFlow account ## Steps ### Step 1: Generate API key Create an API key for programmatic access. ```bash cloudflow apikeys create --name 'CI Integration' --scopes workflows:read,workflows:execute ``` ### Step 2: Store key securely Never commit API keys to source control. Use environment variables or secret management. > **Note:** Rotate keys every 90 days ### Step 3: Test authentication Verify the API key works correctly. ```bash curl -H 'Authorization: Bearer cf_...' https://api.cloudflow.io/v1/me ``` ### Step 4: Implement rate limiting Handle rate limit responses (HTTP 429) with exponential backoff. > **Note:** Default limit is 100 requests per minute ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            },
            {
              "doc_id": "api_users",
              "chunk_id": "api_users_fix_0",
              "content": "# Users API Reference ## Overview The Users API provides programmatic access to users functionality. Base URL: `https://api.cloudflow.io/v1/users` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /users/me Get current user profile. **Parameters:** **Response:** - Success: `200` - Returns user object - Error: `400` - Bad Request **Example:** ```json {\"id\": \"usr_001\", \"email\": \"user@example.com\", \"role\": \"admin\", \"created_at\": \"2023-06-01\"} ``` ### PATCH /users/me Update current user profile. **Parameters:** - `name` (string): Display name - `timezone` (string): Timezone (e.g., America/New_York) **Response:** - Success: `200` - User updated - Error: `400` - Bad Request **Example:** ```json {\"id\": \"usr_001\", \"name\": \"John Doe\", \"timezone\": \"America/New_York\"} ```",
              "score": null
            }
          ],
          "latency_ms": 3.807900999163394,
          "query_id": "cross_015",
          "dimension": "contextual",
          "query": "writing a CLI tool that needs to authenticate users and also support service accounts, what auth methods should I implement"
        },
        {
          "key_facts": [
            "Bearer",
            "API key",
            "OAuth 2.0",
            "Authorization"
          ],
          "found_facts": [
            "Bearer",
            "API key",
            "OAuth 2.0",
            "Authorization"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "api_users",
            "api_integrations",
            "adr_003"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "howto_create_api_integration",
              "chunk_id": "howto_create_api_integration_fix_0",
              "content": "# How to Create API Integration ## Prerequisites - CloudFlow account ## Steps ### Step 1: Generate API key Create an API key for programmatic access. ```bash cloudflow apikeys create --name 'CI Integration' --scopes workflows:read,workflows:execute ``` ### Step 2: Store key securely Never commit API keys to source control. Use environment variables or secret management. > **Note:** Rotate keys every 90 days ### Step 3: Test authentication Verify the API key works correctly. ```bash curl -H 'Authorization: Bearer cf_...' https://api.cloudflow.io/v1/me ``` ### Step 4: Implement rate limiting Handle rate limit responses (HTTP 429) with exponential backoff. > **Note:** Default limit is 100 requests per minute ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            },
            {
              "doc_id": "adr_003",
              "chunk_id": "adr_003_fix_0",
              "content": "# ADR-003: Implement OAuth 2.0 with JWT for Authentication **Status:** Accepted **Date:** 2024-05-08 ## Context We need a secure authentication system that supports API keys, user sessions, and third-party integrations. ## Options Considered ### Option 1: OAuth 2.0 with JWT Industry standard token-based auth. **Pros:** - Industry standard - Stateless validation - Supports refresh tokens - SSO compatible **Cons:** - Token size overhead - Cannot revoke individual tokens - Complexity ### Option 2: Session-based auth Traditional server-side sessions. **Pros:** - Simple implementation - Easy revocation - Smaller payloads **Cons:** - Requires session store - Not suitable for APIs - Scaling challenges ### Option 3: API keys only Simple key-based authentication. **Pros:** - Very simple - Low overhead - Easy to implement **Cons:** - No user context - Hard to rotate - No SSO support ## Decision We will implement OAuth 2.0 with JWT tokens. Access tokens expire after 1 hour, refresh tokens after 7 days. ## Consequences - Implement token rotation on refresh - Support API keys for machine-to-machine auth - Add MFA for admin accounts",
              "score": null
            },
            {
              "doc_id": "api_secrets",
              "chunk_id": "api_secrets_fix_0",
              "content": "# Secrets API Reference ## Overview The Secrets API provides programmatic access to secrets functionality. Base URL: `https://api.cloudflow.io/v1/secrets` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /secrets List secret names (values not returned). **Parameters:** **Response:** - Success: `200` - Returns array of secret metadata - Error: `400` - Bad Request **Example:** ```json {\"secrets\": [{\"name\": \"DATABASE_URL\", \"created_at\": \"2024-01-01\", \"updated_at\": \"2024-01-10\"}]} ``` ### PUT /secrets/{name} Create or update a secret. **Parameters:** - `value` (string): Secret value (encrypted at rest) **Response:** - Success: `200` - Secret saved - Error: `400` - Bad Request **Example:** ```json {\"name\": \"DATABASE_URL\", \"updated_at\": \"2024-01-15T10:00:00Z\"} ``` ### DELETE /secrets/{name} Delete a secret. **Parameters:** **Response:** - Success: `204` - Secret deleted - Error: `400` - Bad Request **Example:** ```json {} ```",
              "score": null
            },
            {
              "doc_id": "api_webhooks",
              "chunk_id": "api_webhooks_fix_0",
              "content": "# Webhooks API Reference ## Overview The Webhooks API provides programmatic access to webhooks functionality. Base URL: `https://api.cloudflow.io/v1/webhooks` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /webhooks List configured webhooks. **Parameters:** - `active` (boolean): Filter by active status **Response:** - Success: `200` - Returns array of webhook objects - Error: `400` - Bad Request **Example:** ```json {\"webhooks\": [{\"id\": \"wh_01\", \"url\": \"https://example.com/hook\", \"events\": [\"workflow.completed\"]}]} ``` ### POST /webhooks Create a new webhook endpoint. **Parameters:** - `url` (string): Webhook URL (must be HTTPS) - `events` (array): Events to subscribe to - `secret` (string): Signing secret for verification **Response:** - Success: `201` - Webhook created - Error: `400` - Bad Request **Example:** ```json {\"id\": \"wh_02\", \"url\": \"https://example.com/new-hook\", \"secret\": \"whsec_...\"} ```",
              "score": null
            },
            {
              "doc_id": "api_workflows",
              "chunk_id": "api_workflows_fix_0",
              "content": "# Workflows API Reference ## Overview The Workflows API provides programmatic access to workflows functionality. Base URL: `https://api.cloudflow.io/v1/workflows` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /workflows List all workflows for the authenticated user. **Parameters:** - `limit` (integer): Maximum number of results (default: 50, max: 200) - `status` (string): Filter by status: active, paused, archived **Response:** - Success: `200` - Returns array of workflow objects - Error: `400` - Bad Request **Example:** ```json {\"workflows\": [{\"id\": \"wf_123\", \"name\": \"Daily Report\", \"status\": \"active\"}]} ``` ### POST /workflows Create a new workflow. **Parameters:** - `name` (string): Workflow name (required) - `definition` (object): Workflow definition in JSON format **Response:** - Success: `201` - Workflow created successfully - Error: `400` - Bad Request **Example:** ```json {\"id\": \"wf_456\", \"name\": \"New Workflow\", \"created_at\": \"2024-01-15T10:00:00Z\"} ``` ### DELETE /workflows/{id} Delete a workflow permanently. **Parameters:** - `id` (string): Workflow ID (required) **Response:** - Success: `204` - Workflow deleted - Error: `404` - Workflow not found **Example:** ```json {} ```",
              "score": null
            }
          ],
          "latency_ms": 3.682136999486829,
          "query_id": "cross_015",
          "dimension": "negation",
          "query": "why can't I just use a simple API key for everything instead of dealing with OAuth flows"
        },
        {
          "key_facts": [
            "GitHub Actions",
            "cloudflow/deploy-action",
            "CLOUDFLOW_API_KEY"
          ],
          "found_facts": [
            "GitHub Actions",
            "cloudflow/deploy-action",
            "CLOUDFLOW_API_KEY"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "howto_set_up_ci/cd_pipeline"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "howto_set_up_ci_cd_pipeline",
              "chunk_id": "howto_set_up_ci_cd_pipeline_fix_0",
              "content": "# How to Set Up CI/CD Pipeline ## Prerequisites - GitHub account - CloudFlow API key - Repository admin access ## Steps ### Step 1: Create GitHub Actions workflow Create a new file at `.github/workflows/cloudflow.yml` with the deployment configuration. ```bash mkdir -p .github/workflows && touch .github/workflows/cloudflow.yml ``` ### Step 2: Configure secrets Add your CloudFlow API key to GitHub secrets. ```bash gh secret set CLOUDFLOW_API_KEY ``` ### Step 3: Add deployment step Add the CloudFlow deployment action to your workflow. > **Note:** Use the official cloudflow/deploy-action@v2 ### Step 4: Test the pipeline Push a commit to trigger the workflow. ```bash git push origin main ``` ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            },
            {
              "doc_id": "howto_configure_custom_domain",
              "chunk_id": "howto_configure_custom_domain_fix_0",
              "content": "# How to Configure Custom Domain ## Prerequisites - Domain with DNS access - CloudFlow Pro plan or higher ## Steps ### Step 1: Verify domain ownership Add a TXT record to verify domain ownership. ```bash cloudflow domains verify --domain app.example.com ``` ### Step 2: Configure DNS Add a CNAME record pointing to your CloudFlow endpoint. > **Note:** CNAME should point to ingress.cloudflow.io ### Step 3: Enable SSL CloudFlow automatically provisions SSL certificates via Let's Encrypt. ```bash cloudflow domains ssl --domain app.example.com ``` ### Step 4: Set as primary Make this the primary domain for your application. ```bash cloudflow domains set-primary --domain app.example.com ``` ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            },
            {
              "doc_id": "howto_create_api_integration",
              "chunk_id": "howto_create_api_integration_fix_0",
              "content": "# How to Create API Integration ## Prerequisites - CloudFlow account ## Steps ### Step 1: Generate API key Create an API key for programmatic access. ```bash cloudflow apikeys create --name 'CI Integration' --scopes workflows:read,workflows:execute ``` ### Step 2: Store key securely Never commit API keys to source control. Use environment variables or secret management. > **Note:** Rotate keys every 90 days ### Step 3: Test authentication Verify the API key works correctly. ```bash curl -H 'Authorization: Bearer cf_...' https://api.cloudflow.io/v1/me ``` ### Step 4: Implement rate limiting Handle rate limit responses (HTTP 429) with exponential backoff. > **Note:** Default limit is 100 requests per minute ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            },
            {
              "doc_id": "howto_configure_single_sign_on",
              "chunk_id": "howto_configure_single_sign_on_fix_0",
              "content": "# How to Configure Single Sign-On ## Prerequisites - Admin access - Enterprise plan - Identity provider access ## Steps ### Step 1: Choose provider CloudFlow supports SAML 2.0, OpenID Connect, and OAuth 2.0 providers. ### Step 2: Get metadata Download CloudFlow's SAML metadata for your IdP configuration. ```bash cloudflow sso metadata --format xml > cloudflow-metadata.xml ``` ### Step 3: Configure IdP Add CloudFlow as a service provider in your identity provider. > **Note:** ACS URL: https://auth.cloudflow.io/saml/callback ### Step 4: Upload IdP metadata Provide your IdP's metadata to CloudFlow. ```bash cloudflow sso configure --idp-metadata ./idp-metadata.xml ``` ### Step 5: Test login Verify SSO login works correctly. ```bash cloudflow sso test ``` ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            },
            {
              "doc_id": "howto_set_up_database_connection",
              "chunk_id": "howto_set_up_database_connection_fix_0",
              "content": "# How to Set Up Database Connection ## Prerequisites - Database with network access to CloudFlow - Connection credentials ## Steps ### Step 1: Add connection secret Store your database connection string as a secret. ```bash cloudflow secrets set DATABASE_URL postgresql://user:pass@host:5432/db ``` ### Step 2: Configure connection pool Set pool size based on your plan limits. Free tier allows 5 connections, Pro allows 20. > **Note:** Pool exhaustion will queue requests ### Step 3: Test connection Verify the database is accessible from CloudFlow. ```bash cloudflow db test --secret DATABASE_URL ``` ### Step 4: Enable SSL For production, always require SSL connections. ```bash cloudflow db configure --require-ssl ``` ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            }
          ],
          "latency_ms": 4.0361059982387815,
          "query_id": "howto_016",
          "dimension": "original",
          "query": "How do I set up CI/CD for CloudFlow?"
        },
        {
          "key_facts": [
            "GitHub Actions",
            "cloudflow/deploy-action",
            "CLOUDFLOW_API_KEY"
          ],
          "found_facts": [
            "GitHub Actions",
            "cloudflow/deploy-action",
            "CLOUDFLOW_API_KEY"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "howto_set_up_ci/cd_pipeline"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "howto_set_up_ci_cd_pipeline",
              "chunk_id": "howto_set_up_ci_cd_pipeline_fix_0",
              "content": "# How to Set Up CI/CD Pipeline ## Prerequisites - GitHub account - CloudFlow API key - Repository admin access ## Steps ### Step 1: Create GitHub Actions workflow Create a new file at `.github/workflows/cloudflow.yml` with the deployment configuration. ```bash mkdir -p .github/workflows && touch .github/workflows/cloudflow.yml ``` ### Step 2: Configure secrets Add your CloudFlow API key to GitHub secrets. ```bash gh secret set CLOUDFLOW_API_KEY ``` ### Step 3: Add deployment step Add the CloudFlow deployment action to your workflow. > **Note:** Use the official cloudflow/deploy-action@v2 ### Step 4: Test the pipeline Push a commit to trigger the workflow. ```bash git push origin main ``` ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            },
            {
              "doc_id": "howto_create_api_integration",
              "chunk_id": "howto_create_api_integration_fix_0",
              "content": "# How to Create API Integration ## Prerequisites - CloudFlow account ## Steps ### Step 1: Generate API key Create an API key for programmatic access. ```bash cloudflow apikeys create --name 'CI Integration' --scopes workflows:read,workflows:execute ``` ### Step 2: Store key securely Never commit API keys to source control. Use environment variables or secret management. > **Note:** Rotate keys every 90 days ### Step 3: Test authentication Verify the API key works correctly. ```bash curl -H 'Authorization: Bearer cf_...' https://api.cloudflow.io/v1/me ``` ### Step 4: Implement rate limiting Handle rate limit responses (HTTP 429) with exponential backoff. > **Note:** Default limit is 100 requests per minute ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            },
            {
              "doc_id": "howto_migrate_from_v1_to_v2_api",
              "chunk_id": "howto_migrate_from_v1_to_v2_api_fix_0",
              "content": "# How to Migrate from v1 to v2 API ## Prerequisites - Existing v1 integration - v2 API access enabled ## Steps ### Step 1: Review breaking changes The v2 API changes authentication from API keys to OAuth 2.0. Review the migration guide. ### Step 2: Update client library Upgrade to the latest SDK version. ```bash npm install @cloudflow/sdk@latest ``` ### Step 3: Update authentication Replace API key authentication with OAuth 2.0 client credentials flow. > **Note:** Generate new credentials in the dashboard ### Step 4: Test in staging Run your integration tests against the v2 staging endpoint. ```bash CLOUDFLOW_API_URL=https://api-v2-staging.cloudflow.io npm test ``` ### Step 5: Switch production Update production configuration to use v2 API. ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            },
            {
              "doc_id": "howto_implement_workflow_versioning",
              "chunk_id": "howto_implement_workflow_versioning_fix_0",
              "content": "# How to Implement Workflow Versioning ## Prerequisites - Existing workflow ## Steps ### Step 1: Enable versioning Turn on versioning for your workflow. This is required for the feature. ```bash cloudflow workflows update wf_123 --enable-versioning ``` ### Step 2: Create new version Save changes as a new version instead of overwriting. ```bash cloudflow workflows publish wf_123 --version 2.0.0 ``` ### Step 3: Set active version Choose which version handles new executions. ```bash cloudflow workflows activate wf_123 --version 2.0.0 ``` ### Step 4: Roll back if needed Quickly revert to a previous version. ```bash cloudflow workflows rollback wf_123 --to-version 1.0.0 ``` ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            },
            {
              "doc_id": "howto_set_up_database_connection",
              "chunk_id": "howto_set_up_database_connection_fix_0",
              "content": "# How to Set Up Database Connection ## Prerequisites - Database with network access to CloudFlow - Connection credentials ## Steps ### Step 1: Add connection secret Store your database connection string as a secret. ```bash cloudflow secrets set DATABASE_URL postgresql://user:pass@host:5432/db ``` ### Step 2: Configure connection pool Set pool size based on your plan limits. Free tier allows 5 connections, Pro allows 20. > **Note:** Pool exhaustion will queue requests ### Step 3: Test connection Verify the database is accessible from CloudFlow. ```bash cloudflow db test --secret DATABASE_URL ``` ### Step 4: Enable SSL For production, always require SSL connections. ```bash cloudflow db configure --require-ssl ``` ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            }
          ],
          "latency_ms": 3.61678499939444,
          "query_id": "howto_016",
          "dimension": "synonym",
          "query": "How do I configure continuous deployment pipelines for CloudFlow?"
        },
        {
          "key_facts": [
            "GitHub Actions",
            "cloudflow/deploy-action",
            "CLOUDFLOW_API_KEY"
          ],
          "found_facts": [
            "GitHub Actions",
            "cloudflow/deploy-action",
            "CLOUDFLOW_API_KEY"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "howto_set_up_ci/cd_pipeline"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "howto_set_up_ci_cd_pipeline",
              "chunk_id": "howto_set_up_ci_cd_pipeline_fix_0",
              "content": "# How to Set Up CI/CD Pipeline ## Prerequisites - GitHub account - CloudFlow API key - Repository admin access ## Steps ### Step 1: Create GitHub Actions workflow Create a new file at `.github/workflows/cloudflow.yml` with the deployment configuration. ```bash mkdir -p .github/workflows && touch .github/workflows/cloudflow.yml ``` ### Step 2: Configure secrets Add your CloudFlow API key to GitHub secrets. ```bash gh secret set CLOUDFLOW_API_KEY ``` ### Step 3: Add deployment step Add the CloudFlow deployment action to your workflow. > **Note:** Use the official cloudflow/deploy-action@v2 ### Step 4: Test the pipeline Push a commit to trigger the workflow. ```bash git push origin main ``` ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            },
            {
              "doc_id": "howto_create_api_integration",
              "chunk_id": "howto_create_api_integration_fix_0",
              "content": "# How to Create API Integration ## Prerequisites - CloudFlow account ## Steps ### Step 1: Generate API key Create an API key for programmatic access. ```bash cloudflow apikeys create --name 'CI Integration' --scopes workflows:read,workflows:execute ``` ### Step 2: Store key securely Never commit API keys to source control. Use environment variables or secret management. > **Note:** Rotate keys every 90 days ### Step 3: Test authentication Verify the API key works correctly. ```bash curl -H 'Authorization: Bearer cf_...' https://api.cloudflow.io/v1/me ``` ### Step 4: Implement rate limiting Handle rate limit responses (HTTP 429) with exponential backoff. > **Note:** Default limit is 100 requests per minute ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            },
            {
              "doc_id": "howto_implement_workflow_versioning",
              "chunk_id": "howto_implement_workflow_versioning_fix_0",
              "content": "# How to Implement Workflow Versioning ## Prerequisites - Existing workflow ## Steps ### Step 1: Enable versioning Turn on versioning for your workflow. This is required for the feature. ```bash cloudflow workflows update wf_123 --enable-versioning ``` ### Step 2: Create new version Save changes as a new version instead of overwriting. ```bash cloudflow workflows publish wf_123 --version 2.0.0 ``` ### Step 3: Set active version Choose which version handles new executions. ```bash cloudflow workflows activate wf_123 --version 2.0.0 ``` ### Step 4: Roll back if needed Quickly revert to a previous version. ```bash cloudflow workflows rollback wf_123 --to-version 1.0.0 ``` ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            },
            {
              "doc_id": "howto_migrate_from_v1_to_v2_api",
              "chunk_id": "howto_migrate_from_v1_to_v2_api_fix_0",
              "content": "# How to Migrate from v1 to v2 API ## Prerequisites - Existing v1 integration - v2 API access enabled ## Steps ### Step 1: Review breaking changes The v2 API changes authentication from API keys to OAuth 2.0. Review the migration guide. ### Step 2: Update client library Upgrade to the latest SDK version. ```bash npm install @cloudflow/sdk@latest ``` ### Step 3: Update authentication Replace API key authentication with OAuth 2.0 client credentials flow. > **Note:** Generate new credentials in the dashboard ### Step 4: Test in staging Run your integration tests against the v2 staging endpoint. ```bash CLOUDFLOW_API_URL=https://api-v2-staging.cloudflow.io npm test ``` ### Step 5: Switch production Update production configuration to use v2 API. ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            },
            {
              "doc_id": "howto_configure_custom_domain",
              "chunk_id": "howto_configure_custom_domain_fix_0",
              "content": "# How to Configure Custom Domain ## Prerequisites - Domain with DNS access - CloudFlow Pro plan or higher ## Steps ### Step 1: Verify domain ownership Add a TXT record to verify domain ownership. ```bash cloudflow domains verify --domain app.example.com ``` ### Step 2: Configure DNS Add a CNAME record pointing to your CloudFlow endpoint. > **Note:** CNAME should point to ingress.cloudflow.io ### Step 3: Enable SSL CloudFlow automatically provisions SSL certificates via Let's Encrypt. ```bash cloudflow domains ssl --domain app.example.com ``` ### Step 4: Set as primary Make this the primary domain for your application. ```bash cloudflow domains set-primary --domain app.example.com ``` ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            }
          ],
          "latency_ms": 3.5568839994084556,
          "query_id": "howto_016",
          "dimension": "problem",
          "query": "I want to automatically deploy my CloudFlow workflows when I push to GitHub but don't know where to start"
        },
        {
          "key_facts": [
            "GitHub Actions",
            "cloudflow/deploy-action",
            "CLOUDFLOW_API_KEY"
          ],
          "found_facts": [
            "GitHub Actions",
            "cloudflow/deploy-action",
            "CLOUDFLOW_API_KEY"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "howto_set_up_ci/cd_pipeline"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "howto_set_up_ci_cd_pipeline",
              "chunk_id": "howto_set_up_ci_cd_pipeline_fix_0",
              "content": "# How to Set Up CI/CD Pipeline ## Prerequisites - GitHub account - CloudFlow API key - Repository admin access ## Steps ### Step 1: Create GitHub Actions workflow Create a new file at `.github/workflows/cloudflow.yml` with the deployment configuration. ```bash mkdir -p .github/workflows && touch .github/workflows/cloudflow.yml ``` ### Step 2: Configure secrets Add your CloudFlow API key to GitHub secrets. ```bash gh secret set CLOUDFLOW_API_KEY ``` ### Step 3: Add deployment step Add the CloudFlow deployment action to your workflow. > **Note:** Use the official cloudflow/deploy-action@v2 ### Step 4: Test the pipeline Push a commit to trigger the workflow. ```bash git push origin main ``` ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            },
            {
              "doc_id": "howto_configure_custom_domain",
              "chunk_id": "howto_configure_custom_domain_fix_0",
              "content": "# How to Configure Custom Domain ## Prerequisites - Domain with DNS access - CloudFlow Pro plan or higher ## Steps ### Step 1: Verify domain ownership Add a TXT record to verify domain ownership. ```bash cloudflow domains verify --domain app.example.com ``` ### Step 2: Configure DNS Add a CNAME record pointing to your CloudFlow endpoint. > **Note:** CNAME should point to ingress.cloudflow.io ### Step 3: Enable SSL CloudFlow automatically provisions SSL certificates via Let's Encrypt. ```bash cloudflow domains ssl --domain app.example.com ``` ### Step 4: Set as primary Make this the primary domain for your application. ```bash cloudflow domains set-primary --domain app.example.com ``` ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            },
            {
              "doc_id": "howto_set_up_database_connection",
              "chunk_id": "howto_set_up_database_connection_fix_0",
              "content": "# How to Set Up Database Connection ## Prerequisites - Database with network access to CloudFlow - Connection credentials ## Steps ### Step 1: Add connection secret Store your database connection string as a secret. ```bash cloudflow secrets set DATABASE_URL postgresql://user:pass@host:5432/db ``` ### Step 2: Configure connection pool Set pool size based on your plan limits. Free tier allows 5 connections, Pro allows 20. > **Note:** Pool exhaustion will queue requests ### Step 3: Test connection Verify the database is accessible from CloudFlow. ```bash cloudflow db test --secret DATABASE_URL ``` ### Step 4: Enable SSL For production, always require SSL connections. ```bash cloudflow db configure --require-ssl ``` ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            },
            {
              "doc_id": "howto_create_api_integration",
              "chunk_id": "howto_create_api_integration_fix_0",
              "content": "# How to Create API Integration ## Prerequisites - CloudFlow account ## Steps ### Step 1: Generate API key Create an API key for programmatic access. ```bash cloudflow apikeys create --name 'CI Integration' --scopes workflows:read,workflows:execute ``` ### Step 2: Store key securely Never commit API keys to source control. Use environment variables or secret management. > **Note:** Rotate keys every 90 days ### Step 3: Test authentication Verify the API key works correctly. ```bash curl -H 'Authorization: Bearer cf_...' https://api.cloudflow.io/v1/me ``` ### Step 4: Implement rate limiting Handle rate limit responses (HTTP 429) with exponential backoff. > **Note:** Default limit is 100 requests per minute ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            },
            {
              "doc_id": "howto_configure_single_sign_on",
              "chunk_id": "howto_configure_single_sign_on_fix_0",
              "content": "# How to Configure Single Sign-On ## Prerequisites - Admin access - Enterprise plan - Identity provider access ## Steps ### Step 1: Choose provider CloudFlow supports SAML 2.0, OpenID Connect, and OAuth 2.0 providers. ### Step 2: Get metadata Download CloudFlow's SAML metadata for your IdP configuration. ```bash cloudflow sso metadata --format xml > cloudflow-metadata.xml ``` ### Step 3: Configure IdP Add CloudFlow as a service provider in your identity provider. > **Note:** ACS URL: https://auth.cloudflow.io/saml/callback ### Step 4: Upload IdP metadata Provide your IdP's metadata to CloudFlow. ```bash cloudflow sso configure --idp-metadata ./idp-metadata.xml ``` ### Step 5: Test login Verify SSO login works correctly. ```bash cloudflow sso test ``` ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            }
          ],
          "latency_ms": 3.7694290003855713,
          "query_id": "howto_016",
          "dimension": "casual",
          "query": "cloudflow ci/cd setup"
        },
        {
          "key_facts": [
            "GitHub Actions",
            "cloudflow/deploy-action",
            "CLOUDFLOW_API_KEY"
          ],
          "found_facts": [
            "GitHub Actions",
            "cloudflow/deploy-action",
            "CLOUDFLOW_API_KEY"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "howto_set_up_ci/cd_pipeline"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "howto_set_up_ci_cd_pipeline",
              "chunk_id": "howto_set_up_ci_cd_pipeline_fix_0",
              "content": "# How to Set Up CI/CD Pipeline ## Prerequisites - GitHub account - CloudFlow API key - Repository admin access ## Steps ### Step 1: Create GitHub Actions workflow Create a new file at `.github/workflows/cloudflow.yml` with the deployment configuration. ```bash mkdir -p .github/workflows && touch .github/workflows/cloudflow.yml ``` ### Step 2: Configure secrets Add your CloudFlow API key to GitHub secrets. ```bash gh secret set CLOUDFLOW_API_KEY ``` ### Step 3: Add deployment step Add the CloudFlow deployment action to your workflow. > **Note:** Use the official cloudflow/deploy-action@v2 ### Step 4: Test the pipeline Push a commit to trigger the workflow. ```bash git push origin main ``` ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            },
            {
              "doc_id": "howto_migrate_from_v1_to_v2_api",
              "chunk_id": "howto_migrate_from_v1_to_v2_api_fix_0",
              "content": "# How to Migrate from v1 to v2 API ## Prerequisites - Existing v1 integration - v2 API access enabled ## Steps ### Step 1: Review breaking changes The v2 API changes authentication from API keys to OAuth 2.0. Review the migration guide. ### Step 2: Update client library Upgrade to the latest SDK version. ```bash npm install @cloudflow/sdk@latest ``` ### Step 3: Update authentication Replace API key authentication with OAuth 2.0 client credentials flow. > **Note:** Generate new credentials in the dashboard ### Step 4: Test in staging Run your integration tests against the v2 staging endpoint. ```bash CLOUDFLOW_API_URL=https://api-v2-staging.cloudflow.io npm test ``` ### Step 5: Switch production Update production configuration to use v2 API. ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            },
            {
              "doc_id": "howto_create_api_integration",
              "chunk_id": "howto_create_api_integration_fix_0",
              "content": "# How to Create API Integration ## Prerequisites - CloudFlow account ## Steps ### Step 1: Generate API key Create an API key for programmatic access. ```bash cloudflow apikeys create --name 'CI Integration' --scopes workflows:read,workflows:execute ``` ### Step 2: Store key securely Never commit API keys to source control. Use environment variables or secret management. > **Note:** Rotate keys every 90 days ### Step 3: Test authentication Verify the API key works correctly. ```bash curl -H 'Authorization: Bearer cf_...' https://api.cloudflow.io/v1/me ``` ### Step 4: Implement rate limiting Handle rate limit responses (HTTP 429) with exponential backoff. > **Note:** Default limit is 100 requests per minute ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            },
            {
              "doc_id": "howto_configure_custom_domain",
              "chunk_id": "howto_configure_custom_domain_fix_0",
              "content": "# How to Configure Custom Domain ## Prerequisites - Domain with DNS access - CloudFlow Pro plan or higher ## Steps ### Step 1: Verify domain ownership Add a TXT record to verify domain ownership. ```bash cloudflow domains verify --domain app.example.com ``` ### Step 2: Configure DNS Add a CNAME record pointing to your CloudFlow endpoint. > **Note:** CNAME should point to ingress.cloudflow.io ### Step 3: Enable SSL CloudFlow automatically provisions SSL certificates via Let's Encrypt. ```bash cloudflow domains ssl --domain app.example.com ``` ### Step 4: Set as primary Make this the primary domain for your application. ```bash cloudflow domains set-primary --domain app.example.com ``` ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            },
            {
              "doc_id": "howto_configure_single_sign_on",
              "chunk_id": "howto_configure_single_sign_on_fix_0",
              "content": "# How to Configure Single Sign-On ## Prerequisites - Admin access - Enterprise plan - Identity provider access ## Steps ### Step 1: Choose provider CloudFlow supports SAML 2.0, OpenID Connect, and OAuth 2.0 providers. ### Step 2: Get metadata Download CloudFlow's SAML metadata for your IdP configuration. ```bash cloudflow sso metadata --format xml > cloudflow-metadata.xml ``` ### Step 3: Configure IdP Add CloudFlow as a service provider in your identity provider. > **Note:** ACS URL: https://auth.cloudflow.io/saml/callback ### Step 4: Upload IdP metadata Provide your IdP's metadata to CloudFlow. ```bash cloudflow sso configure --idp-metadata ./idp-metadata.xml ``` ### Step 5: Test login Verify SSO login works correctly. ```bash cloudflow sso test ``` ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            }
          ],
          "latency_ms": 4.45077900076285,
          "query_id": "howto_016",
          "dimension": "contextual",
          "query": "setting up automated deployments for my team's CloudFlow project, need to integrate with our GitHub repo"
        },
        {
          "key_facts": [
            "GitHub Actions",
            "cloudflow/deploy-action",
            "CLOUDFLOW_API_KEY"
          ],
          "found_facts": [
            "GitHub Actions",
            "cloudflow/deploy-action",
            "CLOUDFLOW_API_KEY"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "howto_set_up_ci/cd_pipeline"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "howto_set_up_ci_cd_pipeline",
              "chunk_id": "howto_set_up_ci_cd_pipeline_fix_0",
              "content": "# How to Set Up CI/CD Pipeline ## Prerequisites - GitHub account - CloudFlow API key - Repository admin access ## Steps ### Step 1: Create GitHub Actions workflow Create a new file at `.github/workflows/cloudflow.yml` with the deployment configuration. ```bash mkdir -p .github/workflows && touch .github/workflows/cloudflow.yml ``` ### Step 2: Configure secrets Add your CloudFlow API key to GitHub secrets. ```bash gh secret set CLOUDFLOW_API_KEY ``` ### Step 3: Add deployment step Add the CloudFlow deployment action to your workflow. > **Note:** Use the official cloudflow/deploy-action@v2 ### Step 4: Test the pipeline Push a commit to trigger the workflow. ```bash git push origin main ``` ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            },
            {
              "doc_id": "release_3_2_0",
              "chunk_id": "release_3_2_0_fix_0",
              "content": "# Release Notes - v3.2.0 **Release Date:** 2024-01-15 ## New Features - Workflow versioning with rollback support - New Python SDK with async support - Bulk operations API for workflows - Custom retry policies per step ## Bug Fixes - Fixed race condition in concurrent execution handler - Resolved memory leak in long-running workflows - Fixed timezone handling in scheduled triggers ## Breaking Changes - **BREAKING:** API v1 endpoints deprecated, will be removed in v4.0 - **BREAKING:** Minimum Python version now 3.9 ## Upgrade Instructions ```bash cloudflow upgrade --version 3.2.0 ``` ## Known Issues None at this time.",
              "score": null
            },
            {
              "doc_id": "release_3_0_0",
              "chunk_id": "release_3_0_0_fix_0",
              "content": "# Release Notes - v3.0.0 **Release Date:** 2023-10-15 ## New Features - Complete UI redesign - New workflow editor with visual builder - Real-time execution monitoring - Team workspaces ## Bug Fixes - Fixed critical security issue in token validation - Resolved data loss bug in failed migrations ## Breaking Changes - **BREAKING:** Authentication API completely rewritten - **BREAKING:** Legacy webhook format no longer supported - **BREAKING:** Minimum Node.js version now 18 ## Upgrade Instructions ```bash cloudflow upgrade --version 3.0.0 ``` ## Known Issues None at this time.",
              "score": null
            },
            {
              "doc_id": "release_2_9_0",
              "chunk_id": "release_2_9_0_fix_0",
              "content": "# Release Notes - v2.9.0 **Release Date:** 2023-08-01 ## New Features - Parallel step execution - Conditional branching in workflows - Slack integration improvements - Performance dashboard ## Bug Fixes - Fixed issue with special characters in secrets - Resolved timeout in large file uploads ## Upgrade Instructions ```bash cloudflow upgrade --version 2.9.0 ``` ## Known Issues None at this time.",
              "score": null
            },
            {
              "doc_id": "howto_debug_failed_execution",
              "chunk_id": "howto_debug_failed_execution_fix_0",
              "content": "# How to Debug Failed Execution ## Prerequisites - Failed execution ID ## Steps ### Step 1: Get execution details Retrieve the full execution record including step outputs. ```bash cloudflow executions get ex_123 --include-steps ``` ### Step 2: View step logs Check logs for the failed step. ```bash cloudflow executions logs ex_123 --step 3 ``` ### Step 3: Inspect input/output Review the data passed between steps. ```bash cloudflow executions inspect ex_123 --step 3 ``` ### Step 4: Enable debug mode Re-run with verbose logging enabled. ```bash cloudflow workflows execute wf_123 --debug ``` ### Step 5: Check error patterns Look for similar failures in recent executions. ```bash cloudflow executions list --status failed --since 24h ``` ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            }
          ],
          "latency_ms": 3.763639000680996,
          "query_id": "howto_016",
          "dimension": "negation",
          "query": "why aren't my CloudFlow deployments happening automatically when I push code?"
        },
        {
          "key_facts": [
            "CNAME",
            "ingress.cloudflow.io",
            "Let's Encrypt"
          ],
          "found_facts": [
            "CNAME",
            "ingress.cloudflow.io",
            "Let's Encrypt"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "howto_configure_custom_domain"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "howto_configure_custom_domain",
              "chunk_id": "howto_configure_custom_domain_fix_0",
              "content": "# How to Configure Custom Domain ## Prerequisites - Domain with DNS access - CloudFlow Pro plan or higher ## Steps ### Step 1: Verify domain ownership Add a TXT record to verify domain ownership. ```bash cloudflow domains verify --domain app.example.com ``` ### Step 2: Configure DNS Add a CNAME record pointing to your CloudFlow endpoint. > **Note:** CNAME should point to ingress.cloudflow.io ### Step 3: Enable SSL CloudFlow automatically provisions SSL certificates via Let's Encrypt. ```bash cloudflow domains ssl --domain app.example.com ``` ### Step 4: Set as primary Make this the primary domain for your application. ```bash cloudflow domains set-primary --domain app.example.com ``` ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            },
            {
              "doc_id": "config_cache",
              "chunk_id": "config_cache_fix_0",
              "content": "# Cache Configuration Reference ## Overview This document describes all configuration options for Cache. ## Environment Variables | Variable | Type | Default | Description | |----------|------|---------|-------------| | `REDIS_URL` | string | required | Redis connection string | | `REDIS_POOL_SIZE` | integer | 10 | Connection pool size | | `CACHE_TTL_SECONDS` | integer | 300 | Default cache TTL | | `CACHE_PREFIX` | string | cf: | Key prefix for namespacing | | `CACHE_COMPRESSION` | boolean | true | Enable value compression | | `CACHE_MAX_SIZE_MB` | integer | 100 | Maximum cache size per pod | ## Configuration File Configuration can also be provided via `cache.yaml`: ```yaml redis_url: required redis_pool_size: 10 cache_ttl_seconds: 300 cache_prefix: cf: cache_compression: true ``` ## Validation Configuration is validated at startup. Invalid configuration will prevent the service from starting.",
              "score": null
            },
            {
              "doc_id": "config_api_gateway",
              "chunk_id": "config_api_gateway_fix_0",
              "content": "# API Gateway Configuration Reference ## Overview This document describes all configuration options for API Gateway. ## Environment Variables | Variable | Type | Default | Description | |----------|------|---------|-------------| | `GATEWAY_PORT` | integer | 8000 | Gateway listen port | | `GATEWAY_RATE_LIMIT` | integer | 100 | Requests per minute per user | | `GATEWAY_TIMEOUT_MS` | integer | 30000 | Request timeout in milliseconds | | `GATEWAY_MAX_BODY_SIZE` | string | 10MB | Maximum request body size | | `GATEWAY_CORS_ORIGINS` | string | * | Allowed CORS origins | | `GATEWAY_SSL_ENABLED` | boolean | true | Enable TLS termination | ## Configuration File Configuration can also be provided via `api gateway.yaml`: ```yaml gateway_port: 8000 gateway_rate_limit: 100 gateway_timeout_ms: 30000 gateway_max_body_size: 10MB gateway_cors_origins: * ``` ## Validation Configuration is validated at startup. Invalid configuration will prevent the service from starting.",
              "score": null
            },
            {
              "doc_id": "config_database",
              "chunk_id": "config_database_fix_0",
              "content": "# Database Configuration Reference ## Overview This document describes all configuration options for Database. ## Environment Variables | Variable | Type | Default | Description | |----------|------|---------|-------------| | `DATABASE_URL` | string | required | PostgreSQL connection string | | `DATABASE_POOL_MIN` | integer | 5 | Minimum connection pool size | | `DATABASE_POOL_MAX` | integer | 20 | Maximum connection pool size | | `DATABASE_STATEMENT_TIMEOUT` | integer | 30000 | Query timeout in milliseconds | | `DATABASE_SSL_MODE` | string | require | SSL mode: disable, require, verify-full | | `DATABASE_MIGRATION_AUTO` | boolean | false | Run migrations on startup | ## Configuration File Configuration can also be provided via `database.yaml`: ```yaml database_url: required database_pool_min: 5 database_pool_max: 20 database_statement_timeout: 30000 database_ssl_mode: require ``` ## Validation Configuration is validated at startup. Invalid configuration will prevent the service from starting.",
              "score": null
            },
            {
              "doc_id": "howto_configure_single_sign_on",
              "chunk_id": "howto_configure_single_sign_on_fix_0",
              "content": "# How to Configure Single Sign-On ## Prerequisites - Admin access - Enterprise plan - Identity provider access ## Steps ### Step 1: Choose provider CloudFlow supports SAML 2.0, OpenID Connect, and OAuth 2.0 providers. ### Step 2: Get metadata Download CloudFlow's SAML metadata for your IdP configuration. ```bash cloudflow sso metadata --format xml > cloudflow-metadata.xml ``` ### Step 3: Configure IdP Add CloudFlow as a service provider in your identity provider. > **Note:** ACS URL: https://auth.cloudflow.io/saml/callback ### Step 4: Upload IdP metadata Provide your IdP's metadata to CloudFlow. ```bash cloudflow sso configure --idp-metadata ./idp-metadata.xml ``` ### Step 5: Test login Verify SSO login works correctly. ```bash cloudflow sso test ``` ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            }
          ],
          "latency_ms": 3.3180089994857553,
          "query_id": "howto_017",
          "dimension": "original",
          "query": "How do I configure a custom domain?"
        },
        {
          "key_facts": [
            "CNAME",
            "ingress.cloudflow.io",
            "Let's Encrypt"
          ],
          "found_facts": [
            "CNAME",
            "ingress.cloudflow.io",
            "Let's Encrypt"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "howto_configure_custom_domain"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "howto_configure_custom_domain",
              "chunk_id": "howto_configure_custom_domain_fix_0",
              "content": "# How to Configure Custom Domain ## Prerequisites - Domain with DNS access - CloudFlow Pro plan or higher ## Steps ### Step 1: Verify domain ownership Add a TXT record to verify domain ownership. ```bash cloudflow domains verify --domain app.example.com ``` ### Step 2: Configure DNS Add a CNAME record pointing to your CloudFlow endpoint. > **Note:** CNAME should point to ingress.cloudflow.io ### Step 3: Enable SSL CloudFlow automatically provisions SSL certificates via Let's Encrypt. ```bash cloudflow domains ssl --domain app.example.com ``` ### Step 4: Set as primary Make this the primary domain for your application. ```bash cloudflow domains set-primary --domain app.example.com ``` ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            },
            {
              "doc_id": "howto_configure_single_sign_on",
              "chunk_id": "howto_configure_single_sign_on_fix_0",
              "content": "# How to Configure Single Sign-On ## Prerequisites - Admin access - Enterprise plan - Identity provider access ## Steps ### Step 1: Choose provider CloudFlow supports SAML 2.0, OpenID Connect, and OAuth 2.0 providers. ### Step 2: Get metadata Download CloudFlow's SAML metadata for your IdP configuration. ```bash cloudflow sso metadata --format xml > cloudflow-metadata.xml ``` ### Step 3: Configure IdP Add CloudFlow as a service provider in your identity provider. > **Note:** ACS URL: https://auth.cloudflow.io/saml/callback ### Step 4: Upload IdP metadata Provide your IdP's metadata to CloudFlow. ```bash cloudflow sso configure --idp-metadata ./idp-metadata.xml ``` ### Step 5: Test login Verify SSO login works correctly. ```bash cloudflow sso test ``` ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            },
            {
              "doc_id": "howto_set_up_ci_cd_pipeline",
              "chunk_id": "howto_set_up_ci_cd_pipeline_fix_0",
              "content": "# How to Set Up CI/CD Pipeline ## Prerequisites - GitHub account - CloudFlow API key - Repository admin access ## Steps ### Step 1: Create GitHub Actions workflow Create a new file at `.github/workflows/cloudflow.yml` with the deployment configuration. ```bash mkdir -p .github/workflows && touch .github/workflows/cloudflow.yml ``` ### Step 2: Configure secrets Add your CloudFlow API key to GitHub secrets. ```bash gh secret set CLOUDFLOW_API_KEY ``` ### Step 3: Add deployment step Add the CloudFlow deployment action to your workflow. > **Note:** Use the official cloudflow/deploy-action@v2 ### Step 4: Test the pipeline Push a commit to trigger the workflow. ```bash git push origin main ``` ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            },
            {
              "doc_id": "howto_set_up_database_connection",
              "chunk_id": "howto_set_up_database_connection_fix_0",
              "content": "# How to Set Up Database Connection ## Prerequisites - Database with network access to CloudFlow - Connection credentials ## Steps ### Step 1: Add connection secret Store your database connection string as a secret. ```bash cloudflow secrets set DATABASE_URL postgresql://user:pass@host:5432/db ``` ### Step 2: Configure connection pool Set pool size based on your plan limits. Free tier allows 5 connections, Pro allows 20. > **Note:** Pool exhaustion will queue requests ### Step 3: Test connection Verify the database is accessible from CloudFlow. ```bash cloudflow db test --secret DATABASE_URL ``` ### Step 4: Enable SSL For production, always require SSL connections. ```bash cloudflow db configure --require-ssl ``` ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            },
            {
              "doc_id": "howto_set_up_data_encryption",
              "chunk_id": "howto_set_up_data_encryption_fix_0",
              "content": "# How to Set Up Data Encryption ## Prerequisites - CloudFlow Enterprise plan ## Steps ### Step 1: Enable encryption at rest All data is encrypted at rest using AES-256 by default. > **Note:** Encryption is automatic for all plans ### Step 2: Configure KMS Bring your own encryption keys using AWS KMS or GCP KMS. ```bash cloudflow encryption configure --kms-key arn:aws:kms:... ``` ### Step 3: Enable field-level encryption Encrypt sensitive fields within your workflow data. ```bash cloudflow encryption add-field --path $.user.ssn --key-alias sensitive-data ``` ### Step 4: Verify encryption Confirm encryption is active for your workspace. ```bash cloudflow encryption status ``` ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            }
          ],
          "latency_ms": 4.057025000292924,
          "query_id": "howto_017",
          "dimension": "synonym",
          "query": "How do I set up my own domain name for CloudFlow?"
        },
        {
          "key_facts": [
            "CNAME",
            "ingress.cloudflow.io",
            "Let's Encrypt"
          ],
          "found_facts": [
            "CNAME",
            "ingress.cloudflow.io",
            "Let's Encrypt"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "howto_configure_custom_domain"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "howto_configure_custom_domain",
              "chunk_id": "howto_configure_custom_domain_fix_0",
              "content": "# How to Configure Custom Domain ## Prerequisites - Domain with DNS access - CloudFlow Pro plan or higher ## Steps ### Step 1: Verify domain ownership Add a TXT record to verify domain ownership. ```bash cloudflow domains verify --domain app.example.com ``` ### Step 2: Configure DNS Add a CNAME record pointing to your CloudFlow endpoint. > **Note:** CNAME should point to ingress.cloudflow.io ### Step 3: Enable SSL CloudFlow automatically provisions SSL certificates via Let's Encrypt. ```bash cloudflow domains ssl --domain app.example.com ``` ### Step 4: Set as primary Make this the primary domain for your application. ```bash cloudflow domains set-primary --domain app.example.com ``` ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            },
            {
              "doc_id": "howto_configure_single_sign_on",
              "chunk_id": "howto_configure_single_sign_on_fix_0",
              "content": "# How to Configure Single Sign-On ## Prerequisites - Admin access - Enterprise plan - Identity provider access ## Steps ### Step 1: Choose provider CloudFlow supports SAML 2.0, OpenID Connect, and OAuth 2.0 providers. ### Step 2: Get metadata Download CloudFlow's SAML metadata for your IdP configuration. ```bash cloudflow sso metadata --format xml > cloudflow-metadata.xml ``` ### Step 3: Configure IdP Add CloudFlow as a service provider in your identity provider. > **Note:** ACS URL: https://auth.cloudflow.io/saml/callback ### Step 4: Upload IdP metadata Provide your IdP's metadata to CloudFlow. ```bash cloudflow sso configure --idp-metadata ./idp-metadata.xml ``` ### Step 5: Test login Verify SSO login works correctly. ```bash cloudflow sso test ``` ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            },
            {
              "doc_id": "api_secrets",
              "chunk_id": "api_secrets_fix_0",
              "content": "# Secrets API Reference ## Overview The Secrets API provides programmatic access to secrets functionality. Base URL: `https://api.cloudflow.io/v1/secrets` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /secrets List secret names (values not returned). **Parameters:** **Response:** - Success: `200` - Returns array of secret metadata - Error: `400` - Bad Request **Example:** ```json {\"secrets\": [{\"name\": \"DATABASE_URL\", \"created_at\": \"2024-01-01\", \"updated_at\": \"2024-01-10\"}]} ``` ### PUT /secrets/{name} Create or update a secret. **Parameters:** - `value` (string): Secret value (encrypted at rest) **Response:** - Success: `200` - Secret saved - Error: `400` - Bad Request **Example:** ```json {\"name\": \"DATABASE_URL\", \"updated_at\": \"2024-01-15T10:00:00Z\"} ``` ### DELETE /secrets/{name} Delete a secret. **Parameters:** **Response:** - Success: `204` - Secret deleted - Error: `400` - Bad Request **Example:** ```json {} ```",
              "score": null
            },
            {
              "doc_id": "howto_set_up_ci_cd_pipeline",
              "chunk_id": "howto_set_up_ci_cd_pipeline_fix_0",
              "content": "# How to Set Up CI/CD Pipeline ## Prerequisites - GitHub account - CloudFlow API key - Repository admin access ## Steps ### Step 1: Create GitHub Actions workflow Create a new file at `.github/workflows/cloudflow.yml` with the deployment configuration. ```bash mkdir -p .github/workflows && touch .github/workflows/cloudflow.yml ``` ### Step 2: Configure secrets Add your CloudFlow API key to GitHub secrets. ```bash gh secret set CLOUDFLOW_API_KEY ``` ### Step 3: Add deployment step Add the CloudFlow deployment action to your workflow. > **Note:** Use the official cloudflow/deploy-action@v2 ### Step 4: Test the pipeline Push a commit to trigger the workflow. ```bash git push origin main ``` ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            },
            {
              "doc_id": "api_integrations",
              "chunk_id": "api_integrations_fix_0",
              "content": "# Integrations API Reference ## Overview The Integrations API provides programmatic access to integrations functionality. Base URL: `https://api.cloudflow.io/v1/integrations` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /integrations List available integrations. **Parameters:** - `category` (string): Filter by category: storage, notification, database **Response:** - Success: `200` - Returns array of integration objects - Error: `400` - Bad Request **Example:** ```json {\"integrations\": [{\"id\": \"int_s3\", \"name\": \"Amazon S3\", \"category\": \"storage\", \"status\": \"connected\"}]} ``` ### POST /integrations/{id}/connect Connect an integration. **Parameters:** - `credentials` (object): Integration-specific credentials **Response:** - Success: `200` - Integration connected - Error: `400` - Bad Request **Example:** ```json {\"id\": \"int_s3\", \"status\": \"connected\", \"connected_at\": \"2024-01-15T10:00:00Z\"} ```",
              "score": null
            }
          ],
          "latency_ms": 4.800780001460225,
          "query_id": "howto_017",
          "dimension": "problem",
          "query": "I want to use app.mycompany.com instead of the default CloudFlow URL"
        },
        {
          "key_facts": [
            "CNAME",
            "ingress.cloudflow.io",
            "Let's Encrypt"
          ],
          "found_facts": [
            "CNAME",
            "ingress.cloudflow.io",
            "Let's Encrypt"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "howto_configure_custom_domain"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "howto_configure_custom_domain",
              "chunk_id": "howto_configure_custom_domain_fix_0",
              "content": "# How to Configure Custom Domain ## Prerequisites - Domain with DNS access - CloudFlow Pro plan or higher ## Steps ### Step 1: Verify domain ownership Add a TXT record to verify domain ownership. ```bash cloudflow domains verify --domain app.example.com ``` ### Step 2: Configure DNS Add a CNAME record pointing to your CloudFlow endpoint. > **Note:** CNAME should point to ingress.cloudflow.io ### Step 3: Enable SSL CloudFlow automatically provisions SSL certificates via Let's Encrypt. ```bash cloudflow domains ssl --domain app.example.com ``` ### Step 4: Set as primary Make this the primary domain for your application. ```bash cloudflow domains set-primary --domain app.example.com ``` ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            },
            {
              "doc_id": "howto_set_up_ci_cd_pipeline",
              "chunk_id": "howto_set_up_ci_cd_pipeline_fix_0",
              "content": "# How to Set Up CI/CD Pipeline ## Prerequisites - GitHub account - CloudFlow API key - Repository admin access ## Steps ### Step 1: Create GitHub Actions workflow Create a new file at `.github/workflows/cloudflow.yml` with the deployment configuration. ```bash mkdir -p .github/workflows && touch .github/workflows/cloudflow.yml ``` ### Step 2: Configure secrets Add your CloudFlow API key to GitHub secrets. ```bash gh secret set CLOUDFLOW_API_KEY ``` ### Step 3: Add deployment step Add the CloudFlow deployment action to your workflow. > **Note:** Use the official cloudflow/deploy-action@v2 ### Step 4: Test the pipeline Push a commit to trigger the workflow. ```bash git push origin main ``` ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            },
            {
              "doc_id": "runbook_ssl_certificate_expiry",
              "chunk_id": "runbook_ssl_certificate_expiry_fix_0",
              "content": "# Runbook: SSL Certificate Expiry ## Severity **Level:** P3 - Medium **Response Time:** 15 minutes ## Symptoms - Certificate expiring within 7 days - SSL handshake failures - Browser security warnings ## Diagnosis ### Step 1 Check certificate expiry: `echo | openssl s_client -connect api.cloudflow.io:443 2>/dev/null | openssl x509 -noout -dates` ### Step 2 Verify cert-manager status: `kubectl get certificates -n production` ### Step 3 Check cert-manager logs: `kubectl logs -l app=cert-manager -n cert-manager --tail=50` ### Step 4 Verify DNS is correct: `dig +short api.cloudflow.io` ## Resolution ### Step 1 Trigger manual renewal: `kubectl delete certificate api-cert -n production && kubectl apply -f certificate.yaml` ### Step 2 Check rate limits: Let's Encrypt has 50 certs/domain/week limit ### Step 3 Use staging issuer for testing: switch to letsencrypt-staging ClusterIssuer ### Step 4 If Let's Encrypt fails, manually upload certificate ## Escalation If certificate cannot be renewed, escalate to security team immediately. Consider using backup certificate. ## Post-Incident - Document timeline in incident log - Schedule post-mortem within 48 hours - Update runbook if new patterns discovered",
              "score": null
            },
            {
              "doc_id": "howto_configure_single_sign_on",
              "chunk_id": "howto_configure_single_sign_on_fix_0",
              "content": "# How to Configure Single Sign-On ## Prerequisites - Admin access - Enterprise plan - Identity provider access ## Steps ### Step 1: Choose provider CloudFlow supports SAML 2.0, OpenID Connect, and OAuth 2.0 providers. ### Step 2: Get metadata Download CloudFlow's SAML metadata for your IdP configuration. ```bash cloudflow sso metadata --format xml > cloudflow-metadata.xml ``` ### Step 3: Configure IdP Add CloudFlow as a service provider in your identity provider. > **Note:** ACS URL: https://auth.cloudflow.io/saml/callback ### Step 4: Upload IdP metadata Provide your IdP's metadata to CloudFlow. ```bash cloudflow sso configure --idp-metadata ./idp-metadata.xml ``` ### Step 5: Test login Verify SSO login works correctly. ```bash cloudflow sso test ``` ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            },
            {
              "doc_id": "howto_set_up_data_encryption",
              "chunk_id": "howto_set_up_data_encryption_fix_0",
              "content": "# How to Set Up Data Encryption ## Prerequisites - CloudFlow Enterprise plan ## Steps ### Step 1: Enable encryption at rest All data is encrypted at rest using AES-256 by default. > **Note:** Encryption is automatic for all plans ### Step 2: Configure KMS Bring your own encryption keys using AWS KMS or GCP KMS. ```bash cloudflow encryption configure --kms-key arn:aws:kms:... ``` ### Step 3: Enable field-level encryption Encrypt sensitive fields within your workflow data. ```bash cloudflow encryption add-field --path $.user.ssn --key-alias sensitive-data ``` ### Step 4: Verify encryption Confirm encryption is active for your workspace. ```bash cloudflow encryption status ``` ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            }
          ],
          "latency_ms": 6.163078000099631,
          "query_id": "howto_017",
          "dimension": "casual",
          "query": "custom domain dns setup cloudflow"
        },
        {
          "key_facts": [
            "CNAME",
            "ingress.cloudflow.io",
            "Let's Encrypt"
          ],
          "found_facts": [
            "CNAME",
            "ingress.cloudflow.io",
            "Let's Encrypt"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "howto_configure_custom_domain"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "howto_configure_custom_domain",
              "chunk_id": "howto_configure_custom_domain_fix_0",
              "content": "# How to Configure Custom Domain ## Prerequisites - Domain with DNS access - CloudFlow Pro plan or higher ## Steps ### Step 1: Verify domain ownership Add a TXT record to verify domain ownership. ```bash cloudflow domains verify --domain app.example.com ``` ### Step 2: Configure DNS Add a CNAME record pointing to your CloudFlow endpoint. > **Note:** CNAME should point to ingress.cloudflow.io ### Step 3: Enable SSL CloudFlow automatically provisions SSL certificates via Let's Encrypt. ```bash cloudflow domains ssl --domain app.example.com ``` ### Step 4: Set as primary Make this the primary domain for your application. ```bash cloudflow domains set-primary --domain app.example.com ``` ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            },
            {
              "doc_id": "runbook_ssl_certificate_expiry",
              "chunk_id": "runbook_ssl_certificate_expiry_fix_0",
              "content": "# Runbook: SSL Certificate Expiry ## Severity **Level:** P3 - Medium **Response Time:** 15 minutes ## Symptoms - Certificate expiring within 7 days - SSL handshake failures - Browser security warnings ## Diagnosis ### Step 1 Check certificate expiry: `echo | openssl s_client -connect api.cloudflow.io:443 2>/dev/null | openssl x509 -noout -dates` ### Step 2 Verify cert-manager status: `kubectl get certificates -n production` ### Step 3 Check cert-manager logs: `kubectl logs -l app=cert-manager -n cert-manager --tail=50` ### Step 4 Verify DNS is correct: `dig +short api.cloudflow.io` ## Resolution ### Step 1 Trigger manual renewal: `kubectl delete certificate api-cert -n production && kubectl apply -f certificate.yaml` ### Step 2 Check rate limits: Let's Encrypt has 50 certs/domain/week limit ### Step 3 Use staging issuer for testing: switch to letsencrypt-staging ClusterIssuer ### Step 4 If Let's Encrypt fails, manually upload certificate ## Escalation If certificate cannot be renewed, escalate to security team immediately. Consider using backup certificate. ## Post-Incident - Document timeline in incident log - Schedule post-mortem within 48 hours - Update runbook if new patterns discovered",
              "score": null
            },
            {
              "doc_id": "howto_configure_single_sign_on",
              "chunk_id": "howto_configure_single_sign_on_fix_0",
              "content": "# How to Configure Single Sign-On ## Prerequisites - Admin access - Enterprise plan - Identity provider access ## Steps ### Step 1: Choose provider CloudFlow supports SAML 2.0, OpenID Connect, and OAuth 2.0 providers. ### Step 2: Get metadata Download CloudFlow's SAML metadata for your IdP configuration. ```bash cloudflow sso metadata --format xml > cloudflow-metadata.xml ``` ### Step 3: Configure IdP Add CloudFlow as a service provider in your identity provider. > **Note:** ACS URL: https://auth.cloudflow.io/saml/callback ### Step 4: Upload IdP metadata Provide your IdP's metadata to CloudFlow. ```bash cloudflow sso configure --idp-metadata ./idp-metadata.xml ``` ### Step 5: Test login Verify SSO login works correctly. ```bash cloudflow sso test ``` ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            },
            {
              "doc_id": "howto_set_up_data_encryption",
              "chunk_id": "howto_set_up_data_encryption_fix_0",
              "content": "# How to Set Up Data Encryption ## Prerequisites - CloudFlow Enterprise plan ## Steps ### Step 1: Enable encryption at rest All data is encrypted at rest using AES-256 by default. > **Note:** Encryption is automatic for all plans ### Step 2: Configure KMS Bring your own encryption keys using AWS KMS or GCP KMS. ```bash cloudflow encryption configure --kms-key arn:aws:kms:... ``` ### Step 3: Enable field-level encryption Encrypt sensitive fields within your workflow data. ```bash cloudflow encryption add-field --path $.user.ssn --key-alias sensitive-data ``` ### Step 4: Verify encryption Confirm encryption is active for your workspace. ```bash cloudflow encryption status ``` ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            },
            {
              "doc_id": "howto_set_up_database_connection",
              "chunk_id": "howto_set_up_database_connection_fix_0",
              "content": "# How to Set Up Database Connection ## Prerequisites - Database with network access to CloudFlow - Connection credentials ## Steps ### Step 1: Add connection secret Store your database connection string as a secret. ```bash cloudflow secrets set DATABASE_URL postgresql://user:pass@host:5432/db ``` ### Step 2: Configure connection pool Set pool size based on your plan limits. Free tier allows 5 connections, Pro allows 20. > **Note:** Pool exhaustion will queue requests ### Step 3: Test connection Verify the database is accessible from CloudFlow. ```bash cloudflow db test --secret DATABASE_URL ``` ### Step 4: Enable SSL For production, always require SSL connections. ```bash cloudflow db configure --require-ssl ``` ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            }
          ],
          "latency_ms": 3.767746000448824,
          "query_id": "howto_017",
          "dimension": "contextual",
          "query": "launching our app publicly and need to point our company domain to CloudFlow with SSL"
        },
        {
          "key_facts": [
            "CNAME",
            "ingress.cloudflow.io",
            "Let's Encrypt"
          ],
          "found_facts": [
            "CNAME",
            "ingress.cloudflow.io",
            "Let's Encrypt"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "howto_configure_custom_domain"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "howto_configure_custom_domain",
              "chunk_id": "howto_configure_custom_domain_fix_0",
              "content": "# How to Configure Custom Domain ## Prerequisites - Domain with DNS access - CloudFlow Pro plan or higher ## Steps ### Step 1: Verify domain ownership Add a TXT record to verify domain ownership. ```bash cloudflow domains verify --domain app.example.com ``` ### Step 2: Configure DNS Add a CNAME record pointing to your CloudFlow endpoint. > **Note:** CNAME should point to ingress.cloudflow.io ### Step 3: Enable SSL CloudFlow automatically provisions SSL certificates via Let's Encrypt. ```bash cloudflow domains ssl --domain app.example.com ``` ### Step 4: Set as primary Make this the primary domain for your application. ```bash cloudflow domains set-primary --domain app.example.com ``` ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            },
            {
              "doc_id": "howto_configure_single_sign_on",
              "chunk_id": "howto_configure_single_sign_on_fix_0",
              "content": "# How to Configure Single Sign-On ## Prerequisites - Admin access - Enterprise plan - Identity provider access ## Steps ### Step 1: Choose provider CloudFlow supports SAML 2.0, OpenID Connect, and OAuth 2.0 providers. ### Step 2: Get metadata Download CloudFlow's SAML metadata for your IdP configuration. ```bash cloudflow sso metadata --format xml > cloudflow-metadata.xml ``` ### Step 3: Configure IdP Add CloudFlow as a service provider in your identity provider. > **Note:** ACS URL: https://auth.cloudflow.io/saml/callback ### Step 4: Upload IdP metadata Provide your IdP's metadata to CloudFlow. ```bash cloudflow sso configure --idp-metadata ./idp-metadata.xml ``` ### Step 5: Test login Verify SSO login works correctly. ```bash cloudflow sso test ``` ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            },
            {
              "doc_id": "runbook_ssl_certificate_expiry",
              "chunk_id": "runbook_ssl_certificate_expiry_fix_0",
              "content": "# Runbook: SSL Certificate Expiry ## Severity **Level:** P3 - Medium **Response Time:** 15 minutes ## Symptoms - Certificate expiring within 7 days - SSL handshake failures - Browser security warnings ## Diagnosis ### Step 1 Check certificate expiry: `echo | openssl s_client -connect api.cloudflow.io:443 2>/dev/null | openssl x509 -noout -dates` ### Step 2 Verify cert-manager status: `kubectl get certificates -n production` ### Step 3 Check cert-manager logs: `kubectl logs -l app=cert-manager -n cert-manager --tail=50` ### Step 4 Verify DNS is correct: `dig +short api.cloudflow.io` ## Resolution ### Step 1 Trigger manual renewal: `kubectl delete certificate api-cert -n production && kubectl apply -f certificate.yaml` ### Step 2 Check rate limits: Let's Encrypt has 50 certs/domain/week limit ### Step 3 Use staging issuer for testing: switch to letsencrypt-staging ClusterIssuer ### Step 4 If Let's Encrypt fails, manually upload certificate ## Escalation If certificate cannot be renewed, escalate to security team immediately. Consider using backup certificate. ## Post-Incident - Document timeline in incident log - Schedule post-mortem within 48 hours - Update runbook if new patterns discovered",
              "score": null
            },
            {
              "doc_id": "api_secrets",
              "chunk_id": "api_secrets_fix_0",
              "content": "# Secrets API Reference ## Overview The Secrets API provides programmatic access to secrets functionality. Base URL: `https://api.cloudflow.io/v1/secrets` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /secrets List secret names (values not returned). **Parameters:** **Response:** - Success: `200` - Returns array of secret metadata - Error: `400` - Bad Request **Example:** ```json {\"secrets\": [{\"name\": \"DATABASE_URL\", \"created_at\": \"2024-01-01\", \"updated_at\": \"2024-01-10\"}]} ``` ### PUT /secrets/{name} Create or update a secret. **Parameters:** - `value` (string): Secret value (encrypted at rest) **Response:** - Success: `200` - Secret saved - Error: `400` - Bad Request **Example:** ```json {\"name\": \"DATABASE_URL\", \"updated_at\": \"2024-01-15T10:00:00Z\"} ``` ### DELETE /secrets/{name} Delete a secret. **Parameters:** **Response:** - Success: `204` - Secret deleted - Error: `400` - Bad Request **Example:** ```json {} ```",
              "score": null
            },
            {
              "doc_id": "howto_set_up_database_connection",
              "chunk_id": "howto_set_up_database_connection_fix_0",
              "content": "# How to Set Up Database Connection ## Prerequisites - Database with network access to CloudFlow - Connection credentials ## Steps ### Step 1: Add connection secret Store your database connection string as a secret. ```bash cloudflow secrets set DATABASE_URL postgresql://user:pass@host:5432/db ``` ### Step 2: Configure connection pool Set pool size based on your plan limits. Free tier allows 5 connections, Pro allows 20. > **Note:** Pool exhaustion will queue requests ### Step 3: Test connection Verify the database is accessible from CloudFlow. ```bash cloudflow db test --secret DATABASE_URL ``` ### Step 4: Enable SSL For production, always require SSL connections. ```bash cloudflow db configure --require-ssl ``` ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            }
          ],
          "latency_ms": 3.3250020005652914,
          "query_id": "howto_017",
          "dimension": "negation",
          "query": "why can't I access my CloudFlow app through my own domain?"
        },
        {
          "key_facts": [
            "retry",
            "exponential backoff",
            "on_failure"
          ],
          "found_facts": [
            "retry",
            "exponential backoff",
            "on_failure"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "howto_implement_retry_logic"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "howto_implement_retry_logic",
              "chunk_id": "howto_implement_retry_logic_fix_0",
              "content": "# How to Implement Retry Logic ## Prerequisites - Existing workflow ## Steps ### Step 1: Define retry policy Add retry configuration to your workflow step. > **Note:** Maximum 5 retry attempts allowed ### Step 2: Configure backoff Set exponential backoff with base delay of 1 second. The formula is: delay = base * 2^attempt. ### Step 3: Add timeout Set maximum execution time to prevent infinite loops. Default timeout is 30 seconds. ### Step 4: Handle final failure Configure on_failure handler to notify or escalate when all retries are exhausted. ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            },
            {
              "doc_id": "adr_005",
              "chunk_id": "adr_005_fix_0",
              "content": "# ADR-005: Adopt Event-Driven Architecture for Workflow Execution **Status:** Accepted **Date:** 2024-12-04 ## Context We need an architecture that supports asynchronous workflow execution, step dependencies, and failure recovery. ## Options Considered ### Option 1: Event-driven with Kafka Publish-subscribe messaging. **Pros:** - Loose coupling - Scalability - Event replay - Audit trail **Cons:** - Eventual consistency - Operational complexity - Message ordering challenges ### Option 2: Request-response Synchronous API calls. **Pros:** - Simple mental model - Immediate feedback - Easy debugging **Cons:** - Tight coupling - Cascading failures - Timeout issues ### Option 3: Polling-based Workers poll for tasks. **Pros:** - Simple implementation - Easy rate limiting - No message broker **Cons:** - Polling overhead - Higher latency - Inefficient at scale ## Decision We will use an event-driven architecture with Kafka for workflow step execution. ## Consequences - Implement idempotent consumers - Use dead letter queues for failed messages - Monitor consumer lag",
              "score": null
            },
            {
              "doc_id": "arch_workflow_engine",
              "chunk_id": "arch_workflow_engine_fix_0",
              "content": "# Workflow Engine Architecture ## Overview The Workflow Engine is the core component responsible for executing user-defined workflows. It uses an event-driven architecture with support for parallel execution, conditional branching, and automatic retries. ## Design Principles - **Idempotency**: All operations can be safely retried without side effects - **Eventual Consistency**: State converges to consistent view within 5 seconds - **Fault Tolerance**: System continues operating despite component failures ## Components ### Scheduler Manages workflow scheduling and triggers. Supports cron expressions, webhooks, and event-based triggers. **Technology:** Custom Go service **Scaling:** Horizontal with leader election ### Executor Runs individual workflow steps in isolated containers. Maximum execution time is 30 minutes per step. **Technology:** Kubernetes Jobs **Scaling:** Auto-scaling based on queue depth ### State Store Persists workflow state, execution history, and step results. Uses optimistic locking for concurrent updates. **Technology:** PostgreSQL with JSONB **Scaling:** Primary-replica with read replicas ## Data Flow Triggers \u2192 Scheduler \u2192 Job Queue \u2192 Executor \u2192 State Store \u2192 Webhooks/Notifications ## Performance Characteristics - **Latency P50:** 45ms - **Latency P99:** 250ms - **Throughput:** 5000 executions/minute",
              "score": null
            },
            {
              "doc_id": "config_workflow_engine",
              "chunk_id": "config_workflow_engine_fix_0",
              "content": "# Workflow Engine Configuration Reference ## Overview This document describes all configuration options for Workflow Engine. ## Environment Variables | Variable | Type | Default | Description | |----------|------|---------|-------------| | `WORKFLOW_MAX_STEPS` | integer | 100 | Maximum steps per workflow | | `WORKFLOW_TIMEOUT_SECONDS` | integer | 3600 | Maximum workflow execution time | | `WORKFLOW_RETRY_LIMIT` | integer | 3 | Default retry attempts for failed steps | | `WORKFLOW_CONCURRENT_LIMIT` | integer | 10 | Maximum concurrent executions per workflow | | `EXECUTOR_POOL_SIZE` | integer | 50 | Number of executor workers | | `EXECUTOR_MEMORY_LIMIT` | string | 512Mi | Memory limit per executor | ## Configuration File Configuration can also be provided via `workflow engine.yaml`: ```yaml workflow_max_steps: 100 workflow_timeout_seconds: 3600 workflow_retry_limit: 3 workflow_concurrent_limit: 10 executor_pool_size: 50 ``` ## Validation Configuration is validated at startup. Invalid configuration will prevent the service from starting.",
              "score": null
            },
            {
              "doc_id": "runbook_database_connection_exhaustion",
              "chunk_id": "runbook_database_connection_exhaustion_fix_0",
              "content": "# Runbook: Database Connection Exhaustion ## Severity **Level:** P1 - Critical **Response Time:** 15 minutes ## Symptoms - Connection pool at 100% - Queries timing out - 'too many connections' errors in logs ## Diagnosis ### Step 1 Check active connections: `SELECT count(*) FROM pg_stat_activity` ### Step 2 Identify long-running queries: `SELECT pid, now() - pg_stat_activity.query_start AS duration, query FROM pg_stat_activity WHERE state != 'idle' ORDER BY duration DESC` ### Step 3 Check PgBouncer stats: `SHOW POOLS` ### Step 4 Verify connection limits: `SHOW max_connections` ## Resolution ### Step 1 Kill long-running queries: `SELECT pg_terminate_backend(pid) FROM pg_stat_activity WHERE duration > interval '5 minutes'` ### Step 2 Increase PgBouncer pool size temporarily: update pgbouncer.ini ### Step 3 Restart affected services to release connections: `kubectl rollout restart deployment/api` ### Step 4 Scale down non-critical services to free connections ## Escalation If connections remain exhausted, escalate to DBA. Consider enabling connection queueing. ## Post-Incident - Document timeline in incident log - Schedule post-mortem within 48 hours - Update runbook if new patterns discovered",
              "score": null
            }
          ],
          "latency_ms": 4.112437998628593,
          "query_id": "howto_018",
          "dimension": "original",
          "query": "How do I implement retry logic?"
        },
        {
          "key_facts": [
            "retry",
            "exponential backoff",
            "on_failure"
          ],
          "found_facts": [
            "retry",
            "exponential backoff",
            "on_failure"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "howto_implement_retry_logic"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "howto_implement_retry_logic",
              "chunk_id": "howto_implement_retry_logic_fix_0",
              "content": "# How to Implement Retry Logic ## Prerequisites - Existing workflow ## Steps ### Step 1: Define retry policy Add retry configuration to your workflow step. > **Note:** Maximum 5 retry attempts allowed ### Step 2: Configure backoff Set exponential backoff with base delay of 1 second. The formula is: delay = base * 2^attempt. ### Step 3: Add timeout Set maximum execution time to prevent infinite loops. Default timeout is 30 seconds. ### Step 4: Handle final failure Configure on_failure handler to notify or escalate when all retries are exhausted. ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            },
            {
              "doc_id": "howto_implement_workflow_versioning",
              "chunk_id": "howto_implement_workflow_versioning_fix_0",
              "content": "# How to Implement Workflow Versioning ## Prerequisites - Existing workflow ## Steps ### Step 1: Enable versioning Turn on versioning for your workflow. This is required for the feature. ```bash cloudflow workflows update wf_123 --enable-versioning ``` ### Step 2: Create new version Save changes as a new version instead of overwriting. ```bash cloudflow workflows publish wf_123 --version 2.0.0 ``` ### Step 3: Set active version Choose which version handles new executions. ```bash cloudflow workflows activate wf_123 --version 2.0.0 ``` ### Step 4: Roll back if needed Quickly revert to a previous version. ```bash cloudflow workflows rollback wf_123 --to-version 1.0.0 ``` ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            },
            {
              "doc_id": "howto_create_scheduled_workflow",
              "chunk_id": "howto_create_scheduled_workflow_fix_0",
              "content": "# How to Create Scheduled Workflow ## Prerequisites - Workflow definition ## Steps ### Step 1: Define schedule Use cron expression to set the schedule. Example: '0 9 * * MON-FRI' runs at 9 AM on weekdays. ### Step 2: Set timezone Specify timezone for the schedule. Default is UTC. > **Note:** Use IANA timezone names like America/New_York ### Step 3: Configure overlap handling Choose behavior when previous execution is still running: skip, queue, or cancel. ### Step 4: Enable notifications Get notified on schedule failures. ```bash cloudflow workflows notify --on-failure ``` ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            },
            {
              "doc_id": "arch_workflow_engine",
              "chunk_id": "arch_workflow_engine_fix_0",
              "content": "# Workflow Engine Architecture ## Overview The Workflow Engine is the core component responsible for executing user-defined workflows. It uses an event-driven architecture with support for parallel execution, conditional branching, and automatic retries. ## Design Principles - **Idempotency**: All operations can be safely retried without side effects - **Eventual Consistency**: State converges to consistent view within 5 seconds - **Fault Tolerance**: System continues operating despite component failures ## Components ### Scheduler Manages workflow scheduling and triggers. Supports cron expressions, webhooks, and event-based triggers. **Technology:** Custom Go service **Scaling:** Horizontal with leader election ### Executor Runs individual workflow steps in isolated containers. Maximum execution time is 30 minutes per step. **Technology:** Kubernetes Jobs **Scaling:** Auto-scaling based on queue depth ### State Store Persists workflow state, execution history, and step results. Uses optimistic locking for concurrent updates. **Technology:** PostgreSQL with JSONB **Scaling:** Primary-replica with read replicas ## Data Flow Triggers \u2192 Scheduler \u2192 Job Queue \u2192 Executor \u2192 State Store \u2192 Webhooks/Notifications ## Performance Characteristics - **Latency P50:** 45ms - **Latency P99:** 250ms - **Throughput:** 5000 executions/minute",
              "score": null
            },
            {
              "doc_id": "config_workflow_engine",
              "chunk_id": "config_workflow_engine_fix_0",
              "content": "# Workflow Engine Configuration Reference ## Overview This document describes all configuration options for Workflow Engine. ## Environment Variables | Variable | Type | Default | Description | |----------|------|---------|-------------| | `WORKFLOW_MAX_STEPS` | integer | 100 | Maximum steps per workflow | | `WORKFLOW_TIMEOUT_SECONDS` | integer | 3600 | Maximum workflow execution time | | `WORKFLOW_RETRY_LIMIT` | integer | 3 | Default retry attempts for failed steps | | `WORKFLOW_CONCURRENT_LIMIT` | integer | 10 | Maximum concurrent executions per workflow | | `EXECUTOR_POOL_SIZE` | integer | 50 | Number of executor workers | | `EXECUTOR_MEMORY_LIMIT` | string | 512Mi | Memory limit per executor | ## Configuration File Configuration can also be provided via `workflow engine.yaml`: ```yaml workflow_max_steps: 100 workflow_timeout_seconds: 3600 workflow_retry_limit: 3 workflow_concurrent_limit: 10 executor_pool_size: 50 ``` ## Validation Configuration is validated at startup. Invalid configuration will prevent the service from starting.",
              "score": null
            }
          ],
          "latency_ms": 4.558409000310348,
          "query_id": "howto_018",
          "dimension": "synonym",
          "query": "How do I add automatic retries to my workflow steps?"
        },
        {
          "key_facts": [
            "retry",
            "exponential backoff",
            "on_failure"
          ],
          "found_facts": [
            "retry",
            "exponential backoff",
            "on_failure"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "howto_implement_retry_logic"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "howto_implement_retry_logic",
              "chunk_id": "howto_implement_retry_logic_fix_0",
              "content": "# How to Implement Retry Logic ## Prerequisites - Existing workflow ## Steps ### Step 1: Define retry policy Add retry configuration to your workflow step. > **Note:** Maximum 5 retry attempts allowed ### Step 2: Configure backoff Set exponential backoff with base delay of 1 second. The formula is: delay = base * 2^attempt. ### Step 3: Add timeout Set maximum execution time to prevent infinite loops. Default timeout is 30 seconds. ### Step 4: Handle final failure Configure on_failure handler to notify or escalate when all retries are exhausted. ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            },
            {
              "doc_id": "howto_debug_failed_execution",
              "chunk_id": "howto_debug_failed_execution_fix_0",
              "content": "# How to Debug Failed Execution ## Prerequisites - Failed execution ID ## Steps ### Step 1: Get execution details Retrieve the full execution record including step outputs. ```bash cloudflow executions get ex_123 --include-steps ``` ### Step 2: View step logs Check logs for the failed step. ```bash cloudflow executions logs ex_123 --step 3 ``` ### Step 3: Inspect input/output Review the data passed between steps. ```bash cloudflow executions inspect ex_123 --step 3 ``` ### Step 4: Enable debug mode Re-run with verbose logging enabled. ```bash cloudflow workflows execute wf_123 --debug ``` ### Step 5: Check error patterns Look for similar failures in recent executions. ```bash cloudflow executions list --status failed --since 24h ``` ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            },
            {
              "doc_id": "api_workflows",
              "chunk_id": "api_workflows_fix_0",
              "content": "# Workflows API Reference ## Overview The Workflows API provides programmatic access to workflows functionality. Base URL: `https://api.cloudflow.io/v1/workflows` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /workflows List all workflows for the authenticated user. **Parameters:** - `limit` (integer): Maximum number of results (default: 50, max: 200) - `status` (string): Filter by status: active, paused, archived **Response:** - Success: `200` - Returns array of workflow objects - Error: `400` - Bad Request **Example:** ```json {\"workflows\": [{\"id\": \"wf_123\", \"name\": \"Daily Report\", \"status\": \"active\"}]} ``` ### POST /workflows Create a new workflow. **Parameters:** - `name` (string): Workflow name (required) - `definition` (object): Workflow definition in JSON format **Response:** - Success: `201` - Workflow created successfully - Error: `400` - Bad Request **Example:** ```json {\"id\": \"wf_456\", \"name\": \"New Workflow\", \"created_at\": \"2024-01-15T10:00:00Z\"} ``` ### DELETE /workflows/{id} Delete a workflow permanently. **Parameters:** - `id` (string): Workflow ID (required) **Response:** - Success: `204` - Workflow deleted - Error: `404` - Workflow not found **Example:** ```json {} ```",
              "score": null
            },
            {
              "doc_id": "release_3_2_0",
              "chunk_id": "release_3_2_0_fix_0",
              "content": "# Release Notes - v3.2.0 **Release Date:** 2024-01-15 ## New Features - Workflow versioning with rollback support - New Python SDK with async support - Bulk operations API for workflows - Custom retry policies per step ## Bug Fixes - Fixed race condition in concurrent execution handler - Resolved memory leak in long-running workflows - Fixed timezone handling in scheduled triggers ## Breaking Changes - **BREAKING:** API v1 endpoints deprecated, will be removed in v4.0 - **BREAKING:** Minimum Python version now 3.9 ## Upgrade Instructions ```bash cloudflow upgrade --version 3.2.0 ``` ## Known Issues None at this time.",
              "score": null
            },
            {
              "doc_id": "howto_create_scheduled_workflow",
              "chunk_id": "howto_create_scheduled_workflow_fix_0",
              "content": "# How to Create Scheduled Workflow ## Prerequisites - Workflow definition ## Steps ### Step 1: Define schedule Use cron expression to set the schedule. Example: '0 9 * * MON-FRI' runs at 9 AM on weekdays. ### Step 2: Set timezone Specify timezone for the schedule. Default is UTC. > **Note:** Use IANA timezone names like America/New_York ### Step 3: Configure overlap handling Choose behavior when previous execution is still running: skip, queue, or cancel. ### Step 4: Enable notifications Get notified on schedule failures. ```bash cloudflow workflows notify --on-failure ``` ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            }
          ],
          "latency_ms": 3.5437390015431447,
          "query_id": "howto_018",
          "dimension": "problem",
          "query": "my workflow steps sometimes fail due to transient errors and I want them to automatically try again"
        },
        {
          "key_facts": [
            "retry",
            "exponential backoff",
            "on_failure"
          ],
          "found_facts": [
            "retry",
            "exponential backoff",
            "on_failure"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "howto_implement_retry_logic"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "howto_implement_retry_logic",
              "chunk_id": "howto_implement_retry_logic_fix_0",
              "content": "# How to Implement Retry Logic ## Prerequisites - Existing workflow ## Steps ### Step 1: Define retry policy Add retry configuration to your workflow step. > **Note:** Maximum 5 retry attempts allowed ### Step 2: Configure backoff Set exponential backoff with base delay of 1 second. The formula is: delay = base * 2^attempt. ### Step 3: Add timeout Set maximum execution time to prevent infinite loops. Default timeout is 30 seconds. ### Step 4: Handle final failure Configure on_failure handler to notify or escalate when all retries are exhausted. ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            },
            {
              "doc_id": "config_workflow_engine",
              "chunk_id": "config_workflow_engine_fix_0",
              "content": "# Workflow Engine Configuration Reference ## Overview This document describes all configuration options for Workflow Engine. ## Environment Variables | Variable | Type | Default | Description | |----------|------|---------|-------------| | `WORKFLOW_MAX_STEPS` | integer | 100 | Maximum steps per workflow | | `WORKFLOW_TIMEOUT_SECONDS` | integer | 3600 | Maximum workflow execution time | | `WORKFLOW_RETRY_LIMIT` | integer | 3 | Default retry attempts for failed steps | | `WORKFLOW_CONCURRENT_LIMIT` | integer | 10 | Maximum concurrent executions per workflow | | `EXECUTOR_POOL_SIZE` | integer | 50 | Number of executor workers | | `EXECUTOR_MEMORY_LIMIT` | string | 512Mi | Memory limit per executor | ## Configuration File Configuration can also be provided via `workflow engine.yaml`: ```yaml workflow_max_steps: 100 workflow_timeout_seconds: 3600 workflow_retry_limit: 3 workflow_concurrent_limit: 10 executor_pool_size: 50 ``` ## Validation Configuration is validated at startup. Invalid configuration will prevent the service from starting.",
              "score": null
            },
            {
              "doc_id": "release_3_2_0",
              "chunk_id": "release_3_2_0_fix_0",
              "content": "# Release Notes - v3.2.0 **Release Date:** 2024-01-15 ## New Features - Workflow versioning with rollback support - New Python SDK with async support - Bulk operations API for workflows - Custom retry policies per step ## Bug Fixes - Fixed race condition in concurrent execution handler - Resolved memory leak in long-running workflows - Fixed timezone handling in scheduled triggers ## Breaking Changes - **BREAKING:** API v1 endpoints deprecated, will be removed in v4.0 - **BREAKING:** Minimum Python version now 3.9 ## Upgrade Instructions ```bash cloudflow upgrade --version 3.2.0 ``` ## Known Issues None at this time.",
              "score": null
            },
            {
              "doc_id": "howto_debug_failed_execution",
              "chunk_id": "howto_debug_failed_execution_fix_0",
              "content": "# How to Debug Failed Execution ## Prerequisites - Failed execution ID ## Steps ### Step 1: Get execution details Retrieve the full execution record including step outputs. ```bash cloudflow executions get ex_123 --include-steps ``` ### Step 2: View step logs Check logs for the failed step. ```bash cloudflow executions logs ex_123 --step 3 ``` ### Step 3: Inspect input/output Review the data passed between steps. ```bash cloudflow executions inspect ex_123 --step 3 ``` ### Step 4: Enable debug mode Re-run with verbose logging enabled. ```bash cloudflow workflows execute wf_123 --debug ``` ### Step 5: Check error patterns Look for similar failures in recent executions. ```bash cloudflow executions list --status failed --since 24h ``` ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            },
            {
              "doc_id": "howto_implement_workflow_versioning",
              "chunk_id": "howto_implement_workflow_versioning_fix_0",
              "content": "# How to Implement Workflow Versioning ## Prerequisites - Existing workflow ## Steps ### Step 1: Enable versioning Turn on versioning for your workflow. This is required for the feature. ```bash cloudflow workflows update wf_123 --enable-versioning ``` ### Step 2: Create new version Save changes as a new version instead of overwriting. ```bash cloudflow workflows publish wf_123 --version 2.0.0 ``` ### Step 3: Set active version Choose which version handles new executions. ```bash cloudflow workflows activate wf_123 --version 2.0.0 ``` ### Step 4: Roll back if needed Quickly revert to a previous version. ```bash cloudflow workflows rollback wf_123 --to-version 1.0.0 ``` ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            }
          ],
          "latency_ms": 3.2166099990718067,
          "query_id": "howto_018",
          "dimension": "casual",
          "query": "workflow retry backoff config"
        },
        {
          "key_facts": [
            "retry",
            "exponential backoff",
            "on_failure"
          ],
          "found_facts": [
            "retry",
            "exponential backoff",
            "on_failure"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "howto_implement_retry_logic"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "howto_implement_retry_logic",
              "chunk_id": "howto_implement_retry_logic_fix_0",
              "content": "# How to Implement Retry Logic ## Prerequisites - Existing workflow ## Steps ### Step 1: Define retry policy Add retry configuration to your workflow step. > **Note:** Maximum 5 retry attempts allowed ### Step 2: Configure backoff Set exponential backoff with base delay of 1 second. The formula is: delay = base * 2^attempt. ### Step 3: Add timeout Set maximum execution time to prevent infinite loops. Default timeout is 30 seconds. ### Step 4: Handle final failure Configure on_failure handler to notify or escalate when all retries are exhausted. ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            },
            {
              "doc_id": "adr_005",
              "chunk_id": "adr_005_fix_0",
              "content": "# ADR-005: Adopt Event-Driven Architecture for Workflow Execution **Status:** Accepted **Date:** 2024-12-04 ## Context We need an architecture that supports asynchronous workflow execution, step dependencies, and failure recovery. ## Options Considered ### Option 1: Event-driven with Kafka Publish-subscribe messaging. **Pros:** - Loose coupling - Scalability - Event replay - Audit trail **Cons:** - Eventual consistency - Operational complexity - Message ordering challenges ### Option 2: Request-response Synchronous API calls. **Pros:** - Simple mental model - Immediate feedback - Easy debugging **Cons:** - Tight coupling - Cascading failures - Timeout issues ### Option 3: Polling-based Workers poll for tasks. **Pros:** - Simple implementation - Easy rate limiting - No message broker **Cons:** - Polling overhead - Higher latency - Inefficient at scale ## Decision We will use an event-driven architecture with Kafka for workflow step execution. ## Consequences - Implement idempotent consumers - Use dead letter queues for failed messages - Monitor consumer lag",
              "score": null
            },
            {
              "doc_id": "api_workflows",
              "chunk_id": "api_workflows_fix_0",
              "content": "# Workflows API Reference ## Overview The Workflows API provides programmatic access to workflows functionality. Base URL: `https://api.cloudflow.io/v1/workflows` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /workflows List all workflows for the authenticated user. **Parameters:** - `limit` (integer): Maximum number of results (default: 50, max: 200) - `status` (string): Filter by status: active, paused, archived **Response:** - Success: `200` - Returns array of workflow objects - Error: `400` - Bad Request **Example:** ```json {\"workflows\": [{\"id\": \"wf_123\", \"name\": \"Daily Report\", \"status\": \"active\"}]} ``` ### POST /workflows Create a new workflow. **Parameters:** - `name` (string): Workflow name (required) - `definition` (object): Workflow definition in JSON format **Response:** - Success: `201` - Workflow created successfully - Error: `400` - Bad Request **Example:** ```json {\"id\": \"wf_456\", \"name\": \"New Workflow\", \"created_at\": \"2024-01-15T10:00:00Z\"} ``` ### DELETE /workflows/{id} Delete a workflow permanently. **Parameters:** - `id` (string): Workflow ID (required) **Response:** - Success: `204` - Workflow deleted - Error: `404` - Workflow not found **Example:** ```json {} ```",
              "score": null
            },
            {
              "doc_id": "release_3_2_0",
              "chunk_id": "release_3_2_0_fix_0",
              "content": "# Release Notes - v3.2.0 **Release Date:** 2024-01-15 ## New Features - Workflow versioning with rollback support - New Python SDK with async support - Bulk operations API for workflows - Custom retry policies per step ## Bug Fixes - Fixed race condition in concurrent execution handler - Resolved memory leak in long-running workflows - Fixed timezone handling in scheduled triggers ## Breaking Changes - **BREAKING:** API v1 endpoints deprecated, will be removed in v4.0 - **BREAKING:** Minimum Python version now 3.9 ## Upgrade Instructions ```bash cloudflow upgrade --version 3.2.0 ``` ## Known Issues None at this time.",
              "score": null
            },
            {
              "doc_id": "api_executions",
              "chunk_id": "api_executions_fix_0",
              "content": "# Executions API Reference ## Overview The Executions API provides programmatic access to executions functionality. Base URL: `https://api.cloudflow.io/v1/executions` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /executions List workflow executions. **Parameters:** - `workflow_id` (string): Filter by workflow ID - `since` (datetime): Filter executions after this timestamp **Response:** - Success: `200` - Returns array of execution objects - Error: `400` - Bad Request **Example:** ```json {\"executions\": [{\"id\": \"ex_789\", \"status\": \"completed\", \"duration_ms\": 1523}]} ``` ### POST /executions/{id}/cancel Cancel a running execution. **Parameters:** - `id` (string): Execution ID - `reason` (string): Cancellation reason (optional) **Response:** - Success: `200` - Execution cancelled - Error: `400` - Bad Request **Example:** ```json {\"id\": \"ex_789\", \"status\": \"cancelled\", \"cancelled_at\": \"2024-01-15T10:05:00Z\"} ```",
              "score": null
            }
          ],
          "latency_ms": 3.6792310002056183,
          "query_id": "howto_018",
          "dimension": "contextual",
          "query": "building a workflow that calls an external API which sometimes times out, need to handle temporary failures gracefully"
        },
        {
          "key_facts": [
            "retry",
            "exponential backoff",
            "on_failure"
          ],
          "found_facts": [
            "retry",
            "exponential backoff",
            "on_failure"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "howto_implement_retry_logic"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "howto_implement_retry_logic",
              "chunk_id": "howto_implement_retry_logic_fix_0",
              "content": "# How to Implement Retry Logic ## Prerequisites - Existing workflow ## Steps ### Step 1: Define retry policy Add retry configuration to your workflow step. > **Note:** Maximum 5 retry attempts allowed ### Step 2: Configure backoff Set exponential backoff with base delay of 1 second. The formula is: delay = base * 2^attempt. ### Step 3: Add timeout Set maximum execution time to prevent infinite loops. Default timeout is 30 seconds. ### Step 4: Handle final failure Configure on_failure handler to notify or escalate when all retries are exhausted. ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            },
            {
              "doc_id": "release_3_2_0",
              "chunk_id": "release_3_2_0_fix_0",
              "content": "# Release Notes - v3.2.0 **Release Date:** 2024-01-15 ## New Features - Workflow versioning with rollback support - New Python SDK with async support - Bulk operations API for workflows - Custom retry policies per step ## Bug Fixes - Fixed race condition in concurrent execution handler - Resolved memory leak in long-running workflows - Fixed timezone handling in scheduled triggers ## Breaking Changes - **BREAKING:** API v1 endpoints deprecated, will be removed in v4.0 - **BREAKING:** Minimum Python version now 3.9 ## Upgrade Instructions ```bash cloudflow upgrade --version 3.2.0 ``` ## Known Issues None at this time.",
              "score": null
            },
            {
              "doc_id": "howto_debug_failed_execution",
              "chunk_id": "howto_debug_failed_execution_fix_0",
              "content": "# How to Debug Failed Execution ## Prerequisites - Failed execution ID ## Steps ### Step 1: Get execution details Retrieve the full execution record including step outputs. ```bash cloudflow executions get ex_123 --include-steps ``` ### Step 2: View step logs Check logs for the failed step. ```bash cloudflow executions logs ex_123 --step 3 ``` ### Step 3: Inspect input/output Review the data passed between steps. ```bash cloudflow executions inspect ex_123 --step 3 ``` ### Step 4: Enable debug mode Re-run with verbose logging enabled. ```bash cloudflow workflows execute wf_123 --debug ``` ### Step 5: Check error patterns Look for similar failures in recent executions. ```bash cloudflow executions list --status failed --since 24h ``` ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            },
            {
              "doc_id": "api_workflows",
              "chunk_id": "api_workflows_fix_0",
              "content": "# Workflows API Reference ## Overview The Workflows API provides programmatic access to workflows functionality. Base URL: `https://api.cloudflow.io/v1/workflows` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /workflows List all workflows for the authenticated user. **Parameters:** - `limit` (integer): Maximum number of results (default: 50, max: 200) - `status` (string): Filter by status: active, paused, archived **Response:** - Success: `200` - Returns array of workflow objects - Error: `400` - Bad Request **Example:** ```json {\"workflows\": [{\"id\": \"wf_123\", \"name\": \"Daily Report\", \"status\": \"active\"}]} ``` ### POST /workflows Create a new workflow. **Parameters:** - `name` (string): Workflow name (required) - `definition` (object): Workflow definition in JSON format **Response:** - Success: `201` - Workflow created successfully - Error: `400` - Bad Request **Example:** ```json {\"id\": \"wf_456\", \"name\": \"New Workflow\", \"created_at\": \"2024-01-15T10:00:00Z\"} ``` ### DELETE /workflows/{id} Delete a workflow permanently. **Parameters:** - `id` (string): Workflow ID (required) **Response:** - Success: `204` - Workflow deleted - Error: `404` - Workflow not found **Example:** ```json {} ```",
              "score": null
            },
            {
              "doc_id": "api_metrics",
              "chunk_id": "api_metrics_fix_0",
              "content": "# Metrics API Reference ## Overview The Metrics API provides programmatic access to metrics functionality. Base URL: `https://api.cloudflow.io/v1/metrics` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /metrics/workflows Get workflow performance metrics. **Parameters:** - `period` (string): Time period: hour, day, week, month - `workflow_id` (string): Filter by workflow ID **Response:** - Success: `200` - Returns metrics data - Error: `400` - Bad Request **Example:** ```json {\"success_rate\": 0.985, \"avg_duration_ms\": 1250, \"p99_duration_ms\": 5000, \"total_executions\": 10000} ```",
              "score": null
            }
          ],
          "latency_ms": 3.7419180007418618,
          "query_id": "howto_018",
          "dimension": "negation",
          "query": "why does my workflow fail permanently instead of retrying when a step errors?"
        },
        {
          "key_facts": [
            "DATABASE_URL",
            "cloudflow secrets set",
            "require-ssl"
          ],
          "found_facts": [
            "DATABASE_URL",
            "cloudflow secrets set",
            "require-ssl"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "howto_set_up_database_connecti"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "config_database",
              "chunk_id": "config_database_fix_0",
              "content": "# Database Configuration Reference ## Overview This document describes all configuration options for Database. ## Environment Variables | Variable | Type | Default | Description | |----------|------|---------|-------------| | `DATABASE_URL` | string | required | PostgreSQL connection string | | `DATABASE_POOL_MIN` | integer | 5 | Minimum connection pool size | | `DATABASE_POOL_MAX` | integer | 20 | Maximum connection pool size | | `DATABASE_STATEMENT_TIMEOUT` | integer | 30000 | Query timeout in milliseconds | | `DATABASE_SSL_MODE` | string | require | SSL mode: disable, require, verify-full | | `DATABASE_MIGRATION_AUTO` | boolean | false | Run migrations on startup | ## Configuration File Configuration can also be provided via `database.yaml`: ```yaml database_url: required database_pool_min: 5 database_pool_max: 20 database_statement_timeout: 30000 database_ssl_mode: require ``` ## Validation Configuration is validated at startup. Invalid configuration will prevent the service from starting.",
              "score": null
            },
            {
              "doc_id": "howto_set_up_database_connection",
              "chunk_id": "howto_set_up_database_connection_fix_0",
              "content": "# How to Set Up Database Connection ## Prerequisites - Database with network access to CloudFlow - Connection credentials ## Steps ### Step 1: Add connection secret Store your database connection string as a secret. ```bash cloudflow secrets set DATABASE_URL postgresql://user:pass@host:5432/db ``` ### Step 2: Configure connection pool Set pool size based on your plan limits. Free tier allows 5 connections, Pro allows 20. > **Note:** Pool exhaustion will queue requests ### Step 3: Test connection Verify the database is accessible from CloudFlow. ```bash cloudflow db test --secret DATABASE_URL ``` ### Step 4: Enable SSL For production, always require SSL connections. ```bash cloudflow db configure --require-ssl ``` ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            },
            {
              "doc_id": "adr_001",
              "chunk_id": "adr_001_fix_0",
              "content": "# ADR-001: Use PostgreSQL for Primary Database **Status:** Accepted **Date:** 2024-11-04 ## Context We need a database to store workflow definitions, execution state, and user data. The database must support ACID transactions, JSON queries, and scale to millions of records. ## Options Considered ### Option 1: PostgreSQL Mature relational database with JSONB support. **Pros:** - ACID compliance - Rich JSONB queries - Team expertise - Proven at scale **Cons:** - Manual sharding for extreme scale - Requires connection pooling ### Option 2: MongoDB Document database with native JSON. **Pros:** - Flexible schema - Built-in sharding - Good for documents **Cons:** - Weaker consistency guarantees - Complex transactions - Higher operational cost ### Option 3: CockroachDB Distributed SQL database. **Pros:** - Automatic sharding - Strong consistency - PostgreSQL compatible **Cons:** - Higher latency - More expensive - Less mature ecosystem ## Decision We will use PostgreSQL as our primary database with JSONB for flexible workflow definitions. ## Consequences - Need PgBouncer for connection pooling - Must plan for sharding at 100M+ records - Requires regular VACUUM maintenance",
              "score": null
            },
            {
              "doc_id": "runbook_database_connection_exhaustion",
              "chunk_id": "runbook_database_connection_exhaustion_fix_0",
              "content": "# Runbook: Database Connection Exhaustion ## Severity **Level:** P1 - Critical **Response Time:** 15 minutes ## Symptoms - Connection pool at 100% - Queries timing out - 'too many connections' errors in logs ## Diagnosis ### Step 1 Check active connections: `SELECT count(*) FROM pg_stat_activity` ### Step 2 Identify long-running queries: `SELECT pid, now() - pg_stat_activity.query_start AS duration, query FROM pg_stat_activity WHERE state != 'idle' ORDER BY duration DESC` ### Step 3 Check PgBouncer stats: `SHOW POOLS` ### Step 4 Verify connection limits: `SHOW max_connections` ## Resolution ### Step 1 Kill long-running queries: `SELECT pg_terminate_backend(pid) FROM pg_stat_activity WHERE duration > interval '5 minutes'` ### Step 2 Increase PgBouncer pool size temporarily: update pgbouncer.ini ### Step 3 Restart affected services to release connections: `kubectl rollout restart deployment/api` ### Step 4 Scale down non-critical services to free connections ## Escalation If connections remain exhausted, escalate to DBA. Consider enabling connection queueing. ## Post-Incident - Document timeline in incident log - Schedule post-mortem within 48 hours - Update runbook if new patterns discovered",
              "score": null
            },
            {
              "doc_id": "config_api_gateway",
              "chunk_id": "config_api_gateway_fix_0",
              "content": "# API Gateway Configuration Reference ## Overview This document describes all configuration options for API Gateway. ## Environment Variables | Variable | Type | Default | Description | |----------|------|---------|-------------| | `GATEWAY_PORT` | integer | 8000 | Gateway listen port | | `GATEWAY_RATE_LIMIT` | integer | 100 | Requests per minute per user | | `GATEWAY_TIMEOUT_MS` | integer | 30000 | Request timeout in milliseconds | | `GATEWAY_MAX_BODY_SIZE` | string | 10MB | Maximum request body size | | `GATEWAY_CORS_ORIGINS` | string | * | Allowed CORS origins | | `GATEWAY_SSL_ENABLED` | boolean | true | Enable TLS termination | ## Configuration File Configuration can also be provided via `api gateway.yaml`: ```yaml gateway_port: 8000 gateway_rate_limit: 100 gateway_timeout_ms: 30000 gateway_max_body_size: 10MB gateway_cors_origins: * ``` ## Validation Configuration is validated at startup. Invalid configuration will prevent the service from starting.",
              "score": null
            }
          ],
          "latency_ms": 3.729215000930708,
          "query_id": "howto_019",
          "dimension": "original",
          "query": "How do I set up database connection?"
        },
        {
          "key_facts": [
            "DATABASE_URL",
            "cloudflow secrets set",
            "require-ssl"
          ],
          "found_facts": [
            "DATABASE_URL",
            "cloudflow secrets set",
            "require-ssl"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "howto_set_up_database_connecti"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "howto_set_up_database_connection",
              "chunk_id": "howto_set_up_database_connection_fix_0",
              "content": "# How to Set Up Database Connection ## Prerequisites - Database with network access to CloudFlow - Connection credentials ## Steps ### Step 1: Add connection secret Store your database connection string as a secret. ```bash cloudflow secrets set DATABASE_URL postgresql://user:pass@host:5432/db ``` ### Step 2: Configure connection pool Set pool size based on your plan limits. Free tier allows 5 connections, Pro allows 20. > **Note:** Pool exhaustion will queue requests ### Step 3: Test connection Verify the database is accessible from CloudFlow. ```bash cloudflow db test --secret DATABASE_URL ``` ### Step 4: Enable SSL For production, always require SSL connections. ```bash cloudflow db configure --require-ssl ``` ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            },
            {
              "doc_id": "howto_configure_custom_domain",
              "chunk_id": "howto_configure_custom_domain_fix_0",
              "content": "# How to Configure Custom Domain ## Prerequisites - Domain with DNS access - CloudFlow Pro plan or higher ## Steps ### Step 1: Verify domain ownership Add a TXT record to verify domain ownership. ```bash cloudflow domains verify --domain app.example.com ``` ### Step 2: Configure DNS Add a CNAME record pointing to your CloudFlow endpoint. > **Note:** CNAME should point to ingress.cloudflow.io ### Step 3: Enable SSL CloudFlow automatically provisions SSL certificates via Let's Encrypt. ```bash cloudflow domains ssl --domain app.example.com ``` ### Step 4: Set as primary Make this the primary domain for your application. ```bash cloudflow domains set-primary --domain app.example.com ``` ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            },
            {
              "doc_id": "api_secrets",
              "chunk_id": "api_secrets_fix_0",
              "content": "# Secrets API Reference ## Overview The Secrets API provides programmatic access to secrets functionality. Base URL: `https://api.cloudflow.io/v1/secrets` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /secrets List secret names (values not returned). **Parameters:** **Response:** - Success: `200` - Returns array of secret metadata - Error: `400` - Bad Request **Example:** ```json {\"secrets\": [{\"name\": \"DATABASE_URL\", \"created_at\": \"2024-01-01\", \"updated_at\": \"2024-01-10\"}]} ``` ### PUT /secrets/{name} Create or update a secret. **Parameters:** - `value` (string): Secret value (encrypted at rest) **Response:** - Success: `200` - Secret saved - Error: `400` - Bad Request **Example:** ```json {\"name\": \"DATABASE_URL\", \"updated_at\": \"2024-01-15T10:00:00Z\"} ``` ### DELETE /secrets/{name} Delete a secret. **Parameters:** **Response:** - Success: `204` - Secret deleted - Error: `400` - Bad Request **Example:** ```json {} ```",
              "score": null
            },
            {
              "doc_id": "api_integrations",
              "chunk_id": "api_integrations_fix_0",
              "content": "# Integrations API Reference ## Overview The Integrations API provides programmatic access to integrations functionality. Base URL: `https://api.cloudflow.io/v1/integrations` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /integrations List available integrations. **Parameters:** - `category` (string): Filter by category: storage, notification, database **Response:** - Success: `200` - Returns array of integration objects - Error: `400` - Bad Request **Example:** ```json {\"integrations\": [{\"id\": \"int_s3\", \"name\": \"Amazon S3\", \"category\": \"storage\", \"status\": \"connected\"}]} ``` ### POST /integrations/{id}/connect Connect an integration. **Parameters:** - `credentials` (object): Integration-specific credentials **Response:** - Success: `200` - Integration connected - Error: `400` - Bad Request **Example:** ```json {\"id\": \"int_s3\", \"status\": \"connected\", \"connected_at\": \"2024-01-15T10:00:00Z\"} ```",
              "score": null
            },
            {
              "doc_id": "howto_set_up_ci_cd_pipeline",
              "chunk_id": "howto_set_up_ci_cd_pipeline_fix_0",
              "content": "# How to Set Up CI/CD Pipeline ## Prerequisites - GitHub account - CloudFlow API key - Repository admin access ## Steps ### Step 1: Create GitHub Actions workflow Create a new file at `.github/workflows/cloudflow.yml` with the deployment configuration. ```bash mkdir -p .github/workflows && touch .github/workflows/cloudflow.yml ``` ### Step 2: Configure secrets Add your CloudFlow API key to GitHub secrets. ```bash gh secret set CLOUDFLOW_API_KEY ``` ### Step 3: Add deployment step Add the CloudFlow deployment action to your workflow. > **Note:** Use the official cloudflow/deploy-action@v2 ### Step 4: Test the pipeline Push a commit to trigger the workflow. ```bash git push origin main ``` ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            }
          ],
          "latency_ms": 3.5855779988196446,
          "query_id": "howto_019",
          "dimension": "synonym",
          "query": "How do I connect CloudFlow to my database?"
        },
        {
          "key_facts": [
            "DATABASE_URL",
            "cloudflow secrets set",
            "require-ssl"
          ],
          "found_facts": [
            "DATABASE_URL",
            "cloudflow secrets set",
            "require-ssl"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "howto_set_up_database_connecti"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "howto_set_up_database_connection",
              "chunk_id": "howto_set_up_database_connection_fix_0",
              "content": "# How to Set Up Database Connection ## Prerequisites - Database with network access to CloudFlow - Connection credentials ## Steps ### Step 1: Add connection secret Store your database connection string as a secret. ```bash cloudflow secrets set DATABASE_URL postgresql://user:pass@host:5432/db ``` ### Step 2: Configure connection pool Set pool size based on your plan limits. Free tier allows 5 connections, Pro allows 20. > **Note:** Pool exhaustion will queue requests ### Step 3: Test connection Verify the database is accessible from CloudFlow. ```bash cloudflow db test --secret DATABASE_URL ``` ### Step 4: Enable SSL For production, always require SSL connections. ```bash cloudflow db configure --require-ssl ``` ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            },
            {
              "doc_id": "howto_set_up_data_encryption",
              "chunk_id": "howto_set_up_data_encryption_fix_0",
              "content": "# How to Set Up Data Encryption ## Prerequisites - CloudFlow Enterprise plan ## Steps ### Step 1: Enable encryption at rest All data is encrypted at rest using AES-256 by default. > **Note:** Encryption is automatic for all plans ### Step 2: Configure KMS Bring your own encryption keys using AWS KMS or GCP KMS. ```bash cloudflow encryption configure --kms-key arn:aws:kms:... ``` ### Step 3: Enable field-level encryption Encrypt sensitive fields within your workflow data. ```bash cloudflow encryption add-field --path $.user.ssn --key-alias sensitive-data ``` ### Step 4: Verify encryption Confirm encryption is active for your workspace. ```bash cloudflow encryption status ``` ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            },
            {
              "doc_id": "howto_set_up_ci_cd_pipeline",
              "chunk_id": "howto_set_up_ci_cd_pipeline_fix_0",
              "content": "# How to Set Up CI/CD Pipeline ## Prerequisites - GitHub account - CloudFlow API key - Repository admin access ## Steps ### Step 1: Create GitHub Actions workflow Create a new file at `.github/workflows/cloudflow.yml` with the deployment configuration. ```bash mkdir -p .github/workflows && touch .github/workflows/cloudflow.yml ``` ### Step 2: Configure secrets Add your CloudFlow API key to GitHub secrets. ```bash gh secret set CLOUDFLOW_API_KEY ``` ### Step 3: Add deployment step Add the CloudFlow deployment action to your workflow. > **Note:** Use the official cloudflow/deploy-action@v2 ### Step 4: Test the pipeline Push a commit to trigger the workflow. ```bash git push origin main ``` ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            },
            {
              "doc_id": "howto_create_api_integration",
              "chunk_id": "howto_create_api_integration_fix_0",
              "content": "# How to Create API Integration ## Prerequisites - CloudFlow account ## Steps ### Step 1: Generate API key Create an API key for programmatic access. ```bash cloudflow apikeys create --name 'CI Integration' --scopes workflows:read,workflows:execute ``` ### Step 2: Store key securely Never commit API keys to source control. Use environment variables or secret management. > **Note:** Rotate keys every 90 days ### Step 3: Test authentication Verify the API key works correctly. ```bash curl -H 'Authorization: Bearer cf_...' https://api.cloudflow.io/v1/me ``` ### Step 4: Implement rate limiting Handle rate limit responses (HTTP 429) with exponential backoff. > **Note:** Default limit is 100 requests per minute ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            },
            {
              "doc_id": "api_secrets",
              "chunk_id": "api_secrets_fix_0",
              "content": "# Secrets API Reference ## Overview The Secrets API provides programmatic access to secrets functionality. Base URL: `https://api.cloudflow.io/v1/secrets` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /secrets List secret names (values not returned). **Parameters:** **Response:** - Success: `200` - Returns array of secret metadata - Error: `400` - Bad Request **Example:** ```json {\"secrets\": [{\"name\": \"DATABASE_URL\", \"created_at\": \"2024-01-01\", \"updated_at\": \"2024-01-10\"}]} ``` ### PUT /secrets/{name} Create or update a secret. **Parameters:** - `value` (string): Secret value (encrypted at rest) **Response:** - Success: `200` - Secret saved - Error: `400` - Bad Request **Example:** ```json {\"name\": \"DATABASE_URL\", \"updated_at\": \"2024-01-15T10:00:00Z\"} ``` ### DELETE /secrets/{name} Delete a secret. **Parameters:** **Response:** - Success: `204` - Secret deleted - Error: `400` - Bad Request **Example:** ```json {} ```",
              "score": null
            }
          ],
          "latency_ms": 3.6176270004943945,
          "query_id": "howto_019",
          "dimension": "problem",
          "query": "I need my CloudFlow workflows to read and write data from my existing database"
        },
        {
          "key_facts": [
            "DATABASE_URL",
            "cloudflow secrets set",
            "require-ssl"
          ],
          "found_facts": [
            "DATABASE_URL",
            "cloudflow secrets set",
            "require-ssl"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "howto_set_up_database_connecti"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "howto_set_up_database_connection",
              "chunk_id": "howto_set_up_database_connection_fix_0",
              "content": "# How to Set Up Database Connection ## Prerequisites - Database with network access to CloudFlow - Connection credentials ## Steps ### Step 1: Add connection secret Store your database connection string as a secret. ```bash cloudflow secrets set DATABASE_URL postgresql://user:pass@host:5432/db ``` ### Step 2: Configure connection pool Set pool size based on your plan limits. Free tier allows 5 connections, Pro allows 20. > **Note:** Pool exhaustion will queue requests ### Step 3: Test connection Verify the database is accessible from CloudFlow. ```bash cloudflow db test --secret DATABASE_URL ``` ### Step 4: Enable SSL For production, always require SSL connections. ```bash cloudflow db configure --require-ssl ``` ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            },
            {
              "doc_id": "howto_configure_custom_domain",
              "chunk_id": "howto_configure_custom_domain_fix_0",
              "content": "# How to Configure Custom Domain ## Prerequisites - Domain with DNS access - CloudFlow Pro plan or higher ## Steps ### Step 1: Verify domain ownership Add a TXT record to verify domain ownership. ```bash cloudflow domains verify --domain app.example.com ``` ### Step 2: Configure DNS Add a CNAME record pointing to your CloudFlow endpoint. > **Note:** CNAME should point to ingress.cloudflow.io ### Step 3: Enable SSL CloudFlow automatically provisions SSL certificates via Let's Encrypt. ```bash cloudflow domains ssl --domain app.example.com ``` ### Step 4: Set as primary Make this the primary domain for your application. ```bash cloudflow domains set-primary --domain app.example.com ``` ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            },
            {
              "doc_id": "api_secrets",
              "chunk_id": "api_secrets_fix_0",
              "content": "# Secrets API Reference ## Overview The Secrets API provides programmatic access to secrets functionality. Base URL: `https://api.cloudflow.io/v1/secrets` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /secrets List secret names (values not returned). **Parameters:** **Response:** - Success: `200` - Returns array of secret metadata - Error: `400` - Bad Request **Example:** ```json {\"secrets\": [{\"name\": \"DATABASE_URL\", \"created_at\": \"2024-01-01\", \"updated_at\": \"2024-01-10\"}]} ``` ### PUT /secrets/{name} Create or update a secret. **Parameters:** - `value` (string): Secret value (encrypted at rest) **Response:** - Success: `200` - Secret saved - Error: `400` - Bad Request **Example:** ```json {\"name\": \"DATABASE_URL\", \"updated_at\": \"2024-01-15T10:00:00Z\"} ``` ### DELETE /secrets/{name} Delete a secret. **Parameters:** **Response:** - Success: `204` - Secret deleted - Error: `400` - Bad Request **Example:** ```json {} ```",
              "score": null
            },
            {
              "doc_id": "howto_configure_single_sign_on",
              "chunk_id": "howto_configure_single_sign_on_fix_0",
              "content": "# How to Configure Single Sign-On ## Prerequisites - Admin access - Enterprise plan - Identity provider access ## Steps ### Step 1: Choose provider CloudFlow supports SAML 2.0, OpenID Connect, and OAuth 2.0 providers. ### Step 2: Get metadata Download CloudFlow's SAML metadata for your IdP configuration. ```bash cloudflow sso metadata --format xml > cloudflow-metadata.xml ``` ### Step 3: Configure IdP Add CloudFlow as a service provider in your identity provider. > **Note:** ACS URL: https://auth.cloudflow.io/saml/callback ### Step 4: Upload IdP metadata Provide your IdP's metadata to CloudFlow. ```bash cloudflow sso configure --idp-metadata ./idp-metadata.xml ``` ### Step 5: Test login Verify SSO login works correctly. ```bash cloudflow sso test ``` ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            },
            {
              "doc_id": "api_integrations",
              "chunk_id": "api_integrations_fix_0",
              "content": "# Integrations API Reference ## Overview The Integrations API provides programmatic access to integrations functionality. Base URL: `https://api.cloudflow.io/v1/integrations` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /integrations List available integrations. **Parameters:** - `category` (string): Filter by category: storage, notification, database **Response:** - Success: `200` - Returns array of integration objects - Error: `400` - Bad Request **Example:** ```json {\"integrations\": [{\"id\": \"int_s3\", \"name\": \"Amazon S3\", \"category\": \"storage\", \"status\": \"connected\"}]} ``` ### POST /integrations/{id}/connect Connect an integration. **Parameters:** - `credentials` (object): Integration-specific credentials **Response:** - Success: `200` - Integration connected - Error: `400` - Bad Request **Example:** ```json {\"id\": \"int_s3\", \"status\": \"connected\", \"connected_at\": \"2024-01-15T10:00:00Z\"} ```",
              "score": null
            }
          ],
          "latency_ms": 4.250054000294767,
          "query_id": "howto_019",
          "dimension": "casual",
          "query": "cloudflow database connection string setup"
        },
        {
          "key_facts": [
            "DATABASE_URL",
            "cloudflow secrets set",
            "require-ssl"
          ],
          "found_facts": [
            "DATABASE_URL",
            "cloudflow secrets set",
            "require-ssl"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "howto_set_up_database_connecti"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "howto_set_up_database_connection",
              "chunk_id": "howto_set_up_database_connection_fix_0",
              "content": "# How to Set Up Database Connection ## Prerequisites - Database with network access to CloudFlow - Connection credentials ## Steps ### Step 1: Add connection secret Store your database connection string as a secret. ```bash cloudflow secrets set DATABASE_URL postgresql://user:pass@host:5432/db ``` ### Step 2: Configure connection pool Set pool size based on your plan limits. Free tier allows 5 connections, Pro allows 20. > **Note:** Pool exhaustion will queue requests ### Step 3: Test connection Verify the database is accessible from CloudFlow. ```bash cloudflow db test --secret DATABASE_URL ``` ### Step 4: Enable SSL For production, always require SSL connections. ```bash cloudflow db configure --require-ssl ``` ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            },
            {
              "doc_id": "howto_configure_custom_domain",
              "chunk_id": "howto_configure_custom_domain_fix_0",
              "content": "# How to Configure Custom Domain ## Prerequisites - Domain with DNS access - CloudFlow Pro plan or higher ## Steps ### Step 1: Verify domain ownership Add a TXT record to verify domain ownership. ```bash cloudflow domains verify --domain app.example.com ``` ### Step 2: Configure DNS Add a CNAME record pointing to your CloudFlow endpoint. > **Note:** CNAME should point to ingress.cloudflow.io ### Step 3: Enable SSL CloudFlow automatically provisions SSL certificates via Let's Encrypt. ```bash cloudflow domains ssl --domain app.example.com ``` ### Step 4: Set as primary Make this the primary domain for your application. ```bash cloudflow domains set-primary --domain app.example.com ``` ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            },
            {
              "doc_id": "howto_set_up_data_encryption",
              "chunk_id": "howto_set_up_data_encryption_fix_0",
              "content": "# How to Set Up Data Encryption ## Prerequisites - CloudFlow Enterprise plan ## Steps ### Step 1: Enable encryption at rest All data is encrypted at rest using AES-256 by default. > **Note:** Encryption is automatic for all plans ### Step 2: Configure KMS Bring your own encryption keys using AWS KMS or GCP KMS. ```bash cloudflow encryption configure --kms-key arn:aws:kms:... ``` ### Step 3: Enable field-level encryption Encrypt sensitive fields within your workflow data. ```bash cloudflow encryption add-field --path $.user.ssn --key-alias sensitive-data ``` ### Step 4: Verify encryption Confirm encryption is active for your workspace. ```bash cloudflow encryption status ``` ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            },
            {
              "doc_id": "howto_migrate_from_v1_to_v2_api",
              "chunk_id": "howto_migrate_from_v1_to_v2_api_fix_0",
              "content": "# How to Migrate from v1 to v2 API ## Prerequisites - Existing v1 integration - v2 API access enabled ## Steps ### Step 1: Review breaking changes The v2 API changes authentication from API keys to OAuth 2.0. Review the migration guide. ### Step 2: Update client library Upgrade to the latest SDK version. ```bash npm install @cloudflow/sdk@latest ``` ### Step 3: Update authentication Replace API key authentication with OAuth 2.0 client credentials flow. > **Note:** Generate new credentials in the dashboard ### Step 4: Test in staging Run your integration tests against the v2 staging endpoint. ```bash CLOUDFLOW_API_URL=https://api-v2-staging.cloudflow.io npm test ``` ### Step 5: Switch production Update production configuration to use v2 API. ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            },
            {
              "doc_id": "howto_create_api_integration",
              "chunk_id": "howto_create_api_integration_fix_0",
              "content": "# How to Create API Integration ## Prerequisites - CloudFlow account ## Steps ### Step 1: Generate API key Create an API key for programmatic access. ```bash cloudflow apikeys create --name 'CI Integration' --scopes workflows:read,workflows:execute ``` ### Step 2: Store key securely Never commit API keys to source control. Use environment variables or secret management. > **Note:** Rotate keys every 90 days ### Step 3: Test authentication Verify the API key works correctly. ```bash curl -H 'Authorization: Bearer cf_...' https://api.cloudflow.io/v1/me ``` ### Step 4: Implement rate limiting Handle rate limit responses (HTTP 429) with exponential backoff. > **Note:** Default limit is 100 requests per minute ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            }
          ],
          "latency_ms": 3.518452000207617,
          "query_id": "howto_019",
          "dimension": "contextual",
          "query": "migrating our app to CloudFlow and need to connect to our production database securely"
        },
        {
          "key_facts": [
            "DATABASE_URL",
            "cloudflow secrets set",
            "require-ssl"
          ],
          "found_facts": [
            "DATABASE_URL",
            "cloudflow secrets set",
            "require-ssl"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "howto_set_up_database_connecti"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "api_secrets",
              "chunk_id": "api_secrets_fix_0",
              "content": "# Secrets API Reference ## Overview The Secrets API provides programmatic access to secrets functionality. Base URL: `https://api.cloudflow.io/v1/secrets` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /secrets List secret names (values not returned). **Parameters:** **Response:** - Success: `200` - Returns array of secret metadata - Error: `400` - Bad Request **Example:** ```json {\"secrets\": [{\"name\": \"DATABASE_URL\", \"created_at\": \"2024-01-01\", \"updated_at\": \"2024-01-10\"}]} ``` ### PUT /secrets/{name} Create or update a secret. **Parameters:** - `value` (string): Secret value (encrypted at rest) **Response:** - Success: `200` - Secret saved - Error: `400` - Bad Request **Example:** ```json {\"name\": \"DATABASE_URL\", \"updated_at\": \"2024-01-15T10:00:00Z\"} ``` ### DELETE /secrets/{name} Delete a secret. **Parameters:** **Response:** - Success: `204` - Secret deleted - Error: `400` - Bad Request **Example:** ```json {} ```",
              "score": null
            },
            {
              "doc_id": "howto_set_up_database_connection",
              "chunk_id": "howto_set_up_database_connection_fix_0",
              "content": "# How to Set Up Database Connection ## Prerequisites - Database with network access to CloudFlow - Connection credentials ## Steps ### Step 1: Add connection secret Store your database connection string as a secret. ```bash cloudflow secrets set DATABASE_URL postgresql://user:pass@host:5432/db ``` ### Step 2: Configure connection pool Set pool size based on your plan limits. Free tier allows 5 connections, Pro allows 20. > **Note:** Pool exhaustion will queue requests ### Step 3: Test connection Verify the database is accessible from CloudFlow. ```bash cloudflow db test --secret DATABASE_URL ``` ### Step 4: Enable SSL For production, always require SSL connections. ```bash cloudflow db configure --require-ssl ``` ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            },
            {
              "doc_id": "api_workflows",
              "chunk_id": "api_workflows_fix_0",
              "content": "# Workflows API Reference ## Overview The Workflows API provides programmatic access to workflows functionality. Base URL: `https://api.cloudflow.io/v1/workflows` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /workflows List all workflows for the authenticated user. **Parameters:** - `limit` (integer): Maximum number of results (default: 50, max: 200) - `status` (string): Filter by status: active, paused, archived **Response:** - Success: `200` - Returns array of workflow objects - Error: `400` - Bad Request **Example:** ```json {\"workflows\": [{\"id\": \"wf_123\", \"name\": \"Daily Report\", \"status\": \"active\"}]} ``` ### POST /workflows Create a new workflow. **Parameters:** - `name` (string): Workflow name (required) - `definition` (object): Workflow definition in JSON format **Response:** - Success: `201` - Workflow created successfully - Error: `400` - Bad Request **Example:** ```json {\"id\": \"wf_456\", \"name\": \"New Workflow\", \"created_at\": \"2024-01-15T10:00:00Z\"} ``` ### DELETE /workflows/{id} Delete a workflow permanently. **Parameters:** - `id` (string): Workflow ID (required) **Response:** - Success: `204` - Workflow deleted - Error: `404` - Workflow not found **Example:** ```json {} ```",
              "score": null
            },
            {
              "doc_id": "api_integrations",
              "chunk_id": "api_integrations_fix_0",
              "content": "# Integrations API Reference ## Overview The Integrations API provides programmatic access to integrations functionality. Base URL: `https://api.cloudflow.io/v1/integrations` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /integrations List available integrations. **Parameters:** - `category` (string): Filter by category: storage, notification, database **Response:** - Success: `200` - Returns array of integration objects - Error: `400` - Bad Request **Example:** ```json {\"integrations\": [{\"id\": \"int_s3\", \"name\": \"Amazon S3\", \"category\": \"storage\", \"status\": \"connected\"}]} ``` ### POST /integrations/{id}/connect Connect an integration. **Parameters:** - `credentials` (object): Integration-specific credentials **Response:** - Success: `200` - Integration connected - Error: `400` - Bad Request **Example:** ```json {\"id\": \"int_s3\", \"status\": \"connected\", \"connected_at\": \"2024-01-15T10:00:00Z\"} ```",
              "score": null
            },
            {
              "doc_id": "api_users",
              "chunk_id": "api_users_fix_0",
              "content": "# Users API Reference ## Overview The Users API provides programmatic access to users functionality. Base URL: `https://api.cloudflow.io/v1/users` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /users/me Get current user profile. **Parameters:** **Response:** - Success: `200` - Returns user object - Error: `400` - Bad Request **Example:** ```json {\"id\": \"usr_001\", \"email\": \"user@example.com\", \"role\": \"admin\", \"created_at\": \"2023-06-01\"} ``` ### PATCH /users/me Update current user profile. **Parameters:** - `name` (string): Display name - `timezone` (string): Timezone (e.g., America/New_York) **Response:** - Success: `200` - User updated - Error: `400` - Bad Request **Example:** ```json {\"id\": \"usr_001\", \"name\": \"John Doe\", \"timezone\": \"America/New_York\"} ```",
              "score": null
            }
          ],
          "latency_ms": 3.666107000753982,
          "query_id": "howto_019",
          "dimension": "negation",
          "query": "why can't my workflow access my database even though I have the credentials?"
        },
        {
          "key_facts": [
            "cron",
            "0 9 * * MON-FRI",
            "timezone"
          ],
          "found_facts": [
            "cron",
            "0 9 * * MON-FRI",
            "timezone"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "howto_create_scheduled_workflo"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "howto_create_scheduled_workflow",
              "chunk_id": "howto_create_scheduled_workflow_fix_0",
              "content": "# How to Create Scheduled Workflow ## Prerequisites - Workflow definition ## Steps ### Step 1: Define schedule Use cron expression to set the schedule. Example: '0 9 * * MON-FRI' runs at 9 AM on weekdays. ### Step 2: Set timezone Specify timezone for the schedule. Default is UTC. > **Note:** Use IANA timezone names like America/New_York ### Step 3: Configure overlap handling Choose behavior when previous execution is still running: skip, queue, or cancel. ### Step 4: Enable notifications Get notified on schedule failures. ```bash cloudflow workflows notify --on-failure ``` ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            },
            {
              "doc_id": "arch_workflow_engine",
              "chunk_id": "arch_workflow_engine_fix_0",
              "content": "# Workflow Engine Architecture ## Overview The Workflow Engine is the core component responsible for executing user-defined workflows. It uses an event-driven architecture with support for parallel execution, conditional branching, and automatic retries. ## Design Principles - **Idempotency**: All operations can be safely retried without side effects - **Eventual Consistency**: State converges to consistent view within 5 seconds - **Fault Tolerance**: System continues operating despite component failures ## Components ### Scheduler Manages workflow scheduling and triggers. Supports cron expressions, webhooks, and event-based triggers. **Technology:** Custom Go service **Scaling:** Horizontal with leader election ### Executor Runs individual workflow steps in isolated containers. Maximum execution time is 30 minutes per step. **Technology:** Kubernetes Jobs **Scaling:** Auto-scaling based on queue depth ### State Store Persists workflow state, execution history, and step results. Uses optimistic locking for concurrent updates. **Technology:** PostgreSQL with JSONB **Scaling:** Primary-replica with read replicas ## Data Flow Triggers \u2192 Scheduler \u2192 Job Queue \u2192 Executor \u2192 State Store \u2192 Webhooks/Notifications ## Performance Characteristics - **Latency P50:** 45ms - **Latency P99:** 250ms - **Throughput:** 5000 executions/minute",
              "score": null
            },
            {
              "doc_id": "howto_set_up_ci_cd_pipeline",
              "chunk_id": "howto_set_up_ci_cd_pipeline_fix_0",
              "content": "# How to Set Up CI/CD Pipeline ## Prerequisites - GitHub account - CloudFlow API key - Repository admin access ## Steps ### Step 1: Create GitHub Actions workflow Create a new file at `.github/workflows/cloudflow.yml` with the deployment configuration. ```bash mkdir -p .github/workflows && touch .github/workflows/cloudflow.yml ``` ### Step 2: Configure secrets Add your CloudFlow API key to GitHub secrets. ```bash gh secret set CLOUDFLOW_API_KEY ``` ### Step 3: Add deployment step Add the CloudFlow deployment action to your workflow. > **Note:** Use the official cloudflow/deploy-action@v2 ### Step 4: Test the pipeline Push a commit to trigger the workflow. ```bash git push origin main ``` ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            },
            {
              "doc_id": "adr_005",
              "chunk_id": "adr_005_fix_0",
              "content": "# ADR-005: Adopt Event-Driven Architecture for Workflow Execution **Status:** Accepted **Date:** 2024-12-04 ## Context We need an architecture that supports asynchronous workflow execution, step dependencies, and failure recovery. ## Options Considered ### Option 1: Event-driven with Kafka Publish-subscribe messaging. **Pros:** - Loose coupling - Scalability - Event replay - Audit trail **Cons:** - Eventual consistency - Operational complexity - Message ordering challenges ### Option 2: Request-response Synchronous API calls. **Pros:** - Simple mental model - Immediate feedback - Easy debugging **Cons:** - Tight coupling - Cascading failures - Timeout issues ### Option 3: Polling-based Workers poll for tasks. **Pros:** - Simple implementation - Easy rate limiting - No message broker **Cons:** - Polling overhead - Higher latency - Inefficient at scale ## Decision We will use an event-driven architecture with Kafka for workflow step execution. ## Consequences - Implement idempotent consumers - Use dead letter queues for failed messages - Monitor consumer lag",
              "score": null
            },
            {
              "doc_id": "howto_implement_retry_logic",
              "chunk_id": "howto_implement_retry_logic_fix_0",
              "content": "# How to Implement Retry Logic ## Prerequisites - Existing workflow ## Steps ### Step 1: Define retry policy Add retry configuration to your workflow step. > **Note:** Maximum 5 retry attempts allowed ### Step 2: Configure backoff Set exponential backoff with base delay of 1 second. The formula is: delay = base * 2^attempt. ### Step 3: Add timeout Set maximum execution time to prevent infinite loops. Default timeout is 30 seconds. ### Step 4: Handle final failure Configure on_failure handler to notify or escalate when all retries are exhausted. ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            }
          ],
          "latency_ms": 3.880577000018093,
          "query_id": "howto_020",
          "dimension": "original",
          "query": "How do I create a scheduled workflow?"
        },
        {
          "key_facts": [
            "cron",
            "0 9 * * MON-FRI",
            "timezone"
          ],
          "found_facts": [
            "cron",
            "0 9 * * MON-FRI",
            "timezone"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "howto_create_scheduled_workflo"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "howto_create_scheduled_workflow",
              "chunk_id": "howto_create_scheduled_workflow_fix_0",
              "content": "# How to Create Scheduled Workflow ## Prerequisites - Workflow definition ## Steps ### Step 1: Define schedule Use cron expression to set the schedule. Example: '0 9 * * MON-FRI' runs at 9 AM on weekdays. ### Step 2: Set timezone Specify timezone for the schedule. Default is UTC. > **Note:** Use IANA timezone names like America/New_York ### Step 3: Configure overlap handling Choose behavior when previous execution is still running: skip, queue, or cancel. ### Step 4: Enable notifications Get notified on schedule failures. ```bash cloudflow workflows notify --on-failure ``` ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            },
            {
              "doc_id": "arch_workflow_engine",
              "chunk_id": "arch_workflow_engine_fix_0",
              "content": "# Workflow Engine Architecture ## Overview The Workflow Engine is the core component responsible for executing user-defined workflows. It uses an event-driven architecture with support for parallel execution, conditional branching, and automatic retries. ## Design Principles - **Idempotency**: All operations can be safely retried without side effects - **Eventual Consistency**: State converges to consistent view within 5 seconds - **Fault Tolerance**: System continues operating despite component failures ## Components ### Scheduler Manages workflow scheduling and triggers. Supports cron expressions, webhooks, and event-based triggers. **Technology:** Custom Go service **Scaling:** Horizontal with leader election ### Executor Runs individual workflow steps in isolated containers. Maximum execution time is 30 minutes per step. **Technology:** Kubernetes Jobs **Scaling:** Auto-scaling based on queue depth ### State Store Persists workflow state, execution history, and step results. Uses optimistic locking for concurrent updates. **Technology:** PostgreSQL with JSONB **Scaling:** Primary-replica with read replicas ## Data Flow Triggers \u2192 Scheduler \u2192 Job Queue \u2192 Executor \u2192 State Store \u2192 Webhooks/Notifications ## Performance Characteristics - **Latency P50:** 45ms - **Latency P99:** 250ms - **Throughput:** 5000 executions/minute",
              "score": null
            },
            {
              "doc_id": "howto_implement_retry_logic",
              "chunk_id": "howto_implement_retry_logic_fix_0",
              "content": "# How to Implement Retry Logic ## Prerequisites - Existing workflow ## Steps ### Step 1: Define retry policy Add retry configuration to your workflow step. > **Note:** Maximum 5 retry attempts allowed ### Step 2: Configure backoff Set exponential backoff with base delay of 1 second. The formula is: delay = base * 2^attempt. ### Step 3: Add timeout Set maximum execution time to prevent infinite loops. Default timeout is 30 seconds. ### Step 4: Handle final failure Configure on_failure handler to notify or escalate when all retries are exhausted. ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            },
            {
              "doc_id": "adr_005",
              "chunk_id": "adr_005_fix_0",
              "content": "# ADR-005: Adopt Event-Driven Architecture for Workflow Execution **Status:** Accepted **Date:** 2024-12-04 ## Context We need an architecture that supports asynchronous workflow execution, step dependencies, and failure recovery. ## Options Considered ### Option 1: Event-driven with Kafka Publish-subscribe messaging. **Pros:** - Loose coupling - Scalability - Event replay - Audit trail **Cons:** - Eventual consistency - Operational complexity - Message ordering challenges ### Option 2: Request-response Synchronous API calls. **Pros:** - Simple mental model - Immediate feedback - Easy debugging **Cons:** - Tight coupling - Cascading failures - Timeout issues ### Option 3: Polling-based Workers poll for tasks. **Pros:** - Simple implementation - Easy rate limiting - No message broker **Cons:** - Polling overhead - Higher latency - Inefficient at scale ## Decision We will use an event-driven architecture with Kafka for workflow step execution. ## Consequences - Implement idempotent consumers - Use dead letter queues for failed messages - Monitor consumer lag",
              "score": null
            },
            {
              "doc_id": "howto_implement_workflow_versioning",
              "chunk_id": "howto_implement_workflow_versioning_fix_0",
              "content": "# How to Implement Workflow Versioning ## Prerequisites - Existing workflow ## Steps ### Step 1: Enable versioning Turn on versioning for your workflow. This is required for the feature. ```bash cloudflow workflows update wf_123 --enable-versioning ``` ### Step 2: Create new version Save changes as a new version instead of overwriting. ```bash cloudflow workflows publish wf_123 --version 2.0.0 ``` ### Step 3: Set active version Choose which version handles new executions. ```bash cloudflow workflows activate wf_123 --version 2.0.0 ``` ### Step 4: Roll back if needed Quickly revert to a previous version. ```bash cloudflow workflows rollback wf_123 --to-version 1.0.0 ``` ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            }
          ],
          "latency_ms": 3.8571429995499784,
          "query_id": "howto_020",
          "dimension": "synonym",
          "query": "How do I run a workflow automatically on a recurring schedule?"
        },
        {
          "key_facts": [
            "cron",
            "0 9 * * MON-FRI",
            "timezone"
          ],
          "found_facts": [
            "cron",
            "0 9 * * MON-FRI",
            "timezone"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "howto_create_scheduled_workflo"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "howto_create_scheduled_workflow",
              "chunk_id": "howto_create_scheduled_workflow_fix_0",
              "content": "# How to Create Scheduled Workflow ## Prerequisites - Workflow definition ## Steps ### Step 1: Define schedule Use cron expression to set the schedule. Example: '0 9 * * MON-FRI' runs at 9 AM on weekdays. ### Step 2: Set timezone Specify timezone for the schedule. Default is UTC. > **Note:** Use IANA timezone names like America/New_York ### Step 3: Configure overlap handling Choose behavior when previous execution is still running: skip, queue, or cancel. ### Step 4: Enable notifications Get notified on schedule failures. ```bash cloudflow workflows notify --on-failure ``` ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            },
            {
              "doc_id": "arch_workflow_engine",
              "chunk_id": "arch_workflow_engine_fix_0",
              "content": "# Workflow Engine Architecture ## Overview The Workflow Engine is the core component responsible for executing user-defined workflows. It uses an event-driven architecture with support for parallel execution, conditional branching, and automatic retries. ## Design Principles - **Idempotency**: All operations can be safely retried without side effects - **Eventual Consistency**: State converges to consistent view within 5 seconds - **Fault Tolerance**: System continues operating despite component failures ## Components ### Scheduler Manages workflow scheduling and triggers. Supports cron expressions, webhooks, and event-based triggers. **Technology:** Custom Go service **Scaling:** Horizontal with leader election ### Executor Runs individual workflow steps in isolated containers. Maximum execution time is 30 minutes per step. **Technology:** Kubernetes Jobs **Scaling:** Auto-scaling based on queue depth ### State Store Persists workflow state, execution history, and step results. Uses optimistic locking for concurrent updates. **Technology:** PostgreSQL with JSONB **Scaling:** Primary-replica with read replicas ## Data Flow Triggers \u2192 Scheduler \u2192 Job Queue \u2192 Executor \u2192 State Store \u2192 Webhooks/Notifications ## Performance Characteristics - **Latency P50:** 45ms - **Latency P99:** 250ms - **Throughput:** 5000 executions/minute",
              "score": null
            },
            {
              "doc_id": "howto_implement_retry_logic",
              "chunk_id": "howto_implement_retry_logic_fix_0",
              "content": "# How to Implement Retry Logic ## Prerequisites - Existing workflow ## Steps ### Step 1: Define retry policy Add retry configuration to your workflow step. > **Note:** Maximum 5 retry attempts allowed ### Step 2: Configure backoff Set exponential backoff with base delay of 1 second. The formula is: delay = base * 2^attempt. ### Step 3: Add timeout Set maximum execution time to prevent infinite loops. Default timeout is 30 seconds. ### Step 4: Handle final failure Configure on_failure handler to notify or escalate when all retries are exhausted. ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            },
            {
              "doc_id": "config_workflow_engine",
              "chunk_id": "config_workflow_engine_fix_0",
              "content": "# Workflow Engine Configuration Reference ## Overview This document describes all configuration options for Workflow Engine. ## Environment Variables | Variable | Type | Default | Description | |----------|------|---------|-------------| | `WORKFLOW_MAX_STEPS` | integer | 100 | Maximum steps per workflow | | `WORKFLOW_TIMEOUT_SECONDS` | integer | 3600 | Maximum workflow execution time | | `WORKFLOW_RETRY_LIMIT` | integer | 3 | Default retry attempts for failed steps | | `WORKFLOW_CONCURRENT_LIMIT` | integer | 10 | Maximum concurrent executions per workflow | | `EXECUTOR_POOL_SIZE` | integer | 50 | Number of executor workers | | `EXECUTOR_MEMORY_LIMIT` | string | 512Mi | Memory limit per executor | ## Configuration File Configuration can also be provided via `workflow engine.yaml`: ```yaml workflow_max_steps: 100 workflow_timeout_seconds: 3600 workflow_retry_limit: 3 workflow_concurrent_limit: 10 executor_pool_size: 50 ``` ## Validation Configuration is validated at startup. Invalid configuration will prevent the service from starting.",
              "score": null
            },
            {
              "doc_id": "howto_set_up_monitoring_alerts",
              "chunk_id": "howto_set_up_monitoring_alerts_fix_0",
              "content": "# How to Set Up Monitoring Alerts ## Prerequisites - CloudFlow Pro plan or higher ## Steps ### Step 1: Define alert rules Create alert conditions based on metrics thresholds. ```bash cloudflow alerts create --name 'High Error Rate' --condition 'error_rate > 0.05' ``` ### Step 2: Configure channels Set up notification channels: email, Slack, or PagerDuty. ```bash cloudflow alerts channel add --type slack --webhook https://hooks.slack.com/... ``` ### Step 3: Set severity levels Assign severity to each alert: critical, warning, or info. > **Note:** Critical alerts bypass quiet hours ### Step 4: Test alerts Trigger a test alert to verify routing. ```bash cloudflow alerts test --name 'High Error Rate' ``` ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            }
          ],
          "latency_ms": 3.583023000828689,
          "query_id": "howto_020",
          "dimension": "problem",
          "query": "I want my workflow to run every weekday morning at 9 AM automatically"
        },
        {
          "key_facts": [
            "cron",
            "0 9 * * MON-FRI",
            "timezone"
          ],
          "found_facts": [
            "cron",
            "0 9 * * MON-FRI",
            "timezone"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "howto_create_scheduled_workflo"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "howto_create_scheduled_workflow",
              "chunk_id": "howto_create_scheduled_workflow_fix_0",
              "content": "# How to Create Scheduled Workflow ## Prerequisites - Workflow definition ## Steps ### Step 1: Define schedule Use cron expression to set the schedule. Example: '0 9 * * MON-FRI' runs at 9 AM on weekdays. ### Step 2: Set timezone Specify timezone for the schedule. Default is UTC. > **Note:** Use IANA timezone names like America/New_York ### Step 3: Configure overlap handling Choose behavior when previous execution is still running: skip, queue, or cancel. ### Step 4: Enable notifications Get notified on schedule failures. ```bash cloudflow workflows notify --on-failure ``` ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            },
            {
              "doc_id": "howto_implement_retry_logic",
              "chunk_id": "howto_implement_retry_logic_fix_0",
              "content": "# How to Implement Retry Logic ## Prerequisites - Existing workflow ## Steps ### Step 1: Define retry policy Add retry configuration to your workflow step. > **Note:** Maximum 5 retry attempts allowed ### Step 2: Configure backoff Set exponential backoff with base delay of 1 second. The formula is: delay = base * 2^attempt. ### Step 3: Add timeout Set maximum execution time to prevent infinite loops. Default timeout is 30 seconds. ### Step 4: Handle final failure Configure on_failure handler to notify or escalate when all retries are exhausted. ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            },
            {
              "doc_id": "howto_create_api_integration",
              "chunk_id": "howto_create_api_integration_fix_0",
              "content": "# How to Create API Integration ## Prerequisites - CloudFlow account ## Steps ### Step 1: Generate API key Create an API key for programmatic access. ```bash cloudflow apikeys create --name 'CI Integration' --scopes workflows:read,workflows:execute ``` ### Step 2: Store key securely Never commit API keys to source control. Use environment variables or secret management. > **Note:** Rotate keys every 90 days ### Step 3: Test authentication Verify the API key works correctly. ```bash curl -H 'Authorization: Bearer cf_...' https://api.cloudflow.io/v1/me ``` ### Step 4: Implement rate limiting Handle rate limit responses (HTTP 429) with exponential backoff. > **Note:** Default limit is 100 requests per minute ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            },
            {
              "doc_id": "api_workflows",
              "chunk_id": "api_workflows_fix_0",
              "content": "# Workflows API Reference ## Overview The Workflows API provides programmatic access to workflows functionality. Base URL: `https://api.cloudflow.io/v1/workflows` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /workflows List all workflows for the authenticated user. **Parameters:** - `limit` (integer): Maximum number of results (default: 50, max: 200) - `status` (string): Filter by status: active, paused, archived **Response:** - Success: `200` - Returns array of workflow objects - Error: `400` - Bad Request **Example:** ```json {\"workflows\": [{\"id\": \"wf_123\", \"name\": \"Daily Report\", \"status\": \"active\"}]} ``` ### POST /workflows Create a new workflow. **Parameters:** - `name` (string): Workflow name (required) - `definition` (object): Workflow definition in JSON format **Response:** - Success: `201` - Workflow created successfully - Error: `400` - Bad Request **Example:** ```json {\"id\": \"wf_456\", \"name\": \"New Workflow\", \"created_at\": \"2024-01-15T10:00:00Z\"} ``` ### DELETE /workflows/{id} Delete a workflow permanently. **Parameters:** - `id` (string): Workflow ID (required) **Response:** - Success: `204` - Workflow deleted - Error: `404` - Workflow not found **Example:** ```json {} ```",
              "score": null
            },
            {
              "doc_id": "api_executions",
              "chunk_id": "api_executions_fix_0",
              "content": "# Executions API Reference ## Overview The Executions API provides programmatic access to executions functionality. Base URL: `https://api.cloudflow.io/v1/executions` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /executions List workflow executions. **Parameters:** - `workflow_id` (string): Filter by workflow ID - `since` (datetime): Filter executions after this timestamp **Response:** - Success: `200` - Returns array of execution objects - Error: `400` - Bad Request **Example:** ```json {\"executions\": [{\"id\": \"ex_789\", \"status\": \"completed\", \"duration_ms\": 1523}]} ``` ### POST /executions/{id}/cancel Cancel a running execution. **Parameters:** - `id` (string): Execution ID - `reason` (string): Cancellation reason (optional) **Response:** - Success: `200` - Execution cancelled - Error: `400` - Bad Request **Example:** ```json {\"id\": \"ex_789\", \"status\": \"cancelled\", \"cancelled_at\": \"2024-01-15T10:05:00Z\"} ```",
              "score": null
            }
          ],
          "latency_ms": 3.3470420003141044,
          "query_id": "howto_020",
          "dimension": "casual",
          "query": "cron schedule workflow cloudflow"
        },
        {
          "key_facts": [
            "cron",
            "0 9 * * MON-FRI",
            "timezone"
          ],
          "found_facts": [
            "cron",
            "0 9 * * MON-FRI",
            "timezone"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "howto_create_scheduled_workflo"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "howto_create_scheduled_workflow",
              "chunk_id": "howto_create_scheduled_workflow_fix_0",
              "content": "# How to Create Scheduled Workflow ## Prerequisites - Workflow definition ## Steps ### Step 1: Define schedule Use cron expression to set the schedule. Example: '0 9 * * MON-FRI' runs at 9 AM on weekdays. ### Step 2: Set timezone Specify timezone for the schedule. Default is UTC. > **Note:** Use IANA timezone names like America/New_York ### Step 3: Configure overlap handling Choose behavior when previous execution is still running: skip, queue, or cancel. ### Step 4: Enable notifications Get notified on schedule failures. ```bash cloudflow workflows notify --on-failure ``` ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            },
            {
              "doc_id": "arch_workflow_engine",
              "chunk_id": "arch_workflow_engine_fix_0",
              "content": "# Workflow Engine Architecture ## Overview The Workflow Engine is the core component responsible for executing user-defined workflows. It uses an event-driven architecture with support for parallel execution, conditional branching, and automatic retries. ## Design Principles - **Idempotency**: All operations can be safely retried without side effects - **Eventual Consistency**: State converges to consistent view within 5 seconds - **Fault Tolerance**: System continues operating despite component failures ## Components ### Scheduler Manages workflow scheduling and triggers. Supports cron expressions, webhooks, and event-based triggers. **Technology:** Custom Go service **Scaling:** Horizontal with leader election ### Executor Runs individual workflow steps in isolated containers. Maximum execution time is 30 minutes per step. **Technology:** Kubernetes Jobs **Scaling:** Auto-scaling based on queue depth ### State Store Persists workflow state, execution history, and step results. Uses optimistic locking for concurrent updates. **Technology:** PostgreSQL with JSONB **Scaling:** Primary-replica with read replicas ## Data Flow Triggers \u2192 Scheduler \u2192 Job Queue \u2192 Executor \u2192 State Store \u2192 Webhooks/Notifications ## Performance Characteristics - **Latency P50:** 45ms - **Latency P99:** 250ms - **Throughput:** 5000 executions/minute",
              "score": null
            },
            {
              "doc_id": "api_metrics",
              "chunk_id": "api_metrics_fix_0",
              "content": "# Metrics API Reference ## Overview The Metrics API provides programmatic access to metrics functionality. Base URL: `https://api.cloudflow.io/v1/metrics` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /metrics/workflows Get workflow performance metrics. **Parameters:** - `period` (string): Time period: hour, day, week, month - `workflow_id` (string): Filter by workflow ID **Response:** - Success: `200` - Returns metrics data - Error: `400` - Bad Request **Example:** ```json {\"success_rate\": 0.985, \"avg_duration_ms\": 1250, \"p99_duration_ms\": 5000, \"total_executions\": 10000} ```",
              "score": null
            },
            {
              "doc_id": "api_workflows",
              "chunk_id": "api_workflows_fix_0",
              "content": "# Workflows API Reference ## Overview The Workflows API provides programmatic access to workflows functionality. Base URL: `https://api.cloudflow.io/v1/workflows` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /workflows List all workflows for the authenticated user. **Parameters:** - `limit` (integer): Maximum number of results (default: 50, max: 200) - `status` (string): Filter by status: active, paused, archived **Response:** - Success: `200` - Returns array of workflow objects - Error: `400` - Bad Request **Example:** ```json {\"workflows\": [{\"id\": \"wf_123\", \"name\": \"Daily Report\", \"status\": \"active\"}]} ``` ### POST /workflows Create a new workflow. **Parameters:** - `name` (string): Workflow name (required) - `definition` (object): Workflow definition in JSON format **Response:** - Success: `201` - Workflow created successfully - Error: `400` - Bad Request **Example:** ```json {\"id\": \"wf_456\", \"name\": \"New Workflow\", \"created_at\": \"2024-01-15T10:00:00Z\"} ``` ### DELETE /workflows/{id} Delete a workflow permanently. **Parameters:** - `id` (string): Workflow ID (required) **Response:** - Success: `204` - Workflow deleted - Error: `404` - Workflow not found **Example:** ```json {} ```",
              "score": null
            },
            {
              "doc_id": "config_workflow_engine",
              "chunk_id": "config_workflow_engine_fix_0",
              "content": "# Workflow Engine Configuration Reference ## Overview This document describes all configuration options for Workflow Engine. ## Environment Variables | Variable | Type | Default | Description | |----------|------|---------|-------------| | `WORKFLOW_MAX_STEPS` | integer | 100 | Maximum steps per workflow | | `WORKFLOW_TIMEOUT_SECONDS` | integer | 3600 | Maximum workflow execution time | | `WORKFLOW_RETRY_LIMIT` | integer | 3 | Default retry attempts for failed steps | | `WORKFLOW_CONCURRENT_LIMIT` | integer | 10 | Maximum concurrent executions per workflow | | `EXECUTOR_POOL_SIZE` | integer | 50 | Number of executor workers | | `EXECUTOR_MEMORY_LIMIT` | string | 512Mi | Memory limit per executor | ## Configuration File Configuration can also be provided via `workflow engine.yaml`: ```yaml workflow_max_steps: 100 workflow_timeout_seconds: 3600 workflow_retry_limit: 3 workflow_concurrent_limit: 10 executor_pool_size: 50 ``` ## Validation Configuration is validated at startup. Invalid configuration will prevent the service from starting.",
              "score": null
            }
          ],
          "latency_ms": 3.453121000347892,
          "query_id": "howto_020",
          "dimension": "contextual",
          "query": "building a daily report generation workflow that needs to run at a specific time each day"
        },
        {
          "key_facts": [
            "cron",
            "0 9 * * MON-FRI",
            "timezone"
          ],
          "found_facts": [
            "cron",
            "0 9 * * MON-FRI",
            "timezone"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "howto_create_scheduled_workflo"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "howto_create_scheduled_workflow",
              "chunk_id": "howto_create_scheduled_workflow_fix_0",
              "content": "# How to Create Scheduled Workflow ## Prerequisites - Workflow definition ## Steps ### Step 1: Define schedule Use cron expression to set the schedule. Example: '0 9 * * MON-FRI' runs at 9 AM on weekdays. ### Step 2: Set timezone Specify timezone for the schedule. Default is UTC. > **Note:** Use IANA timezone names like America/New_York ### Step 3: Configure overlap handling Choose behavior when previous execution is still running: skip, queue, or cancel. ### Step 4: Enable notifications Get notified on schedule failures. ```bash cloudflow workflows notify --on-failure ``` ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            },
            {
              "doc_id": "release_3_2_0",
              "chunk_id": "release_3_2_0_fix_0",
              "content": "# Release Notes - v3.2.0 **Release Date:** 2024-01-15 ## New Features - Workflow versioning with rollback support - New Python SDK with async support - Bulk operations API for workflows - Custom retry policies per step ## Bug Fixes - Fixed race condition in concurrent execution handler - Resolved memory leak in long-running workflows - Fixed timezone handling in scheduled triggers ## Breaking Changes - **BREAKING:** API v1 endpoints deprecated, will be removed in v4.0 - **BREAKING:** Minimum Python version now 3.9 ## Upgrade Instructions ```bash cloudflow upgrade --version 3.2.0 ``` ## Known Issues None at this time.",
              "score": null
            },
            {
              "doc_id": "arch_workflow_engine",
              "chunk_id": "arch_workflow_engine_fix_0",
              "content": "# Workflow Engine Architecture ## Overview The Workflow Engine is the core component responsible for executing user-defined workflows. It uses an event-driven architecture with support for parallel execution, conditional branching, and automatic retries. ## Design Principles - **Idempotency**: All operations can be safely retried without side effects - **Eventual Consistency**: State converges to consistent view within 5 seconds - **Fault Tolerance**: System continues operating despite component failures ## Components ### Scheduler Manages workflow scheduling and triggers. Supports cron expressions, webhooks, and event-based triggers. **Technology:** Custom Go service **Scaling:** Horizontal with leader election ### Executor Runs individual workflow steps in isolated containers. Maximum execution time is 30 minutes per step. **Technology:** Kubernetes Jobs **Scaling:** Auto-scaling based on queue depth ### State Store Persists workflow state, execution history, and step results. Uses optimistic locking for concurrent updates. **Technology:** PostgreSQL with JSONB **Scaling:** Primary-replica with read replicas ## Data Flow Triggers \u2192 Scheduler \u2192 Job Queue \u2192 Executor \u2192 State Store \u2192 Webhooks/Notifications ## Performance Characteristics - **Latency P50:** 45ms - **Latency P99:** 250ms - **Throughput:** 5000 executions/minute",
              "score": null
            },
            {
              "doc_id": "api_executions",
              "chunk_id": "api_executions_fix_0",
              "content": "# Executions API Reference ## Overview The Executions API provides programmatic access to executions functionality. Base URL: `https://api.cloudflow.io/v1/executions` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /executions List workflow executions. **Parameters:** - `workflow_id` (string): Filter by workflow ID - `since` (datetime): Filter executions after this timestamp **Response:** - Success: `200` - Returns array of execution objects - Error: `400` - Bad Request **Example:** ```json {\"executions\": [{\"id\": \"ex_789\", \"status\": \"completed\", \"duration_ms\": 1523}]} ``` ### POST /executions/{id}/cancel Cancel a running execution. **Parameters:** - `id` (string): Execution ID - `reason` (string): Cancellation reason (optional) **Response:** - Success: `200` - Execution cancelled - Error: `400` - Bad Request **Example:** ```json {\"id\": \"ex_789\", \"status\": \"cancelled\", \"cancelled_at\": \"2024-01-15T10:05:00Z\"} ```",
              "score": null
            },
            {
              "doc_id": "api_workflows",
              "chunk_id": "api_workflows_fix_0",
              "content": "# Workflows API Reference ## Overview The Workflows API provides programmatic access to workflows functionality. Base URL: `https://api.cloudflow.io/v1/workflows` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /workflows List all workflows for the authenticated user. **Parameters:** - `limit` (integer): Maximum number of results (default: 50, max: 200) - `status` (string): Filter by status: active, paused, archived **Response:** - Success: `200` - Returns array of workflow objects - Error: `400` - Bad Request **Example:** ```json {\"workflows\": [{\"id\": \"wf_123\", \"name\": \"Daily Report\", \"status\": \"active\"}]} ``` ### POST /workflows Create a new workflow. **Parameters:** - `name` (string): Workflow name (required) - `definition` (object): Workflow definition in JSON format **Response:** - Success: `201` - Workflow created successfully - Error: `400` - Bad Request **Example:** ```json {\"id\": \"wf_456\", \"name\": \"New Workflow\", \"created_at\": \"2024-01-15T10:00:00Z\"} ``` ### DELETE /workflows/{id} Delete a workflow permanently. **Parameters:** - `id` (string): Workflow ID (required) **Response:** - Success: `204` - Workflow deleted - Error: `404` - Workflow not found **Example:** ```json {} ```",
              "score": null
            }
          ],
          "latency_ms": 4.959915999279474,
          "query_id": "howto_020",
          "dimension": "negation",
          "query": "why isn't my workflow running at the scheduled time I configured?"
        },
        {
          "key_facts": [
            "SAML 2.0",
            "OpenID Connect",
            "ACS URL"
          ],
          "found_facts": [
            "SAML 2.0",
            "OpenID Connect",
            "ACS URL"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "howto_configure_single_sign-on"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "howto_configure_single_sign_on",
              "chunk_id": "howto_configure_single_sign_on_fix_0",
              "content": "# How to Configure Single Sign-On ## Prerequisites - Admin access - Enterprise plan - Identity provider access ## Steps ### Step 1: Choose provider CloudFlow supports SAML 2.0, OpenID Connect, and OAuth 2.0 providers. ### Step 2: Get metadata Download CloudFlow's SAML metadata for your IdP configuration. ```bash cloudflow sso metadata --format xml > cloudflow-metadata.xml ``` ### Step 3: Configure IdP Add CloudFlow as a service provider in your identity provider. > **Note:** ACS URL: https://auth.cloudflow.io/saml/callback ### Step 4: Upload IdP metadata Provide your IdP's metadata to CloudFlow. ```bash cloudflow sso configure --idp-metadata ./idp-metadata.xml ``` ### Step 5: Test login Verify SSO login works correctly. ```bash cloudflow sso test ``` ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            },
            {
              "doc_id": "arch_authentication_service",
              "chunk_id": "arch_authentication_service_fix_0",
              "content": "# Authentication Service Architecture ## Overview The Authentication Service handles user identity, session management, and token issuance. Supports OAuth 2.0, SAML, and API key authentication. ## Design Principles - **Zero Knowledge**: Passwords never stored in plaintext - **Token Rotation**: Refresh tokens rotate on each use ## Components ### Identity Provider Manages user accounts and credentials. Password hashing uses bcrypt with cost factor 12. **Technology:** Custom Go service **Scaling:** Horizontal stateless ### Token Service Issues and validates access tokens. Token lifetime is 1 hour with 7-day refresh tokens. **Technology:** JWT with RS256 **Scaling:** Horizontal stateless ### SSO Gateway Integrates with external identity providers. Supports Google, Microsoft, Okta, and custom SAML. **Technology:** Dex OIDC proxy **Scaling:** Active-passive ## Data Flow Credentials \u2192 Validation \u2192 Token Issue \u2192 Token Refresh \u2192 Logout ## Performance Characteristics - **Latency P50:** 50ms - **Latency P99:** 200ms - **Throughput:** 5000 auth/second",
              "score": null
            },
            {
              "doc_id": "adr_003",
              "chunk_id": "adr_003_fix_0",
              "content": "# ADR-003: Implement OAuth 2.0 with JWT for Authentication **Status:** Accepted **Date:** 2024-05-08 ## Context We need a secure authentication system that supports API keys, user sessions, and third-party integrations. ## Options Considered ### Option 1: OAuth 2.0 with JWT Industry standard token-based auth. **Pros:** - Industry standard - Stateless validation - Supports refresh tokens - SSO compatible **Cons:** - Token size overhead - Cannot revoke individual tokens - Complexity ### Option 2: Session-based auth Traditional server-side sessions. **Pros:** - Simple implementation - Easy revocation - Smaller payloads **Cons:** - Requires session store - Not suitable for APIs - Scaling challenges ### Option 3: API keys only Simple key-based authentication. **Pros:** - Very simple - Low overhead - Easy to implement **Cons:** - No user context - Hard to rotate - No SSO support ## Decision We will implement OAuth 2.0 with JWT tokens. Access tokens expire after 1 hour, refresh tokens after 7 days. ## Consequences - Implement token rotation on refresh - Support API keys for machine-to-machine auth - Add MFA for admin accounts",
              "score": null
            },
            {
              "doc_id": "config_database",
              "chunk_id": "config_database_fix_0",
              "content": "# Database Configuration Reference ## Overview This document describes all configuration options for Database. ## Environment Variables | Variable | Type | Default | Description | |----------|------|---------|-------------| | `DATABASE_URL` | string | required | PostgreSQL connection string | | `DATABASE_POOL_MIN` | integer | 5 | Minimum connection pool size | | `DATABASE_POOL_MAX` | integer | 20 | Maximum connection pool size | | `DATABASE_STATEMENT_TIMEOUT` | integer | 30000 | Query timeout in milliseconds | | `DATABASE_SSL_MODE` | string | require | SSL mode: disable, require, verify-full | | `DATABASE_MIGRATION_AUTO` | boolean | false | Run migrations on startup | ## Configuration File Configuration can also be provided via `database.yaml`: ```yaml database_url: required database_pool_min: 5 database_pool_max: 20 database_statement_timeout: 30000 database_ssl_mode: require ``` ## Validation Configuration is validated at startup. Invalid configuration will prevent the service from starting.",
              "score": null
            },
            {
              "doc_id": "config_api_gateway",
              "chunk_id": "config_api_gateway_fix_0",
              "content": "# API Gateway Configuration Reference ## Overview This document describes all configuration options for API Gateway. ## Environment Variables | Variable | Type | Default | Description | |----------|------|---------|-------------| | `GATEWAY_PORT` | integer | 8000 | Gateway listen port | | `GATEWAY_RATE_LIMIT` | integer | 100 | Requests per minute per user | | `GATEWAY_TIMEOUT_MS` | integer | 30000 | Request timeout in milliseconds | | `GATEWAY_MAX_BODY_SIZE` | string | 10MB | Maximum request body size | | `GATEWAY_CORS_ORIGINS` | string | * | Allowed CORS origins | | `GATEWAY_SSL_ENABLED` | boolean | true | Enable TLS termination | ## Configuration File Configuration can also be provided via `api gateway.yaml`: ```yaml gateway_port: 8000 gateway_rate_limit: 100 gateway_timeout_ms: 30000 gateway_max_body_size: 10MB gateway_cors_origins: * ``` ## Validation Configuration is validated at startup. Invalid configuration will prevent the service from starting.",
              "score": null
            }
          ],
          "latency_ms": 4.107019000002765,
          "query_id": "howto_021",
          "dimension": "original",
          "query": "How do I configure SSO?"
        },
        {
          "key_facts": [
            "SAML 2.0",
            "OpenID Connect",
            "ACS URL"
          ],
          "found_facts": [
            "SAML 2.0",
            "OpenID Connect",
            "ACS URL"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "howto_configure_single_sign-on"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "howto_configure_single_sign_on",
              "chunk_id": "howto_configure_single_sign_on_fix_0",
              "content": "# How to Configure Single Sign-On ## Prerequisites - Admin access - Enterprise plan - Identity provider access ## Steps ### Step 1: Choose provider CloudFlow supports SAML 2.0, OpenID Connect, and OAuth 2.0 providers. ### Step 2: Get metadata Download CloudFlow's SAML metadata for your IdP configuration. ```bash cloudflow sso metadata --format xml > cloudflow-metadata.xml ``` ### Step 3: Configure IdP Add CloudFlow as a service provider in your identity provider. > **Note:** ACS URL: https://auth.cloudflow.io/saml/callback ### Step 4: Upload IdP metadata Provide your IdP's metadata to CloudFlow. ```bash cloudflow sso configure --idp-metadata ./idp-metadata.xml ``` ### Step 5: Test login Verify SSO login works correctly. ```bash cloudflow sso test ``` ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            },
            {
              "doc_id": "howto_create_api_integration",
              "chunk_id": "howto_create_api_integration_fix_0",
              "content": "# How to Create API Integration ## Prerequisites - CloudFlow account ## Steps ### Step 1: Generate API key Create an API key for programmatic access. ```bash cloudflow apikeys create --name 'CI Integration' --scopes workflows:read,workflows:execute ``` ### Step 2: Store key securely Never commit API keys to source control. Use environment variables or secret management. > **Note:** Rotate keys every 90 days ### Step 3: Test authentication Verify the API key works correctly. ```bash curl -H 'Authorization: Bearer cf_...' https://api.cloudflow.io/v1/me ``` ### Step 4: Implement rate limiting Handle rate limit responses (HTTP 429) with exponential backoff. > **Note:** Default limit is 100 requests per minute ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            },
            {
              "doc_id": "howto_configure_custom_domain",
              "chunk_id": "howto_configure_custom_domain_fix_0",
              "content": "# How to Configure Custom Domain ## Prerequisites - Domain with DNS access - CloudFlow Pro plan or higher ## Steps ### Step 1: Verify domain ownership Add a TXT record to verify domain ownership. ```bash cloudflow domains verify --domain app.example.com ``` ### Step 2: Configure DNS Add a CNAME record pointing to your CloudFlow endpoint. > **Note:** CNAME should point to ingress.cloudflow.io ### Step 3: Enable SSL CloudFlow automatically provisions SSL certificates via Let's Encrypt. ```bash cloudflow domains ssl --domain app.example.com ``` ### Step 4: Set as primary Make this the primary domain for your application. ```bash cloudflow domains set-primary --domain app.example.com ``` ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            },
            {
              "doc_id": "howto_set_up_data_encryption",
              "chunk_id": "howto_set_up_data_encryption_fix_0",
              "content": "# How to Set Up Data Encryption ## Prerequisites - CloudFlow Enterprise plan ## Steps ### Step 1: Enable encryption at rest All data is encrypted at rest using AES-256 by default. > **Note:** Encryption is automatic for all plans ### Step 2: Configure KMS Bring your own encryption keys using AWS KMS or GCP KMS. ```bash cloudflow encryption configure --kms-key arn:aws:kms:... ``` ### Step 3: Enable field-level encryption Encrypt sensitive fields within your workflow data. ```bash cloudflow encryption add-field --path $.user.ssn --key-alias sensitive-data ``` ### Step 4: Verify encryption Confirm encryption is active for your workspace. ```bash cloudflow encryption status ``` ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            },
            {
              "doc_id": "howto_set_up_ci_cd_pipeline",
              "chunk_id": "howto_set_up_ci_cd_pipeline_fix_0",
              "content": "# How to Set Up CI/CD Pipeline ## Prerequisites - GitHub account - CloudFlow API key - Repository admin access ## Steps ### Step 1: Create GitHub Actions workflow Create a new file at `.github/workflows/cloudflow.yml` with the deployment configuration. ```bash mkdir -p .github/workflows && touch .github/workflows/cloudflow.yml ``` ### Step 2: Configure secrets Add your CloudFlow API key to GitHub secrets. ```bash gh secret set CLOUDFLOW_API_KEY ``` ### Step 3: Add deployment step Add the CloudFlow deployment action to your workflow. > **Note:** Use the official cloudflow/deploy-action@v2 ### Step 4: Test the pipeline Push a commit to trigger the workflow. ```bash git push origin main ``` ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            }
          ],
          "latency_ms": 3.854388000036124,
          "query_id": "howto_021",
          "dimension": "synonym",
          "query": "How do I set up single sign-on authentication for CloudFlow?"
        },
        {
          "key_facts": [
            "SAML 2.0",
            "OpenID Connect",
            "ACS URL"
          ],
          "found_facts": [
            "SAML 2.0",
            "OpenID Connect",
            "ACS URL"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "howto_configure_single_sign-on"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "howto_configure_single_sign_on",
              "chunk_id": "howto_configure_single_sign_on_fix_0",
              "content": "# How to Configure Single Sign-On ## Prerequisites - Admin access - Enterprise plan - Identity provider access ## Steps ### Step 1: Choose provider CloudFlow supports SAML 2.0, OpenID Connect, and OAuth 2.0 providers. ### Step 2: Get metadata Download CloudFlow's SAML metadata for your IdP configuration. ```bash cloudflow sso metadata --format xml > cloudflow-metadata.xml ``` ### Step 3: Configure IdP Add CloudFlow as a service provider in your identity provider. > **Note:** ACS URL: https://auth.cloudflow.io/saml/callback ### Step 4: Upload IdP metadata Provide your IdP's metadata to CloudFlow. ```bash cloudflow sso configure --idp-metadata ./idp-metadata.xml ``` ### Step 5: Test login Verify SSO login works correctly. ```bash cloudflow sso test ``` ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            },
            {
              "doc_id": "howto_configure_custom_domain",
              "chunk_id": "howto_configure_custom_domain_fix_0",
              "content": "# How to Configure Custom Domain ## Prerequisites - Domain with DNS access - CloudFlow Pro plan or higher ## Steps ### Step 1: Verify domain ownership Add a TXT record to verify domain ownership. ```bash cloudflow domains verify --domain app.example.com ``` ### Step 2: Configure DNS Add a CNAME record pointing to your CloudFlow endpoint. > **Note:** CNAME should point to ingress.cloudflow.io ### Step 3: Enable SSL CloudFlow automatically provisions SSL certificates via Let's Encrypt. ```bash cloudflow domains ssl --domain app.example.com ``` ### Step 4: Set as primary Make this the primary domain for your application. ```bash cloudflow domains set-primary --domain app.example.com ``` ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            },
            {
              "doc_id": "howto_migrate_from_v1_to_v2_api",
              "chunk_id": "howto_migrate_from_v1_to_v2_api_fix_0",
              "content": "# How to Migrate from v1 to v2 API ## Prerequisites - Existing v1 integration - v2 API access enabled ## Steps ### Step 1: Review breaking changes The v2 API changes authentication from API keys to OAuth 2.0. Review the migration guide. ### Step 2: Update client library Upgrade to the latest SDK version. ```bash npm install @cloudflow/sdk@latest ``` ### Step 3: Update authentication Replace API key authentication with OAuth 2.0 client credentials flow. > **Note:** Generate new credentials in the dashboard ### Step 4: Test in staging Run your integration tests against the v2 staging endpoint. ```bash CLOUDFLOW_API_URL=https://api-v2-staging.cloudflow.io npm test ``` ### Step 5: Switch production Update production configuration to use v2 API. ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            },
            {
              "doc_id": "howto_set_up_data_encryption",
              "chunk_id": "howto_set_up_data_encryption_fix_0",
              "content": "# How to Set Up Data Encryption ## Prerequisites - CloudFlow Enterprise plan ## Steps ### Step 1: Enable encryption at rest All data is encrypted at rest using AES-256 by default. > **Note:** Encryption is automatic for all plans ### Step 2: Configure KMS Bring your own encryption keys using AWS KMS or GCP KMS. ```bash cloudflow encryption configure --kms-key arn:aws:kms:... ``` ### Step 3: Enable field-level encryption Encrypt sensitive fields within your workflow data. ```bash cloudflow encryption add-field --path $.user.ssn --key-alias sensitive-data ``` ### Step 4: Verify encryption Confirm encryption is active for your workspace. ```bash cloudflow encryption status ``` ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            },
            {
              "doc_id": "api_teams",
              "chunk_id": "api_teams_fix_0",
              "content": "# Teams API Reference ## Overview The Teams API provides programmatic access to teams functionality. Base URL: `https://api.cloudflow.io/v1/teams` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /teams List teams the user belongs to. **Parameters:** **Response:** - Success: `200` - Returns array of team objects - Error: `400` - Bad Request **Example:** ```json {\"teams\": [{\"id\": \"team_01\", \"name\": \"Engineering\", \"member_count\": 15}]} ``` ### POST /teams/{id}/members Add a member to a team. **Parameters:** - `user_id` (string): User ID to add - `role` (string): Role: owner, admin, member, viewer **Response:** - Success: `201` - Member added - Error: `400` - Bad Request **Example:** ```json {\"team_id\": \"team_01\", \"user_id\": \"usr_002\", \"role\": \"member\"} ```",
              "score": null
            }
          ],
          "latency_ms": 3.5385800001677126,
          "query_id": "howto_021",
          "dimension": "problem",
          "query": "I want my team to log into CloudFlow using our company's Okta/Azure AD credentials"
        },
        {
          "key_facts": [
            "SAML 2.0",
            "OpenID Connect",
            "ACS URL"
          ],
          "found_facts": [
            "SAML 2.0",
            "OpenID Connect",
            "ACS URL"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "howto_configure_single_sign-on"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "howto_configure_single_sign_on",
              "chunk_id": "howto_configure_single_sign_on_fix_0",
              "content": "# How to Configure Single Sign-On ## Prerequisites - Admin access - Enterprise plan - Identity provider access ## Steps ### Step 1: Choose provider CloudFlow supports SAML 2.0, OpenID Connect, and OAuth 2.0 providers. ### Step 2: Get metadata Download CloudFlow's SAML metadata for your IdP configuration. ```bash cloudflow sso metadata --format xml > cloudflow-metadata.xml ``` ### Step 3: Configure IdP Add CloudFlow as a service provider in your identity provider. > **Note:** ACS URL: https://auth.cloudflow.io/saml/callback ### Step 4: Upload IdP metadata Provide your IdP's metadata to CloudFlow. ```bash cloudflow sso configure --idp-metadata ./idp-metadata.xml ``` ### Step 5: Test login Verify SSO login works correctly. ```bash cloudflow sso test ``` ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            },
            {
              "doc_id": "howto_configure_custom_domain",
              "chunk_id": "howto_configure_custom_domain_fix_0",
              "content": "# How to Configure Custom Domain ## Prerequisites - Domain with DNS access - CloudFlow Pro plan or higher ## Steps ### Step 1: Verify domain ownership Add a TXT record to verify domain ownership. ```bash cloudflow domains verify --domain app.example.com ``` ### Step 2: Configure DNS Add a CNAME record pointing to your CloudFlow endpoint. > **Note:** CNAME should point to ingress.cloudflow.io ### Step 3: Enable SSL CloudFlow automatically provisions SSL certificates via Let's Encrypt. ```bash cloudflow domains ssl --domain app.example.com ``` ### Step 4: Set as primary Make this the primary domain for your application. ```bash cloudflow domains set-primary --domain app.example.com ``` ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            },
            {
              "doc_id": "howto_set_up_data_encryption",
              "chunk_id": "howto_set_up_data_encryption_fix_0",
              "content": "# How to Set Up Data Encryption ## Prerequisites - CloudFlow Enterprise plan ## Steps ### Step 1: Enable encryption at rest All data is encrypted at rest using AES-256 by default. > **Note:** Encryption is automatic for all plans ### Step 2: Configure KMS Bring your own encryption keys using AWS KMS or GCP KMS. ```bash cloudflow encryption configure --kms-key arn:aws:kms:... ``` ### Step 3: Enable field-level encryption Encrypt sensitive fields within your workflow data. ```bash cloudflow encryption add-field --path $.user.ssn --key-alias sensitive-data ``` ### Step 4: Verify encryption Confirm encryption is active for your workspace. ```bash cloudflow encryption status ``` ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            },
            {
              "doc_id": "howto_set_up_ci_cd_pipeline",
              "chunk_id": "howto_set_up_ci_cd_pipeline_fix_0",
              "content": "# How to Set Up CI/CD Pipeline ## Prerequisites - GitHub account - CloudFlow API key - Repository admin access ## Steps ### Step 1: Create GitHub Actions workflow Create a new file at `.github/workflows/cloudflow.yml` with the deployment configuration. ```bash mkdir -p .github/workflows && touch .github/workflows/cloudflow.yml ``` ### Step 2: Configure secrets Add your CloudFlow API key to GitHub secrets. ```bash gh secret set CLOUDFLOW_API_KEY ``` ### Step 3: Add deployment step Add the CloudFlow deployment action to your workflow. > **Note:** Use the official cloudflow/deploy-action@v2 ### Step 4: Test the pipeline Push a commit to trigger the workflow. ```bash git push origin main ``` ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            },
            {
              "doc_id": "howto_create_api_integration",
              "chunk_id": "howto_create_api_integration_fix_0",
              "content": "# How to Create API Integration ## Prerequisites - CloudFlow account ## Steps ### Step 1: Generate API key Create an API key for programmatic access. ```bash cloudflow apikeys create --name 'CI Integration' --scopes workflows:read,workflows:execute ``` ### Step 2: Store key securely Never commit API keys to source control. Use environment variables or secret management. > **Note:** Rotate keys every 90 days ### Step 3: Test authentication Verify the API key works correctly. ```bash curl -H 'Authorization: Bearer cf_...' https://api.cloudflow.io/v1/me ``` ### Step 4: Implement rate limiting Handle rate limit responses (HTTP 429) with exponential backoff. > **Note:** Default limit is 100 requests per minute ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            }
          ],
          "latency_ms": 3.281862000221736,
          "query_id": "howto_021",
          "dimension": "casual",
          "query": "cloudflow sso setup"
        },
        {
          "key_facts": [
            "SAML 2.0",
            "OpenID Connect",
            "ACS URL"
          ],
          "found_facts": [
            "SAML 2.0",
            "OpenID Connect",
            "ACS URL"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "howto_configure_single_sign-on"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "howto_configure_single_sign_on",
              "chunk_id": "howto_configure_single_sign_on_fix_0",
              "content": "# How to Configure Single Sign-On ## Prerequisites - Admin access - Enterprise plan - Identity provider access ## Steps ### Step 1: Choose provider CloudFlow supports SAML 2.0, OpenID Connect, and OAuth 2.0 providers. ### Step 2: Get metadata Download CloudFlow's SAML metadata for your IdP configuration. ```bash cloudflow sso metadata --format xml > cloudflow-metadata.xml ``` ### Step 3: Configure IdP Add CloudFlow as a service provider in your identity provider. > **Note:** ACS URL: https://auth.cloudflow.io/saml/callback ### Step 4: Upload IdP metadata Provide your IdP's metadata to CloudFlow. ```bash cloudflow sso configure --idp-metadata ./idp-metadata.xml ``` ### Step 5: Test login Verify SSO login works correctly. ```bash cloudflow sso test ``` ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            },
            {
              "doc_id": "arch_authentication_service",
              "chunk_id": "arch_authentication_service_fix_0",
              "content": "# Authentication Service Architecture ## Overview The Authentication Service handles user identity, session management, and token issuance. Supports OAuth 2.0, SAML, and API key authentication. ## Design Principles - **Zero Knowledge**: Passwords never stored in plaintext - **Token Rotation**: Refresh tokens rotate on each use ## Components ### Identity Provider Manages user accounts and credentials. Password hashing uses bcrypt with cost factor 12. **Technology:** Custom Go service **Scaling:** Horizontal stateless ### Token Service Issues and validates access tokens. Token lifetime is 1 hour with 7-day refresh tokens. **Technology:** JWT with RS256 **Scaling:** Horizontal stateless ### SSO Gateway Integrates with external identity providers. Supports Google, Microsoft, Okta, and custom SAML. **Technology:** Dex OIDC proxy **Scaling:** Active-passive ## Data Flow Credentials \u2192 Validation \u2192 Token Issue \u2192 Token Refresh \u2192 Logout ## Performance Characteristics - **Latency P50:** 50ms - **Latency P99:** 200ms - **Throughput:** 5000 auth/second",
              "score": null
            },
            {
              "doc_id": "arch_api_gateway",
              "chunk_id": "arch_api_gateway_fix_0",
              "content": "# API Gateway Architecture ## Overview The API Gateway handles all incoming HTTP requests, providing authentication, rate limiting, and request routing. Built on Kong with custom plugins for CloudFlow-specific functionality. ## Design Principles - **Zero Trust**: Every request is authenticated and authorized - **Defense in Depth**: Multiple layers of security validation ## Components ### Kong Gateway Primary ingress controller. Handles TLS termination, request routing, and plugin execution. **Technology:** Kong 3.4 on Kubernetes **Scaling:** Horizontal with HPA ### Auth Plugin Validates JWT tokens, API keys, and OAuth2 flows. Token validation latency under 5ms. **Technology:** Custom Lua plugin **Scaling:** Stateless ### Rate Limiter Enforces per-user and per-IP rate limits. Default limit is 100 requests per minute. **Technology:** Redis-backed sliding window **Scaling:** Shared Redis cluster ## Data Flow Client \u2192 TLS \u2192 Kong \u2192 Auth \u2192 Rate Limit \u2192 Backend Services ## Performance Characteristics - **Latency P50:** 12ms - **Latency P99:** 45ms - **Throughput:** 50000 requests/second",
              "score": null
            },
            {
              "doc_id": "howto_configure_custom_domain",
              "chunk_id": "howto_configure_custom_domain_fix_0",
              "content": "# How to Configure Custom Domain ## Prerequisites - Domain with DNS access - CloudFlow Pro plan or higher ## Steps ### Step 1: Verify domain ownership Add a TXT record to verify domain ownership. ```bash cloudflow domains verify --domain app.example.com ``` ### Step 2: Configure DNS Add a CNAME record pointing to your CloudFlow endpoint. > **Note:** CNAME should point to ingress.cloudflow.io ### Step 3: Enable SSL CloudFlow automatically provisions SSL certificates via Let's Encrypt. ```bash cloudflow domains ssl --domain app.example.com ``` ### Step 4: Set as primary Make this the primary domain for your application. ```bash cloudflow domains set-primary --domain app.example.com ``` ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            },
            {
              "doc_id": "howto_set_up_data_encryption",
              "chunk_id": "howto_set_up_data_encryption_fix_0",
              "content": "# How to Set Up Data Encryption ## Prerequisites - CloudFlow Enterprise plan ## Steps ### Step 1: Enable encryption at rest All data is encrypted at rest using AES-256 by default. > **Note:** Encryption is automatic for all plans ### Step 2: Configure KMS Bring your own encryption keys using AWS KMS or GCP KMS. ```bash cloudflow encryption configure --kms-key arn:aws:kms:... ``` ### Step 3: Enable field-level encryption Encrypt sensitive fields within your workflow data. ```bash cloudflow encryption add-field --path $.user.ssn --key-alias sensitive-data ``` ### Step 4: Verify encryption Confirm encryption is active for your workspace. ```bash cloudflow encryption status ``` ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            }
          ],
          "latency_ms": 4.144778000409133,
          "query_id": "howto_021",
          "dimension": "contextual",
          "query": "enterprise security requires us to integrate CloudFlow with our identity provider for centralized authentication"
        },
        {
          "key_facts": [
            "SAML 2.0",
            "OpenID Connect",
            "ACS URL"
          ],
          "found_facts": [
            "SAML 2.0",
            "OpenID Connect",
            "ACS URL"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "howto_configure_single_sign-on"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "arch_authentication_service",
              "chunk_id": "arch_authentication_service_fix_0",
              "content": "# Authentication Service Architecture ## Overview The Authentication Service handles user identity, session management, and token issuance. Supports OAuth 2.0, SAML, and API key authentication. ## Design Principles - **Zero Knowledge**: Passwords never stored in plaintext - **Token Rotation**: Refresh tokens rotate on each use ## Components ### Identity Provider Manages user accounts and credentials. Password hashing uses bcrypt with cost factor 12. **Technology:** Custom Go service **Scaling:** Horizontal stateless ### Token Service Issues and validates access tokens. Token lifetime is 1 hour with 7-day refresh tokens. **Technology:** JWT with RS256 **Scaling:** Horizontal stateless ### SSO Gateway Integrates with external identity providers. Supports Google, Microsoft, Okta, and custom SAML. **Technology:** Dex OIDC proxy **Scaling:** Active-passive ## Data Flow Credentials \u2192 Validation \u2192 Token Issue \u2192 Token Refresh \u2192 Logout ## Performance Characteristics - **Latency P50:** 50ms - **Latency P99:** 200ms - **Throughput:** 5000 auth/second",
              "score": null
            },
            {
              "doc_id": "adr_003",
              "chunk_id": "adr_003_fix_0",
              "content": "# ADR-003: Implement OAuth 2.0 with JWT for Authentication **Status:** Accepted **Date:** 2024-05-08 ## Context We need a secure authentication system that supports API keys, user sessions, and third-party integrations. ## Options Considered ### Option 1: OAuth 2.0 with JWT Industry standard token-based auth. **Pros:** - Industry standard - Stateless validation - Supports refresh tokens - SSO compatible **Cons:** - Token size overhead - Cannot revoke individual tokens - Complexity ### Option 2: Session-based auth Traditional server-side sessions. **Pros:** - Simple implementation - Easy revocation - Smaller payloads **Cons:** - Requires session store - Not suitable for APIs - Scaling challenges ### Option 3: API keys only Simple key-based authentication. **Pros:** - Very simple - Low overhead - Easy to implement **Cons:** - No user context - Hard to rotate - No SSO support ## Decision We will implement OAuth 2.0 with JWT tokens. Access tokens expire after 1 hour, refresh tokens after 7 days. ## Consequences - Implement token rotation on refresh - Support API keys for machine-to-machine auth - Add MFA for admin accounts",
              "score": null
            },
            {
              "doc_id": "howto_configure_single_sign_on",
              "chunk_id": "howto_configure_single_sign_on_fix_0",
              "content": "# How to Configure Single Sign-On ## Prerequisites - Admin access - Enterprise plan - Identity provider access ## Steps ### Step 1: Choose provider CloudFlow supports SAML 2.0, OpenID Connect, and OAuth 2.0 providers. ### Step 2: Get metadata Download CloudFlow's SAML metadata for your IdP configuration. ```bash cloudflow sso metadata --format xml > cloudflow-metadata.xml ``` ### Step 3: Configure IdP Add CloudFlow as a service provider in your identity provider. > **Note:** ACS URL: https://auth.cloudflow.io/saml/callback ### Step 4: Upload IdP metadata Provide your IdP's metadata to CloudFlow. ```bash cloudflow sso configure --idp-metadata ./idp-metadata.xml ``` ### Step 5: Test login Verify SSO login works correctly. ```bash cloudflow sso test ``` ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            },
            {
              "doc_id": "api_users",
              "chunk_id": "api_users_fix_0",
              "content": "# Users API Reference ## Overview The Users API provides programmatic access to users functionality. Base URL: `https://api.cloudflow.io/v1/users` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /users/me Get current user profile. **Parameters:** **Response:** - Success: `200` - Returns user object - Error: `400` - Bad Request **Example:** ```json {\"id\": \"usr_001\", \"email\": \"user@example.com\", \"role\": \"admin\", \"created_at\": \"2023-06-01\"} ``` ### PATCH /users/me Update current user profile. **Parameters:** - `name` (string): Display name - `timezone` (string): Timezone (e.g., America/New_York) **Response:** - Success: `200` - User updated - Error: `400` - Bad Request **Example:** ```json {\"id\": \"usr_001\", \"name\": \"John Doe\", \"timezone\": \"America/New_York\"} ```",
              "score": null
            },
            {
              "doc_id": "api_teams",
              "chunk_id": "api_teams_fix_0",
              "content": "# Teams API Reference ## Overview The Teams API provides programmatic access to teams functionality. Base URL: `https://api.cloudflow.io/v1/teams` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /teams List teams the user belongs to. **Parameters:** **Response:** - Success: `200` - Returns array of team objects - Error: `400` - Bad Request **Example:** ```json {\"teams\": [{\"id\": \"team_01\", \"name\": \"Engineering\", \"member_count\": 15}]} ``` ### POST /teams/{id}/members Add a member to a team. **Parameters:** - `user_id` (string): User ID to add - `role` (string): Role: owner, admin, member, viewer **Response:** - Success: `201` - Member added - Error: `400` - Bad Request **Example:** ```json {\"team_id\": \"team_01\", \"user_id\": \"usr_002\", \"role\": \"member\"} ```",
              "score": null
            }
          ],
          "latency_ms": 4.0421279991278425,
          "query_id": "howto_021",
          "dimension": "negation",
          "query": "why can't my users log in with their corporate credentials?"
        },
        {
          "key_facts": [
            "cloudflow executions get",
            "cloudflow executions logs",
            "--debug"
          ],
          "found_facts": [
            "cloudflow executions get",
            "cloudflow executions logs",
            "--debug"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "howto_debug_failed_execution"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "howto_debug_failed_execution",
              "chunk_id": "howto_debug_failed_execution_fix_0",
              "content": "# How to Debug Failed Execution ## Prerequisites - Failed execution ID ## Steps ### Step 1: Get execution details Retrieve the full execution record including step outputs. ```bash cloudflow executions get ex_123 --include-steps ``` ### Step 2: View step logs Check logs for the failed step. ```bash cloudflow executions logs ex_123 --step 3 ``` ### Step 3: Inspect input/output Review the data passed between steps. ```bash cloudflow executions inspect ex_123 --step 3 ``` ### Step 4: Enable debug mode Re-run with verbose logging enabled. ```bash cloudflow workflows execute wf_123 --debug ``` ### Step 5: Check error patterns Look for similar failures in recent executions. ```bash cloudflow executions list --status failed --since 24h ``` ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            },
            {
              "doc_id": "howto_implement_retry_logic",
              "chunk_id": "howto_implement_retry_logic_fix_0",
              "content": "# How to Implement Retry Logic ## Prerequisites - Existing workflow ## Steps ### Step 1: Define retry policy Add retry configuration to your workflow step. > **Note:** Maximum 5 retry attempts allowed ### Step 2: Configure backoff Set exponential backoff with base delay of 1 second. The formula is: delay = base * 2^attempt. ### Step 3: Add timeout Set maximum execution time to prevent infinite loops. Default timeout is 30 seconds. ### Step 4: Handle final failure Configure on_failure handler to notify or escalate when all retries are exhausted. ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            },
            {
              "doc_id": "api_executions",
              "chunk_id": "api_executions_fix_0",
              "content": "# Executions API Reference ## Overview The Executions API provides programmatic access to executions functionality. Base URL: `https://api.cloudflow.io/v1/executions` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /executions List workflow executions. **Parameters:** - `workflow_id` (string): Filter by workflow ID - `since` (datetime): Filter executions after this timestamp **Response:** - Success: `200` - Returns array of execution objects - Error: `400` - Bad Request **Example:** ```json {\"executions\": [{\"id\": \"ex_789\", \"status\": \"completed\", \"duration_ms\": 1523}]} ``` ### POST /executions/{id}/cancel Cancel a running execution. **Parameters:** - `id` (string): Execution ID - `reason` (string): Cancellation reason (optional) **Response:** - Success: `200` - Execution cancelled - Error: `400` - Bad Request **Example:** ```json {\"id\": \"ex_789\", \"status\": \"cancelled\", \"cancelled_at\": \"2024-01-15T10:05:00Z\"} ```",
              "score": null
            },
            {
              "doc_id": "runbook_database_connection_exhaustion",
              "chunk_id": "runbook_database_connection_exhaustion_fix_0",
              "content": "# Runbook: Database Connection Exhaustion ## Severity **Level:** P1 - Critical **Response Time:** 15 minutes ## Symptoms - Connection pool at 100% - Queries timing out - 'too many connections' errors in logs ## Diagnosis ### Step 1 Check active connections: `SELECT count(*) FROM pg_stat_activity` ### Step 2 Identify long-running queries: `SELECT pid, now() - pg_stat_activity.query_start AS duration, query FROM pg_stat_activity WHERE state != 'idle' ORDER BY duration DESC` ### Step 3 Check PgBouncer stats: `SHOW POOLS` ### Step 4 Verify connection limits: `SHOW max_connections` ## Resolution ### Step 1 Kill long-running queries: `SELECT pg_terminate_backend(pid) FROM pg_stat_activity WHERE duration > interval '5 minutes'` ### Step 2 Increase PgBouncer pool size temporarily: update pgbouncer.ini ### Step 3 Restart affected services to release connections: `kubectl rollout restart deployment/api` ### Step 4 Scale down non-critical services to free connections ## Escalation If connections remain exhausted, escalate to DBA. Consider enabling connection queueing. ## Post-Incident - Document timeline in incident log - Schedule post-mortem within 48 hours - Update runbook if new patterns discovered",
              "score": null
            },
            {
              "doc_id": "runbook_api_gateway_5xx_errors",
              "chunk_id": "runbook_api_gateway_5xx_errors_fix_0",
              "content": "# Runbook: API Gateway 5xx Errors ## Severity **Level:** P3 - Medium **Response Time:** 1 hour ## Symptoms - Error rate > 1% - 5xx responses in access logs - Downstream service failures ## Diagnosis ### Step 1 Check Kong error logs: `kubectl logs -l app=kong -n kong --tail=100 | grep -E '5[0-9]{2}'` ### Step 2 Verify backend health: `kubectl get pods -n production -o wide` ### Step 3 Check service endpoints: `kubectl get endpoints -n production` ### Step 4 Test backend directly: `kubectl exec -it kong-xxx -n kong -- curl http://api.production.svc:8080/health` ## Resolution ### Step 1 Restart unhealthy backends: `kubectl rollout restart deployment/api -n production` ### Step 2 Enable circuit breaker if cascading: update Kong circuit-breaker plugin ### Step 3 Scale up backends: `kubectl scale deployment/api --replicas=5 -n production` ### Step 4 Check for resource exhaustion on backend pods ### Step 5 Verify network policies allow traffic ## Escalation If 5xx persists > 15 minutes, page on-call. May indicate infrastructure issue. ## Post-Incident - Document timeline in incident log - Schedule post-mortem within 48 hours - Update runbook if new patterns discovered",
              "score": null
            }
          ],
          "latency_ms": 3.34094199934043,
          "query_id": "howto_022",
          "dimension": "original",
          "query": "How do I debug a failed execution?"
        },
        {
          "key_facts": [
            "cloudflow executions get",
            "cloudflow executions logs",
            "--debug"
          ],
          "found_facts": [
            "cloudflow executions get",
            "cloudflow executions logs",
            "--debug"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "howto_debug_failed_execution"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "howto_debug_failed_execution",
              "chunk_id": "howto_debug_failed_execution_fix_0",
              "content": "# How to Debug Failed Execution ## Prerequisites - Failed execution ID ## Steps ### Step 1: Get execution details Retrieve the full execution record including step outputs. ```bash cloudflow executions get ex_123 --include-steps ``` ### Step 2: View step logs Check logs for the failed step. ```bash cloudflow executions logs ex_123 --step 3 ``` ### Step 3: Inspect input/output Review the data passed between steps. ```bash cloudflow executions inspect ex_123 --step 3 ``` ### Step 4: Enable debug mode Re-run with verbose logging enabled. ```bash cloudflow workflows execute wf_123 --debug ``` ### Step 5: Check error patterns Look for similar failures in recent executions. ```bash cloudflow executions list --status failed --since 24h ``` ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            },
            {
              "doc_id": "release_3_2_0",
              "chunk_id": "release_3_2_0_fix_0",
              "content": "# Release Notes - v3.2.0 **Release Date:** 2024-01-15 ## New Features - Workflow versioning with rollback support - New Python SDK with async support - Bulk operations API for workflows - Custom retry policies per step ## Bug Fixes - Fixed race condition in concurrent execution handler - Resolved memory leak in long-running workflows - Fixed timezone handling in scheduled triggers ## Breaking Changes - **BREAKING:** API v1 endpoints deprecated, will be removed in v4.0 - **BREAKING:** Minimum Python version now 3.9 ## Upgrade Instructions ```bash cloudflow upgrade --version 3.2.0 ``` ## Known Issues None at this time.",
              "score": null
            },
            {
              "doc_id": "howto_implement_retry_logic",
              "chunk_id": "howto_implement_retry_logic_fix_0",
              "content": "# How to Implement Retry Logic ## Prerequisites - Existing workflow ## Steps ### Step 1: Define retry policy Add retry configuration to your workflow step. > **Note:** Maximum 5 retry attempts allowed ### Step 2: Configure backoff Set exponential backoff with base delay of 1 second. The formula is: delay = base * 2^attempt. ### Step 3: Add timeout Set maximum execution time to prevent infinite loops. Default timeout is 30 seconds. ### Step 4: Handle final failure Configure on_failure handler to notify or escalate when all retries are exhausted. ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            },
            {
              "doc_id": "api_workflows",
              "chunk_id": "api_workflows_fix_0",
              "content": "# Workflows API Reference ## Overview The Workflows API provides programmatic access to workflows functionality. Base URL: `https://api.cloudflow.io/v1/workflows` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /workflows List all workflows for the authenticated user. **Parameters:** - `limit` (integer): Maximum number of results (default: 50, max: 200) - `status` (string): Filter by status: active, paused, archived **Response:** - Success: `200` - Returns array of workflow objects - Error: `400` - Bad Request **Example:** ```json {\"workflows\": [{\"id\": \"wf_123\", \"name\": \"Daily Report\", \"status\": \"active\"}]} ``` ### POST /workflows Create a new workflow. **Parameters:** - `name` (string): Workflow name (required) - `definition` (object): Workflow definition in JSON format **Response:** - Success: `201` - Workflow created successfully - Error: `400` - Bad Request **Example:** ```json {\"id\": \"wf_456\", \"name\": \"New Workflow\", \"created_at\": \"2024-01-15T10:00:00Z\"} ``` ### DELETE /workflows/{id} Delete a workflow permanently. **Parameters:** - `id` (string): Workflow ID (required) **Response:** - Success: `204` - Workflow deleted - Error: `404` - Workflow not found **Example:** ```json {} ```",
              "score": null
            },
            {
              "doc_id": "release_3_0_0",
              "chunk_id": "release_3_0_0_fix_0",
              "content": "# Release Notes - v3.0.0 **Release Date:** 2023-10-15 ## New Features - Complete UI redesign - New workflow editor with visual builder - Real-time execution monitoring - Team workspaces ## Bug Fixes - Fixed critical security issue in token validation - Resolved data loss bug in failed migrations ## Breaking Changes - **BREAKING:** Authentication API completely rewritten - **BREAKING:** Legacy webhook format no longer supported - **BREAKING:** Minimum Node.js version now 18 ## Upgrade Instructions ```bash cloudflow upgrade --version 3.0.0 ``` ## Known Issues None at this time.",
              "score": null
            }
          ],
          "latency_ms": 3.3733520012901863,
          "query_id": "howto_022",
          "dimension": "synonym",
          "query": "How do I troubleshoot a workflow that errored out?"
        },
        {
          "key_facts": [
            "cloudflow executions get",
            "cloudflow executions logs",
            "--debug"
          ],
          "found_facts": [
            "cloudflow executions get",
            "cloudflow executions logs",
            "--debug"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "howto_debug_failed_execution"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "howto_debug_failed_execution",
              "chunk_id": "howto_debug_failed_execution_fix_0",
              "content": "# How to Debug Failed Execution ## Prerequisites - Failed execution ID ## Steps ### Step 1: Get execution details Retrieve the full execution record including step outputs. ```bash cloudflow executions get ex_123 --include-steps ``` ### Step 2: View step logs Check logs for the failed step. ```bash cloudflow executions logs ex_123 --step 3 ``` ### Step 3: Inspect input/output Review the data passed between steps. ```bash cloudflow executions inspect ex_123 --step 3 ``` ### Step 4: Enable debug mode Re-run with verbose logging enabled. ```bash cloudflow workflows execute wf_123 --debug ``` ### Step 5: Check error patterns Look for similar failures in recent executions. ```bash cloudflow executions list --status failed --since 24h ``` ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            },
            {
              "doc_id": "howto_implement_retry_logic",
              "chunk_id": "howto_implement_retry_logic_fix_0",
              "content": "# How to Implement Retry Logic ## Prerequisites - Existing workflow ## Steps ### Step 1: Define retry policy Add retry configuration to your workflow step. > **Note:** Maximum 5 retry attempts allowed ### Step 2: Configure backoff Set exponential backoff with base delay of 1 second. The formula is: delay = base * 2^attempt. ### Step 3: Add timeout Set maximum execution time to prevent infinite loops. Default timeout is 30 seconds. ### Step 4: Handle final failure Configure on_failure handler to notify or escalate when all retries are exhausted. ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            },
            {
              "doc_id": "api_workflows",
              "chunk_id": "api_workflows_fix_0",
              "content": "# Workflows API Reference ## Overview The Workflows API provides programmatic access to workflows functionality. Base URL: `https://api.cloudflow.io/v1/workflows` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /workflows List all workflows for the authenticated user. **Parameters:** - `limit` (integer): Maximum number of results (default: 50, max: 200) - `status` (string): Filter by status: active, paused, archived **Response:** - Success: `200` - Returns array of workflow objects - Error: `400` - Bad Request **Example:** ```json {\"workflows\": [{\"id\": \"wf_123\", \"name\": \"Daily Report\", \"status\": \"active\"}]} ``` ### POST /workflows Create a new workflow. **Parameters:** - `name` (string): Workflow name (required) - `definition` (object): Workflow definition in JSON format **Response:** - Success: `201` - Workflow created successfully - Error: `400` - Bad Request **Example:** ```json {\"id\": \"wf_456\", \"name\": \"New Workflow\", \"created_at\": \"2024-01-15T10:00:00Z\"} ``` ### DELETE /workflows/{id} Delete a workflow permanently. **Parameters:** - `id` (string): Workflow ID (required) **Response:** - Success: `204` - Workflow deleted - Error: `404` - Workflow not found **Example:** ```json {} ```",
              "score": null
            },
            {
              "doc_id": "release_3_2_0",
              "chunk_id": "release_3_2_0_fix_0",
              "content": "# Release Notes - v3.2.0 **Release Date:** 2024-01-15 ## New Features - Workflow versioning with rollback support - New Python SDK with async support - Bulk operations API for workflows - Custom retry policies per step ## Bug Fixes - Fixed race condition in concurrent execution handler - Resolved memory leak in long-running workflows - Fixed timezone handling in scheduled triggers ## Breaking Changes - **BREAKING:** API v1 endpoints deprecated, will be removed in v4.0 - **BREAKING:** Minimum Python version now 3.9 ## Upgrade Instructions ```bash cloudflow upgrade --version 3.2.0 ``` ## Known Issues None at this time.",
              "score": null
            },
            {
              "doc_id": "howto_implement_workflow_versioning",
              "chunk_id": "howto_implement_workflow_versioning_fix_0",
              "content": "# How to Implement Workflow Versioning ## Prerequisites - Existing workflow ## Steps ### Step 1: Enable versioning Turn on versioning for your workflow. This is required for the feature. ```bash cloudflow workflows update wf_123 --enable-versioning ``` ### Step 2: Create new version Save changes as a new version instead of overwriting. ```bash cloudflow workflows publish wf_123 --version 2.0.0 ``` ### Step 3: Set active version Choose which version handles new executions. ```bash cloudflow workflows activate wf_123 --version 2.0.0 ``` ### Step 4: Roll back if needed Quickly revert to a previous version. ```bash cloudflow workflows rollback wf_123 --to-version 1.0.0 ``` ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            }
          ],
          "latency_ms": 3.649786998721538,
          "query_id": "howto_022",
          "dimension": "problem",
          "query": "my workflow failed and I need to figure out which step caused the error and why"
        },
        {
          "key_facts": [
            "cloudflow executions get",
            "cloudflow executions logs",
            "--debug"
          ],
          "found_facts": [
            "cloudflow executions get",
            "cloudflow executions logs",
            "--debug"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "howto_debug_failed_execution"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "howto_debug_failed_execution",
              "chunk_id": "howto_debug_failed_execution_fix_0",
              "content": "# How to Debug Failed Execution ## Prerequisites - Failed execution ID ## Steps ### Step 1: Get execution details Retrieve the full execution record including step outputs. ```bash cloudflow executions get ex_123 --include-steps ``` ### Step 2: View step logs Check logs for the failed step. ```bash cloudflow executions logs ex_123 --step 3 ``` ### Step 3: Inspect input/output Review the data passed between steps. ```bash cloudflow executions inspect ex_123 --step 3 ``` ### Step 4: Enable debug mode Re-run with verbose logging enabled. ```bash cloudflow workflows execute wf_123 --debug ``` ### Step 5: Check error patterns Look for similar failures in recent executions. ```bash cloudflow executions list --status failed --since 24h ``` ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            },
            {
              "doc_id": "release_3_0_0",
              "chunk_id": "release_3_0_0_fix_0",
              "content": "# Release Notes - v3.0.0 **Release Date:** 2023-10-15 ## New Features - Complete UI redesign - New workflow editor with visual builder - Real-time execution monitoring - Team workspaces ## Bug Fixes - Fixed critical security issue in token validation - Resolved data loss bug in failed migrations ## Breaking Changes - **BREAKING:** Authentication API completely rewritten - **BREAKING:** Legacy webhook format no longer supported - **BREAKING:** Minimum Node.js version now 18 ## Upgrade Instructions ```bash cloudflow upgrade --version 3.0.0 ``` ## Known Issues None at this time.",
              "score": null
            },
            {
              "doc_id": "release_3_2_0",
              "chunk_id": "release_3_2_0_fix_0",
              "content": "# Release Notes - v3.2.0 **Release Date:** 2024-01-15 ## New Features - Workflow versioning with rollback support - New Python SDK with async support - Bulk operations API for workflows - Custom retry policies per step ## Bug Fixes - Fixed race condition in concurrent execution handler - Resolved memory leak in long-running workflows - Fixed timezone handling in scheduled triggers ## Breaking Changes - **BREAKING:** API v1 endpoints deprecated, will be removed in v4.0 - **BREAKING:** Minimum Python version now 3.9 ## Upgrade Instructions ```bash cloudflow upgrade --version 3.2.0 ``` ## Known Issues None at this time.",
              "score": null
            },
            {
              "doc_id": "api_audit",
              "chunk_id": "api_audit_fix_0",
              "content": "# Audit API Reference ## Overview The Audit API provides programmatic access to audit functionality. Base URL: `https://api.cloudflow.io/v1/audit` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /audit/logs Retrieve audit logs for compliance. **Parameters:** - `start_date` (date): Start date for log range - `end_date` (date): End date for log range - `actor` (string): Filter by user ID **Response:** - Success: `200` - Returns array of audit log entries - Error: `400` - Bad Request **Example:** ```json {\"logs\": [{\"timestamp\": \"2024-01-15T10:00:00Z\", \"actor\": \"usr_001\", \"action\": \"workflow.created\", \"resource\": \"wf_123\"}]} ```",
              "score": null
            },
            {
              "doc_id": "runbook_ssl_certificate_expiry",
              "chunk_id": "runbook_ssl_certificate_expiry_fix_0",
              "content": "# Runbook: SSL Certificate Expiry ## Severity **Level:** P3 - Medium **Response Time:** 15 minutes ## Symptoms - Certificate expiring within 7 days - SSL handshake failures - Browser security warnings ## Diagnosis ### Step 1 Check certificate expiry: `echo | openssl s_client -connect api.cloudflow.io:443 2>/dev/null | openssl x509 -noout -dates` ### Step 2 Verify cert-manager status: `kubectl get certificates -n production` ### Step 3 Check cert-manager logs: `kubectl logs -l app=cert-manager -n cert-manager --tail=50` ### Step 4 Verify DNS is correct: `dig +short api.cloudflow.io` ## Resolution ### Step 1 Trigger manual renewal: `kubectl delete certificate api-cert -n production && kubectl apply -f certificate.yaml` ### Step 2 Check rate limits: Let's Encrypt has 50 certs/domain/week limit ### Step 3 Use staging issuer for testing: switch to letsencrypt-staging ClusterIssuer ### Step 4 If Let's Encrypt fails, manually upload certificate ## Escalation If certificate cannot be renewed, escalate to security team immediately. Consider using backup certificate. ## Post-Incident - Document timeline in incident log - Schedule post-mortem within 48 hours - Update runbook if new patterns discovered",
              "score": null
            }
          ],
          "latency_ms": 4.506222001509741,
          "query_id": "howto_022",
          "dimension": "casual",
          "query": "cloudflow execution logs debug failed"
        },
        {
          "key_facts": [
            "cloudflow executions get",
            "cloudflow executions logs",
            "--debug"
          ],
          "found_facts": [
            "cloudflow executions get",
            "cloudflow executions logs",
            "--debug"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "howto_debug_failed_execution"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "howto_debug_failed_execution",
              "chunk_id": "howto_debug_failed_execution_fix_0",
              "content": "# How to Debug Failed Execution ## Prerequisites - Failed execution ID ## Steps ### Step 1: Get execution details Retrieve the full execution record including step outputs. ```bash cloudflow executions get ex_123 --include-steps ``` ### Step 2: View step logs Check logs for the failed step. ```bash cloudflow executions logs ex_123 --step 3 ``` ### Step 3: Inspect input/output Review the data passed between steps. ```bash cloudflow executions inspect ex_123 --step 3 ``` ### Step 4: Enable debug mode Re-run with verbose logging enabled. ```bash cloudflow workflows execute wf_123 --debug ``` ### Step 5: Check error patterns Look for similar failures in recent executions. ```bash cloudflow executions list --status failed --since 24h ``` ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            },
            {
              "doc_id": "release_3_2_0",
              "chunk_id": "release_3_2_0_fix_0",
              "content": "# Release Notes - v3.2.0 **Release Date:** 2024-01-15 ## New Features - Workflow versioning with rollback support - New Python SDK with async support - Bulk operations API for workflows - Custom retry policies per step ## Bug Fixes - Fixed race condition in concurrent execution handler - Resolved memory leak in long-running workflows - Fixed timezone handling in scheduled triggers ## Breaking Changes - **BREAKING:** API v1 endpoints deprecated, will be removed in v4.0 - **BREAKING:** Minimum Python version now 3.9 ## Upgrade Instructions ```bash cloudflow upgrade --version 3.2.0 ``` ## Known Issues None at this time.",
              "score": null
            },
            {
              "doc_id": "howto_implement_retry_logic",
              "chunk_id": "howto_implement_retry_logic_fix_0",
              "content": "# How to Implement Retry Logic ## Prerequisites - Existing workflow ## Steps ### Step 1: Define retry policy Add retry configuration to your workflow step. > **Note:** Maximum 5 retry attempts allowed ### Step 2: Configure backoff Set exponential backoff with base delay of 1 second. The formula is: delay = base * 2^attempt. ### Step 3: Add timeout Set maximum execution time to prevent infinite loops. Default timeout is 30 seconds. ### Step 4: Handle final failure Configure on_failure handler to notify or escalate when all retries are exhausted. ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            },
            {
              "doc_id": "release_3_0_0",
              "chunk_id": "release_3_0_0_fix_0",
              "content": "# Release Notes - v3.0.0 **Release Date:** 2023-10-15 ## New Features - Complete UI redesign - New workflow editor with visual builder - Real-time execution monitoring - Team workspaces ## Bug Fixes - Fixed critical security issue in token validation - Resolved data loss bug in failed migrations ## Breaking Changes - **BREAKING:** Authentication API completely rewritten - **BREAKING:** Legacy webhook format no longer supported - **BREAKING:** Minimum Node.js version now 18 ## Upgrade Instructions ```bash cloudflow upgrade --version 3.0.0 ``` ## Known Issues None at this time.",
              "score": null
            },
            {
              "doc_id": "api_workflows",
              "chunk_id": "api_workflows_fix_0",
              "content": "# Workflows API Reference ## Overview The Workflows API provides programmatic access to workflows functionality. Base URL: `https://api.cloudflow.io/v1/workflows` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /workflows List all workflows for the authenticated user. **Parameters:** - `limit` (integer): Maximum number of results (default: 50, max: 200) - `status` (string): Filter by status: active, paused, archived **Response:** - Success: `200` - Returns array of workflow objects - Error: `400` - Bad Request **Example:** ```json {\"workflows\": [{\"id\": \"wf_123\", \"name\": \"Daily Report\", \"status\": \"active\"}]} ``` ### POST /workflows Create a new workflow. **Parameters:** - `name` (string): Workflow name (required) - `definition` (object): Workflow definition in JSON format **Response:** - Success: `201` - Workflow created successfully - Error: `400` - Bad Request **Example:** ```json {\"id\": \"wf_456\", \"name\": \"New Workflow\", \"created_at\": \"2024-01-15T10:00:00Z\"} ``` ### DELETE /workflows/{id} Delete a workflow permanently. **Parameters:** - `id` (string): Workflow ID (required) **Response:** - Success: `204` - Workflow deleted - Error: `404` - Workflow not found **Example:** ```json {} ```",
              "score": null
            }
          ],
          "latency_ms": 3.766023999560275,
          "query_id": "howto_022",
          "dimension": "contextual",
          "query": "production workflow started failing last night and I need to investigate what went wrong"
        },
        {
          "key_facts": [
            "cloudflow executions get",
            "cloudflow executions logs",
            "--debug"
          ],
          "found_facts": [
            "cloudflow executions get",
            "cloudflow executions logs",
            "--debug"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "howto_debug_failed_execution"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "howto_debug_failed_execution",
              "chunk_id": "howto_debug_failed_execution_fix_0",
              "content": "# How to Debug Failed Execution ## Prerequisites - Failed execution ID ## Steps ### Step 1: Get execution details Retrieve the full execution record including step outputs. ```bash cloudflow executions get ex_123 --include-steps ``` ### Step 2: View step logs Check logs for the failed step. ```bash cloudflow executions logs ex_123 --step 3 ``` ### Step 3: Inspect input/output Review the data passed between steps. ```bash cloudflow executions inspect ex_123 --step 3 ``` ### Step 4: Enable debug mode Re-run with verbose logging enabled. ```bash cloudflow workflows execute wf_123 --debug ``` ### Step 5: Check error patterns Look for similar failures in recent executions. ```bash cloudflow executions list --status failed --since 24h ``` ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            },
            {
              "doc_id": "howto_implement_retry_logic",
              "chunk_id": "howto_implement_retry_logic_fix_0",
              "content": "# How to Implement Retry Logic ## Prerequisites - Existing workflow ## Steps ### Step 1: Define retry policy Add retry configuration to your workflow step. > **Note:** Maximum 5 retry attempts allowed ### Step 2: Configure backoff Set exponential backoff with base delay of 1 second. The formula is: delay = base * 2^attempt. ### Step 3: Add timeout Set maximum execution time to prevent infinite loops. Default timeout is 30 seconds. ### Step 4: Handle final failure Configure on_failure handler to notify or escalate when all retries are exhausted. ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            },
            {
              "doc_id": "release_3_2_0",
              "chunk_id": "release_3_2_0_fix_0",
              "content": "# Release Notes - v3.2.0 **Release Date:** 2024-01-15 ## New Features - Workflow versioning with rollback support - New Python SDK with async support - Bulk operations API for workflows - Custom retry policies per step ## Bug Fixes - Fixed race condition in concurrent execution handler - Resolved memory leak in long-running workflows - Fixed timezone handling in scheduled triggers ## Breaking Changes - **BREAKING:** API v1 endpoints deprecated, will be removed in v4.0 - **BREAKING:** Minimum Python version now 3.9 ## Upgrade Instructions ```bash cloudflow upgrade --version 3.2.0 ``` ## Known Issues None at this time.",
              "score": null
            },
            {
              "doc_id": "api_workflows",
              "chunk_id": "api_workflows_fix_0",
              "content": "# Workflows API Reference ## Overview The Workflows API provides programmatic access to workflows functionality. Base URL: `https://api.cloudflow.io/v1/workflows` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /workflows List all workflows for the authenticated user. **Parameters:** - `limit` (integer): Maximum number of results (default: 50, max: 200) - `status` (string): Filter by status: active, paused, archived **Response:** - Success: `200` - Returns array of workflow objects - Error: `400` - Bad Request **Example:** ```json {\"workflows\": [{\"id\": \"wf_123\", \"name\": \"Daily Report\", \"status\": \"active\"}]} ``` ### POST /workflows Create a new workflow. **Parameters:** - `name` (string): Workflow name (required) - `definition` (object): Workflow definition in JSON format **Response:** - Success: `201` - Workflow created successfully - Error: `400` - Bad Request **Example:** ```json {\"id\": \"wf_456\", \"name\": \"New Workflow\", \"created_at\": \"2024-01-15T10:00:00Z\"} ``` ### DELETE /workflows/{id} Delete a workflow permanently. **Parameters:** - `id` (string): Workflow ID (required) **Response:** - Success: `204` - Workflow deleted - Error: `404` - Workflow not found **Example:** ```json {} ```",
              "score": null
            },
            {
              "doc_id": "release_3_0_0",
              "chunk_id": "release_3_0_0_fix_0",
              "content": "# Release Notes - v3.0.0 **Release Date:** 2023-10-15 ## New Features - Complete UI redesign - New workflow editor with visual builder - Real-time execution monitoring - Team workspaces ## Bug Fixes - Fixed critical security issue in token validation - Resolved data loss bug in failed migrations ## Breaking Changes - **BREAKING:** Authentication API completely rewritten - **BREAKING:** Legacy webhook format no longer supported - **BREAKING:** Minimum Node.js version now 18 ## Upgrade Instructions ```bash cloudflow upgrade --version 3.0.0 ``` ## Known Issues None at this time.",
              "score": null
            }
          ],
          "latency_ms": 3.5226300005888334,
          "query_id": "howto_022",
          "dimension": "negation",
          "query": "why did my workflow fail and where can I see what happened?"
        },
        {
          "key_facts": [
            "PostgreSQL",
            "MongoDB",
            "CockroachDB",
            "JSONB",
            "Team expertise"
          ],
          "found_facts": [
            "PostgreSQL",
            "MongoDB",
            "CockroachDB",
            "JSONB",
            "Team expertise"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "adr_001"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "adr_001",
              "chunk_id": "adr_001_fix_0",
              "content": "# ADR-001: Use PostgreSQL for Primary Database **Status:** Accepted **Date:** 2024-11-04 ## Context We need a database to store workflow definitions, execution state, and user data. The database must support ACID transactions, JSON queries, and scale to millions of records. ## Options Considered ### Option 1: PostgreSQL Mature relational database with JSONB support. **Pros:** - ACID compliance - Rich JSONB queries - Team expertise - Proven at scale **Cons:** - Manual sharding for extreme scale - Requires connection pooling ### Option 2: MongoDB Document database with native JSON. **Pros:** - Flexible schema - Built-in sharding - Good for documents **Cons:** - Weaker consistency guarantees - Complex transactions - Higher operational cost ### Option 3: CockroachDB Distributed SQL database. **Pros:** - Automatic sharding - Strong consistency - PostgreSQL compatible **Cons:** - Higher latency - More expensive - Less mature ecosystem ## Decision We will use PostgreSQL as our primary database with JSONB for flexible workflow definitions. ## Consequences - Need PgBouncer for connection pooling - Must plan for sharding at 100M+ records - Requires regular VACUUM maintenance",
              "score": null
            },
            {
              "doc_id": "config_database",
              "chunk_id": "config_database_fix_0",
              "content": "# Database Configuration Reference ## Overview This document describes all configuration options for Database. ## Environment Variables | Variable | Type | Default | Description | |----------|------|---------|-------------| | `DATABASE_URL` | string | required | PostgreSQL connection string | | `DATABASE_POOL_MIN` | integer | 5 | Minimum connection pool size | | `DATABASE_POOL_MAX` | integer | 20 | Maximum connection pool size | | `DATABASE_STATEMENT_TIMEOUT` | integer | 30000 | Query timeout in milliseconds | | `DATABASE_SSL_MODE` | string | require | SSL mode: disable, require, verify-full | | `DATABASE_MIGRATION_AUTO` | boolean | false | Run migrations on startup | ## Configuration File Configuration can also be provided via `database.yaml`: ```yaml database_url: required database_pool_min: 5 database_pool_max: 20 database_statement_timeout: 30000 database_ssl_mode: require ``` ## Validation Configuration is validated at startup. Invalid configuration will prevent the service from starting.",
              "score": null
            },
            {
              "doc_id": "howto_set_up_database_connection",
              "chunk_id": "howto_set_up_database_connection_fix_0",
              "content": "# How to Set Up Database Connection ## Prerequisites - Database with network access to CloudFlow - Connection credentials ## Steps ### Step 1: Add connection secret Store your database connection string as a secret. ```bash cloudflow secrets set DATABASE_URL postgresql://user:pass@host:5432/db ``` ### Step 2: Configure connection pool Set pool size based on your plan limits. Free tier allows 5 connections, Pro allows 20. > **Note:** Pool exhaustion will queue requests ### Step 3: Test connection Verify the database is accessible from CloudFlow. ```bash cloudflow db test --secret DATABASE_URL ``` ### Step 4: Enable SSL For production, always require SSL connections. ```bash cloudflow db configure --require-ssl ``` ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            },
            {
              "doc_id": "arch_data_pipeline",
              "chunk_id": "arch_data_pipeline_fix_0",
              "content": "# Data Pipeline Architecture ## Overview The Data Pipeline processes and transforms data between workflow steps. It supports batch processing up to 10GB and streaming for real-time use cases. ## Design Principles - **Exactly-Once Processing**: Each record is processed exactly once - **Backpressure Handling**: System gracefully handles varying load ## Components ### Stream Processor Handles real-time data streams. Maximum throughput of 100000 messages per second. **Technology:** Apache Kafka with Flink **Scaling:** Partition-based scaling ### Batch Processor Processes large batch jobs. Supports data formats: JSON, CSV, Parquet, Avro. **Technology:** Apache Spark on Kubernetes **Scaling:** Dynamic resource allocation ### Data Lake Long-term data storage with time-travel queries. Retention configurable up to 7 years. **Technology:** S3 with Iceberg tables **Scaling:** Unlimited with tiered storage ## Data Flow Input \u2192 Validation \u2192 Transform \u2192 Enrich \u2192 Output ## Performance Characteristics - **Latency P50:** 100ms (streaming), 5min (batch) - **Latency P99:** 500ms (streaming), 30min (batch) - **Throughput:** 100000 messages/second (streaming)",
              "score": null
            },
            {
              "doc_id": "runbook_database_connection_exhaustion",
              "chunk_id": "runbook_database_connection_exhaustion_fix_0",
              "content": "# Runbook: Database Connection Exhaustion ## Severity **Level:** P1 - Critical **Response Time:** 15 minutes ## Symptoms - Connection pool at 100% - Queries timing out - 'too many connections' errors in logs ## Diagnosis ### Step 1 Check active connections: `SELECT count(*) FROM pg_stat_activity` ### Step 2 Identify long-running queries: `SELECT pid, now() - pg_stat_activity.query_start AS duration, query FROM pg_stat_activity WHERE state != 'idle' ORDER BY duration DESC` ### Step 3 Check PgBouncer stats: `SHOW POOLS` ### Step 4 Verify connection limits: `SHOW max_connections` ## Resolution ### Step 1 Kill long-running queries: `SELECT pg_terminate_backend(pid) FROM pg_stat_activity WHERE duration > interval '5 minutes'` ### Step 2 Increase PgBouncer pool size temporarily: update pgbouncer.ini ### Step 3 Restart affected services to release connections: `kubectl rollout restart deployment/api` ### Step 4 Scale down non-critical services to free connections ## Escalation If connections remain exhausted, escalate to DBA. Consider enabling connection queueing. ## Post-Incident - Document timeline in incident log - Schedule post-mortem within 48 hours - Update runbook if new patterns discovered",
              "score": null
            }
          ],
          "latency_ms": 3.487735000817338,
          "query_id": "compare_023",
          "dimension": "original",
          "query": "What database options were considered and why was PostgreSQL chosen?"
        },
        {
          "key_facts": [
            "PostgreSQL",
            "MongoDB",
            "CockroachDB",
            "JSONB",
            "Team expertise"
          ],
          "found_facts": [
            "PostgreSQL",
            "MongoDB",
            "CockroachDB",
            "JSONB",
            "Team expertise"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "adr_001"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "adr_001",
              "chunk_id": "adr_001_fix_0",
              "content": "# ADR-001: Use PostgreSQL for Primary Database **Status:** Accepted **Date:** 2024-11-04 ## Context We need a database to store workflow definitions, execution state, and user data. The database must support ACID transactions, JSON queries, and scale to millions of records. ## Options Considered ### Option 1: PostgreSQL Mature relational database with JSONB support. **Pros:** - ACID compliance - Rich JSONB queries - Team expertise - Proven at scale **Cons:** - Manual sharding for extreme scale - Requires connection pooling ### Option 2: MongoDB Document database with native JSON. **Pros:** - Flexible schema - Built-in sharding - Good for documents **Cons:** - Weaker consistency guarantees - Complex transactions - Higher operational cost ### Option 3: CockroachDB Distributed SQL database. **Pros:** - Automatic sharding - Strong consistency - PostgreSQL compatible **Cons:** - Higher latency - More expensive - Less mature ecosystem ## Decision We will use PostgreSQL as our primary database with JSONB for flexible workflow definitions. ## Consequences - Need PgBouncer for connection pooling - Must plan for sharding at 100M+ records - Requires regular VACUUM maintenance",
              "score": null
            },
            {
              "doc_id": "arch_data_pipeline",
              "chunk_id": "arch_data_pipeline_fix_0",
              "content": "# Data Pipeline Architecture ## Overview The Data Pipeline processes and transforms data between workflow steps. It supports batch processing up to 10GB and streaming for real-time use cases. ## Design Principles - **Exactly-Once Processing**: Each record is processed exactly once - **Backpressure Handling**: System gracefully handles varying load ## Components ### Stream Processor Handles real-time data streams. Maximum throughput of 100000 messages per second. **Technology:** Apache Kafka with Flink **Scaling:** Partition-based scaling ### Batch Processor Processes large batch jobs. Supports data formats: JSON, CSV, Parquet, Avro. **Technology:** Apache Spark on Kubernetes **Scaling:** Dynamic resource allocation ### Data Lake Long-term data storage with time-travel queries. Retention configurable up to 7 years. **Technology:** S3 with Iceberg tables **Scaling:** Unlimited with tiered storage ## Data Flow Input \u2192 Validation \u2192 Transform \u2192 Enrich \u2192 Output ## Performance Characteristics - **Latency P50:** 100ms (streaming), 5min (batch) - **Latency P99:** 500ms (streaming), 30min (batch) - **Throughput:** 100000 messages/second (streaming)",
              "score": null
            },
            {
              "doc_id": "adr_008",
              "chunk_id": "adr_008_fix_0",
              "content": "# ADR-008: Use S3 for Object Storage **Status:** Accepted **Date:** 2024-10-14 ## Context We need storage for workflow artifacts, execution outputs, and user uploads. ## Options Considered ### Option 1: Amazon S3 Object storage service. **Pros:** - 99.999999999% durability - Lifecycle policies - Event notifications - CDN integration **Cons:** - AWS lock-in - Eventual consistency for overwrites - Cost at scale ### Option 2: Self-hosted MinIO S3-compatible object storage. **Pros:** - No vendor lock-in - S3 compatible - On-premises option **Cons:** - Operational burden - Lower durability guarantees - Scaling complexity ### Option 3: Google Cloud Storage GCP object storage. **Pros:** - Strong consistency - Good performance - Unified billing **Cons:** - GCP lock-in - Fewer features than S3 - Smaller ecosystem ## Decision We will use Amazon S3 for object storage with intelligent tiering for cost optimization. ## Consequences - Enable versioning for critical buckets - Implement lifecycle policies - Use pre-signed URLs for uploads - Enable server-side encryption",
              "score": null
            },
            {
              "doc_id": "arch_search_service",
              "chunk_id": "arch_search_service_fix_0",
              "content": "# Search Service Architecture ## Overview The Search Service provides full-text and semantic search across workflows, executions, and documentation. Built on Elasticsearch with custom analyzers for technical content. ## Design Principles - **Relevance First**: Results ranked by relevance, not recency - **Typo Tolerance**: Handles misspellings and variations ## Components ### Indexer Indexes documents in near real-time. Typical indexing latency is 500ms. **Technology:** Custom Go service **Scaling:** Queue-based with backpressure ### Query Engine Executes search queries. Supports filters, facets, and aggregations. **Technology:** Elasticsearch 8.x **Scaling:** 3-node cluster with replicas ### Suggestion Service Provides autocomplete suggestions. Returns suggestions within 20ms. **Technology:** Completion suggester **Scaling:** In-memory FST ## Data Flow Document \u2192 Analyzer \u2192 Index \u2192 Query \u2192 Rank \u2192 Results ## Performance Characteristics - **Latency P50:** 25ms - **Latency P99:** 100ms - **Throughput:** 2000 queries/second",
              "score": null
            },
            {
              "doc_id": "adr_004",
              "chunk_id": "adr_004_fix_0",
              "content": "# ADR-004: Use Redis for Caching and Session Storage **Status:** Accepted **Date:** 2024-04-05 ## Context We need a fast data store for caching API responses, session data, and rate limiting counters. ## Options Considered ### Option 1: Redis Cluster In-memory data structure store. **Pros:** - Sub-millisecond latency - Rich data structures - Pub/sub support - Clustering **Cons:** - Memory cost - Persistence complexity - Cluster management ### Option 2: Memcached Simple key-value cache. **Pros:** - Simple operation - Multi-threaded - Lower memory overhead **Cons:** - No persistence - Limited data types - No clustering ### Option 3: Application-level cache In-process caching only. **Pros:** - No external dependency - Fastest possible - Simple **Cons:** - Not shared across pods - Memory pressure - Cold starts ## Decision We will use Redis Cluster for caching with a 6-node cluster for high availability. ## Consequences - Implement cache-aside pattern - Set appropriate TTLs - Monitor memory usage - Plan for cache stampede prevention",
              "score": null
            }
          ],
          "latency_ms": 3.7178229995333822,
          "query_id": "compare_023",
          "dimension": "synonym",
          "query": "What data stores did we evaluate before picking Postgres?"
        },
        {
          "key_facts": [
            "PostgreSQL",
            "MongoDB",
            "CockroachDB",
            "JSONB",
            "Team expertise"
          ],
          "found_facts": [
            "PostgreSQL",
            "MongoDB",
            "CockroachDB",
            "JSONB",
            "Team expertise"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "adr_001"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "adr_001",
              "chunk_id": "adr_001_fix_0",
              "content": "# ADR-001: Use PostgreSQL for Primary Database **Status:** Accepted **Date:** 2024-11-04 ## Context We need a database to store workflow definitions, execution state, and user data. The database must support ACID transactions, JSON queries, and scale to millions of records. ## Options Considered ### Option 1: PostgreSQL Mature relational database with JSONB support. **Pros:** - ACID compliance - Rich JSONB queries - Team expertise - Proven at scale **Cons:** - Manual sharding for extreme scale - Requires connection pooling ### Option 2: MongoDB Document database with native JSON. **Pros:** - Flexible schema - Built-in sharding - Good for documents **Cons:** - Weaker consistency guarantees - Complex transactions - Higher operational cost ### Option 3: CockroachDB Distributed SQL database. **Pros:** - Automatic sharding - Strong consistency - PostgreSQL compatible **Cons:** - Higher latency - More expensive - Less mature ecosystem ## Decision We will use PostgreSQL as our primary database with JSONB for flexible workflow definitions. ## Consequences - Need PgBouncer for connection pooling - Must plan for sharding at 100M+ records - Requires regular VACUUM maintenance",
              "score": null
            },
            {
              "doc_id": "arch_search_service",
              "chunk_id": "arch_search_service_fix_0",
              "content": "# Search Service Architecture ## Overview The Search Service provides full-text and semantic search across workflows, executions, and documentation. Built on Elasticsearch with custom analyzers for technical content. ## Design Principles - **Relevance First**: Results ranked by relevance, not recency - **Typo Tolerance**: Handles misspellings and variations ## Components ### Indexer Indexes documents in near real-time. Typical indexing latency is 500ms. **Technology:** Custom Go service **Scaling:** Queue-based with backpressure ### Query Engine Executes search queries. Supports filters, facets, and aggregations. **Technology:** Elasticsearch 8.x **Scaling:** 3-node cluster with replicas ### Suggestion Service Provides autocomplete suggestions. Returns suggestions within 20ms. **Technology:** Completion suggester **Scaling:** In-memory FST ## Data Flow Document \u2192 Analyzer \u2192 Index \u2192 Query \u2192 Rank \u2192 Results ## Performance Characteristics - **Latency P50:** 25ms - **Latency P99:** 100ms - **Throughput:** 2000 queries/second",
              "score": null
            },
            {
              "doc_id": "config_database",
              "chunk_id": "config_database_fix_0",
              "content": "# Database Configuration Reference ## Overview This document describes all configuration options for Database. ## Environment Variables | Variable | Type | Default | Description | |----------|------|---------|-------------| | `DATABASE_URL` | string | required | PostgreSQL connection string | | `DATABASE_POOL_MIN` | integer | 5 | Minimum connection pool size | | `DATABASE_POOL_MAX` | integer | 20 | Maximum connection pool size | | `DATABASE_STATEMENT_TIMEOUT` | integer | 30000 | Query timeout in milliseconds | | `DATABASE_SSL_MODE` | string | require | SSL mode: disable, require, verify-full | | `DATABASE_MIGRATION_AUTO` | boolean | false | Run migrations on startup | ## Configuration File Configuration can also be provided via `database.yaml`: ```yaml database_url: required database_pool_min: 5 database_pool_max: 20 database_statement_timeout: 30000 database_ssl_mode: require ``` ## Validation Configuration is validated at startup. Invalid configuration will prevent the service from starting.",
              "score": null
            },
            {
              "doc_id": "arch_monitoring_stack",
              "chunk_id": "arch_monitoring_stack_fix_0",
              "content": "# Monitoring Stack Architecture ## Overview The Monitoring Stack provides observability into system health, performance, and errors. Includes metrics, logs, traces, and alerting. ## Design Principles - **Observability**: Every component emits metrics and traces - **Actionable Alerts**: Alerts include runbook links ## Components ### Metrics Collects time-series metrics. Retention is 15 days local, 1 year in object storage. **Technology:** Prometheus with Thanos **Scaling:** Federated with long-term storage ### Logging Centralized log aggregation. Supports structured JSON logs with label indexing. **Technology:** Loki with Grafana **Scaling:** Distributed with sharding ### Tracing Distributed request tracing. Trace retention is 7 days. **Technology:** Jaeger with OpenTelemetry **Scaling:** Sampling at 10% ### Alerting Routes alerts to on-call engineers. Supports escalation policies and silencing. **Technology:** Alertmanager with PagerDuty **Scaling:** HA pair ## Data Flow Application \u2192 Collector \u2192 Storage \u2192 Query \u2192 Dashboard/Alert ## Performance Characteristics - **Latency P50:** N/A - **Latency P99:** N/A - **Throughput:** 1M metrics/second, 100GB logs/day",
              "score": null
            },
            {
              "doc_id": "config_workflow_engine",
              "chunk_id": "config_workflow_engine_fix_0",
              "content": "# Workflow Engine Configuration Reference ## Overview This document describes all configuration options for Workflow Engine. ## Environment Variables | Variable | Type | Default | Description | |----------|------|---------|-------------| | `WORKFLOW_MAX_STEPS` | integer | 100 | Maximum steps per workflow | | `WORKFLOW_TIMEOUT_SECONDS` | integer | 3600 | Maximum workflow execution time | | `WORKFLOW_RETRY_LIMIT` | integer | 3 | Default retry attempts for failed steps | | `WORKFLOW_CONCURRENT_LIMIT` | integer | 10 | Maximum concurrent executions per workflow | | `EXECUTOR_POOL_SIZE` | integer | 50 | Number of executor workers | | `EXECUTOR_MEMORY_LIMIT` | string | 512Mi | Memory limit per executor | ## Configuration File Configuration can also be provided via `workflow engine.yaml`: ```yaml workflow_max_steps: 100 workflow_timeout_seconds: 3600 workflow_retry_limit: 3 workflow_concurrent_limit: 10 executor_pool_size: 50 ``` ## Validation Configuration is validated at startup. Invalid configuration will prevent the service from starting.",
              "score": null
            }
          ],
          "latency_ms": 4.207625999697484,
          "query_id": "compare_023",
          "dimension": "problem",
          "query": "We're reviewing our db choice and need to document the alternatives we rejected"
        },
        {
          "key_facts": [
            "PostgreSQL",
            "MongoDB",
            "CockroachDB",
            "JSONB",
            "Team expertise"
          ],
          "found_facts": [
            "PostgreSQL",
            "MongoDB",
            "CockroachDB",
            "JSONB",
            "Team expertise"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "adr_001"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "adr_001",
              "chunk_id": "adr_001_fix_0",
              "content": "# ADR-001: Use PostgreSQL for Primary Database **Status:** Accepted **Date:** 2024-11-04 ## Context We need a database to store workflow definitions, execution state, and user data. The database must support ACID transactions, JSON queries, and scale to millions of records. ## Options Considered ### Option 1: PostgreSQL Mature relational database with JSONB support. **Pros:** - ACID compliance - Rich JSONB queries - Team expertise - Proven at scale **Cons:** - Manual sharding for extreme scale - Requires connection pooling ### Option 2: MongoDB Document database with native JSON. **Pros:** - Flexible schema - Built-in sharding - Good for documents **Cons:** - Weaker consistency guarantees - Complex transactions - Higher operational cost ### Option 3: CockroachDB Distributed SQL database. **Pros:** - Automatic sharding - Strong consistency - PostgreSQL compatible **Cons:** - Higher latency - More expensive - Less mature ecosystem ## Decision We will use PostgreSQL as our primary database with JSONB for flexible workflow definitions. ## Consequences - Need PgBouncer for connection pooling - Must plan for sharding at 100M+ records - Requires regular VACUUM maintenance",
              "score": null
            },
            {
              "doc_id": "howto_set_up_database_connection",
              "chunk_id": "howto_set_up_database_connection_fix_0",
              "content": "# How to Set Up Database Connection ## Prerequisites - Database with network access to CloudFlow - Connection credentials ## Steps ### Step 1: Add connection secret Store your database connection string as a secret. ```bash cloudflow secrets set DATABASE_URL postgresql://user:pass@host:5432/db ``` ### Step 2: Configure connection pool Set pool size based on your plan limits. Free tier allows 5 connections, Pro allows 20. > **Note:** Pool exhaustion will queue requests ### Step 3: Test connection Verify the database is accessible from CloudFlow. ```bash cloudflow db test --secret DATABASE_URL ``` ### Step 4: Enable SSL For production, always require SSL connections. ```bash cloudflow db configure --require-ssl ``` ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            },
            {
              "doc_id": "adr_008",
              "chunk_id": "adr_008_fix_0",
              "content": "# ADR-008: Use S3 for Object Storage **Status:** Accepted **Date:** 2024-10-14 ## Context We need storage for workflow artifacts, execution outputs, and user uploads. ## Options Considered ### Option 1: Amazon S3 Object storage service. **Pros:** - 99.999999999% durability - Lifecycle policies - Event notifications - CDN integration **Cons:** - AWS lock-in - Eventual consistency for overwrites - Cost at scale ### Option 2: Self-hosted MinIO S3-compatible object storage. **Pros:** - No vendor lock-in - S3 compatible - On-premises option **Cons:** - Operational burden - Lower durability guarantees - Scaling complexity ### Option 3: Google Cloud Storage GCP object storage. **Pros:** - Strong consistency - Good performance - Unified billing **Cons:** - GCP lock-in - Fewer features than S3 - Smaller ecosystem ## Decision We will use Amazon S3 for object storage with intelligent tiering for cost optimization. ## Consequences - Enable versioning for critical buckets - Implement lifecycle policies - Use pre-signed URLs for uploads - Enable server-side encryption",
              "score": null
            },
            {
              "doc_id": "arch_workflow_engine",
              "chunk_id": "arch_workflow_engine_fix_0",
              "content": "# Workflow Engine Architecture ## Overview The Workflow Engine is the core component responsible for executing user-defined workflows. It uses an event-driven architecture with support for parallel execution, conditional branching, and automatic retries. ## Design Principles - **Idempotency**: All operations can be safely retried without side effects - **Eventual Consistency**: State converges to consistent view within 5 seconds - **Fault Tolerance**: System continues operating despite component failures ## Components ### Scheduler Manages workflow scheduling and triggers. Supports cron expressions, webhooks, and event-based triggers. **Technology:** Custom Go service **Scaling:** Horizontal with leader election ### Executor Runs individual workflow steps in isolated containers. Maximum execution time is 30 minutes per step. **Technology:** Kubernetes Jobs **Scaling:** Auto-scaling based on queue depth ### State Store Persists workflow state, execution history, and step results. Uses optimistic locking for concurrent updates. **Technology:** PostgreSQL with JSONB **Scaling:** Primary-replica with read replicas ## Data Flow Triggers \u2192 Scheduler \u2192 Job Queue \u2192 Executor \u2192 State Store \u2192 Webhooks/Notifications ## Performance Characteristics - **Latency P50:** 45ms - **Latency P99:** 250ms - **Throughput:** 5000 executions/minute",
              "score": null
            },
            {
              "doc_id": "arch_data_pipeline",
              "chunk_id": "arch_data_pipeline_fix_0",
              "content": "# Data Pipeline Architecture ## Overview The Data Pipeline processes and transforms data between workflow steps. It supports batch processing up to 10GB and streaming for real-time use cases. ## Design Principles - **Exactly-Once Processing**: Each record is processed exactly once - **Backpressure Handling**: System gracefully handles varying load ## Components ### Stream Processor Handles real-time data streams. Maximum throughput of 100000 messages per second. **Technology:** Apache Kafka with Flink **Scaling:** Partition-based scaling ### Batch Processor Processes large batch jobs. Supports data formats: JSON, CSV, Parquet, Avro. **Technology:** Apache Spark on Kubernetes **Scaling:** Dynamic resource allocation ### Data Lake Long-term data storage with time-travel queries. Retention configurable up to 7 years. **Technology:** S3 with Iceberg tables **Scaling:** Unlimited with tiered storage ## Data Flow Input \u2192 Validation \u2192 Transform \u2192 Enrich \u2192 Output ## Performance Characteristics - **Latency P50:** 100ms (streaming), 5min (batch) - **Latency P99:** 500ms (streaming), 30min (batch) - **Throughput:** 100000 messages/second (streaming)",
              "score": null
            }
          ],
          "latency_ms": 3.707433999807108,
          "query_id": "compare_023",
          "dimension": "casual",
          "query": "postgres vs mongodb vs cockroach decision"
        },
        {
          "key_facts": [
            "PostgreSQL",
            "MongoDB",
            "CockroachDB",
            "JSONB",
            "Team expertise"
          ],
          "found_facts": [
            "PostgreSQL",
            "MongoDB",
            "CockroachDB",
            "JSONB",
            "Team expertise"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "adr_001"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "adr_001",
              "chunk_id": "adr_001_fix_0",
              "content": "# ADR-001: Use PostgreSQL for Primary Database **Status:** Accepted **Date:** 2024-11-04 ## Context We need a database to store workflow definitions, execution state, and user data. The database must support ACID transactions, JSON queries, and scale to millions of records. ## Options Considered ### Option 1: PostgreSQL Mature relational database with JSONB support. **Pros:** - ACID compliance - Rich JSONB queries - Team expertise - Proven at scale **Cons:** - Manual sharding for extreme scale - Requires connection pooling ### Option 2: MongoDB Document database with native JSON. **Pros:** - Flexible schema - Built-in sharding - Good for documents **Cons:** - Weaker consistency guarantees - Complex transactions - Higher operational cost ### Option 3: CockroachDB Distributed SQL database. **Pros:** - Automatic sharding - Strong consistency - PostgreSQL compatible **Cons:** - Higher latency - More expensive - Less mature ecosystem ## Decision We will use PostgreSQL as our primary database with JSONB for flexible workflow definitions. ## Consequences - Need PgBouncer for connection pooling - Must plan for sharding at 100M+ records - Requires regular VACUUM maintenance",
              "score": null
            },
            {
              "doc_id": "howto_set_up_database_connection",
              "chunk_id": "howto_set_up_database_connection_fix_0",
              "content": "# How to Set Up Database Connection ## Prerequisites - Database with network access to CloudFlow - Connection credentials ## Steps ### Step 1: Add connection secret Store your database connection string as a secret. ```bash cloudflow secrets set DATABASE_URL postgresql://user:pass@host:5432/db ``` ### Step 2: Configure connection pool Set pool size based on your plan limits. Free tier allows 5 connections, Pro allows 20. > **Note:** Pool exhaustion will queue requests ### Step 3: Test connection Verify the database is accessible from CloudFlow. ```bash cloudflow db test --secret DATABASE_URL ``` ### Step 4: Enable SSL For production, always require SSL connections. ```bash cloudflow db configure --require-ssl ``` ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            },
            {
              "doc_id": "config_database",
              "chunk_id": "config_database_fix_0",
              "content": "# Database Configuration Reference ## Overview This document describes all configuration options for Database. ## Environment Variables | Variable | Type | Default | Description | |----------|------|---------|-------------| | `DATABASE_URL` | string | required | PostgreSQL connection string | | `DATABASE_POOL_MIN` | integer | 5 | Minimum connection pool size | | `DATABASE_POOL_MAX` | integer | 20 | Maximum connection pool size | | `DATABASE_STATEMENT_TIMEOUT` | integer | 30000 | Query timeout in milliseconds | | `DATABASE_SSL_MODE` | string | require | SSL mode: disable, require, verify-full | | `DATABASE_MIGRATION_AUTO` | boolean | false | Run migrations on startup | ## Configuration File Configuration can also be provided via `database.yaml`: ```yaml database_url: required database_pool_min: 5 database_pool_max: 20 database_statement_timeout: 30000 database_ssl_mode: require ``` ## Validation Configuration is validated at startup. Invalid configuration will prevent the service from starting.",
              "score": null
            },
            {
              "doc_id": "adr_008",
              "chunk_id": "adr_008_fix_0",
              "content": "# ADR-008: Use S3 for Object Storage **Status:** Accepted **Date:** 2024-10-14 ## Context We need storage for workflow artifacts, execution outputs, and user uploads. ## Options Considered ### Option 1: Amazon S3 Object storage service. **Pros:** - 99.999999999% durability - Lifecycle policies - Event notifications - CDN integration **Cons:** - AWS lock-in - Eventual consistency for overwrites - Cost at scale ### Option 2: Self-hosted MinIO S3-compatible object storage. **Pros:** - No vendor lock-in - S3 compatible - On-premises option **Cons:** - Operational burden - Lower durability guarantees - Scaling complexity ### Option 3: Google Cloud Storage GCP object storage. **Pros:** - Strong consistency - Good performance - Unified billing **Cons:** - GCP lock-in - Fewer features than S3 - Smaller ecosystem ## Decision We will use Amazon S3 for object storage with intelligent tiering for cost optimization. ## Consequences - Enable versioning for critical buckets - Implement lifecycle policies - Use pre-signed URLs for uploads - Enable server-side encryption",
              "score": null
            },
            {
              "doc_id": "release_3_1_0",
              "chunk_id": "release_3_1_0_fix_0",
              "content": "# Release Notes - v3.1.0 **Release Date:** 2023-12-01 ## New Features - GraphQL API beta release - Workflow templates marketplace - Enhanced audit logging - SSO support for Okta ## Bug Fixes - Fixed webhook delivery retry logic - Resolved issue with large payload handling - Fixed dashboard loading performance ## Upgrade Instructions ```bash cloudflow upgrade --version 3.1.0 ``` ## Known Issues None at this time.",
              "score": null
            }
          ],
          "latency_ms": 4.134669999984908,
          "query_id": "compare_023",
          "dimension": "contextual",
          "query": "New team member is asking why we use PostgreSQL instead of MongoDB - what's the rationale?"
        },
        {
          "key_facts": [
            "PostgreSQL",
            "MongoDB",
            "CockroachDB",
            "JSONB",
            "Team expertise"
          ],
          "found_facts": [
            "PostgreSQL",
            "MongoDB",
            "CockroachDB",
            "JSONB",
            "Team expertise"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "adr_001"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "adr_001",
              "chunk_id": "adr_001_fix_0",
              "content": "# ADR-001: Use PostgreSQL for Primary Database **Status:** Accepted **Date:** 2024-11-04 ## Context We need a database to store workflow definitions, execution state, and user data. The database must support ACID transactions, JSON queries, and scale to millions of records. ## Options Considered ### Option 1: PostgreSQL Mature relational database with JSONB support. **Pros:** - ACID compliance - Rich JSONB queries - Team expertise - Proven at scale **Cons:** - Manual sharding for extreme scale - Requires connection pooling ### Option 2: MongoDB Document database with native JSON. **Pros:** - Flexible schema - Built-in sharding - Good for documents **Cons:** - Weaker consistency guarantees - Complex transactions - Higher operational cost ### Option 3: CockroachDB Distributed SQL database. **Pros:** - Automatic sharding - Strong consistency - PostgreSQL compatible **Cons:** - Higher latency - More expensive - Less mature ecosystem ## Decision We will use PostgreSQL as our primary database with JSONB for flexible workflow definitions. ## Consequences - Need PgBouncer for connection pooling - Must plan for sharding at 100M+ records - Requires regular VACUUM maintenance",
              "score": null
            },
            {
              "doc_id": "adr_008",
              "chunk_id": "adr_008_fix_0",
              "content": "# ADR-008: Use S3 for Object Storage **Status:** Accepted **Date:** 2024-10-14 ## Context We need storage for workflow artifacts, execution outputs, and user uploads. ## Options Considered ### Option 1: Amazon S3 Object storage service. **Pros:** - 99.999999999% durability - Lifecycle policies - Event notifications - CDN integration **Cons:** - AWS lock-in - Eventual consistency for overwrites - Cost at scale ### Option 2: Self-hosted MinIO S3-compatible object storage. **Pros:** - No vendor lock-in - S3 compatible - On-premises option **Cons:** - Operational burden - Lower durability guarantees - Scaling complexity ### Option 3: Google Cloud Storage GCP object storage. **Pros:** - Strong consistency - Good performance - Unified billing **Cons:** - GCP lock-in - Fewer features than S3 - Smaller ecosystem ## Decision We will use Amazon S3 for object storage with intelligent tiering for cost optimization. ## Consequences - Enable versioning for critical buckets - Implement lifecycle policies - Use pre-signed URLs for uploads - Enable server-side encryption",
              "score": null
            },
            {
              "doc_id": "arch_data_pipeline",
              "chunk_id": "arch_data_pipeline_fix_0",
              "content": "# Data Pipeline Architecture ## Overview The Data Pipeline processes and transforms data between workflow steps. It supports batch processing up to 10GB and streaming for real-time use cases. ## Design Principles - **Exactly-Once Processing**: Each record is processed exactly once - **Backpressure Handling**: System gracefully handles varying load ## Components ### Stream Processor Handles real-time data streams. Maximum throughput of 100000 messages per second. **Technology:** Apache Kafka with Flink **Scaling:** Partition-based scaling ### Batch Processor Processes large batch jobs. Supports data formats: JSON, CSV, Parquet, Avro. **Technology:** Apache Spark on Kubernetes **Scaling:** Dynamic resource allocation ### Data Lake Long-term data storage with time-travel queries. Retention configurable up to 7 years. **Technology:** S3 with Iceberg tables **Scaling:** Unlimited with tiered storage ## Data Flow Input \u2192 Validation \u2192 Transform \u2192 Enrich \u2192 Output ## Performance Characteristics - **Latency P50:** 100ms (streaming), 5min (batch) - **Latency P99:** 500ms (streaming), 30min (batch) - **Throughput:** 100000 messages/second (streaming)",
              "score": null
            },
            {
              "doc_id": "arch_monitoring_stack",
              "chunk_id": "arch_monitoring_stack_fix_0",
              "content": "# Monitoring Stack Architecture ## Overview The Monitoring Stack provides observability into system health, performance, and errors. Includes metrics, logs, traces, and alerting. ## Design Principles - **Observability**: Every component emits metrics and traces - **Actionable Alerts**: Alerts include runbook links ## Components ### Metrics Collects time-series metrics. Retention is 15 days local, 1 year in object storage. **Technology:** Prometheus with Thanos **Scaling:** Federated with long-term storage ### Logging Centralized log aggregation. Supports structured JSON logs with label indexing. **Technology:** Loki with Grafana **Scaling:** Distributed with sharding ### Tracing Distributed request tracing. Trace retention is 7 days. **Technology:** Jaeger with OpenTelemetry **Scaling:** Sampling at 10% ### Alerting Routes alerts to on-call engineers. Supports escalation policies and silencing. **Technology:** Alertmanager with PagerDuty **Scaling:** HA pair ## Data Flow Application \u2192 Collector \u2192 Storage \u2192 Query \u2192 Dashboard/Alert ## Performance Characteristics - **Latency P50:** N/A - **Latency P99:** N/A - **Throughput:** 1M metrics/second, 100GB logs/day",
              "score": null
            },
            {
              "doc_id": "arch_search_service",
              "chunk_id": "arch_search_service_fix_0",
              "content": "# Search Service Architecture ## Overview The Search Service provides full-text and semantic search across workflows, executions, and documentation. Built on Elasticsearch with custom analyzers for technical content. ## Design Principles - **Relevance First**: Results ranked by relevance, not recency - **Typo Tolerance**: Handles misspellings and variations ## Components ### Indexer Indexes documents in near real-time. Typical indexing latency is 500ms. **Technology:** Custom Go service **Scaling:** Queue-based with backpressure ### Query Engine Executes search queries. Supports filters, facets, and aggregations. **Technology:** Elasticsearch 8.x **Scaling:** 3-node cluster with replicas ### Suggestion Service Provides autocomplete suggestions. Returns suggestions within 20ms. **Technology:** Completion suggester **Scaling:** In-memory FST ## Data Flow Document \u2192 Analyzer \u2192 Index \u2192 Query \u2192 Rank \u2192 Results ## Performance Characteristics - **Latency P50:** 25ms - **Latency P99:** 100ms - **Throughput:** 2000 queries/second",
              "score": null
            }
          ],
          "latency_ms": 3.8862469991727266,
          "query_id": "compare_023",
          "dimension": "negation",
          "query": "Why didn't we go with MongoDB or CockroachDB?"
        },
        {
          "key_facts": [
            "Kubernetes",
            "ECS",
            "industry standard",
            "AWS lock-in",
            "ecosystem"
          ],
          "found_facts": [
            "Kubernetes",
            "ECS",
            "industry standard",
            "AWS lock-in",
            "ecosystem"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "adr_002"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "adr_002",
              "chunk_id": "adr_002_fix_0",
              "content": "# ADR-002: Adopt Kubernetes for Container Orchestration **Status:** Accepted **Date:** 2024-01-24 ## Context We need a platform to deploy and scale our microservices. The platform must support auto-scaling, rolling deployments, and multi-region deployment. ## Options Considered ### Option 1: Kubernetes (EKS) Industry-standard container orchestration. **Pros:** - Industry standard - Rich ecosystem - Cloud agnostic - Auto-scaling **Cons:** - Operational complexity - Learning curve - Resource overhead ### Option 2: AWS ECS Amazon's native container service. **Pros:** - Simple operation - Native AWS integration - Lower overhead **Cons:** - AWS lock-in - Less flexible - Smaller community ### Option 3: Serverless (Lambda) Function-as-a-service approach. **Pros:** - Zero infrastructure - Pay per use - Auto-scaling **Cons:** - Cold starts - 15-minute limit - Vendor lock-in ## Decision We will use Kubernetes on EKS for container orchestration with Helm for deployments. ## Consequences - Need platform team expertise - Implement GitOps with ArgoCD - Use managed node groups for easier operations",
              "score": null
            },
            {
              "doc_id": "arch_workflow_engine",
              "chunk_id": "arch_workflow_engine_fix_0",
              "content": "# Workflow Engine Architecture ## Overview The Workflow Engine is the core component responsible for executing user-defined workflows. It uses an event-driven architecture with support for parallel execution, conditional branching, and automatic retries. ## Design Principles - **Idempotency**: All operations can be safely retried without side effects - **Eventual Consistency**: State converges to consistent view within 5 seconds - **Fault Tolerance**: System continues operating despite component failures ## Components ### Scheduler Manages workflow scheduling and triggers. Supports cron expressions, webhooks, and event-based triggers. **Technology:** Custom Go service **Scaling:** Horizontal with leader election ### Executor Runs individual workflow steps in isolated containers. Maximum execution time is 30 minutes per step. **Technology:** Kubernetes Jobs **Scaling:** Auto-scaling based on queue depth ### State Store Persists workflow state, execution history, and step results. Uses optimistic locking for concurrent updates. **Technology:** PostgreSQL with JSONB **Scaling:** Primary-replica with read replicas ## Data Flow Triggers \u2192 Scheduler \u2192 Job Queue \u2192 Executor \u2192 State Store \u2192 Webhooks/Notifications ## Performance Characteristics - **Latency P50:** 45ms - **Latency P99:** 250ms - **Throughput:** 5000 executions/minute",
              "score": null
            },
            {
              "doc_id": "runbook_high_cpu_usage",
              "chunk_id": "runbook_high_cpu_usage_fix_0",
              "content": "# Runbook: High CPU Usage ## Severity **Level:** P1 - Critical **Response Time:** 15 minutes ## Symptoms - CPU utilization > 80% sustained - Increased response latency - Autoscaler at maximum replicas ## Diagnosis ### Step 1 Check current CPU usage: `kubectl top pods -n production` ### Step 2 Identify hot pods: `kubectl top pods --sort-by=cpu -n production | head -10` ### Step 3 Check for recent deployments: `kubectl rollout history deployment/api -n production` ### Step 4 Profile the application: `cloudflow debug profile --pod api-xyz --duration 60s` ## Resolution ### Step 1 Scale up if traffic-related: `kubectl scale deployment/api --replicas=10 -n production` ### Step 2 Restart pods if memory leak suspected: `kubectl rollout restart deployment/api -n production` ### Step 3 Roll back recent deployment if regression: `kubectl rollout undo deployment/api -n production` ### Step 4 Add resource limits if unbounded: update deployment with CPU limits ## Escalation If CPU remains high after scaling, escalate to platform team. Page on-call if affecting users. ## Post-Incident - Document timeline in incident log - Schedule post-mortem within 48 hours - Update runbook if new patterns discovered",
              "score": null
            },
            {
              "doc_id": "arch_api_gateway",
              "chunk_id": "arch_api_gateway_fix_0",
              "content": "# API Gateway Architecture ## Overview The API Gateway handles all incoming HTTP requests, providing authentication, rate limiting, and request routing. Built on Kong with custom plugins for CloudFlow-specific functionality. ## Design Principles - **Zero Trust**: Every request is authenticated and authorized - **Defense in Depth**: Multiple layers of security validation ## Components ### Kong Gateway Primary ingress controller. Handles TLS termination, request routing, and plugin execution. **Technology:** Kong 3.4 on Kubernetes **Scaling:** Horizontal with HPA ### Auth Plugin Validates JWT tokens, API keys, and OAuth2 flows. Token validation latency under 5ms. **Technology:** Custom Lua plugin **Scaling:** Stateless ### Rate Limiter Enforces per-user and per-IP rate limits. Default limit is 100 requests per minute. **Technology:** Redis-backed sliding window **Scaling:** Shared Redis cluster ## Data Flow Client \u2192 TLS \u2192 Kong \u2192 Auth \u2192 Rate Limit \u2192 Backend Services ## Performance Characteristics - **Latency P50:** 12ms - **Latency P99:** 45ms - **Throughput:** 50000 requests/second",
              "score": null
            },
            {
              "doc_id": "runbook_api_gateway_5xx_errors",
              "chunk_id": "runbook_api_gateway_5xx_errors_fix_0",
              "content": "# Runbook: API Gateway 5xx Errors ## Severity **Level:** P3 - Medium **Response Time:** 1 hour ## Symptoms - Error rate > 1% - 5xx responses in access logs - Downstream service failures ## Diagnosis ### Step 1 Check Kong error logs: `kubectl logs -l app=kong -n kong --tail=100 | grep -E '5[0-9]{2}'` ### Step 2 Verify backend health: `kubectl get pods -n production -o wide` ### Step 3 Check service endpoints: `kubectl get endpoints -n production` ### Step 4 Test backend directly: `kubectl exec -it kong-xxx -n kong -- curl http://api.production.svc:8080/health` ## Resolution ### Step 1 Restart unhealthy backends: `kubectl rollout restart deployment/api -n production` ### Step 2 Enable circuit breaker if cascading: update Kong circuit-breaker plugin ### Step 3 Scale up backends: `kubectl scale deployment/api --replicas=5 -n production` ### Step 4 Check for resource exhaustion on backend pods ### Step 5 Verify network policies allow traffic ## Escalation If 5xx persists > 15 minutes, page on-call. May indicate infrastructure issue. ## Post-Incident - Document timeline in incident log - Schedule post-mortem within 48 hours - Update runbook if new patterns discovered",
              "score": null
            }
          ],
          "latency_ms": 3.2248950010398403,
          "query_id": "compare_024",
          "dimension": "original",
          "query": "Compare Kubernetes vs ECS for container orchestration"
        },
        {
          "key_facts": [
            "Kubernetes",
            "ECS",
            "industry standard",
            "AWS lock-in",
            "ecosystem"
          ],
          "found_facts": [
            "Kubernetes",
            "ECS",
            "industry standard",
            "AWS lock-in",
            "ecosystem"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "adr_002"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "adr_002",
              "chunk_id": "adr_002_fix_0",
              "content": "# ADR-002: Adopt Kubernetes for Container Orchestration **Status:** Accepted **Date:** 2024-01-24 ## Context We need a platform to deploy and scale our microservices. The platform must support auto-scaling, rolling deployments, and multi-region deployment. ## Options Considered ### Option 1: Kubernetes (EKS) Industry-standard container orchestration. **Pros:** - Industry standard - Rich ecosystem - Cloud agnostic - Auto-scaling **Cons:** - Operational complexity - Learning curve - Resource overhead ### Option 2: AWS ECS Amazon's native container service. **Pros:** - Simple operation - Native AWS integration - Lower overhead **Cons:** - AWS lock-in - Less flexible - Smaller community ### Option 3: Serverless (Lambda) Function-as-a-service approach. **Pros:** - Zero infrastructure - Pay per use - Auto-scaling **Cons:** - Cold starts - 15-minute limit - Vendor lock-in ## Decision We will use Kubernetes on EKS for container orchestration with Helm for deployments. ## Consequences - Need platform team expertise - Implement GitOps with ArgoCD - Use managed node groups for easier operations",
              "score": null
            },
            {
              "doc_id": "adr_008",
              "chunk_id": "adr_008_fix_0",
              "content": "# ADR-008: Use S3 for Object Storage **Status:** Accepted **Date:** 2024-10-14 ## Context We need storage for workflow artifacts, execution outputs, and user uploads. ## Options Considered ### Option 1: Amazon S3 Object storage service. **Pros:** - 99.999999999% durability - Lifecycle policies - Event notifications - CDN integration **Cons:** - AWS lock-in - Eventual consistency for overwrites - Cost at scale ### Option 2: Self-hosted MinIO S3-compatible object storage. **Pros:** - No vendor lock-in - S3 compatible - On-premises option **Cons:** - Operational burden - Lower durability guarantees - Scaling complexity ### Option 3: Google Cloud Storage GCP object storage. **Pros:** - Strong consistency - Good performance - Unified billing **Cons:** - GCP lock-in - Fewer features than S3 - Smaller ecosystem ## Decision We will use Amazon S3 for object storage with intelligent tiering for cost optimization. ## Consequences - Enable versioning for critical buckets - Implement lifecycle policies - Use pre-signed URLs for uploads - Enable server-side encryption",
              "score": null
            },
            {
              "doc_id": "runbook_high_cpu_usage",
              "chunk_id": "runbook_high_cpu_usage_fix_0",
              "content": "# Runbook: High CPU Usage ## Severity **Level:** P1 - Critical **Response Time:** 15 minutes ## Symptoms - CPU utilization > 80% sustained - Increased response latency - Autoscaler at maximum replicas ## Diagnosis ### Step 1 Check current CPU usage: `kubectl top pods -n production` ### Step 2 Identify hot pods: `kubectl top pods --sort-by=cpu -n production | head -10` ### Step 3 Check for recent deployments: `kubectl rollout history deployment/api -n production` ### Step 4 Profile the application: `cloudflow debug profile --pod api-xyz --duration 60s` ## Resolution ### Step 1 Scale up if traffic-related: `kubectl scale deployment/api --replicas=10 -n production` ### Step 2 Restart pods if memory leak suspected: `kubectl rollout restart deployment/api -n production` ### Step 3 Roll back recent deployment if regression: `kubectl rollout undo deployment/api -n production` ### Step 4 Add resource limits if unbounded: update deployment with CPU limits ## Escalation If CPU remains high after scaling, escalate to platform team. Page on-call if affecting users. ## Post-Incident - Document timeline in incident log - Schedule post-mortem within 48 hours - Update runbook if new patterns discovered",
              "score": null
            },
            {
              "doc_id": "arch_workflow_engine",
              "chunk_id": "arch_workflow_engine_fix_0",
              "content": "# Workflow Engine Architecture ## Overview The Workflow Engine is the core component responsible for executing user-defined workflows. It uses an event-driven architecture with support for parallel execution, conditional branching, and automatic retries. ## Design Principles - **Idempotency**: All operations can be safely retried without side effects - **Eventual Consistency**: State converges to consistent view within 5 seconds - **Fault Tolerance**: System continues operating despite component failures ## Components ### Scheduler Manages workflow scheduling and triggers. Supports cron expressions, webhooks, and event-based triggers. **Technology:** Custom Go service **Scaling:** Horizontal with leader election ### Executor Runs individual workflow steps in isolated containers. Maximum execution time is 30 minutes per step. **Technology:** Kubernetes Jobs **Scaling:** Auto-scaling based on queue depth ### State Store Persists workflow state, execution history, and step results. Uses optimistic locking for concurrent updates. **Technology:** PostgreSQL with JSONB **Scaling:** Primary-replica with read replicas ## Data Flow Triggers \u2192 Scheduler \u2192 Job Queue \u2192 Executor \u2192 State Store \u2192 Webhooks/Notifications ## Performance Characteristics - **Latency P50:** 45ms - **Latency P99:** 250ms - **Throughput:** 5000 executions/minute",
              "score": null
            },
            {
              "doc_id": "arch_api_gateway",
              "chunk_id": "arch_api_gateway_fix_0",
              "content": "# API Gateway Architecture ## Overview The API Gateway handles all incoming HTTP requests, providing authentication, rate limiting, and request routing. Built on Kong with custom plugins for CloudFlow-specific functionality. ## Design Principles - **Zero Trust**: Every request is authenticated and authorized - **Defense in Depth**: Multiple layers of security validation ## Components ### Kong Gateway Primary ingress controller. Handles TLS termination, request routing, and plugin execution. **Technology:** Kong 3.4 on Kubernetes **Scaling:** Horizontal with HPA ### Auth Plugin Validates JWT tokens, API keys, and OAuth2 flows. Token validation latency under 5ms. **Technology:** Custom Lua plugin **Scaling:** Stateless ### Rate Limiter Enforces per-user and per-IP rate limits. Default limit is 100 requests per minute. **Technology:** Redis-backed sliding window **Scaling:** Shared Redis cluster ## Data Flow Client \u2192 TLS \u2192 Kong \u2192 Auth \u2192 Rate Limit \u2192 Backend Services ## Performance Characteristics - **Latency P50:** 12ms - **Latency P99:** 45ms - **Throughput:** 50000 requests/second",
              "score": null
            }
          ],
          "latency_ms": 3.4411379983794177,
          "query_id": "compare_024",
          "dimension": "synonym",
          "query": "K8s versus AWS container service - what's the difference?"
        },
        {
          "key_facts": [
            "Kubernetes",
            "ECS",
            "industry standard",
            "AWS lock-in",
            "ecosystem"
          ],
          "found_facts": [
            "Kubernetes",
            "ECS",
            "industry standard",
            "AWS lock-in",
            "ecosystem"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "adr_002"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "adr_002",
              "chunk_id": "adr_002_fix_0",
              "content": "# ADR-002: Adopt Kubernetes for Container Orchestration **Status:** Accepted **Date:** 2024-01-24 ## Context We need a platform to deploy and scale our microservices. The platform must support auto-scaling, rolling deployments, and multi-region deployment. ## Options Considered ### Option 1: Kubernetes (EKS) Industry-standard container orchestration. **Pros:** - Industry standard - Rich ecosystem - Cloud agnostic - Auto-scaling **Cons:** - Operational complexity - Learning curve - Resource overhead ### Option 2: AWS ECS Amazon's native container service. **Pros:** - Simple operation - Native AWS integration - Lower overhead **Cons:** - AWS lock-in - Less flexible - Smaller community ### Option 3: Serverless (Lambda) Function-as-a-service approach. **Pros:** - Zero infrastructure - Pay per use - Auto-scaling **Cons:** - Cold starts - 15-minute limit - Vendor lock-in ## Decision We will use Kubernetes on EKS for container orchestration with Helm for deployments. ## Consequences - Need platform team expertise - Implement GitOps with ArgoCD - Use managed node groups for easier operations",
              "score": null
            },
            {
              "doc_id": "arch_workflow_engine",
              "chunk_id": "arch_workflow_engine_fix_0",
              "content": "# Workflow Engine Architecture ## Overview The Workflow Engine is the core component responsible for executing user-defined workflows. It uses an event-driven architecture with support for parallel execution, conditional branching, and automatic retries. ## Design Principles - **Idempotency**: All operations can be safely retried without side effects - **Eventual Consistency**: State converges to consistent view within 5 seconds - **Fault Tolerance**: System continues operating despite component failures ## Components ### Scheduler Manages workflow scheduling and triggers. Supports cron expressions, webhooks, and event-based triggers. **Technology:** Custom Go service **Scaling:** Horizontal with leader election ### Executor Runs individual workflow steps in isolated containers. Maximum execution time is 30 minutes per step. **Technology:** Kubernetes Jobs **Scaling:** Auto-scaling based on queue depth ### State Store Persists workflow state, execution history, and step results. Uses optimistic locking for concurrent updates. **Technology:** PostgreSQL with JSONB **Scaling:** Primary-replica with read replicas ## Data Flow Triggers \u2192 Scheduler \u2192 Job Queue \u2192 Executor \u2192 State Store \u2192 Webhooks/Notifications ## Performance Characteristics - **Latency P50:** 45ms - **Latency P99:** 250ms - **Throughput:** 5000 executions/minute",
              "score": null
            },
            {
              "doc_id": "adr_008",
              "chunk_id": "adr_008_fix_0",
              "content": "# ADR-008: Use S3 for Object Storage **Status:** Accepted **Date:** 2024-10-14 ## Context We need storage for workflow artifacts, execution outputs, and user uploads. ## Options Considered ### Option 1: Amazon S3 Object storage service. **Pros:** - 99.999999999% durability - Lifecycle policies - Event notifications - CDN integration **Cons:** - AWS lock-in - Eventual consistency for overwrites - Cost at scale ### Option 2: Self-hosted MinIO S3-compatible object storage. **Pros:** - No vendor lock-in - S3 compatible - On-premises option **Cons:** - Operational burden - Lower durability guarantees - Scaling complexity ### Option 3: Google Cloud Storage GCP object storage. **Pros:** - Strong consistency - Good performance - Unified billing **Cons:** - GCP lock-in - Fewer features than S3 - Smaller ecosystem ## Decision We will use Amazon S3 for object storage with intelligent tiering for cost optimization. ## Consequences - Enable versioning for critical buckets - Implement lifecycle policies - Use pre-signed URLs for uploads - Enable server-side encryption",
              "score": null
            },
            {
              "doc_id": "adr_004",
              "chunk_id": "adr_004_fix_0",
              "content": "# ADR-004: Use Redis for Caching and Session Storage **Status:** Accepted **Date:** 2024-04-05 ## Context We need a fast data store for caching API responses, session data, and rate limiting counters. ## Options Considered ### Option 1: Redis Cluster In-memory data structure store. **Pros:** - Sub-millisecond latency - Rich data structures - Pub/sub support - Clustering **Cons:** - Memory cost - Persistence complexity - Cluster management ### Option 2: Memcached Simple key-value cache. **Pros:** - Simple operation - Multi-threaded - Lower memory overhead **Cons:** - No persistence - Limited data types - No clustering ### Option 3: Application-level cache In-process caching only. **Pros:** - No external dependency - Fastest possible - Simple **Cons:** - Not shared across pods - Memory pressure - Cold starts ## Decision We will use Redis Cluster for caching with a 6-node cluster for high availability. ## Consequences - Implement cache-aside pattern - Set appropriate TTLs - Monitor memory usage - Plan for cache stampede prevention",
              "score": null
            },
            {
              "doc_id": "arch_caching_layer",
              "chunk_id": "arch_caching_layer_fix_0",
              "content": "# Caching Layer Architecture ## Overview The Caching Layer reduces latency and database load by caching frequently accessed data. Uses a multi-tier strategy with local and distributed caches. ## Design Principles - **Cache Invalidation**: Explicit invalidation on writes - **Graceful Degradation**: System works without cache ## Components ### Local Cache First-level cache with 100ms TTL. Reduces Redis round-trips for hot data. **Technology:** Ristretto in-memory cache **Scaling:** Per-pod memory allocation ### Distributed Cache Second-level cache with configurable TTL. Maximum key size 512MB. **Technology:** Redis Cluster 7.x **Scaling:** 6-node cluster with sharding ### CDN Caches static assets and API responses. Cache hit ratio target 95%. **Technology:** CloudFront with Lambda@Edge **Scaling:** Global edge locations ## Data Flow Request \u2192 Local Cache \u2192 Redis \u2192 Database \u2192 Cache Population ## Performance Characteristics - **Latency P50:** 1ms (hit), 50ms (miss) - **Latency P99:** 5ms (hit), 200ms (miss) - **Throughput:** 100000 reads/second",
              "score": null
            }
          ],
          "latency_ms": 4.017061000922695,
          "query_id": "compare_024",
          "dimension": "problem",
          "query": "Trying to decide on our container platform and need to understand the vendor lock-in tradeoffs"
        },
        {
          "key_facts": [
            "Kubernetes",
            "ECS",
            "industry standard",
            "AWS lock-in",
            "ecosystem"
          ],
          "found_facts": [
            "Kubernetes",
            "ECS",
            "industry standard",
            "AWS lock-in",
            "ecosystem"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "adr_002"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "adr_002",
              "chunk_id": "adr_002_fix_0",
              "content": "# ADR-002: Adopt Kubernetes for Container Orchestration **Status:** Accepted **Date:** 2024-01-24 ## Context We need a platform to deploy and scale our microservices. The platform must support auto-scaling, rolling deployments, and multi-region deployment. ## Options Considered ### Option 1: Kubernetes (EKS) Industry-standard container orchestration. **Pros:** - Industry standard - Rich ecosystem - Cloud agnostic - Auto-scaling **Cons:** - Operational complexity - Learning curve - Resource overhead ### Option 2: AWS ECS Amazon's native container service. **Pros:** - Simple operation - Native AWS integration - Lower overhead **Cons:** - AWS lock-in - Less flexible - Smaller community ### Option 3: Serverless (Lambda) Function-as-a-service approach. **Pros:** - Zero infrastructure - Pay per use - Auto-scaling **Cons:** - Cold starts - 15-minute limit - Vendor lock-in ## Decision We will use Kubernetes on EKS for container orchestration with Helm for deployments. ## Consequences - Need platform team expertise - Implement GitOps with ArgoCD - Use managed node groups for easier operations",
              "score": null
            },
            {
              "doc_id": "adr_008",
              "chunk_id": "adr_008_fix_0",
              "content": "# ADR-008: Use S3 for Object Storage **Status:** Accepted **Date:** 2024-10-14 ## Context We need storage for workflow artifacts, execution outputs, and user uploads. ## Options Considered ### Option 1: Amazon S3 Object storage service. **Pros:** - 99.999999999% durability - Lifecycle policies - Event notifications - CDN integration **Cons:** - AWS lock-in - Eventual consistency for overwrites - Cost at scale ### Option 2: Self-hosted MinIO S3-compatible object storage. **Pros:** - No vendor lock-in - S3 compatible - On-premises option **Cons:** - Operational burden - Lower durability guarantees - Scaling complexity ### Option 3: Google Cloud Storage GCP object storage. **Pros:** - Strong consistency - Good performance - Unified billing **Cons:** - GCP lock-in - Fewer features than S3 - Smaller ecosystem ## Decision We will use Amazon S3 for object storage with intelligent tiering for cost optimization. ## Consequences - Enable versioning for critical buckets - Implement lifecycle policies - Use pre-signed URLs for uploads - Enable server-side encryption",
              "score": null
            },
            {
              "doc_id": "adr_006",
              "chunk_id": "adr_006_fix_0",
              "content": "# ADR-006: Use Terraform for Infrastructure as Code **Status:** Accepted **Date:** 2024-11-24 ## Context We need a way to manage cloud infrastructure consistently across environments. ## Options Considered ### Option 1: Terraform Multi-cloud IaC tool. **Pros:** - Multi-cloud support - Large community - State management - Plan before apply **Cons:** - State file management - Learning curve - HCL limitations ### Option 2: AWS CloudFormation AWS-native IaC. **Pros:** - Native AWS support - No state file - Drift detection **Cons:** - AWS only - Verbose YAML/JSON - Slower development ### Option 3: Pulumi IaC with real programming languages. **Pros:** - Real languages - Type safety - Reusable components **Cons:** - Smaller community - Vendor lock-in - Commercial features ## Decision We will use Terraform with S3 backend for state and DynamoDB for locking. ## Consequences - Implement module structure - Use workspaces for environments - Automate with Atlantis",
              "score": null
            },
            {
              "doc_id": "arch_api_gateway",
              "chunk_id": "arch_api_gateway_fix_0",
              "content": "# API Gateway Architecture ## Overview The API Gateway handles all incoming HTTP requests, providing authentication, rate limiting, and request routing. Built on Kong with custom plugins for CloudFlow-specific functionality. ## Design Principles - **Zero Trust**: Every request is authenticated and authorized - **Defense in Depth**: Multiple layers of security validation ## Components ### Kong Gateway Primary ingress controller. Handles TLS termination, request routing, and plugin execution. **Technology:** Kong 3.4 on Kubernetes **Scaling:** Horizontal with HPA ### Auth Plugin Validates JWT tokens, API keys, and OAuth2 flows. Token validation latency under 5ms. **Technology:** Custom Lua plugin **Scaling:** Stateless ### Rate Limiter Enforces per-user and per-IP rate limits. Default limit is 100 requests per minute. **Technology:** Redis-backed sliding window **Scaling:** Shared Redis cluster ## Data Flow Client \u2192 TLS \u2192 Kong \u2192 Auth \u2192 Rate Limit \u2192 Backend Services ## Performance Characteristics - **Latency P50:** 12ms - **Latency P99:** 45ms - **Throughput:** 50000 requests/second",
              "score": null
            },
            {
              "doc_id": "runbook_high_cpu_usage",
              "chunk_id": "runbook_high_cpu_usage_fix_0",
              "content": "# Runbook: High CPU Usage ## Severity **Level:** P1 - Critical **Response Time:** 15 minutes ## Symptoms - CPU utilization > 80% sustained - Increased response latency - Autoscaler at maximum replicas ## Diagnosis ### Step 1 Check current CPU usage: `kubectl top pods -n production` ### Step 2 Identify hot pods: `kubectl top pods --sort-by=cpu -n production | head -10` ### Step 3 Check for recent deployments: `kubectl rollout history deployment/api -n production` ### Step 4 Profile the application: `cloudflow debug profile --pod api-xyz --duration 60s` ## Resolution ### Step 1 Scale up if traffic-related: `kubectl scale deployment/api --replicas=10 -n production` ### Step 2 Restart pods if memory leak suspected: `kubectl rollout restart deployment/api -n production` ### Step 3 Roll back recent deployment if regression: `kubectl rollout undo deployment/api -n production` ### Step 4 Add resource limits if unbounded: update deployment with CPU limits ## Escalation If CPU remains high after scaling, escalate to platform team. Page on-call if affecting users. ## Post-Incident - Document timeline in incident log - Schedule post-mortem within 48 hours - Update runbook if new patterns discovered",
              "score": null
            }
          ],
          "latency_ms": 3.573855001377524,
          "query_id": "compare_024",
          "dimension": "casual",
          "query": "k8s vs ecs pros cons"
        },
        {
          "key_facts": [
            "Kubernetes",
            "ECS",
            "industry standard",
            "AWS lock-in",
            "ecosystem"
          ],
          "found_facts": [
            "Kubernetes",
            "ECS",
            "industry standard",
            "AWS lock-in",
            "ecosystem"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "adr_002"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "adr_002",
              "chunk_id": "adr_002_fix_0",
              "content": "# ADR-002: Adopt Kubernetes for Container Orchestration **Status:** Accepted **Date:** 2024-01-24 ## Context We need a platform to deploy and scale our microservices. The platform must support auto-scaling, rolling deployments, and multi-region deployment. ## Options Considered ### Option 1: Kubernetes (EKS) Industry-standard container orchestration. **Pros:** - Industry standard - Rich ecosystem - Cloud agnostic - Auto-scaling **Cons:** - Operational complexity - Learning curve - Resource overhead ### Option 2: AWS ECS Amazon's native container service. **Pros:** - Simple operation - Native AWS integration - Lower overhead **Cons:** - AWS lock-in - Less flexible - Smaller community ### Option 3: Serverless (Lambda) Function-as-a-service approach. **Pros:** - Zero infrastructure - Pay per use - Auto-scaling **Cons:** - Cold starts - 15-minute limit - Vendor lock-in ## Decision We will use Kubernetes on EKS for container orchestration with Helm for deployments. ## Consequences - Need platform team expertise - Implement GitOps with ArgoCD - Use managed node groups for easier operations",
              "score": null
            },
            {
              "doc_id": "adr_006",
              "chunk_id": "adr_006_fix_0",
              "content": "# ADR-006: Use Terraform for Infrastructure as Code **Status:** Accepted **Date:** 2024-11-24 ## Context We need a way to manage cloud infrastructure consistently across environments. ## Options Considered ### Option 1: Terraform Multi-cloud IaC tool. **Pros:** - Multi-cloud support - Large community - State management - Plan before apply **Cons:** - State file management - Learning curve - HCL limitations ### Option 2: AWS CloudFormation AWS-native IaC. **Pros:** - Native AWS support - No state file - Drift detection **Cons:** - AWS only - Verbose YAML/JSON - Slower development ### Option 3: Pulumi IaC with real programming languages. **Pros:** - Real languages - Type safety - Reusable components **Cons:** - Smaller community - Vendor lock-in - Commercial features ## Decision We will use Terraform with S3 backend for state and DynamoDB for locking. ## Consequences - Implement module structure - Use workspaces for environments - Automate with Atlantis",
              "score": null
            },
            {
              "doc_id": "arch_workflow_engine",
              "chunk_id": "arch_workflow_engine_fix_0",
              "content": "# Workflow Engine Architecture ## Overview The Workflow Engine is the core component responsible for executing user-defined workflows. It uses an event-driven architecture with support for parallel execution, conditional branching, and automatic retries. ## Design Principles - **Idempotency**: All operations can be safely retried without side effects - **Eventual Consistency**: State converges to consistent view within 5 seconds - **Fault Tolerance**: System continues operating despite component failures ## Components ### Scheduler Manages workflow scheduling and triggers. Supports cron expressions, webhooks, and event-based triggers. **Technology:** Custom Go service **Scaling:** Horizontal with leader election ### Executor Runs individual workflow steps in isolated containers. Maximum execution time is 30 minutes per step. **Technology:** Kubernetes Jobs **Scaling:** Auto-scaling based on queue depth ### State Store Persists workflow state, execution history, and step results. Uses optimistic locking for concurrent updates. **Technology:** PostgreSQL with JSONB **Scaling:** Primary-replica with read replicas ## Data Flow Triggers \u2192 Scheduler \u2192 Job Queue \u2192 Executor \u2192 State Store \u2192 Webhooks/Notifications ## Performance Characteristics - **Latency P50:** 45ms - **Latency P99:** 250ms - **Throughput:** 5000 executions/minute",
              "score": null
            },
            {
              "doc_id": "arch_api_gateway",
              "chunk_id": "arch_api_gateway_fix_0",
              "content": "# API Gateway Architecture ## Overview The API Gateway handles all incoming HTTP requests, providing authentication, rate limiting, and request routing. Built on Kong with custom plugins for CloudFlow-specific functionality. ## Design Principles - **Zero Trust**: Every request is authenticated and authorized - **Defense in Depth**: Multiple layers of security validation ## Components ### Kong Gateway Primary ingress controller. Handles TLS termination, request routing, and plugin execution. **Technology:** Kong 3.4 on Kubernetes **Scaling:** Horizontal with HPA ### Auth Plugin Validates JWT tokens, API keys, and OAuth2 flows. Token validation latency under 5ms. **Technology:** Custom Lua plugin **Scaling:** Stateless ### Rate Limiter Enforces per-user and per-IP rate limits. Default limit is 100 requests per minute. **Technology:** Redis-backed sliding window **Scaling:** Shared Redis cluster ## Data Flow Client \u2192 TLS \u2192 Kong \u2192 Auth \u2192 Rate Limit \u2192 Backend Services ## Performance Characteristics - **Latency P50:** 12ms - **Latency P99:** 45ms - **Throughput:** 50000 requests/second",
              "score": null
            },
            {
              "doc_id": "runbook_api_gateway_5xx_errors",
              "chunk_id": "runbook_api_gateway_5xx_errors_fix_0",
              "content": "# Runbook: API Gateway 5xx Errors ## Severity **Level:** P3 - Medium **Response Time:** 1 hour ## Symptoms - Error rate > 1% - 5xx responses in access logs - Downstream service failures ## Diagnosis ### Step 1 Check Kong error logs: `kubectl logs -l app=kong -n kong --tail=100 | grep -E '5[0-9]{2}'` ### Step 2 Verify backend health: `kubectl get pods -n production -o wide` ### Step 3 Check service endpoints: `kubectl get endpoints -n production` ### Step 4 Test backend directly: `kubectl exec -it kong-xxx -n kong -- curl http://api.production.svc:8080/health` ## Resolution ### Step 1 Restart unhealthy backends: `kubectl rollout restart deployment/api -n production` ### Step 2 Enable circuit breaker if cascading: update Kong circuit-breaker plugin ### Step 3 Scale up backends: `kubectl scale deployment/api --replicas=5 -n production` ### Step 4 Check for resource exhaustion on backend pods ### Step 5 Verify network policies allow traffic ## Escalation If 5xx persists > 15 minutes, page on-call. May indicate infrastructure issue. ## Post-Incident - Document timeline in incident log - Schedule post-mortem within 48 hours - Update runbook if new patterns discovered",
              "score": null
            }
          ],
          "latency_ms": 3.5580159983510384,
          "query_id": "compare_024",
          "dimension": "contextual",
          "query": "Infra team is proposing we move to ECS for simpler ops - what did we consider when we chose Kubernetes?"
        },
        {
          "key_facts": [
            "Kubernetes",
            "ECS",
            "industry standard",
            "AWS lock-in",
            "ecosystem"
          ],
          "found_facts": [
            "Kubernetes",
            "ECS",
            "industry standard",
            "AWS lock-in",
            "ecosystem"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "adr_002"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "adr_002",
              "chunk_id": "adr_002_fix_0",
              "content": "# ADR-002: Adopt Kubernetes for Container Orchestration **Status:** Accepted **Date:** 2024-01-24 ## Context We need a platform to deploy and scale our microservices. The platform must support auto-scaling, rolling deployments, and multi-region deployment. ## Options Considered ### Option 1: Kubernetes (EKS) Industry-standard container orchestration. **Pros:** - Industry standard - Rich ecosystem - Cloud agnostic - Auto-scaling **Cons:** - Operational complexity - Learning curve - Resource overhead ### Option 2: AWS ECS Amazon's native container service. **Pros:** - Simple operation - Native AWS integration - Lower overhead **Cons:** - AWS lock-in - Less flexible - Smaller community ### Option 3: Serverless (Lambda) Function-as-a-service approach. **Pros:** - Zero infrastructure - Pay per use - Auto-scaling **Cons:** - Cold starts - 15-minute limit - Vendor lock-in ## Decision We will use Kubernetes on EKS for container orchestration with Helm for deployments. ## Consequences - Need platform team expertise - Implement GitOps with ArgoCD - Use managed node groups for easier operations",
              "score": null
            },
            {
              "doc_id": "runbook_api_gateway_5xx_errors",
              "chunk_id": "runbook_api_gateway_5xx_errors_fix_0",
              "content": "# Runbook: API Gateway 5xx Errors ## Severity **Level:** P3 - Medium **Response Time:** 1 hour ## Symptoms - Error rate > 1% - 5xx responses in access logs - Downstream service failures ## Diagnosis ### Step 1 Check Kong error logs: `kubectl logs -l app=kong -n kong --tail=100 | grep -E '5[0-9]{2}'` ### Step 2 Verify backend health: `kubectl get pods -n production -o wide` ### Step 3 Check service endpoints: `kubectl get endpoints -n production` ### Step 4 Test backend directly: `kubectl exec -it kong-xxx -n kong -- curl http://api.production.svc:8080/health` ## Resolution ### Step 1 Restart unhealthy backends: `kubectl rollout restart deployment/api -n production` ### Step 2 Enable circuit breaker if cascading: update Kong circuit-breaker plugin ### Step 3 Scale up backends: `kubectl scale deployment/api --replicas=5 -n production` ### Step 4 Check for resource exhaustion on backend pods ### Step 5 Verify network policies allow traffic ## Escalation If 5xx persists > 15 minutes, page on-call. May indicate infrastructure issue. ## Post-Incident - Document timeline in incident log - Schedule post-mortem within 48 hours - Update runbook if new patterns discovered",
              "score": null
            },
            {
              "doc_id": "runbook_high_cpu_usage",
              "chunk_id": "runbook_high_cpu_usage_fix_0",
              "content": "# Runbook: High CPU Usage ## Severity **Level:** P1 - Critical **Response Time:** 15 minutes ## Symptoms - CPU utilization > 80% sustained - Increased response latency - Autoscaler at maximum replicas ## Diagnosis ### Step 1 Check current CPU usage: `kubectl top pods -n production` ### Step 2 Identify hot pods: `kubectl top pods --sort-by=cpu -n production | head -10` ### Step 3 Check for recent deployments: `kubectl rollout history deployment/api -n production` ### Step 4 Profile the application: `cloudflow debug profile --pod api-xyz --duration 60s` ## Resolution ### Step 1 Scale up if traffic-related: `kubectl scale deployment/api --replicas=10 -n production` ### Step 2 Restart pods if memory leak suspected: `kubectl rollout restart deployment/api -n production` ### Step 3 Roll back recent deployment if regression: `kubectl rollout undo deployment/api -n production` ### Step 4 Add resource limits if unbounded: update deployment with CPU limits ## Escalation If CPU remains high after scaling, escalate to platform team. Page on-call if affecting users. ## Post-Incident - Document timeline in incident log - Schedule post-mortem within 48 hours - Update runbook if new patterns discovered",
              "score": null
            },
            {
              "doc_id": "adr_006",
              "chunk_id": "adr_006_fix_0",
              "content": "# ADR-006: Use Terraform for Infrastructure as Code **Status:** Accepted **Date:** 2024-11-24 ## Context We need a way to manage cloud infrastructure consistently across environments. ## Options Considered ### Option 1: Terraform Multi-cloud IaC tool. **Pros:** - Multi-cloud support - Large community - State management - Plan before apply **Cons:** - State file management - Learning curve - HCL limitations ### Option 2: AWS CloudFormation AWS-native IaC. **Pros:** - Native AWS support - No state file - Drift detection **Cons:** - AWS only - Verbose YAML/JSON - Slower development ### Option 3: Pulumi IaC with real programming languages. **Pros:** - Real languages - Type safety - Reusable components **Cons:** - Smaller community - Vendor lock-in - Commercial features ## Decision We will use Terraform with S3 backend for state and DynamoDB for locking. ## Consequences - Implement module structure - Use workspaces for environments - Automate with Atlantis",
              "score": null
            },
            {
              "doc_id": "arch_workflow_engine",
              "chunk_id": "arch_workflow_engine_fix_0",
              "content": "# Workflow Engine Architecture ## Overview The Workflow Engine is the core component responsible for executing user-defined workflows. It uses an event-driven architecture with support for parallel execution, conditional branching, and automatic retries. ## Design Principles - **Idempotency**: All operations can be safely retried without side effects - **Eventual Consistency**: State converges to consistent view within 5 seconds - **Fault Tolerance**: System continues operating despite component failures ## Components ### Scheduler Manages workflow scheduling and triggers. Supports cron expressions, webhooks, and event-based triggers. **Technology:** Custom Go service **Scaling:** Horizontal with leader election ### Executor Runs individual workflow steps in isolated containers. Maximum execution time is 30 minutes per step. **Technology:** Kubernetes Jobs **Scaling:** Auto-scaling based on queue depth ### State Store Persists workflow state, execution history, and step results. Uses optimistic locking for concurrent updates. **Technology:** PostgreSQL with JSONB **Scaling:** Primary-replica with read replicas ## Data Flow Triggers \u2192 Scheduler \u2192 Job Queue \u2192 Executor \u2192 State Store \u2192 Webhooks/Notifications ## Performance Characteristics - **Latency P50:** 45ms - **Latency P99:** 250ms - **Throughput:** 5000 executions/minute",
              "score": null
            }
          ],
          "latency_ms": 3.5629849990073126,
          "query_id": "compare_024",
          "dimension": "negation",
          "query": "Why did we pick Kubernetes over ECS despite the complexity?"
        },
        {
          "key_facts": [
            "OAuth 2.0",
            "Session-based",
            "API keys",
            "stateless validation"
          ],
          "found_facts": [
            "OAuth 2.0",
            "Session-based",
            "API keys",
            "stateless validation"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "adr_003"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "arch_authentication_service",
              "chunk_id": "arch_authentication_service_fix_0",
              "content": "# Authentication Service Architecture ## Overview The Authentication Service handles user identity, session management, and token issuance. Supports OAuth 2.0, SAML, and API key authentication. ## Design Principles - **Zero Knowledge**: Passwords never stored in plaintext - **Token Rotation**: Refresh tokens rotate on each use ## Components ### Identity Provider Manages user accounts and credentials. Password hashing uses bcrypt with cost factor 12. **Technology:** Custom Go service **Scaling:** Horizontal stateless ### Token Service Issues and validates access tokens. Token lifetime is 1 hour with 7-day refresh tokens. **Technology:** JWT with RS256 **Scaling:** Horizontal stateless ### SSO Gateway Integrates with external identity providers. Supports Google, Microsoft, Okta, and custom SAML. **Technology:** Dex OIDC proxy **Scaling:** Active-passive ## Data Flow Credentials \u2192 Validation \u2192 Token Issue \u2192 Token Refresh \u2192 Logout ## Performance Characteristics - **Latency P50:** 50ms - **Latency P99:** 200ms - **Throughput:** 5000 auth/second",
              "score": null
            },
            {
              "doc_id": "adr_003",
              "chunk_id": "adr_003_fix_0",
              "content": "# ADR-003: Implement OAuth 2.0 with JWT for Authentication **Status:** Accepted **Date:** 2024-05-08 ## Context We need a secure authentication system that supports API keys, user sessions, and third-party integrations. ## Options Considered ### Option 1: OAuth 2.0 with JWT Industry standard token-based auth. **Pros:** - Industry standard - Stateless validation - Supports refresh tokens - SSO compatible **Cons:** - Token size overhead - Cannot revoke individual tokens - Complexity ### Option 2: Session-based auth Traditional server-side sessions. **Pros:** - Simple implementation - Easy revocation - Smaller payloads **Cons:** - Requires session store - Not suitable for APIs - Scaling challenges ### Option 3: API keys only Simple key-based authentication. **Pros:** - Very simple - Low overhead - Easy to implement **Cons:** - No user context - Hard to rotate - No SSO support ## Decision We will implement OAuth 2.0 with JWT tokens. Access tokens expire after 1 hour, refresh tokens after 7 days. ## Consequences - Implement token rotation on refresh - Support API keys for machine-to-machine auth - Add MFA for admin accounts",
              "score": null
            },
            {
              "doc_id": "howto_configure_single_sign_on",
              "chunk_id": "howto_configure_single_sign_on_fix_0",
              "content": "# How to Configure Single Sign-On ## Prerequisites - Admin access - Enterprise plan - Identity provider access ## Steps ### Step 1: Choose provider CloudFlow supports SAML 2.0, OpenID Connect, and OAuth 2.0 providers. ### Step 2: Get metadata Download CloudFlow's SAML metadata for your IdP configuration. ```bash cloudflow sso metadata --format xml > cloudflow-metadata.xml ``` ### Step 3: Configure IdP Add CloudFlow as a service provider in your identity provider. > **Note:** ACS URL: https://auth.cloudflow.io/saml/callback ### Step 4: Upload IdP metadata Provide your IdP's metadata to CloudFlow. ```bash cloudflow sso configure --idp-metadata ./idp-metadata.xml ``` ### Step 5: Test login Verify SSO login works correctly. ```bash cloudflow sso test ``` ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            },
            {
              "doc_id": "arch_api_gateway",
              "chunk_id": "arch_api_gateway_fix_0",
              "content": "# API Gateway Architecture ## Overview The API Gateway handles all incoming HTTP requests, providing authentication, rate limiting, and request routing. Built on Kong with custom plugins for CloudFlow-specific functionality. ## Design Principles - **Zero Trust**: Every request is authenticated and authorized - **Defense in Depth**: Multiple layers of security validation ## Components ### Kong Gateway Primary ingress controller. Handles TLS termination, request routing, and plugin execution. **Technology:** Kong 3.4 on Kubernetes **Scaling:** Horizontal with HPA ### Auth Plugin Validates JWT tokens, API keys, and OAuth2 flows. Token validation latency under 5ms. **Technology:** Custom Lua plugin **Scaling:** Stateless ### Rate Limiter Enforces per-user and per-IP rate limits. Default limit is 100 requests per minute. **Technology:** Redis-backed sliding window **Scaling:** Shared Redis cluster ## Data Flow Client \u2192 TLS \u2192 Kong \u2192 Auth \u2192 Rate Limit \u2192 Backend Services ## Performance Characteristics - **Latency P50:** 12ms - **Latency P99:** 45ms - **Throughput:** 50000 requests/second",
              "score": null
            },
            {
              "doc_id": "arch_monitoring_stack",
              "chunk_id": "arch_monitoring_stack_fix_0",
              "content": "# Monitoring Stack Architecture ## Overview The Monitoring Stack provides observability into system health, performance, and errors. Includes metrics, logs, traces, and alerting. ## Design Principles - **Observability**: Every component emits metrics and traces - **Actionable Alerts**: Alerts include runbook links ## Components ### Metrics Collects time-series metrics. Retention is 15 days local, 1 year in object storage. **Technology:** Prometheus with Thanos **Scaling:** Federated with long-term storage ### Logging Centralized log aggregation. Supports structured JSON logs with label indexing. **Technology:** Loki with Grafana **Scaling:** Distributed with sharding ### Tracing Distributed request tracing. Trace retention is 7 days. **Technology:** Jaeger with OpenTelemetry **Scaling:** Sampling at 10% ### Alerting Routes alerts to on-call engineers. Supports escalation policies and silencing. **Technology:** Alertmanager with PagerDuty **Scaling:** HA pair ## Data Flow Application \u2192 Collector \u2192 Storage \u2192 Query \u2192 Dashboard/Alert ## Performance Characteristics - **Latency P50:** N/A - **Latency P99:** N/A - **Throughput:** 1M metrics/second, 100GB logs/day",
              "score": null
            }
          ],
          "latency_ms": 3.265670999098802,
          "query_id": "compare_025",
          "dimension": "original",
          "query": "What authentication options were evaluated?"
        },
        {
          "key_facts": [
            "OAuth 2.0",
            "Session-based",
            "API keys",
            "stateless validation"
          ],
          "found_facts": [
            "OAuth 2.0",
            "Session-based",
            "API keys",
            "stateless validation"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "adr_003"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "adr_003",
              "chunk_id": "adr_003_fix_0",
              "content": "# ADR-003: Implement OAuth 2.0 with JWT for Authentication **Status:** Accepted **Date:** 2024-05-08 ## Context We need a secure authentication system that supports API keys, user sessions, and third-party integrations. ## Options Considered ### Option 1: OAuth 2.0 with JWT Industry standard token-based auth. **Pros:** - Industry standard - Stateless validation - Supports refresh tokens - SSO compatible **Cons:** - Token size overhead - Cannot revoke individual tokens - Complexity ### Option 2: Session-based auth Traditional server-side sessions. **Pros:** - Simple implementation - Easy revocation - Smaller payloads **Cons:** - Requires session store - Not suitable for APIs - Scaling challenges ### Option 3: API keys only Simple key-based authentication. **Pros:** - Very simple - Low overhead - Easy to implement **Cons:** - No user context - Hard to rotate - No SSO support ## Decision We will implement OAuth 2.0 with JWT tokens. Access tokens expire after 1 hour, refresh tokens after 7 days. ## Consequences - Implement token rotation on refresh - Support API keys for machine-to-machine auth - Add MFA for admin accounts",
              "score": null
            },
            {
              "doc_id": "arch_authentication_service",
              "chunk_id": "arch_authentication_service_fix_0",
              "content": "# Authentication Service Architecture ## Overview The Authentication Service handles user identity, session management, and token issuance. Supports OAuth 2.0, SAML, and API key authentication. ## Design Principles - **Zero Knowledge**: Passwords never stored in plaintext - **Token Rotation**: Refresh tokens rotate on each use ## Components ### Identity Provider Manages user accounts and credentials. Password hashing uses bcrypt with cost factor 12. **Technology:** Custom Go service **Scaling:** Horizontal stateless ### Token Service Issues and validates access tokens. Token lifetime is 1 hour with 7-day refresh tokens. **Technology:** JWT with RS256 **Scaling:** Horizontal stateless ### SSO Gateway Integrates with external identity providers. Supports Google, Microsoft, Okta, and custom SAML. **Technology:** Dex OIDC proxy **Scaling:** Active-passive ## Data Flow Credentials \u2192 Validation \u2192 Token Issue \u2192 Token Refresh \u2192 Logout ## Performance Characteristics - **Latency P50:** 50ms - **Latency P99:** 200ms - **Throughput:** 5000 auth/second",
              "score": null
            },
            {
              "doc_id": "howto_configure_single_sign_on",
              "chunk_id": "howto_configure_single_sign_on_fix_0",
              "content": "# How to Configure Single Sign-On ## Prerequisites - Admin access - Enterprise plan - Identity provider access ## Steps ### Step 1: Choose provider CloudFlow supports SAML 2.0, OpenID Connect, and OAuth 2.0 providers. ### Step 2: Get metadata Download CloudFlow's SAML metadata for your IdP configuration. ```bash cloudflow sso metadata --format xml > cloudflow-metadata.xml ``` ### Step 3: Configure IdP Add CloudFlow as a service provider in your identity provider. > **Note:** ACS URL: https://auth.cloudflow.io/saml/callback ### Step 4: Upload IdP metadata Provide your IdP's metadata to CloudFlow. ```bash cloudflow sso configure --idp-metadata ./idp-metadata.xml ``` ### Step 5: Test login Verify SSO login works correctly. ```bash cloudflow sso test ``` ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            },
            {
              "doc_id": "arch_api_gateway",
              "chunk_id": "arch_api_gateway_fix_0",
              "content": "# API Gateway Architecture ## Overview The API Gateway handles all incoming HTTP requests, providing authentication, rate limiting, and request routing. Built on Kong with custom plugins for CloudFlow-specific functionality. ## Design Principles - **Zero Trust**: Every request is authenticated and authorized - **Defense in Depth**: Multiple layers of security validation ## Components ### Kong Gateway Primary ingress controller. Handles TLS termination, request routing, and plugin execution. **Technology:** Kong 3.4 on Kubernetes **Scaling:** Horizontal with HPA ### Auth Plugin Validates JWT tokens, API keys, and OAuth2 flows. Token validation latency under 5ms. **Technology:** Custom Lua plugin **Scaling:** Stateless ### Rate Limiter Enforces per-user and per-IP rate limits. Default limit is 100 requests per minute. **Technology:** Redis-backed sliding window **Scaling:** Shared Redis cluster ## Data Flow Client \u2192 TLS \u2192 Kong \u2192 Auth \u2192 Rate Limit \u2192 Backend Services ## Performance Characteristics - **Latency P50:** 12ms - **Latency P99:** 45ms - **Throughput:** 50000 requests/second",
              "score": null
            },
            {
              "doc_id": "arch_monitoring_stack",
              "chunk_id": "arch_monitoring_stack_fix_0",
              "content": "# Monitoring Stack Architecture ## Overview The Monitoring Stack provides observability into system health, performance, and errors. Includes metrics, logs, traces, and alerting. ## Design Principles - **Observability**: Every component emits metrics and traces - **Actionable Alerts**: Alerts include runbook links ## Components ### Metrics Collects time-series metrics. Retention is 15 days local, 1 year in object storage. **Technology:** Prometheus with Thanos **Scaling:** Federated with long-term storage ### Logging Centralized log aggregation. Supports structured JSON logs with label indexing. **Technology:** Loki with Grafana **Scaling:** Distributed with sharding ### Tracing Distributed request tracing. Trace retention is 7 days. **Technology:** Jaeger with OpenTelemetry **Scaling:** Sampling at 10% ### Alerting Routes alerts to on-call engineers. Supports escalation policies and silencing. **Technology:** Alertmanager with PagerDuty **Scaling:** HA pair ## Data Flow Application \u2192 Collector \u2192 Storage \u2192 Query \u2192 Dashboard/Alert ## Performance Characteristics - **Latency P50:** N/A - **Latency P99:** N/A - **Throughput:** 1M metrics/second, 100GB logs/day",
              "score": null
            }
          ],
          "latency_ms": 3.8698470016242936,
          "query_id": "compare_025",
          "dimension": "synonym",
          "query": "What auth approaches did we consider for the login system?"
        },
        {
          "key_facts": [
            "OAuth 2.0",
            "Session-based",
            "API keys",
            "stateless validation"
          ],
          "found_facts": [
            "OAuth 2.0",
            "Session-based",
            "API keys",
            "stateless validation"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "adr_003"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "api_audit",
              "chunk_id": "api_audit_fix_0",
              "content": "# Audit API Reference ## Overview The Audit API provides programmatic access to audit functionality. Base URL: `https://api.cloudflow.io/v1/audit` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /audit/logs Retrieve audit logs for compliance. **Parameters:** - `start_date` (date): Start date for log range - `end_date` (date): End date for log range - `actor` (string): Filter by user ID **Response:** - Success: `200` - Returns array of audit log entries - Error: `400` - Bad Request **Example:** ```json {\"logs\": [{\"timestamp\": \"2024-01-15T10:00:00Z\", \"actor\": \"usr_001\", \"action\": \"workflow.created\", \"resource\": \"wf_123\"}]} ```",
              "score": null
            },
            {
              "doc_id": "adr_003",
              "chunk_id": "adr_003_fix_0",
              "content": "# ADR-003: Implement OAuth 2.0 with JWT for Authentication **Status:** Accepted **Date:** 2024-05-08 ## Context We need a secure authentication system that supports API keys, user sessions, and third-party integrations. ## Options Considered ### Option 1: OAuth 2.0 with JWT Industry standard token-based auth. **Pros:** - Industry standard - Stateless validation - Supports refresh tokens - SSO compatible **Cons:** - Token size overhead - Cannot revoke individual tokens - Complexity ### Option 2: Session-based auth Traditional server-side sessions. **Pros:** - Simple implementation - Easy revocation - Smaller payloads **Cons:** - Requires session store - Not suitable for APIs - Scaling challenges ### Option 3: API keys only Simple key-based authentication. **Pros:** - Very simple - Low overhead - Easy to implement **Cons:** - No user context - Hard to rotate - No SSO support ## Decision We will implement OAuth 2.0 with JWT tokens. Access tokens expire after 1 hour, refresh tokens after 7 days. ## Consequences - Implement token rotation on refresh - Support API keys for machine-to-machine auth - Add MFA for admin accounts",
              "score": null
            },
            {
              "doc_id": "arch_authentication_service",
              "chunk_id": "arch_authentication_service_fix_0",
              "content": "# Authentication Service Architecture ## Overview The Authentication Service handles user identity, session management, and token issuance. Supports OAuth 2.0, SAML, and API key authentication. ## Design Principles - **Zero Knowledge**: Passwords never stored in plaintext - **Token Rotation**: Refresh tokens rotate on each use ## Components ### Identity Provider Manages user accounts and credentials. Password hashing uses bcrypt with cost factor 12. **Technology:** Custom Go service **Scaling:** Horizontal stateless ### Token Service Issues and validates access tokens. Token lifetime is 1 hour with 7-day refresh tokens. **Technology:** JWT with RS256 **Scaling:** Horizontal stateless ### SSO Gateway Integrates with external identity providers. Supports Google, Microsoft, Okta, and custom SAML. **Technology:** Dex OIDC proxy **Scaling:** Active-passive ## Data Flow Credentials \u2192 Validation \u2192 Token Issue \u2192 Token Refresh \u2192 Logout ## Performance Characteristics - **Latency P50:** 50ms - **Latency P99:** 200ms - **Throughput:** 5000 auth/second",
              "score": null
            },
            {
              "doc_id": "howto_configure_single_sign_on",
              "chunk_id": "howto_configure_single_sign_on_fix_0",
              "content": "# How to Configure Single Sign-On ## Prerequisites - Admin access - Enterprise plan - Identity provider access ## Steps ### Step 1: Choose provider CloudFlow supports SAML 2.0, OpenID Connect, and OAuth 2.0 providers. ### Step 2: Get metadata Download CloudFlow's SAML metadata for your IdP configuration. ```bash cloudflow sso metadata --format xml > cloudflow-metadata.xml ``` ### Step 3: Configure IdP Add CloudFlow as a service provider in your identity provider. > **Note:** ACS URL: https://auth.cloudflow.io/saml/callback ### Step 4: Upload IdP metadata Provide your IdP's metadata to CloudFlow. ```bash cloudflow sso configure --idp-metadata ./idp-metadata.xml ``` ### Step 5: Test login Verify SSO login works correctly. ```bash cloudflow sso test ``` ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            },
            {
              "doc_id": "arch_monitoring_stack",
              "chunk_id": "arch_monitoring_stack_fix_0",
              "content": "# Monitoring Stack Architecture ## Overview The Monitoring Stack provides observability into system health, performance, and errors. Includes metrics, logs, traces, and alerting. ## Design Principles - **Observability**: Every component emits metrics and traces - **Actionable Alerts**: Alerts include runbook links ## Components ### Metrics Collects time-series metrics. Retention is 15 days local, 1 year in object storage. **Technology:** Prometheus with Thanos **Scaling:** Federated with long-term storage ### Logging Centralized log aggregation. Supports structured JSON logs with label indexing. **Technology:** Loki with Grafana **Scaling:** Distributed with sharding ### Tracing Distributed request tracing. Trace retention is 7 days. **Technology:** Jaeger with OpenTelemetry **Scaling:** Sampling at 10% ### Alerting Routes alerts to on-call engineers. Supports escalation policies and silencing. **Technology:** Alertmanager with PagerDuty **Scaling:** HA pair ## Data Flow Application \u2192 Collector \u2192 Storage \u2192 Query \u2192 Dashboard/Alert ## Performance Characteristics - **Latency P50:** N/A - **Latency P99:** N/A - **Throughput:** 1M metrics/second, 100GB logs/day",
              "score": null
            }
          ],
          "latency_ms": 5.199002000153996,
          "query_id": "compare_025",
          "dimension": "problem",
          "query": "Security audit wants to know what alternatives we rejected for our auth implementation"
        },
        {
          "key_facts": [
            "OAuth 2.0",
            "Session-based",
            "API keys",
            "stateless validation"
          ],
          "found_facts": [
            "OAuth 2.0",
            "Session-based",
            "API keys",
            "stateless validation"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "adr_003"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "adr_003",
              "chunk_id": "adr_003_fix_0",
              "content": "# ADR-003: Implement OAuth 2.0 with JWT for Authentication **Status:** Accepted **Date:** 2024-05-08 ## Context We need a secure authentication system that supports API keys, user sessions, and third-party integrations. ## Options Considered ### Option 1: OAuth 2.0 with JWT Industry standard token-based auth. **Pros:** - Industry standard - Stateless validation - Supports refresh tokens - SSO compatible **Cons:** - Token size overhead - Cannot revoke individual tokens - Complexity ### Option 2: Session-based auth Traditional server-side sessions. **Pros:** - Simple implementation - Easy revocation - Smaller payloads **Cons:** - Requires session store - Not suitable for APIs - Scaling challenges ### Option 3: API keys only Simple key-based authentication. **Pros:** - Very simple - Low overhead - Easy to implement **Cons:** - No user context - Hard to rotate - No SSO support ## Decision We will implement OAuth 2.0 with JWT tokens. Access tokens expire after 1 hour, refresh tokens after 7 days. ## Consequences - Implement token rotation on refresh - Support API keys for machine-to-machine auth - Add MFA for admin accounts",
              "score": null
            },
            {
              "doc_id": "arch_authentication_service",
              "chunk_id": "arch_authentication_service_fix_0",
              "content": "# Authentication Service Architecture ## Overview The Authentication Service handles user identity, session management, and token issuance. Supports OAuth 2.0, SAML, and API key authentication. ## Design Principles - **Zero Knowledge**: Passwords never stored in plaintext - **Token Rotation**: Refresh tokens rotate on each use ## Components ### Identity Provider Manages user accounts and credentials. Password hashing uses bcrypt with cost factor 12. **Technology:** Custom Go service **Scaling:** Horizontal stateless ### Token Service Issues and validates access tokens. Token lifetime is 1 hour with 7-day refresh tokens. **Technology:** JWT with RS256 **Scaling:** Horizontal stateless ### SSO Gateway Integrates with external identity providers. Supports Google, Microsoft, Okta, and custom SAML. **Technology:** Dex OIDC proxy **Scaling:** Active-passive ## Data Flow Credentials \u2192 Validation \u2192 Token Issue \u2192 Token Refresh \u2192 Logout ## Performance Characteristics - **Latency P50:** 50ms - **Latency P99:** 200ms - **Throughput:** 5000 auth/second",
              "score": null
            },
            {
              "doc_id": "howto_migrate_from_v1_to_v2_api",
              "chunk_id": "howto_migrate_from_v1_to_v2_api_fix_0",
              "content": "# How to Migrate from v1 to v2 API ## Prerequisites - Existing v1 integration - v2 API access enabled ## Steps ### Step 1: Review breaking changes The v2 API changes authentication from API keys to OAuth 2.0. Review the migration guide. ### Step 2: Update client library Upgrade to the latest SDK version. ```bash npm install @cloudflow/sdk@latest ``` ### Step 3: Update authentication Replace API key authentication with OAuth 2.0 client credentials flow. > **Note:** Generate new credentials in the dashboard ### Step 4: Test in staging Run your integration tests against the v2 staging endpoint. ```bash CLOUDFLOW_API_URL=https://api-v2-staging.cloudflow.io npm test ``` ### Step 5: Switch production Update production configuration to use v2 API. ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            },
            {
              "doc_id": "howto_create_api_integration",
              "chunk_id": "howto_create_api_integration_fix_0",
              "content": "# How to Create API Integration ## Prerequisites - CloudFlow account ## Steps ### Step 1: Generate API key Create an API key for programmatic access. ```bash cloudflow apikeys create --name 'CI Integration' --scopes workflows:read,workflows:execute ``` ### Step 2: Store key securely Never commit API keys to source control. Use environment variables or secret management. > **Note:** Rotate keys every 90 days ### Step 3: Test authentication Verify the API key works correctly. ```bash curl -H 'Authorization: Bearer cf_...' https://api.cloudflow.io/v1/me ``` ### Step 4: Implement rate limiting Handle rate limit responses (HTTP 429) with exponential backoff. > **Note:** Default limit is 100 requests per minute ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            },
            {
              "doc_id": "arch_api_gateway",
              "chunk_id": "arch_api_gateway_fix_0",
              "content": "# API Gateway Architecture ## Overview The API Gateway handles all incoming HTTP requests, providing authentication, rate limiting, and request routing. Built on Kong with custom plugins for CloudFlow-specific functionality. ## Design Principles - **Zero Trust**: Every request is authenticated and authorized - **Defense in Depth**: Multiple layers of security validation ## Components ### Kong Gateway Primary ingress controller. Handles TLS termination, request routing, and plugin execution. **Technology:** Kong 3.4 on Kubernetes **Scaling:** Horizontal with HPA ### Auth Plugin Validates JWT tokens, API keys, and OAuth2 flows. Token validation latency under 5ms. **Technology:** Custom Lua plugin **Scaling:** Stateless ### Rate Limiter Enforces per-user and per-IP rate limits. Default limit is 100 requests per minute. **Technology:** Redis-backed sliding window **Scaling:** Shared Redis cluster ## Data Flow Client \u2192 TLS \u2192 Kong \u2192 Auth \u2192 Rate Limit \u2192 Backend Services ## Performance Characteristics - **Latency P50:** 12ms - **Latency P99:** 45ms - **Throughput:** 50000 requests/second",
              "score": null
            }
          ],
          "latency_ms": 3.7769229984405683,
          "query_id": "compare_025",
          "dimension": "casual",
          "query": "oauth vs session vs api key auth comparison"
        },
        {
          "key_facts": [
            "OAuth 2.0",
            "Session-based",
            "API keys",
            "stateless validation"
          ],
          "found_facts": [
            "OAuth 2.0",
            "API keys"
          ],
          "missed_facts": [
            "Session-based",
            "stateless validation"
          ],
          "coverage": 0.5,
          "expected_docs": [
            "adr_003"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "howto_configure_single_sign_on",
              "chunk_id": "howto_configure_single_sign_on_fix_0",
              "content": "# How to Configure Single Sign-On ## Prerequisites - Admin access - Enterprise plan - Identity provider access ## Steps ### Step 1: Choose provider CloudFlow supports SAML 2.0, OpenID Connect, and OAuth 2.0 providers. ### Step 2: Get metadata Download CloudFlow's SAML metadata for your IdP configuration. ```bash cloudflow sso metadata --format xml > cloudflow-metadata.xml ``` ### Step 3: Configure IdP Add CloudFlow as a service provider in your identity provider. > **Note:** ACS URL: https://auth.cloudflow.io/saml/callback ### Step 4: Upload IdP metadata Provide your IdP's metadata to CloudFlow. ```bash cloudflow sso configure --idp-metadata ./idp-metadata.xml ``` ### Step 5: Test login Verify SSO login works correctly. ```bash cloudflow sso test ``` ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            },
            {
              "doc_id": "arch_api_gateway",
              "chunk_id": "arch_api_gateway_fix_0",
              "content": "# API Gateway Architecture ## Overview The API Gateway handles all incoming HTTP requests, providing authentication, rate limiting, and request routing. Built on Kong with custom plugins for CloudFlow-specific functionality. ## Design Principles - **Zero Trust**: Every request is authenticated and authorized - **Defense in Depth**: Multiple layers of security validation ## Components ### Kong Gateway Primary ingress controller. Handles TLS termination, request routing, and plugin execution. **Technology:** Kong 3.4 on Kubernetes **Scaling:** Horizontal with HPA ### Auth Plugin Validates JWT tokens, API keys, and OAuth2 flows. Token validation latency under 5ms. **Technology:** Custom Lua plugin **Scaling:** Stateless ### Rate Limiter Enforces per-user and per-IP rate limits. Default limit is 100 requests per minute. **Technology:** Redis-backed sliding window **Scaling:** Shared Redis cluster ## Data Flow Client \u2192 TLS \u2192 Kong \u2192 Auth \u2192 Rate Limit \u2192 Backend Services ## Performance Characteristics - **Latency P50:** 12ms - **Latency P99:** 45ms - **Throughput:** 50000 requests/second",
              "score": null
            },
            {
              "doc_id": "howto_create_api_integration",
              "chunk_id": "howto_create_api_integration_fix_0",
              "content": "# How to Create API Integration ## Prerequisites - CloudFlow account ## Steps ### Step 1: Generate API key Create an API key for programmatic access. ```bash cloudflow apikeys create --name 'CI Integration' --scopes workflows:read,workflows:execute ``` ### Step 2: Store key securely Never commit API keys to source control. Use environment variables or secret management. > **Note:** Rotate keys every 90 days ### Step 3: Test authentication Verify the API key works correctly. ```bash curl -H 'Authorization: Bearer cf_...' https://api.cloudflow.io/v1/me ``` ### Step 4: Implement rate limiting Handle rate limit responses (HTTP 429) with exponential backoff. > **Note:** Default limit is 100 requests per minute ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            },
            {
              "doc_id": "howto_configure_custom_domain",
              "chunk_id": "howto_configure_custom_domain_fix_0",
              "content": "# How to Configure Custom Domain ## Prerequisites - Domain with DNS access - CloudFlow Pro plan or higher ## Steps ### Step 1: Verify domain ownership Add a TXT record to verify domain ownership. ```bash cloudflow domains verify --domain app.example.com ``` ### Step 2: Configure DNS Add a CNAME record pointing to your CloudFlow endpoint. > **Note:** CNAME should point to ingress.cloudflow.io ### Step 3: Enable SSL CloudFlow automatically provisions SSL certificates via Let's Encrypt. ```bash cloudflow domains ssl --domain app.example.com ``` ### Step 4: Set as primary Make this the primary domain for your application. ```bash cloudflow domains set-primary --domain app.example.com ``` ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            },
            {
              "doc_id": "howto_set_up_data_encryption",
              "chunk_id": "howto_set_up_data_encryption_fix_0",
              "content": "# How to Set Up Data Encryption ## Prerequisites - CloudFlow Enterprise plan ## Steps ### Step 1: Enable encryption at rest All data is encrypted at rest using AES-256 by default. > **Note:** Encryption is automatic for all plans ### Step 2: Configure KMS Bring your own encryption keys using AWS KMS or GCP KMS. ```bash cloudflow encryption configure --kms-key arn:aws:kms:... ``` ### Step 3: Enable field-level encryption Encrypt sensitive fields within your workflow data. ```bash cloudflow encryption add-field --path $.user.ssn --key-alias sensitive-data ``` ### Step 4: Verify encryption Confirm encryption is active for your workspace. ```bash cloudflow encryption status ``` ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            }
          ],
          "latency_ms": 3.530555000907043,
          "query_id": "compare_025",
          "dimension": "contextual",
          "query": "Building a new service that needs auth - what authentication strategy does CloudFlow use and why?"
        },
        {
          "key_facts": [
            "OAuth 2.0",
            "Session-based",
            "API keys",
            "stateless validation"
          ],
          "found_facts": [
            "OAuth 2.0",
            "Session-based",
            "API keys",
            "stateless validation"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "adr_003"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "adr_003",
              "chunk_id": "adr_003_fix_0",
              "content": "# ADR-003: Implement OAuth 2.0 with JWT for Authentication **Status:** Accepted **Date:** 2024-05-08 ## Context We need a secure authentication system that supports API keys, user sessions, and third-party integrations. ## Options Considered ### Option 1: OAuth 2.0 with JWT Industry standard token-based auth. **Pros:** - Industry standard - Stateless validation - Supports refresh tokens - SSO compatible **Cons:** - Token size overhead - Cannot revoke individual tokens - Complexity ### Option 2: Session-based auth Traditional server-side sessions. **Pros:** - Simple implementation - Easy revocation - Smaller payloads **Cons:** - Requires session store - Not suitable for APIs - Scaling challenges ### Option 3: API keys only Simple key-based authentication. **Pros:** - Very simple - Low overhead - Easy to implement **Cons:** - No user context - Hard to rotate - No SSO support ## Decision We will implement OAuth 2.0 with JWT tokens. Access tokens expire after 1 hour, refresh tokens after 7 days. ## Consequences - Implement token rotation on refresh - Support API keys for machine-to-machine auth - Add MFA for admin accounts",
              "score": null
            },
            {
              "doc_id": "arch_authentication_service",
              "chunk_id": "arch_authentication_service_fix_0",
              "content": "# Authentication Service Architecture ## Overview The Authentication Service handles user identity, session management, and token issuance. Supports OAuth 2.0, SAML, and API key authentication. ## Design Principles - **Zero Knowledge**: Passwords never stored in plaintext - **Token Rotation**: Refresh tokens rotate on each use ## Components ### Identity Provider Manages user accounts and credentials. Password hashing uses bcrypt with cost factor 12. **Technology:** Custom Go service **Scaling:** Horizontal stateless ### Token Service Issues and validates access tokens. Token lifetime is 1 hour with 7-day refresh tokens. **Technology:** JWT with RS256 **Scaling:** Horizontal stateless ### SSO Gateway Integrates with external identity providers. Supports Google, Microsoft, Okta, and custom SAML. **Technology:** Dex OIDC proxy **Scaling:** Active-passive ## Data Flow Credentials \u2192 Validation \u2192 Token Issue \u2192 Token Refresh \u2192 Logout ## Performance Characteristics - **Latency P50:** 50ms - **Latency P99:** 200ms - **Throughput:** 5000 auth/second",
              "score": null
            },
            {
              "doc_id": "arch_api_gateway",
              "chunk_id": "arch_api_gateway_fix_0",
              "content": "# API Gateway Architecture ## Overview The API Gateway handles all incoming HTTP requests, providing authentication, rate limiting, and request routing. Built on Kong with custom plugins for CloudFlow-specific functionality. ## Design Principles - **Zero Trust**: Every request is authenticated and authorized - **Defense in Depth**: Multiple layers of security validation ## Components ### Kong Gateway Primary ingress controller. Handles TLS termination, request routing, and plugin execution. **Technology:** Kong 3.4 on Kubernetes **Scaling:** Horizontal with HPA ### Auth Plugin Validates JWT tokens, API keys, and OAuth2 flows. Token validation latency under 5ms. **Technology:** Custom Lua plugin **Scaling:** Stateless ### Rate Limiter Enforces per-user and per-IP rate limits. Default limit is 100 requests per minute. **Technology:** Redis-backed sliding window **Scaling:** Shared Redis cluster ## Data Flow Client \u2192 TLS \u2192 Kong \u2192 Auth \u2192 Rate Limit \u2192 Backend Services ## Performance Characteristics - **Latency P50:** 12ms - **Latency P99:** 45ms - **Throughput:** 50000 requests/second",
              "score": null
            },
            {
              "doc_id": "howto_configure_single_sign_on",
              "chunk_id": "howto_configure_single_sign_on_fix_0",
              "content": "# How to Configure Single Sign-On ## Prerequisites - Admin access - Enterprise plan - Identity provider access ## Steps ### Step 1: Choose provider CloudFlow supports SAML 2.0, OpenID Connect, and OAuth 2.0 providers. ### Step 2: Get metadata Download CloudFlow's SAML metadata for your IdP configuration. ```bash cloudflow sso metadata --format xml > cloudflow-metadata.xml ``` ### Step 3: Configure IdP Add CloudFlow as a service provider in your identity provider. > **Note:** ACS URL: https://auth.cloudflow.io/saml/callback ### Step 4: Upload IdP metadata Provide your IdP's metadata to CloudFlow. ```bash cloudflow sso configure --idp-metadata ./idp-metadata.xml ``` ### Step 5: Test login Verify SSO login works correctly. ```bash cloudflow sso test ``` ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            },
            {
              "doc_id": "arch_monitoring_stack",
              "chunk_id": "arch_monitoring_stack_fix_0",
              "content": "# Monitoring Stack Architecture ## Overview The Monitoring Stack provides observability into system health, performance, and errors. Includes metrics, logs, traces, and alerting. ## Design Principles - **Observability**: Every component emits metrics and traces - **Actionable Alerts**: Alerts include runbook links ## Components ### Metrics Collects time-series metrics. Retention is 15 days local, 1 year in object storage. **Technology:** Prometheus with Thanos **Scaling:** Federated with long-term storage ### Logging Centralized log aggregation. Supports structured JSON logs with label indexing. **Technology:** Loki with Grafana **Scaling:** Distributed with sharding ### Tracing Distributed request tracing. Trace retention is 7 days. **Technology:** Jaeger with OpenTelemetry **Scaling:** Sampling at 10% ### Alerting Routes alerts to on-call engineers. Supports escalation policies and silencing. **Technology:** Alertmanager with PagerDuty **Scaling:** HA pair ## Data Flow Application \u2192 Collector \u2192 Storage \u2192 Query \u2192 Dashboard/Alert ## Performance Characteristics - **Latency P50:** N/A - **Latency P99:** N/A - **Throughput:** 1M metrics/second, 100GB logs/day",
              "score": null
            }
          ],
          "latency_ms": 3.442450999500579,
          "query_id": "compare_025",
          "dimension": "negation",
          "query": "Why didn't we just use simple session-based authentication?"
        },
        {
          "key_facts": [
            "Redis Cluster",
            "Memcached",
            "data structures",
            "persistence"
          ],
          "found_facts": [
            "Redis Cluster",
            "Memcached",
            "data structures",
            "persistence"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "adr_004"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "adr_004",
              "chunk_id": "adr_004_fix_0",
              "content": "# ADR-004: Use Redis for Caching and Session Storage **Status:** Accepted **Date:** 2024-04-05 ## Context We need a fast data store for caching API responses, session data, and rate limiting counters. ## Options Considered ### Option 1: Redis Cluster In-memory data structure store. **Pros:** - Sub-millisecond latency - Rich data structures - Pub/sub support - Clustering **Cons:** - Memory cost - Persistence complexity - Cluster management ### Option 2: Memcached Simple key-value cache. **Pros:** - Simple operation - Multi-threaded - Lower memory overhead **Cons:** - No persistence - Limited data types - No clustering ### Option 3: Application-level cache In-process caching only. **Pros:** - No external dependency - Fastest possible - Simple **Cons:** - Not shared across pods - Memory pressure - Cold starts ## Decision We will use Redis Cluster for caching with a 6-node cluster for high availability. ## Consequences - Implement cache-aside pattern - Set appropriate TTLs - Monitor memory usage - Plan for cache stampede prevention",
              "score": null
            },
            {
              "doc_id": "arch_caching_layer",
              "chunk_id": "arch_caching_layer_fix_0",
              "content": "# Caching Layer Architecture ## Overview The Caching Layer reduces latency and database load by caching frequently accessed data. Uses a multi-tier strategy with local and distributed caches. ## Design Principles - **Cache Invalidation**: Explicit invalidation on writes - **Graceful Degradation**: System works without cache ## Components ### Local Cache First-level cache with 100ms TTL. Reduces Redis round-trips for hot data. **Technology:** Ristretto in-memory cache **Scaling:** Per-pod memory allocation ### Distributed Cache Second-level cache with configurable TTL. Maximum key size 512MB. **Technology:** Redis Cluster 7.x **Scaling:** 6-node cluster with sharding ### CDN Caches static assets and API responses. Cache hit ratio target 95%. **Technology:** CloudFront with Lambda@Edge **Scaling:** Global edge locations ## Data Flow Request \u2192 Local Cache \u2192 Redis \u2192 Database \u2192 Cache Population ## Performance Characteristics - **Latency P50:** 1ms (hit), 50ms (miss) - **Latency P99:** 5ms (hit), 200ms (miss) - **Throughput:** 100000 reads/second",
              "score": null
            },
            {
              "doc_id": "config_cache",
              "chunk_id": "config_cache_fix_0",
              "content": "# Cache Configuration Reference ## Overview This document describes all configuration options for Cache. ## Environment Variables | Variable | Type | Default | Description | |----------|------|---------|-------------| | `REDIS_URL` | string | required | Redis connection string | | `REDIS_POOL_SIZE` | integer | 10 | Connection pool size | | `CACHE_TTL_SECONDS` | integer | 300 | Default cache TTL | | `CACHE_PREFIX` | string | cf: | Key prefix for namespacing | | `CACHE_COMPRESSION` | boolean | true | Enable value compression | | `CACHE_MAX_SIZE_MB` | integer | 100 | Maximum cache size per pod | ## Configuration File Configuration can also be provided via `cache.yaml`: ```yaml redis_url: required redis_pool_size: 10 cache_ttl_seconds: 300 cache_prefix: cf: cache_compression: true ``` ## Validation Configuration is validated at startup. Invalid configuration will prevent the service from starting.",
              "score": null
            },
            {
              "doc_id": "adr_007",
              "chunk_id": "adr_007_fix_0",
              "content": "# ADR-007: Implement Rate Limiting at API Gateway **Status:** Accepted **Date:** 2024-09-03 ## Context We need to protect our services from abuse and ensure fair resource usage across customers. ## Options Considered ### Option 1: Sliding window at gateway Rate limit at API gateway level. **Pros:** - Centralized enforcement - Low latency - Easy configuration **Cons:** - Gateway becomes bottleneck - Coarse granularity - Redis dependency ### Option 2: Per-service limiting Each service implements its own limits. **Pros:** - Fine-grained control - Service autonomy - No single point of failure **Cons:** - Inconsistent implementation - Higher complexity - Distributed state ### Option 3: Token bucket Token-based rate limiting. **Pros:** - Allows bursts - Smooth rate limiting - Well understood **Cons:** - More complex - Harder to explain to users - State management ## Decision We will implement sliding window rate limiting at the Kong API Gateway using Redis. ## Consequences - Default limit: 100 requests per minute - Implement per-plan limits - Return rate limit headers - Add retry-after on 429",
              "score": null
            },
            {
              "doc_id": "adr_008",
              "chunk_id": "adr_008_fix_0",
              "content": "# ADR-008: Use S3 for Object Storage **Status:** Accepted **Date:** 2024-10-14 ## Context We need storage for workflow artifacts, execution outputs, and user uploads. ## Options Considered ### Option 1: Amazon S3 Object storage service. **Pros:** - 99.999999999% durability - Lifecycle policies - Event notifications - CDN integration **Cons:** - AWS lock-in - Eventual consistency for overwrites - Cost at scale ### Option 2: Self-hosted MinIO S3-compatible object storage. **Pros:** - No vendor lock-in - S3 compatible - On-premises option **Cons:** - Operational burden - Lower durability guarantees - Scaling complexity ### Option 3: Google Cloud Storage GCP object storage. **Pros:** - Strong consistency - Good performance - Unified billing **Cons:** - GCP lock-in - Fewer features than S3 - Smaller ecosystem ## Decision We will use Amazon S3 for object storage with intelligent tiering for cost optimization. ## Consequences - Enable versioning for critical buckets - Implement lifecycle policies - Use pre-signed URLs for uploads - Enable server-side encryption",
              "score": null
            }
          ],
          "latency_ms": 5.294851000144263,
          "query_id": "compare_026",
          "dimension": "original",
          "query": "Compare caching options: Redis vs Memcached"
        },
        {
          "key_facts": [
            "Redis Cluster",
            "Memcached",
            "data structures",
            "persistence"
          ],
          "found_facts": [
            "Redis Cluster",
            "Memcached",
            "data structures",
            "persistence"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "adr_004"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "arch_caching_layer",
              "chunk_id": "arch_caching_layer_fix_0",
              "content": "# Caching Layer Architecture ## Overview The Caching Layer reduces latency and database load by caching frequently accessed data. Uses a multi-tier strategy with local and distributed caches. ## Design Principles - **Cache Invalidation**: Explicit invalidation on writes - **Graceful Degradation**: System works without cache ## Components ### Local Cache First-level cache with 100ms TTL. Reduces Redis round-trips for hot data. **Technology:** Ristretto in-memory cache **Scaling:** Per-pod memory allocation ### Distributed Cache Second-level cache with configurable TTL. Maximum key size 512MB. **Technology:** Redis Cluster 7.x **Scaling:** 6-node cluster with sharding ### CDN Caches static assets and API responses. Cache hit ratio target 95%. **Technology:** CloudFront with Lambda@Edge **Scaling:** Global edge locations ## Data Flow Request \u2192 Local Cache \u2192 Redis \u2192 Database \u2192 Cache Population ## Performance Characteristics - **Latency P50:** 1ms (hit), 50ms (miss) - **Latency P99:** 5ms (hit), 200ms (miss) - **Throughput:** 100000 reads/second",
              "score": null
            },
            {
              "doc_id": "adr_004",
              "chunk_id": "adr_004_fix_0",
              "content": "# ADR-004: Use Redis for Caching and Session Storage **Status:** Accepted **Date:** 2024-04-05 ## Context We need a fast data store for caching API responses, session data, and rate limiting counters. ## Options Considered ### Option 1: Redis Cluster In-memory data structure store. **Pros:** - Sub-millisecond latency - Rich data structures - Pub/sub support - Clustering **Cons:** - Memory cost - Persistence complexity - Cluster management ### Option 2: Memcached Simple key-value cache. **Pros:** - Simple operation - Multi-threaded - Lower memory overhead **Cons:** - No persistence - Limited data types - No clustering ### Option 3: Application-level cache In-process caching only. **Pros:** - No external dependency - Fastest possible - Simple **Cons:** - Not shared across pods - Memory pressure - Cold starts ## Decision We will use Redis Cluster for caching with a 6-node cluster for high availability. ## Consequences - Implement cache-aside pattern - Set appropriate TTLs - Monitor memory usage - Plan for cache stampede prevention",
              "score": null
            },
            {
              "doc_id": "config_cache",
              "chunk_id": "config_cache_fix_0",
              "content": "# Cache Configuration Reference ## Overview This document describes all configuration options for Cache. ## Environment Variables | Variable | Type | Default | Description | |----------|------|---------|-------------| | `REDIS_URL` | string | required | Redis connection string | | `REDIS_POOL_SIZE` | integer | 10 | Connection pool size | | `CACHE_TTL_SECONDS` | integer | 300 | Default cache TTL | | `CACHE_PREFIX` | string | cf: | Key prefix for namespacing | | `CACHE_COMPRESSION` | boolean | true | Enable value compression | | `CACHE_MAX_SIZE_MB` | integer | 100 | Maximum cache size per pod | ## Configuration File Configuration can also be provided via `cache.yaml`: ```yaml redis_url: required redis_pool_size: 10 cache_ttl_seconds: 300 cache_prefix: cf: cache_compression: true ``` ## Validation Configuration is validated at startup. Invalid configuration will prevent the service from starting.",
              "score": null
            },
            {
              "doc_id": "adr_007",
              "chunk_id": "adr_007_fix_0",
              "content": "# ADR-007: Implement Rate Limiting at API Gateway **Status:** Accepted **Date:** 2024-09-03 ## Context We need to protect our services from abuse and ensure fair resource usage across customers. ## Options Considered ### Option 1: Sliding window at gateway Rate limit at API gateway level. **Pros:** - Centralized enforcement - Low latency - Easy configuration **Cons:** - Gateway becomes bottleneck - Coarse granularity - Redis dependency ### Option 2: Per-service limiting Each service implements its own limits. **Pros:** - Fine-grained control - Service autonomy - No single point of failure **Cons:** - Inconsistent implementation - Higher complexity - Distributed state ### Option 3: Token bucket Token-based rate limiting. **Pros:** - Allows bursts - Smooth rate limiting - Well understood **Cons:** - More complex - Harder to explain to users - State management ## Decision We will implement sliding window rate limiting at the Kong API Gateway using Redis. ## Consequences - Default limit: 100 requests per minute - Implement per-plan limits - Return rate limit headers - Add retry-after on 429",
              "score": null
            },
            {
              "doc_id": "arch_data_pipeline",
              "chunk_id": "arch_data_pipeline_fix_0",
              "content": "# Data Pipeline Architecture ## Overview The Data Pipeline processes and transforms data between workflow steps. It supports batch processing up to 10GB and streaming for real-time use cases. ## Design Principles - **Exactly-Once Processing**: Each record is processed exactly once - **Backpressure Handling**: System gracefully handles varying load ## Components ### Stream Processor Handles real-time data streams. Maximum throughput of 100000 messages per second. **Technology:** Apache Kafka with Flink **Scaling:** Partition-based scaling ### Batch Processor Processes large batch jobs. Supports data formats: JSON, CSV, Parquet, Avro. **Technology:** Apache Spark on Kubernetes **Scaling:** Dynamic resource allocation ### Data Lake Long-term data storage with time-travel queries. Retention configurable up to 7 years. **Technology:** S3 with Iceberg tables **Scaling:** Unlimited with tiered storage ## Data Flow Input \u2192 Validation \u2192 Transform \u2192 Enrich \u2192 Output ## Performance Characteristics - **Latency P50:** 100ms (streaming), 5min (batch) - **Latency P99:** 500ms (streaming), 30min (batch) - **Throughput:** 100000 messages/second (streaming)",
              "score": null
            }
          ],
          "latency_ms": 3.762587000892381,
          "query_id": "compare_026",
          "dimension": "synonym",
          "query": "What's the difference between Redis and Memcached for our cache layer?"
        },
        {
          "key_facts": [
            "Redis Cluster",
            "Memcached",
            "data structures",
            "persistence"
          ],
          "found_facts": [
            "Redis Cluster",
            "Memcached",
            "data structures",
            "persistence"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "adr_004"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "adr_004",
              "chunk_id": "adr_004_fix_0",
              "content": "# ADR-004: Use Redis for Caching and Session Storage **Status:** Accepted **Date:** 2024-04-05 ## Context We need a fast data store for caching API responses, session data, and rate limiting counters. ## Options Considered ### Option 1: Redis Cluster In-memory data structure store. **Pros:** - Sub-millisecond latency - Rich data structures - Pub/sub support - Clustering **Cons:** - Memory cost - Persistence complexity - Cluster management ### Option 2: Memcached Simple key-value cache. **Pros:** - Simple operation - Multi-threaded - Lower memory overhead **Cons:** - No persistence - Limited data types - No clustering ### Option 3: Application-level cache In-process caching only. **Pros:** - No external dependency - Fastest possible - Simple **Cons:** - Not shared across pods - Memory pressure - Cold starts ## Decision We will use Redis Cluster for caching with a 6-node cluster for high availability. ## Consequences - Implement cache-aside pattern - Set appropriate TTLs - Monitor memory usage - Plan for cache stampede prevention",
              "score": null
            },
            {
              "doc_id": "arch_caching_layer",
              "chunk_id": "arch_caching_layer_fix_0",
              "content": "# Caching Layer Architecture ## Overview The Caching Layer reduces latency and database load by caching frequently accessed data. Uses a multi-tier strategy with local and distributed caches. ## Design Principles - **Cache Invalidation**: Explicit invalidation on writes - **Graceful Degradation**: System works without cache ## Components ### Local Cache First-level cache with 100ms TTL. Reduces Redis round-trips for hot data. **Technology:** Ristretto in-memory cache **Scaling:** Per-pod memory allocation ### Distributed Cache Second-level cache with configurable TTL. Maximum key size 512MB. **Technology:** Redis Cluster 7.x **Scaling:** 6-node cluster with sharding ### CDN Caches static assets and API responses. Cache hit ratio target 95%. **Technology:** CloudFront with Lambda@Edge **Scaling:** Global edge locations ## Data Flow Request \u2192 Local Cache \u2192 Redis \u2192 Database \u2192 Cache Population ## Performance Characteristics - **Latency P50:** 1ms (hit), 50ms (miss) - **Latency P99:** 5ms (hit), 200ms (miss) - **Throughput:** 100000 reads/second",
              "score": null
            },
            {
              "doc_id": "config_cache",
              "chunk_id": "config_cache_fix_0",
              "content": "# Cache Configuration Reference ## Overview This document describes all configuration options for Cache. ## Environment Variables | Variable | Type | Default | Description | |----------|------|---------|-------------| | `REDIS_URL` | string | required | Redis connection string | | `REDIS_POOL_SIZE` | integer | 10 | Connection pool size | | `CACHE_TTL_SECONDS` | integer | 300 | Default cache TTL | | `CACHE_PREFIX` | string | cf: | Key prefix for namespacing | | `CACHE_COMPRESSION` | boolean | true | Enable value compression | | `CACHE_MAX_SIZE_MB` | integer | 100 | Maximum cache size per pod | ## Configuration File Configuration can also be provided via `cache.yaml`: ```yaml redis_url: required redis_pool_size: 10 cache_ttl_seconds: 300 cache_prefix: cf: cache_compression: true ``` ## Validation Configuration is validated at startup. Invalid configuration will prevent the service from starting.",
              "score": null
            },
            {
              "doc_id": "arch_data_pipeline",
              "chunk_id": "arch_data_pipeline_fix_0",
              "content": "# Data Pipeline Architecture ## Overview The Data Pipeline processes and transforms data between workflow steps. It supports batch processing up to 10GB and streaming for real-time use cases. ## Design Principles - **Exactly-Once Processing**: Each record is processed exactly once - **Backpressure Handling**: System gracefully handles varying load ## Components ### Stream Processor Handles real-time data streams. Maximum throughput of 100000 messages per second. **Technology:** Apache Kafka with Flink **Scaling:** Partition-based scaling ### Batch Processor Processes large batch jobs. Supports data formats: JSON, CSV, Parquet, Avro. **Technology:** Apache Spark on Kubernetes **Scaling:** Dynamic resource allocation ### Data Lake Long-term data storage with time-travel queries. Retention configurable up to 7 years. **Technology:** S3 with Iceberg tables **Scaling:** Unlimited with tiered storage ## Data Flow Input \u2192 Validation \u2192 Transform \u2192 Enrich \u2192 Output ## Performance Characteristics - **Latency P50:** 100ms (streaming), 5min (batch) - **Latency P99:** 500ms (streaming), 30min (batch) - **Throughput:** 100000 messages/second (streaming)",
              "score": null
            },
            {
              "doc_id": "adr_007",
              "chunk_id": "adr_007_fix_0",
              "content": "# ADR-007: Implement Rate Limiting at API Gateway **Status:** Accepted **Date:** 2024-09-03 ## Context We need to protect our services from abuse and ensure fair resource usage across customers. ## Options Considered ### Option 1: Sliding window at gateway Rate limit at API gateway level. **Pros:** - Centralized enforcement - Low latency - Easy configuration **Cons:** - Gateway becomes bottleneck - Coarse granularity - Redis dependency ### Option 2: Per-service limiting Each service implements its own limits. **Pros:** - Fine-grained control - Service autonomy - No single point of failure **Cons:** - Inconsistent implementation - Higher complexity - Distributed state ### Option 3: Token bucket Token-based rate limiting. **Pros:** - Allows bursts - Smooth rate limiting - Well understood **Cons:** - More complex - Harder to explain to users - State management ## Decision We will implement sliding window rate limiting at the Kong API Gateway using Redis. ## Consequences - Default limit: 100 requests per minute - Implement per-plan limits - Return rate limit headers - Add retry-after on 429",
              "score": null
            }
          ],
          "latency_ms": 3.6145810008747503,
          "query_id": "compare_026",
          "dimension": "problem",
          "query": "Cache is getting expensive - should we switch from Redis to Memcached to save on memory?"
        },
        {
          "key_facts": [
            "Redis Cluster",
            "Memcached",
            "data structures",
            "persistence"
          ],
          "found_facts": [
            "Redis Cluster",
            "Memcached",
            "data structures",
            "persistence"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "adr_004"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "arch_caching_layer",
              "chunk_id": "arch_caching_layer_fix_0",
              "content": "# Caching Layer Architecture ## Overview The Caching Layer reduces latency and database load by caching frequently accessed data. Uses a multi-tier strategy with local and distributed caches. ## Design Principles - **Cache Invalidation**: Explicit invalidation on writes - **Graceful Degradation**: System works without cache ## Components ### Local Cache First-level cache with 100ms TTL. Reduces Redis round-trips for hot data. **Technology:** Ristretto in-memory cache **Scaling:** Per-pod memory allocation ### Distributed Cache Second-level cache with configurable TTL. Maximum key size 512MB. **Technology:** Redis Cluster 7.x **Scaling:** 6-node cluster with sharding ### CDN Caches static assets and API responses. Cache hit ratio target 95%. **Technology:** CloudFront with Lambda@Edge **Scaling:** Global edge locations ## Data Flow Request \u2192 Local Cache \u2192 Redis \u2192 Database \u2192 Cache Population ## Performance Characteristics - **Latency P50:** 1ms (hit), 50ms (miss) - **Latency P99:** 5ms (hit), 200ms (miss) - **Throughput:** 100000 reads/second",
              "score": null
            },
            {
              "doc_id": "adr_004",
              "chunk_id": "adr_004_fix_0",
              "content": "# ADR-004: Use Redis for Caching and Session Storage **Status:** Accepted **Date:** 2024-04-05 ## Context We need a fast data store for caching API responses, session data, and rate limiting counters. ## Options Considered ### Option 1: Redis Cluster In-memory data structure store. **Pros:** - Sub-millisecond latency - Rich data structures - Pub/sub support - Clustering **Cons:** - Memory cost - Persistence complexity - Cluster management ### Option 2: Memcached Simple key-value cache. **Pros:** - Simple operation - Multi-threaded - Lower memory overhead **Cons:** - No persistence - Limited data types - No clustering ### Option 3: Application-level cache In-process caching only. **Pros:** - No external dependency - Fastest possible - Simple **Cons:** - Not shared across pods - Memory pressure - Cold starts ## Decision We will use Redis Cluster for caching with a 6-node cluster for high availability. ## Consequences - Implement cache-aside pattern - Set appropriate TTLs - Monitor memory usage - Plan for cache stampede prevention",
              "score": null
            },
            {
              "doc_id": "config_cache",
              "chunk_id": "config_cache_fix_0",
              "content": "# Cache Configuration Reference ## Overview This document describes all configuration options for Cache. ## Environment Variables | Variable | Type | Default | Description | |----------|------|---------|-------------| | `REDIS_URL` | string | required | Redis connection string | | `REDIS_POOL_SIZE` | integer | 10 | Connection pool size | | `CACHE_TTL_SECONDS` | integer | 300 | Default cache TTL | | `CACHE_PREFIX` | string | cf: | Key prefix for namespacing | | `CACHE_COMPRESSION` | boolean | true | Enable value compression | | `CACHE_MAX_SIZE_MB` | integer | 100 | Maximum cache size per pod | ## Configuration File Configuration can also be provided via `cache.yaml`: ```yaml redis_url: required redis_pool_size: 10 cache_ttl_seconds: 300 cache_prefix: cf: cache_compression: true ``` ## Validation Configuration is validated at startup. Invalid configuration will prevent the service from starting.",
              "score": null
            },
            {
              "doc_id": "adr_007",
              "chunk_id": "adr_007_fix_0",
              "content": "# ADR-007: Implement Rate Limiting at API Gateway **Status:** Accepted **Date:** 2024-09-03 ## Context We need to protect our services from abuse and ensure fair resource usage across customers. ## Options Considered ### Option 1: Sliding window at gateway Rate limit at API gateway level. **Pros:** - Centralized enforcement - Low latency - Easy configuration **Cons:** - Gateway becomes bottleneck - Coarse granularity - Redis dependency ### Option 2: Per-service limiting Each service implements its own limits. **Pros:** - Fine-grained control - Service autonomy - No single point of failure **Cons:** - Inconsistent implementation - Higher complexity - Distributed state ### Option 3: Token bucket Token-based rate limiting. **Pros:** - Allows bursts - Smooth rate limiting - Well understood **Cons:** - More complex - Harder to explain to users - State management ## Decision We will implement sliding window rate limiting at the Kong API Gateway using Redis. ## Consequences - Default limit: 100 requests per minute - Implement per-plan limits - Return rate limit headers - Add retry-after on 429",
              "score": null
            },
            {
              "doc_id": "arch_workflow_engine",
              "chunk_id": "arch_workflow_engine_fix_0",
              "content": "# Workflow Engine Architecture ## Overview The Workflow Engine is the core component responsible for executing user-defined workflows. It uses an event-driven architecture with support for parallel execution, conditional branching, and automatic retries. ## Design Principles - **Idempotency**: All operations can be safely retried without side effects - **Eventual Consistency**: State converges to consistent view within 5 seconds - **Fault Tolerance**: System continues operating despite component failures ## Components ### Scheduler Manages workflow scheduling and triggers. Supports cron expressions, webhooks, and event-based triggers. **Technology:** Custom Go service **Scaling:** Horizontal with leader election ### Executor Runs individual workflow steps in isolated containers. Maximum execution time is 30 minutes per step. **Technology:** Kubernetes Jobs **Scaling:** Auto-scaling based on queue depth ### State Store Persists workflow state, execution history, and step results. Uses optimistic locking for concurrent updates. **Technology:** PostgreSQL with JSONB **Scaling:** Primary-replica with read replicas ## Data Flow Triggers \u2192 Scheduler \u2192 Job Queue \u2192 Executor \u2192 State Store \u2192 Webhooks/Notifications ## Performance Characteristics - **Latency P50:** 45ms - **Latency P99:** 250ms - **Throughput:** 5000 executions/minute",
              "score": null
            }
          ],
          "latency_ms": 3.5229309996793745,
          "query_id": "compare_026",
          "dimension": "casual",
          "query": "redis vs memcached decision"
        },
        {
          "key_facts": [
            "Redis Cluster",
            "Memcached",
            "data structures",
            "persistence"
          ],
          "found_facts": [
            "Redis Cluster",
            "Memcached",
            "data structures",
            "persistence"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "adr_004"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "arch_caching_layer",
              "chunk_id": "arch_caching_layer_fix_0",
              "content": "# Caching Layer Architecture ## Overview The Caching Layer reduces latency and database load by caching frequently accessed data. Uses a multi-tier strategy with local and distributed caches. ## Design Principles - **Cache Invalidation**: Explicit invalidation on writes - **Graceful Degradation**: System works without cache ## Components ### Local Cache First-level cache with 100ms TTL. Reduces Redis round-trips for hot data. **Technology:** Ristretto in-memory cache **Scaling:** Per-pod memory allocation ### Distributed Cache Second-level cache with configurable TTL. Maximum key size 512MB. **Technology:** Redis Cluster 7.x **Scaling:** 6-node cluster with sharding ### CDN Caches static assets and API responses. Cache hit ratio target 95%. **Technology:** CloudFront with Lambda@Edge **Scaling:** Global edge locations ## Data Flow Request \u2192 Local Cache \u2192 Redis \u2192 Database \u2192 Cache Population ## Performance Characteristics - **Latency P50:** 1ms (hit), 50ms (miss) - **Latency P99:** 5ms (hit), 200ms (miss) - **Throughput:** 100000 reads/second",
              "score": null
            },
            {
              "doc_id": "adr_004",
              "chunk_id": "adr_004_fix_0",
              "content": "# ADR-004: Use Redis for Caching and Session Storage **Status:** Accepted **Date:** 2024-04-05 ## Context We need a fast data store for caching API responses, session data, and rate limiting counters. ## Options Considered ### Option 1: Redis Cluster In-memory data structure store. **Pros:** - Sub-millisecond latency - Rich data structures - Pub/sub support - Clustering **Cons:** - Memory cost - Persistence complexity - Cluster management ### Option 2: Memcached Simple key-value cache. **Pros:** - Simple operation - Multi-threaded - Lower memory overhead **Cons:** - No persistence - Limited data types - No clustering ### Option 3: Application-level cache In-process caching only. **Pros:** - No external dependency - Fastest possible - Simple **Cons:** - Not shared across pods - Memory pressure - Cold starts ## Decision We will use Redis Cluster for caching with a 6-node cluster for high availability. ## Consequences - Implement cache-aside pattern - Set appropriate TTLs - Monitor memory usage - Plan for cache stampede prevention",
              "score": null
            },
            {
              "doc_id": "config_cache",
              "chunk_id": "config_cache_fix_0",
              "content": "# Cache Configuration Reference ## Overview This document describes all configuration options for Cache. ## Environment Variables | Variable | Type | Default | Description | |----------|------|---------|-------------| | `REDIS_URL` | string | required | Redis connection string | | `REDIS_POOL_SIZE` | integer | 10 | Connection pool size | | `CACHE_TTL_SECONDS` | integer | 300 | Default cache TTL | | `CACHE_PREFIX` | string | cf: | Key prefix for namespacing | | `CACHE_COMPRESSION` | boolean | true | Enable value compression | | `CACHE_MAX_SIZE_MB` | integer | 100 | Maximum cache size per pod | ## Configuration File Configuration can also be provided via `cache.yaml`: ```yaml redis_url: required redis_pool_size: 10 cache_ttl_seconds: 300 cache_prefix: cf: cache_compression: true ``` ## Validation Configuration is validated at startup. Invalid configuration will prevent the service from starting.",
              "score": null
            },
            {
              "doc_id": "adr_002",
              "chunk_id": "adr_002_fix_0",
              "content": "# ADR-002: Adopt Kubernetes for Container Orchestration **Status:** Accepted **Date:** 2024-01-24 ## Context We need a platform to deploy and scale our microservices. The platform must support auto-scaling, rolling deployments, and multi-region deployment. ## Options Considered ### Option 1: Kubernetes (EKS) Industry-standard container orchestration. **Pros:** - Industry standard - Rich ecosystem - Cloud agnostic - Auto-scaling **Cons:** - Operational complexity - Learning curve - Resource overhead ### Option 2: AWS ECS Amazon's native container service. **Pros:** - Simple operation - Native AWS integration - Lower overhead **Cons:** - AWS lock-in - Less flexible - Smaller community ### Option 3: Serverless (Lambda) Function-as-a-service approach. **Pros:** - Zero infrastructure - Pay per use - Auto-scaling **Cons:** - Cold starts - 15-minute limit - Vendor lock-in ## Decision We will use Kubernetes on EKS for container orchestration with Helm for deployments. ## Consequences - Need platform team expertise - Implement GitOps with ArgoCD - Use managed node groups for easier operations",
              "score": null
            },
            {
              "doc_id": "adr_007",
              "chunk_id": "adr_007_fix_0",
              "content": "# ADR-007: Implement Rate Limiting at API Gateway **Status:** Accepted **Date:** 2024-09-03 ## Context We need to protect our services from abuse and ensure fair resource usage across customers. ## Options Considered ### Option 1: Sliding window at gateway Rate limit at API gateway level. **Pros:** - Centralized enforcement - Low latency - Easy configuration **Cons:** - Gateway becomes bottleneck - Coarse granularity - Redis dependency ### Option 2: Per-service limiting Each service implements its own limits. **Pros:** - Fine-grained control - Service autonomy - No single point of failure **Cons:** - Inconsistent implementation - Higher complexity - Distributed state ### Option 3: Token bucket Token-based rate limiting. **Pros:** - Allows bursts - Smooth rate limiting - Well understood **Cons:** - More complex - Harder to explain to users - State management ## Decision We will implement sliding window rate limiting at the Kong API Gateway using Redis. ## Consequences - Default limit: 100 requests per minute - Implement per-plan limits - Return rate limit headers - Add retry-after on 429",
              "score": null
            }
          ],
          "latency_ms": 3.700391000165837,
          "query_id": "compare_026",
          "dimension": "contextual",
          "query": "Evaluating cache solutions for a new microservice - why did we standardize on Redis over Memcached?"
        },
        {
          "key_facts": [
            "Redis Cluster",
            "Memcached",
            "data structures",
            "persistence"
          ],
          "found_facts": [
            "Redis Cluster",
            "Memcached",
            "data structures",
            "persistence"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "adr_004"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "adr_004",
              "chunk_id": "adr_004_fix_0",
              "content": "# ADR-004: Use Redis for Caching and Session Storage **Status:** Accepted **Date:** 2024-04-05 ## Context We need a fast data store for caching API responses, session data, and rate limiting counters. ## Options Considered ### Option 1: Redis Cluster In-memory data structure store. **Pros:** - Sub-millisecond latency - Rich data structures - Pub/sub support - Clustering **Cons:** - Memory cost - Persistence complexity - Cluster management ### Option 2: Memcached Simple key-value cache. **Pros:** - Simple operation - Multi-threaded - Lower memory overhead **Cons:** - No persistence - Limited data types - No clustering ### Option 3: Application-level cache In-process caching only. **Pros:** - No external dependency - Fastest possible - Simple **Cons:** - Not shared across pods - Memory pressure - Cold starts ## Decision We will use Redis Cluster for caching with a 6-node cluster for high availability. ## Consequences - Implement cache-aside pattern - Set appropriate TTLs - Monitor memory usage - Plan for cache stampede prevention",
              "score": null
            },
            {
              "doc_id": "arch_caching_layer",
              "chunk_id": "arch_caching_layer_fix_0",
              "content": "# Caching Layer Architecture ## Overview The Caching Layer reduces latency and database load by caching frequently accessed data. Uses a multi-tier strategy with local and distributed caches. ## Design Principles - **Cache Invalidation**: Explicit invalidation on writes - **Graceful Degradation**: System works without cache ## Components ### Local Cache First-level cache with 100ms TTL. Reduces Redis round-trips for hot data. **Technology:** Ristretto in-memory cache **Scaling:** Per-pod memory allocation ### Distributed Cache Second-level cache with configurable TTL. Maximum key size 512MB. **Technology:** Redis Cluster 7.x **Scaling:** 6-node cluster with sharding ### CDN Caches static assets and API responses. Cache hit ratio target 95%. **Technology:** CloudFront with Lambda@Edge **Scaling:** Global edge locations ## Data Flow Request \u2192 Local Cache \u2192 Redis \u2192 Database \u2192 Cache Population ## Performance Characteristics - **Latency P50:** 1ms (hit), 50ms (miss) - **Latency P99:** 5ms (hit), 200ms (miss) - **Throughput:** 100000 reads/second",
              "score": null
            },
            {
              "doc_id": "arch_data_pipeline",
              "chunk_id": "arch_data_pipeline_fix_0",
              "content": "# Data Pipeline Architecture ## Overview The Data Pipeline processes and transforms data between workflow steps. It supports batch processing up to 10GB and streaming for real-time use cases. ## Design Principles - **Exactly-Once Processing**: Each record is processed exactly once - **Backpressure Handling**: System gracefully handles varying load ## Components ### Stream Processor Handles real-time data streams. Maximum throughput of 100000 messages per second. **Technology:** Apache Kafka with Flink **Scaling:** Partition-based scaling ### Batch Processor Processes large batch jobs. Supports data formats: JSON, CSV, Parquet, Avro. **Technology:** Apache Spark on Kubernetes **Scaling:** Dynamic resource allocation ### Data Lake Long-term data storage with time-travel queries. Retention configurable up to 7 years. **Technology:** S3 with Iceberg tables **Scaling:** Unlimited with tiered storage ## Data Flow Input \u2192 Validation \u2192 Transform \u2192 Enrich \u2192 Output ## Performance Characteristics - **Latency P50:** 100ms (streaming), 5min (batch) - **Latency P99:** 500ms (streaming), 30min (batch) - **Throughput:** 100000 messages/second (streaming)",
              "score": null
            },
            {
              "doc_id": "adr_008",
              "chunk_id": "adr_008_fix_0",
              "content": "# ADR-008: Use S3 for Object Storage **Status:** Accepted **Date:** 2024-10-14 ## Context We need storage for workflow artifacts, execution outputs, and user uploads. ## Options Considered ### Option 1: Amazon S3 Object storage service. **Pros:** - 99.999999999% durability - Lifecycle policies - Event notifications - CDN integration **Cons:** - AWS lock-in - Eventual consistency for overwrites - Cost at scale ### Option 2: Self-hosted MinIO S3-compatible object storage. **Pros:** - No vendor lock-in - S3 compatible - On-premises option **Cons:** - Operational burden - Lower durability guarantees - Scaling complexity ### Option 3: Google Cloud Storage GCP object storage. **Pros:** - Strong consistency - Good performance - Unified billing **Cons:** - GCP lock-in - Fewer features than S3 - Smaller ecosystem ## Decision We will use Amazon S3 for object storage with intelligent tiering for cost optimization. ## Consequences - Enable versioning for critical buckets - Implement lifecycle policies - Use pre-signed URLs for uploads - Enable server-side encryption",
              "score": null
            },
            {
              "doc_id": "arch_workflow_engine",
              "chunk_id": "arch_workflow_engine_fix_0",
              "content": "# Workflow Engine Architecture ## Overview The Workflow Engine is the core component responsible for executing user-defined workflows. It uses an event-driven architecture with support for parallel execution, conditional branching, and automatic retries. ## Design Principles - **Idempotency**: All operations can be safely retried without side effects - **Eventual Consistency**: State converges to consistent view within 5 seconds - **Fault Tolerance**: System continues operating despite component failures ## Components ### Scheduler Manages workflow scheduling and triggers. Supports cron expressions, webhooks, and event-based triggers. **Technology:** Custom Go service **Scaling:** Horizontal with leader election ### Executor Runs individual workflow steps in isolated containers. Maximum execution time is 30 minutes per step. **Technology:** Kubernetes Jobs **Scaling:** Auto-scaling based on queue depth ### State Store Persists workflow state, execution history, and step results. Uses optimistic locking for concurrent updates. **Technology:** PostgreSQL with JSONB **Scaling:** Primary-replica with read replicas ## Data Flow Triggers \u2192 Scheduler \u2192 Job Queue \u2192 Executor \u2192 State Store \u2192 Webhooks/Notifications ## Performance Characteristics - **Latency P50:** 45ms - **Latency P99:** 250ms - **Throughput:** 5000 executions/minute",
              "score": null
            }
          ],
          "latency_ms": 3.9077980000001844,
          "query_id": "compare_026",
          "dimension": "negation",
          "query": "Why isn't Memcached good enough if it's simpler to operate?"
        },
        {
          "key_facts": [
            "kubectl top pods",
            "cloudflow debug profile",
            "rollout restart"
          ],
          "found_facts": [
            "kubectl top pods",
            "cloudflow debug profile",
            "rollout restart"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "runbook_high_cpu_usage"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "runbook_high_cpu_usage",
              "chunk_id": "runbook_high_cpu_usage_fix_0",
              "content": "# Runbook: High CPU Usage ## Severity **Level:** P1 - Critical **Response Time:** 15 minutes ## Symptoms - CPU utilization > 80% sustained - Increased response latency - Autoscaler at maximum replicas ## Diagnosis ### Step 1 Check current CPU usage: `kubectl top pods -n production` ### Step 2 Identify hot pods: `kubectl top pods --sort-by=cpu -n production | head -10` ### Step 3 Check for recent deployments: `kubectl rollout history deployment/api -n production` ### Step 4 Profile the application: `cloudflow debug profile --pod api-xyz --duration 60s` ## Resolution ### Step 1 Scale up if traffic-related: `kubectl scale deployment/api --replicas=10 -n production` ### Step 2 Restart pods if memory leak suspected: `kubectl rollout restart deployment/api -n production` ### Step 3 Roll back recent deployment if regression: `kubectl rollout undo deployment/api -n production` ### Step 4 Add resource limits if unbounded: update deployment with CPU limits ## Escalation If CPU remains high after scaling, escalate to platform team. Page on-call if affecting users. ## Post-Incident - Document timeline in incident log - Schedule post-mortem within 48 hours - Update runbook if new patterns discovered",
              "score": null
            },
            {
              "doc_id": "runbook_out_of_memory_oom_kills",
              "chunk_id": "runbook_out_of_memory_oom_kills_fix_0",
              "content": "# Runbook: Out of Memory (OOM) Kills ## Severity **Level:** P3 - Medium **Response Time:** 15 minutes ## Symptoms - Pods restarting with OOMKilled status - Memory usage at limit - Application slowdown before restart ## Diagnosis ### Step 1 Check OOM events: `kubectl get events -n production --field-selector reason=OOMKilled` ### Step 2 Review memory usage: `kubectl top pods -n production --sort-by=memory` ### Step 3 Check container limits: `kubectl describe pod <pod-name> -n production | grep -A5 Limits` ### Step 4 Analyze heap dumps if available: `cloudflow debug heapdump --pod <pod-name>` ## Resolution ### Step 1 Increase memory limit: update deployment resources.limits.memory ### Step 2 Add memory request equal to limit to guarantee allocation ### Step 3 Enable JVM heap dump on OOM: add -XX:+HeapDumpOnOutOfMemoryError ### Step 4 Investigate memory leaks with profiling tools ### Step 5 Consider horizontal scaling instead of vertical ## Escalation If OOMs continue after limit increase, escalate to development team for memory leak investigation. ## Post-Incident - Document timeline in incident log - Schedule post-mortem within 48 hours - Update runbook if new patterns discovered",
              "score": null
            },
            {
              "doc_id": "runbook_database_connection_exhaustion",
              "chunk_id": "runbook_database_connection_exhaustion_fix_0",
              "content": "# Runbook: Database Connection Exhaustion ## Severity **Level:** P1 - Critical **Response Time:** 15 minutes ## Symptoms - Connection pool at 100% - Queries timing out - 'too many connections' errors in logs ## Diagnosis ### Step 1 Check active connections: `SELECT count(*) FROM pg_stat_activity` ### Step 2 Identify long-running queries: `SELECT pid, now() - pg_stat_activity.query_start AS duration, query FROM pg_stat_activity WHERE state != 'idle' ORDER BY duration DESC` ### Step 3 Check PgBouncer stats: `SHOW POOLS` ### Step 4 Verify connection limits: `SHOW max_connections` ## Resolution ### Step 1 Kill long-running queries: `SELECT pg_terminate_backend(pid) FROM pg_stat_activity WHERE duration > interval '5 minutes'` ### Step 2 Increase PgBouncer pool size temporarily: update pgbouncer.ini ### Step 3 Restart affected services to release connections: `kubectl rollout restart deployment/api` ### Step 4 Scale down non-critical services to free connections ## Escalation If connections remain exhausted, escalate to DBA. Consider enabling connection queueing. ## Post-Incident - Document timeline in incident log - Schedule post-mortem within 48 hours - Update runbook if new patterns discovered",
              "score": null
            },
            {
              "doc_id": "arch_monitoring_stack",
              "chunk_id": "arch_monitoring_stack_fix_0",
              "content": "# Monitoring Stack Architecture ## Overview The Monitoring Stack provides observability into system health, performance, and errors. Includes metrics, logs, traces, and alerting. ## Design Principles - **Observability**: Every component emits metrics and traces - **Actionable Alerts**: Alerts include runbook links ## Components ### Metrics Collects time-series metrics. Retention is 15 days local, 1 year in object storage. **Technology:** Prometheus with Thanos **Scaling:** Federated with long-term storage ### Logging Centralized log aggregation. Supports structured JSON logs with label indexing. **Technology:** Loki with Grafana **Scaling:** Distributed with sharding ### Tracing Distributed request tracing. Trace retention is 7 days. **Technology:** Jaeger with OpenTelemetry **Scaling:** Sampling at 10% ### Alerting Routes alerts to on-call engineers. Supports escalation policies and silencing. **Technology:** Alertmanager with PagerDuty **Scaling:** HA pair ## Data Flow Application \u2192 Collector \u2192 Storage \u2192 Query \u2192 Dashboard/Alert ## Performance Characteristics - **Latency P50:** N/A - **Latency P99:** N/A - **Throughput:** 1M metrics/second, 100GB logs/day",
              "score": null
            },
            {
              "doc_id": "runbook_kafka_consumer_lag",
              "chunk_id": "runbook_kafka_consumer_lag_fix_0",
              "content": "# Runbook: Kafka Consumer Lag ## Severity **Level:** P1 - Critical **Response Time:** 1 hour ## Symptoms - Consumer lag > 10000 messages - Processing delays reported - Alerts from lag monitor ## Diagnosis ### Step 1 Check consumer group lag: `kafka-consumer-groups.sh --bootstrap-server kafka:9092 --describe --group workflow-executor` ### Step 2 Verify consumer health: `kubectl get pods -l app=workflow-executor -n production` ### Step 3 Check for processing errors: `kubectl logs -l app=workflow-executor --tail=100 -n production | grep ERROR` ### Step 4 Monitor throughput: `cloudflow metrics get kafka.consumer.records_per_second` ## Resolution ### Step 1 Scale up consumers: `kubectl scale deployment/workflow-executor --replicas=10 -n production` ### Step 2 Reset offset if messages are stale: `kafka-consumer-groups.sh --bootstrap-server kafka:9092 --group workflow-executor --reset-offsets --to-latest --execute --topic workflows` ### Step 3 Pause producer if backpressure needed: `cloudflow workflows pause-triggers` ### Step 4 Increase partition count for parallelism (requires coordination) ## Escalation If lag persists > 30 minutes, page platform team. May need Kafka cluster scaling. ## Post-Incident - Document timeline in incident log - Schedule post-mortem within 48 hours - Update runbook if new patterns discovered",
              "score": null
            }
          ],
          "latency_ms": 3.380604999620118,
          "query_id": "troubleshoot_027",
          "dimension": "original",
          "query": "How do I diagnose high CPU usage?"
        },
        {
          "key_facts": [
            "kubectl top pods",
            "cloudflow debug profile",
            "rollout restart"
          ],
          "found_facts": [
            "kubectl top pods",
            "cloudflow debug profile",
            "rollout restart"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "runbook_high_cpu_usage"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "runbook_high_cpu_usage",
              "chunk_id": "runbook_high_cpu_usage_fix_0",
              "content": "# Runbook: High CPU Usage ## Severity **Level:** P1 - Critical **Response Time:** 15 minutes ## Symptoms - CPU utilization > 80% sustained - Increased response latency - Autoscaler at maximum replicas ## Diagnosis ### Step 1 Check current CPU usage: `kubectl top pods -n production` ### Step 2 Identify hot pods: `kubectl top pods --sort-by=cpu -n production | head -10` ### Step 3 Check for recent deployments: `kubectl rollout history deployment/api -n production` ### Step 4 Profile the application: `cloudflow debug profile --pod api-xyz --duration 60s` ## Resolution ### Step 1 Scale up if traffic-related: `kubectl scale deployment/api --replicas=10 -n production` ### Step 2 Restart pods if memory leak suspected: `kubectl rollout restart deployment/api -n production` ### Step 3 Roll back recent deployment if regression: `kubectl rollout undo deployment/api -n production` ### Step 4 Add resource limits if unbounded: update deployment with CPU limits ## Escalation If CPU remains high after scaling, escalate to platform team. Page on-call if affecting users. ## Post-Incident - Document timeline in incident log - Schedule post-mortem within 48 hours - Update runbook if new patterns discovered",
              "score": null
            },
            {
              "doc_id": "runbook_out_of_memory_oom_kills",
              "chunk_id": "runbook_out_of_memory_oom_kills_fix_0",
              "content": "# Runbook: Out of Memory (OOM) Kills ## Severity **Level:** P3 - Medium **Response Time:** 15 minutes ## Symptoms - Pods restarting with OOMKilled status - Memory usage at limit - Application slowdown before restart ## Diagnosis ### Step 1 Check OOM events: `kubectl get events -n production --field-selector reason=OOMKilled` ### Step 2 Review memory usage: `kubectl top pods -n production --sort-by=memory` ### Step 3 Check container limits: `kubectl describe pod <pod-name> -n production | grep -A5 Limits` ### Step 4 Analyze heap dumps if available: `cloudflow debug heapdump --pod <pod-name>` ## Resolution ### Step 1 Increase memory limit: update deployment resources.limits.memory ### Step 2 Add memory request equal to limit to guarantee allocation ### Step 3 Enable JVM heap dump on OOM: add -XX:+HeapDumpOnOutOfMemoryError ### Step 4 Investigate memory leaks with profiling tools ### Step 5 Consider horizontal scaling instead of vertical ## Escalation If OOMs continue after limit increase, escalate to development team for memory leak investigation. ## Post-Incident - Document timeline in incident log - Schedule post-mortem within 48 hours - Update runbook if new patterns discovered",
              "score": null
            },
            {
              "doc_id": "runbook_kafka_consumer_lag",
              "chunk_id": "runbook_kafka_consumer_lag_fix_0",
              "content": "# Runbook: Kafka Consumer Lag ## Severity **Level:** P1 - Critical **Response Time:** 1 hour ## Symptoms - Consumer lag > 10000 messages - Processing delays reported - Alerts from lag monitor ## Diagnosis ### Step 1 Check consumer group lag: `kafka-consumer-groups.sh --bootstrap-server kafka:9092 --describe --group workflow-executor` ### Step 2 Verify consumer health: `kubectl get pods -l app=workflow-executor -n production` ### Step 3 Check for processing errors: `kubectl logs -l app=workflow-executor --tail=100 -n production | grep ERROR` ### Step 4 Monitor throughput: `cloudflow metrics get kafka.consumer.records_per_second` ## Resolution ### Step 1 Scale up consumers: `kubectl scale deployment/workflow-executor --replicas=10 -n production` ### Step 2 Reset offset if messages are stale: `kafka-consumer-groups.sh --bootstrap-server kafka:9092 --group workflow-executor --reset-offsets --to-latest --execute --topic workflows` ### Step 3 Pause producer if backpressure needed: `cloudflow workflows pause-triggers` ### Step 4 Increase partition count for parallelism (requires coordination) ## Escalation If lag persists > 30 minutes, page platform team. May need Kafka cluster scaling. ## Post-Incident - Document timeline in incident log - Schedule post-mortem within 48 hours - Update runbook if new patterns discovered",
              "score": null
            },
            {
              "doc_id": "runbook_database_connection_exhaustion",
              "chunk_id": "runbook_database_connection_exhaustion_fix_0",
              "content": "# Runbook: Database Connection Exhaustion ## Severity **Level:** P1 - Critical **Response Time:** 15 minutes ## Symptoms - Connection pool at 100% - Queries timing out - 'too many connections' errors in logs ## Diagnosis ### Step 1 Check active connections: `SELECT count(*) FROM pg_stat_activity` ### Step 2 Identify long-running queries: `SELECT pid, now() - pg_stat_activity.query_start AS duration, query FROM pg_stat_activity WHERE state != 'idle' ORDER BY duration DESC` ### Step 3 Check PgBouncer stats: `SHOW POOLS` ### Step 4 Verify connection limits: `SHOW max_connections` ## Resolution ### Step 1 Kill long-running queries: `SELECT pg_terminate_backend(pid) FROM pg_stat_activity WHERE duration > interval '5 minutes'` ### Step 2 Increase PgBouncer pool size temporarily: update pgbouncer.ini ### Step 3 Restart affected services to release connections: `kubectl rollout restart deployment/api` ### Step 4 Scale down non-critical services to free connections ## Escalation If connections remain exhausted, escalate to DBA. Consider enabling connection queueing. ## Post-Incident - Document timeline in incident log - Schedule post-mortem within 48 hours - Update runbook if new patterns discovered",
              "score": null
            },
            {
              "doc_id": "arch_workflow_engine",
              "chunk_id": "arch_workflow_engine_fix_0",
              "content": "# Workflow Engine Architecture ## Overview The Workflow Engine is the core component responsible for executing user-defined workflows. It uses an event-driven architecture with support for parallel execution, conditional branching, and automatic retries. ## Design Principles - **Idempotency**: All operations can be safely retried without side effects - **Eventual Consistency**: State converges to consistent view within 5 seconds - **Fault Tolerance**: System continues operating despite component failures ## Components ### Scheduler Manages workflow scheduling and triggers. Supports cron expressions, webhooks, and event-based triggers. **Technology:** Custom Go service **Scaling:** Horizontal with leader election ### Executor Runs individual workflow steps in isolated containers. Maximum execution time is 30 minutes per step. **Technology:** Kubernetes Jobs **Scaling:** Auto-scaling based on queue depth ### State Store Persists workflow state, execution history, and step results. Uses optimistic locking for concurrent updates. **Technology:** PostgreSQL with JSONB **Scaling:** Primary-replica with read replicas ## Data Flow Triggers \u2192 Scheduler \u2192 Job Queue \u2192 Executor \u2192 State Store \u2192 Webhooks/Notifications ## Performance Characteristics - **Latency P50:** 45ms - **Latency P99:** 250ms - **Throughput:** 5000 executions/minute",
              "score": null
            }
          ],
          "latency_ms": 3.7733769986516563,
          "query_id": "troubleshoot_027",
          "dimension": "synonym",
          "query": "How do I troubleshoot elevated processor utilization in the cluster?"
        },
        {
          "key_facts": [
            "kubectl top pods",
            "cloudflow debug profile",
            "rollout restart"
          ],
          "found_facts": [
            "kubectl top pods",
            "cloudflow debug profile",
            "rollout restart"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "runbook_high_cpu_usage"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "runbook_high_cpu_usage",
              "chunk_id": "runbook_high_cpu_usage_fix_0",
              "content": "# Runbook: High CPU Usage ## Severity **Level:** P1 - Critical **Response Time:** 15 minutes ## Symptoms - CPU utilization > 80% sustained - Increased response latency - Autoscaler at maximum replicas ## Diagnosis ### Step 1 Check current CPU usage: `kubectl top pods -n production` ### Step 2 Identify hot pods: `kubectl top pods --sort-by=cpu -n production | head -10` ### Step 3 Check for recent deployments: `kubectl rollout history deployment/api -n production` ### Step 4 Profile the application: `cloudflow debug profile --pod api-xyz --duration 60s` ## Resolution ### Step 1 Scale up if traffic-related: `kubectl scale deployment/api --replicas=10 -n production` ### Step 2 Restart pods if memory leak suspected: `kubectl rollout restart deployment/api -n production` ### Step 3 Roll back recent deployment if regression: `kubectl rollout undo deployment/api -n production` ### Step 4 Add resource limits if unbounded: update deployment with CPU limits ## Escalation If CPU remains high after scaling, escalate to platform team. Page on-call if affecting users. ## Post-Incident - Document timeline in incident log - Schedule post-mortem within 48 hours - Update runbook if new patterns discovered",
              "score": null
            },
            {
              "doc_id": "runbook_out_of_memory_oom_kills",
              "chunk_id": "runbook_out_of_memory_oom_kills_fix_0",
              "content": "# Runbook: Out of Memory (OOM) Kills ## Severity **Level:** P3 - Medium **Response Time:** 15 minutes ## Symptoms - Pods restarting with OOMKilled status - Memory usage at limit - Application slowdown before restart ## Diagnosis ### Step 1 Check OOM events: `kubectl get events -n production --field-selector reason=OOMKilled` ### Step 2 Review memory usage: `kubectl top pods -n production --sort-by=memory` ### Step 3 Check container limits: `kubectl describe pod <pod-name> -n production | grep -A5 Limits` ### Step 4 Analyze heap dumps if available: `cloudflow debug heapdump --pod <pod-name>` ## Resolution ### Step 1 Increase memory limit: update deployment resources.limits.memory ### Step 2 Add memory request equal to limit to guarantee allocation ### Step 3 Enable JVM heap dump on OOM: add -XX:+HeapDumpOnOutOfMemoryError ### Step 4 Investigate memory leaks with profiling tools ### Step 5 Consider horizontal scaling instead of vertical ## Escalation If OOMs continue after limit increase, escalate to development team for memory leak investigation. ## Post-Incident - Document timeline in incident log - Schedule post-mortem within 48 hours - Update runbook if new patterns discovered",
              "score": null
            },
            {
              "doc_id": "runbook_kafka_consumer_lag",
              "chunk_id": "runbook_kafka_consumer_lag_fix_0",
              "content": "# Runbook: Kafka Consumer Lag ## Severity **Level:** P1 - Critical **Response Time:** 1 hour ## Symptoms - Consumer lag > 10000 messages - Processing delays reported - Alerts from lag monitor ## Diagnosis ### Step 1 Check consumer group lag: `kafka-consumer-groups.sh --bootstrap-server kafka:9092 --describe --group workflow-executor` ### Step 2 Verify consumer health: `kubectl get pods -l app=workflow-executor -n production` ### Step 3 Check for processing errors: `kubectl logs -l app=workflow-executor --tail=100 -n production | grep ERROR` ### Step 4 Monitor throughput: `cloudflow metrics get kafka.consumer.records_per_second` ## Resolution ### Step 1 Scale up consumers: `kubectl scale deployment/workflow-executor --replicas=10 -n production` ### Step 2 Reset offset if messages are stale: `kafka-consumer-groups.sh --bootstrap-server kafka:9092 --group workflow-executor --reset-offsets --to-latest --execute --topic workflows` ### Step 3 Pause producer if backpressure needed: `cloudflow workflows pause-triggers` ### Step 4 Increase partition count for parallelism (requires coordination) ## Escalation If lag persists > 30 minutes, page platform team. May need Kafka cluster scaling. ## Post-Incident - Document timeline in incident log - Schedule post-mortem within 48 hours - Update runbook if new patterns discovered",
              "score": null
            },
            {
              "doc_id": "runbook_api_gateway_5xx_errors",
              "chunk_id": "runbook_api_gateway_5xx_errors_fix_0",
              "content": "# Runbook: API Gateway 5xx Errors ## Severity **Level:** P3 - Medium **Response Time:** 1 hour ## Symptoms - Error rate > 1% - 5xx responses in access logs - Downstream service failures ## Diagnosis ### Step 1 Check Kong error logs: `kubectl logs -l app=kong -n kong --tail=100 | grep -E '5[0-9]{2}'` ### Step 2 Verify backend health: `kubectl get pods -n production -o wide` ### Step 3 Check service endpoints: `kubectl get endpoints -n production` ### Step 4 Test backend directly: `kubectl exec -it kong-xxx -n kong -- curl http://api.production.svc:8080/health` ## Resolution ### Step 1 Restart unhealthy backends: `kubectl rollout restart deployment/api -n production` ### Step 2 Enable circuit breaker if cascading: update Kong circuit-breaker plugin ### Step 3 Scale up backends: `kubectl scale deployment/api --replicas=5 -n production` ### Step 4 Check for resource exhaustion on backend pods ### Step 5 Verify network policies allow traffic ## Escalation If 5xx persists > 15 minutes, page on-call. May indicate infrastructure issue. ## Post-Incident - Document timeline in incident log - Schedule post-mortem within 48 hours - Update runbook if new patterns discovered",
              "score": null
            },
            {
              "doc_id": "runbook_database_connection_exhaustion",
              "chunk_id": "runbook_database_connection_exhaustion_fix_0",
              "content": "# Runbook: Database Connection Exhaustion ## Severity **Level:** P1 - Critical **Response Time:** 15 minutes ## Symptoms - Connection pool at 100% - Queries timing out - 'too many connections' errors in logs ## Diagnosis ### Step 1 Check active connections: `SELECT count(*) FROM pg_stat_activity` ### Step 2 Identify long-running queries: `SELECT pid, now() - pg_stat_activity.query_start AS duration, query FROM pg_stat_activity WHERE state != 'idle' ORDER BY duration DESC` ### Step 3 Check PgBouncer stats: `SHOW POOLS` ### Step 4 Verify connection limits: `SHOW max_connections` ## Resolution ### Step 1 Kill long-running queries: `SELECT pg_terminate_backend(pid) FROM pg_stat_activity WHERE duration > interval '5 minutes'` ### Step 2 Increase PgBouncer pool size temporarily: update pgbouncer.ini ### Step 3 Restart affected services to release connections: `kubectl rollout restart deployment/api` ### Step 4 Scale down non-critical services to free connections ## Escalation If connections remain exhausted, escalate to DBA. Consider enabling connection queueing. ## Post-Incident - Document timeline in incident log - Schedule post-mortem within 48 hours - Update runbook if new patterns discovered",
              "score": null
            }
          ],
          "latency_ms": 3.7210190002952004,
          "query_id": "troubleshoot_027",
          "dimension": "problem",
          "query": "kubectl top shows pods at 95% CPU and response times are spiking to 5+ seconds"
        },
        {
          "key_facts": [
            "kubectl top pods",
            "cloudflow debug profile",
            "rollout restart"
          ],
          "found_facts": [
            "kubectl top pods",
            "cloudflow debug profile",
            "rollout restart"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "runbook_high_cpu_usage"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "runbook_high_cpu_usage",
              "chunk_id": "runbook_high_cpu_usage_fix_0",
              "content": "# Runbook: High CPU Usage ## Severity **Level:** P1 - Critical **Response Time:** 15 minutes ## Symptoms - CPU utilization > 80% sustained - Increased response latency - Autoscaler at maximum replicas ## Diagnosis ### Step 1 Check current CPU usage: `kubectl top pods -n production` ### Step 2 Identify hot pods: `kubectl top pods --sort-by=cpu -n production | head -10` ### Step 3 Check for recent deployments: `kubectl rollout history deployment/api -n production` ### Step 4 Profile the application: `cloudflow debug profile --pod api-xyz --duration 60s` ## Resolution ### Step 1 Scale up if traffic-related: `kubectl scale deployment/api --replicas=10 -n production` ### Step 2 Restart pods if memory leak suspected: `kubectl rollout restart deployment/api -n production` ### Step 3 Roll back recent deployment if regression: `kubectl rollout undo deployment/api -n production` ### Step 4 Add resource limits if unbounded: update deployment with CPU limits ## Escalation If CPU remains high after scaling, escalate to platform team. Page on-call if affecting users. ## Post-Incident - Document timeline in incident log - Schedule post-mortem within 48 hours - Update runbook if new patterns discovered",
              "score": null
            },
            {
              "doc_id": "runbook_out_of_memory_oom_kills",
              "chunk_id": "runbook_out_of_memory_oom_kills_fix_0",
              "content": "# Runbook: Out of Memory (OOM) Kills ## Severity **Level:** P3 - Medium **Response Time:** 15 minutes ## Symptoms - Pods restarting with OOMKilled status - Memory usage at limit - Application slowdown before restart ## Diagnosis ### Step 1 Check OOM events: `kubectl get events -n production --field-selector reason=OOMKilled` ### Step 2 Review memory usage: `kubectl top pods -n production --sort-by=memory` ### Step 3 Check container limits: `kubectl describe pod <pod-name> -n production | grep -A5 Limits` ### Step 4 Analyze heap dumps if available: `cloudflow debug heapdump --pod <pod-name>` ## Resolution ### Step 1 Increase memory limit: update deployment resources.limits.memory ### Step 2 Add memory request equal to limit to guarantee allocation ### Step 3 Enable JVM heap dump on OOM: add -XX:+HeapDumpOnOutOfMemoryError ### Step 4 Investigate memory leaks with profiling tools ### Step 5 Consider horizontal scaling instead of vertical ## Escalation If OOMs continue after limit increase, escalate to development team for memory leak investigation. ## Post-Incident - Document timeline in incident log - Schedule post-mortem within 48 hours - Update runbook if new patterns discovered",
              "score": null
            },
            {
              "doc_id": "runbook_api_gateway_5xx_errors",
              "chunk_id": "runbook_api_gateway_5xx_errors_fix_0",
              "content": "# Runbook: API Gateway 5xx Errors ## Severity **Level:** P3 - Medium **Response Time:** 1 hour ## Symptoms - Error rate > 1% - 5xx responses in access logs - Downstream service failures ## Diagnosis ### Step 1 Check Kong error logs: `kubectl logs -l app=kong -n kong --tail=100 | grep -E '5[0-9]{2}'` ### Step 2 Verify backend health: `kubectl get pods -n production -o wide` ### Step 3 Check service endpoints: `kubectl get endpoints -n production` ### Step 4 Test backend directly: `kubectl exec -it kong-xxx -n kong -- curl http://api.production.svc:8080/health` ## Resolution ### Step 1 Restart unhealthy backends: `kubectl rollout restart deployment/api -n production` ### Step 2 Enable circuit breaker if cascading: update Kong circuit-breaker plugin ### Step 3 Scale up backends: `kubectl scale deployment/api --replicas=5 -n production` ### Step 4 Check for resource exhaustion on backend pods ### Step 5 Verify network policies allow traffic ## Escalation If 5xx persists > 15 minutes, page on-call. May indicate infrastructure issue. ## Post-Incident - Document timeline in incident log - Schedule post-mortem within 48 hours - Update runbook if new patterns discovered",
              "score": null
            },
            {
              "doc_id": "runbook_kafka_consumer_lag",
              "chunk_id": "runbook_kafka_consumer_lag_fix_0",
              "content": "# Runbook: Kafka Consumer Lag ## Severity **Level:** P1 - Critical **Response Time:** 1 hour ## Symptoms - Consumer lag > 10000 messages - Processing delays reported - Alerts from lag monitor ## Diagnosis ### Step 1 Check consumer group lag: `kafka-consumer-groups.sh --bootstrap-server kafka:9092 --describe --group workflow-executor` ### Step 2 Verify consumer health: `kubectl get pods -l app=workflow-executor -n production` ### Step 3 Check for processing errors: `kubectl logs -l app=workflow-executor --tail=100 -n production | grep ERROR` ### Step 4 Monitor throughput: `cloudflow metrics get kafka.consumer.records_per_second` ## Resolution ### Step 1 Scale up consumers: `kubectl scale deployment/workflow-executor --replicas=10 -n production` ### Step 2 Reset offset if messages are stale: `kafka-consumer-groups.sh --bootstrap-server kafka:9092 --group workflow-executor --reset-offsets --to-latest --execute --topic workflows` ### Step 3 Pause producer if backpressure needed: `cloudflow workflows pause-triggers` ### Step 4 Increase partition count for parallelism (requires coordination) ## Escalation If lag persists > 30 minutes, page platform team. May need Kafka cluster scaling. ## Post-Incident - Document timeline in incident log - Schedule post-mortem within 48 hours - Update runbook if new patterns discovered",
              "score": null
            },
            {
              "doc_id": "arch_workflow_engine",
              "chunk_id": "arch_workflow_engine_fix_0",
              "content": "# Workflow Engine Architecture ## Overview The Workflow Engine is the core component responsible for executing user-defined workflows. It uses an event-driven architecture with support for parallel execution, conditional branching, and automatic retries. ## Design Principles - **Idempotency**: All operations can be safely retried without side effects - **Eventual Consistency**: State converges to consistent view within 5 seconds - **Fault Tolerance**: System continues operating despite component failures ## Components ### Scheduler Manages workflow scheduling and triggers. Supports cron expressions, webhooks, and event-based triggers. **Technology:** Custom Go service **Scaling:** Horizontal with leader election ### Executor Runs individual workflow steps in isolated containers. Maximum execution time is 30 minutes per step. **Technology:** Kubernetes Jobs **Scaling:** Auto-scaling based on queue depth ### State Store Persists workflow state, execution history, and step results. Uses optimistic locking for concurrent updates. **Technology:** PostgreSQL with JSONB **Scaling:** Primary-replica with read replicas ## Data Flow Triggers \u2192 Scheduler \u2192 Job Queue \u2192 Executor \u2192 State Store \u2192 Webhooks/Notifications ## Performance Characteristics - **Latency P50:** 45ms - **Latency P99:** 250ms - **Throughput:** 5000 executions/minute",
              "score": null
            }
          ],
          "latency_ms": 3.5589679991971934,
          "query_id": "troubleshoot_027",
          "dimension": "casual",
          "query": "pods maxing out cpu, what now"
        },
        {
          "key_facts": [
            "kubectl top pods",
            "cloudflow debug profile",
            "rollout restart"
          ],
          "found_facts": [
            "kubectl top pods",
            "cloudflow debug profile",
            "rollout restart"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "runbook_high_cpu_usage"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "runbook_high_cpu_usage",
              "chunk_id": "runbook_high_cpu_usage_fix_0",
              "content": "# Runbook: High CPU Usage ## Severity **Level:** P1 - Critical **Response Time:** 15 minutes ## Symptoms - CPU utilization > 80% sustained - Increased response latency - Autoscaler at maximum replicas ## Diagnosis ### Step 1 Check current CPU usage: `kubectl top pods -n production` ### Step 2 Identify hot pods: `kubectl top pods --sort-by=cpu -n production | head -10` ### Step 3 Check for recent deployments: `kubectl rollout history deployment/api -n production` ### Step 4 Profile the application: `cloudflow debug profile --pod api-xyz --duration 60s` ## Resolution ### Step 1 Scale up if traffic-related: `kubectl scale deployment/api --replicas=10 -n production` ### Step 2 Restart pods if memory leak suspected: `kubectl rollout restart deployment/api -n production` ### Step 3 Roll back recent deployment if regression: `kubectl rollout undo deployment/api -n production` ### Step 4 Add resource limits if unbounded: update deployment with CPU limits ## Escalation If CPU remains high after scaling, escalate to platform team. Page on-call if affecting users. ## Post-Incident - Document timeline in incident log - Schedule post-mortem within 48 hours - Update runbook if new patterns discovered",
              "score": null
            },
            {
              "doc_id": "runbook_out_of_memory_oom_kills",
              "chunk_id": "runbook_out_of_memory_oom_kills_fix_0",
              "content": "# Runbook: Out of Memory (OOM) Kills ## Severity **Level:** P3 - Medium **Response Time:** 15 minutes ## Symptoms - Pods restarting with OOMKilled status - Memory usage at limit - Application slowdown before restart ## Diagnosis ### Step 1 Check OOM events: `kubectl get events -n production --field-selector reason=OOMKilled` ### Step 2 Review memory usage: `kubectl top pods -n production --sort-by=memory` ### Step 3 Check container limits: `kubectl describe pod <pod-name> -n production | grep -A5 Limits` ### Step 4 Analyze heap dumps if available: `cloudflow debug heapdump --pod <pod-name>` ## Resolution ### Step 1 Increase memory limit: update deployment resources.limits.memory ### Step 2 Add memory request equal to limit to guarantee allocation ### Step 3 Enable JVM heap dump on OOM: add -XX:+HeapDumpOnOutOfMemoryError ### Step 4 Investigate memory leaks with profiling tools ### Step 5 Consider horizontal scaling instead of vertical ## Escalation If OOMs continue after limit increase, escalate to development team for memory leak investigation. ## Post-Incident - Document timeline in incident log - Schedule post-mortem within 48 hours - Update runbook if new patterns discovered",
              "score": null
            },
            {
              "doc_id": "runbook_kafka_consumer_lag",
              "chunk_id": "runbook_kafka_consumer_lag_fix_0",
              "content": "# Runbook: Kafka Consumer Lag ## Severity **Level:** P1 - Critical **Response Time:** 1 hour ## Symptoms - Consumer lag > 10000 messages - Processing delays reported - Alerts from lag monitor ## Diagnosis ### Step 1 Check consumer group lag: `kafka-consumer-groups.sh --bootstrap-server kafka:9092 --describe --group workflow-executor` ### Step 2 Verify consumer health: `kubectl get pods -l app=workflow-executor -n production` ### Step 3 Check for processing errors: `kubectl logs -l app=workflow-executor --tail=100 -n production | grep ERROR` ### Step 4 Monitor throughput: `cloudflow metrics get kafka.consumer.records_per_second` ## Resolution ### Step 1 Scale up consumers: `kubectl scale deployment/workflow-executor --replicas=10 -n production` ### Step 2 Reset offset if messages are stale: `kafka-consumer-groups.sh --bootstrap-server kafka:9092 --group workflow-executor --reset-offsets --to-latest --execute --topic workflows` ### Step 3 Pause producer if backpressure needed: `cloudflow workflows pause-triggers` ### Step 4 Increase partition count for parallelism (requires coordination) ## Escalation If lag persists > 30 minutes, page platform team. May need Kafka cluster scaling. ## Post-Incident - Document timeline in incident log - Schedule post-mortem within 48 hours - Update runbook if new patterns discovered",
              "score": null
            },
            {
              "doc_id": "runbook_database_connection_exhaustion",
              "chunk_id": "runbook_database_connection_exhaustion_fix_0",
              "content": "# Runbook: Database Connection Exhaustion ## Severity **Level:** P1 - Critical **Response Time:** 15 minutes ## Symptoms - Connection pool at 100% - Queries timing out - 'too many connections' errors in logs ## Diagnosis ### Step 1 Check active connections: `SELECT count(*) FROM pg_stat_activity` ### Step 2 Identify long-running queries: `SELECT pid, now() - pg_stat_activity.query_start AS duration, query FROM pg_stat_activity WHERE state != 'idle' ORDER BY duration DESC` ### Step 3 Check PgBouncer stats: `SHOW POOLS` ### Step 4 Verify connection limits: `SHOW max_connections` ## Resolution ### Step 1 Kill long-running queries: `SELECT pg_terminate_backend(pid) FROM pg_stat_activity WHERE duration > interval '5 minutes'` ### Step 2 Increase PgBouncer pool size temporarily: update pgbouncer.ini ### Step 3 Restart affected services to release connections: `kubectl rollout restart deployment/api` ### Step 4 Scale down non-critical services to free connections ## Escalation If connections remain exhausted, escalate to DBA. Consider enabling connection queueing. ## Post-Incident - Document timeline in incident log - Schedule post-mortem within 48 hours - Update runbook if new patterns discovered",
              "score": null
            },
            {
              "doc_id": "arch_workflow_engine",
              "chunk_id": "arch_workflow_engine_fix_0",
              "content": "# Workflow Engine Architecture ## Overview The Workflow Engine is the core component responsible for executing user-defined workflows. It uses an event-driven architecture with support for parallel execution, conditional branching, and automatic retries. ## Design Principles - **Idempotency**: All operations can be safely retried without side effects - **Eventual Consistency**: State converges to consistent view within 5 seconds - **Fault Tolerance**: System continues operating despite component failures ## Components ### Scheduler Manages workflow scheduling and triggers. Supports cron expressions, webhooks, and event-based triggers. **Technology:** Custom Go service **Scaling:** Horizontal with leader election ### Executor Runs individual workflow steps in isolated containers. Maximum execution time is 30 minutes per step. **Technology:** Kubernetes Jobs **Scaling:** Auto-scaling based on queue depth ### State Store Persists workflow state, execution history, and step results. Uses optimistic locking for concurrent updates. **Technology:** PostgreSQL with JSONB **Scaling:** Primary-replica with read replicas ## Data Flow Triggers \u2192 Scheduler \u2192 Job Queue \u2192 Executor \u2192 State Store \u2192 Webhooks/Notifications ## Performance Characteristics - **Latency P50:** 45ms - **Latency P99:** 250ms - **Throughput:** 5000 executions/minute",
              "score": null
            }
          ],
          "latency_ms": 3.6880780007777503,
          "query_id": "troubleshoot_027",
          "dimension": "contextual",
          "query": "getting paged about high CPU in production, autoscaler is already at max replicas, need to find the root cause"
        },
        {
          "key_facts": [
            "kubectl top pods",
            "cloudflow debug profile",
            "rollout restart"
          ],
          "found_facts": [
            "kubectl top pods",
            "cloudflow debug profile",
            "rollout restart"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "runbook_high_cpu_usage"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "runbook_high_cpu_usage",
              "chunk_id": "runbook_high_cpu_usage_fix_0",
              "content": "# Runbook: High CPU Usage ## Severity **Level:** P1 - Critical **Response Time:** 15 minutes ## Symptoms - CPU utilization > 80% sustained - Increased response latency - Autoscaler at maximum replicas ## Diagnosis ### Step 1 Check current CPU usage: `kubectl top pods -n production` ### Step 2 Identify hot pods: `kubectl top pods --sort-by=cpu -n production | head -10` ### Step 3 Check for recent deployments: `kubectl rollout history deployment/api -n production` ### Step 4 Profile the application: `cloudflow debug profile --pod api-xyz --duration 60s` ## Resolution ### Step 1 Scale up if traffic-related: `kubectl scale deployment/api --replicas=10 -n production` ### Step 2 Restart pods if memory leak suspected: `kubectl rollout restart deployment/api -n production` ### Step 3 Roll back recent deployment if regression: `kubectl rollout undo deployment/api -n production` ### Step 4 Add resource limits if unbounded: update deployment with CPU limits ## Escalation If CPU remains high after scaling, escalate to platform team. Page on-call if affecting users. ## Post-Incident - Document timeline in incident log - Schedule post-mortem within 48 hours - Update runbook if new patterns discovered",
              "score": null
            },
            {
              "doc_id": "runbook_kafka_consumer_lag",
              "chunk_id": "runbook_kafka_consumer_lag_fix_0",
              "content": "# Runbook: Kafka Consumer Lag ## Severity **Level:** P1 - Critical **Response Time:** 1 hour ## Symptoms - Consumer lag > 10000 messages - Processing delays reported - Alerts from lag monitor ## Diagnosis ### Step 1 Check consumer group lag: `kafka-consumer-groups.sh --bootstrap-server kafka:9092 --describe --group workflow-executor` ### Step 2 Verify consumer health: `kubectl get pods -l app=workflow-executor -n production` ### Step 3 Check for processing errors: `kubectl logs -l app=workflow-executor --tail=100 -n production | grep ERROR` ### Step 4 Monitor throughput: `cloudflow metrics get kafka.consumer.records_per_second` ## Resolution ### Step 1 Scale up consumers: `kubectl scale deployment/workflow-executor --replicas=10 -n production` ### Step 2 Reset offset if messages are stale: `kafka-consumer-groups.sh --bootstrap-server kafka:9092 --group workflow-executor --reset-offsets --to-latest --execute --topic workflows` ### Step 3 Pause producer if backpressure needed: `cloudflow workflows pause-triggers` ### Step 4 Increase partition count for parallelism (requires coordination) ## Escalation If lag persists > 30 minutes, page platform team. May need Kafka cluster scaling. ## Post-Incident - Document timeline in incident log - Schedule post-mortem within 48 hours - Update runbook if new patterns discovered",
              "score": null
            },
            {
              "doc_id": "runbook_out_of_memory_oom_kills",
              "chunk_id": "runbook_out_of_memory_oom_kills_fix_0",
              "content": "# Runbook: Out of Memory (OOM) Kills ## Severity **Level:** P3 - Medium **Response Time:** 15 minutes ## Symptoms - Pods restarting with OOMKilled status - Memory usage at limit - Application slowdown before restart ## Diagnosis ### Step 1 Check OOM events: `kubectl get events -n production --field-selector reason=OOMKilled` ### Step 2 Review memory usage: `kubectl top pods -n production --sort-by=memory` ### Step 3 Check container limits: `kubectl describe pod <pod-name> -n production | grep -A5 Limits` ### Step 4 Analyze heap dumps if available: `cloudflow debug heapdump --pod <pod-name>` ## Resolution ### Step 1 Increase memory limit: update deployment resources.limits.memory ### Step 2 Add memory request equal to limit to guarantee allocation ### Step 3 Enable JVM heap dump on OOM: add -XX:+HeapDumpOnOutOfMemoryError ### Step 4 Investigate memory leaks with profiling tools ### Step 5 Consider horizontal scaling instead of vertical ## Escalation If OOMs continue after limit increase, escalate to development team for memory leak investigation. ## Post-Incident - Document timeline in incident log - Schedule post-mortem within 48 hours - Update runbook if new patterns discovered",
              "score": null
            },
            {
              "doc_id": "adr_002",
              "chunk_id": "adr_002_fix_0",
              "content": "# ADR-002: Adopt Kubernetes for Container Orchestration **Status:** Accepted **Date:** 2024-01-24 ## Context We need a platform to deploy and scale our microservices. The platform must support auto-scaling, rolling deployments, and multi-region deployment. ## Options Considered ### Option 1: Kubernetes (EKS) Industry-standard container orchestration. **Pros:** - Industry standard - Rich ecosystem - Cloud agnostic - Auto-scaling **Cons:** - Operational complexity - Learning curve - Resource overhead ### Option 2: AWS ECS Amazon's native container service. **Pros:** - Simple operation - Native AWS integration - Lower overhead **Cons:** - AWS lock-in - Less flexible - Smaller community ### Option 3: Serverless (Lambda) Function-as-a-service approach. **Pros:** - Zero infrastructure - Pay per use - Auto-scaling **Cons:** - Cold starts - 15-minute limit - Vendor lock-in ## Decision We will use Kubernetes on EKS for container orchestration with Helm for deployments. ## Consequences - Need platform team expertise - Implement GitOps with ArgoCD - Use managed node groups for easier operations",
              "score": null
            },
            {
              "doc_id": "runbook_api_gateway_5xx_errors",
              "chunk_id": "runbook_api_gateway_5xx_errors_fix_0",
              "content": "# Runbook: API Gateway 5xx Errors ## Severity **Level:** P3 - Medium **Response Time:** 1 hour ## Symptoms - Error rate > 1% - 5xx responses in access logs - Downstream service failures ## Diagnosis ### Step 1 Check Kong error logs: `kubectl logs -l app=kong -n kong --tail=100 | grep -E '5[0-9]{2}'` ### Step 2 Verify backend health: `kubectl get pods -n production -o wide` ### Step 3 Check service endpoints: `kubectl get endpoints -n production` ### Step 4 Test backend directly: `kubectl exec -it kong-xxx -n kong -- curl http://api.production.svc:8080/health` ## Resolution ### Step 1 Restart unhealthy backends: `kubectl rollout restart deployment/api -n production` ### Step 2 Enable circuit breaker if cascading: update Kong circuit-breaker plugin ### Step 3 Scale up backends: `kubectl scale deployment/api --replicas=5 -n production` ### Step 4 Check for resource exhaustion on backend pods ### Step 5 Verify network policies allow traffic ## Escalation If 5xx persists > 15 minutes, page on-call. May indicate infrastructure issue. ## Post-Incident - Document timeline in incident log - Schedule post-mortem within 48 hours - Update runbook if new patterns discovered",
              "score": null
            }
          ],
          "latency_ms": 4.232522000165773,
          "query_id": "troubleshoot_027",
          "dimension": "negation",
          "query": "why is CPU stuck at 80% even after I scaled up the deployment?"
        },
        {
          "key_facts": [
            "pg_stat_activity",
            "SHOW POOLS",
            "pg_terminate_backend"
          ],
          "found_facts": [
            "pg_stat_activity",
            "SHOW POOLS",
            "pg_terminate_backend"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "runbook_database_connection_ex"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "runbook_database_connection_exhaustion",
              "chunk_id": "runbook_database_connection_exhaustion_fix_0",
              "content": "# Runbook: Database Connection Exhaustion ## Severity **Level:** P1 - Critical **Response Time:** 15 minutes ## Symptoms - Connection pool at 100% - Queries timing out - 'too many connections' errors in logs ## Diagnosis ### Step 1 Check active connections: `SELECT count(*) FROM pg_stat_activity` ### Step 2 Identify long-running queries: `SELECT pid, now() - pg_stat_activity.query_start AS duration, query FROM pg_stat_activity WHERE state != 'idle' ORDER BY duration DESC` ### Step 3 Check PgBouncer stats: `SHOW POOLS` ### Step 4 Verify connection limits: `SHOW max_connections` ## Resolution ### Step 1 Kill long-running queries: `SELECT pg_terminate_backend(pid) FROM pg_stat_activity WHERE duration > interval '5 minutes'` ### Step 2 Increase PgBouncer pool size temporarily: update pgbouncer.ini ### Step 3 Restart affected services to release connections: `kubectl rollout restart deployment/api` ### Step 4 Scale down non-critical services to free connections ## Escalation If connections remain exhausted, escalate to DBA. Consider enabling connection queueing. ## Post-Incident - Document timeline in incident log - Schedule post-mortem within 48 hours - Update runbook if new patterns discovered",
              "score": null
            },
            {
              "doc_id": "config_database",
              "chunk_id": "config_database_fix_0",
              "content": "# Database Configuration Reference ## Overview This document describes all configuration options for Database. ## Environment Variables | Variable | Type | Default | Description | |----------|------|---------|-------------| | `DATABASE_URL` | string | required | PostgreSQL connection string | | `DATABASE_POOL_MIN` | integer | 5 | Minimum connection pool size | | `DATABASE_POOL_MAX` | integer | 20 | Maximum connection pool size | | `DATABASE_STATEMENT_TIMEOUT` | integer | 30000 | Query timeout in milliseconds | | `DATABASE_SSL_MODE` | string | require | SSL mode: disable, require, verify-full | | `DATABASE_MIGRATION_AUTO` | boolean | false | Run migrations on startup | ## Configuration File Configuration can also be provided via `database.yaml`: ```yaml database_url: required database_pool_min: 5 database_pool_max: 20 database_statement_timeout: 30000 database_ssl_mode: require ``` ## Validation Configuration is validated at startup. Invalid configuration will prevent the service from starting.",
              "score": null
            },
            {
              "doc_id": "howto_set_up_database_connection",
              "chunk_id": "howto_set_up_database_connection_fix_0",
              "content": "# How to Set Up Database Connection ## Prerequisites - Database with network access to CloudFlow - Connection credentials ## Steps ### Step 1: Add connection secret Store your database connection string as a secret. ```bash cloudflow secrets set DATABASE_URL postgresql://user:pass@host:5432/db ``` ### Step 2: Configure connection pool Set pool size based on your plan limits. Free tier allows 5 connections, Pro allows 20. > **Note:** Pool exhaustion will queue requests ### Step 3: Test connection Verify the database is accessible from CloudFlow. ```bash cloudflow db test --secret DATABASE_URL ``` ### Step 4: Enable SSL For production, always require SSL connections. ```bash cloudflow db configure --require-ssl ``` ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            },
            {
              "doc_id": "howto_implement_retry_logic",
              "chunk_id": "howto_implement_retry_logic_fix_0",
              "content": "# How to Implement Retry Logic ## Prerequisites - Existing workflow ## Steps ### Step 1: Define retry policy Add retry configuration to your workflow step. > **Note:** Maximum 5 retry attempts allowed ### Step 2: Configure backoff Set exponential backoff with base delay of 1 second. The formula is: delay = base * 2^attempt. ### Step 3: Add timeout Set maximum execution time to prevent infinite loops. Default timeout is 30 seconds. ### Step 4: Handle final failure Configure on_failure handler to notify or escalate when all retries are exhausted. ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            },
            {
              "doc_id": "runbook_out_of_memory_oom_kills",
              "chunk_id": "runbook_out_of_memory_oom_kills_fix_0",
              "content": "# Runbook: Out of Memory (OOM) Kills ## Severity **Level:** P3 - Medium **Response Time:** 15 minutes ## Symptoms - Pods restarting with OOMKilled status - Memory usage at limit - Application slowdown before restart ## Diagnosis ### Step 1 Check OOM events: `kubectl get events -n production --field-selector reason=OOMKilled` ### Step 2 Review memory usage: `kubectl top pods -n production --sort-by=memory` ### Step 3 Check container limits: `kubectl describe pod <pod-name> -n production | grep -A5 Limits` ### Step 4 Analyze heap dumps if available: `cloudflow debug heapdump --pod <pod-name>` ## Resolution ### Step 1 Increase memory limit: update deployment resources.limits.memory ### Step 2 Add memory request equal to limit to guarantee allocation ### Step 3 Enable JVM heap dump on OOM: add -XX:+HeapDumpOnOutOfMemoryError ### Step 4 Investigate memory leaks with profiling tools ### Step 5 Consider horizontal scaling instead of vertical ## Escalation If OOMs continue after limit increase, escalate to development team for memory leak investigation. ## Post-Incident - Document timeline in incident log - Schedule post-mortem within 48 hours - Update runbook if new patterns discovered",
              "score": null
            }
          ],
          "latency_ms": 3.5392610006965697,
          "query_id": "troubleshoot_028",
          "dimension": "original",
          "query": "What to do when database connections are exhausted?"
        },
        {
          "key_facts": [
            "pg_stat_activity",
            "SHOW POOLS",
            "pg_terminate_backend"
          ],
          "found_facts": [
            "pg_stat_activity",
            "SHOW POOLS",
            "pg_terminate_backend"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "runbook_database_connection_ex"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "runbook_database_connection_exhaustion",
              "chunk_id": "runbook_database_connection_exhaustion_fix_0",
              "content": "# Runbook: Database Connection Exhaustion ## Severity **Level:** P1 - Critical **Response Time:** 15 minutes ## Symptoms - Connection pool at 100% - Queries timing out - 'too many connections' errors in logs ## Diagnosis ### Step 1 Check active connections: `SELECT count(*) FROM pg_stat_activity` ### Step 2 Identify long-running queries: `SELECT pid, now() - pg_stat_activity.query_start AS duration, query FROM pg_stat_activity WHERE state != 'idle' ORDER BY duration DESC` ### Step 3 Check PgBouncer stats: `SHOW POOLS` ### Step 4 Verify connection limits: `SHOW max_connections` ## Resolution ### Step 1 Kill long-running queries: `SELECT pg_terminate_backend(pid) FROM pg_stat_activity WHERE duration > interval '5 minutes'` ### Step 2 Increase PgBouncer pool size temporarily: update pgbouncer.ini ### Step 3 Restart affected services to release connections: `kubectl rollout restart deployment/api` ### Step 4 Scale down non-critical services to free connections ## Escalation If connections remain exhausted, escalate to DBA. Consider enabling connection queueing. ## Post-Incident - Document timeline in incident log - Schedule post-mortem within 48 hours - Update runbook if new patterns discovered",
              "score": null
            },
            {
              "doc_id": "config_database",
              "chunk_id": "config_database_fix_0",
              "content": "# Database Configuration Reference ## Overview This document describes all configuration options for Database. ## Environment Variables | Variable | Type | Default | Description | |----------|------|---------|-------------| | `DATABASE_URL` | string | required | PostgreSQL connection string | | `DATABASE_POOL_MIN` | integer | 5 | Minimum connection pool size | | `DATABASE_POOL_MAX` | integer | 20 | Maximum connection pool size | | `DATABASE_STATEMENT_TIMEOUT` | integer | 30000 | Query timeout in milliseconds | | `DATABASE_SSL_MODE` | string | require | SSL mode: disable, require, verify-full | | `DATABASE_MIGRATION_AUTO` | boolean | false | Run migrations on startup | ## Configuration File Configuration can also be provided via `database.yaml`: ```yaml database_url: required database_pool_min: 5 database_pool_max: 20 database_statement_timeout: 30000 database_ssl_mode: require ``` ## Validation Configuration is validated at startup. Invalid configuration will prevent the service from starting.",
              "score": null
            },
            {
              "doc_id": "howto_set_up_database_connection",
              "chunk_id": "howto_set_up_database_connection_fix_0",
              "content": "# How to Set Up Database Connection ## Prerequisites - Database with network access to CloudFlow - Connection credentials ## Steps ### Step 1: Add connection secret Store your database connection string as a secret. ```bash cloudflow secrets set DATABASE_URL postgresql://user:pass@host:5432/db ``` ### Step 2: Configure connection pool Set pool size based on your plan limits. Free tier allows 5 connections, Pro allows 20. > **Note:** Pool exhaustion will queue requests ### Step 3: Test connection Verify the database is accessible from CloudFlow. ```bash cloudflow db test --secret DATABASE_URL ``` ### Step 4: Enable SSL For production, always require SSL connections. ```bash cloudflow db configure --require-ssl ``` ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            },
            {
              "doc_id": "arch_data_pipeline",
              "chunk_id": "arch_data_pipeline_fix_0",
              "content": "# Data Pipeline Architecture ## Overview The Data Pipeline processes and transforms data between workflow steps. It supports batch processing up to 10GB and streaming for real-time use cases. ## Design Principles - **Exactly-Once Processing**: Each record is processed exactly once - **Backpressure Handling**: System gracefully handles varying load ## Components ### Stream Processor Handles real-time data streams. Maximum throughput of 100000 messages per second. **Technology:** Apache Kafka with Flink **Scaling:** Partition-based scaling ### Batch Processor Processes large batch jobs. Supports data formats: JSON, CSV, Parquet, Avro. **Technology:** Apache Spark on Kubernetes **Scaling:** Dynamic resource allocation ### Data Lake Long-term data storage with time-travel queries. Retention configurable up to 7 years. **Technology:** S3 with Iceberg tables **Scaling:** Unlimited with tiered storage ## Data Flow Input \u2192 Validation \u2192 Transform \u2192 Enrich \u2192 Output ## Performance Characteristics - **Latency P50:** 100ms (streaming), 5min (batch) - **Latency P99:** 500ms (streaming), 30min (batch) - **Throughput:** 100000 messages/second (streaming)",
              "score": null
            },
            {
              "doc_id": "arch_caching_layer",
              "chunk_id": "arch_caching_layer_fix_0",
              "content": "# Caching Layer Architecture ## Overview The Caching Layer reduces latency and database load by caching frequently accessed data. Uses a multi-tier strategy with local and distributed caches. ## Design Principles - **Cache Invalidation**: Explicit invalidation on writes - **Graceful Degradation**: System works without cache ## Components ### Local Cache First-level cache with 100ms TTL. Reduces Redis round-trips for hot data. **Technology:** Ristretto in-memory cache **Scaling:** Per-pod memory allocation ### Distributed Cache Second-level cache with configurable TTL. Maximum key size 512MB. **Technology:** Redis Cluster 7.x **Scaling:** 6-node cluster with sharding ### CDN Caches static assets and API responses. Cache hit ratio target 95%. **Technology:** CloudFront with Lambda@Edge **Scaling:** Global edge locations ## Data Flow Request \u2192 Local Cache \u2192 Redis \u2192 Database \u2192 Cache Population ## Performance Characteristics - **Latency P50:** 1ms (hit), 50ms (miss) - **Latency P99:** 5ms (hit), 200ms (miss) - **Throughput:** 100000 reads/second",
              "score": null
            }
          ],
          "latency_ms": 4.458192999663879,
          "query_id": "troubleshoot_028",
          "dimension": "synonym",
          "query": "How do I handle DB connection pool depletion?"
        },
        {
          "key_facts": [
            "pg_stat_activity",
            "SHOW POOLS",
            "pg_terminate_backend"
          ],
          "found_facts": [
            "pg_stat_activity",
            "SHOW POOLS",
            "pg_terminate_backend"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "runbook_database_connection_ex"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "runbook_database_connection_exhaustion",
              "chunk_id": "runbook_database_connection_exhaustion_fix_0",
              "content": "# Runbook: Database Connection Exhaustion ## Severity **Level:** P1 - Critical **Response Time:** 15 minutes ## Symptoms - Connection pool at 100% - Queries timing out - 'too many connections' errors in logs ## Diagnosis ### Step 1 Check active connections: `SELECT count(*) FROM pg_stat_activity` ### Step 2 Identify long-running queries: `SELECT pid, now() - pg_stat_activity.query_start AS duration, query FROM pg_stat_activity WHERE state != 'idle' ORDER BY duration DESC` ### Step 3 Check PgBouncer stats: `SHOW POOLS` ### Step 4 Verify connection limits: `SHOW max_connections` ## Resolution ### Step 1 Kill long-running queries: `SELECT pg_terminate_backend(pid) FROM pg_stat_activity WHERE duration > interval '5 minutes'` ### Step 2 Increase PgBouncer pool size temporarily: update pgbouncer.ini ### Step 3 Restart affected services to release connections: `kubectl rollout restart deployment/api` ### Step 4 Scale down non-critical services to free connections ## Escalation If connections remain exhausted, escalate to DBA. Consider enabling connection queueing. ## Post-Incident - Document timeline in incident log - Schedule post-mortem within 48 hours - Update runbook if new patterns discovered",
              "score": null
            },
            {
              "doc_id": "arch_monitoring_stack",
              "chunk_id": "arch_monitoring_stack_fix_0",
              "content": "# Monitoring Stack Architecture ## Overview The Monitoring Stack provides observability into system health, performance, and errors. Includes metrics, logs, traces, and alerting. ## Design Principles - **Observability**: Every component emits metrics and traces - **Actionable Alerts**: Alerts include runbook links ## Components ### Metrics Collects time-series metrics. Retention is 15 days local, 1 year in object storage. **Technology:** Prometheus with Thanos **Scaling:** Federated with long-term storage ### Logging Centralized log aggregation. Supports structured JSON logs with label indexing. **Technology:** Loki with Grafana **Scaling:** Distributed with sharding ### Tracing Distributed request tracing. Trace retention is 7 days. **Technology:** Jaeger with OpenTelemetry **Scaling:** Sampling at 10% ### Alerting Routes alerts to on-call engineers. Supports escalation policies and silencing. **Technology:** Alertmanager with PagerDuty **Scaling:** HA pair ## Data Flow Application \u2192 Collector \u2192 Storage \u2192 Query \u2192 Dashboard/Alert ## Performance Characteristics - **Latency P50:** N/A - **Latency P99:** N/A - **Throughput:** 1M metrics/second, 100GB logs/day",
              "score": null
            },
            {
              "doc_id": "adr_007",
              "chunk_id": "adr_007_fix_0",
              "content": "# ADR-007: Implement Rate Limiting at API Gateway **Status:** Accepted **Date:** 2024-09-03 ## Context We need to protect our services from abuse and ensure fair resource usage across customers. ## Options Considered ### Option 1: Sliding window at gateway Rate limit at API gateway level. **Pros:** - Centralized enforcement - Low latency - Easy configuration **Cons:** - Gateway becomes bottleneck - Coarse granularity - Redis dependency ### Option 2: Per-service limiting Each service implements its own limits. **Pros:** - Fine-grained control - Service autonomy - No single point of failure **Cons:** - Inconsistent implementation - Higher complexity - Distributed state ### Option 3: Token bucket Token-based rate limiting. **Pros:** - Allows bursts - Smooth rate limiting - Well understood **Cons:** - More complex - Harder to explain to users - State management ## Decision We will implement sliding window rate limiting at the Kong API Gateway using Redis. ## Consequences - Default limit: 100 requests per minute - Implement per-plan limits - Return rate limit headers - Add retry-after on 429",
              "score": null
            },
            {
              "doc_id": "release_3_1_0",
              "chunk_id": "release_3_1_0_fix_0",
              "content": "# Release Notes - v3.1.0 **Release Date:** 2023-12-01 ## New Features - GraphQL API beta release - Workflow templates marketplace - Enhanced audit logging - SSO support for Okta ## Bug Fixes - Fixed webhook delivery retry logic - Resolved issue with large payload handling - Fixed dashboard loading performance ## Upgrade Instructions ```bash cloudflow upgrade --version 3.1.0 ``` ## Known Issues None at this time.",
              "score": null
            },
            {
              "doc_id": "runbook_high_cpu_usage",
              "chunk_id": "runbook_high_cpu_usage_fix_0",
              "content": "# Runbook: High CPU Usage ## Severity **Level:** P1 - Critical **Response Time:** 15 minutes ## Symptoms - CPU utilization > 80% sustained - Increased response latency - Autoscaler at maximum replicas ## Diagnosis ### Step 1 Check current CPU usage: `kubectl top pods -n production` ### Step 2 Identify hot pods: `kubectl top pods --sort-by=cpu -n production | head -10` ### Step 3 Check for recent deployments: `kubectl rollout history deployment/api -n production` ### Step 4 Profile the application: `cloudflow debug profile --pod api-xyz --duration 60s` ## Resolution ### Step 1 Scale up if traffic-related: `kubectl scale deployment/api --replicas=10 -n production` ### Step 2 Restart pods if memory leak suspected: `kubectl rollout restart deployment/api -n production` ### Step 3 Roll back recent deployment if regression: `kubectl rollout undo deployment/api -n production` ### Step 4 Add resource limits if unbounded: update deployment with CPU limits ## Escalation If CPU remains high after scaling, escalate to platform team. Page on-call if affecting users. ## Post-Incident - Document timeline in incident log - Schedule post-mortem within 48 hours - Update runbook if new patterns discovered",
              "score": null
            }
          ],
          "latency_ms": 3.9989270007936284,
          "query_id": "troubleshoot_028",
          "dimension": "problem",
          "query": "seeing 'too many connections' errors in logs and queries are timing out after 30 seconds"
        },
        {
          "key_facts": [
            "pg_stat_activity",
            "SHOW POOLS",
            "pg_terminate_backend"
          ],
          "found_facts": [
            "pg_stat_activity",
            "SHOW POOLS",
            "pg_terminate_backend"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "runbook_database_connection_ex"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "runbook_database_connection_exhaustion",
              "chunk_id": "runbook_database_connection_exhaustion_fix_0",
              "content": "# Runbook: Database Connection Exhaustion ## Severity **Level:** P1 - Critical **Response Time:** 15 minutes ## Symptoms - Connection pool at 100% - Queries timing out - 'too many connections' errors in logs ## Diagnosis ### Step 1 Check active connections: `SELECT count(*) FROM pg_stat_activity` ### Step 2 Identify long-running queries: `SELECT pid, now() - pg_stat_activity.query_start AS duration, query FROM pg_stat_activity WHERE state != 'idle' ORDER BY duration DESC` ### Step 3 Check PgBouncer stats: `SHOW POOLS` ### Step 4 Verify connection limits: `SHOW max_connections` ## Resolution ### Step 1 Kill long-running queries: `SELECT pg_terminate_backend(pid) FROM pg_stat_activity WHERE duration > interval '5 minutes'` ### Step 2 Increase PgBouncer pool size temporarily: update pgbouncer.ini ### Step 3 Restart affected services to release connections: `kubectl rollout restart deployment/api` ### Step 4 Scale down non-critical services to free connections ## Escalation If connections remain exhausted, escalate to DBA. Consider enabling connection queueing. ## Post-Incident - Document timeline in incident log - Schedule post-mortem within 48 hours - Update runbook if new patterns discovered",
              "score": null
            },
            {
              "doc_id": "config_database",
              "chunk_id": "config_database_fix_0",
              "content": "# Database Configuration Reference ## Overview This document describes all configuration options for Database. ## Environment Variables | Variable | Type | Default | Description | |----------|------|---------|-------------| | `DATABASE_URL` | string | required | PostgreSQL connection string | | `DATABASE_POOL_MIN` | integer | 5 | Minimum connection pool size | | `DATABASE_POOL_MAX` | integer | 20 | Maximum connection pool size | | `DATABASE_STATEMENT_TIMEOUT` | integer | 30000 | Query timeout in milliseconds | | `DATABASE_SSL_MODE` | string | require | SSL mode: disable, require, verify-full | | `DATABASE_MIGRATION_AUTO` | boolean | false | Run migrations on startup | ## Configuration File Configuration can also be provided via `database.yaml`: ```yaml database_url: required database_pool_min: 5 database_pool_max: 20 database_statement_timeout: 30000 database_ssl_mode: require ``` ## Validation Configuration is validated at startup. Invalid configuration will prevent the service from starting.",
              "score": null
            },
            {
              "doc_id": "howto_set_up_database_connection",
              "chunk_id": "howto_set_up_database_connection_fix_0",
              "content": "# How to Set Up Database Connection ## Prerequisites - Database with network access to CloudFlow - Connection credentials ## Steps ### Step 1: Add connection secret Store your database connection string as a secret. ```bash cloudflow secrets set DATABASE_URL postgresql://user:pass@host:5432/db ``` ### Step 2: Configure connection pool Set pool size based on your plan limits. Free tier allows 5 connections, Pro allows 20. > **Note:** Pool exhaustion will queue requests ### Step 3: Test connection Verify the database is accessible from CloudFlow. ```bash cloudflow db test --secret DATABASE_URL ``` ### Step 4: Enable SSL For production, always require SSL connections. ```bash cloudflow db configure --require-ssl ``` ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            },
            {
              "doc_id": "runbook_out_of_memory_oom_kills",
              "chunk_id": "runbook_out_of_memory_oom_kills_fix_0",
              "content": "# Runbook: Out of Memory (OOM) Kills ## Severity **Level:** P3 - Medium **Response Time:** 15 minutes ## Symptoms - Pods restarting with OOMKilled status - Memory usage at limit - Application slowdown before restart ## Diagnosis ### Step 1 Check OOM events: `kubectl get events -n production --field-selector reason=OOMKilled` ### Step 2 Review memory usage: `kubectl top pods -n production --sort-by=memory` ### Step 3 Check container limits: `kubectl describe pod <pod-name> -n production | grep -A5 Limits` ### Step 4 Analyze heap dumps if available: `cloudflow debug heapdump --pod <pod-name>` ## Resolution ### Step 1 Increase memory limit: update deployment resources.limits.memory ### Step 2 Add memory request equal to limit to guarantee allocation ### Step 3 Enable JVM heap dump on OOM: add -XX:+HeapDumpOnOutOfMemoryError ### Step 4 Investigate memory leaks with profiling tools ### Step 5 Consider horizontal scaling instead of vertical ## Escalation If OOMs continue after limit increase, escalate to development team for memory leak investigation. ## Post-Incident - Document timeline in incident log - Schedule post-mortem within 48 hours - Update runbook if new patterns discovered",
              "score": null
            },
            {
              "doc_id": "config_workflow_engine",
              "chunk_id": "config_workflow_engine_fix_0",
              "content": "# Workflow Engine Configuration Reference ## Overview This document describes all configuration options for Workflow Engine. ## Environment Variables | Variable | Type | Default | Description | |----------|------|---------|-------------| | `WORKFLOW_MAX_STEPS` | integer | 100 | Maximum steps per workflow | | `WORKFLOW_TIMEOUT_SECONDS` | integer | 3600 | Maximum workflow execution time | | `WORKFLOW_RETRY_LIMIT` | integer | 3 | Default retry attempts for failed steps | | `WORKFLOW_CONCURRENT_LIMIT` | integer | 10 | Maximum concurrent executions per workflow | | `EXECUTOR_POOL_SIZE` | integer | 50 | Number of executor workers | | `EXECUTOR_MEMORY_LIMIT` | string | 512Mi | Memory limit per executor | ## Configuration File Configuration can also be provided via `workflow engine.yaml`: ```yaml workflow_max_steps: 100 workflow_timeout_seconds: 3600 workflow_retry_limit: 3 workflow_concurrent_limit: 10 executor_pool_size: 50 ``` ## Validation Configuration is validated at startup. Invalid configuration will prevent the service from starting.",
              "score": null
            }
          ],
          "latency_ms": 3.5973390004073735,
          "query_id": "troubleshoot_028",
          "dimension": "casual",
          "query": "db connection pool maxed out"
        },
        {
          "key_facts": [
            "pg_stat_activity",
            "SHOW POOLS",
            "pg_terminate_backend"
          ],
          "found_facts": [
            "pg_stat_activity",
            "SHOW POOLS",
            "pg_terminate_backend"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "runbook_database_connection_ex"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "runbook_database_connection_exhaustion",
              "chunk_id": "runbook_database_connection_exhaustion_fix_0",
              "content": "# Runbook: Database Connection Exhaustion ## Severity **Level:** P1 - Critical **Response Time:** 15 minutes ## Symptoms - Connection pool at 100% - Queries timing out - 'too many connections' errors in logs ## Diagnosis ### Step 1 Check active connections: `SELECT count(*) FROM pg_stat_activity` ### Step 2 Identify long-running queries: `SELECT pid, now() - pg_stat_activity.query_start AS duration, query FROM pg_stat_activity WHERE state != 'idle' ORDER BY duration DESC` ### Step 3 Check PgBouncer stats: `SHOW POOLS` ### Step 4 Verify connection limits: `SHOW max_connections` ## Resolution ### Step 1 Kill long-running queries: `SELECT pg_terminate_backend(pid) FROM pg_stat_activity WHERE duration > interval '5 minutes'` ### Step 2 Increase PgBouncer pool size temporarily: update pgbouncer.ini ### Step 3 Restart affected services to release connections: `kubectl rollout restart deployment/api` ### Step 4 Scale down non-critical services to free connections ## Escalation If connections remain exhausted, escalate to DBA. Consider enabling connection queueing. ## Post-Incident - Document timeline in incident log - Schedule post-mortem within 48 hours - Update runbook if new patterns discovered",
              "score": null
            },
            {
              "doc_id": "config_database",
              "chunk_id": "config_database_fix_0",
              "content": "# Database Configuration Reference ## Overview This document describes all configuration options for Database. ## Environment Variables | Variable | Type | Default | Description | |----------|------|---------|-------------| | `DATABASE_URL` | string | required | PostgreSQL connection string | | `DATABASE_POOL_MIN` | integer | 5 | Minimum connection pool size | | `DATABASE_POOL_MAX` | integer | 20 | Maximum connection pool size | | `DATABASE_STATEMENT_TIMEOUT` | integer | 30000 | Query timeout in milliseconds | | `DATABASE_SSL_MODE` | string | require | SSL mode: disable, require, verify-full | | `DATABASE_MIGRATION_AUTO` | boolean | false | Run migrations on startup | ## Configuration File Configuration can also be provided via `database.yaml`: ```yaml database_url: required database_pool_min: 5 database_pool_max: 20 database_statement_timeout: 30000 database_ssl_mode: require ``` ## Validation Configuration is validated at startup. Invalid configuration will prevent the service from starting.",
              "score": null
            },
            {
              "doc_id": "howto_set_up_database_connection",
              "chunk_id": "howto_set_up_database_connection_fix_0",
              "content": "# How to Set Up Database Connection ## Prerequisites - Database with network access to CloudFlow - Connection credentials ## Steps ### Step 1: Add connection secret Store your database connection string as a secret. ```bash cloudflow secrets set DATABASE_URL postgresql://user:pass@host:5432/db ``` ### Step 2: Configure connection pool Set pool size based on your plan limits. Free tier allows 5 connections, Pro allows 20. > **Note:** Pool exhaustion will queue requests ### Step 3: Test connection Verify the database is accessible from CloudFlow. ```bash cloudflow db test --secret DATABASE_URL ``` ### Step 4: Enable SSL For production, always require SSL connections. ```bash cloudflow db configure --require-ssl ``` ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            },
            {
              "doc_id": "release_3_0_0",
              "chunk_id": "release_3_0_0_fix_0",
              "content": "# Release Notes - v3.0.0 **Release Date:** 2023-10-15 ## New Features - Complete UI redesign - New workflow editor with visual builder - Real-time execution monitoring - Team workspaces ## Bug Fixes - Fixed critical security issue in token validation - Resolved data loss bug in failed migrations ## Breaking Changes - **BREAKING:** Authentication API completely rewritten - **BREAKING:** Legacy webhook format no longer supported - **BREAKING:** Minimum Node.js version now 18 ## Upgrade Instructions ```bash cloudflow upgrade --version 3.0.0 ``` ## Known Issues None at this time.",
              "score": null
            },
            {
              "doc_id": "api_users",
              "chunk_id": "api_users_fix_0",
              "content": "# Users API Reference ## Overview The Users API provides programmatic access to users functionality. Base URL: `https://api.cloudflow.io/v1/users` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /users/me Get current user profile. **Parameters:** **Response:** - Success: `200` - Returns user object - Error: `400` - Bad Request **Example:** ```json {\"id\": \"usr_001\", \"email\": \"user@example.com\", \"role\": \"admin\", \"created_at\": \"2023-06-01\"} ``` ### PATCH /users/me Update current user profile. **Parameters:** - `name` (string): Display name - `timezone` (string): Timezone (e.g., America/New_York) **Response:** - Success: `200` - User updated - Error: `400` - Bad Request **Example:** ```json {\"id\": \"usr_001\", \"name\": \"John Doe\", \"timezone\": \"America/New_York\"} ```",
              "score": null
            }
          ],
          "latency_ms": 3.5869599996658508,
          "query_id": "troubleshoot_028",
          "dimension": "contextual",
          "query": "just deployed a new feature and now the database is rejecting new connections, users are seeing errors"
        },
        {
          "key_facts": [
            "pg_stat_activity",
            "SHOW POOLS",
            "pg_terminate_backend"
          ],
          "found_facts": [
            "pg_stat_activity",
            "SHOW POOLS",
            "pg_terminate_backend"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "runbook_database_connection_ex"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "runbook_database_connection_exhaustion",
              "chunk_id": "runbook_database_connection_exhaustion_fix_0",
              "content": "# Runbook: Database Connection Exhaustion ## Severity **Level:** P1 - Critical **Response Time:** 15 minutes ## Symptoms - Connection pool at 100% - Queries timing out - 'too many connections' errors in logs ## Diagnosis ### Step 1 Check active connections: `SELECT count(*) FROM pg_stat_activity` ### Step 2 Identify long-running queries: `SELECT pid, now() - pg_stat_activity.query_start AS duration, query FROM pg_stat_activity WHERE state != 'idle' ORDER BY duration DESC` ### Step 3 Check PgBouncer stats: `SHOW POOLS` ### Step 4 Verify connection limits: `SHOW max_connections` ## Resolution ### Step 1 Kill long-running queries: `SELECT pg_terminate_backend(pid) FROM pg_stat_activity WHERE duration > interval '5 minutes'` ### Step 2 Increase PgBouncer pool size temporarily: update pgbouncer.ini ### Step 3 Restart affected services to release connections: `kubectl rollout restart deployment/api` ### Step 4 Scale down non-critical services to free connections ## Escalation If connections remain exhausted, escalate to DBA. Consider enabling connection queueing. ## Post-Incident - Document timeline in incident log - Schedule post-mortem within 48 hours - Update runbook if new patterns discovered",
              "score": null
            },
            {
              "doc_id": "config_database",
              "chunk_id": "config_database_fix_0",
              "content": "# Database Configuration Reference ## Overview This document describes all configuration options for Database. ## Environment Variables | Variable | Type | Default | Description | |----------|------|---------|-------------| | `DATABASE_URL` | string | required | PostgreSQL connection string | | `DATABASE_POOL_MIN` | integer | 5 | Minimum connection pool size | | `DATABASE_POOL_MAX` | integer | 20 | Maximum connection pool size | | `DATABASE_STATEMENT_TIMEOUT` | integer | 30000 | Query timeout in milliseconds | | `DATABASE_SSL_MODE` | string | require | SSL mode: disable, require, verify-full | | `DATABASE_MIGRATION_AUTO` | boolean | false | Run migrations on startup | ## Configuration File Configuration can also be provided via `database.yaml`: ```yaml database_url: required database_pool_min: 5 database_pool_max: 20 database_statement_timeout: 30000 database_ssl_mode: require ``` ## Validation Configuration is validated at startup. Invalid configuration will prevent the service from starting.",
              "score": null
            },
            {
              "doc_id": "howto_set_up_database_connection",
              "chunk_id": "howto_set_up_database_connection_fix_0",
              "content": "# How to Set Up Database Connection ## Prerequisites - Database with network access to CloudFlow - Connection credentials ## Steps ### Step 1: Add connection secret Store your database connection string as a secret. ```bash cloudflow secrets set DATABASE_URL postgresql://user:pass@host:5432/db ``` ### Step 2: Configure connection pool Set pool size based on your plan limits. Free tier allows 5 connections, Pro allows 20. > **Note:** Pool exhaustion will queue requests ### Step 3: Test connection Verify the database is accessible from CloudFlow. ```bash cloudflow db test --secret DATABASE_URL ``` ### Step 4: Enable SSL For production, always require SSL connections. ```bash cloudflow db configure --require-ssl ``` ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            },
            {
              "doc_id": "api_integrations",
              "chunk_id": "api_integrations_fix_0",
              "content": "# Integrations API Reference ## Overview The Integrations API provides programmatic access to integrations functionality. Base URL: `https://api.cloudflow.io/v1/integrations` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /integrations List available integrations. **Parameters:** - `category` (string): Filter by category: storage, notification, database **Response:** - Success: `200` - Returns array of integration objects - Error: `400` - Bad Request **Example:** ```json {\"integrations\": [{\"id\": \"int_s3\", \"name\": \"Amazon S3\", \"category\": \"storage\", \"status\": \"connected\"}]} ``` ### POST /integrations/{id}/connect Connect an integration. **Parameters:** - `credentials` (object): Integration-specific credentials **Response:** - Success: `200` - Integration connected - Error: `400` - Bad Request **Example:** ```json {\"id\": \"int_s3\", \"status\": \"connected\", \"connected_at\": \"2024-01-15T10:00:00Z\"} ```",
              "score": null
            },
            {
              "doc_id": "runbook_ssl_certificate_expiry",
              "chunk_id": "runbook_ssl_certificate_expiry_fix_0",
              "content": "# Runbook: SSL Certificate Expiry ## Severity **Level:** P3 - Medium **Response Time:** 15 minutes ## Symptoms - Certificate expiring within 7 days - SSL handshake failures - Browser security warnings ## Diagnosis ### Step 1 Check certificate expiry: `echo | openssl s_client -connect api.cloudflow.io:443 2>/dev/null | openssl x509 -noout -dates` ### Step 2 Verify cert-manager status: `kubectl get certificates -n production` ### Step 3 Check cert-manager logs: `kubectl logs -l app=cert-manager -n cert-manager --tail=50` ### Step 4 Verify DNS is correct: `dig +short api.cloudflow.io` ## Resolution ### Step 1 Trigger manual renewal: `kubectl delete certificate api-cert -n production && kubectl apply -f certificate.yaml` ### Step 2 Check rate limits: Let's Encrypt has 50 certs/domain/week limit ### Step 3 Use staging issuer for testing: switch to letsencrypt-staging ClusterIssuer ### Step 4 If Let's Encrypt fails, manually upload certificate ## Escalation If certificate cannot be renewed, escalate to security team immediately. Consider using backup certificate. ## Post-Incident - Document timeline in incident log - Schedule post-mortem within 48 hours - Update runbook if new patterns discovered",
              "score": null
            }
          ],
          "latency_ms": 3.6410309985512868,
          "query_id": "troubleshoot_028",
          "dimension": "negation",
          "query": "why can't my app connect to the database anymore, it was working fine yesterday"
        },
        {
          "key_facts": [
            "kafka-consumer-groups.sh",
            "consumer lag",
            "reset-offsets"
          ],
          "found_facts": [
            "kafka-consumer-groups.sh",
            "consumer lag",
            "reset-offsets"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "runbook_kafka_consumer_lag"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "runbook_kafka_consumer_lag",
              "chunk_id": "runbook_kafka_consumer_lag_fix_0",
              "content": "# Runbook: Kafka Consumer Lag ## Severity **Level:** P1 - Critical **Response Time:** 1 hour ## Symptoms - Consumer lag > 10000 messages - Processing delays reported - Alerts from lag monitor ## Diagnosis ### Step 1 Check consumer group lag: `kafka-consumer-groups.sh --bootstrap-server kafka:9092 --describe --group workflow-executor` ### Step 2 Verify consumer health: `kubectl get pods -l app=workflow-executor -n production` ### Step 3 Check for processing errors: `kubectl logs -l app=workflow-executor --tail=100 -n production | grep ERROR` ### Step 4 Monitor throughput: `cloudflow metrics get kafka.consumer.records_per_second` ## Resolution ### Step 1 Scale up consumers: `kubectl scale deployment/workflow-executor --replicas=10 -n production` ### Step 2 Reset offset if messages are stale: `kafka-consumer-groups.sh --bootstrap-server kafka:9092 --group workflow-executor --reset-offsets --to-latest --execute --topic workflows` ### Step 3 Pause producer if backpressure needed: `cloudflow workflows pause-triggers` ### Step 4 Increase partition count for parallelism (requires coordination) ## Escalation If lag persists > 30 minutes, page platform team. May need Kafka cluster scaling. ## Post-Incident - Document timeline in incident log - Schedule post-mortem within 48 hours - Update runbook if new patterns discovered",
              "score": null
            },
            {
              "doc_id": "adr_005",
              "chunk_id": "adr_005_fix_0",
              "content": "# ADR-005: Adopt Event-Driven Architecture for Workflow Execution **Status:** Accepted **Date:** 2024-12-04 ## Context We need an architecture that supports asynchronous workflow execution, step dependencies, and failure recovery. ## Options Considered ### Option 1: Event-driven with Kafka Publish-subscribe messaging. **Pros:** - Loose coupling - Scalability - Event replay - Audit trail **Cons:** - Eventual consistency - Operational complexity - Message ordering challenges ### Option 2: Request-response Synchronous API calls. **Pros:** - Simple mental model - Immediate feedback - Easy debugging **Cons:** - Tight coupling - Cascading failures - Timeout issues ### Option 3: Polling-based Workers poll for tasks. **Pros:** - Simple implementation - Easy rate limiting - No message broker **Cons:** - Polling overhead - Higher latency - Inefficient at scale ## Decision We will use an event-driven architecture with Kafka for workflow step execution. ## Consequences - Implement idempotent consumers - Use dead letter queues for failed messages - Monitor consumer lag",
              "score": null
            },
            {
              "doc_id": "arch_data_pipeline",
              "chunk_id": "arch_data_pipeline_fix_0",
              "content": "# Data Pipeline Architecture ## Overview The Data Pipeline processes and transforms data between workflow steps. It supports batch processing up to 10GB and streaming for real-time use cases. ## Design Principles - **Exactly-Once Processing**: Each record is processed exactly once - **Backpressure Handling**: System gracefully handles varying load ## Components ### Stream Processor Handles real-time data streams. Maximum throughput of 100000 messages per second. **Technology:** Apache Kafka with Flink **Scaling:** Partition-based scaling ### Batch Processor Processes large batch jobs. Supports data formats: JSON, CSV, Parquet, Avro. **Technology:** Apache Spark on Kubernetes **Scaling:** Dynamic resource allocation ### Data Lake Long-term data storage with time-travel queries. Retention configurable up to 7 years. **Technology:** S3 with Iceberg tables **Scaling:** Unlimited with tiered storage ## Data Flow Input \u2192 Validation \u2192 Transform \u2192 Enrich \u2192 Output ## Performance Characteristics - **Latency P50:** 100ms (streaming), 5min (batch) - **Latency P99:** 500ms (streaming), 30min (batch) - **Throughput:** 100000 messages/second (streaming)",
              "score": null
            },
            {
              "doc_id": "runbook_high_cpu_usage",
              "chunk_id": "runbook_high_cpu_usage_fix_0",
              "content": "# Runbook: High CPU Usage ## Severity **Level:** P1 - Critical **Response Time:** 15 minutes ## Symptoms - CPU utilization > 80% sustained - Increased response latency - Autoscaler at maximum replicas ## Diagnosis ### Step 1 Check current CPU usage: `kubectl top pods -n production` ### Step 2 Identify hot pods: `kubectl top pods --sort-by=cpu -n production | head -10` ### Step 3 Check for recent deployments: `kubectl rollout history deployment/api -n production` ### Step 4 Profile the application: `cloudflow debug profile --pod api-xyz --duration 60s` ## Resolution ### Step 1 Scale up if traffic-related: `kubectl scale deployment/api --replicas=10 -n production` ### Step 2 Restart pods if memory leak suspected: `kubectl rollout restart deployment/api -n production` ### Step 3 Roll back recent deployment if regression: `kubectl rollout undo deployment/api -n production` ### Step 4 Add resource limits if unbounded: update deployment with CPU limits ## Escalation If CPU remains high after scaling, escalate to platform team. Page on-call if affecting users. ## Post-Incident - Document timeline in incident log - Schedule post-mortem within 48 hours - Update runbook if new patterns discovered",
              "score": null
            },
            {
              "doc_id": "runbook_database_connection_exhaustion",
              "chunk_id": "runbook_database_connection_exhaustion_fix_0",
              "content": "# Runbook: Database Connection Exhaustion ## Severity **Level:** P1 - Critical **Response Time:** 15 minutes ## Symptoms - Connection pool at 100% - Queries timing out - 'too many connections' errors in logs ## Diagnosis ### Step 1 Check active connections: `SELECT count(*) FROM pg_stat_activity` ### Step 2 Identify long-running queries: `SELECT pid, now() - pg_stat_activity.query_start AS duration, query FROM pg_stat_activity WHERE state != 'idle' ORDER BY duration DESC` ### Step 3 Check PgBouncer stats: `SHOW POOLS` ### Step 4 Verify connection limits: `SHOW max_connections` ## Resolution ### Step 1 Kill long-running queries: `SELECT pg_terminate_backend(pid) FROM pg_stat_activity WHERE duration > interval '5 minutes'` ### Step 2 Increase PgBouncer pool size temporarily: update pgbouncer.ini ### Step 3 Restart affected services to release connections: `kubectl rollout restart deployment/api` ### Step 4 Scale down non-critical services to free connections ## Escalation If connections remain exhausted, escalate to DBA. Consider enabling connection queueing. ## Post-Incident - Document timeline in incident log - Schedule post-mortem within 48 hours - Update runbook if new patterns discovered",
              "score": null
            }
          ],
          "latency_ms": 3.744282999832649,
          "query_id": "troubleshoot_029",
          "dimension": "original",
          "query": "How to handle Kafka consumer lag?"
        },
        {
          "key_facts": [
            "kafka-consumer-groups.sh",
            "consumer lag",
            "reset-offsets"
          ],
          "found_facts": [
            "kafka-consumer-groups.sh",
            "consumer lag",
            "reset-offsets"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "runbook_kafka_consumer_lag"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "runbook_kafka_consumer_lag",
              "chunk_id": "runbook_kafka_consumer_lag_fix_0",
              "content": "# Runbook: Kafka Consumer Lag ## Severity **Level:** P1 - Critical **Response Time:** 1 hour ## Symptoms - Consumer lag > 10000 messages - Processing delays reported - Alerts from lag monitor ## Diagnosis ### Step 1 Check consumer group lag: `kafka-consumer-groups.sh --bootstrap-server kafka:9092 --describe --group workflow-executor` ### Step 2 Verify consumer health: `kubectl get pods -l app=workflow-executor -n production` ### Step 3 Check for processing errors: `kubectl logs -l app=workflow-executor --tail=100 -n production | grep ERROR` ### Step 4 Monitor throughput: `cloudflow metrics get kafka.consumer.records_per_second` ## Resolution ### Step 1 Scale up consumers: `kubectl scale deployment/workflow-executor --replicas=10 -n production` ### Step 2 Reset offset if messages are stale: `kafka-consumer-groups.sh --bootstrap-server kafka:9092 --group workflow-executor --reset-offsets --to-latest --execute --topic workflows` ### Step 3 Pause producer if backpressure needed: `cloudflow workflows pause-triggers` ### Step 4 Increase partition count for parallelism (requires coordination) ## Escalation If lag persists > 30 minutes, page platform team. May need Kafka cluster scaling. ## Post-Incident - Document timeline in incident log - Schedule post-mortem within 48 hours - Update runbook if new patterns discovered",
              "score": null
            },
            {
              "doc_id": "adr_005",
              "chunk_id": "adr_005_fix_0",
              "content": "# ADR-005: Adopt Event-Driven Architecture for Workflow Execution **Status:** Accepted **Date:** 2024-12-04 ## Context We need an architecture that supports asynchronous workflow execution, step dependencies, and failure recovery. ## Options Considered ### Option 1: Event-driven with Kafka Publish-subscribe messaging. **Pros:** - Loose coupling - Scalability - Event replay - Audit trail **Cons:** - Eventual consistency - Operational complexity - Message ordering challenges ### Option 2: Request-response Synchronous API calls. **Pros:** - Simple mental model - Immediate feedback - Easy debugging **Cons:** - Tight coupling - Cascading failures - Timeout issues ### Option 3: Polling-based Workers poll for tasks. **Pros:** - Simple implementation - Easy rate limiting - No message broker **Cons:** - Polling overhead - Higher latency - Inefficient at scale ## Decision We will use an event-driven architecture with Kafka for workflow step execution. ## Consequences - Implement idempotent consumers - Use dead letter queues for failed messages - Monitor consumer lag",
              "score": null
            },
            {
              "doc_id": "arch_data_pipeline",
              "chunk_id": "arch_data_pipeline_fix_0",
              "content": "# Data Pipeline Architecture ## Overview The Data Pipeline processes and transforms data between workflow steps. It supports batch processing up to 10GB and streaming for real-time use cases. ## Design Principles - **Exactly-Once Processing**: Each record is processed exactly once - **Backpressure Handling**: System gracefully handles varying load ## Components ### Stream Processor Handles real-time data streams. Maximum throughput of 100000 messages per second. **Technology:** Apache Kafka with Flink **Scaling:** Partition-based scaling ### Batch Processor Processes large batch jobs. Supports data formats: JSON, CSV, Parquet, Avro. **Technology:** Apache Spark on Kubernetes **Scaling:** Dynamic resource allocation ### Data Lake Long-term data storage with time-travel queries. Retention configurable up to 7 years. **Technology:** S3 with Iceberg tables **Scaling:** Unlimited with tiered storage ## Data Flow Input \u2192 Validation \u2192 Transform \u2192 Enrich \u2192 Output ## Performance Characteristics - **Latency P50:** 100ms (streaming), 5min (batch) - **Latency P99:** 500ms (streaming), 30min (batch) - **Throughput:** 100000 messages/second (streaming)",
              "score": null
            },
            {
              "doc_id": "runbook_database_connection_exhaustion",
              "chunk_id": "runbook_database_connection_exhaustion_fix_0",
              "content": "# Runbook: Database Connection Exhaustion ## Severity **Level:** P1 - Critical **Response Time:** 15 minutes ## Symptoms - Connection pool at 100% - Queries timing out - 'too many connections' errors in logs ## Diagnosis ### Step 1 Check active connections: `SELECT count(*) FROM pg_stat_activity` ### Step 2 Identify long-running queries: `SELECT pid, now() - pg_stat_activity.query_start AS duration, query FROM pg_stat_activity WHERE state != 'idle' ORDER BY duration DESC` ### Step 3 Check PgBouncer stats: `SHOW POOLS` ### Step 4 Verify connection limits: `SHOW max_connections` ## Resolution ### Step 1 Kill long-running queries: `SELECT pg_terminate_backend(pid) FROM pg_stat_activity WHERE duration > interval '5 minutes'` ### Step 2 Increase PgBouncer pool size temporarily: update pgbouncer.ini ### Step 3 Restart affected services to release connections: `kubectl rollout restart deployment/api` ### Step 4 Scale down non-critical services to free connections ## Escalation If connections remain exhausted, escalate to DBA. Consider enabling connection queueing. ## Post-Incident - Document timeline in incident log - Schedule post-mortem within 48 hours - Update runbook if new patterns discovered",
              "score": null
            },
            {
              "doc_id": "arch_notification_service",
              "chunk_id": "arch_notification_service_fix_0",
              "content": "# Notification Service Architecture ## Overview The Notification Service delivers alerts and updates through multiple channels including email, Slack, webhooks, and mobile push notifications. ## Design Principles - **Delivery Guarantee**: At-least-once delivery with deduplication - **Channel Abstraction**: Unified API regardless of delivery channel ## Components ### Dispatcher Routes notifications to appropriate channel handlers. Supports priority queues for urgent notifications. **Technology:** Go microservice **Scaling:** Horizontal with queue partitioning ### Email Provider Sends transactional emails. Maximum 10000 emails per hour per account. **Technology:** SendGrid integration **Scaling:** API rate limited ### Slack Integration Posts messages to Slack channels. Supports message formatting and interactive components. **Technology:** Slack Bolt SDK **Scaling:** Per-workspace rate limits ## Data Flow Event \u2192 Template \u2192 Dispatcher \u2192 Channel \u2192 Delivery Confirmation ## Performance Characteristics - **Latency P50:** 200ms - **Latency P99:** 2000ms - **Throughput:** 10000 notifications/minute",
              "score": null
            }
          ],
          "latency_ms": 3.456436999840662,
          "query_id": "troubleshoot_029",
          "dimension": "synonym",
          "query": "How do I fix message queue processing delays in Kafka?"
        },
        {
          "key_facts": [
            "kafka-consumer-groups.sh",
            "consumer lag",
            "reset-offsets"
          ],
          "found_facts": [
            "kafka-consumer-groups.sh",
            "consumer lag",
            "reset-offsets"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "runbook_kafka_consumer_lag"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "runbook_kafka_consumer_lag",
              "chunk_id": "runbook_kafka_consumer_lag_fix_0",
              "content": "# Runbook: Kafka Consumer Lag ## Severity **Level:** P1 - Critical **Response Time:** 1 hour ## Symptoms - Consumer lag > 10000 messages - Processing delays reported - Alerts from lag monitor ## Diagnosis ### Step 1 Check consumer group lag: `kafka-consumer-groups.sh --bootstrap-server kafka:9092 --describe --group workflow-executor` ### Step 2 Verify consumer health: `kubectl get pods -l app=workflow-executor -n production` ### Step 3 Check for processing errors: `kubectl logs -l app=workflow-executor --tail=100 -n production | grep ERROR` ### Step 4 Monitor throughput: `cloudflow metrics get kafka.consumer.records_per_second` ## Resolution ### Step 1 Scale up consumers: `kubectl scale deployment/workflow-executor --replicas=10 -n production` ### Step 2 Reset offset if messages are stale: `kafka-consumer-groups.sh --bootstrap-server kafka:9092 --group workflow-executor --reset-offsets --to-latest --execute --topic workflows` ### Step 3 Pause producer if backpressure needed: `cloudflow workflows pause-triggers` ### Step 4 Increase partition count for parallelism (requires coordination) ## Escalation If lag persists > 30 minutes, page platform team. May need Kafka cluster scaling. ## Post-Incident - Document timeline in incident log - Schedule post-mortem within 48 hours - Update runbook if new patterns discovered",
              "score": null
            },
            {
              "doc_id": "adr_005",
              "chunk_id": "adr_005_fix_0",
              "content": "# ADR-005: Adopt Event-Driven Architecture for Workflow Execution **Status:** Accepted **Date:** 2024-12-04 ## Context We need an architecture that supports asynchronous workflow execution, step dependencies, and failure recovery. ## Options Considered ### Option 1: Event-driven with Kafka Publish-subscribe messaging. **Pros:** - Loose coupling - Scalability - Event replay - Audit trail **Cons:** - Eventual consistency - Operational complexity - Message ordering challenges ### Option 2: Request-response Synchronous API calls. **Pros:** - Simple mental model - Immediate feedback - Easy debugging **Cons:** - Tight coupling - Cascading failures - Timeout issues ### Option 3: Polling-based Workers poll for tasks. **Pros:** - Simple implementation - Easy rate limiting - No message broker **Cons:** - Polling overhead - Higher latency - Inefficient at scale ## Decision We will use an event-driven architecture with Kafka for workflow step execution. ## Consequences - Implement idempotent consumers - Use dead letter queues for failed messages - Monitor consumer lag",
              "score": null
            },
            {
              "doc_id": "arch_data_pipeline",
              "chunk_id": "arch_data_pipeline_fix_0",
              "content": "# Data Pipeline Architecture ## Overview The Data Pipeline processes and transforms data between workflow steps. It supports batch processing up to 10GB and streaming for real-time use cases. ## Design Principles - **Exactly-Once Processing**: Each record is processed exactly once - **Backpressure Handling**: System gracefully handles varying load ## Components ### Stream Processor Handles real-time data streams. Maximum throughput of 100000 messages per second. **Technology:** Apache Kafka with Flink **Scaling:** Partition-based scaling ### Batch Processor Processes large batch jobs. Supports data formats: JSON, CSV, Parquet, Avro. **Technology:** Apache Spark on Kubernetes **Scaling:** Dynamic resource allocation ### Data Lake Long-term data storage with time-travel queries. Retention configurable up to 7 years. **Technology:** S3 with Iceberg tables **Scaling:** Unlimited with tiered storage ## Data Flow Input \u2192 Validation \u2192 Transform \u2192 Enrich \u2192 Output ## Performance Characteristics - **Latency P50:** 100ms (streaming), 5min (batch) - **Latency P99:** 500ms (streaming), 30min (batch) - **Throughput:** 100000 messages/second (streaming)",
              "score": null
            },
            {
              "doc_id": "runbook_high_cpu_usage",
              "chunk_id": "runbook_high_cpu_usage_fix_0",
              "content": "# Runbook: High CPU Usage ## Severity **Level:** P1 - Critical **Response Time:** 15 minutes ## Symptoms - CPU utilization > 80% sustained - Increased response latency - Autoscaler at maximum replicas ## Diagnosis ### Step 1 Check current CPU usage: `kubectl top pods -n production` ### Step 2 Identify hot pods: `kubectl top pods --sort-by=cpu -n production | head -10` ### Step 3 Check for recent deployments: `kubectl rollout history deployment/api -n production` ### Step 4 Profile the application: `cloudflow debug profile --pod api-xyz --duration 60s` ## Resolution ### Step 1 Scale up if traffic-related: `kubectl scale deployment/api --replicas=10 -n production` ### Step 2 Restart pods if memory leak suspected: `kubectl rollout restart deployment/api -n production` ### Step 3 Roll back recent deployment if regression: `kubectl rollout undo deployment/api -n production` ### Step 4 Add resource limits if unbounded: update deployment with CPU limits ## Escalation If CPU remains high after scaling, escalate to platform team. Page on-call if affecting users. ## Post-Incident - Document timeline in incident log - Schedule post-mortem within 48 hours - Update runbook if new patterns discovered",
              "score": null
            },
            {
              "doc_id": "runbook_database_connection_exhaustion",
              "chunk_id": "runbook_database_connection_exhaustion_fix_0",
              "content": "# Runbook: Database Connection Exhaustion ## Severity **Level:** P1 - Critical **Response Time:** 15 minutes ## Symptoms - Connection pool at 100% - Queries timing out - 'too many connections' errors in logs ## Diagnosis ### Step 1 Check active connections: `SELECT count(*) FROM pg_stat_activity` ### Step 2 Identify long-running queries: `SELECT pid, now() - pg_stat_activity.query_start AS duration, query FROM pg_stat_activity WHERE state != 'idle' ORDER BY duration DESC` ### Step 3 Check PgBouncer stats: `SHOW POOLS` ### Step 4 Verify connection limits: `SHOW max_connections` ## Resolution ### Step 1 Kill long-running queries: `SELECT pg_terminate_backend(pid) FROM pg_stat_activity WHERE duration > interval '5 minutes'` ### Step 2 Increase PgBouncer pool size temporarily: update pgbouncer.ini ### Step 3 Restart affected services to release connections: `kubectl rollout restart deployment/api` ### Step 4 Scale down non-critical services to free connections ## Escalation If connections remain exhausted, escalate to DBA. Consider enabling connection queueing. ## Post-Incident - Document timeline in incident log - Schedule post-mortem within 48 hours - Update runbook if new patterns discovered",
              "score": null
            }
          ],
          "latency_ms": 3.803973000685801,
          "query_id": "troubleshoot_029",
          "dimension": "problem",
          "query": "kafka-consumer-groups shows 50000 messages behind and workflows are delayed by 10+ minutes"
        },
        {
          "key_facts": [
            "kafka-consumer-groups.sh",
            "consumer lag",
            "reset-offsets"
          ],
          "found_facts": [
            "kafka-consumer-groups.sh",
            "consumer lag",
            "reset-offsets"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "runbook_kafka_consumer_lag"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "runbook_kafka_consumer_lag",
              "chunk_id": "runbook_kafka_consumer_lag_fix_0",
              "content": "# Runbook: Kafka Consumer Lag ## Severity **Level:** P1 - Critical **Response Time:** 1 hour ## Symptoms - Consumer lag > 10000 messages - Processing delays reported - Alerts from lag monitor ## Diagnosis ### Step 1 Check consumer group lag: `kafka-consumer-groups.sh --bootstrap-server kafka:9092 --describe --group workflow-executor` ### Step 2 Verify consumer health: `kubectl get pods -l app=workflow-executor -n production` ### Step 3 Check for processing errors: `kubectl logs -l app=workflow-executor --tail=100 -n production | grep ERROR` ### Step 4 Monitor throughput: `cloudflow metrics get kafka.consumer.records_per_second` ## Resolution ### Step 1 Scale up consumers: `kubectl scale deployment/workflow-executor --replicas=10 -n production` ### Step 2 Reset offset if messages are stale: `kafka-consumer-groups.sh --bootstrap-server kafka:9092 --group workflow-executor --reset-offsets --to-latest --execute --topic workflows` ### Step 3 Pause producer if backpressure needed: `cloudflow workflows pause-triggers` ### Step 4 Increase partition count for parallelism (requires coordination) ## Escalation If lag persists > 30 minutes, page platform team. May need Kafka cluster scaling. ## Post-Incident - Document timeline in incident log - Schedule post-mortem within 48 hours - Update runbook if new patterns discovered",
              "score": null
            },
            {
              "doc_id": "arch_data_pipeline",
              "chunk_id": "arch_data_pipeline_fix_0",
              "content": "# Data Pipeline Architecture ## Overview The Data Pipeline processes and transforms data between workflow steps. It supports batch processing up to 10GB and streaming for real-time use cases. ## Design Principles - **Exactly-Once Processing**: Each record is processed exactly once - **Backpressure Handling**: System gracefully handles varying load ## Components ### Stream Processor Handles real-time data streams. Maximum throughput of 100000 messages per second. **Technology:** Apache Kafka with Flink **Scaling:** Partition-based scaling ### Batch Processor Processes large batch jobs. Supports data formats: JSON, CSV, Parquet, Avro. **Technology:** Apache Spark on Kubernetes **Scaling:** Dynamic resource allocation ### Data Lake Long-term data storage with time-travel queries. Retention configurable up to 7 years. **Technology:** S3 with Iceberg tables **Scaling:** Unlimited with tiered storage ## Data Flow Input \u2192 Validation \u2192 Transform \u2192 Enrich \u2192 Output ## Performance Characteristics - **Latency P50:** 100ms (streaming), 5min (batch) - **Latency P99:** 500ms (streaming), 30min (batch) - **Throughput:** 100000 messages/second (streaming)",
              "score": null
            },
            {
              "doc_id": "adr_005",
              "chunk_id": "adr_005_fix_0",
              "content": "# ADR-005: Adopt Event-Driven Architecture for Workflow Execution **Status:** Accepted **Date:** 2024-12-04 ## Context We need an architecture that supports asynchronous workflow execution, step dependencies, and failure recovery. ## Options Considered ### Option 1: Event-driven with Kafka Publish-subscribe messaging. **Pros:** - Loose coupling - Scalability - Event replay - Audit trail **Cons:** - Eventual consistency - Operational complexity - Message ordering challenges ### Option 2: Request-response Synchronous API calls. **Pros:** - Simple mental model - Immediate feedback - Easy debugging **Cons:** - Tight coupling - Cascading failures - Timeout issues ### Option 3: Polling-based Workers poll for tasks. **Pros:** - Simple implementation - Easy rate limiting - No message broker **Cons:** - Polling overhead - Higher latency - Inefficient at scale ## Decision We will use an event-driven architecture with Kafka for workflow step execution. ## Consequences - Implement idempotent consumers - Use dead letter queues for failed messages - Monitor consumer lag",
              "score": null
            },
            {
              "doc_id": "runbook_database_connection_exhaustion",
              "chunk_id": "runbook_database_connection_exhaustion_fix_0",
              "content": "# Runbook: Database Connection Exhaustion ## Severity **Level:** P1 - Critical **Response Time:** 15 minutes ## Symptoms - Connection pool at 100% - Queries timing out - 'too many connections' errors in logs ## Diagnosis ### Step 1 Check active connections: `SELECT count(*) FROM pg_stat_activity` ### Step 2 Identify long-running queries: `SELECT pid, now() - pg_stat_activity.query_start AS duration, query FROM pg_stat_activity WHERE state != 'idle' ORDER BY duration DESC` ### Step 3 Check PgBouncer stats: `SHOW POOLS` ### Step 4 Verify connection limits: `SHOW max_connections` ## Resolution ### Step 1 Kill long-running queries: `SELECT pg_terminate_backend(pid) FROM pg_stat_activity WHERE duration > interval '5 minutes'` ### Step 2 Increase PgBouncer pool size temporarily: update pgbouncer.ini ### Step 3 Restart affected services to release connections: `kubectl rollout restart deployment/api` ### Step 4 Scale down non-critical services to free connections ## Escalation If connections remain exhausted, escalate to DBA. Consider enabling connection queueing. ## Post-Incident - Document timeline in incident log - Schedule post-mortem within 48 hours - Update runbook if new patterns discovered",
              "score": null
            },
            {
              "doc_id": "runbook_high_cpu_usage",
              "chunk_id": "runbook_high_cpu_usage_fix_0",
              "content": "# Runbook: High CPU Usage ## Severity **Level:** P1 - Critical **Response Time:** 15 minutes ## Symptoms - CPU utilization > 80% sustained - Increased response latency - Autoscaler at maximum replicas ## Diagnosis ### Step 1 Check current CPU usage: `kubectl top pods -n production` ### Step 2 Identify hot pods: `kubectl top pods --sort-by=cpu -n production | head -10` ### Step 3 Check for recent deployments: `kubectl rollout history deployment/api -n production` ### Step 4 Profile the application: `cloudflow debug profile --pod api-xyz --duration 60s` ## Resolution ### Step 1 Scale up if traffic-related: `kubectl scale deployment/api --replicas=10 -n production` ### Step 2 Restart pods if memory leak suspected: `kubectl rollout restart deployment/api -n production` ### Step 3 Roll back recent deployment if regression: `kubectl rollout undo deployment/api -n production` ### Step 4 Add resource limits if unbounded: update deployment with CPU limits ## Escalation If CPU remains high after scaling, escalate to platform team. Page on-call if affecting users. ## Post-Incident - Document timeline in incident log - Schedule post-mortem within 48 hours - Update runbook if new patterns discovered",
              "score": null
            }
          ],
          "latency_ms": 3.9964520001376513,
          "query_id": "troubleshoot_029",
          "dimension": "casual",
          "query": "kafka lag through the roof, consumers cant keep up"
        },
        {
          "key_facts": [
            "kafka-consumer-groups.sh",
            "consumer lag",
            "reset-offsets"
          ],
          "found_facts": [
            "kafka-consumer-groups.sh",
            "consumer lag",
            "reset-offsets"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "runbook_kafka_consumer_lag"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "runbook_kafka_consumer_lag",
              "chunk_id": "runbook_kafka_consumer_lag_fix_0",
              "content": "# Runbook: Kafka Consumer Lag ## Severity **Level:** P1 - Critical **Response Time:** 1 hour ## Symptoms - Consumer lag > 10000 messages - Processing delays reported - Alerts from lag monitor ## Diagnosis ### Step 1 Check consumer group lag: `kafka-consumer-groups.sh --bootstrap-server kafka:9092 --describe --group workflow-executor` ### Step 2 Verify consumer health: `kubectl get pods -l app=workflow-executor -n production` ### Step 3 Check for processing errors: `kubectl logs -l app=workflow-executor --tail=100 -n production | grep ERROR` ### Step 4 Monitor throughput: `cloudflow metrics get kafka.consumer.records_per_second` ## Resolution ### Step 1 Scale up consumers: `kubectl scale deployment/workflow-executor --replicas=10 -n production` ### Step 2 Reset offset if messages are stale: `kafka-consumer-groups.sh --bootstrap-server kafka:9092 --group workflow-executor --reset-offsets --to-latest --execute --topic workflows` ### Step 3 Pause producer if backpressure needed: `cloudflow workflows pause-triggers` ### Step 4 Increase partition count for parallelism (requires coordination) ## Escalation If lag persists > 30 minutes, page platform team. May need Kafka cluster scaling. ## Post-Incident - Document timeline in incident log - Schedule post-mortem within 48 hours - Update runbook if new patterns discovered",
              "score": null
            },
            {
              "doc_id": "adr_005",
              "chunk_id": "adr_005_fix_0",
              "content": "# ADR-005: Adopt Event-Driven Architecture for Workflow Execution **Status:** Accepted **Date:** 2024-12-04 ## Context We need an architecture that supports asynchronous workflow execution, step dependencies, and failure recovery. ## Options Considered ### Option 1: Event-driven with Kafka Publish-subscribe messaging. **Pros:** - Loose coupling - Scalability - Event replay - Audit trail **Cons:** - Eventual consistency - Operational complexity - Message ordering challenges ### Option 2: Request-response Synchronous API calls. **Pros:** - Simple mental model - Immediate feedback - Easy debugging **Cons:** - Tight coupling - Cascading failures - Timeout issues ### Option 3: Polling-based Workers poll for tasks. **Pros:** - Simple implementation - Easy rate limiting - No message broker **Cons:** - Polling overhead - Higher latency - Inefficient at scale ## Decision We will use an event-driven architecture with Kafka for workflow step execution. ## Consequences - Implement idempotent consumers - Use dead letter queues for failed messages - Monitor consumer lag",
              "score": null
            },
            {
              "doc_id": "arch_data_pipeline",
              "chunk_id": "arch_data_pipeline_fix_0",
              "content": "# Data Pipeline Architecture ## Overview The Data Pipeline processes and transforms data between workflow steps. It supports batch processing up to 10GB and streaming for real-time use cases. ## Design Principles - **Exactly-Once Processing**: Each record is processed exactly once - **Backpressure Handling**: System gracefully handles varying load ## Components ### Stream Processor Handles real-time data streams. Maximum throughput of 100000 messages per second. **Technology:** Apache Kafka with Flink **Scaling:** Partition-based scaling ### Batch Processor Processes large batch jobs. Supports data formats: JSON, CSV, Parquet, Avro. **Technology:** Apache Spark on Kubernetes **Scaling:** Dynamic resource allocation ### Data Lake Long-term data storage with time-travel queries. Retention configurable up to 7 years. **Technology:** S3 with Iceberg tables **Scaling:** Unlimited with tiered storage ## Data Flow Input \u2192 Validation \u2192 Transform \u2192 Enrich \u2192 Output ## Performance Characteristics - **Latency P50:** 100ms (streaming), 5min (batch) - **Latency P99:** 500ms (streaming), 30min (batch) - **Throughput:** 100000 messages/second (streaming)",
              "score": null
            },
            {
              "doc_id": "runbook_api_gateway_5xx_errors",
              "chunk_id": "runbook_api_gateway_5xx_errors_fix_0",
              "content": "# Runbook: API Gateway 5xx Errors ## Severity **Level:** P3 - Medium **Response Time:** 1 hour ## Symptoms - Error rate > 1% - 5xx responses in access logs - Downstream service failures ## Diagnosis ### Step 1 Check Kong error logs: `kubectl logs -l app=kong -n kong --tail=100 | grep -E '5[0-9]{2}'` ### Step 2 Verify backend health: `kubectl get pods -n production -o wide` ### Step 3 Check service endpoints: `kubectl get endpoints -n production` ### Step 4 Test backend directly: `kubectl exec -it kong-xxx -n kong -- curl http://api.production.svc:8080/health` ## Resolution ### Step 1 Restart unhealthy backends: `kubectl rollout restart deployment/api -n production` ### Step 2 Enable circuit breaker if cascading: update Kong circuit-breaker plugin ### Step 3 Scale up backends: `kubectl scale deployment/api --replicas=5 -n production` ### Step 4 Check for resource exhaustion on backend pods ### Step 5 Verify network policies allow traffic ## Escalation If 5xx persists > 15 minutes, page on-call. May indicate infrastructure issue. ## Post-Incident - Document timeline in incident log - Schedule post-mortem within 48 hours - Update runbook if new patterns discovered",
              "score": null
            },
            {
              "doc_id": "runbook_high_cpu_usage",
              "chunk_id": "runbook_high_cpu_usage_fix_0",
              "content": "# Runbook: High CPU Usage ## Severity **Level:** P1 - Critical **Response Time:** 15 minutes ## Symptoms - CPU utilization > 80% sustained - Increased response latency - Autoscaler at maximum replicas ## Diagnosis ### Step 1 Check current CPU usage: `kubectl top pods -n production` ### Step 2 Identify hot pods: `kubectl top pods --sort-by=cpu -n production | head -10` ### Step 3 Check for recent deployments: `kubectl rollout history deployment/api -n production` ### Step 4 Profile the application: `cloudflow debug profile --pod api-xyz --duration 60s` ## Resolution ### Step 1 Scale up if traffic-related: `kubectl scale deployment/api --replicas=10 -n production` ### Step 2 Restart pods if memory leak suspected: `kubectl rollout restart deployment/api -n production` ### Step 3 Roll back recent deployment if regression: `kubectl rollout undo deployment/api -n production` ### Step 4 Add resource limits if unbounded: update deployment with CPU limits ## Escalation If CPU remains high after scaling, escalate to platform team. Page on-call if affecting users. ## Post-Incident - Document timeline in incident log - Schedule post-mortem within 48 hours - Update runbook if new patterns discovered",
              "score": null
            }
          ],
          "latency_ms": 3.8182200005394407,
          "query_id": "troubleshoot_029",
          "dimension": "contextual",
          "query": "we had a traffic spike and now Kafka consumers are way behind, need to catch up before the backlog gets worse"
        },
        {
          "key_facts": [
            "kafka-consumer-groups.sh",
            "consumer lag",
            "reset-offsets"
          ],
          "found_facts": [
            "kafka-consumer-groups.sh",
            "consumer lag",
            "reset-offsets"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "runbook_kafka_consumer_lag"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "runbook_kafka_consumer_lag",
              "chunk_id": "runbook_kafka_consumer_lag_fix_0",
              "content": "# Runbook: Kafka Consumer Lag ## Severity **Level:** P1 - Critical **Response Time:** 1 hour ## Symptoms - Consumer lag > 10000 messages - Processing delays reported - Alerts from lag monitor ## Diagnosis ### Step 1 Check consumer group lag: `kafka-consumer-groups.sh --bootstrap-server kafka:9092 --describe --group workflow-executor` ### Step 2 Verify consumer health: `kubectl get pods -l app=workflow-executor -n production` ### Step 3 Check for processing errors: `kubectl logs -l app=workflow-executor --tail=100 -n production | grep ERROR` ### Step 4 Monitor throughput: `cloudflow metrics get kafka.consumer.records_per_second` ## Resolution ### Step 1 Scale up consumers: `kubectl scale deployment/workflow-executor --replicas=10 -n production` ### Step 2 Reset offset if messages are stale: `kafka-consumer-groups.sh --bootstrap-server kafka:9092 --group workflow-executor --reset-offsets --to-latest --execute --topic workflows` ### Step 3 Pause producer if backpressure needed: `cloudflow workflows pause-triggers` ### Step 4 Increase partition count for parallelism (requires coordination) ## Escalation If lag persists > 30 minutes, page platform team. May need Kafka cluster scaling. ## Post-Incident - Document timeline in incident log - Schedule post-mortem within 48 hours - Update runbook if new patterns discovered",
              "score": null
            },
            {
              "doc_id": "adr_005",
              "chunk_id": "adr_005_fix_0",
              "content": "# ADR-005: Adopt Event-Driven Architecture for Workflow Execution **Status:** Accepted **Date:** 2024-12-04 ## Context We need an architecture that supports asynchronous workflow execution, step dependencies, and failure recovery. ## Options Considered ### Option 1: Event-driven with Kafka Publish-subscribe messaging. **Pros:** - Loose coupling - Scalability - Event replay - Audit trail **Cons:** - Eventual consistency - Operational complexity - Message ordering challenges ### Option 2: Request-response Synchronous API calls. **Pros:** - Simple mental model - Immediate feedback - Easy debugging **Cons:** - Tight coupling - Cascading failures - Timeout issues ### Option 3: Polling-based Workers poll for tasks. **Pros:** - Simple implementation - Easy rate limiting - No message broker **Cons:** - Polling overhead - Higher latency - Inefficient at scale ## Decision We will use an event-driven architecture with Kafka for workflow step execution. ## Consequences - Implement idempotent consumers - Use dead letter queues for failed messages - Monitor consumer lag",
              "score": null
            },
            {
              "doc_id": "arch_data_pipeline",
              "chunk_id": "arch_data_pipeline_fix_0",
              "content": "# Data Pipeline Architecture ## Overview The Data Pipeline processes and transforms data between workflow steps. It supports batch processing up to 10GB and streaming for real-time use cases. ## Design Principles - **Exactly-Once Processing**: Each record is processed exactly once - **Backpressure Handling**: System gracefully handles varying load ## Components ### Stream Processor Handles real-time data streams. Maximum throughput of 100000 messages per second. **Technology:** Apache Kafka with Flink **Scaling:** Partition-based scaling ### Batch Processor Processes large batch jobs. Supports data formats: JSON, CSV, Parquet, Avro. **Technology:** Apache Spark on Kubernetes **Scaling:** Dynamic resource allocation ### Data Lake Long-term data storage with time-travel queries. Retention configurable up to 7 years. **Technology:** S3 with Iceberg tables **Scaling:** Unlimited with tiered storage ## Data Flow Input \u2192 Validation \u2192 Transform \u2192 Enrich \u2192 Output ## Performance Characteristics - **Latency P50:** 100ms (streaming), 5min (batch) - **Latency P99:** 500ms (streaming), 30min (batch) - **Throughput:** 100000 messages/second (streaming)",
              "score": null
            },
            {
              "doc_id": "arch_notification_service",
              "chunk_id": "arch_notification_service_fix_0",
              "content": "# Notification Service Architecture ## Overview The Notification Service delivers alerts and updates through multiple channels including email, Slack, webhooks, and mobile push notifications. ## Design Principles - **Delivery Guarantee**: At-least-once delivery with deduplication - **Channel Abstraction**: Unified API regardless of delivery channel ## Components ### Dispatcher Routes notifications to appropriate channel handlers. Supports priority queues for urgent notifications. **Technology:** Go microservice **Scaling:** Horizontal with queue partitioning ### Email Provider Sends transactional emails. Maximum 10000 emails per hour per account. **Technology:** SendGrid integration **Scaling:** API rate limited ### Slack Integration Posts messages to Slack channels. Supports message formatting and interactive components. **Technology:** Slack Bolt SDK **Scaling:** Per-workspace rate limits ## Data Flow Event \u2192 Template \u2192 Dispatcher \u2192 Channel \u2192 Delivery Confirmation ## Performance Characteristics - **Latency P50:** 200ms - **Latency P99:** 2000ms - **Throughput:** 10000 notifications/minute",
              "score": null
            },
            {
              "doc_id": "runbook_high_cpu_usage",
              "chunk_id": "runbook_high_cpu_usage_fix_0",
              "content": "# Runbook: High CPU Usage ## Severity **Level:** P1 - Critical **Response Time:** 15 minutes ## Symptoms - CPU utilization > 80% sustained - Increased response latency - Autoscaler at maximum replicas ## Diagnosis ### Step 1 Check current CPU usage: `kubectl top pods -n production` ### Step 2 Identify hot pods: `kubectl top pods --sort-by=cpu -n production | head -10` ### Step 3 Check for recent deployments: `kubectl rollout history deployment/api -n production` ### Step 4 Profile the application: `cloudflow debug profile --pod api-xyz --duration 60s` ## Resolution ### Step 1 Scale up if traffic-related: `kubectl scale deployment/api --replicas=10 -n production` ### Step 2 Restart pods if memory leak suspected: `kubectl rollout restart deployment/api -n production` ### Step 3 Roll back recent deployment if regression: `kubectl rollout undo deployment/api -n production` ### Step 4 Add resource limits if unbounded: update deployment with CPU limits ## Escalation If CPU remains high after scaling, escalate to platform team. Page on-call if affecting users. ## Post-Incident - Document timeline in incident log - Schedule post-mortem within 48 hours - Update runbook if new patterns discovered",
              "score": null
            }
          ],
          "latency_ms": 3.4318910002184566,
          "query_id": "troubleshoot_029",
          "dimension": "negation",
          "query": "why aren't my Kafka consumers processing messages fast enough?"
        },
        {
          "key_facts": [
            "openssl s_client",
            "cert-manager",
            "Let's Encrypt"
          ],
          "found_facts": [
            "openssl s_client",
            "cert-manager",
            "Let's Encrypt"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "runbook_ssl_certificate_expir"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "runbook_ssl_certificate_expiry",
              "chunk_id": "runbook_ssl_certificate_expiry_fix_0",
              "content": "# Runbook: SSL Certificate Expiry ## Severity **Level:** P3 - Medium **Response Time:** 15 minutes ## Symptoms - Certificate expiring within 7 days - SSL handshake failures - Browser security warnings ## Diagnosis ### Step 1 Check certificate expiry: `echo | openssl s_client -connect api.cloudflow.io:443 2>/dev/null | openssl x509 -noout -dates` ### Step 2 Verify cert-manager status: `kubectl get certificates -n production` ### Step 3 Check cert-manager logs: `kubectl logs -l app=cert-manager -n cert-manager --tail=50` ### Step 4 Verify DNS is correct: `dig +short api.cloudflow.io` ## Resolution ### Step 1 Trigger manual renewal: `kubectl delete certificate api-cert -n production && kubectl apply -f certificate.yaml` ### Step 2 Check rate limits: Let's Encrypt has 50 certs/domain/week limit ### Step 3 Use staging issuer for testing: switch to letsencrypt-staging ClusterIssuer ### Step 4 If Let's Encrypt fails, manually upload certificate ## Escalation If certificate cannot be renewed, escalate to security team immediately. Consider using backup certificate. ## Post-Incident - Document timeline in incident log - Schedule post-mortem within 48 hours - Update runbook if new patterns discovered",
              "score": null
            },
            {
              "doc_id": "arch_authentication_service",
              "chunk_id": "arch_authentication_service_fix_0",
              "content": "# Authentication Service Architecture ## Overview The Authentication Service handles user identity, session management, and token issuance. Supports OAuth 2.0, SAML, and API key authentication. ## Design Principles - **Zero Knowledge**: Passwords never stored in plaintext - **Token Rotation**: Refresh tokens rotate on each use ## Components ### Identity Provider Manages user accounts and credentials. Password hashing uses bcrypt with cost factor 12. **Technology:** Custom Go service **Scaling:** Horizontal stateless ### Token Service Issues and validates access tokens. Token lifetime is 1 hour with 7-day refresh tokens. **Technology:** JWT with RS256 **Scaling:** Horizontal stateless ### SSO Gateway Integrates with external identity providers. Supports Google, Microsoft, Okta, and custom SAML. **Technology:** Dex OIDC proxy **Scaling:** Active-passive ## Data Flow Credentials \u2192 Validation \u2192 Token Issue \u2192 Token Refresh \u2192 Logout ## Performance Characteristics - **Latency P50:** 50ms - **Latency P99:** 200ms - **Throughput:** 5000 auth/second",
              "score": null
            },
            {
              "doc_id": "howto_create_api_integration",
              "chunk_id": "howto_create_api_integration_fix_0",
              "content": "# How to Create API Integration ## Prerequisites - CloudFlow account ## Steps ### Step 1: Generate API key Create an API key for programmatic access. ```bash cloudflow apikeys create --name 'CI Integration' --scopes workflows:read,workflows:execute ``` ### Step 2: Store key securely Never commit API keys to source control. Use environment variables or secret management. > **Note:** Rotate keys every 90 days ### Step 3: Test authentication Verify the API key works correctly. ```bash curl -H 'Authorization: Bearer cf_...' https://api.cloudflow.io/v1/me ``` ### Step 4: Implement rate limiting Handle rate limit responses (HTTP 429) with exponential backoff. > **Note:** Default limit is 100 requests per minute ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            },
            {
              "doc_id": "howto_configure_single_sign_on",
              "chunk_id": "howto_configure_single_sign_on_fix_0",
              "content": "# How to Configure Single Sign-On ## Prerequisites - Admin access - Enterprise plan - Identity provider access ## Steps ### Step 1: Choose provider CloudFlow supports SAML 2.0, OpenID Connect, and OAuth 2.0 providers. ### Step 2: Get metadata Download CloudFlow's SAML metadata for your IdP configuration. ```bash cloudflow sso metadata --format xml > cloudflow-metadata.xml ``` ### Step 3: Configure IdP Add CloudFlow as a service provider in your identity provider. > **Note:** ACS URL: https://auth.cloudflow.io/saml/callback ### Step 4: Upload IdP metadata Provide your IdP's metadata to CloudFlow. ```bash cloudflow sso configure --idp-metadata ./idp-metadata.xml ``` ### Step 5: Test login Verify SSO login works correctly. ```bash cloudflow sso test ``` ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            },
            {
              "doc_id": "config_database",
              "chunk_id": "config_database_fix_0",
              "content": "# Database Configuration Reference ## Overview This document describes all configuration options for Database. ## Environment Variables | Variable | Type | Default | Description | |----------|------|---------|-------------| | `DATABASE_URL` | string | required | PostgreSQL connection string | | `DATABASE_POOL_MIN` | integer | 5 | Minimum connection pool size | | `DATABASE_POOL_MAX` | integer | 20 | Maximum connection pool size | | `DATABASE_STATEMENT_TIMEOUT` | integer | 30000 | Query timeout in milliseconds | | `DATABASE_SSL_MODE` | string | require | SSL mode: disable, require, verify-full | | `DATABASE_MIGRATION_AUTO` | boolean | false | Run migrations on startup | ## Configuration File Configuration can also be provided via `database.yaml`: ```yaml database_url: required database_pool_min: 5 database_pool_max: 20 database_statement_timeout: 30000 database_ssl_mode: require ``` ## Validation Configuration is validated at startup. Invalid configuration will prevent the service from starting.",
              "score": null
            }
          ],
          "latency_ms": 3.5360950005269842,
          "query_id": "troubleshoot_030",
          "dimension": "original",
          "query": "What to do when SSL certificate is expiring?"
        },
        {
          "key_facts": [
            "openssl s_client",
            "cert-manager",
            "Let's Encrypt"
          ],
          "found_facts": [
            "openssl s_client",
            "cert-manager",
            "Let's Encrypt"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "runbook_ssl_certificate_expir"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "runbook_ssl_certificate_expiry",
              "chunk_id": "runbook_ssl_certificate_expiry_fix_0",
              "content": "# Runbook: SSL Certificate Expiry ## Severity **Level:** P3 - Medium **Response Time:** 15 minutes ## Symptoms - Certificate expiring within 7 days - SSL handshake failures - Browser security warnings ## Diagnosis ### Step 1 Check certificate expiry: `echo | openssl s_client -connect api.cloudflow.io:443 2>/dev/null | openssl x509 -noout -dates` ### Step 2 Verify cert-manager status: `kubectl get certificates -n production` ### Step 3 Check cert-manager logs: `kubectl logs -l app=cert-manager -n cert-manager --tail=50` ### Step 4 Verify DNS is correct: `dig +short api.cloudflow.io` ## Resolution ### Step 1 Trigger manual renewal: `kubectl delete certificate api-cert -n production && kubectl apply -f certificate.yaml` ### Step 2 Check rate limits: Let's Encrypt has 50 certs/domain/week limit ### Step 3 Use staging issuer for testing: switch to letsencrypt-staging ClusterIssuer ### Step 4 If Let's Encrypt fails, manually upload certificate ## Escalation If certificate cannot be renewed, escalate to security team immediately. Consider using backup certificate. ## Post-Incident - Document timeline in incident log - Schedule post-mortem within 48 hours - Update runbook if new patterns discovered",
              "score": null
            },
            {
              "doc_id": "arch_authentication_service",
              "chunk_id": "arch_authentication_service_fix_0",
              "content": "# Authentication Service Architecture ## Overview The Authentication Service handles user identity, session management, and token issuance. Supports OAuth 2.0, SAML, and API key authentication. ## Design Principles - **Zero Knowledge**: Passwords never stored in plaintext - **Token Rotation**: Refresh tokens rotate on each use ## Components ### Identity Provider Manages user accounts and credentials. Password hashing uses bcrypt with cost factor 12. **Technology:** Custom Go service **Scaling:** Horizontal stateless ### Token Service Issues and validates access tokens. Token lifetime is 1 hour with 7-day refresh tokens. **Technology:** JWT with RS256 **Scaling:** Horizontal stateless ### SSO Gateway Integrates with external identity providers. Supports Google, Microsoft, Okta, and custom SAML. **Technology:** Dex OIDC proxy **Scaling:** Active-passive ## Data Flow Credentials \u2192 Validation \u2192 Token Issue \u2192 Token Refresh \u2192 Logout ## Performance Characteristics - **Latency P50:** 50ms - **Latency P99:** 200ms - **Throughput:** 5000 auth/second",
              "score": null
            },
            {
              "doc_id": "config_api_gateway",
              "chunk_id": "config_api_gateway_fix_0",
              "content": "# API Gateway Configuration Reference ## Overview This document describes all configuration options for API Gateway. ## Environment Variables | Variable | Type | Default | Description | |----------|------|---------|-------------| | `GATEWAY_PORT` | integer | 8000 | Gateway listen port | | `GATEWAY_RATE_LIMIT` | integer | 100 | Requests per minute per user | | `GATEWAY_TIMEOUT_MS` | integer | 30000 | Request timeout in milliseconds | | `GATEWAY_MAX_BODY_SIZE` | string | 10MB | Maximum request body size | | `GATEWAY_CORS_ORIGINS` | string | * | Allowed CORS origins | | `GATEWAY_SSL_ENABLED` | boolean | true | Enable TLS termination | ## Configuration File Configuration can also be provided via `api gateway.yaml`: ```yaml gateway_port: 8000 gateway_rate_limit: 100 gateway_timeout_ms: 30000 gateway_max_body_size: 10MB gateway_cors_origins: * ``` ## Validation Configuration is validated at startup. Invalid configuration will prevent the service from starting.",
              "score": null
            },
            {
              "doc_id": "arch_api_gateway",
              "chunk_id": "arch_api_gateway_fix_0",
              "content": "# API Gateway Architecture ## Overview The API Gateway handles all incoming HTTP requests, providing authentication, rate limiting, and request routing. Built on Kong with custom plugins for CloudFlow-specific functionality. ## Design Principles - **Zero Trust**: Every request is authenticated and authorized - **Defense in Depth**: Multiple layers of security validation ## Components ### Kong Gateway Primary ingress controller. Handles TLS termination, request routing, and plugin execution. **Technology:** Kong 3.4 on Kubernetes **Scaling:** Horizontal with HPA ### Auth Plugin Validates JWT tokens, API keys, and OAuth2 flows. Token validation latency under 5ms. **Technology:** Custom Lua plugin **Scaling:** Stateless ### Rate Limiter Enforces per-user and per-IP rate limits. Default limit is 100 requests per minute. **Technology:** Redis-backed sliding window **Scaling:** Shared Redis cluster ## Data Flow Client \u2192 TLS \u2192 Kong \u2192 Auth \u2192 Rate Limit \u2192 Backend Services ## Performance Characteristics - **Latency P50:** 12ms - **Latency P99:** 45ms - **Throughput:** 50000 requests/second",
              "score": null
            },
            {
              "doc_id": "howto_implement_workflow_versioning",
              "chunk_id": "howto_implement_workflow_versioning_fix_0",
              "content": "# How to Implement Workflow Versioning ## Prerequisites - Existing workflow ## Steps ### Step 1: Enable versioning Turn on versioning for your workflow. This is required for the feature. ```bash cloudflow workflows update wf_123 --enable-versioning ``` ### Step 2: Create new version Save changes as a new version instead of overwriting. ```bash cloudflow workflows publish wf_123 --version 2.0.0 ``` ### Step 3: Set active version Choose which version handles new executions. ```bash cloudflow workflows activate wf_123 --version 2.0.0 ``` ### Step 4: Roll back if needed Quickly revert to a previous version. ```bash cloudflow workflows rollback wf_123 --to-version 1.0.0 ``` ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            }
          ],
          "latency_ms": 3.995891998783918,
          "query_id": "troubleshoot_030",
          "dimension": "synonym",
          "query": "How do I renew TLS certs before they expire?"
        },
        {
          "key_facts": [
            "openssl s_client",
            "cert-manager",
            "Let's Encrypt"
          ],
          "found_facts": [
            "openssl s_client",
            "cert-manager",
            "Let's Encrypt"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "runbook_ssl_certificate_expir"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "runbook_ssl_certificate_expiry",
              "chunk_id": "runbook_ssl_certificate_expiry_fix_0",
              "content": "# Runbook: SSL Certificate Expiry ## Severity **Level:** P3 - Medium **Response Time:** 15 minutes ## Symptoms - Certificate expiring within 7 days - SSL handshake failures - Browser security warnings ## Diagnosis ### Step 1 Check certificate expiry: `echo | openssl s_client -connect api.cloudflow.io:443 2>/dev/null | openssl x509 -noout -dates` ### Step 2 Verify cert-manager status: `kubectl get certificates -n production` ### Step 3 Check cert-manager logs: `kubectl logs -l app=cert-manager -n cert-manager --tail=50` ### Step 4 Verify DNS is correct: `dig +short api.cloudflow.io` ## Resolution ### Step 1 Trigger manual renewal: `kubectl delete certificate api-cert -n production && kubectl apply -f certificate.yaml` ### Step 2 Check rate limits: Let's Encrypt has 50 certs/domain/week limit ### Step 3 Use staging issuer for testing: switch to letsencrypt-staging ClusterIssuer ### Step 4 If Let's Encrypt fails, manually upload certificate ## Escalation If certificate cannot be renewed, escalate to security team immediately. Consider using backup certificate. ## Post-Incident - Document timeline in incident log - Schedule post-mortem within 48 hours - Update runbook if new patterns discovered",
              "score": null
            },
            {
              "doc_id": "arch_authentication_service",
              "chunk_id": "arch_authentication_service_fix_0",
              "content": "# Authentication Service Architecture ## Overview The Authentication Service handles user identity, session management, and token issuance. Supports OAuth 2.0, SAML, and API key authentication. ## Design Principles - **Zero Knowledge**: Passwords never stored in plaintext - **Token Rotation**: Refresh tokens rotate on each use ## Components ### Identity Provider Manages user accounts and credentials. Password hashing uses bcrypt with cost factor 12. **Technology:** Custom Go service **Scaling:** Horizontal stateless ### Token Service Issues and validates access tokens. Token lifetime is 1 hour with 7-day refresh tokens. **Technology:** JWT with RS256 **Scaling:** Horizontal stateless ### SSO Gateway Integrates with external identity providers. Supports Google, Microsoft, Okta, and custom SAML. **Technology:** Dex OIDC proxy **Scaling:** Active-passive ## Data Flow Credentials \u2192 Validation \u2192 Token Issue \u2192 Token Refresh \u2192 Logout ## Performance Characteristics - **Latency P50:** 50ms - **Latency P99:** 200ms - **Throughput:** 5000 auth/second",
              "score": null
            },
            {
              "doc_id": "api_secrets",
              "chunk_id": "api_secrets_fix_0",
              "content": "# Secrets API Reference ## Overview The Secrets API provides programmatic access to secrets functionality. Base URL: `https://api.cloudflow.io/v1/secrets` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /secrets List secret names (values not returned). **Parameters:** **Response:** - Success: `200` - Returns array of secret metadata - Error: `400` - Bad Request **Example:** ```json {\"secrets\": [{\"name\": \"DATABASE_URL\", \"created_at\": \"2024-01-01\", \"updated_at\": \"2024-01-10\"}]} ``` ### PUT /secrets/{name} Create or update a secret. **Parameters:** - `value` (string): Secret value (encrypted at rest) **Response:** - Success: `200` - Secret saved - Error: `400` - Bad Request **Example:** ```json {\"name\": \"DATABASE_URL\", \"updated_at\": \"2024-01-15T10:00:00Z\"} ``` ### DELETE /secrets/{name} Delete a secret. **Parameters:** **Response:** - Success: `204` - Secret deleted - Error: `400` - Bad Request **Example:** ```json {} ```",
              "score": null
            },
            {
              "doc_id": "release_3_2_0",
              "chunk_id": "release_3_2_0_fix_0",
              "content": "# Release Notes - v3.2.0 **Release Date:** 2024-01-15 ## New Features - Workflow versioning with rollback support - New Python SDK with async support - Bulk operations API for workflows - Custom retry policies per step ## Bug Fixes - Fixed race condition in concurrent execution handler - Resolved memory leak in long-running workflows - Fixed timezone handling in scheduled triggers ## Breaking Changes - **BREAKING:** API v1 endpoints deprecated, will be removed in v4.0 - **BREAKING:** Minimum Python version now 3.9 ## Upgrade Instructions ```bash cloudflow upgrade --version 3.2.0 ``` ## Known Issues None at this time.",
              "score": null
            },
            {
              "doc_id": "api_users",
              "chunk_id": "api_users_fix_0",
              "content": "# Users API Reference ## Overview The Users API provides programmatic access to users functionality. Base URL: `https://api.cloudflow.io/v1/users` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /users/me Get current user profile. **Parameters:** **Response:** - Success: `200` - Returns user object - Error: `400` - Bad Request **Example:** ```json {\"id\": \"usr_001\", \"email\": \"user@example.com\", \"role\": \"admin\", \"created_at\": \"2023-06-01\"} ``` ### PATCH /users/me Update current user profile. **Parameters:** - `name` (string): Display name - `timezone` (string): Timezone (e.g., America/New_York) **Response:** - Success: `200` - User updated - Error: `400` - Bad Request **Example:** ```json {\"id\": \"usr_001\", \"name\": \"John Doe\", \"timezone\": \"America/New_York\"} ```",
              "score": null
            }
          ],
          "latency_ms": 4.961470000125701,
          "query_id": "troubleshoot_030",
          "dimension": "problem",
          "query": "users are getting browser security warnings and openssl shows cert expires in 3 days"
        },
        {
          "key_facts": [
            "openssl s_client",
            "cert-manager",
            "Let's Encrypt"
          ],
          "found_facts": [
            "openssl s_client",
            "cert-manager",
            "Let's Encrypt"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "runbook_ssl_certificate_expir"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "runbook_ssl_certificate_expiry",
              "chunk_id": "runbook_ssl_certificate_expiry_fix_0",
              "content": "# Runbook: SSL Certificate Expiry ## Severity **Level:** P3 - Medium **Response Time:** 15 minutes ## Symptoms - Certificate expiring within 7 days - SSL handshake failures - Browser security warnings ## Diagnosis ### Step 1 Check certificate expiry: `echo | openssl s_client -connect api.cloudflow.io:443 2>/dev/null | openssl x509 -noout -dates` ### Step 2 Verify cert-manager status: `kubectl get certificates -n production` ### Step 3 Check cert-manager logs: `kubectl logs -l app=cert-manager -n cert-manager --tail=50` ### Step 4 Verify DNS is correct: `dig +short api.cloudflow.io` ## Resolution ### Step 1 Trigger manual renewal: `kubectl delete certificate api-cert -n production && kubectl apply -f certificate.yaml` ### Step 2 Check rate limits: Let's Encrypt has 50 certs/domain/week limit ### Step 3 Use staging issuer for testing: switch to letsencrypt-staging ClusterIssuer ### Step 4 If Let's Encrypt fails, manually upload certificate ## Escalation If certificate cannot be renewed, escalate to security team immediately. Consider using backup certificate. ## Post-Incident - Document timeline in incident log - Schedule post-mortem within 48 hours - Update runbook if new patterns discovered",
              "score": null
            },
            {
              "doc_id": "arch_authentication_service",
              "chunk_id": "arch_authentication_service_fix_0",
              "content": "# Authentication Service Architecture ## Overview The Authentication Service handles user identity, session management, and token issuance. Supports OAuth 2.0, SAML, and API key authentication. ## Design Principles - **Zero Knowledge**: Passwords never stored in plaintext - **Token Rotation**: Refresh tokens rotate on each use ## Components ### Identity Provider Manages user accounts and credentials. Password hashing uses bcrypt with cost factor 12. **Technology:** Custom Go service **Scaling:** Horizontal stateless ### Token Service Issues and validates access tokens. Token lifetime is 1 hour with 7-day refresh tokens. **Technology:** JWT with RS256 **Scaling:** Horizontal stateless ### SSO Gateway Integrates with external identity providers. Supports Google, Microsoft, Okta, and custom SAML. **Technology:** Dex OIDC proxy **Scaling:** Active-passive ## Data Flow Credentials \u2192 Validation \u2192 Token Issue \u2192 Token Refresh \u2192 Logout ## Performance Characteristics - **Latency P50:** 50ms - **Latency P99:** 200ms - **Throughput:** 5000 auth/second",
              "score": null
            },
            {
              "doc_id": "adr_003",
              "chunk_id": "adr_003_fix_0",
              "content": "# ADR-003: Implement OAuth 2.0 with JWT for Authentication **Status:** Accepted **Date:** 2024-05-08 ## Context We need a secure authentication system that supports API keys, user sessions, and third-party integrations. ## Options Considered ### Option 1: OAuth 2.0 with JWT Industry standard token-based auth. **Pros:** - Industry standard - Stateless validation - Supports refresh tokens - SSO compatible **Cons:** - Token size overhead - Cannot revoke individual tokens - Complexity ### Option 2: Session-based auth Traditional server-side sessions. **Pros:** - Simple implementation - Easy revocation - Smaller payloads **Cons:** - Requires session store - Not suitable for APIs - Scaling challenges ### Option 3: API keys only Simple key-based authentication. **Pros:** - Very simple - Low overhead - Easy to implement **Cons:** - No user context - Hard to rotate - No SSO support ## Decision We will implement OAuth 2.0 with JWT tokens. Access tokens expire after 1 hour, refresh tokens after 7 days. ## Consequences - Implement token rotation on refresh - Support API keys for machine-to-machine auth - Add MFA for admin accounts",
              "score": null
            },
            {
              "doc_id": "config_api_gateway",
              "chunk_id": "config_api_gateway_fix_0",
              "content": "# API Gateway Configuration Reference ## Overview This document describes all configuration options for API Gateway. ## Environment Variables | Variable | Type | Default | Description | |----------|------|---------|-------------| | `GATEWAY_PORT` | integer | 8000 | Gateway listen port | | `GATEWAY_RATE_LIMIT` | integer | 100 | Requests per minute per user | | `GATEWAY_TIMEOUT_MS` | integer | 30000 | Request timeout in milliseconds | | `GATEWAY_MAX_BODY_SIZE` | string | 10MB | Maximum request body size | | `GATEWAY_CORS_ORIGINS` | string | * | Allowed CORS origins | | `GATEWAY_SSL_ENABLED` | boolean | true | Enable TLS termination | ## Configuration File Configuration can also be provided via `api gateway.yaml`: ```yaml gateway_port: 8000 gateway_rate_limit: 100 gateway_timeout_ms: 30000 gateway_max_body_size: 10MB gateway_cors_origins: * ``` ## Validation Configuration is validated at startup. Invalid configuration will prevent the service from starting.",
              "score": null
            },
            {
              "doc_id": "howto_implement_retry_logic",
              "chunk_id": "howto_implement_retry_logic_fix_0",
              "content": "# How to Implement Retry Logic ## Prerequisites - Existing workflow ## Steps ### Step 1: Define retry policy Add retry configuration to your workflow step. > **Note:** Maximum 5 retry attempts allowed ### Step 2: Configure backoff Set exponential backoff with base delay of 1 second. The formula is: delay = base * 2^attempt. ### Step 3: Add timeout Set maximum execution time to prevent infinite loops. Default timeout is 30 seconds. ### Step 4: Handle final failure Configure on_failure handler to notify or escalate when all retries are exhausted. ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            }
          ],
          "latency_ms": 3.5722720003832364,
          "query_id": "troubleshoot_030",
          "dimension": "casual",
          "query": "ssl cert about to expire, how to renew"
        },
        {
          "key_facts": [
            "openssl s_client",
            "cert-manager",
            "Let's Encrypt"
          ],
          "found_facts": [
            "openssl s_client",
            "cert-manager",
            "Let's Encrypt"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "runbook_ssl_certificate_expir"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "runbook_ssl_certificate_expiry",
              "chunk_id": "runbook_ssl_certificate_expiry_fix_0",
              "content": "# Runbook: SSL Certificate Expiry ## Severity **Level:** P3 - Medium **Response Time:** 15 minutes ## Symptoms - Certificate expiring within 7 days - SSL handshake failures - Browser security warnings ## Diagnosis ### Step 1 Check certificate expiry: `echo | openssl s_client -connect api.cloudflow.io:443 2>/dev/null | openssl x509 -noout -dates` ### Step 2 Verify cert-manager status: `kubectl get certificates -n production` ### Step 3 Check cert-manager logs: `kubectl logs -l app=cert-manager -n cert-manager --tail=50` ### Step 4 Verify DNS is correct: `dig +short api.cloudflow.io` ## Resolution ### Step 1 Trigger manual renewal: `kubectl delete certificate api-cert -n production && kubectl apply -f certificate.yaml` ### Step 2 Check rate limits: Let's Encrypt has 50 certs/domain/week limit ### Step 3 Use staging issuer for testing: switch to letsencrypt-staging ClusterIssuer ### Step 4 If Let's Encrypt fails, manually upload certificate ## Escalation If certificate cannot be renewed, escalate to security team immediately. Consider using backup certificate. ## Post-Incident - Document timeline in incident log - Schedule post-mortem within 48 hours - Update runbook if new patterns discovered",
              "score": null
            },
            {
              "doc_id": "howto_set_up_data_encryption",
              "chunk_id": "howto_set_up_data_encryption_fix_0",
              "content": "# How to Set Up Data Encryption ## Prerequisites - CloudFlow Enterprise plan ## Steps ### Step 1: Enable encryption at rest All data is encrypted at rest using AES-256 by default. > **Note:** Encryption is automatic for all plans ### Step 2: Configure KMS Bring your own encryption keys using AWS KMS or GCP KMS. ```bash cloudflow encryption configure --kms-key arn:aws:kms:... ``` ### Step 3: Enable field-level encryption Encrypt sensitive fields within your workflow data. ```bash cloudflow encryption add-field --path $.user.ssn --key-alias sensitive-data ``` ### Step 4: Verify encryption Confirm encryption is active for your workspace. ```bash cloudflow encryption status ``` ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            },
            {
              "doc_id": "arch_authentication_service",
              "chunk_id": "arch_authentication_service_fix_0",
              "content": "# Authentication Service Architecture ## Overview The Authentication Service handles user identity, session management, and token issuance. Supports OAuth 2.0, SAML, and API key authentication. ## Design Principles - **Zero Knowledge**: Passwords never stored in plaintext - **Token Rotation**: Refresh tokens rotate on each use ## Components ### Identity Provider Manages user accounts and credentials. Password hashing uses bcrypt with cost factor 12. **Technology:** Custom Go service **Scaling:** Horizontal stateless ### Token Service Issues and validates access tokens. Token lifetime is 1 hour with 7-day refresh tokens. **Technology:** JWT with RS256 **Scaling:** Horizontal stateless ### SSO Gateway Integrates with external identity providers. Supports Google, Microsoft, Okta, and custom SAML. **Technology:** Dex OIDC proxy **Scaling:** Active-passive ## Data Flow Credentials \u2192 Validation \u2192 Token Issue \u2192 Token Refresh \u2192 Logout ## Performance Characteristics - **Latency P50:** 50ms - **Latency P99:** 200ms - **Throughput:** 5000 auth/second",
              "score": null
            },
            {
              "doc_id": "api_secrets",
              "chunk_id": "api_secrets_fix_0",
              "content": "# Secrets API Reference ## Overview The Secrets API provides programmatic access to secrets functionality. Base URL: `https://api.cloudflow.io/v1/secrets` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /secrets List secret names (values not returned). **Parameters:** **Response:** - Success: `200` - Returns array of secret metadata - Error: `400` - Bad Request **Example:** ```json {\"secrets\": [{\"name\": \"DATABASE_URL\", \"created_at\": \"2024-01-01\", \"updated_at\": \"2024-01-10\"}]} ``` ### PUT /secrets/{name} Create or update a secret. **Parameters:** - `value` (string): Secret value (encrypted at rest) **Response:** - Success: `200` - Secret saved - Error: `400` - Bad Request **Example:** ```json {\"name\": \"DATABASE_URL\", \"updated_at\": \"2024-01-15T10:00:00Z\"} ``` ### DELETE /secrets/{name} Delete a secret. **Parameters:** **Response:** - Success: `204` - Secret deleted - Error: `400` - Bad Request **Example:** ```json {} ```",
              "score": null
            },
            {
              "doc_id": "howto_set_up_monitoring_alerts",
              "chunk_id": "howto_set_up_monitoring_alerts_fix_0",
              "content": "# How to Set Up Monitoring Alerts ## Prerequisites - CloudFlow Pro plan or higher ## Steps ### Step 1: Define alert rules Create alert conditions based on metrics thresholds. ```bash cloudflow alerts create --name 'High Error Rate' --condition 'error_rate > 0.05' ``` ### Step 2: Configure channels Set up notification channels: email, Slack, or PagerDuty. ```bash cloudflow alerts channel add --type slack --webhook https://hooks.slack.com/... ``` ### Step 3: Set severity levels Assign severity to each alert: critical, warning, or info. > **Note:** Critical alerts bypass quiet hours ### Step 4: Test alerts Trigger a test alert to verify routing. ```bash cloudflow alerts test --name 'High Error Rate' ``` ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            }
          ],
          "latency_ms": 3.637092999269953,
          "query_id": "troubleshoot_030",
          "dimension": "contextual",
          "query": "got an alert that our production SSL certificate expires next week, need to renew with Let's Encrypt"
        },
        {
          "key_facts": [
            "openssl s_client",
            "cert-manager",
            "Let's Encrypt"
          ],
          "found_facts": [
            "openssl s_client",
            "cert-manager",
            "Let's Encrypt"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "runbook_ssl_certificate_expir"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "runbook_ssl_certificate_expiry",
              "chunk_id": "runbook_ssl_certificate_expiry_fix_0",
              "content": "# Runbook: SSL Certificate Expiry ## Severity **Level:** P3 - Medium **Response Time:** 15 minutes ## Symptoms - Certificate expiring within 7 days - SSL handshake failures - Browser security warnings ## Diagnosis ### Step 1 Check certificate expiry: `echo | openssl s_client -connect api.cloudflow.io:443 2>/dev/null | openssl x509 -noout -dates` ### Step 2 Verify cert-manager status: `kubectl get certificates -n production` ### Step 3 Check cert-manager logs: `kubectl logs -l app=cert-manager -n cert-manager --tail=50` ### Step 4 Verify DNS is correct: `dig +short api.cloudflow.io` ## Resolution ### Step 1 Trigger manual renewal: `kubectl delete certificate api-cert -n production && kubectl apply -f certificate.yaml` ### Step 2 Check rate limits: Let's Encrypt has 50 certs/domain/week limit ### Step 3 Use staging issuer for testing: switch to letsencrypt-staging ClusterIssuer ### Step 4 If Let's Encrypt fails, manually upload certificate ## Escalation If certificate cannot be renewed, escalate to security team immediately. Consider using backup certificate. ## Post-Incident - Document timeline in incident log - Schedule post-mortem within 48 hours - Update runbook if new patterns discovered",
              "score": null
            },
            {
              "doc_id": "arch_authentication_service",
              "chunk_id": "arch_authentication_service_fix_0",
              "content": "# Authentication Service Architecture ## Overview The Authentication Service handles user identity, session management, and token issuance. Supports OAuth 2.0, SAML, and API key authentication. ## Design Principles - **Zero Knowledge**: Passwords never stored in plaintext - **Token Rotation**: Refresh tokens rotate on each use ## Components ### Identity Provider Manages user accounts and credentials. Password hashing uses bcrypt with cost factor 12. **Technology:** Custom Go service **Scaling:** Horizontal stateless ### Token Service Issues and validates access tokens. Token lifetime is 1 hour with 7-day refresh tokens. **Technology:** JWT with RS256 **Scaling:** Horizontal stateless ### SSO Gateway Integrates with external identity providers. Supports Google, Microsoft, Okta, and custom SAML. **Technology:** Dex OIDC proxy **Scaling:** Active-passive ## Data Flow Credentials \u2192 Validation \u2192 Token Issue \u2192 Token Refresh \u2192 Logout ## Performance Characteristics - **Latency P50:** 50ms - **Latency P99:** 200ms - **Throughput:** 5000 auth/second",
              "score": null
            },
            {
              "doc_id": "howto_configure_custom_domain",
              "chunk_id": "howto_configure_custom_domain_fix_0",
              "content": "# How to Configure Custom Domain ## Prerequisites - Domain with DNS access - CloudFlow Pro plan or higher ## Steps ### Step 1: Verify domain ownership Add a TXT record to verify domain ownership. ```bash cloudflow domains verify --domain app.example.com ``` ### Step 2: Configure DNS Add a CNAME record pointing to your CloudFlow endpoint. > **Note:** CNAME should point to ingress.cloudflow.io ### Step 3: Enable SSL CloudFlow automatically provisions SSL certificates via Let's Encrypt. ```bash cloudflow domains ssl --domain app.example.com ``` ### Step 4: Set as primary Make this the primary domain for your application. ```bash cloudflow domains set-primary --domain app.example.com ``` ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            },
            {
              "doc_id": "adr_003",
              "chunk_id": "adr_003_fix_0",
              "content": "# ADR-003: Implement OAuth 2.0 with JWT for Authentication **Status:** Accepted **Date:** 2024-05-08 ## Context We need a secure authentication system that supports API keys, user sessions, and third-party integrations. ## Options Considered ### Option 1: OAuth 2.0 with JWT Industry standard token-based auth. **Pros:** - Industry standard - Stateless validation - Supports refresh tokens - SSO compatible **Cons:** - Token size overhead - Cannot revoke individual tokens - Complexity ### Option 2: Session-based auth Traditional server-side sessions. **Pros:** - Simple implementation - Easy revocation - Smaller payloads **Cons:** - Requires session store - Not suitable for APIs - Scaling challenges ### Option 3: API keys only Simple key-based authentication. **Pros:** - Very simple - Low overhead - Easy to implement **Cons:** - No user context - Hard to rotate - No SSO support ## Decision We will implement OAuth 2.0 with JWT tokens. Access tokens expire after 1 hour, refresh tokens after 7 days. ## Consequences - Implement token rotation on refresh - Support API keys for machine-to-machine auth - Add MFA for admin accounts",
              "score": null
            },
            {
              "doc_id": "howto_configure_single_sign_on",
              "chunk_id": "howto_configure_single_sign_on_fix_0",
              "content": "# How to Configure Single Sign-On ## Prerequisites - Admin access - Enterprise plan - Identity provider access ## Steps ### Step 1: Choose provider CloudFlow supports SAML 2.0, OpenID Connect, and OAuth 2.0 providers. ### Step 2: Get metadata Download CloudFlow's SAML metadata for your IdP configuration. ```bash cloudflow sso metadata --format xml > cloudflow-metadata.xml ``` ### Step 3: Configure IdP Add CloudFlow as a service provider in your identity provider. > **Note:** ACS URL: https://auth.cloudflow.io/saml/callback ### Step 4: Upload IdP metadata Provide your IdP's metadata to CloudFlow. ```bash cloudflow sso configure --idp-metadata ./idp-metadata.xml ``` ### Step 5: Test login Verify SSO login works correctly. ```bash cloudflow sso test ``` ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            }
          ],
          "latency_ms": 3.441217999352375,
          "query_id": "troubleshoot_030",
          "dimension": "negation",
          "query": "why didn't cert-manager auto-renew our certificate?"
        },
        {
          "key_facts": [
            "OOMKilled",
            "resources.limits.memory",
            "HeapDumpOnOutOfMemoryError"
          ],
          "found_facts": [
            "OOMKilled",
            "resources.limits.memory",
            "HeapDumpOnOutOfMemoryError"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "runbook_out_of_memory_oom_kill"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "runbook_out_of_memory_oom_kills",
              "chunk_id": "runbook_out_of_memory_oom_kills_fix_0",
              "content": "# Runbook: Out of Memory (OOM) Kills ## Severity **Level:** P3 - Medium **Response Time:** 15 minutes ## Symptoms - Pods restarting with OOMKilled status - Memory usage at limit - Application slowdown before restart ## Diagnosis ### Step 1 Check OOM events: `kubectl get events -n production --field-selector reason=OOMKilled` ### Step 2 Review memory usage: `kubectl top pods -n production --sort-by=memory` ### Step 3 Check container limits: `kubectl describe pod <pod-name> -n production | grep -A5 Limits` ### Step 4 Analyze heap dumps if available: `cloudflow debug heapdump --pod <pod-name>` ## Resolution ### Step 1 Increase memory limit: update deployment resources.limits.memory ### Step 2 Add memory request equal to limit to guarantee allocation ### Step 3 Enable JVM heap dump on OOM: add -XX:+HeapDumpOnOutOfMemoryError ### Step 4 Investigate memory leaks with profiling tools ### Step 5 Consider horizontal scaling instead of vertical ## Escalation If OOMs continue after limit increase, escalate to development team for memory leak investigation. ## Post-Incident - Document timeline in incident log - Schedule post-mortem within 48 hours - Update runbook if new patterns discovered",
              "score": null
            },
            {
              "doc_id": "runbook_database_connection_exhaustion",
              "chunk_id": "runbook_database_connection_exhaustion_fix_0",
              "content": "# Runbook: Database Connection Exhaustion ## Severity **Level:** P1 - Critical **Response Time:** 15 minutes ## Symptoms - Connection pool at 100% - Queries timing out - 'too many connections' errors in logs ## Diagnosis ### Step 1 Check active connections: `SELECT count(*) FROM pg_stat_activity` ### Step 2 Identify long-running queries: `SELECT pid, now() - pg_stat_activity.query_start AS duration, query FROM pg_stat_activity WHERE state != 'idle' ORDER BY duration DESC` ### Step 3 Check PgBouncer stats: `SHOW POOLS` ### Step 4 Verify connection limits: `SHOW max_connections` ## Resolution ### Step 1 Kill long-running queries: `SELECT pg_terminate_backend(pid) FROM pg_stat_activity WHERE duration > interval '5 minutes'` ### Step 2 Increase PgBouncer pool size temporarily: update pgbouncer.ini ### Step 3 Restart affected services to release connections: `kubectl rollout restart deployment/api` ### Step 4 Scale down non-critical services to free connections ## Escalation If connections remain exhausted, escalate to DBA. Consider enabling connection queueing. ## Post-Incident - Document timeline in incident log - Schedule post-mortem within 48 hours - Update runbook if new patterns discovered",
              "score": null
            },
            {
              "doc_id": "runbook_high_cpu_usage",
              "chunk_id": "runbook_high_cpu_usage_fix_0",
              "content": "# Runbook: High CPU Usage ## Severity **Level:** P1 - Critical **Response Time:** 15 minutes ## Symptoms - CPU utilization > 80% sustained - Increased response latency - Autoscaler at maximum replicas ## Diagnosis ### Step 1 Check current CPU usage: `kubectl top pods -n production` ### Step 2 Identify hot pods: `kubectl top pods --sort-by=cpu -n production | head -10` ### Step 3 Check for recent deployments: `kubectl rollout history deployment/api -n production` ### Step 4 Profile the application: `cloudflow debug profile --pod api-xyz --duration 60s` ## Resolution ### Step 1 Scale up if traffic-related: `kubectl scale deployment/api --replicas=10 -n production` ### Step 2 Restart pods if memory leak suspected: `kubectl rollout restart deployment/api -n production` ### Step 3 Roll back recent deployment if regression: `kubectl rollout undo deployment/api -n production` ### Step 4 Add resource limits if unbounded: update deployment with CPU limits ## Escalation If CPU remains high after scaling, escalate to platform team. Page on-call if affecting users. ## Post-Incident - Document timeline in incident log - Schedule post-mortem within 48 hours - Update runbook if new patterns discovered",
              "score": null
            },
            {
              "doc_id": "runbook_api_gateway_5xx_errors",
              "chunk_id": "runbook_api_gateway_5xx_errors_fix_0",
              "content": "# Runbook: API Gateway 5xx Errors ## Severity **Level:** P3 - Medium **Response Time:** 1 hour ## Symptoms - Error rate > 1% - 5xx responses in access logs - Downstream service failures ## Diagnosis ### Step 1 Check Kong error logs: `kubectl logs -l app=kong -n kong --tail=100 | grep -E '5[0-9]{2}'` ### Step 2 Verify backend health: `kubectl get pods -n production -o wide` ### Step 3 Check service endpoints: `kubectl get endpoints -n production` ### Step 4 Test backend directly: `kubectl exec -it kong-xxx -n kong -- curl http://api.production.svc:8080/health` ## Resolution ### Step 1 Restart unhealthy backends: `kubectl rollout restart deployment/api -n production` ### Step 2 Enable circuit breaker if cascading: update Kong circuit-breaker plugin ### Step 3 Scale up backends: `kubectl scale deployment/api --replicas=5 -n production` ### Step 4 Check for resource exhaustion on backend pods ### Step 5 Verify network policies allow traffic ## Escalation If 5xx persists > 15 minutes, page on-call. May indicate infrastructure issue. ## Post-Incident - Document timeline in incident log - Schedule post-mortem within 48 hours - Update runbook if new patterns discovered",
              "score": null
            },
            {
              "doc_id": "howto_debug_failed_execution",
              "chunk_id": "howto_debug_failed_execution_fix_0",
              "content": "# How to Debug Failed Execution ## Prerequisites - Failed execution ID ## Steps ### Step 1: Get execution details Retrieve the full execution record including step outputs. ```bash cloudflow executions get ex_123 --include-steps ``` ### Step 2: View step logs Check logs for the failed step. ```bash cloudflow executions logs ex_123 --step 3 ``` ### Step 3: Inspect input/output Review the data passed between steps. ```bash cloudflow executions inspect ex_123 --step 3 ``` ### Step 4: Enable debug mode Re-run with verbose logging enabled. ```bash cloudflow workflows execute wf_123 --debug ``` ### Step 5: Check error patterns Look for similar failures in recent executions. ```bash cloudflow executions list --status failed --since 24h ``` ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            }
          ],
          "latency_ms": 3.8992019999568583,
          "query_id": "troubleshoot_031",
          "dimension": "original",
          "query": "How to fix OOM kills?"
        },
        {
          "key_facts": [
            "OOMKilled",
            "resources.limits.memory",
            "HeapDumpOnOutOfMemoryError"
          ],
          "found_facts": [
            "OOMKilled",
            "resources.limits.memory",
            "HeapDumpOnOutOfMemoryError"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "runbook_out_of_memory_oom_kill"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "runbook_out_of_memory_oom_kills",
              "chunk_id": "runbook_out_of_memory_oom_kills_fix_0",
              "content": "# Runbook: Out of Memory (OOM) Kills ## Severity **Level:** P3 - Medium **Response Time:** 15 minutes ## Symptoms - Pods restarting with OOMKilled status - Memory usage at limit - Application slowdown before restart ## Diagnosis ### Step 1 Check OOM events: `kubectl get events -n production --field-selector reason=OOMKilled` ### Step 2 Review memory usage: `kubectl top pods -n production --sort-by=memory` ### Step 3 Check container limits: `kubectl describe pod <pod-name> -n production | grep -A5 Limits` ### Step 4 Analyze heap dumps if available: `cloudflow debug heapdump --pod <pod-name>` ## Resolution ### Step 1 Increase memory limit: update deployment resources.limits.memory ### Step 2 Add memory request equal to limit to guarantee allocation ### Step 3 Enable JVM heap dump on OOM: add -XX:+HeapDumpOnOutOfMemoryError ### Step 4 Investigate memory leaks with profiling tools ### Step 5 Consider horizontal scaling instead of vertical ## Escalation If OOMs continue after limit increase, escalate to development team for memory leak investigation. ## Post-Incident - Document timeline in incident log - Schedule post-mortem within 48 hours - Update runbook if new patterns discovered",
              "score": null
            },
            {
              "doc_id": "release_3_2_0",
              "chunk_id": "release_3_2_0_fix_0",
              "content": "# Release Notes - v3.2.0 **Release Date:** 2024-01-15 ## New Features - Workflow versioning with rollback support - New Python SDK with async support - Bulk operations API for workflows - Custom retry policies per step ## Bug Fixes - Fixed race condition in concurrent execution handler - Resolved memory leak in long-running workflows - Fixed timezone handling in scheduled triggers ## Breaking Changes - **BREAKING:** API v1 endpoints deprecated, will be removed in v4.0 - **BREAKING:** Minimum Python version now 3.9 ## Upgrade Instructions ```bash cloudflow upgrade --version 3.2.0 ``` ## Known Issues None at this time.",
              "score": null
            },
            {
              "doc_id": "runbook_high_cpu_usage",
              "chunk_id": "runbook_high_cpu_usage_fix_0",
              "content": "# Runbook: High CPU Usage ## Severity **Level:** P1 - Critical **Response Time:** 15 minutes ## Symptoms - CPU utilization > 80% sustained - Increased response latency - Autoscaler at maximum replicas ## Diagnosis ### Step 1 Check current CPU usage: `kubectl top pods -n production` ### Step 2 Identify hot pods: `kubectl top pods --sort-by=cpu -n production | head -10` ### Step 3 Check for recent deployments: `kubectl rollout history deployment/api -n production` ### Step 4 Profile the application: `cloudflow debug profile --pod api-xyz --duration 60s` ## Resolution ### Step 1 Scale up if traffic-related: `kubectl scale deployment/api --replicas=10 -n production` ### Step 2 Restart pods if memory leak suspected: `kubectl rollout restart deployment/api -n production` ### Step 3 Roll back recent deployment if regression: `kubectl rollout undo deployment/api -n production` ### Step 4 Add resource limits if unbounded: update deployment with CPU limits ## Escalation If CPU remains high after scaling, escalate to platform team. Page on-call if affecting users. ## Post-Incident - Document timeline in incident log - Schedule post-mortem within 48 hours - Update runbook if new patterns discovered",
              "score": null
            },
            {
              "doc_id": "runbook_database_connection_exhaustion",
              "chunk_id": "runbook_database_connection_exhaustion_fix_0",
              "content": "# Runbook: Database Connection Exhaustion ## Severity **Level:** P1 - Critical **Response Time:** 15 minutes ## Symptoms - Connection pool at 100% - Queries timing out - 'too many connections' errors in logs ## Diagnosis ### Step 1 Check active connections: `SELECT count(*) FROM pg_stat_activity` ### Step 2 Identify long-running queries: `SELECT pid, now() - pg_stat_activity.query_start AS duration, query FROM pg_stat_activity WHERE state != 'idle' ORDER BY duration DESC` ### Step 3 Check PgBouncer stats: `SHOW POOLS` ### Step 4 Verify connection limits: `SHOW max_connections` ## Resolution ### Step 1 Kill long-running queries: `SELECT pg_terminate_backend(pid) FROM pg_stat_activity WHERE duration > interval '5 minutes'` ### Step 2 Increase PgBouncer pool size temporarily: update pgbouncer.ini ### Step 3 Restart affected services to release connections: `kubectl rollout restart deployment/api` ### Step 4 Scale down non-critical services to free connections ## Escalation If connections remain exhausted, escalate to DBA. Consider enabling connection queueing. ## Post-Incident - Document timeline in incident log - Schedule post-mortem within 48 hours - Update runbook if new patterns discovered",
              "score": null
            },
            {
              "doc_id": "howto_debug_failed_execution",
              "chunk_id": "howto_debug_failed_execution_fix_0",
              "content": "# How to Debug Failed Execution ## Prerequisites - Failed execution ID ## Steps ### Step 1: Get execution details Retrieve the full execution record including step outputs. ```bash cloudflow executions get ex_123 --include-steps ``` ### Step 2: View step logs Check logs for the failed step. ```bash cloudflow executions logs ex_123 --step 3 ``` ### Step 3: Inspect input/output Review the data passed between steps. ```bash cloudflow executions inspect ex_123 --step 3 ``` ### Step 4: Enable debug mode Re-run with verbose logging enabled. ```bash cloudflow workflows execute wf_123 --debug ``` ### Step 5: Check error patterns Look for similar failures in recent executions. ```bash cloudflow executions list --status failed --since 24h ``` ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            }
          ],
          "latency_ms": 4.255034000379965,
          "query_id": "troubleshoot_031",
          "dimension": "synonym",
          "query": "How do I resolve out-of-memory container terminations?"
        },
        {
          "key_facts": [
            "OOMKilled",
            "resources.limits.memory",
            "HeapDumpOnOutOfMemoryError"
          ],
          "found_facts": [
            "OOMKilled",
            "resources.limits.memory",
            "HeapDumpOnOutOfMemoryError"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "runbook_out_of_memory_oom_kill"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "runbook_out_of_memory_oom_kills",
              "chunk_id": "runbook_out_of_memory_oom_kills_fix_0",
              "content": "# Runbook: Out of Memory (OOM) Kills ## Severity **Level:** P3 - Medium **Response Time:** 15 minutes ## Symptoms - Pods restarting with OOMKilled status - Memory usage at limit - Application slowdown before restart ## Diagnosis ### Step 1 Check OOM events: `kubectl get events -n production --field-selector reason=OOMKilled` ### Step 2 Review memory usage: `kubectl top pods -n production --sort-by=memory` ### Step 3 Check container limits: `kubectl describe pod <pod-name> -n production | grep -A5 Limits` ### Step 4 Analyze heap dumps if available: `cloudflow debug heapdump --pod <pod-name>` ## Resolution ### Step 1 Increase memory limit: update deployment resources.limits.memory ### Step 2 Add memory request equal to limit to guarantee allocation ### Step 3 Enable JVM heap dump on OOM: add -XX:+HeapDumpOnOutOfMemoryError ### Step 4 Investigate memory leaks with profiling tools ### Step 5 Consider horizontal scaling instead of vertical ## Escalation If OOMs continue after limit increase, escalate to development team for memory leak investigation. ## Post-Incident - Document timeline in incident log - Schedule post-mortem within 48 hours - Update runbook if new patterns discovered",
              "score": null
            },
            {
              "doc_id": "runbook_high_cpu_usage",
              "chunk_id": "runbook_high_cpu_usage_fix_0",
              "content": "# Runbook: High CPU Usage ## Severity **Level:** P1 - Critical **Response Time:** 15 minutes ## Symptoms - CPU utilization > 80% sustained - Increased response latency - Autoscaler at maximum replicas ## Diagnosis ### Step 1 Check current CPU usage: `kubectl top pods -n production` ### Step 2 Identify hot pods: `kubectl top pods --sort-by=cpu -n production | head -10` ### Step 3 Check for recent deployments: `kubectl rollout history deployment/api -n production` ### Step 4 Profile the application: `cloudflow debug profile --pod api-xyz --duration 60s` ## Resolution ### Step 1 Scale up if traffic-related: `kubectl scale deployment/api --replicas=10 -n production` ### Step 2 Restart pods if memory leak suspected: `kubectl rollout restart deployment/api -n production` ### Step 3 Roll back recent deployment if regression: `kubectl rollout undo deployment/api -n production` ### Step 4 Add resource limits if unbounded: update deployment with CPU limits ## Escalation If CPU remains high after scaling, escalate to platform team. Page on-call if affecting users. ## Post-Incident - Document timeline in incident log - Schedule post-mortem within 48 hours - Update runbook if new patterns discovered",
              "score": null
            },
            {
              "doc_id": "runbook_api_gateway_5xx_errors",
              "chunk_id": "runbook_api_gateway_5xx_errors_fix_0",
              "content": "# Runbook: API Gateway 5xx Errors ## Severity **Level:** P3 - Medium **Response Time:** 1 hour ## Symptoms - Error rate > 1% - 5xx responses in access logs - Downstream service failures ## Diagnosis ### Step 1 Check Kong error logs: `kubectl logs -l app=kong -n kong --tail=100 | grep -E '5[0-9]{2}'` ### Step 2 Verify backend health: `kubectl get pods -n production -o wide` ### Step 3 Check service endpoints: `kubectl get endpoints -n production` ### Step 4 Test backend directly: `kubectl exec -it kong-xxx -n kong -- curl http://api.production.svc:8080/health` ## Resolution ### Step 1 Restart unhealthy backends: `kubectl rollout restart deployment/api -n production` ### Step 2 Enable circuit breaker if cascading: update Kong circuit-breaker plugin ### Step 3 Scale up backends: `kubectl scale deployment/api --replicas=5 -n production` ### Step 4 Check for resource exhaustion on backend pods ### Step 5 Verify network policies allow traffic ## Escalation If 5xx persists > 15 minutes, page on-call. May indicate infrastructure issue. ## Post-Incident - Document timeline in incident log - Schedule post-mortem within 48 hours - Update runbook if new patterns discovered",
              "score": null
            },
            {
              "doc_id": "runbook_database_connection_exhaustion",
              "chunk_id": "runbook_database_connection_exhaustion_fix_0",
              "content": "# Runbook: Database Connection Exhaustion ## Severity **Level:** P1 - Critical **Response Time:** 15 minutes ## Symptoms - Connection pool at 100% - Queries timing out - 'too many connections' errors in logs ## Diagnosis ### Step 1 Check active connections: `SELECT count(*) FROM pg_stat_activity` ### Step 2 Identify long-running queries: `SELECT pid, now() - pg_stat_activity.query_start AS duration, query FROM pg_stat_activity WHERE state != 'idle' ORDER BY duration DESC` ### Step 3 Check PgBouncer stats: `SHOW POOLS` ### Step 4 Verify connection limits: `SHOW max_connections` ## Resolution ### Step 1 Kill long-running queries: `SELECT pg_terminate_backend(pid) FROM pg_stat_activity WHERE duration > interval '5 minutes'` ### Step 2 Increase PgBouncer pool size temporarily: update pgbouncer.ini ### Step 3 Restart affected services to release connections: `kubectl rollout restart deployment/api` ### Step 4 Scale down non-critical services to free connections ## Escalation If connections remain exhausted, escalate to DBA. Consider enabling connection queueing. ## Post-Incident - Document timeline in incident log - Schedule post-mortem within 48 hours - Update runbook if new patterns discovered",
              "score": null
            },
            {
              "doc_id": "runbook_kafka_consumer_lag",
              "chunk_id": "runbook_kafka_consumer_lag_fix_0",
              "content": "# Runbook: Kafka Consumer Lag ## Severity **Level:** P1 - Critical **Response Time:** 1 hour ## Symptoms - Consumer lag > 10000 messages - Processing delays reported - Alerts from lag monitor ## Diagnosis ### Step 1 Check consumer group lag: `kafka-consumer-groups.sh --bootstrap-server kafka:9092 --describe --group workflow-executor` ### Step 2 Verify consumer health: `kubectl get pods -l app=workflow-executor -n production` ### Step 3 Check for processing errors: `kubectl logs -l app=workflow-executor --tail=100 -n production | grep ERROR` ### Step 4 Monitor throughput: `cloudflow metrics get kafka.consumer.records_per_second` ## Resolution ### Step 1 Scale up consumers: `kubectl scale deployment/workflow-executor --replicas=10 -n production` ### Step 2 Reset offset if messages are stale: `kafka-consumer-groups.sh --bootstrap-server kafka:9092 --group workflow-executor --reset-offsets --to-latest --execute --topic workflows` ### Step 3 Pause producer if backpressure needed: `cloudflow workflows pause-triggers` ### Step 4 Increase partition count for parallelism (requires coordination) ## Escalation If lag persists > 30 minutes, page platform team. May need Kafka cluster scaling. ## Post-Incident - Document timeline in incident log - Schedule post-mortem within 48 hours - Update runbook if new patterns discovered",
              "score": null
            }
          ],
          "latency_ms": 3.8226889992074575,
          "query_id": "troubleshoot_031",
          "dimension": "problem",
          "query": "pods keep restarting with OOMKilled status and kubectl describe shows memory at limit"
        },
        {
          "key_facts": [
            "OOMKilled",
            "resources.limits.memory",
            "HeapDumpOnOutOfMemoryError"
          ],
          "found_facts": [
            "OOMKilled",
            "resources.limits.memory",
            "HeapDumpOnOutOfMemoryError"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "runbook_out_of_memory_oom_kill"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "runbook_out_of_memory_oom_kills",
              "chunk_id": "runbook_out_of_memory_oom_kills_fix_0",
              "content": "# Runbook: Out of Memory (OOM) Kills ## Severity **Level:** P3 - Medium **Response Time:** 15 minutes ## Symptoms - Pods restarting with OOMKilled status - Memory usage at limit - Application slowdown before restart ## Diagnosis ### Step 1 Check OOM events: `kubectl get events -n production --field-selector reason=OOMKilled` ### Step 2 Review memory usage: `kubectl top pods -n production --sort-by=memory` ### Step 3 Check container limits: `kubectl describe pod <pod-name> -n production | grep -A5 Limits` ### Step 4 Analyze heap dumps if available: `cloudflow debug heapdump --pod <pod-name>` ## Resolution ### Step 1 Increase memory limit: update deployment resources.limits.memory ### Step 2 Add memory request equal to limit to guarantee allocation ### Step 3 Enable JVM heap dump on OOM: add -XX:+HeapDumpOnOutOfMemoryError ### Step 4 Investigate memory leaks with profiling tools ### Step 5 Consider horizontal scaling instead of vertical ## Escalation If OOMs continue after limit increase, escalate to development team for memory leak investigation. ## Post-Incident - Document timeline in incident log - Schedule post-mortem within 48 hours - Update runbook if new patterns discovered",
              "score": null
            },
            {
              "doc_id": "runbook_high_cpu_usage",
              "chunk_id": "runbook_high_cpu_usage_fix_0",
              "content": "# Runbook: High CPU Usage ## Severity **Level:** P1 - Critical **Response Time:** 15 minutes ## Symptoms - CPU utilization > 80% sustained - Increased response latency - Autoscaler at maximum replicas ## Diagnosis ### Step 1 Check current CPU usage: `kubectl top pods -n production` ### Step 2 Identify hot pods: `kubectl top pods --sort-by=cpu -n production | head -10` ### Step 3 Check for recent deployments: `kubectl rollout history deployment/api -n production` ### Step 4 Profile the application: `cloudflow debug profile --pod api-xyz --duration 60s` ## Resolution ### Step 1 Scale up if traffic-related: `kubectl scale deployment/api --replicas=10 -n production` ### Step 2 Restart pods if memory leak suspected: `kubectl rollout restart deployment/api -n production` ### Step 3 Roll back recent deployment if regression: `kubectl rollout undo deployment/api -n production` ### Step 4 Add resource limits if unbounded: update deployment with CPU limits ## Escalation If CPU remains high after scaling, escalate to platform team. Page on-call if affecting users. ## Post-Incident - Document timeline in incident log - Schedule post-mortem within 48 hours - Update runbook if new patterns discovered",
              "score": null
            },
            {
              "doc_id": "runbook_api_gateway_5xx_errors",
              "chunk_id": "runbook_api_gateway_5xx_errors_fix_0",
              "content": "# Runbook: API Gateway 5xx Errors ## Severity **Level:** P3 - Medium **Response Time:** 1 hour ## Symptoms - Error rate > 1% - 5xx responses in access logs - Downstream service failures ## Diagnosis ### Step 1 Check Kong error logs: `kubectl logs -l app=kong -n kong --tail=100 | grep -E '5[0-9]{2}'` ### Step 2 Verify backend health: `kubectl get pods -n production -o wide` ### Step 3 Check service endpoints: `kubectl get endpoints -n production` ### Step 4 Test backend directly: `kubectl exec -it kong-xxx -n kong -- curl http://api.production.svc:8080/health` ## Resolution ### Step 1 Restart unhealthy backends: `kubectl rollout restart deployment/api -n production` ### Step 2 Enable circuit breaker if cascading: update Kong circuit-breaker plugin ### Step 3 Scale up backends: `kubectl scale deployment/api --replicas=5 -n production` ### Step 4 Check for resource exhaustion on backend pods ### Step 5 Verify network policies allow traffic ## Escalation If 5xx persists > 15 minutes, page on-call. May indicate infrastructure issue. ## Post-Incident - Document timeline in incident log - Schedule post-mortem within 48 hours - Update runbook if new patterns discovered",
              "score": null
            },
            {
              "doc_id": "adr_004",
              "chunk_id": "adr_004_fix_0",
              "content": "# ADR-004: Use Redis for Caching and Session Storage **Status:** Accepted **Date:** 2024-04-05 ## Context We need a fast data store for caching API responses, session data, and rate limiting counters. ## Options Considered ### Option 1: Redis Cluster In-memory data structure store. **Pros:** - Sub-millisecond latency - Rich data structures - Pub/sub support - Clustering **Cons:** - Memory cost - Persistence complexity - Cluster management ### Option 2: Memcached Simple key-value cache. **Pros:** - Simple operation - Multi-threaded - Lower memory overhead **Cons:** - No persistence - Limited data types - No clustering ### Option 3: Application-level cache In-process caching only. **Pros:** - No external dependency - Fastest possible - Simple **Cons:** - Not shared across pods - Memory pressure - Cold starts ## Decision We will use Redis Cluster for caching with a 6-node cluster for high availability. ## Consequences - Implement cache-aside pattern - Set appropriate TTLs - Monitor memory usage - Plan for cache stampede prevention",
              "score": null
            },
            {
              "doc_id": "runbook_database_connection_exhaustion",
              "chunk_id": "runbook_database_connection_exhaustion_fix_0",
              "content": "# Runbook: Database Connection Exhaustion ## Severity **Level:** P1 - Critical **Response Time:** 15 minutes ## Symptoms - Connection pool at 100% - Queries timing out - 'too many connections' errors in logs ## Diagnosis ### Step 1 Check active connections: `SELECT count(*) FROM pg_stat_activity` ### Step 2 Identify long-running queries: `SELECT pid, now() - pg_stat_activity.query_start AS duration, query FROM pg_stat_activity WHERE state != 'idle' ORDER BY duration DESC` ### Step 3 Check PgBouncer stats: `SHOW POOLS` ### Step 4 Verify connection limits: `SHOW max_connections` ## Resolution ### Step 1 Kill long-running queries: `SELECT pg_terminate_backend(pid) FROM pg_stat_activity WHERE duration > interval '5 minutes'` ### Step 2 Increase PgBouncer pool size temporarily: update pgbouncer.ini ### Step 3 Restart affected services to release connections: `kubectl rollout restart deployment/api` ### Step 4 Scale down non-critical services to free connections ## Escalation If connections remain exhausted, escalate to DBA. Consider enabling connection queueing. ## Post-Incident - Document timeline in incident log - Schedule post-mortem within 48 hours - Update runbook if new patterns discovered",
              "score": null
            }
          ],
          "latency_ms": 3.368423000210896,
          "query_id": "troubleshoot_031",
          "dimension": "casual",
          "query": "pods crashing oom, need more memory?"
        },
        {
          "key_facts": [
            "OOMKilled",
            "resources.limits.memory",
            "HeapDumpOnOutOfMemoryError"
          ],
          "found_facts": [
            "OOMKilled",
            "resources.limits.memory",
            "HeapDumpOnOutOfMemoryError"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "runbook_out_of_memory_oom_kill"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "runbook_out_of_memory_oom_kills",
              "chunk_id": "runbook_out_of_memory_oom_kills_fix_0",
              "content": "# Runbook: Out of Memory (OOM) Kills ## Severity **Level:** P3 - Medium **Response Time:** 15 minutes ## Symptoms - Pods restarting with OOMKilled status - Memory usage at limit - Application slowdown before restart ## Diagnosis ### Step 1 Check OOM events: `kubectl get events -n production --field-selector reason=OOMKilled` ### Step 2 Review memory usage: `kubectl top pods -n production --sort-by=memory` ### Step 3 Check container limits: `kubectl describe pod <pod-name> -n production | grep -A5 Limits` ### Step 4 Analyze heap dumps if available: `cloudflow debug heapdump --pod <pod-name>` ## Resolution ### Step 1 Increase memory limit: update deployment resources.limits.memory ### Step 2 Add memory request equal to limit to guarantee allocation ### Step 3 Enable JVM heap dump on OOM: add -XX:+HeapDumpOnOutOfMemoryError ### Step 4 Investigate memory leaks with profiling tools ### Step 5 Consider horizontal scaling instead of vertical ## Escalation If OOMs continue after limit increase, escalate to development team for memory leak investigation. ## Post-Incident - Document timeline in incident log - Schedule post-mortem within 48 hours - Update runbook if new patterns discovered",
              "score": null
            },
            {
              "doc_id": "arch_data_pipeline",
              "chunk_id": "arch_data_pipeline_fix_0",
              "content": "# Data Pipeline Architecture ## Overview The Data Pipeline processes and transforms data between workflow steps. It supports batch processing up to 10GB and streaming for real-time use cases. ## Design Principles - **Exactly-Once Processing**: Each record is processed exactly once - **Backpressure Handling**: System gracefully handles varying load ## Components ### Stream Processor Handles real-time data streams. Maximum throughput of 100000 messages per second. **Technology:** Apache Kafka with Flink **Scaling:** Partition-based scaling ### Batch Processor Processes large batch jobs. Supports data formats: JSON, CSV, Parquet, Avro. **Technology:** Apache Spark on Kubernetes **Scaling:** Dynamic resource allocation ### Data Lake Long-term data storage with time-travel queries. Retention configurable up to 7 years. **Technology:** S3 with Iceberg tables **Scaling:** Unlimited with tiered storage ## Data Flow Input \u2192 Validation \u2192 Transform \u2192 Enrich \u2192 Output ## Performance Characteristics - **Latency P50:** 100ms (streaming), 5min (batch) - **Latency P99:** 500ms (streaming), 30min (batch) - **Throughput:** 100000 messages/second (streaming)",
              "score": null
            },
            {
              "doc_id": "runbook_database_connection_exhaustion",
              "chunk_id": "runbook_database_connection_exhaustion_fix_0",
              "content": "# Runbook: Database Connection Exhaustion ## Severity **Level:** P1 - Critical **Response Time:** 15 minutes ## Symptoms - Connection pool at 100% - Queries timing out - 'too many connections' errors in logs ## Diagnosis ### Step 1 Check active connections: `SELECT count(*) FROM pg_stat_activity` ### Step 2 Identify long-running queries: `SELECT pid, now() - pg_stat_activity.query_start AS duration, query FROM pg_stat_activity WHERE state != 'idle' ORDER BY duration DESC` ### Step 3 Check PgBouncer stats: `SHOW POOLS` ### Step 4 Verify connection limits: `SHOW max_connections` ## Resolution ### Step 1 Kill long-running queries: `SELECT pg_terminate_backend(pid) FROM pg_stat_activity WHERE duration > interval '5 minutes'` ### Step 2 Increase PgBouncer pool size temporarily: update pgbouncer.ini ### Step 3 Restart affected services to release connections: `kubectl rollout restart deployment/api` ### Step 4 Scale down non-critical services to free connections ## Escalation If connections remain exhausted, escalate to DBA. Consider enabling connection queueing. ## Post-Incident - Document timeline in incident log - Schedule post-mortem within 48 hours - Update runbook if new patterns discovered",
              "score": null
            },
            {
              "doc_id": "arch_monitoring_stack",
              "chunk_id": "arch_monitoring_stack_fix_0",
              "content": "# Monitoring Stack Architecture ## Overview The Monitoring Stack provides observability into system health, performance, and errors. Includes metrics, logs, traces, and alerting. ## Design Principles - **Observability**: Every component emits metrics and traces - **Actionable Alerts**: Alerts include runbook links ## Components ### Metrics Collects time-series metrics. Retention is 15 days local, 1 year in object storage. **Technology:** Prometheus with Thanos **Scaling:** Federated with long-term storage ### Logging Centralized log aggregation. Supports structured JSON logs with label indexing. **Technology:** Loki with Grafana **Scaling:** Distributed with sharding ### Tracing Distributed request tracing. Trace retention is 7 days. **Technology:** Jaeger with OpenTelemetry **Scaling:** Sampling at 10% ### Alerting Routes alerts to on-call engineers. Supports escalation policies and silencing. **Technology:** Alertmanager with PagerDuty **Scaling:** HA pair ## Data Flow Application \u2192 Collector \u2192 Storage \u2192 Query \u2192 Dashboard/Alert ## Performance Characteristics - **Latency P50:** N/A - **Latency P99:** N/A - **Throughput:** 1M metrics/second, 100GB logs/day",
              "score": null
            },
            {
              "doc_id": "runbook_high_cpu_usage",
              "chunk_id": "runbook_high_cpu_usage_fix_0",
              "content": "# Runbook: High CPU Usage ## Severity **Level:** P1 - Critical **Response Time:** 15 minutes ## Symptoms - CPU utilization > 80% sustained - Increased response latency - Autoscaler at maximum replicas ## Diagnosis ### Step 1 Check current CPU usage: `kubectl top pods -n production` ### Step 2 Identify hot pods: `kubectl top pods --sort-by=cpu -n production | head -10` ### Step 3 Check for recent deployments: `kubectl rollout history deployment/api -n production` ### Step 4 Profile the application: `cloudflow debug profile --pod api-xyz --duration 60s` ## Resolution ### Step 1 Scale up if traffic-related: `kubectl scale deployment/api --replicas=10 -n production` ### Step 2 Restart pods if memory leak suspected: `kubectl rollout restart deployment/api -n production` ### Step 3 Roll back recent deployment if regression: `kubectl rollout undo deployment/api -n production` ### Step 4 Add resource limits if unbounded: update deployment with CPU limits ## Escalation If CPU remains high after scaling, escalate to platform team. Page on-call if affecting users. ## Post-Incident - Document timeline in incident log - Schedule post-mortem within 48 hours - Update runbook if new patterns discovered",
              "score": null
            }
          ],
          "latency_ms": 3.885124999214895,
          "query_id": "troubleshoot_031",
          "dimension": "contextual",
          "query": "our Java app keeps getting OOM killed after processing large files, need to figure out if it's a leak or just undersized"
        },
        {
          "key_facts": [
            "OOMKilled",
            "resources.limits.memory",
            "HeapDumpOnOutOfMemoryError"
          ],
          "found_facts": [
            "OOMKilled",
            "resources.limits.memory",
            "HeapDumpOnOutOfMemoryError"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "runbook_out_of_memory_oom_kill"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "runbook_out_of_memory_oom_kills",
              "chunk_id": "runbook_out_of_memory_oom_kills_fix_0",
              "content": "# Runbook: Out of Memory (OOM) Kills ## Severity **Level:** P3 - Medium **Response Time:** 15 minutes ## Symptoms - Pods restarting with OOMKilled status - Memory usage at limit - Application slowdown before restart ## Diagnosis ### Step 1 Check OOM events: `kubectl get events -n production --field-selector reason=OOMKilled` ### Step 2 Review memory usage: `kubectl top pods -n production --sort-by=memory` ### Step 3 Check container limits: `kubectl describe pod <pod-name> -n production | grep -A5 Limits` ### Step 4 Analyze heap dumps if available: `cloudflow debug heapdump --pod <pod-name>` ## Resolution ### Step 1 Increase memory limit: update deployment resources.limits.memory ### Step 2 Add memory request equal to limit to guarantee allocation ### Step 3 Enable JVM heap dump on OOM: add -XX:+HeapDumpOnOutOfMemoryError ### Step 4 Investigate memory leaks with profiling tools ### Step 5 Consider horizontal scaling instead of vertical ## Escalation If OOMs continue after limit increase, escalate to development team for memory leak investigation. ## Post-Incident - Document timeline in incident log - Schedule post-mortem within 48 hours - Update runbook if new patterns discovered",
              "score": null
            },
            {
              "doc_id": "adr_004",
              "chunk_id": "adr_004_fix_0",
              "content": "# ADR-004: Use Redis for Caching and Session Storage **Status:** Accepted **Date:** 2024-04-05 ## Context We need a fast data store for caching API responses, session data, and rate limiting counters. ## Options Considered ### Option 1: Redis Cluster In-memory data structure store. **Pros:** - Sub-millisecond latency - Rich data structures - Pub/sub support - Clustering **Cons:** - Memory cost - Persistence complexity - Cluster management ### Option 2: Memcached Simple key-value cache. **Pros:** - Simple operation - Multi-threaded - Lower memory overhead **Cons:** - No persistence - Limited data types - No clustering ### Option 3: Application-level cache In-process caching only. **Pros:** - No external dependency - Fastest possible - Simple **Cons:** - Not shared across pods - Memory pressure - Cold starts ## Decision We will use Redis Cluster for caching with a 6-node cluster for high availability. ## Consequences - Implement cache-aside pattern - Set appropriate TTLs - Monitor memory usage - Plan for cache stampede prevention",
              "score": null
            },
            {
              "doc_id": "runbook_high_cpu_usage",
              "chunk_id": "runbook_high_cpu_usage_fix_0",
              "content": "# Runbook: High CPU Usage ## Severity **Level:** P1 - Critical **Response Time:** 15 minutes ## Symptoms - CPU utilization > 80% sustained - Increased response latency - Autoscaler at maximum replicas ## Diagnosis ### Step 1 Check current CPU usage: `kubectl top pods -n production` ### Step 2 Identify hot pods: `kubectl top pods --sort-by=cpu -n production | head -10` ### Step 3 Check for recent deployments: `kubectl rollout history deployment/api -n production` ### Step 4 Profile the application: `cloudflow debug profile --pod api-xyz --duration 60s` ## Resolution ### Step 1 Scale up if traffic-related: `kubectl scale deployment/api --replicas=10 -n production` ### Step 2 Restart pods if memory leak suspected: `kubectl rollout restart deployment/api -n production` ### Step 3 Roll back recent deployment if regression: `kubectl rollout undo deployment/api -n production` ### Step 4 Add resource limits if unbounded: update deployment with CPU limits ## Escalation If CPU remains high after scaling, escalate to platform team. Page on-call if affecting users. ## Post-Incident - Document timeline in incident log - Schedule post-mortem within 48 hours - Update runbook if new patterns discovered",
              "score": null
            },
            {
              "doc_id": "arch_caching_layer",
              "chunk_id": "arch_caching_layer_fix_0",
              "content": "# Caching Layer Architecture ## Overview The Caching Layer reduces latency and database load by caching frequently accessed data. Uses a multi-tier strategy with local and distributed caches. ## Design Principles - **Cache Invalidation**: Explicit invalidation on writes - **Graceful Degradation**: System works without cache ## Components ### Local Cache First-level cache with 100ms TTL. Reduces Redis round-trips for hot data. **Technology:** Ristretto in-memory cache **Scaling:** Per-pod memory allocation ### Distributed Cache Second-level cache with configurable TTL. Maximum key size 512MB. **Technology:** Redis Cluster 7.x **Scaling:** 6-node cluster with sharding ### CDN Caches static assets and API responses. Cache hit ratio target 95%. **Technology:** CloudFront with Lambda@Edge **Scaling:** Global edge locations ## Data Flow Request \u2192 Local Cache \u2192 Redis \u2192 Database \u2192 Cache Population ## Performance Characteristics - **Latency P50:** 1ms (hit), 50ms (miss) - **Latency P99:** 5ms (hit), 200ms (miss) - **Throughput:** 100000 reads/second",
              "score": null
            },
            {
              "doc_id": "release_3_2_0",
              "chunk_id": "release_3_2_0_fix_0",
              "content": "# Release Notes - v3.2.0 **Release Date:** 2024-01-15 ## New Features - Workflow versioning with rollback support - New Python SDK with async support - Bulk operations API for workflows - Custom retry policies per step ## Bug Fixes - Fixed race condition in concurrent execution handler - Resolved memory leak in long-running workflows - Fixed timezone handling in scheduled triggers ## Breaking Changes - **BREAKING:** API v1 endpoints deprecated, will be removed in v4.0 - **BREAKING:** Minimum Python version now 3.9 ## Upgrade Instructions ```bash cloudflow upgrade --version 3.2.0 ``` ## Known Issues None at this time.",
              "score": null
            }
          ],
          "latency_ms": 4.829643999983091,
          "query_id": "troubleshoot_031",
          "dimension": "negation",
          "query": "why do my containers keep dying from memory issues even with 4GB limit?"
        },
        {
          "key_facts": [
            "kubectl logs",
            "circuit breaker",
            "kubectl get endpoints"
          ],
          "found_facts": [
            "kubectl logs",
            "circuit breaker",
            "kubectl get endpoints"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "runbook_api_gateway_5xx_error"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "config_api_gateway",
              "chunk_id": "config_api_gateway_fix_0",
              "content": "# API Gateway Configuration Reference ## Overview This document describes all configuration options for API Gateway. ## Environment Variables | Variable | Type | Default | Description | |----------|------|---------|-------------| | `GATEWAY_PORT` | integer | 8000 | Gateway listen port | | `GATEWAY_RATE_LIMIT` | integer | 100 | Requests per minute per user | | `GATEWAY_TIMEOUT_MS` | integer | 30000 | Request timeout in milliseconds | | `GATEWAY_MAX_BODY_SIZE` | string | 10MB | Maximum request body size | | `GATEWAY_CORS_ORIGINS` | string | * | Allowed CORS origins | | `GATEWAY_SSL_ENABLED` | boolean | true | Enable TLS termination | ## Configuration File Configuration can also be provided via `api gateway.yaml`: ```yaml gateway_port: 8000 gateway_rate_limit: 100 gateway_timeout_ms: 30000 gateway_max_body_size: 10MB gateway_cors_origins: * ``` ## Validation Configuration is validated at startup. Invalid configuration will prevent the service from starting.",
              "score": null
            },
            {
              "doc_id": "runbook_api_gateway_5xx_errors",
              "chunk_id": "runbook_api_gateway_5xx_errors_fix_0",
              "content": "# Runbook: API Gateway 5xx Errors ## Severity **Level:** P3 - Medium **Response Time:** 1 hour ## Symptoms - Error rate > 1% - 5xx responses in access logs - Downstream service failures ## Diagnosis ### Step 1 Check Kong error logs: `kubectl logs -l app=kong -n kong --tail=100 | grep -E '5[0-9]{2}'` ### Step 2 Verify backend health: `kubectl get pods -n production -o wide` ### Step 3 Check service endpoints: `kubectl get endpoints -n production` ### Step 4 Test backend directly: `kubectl exec -it kong-xxx -n kong -- curl http://api.production.svc:8080/health` ## Resolution ### Step 1 Restart unhealthy backends: `kubectl rollout restart deployment/api -n production` ### Step 2 Enable circuit breaker if cascading: update Kong circuit-breaker plugin ### Step 3 Scale up backends: `kubectl scale deployment/api --replicas=5 -n production` ### Step 4 Check for resource exhaustion on backend pods ### Step 5 Verify network policies allow traffic ## Escalation If 5xx persists > 15 minutes, page on-call. May indicate infrastructure issue. ## Post-Incident - Document timeline in incident log - Schedule post-mortem within 48 hours - Update runbook if new patterns discovered",
              "score": null
            },
            {
              "doc_id": "arch_api_gateway",
              "chunk_id": "arch_api_gateway_fix_0",
              "content": "# API Gateway Architecture ## Overview The API Gateway handles all incoming HTTP requests, providing authentication, rate limiting, and request routing. Built on Kong with custom plugins for CloudFlow-specific functionality. ## Design Principles - **Zero Trust**: Every request is authenticated and authorized - **Defense in Depth**: Multiple layers of security validation ## Components ### Kong Gateway Primary ingress controller. Handles TLS termination, request routing, and plugin execution. **Technology:** Kong 3.4 on Kubernetes **Scaling:** Horizontal with HPA ### Auth Plugin Validates JWT tokens, API keys, and OAuth2 flows. Token validation latency under 5ms. **Technology:** Custom Lua plugin **Scaling:** Stateless ### Rate Limiter Enforces per-user and per-IP rate limits. Default limit is 100 requests per minute. **Technology:** Redis-backed sliding window **Scaling:** Shared Redis cluster ## Data Flow Client \u2192 TLS \u2192 Kong \u2192 Auth \u2192 Rate Limit \u2192 Backend Services ## Performance Characteristics - **Latency P50:** 12ms - **Latency P99:** 45ms - **Throughput:** 50000 requests/second",
              "score": null
            },
            {
              "doc_id": "api_integrations",
              "chunk_id": "api_integrations_fix_0",
              "content": "# Integrations API Reference ## Overview The Integrations API provides programmatic access to integrations functionality. Base URL: `https://api.cloudflow.io/v1/integrations` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /integrations List available integrations. **Parameters:** - `category` (string): Filter by category: storage, notification, database **Response:** - Success: `200` - Returns array of integration objects - Error: `400` - Bad Request **Example:** ```json {\"integrations\": [{\"id\": \"int_s3\", \"name\": \"Amazon S3\", \"category\": \"storage\", \"status\": \"connected\"}]} ``` ### POST /integrations/{id}/connect Connect an integration. **Parameters:** - `credentials` (object): Integration-specific credentials **Response:** - Success: `200` - Integration connected - Error: `400` - Bad Request **Example:** ```json {\"id\": \"int_s3\", \"status\": \"connected\", \"connected_at\": \"2024-01-15T10:00:00Z\"} ```",
              "score": null
            },
            {
              "doc_id": "api_billing",
              "chunk_id": "api_billing_fix_0",
              "content": "# Billing API Reference ## Overview The Billing API provides programmatic access to billing functionality. Base URL: `https://api.cloudflow.io/v1/billing` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /billing/usage Get current billing period usage. **Parameters:** - `breakdown` (boolean): Include detailed breakdown by resource **Response:** - Success: `200` - Returns usage summary - Error: `400` - Bad Request **Example:** ```json {\"period\": \"2024-01\", \"executions\": 15000, \"compute_minutes\": 2500, \"storage_gb\": 50} ``` ### GET /billing/invoices List past invoices. **Parameters:** - `year` (integer): Filter by year **Response:** - Success: `200` - Returns array of invoice objects - Error: `400` - Bad Request **Example:** ```json {\"invoices\": [{\"id\": \"inv_202401\", \"amount\": 299.00, \"status\": \"paid\", \"date\": \"2024-01-01\"}]} ```",
              "score": null
            }
          ],
          "latency_ms": 3.8476350000564707,
          "query_id": "troubleshoot_032",
          "dimension": "original",
          "query": "How to troubleshoot API Gateway 5xx errors?"
        },
        {
          "key_facts": [
            "kubectl logs",
            "circuit breaker",
            "kubectl get endpoints"
          ],
          "found_facts": [
            "kubectl logs",
            "circuit breaker",
            "kubectl get endpoints"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "runbook_api_gateway_5xx_error"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "howto_debug_failed_execution",
              "chunk_id": "howto_debug_failed_execution_fix_0",
              "content": "# How to Debug Failed Execution ## Prerequisites - Failed execution ID ## Steps ### Step 1: Get execution details Retrieve the full execution record including step outputs. ```bash cloudflow executions get ex_123 --include-steps ``` ### Step 2: View step logs Check logs for the failed step. ```bash cloudflow executions logs ex_123 --step 3 ``` ### Step 3: Inspect input/output Review the data passed between steps. ```bash cloudflow executions inspect ex_123 --step 3 ``` ### Step 4: Enable debug mode Re-run with verbose logging enabled. ```bash cloudflow workflows execute wf_123 --debug ``` ### Step 5: Check error patterns Look for similar failures in recent executions. ```bash cloudflow executions list --status failed --since 24h ``` ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            },
            {
              "doc_id": "runbook_api_gateway_5xx_errors",
              "chunk_id": "runbook_api_gateway_5xx_errors_fix_0",
              "content": "# Runbook: API Gateway 5xx Errors ## Severity **Level:** P3 - Medium **Response Time:** 1 hour ## Symptoms - Error rate > 1% - 5xx responses in access logs - Downstream service failures ## Diagnosis ### Step 1 Check Kong error logs: `kubectl logs -l app=kong -n kong --tail=100 | grep -E '5[0-9]{2}'` ### Step 2 Verify backend health: `kubectl get pods -n production -o wide` ### Step 3 Check service endpoints: `kubectl get endpoints -n production` ### Step 4 Test backend directly: `kubectl exec -it kong-xxx -n kong -- curl http://api.production.svc:8080/health` ## Resolution ### Step 1 Restart unhealthy backends: `kubectl rollout restart deployment/api -n production` ### Step 2 Enable circuit breaker if cascading: update Kong circuit-breaker plugin ### Step 3 Scale up backends: `kubectl scale deployment/api --replicas=5 -n production` ### Step 4 Check for resource exhaustion on backend pods ### Step 5 Verify network policies allow traffic ## Escalation If 5xx persists > 15 minutes, page on-call. May indicate infrastructure issue. ## Post-Incident - Document timeline in incident log - Schedule post-mortem within 48 hours - Update runbook if new patterns discovered",
              "score": null
            },
            {
              "doc_id": "runbook_high_cpu_usage",
              "chunk_id": "runbook_high_cpu_usage_fix_0",
              "content": "# Runbook: High CPU Usage ## Severity **Level:** P1 - Critical **Response Time:** 15 minutes ## Symptoms - CPU utilization > 80% sustained - Increased response latency - Autoscaler at maximum replicas ## Diagnosis ### Step 1 Check current CPU usage: `kubectl top pods -n production` ### Step 2 Identify hot pods: `kubectl top pods --sort-by=cpu -n production | head -10` ### Step 3 Check for recent deployments: `kubectl rollout history deployment/api -n production` ### Step 4 Profile the application: `cloudflow debug profile --pod api-xyz --duration 60s` ## Resolution ### Step 1 Scale up if traffic-related: `kubectl scale deployment/api --replicas=10 -n production` ### Step 2 Restart pods if memory leak suspected: `kubectl rollout restart deployment/api -n production` ### Step 3 Roll back recent deployment if regression: `kubectl rollout undo deployment/api -n production` ### Step 4 Add resource limits if unbounded: update deployment with CPU limits ## Escalation If CPU remains high after scaling, escalate to platform team. Page on-call if affecting users. ## Post-Incident - Document timeline in incident log - Schedule post-mortem within 48 hours - Update runbook if new patterns discovered",
              "score": null
            },
            {
              "doc_id": "runbook_database_connection_exhaustion",
              "chunk_id": "runbook_database_connection_exhaustion_fix_0",
              "content": "# Runbook: Database Connection Exhaustion ## Severity **Level:** P1 - Critical **Response Time:** 15 minutes ## Symptoms - Connection pool at 100% - Queries timing out - 'too many connections' errors in logs ## Diagnosis ### Step 1 Check active connections: `SELECT count(*) FROM pg_stat_activity` ### Step 2 Identify long-running queries: `SELECT pid, now() - pg_stat_activity.query_start AS duration, query FROM pg_stat_activity WHERE state != 'idle' ORDER BY duration DESC` ### Step 3 Check PgBouncer stats: `SHOW POOLS` ### Step 4 Verify connection limits: `SHOW max_connections` ## Resolution ### Step 1 Kill long-running queries: `SELECT pg_terminate_backend(pid) FROM pg_stat_activity WHERE duration > interval '5 minutes'` ### Step 2 Increase PgBouncer pool size temporarily: update pgbouncer.ini ### Step 3 Restart affected services to release connections: `kubectl rollout restart deployment/api` ### Step 4 Scale down non-critical services to free connections ## Escalation If connections remain exhausted, escalate to DBA. Consider enabling connection queueing. ## Post-Incident - Document timeline in incident log - Schedule post-mortem within 48 hours - Update runbook if new patterns discovered",
              "score": null
            },
            {
              "doc_id": "runbook_ssl_certificate_expiry",
              "chunk_id": "runbook_ssl_certificate_expiry_fix_0",
              "content": "# Runbook: SSL Certificate Expiry ## Severity **Level:** P3 - Medium **Response Time:** 15 minutes ## Symptoms - Certificate expiring within 7 days - SSL handshake failures - Browser security warnings ## Diagnosis ### Step 1 Check certificate expiry: `echo | openssl s_client -connect api.cloudflow.io:443 2>/dev/null | openssl x509 -noout -dates` ### Step 2 Verify cert-manager status: `kubectl get certificates -n production` ### Step 3 Check cert-manager logs: `kubectl logs -l app=cert-manager -n cert-manager --tail=50` ### Step 4 Verify DNS is correct: `dig +short api.cloudflow.io` ## Resolution ### Step 1 Trigger manual renewal: `kubectl delete certificate api-cert -n production && kubectl apply -f certificate.yaml` ### Step 2 Check rate limits: Let's Encrypt has 50 certs/domain/week limit ### Step 3 Use staging issuer for testing: switch to letsencrypt-staging ClusterIssuer ### Step 4 If Let's Encrypt fails, manually upload certificate ## Escalation If certificate cannot be renewed, escalate to security team immediately. Consider using backup certificate. ## Post-Incident - Document timeline in incident log - Schedule post-mortem within 48 hours - Update runbook if new patterns discovered",
              "score": null
            }
          ],
          "latency_ms": 3.3346700001857243,
          "query_id": "troubleshoot_032",
          "dimension": "synonym",
          "query": "How do I debug server errors from the ingress layer?"
        },
        {
          "key_facts": [
            "kubectl logs",
            "circuit breaker",
            "kubectl get endpoints"
          ],
          "found_facts": [
            "kubectl logs",
            "circuit breaker",
            "kubectl get endpoints"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "runbook_api_gateway_5xx_error"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "runbook_api_gateway_5xx_errors",
              "chunk_id": "runbook_api_gateway_5xx_errors_fix_0",
              "content": "# Runbook: API Gateway 5xx Errors ## Severity **Level:** P3 - Medium **Response Time:** 1 hour ## Symptoms - Error rate > 1% - 5xx responses in access logs - Downstream service failures ## Diagnosis ### Step 1 Check Kong error logs: `kubectl logs -l app=kong -n kong --tail=100 | grep -E '5[0-9]{2}'` ### Step 2 Verify backend health: `kubectl get pods -n production -o wide` ### Step 3 Check service endpoints: `kubectl get endpoints -n production` ### Step 4 Test backend directly: `kubectl exec -it kong-xxx -n kong -- curl http://api.production.svc:8080/health` ## Resolution ### Step 1 Restart unhealthy backends: `kubectl rollout restart deployment/api -n production` ### Step 2 Enable circuit breaker if cascading: update Kong circuit-breaker plugin ### Step 3 Scale up backends: `kubectl scale deployment/api --replicas=5 -n production` ### Step 4 Check for resource exhaustion on backend pods ### Step 5 Verify network policies allow traffic ## Escalation If 5xx persists > 15 minutes, page on-call. May indicate infrastructure issue. ## Post-Incident - Document timeline in incident log - Schedule post-mortem within 48 hours - Update runbook if new patterns discovered",
              "score": null
            },
            {
              "doc_id": "arch_api_gateway",
              "chunk_id": "arch_api_gateway_fix_0",
              "content": "# API Gateway Architecture ## Overview The API Gateway handles all incoming HTTP requests, providing authentication, rate limiting, and request routing. Built on Kong with custom plugins for CloudFlow-specific functionality. ## Design Principles - **Zero Trust**: Every request is authenticated and authorized - **Defense in Depth**: Multiple layers of security validation ## Components ### Kong Gateway Primary ingress controller. Handles TLS termination, request routing, and plugin execution. **Technology:** Kong 3.4 on Kubernetes **Scaling:** Horizontal with HPA ### Auth Plugin Validates JWT tokens, API keys, and OAuth2 flows. Token validation latency under 5ms. **Technology:** Custom Lua plugin **Scaling:** Stateless ### Rate Limiter Enforces per-user and per-IP rate limits. Default limit is 100 requests per minute. **Technology:** Redis-backed sliding window **Scaling:** Shared Redis cluster ## Data Flow Client \u2192 TLS \u2192 Kong \u2192 Auth \u2192 Rate Limit \u2192 Backend Services ## Performance Characteristics - **Latency P50:** 12ms - **Latency P99:** 45ms - **Throughput:** 50000 requests/second",
              "score": null
            },
            {
              "doc_id": "config_api_gateway",
              "chunk_id": "config_api_gateway_fix_0",
              "content": "# API Gateway Configuration Reference ## Overview This document describes all configuration options for API Gateway. ## Environment Variables | Variable | Type | Default | Description | |----------|------|---------|-------------| | `GATEWAY_PORT` | integer | 8000 | Gateway listen port | | `GATEWAY_RATE_LIMIT` | integer | 100 | Requests per minute per user | | `GATEWAY_TIMEOUT_MS` | integer | 30000 | Request timeout in milliseconds | | `GATEWAY_MAX_BODY_SIZE` | string | 10MB | Maximum request body size | | `GATEWAY_CORS_ORIGINS` | string | * | Allowed CORS origins | | `GATEWAY_SSL_ENABLED` | boolean | true | Enable TLS termination | ## Configuration File Configuration can also be provided via `api gateway.yaml`: ```yaml gateway_port: 8000 gateway_rate_limit: 100 gateway_timeout_ms: 30000 gateway_max_body_size: 10MB gateway_cors_origins: * ``` ## Validation Configuration is validated at startup. Invalid configuration will prevent the service from starting.",
              "score": null
            },
            {
              "doc_id": "adr_007",
              "chunk_id": "adr_007_fix_0",
              "content": "# ADR-007: Implement Rate Limiting at API Gateway **Status:** Accepted **Date:** 2024-09-03 ## Context We need to protect our services from abuse and ensure fair resource usage across customers. ## Options Considered ### Option 1: Sliding window at gateway Rate limit at API gateway level. **Pros:** - Centralized enforcement - Low latency - Easy configuration **Cons:** - Gateway becomes bottleneck - Coarse granularity - Redis dependency ### Option 2: Per-service limiting Each service implements its own limits. **Pros:** - Fine-grained control - Service autonomy - No single point of failure **Cons:** - Inconsistent implementation - Higher complexity - Distributed state ### Option 3: Token bucket Token-based rate limiting. **Pros:** - Allows bursts - Smooth rate limiting - Well understood **Cons:** - More complex - Harder to explain to users - State management ## Decision We will implement sliding window rate limiting at the Kong API Gateway using Redis. ## Consequences - Default limit: 100 requests per minute - Implement per-plan limits - Return rate limit headers - Add retry-after on 429",
              "score": null
            },
            {
              "doc_id": "api_audit",
              "chunk_id": "api_audit_fix_0",
              "content": "# Audit API Reference ## Overview The Audit API provides programmatic access to audit functionality. Base URL: `https://api.cloudflow.io/v1/audit` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /audit/logs Retrieve audit logs for compliance. **Parameters:** - `start_date` (date): Start date for log range - `end_date` (date): End date for log range - `actor` (string): Filter by user ID **Response:** - Success: `200` - Returns array of audit log entries - Error: `400` - Bad Request **Example:** ```json {\"logs\": [{\"timestamp\": \"2024-01-15T10:00:00Z\", \"actor\": \"usr_001\", \"action\": \"workflow.created\", \"resource\": \"wf_123\"}]} ```",
              "score": null
            }
          ],
          "latency_ms": 3.4145580011681886,
          "query_id": "troubleshoot_032",
          "dimension": "problem",
          "query": "error rate jumped to 5% and gateway logs show 502 Bad Gateway errors from backend services"
        },
        {
          "key_facts": [
            "kubectl logs",
            "circuit breaker",
            "kubectl get endpoints"
          ],
          "found_facts": [
            "kubectl logs",
            "circuit breaker",
            "kubectl get endpoints"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "runbook_api_gateway_5xx_error"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "config_api_gateway",
              "chunk_id": "config_api_gateway_fix_0",
              "content": "# API Gateway Configuration Reference ## Overview This document describes all configuration options for API Gateway. ## Environment Variables | Variable | Type | Default | Description | |----------|------|---------|-------------| | `GATEWAY_PORT` | integer | 8000 | Gateway listen port | | `GATEWAY_RATE_LIMIT` | integer | 100 | Requests per minute per user | | `GATEWAY_TIMEOUT_MS` | integer | 30000 | Request timeout in milliseconds | | `GATEWAY_MAX_BODY_SIZE` | string | 10MB | Maximum request body size | | `GATEWAY_CORS_ORIGINS` | string | * | Allowed CORS origins | | `GATEWAY_SSL_ENABLED` | boolean | true | Enable TLS termination | ## Configuration File Configuration can also be provided via `api gateway.yaml`: ```yaml gateway_port: 8000 gateway_rate_limit: 100 gateway_timeout_ms: 30000 gateway_max_body_size: 10MB gateway_cors_origins: * ``` ## Validation Configuration is validated at startup. Invalid configuration will prevent the service from starting.",
              "score": null
            },
            {
              "doc_id": "arch_api_gateway",
              "chunk_id": "arch_api_gateway_fix_0",
              "content": "# API Gateway Architecture ## Overview The API Gateway handles all incoming HTTP requests, providing authentication, rate limiting, and request routing. Built on Kong with custom plugins for CloudFlow-specific functionality. ## Design Principles - **Zero Trust**: Every request is authenticated and authorized - **Defense in Depth**: Multiple layers of security validation ## Components ### Kong Gateway Primary ingress controller. Handles TLS termination, request routing, and plugin execution. **Technology:** Kong 3.4 on Kubernetes **Scaling:** Horizontal with HPA ### Auth Plugin Validates JWT tokens, API keys, and OAuth2 flows. Token validation latency under 5ms. **Technology:** Custom Lua plugin **Scaling:** Stateless ### Rate Limiter Enforces per-user and per-IP rate limits. Default limit is 100 requests per minute. **Technology:** Redis-backed sliding window **Scaling:** Shared Redis cluster ## Data Flow Client \u2192 TLS \u2192 Kong \u2192 Auth \u2192 Rate Limit \u2192 Backend Services ## Performance Characteristics - **Latency P50:** 12ms - **Latency P99:** 45ms - **Throughput:** 50000 requests/second",
              "score": null
            },
            {
              "doc_id": "runbook_api_gateway_5xx_errors",
              "chunk_id": "runbook_api_gateway_5xx_errors_fix_0",
              "content": "# Runbook: API Gateway 5xx Errors ## Severity **Level:** P3 - Medium **Response Time:** 1 hour ## Symptoms - Error rate > 1% - 5xx responses in access logs - Downstream service failures ## Diagnosis ### Step 1 Check Kong error logs: `kubectl logs -l app=kong -n kong --tail=100 | grep -E '5[0-9]{2}'` ### Step 2 Verify backend health: `kubectl get pods -n production -o wide` ### Step 3 Check service endpoints: `kubectl get endpoints -n production` ### Step 4 Test backend directly: `kubectl exec -it kong-xxx -n kong -- curl http://api.production.svc:8080/health` ## Resolution ### Step 1 Restart unhealthy backends: `kubectl rollout restart deployment/api -n production` ### Step 2 Enable circuit breaker if cascading: update Kong circuit-breaker plugin ### Step 3 Scale up backends: `kubectl scale deployment/api --replicas=5 -n production` ### Step 4 Check for resource exhaustion on backend pods ### Step 5 Verify network policies allow traffic ## Escalation If 5xx persists > 15 minutes, page on-call. May indicate infrastructure issue. ## Post-Incident - Document timeline in incident log - Schedule post-mortem within 48 hours - Update runbook if new patterns discovered",
              "score": null
            },
            {
              "doc_id": "release_3_0_0",
              "chunk_id": "release_3_0_0_fix_0",
              "content": "# Release Notes - v3.0.0 **Release Date:** 2023-10-15 ## New Features - Complete UI redesign - New workflow editor with visual builder - Real-time execution monitoring - Team workspaces ## Bug Fixes - Fixed critical security issue in token validation - Resolved data loss bug in failed migrations ## Breaking Changes - **BREAKING:** Authentication API completely rewritten - **BREAKING:** Legacy webhook format no longer supported - **BREAKING:** Minimum Node.js version now 18 ## Upgrade Instructions ```bash cloudflow upgrade --version 3.0.0 ``` ## Known Issues None at this time.",
              "score": null
            },
            {
              "doc_id": "adr_007",
              "chunk_id": "adr_007_fix_0",
              "content": "# ADR-007: Implement Rate Limiting at API Gateway **Status:** Accepted **Date:** 2024-09-03 ## Context We need to protect our services from abuse and ensure fair resource usage across customers. ## Options Considered ### Option 1: Sliding window at gateway Rate limit at API gateway level. **Pros:** - Centralized enforcement - Low latency - Easy configuration **Cons:** - Gateway becomes bottleneck - Coarse granularity - Redis dependency ### Option 2: Per-service limiting Each service implements its own limits. **Pros:** - Fine-grained control - Service autonomy - No single point of failure **Cons:** - Inconsistent implementation - Higher complexity - Distributed state ### Option 3: Token bucket Token-based rate limiting. **Pros:** - Allows bursts - Smooth rate limiting - Well understood **Cons:** - More complex - Harder to explain to users - State management ## Decision We will implement sliding window rate limiting at the Kong API Gateway using Redis. ## Consequences - Default limit: 100 requests per minute - Implement per-plan limits - Return rate limit headers - Add retry-after on 429",
              "score": null
            }
          ],
          "latency_ms": 3.5523349997674813,
          "query_id": "troubleshoot_032",
          "dimension": "casual",
          "query": "gateway throwing 500s, backends look fine though"
        },
        {
          "key_facts": [
            "kubectl logs",
            "circuit breaker",
            "kubectl get endpoints"
          ],
          "found_facts": [
            "kubectl logs",
            "circuit breaker",
            "kubectl get endpoints"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "runbook_api_gateway_5xx_error"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "runbook_api_gateway_5xx_errors",
              "chunk_id": "runbook_api_gateway_5xx_errors_fix_0",
              "content": "# Runbook: API Gateway 5xx Errors ## Severity **Level:** P3 - Medium **Response Time:** 1 hour ## Symptoms - Error rate > 1% - 5xx responses in access logs - Downstream service failures ## Diagnosis ### Step 1 Check Kong error logs: `kubectl logs -l app=kong -n kong --tail=100 | grep -E '5[0-9]{2}'` ### Step 2 Verify backend health: `kubectl get pods -n production -o wide` ### Step 3 Check service endpoints: `kubectl get endpoints -n production` ### Step 4 Test backend directly: `kubectl exec -it kong-xxx -n kong -- curl http://api.production.svc:8080/health` ## Resolution ### Step 1 Restart unhealthy backends: `kubectl rollout restart deployment/api -n production` ### Step 2 Enable circuit breaker if cascading: update Kong circuit-breaker plugin ### Step 3 Scale up backends: `kubectl scale deployment/api --replicas=5 -n production` ### Step 4 Check for resource exhaustion on backend pods ### Step 5 Verify network policies allow traffic ## Escalation If 5xx persists > 15 minutes, page on-call. May indicate infrastructure issue. ## Post-Incident - Document timeline in incident log - Schedule post-mortem within 48 hours - Update runbook if new patterns discovered",
              "score": null
            },
            {
              "doc_id": "arch_api_gateway",
              "chunk_id": "arch_api_gateway_fix_0",
              "content": "# API Gateway Architecture ## Overview The API Gateway handles all incoming HTTP requests, providing authentication, rate limiting, and request routing. Built on Kong with custom plugins for CloudFlow-specific functionality. ## Design Principles - **Zero Trust**: Every request is authenticated and authorized - **Defense in Depth**: Multiple layers of security validation ## Components ### Kong Gateway Primary ingress controller. Handles TLS termination, request routing, and plugin execution. **Technology:** Kong 3.4 on Kubernetes **Scaling:** Horizontal with HPA ### Auth Plugin Validates JWT tokens, API keys, and OAuth2 flows. Token validation latency under 5ms. **Technology:** Custom Lua plugin **Scaling:** Stateless ### Rate Limiter Enforces per-user and per-IP rate limits. Default limit is 100 requests per minute. **Technology:** Redis-backed sliding window **Scaling:** Shared Redis cluster ## Data Flow Client \u2192 TLS \u2192 Kong \u2192 Auth \u2192 Rate Limit \u2192 Backend Services ## Performance Characteristics - **Latency P50:** 12ms - **Latency P99:** 45ms - **Throughput:** 50000 requests/second",
              "score": null
            },
            {
              "doc_id": "config_api_gateway",
              "chunk_id": "config_api_gateway_fix_0",
              "content": "# API Gateway Configuration Reference ## Overview This document describes all configuration options for API Gateway. ## Environment Variables | Variable | Type | Default | Description | |----------|------|---------|-------------| | `GATEWAY_PORT` | integer | 8000 | Gateway listen port | | `GATEWAY_RATE_LIMIT` | integer | 100 | Requests per minute per user | | `GATEWAY_TIMEOUT_MS` | integer | 30000 | Request timeout in milliseconds | | `GATEWAY_MAX_BODY_SIZE` | string | 10MB | Maximum request body size | | `GATEWAY_CORS_ORIGINS` | string | * | Allowed CORS origins | | `GATEWAY_SSL_ENABLED` | boolean | true | Enable TLS termination | ## Configuration File Configuration can also be provided via `api gateway.yaml`: ```yaml gateway_port: 8000 gateway_rate_limit: 100 gateway_timeout_ms: 30000 gateway_max_body_size: 10MB gateway_cors_origins: * ``` ## Validation Configuration is validated at startup. Invalid configuration will prevent the service from starting.",
              "score": null
            },
            {
              "doc_id": "adr_007",
              "chunk_id": "adr_007_fix_0",
              "content": "# ADR-007: Implement Rate Limiting at API Gateway **Status:** Accepted **Date:** 2024-09-03 ## Context We need to protect our services from abuse and ensure fair resource usage across customers. ## Options Considered ### Option 1: Sliding window at gateway Rate limit at API gateway level. **Pros:** - Centralized enforcement - Low latency - Easy configuration **Cons:** - Gateway becomes bottleneck - Coarse granularity - Redis dependency ### Option 2: Per-service limiting Each service implements its own limits. **Pros:** - Fine-grained control - Service autonomy - No single point of failure **Cons:** - Inconsistent implementation - Higher complexity - Distributed state ### Option 3: Token bucket Token-based rate limiting. **Pros:** - Allows bursts - Smooth rate limiting - Well understood **Cons:** - More complex - Harder to explain to users - State management ## Decision We will implement sliding window rate limiting at the Kong API Gateway using Redis. ## Consequences - Default limit: 100 requests per minute - Implement per-plan limits - Return rate limit headers - Add retry-after on 429",
              "score": null
            },
            {
              "doc_id": "api_metrics",
              "chunk_id": "api_metrics_fix_0",
              "content": "# Metrics API Reference ## Overview The Metrics API provides programmatic access to metrics functionality. Base URL: `https://api.cloudflow.io/v1/metrics` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /metrics/workflows Get workflow performance metrics. **Parameters:** - `period` (string): Time period: hour, day, week, month - `workflow_id` (string): Filter by workflow ID **Response:** - Success: `200` - Returns metrics data - Error: `400` - Bad Request **Example:** ```json {\"success_rate\": 0.985, \"avg_duration_ms\": 1250, \"p99_duration_ms\": 5000, \"total_executions\": 10000} ```",
              "score": null
            }
          ],
          "latency_ms": 3.3688940002321033,
          "query_id": "troubleshoot_032",
          "dimension": "contextual",
          "query": "customers are reporting intermittent errors, monitoring shows 5xx spike from API gateway after we scaled down backends"
        },
        {
          "key_facts": [
            "kubectl logs",
            "circuit breaker",
            "kubectl get endpoints"
          ],
          "found_facts": [
            "kubectl logs",
            "circuit breaker",
            "kubectl get endpoints"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "runbook_api_gateway_5xx_error"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "runbook_api_gateway_5xx_errors",
              "chunk_id": "runbook_api_gateway_5xx_errors_fix_0",
              "content": "# Runbook: API Gateway 5xx Errors ## Severity **Level:** P3 - Medium **Response Time:** 1 hour ## Symptoms - Error rate > 1% - 5xx responses in access logs - Downstream service failures ## Diagnosis ### Step 1 Check Kong error logs: `kubectl logs -l app=kong -n kong --tail=100 | grep -E '5[0-9]{2}'` ### Step 2 Verify backend health: `kubectl get pods -n production -o wide` ### Step 3 Check service endpoints: `kubectl get endpoints -n production` ### Step 4 Test backend directly: `kubectl exec -it kong-xxx -n kong -- curl http://api.production.svc:8080/health` ## Resolution ### Step 1 Restart unhealthy backends: `kubectl rollout restart deployment/api -n production` ### Step 2 Enable circuit breaker if cascading: update Kong circuit-breaker plugin ### Step 3 Scale up backends: `kubectl scale deployment/api --replicas=5 -n production` ### Step 4 Check for resource exhaustion on backend pods ### Step 5 Verify network policies allow traffic ## Escalation If 5xx persists > 15 minutes, page on-call. May indicate infrastructure issue. ## Post-Incident - Document timeline in incident log - Schedule post-mortem within 48 hours - Update runbook if new patterns discovered",
              "score": null
            },
            {
              "doc_id": "runbook_high_cpu_usage",
              "chunk_id": "runbook_high_cpu_usage_fix_0",
              "content": "# Runbook: High CPU Usage ## Severity **Level:** P1 - Critical **Response Time:** 15 minutes ## Symptoms - CPU utilization > 80% sustained - Increased response latency - Autoscaler at maximum replicas ## Diagnosis ### Step 1 Check current CPU usage: `kubectl top pods -n production` ### Step 2 Identify hot pods: `kubectl top pods --sort-by=cpu -n production | head -10` ### Step 3 Check for recent deployments: `kubectl rollout history deployment/api -n production` ### Step 4 Profile the application: `cloudflow debug profile --pod api-xyz --duration 60s` ## Resolution ### Step 1 Scale up if traffic-related: `kubectl scale deployment/api --replicas=10 -n production` ### Step 2 Restart pods if memory leak suspected: `kubectl rollout restart deployment/api -n production` ### Step 3 Roll back recent deployment if regression: `kubectl rollout undo deployment/api -n production` ### Step 4 Add resource limits if unbounded: update deployment with CPU limits ## Escalation If CPU remains high after scaling, escalate to platform team. Page on-call if affecting users. ## Post-Incident - Document timeline in incident log - Schedule post-mortem within 48 hours - Update runbook if new patterns discovered",
              "score": null
            },
            {
              "doc_id": "arch_api_gateway",
              "chunk_id": "arch_api_gateway_fix_0",
              "content": "# API Gateway Architecture ## Overview The API Gateway handles all incoming HTTP requests, providing authentication, rate limiting, and request routing. Built on Kong with custom plugins for CloudFlow-specific functionality. ## Design Principles - **Zero Trust**: Every request is authenticated and authorized - **Defense in Depth**: Multiple layers of security validation ## Components ### Kong Gateway Primary ingress controller. Handles TLS termination, request routing, and plugin execution. **Technology:** Kong 3.4 on Kubernetes **Scaling:** Horizontal with HPA ### Auth Plugin Validates JWT tokens, API keys, and OAuth2 flows. Token validation latency under 5ms. **Technology:** Custom Lua plugin **Scaling:** Stateless ### Rate Limiter Enforces per-user and per-IP rate limits. Default limit is 100 requests per minute. **Technology:** Redis-backed sliding window **Scaling:** Shared Redis cluster ## Data Flow Client \u2192 TLS \u2192 Kong \u2192 Auth \u2192 Rate Limit \u2192 Backend Services ## Performance Characteristics - **Latency P50:** 12ms - **Latency P99:** 45ms - **Throughput:** 50000 requests/second",
              "score": null
            },
            {
              "doc_id": "config_api_gateway",
              "chunk_id": "config_api_gateway_fix_0",
              "content": "# API Gateway Configuration Reference ## Overview This document describes all configuration options for API Gateway. ## Environment Variables | Variable | Type | Default | Description | |----------|------|---------|-------------| | `GATEWAY_PORT` | integer | 8000 | Gateway listen port | | `GATEWAY_RATE_LIMIT` | integer | 100 | Requests per minute per user | | `GATEWAY_TIMEOUT_MS` | integer | 30000 | Request timeout in milliseconds | | `GATEWAY_MAX_BODY_SIZE` | string | 10MB | Maximum request body size | | `GATEWAY_CORS_ORIGINS` | string | * | Allowed CORS origins | | `GATEWAY_SSL_ENABLED` | boolean | true | Enable TLS termination | ## Configuration File Configuration can also be provided via `api gateway.yaml`: ```yaml gateway_port: 8000 gateway_rate_limit: 100 gateway_timeout_ms: 30000 gateway_max_body_size: 10MB gateway_cors_origins: * ``` ## Validation Configuration is validated at startup. Invalid configuration will prevent the service from starting.",
              "score": null
            },
            {
              "doc_id": "api_integrations",
              "chunk_id": "api_integrations_fix_0",
              "content": "# Integrations API Reference ## Overview The Integrations API provides programmatic access to integrations functionality. Base URL: `https://api.cloudflow.io/v1/integrations` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /integrations List available integrations. **Parameters:** - `category` (string): Filter by category: storage, notification, database **Response:** - Success: `200` - Returns array of integration objects - Error: `400` - Bad Request **Example:** ```json {\"integrations\": [{\"id\": \"int_s3\", \"name\": \"Amazon S3\", \"category\": \"storage\", \"status\": \"connected\"}]} ``` ### POST /integrations/{id}/connect Connect an integration. **Parameters:** - `credentials` (object): Integration-specific credentials **Response:** - Success: `200` - Integration connected - Error: `400` - Bad Request **Example:** ```json {\"id\": \"int_s3\", \"status\": \"connected\", \"connected_at\": \"2024-01-15T10:00:00Z\"} ```",
              "score": null
            }
          ],
          "latency_ms": 3.621142999691074,
          "query_id": "troubleshoot_032",
          "dimension": "negation",
          "query": "why is the API gateway returning 503 when all my backend pods are running?"
        },
        {
          "key_facts": [
            "Scheduler",
            "Executor",
            "State Store",
            "PostgreSQL with JSONB",
            "5000 executions/minute"
          ],
          "found_facts": [
            "Scheduler",
            "Executor",
            "State Store",
            "PostgreSQL with JSONB",
            "5000 executions/minute"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "arch_workflow_engine"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "arch_workflow_engine",
              "chunk_id": "arch_workflow_engine_fix_0",
              "content": "# Workflow Engine Architecture ## Overview The Workflow Engine is the core component responsible for executing user-defined workflows. It uses an event-driven architecture with support for parallel execution, conditional branching, and automatic retries. ## Design Principles - **Idempotency**: All operations can be safely retried without side effects - **Eventual Consistency**: State converges to consistent view within 5 seconds - **Fault Tolerance**: System continues operating despite component failures ## Components ### Scheduler Manages workflow scheduling and triggers. Supports cron expressions, webhooks, and event-based triggers. **Technology:** Custom Go service **Scaling:** Horizontal with leader election ### Executor Runs individual workflow steps in isolated containers. Maximum execution time is 30 minutes per step. **Technology:** Kubernetes Jobs **Scaling:** Auto-scaling based on queue depth ### State Store Persists workflow state, execution history, and step results. Uses optimistic locking for concurrent updates. **Technology:** PostgreSQL with JSONB **Scaling:** Primary-replica with read replicas ## Data Flow Triggers \u2192 Scheduler \u2192 Job Queue \u2192 Executor \u2192 State Store \u2192 Webhooks/Notifications ## Performance Characteristics - **Latency P50:** 45ms - **Latency P99:** 250ms - **Throughput:** 5000 executions/minute",
              "score": null
            },
            {
              "doc_id": "config_workflow_engine",
              "chunk_id": "config_workflow_engine_fix_0",
              "content": "# Workflow Engine Configuration Reference ## Overview This document describes all configuration options for Workflow Engine. ## Environment Variables | Variable | Type | Default | Description | |----------|------|---------|-------------| | `WORKFLOW_MAX_STEPS` | integer | 100 | Maximum steps per workflow | | `WORKFLOW_TIMEOUT_SECONDS` | integer | 3600 | Maximum workflow execution time | | `WORKFLOW_RETRY_LIMIT` | integer | 3 | Default retry attempts for failed steps | | `WORKFLOW_CONCURRENT_LIMIT` | integer | 10 | Maximum concurrent executions per workflow | | `EXECUTOR_POOL_SIZE` | integer | 50 | Number of executor workers | | `EXECUTOR_MEMORY_LIMIT` | string | 512Mi | Memory limit per executor | ## Configuration File Configuration can also be provided via `workflow engine.yaml`: ```yaml workflow_max_steps: 100 workflow_timeout_seconds: 3600 workflow_retry_limit: 3 workflow_concurrent_limit: 10 executor_pool_size: 50 ``` ## Validation Configuration is validated at startup. Invalid configuration will prevent the service from starting.",
              "score": null
            },
            {
              "doc_id": "adr_005",
              "chunk_id": "adr_005_fix_0",
              "content": "# ADR-005: Adopt Event-Driven Architecture for Workflow Execution **Status:** Accepted **Date:** 2024-12-04 ## Context We need an architecture that supports asynchronous workflow execution, step dependencies, and failure recovery. ## Options Considered ### Option 1: Event-driven with Kafka Publish-subscribe messaging. **Pros:** - Loose coupling - Scalability - Event replay - Audit trail **Cons:** - Eventual consistency - Operational complexity - Message ordering challenges ### Option 2: Request-response Synchronous API calls. **Pros:** - Simple mental model - Immediate feedback - Easy debugging **Cons:** - Tight coupling - Cascading failures - Timeout issues ### Option 3: Polling-based Workers poll for tasks. **Pros:** - Simple implementation - Easy rate limiting - No message broker **Cons:** - Polling overhead - Higher latency - Inefficient at scale ## Decision We will use an event-driven architecture with Kafka for workflow step execution. ## Consequences - Implement idempotent consumers - Use dead letter queues for failed messages - Monitor consumer lag",
              "score": null
            },
            {
              "doc_id": "api_workflows",
              "chunk_id": "api_workflows_fix_0",
              "content": "# Workflows API Reference ## Overview The Workflows API provides programmatic access to workflows functionality. Base URL: `https://api.cloudflow.io/v1/workflows` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /workflows List all workflows for the authenticated user. **Parameters:** - `limit` (integer): Maximum number of results (default: 50, max: 200) - `status` (string): Filter by status: active, paused, archived **Response:** - Success: `200` - Returns array of workflow objects - Error: `400` - Bad Request **Example:** ```json {\"workflows\": [{\"id\": \"wf_123\", \"name\": \"Daily Report\", \"status\": \"active\"}]} ``` ### POST /workflows Create a new workflow. **Parameters:** - `name` (string): Workflow name (required) - `definition` (object): Workflow definition in JSON format **Response:** - Success: `201` - Workflow created successfully - Error: `400` - Bad Request **Example:** ```json {\"id\": \"wf_456\", \"name\": \"New Workflow\", \"created_at\": \"2024-01-15T10:00:00Z\"} ``` ### DELETE /workflows/{id} Delete a workflow permanently. **Parameters:** - `id` (string): Workflow ID (required) **Response:** - Success: `204` - Workflow deleted - Error: `404` - Workflow not found **Example:** ```json {} ```",
              "score": null
            },
            {
              "doc_id": "arch_data_pipeline",
              "chunk_id": "arch_data_pipeline_fix_0",
              "content": "# Data Pipeline Architecture ## Overview The Data Pipeline processes and transforms data between workflow steps. It supports batch processing up to 10GB and streaming for real-time use cases. ## Design Principles - **Exactly-Once Processing**: Each record is processed exactly once - **Backpressure Handling**: System gracefully handles varying load ## Components ### Stream Processor Handles real-time data streams. Maximum throughput of 100000 messages per second. **Technology:** Apache Kafka with Flink **Scaling:** Partition-based scaling ### Batch Processor Processes large batch jobs. Supports data formats: JSON, CSV, Parquet, Avro. **Technology:** Apache Spark on Kubernetes **Scaling:** Dynamic resource allocation ### Data Lake Long-term data storage with time-travel queries. Retention configurable up to 7 years. **Technology:** S3 with Iceberg tables **Scaling:** Unlimited with tiered storage ## Data Flow Input \u2192 Validation \u2192 Transform \u2192 Enrich \u2192 Output ## Performance Characteristics - **Latency P50:** 100ms (streaming), 5min (batch) - **Latency P99:** 500ms (streaming), 30min (batch) - **Throughput:** 100000 messages/second (streaming)",
              "score": null
            }
          ],
          "latency_ms": 3.507071000058204,
          "query_id": "arch_033",
          "dimension": "original",
          "query": "Explain the Workflow Engine architecture"
        },
        {
          "key_facts": [
            "Scheduler",
            "Executor",
            "State Store",
            "PostgreSQL with JSONB",
            "5000 executions/minute"
          ],
          "found_facts": [
            "Scheduler",
            "Executor",
            "State Store",
            "PostgreSQL with JSONB",
            "5000 executions/minute"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "arch_workflow_engine"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "arch_workflow_engine",
              "chunk_id": "arch_workflow_engine_fix_0",
              "content": "# Workflow Engine Architecture ## Overview The Workflow Engine is the core component responsible for executing user-defined workflows. It uses an event-driven architecture with support for parallel execution, conditional branching, and automatic retries. ## Design Principles - **Idempotency**: All operations can be safely retried without side effects - **Eventual Consistency**: State converges to consistent view within 5 seconds - **Fault Tolerance**: System continues operating despite component failures ## Components ### Scheduler Manages workflow scheduling and triggers. Supports cron expressions, webhooks, and event-based triggers. **Technology:** Custom Go service **Scaling:** Horizontal with leader election ### Executor Runs individual workflow steps in isolated containers. Maximum execution time is 30 minutes per step. **Technology:** Kubernetes Jobs **Scaling:** Auto-scaling based on queue depth ### State Store Persists workflow state, execution history, and step results. Uses optimistic locking for concurrent updates. **Technology:** PostgreSQL with JSONB **Scaling:** Primary-replica with read replicas ## Data Flow Triggers \u2192 Scheduler \u2192 Job Queue \u2192 Executor \u2192 State Store \u2192 Webhooks/Notifications ## Performance Characteristics - **Latency P50:** 45ms - **Latency P99:** 250ms - **Throughput:** 5000 executions/minute",
              "score": null
            },
            {
              "doc_id": "adr_005",
              "chunk_id": "adr_005_fix_0",
              "content": "# ADR-005: Adopt Event-Driven Architecture for Workflow Execution **Status:** Accepted **Date:** 2024-12-04 ## Context We need an architecture that supports asynchronous workflow execution, step dependencies, and failure recovery. ## Options Considered ### Option 1: Event-driven with Kafka Publish-subscribe messaging. **Pros:** - Loose coupling - Scalability - Event replay - Audit trail **Cons:** - Eventual consistency - Operational complexity - Message ordering challenges ### Option 2: Request-response Synchronous API calls. **Pros:** - Simple mental model - Immediate feedback - Easy debugging **Cons:** - Tight coupling - Cascading failures - Timeout issues ### Option 3: Polling-based Workers poll for tasks. **Pros:** - Simple implementation - Easy rate limiting - No message broker **Cons:** - Polling overhead - Higher latency - Inefficient at scale ## Decision We will use an event-driven architecture with Kafka for workflow step execution. ## Consequences - Implement idempotent consumers - Use dead letter queues for failed messages - Monitor consumer lag",
              "score": null
            },
            {
              "doc_id": "api_workflows",
              "chunk_id": "api_workflows_fix_0",
              "content": "# Workflows API Reference ## Overview The Workflows API provides programmatic access to workflows functionality. Base URL: `https://api.cloudflow.io/v1/workflows` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /workflows List all workflows for the authenticated user. **Parameters:** - `limit` (integer): Maximum number of results (default: 50, max: 200) - `status` (string): Filter by status: active, paused, archived **Response:** - Success: `200` - Returns array of workflow objects - Error: `400` - Bad Request **Example:** ```json {\"workflows\": [{\"id\": \"wf_123\", \"name\": \"Daily Report\", \"status\": \"active\"}]} ``` ### POST /workflows Create a new workflow. **Parameters:** - `name` (string): Workflow name (required) - `definition` (object): Workflow definition in JSON format **Response:** - Success: `201` - Workflow created successfully - Error: `400` - Bad Request **Example:** ```json {\"id\": \"wf_456\", \"name\": \"New Workflow\", \"created_at\": \"2024-01-15T10:00:00Z\"} ``` ### DELETE /workflows/{id} Delete a workflow permanently. **Parameters:** - `id` (string): Workflow ID (required) **Response:** - Success: `204` - Workflow deleted - Error: `404` - Workflow not found **Example:** ```json {} ```",
              "score": null
            },
            {
              "doc_id": "config_workflow_engine",
              "chunk_id": "config_workflow_engine_fix_0",
              "content": "# Workflow Engine Configuration Reference ## Overview This document describes all configuration options for Workflow Engine. ## Environment Variables | Variable | Type | Default | Description | |----------|------|---------|-------------| | `WORKFLOW_MAX_STEPS` | integer | 100 | Maximum steps per workflow | | `WORKFLOW_TIMEOUT_SECONDS` | integer | 3600 | Maximum workflow execution time | | `WORKFLOW_RETRY_LIMIT` | integer | 3 | Default retry attempts for failed steps | | `WORKFLOW_CONCURRENT_LIMIT` | integer | 10 | Maximum concurrent executions per workflow | | `EXECUTOR_POOL_SIZE` | integer | 50 | Number of executor workers | | `EXECUTOR_MEMORY_LIMIT` | string | 512Mi | Memory limit per executor | ## Configuration File Configuration can also be provided via `workflow engine.yaml`: ```yaml workflow_max_steps: 100 workflow_timeout_seconds: 3600 workflow_retry_limit: 3 workflow_concurrent_limit: 10 executor_pool_size: 50 ``` ## Validation Configuration is validated at startup. Invalid configuration will prevent the service from starting.",
              "score": null
            },
            {
              "doc_id": "release_3_2_0",
              "chunk_id": "release_3_2_0_fix_0",
              "content": "# Release Notes - v3.2.0 **Release Date:** 2024-01-15 ## New Features - Workflow versioning with rollback support - New Python SDK with async support - Bulk operations API for workflows - Custom retry policies per step ## Bug Fixes - Fixed race condition in concurrent execution handler - Resolved memory leak in long-running workflows - Fixed timezone handling in scheduled triggers ## Breaking Changes - **BREAKING:** API v1 endpoints deprecated, will be removed in v4.0 - **BREAKING:** Minimum Python version now 3.9 ## Upgrade Instructions ```bash cloudflow upgrade --version 3.2.0 ``` ## Known Issues None at this time.",
              "score": null
            }
          ],
          "latency_ms": 3.942130999348592,
          "query_id": "arch_033",
          "dimension": "synonym",
          "query": "What is the design of the workflow orchestration system?"
        },
        {
          "key_facts": [
            "Scheduler",
            "Executor",
            "State Store",
            "PostgreSQL with JSONB",
            "5000 executions/minute"
          ],
          "found_facts": [
            "Scheduler",
            "Executor",
            "State Store",
            "PostgreSQL with JSONB",
            "5000 executions/minute"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "arch_workflow_engine"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "arch_workflow_engine",
              "chunk_id": "arch_workflow_engine_fix_0",
              "content": "# Workflow Engine Architecture ## Overview The Workflow Engine is the core component responsible for executing user-defined workflows. It uses an event-driven architecture with support for parallel execution, conditional branching, and automatic retries. ## Design Principles - **Idempotency**: All operations can be safely retried without side effects - **Eventual Consistency**: State converges to consistent view within 5 seconds - **Fault Tolerance**: System continues operating despite component failures ## Components ### Scheduler Manages workflow scheduling and triggers. Supports cron expressions, webhooks, and event-based triggers. **Technology:** Custom Go service **Scaling:** Horizontal with leader election ### Executor Runs individual workflow steps in isolated containers. Maximum execution time is 30 minutes per step. **Technology:** Kubernetes Jobs **Scaling:** Auto-scaling based on queue depth ### State Store Persists workflow state, execution history, and step results. Uses optimistic locking for concurrent updates. **Technology:** PostgreSQL with JSONB **Scaling:** Primary-replica with read replicas ## Data Flow Triggers \u2192 Scheduler \u2192 Job Queue \u2192 Executor \u2192 State Store \u2192 Webhooks/Notifications ## Performance Characteristics - **Latency P50:** 45ms - **Latency P99:** 250ms - **Throughput:** 5000 executions/minute",
              "score": null
            },
            {
              "doc_id": "arch_data_pipeline",
              "chunk_id": "arch_data_pipeline_fix_0",
              "content": "# Data Pipeline Architecture ## Overview The Data Pipeline processes and transforms data between workflow steps. It supports batch processing up to 10GB and streaming for real-time use cases. ## Design Principles - **Exactly-Once Processing**: Each record is processed exactly once - **Backpressure Handling**: System gracefully handles varying load ## Components ### Stream Processor Handles real-time data streams. Maximum throughput of 100000 messages per second. **Technology:** Apache Kafka with Flink **Scaling:** Partition-based scaling ### Batch Processor Processes large batch jobs. Supports data formats: JSON, CSV, Parquet, Avro. **Technology:** Apache Spark on Kubernetes **Scaling:** Dynamic resource allocation ### Data Lake Long-term data storage with time-travel queries. Retention configurable up to 7 years. **Technology:** S3 with Iceberg tables **Scaling:** Unlimited with tiered storage ## Data Flow Input \u2192 Validation \u2192 Transform \u2192 Enrich \u2192 Output ## Performance Characteristics - **Latency P50:** 100ms (streaming), 5min (batch) - **Latency P99:** 500ms (streaming), 30min (batch) - **Throughput:** 100000 messages/second (streaming)",
              "score": null
            },
            {
              "doc_id": "config_workflow_engine",
              "chunk_id": "config_workflow_engine_fix_0",
              "content": "# Workflow Engine Configuration Reference ## Overview This document describes all configuration options for Workflow Engine. ## Environment Variables | Variable | Type | Default | Description | |----------|------|---------|-------------| | `WORKFLOW_MAX_STEPS` | integer | 100 | Maximum steps per workflow | | `WORKFLOW_TIMEOUT_SECONDS` | integer | 3600 | Maximum workflow execution time | | `WORKFLOW_RETRY_LIMIT` | integer | 3 | Default retry attempts for failed steps | | `WORKFLOW_CONCURRENT_LIMIT` | integer | 10 | Maximum concurrent executions per workflow | | `EXECUTOR_POOL_SIZE` | integer | 50 | Number of executor workers | | `EXECUTOR_MEMORY_LIMIT` | string | 512Mi | Memory limit per executor | ## Configuration File Configuration can also be provided via `workflow engine.yaml`: ```yaml workflow_max_steps: 100 workflow_timeout_seconds: 3600 workflow_retry_limit: 3 workflow_concurrent_limit: 10 executor_pool_size: 50 ``` ## Validation Configuration is validated at startup. Invalid configuration will prevent the service from starting.",
              "score": null
            },
            {
              "doc_id": "runbook_kafka_consumer_lag",
              "chunk_id": "runbook_kafka_consumer_lag_fix_0",
              "content": "# Runbook: Kafka Consumer Lag ## Severity **Level:** P1 - Critical **Response Time:** 1 hour ## Symptoms - Consumer lag > 10000 messages - Processing delays reported - Alerts from lag monitor ## Diagnosis ### Step 1 Check consumer group lag: `kafka-consumer-groups.sh --bootstrap-server kafka:9092 --describe --group workflow-executor` ### Step 2 Verify consumer health: `kubectl get pods -l app=workflow-executor -n production` ### Step 3 Check for processing errors: `kubectl logs -l app=workflow-executor --tail=100 -n production | grep ERROR` ### Step 4 Monitor throughput: `cloudflow metrics get kafka.consumer.records_per_second` ## Resolution ### Step 1 Scale up consumers: `kubectl scale deployment/workflow-executor --replicas=10 -n production` ### Step 2 Reset offset if messages are stale: `kafka-consumer-groups.sh --bootstrap-server kafka:9092 --group workflow-executor --reset-offsets --to-latest --execute --topic workflows` ### Step 3 Pause producer if backpressure needed: `cloudflow workflows pause-triggers` ### Step 4 Increase partition count for parallelism (requires coordination) ## Escalation If lag persists > 30 minutes, page platform team. May need Kafka cluster scaling. ## Post-Incident - Document timeline in incident log - Schedule post-mortem within 48 hours - Update runbook if new patterns discovered",
              "score": null
            },
            {
              "doc_id": "adr_005",
              "chunk_id": "adr_005_fix_0",
              "content": "# ADR-005: Adopt Event-Driven Architecture for Workflow Execution **Status:** Accepted **Date:** 2024-12-04 ## Context We need an architecture that supports asynchronous workflow execution, step dependencies, and failure recovery. ## Options Considered ### Option 1: Event-driven with Kafka Publish-subscribe messaging. **Pros:** - Loose coupling - Scalability - Event replay - Audit trail **Cons:** - Eventual consistency - Operational complexity - Message ordering challenges ### Option 2: Request-response Synchronous API calls. **Pros:** - Simple mental model - Immediate feedback - Easy debugging **Cons:** - Tight coupling - Cascading failures - Timeout issues ### Option 3: Polling-based Workers poll for tasks. **Pros:** - Simple implementation - Easy rate limiting - No message broker **Cons:** - Polling overhead - Higher latency - Inefficient at scale ## Decision We will use an event-driven architecture with Kafka for workflow step execution. ## Consequences - Implement idempotent consumers - Use dead letter queues for failed messages - Monitor consumer lag",
              "score": null
            }
          ],
          "latency_ms": 3.9513179999630665,
          "query_id": "arch_033",
          "dimension": "problem",
          "query": "I'm seeing bottlenecks in workflow execution - need to understand how the scheduler and executor interact"
        },
        {
          "key_facts": [
            "Scheduler",
            "Executor",
            "State Store",
            "PostgreSQL with JSONB",
            "5000 executions/minute"
          ],
          "found_facts": [
            "Scheduler",
            "Executor",
            "State Store",
            "PostgreSQL with JSONB",
            "5000 executions/minute"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "arch_workflow_engine"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "config_workflow_engine",
              "chunk_id": "config_workflow_engine_fix_0",
              "content": "# Workflow Engine Configuration Reference ## Overview This document describes all configuration options for Workflow Engine. ## Environment Variables | Variable | Type | Default | Description | |----------|------|---------|-------------| | `WORKFLOW_MAX_STEPS` | integer | 100 | Maximum steps per workflow | | `WORKFLOW_TIMEOUT_SECONDS` | integer | 3600 | Maximum workflow execution time | | `WORKFLOW_RETRY_LIMIT` | integer | 3 | Default retry attempts for failed steps | | `WORKFLOW_CONCURRENT_LIMIT` | integer | 10 | Maximum concurrent executions per workflow | | `EXECUTOR_POOL_SIZE` | integer | 50 | Number of executor workers | | `EXECUTOR_MEMORY_LIMIT` | string | 512Mi | Memory limit per executor | ## Configuration File Configuration can also be provided via `workflow engine.yaml`: ```yaml workflow_max_steps: 100 workflow_timeout_seconds: 3600 workflow_retry_limit: 3 workflow_concurrent_limit: 10 executor_pool_size: 50 ``` ## Validation Configuration is validated at startup. Invalid configuration will prevent the service from starting.",
              "score": null
            },
            {
              "doc_id": "arch_workflow_engine",
              "chunk_id": "arch_workflow_engine_fix_0",
              "content": "# Workflow Engine Architecture ## Overview The Workflow Engine is the core component responsible for executing user-defined workflows. It uses an event-driven architecture with support for parallel execution, conditional branching, and automatic retries. ## Design Principles - **Idempotency**: All operations can be safely retried without side effects - **Eventual Consistency**: State converges to consistent view within 5 seconds - **Fault Tolerance**: System continues operating despite component failures ## Components ### Scheduler Manages workflow scheduling and triggers. Supports cron expressions, webhooks, and event-based triggers. **Technology:** Custom Go service **Scaling:** Horizontal with leader election ### Executor Runs individual workflow steps in isolated containers. Maximum execution time is 30 minutes per step. **Technology:** Kubernetes Jobs **Scaling:** Auto-scaling based on queue depth ### State Store Persists workflow state, execution history, and step results. Uses optimistic locking for concurrent updates. **Technology:** PostgreSQL with JSONB **Scaling:** Primary-replica with read replicas ## Data Flow Triggers \u2192 Scheduler \u2192 Job Queue \u2192 Executor \u2192 State Store \u2192 Webhooks/Notifications ## Performance Characteristics - **Latency P50:** 45ms - **Latency P99:** 250ms - **Throughput:** 5000 executions/minute",
              "score": null
            },
            {
              "doc_id": "release_3_2_0",
              "chunk_id": "release_3_2_0_fix_0",
              "content": "# Release Notes - v3.2.0 **Release Date:** 2024-01-15 ## New Features - Workflow versioning with rollback support - New Python SDK with async support - Bulk operations API for workflows - Custom retry policies per step ## Bug Fixes - Fixed race condition in concurrent execution handler - Resolved memory leak in long-running workflows - Fixed timezone handling in scheduled triggers ## Breaking Changes - **BREAKING:** API v1 endpoints deprecated, will be removed in v4.0 - **BREAKING:** Minimum Python version now 3.9 ## Upgrade Instructions ```bash cloudflow upgrade --version 3.2.0 ``` ## Known Issues None at this time.",
              "score": null
            },
            {
              "doc_id": "api_workflows",
              "chunk_id": "api_workflows_fix_0",
              "content": "# Workflows API Reference ## Overview The Workflows API provides programmatic access to workflows functionality. Base URL: `https://api.cloudflow.io/v1/workflows` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /workflows List all workflows for the authenticated user. **Parameters:** - `limit` (integer): Maximum number of results (default: 50, max: 200) - `status` (string): Filter by status: active, paused, archived **Response:** - Success: `200` - Returns array of workflow objects - Error: `400` - Bad Request **Example:** ```json {\"workflows\": [{\"id\": \"wf_123\", \"name\": \"Daily Report\", \"status\": \"active\"}]} ``` ### POST /workflows Create a new workflow. **Parameters:** - `name` (string): Workflow name (required) - `definition` (object): Workflow definition in JSON format **Response:** - Success: `201` - Workflow created successfully - Error: `400` - Bad Request **Example:** ```json {\"id\": \"wf_456\", \"name\": \"New Workflow\", \"created_at\": \"2024-01-15T10:00:00Z\"} ``` ### DELETE /workflows/{id} Delete a workflow permanently. **Parameters:** - `id` (string): Workflow ID (required) **Response:** - Success: `204` - Workflow deleted - Error: `404` - Workflow not found **Example:** ```json {} ```",
              "score": null
            },
            {
              "doc_id": "api_metrics",
              "chunk_id": "api_metrics_fix_0",
              "content": "# Metrics API Reference ## Overview The Metrics API provides programmatic access to metrics functionality. Base URL: `https://api.cloudflow.io/v1/metrics` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /metrics/workflows Get workflow performance metrics. **Parameters:** - `period` (string): Time period: hour, day, week, month - `workflow_id` (string): Filter by workflow ID **Response:** - Success: `200` - Returns metrics data - Error: `400` - Bad Request **Example:** ```json {\"success_rate\": 0.985, \"avg_duration_ms\": 1250, \"p99_duration_ms\": 5000, \"total_executions\": 10000} ```",
              "score": null
            }
          ],
          "latency_ms": 3.3284680012002354,
          "query_id": "arch_033",
          "dimension": "casual",
          "query": "workflow engine internals"
        },
        {
          "key_facts": [
            "Scheduler",
            "Executor",
            "State Store",
            "PostgreSQL with JSONB",
            "5000 executions/minute"
          ],
          "found_facts": [
            "Scheduler",
            "Executor",
            "State Store",
            "PostgreSQL with JSONB",
            "5000 executions/minute"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "arch_workflow_engine"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "arch_workflow_engine",
              "chunk_id": "arch_workflow_engine_fix_0",
              "content": "# Workflow Engine Architecture ## Overview The Workflow Engine is the core component responsible for executing user-defined workflows. It uses an event-driven architecture with support for parallel execution, conditional branching, and automatic retries. ## Design Principles - **Idempotency**: All operations can be safely retried without side effects - **Eventual Consistency**: State converges to consistent view within 5 seconds - **Fault Tolerance**: System continues operating despite component failures ## Components ### Scheduler Manages workflow scheduling and triggers. Supports cron expressions, webhooks, and event-based triggers. **Technology:** Custom Go service **Scaling:** Horizontal with leader election ### Executor Runs individual workflow steps in isolated containers. Maximum execution time is 30 minutes per step. **Technology:** Kubernetes Jobs **Scaling:** Auto-scaling based on queue depth ### State Store Persists workflow state, execution history, and step results. Uses optimistic locking for concurrent updates. **Technology:** PostgreSQL with JSONB **Scaling:** Primary-replica with read replicas ## Data Flow Triggers \u2192 Scheduler \u2192 Job Queue \u2192 Executor \u2192 State Store \u2192 Webhooks/Notifications ## Performance Characteristics - **Latency P50:** 45ms - **Latency P99:** 250ms - **Throughput:** 5000 executions/minute",
              "score": null
            },
            {
              "doc_id": "howto_create_scheduled_workflow",
              "chunk_id": "howto_create_scheduled_workflow_fix_0",
              "content": "# How to Create Scheduled Workflow ## Prerequisites - Workflow definition ## Steps ### Step 1: Define schedule Use cron expression to set the schedule. Example: '0 9 * * MON-FRI' runs at 9 AM on weekdays. ### Step 2: Set timezone Specify timezone for the schedule. Default is UTC. > **Note:** Use IANA timezone names like America/New_York ### Step 3: Configure overlap handling Choose behavior when previous execution is still running: skip, queue, or cancel. ### Step 4: Enable notifications Get notified on schedule failures. ```bash cloudflow workflows notify --on-failure ``` ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            },
            {
              "doc_id": "api_executions",
              "chunk_id": "api_executions_fix_0",
              "content": "# Executions API Reference ## Overview The Executions API provides programmatic access to executions functionality. Base URL: `https://api.cloudflow.io/v1/executions` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /executions List workflow executions. **Parameters:** - `workflow_id` (string): Filter by workflow ID - `since` (datetime): Filter executions after this timestamp **Response:** - Success: `200` - Returns array of execution objects - Error: `400` - Bad Request **Example:** ```json {\"executions\": [{\"id\": \"ex_789\", \"status\": \"completed\", \"duration_ms\": 1523}]} ``` ### POST /executions/{id}/cancel Cancel a running execution. **Parameters:** - `id` (string): Execution ID - `reason` (string): Cancellation reason (optional) **Response:** - Success: `200` - Execution cancelled - Error: `400` - Bad Request **Example:** ```json {\"id\": \"ex_789\", \"status\": \"cancelled\", \"cancelled_at\": \"2024-01-15T10:05:00Z\"} ```",
              "score": null
            },
            {
              "doc_id": "api_workflows",
              "chunk_id": "api_workflows_fix_0",
              "content": "# Workflows API Reference ## Overview The Workflows API provides programmatic access to workflows functionality. Base URL: `https://api.cloudflow.io/v1/workflows` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /workflows List all workflows for the authenticated user. **Parameters:** - `limit` (integer): Maximum number of results (default: 50, max: 200) - `status` (string): Filter by status: active, paused, archived **Response:** - Success: `200` - Returns array of workflow objects - Error: `400` - Bad Request **Example:** ```json {\"workflows\": [{\"id\": \"wf_123\", \"name\": \"Daily Report\", \"status\": \"active\"}]} ``` ### POST /workflows Create a new workflow. **Parameters:** - `name` (string): Workflow name (required) - `definition` (object): Workflow definition in JSON format **Response:** - Success: `201` - Workflow created successfully - Error: `400` - Bad Request **Example:** ```json {\"id\": \"wf_456\", \"name\": \"New Workflow\", \"created_at\": \"2024-01-15T10:00:00Z\"} ``` ### DELETE /workflows/{id} Delete a workflow permanently. **Parameters:** - `id` (string): Workflow ID (required) **Response:** - Success: `204` - Workflow deleted - Error: `404` - Workflow not found **Example:** ```json {} ```",
              "score": null
            },
            {
              "doc_id": "adr_005",
              "chunk_id": "adr_005_fix_0",
              "content": "# ADR-005: Adopt Event-Driven Architecture for Workflow Execution **Status:** Accepted **Date:** 2024-12-04 ## Context We need an architecture that supports asynchronous workflow execution, step dependencies, and failure recovery. ## Options Considered ### Option 1: Event-driven with Kafka Publish-subscribe messaging. **Pros:** - Loose coupling - Scalability - Event replay - Audit trail **Cons:** - Eventual consistency - Operational complexity - Message ordering challenges ### Option 2: Request-response Synchronous API calls. **Pros:** - Simple mental model - Immediate feedback - Easy debugging **Cons:** - Tight coupling - Cascading failures - Timeout issues ### Option 3: Polling-based Workers poll for tasks. **Pros:** - Simple implementation - Easy rate limiting - No message broker **Cons:** - Polling overhead - Higher latency - Inefficient at scale ## Decision We will use an event-driven architecture with Kafka for workflow step execution. ## Consequences - Implement idempotent consumers - Use dead letter queues for failed messages - Monitor consumer lag",
              "score": null
            }
          ],
          "latency_ms": 3.9857219999248628,
          "query_id": "arch_033",
          "dimension": "contextual",
          "query": "Onboarding to the team and trying to understand how workflows get scheduled, executed, and where state is stored"
        },
        {
          "key_facts": [
            "Scheduler",
            "Executor",
            "State Store",
            "PostgreSQL with JSONB",
            "5000 executions/minute"
          ],
          "found_facts": [
            "Scheduler",
            "Executor",
            "State Store",
            "PostgreSQL with JSONB",
            "5000 executions/minute"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "arch_workflow_engine"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "arch_workflow_engine",
              "chunk_id": "arch_workflow_engine_fix_0",
              "content": "# Workflow Engine Architecture ## Overview The Workflow Engine is the core component responsible for executing user-defined workflows. It uses an event-driven architecture with support for parallel execution, conditional branching, and automatic retries. ## Design Principles - **Idempotency**: All operations can be safely retried without side effects - **Eventual Consistency**: State converges to consistent view within 5 seconds - **Fault Tolerance**: System continues operating despite component failures ## Components ### Scheduler Manages workflow scheduling and triggers. Supports cron expressions, webhooks, and event-based triggers. **Technology:** Custom Go service **Scaling:** Horizontal with leader election ### Executor Runs individual workflow steps in isolated containers. Maximum execution time is 30 minutes per step. **Technology:** Kubernetes Jobs **Scaling:** Auto-scaling based on queue depth ### State Store Persists workflow state, execution history, and step results. Uses optimistic locking for concurrent updates. **Technology:** PostgreSQL with JSONB **Scaling:** Primary-replica with read replicas ## Data Flow Triggers \u2192 Scheduler \u2192 Job Queue \u2192 Executor \u2192 State Store \u2192 Webhooks/Notifications ## Performance Characteristics - **Latency P50:** 45ms - **Latency P99:** 250ms - **Throughput:** 5000 executions/minute",
              "score": null
            },
            {
              "doc_id": "config_workflow_engine",
              "chunk_id": "config_workflow_engine_fix_0",
              "content": "# Workflow Engine Configuration Reference ## Overview This document describes all configuration options for Workflow Engine. ## Environment Variables | Variable | Type | Default | Description | |----------|------|---------|-------------| | `WORKFLOW_MAX_STEPS` | integer | 100 | Maximum steps per workflow | | `WORKFLOW_TIMEOUT_SECONDS` | integer | 3600 | Maximum workflow execution time | | `WORKFLOW_RETRY_LIMIT` | integer | 3 | Default retry attempts for failed steps | | `WORKFLOW_CONCURRENT_LIMIT` | integer | 10 | Maximum concurrent executions per workflow | | `EXECUTOR_POOL_SIZE` | integer | 50 | Number of executor workers | | `EXECUTOR_MEMORY_LIMIT` | string | 512Mi | Memory limit per executor | ## Configuration File Configuration can also be provided via `workflow engine.yaml`: ```yaml workflow_max_steps: 100 workflow_timeout_seconds: 3600 workflow_retry_limit: 3 workflow_concurrent_limit: 10 executor_pool_size: 50 ``` ## Validation Configuration is validated at startup. Invalid configuration will prevent the service from starting.",
              "score": null
            },
            {
              "doc_id": "howto_implement_retry_logic",
              "chunk_id": "howto_implement_retry_logic_fix_0",
              "content": "# How to Implement Retry Logic ## Prerequisites - Existing workflow ## Steps ### Step 1: Define retry policy Add retry configuration to your workflow step. > **Note:** Maximum 5 retry attempts allowed ### Step 2: Configure backoff Set exponential backoff with base delay of 1 second. The formula is: delay = base * 2^attempt. ### Step 3: Add timeout Set maximum execution time to prevent infinite loops. Default timeout is 30 seconds. ### Step 4: Handle final failure Configure on_failure handler to notify or escalate when all retries are exhausted. ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            },
            {
              "doc_id": "api_executions",
              "chunk_id": "api_executions_fix_0",
              "content": "# Executions API Reference ## Overview The Executions API provides programmatic access to executions functionality. Base URL: `https://api.cloudflow.io/v1/executions` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /executions List workflow executions. **Parameters:** - `workflow_id` (string): Filter by workflow ID - `since` (datetime): Filter executions after this timestamp **Response:** - Success: `200` - Returns array of execution objects - Error: `400` - Bad Request **Example:** ```json {\"executions\": [{\"id\": \"ex_789\", \"status\": \"completed\", \"duration_ms\": 1523}]} ``` ### POST /executions/{id}/cancel Cancel a running execution. **Parameters:** - `id` (string): Execution ID - `reason` (string): Cancellation reason (optional) **Response:** - Success: `200` - Execution cancelled - Error: `400` - Bad Request **Example:** ```json {\"id\": \"ex_789\", \"status\": \"cancelled\", \"cancelled_at\": \"2024-01-15T10:05:00Z\"} ```",
              "score": null
            },
            {
              "doc_id": "adr_005",
              "chunk_id": "adr_005_fix_0",
              "content": "# ADR-005: Adopt Event-Driven Architecture for Workflow Execution **Status:** Accepted **Date:** 2024-12-04 ## Context We need an architecture that supports asynchronous workflow execution, step dependencies, and failure recovery. ## Options Considered ### Option 1: Event-driven with Kafka Publish-subscribe messaging. **Pros:** - Loose coupling - Scalability - Event replay - Audit trail **Cons:** - Eventual consistency - Operational complexity - Message ordering challenges ### Option 2: Request-response Synchronous API calls. **Pros:** - Simple mental model - Immediate feedback - Easy debugging **Cons:** - Tight coupling - Cascading failures - Timeout issues ### Option 3: Polling-based Workers poll for tasks. **Pros:** - Simple implementation - Easy rate limiting - No message broker **Cons:** - Polling overhead - Higher latency - Inefficient at scale ## Decision We will use an event-driven architecture with Kafka for workflow step execution. ## Consequences - Implement idempotent consumers - Use dead letter queues for failed messages - Monitor consumer lag",
              "score": null
            }
          ],
          "latency_ms": 5.245447999186581,
          "query_id": "arch_033",
          "dimension": "negation",
          "query": "I don't understand how the workflow engine handles parallel execution and retries"
        },
        {
          "key_facts": [
            "Kong",
            "TLS termination",
            "50000 requests/second",
            "5ms"
          ],
          "found_facts": [
            "Kong",
            "TLS termination",
            "50000 requests/second",
            "5ms"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "arch_api_gateway"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "config_api_gateway",
              "chunk_id": "config_api_gateway_fix_0",
              "content": "# API Gateway Configuration Reference ## Overview This document describes all configuration options for API Gateway. ## Environment Variables | Variable | Type | Default | Description | |----------|------|---------|-------------| | `GATEWAY_PORT` | integer | 8000 | Gateway listen port | | `GATEWAY_RATE_LIMIT` | integer | 100 | Requests per minute per user | | `GATEWAY_TIMEOUT_MS` | integer | 30000 | Request timeout in milliseconds | | `GATEWAY_MAX_BODY_SIZE` | string | 10MB | Maximum request body size | | `GATEWAY_CORS_ORIGINS` | string | * | Allowed CORS origins | | `GATEWAY_SSL_ENABLED` | boolean | true | Enable TLS termination | ## Configuration File Configuration can also be provided via `api gateway.yaml`: ```yaml gateway_port: 8000 gateway_rate_limit: 100 gateway_timeout_ms: 30000 gateway_max_body_size: 10MB gateway_cors_origins: * ``` ## Validation Configuration is validated at startup. Invalid configuration will prevent the service from starting.",
              "score": null
            },
            {
              "doc_id": "arch_api_gateway",
              "chunk_id": "arch_api_gateway_fix_0",
              "content": "# API Gateway Architecture ## Overview The API Gateway handles all incoming HTTP requests, providing authentication, rate limiting, and request routing. Built on Kong with custom plugins for CloudFlow-specific functionality. ## Design Principles - **Zero Trust**: Every request is authenticated and authorized - **Defense in Depth**: Multiple layers of security validation ## Components ### Kong Gateway Primary ingress controller. Handles TLS termination, request routing, and plugin execution. **Technology:** Kong 3.4 on Kubernetes **Scaling:** Horizontal with HPA ### Auth Plugin Validates JWT tokens, API keys, and OAuth2 flows. Token validation latency under 5ms. **Technology:** Custom Lua plugin **Scaling:** Stateless ### Rate Limiter Enforces per-user and per-IP rate limits. Default limit is 100 requests per minute. **Technology:** Redis-backed sliding window **Scaling:** Shared Redis cluster ## Data Flow Client \u2192 TLS \u2192 Kong \u2192 Auth \u2192 Rate Limit \u2192 Backend Services ## Performance Characteristics - **Latency P50:** 12ms - **Latency P99:** 45ms - **Throughput:** 50000 requests/second",
              "score": null
            },
            {
              "doc_id": "adr_007",
              "chunk_id": "adr_007_fix_0",
              "content": "# ADR-007: Implement Rate Limiting at API Gateway **Status:** Accepted **Date:** 2024-09-03 ## Context We need to protect our services from abuse and ensure fair resource usage across customers. ## Options Considered ### Option 1: Sliding window at gateway Rate limit at API gateway level. **Pros:** - Centralized enforcement - Low latency - Easy configuration **Cons:** - Gateway becomes bottleneck - Coarse granularity - Redis dependency ### Option 2: Per-service limiting Each service implements its own limits. **Pros:** - Fine-grained control - Service autonomy - No single point of failure **Cons:** - Inconsistent implementation - Higher complexity - Distributed state ### Option 3: Token bucket Token-based rate limiting. **Pros:** - Allows bursts - Smooth rate limiting - Well understood **Cons:** - More complex - Harder to explain to users - State management ## Decision We will implement sliding window rate limiting at the Kong API Gateway using Redis. ## Consequences - Default limit: 100 requests per minute - Implement per-plan limits - Return rate limit headers - Add retry-after on 429",
              "score": null
            },
            {
              "doc_id": "arch_authentication_service",
              "chunk_id": "arch_authentication_service_fix_0",
              "content": "# Authentication Service Architecture ## Overview The Authentication Service handles user identity, session management, and token issuance. Supports OAuth 2.0, SAML, and API key authentication. ## Design Principles - **Zero Knowledge**: Passwords never stored in plaintext - **Token Rotation**: Refresh tokens rotate on each use ## Components ### Identity Provider Manages user accounts and credentials. Password hashing uses bcrypt with cost factor 12. **Technology:** Custom Go service **Scaling:** Horizontal stateless ### Token Service Issues and validates access tokens. Token lifetime is 1 hour with 7-day refresh tokens. **Technology:** JWT with RS256 **Scaling:** Horizontal stateless ### SSO Gateway Integrates with external identity providers. Supports Google, Microsoft, Okta, and custom SAML. **Technology:** Dex OIDC proxy **Scaling:** Active-passive ## Data Flow Credentials \u2192 Validation \u2192 Token Issue \u2192 Token Refresh \u2192 Logout ## Performance Characteristics - **Latency P50:** 50ms - **Latency P99:** 200ms - **Throughput:** 5000 auth/second",
              "score": null
            },
            {
              "doc_id": "api_webhooks",
              "chunk_id": "api_webhooks_fix_0",
              "content": "# Webhooks API Reference ## Overview The Webhooks API provides programmatic access to webhooks functionality. Base URL: `https://api.cloudflow.io/v1/webhooks` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /webhooks List configured webhooks. **Parameters:** - `active` (boolean): Filter by active status **Response:** - Success: `200` - Returns array of webhook objects - Error: `400` - Bad Request **Example:** ```json {\"webhooks\": [{\"id\": \"wh_01\", \"url\": \"https://example.com/hook\", \"events\": [\"workflow.completed\"]}]} ``` ### POST /webhooks Create a new webhook endpoint. **Parameters:** - `url` (string): Webhook URL (must be HTTPS) - `events` (array): Events to subscribe to - `secret` (string): Signing secret for verification **Response:** - Success: `201` - Webhook created - Error: `400` - Bad Request **Example:** ```json {\"id\": \"wh_02\", \"url\": \"https://example.com/new-hook\", \"secret\": \"whsec_...\"} ```",
              "score": null
            }
          ],
          "latency_ms": 4.248031000315677,
          "query_id": "arch_034",
          "dimension": "original",
          "query": "How does the API Gateway work?"
        },
        {
          "key_facts": [
            "Kong",
            "TLS termination",
            "50000 requests/second",
            "5ms"
          ],
          "found_facts": [
            "Kong",
            "TLS termination",
            "50000 requests/second",
            "5ms"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "arch_api_gateway"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "arch_api_gateway",
              "chunk_id": "arch_api_gateway_fix_0",
              "content": "# API Gateway Architecture ## Overview The API Gateway handles all incoming HTTP requests, providing authentication, rate limiting, and request routing. Built on Kong with custom plugins for CloudFlow-specific functionality. ## Design Principles - **Zero Trust**: Every request is authenticated and authorized - **Defense in Depth**: Multiple layers of security validation ## Components ### Kong Gateway Primary ingress controller. Handles TLS termination, request routing, and plugin execution. **Technology:** Kong 3.4 on Kubernetes **Scaling:** Horizontal with HPA ### Auth Plugin Validates JWT tokens, API keys, and OAuth2 flows. Token validation latency under 5ms. **Technology:** Custom Lua plugin **Scaling:** Stateless ### Rate Limiter Enforces per-user and per-IP rate limits. Default limit is 100 requests per minute. **Technology:** Redis-backed sliding window **Scaling:** Shared Redis cluster ## Data Flow Client \u2192 TLS \u2192 Kong \u2192 Auth \u2192 Rate Limit \u2192 Backend Services ## Performance Characteristics - **Latency P50:** 12ms - **Latency P99:** 45ms - **Throughput:** 50000 requests/second",
              "score": null
            },
            {
              "doc_id": "config_api_gateway",
              "chunk_id": "config_api_gateway_fix_0",
              "content": "# API Gateway Configuration Reference ## Overview This document describes all configuration options for API Gateway. ## Environment Variables | Variable | Type | Default | Description | |----------|------|---------|-------------| | `GATEWAY_PORT` | integer | 8000 | Gateway listen port | | `GATEWAY_RATE_LIMIT` | integer | 100 | Requests per minute per user | | `GATEWAY_TIMEOUT_MS` | integer | 30000 | Request timeout in milliseconds | | `GATEWAY_MAX_BODY_SIZE` | string | 10MB | Maximum request body size | | `GATEWAY_CORS_ORIGINS` | string | * | Allowed CORS origins | | `GATEWAY_SSL_ENABLED` | boolean | true | Enable TLS termination | ## Configuration File Configuration can also be provided via `api gateway.yaml`: ```yaml gateway_port: 8000 gateway_rate_limit: 100 gateway_timeout_ms: 30000 gateway_max_body_size: 10MB gateway_cors_origins: * ``` ## Validation Configuration is validated at startup. Invalid configuration will prevent the service from starting.",
              "score": null
            },
            {
              "doc_id": "adr_007",
              "chunk_id": "adr_007_fix_0",
              "content": "# ADR-007: Implement Rate Limiting at API Gateway **Status:** Accepted **Date:** 2024-09-03 ## Context We need to protect our services from abuse and ensure fair resource usage across customers. ## Options Considered ### Option 1: Sliding window at gateway Rate limit at API gateway level. **Pros:** - Centralized enforcement - Low latency - Easy configuration **Cons:** - Gateway becomes bottleneck - Coarse granularity - Redis dependency ### Option 2: Per-service limiting Each service implements its own limits. **Pros:** - Fine-grained control - Service autonomy - No single point of failure **Cons:** - Inconsistent implementation - Higher complexity - Distributed state ### Option 3: Token bucket Token-based rate limiting. **Pros:** - Allows bursts - Smooth rate limiting - Well understood **Cons:** - More complex - Harder to explain to users - State management ## Decision We will implement sliding window rate limiting at the Kong API Gateway using Redis. ## Consequences - Default limit: 100 requests per minute - Implement per-plan limits - Return rate limit headers - Add retry-after on 429",
              "score": null
            },
            {
              "doc_id": "howto_configure_custom_domain",
              "chunk_id": "howto_configure_custom_domain_fix_0",
              "content": "# How to Configure Custom Domain ## Prerequisites - Domain with DNS access - CloudFlow Pro plan or higher ## Steps ### Step 1: Verify domain ownership Add a TXT record to verify domain ownership. ```bash cloudflow domains verify --domain app.example.com ``` ### Step 2: Configure DNS Add a CNAME record pointing to your CloudFlow endpoint. > **Note:** CNAME should point to ingress.cloudflow.io ### Step 3: Enable SSL CloudFlow automatically provisions SSL certificates via Let's Encrypt. ```bash cloudflow domains ssl --domain app.example.com ``` ### Step 4: Set as primary Make this the primary domain for your application. ```bash cloudflow domains set-primary --domain app.example.com ``` ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            },
            {
              "doc_id": "arch_workflow_engine",
              "chunk_id": "arch_workflow_engine_fix_0",
              "content": "# Workflow Engine Architecture ## Overview The Workflow Engine is the core component responsible for executing user-defined workflows. It uses an event-driven architecture with support for parallel execution, conditional branching, and automatic retries. ## Design Principles - **Idempotency**: All operations can be safely retried without side effects - **Eventual Consistency**: State converges to consistent view within 5 seconds - **Fault Tolerance**: System continues operating despite component failures ## Components ### Scheduler Manages workflow scheduling and triggers. Supports cron expressions, webhooks, and event-based triggers. **Technology:** Custom Go service **Scaling:** Horizontal with leader election ### Executor Runs individual workflow steps in isolated containers. Maximum execution time is 30 minutes per step. **Technology:** Kubernetes Jobs **Scaling:** Auto-scaling based on queue depth ### State Store Persists workflow state, execution history, and step results. Uses optimistic locking for concurrent updates. **Technology:** PostgreSQL with JSONB **Scaling:** Primary-replica with read replicas ## Data Flow Triggers \u2192 Scheduler \u2192 Job Queue \u2192 Executor \u2192 State Store \u2192 Webhooks/Notifications ## Performance Characteristics - **Latency P50:** 45ms - **Latency P99:** 250ms - **Throughput:** 5000 executions/minute",
              "score": null
            }
          ],
          "latency_ms": 3.6194399999658344,
          "query_id": "arch_034",
          "dimension": "synonym",
          "query": "What is the design of the ingress layer and request routing?"
        },
        {
          "key_facts": [
            "Kong",
            "TLS termination",
            "50000 requests/second",
            "5ms"
          ],
          "found_facts": [
            "Kong",
            "TLS termination",
            "50000 requests/second",
            "5ms"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "arch_api_gateway"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "arch_api_gateway",
              "chunk_id": "arch_api_gateway_fix_0",
              "content": "# API Gateway Architecture ## Overview The API Gateway handles all incoming HTTP requests, providing authentication, rate limiting, and request routing. Built on Kong with custom plugins for CloudFlow-specific functionality. ## Design Principles - **Zero Trust**: Every request is authenticated and authorized - **Defense in Depth**: Multiple layers of security validation ## Components ### Kong Gateway Primary ingress controller. Handles TLS termination, request routing, and plugin execution. **Technology:** Kong 3.4 on Kubernetes **Scaling:** Horizontal with HPA ### Auth Plugin Validates JWT tokens, API keys, and OAuth2 flows. Token validation latency under 5ms. **Technology:** Custom Lua plugin **Scaling:** Stateless ### Rate Limiter Enforces per-user and per-IP rate limits. Default limit is 100 requests per minute. **Technology:** Redis-backed sliding window **Scaling:** Shared Redis cluster ## Data Flow Client \u2192 TLS \u2192 Kong \u2192 Auth \u2192 Rate Limit \u2192 Backend Services ## Performance Characteristics - **Latency P50:** 12ms - **Latency P99:** 45ms - **Throughput:** 50000 requests/second",
              "score": null
            },
            {
              "doc_id": "adr_007",
              "chunk_id": "adr_007_fix_0",
              "content": "# ADR-007: Implement Rate Limiting at API Gateway **Status:** Accepted **Date:** 2024-09-03 ## Context We need to protect our services from abuse and ensure fair resource usage across customers. ## Options Considered ### Option 1: Sliding window at gateway Rate limit at API gateway level. **Pros:** - Centralized enforcement - Low latency - Easy configuration **Cons:** - Gateway becomes bottleneck - Coarse granularity - Redis dependency ### Option 2: Per-service limiting Each service implements its own limits. **Pros:** - Fine-grained control - Service autonomy - No single point of failure **Cons:** - Inconsistent implementation - Higher complexity - Distributed state ### Option 3: Token bucket Token-based rate limiting. **Pros:** - Allows bursts - Smooth rate limiting - Well understood **Cons:** - More complex - Harder to explain to users - State management ## Decision We will implement sliding window rate limiting at the Kong API Gateway using Redis. ## Consequences - Default limit: 100 requests per minute - Implement per-plan limits - Return rate limit headers - Add retry-after on 429",
              "score": null
            },
            {
              "doc_id": "config_api_gateway",
              "chunk_id": "config_api_gateway_fix_0",
              "content": "# API Gateway Configuration Reference ## Overview This document describes all configuration options for API Gateway. ## Environment Variables | Variable | Type | Default | Description | |----------|------|---------|-------------| | `GATEWAY_PORT` | integer | 8000 | Gateway listen port | | `GATEWAY_RATE_LIMIT` | integer | 100 | Requests per minute per user | | `GATEWAY_TIMEOUT_MS` | integer | 30000 | Request timeout in milliseconds | | `GATEWAY_MAX_BODY_SIZE` | string | 10MB | Maximum request body size | | `GATEWAY_CORS_ORIGINS` | string | * | Allowed CORS origins | | `GATEWAY_SSL_ENABLED` | boolean | true | Enable TLS termination | ## Configuration File Configuration can also be provided via `api gateway.yaml`: ```yaml gateway_port: 8000 gateway_rate_limit: 100 gateway_timeout_ms: 30000 gateway_max_body_size: 10MB gateway_cors_origins: * ``` ## Validation Configuration is validated at startup. Invalid configuration will prevent the service from starting.",
              "score": null
            },
            {
              "doc_id": "arch_authentication_service",
              "chunk_id": "arch_authentication_service_fix_0",
              "content": "# Authentication Service Architecture ## Overview The Authentication Service handles user identity, session management, and token issuance. Supports OAuth 2.0, SAML, and API key authentication. ## Design Principles - **Zero Knowledge**: Passwords never stored in plaintext - **Token Rotation**: Refresh tokens rotate on each use ## Components ### Identity Provider Manages user accounts and credentials. Password hashing uses bcrypt with cost factor 12. **Technology:** Custom Go service **Scaling:** Horizontal stateless ### Token Service Issues and validates access tokens. Token lifetime is 1 hour with 7-day refresh tokens. **Technology:** JWT with RS256 **Scaling:** Horizontal stateless ### SSO Gateway Integrates with external identity providers. Supports Google, Microsoft, Okta, and custom SAML. **Technology:** Dex OIDC proxy **Scaling:** Active-passive ## Data Flow Credentials \u2192 Validation \u2192 Token Issue \u2192 Token Refresh \u2192 Logout ## Performance Characteristics - **Latency P50:** 50ms - **Latency P99:** 200ms - **Throughput:** 5000 auth/second",
              "score": null
            },
            {
              "doc_id": "howto_create_api_integration",
              "chunk_id": "howto_create_api_integration_fix_0",
              "content": "# How to Create API Integration ## Prerequisites - CloudFlow account ## Steps ### Step 1: Generate API key Create an API key for programmatic access. ```bash cloudflow apikeys create --name 'CI Integration' --scopes workflows:read,workflows:execute ``` ### Step 2: Store key securely Never commit API keys to source control. Use environment variables or secret management. > **Note:** Rotate keys every 90 days ### Step 3: Test authentication Verify the API key works correctly. ```bash curl -H 'Authorization: Bearer cf_...' https://api.cloudflow.io/v1/me ``` ### Step 4: Implement rate limiting Handle rate limit responses (HTTP 429) with exponential backoff. > **Note:** Default limit is 100 requests per minute ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            }
          ],
          "latency_ms": 3.433555000810884,
          "query_id": "arch_034",
          "dimension": "problem",
          "query": "Getting 429 errors and want to understand how rate limiting and auth validation happen at the gateway"
        },
        {
          "key_facts": [
            "Kong",
            "TLS termination",
            "50000 requests/second",
            "5ms"
          ],
          "found_facts": [
            "Kong",
            "TLS termination",
            "50000 requests/second",
            "5ms"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "arch_api_gateway"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "howto_create_api_integration",
              "chunk_id": "howto_create_api_integration_fix_0",
              "content": "# How to Create API Integration ## Prerequisites - CloudFlow account ## Steps ### Step 1: Generate API key Create an API key for programmatic access. ```bash cloudflow apikeys create --name 'CI Integration' --scopes workflows:read,workflows:execute ``` ### Step 2: Store key securely Never commit API keys to source control. Use environment variables or secret management. > **Note:** Rotate keys every 90 days ### Step 3: Test authentication Verify the API key works correctly. ```bash curl -H 'Authorization: Bearer cf_...' https://api.cloudflow.io/v1/me ``` ### Step 4: Implement rate limiting Handle rate limit responses (HTTP 429) with exponential backoff. > **Note:** Default limit is 100 requests per minute ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            },
            {
              "doc_id": "howto_migrate_from_v1_to_v2_api",
              "chunk_id": "howto_migrate_from_v1_to_v2_api_fix_0",
              "content": "# How to Migrate from v1 to v2 API ## Prerequisites - Existing v1 integration - v2 API access enabled ## Steps ### Step 1: Review breaking changes The v2 API changes authentication from API keys to OAuth 2.0. Review the migration guide. ### Step 2: Update client library Upgrade to the latest SDK version. ```bash npm install @cloudflow/sdk@latest ``` ### Step 3: Update authentication Replace API key authentication with OAuth 2.0 client credentials flow. > **Note:** Generate new credentials in the dashboard ### Step 4: Test in staging Run your integration tests against the v2 staging endpoint. ```bash CLOUDFLOW_API_URL=https://api-v2-staging.cloudflow.io npm test ``` ### Step 5: Switch production Update production configuration to use v2 API. ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            },
            {
              "doc_id": "howto_set_up_ci_cd_pipeline",
              "chunk_id": "howto_set_up_ci_cd_pipeline_fix_0",
              "content": "# How to Set Up CI/CD Pipeline ## Prerequisites - GitHub account - CloudFlow API key - Repository admin access ## Steps ### Step 1: Create GitHub Actions workflow Create a new file at `.github/workflows/cloudflow.yml` with the deployment configuration. ```bash mkdir -p .github/workflows && touch .github/workflows/cloudflow.yml ``` ### Step 2: Configure secrets Add your CloudFlow API key to GitHub secrets. ```bash gh secret set CLOUDFLOW_API_KEY ``` ### Step 3: Add deployment step Add the CloudFlow deployment action to your workflow. > **Note:** Use the official cloudflow/deploy-action@v2 ### Step 4: Test the pipeline Push a commit to trigger the workflow. ```bash git push origin main ``` ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            },
            {
              "doc_id": "arch_api_gateway",
              "chunk_id": "arch_api_gateway_fix_0",
              "content": "# API Gateway Architecture ## Overview The API Gateway handles all incoming HTTP requests, providing authentication, rate limiting, and request routing. Built on Kong with custom plugins for CloudFlow-specific functionality. ## Design Principles - **Zero Trust**: Every request is authenticated and authorized - **Defense in Depth**: Multiple layers of security validation ## Components ### Kong Gateway Primary ingress controller. Handles TLS termination, request routing, and plugin execution. **Technology:** Kong 3.4 on Kubernetes **Scaling:** Horizontal with HPA ### Auth Plugin Validates JWT tokens, API keys, and OAuth2 flows. Token validation latency under 5ms. **Technology:** Custom Lua plugin **Scaling:** Stateless ### Rate Limiter Enforces per-user and per-IP rate limits. Default limit is 100 requests per minute. **Technology:** Redis-backed sliding window **Scaling:** Shared Redis cluster ## Data Flow Client \u2192 TLS \u2192 Kong \u2192 Auth \u2192 Rate Limit \u2192 Backend Services ## Performance Characteristics - **Latency P50:** 12ms - **Latency P99:** 45ms - **Throughput:** 50000 requests/second",
              "score": null
            },
            {
              "doc_id": "config_api_gateway",
              "chunk_id": "config_api_gateway_fix_0",
              "content": "# API Gateway Configuration Reference ## Overview This document describes all configuration options for API Gateway. ## Environment Variables | Variable | Type | Default | Description | |----------|------|---------|-------------| | `GATEWAY_PORT` | integer | 8000 | Gateway listen port | | `GATEWAY_RATE_LIMIT` | integer | 100 | Requests per minute per user | | `GATEWAY_TIMEOUT_MS` | integer | 30000 | Request timeout in milliseconds | | `GATEWAY_MAX_BODY_SIZE` | string | 10MB | Maximum request body size | | `GATEWAY_CORS_ORIGINS` | string | * | Allowed CORS origins | | `GATEWAY_SSL_ENABLED` | boolean | true | Enable TLS termination | ## Configuration File Configuration can also be provided via `api gateway.yaml`: ```yaml gateway_port: 8000 gateway_rate_limit: 100 gateway_timeout_ms: 30000 gateway_max_body_size: 10MB gateway_cors_origins: * ``` ## Validation Configuration is validated at startup. Invalid configuration will prevent the service from starting.",
              "score": null
            }
          ],
          "latency_ms": 4.2530200007604435,
          "query_id": "arch_034",
          "dimension": "casual",
          "query": "api gateway setup cloudflow"
        },
        {
          "key_facts": [
            "Kong",
            "TLS termination",
            "50000 requests/second",
            "5ms"
          ],
          "found_facts": [
            "Kong",
            "TLS termination",
            "50000 requests/second",
            "5ms"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "arch_api_gateway"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "arch_api_gateway",
              "chunk_id": "arch_api_gateway_fix_0",
              "content": "# API Gateway Architecture ## Overview The API Gateway handles all incoming HTTP requests, providing authentication, rate limiting, and request routing. Built on Kong with custom plugins for CloudFlow-specific functionality. ## Design Principles - **Zero Trust**: Every request is authenticated and authorized - **Defense in Depth**: Multiple layers of security validation ## Components ### Kong Gateway Primary ingress controller. Handles TLS termination, request routing, and plugin execution. **Technology:** Kong 3.4 on Kubernetes **Scaling:** Horizontal with HPA ### Auth Plugin Validates JWT tokens, API keys, and OAuth2 flows. Token validation latency under 5ms. **Technology:** Custom Lua plugin **Scaling:** Stateless ### Rate Limiter Enforces per-user and per-IP rate limits. Default limit is 100 requests per minute. **Technology:** Redis-backed sliding window **Scaling:** Shared Redis cluster ## Data Flow Client \u2192 TLS \u2192 Kong \u2192 Auth \u2192 Rate Limit \u2192 Backend Services ## Performance Characteristics - **Latency P50:** 12ms - **Latency P99:** 45ms - **Throughput:** 50000 requests/second",
              "score": null
            },
            {
              "doc_id": "adr_007",
              "chunk_id": "adr_007_fix_0",
              "content": "# ADR-007: Implement Rate Limiting at API Gateway **Status:** Accepted **Date:** 2024-09-03 ## Context We need to protect our services from abuse and ensure fair resource usage across customers. ## Options Considered ### Option 1: Sliding window at gateway Rate limit at API gateway level. **Pros:** - Centralized enforcement - Low latency - Easy configuration **Cons:** - Gateway becomes bottleneck - Coarse granularity - Redis dependency ### Option 2: Per-service limiting Each service implements its own limits. **Pros:** - Fine-grained control - Service autonomy - No single point of failure **Cons:** - Inconsistent implementation - Higher complexity - Distributed state ### Option 3: Token bucket Token-based rate limiting. **Pros:** - Allows bursts - Smooth rate limiting - Well understood **Cons:** - More complex - Harder to explain to users - State management ## Decision We will implement sliding window rate limiting at the Kong API Gateway using Redis. ## Consequences - Default limit: 100 requests per minute - Implement per-plan limits - Return rate limit headers - Add retry-after on 429",
              "score": null
            },
            {
              "doc_id": "arch_authentication_service",
              "chunk_id": "arch_authentication_service_fix_0",
              "content": "# Authentication Service Architecture ## Overview The Authentication Service handles user identity, session management, and token issuance. Supports OAuth 2.0, SAML, and API key authentication. ## Design Principles - **Zero Knowledge**: Passwords never stored in plaintext - **Token Rotation**: Refresh tokens rotate on each use ## Components ### Identity Provider Manages user accounts and credentials. Password hashing uses bcrypt with cost factor 12. **Technology:** Custom Go service **Scaling:** Horizontal stateless ### Token Service Issues and validates access tokens. Token lifetime is 1 hour with 7-day refresh tokens. **Technology:** JWT with RS256 **Scaling:** Horizontal stateless ### SSO Gateway Integrates with external identity providers. Supports Google, Microsoft, Okta, and custom SAML. **Technology:** Dex OIDC proxy **Scaling:** Active-passive ## Data Flow Credentials \u2192 Validation \u2192 Token Issue \u2192 Token Refresh \u2192 Logout ## Performance Characteristics - **Latency P50:** 50ms - **Latency P99:** 200ms - **Throughput:** 5000 auth/second",
              "score": null
            },
            {
              "doc_id": "adr_003",
              "chunk_id": "adr_003_fix_0",
              "content": "# ADR-003: Implement OAuth 2.0 with JWT for Authentication **Status:** Accepted **Date:** 2024-05-08 ## Context We need a secure authentication system that supports API keys, user sessions, and third-party integrations. ## Options Considered ### Option 1: OAuth 2.0 with JWT Industry standard token-based auth. **Pros:** - Industry standard - Stateless validation - Supports refresh tokens - SSO compatible **Cons:** - Token size overhead - Cannot revoke individual tokens - Complexity ### Option 2: Session-based auth Traditional server-side sessions. **Pros:** - Simple implementation - Easy revocation - Smaller payloads **Cons:** - Requires session store - Not suitable for APIs - Scaling challenges ### Option 3: API keys only Simple key-based authentication. **Pros:** - Very simple - Low overhead - Easy to implement **Cons:** - No user context - Hard to rotate - No SSO support ## Decision We will implement OAuth 2.0 with JWT tokens. Access tokens expire after 1 hour, refresh tokens after 7 days. ## Consequences - Implement token rotation on refresh - Support API keys for machine-to-machine auth - Add MFA for admin accounts",
              "score": null
            },
            {
              "doc_id": "howto_create_api_integration",
              "chunk_id": "howto_create_api_integration_fix_0",
              "content": "# How to Create API Integration ## Prerequisites - CloudFlow account ## Steps ### Step 1: Generate API key Create an API key for programmatic access. ```bash cloudflow apikeys create --name 'CI Integration' --scopes workflows:read,workflows:execute ``` ### Step 2: Store key securely Never commit API keys to source control. Use environment variables or secret management. > **Note:** Rotate keys every 90 days ### Step 3: Test authentication Verify the API key works correctly. ```bash curl -H 'Authorization: Bearer cf_...' https://api.cloudflow.io/v1/me ``` ### Step 4: Implement rate limiting Handle rate limit responses (HTTP 429) with exponential backoff. > **Note:** Default limit is 100 requests per minute ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            }
          ],
          "latency_ms": 4.457861999981105,
          "query_id": "arch_034",
          "dimension": "contextual",
          "query": "Doing a security review and need to understand how requests flow through authentication and rate limiting before hitting backend services"
        },
        {
          "key_facts": [
            "Kong",
            "TLS termination",
            "50000 requests/second",
            "5ms"
          ],
          "found_facts": [
            "Kong",
            "TLS termination",
            "50000 requests/second",
            "5ms"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "arch_api_gateway"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "config_api_gateway",
              "chunk_id": "config_api_gateway_fix_0",
              "content": "# API Gateway Configuration Reference ## Overview This document describes all configuration options for API Gateway. ## Environment Variables | Variable | Type | Default | Description | |----------|------|---------|-------------| | `GATEWAY_PORT` | integer | 8000 | Gateway listen port | | `GATEWAY_RATE_LIMIT` | integer | 100 | Requests per minute per user | | `GATEWAY_TIMEOUT_MS` | integer | 30000 | Request timeout in milliseconds | | `GATEWAY_MAX_BODY_SIZE` | string | 10MB | Maximum request body size | | `GATEWAY_CORS_ORIGINS` | string | * | Allowed CORS origins | | `GATEWAY_SSL_ENABLED` | boolean | true | Enable TLS termination | ## Configuration File Configuration can also be provided via `api gateway.yaml`: ```yaml gateway_port: 8000 gateway_rate_limit: 100 gateway_timeout_ms: 30000 gateway_max_body_size: 10MB gateway_cors_origins: * ``` ## Validation Configuration is validated at startup. Invalid configuration will prevent the service from starting.",
              "score": null
            },
            {
              "doc_id": "arch_api_gateway",
              "chunk_id": "arch_api_gateway_fix_0",
              "content": "# API Gateway Architecture ## Overview The API Gateway handles all incoming HTTP requests, providing authentication, rate limiting, and request routing. Built on Kong with custom plugins for CloudFlow-specific functionality. ## Design Principles - **Zero Trust**: Every request is authenticated and authorized - **Defense in Depth**: Multiple layers of security validation ## Components ### Kong Gateway Primary ingress controller. Handles TLS termination, request routing, and plugin execution. **Technology:** Kong 3.4 on Kubernetes **Scaling:** Horizontal with HPA ### Auth Plugin Validates JWT tokens, API keys, and OAuth2 flows. Token validation latency under 5ms. **Technology:** Custom Lua plugin **Scaling:** Stateless ### Rate Limiter Enforces per-user and per-IP rate limits. Default limit is 100 requests per minute. **Technology:** Redis-backed sliding window **Scaling:** Shared Redis cluster ## Data Flow Client \u2192 TLS \u2192 Kong \u2192 Auth \u2192 Rate Limit \u2192 Backend Services ## Performance Characteristics - **Latency P50:** 12ms - **Latency P99:** 45ms - **Throughput:** 50000 requests/second",
              "score": null
            },
            {
              "doc_id": "arch_authentication_service",
              "chunk_id": "arch_authentication_service_fix_0",
              "content": "# Authentication Service Architecture ## Overview The Authentication Service handles user identity, session management, and token issuance. Supports OAuth 2.0, SAML, and API key authentication. ## Design Principles - **Zero Knowledge**: Passwords never stored in plaintext - **Token Rotation**: Refresh tokens rotate on each use ## Components ### Identity Provider Manages user accounts and credentials. Password hashing uses bcrypt with cost factor 12. **Technology:** Custom Go service **Scaling:** Horizontal stateless ### Token Service Issues and validates access tokens. Token lifetime is 1 hour with 7-day refresh tokens. **Technology:** JWT with RS256 **Scaling:** Horizontal stateless ### SSO Gateway Integrates with external identity providers. Supports Google, Microsoft, Okta, and custom SAML. **Technology:** Dex OIDC proxy **Scaling:** Active-passive ## Data Flow Credentials \u2192 Validation \u2192 Token Issue \u2192 Token Refresh \u2192 Logout ## Performance Characteristics - **Latency P50:** 50ms - **Latency P99:** 200ms - **Throughput:** 5000 auth/second",
              "score": null
            },
            {
              "doc_id": "runbook_ssl_certificate_expiry",
              "chunk_id": "runbook_ssl_certificate_expiry_fix_0",
              "content": "# Runbook: SSL Certificate Expiry ## Severity **Level:** P3 - Medium **Response Time:** 15 minutes ## Symptoms - Certificate expiring within 7 days - SSL handshake failures - Browser security warnings ## Diagnosis ### Step 1 Check certificate expiry: `echo | openssl s_client -connect api.cloudflow.io:443 2>/dev/null | openssl x509 -noout -dates` ### Step 2 Verify cert-manager status: `kubectl get certificates -n production` ### Step 3 Check cert-manager logs: `kubectl logs -l app=cert-manager -n cert-manager --tail=50` ### Step 4 Verify DNS is correct: `dig +short api.cloudflow.io` ## Resolution ### Step 1 Trigger manual renewal: `kubectl delete certificate api-cert -n production && kubectl apply -f certificate.yaml` ### Step 2 Check rate limits: Let's Encrypt has 50 certs/domain/week limit ### Step 3 Use staging issuer for testing: switch to letsencrypt-staging ClusterIssuer ### Step 4 If Let's Encrypt fails, manually upload certificate ## Escalation If certificate cannot be renewed, escalate to security team immediately. Consider using backup certificate. ## Post-Incident - Document timeline in incident log - Schedule post-mortem within 48 hours - Update runbook if new patterns discovered",
              "score": null
            },
            {
              "doc_id": "howto_create_api_integration",
              "chunk_id": "howto_create_api_integration_fix_0",
              "content": "# How to Create API Integration ## Prerequisites - CloudFlow account ## Steps ### Step 1: Generate API key Create an API key for programmatic access. ```bash cloudflow apikeys create --name 'CI Integration' --scopes workflows:read,workflows:execute ``` ### Step 2: Store key securely Never commit API keys to source control. Use environment variables or secret management. > **Note:** Rotate keys every 90 days ### Step 3: Test authentication Verify the API key works correctly. ```bash curl -H 'Authorization: Bearer cf_...' https://api.cloudflow.io/v1/me ``` ### Step 4: Implement rate limiting Handle rate limit responses (HTTP 429) with exponential backoff. > **Note:** Default limit is 100 requests per minute ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            }
          ],
          "latency_ms": 3.636702000221703,
          "query_id": "arch_034",
          "dimension": "negation",
          "query": "I don't get how TLS termination and token validation work in the API gateway"
        },
        {
          "key_facts": [
            "Kafka",
            "Flink",
            "Spark",
            "100000 messages per second",
            "Iceberg"
          ],
          "found_facts": [
            "Kafka",
            "Flink",
            "Spark",
            "100000 messages per second",
            "Iceberg"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "arch_data_pipeline"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "arch_data_pipeline",
              "chunk_id": "arch_data_pipeline_fix_0",
              "content": "# Data Pipeline Architecture ## Overview The Data Pipeline processes and transforms data between workflow steps. It supports batch processing up to 10GB and streaming for real-time use cases. ## Design Principles - **Exactly-Once Processing**: Each record is processed exactly once - **Backpressure Handling**: System gracefully handles varying load ## Components ### Stream Processor Handles real-time data streams. Maximum throughput of 100000 messages per second. **Technology:** Apache Kafka with Flink **Scaling:** Partition-based scaling ### Batch Processor Processes large batch jobs. Supports data formats: JSON, CSV, Parquet, Avro. **Technology:** Apache Spark on Kubernetes **Scaling:** Dynamic resource allocation ### Data Lake Long-term data storage with time-travel queries. Retention configurable up to 7 years. **Technology:** S3 with Iceberg tables **Scaling:** Unlimited with tiered storage ## Data Flow Input \u2192 Validation \u2192 Transform \u2192 Enrich \u2192 Output ## Performance Characteristics - **Latency P50:** 100ms (streaming), 5min (batch) - **Latency P99:** 500ms (streaming), 30min (batch) - **Throughput:** 100000 messages/second (streaming)",
              "score": null
            },
            {
              "doc_id": "arch_workflow_engine",
              "chunk_id": "arch_workflow_engine_fix_0",
              "content": "# Workflow Engine Architecture ## Overview The Workflow Engine is the core component responsible for executing user-defined workflows. It uses an event-driven architecture with support for parallel execution, conditional branching, and automatic retries. ## Design Principles - **Idempotency**: All operations can be safely retried without side effects - **Eventual Consistency**: State converges to consistent view within 5 seconds - **Fault Tolerance**: System continues operating despite component failures ## Components ### Scheduler Manages workflow scheduling and triggers. Supports cron expressions, webhooks, and event-based triggers. **Technology:** Custom Go service **Scaling:** Horizontal with leader election ### Executor Runs individual workflow steps in isolated containers. Maximum execution time is 30 minutes per step. **Technology:** Kubernetes Jobs **Scaling:** Auto-scaling based on queue depth ### State Store Persists workflow state, execution history, and step results. Uses optimistic locking for concurrent updates. **Technology:** PostgreSQL with JSONB **Scaling:** Primary-replica with read replicas ## Data Flow Triggers \u2192 Scheduler \u2192 Job Queue \u2192 Executor \u2192 State Store \u2192 Webhooks/Notifications ## Performance Characteristics - **Latency P50:** 45ms - **Latency P99:** 250ms - **Throughput:** 5000 executions/minute",
              "score": null
            },
            {
              "doc_id": "adr_001",
              "chunk_id": "adr_001_fix_0",
              "content": "# ADR-001: Use PostgreSQL for Primary Database **Status:** Accepted **Date:** 2024-11-04 ## Context We need a database to store workflow definitions, execution state, and user data. The database must support ACID transactions, JSON queries, and scale to millions of records. ## Options Considered ### Option 1: PostgreSQL Mature relational database with JSONB support. **Pros:** - ACID compliance - Rich JSONB queries - Team expertise - Proven at scale **Cons:** - Manual sharding for extreme scale - Requires connection pooling ### Option 2: MongoDB Document database with native JSON. **Pros:** - Flexible schema - Built-in sharding - Good for documents **Cons:** - Weaker consistency guarantees - Complex transactions - Higher operational cost ### Option 3: CockroachDB Distributed SQL database. **Pros:** - Automatic sharding - Strong consistency - PostgreSQL compatible **Cons:** - Higher latency - More expensive - Less mature ecosystem ## Decision We will use PostgreSQL as our primary database with JSONB for flexible workflow definitions. ## Consequences - Need PgBouncer for connection pooling - Must plan for sharding at 100M+ records - Requires regular VACUUM maintenance",
              "score": null
            },
            {
              "doc_id": "adr_005",
              "chunk_id": "adr_005_fix_0",
              "content": "# ADR-005: Adopt Event-Driven Architecture for Workflow Execution **Status:** Accepted **Date:** 2024-12-04 ## Context We need an architecture that supports asynchronous workflow execution, step dependencies, and failure recovery. ## Options Considered ### Option 1: Event-driven with Kafka Publish-subscribe messaging. **Pros:** - Loose coupling - Scalability - Event replay - Audit trail **Cons:** - Eventual consistency - Operational complexity - Message ordering challenges ### Option 2: Request-response Synchronous API calls. **Pros:** - Simple mental model - Immediate feedback - Easy debugging **Cons:** - Tight coupling - Cascading failures - Timeout issues ### Option 3: Polling-based Workers poll for tasks. **Pros:** - Simple implementation - Easy rate limiting - No message broker **Cons:** - Polling overhead - Higher latency - Inefficient at scale ## Decision We will use an event-driven architecture with Kafka for workflow step execution. ## Consequences - Implement idempotent consumers - Use dead letter queues for failed messages - Monitor consumer lag",
              "score": null
            },
            {
              "doc_id": "arch_caching_layer",
              "chunk_id": "arch_caching_layer_fix_0",
              "content": "# Caching Layer Architecture ## Overview The Caching Layer reduces latency and database load by caching frequently accessed data. Uses a multi-tier strategy with local and distributed caches. ## Design Principles - **Cache Invalidation**: Explicit invalidation on writes - **Graceful Degradation**: System works without cache ## Components ### Local Cache First-level cache with 100ms TTL. Reduces Redis round-trips for hot data. **Technology:** Ristretto in-memory cache **Scaling:** Per-pod memory allocation ### Distributed Cache Second-level cache with configurable TTL. Maximum key size 512MB. **Technology:** Redis Cluster 7.x **Scaling:** 6-node cluster with sharding ### CDN Caches static assets and API responses. Cache hit ratio target 95%. **Technology:** CloudFront with Lambda@Edge **Scaling:** Global edge locations ## Data Flow Request \u2192 Local Cache \u2192 Redis \u2192 Database \u2192 Cache Population ## Performance Characteristics - **Latency P50:** 1ms (hit), 50ms (miss) - **Latency P99:** 5ms (hit), 200ms (miss) - **Throughput:** 100000 reads/second",
              "score": null
            }
          ],
          "latency_ms": 3.2999049999489216,
          "query_id": "arch_035",
          "dimension": "original",
          "query": "Describe the Data Pipeline architecture"
        },
        {
          "key_facts": [
            "Kafka",
            "Flink",
            "Spark",
            "100000 messages per second",
            "Iceberg"
          ],
          "found_facts": [
            "Kafka",
            "Flink",
            "Spark",
            "100000 messages per second",
            "Iceberg"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "arch_data_pipeline"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "arch_data_pipeline",
              "chunk_id": "arch_data_pipeline_fix_0",
              "content": "# Data Pipeline Architecture ## Overview The Data Pipeline processes and transforms data between workflow steps. It supports batch processing up to 10GB and streaming for real-time use cases. ## Design Principles - **Exactly-Once Processing**: Each record is processed exactly once - **Backpressure Handling**: System gracefully handles varying load ## Components ### Stream Processor Handles real-time data streams. Maximum throughput of 100000 messages per second. **Technology:** Apache Kafka with Flink **Scaling:** Partition-based scaling ### Batch Processor Processes large batch jobs. Supports data formats: JSON, CSV, Parquet, Avro. **Technology:** Apache Spark on Kubernetes **Scaling:** Dynamic resource allocation ### Data Lake Long-term data storage with time-travel queries. Retention configurable up to 7 years. **Technology:** S3 with Iceberg tables **Scaling:** Unlimited with tiered storage ## Data Flow Input \u2192 Validation \u2192 Transform \u2192 Enrich \u2192 Output ## Performance Characteristics - **Latency P50:** 100ms (streaming), 5min (batch) - **Latency P99:** 500ms (streaming), 30min (batch) - **Throughput:** 100000 messages/second (streaming)",
              "score": null
            },
            {
              "doc_id": "arch_workflow_engine",
              "chunk_id": "arch_workflow_engine_fix_0",
              "content": "# Workflow Engine Architecture ## Overview The Workflow Engine is the core component responsible for executing user-defined workflows. It uses an event-driven architecture with support for parallel execution, conditional branching, and automatic retries. ## Design Principles - **Idempotency**: All operations can be safely retried without side effects - **Eventual Consistency**: State converges to consistent view within 5 seconds - **Fault Tolerance**: System continues operating despite component failures ## Components ### Scheduler Manages workflow scheduling and triggers. Supports cron expressions, webhooks, and event-based triggers. **Technology:** Custom Go service **Scaling:** Horizontal with leader election ### Executor Runs individual workflow steps in isolated containers. Maximum execution time is 30 minutes per step. **Technology:** Kubernetes Jobs **Scaling:** Auto-scaling based on queue depth ### State Store Persists workflow state, execution history, and step results. Uses optimistic locking for concurrent updates. **Technology:** PostgreSQL with JSONB **Scaling:** Primary-replica with read replicas ## Data Flow Triggers \u2192 Scheduler \u2192 Job Queue \u2192 Executor \u2192 State Store \u2192 Webhooks/Notifications ## Performance Characteristics - **Latency P50:** 45ms - **Latency P99:** 250ms - **Throughput:** 5000 executions/minute",
              "score": null
            },
            {
              "doc_id": "arch_caching_layer",
              "chunk_id": "arch_caching_layer_fix_0",
              "content": "# Caching Layer Architecture ## Overview The Caching Layer reduces latency and database load by caching frequently accessed data. Uses a multi-tier strategy with local and distributed caches. ## Design Principles - **Cache Invalidation**: Explicit invalidation on writes - **Graceful Degradation**: System works without cache ## Components ### Local Cache First-level cache with 100ms TTL. Reduces Redis round-trips for hot data. **Technology:** Ristretto in-memory cache **Scaling:** Per-pod memory allocation ### Distributed Cache Second-level cache with configurable TTL. Maximum key size 512MB. **Technology:** Redis Cluster 7.x **Scaling:** 6-node cluster with sharding ### CDN Caches static assets and API responses. Cache hit ratio target 95%. **Technology:** CloudFront with Lambda@Edge **Scaling:** Global edge locations ## Data Flow Request \u2192 Local Cache \u2192 Redis \u2192 Database \u2192 Cache Population ## Performance Characteristics - **Latency P50:** 1ms (hit), 50ms (miss) - **Latency P99:** 5ms (hit), 200ms (miss) - **Throughput:** 100000 reads/second",
              "score": null
            },
            {
              "doc_id": "adr_005",
              "chunk_id": "adr_005_fix_0",
              "content": "# ADR-005: Adopt Event-Driven Architecture for Workflow Execution **Status:** Accepted **Date:** 2024-12-04 ## Context We need an architecture that supports asynchronous workflow execution, step dependencies, and failure recovery. ## Options Considered ### Option 1: Event-driven with Kafka Publish-subscribe messaging. **Pros:** - Loose coupling - Scalability - Event replay - Audit trail **Cons:** - Eventual consistency - Operational complexity - Message ordering challenges ### Option 2: Request-response Synchronous API calls. **Pros:** - Simple mental model - Immediate feedback - Easy debugging **Cons:** - Tight coupling - Cascading failures - Timeout issues ### Option 3: Polling-based Workers poll for tasks. **Pros:** - Simple implementation - Easy rate limiting - No message broker **Cons:** - Polling overhead - Higher latency - Inefficient at scale ## Decision We will use an event-driven architecture with Kafka for workflow step execution. ## Consequences - Implement idempotent consumers - Use dead letter queues for failed messages - Monitor consumer lag",
              "score": null
            },
            {
              "doc_id": "adr_007",
              "chunk_id": "adr_007_fix_0",
              "content": "# ADR-007: Implement Rate Limiting at API Gateway **Status:** Accepted **Date:** 2024-09-03 ## Context We need to protect our services from abuse and ensure fair resource usage across customers. ## Options Considered ### Option 1: Sliding window at gateway Rate limit at API gateway level. **Pros:** - Centralized enforcement - Low latency - Easy configuration **Cons:** - Gateway becomes bottleneck - Coarse granularity - Redis dependency ### Option 2: Per-service limiting Each service implements its own limits. **Pros:** - Fine-grained control - Service autonomy - No single point of failure **Cons:** - Inconsistent implementation - Higher complexity - Distributed state ### Option 3: Token bucket Token-based rate limiting. **Pros:** - Allows bursts - Smooth rate limiting - Well understood **Cons:** - More complex - Harder to explain to users - State management ## Decision We will implement sliding window rate limiting at the Kong API Gateway using Redis. ## Consequences - Default limit: 100 requests per minute - Implement per-plan limits - Return rate limit headers - Add retry-after on 429",
              "score": null
            }
          ],
          "latency_ms": 3.4458370009815553,
          "query_id": "arch_035",
          "dimension": "synonym",
          "query": "How is the ETL and stream processing system designed?"
        },
        {
          "key_facts": [
            "Kafka",
            "Flink",
            "Spark",
            "100000 messages per second",
            "Iceberg"
          ],
          "found_facts": [
            "Kafka",
            "Flink",
            "Spark",
            "100000 messages per second",
            "Iceberg"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "arch_data_pipeline"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "arch_data_pipeline",
              "chunk_id": "arch_data_pipeline_fix_0",
              "content": "# Data Pipeline Architecture ## Overview The Data Pipeline processes and transforms data between workflow steps. It supports batch processing up to 10GB and streaming for real-time use cases. ## Design Principles - **Exactly-Once Processing**: Each record is processed exactly once - **Backpressure Handling**: System gracefully handles varying load ## Components ### Stream Processor Handles real-time data streams. Maximum throughput of 100000 messages per second. **Technology:** Apache Kafka with Flink **Scaling:** Partition-based scaling ### Batch Processor Processes large batch jobs. Supports data formats: JSON, CSV, Parquet, Avro. **Technology:** Apache Spark on Kubernetes **Scaling:** Dynamic resource allocation ### Data Lake Long-term data storage with time-travel queries. Retention configurable up to 7 years. **Technology:** S3 with Iceberg tables **Scaling:** Unlimited with tiered storage ## Data Flow Input \u2192 Validation \u2192 Transform \u2192 Enrich \u2192 Output ## Performance Characteristics - **Latency P50:** 100ms (streaming), 5min (batch) - **Latency P99:** 500ms (streaming), 30min (batch) - **Throughput:** 100000 messages/second (streaming)",
              "score": null
            },
            {
              "doc_id": "arch_caching_layer",
              "chunk_id": "arch_caching_layer_fix_0",
              "content": "# Caching Layer Architecture ## Overview The Caching Layer reduces latency and database load by caching frequently accessed data. Uses a multi-tier strategy with local and distributed caches. ## Design Principles - **Cache Invalidation**: Explicit invalidation on writes - **Graceful Degradation**: System works without cache ## Components ### Local Cache First-level cache with 100ms TTL. Reduces Redis round-trips for hot data. **Technology:** Ristretto in-memory cache **Scaling:** Per-pod memory allocation ### Distributed Cache Second-level cache with configurable TTL. Maximum key size 512MB. **Technology:** Redis Cluster 7.x **Scaling:** 6-node cluster with sharding ### CDN Caches static assets and API responses. Cache hit ratio target 95%. **Technology:** CloudFront with Lambda@Edge **Scaling:** Global edge locations ## Data Flow Request \u2192 Local Cache \u2192 Redis \u2192 Database \u2192 Cache Population ## Performance Characteristics - **Latency P50:** 1ms (hit), 50ms (miss) - **Latency P99:** 5ms (hit), 200ms (miss) - **Throughput:** 100000 reads/second",
              "score": null
            },
            {
              "doc_id": "arch_workflow_engine",
              "chunk_id": "arch_workflow_engine_fix_0",
              "content": "# Workflow Engine Architecture ## Overview The Workflow Engine is the core component responsible for executing user-defined workflows. It uses an event-driven architecture with support for parallel execution, conditional branching, and automatic retries. ## Design Principles - **Idempotency**: All operations can be safely retried without side effects - **Eventual Consistency**: State converges to consistent view within 5 seconds - **Fault Tolerance**: System continues operating despite component failures ## Components ### Scheduler Manages workflow scheduling and triggers. Supports cron expressions, webhooks, and event-based triggers. **Technology:** Custom Go service **Scaling:** Horizontal with leader election ### Executor Runs individual workflow steps in isolated containers. Maximum execution time is 30 minutes per step. **Technology:** Kubernetes Jobs **Scaling:** Auto-scaling based on queue depth ### State Store Persists workflow state, execution history, and step results. Uses optimistic locking for concurrent updates. **Technology:** PostgreSQL with JSONB **Scaling:** Primary-replica with read replicas ## Data Flow Triggers \u2192 Scheduler \u2192 Job Queue \u2192 Executor \u2192 State Store \u2192 Webhooks/Notifications ## Performance Characteristics - **Latency P50:** 45ms - **Latency P99:** 250ms - **Throughput:** 5000 executions/minute",
              "score": null
            },
            {
              "doc_id": "adr_004",
              "chunk_id": "adr_004_fix_0",
              "content": "# ADR-004: Use Redis for Caching and Session Storage **Status:** Accepted **Date:** 2024-04-05 ## Context We need a fast data store for caching API responses, session data, and rate limiting counters. ## Options Considered ### Option 1: Redis Cluster In-memory data structure store. **Pros:** - Sub-millisecond latency - Rich data structures - Pub/sub support - Clustering **Cons:** - Memory cost - Persistence complexity - Cluster management ### Option 2: Memcached Simple key-value cache. **Pros:** - Simple operation - Multi-threaded - Lower memory overhead **Cons:** - No persistence - Limited data types - No clustering ### Option 3: Application-level cache In-process caching only. **Pros:** - No external dependency - Fastest possible - Simple **Cons:** - Not shared across pods - Memory pressure - Cold starts ## Decision We will use Redis Cluster for caching with a 6-node cluster for high availability. ## Consequences - Implement cache-aside pattern - Set appropriate TTLs - Monitor memory usage - Plan for cache stampede prevention",
              "score": null
            },
            {
              "doc_id": "runbook_kafka_consumer_lag",
              "chunk_id": "runbook_kafka_consumer_lag_fix_0",
              "content": "# Runbook: Kafka Consumer Lag ## Severity **Level:** P1 - Critical **Response Time:** 1 hour ## Symptoms - Consumer lag > 10000 messages - Processing delays reported - Alerts from lag monitor ## Diagnosis ### Step 1 Check consumer group lag: `kafka-consumer-groups.sh --bootstrap-server kafka:9092 --describe --group workflow-executor` ### Step 2 Verify consumer health: `kubectl get pods -l app=workflow-executor -n production` ### Step 3 Check for processing errors: `kubectl logs -l app=workflow-executor --tail=100 -n production | grep ERROR` ### Step 4 Monitor throughput: `cloudflow metrics get kafka.consumer.records_per_second` ## Resolution ### Step 1 Scale up consumers: `kubectl scale deployment/workflow-executor --replicas=10 -n production` ### Step 2 Reset offset if messages are stale: `kafka-consumer-groups.sh --bootstrap-server kafka:9092 --group workflow-executor --reset-offsets --to-latest --execute --topic workflows` ### Step 3 Pause producer if backpressure needed: `cloudflow workflows pause-triggers` ### Step 4 Increase partition count for parallelism (requires coordination) ## Escalation If lag persists > 30 minutes, page platform team. May need Kafka cluster scaling. ## Post-Incident - Document timeline in incident log - Schedule post-mortem within 48 hours - Update runbook if new patterns discovered",
              "score": null
            }
          ],
          "latency_ms": 4.391847998704179,
          "query_id": "arch_035",
          "dimension": "problem",
          "query": "Seeing data processing delays and need to understand the batch vs streaming components and their throughput limits"
        },
        {
          "key_facts": [
            "Kafka",
            "Flink",
            "Spark",
            "100000 messages per second",
            "Iceberg"
          ],
          "found_facts": [
            "Kafka",
            "Flink",
            "Spark",
            "100000 messages per second",
            "Iceberg"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "arch_data_pipeline"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "arch_data_pipeline",
              "chunk_id": "arch_data_pipeline_fix_0",
              "content": "# Data Pipeline Architecture ## Overview The Data Pipeline processes and transforms data between workflow steps. It supports batch processing up to 10GB and streaming for real-time use cases. ## Design Principles - **Exactly-Once Processing**: Each record is processed exactly once - **Backpressure Handling**: System gracefully handles varying load ## Components ### Stream Processor Handles real-time data streams. Maximum throughput of 100000 messages per second. **Technology:** Apache Kafka with Flink **Scaling:** Partition-based scaling ### Batch Processor Processes large batch jobs. Supports data formats: JSON, CSV, Parquet, Avro. **Technology:** Apache Spark on Kubernetes **Scaling:** Dynamic resource allocation ### Data Lake Long-term data storage with time-travel queries. Retention configurable up to 7 years. **Technology:** S3 with Iceberg tables **Scaling:** Unlimited with tiered storage ## Data Flow Input \u2192 Validation \u2192 Transform \u2192 Enrich \u2192 Output ## Performance Characteristics - **Latency P50:** 100ms (streaming), 5min (batch) - **Latency P99:** 500ms (streaming), 30min (batch) - **Throughput:** 100000 messages/second (streaming)",
              "score": null
            },
            {
              "doc_id": "adr_001",
              "chunk_id": "adr_001_fix_0",
              "content": "# ADR-001: Use PostgreSQL for Primary Database **Status:** Accepted **Date:** 2024-11-04 ## Context We need a database to store workflow definitions, execution state, and user data. The database must support ACID transactions, JSON queries, and scale to millions of records. ## Options Considered ### Option 1: PostgreSQL Mature relational database with JSONB support. **Pros:** - ACID compliance - Rich JSONB queries - Team expertise - Proven at scale **Cons:** - Manual sharding for extreme scale - Requires connection pooling ### Option 2: MongoDB Document database with native JSON. **Pros:** - Flexible schema - Built-in sharding - Good for documents **Cons:** - Weaker consistency guarantees - Complex transactions - Higher operational cost ### Option 3: CockroachDB Distributed SQL database. **Pros:** - Automatic sharding - Strong consistency - PostgreSQL compatible **Cons:** - Higher latency - More expensive - Less mature ecosystem ## Decision We will use PostgreSQL as our primary database with JSONB for flexible workflow definitions. ## Consequences - Need PgBouncer for connection pooling - Must plan for sharding at 100M+ records - Requires regular VACUUM maintenance",
              "score": null
            },
            {
              "doc_id": "arch_workflow_engine",
              "chunk_id": "arch_workflow_engine_fix_0",
              "content": "# Workflow Engine Architecture ## Overview The Workflow Engine is the core component responsible for executing user-defined workflows. It uses an event-driven architecture with support for parallel execution, conditional branching, and automatic retries. ## Design Principles - **Idempotency**: All operations can be safely retried without side effects - **Eventual Consistency**: State converges to consistent view within 5 seconds - **Fault Tolerance**: System continues operating despite component failures ## Components ### Scheduler Manages workflow scheduling and triggers. Supports cron expressions, webhooks, and event-based triggers. **Technology:** Custom Go service **Scaling:** Horizontal with leader election ### Executor Runs individual workflow steps in isolated containers. Maximum execution time is 30 minutes per step. **Technology:** Kubernetes Jobs **Scaling:** Auto-scaling based on queue depth ### State Store Persists workflow state, execution history, and step results. Uses optimistic locking for concurrent updates. **Technology:** PostgreSQL with JSONB **Scaling:** Primary-replica with read replicas ## Data Flow Triggers \u2192 Scheduler \u2192 Job Queue \u2192 Executor \u2192 State Store \u2192 Webhooks/Notifications ## Performance Characteristics - **Latency P50:** 45ms - **Latency P99:** 250ms - **Throughput:** 5000 executions/minute",
              "score": null
            },
            {
              "doc_id": "adr_005",
              "chunk_id": "adr_005_fix_0",
              "content": "# ADR-005: Adopt Event-Driven Architecture for Workflow Execution **Status:** Accepted **Date:** 2024-12-04 ## Context We need an architecture that supports asynchronous workflow execution, step dependencies, and failure recovery. ## Options Considered ### Option 1: Event-driven with Kafka Publish-subscribe messaging. **Pros:** - Loose coupling - Scalability - Event replay - Audit trail **Cons:** - Eventual consistency - Operational complexity - Message ordering challenges ### Option 2: Request-response Synchronous API calls. **Pros:** - Simple mental model - Immediate feedback - Easy debugging **Cons:** - Tight coupling - Cascading failures - Timeout issues ### Option 3: Polling-based Workers poll for tasks. **Pros:** - Simple implementation - Easy rate limiting - No message broker **Cons:** - Polling overhead - Higher latency - Inefficient at scale ## Decision We will use an event-driven architecture with Kafka for workflow step execution. ## Consequences - Implement idempotent consumers - Use dead letter queues for failed messages - Monitor consumer lag",
              "score": null
            },
            {
              "doc_id": "arch_caching_layer",
              "chunk_id": "arch_caching_layer_fix_0",
              "content": "# Caching Layer Architecture ## Overview The Caching Layer reduces latency and database load by caching frequently accessed data. Uses a multi-tier strategy with local and distributed caches. ## Design Principles - **Cache Invalidation**: Explicit invalidation on writes - **Graceful Degradation**: System works without cache ## Components ### Local Cache First-level cache with 100ms TTL. Reduces Redis round-trips for hot data. **Technology:** Ristretto in-memory cache **Scaling:** Per-pod memory allocation ### Distributed Cache Second-level cache with configurable TTL. Maximum key size 512MB. **Technology:** Redis Cluster 7.x **Scaling:** 6-node cluster with sharding ### CDN Caches static assets and API responses. Cache hit ratio target 95%. **Technology:** CloudFront with Lambda@Edge **Scaling:** Global edge locations ## Data Flow Request \u2192 Local Cache \u2192 Redis \u2192 Database \u2192 Cache Population ## Performance Characteristics - **Latency P50:** 1ms (hit), 50ms (miss) - **Latency P99:** 5ms (hit), 200ms (miss) - **Throughput:** 100000 reads/second",
              "score": null
            }
          ],
          "latency_ms": 3.6127780003880616,
          "query_id": "arch_035",
          "dimension": "casual",
          "query": "data pipeline architecture"
        },
        {
          "key_facts": [
            "Kafka",
            "Flink",
            "Spark",
            "100000 messages per second",
            "Iceberg"
          ],
          "found_facts": [
            "Kafka",
            "Flink",
            "Spark",
            "100000 messages per second",
            "Iceberg"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "arch_data_pipeline"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "arch_data_pipeline",
              "chunk_id": "arch_data_pipeline_fix_0",
              "content": "# Data Pipeline Architecture ## Overview The Data Pipeline processes and transforms data between workflow steps. It supports batch processing up to 10GB and streaming for real-time use cases. ## Design Principles - **Exactly-Once Processing**: Each record is processed exactly once - **Backpressure Handling**: System gracefully handles varying load ## Components ### Stream Processor Handles real-time data streams. Maximum throughput of 100000 messages per second. **Technology:** Apache Kafka with Flink **Scaling:** Partition-based scaling ### Batch Processor Processes large batch jobs. Supports data formats: JSON, CSV, Parquet, Avro. **Technology:** Apache Spark on Kubernetes **Scaling:** Dynamic resource allocation ### Data Lake Long-term data storage with time-travel queries. Retention configurable up to 7 years. **Technology:** S3 with Iceberg tables **Scaling:** Unlimited with tiered storage ## Data Flow Input \u2192 Validation \u2192 Transform \u2192 Enrich \u2192 Output ## Performance Characteristics - **Latency P50:** 100ms (streaming), 5min (batch) - **Latency P99:** 500ms (streaming), 30min (batch) - **Throughput:** 100000 messages/second (streaming)",
              "score": null
            },
            {
              "doc_id": "arch_workflow_engine",
              "chunk_id": "arch_workflow_engine_fix_0",
              "content": "# Workflow Engine Architecture ## Overview The Workflow Engine is the core component responsible for executing user-defined workflows. It uses an event-driven architecture with support for parallel execution, conditional branching, and automatic retries. ## Design Principles - **Idempotency**: All operations can be safely retried without side effects - **Eventual Consistency**: State converges to consistent view within 5 seconds - **Fault Tolerance**: System continues operating despite component failures ## Components ### Scheduler Manages workflow scheduling and triggers. Supports cron expressions, webhooks, and event-based triggers. **Technology:** Custom Go service **Scaling:** Horizontal with leader election ### Executor Runs individual workflow steps in isolated containers. Maximum execution time is 30 minutes per step. **Technology:** Kubernetes Jobs **Scaling:** Auto-scaling based on queue depth ### State Store Persists workflow state, execution history, and step results. Uses optimistic locking for concurrent updates. **Technology:** PostgreSQL with JSONB **Scaling:** Primary-replica with read replicas ## Data Flow Triggers \u2192 Scheduler \u2192 Job Queue \u2192 Executor \u2192 State Store \u2192 Webhooks/Notifications ## Performance Characteristics - **Latency P50:** 45ms - **Latency P99:** 250ms - **Throughput:** 5000 executions/minute",
              "score": null
            },
            {
              "doc_id": "arch_api_gateway",
              "chunk_id": "arch_api_gateway_fix_0",
              "content": "# API Gateway Architecture ## Overview The API Gateway handles all incoming HTTP requests, providing authentication, rate limiting, and request routing. Built on Kong with custom plugins for CloudFlow-specific functionality. ## Design Principles - **Zero Trust**: Every request is authenticated and authorized - **Defense in Depth**: Multiple layers of security validation ## Components ### Kong Gateway Primary ingress controller. Handles TLS termination, request routing, and plugin execution. **Technology:** Kong 3.4 on Kubernetes **Scaling:** Horizontal with HPA ### Auth Plugin Validates JWT tokens, API keys, and OAuth2 flows. Token validation latency under 5ms. **Technology:** Custom Lua plugin **Scaling:** Stateless ### Rate Limiter Enforces per-user and per-IP rate limits. Default limit is 100 requests per minute. **Technology:** Redis-backed sliding window **Scaling:** Shared Redis cluster ## Data Flow Client \u2192 TLS \u2192 Kong \u2192 Auth \u2192 Rate Limit \u2192 Backend Services ## Performance Characteristics - **Latency P50:** 12ms - **Latency P99:** 45ms - **Throughput:** 50000 requests/second",
              "score": null
            },
            {
              "doc_id": "howto_set_up_database_connection",
              "chunk_id": "howto_set_up_database_connection_fix_0",
              "content": "# How to Set Up Database Connection ## Prerequisites - Database with network access to CloudFlow - Connection credentials ## Steps ### Step 1: Add connection secret Store your database connection string as a secret. ```bash cloudflow secrets set DATABASE_URL postgresql://user:pass@host:5432/db ``` ### Step 2: Configure connection pool Set pool size based on your plan limits. Free tier allows 5 connections, Pro allows 20. > **Note:** Pool exhaustion will queue requests ### Step 3: Test connection Verify the database is accessible from CloudFlow. ```bash cloudflow db test --secret DATABASE_URL ``` ### Step 4: Enable SSL For production, always require SSL connections. ```bash cloudflow db configure --require-ssl ``` ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            },
            {
              "doc_id": "adr_008",
              "chunk_id": "adr_008_fix_0",
              "content": "# ADR-008: Use S3 for Object Storage **Status:** Accepted **Date:** 2024-10-14 ## Context We need storage for workflow artifacts, execution outputs, and user uploads. ## Options Considered ### Option 1: Amazon S3 Object storage service. **Pros:** - 99.999999999% durability - Lifecycle policies - Event notifications - CDN integration **Cons:** - AWS lock-in - Eventual consistency for overwrites - Cost at scale ### Option 2: Self-hosted MinIO S3-compatible object storage. **Pros:** - No vendor lock-in - S3 compatible - On-premises option **Cons:** - Operational burden - Lower durability guarantees - Scaling complexity ### Option 3: Google Cloud Storage GCP object storage. **Pros:** - Strong consistency - Good performance - Unified billing **Cons:** - GCP lock-in - Fewer features than S3 - Smaller ecosystem ## Decision We will use Amazon S3 for object storage with intelligent tiering for cost optimization. ## Consequences - Enable versioning for critical buckets - Implement lifecycle policies - Use pre-signed URLs for uploads - Enable server-side encryption",
              "score": null
            }
          ],
          "latency_ms": 3.4485820015106583,
          "query_id": "arch_035",
          "dimension": "contextual",
          "query": "Planning a high-volume data integration and need to understand how CloudFlow handles streaming and batch processing at scale"
        },
        {
          "key_facts": [
            "Kafka",
            "Flink",
            "Spark",
            "100000 messages per second",
            "Iceberg"
          ],
          "found_facts": [
            "Kafka",
            "Flink",
            "Spark",
            "100000 messages per second",
            "Iceberg"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "arch_data_pipeline"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "arch_data_pipeline",
              "chunk_id": "arch_data_pipeline_fix_0",
              "content": "# Data Pipeline Architecture ## Overview The Data Pipeline processes and transforms data between workflow steps. It supports batch processing up to 10GB and streaming for real-time use cases. ## Design Principles - **Exactly-Once Processing**: Each record is processed exactly once - **Backpressure Handling**: System gracefully handles varying load ## Components ### Stream Processor Handles real-time data streams. Maximum throughput of 100000 messages per second. **Technology:** Apache Kafka with Flink **Scaling:** Partition-based scaling ### Batch Processor Processes large batch jobs. Supports data formats: JSON, CSV, Parquet, Avro. **Technology:** Apache Spark on Kubernetes **Scaling:** Dynamic resource allocation ### Data Lake Long-term data storage with time-travel queries. Retention configurable up to 7 years. **Technology:** S3 with Iceberg tables **Scaling:** Unlimited with tiered storage ## Data Flow Input \u2192 Validation \u2192 Transform \u2192 Enrich \u2192 Output ## Performance Characteristics - **Latency P50:** 100ms (streaming), 5min (batch) - **Latency P99:** 500ms (streaming), 30min (batch) - **Throughput:** 100000 messages/second (streaming)",
              "score": null
            },
            {
              "doc_id": "arch_workflow_engine",
              "chunk_id": "arch_workflow_engine_fix_0",
              "content": "# Workflow Engine Architecture ## Overview The Workflow Engine is the core component responsible for executing user-defined workflows. It uses an event-driven architecture with support for parallel execution, conditional branching, and automatic retries. ## Design Principles - **Idempotency**: All operations can be safely retried without side effects - **Eventual Consistency**: State converges to consistent view within 5 seconds - **Fault Tolerance**: System continues operating despite component failures ## Components ### Scheduler Manages workflow scheduling and triggers. Supports cron expressions, webhooks, and event-based triggers. **Technology:** Custom Go service **Scaling:** Horizontal with leader election ### Executor Runs individual workflow steps in isolated containers. Maximum execution time is 30 minutes per step. **Technology:** Kubernetes Jobs **Scaling:** Auto-scaling based on queue depth ### State Store Persists workflow state, execution history, and step results. Uses optimistic locking for concurrent updates. **Technology:** PostgreSQL with JSONB **Scaling:** Primary-replica with read replicas ## Data Flow Triggers \u2192 Scheduler \u2192 Job Queue \u2192 Executor \u2192 State Store \u2192 Webhooks/Notifications ## Performance Characteristics - **Latency P50:** 45ms - **Latency P99:** 250ms - **Throughput:** 5000 executions/minute",
              "score": null
            },
            {
              "doc_id": "arch_caching_layer",
              "chunk_id": "arch_caching_layer_fix_0",
              "content": "# Caching Layer Architecture ## Overview The Caching Layer reduces latency and database load by caching frequently accessed data. Uses a multi-tier strategy with local and distributed caches. ## Design Principles - **Cache Invalidation**: Explicit invalidation on writes - **Graceful Degradation**: System works without cache ## Components ### Local Cache First-level cache with 100ms TTL. Reduces Redis round-trips for hot data. **Technology:** Ristretto in-memory cache **Scaling:** Per-pod memory allocation ### Distributed Cache Second-level cache with configurable TTL. Maximum key size 512MB. **Technology:** Redis Cluster 7.x **Scaling:** 6-node cluster with sharding ### CDN Caches static assets and API responses. Cache hit ratio target 95%. **Technology:** CloudFront with Lambda@Edge **Scaling:** Global edge locations ## Data Flow Request \u2192 Local Cache \u2192 Redis \u2192 Database \u2192 Cache Population ## Performance Characteristics - **Latency P50:** 1ms (hit), 50ms (miss) - **Latency P99:** 5ms (hit), 200ms (miss) - **Throughput:** 100000 reads/second",
              "score": null
            },
            {
              "doc_id": "adr_004",
              "chunk_id": "adr_004_fix_0",
              "content": "# ADR-004: Use Redis for Caching and Session Storage **Status:** Accepted **Date:** 2024-04-05 ## Context We need a fast data store for caching API responses, session data, and rate limiting counters. ## Options Considered ### Option 1: Redis Cluster In-memory data structure store. **Pros:** - Sub-millisecond latency - Rich data structures - Pub/sub support - Clustering **Cons:** - Memory cost - Persistence complexity - Cluster management ### Option 2: Memcached Simple key-value cache. **Pros:** - Simple operation - Multi-threaded - Lower memory overhead **Cons:** - No persistence - Limited data types - No clustering ### Option 3: Application-level cache In-process caching only. **Pros:** - No external dependency - Fastest possible - Simple **Cons:** - Not shared across pods - Memory pressure - Cold starts ## Decision We will use Redis Cluster for caching with a 6-node cluster for high availability. ## Consequences - Implement cache-aside pattern - Set appropriate TTLs - Monitor memory usage - Plan for cache stampede prevention",
              "score": null
            },
            {
              "doc_id": "runbook_high_cpu_usage",
              "chunk_id": "runbook_high_cpu_usage_fix_0",
              "content": "# Runbook: High CPU Usage ## Severity **Level:** P1 - Critical **Response Time:** 15 minutes ## Symptoms - CPU utilization > 80% sustained - Increased response latency - Autoscaler at maximum replicas ## Diagnosis ### Step 1 Check current CPU usage: `kubectl top pods -n production` ### Step 2 Identify hot pods: `kubectl top pods --sort-by=cpu -n production | head -10` ### Step 3 Check for recent deployments: `kubectl rollout history deployment/api -n production` ### Step 4 Profile the application: `cloudflow debug profile --pod api-xyz --duration 60s` ## Resolution ### Step 1 Scale up if traffic-related: `kubectl scale deployment/api --replicas=10 -n production` ### Step 2 Restart pods if memory leak suspected: `kubectl rollout restart deployment/api -n production` ### Step 3 Roll back recent deployment if regression: `kubectl rollout undo deployment/api -n production` ### Step 4 Add resource limits if unbounded: update deployment with CPU limits ## Escalation If CPU remains high after scaling, escalate to platform team. Page on-call if affecting users. ## Post-Incident - Document timeline in incident log - Schedule post-mortem within 48 hours - Update runbook if new patterns discovered",
              "score": null
            }
          ],
          "latency_ms": 3.4960709999722894,
          "query_id": "arch_035",
          "dimension": "negation",
          "query": "I'm confused about how data moves between stream processor, batch processor, and data lake"
        },
        {
          "key_facts": [
            "Elasticsearch",
            "500ms",
            "autocomplete",
            "20ms"
          ],
          "found_facts": [
            "Elasticsearch",
            "500ms",
            "autocomplete",
            "20ms"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "arch_search_service"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "arch_search_service",
              "chunk_id": "arch_search_service_fix_0",
              "content": "# Search Service Architecture ## Overview The Search Service provides full-text and semantic search across workflows, executions, and documentation. Built on Elasticsearch with custom analyzers for technical content. ## Design Principles - **Relevance First**: Results ranked by relevance, not recency - **Typo Tolerance**: Handles misspellings and variations ## Components ### Indexer Indexes documents in near real-time. Typical indexing latency is 500ms. **Technology:** Custom Go service **Scaling:** Queue-based with backpressure ### Query Engine Executes search queries. Supports filters, facets, and aggregations. **Technology:** Elasticsearch 8.x **Scaling:** 3-node cluster with replicas ### Suggestion Service Provides autocomplete suggestions. Returns suggestions within 20ms. **Technology:** Completion suggester **Scaling:** In-memory FST ## Data Flow Document \u2192 Analyzer \u2192 Index \u2192 Query \u2192 Rank \u2192 Results ## Performance Characteristics - **Latency P50:** 25ms - **Latency P99:** 100ms - **Throughput:** 2000 queries/second",
              "score": null
            },
            {
              "doc_id": "arch_monitoring_stack",
              "chunk_id": "arch_monitoring_stack_fix_0",
              "content": "# Monitoring Stack Architecture ## Overview The Monitoring Stack provides observability into system health, performance, and errors. Includes metrics, logs, traces, and alerting. ## Design Principles - **Observability**: Every component emits metrics and traces - **Actionable Alerts**: Alerts include runbook links ## Components ### Metrics Collects time-series metrics. Retention is 15 days local, 1 year in object storage. **Technology:** Prometheus with Thanos **Scaling:** Federated with long-term storage ### Logging Centralized log aggregation. Supports structured JSON logs with label indexing. **Technology:** Loki with Grafana **Scaling:** Distributed with sharding ### Tracing Distributed request tracing. Trace retention is 7 days. **Technology:** Jaeger with OpenTelemetry **Scaling:** Sampling at 10% ### Alerting Routes alerts to on-call engineers. Supports escalation policies and silencing. **Technology:** Alertmanager with PagerDuty **Scaling:** HA pair ## Data Flow Application \u2192 Collector \u2192 Storage \u2192 Query \u2192 Dashboard/Alert ## Performance Characteristics - **Latency P50:** N/A - **Latency P99:** N/A - **Throughput:** 1M metrics/second, 100GB logs/day",
              "score": null
            },
            {
              "doc_id": "arch_data_pipeline",
              "chunk_id": "arch_data_pipeline_fix_0",
              "content": "# Data Pipeline Architecture ## Overview The Data Pipeline processes and transforms data between workflow steps. It supports batch processing up to 10GB and streaming for real-time use cases. ## Design Principles - **Exactly-Once Processing**: Each record is processed exactly once - **Backpressure Handling**: System gracefully handles varying load ## Components ### Stream Processor Handles real-time data streams. Maximum throughput of 100000 messages per second. **Technology:** Apache Kafka with Flink **Scaling:** Partition-based scaling ### Batch Processor Processes large batch jobs. Supports data formats: JSON, CSV, Parquet, Avro. **Technology:** Apache Spark on Kubernetes **Scaling:** Dynamic resource allocation ### Data Lake Long-term data storage with time-travel queries. Retention configurable up to 7 years. **Technology:** S3 with Iceberg tables **Scaling:** Unlimited with tiered storage ## Data Flow Input \u2192 Validation \u2192 Transform \u2192 Enrich \u2192 Output ## Performance Characteristics - **Latency P50:** 100ms (streaming), 5min (batch) - **Latency P99:** 500ms (streaming), 30min (batch) - **Throughput:** 100000 messages/second (streaming)",
              "score": null
            },
            {
              "doc_id": "arch_authentication_service",
              "chunk_id": "arch_authentication_service_fix_0",
              "content": "# Authentication Service Architecture ## Overview The Authentication Service handles user identity, session management, and token issuance. Supports OAuth 2.0, SAML, and API key authentication. ## Design Principles - **Zero Knowledge**: Passwords never stored in plaintext - **Token Rotation**: Refresh tokens rotate on each use ## Components ### Identity Provider Manages user accounts and credentials. Password hashing uses bcrypt with cost factor 12. **Technology:** Custom Go service **Scaling:** Horizontal stateless ### Token Service Issues and validates access tokens. Token lifetime is 1 hour with 7-day refresh tokens. **Technology:** JWT with RS256 **Scaling:** Horizontal stateless ### SSO Gateway Integrates with external identity providers. Supports Google, Microsoft, Okta, and custom SAML. **Technology:** Dex OIDC proxy **Scaling:** Active-passive ## Data Flow Credentials \u2192 Validation \u2192 Token Issue \u2192 Token Refresh \u2192 Logout ## Performance Characteristics - **Latency P50:** 50ms - **Latency P99:** 200ms - **Throughput:** 5000 auth/second",
              "score": null
            },
            {
              "doc_id": "arch_caching_layer",
              "chunk_id": "arch_caching_layer_fix_0",
              "content": "# Caching Layer Architecture ## Overview The Caching Layer reduces latency and database load by caching frequently accessed data. Uses a multi-tier strategy with local and distributed caches. ## Design Principles - **Cache Invalidation**: Explicit invalidation on writes - **Graceful Degradation**: System works without cache ## Components ### Local Cache First-level cache with 100ms TTL. Reduces Redis round-trips for hot data. **Technology:** Ristretto in-memory cache **Scaling:** Per-pod memory allocation ### Distributed Cache Second-level cache with configurable TTL. Maximum key size 512MB. **Technology:** Redis Cluster 7.x **Scaling:** 6-node cluster with sharding ### CDN Caches static assets and API responses. Cache hit ratio target 95%. **Technology:** CloudFront with Lambda@Edge **Scaling:** Global edge locations ## Data Flow Request \u2192 Local Cache \u2192 Redis \u2192 Database \u2192 Cache Population ## Performance Characteristics - **Latency P50:** 1ms (hit), 50ms (miss) - **Latency P99:** 5ms (hit), 200ms (miss) - **Throughput:** 100000 reads/second",
              "score": null
            }
          ],
          "latency_ms": 4.3190529995627,
          "query_id": "arch_036",
          "dimension": "original",
          "query": "How does the Search Service work?"
        },
        {
          "key_facts": [
            "Elasticsearch",
            "500ms",
            "autocomplete",
            "20ms"
          ],
          "found_facts": [
            "Elasticsearch",
            "500ms",
            "autocomplete",
            "20ms"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "arch_search_service"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "arch_search_service",
              "chunk_id": "arch_search_service_fix_0",
              "content": "# Search Service Architecture ## Overview The Search Service provides full-text and semantic search across workflows, executions, and documentation. Built on Elasticsearch with custom analyzers for technical content. ## Design Principles - **Relevance First**: Results ranked by relevance, not recency - **Typo Tolerance**: Handles misspellings and variations ## Components ### Indexer Indexes documents in near real-time. Typical indexing latency is 500ms. **Technology:** Custom Go service **Scaling:** Queue-based with backpressure ### Query Engine Executes search queries. Supports filters, facets, and aggregations. **Technology:** Elasticsearch 8.x **Scaling:** 3-node cluster with replicas ### Suggestion Service Provides autocomplete suggestions. Returns suggestions within 20ms. **Technology:** Completion suggester **Scaling:** In-memory FST ## Data Flow Document \u2192 Analyzer \u2192 Index \u2192 Query \u2192 Rank \u2192 Results ## Performance Characteristics - **Latency P50:** 25ms - **Latency P99:** 100ms - **Throughput:** 2000 queries/second",
              "score": null
            },
            {
              "doc_id": "arch_data_pipeline",
              "chunk_id": "arch_data_pipeline_fix_0",
              "content": "# Data Pipeline Architecture ## Overview The Data Pipeline processes and transforms data between workflow steps. It supports batch processing up to 10GB and streaming for real-time use cases. ## Design Principles - **Exactly-Once Processing**: Each record is processed exactly once - **Backpressure Handling**: System gracefully handles varying load ## Components ### Stream Processor Handles real-time data streams. Maximum throughput of 100000 messages per second. **Technology:** Apache Kafka with Flink **Scaling:** Partition-based scaling ### Batch Processor Processes large batch jobs. Supports data formats: JSON, CSV, Parquet, Avro. **Technology:** Apache Spark on Kubernetes **Scaling:** Dynamic resource allocation ### Data Lake Long-term data storage with time-travel queries. Retention configurable up to 7 years. **Technology:** S3 with Iceberg tables **Scaling:** Unlimited with tiered storage ## Data Flow Input \u2192 Validation \u2192 Transform \u2192 Enrich \u2192 Output ## Performance Characteristics - **Latency P50:** 100ms (streaming), 5min (batch) - **Latency P99:** 500ms (streaming), 30min (batch) - **Throughput:** 100000 messages/second (streaming)",
              "score": null
            },
            {
              "doc_id": "arch_caching_layer",
              "chunk_id": "arch_caching_layer_fix_0",
              "content": "# Caching Layer Architecture ## Overview The Caching Layer reduces latency and database load by caching frequently accessed data. Uses a multi-tier strategy with local and distributed caches. ## Design Principles - **Cache Invalidation**: Explicit invalidation on writes - **Graceful Degradation**: System works without cache ## Components ### Local Cache First-level cache with 100ms TTL. Reduces Redis round-trips for hot data. **Technology:** Ristretto in-memory cache **Scaling:** Per-pod memory allocation ### Distributed Cache Second-level cache with configurable TTL. Maximum key size 512MB. **Technology:** Redis Cluster 7.x **Scaling:** 6-node cluster with sharding ### CDN Caches static assets and API responses. Cache hit ratio target 95%. **Technology:** CloudFront with Lambda@Edge **Scaling:** Global edge locations ## Data Flow Request \u2192 Local Cache \u2192 Redis \u2192 Database \u2192 Cache Population ## Performance Characteristics - **Latency P50:** 1ms (hit), 50ms (miss) - **Latency P99:** 5ms (hit), 200ms (miss) - **Throughput:** 100000 reads/second",
              "score": null
            },
            {
              "doc_id": "arch_monitoring_stack",
              "chunk_id": "arch_monitoring_stack_fix_0",
              "content": "# Monitoring Stack Architecture ## Overview The Monitoring Stack provides observability into system health, performance, and errors. Includes metrics, logs, traces, and alerting. ## Design Principles - **Observability**: Every component emits metrics and traces - **Actionable Alerts**: Alerts include runbook links ## Components ### Metrics Collects time-series metrics. Retention is 15 days local, 1 year in object storage. **Technology:** Prometheus with Thanos **Scaling:** Federated with long-term storage ### Logging Centralized log aggregation. Supports structured JSON logs with label indexing. **Technology:** Loki with Grafana **Scaling:** Distributed with sharding ### Tracing Distributed request tracing. Trace retention is 7 days. **Technology:** Jaeger with OpenTelemetry **Scaling:** Sampling at 10% ### Alerting Routes alerts to on-call engineers. Supports escalation policies and silencing. **Technology:** Alertmanager with PagerDuty **Scaling:** HA pair ## Data Flow Application \u2192 Collector \u2192 Storage \u2192 Query \u2192 Dashboard/Alert ## Performance Characteristics - **Latency P50:** N/A - **Latency P99:** N/A - **Throughput:** 1M metrics/second, 100GB logs/day",
              "score": null
            },
            {
              "doc_id": "arch_workflow_engine",
              "chunk_id": "arch_workflow_engine_fix_0",
              "content": "# Workflow Engine Architecture ## Overview The Workflow Engine is the core component responsible for executing user-defined workflows. It uses an event-driven architecture with support for parallel execution, conditional branching, and automatic retries. ## Design Principles - **Idempotency**: All operations can be safely retried without side effects - **Eventual Consistency**: State converges to consistent view within 5 seconds - **Fault Tolerance**: System continues operating despite component failures ## Components ### Scheduler Manages workflow scheduling and triggers. Supports cron expressions, webhooks, and event-based triggers. **Technology:** Custom Go service **Scaling:** Horizontal with leader election ### Executor Runs individual workflow steps in isolated containers. Maximum execution time is 30 minutes per step. **Technology:** Kubernetes Jobs **Scaling:** Auto-scaling based on queue depth ### State Store Persists workflow state, execution history, and step results. Uses optimistic locking for concurrent updates. **Technology:** PostgreSQL with JSONB **Scaling:** Primary-replica with read replicas ## Data Flow Triggers \u2192 Scheduler \u2192 Job Queue \u2192 Executor \u2192 State Store \u2192 Webhooks/Notifications ## Performance Characteristics - **Latency P50:** 45ms - **Latency P99:** 250ms - **Throughput:** 5000 executions/minute",
              "score": null
            }
          ],
          "latency_ms": 3.7285840007825755,
          "query_id": "arch_036",
          "dimension": "synonym",
          "query": "What is the full-text search system design and how does indexing work?"
        },
        {
          "key_facts": [
            "Elasticsearch",
            "500ms",
            "autocomplete",
            "20ms"
          ],
          "found_facts": [
            "Elasticsearch",
            "500ms",
            "autocomplete",
            "20ms"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "arch_search_service"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "arch_search_service",
              "chunk_id": "arch_search_service_fix_0",
              "content": "# Search Service Architecture ## Overview The Search Service provides full-text and semantic search across workflows, executions, and documentation. Built on Elasticsearch with custom analyzers for technical content. ## Design Principles - **Relevance First**: Results ranked by relevance, not recency - **Typo Tolerance**: Handles misspellings and variations ## Components ### Indexer Indexes documents in near real-time. Typical indexing latency is 500ms. **Technology:** Custom Go service **Scaling:** Queue-based with backpressure ### Query Engine Executes search queries. Supports filters, facets, and aggregations. **Technology:** Elasticsearch 8.x **Scaling:** 3-node cluster with replicas ### Suggestion Service Provides autocomplete suggestions. Returns suggestions within 20ms. **Technology:** Completion suggester **Scaling:** In-memory FST ## Data Flow Document \u2192 Analyzer \u2192 Index \u2192 Query \u2192 Rank \u2192 Results ## Performance Characteristics - **Latency P50:** 25ms - **Latency P99:** 100ms - **Throughput:** 2000 queries/second",
              "score": null
            },
            {
              "doc_id": "arch_caching_layer",
              "chunk_id": "arch_caching_layer_fix_0",
              "content": "# Caching Layer Architecture ## Overview The Caching Layer reduces latency and database load by caching frequently accessed data. Uses a multi-tier strategy with local and distributed caches. ## Design Principles - **Cache Invalidation**: Explicit invalidation on writes - **Graceful Degradation**: System works without cache ## Components ### Local Cache First-level cache with 100ms TTL. Reduces Redis round-trips for hot data. **Technology:** Ristretto in-memory cache **Scaling:** Per-pod memory allocation ### Distributed Cache Second-level cache with configurable TTL. Maximum key size 512MB. **Technology:** Redis Cluster 7.x **Scaling:** 6-node cluster with sharding ### CDN Caches static assets and API responses. Cache hit ratio target 95%. **Technology:** CloudFront with Lambda@Edge **Scaling:** Global edge locations ## Data Flow Request \u2192 Local Cache \u2192 Redis \u2192 Database \u2192 Cache Population ## Performance Characteristics - **Latency P50:** 1ms (hit), 50ms (miss) - **Latency P99:** 5ms (hit), 200ms (miss) - **Throughput:** 100000 reads/second",
              "score": null
            },
            {
              "doc_id": "arch_data_pipeline",
              "chunk_id": "arch_data_pipeline_fix_0",
              "content": "# Data Pipeline Architecture ## Overview The Data Pipeline processes and transforms data between workflow steps. It supports batch processing up to 10GB and streaming for real-time use cases. ## Design Principles - **Exactly-Once Processing**: Each record is processed exactly once - **Backpressure Handling**: System gracefully handles varying load ## Components ### Stream Processor Handles real-time data streams. Maximum throughput of 100000 messages per second. **Technology:** Apache Kafka with Flink **Scaling:** Partition-based scaling ### Batch Processor Processes large batch jobs. Supports data formats: JSON, CSV, Parquet, Avro. **Technology:** Apache Spark on Kubernetes **Scaling:** Dynamic resource allocation ### Data Lake Long-term data storage with time-travel queries. Retention configurable up to 7 years. **Technology:** S3 with Iceberg tables **Scaling:** Unlimited with tiered storage ## Data Flow Input \u2192 Validation \u2192 Transform \u2192 Enrich \u2192 Output ## Performance Characteristics - **Latency P50:** 100ms (streaming), 5min (batch) - **Latency P99:** 500ms (streaming), 30min (batch) - **Throughput:** 100000 messages/second (streaming)",
              "score": null
            },
            {
              "doc_id": "runbook_database_connection_exhaustion",
              "chunk_id": "runbook_database_connection_exhaustion_fix_0",
              "content": "# Runbook: Database Connection Exhaustion ## Severity **Level:** P1 - Critical **Response Time:** 15 minutes ## Symptoms - Connection pool at 100% - Queries timing out - 'too many connections' errors in logs ## Diagnosis ### Step 1 Check active connections: `SELECT count(*) FROM pg_stat_activity` ### Step 2 Identify long-running queries: `SELECT pid, now() - pg_stat_activity.query_start AS duration, query FROM pg_stat_activity WHERE state != 'idle' ORDER BY duration DESC` ### Step 3 Check PgBouncer stats: `SHOW POOLS` ### Step 4 Verify connection limits: `SHOW max_connections` ## Resolution ### Step 1 Kill long-running queries: `SELECT pg_terminate_backend(pid) FROM pg_stat_activity WHERE duration > interval '5 minutes'` ### Step 2 Increase PgBouncer pool size temporarily: update pgbouncer.ini ### Step 3 Restart affected services to release connections: `kubectl rollout restart deployment/api` ### Step 4 Scale down non-critical services to free connections ## Escalation If connections remain exhausted, escalate to DBA. Consider enabling connection queueing. ## Post-Incident - Document timeline in incident log - Schedule post-mortem within 48 hours - Update runbook if new patterns discovered",
              "score": null
            },
            {
              "doc_id": "adr_004",
              "chunk_id": "adr_004_fix_0",
              "content": "# ADR-004: Use Redis for Caching and Session Storage **Status:** Accepted **Date:** 2024-04-05 ## Context We need a fast data store for caching API responses, session data, and rate limiting counters. ## Options Considered ### Option 1: Redis Cluster In-memory data structure store. **Pros:** - Sub-millisecond latency - Rich data structures - Pub/sub support - Clustering **Cons:** - Memory cost - Persistence complexity - Cluster management ### Option 2: Memcached Simple key-value cache. **Pros:** - Simple operation - Multi-threaded - Lower memory overhead **Cons:** - No persistence - Limited data types - No clustering ### Option 3: Application-level cache In-process caching only. **Pros:** - No external dependency - Fastest possible - Simple **Cons:** - Not shared across pods - Memory pressure - Cold starts ## Decision We will use Redis Cluster for caching with a 6-node cluster for high availability. ## Consequences - Implement cache-aside pattern - Set appropriate TTLs - Monitor memory usage - Plan for cache stampede prevention",
              "score": null
            }
          ],
          "latency_ms": 3.6225059993739706,
          "query_id": "arch_036",
          "dimension": "problem",
          "query": "Search results are taking too long - need to understand the query engine and indexing latency"
        },
        {
          "key_facts": [
            "Elasticsearch",
            "500ms",
            "autocomplete",
            "20ms"
          ],
          "found_facts": [
            "Elasticsearch",
            "500ms",
            "autocomplete",
            "20ms"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "arch_search_service"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "arch_search_service",
              "chunk_id": "arch_search_service_fix_0",
              "content": "# Search Service Architecture ## Overview The Search Service provides full-text and semantic search across workflows, executions, and documentation. Built on Elasticsearch with custom analyzers for technical content. ## Design Principles - **Relevance First**: Results ranked by relevance, not recency - **Typo Tolerance**: Handles misspellings and variations ## Components ### Indexer Indexes documents in near real-time. Typical indexing latency is 500ms. **Technology:** Custom Go service **Scaling:** Queue-based with backpressure ### Query Engine Executes search queries. Supports filters, facets, and aggregations. **Technology:** Elasticsearch 8.x **Scaling:** 3-node cluster with replicas ### Suggestion Service Provides autocomplete suggestions. Returns suggestions within 20ms. **Technology:** Completion suggester **Scaling:** In-memory FST ## Data Flow Document \u2192 Analyzer \u2192 Index \u2192 Query \u2192 Rank \u2192 Results ## Performance Characteristics - **Latency P50:** 25ms - **Latency P99:** 100ms - **Throughput:** 2000 queries/second",
              "score": null
            },
            {
              "doc_id": "adr_004",
              "chunk_id": "adr_004_fix_0",
              "content": "# ADR-004: Use Redis for Caching and Session Storage **Status:** Accepted **Date:** 2024-04-05 ## Context We need a fast data store for caching API responses, session data, and rate limiting counters. ## Options Considered ### Option 1: Redis Cluster In-memory data structure store. **Pros:** - Sub-millisecond latency - Rich data structures - Pub/sub support - Clustering **Cons:** - Memory cost - Persistence complexity - Cluster management ### Option 2: Memcached Simple key-value cache. **Pros:** - Simple operation - Multi-threaded - Lower memory overhead **Cons:** - No persistence - Limited data types - No clustering ### Option 3: Application-level cache In-process caching only. **Pros:** - No external dependency - Fastest possible - Simple **Cons:** - Not shared across pods - Memory pressure - Cold starts ## Decision We will use Redis Cluster for caching with a 6-node cluster for high availability. ## Consequences - Implement cache-aside pattern - Set appropriate TTLs - Monitor memory usage - Plan for cache stampede prevention",
              "score": null
            },
            {
              "doc_id": "arch_data_pipeline",
              "chunk_id": "arch_data_pipeline_fix_0",
              "content": "# Data Pipeline Architecture ## Overview The Data Pipeline processes and transforms data between workflow steps. It supports batch processing up to 10GB and streaming for real-time use cases. ## Design Principles - **Exactly-Once Processing**: Each record is processed exactly once - **Backpressure Handling**: System gracefully handles varying load ## Components ### Stream Processor Handles real-time data streams. Maximum throughput of 100000 messages per second. **Technology:** Apache Kafka with Flink **Scaling:** Partition-based scaling ### Batch Processor Processes large batch jobs. Supports data formats: JSON, CSV, Parquet, Avro. **Technology:** Apache Spark on Kubernetes **Scaling:** Dynamic resource allocation ### Data Lake Long-term data storage with time-travel queries. Retention configurable up to 7 years. **Technology:** S3 with Iceberg tables **Scaling:** Unlimited with tiered storage ## Data Flow Input \u2192 Validation \u2192 Transform \u2192 Enrich \u2192 Output ## Performance Characteristics - **Latency P50:** 100ms (streaming), 5min (batch) - **Latency P99:** 500ms (streaming), 30min (batch) - **Throughput:** 100000 messages/second (streaming)",
              "score": null
            },
            {
              "doc_id": "arch_caching_layer",
              "chunk_id": "arch_caching_layer_fix_0",
              "content": "# Caching Layer Architecture ## Overview The Caching Layer reduces latency and database load by caching frequently accessed data. Uses a multi-tier strategy with local and distributed caches. ## Design Principles - **Cache Invalidation**: Explicit invalidation on writes - **Graceful Degradation**: System works without cache ## Components ### Local Cache First-level cache with 100ms TTL. Reduces Redis round-trips for hot data. **Technology:** Ristretto in-memory cache **Scaling:** Per-pod memory allocation ### Distributed Cache Second-level cache with configurable TTL. Maximum key size 512MB. **Technology:** Redis Cluster 7.x **Scaling:** 6-node cluster with sharding ### CDN Caches static assets and API responses. Cache hit ratio target 95%. **Technology:** CloudFront with Lambda@Edge **Scaling:** Global edge locations ## Data Flow Request \u2192 Local Cache \u2192 Redis \u2192 Database \u2192 Cache Population ## Performance Characteristics - **Latency P50:** 1ms (hit), 50ms (miss) - **Latency P99:** 5ms (hit), 200ms (miss) - **Throughput:** 100000 reads/second",
              "score": null
            },
            {
              "doc_id": "adr_001",
              "chunk_id": "adr_001_fix_0",
              "content": "# ADR-001: Use PostgreSQL for Primary Database **Status:** Accepted **Date:** 2024-11-04 ## Context We need a database to store workflow definitions, execution state, and user data. The database must support ACID transactions, JSON queries, and scale to millions of records. ## Options Considered ### Option 1: PostgreSQL Mature relational database with JSONB support. **Pros:** - ACID compliance - Rich JSONB queries - Team expertise - Proven at scale **Cons:** - Manual sharding for extreme scale - Requires connection pooling ### Option 2: MongoDB Document database with native JSON. **Pros:** - Flexible schema - Built-in sharding - Good for documents **Cons:** - Weaker consistency guarantees - Complex transactions - Higher operational cost ### Option 3: CockroachDB Distributed SQL database. **Pros:** - Automatic sharding - Strong consistency - PostgreSQL compatible **Cons:** - Higher latency - More expensive - Less mature ecosystem ## Decision We will use PostgreSQL as our primary database with JSONB for flexible workflow definitions. ## Consequences - Need PgBouncer for connection pooling - Must plan for sharding at 100M+ records - Requires regular VACUUM maintenance",
              "score": null
            }
          ],
          "latency_ms": 3.2565140008955495,
          "query_id": "arch_036",
          "dimension": "casual",
          "query": "search service architecture"
        },
        {
          "key_facts": [
            "Elasticsearch",
            "500ms",
            "autocomplete",
            "20ms"
          ],
          "found_facts": [
            "Elasticsearch",
            "500ms",
            "autocomplete",
            "20ms"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "arch_search_service"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "arch_search_service",
              "chunk_id": "arch_search_service_fix_0",
              "content": "# Search Service Architecture ## Overview The Search Service provides full-text and semantic search across workflows, executions, and documentation. Built on Elasticsearch with custom analyzers for technical content. ## Design Principles - **Relevance First**: Results ranked by relevance, not recency - **Typo Tolerance**: Handles misspellings and variations ## Components ### Indexer Indexes documents in near real-time. Typical indexing latency is 500ms. **Technology:** Custom Go service **Scaling:** Queue-based with backpressure ### Query Engine Executes search queries. Supports filters, facets, and aggregations. **Technology:** Elasticsearch 8.x **Scaling:** 3-node cluster with replicas ### Suggestion Service Provides autocomplete suggestions. Returns suggestions within 20ms. **Technology:** Completion suggester **Scaling:** In-memory FST ## Data Flow Document \u2192 Analyzer \u2192 Index \u2192 Query \u2192 Rank \u2192 Results ## Performance Characteristics - **Latency P50:** 25ms - **Latency P99:** 100ms - **Throughput:** 2000 queries/second",
              "score": null
            },
            {
              "doc_id": "arch_api_gateway",
              "chunk_id": "arch_api_gateway_fix_0",
              "content": "# API Gateway Architecture ## Overview The API Gateway handles all incoming HTTP requests, providing authentication, rate limiting, and request routing. Built on Kong with custom plugins for CloudFlow-specific functionality. ## Design Principles - **Zero Trust**: Every request is authenticated and authorized - **Defense in Depth**: Multiple layers of security validation ## Components ### Kong Gateway Primary ingress controller. Handles TLS termination, request routing, and plugin execution. **Technology:** Kong 3.4 on Kubernetes **Scaling:** Horizontal with HPA ### Auth Plugin Validates JWT tokens, API keys, and OAuth2 flows. Token validation latency under 5ms. **Technology:** Custom Lua plugin **Scaling:** Stateless ### Rate Limiter Enforces per-user and per-IP rate limits. Default limit is 100 requests per minute. **Technology:** Redis-backed sliding window **Scaling:** Shared Redis cluster ## Data Flow Client \u2192 TLS \u2192 Kong \u2192 Auth \u2192 Rate Limit \u2192 Backend Services ## Performance Characteristics - **Latency P50:** 12ms - **Latency P99:** 45ms - **Throughput:** 50000 requests/second",
              "score": null
            },
            {
              "doc_id": "howto_implement_retry_logic",
              "chunk_id": "howto_implement_retry_logic_fix_0",
              "content": "# How to Implement Retry Logic ## Prerequisites - Existing workflow ## Steps ### Step 1: Define retry policy Add retry configuration to your workflow step. > **Note:** Maximum 5 retry attempts allowed ### Step 2: Configure backoff Set exponential backoff with base delay of 1 second. The formula is: delay = base * 2^attempt. ### Step 3: Add timeout Set maximum execution time to prevent infinite loops. Default timeout is 30 seconds. ### Step 4: Handle final failure Configure on_failure handler to notify or escalate when all retries are exhausted. ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            },
            {
              "doc_id": "api_metrics",
              "chunk_id": "api_metrics_fix_0",
              "content": "# Metrics API Reference ## Overview The Metrics API provides programmatic access to metrics functionality. Base URL: `https://api.cloudflow.io/v1/metrics` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /metrics/workflows Get workflow performance metrics. **Parameters:** - `period` (string): Time period: hour, day, week, month - `workflow_id` (string): Filter by workflow ID **Response:** - Success: `200` - Returns metrics data - Error: `400` - Bad Request **Example:** ```json {\"success_rate\": 0.985, \"avg_duration_ms\": 1250, \"p99_duration_ms\": 5000, \"total_executions\": 10000} ```",
              "score": null
            },
            {
              "doc_id": "howto_create_api_integration",
              "chunk_id": "howto_create_api_integration_fix_0",
              "content": "# How to Create API Integration ## Prerequisites - CloudFlow account ## Steps ### Step 1: Generate API key Create an API key for programmatic access. ```bash cloudflow apikeys create --name 'CI Integration' --scopes workflows:read,workflows:execute ``` ### Step 2: Store key securely Never commit API keys to source control. Use environment variables or secret management. > **Note:** Rotate keys every 90 days ### Step 3: Test authentication Verify the API key works correctly. ```bash curl -H 'Authorization: Bearer cf_...' https://api.cloudflow.io/v1/me ``` ### Step 4: Implement rate limiting Handle rate limit responses (HTTP 429) with exponential backoff. > **Note:** Default limit is 100 requests per minute ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            }
          ],
          "latency_ms": 4.210451001199544,
          "query_id": "arch_036",
          "dimension": "contextual",
          "query": "Building an autocomplete feature and need to understand how CloudFlow's suggestion service works and its response time"
        },
        {
          "key_facts": [
            "Elasticsearch",
            "500ms",
            "autocomplete",
            "20ms"
          ],
          "found_facts": [
            "Elasticsearch",
            "500ms",
            "autocomplete",
            "20ms"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "arch_search_service"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "arch_search_service",
              "chunk_id": "arch_search_service_fix_0",
              "content": "# Search Service Architecture ## Overview The Search Service provides full-text and semantic search across workflows, executions, and documentation. Built on Elasticsearch with custom analyzers for technical content. ## Design Principles - **Relevance First**: Results ranked by relevance, not recency - **Typo Tolerance**: Handles misspellings and variations ## Components ### Indexer Indexes documents in near real-time. Typical indexing latency is 500ms. **Technology:** Custom Go service **Scaling:** Queue-based with backpressure ### Query Engine Executes search queries. Supports filters, facets, and aggregations. **Technology:** Elasticsearch 8.x **Scaling:** 3-node cluster with replicas ### Suggestion Service Provides autocomplete suggestions. Returns suggestions within 20ms. **Technology:** Completion suggester **Scaling:** In-memory FST ## Data Flow Document \u2192 Analyzer \u2192 Index \u2192 Query \u2192 Rank \u2192 Results ## Performance Characteristics - **Latency P50:** 25ms - **Latency P99:** 100ms - **Throughput:** 2000 queries/second",
              "score": null
            },
            {
              "doc_id": "adr_006",
              "chunk_id": "adr_006_fix_0",
              "content": "# ADR-006: Use Terraform for Infrastructure as Code **Status:** Accepted **Date:** 2024-11-24 ## Context We need a way to manage cloud infrastructure consistently across environments. ## Options Considered ### Option 1: Terraform Multi-cloud IaC tool. **Pros:** - Multi-cloud support - Large community - State management - Plan before apply **Cons:** - State file management - Learning curve - HCL limitations ### Option 2: AWS CloudFormation AWS-native IaC. **Pros:** - Native AWS support - No state file - Drift detection **Cons:** - AWS only - Verbose YAML/JSON - Slower development ### Option 3: Pulumi IaC with real programming languages. **Pros:** - Real languages - Type safety - Reusable components **Cons:** - Smaller community - Vendor lock-in - Commercial features ## Decision We will use Terraform with S3 backend for state and DynamoDB for locking. ## Consequences - Implement module structure - Use workspaces for environments - Automate with Atlantis",
              "score": null
            },
            {
              "doc_id": "arch_data_pipeline",
              "chunk_id": "arch_data_pipeline_fix_0",
              "content": "# Data Pipeline Architecture ## Overview The Data Pipeline processes and transforms data between workflow steps. It supports batch processing up to 10GB and streaming for real-time use cases. ## Design Principles - **Exactly-Once Processing**: Each record is processed exactly once - **Backpressure Handling**: System gracefully handles varying load ## Components ### Stream Processor Handles real-time data streams. Maximum throughput of 100000 messages per second. **Technology:** Apache Kafka with Flink **Scaling:** Partition-based scaling ### Batch Processor Processes large batch jobs. Supports data formats: JSON, CSV, Parquet, Avro. **Technology:** Apache Spark on Kubernetes **Scaling:** Dynamic resource allocation ### Data Lake Long-term data storage with time-travel queries. Retention configurable up to 7 years. **Technology:** S3 with Iceberg tables **Scaling:** Unlimited with tiered storage ## Data Flow Input \u2192 Validation \u2192 Transform \u2192 Enrich \u2192 Output ## Performance Characteristics - **Latency P50:** 100ms (streaming), 5min (batch) - **Latency P99:** 500ms (streaming), 30min (batch) - **Throughput:** 100000 messages/second (streaming)",
              "score": null
            },
            {
              "doc_id": "config_workflow_engine",
              "chunk_id": "config_workflow_engine_fix_0",
              "content": "# Workflow Engine Configuration Reference ## Overview This document describes all configuration options for Workflow Engine. ## Environment Variables | Variable | Type | Default | Description | |----------|------|---------|-------------| | `WORKFLOW_MAX_STEPS` | integer | 100 | Maximum steps per workflow | | `WORKFLOW_TIMEOUT_SECONDS` | integer | 3600 | Maximum workflow execution time | | `WORKFLOW_RETRY_LIMIT` | integer | 3 | Default retry attempts for failed steps | | `WORKFLOW_CONCURRENT_LIMIT` | integer | 10 | Maximum concurrent executions per workflow | | `EXECUTOR_POOL_SIZE` | integer | 50 | Number of executor workers | | `EXECUTOR_MEMORY_LIMIT` | string | 512Mi | Memory limit per executor | ## Configuration File Configuration can also be provided via `workflow engine.yaml`: ```yaml workflow_max_steps: 100 workflow_timeout_seconds: 3600 workflow_retry_limit: 3 workflow_concurrent_limit: 10 executor_pool_size: 50 ``` ## Validation Configuration is validated at startup. Invalid configuration will prevent the service from starting.",
              "score": null
            },
            {
              "doc_id": "arch_workflow_engine",
              "chunk_id": "arch_workflow_engine_fix_0",
              "content": "# Workflow Engine Architecture ## Overview The Workflow Engine is the core component responsible for executing user-defined workflows. It uses an event-driven architecture with support for parallel execution, conditional branching, and automatic retries. ## Design Principles - **Idempotency**: All operations can be safely retried without side effects - **Eventual Consistency**: State converges to consistent view within 5 seconds - **Fault Tolerance**: System continues operating despite component failures ## Components ### Scheduler Manages workflow scheduling and triggers. Supports cron expressions, webhooks, and event-based triggers. **Technology:** Custom Go service **Scaling:** Horizontal with leader election ### Executor Runs individual workflow steps in isolated containers. Maximum execution time is 30 minutes per step. **Technology:** Kubernetes Jobs **Scaling:** Auto-scaling based on queue depth ### State Store Persists workflow state, execution history, and step results. Uses optimistic locking for concurrent updates. **Technology:** PostgreSQL with JSONB **Scaling:** Primary-replica with read replicas ## Data Flow Triggers \u2192 Scheduler \u2192 Job Queue \u2192 Executor \u2192 State Store \u2192 Webhooks/Notifications ## Performance Characteristics - **Latency P50:** 45ms - **Latency P99:** 250ms - **Throughput:** 5000 executions/minute",
              "score": null
            }
          ],
          "latency_ms": 3.578243000447401,
          "query_id": "arch_036",
          "dimension": "negation",
          "query": "I don't understand how documents get indexed and how autocomplete suggestions are generated"
        },
        {
          "key_facts": [
            "bcrypt",
            "cost factor 12",
            "RS256",
            "refresh tokens"
          ],
          "found_facts": [
            "bcrypt",
            "cost factor 12",
            "RS256",
            "refresh tokens"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "arch_authentication_service"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "arch_authentication_service",
              "chunk_id": "arch_authentication_service_fix_0",
              "content": "# Authentication Service Architecture ## Overview The Authentication Service handles user identity, session management, and token issuance. Supports OAuth 2.0, SAML, and API key authentication. ## Design Principles - **Zero Knowledge**: Passwords never stored in plaintext - **Token Rotation**: Refresh tokens rotate on each use ## Components ### Identity Provider Manages user accounts and credentials. Password hashing uses bcrypt with cost factor 12. **Technology:** Custom Go service **Scaling:** Horizontal stateless ### Token Service Issues and validates access tokens. Token lifetime is 1 hour with 7-day refresh tokens. **Technology:** JWT with RS256 **Scaling:** Horizontal stateless ### SSO Gateway Integrates with external identity providers. Supports Google, Microsoft, Okta, and custom SAML. **Technology:** Dex OIDC proxy **Scaling:** Active-passive ## Data Flow Credentials \u2192 Validation \u2192 Token Issue \u2192 Token Refresh \u2192 Logout ## Performance Characteristics - **Latency P50:** 50ms - **Latency P99:** 200ms - **Throughput:** 5000 auth/second",
              "score": null
            },
            {
              "doc_id": "adr_003",
              "chunk_id": "adr_003_fix_0",
              "content": "# ADR-003: Implement OAuth 2.0 with JWT for Authentication **Status:** Accepted **Date:** 2024-05-08 ## Context We need a secure authentication system that supports API keys, user sessions, and third-party integrations. ## Options Considered ### Option 1: OAuth 2.0 with JWT Industry standard token-based auth. **Pros:** - Industry standard - Stateless validation - Supports refresh tokens - SSO compatible **Cons:** - Token size overhead - Cannot revoke individual tokens - Complexity ### Option 2: Session-based auth Traditional server-side sessions. **Pros:** - Simple implementation - Easy revocation - Smaller payloads **Cons:** - Requires session store - Not suitable for APIs - Scaling challenges ### Option 3: API keys only Simple key-based authentication. **Pros:** - Very simple - Low overhead - Easy to implement **Cons:** - No user context - Hard to rotate - No SSO support ## Decision We will implement OAuth 2.0 with JWT tokens. Access tokens expire after 1 hour, refresh tokens after 7 days. ## Consequences - Implement token rotation on refresh - Support API keys for machine-to-machine auth - Add MFA for admin accounts",
              "score": null
            },
            {
              "doc_id": "arch_api_gateway",
              "chunk_id": "arch_api_gateway_fix_0",
              "content": "# API Gateway Architecture ## Overview The API Gateway handles all incoming HTTP requests, providing authentication, rate limiting, and request routing. Built on Kong with custom plugins for CloudFlow-specific functionality. ## Design Principles - **Zero Trust**: Every request is authenticated and authorized - **Defense in Depth**: Multiple layers of security validation ## Components ### Kong Gateway Primary ingress controller. Handles TLS termination, request routing, and plugin execution. **Technology:** Kong 3.4 on Kubernetes **Scaling:** Horizontal with HPA ### Auth Plugin Validates JWT tokens, API keys, and OAuth2 flows. Token validation latency under 5ms. **Technology:** Custom Lua plugin **Scaling:** Stateless ### Rate Limiter Enforces per-user and per-IP rate limits. Default limit is 100 requests per minute. **Technology:** Redis-backed sliding window **Scaling:** Shared Redis cluster ## Data Flow Client \u2192 TLS \u2192 Kong \u2192 Auth \u2192 Rate Limit \u2192 Backend Services ## Performance Characteristics - **Latency P50:** 12ms - **Latency P99:** 45ms - **Throughput:** 50000 requests/second",
              "score": null
            },
            {
              "doc_id": "howto_configure_single_sign_on",
              "chunk_id": "howto_configure_single_sign_on_fix_0",
              "content": "# How to Configure Single Sign-On ## Prerequisites - Admin access - Enterprise plan - Identity provider access ## Steps ### Step 1: Choose provider CloudFlow supports SAML 2.0, OpenID Connect, and OAuth 2.0 providers. ### Step 2: Get metadata Download CloudFlow's SAML metadata for your IdP configuration. ```bash cloudflow sso metadata --format xml > cloudflow-metadata.xml ``` ### Step 3: Configure IdP Add CloudFlow as a service provider in your identity provider. > **Note:** ACS URL: https://auth.cloudflow.io/saml/callback ### Step 4: Upload IdP metadata Provide your IdP's metadata to CloudFlow. ```bash cloudflow sso configure --idp-metadata ./idp-metadata.xml ``` ### Step 5: Test login Verify SSO login works correctly. ```bash cloudflow sso test ``` ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            },
            {
              "doc_id": "arch_monitoring_stack",
              "chunk_id": "arch_monitoring_stack_fix_0",
              "content": "# Monitoring Stack Architecture ## Overview The Monitoring Stack provides observability into system health, performance, and errors. Includes metrics, logs, traces, and alerting. ## Design Principles - **Observability**: Every component emits metrics and traces - **Actionable Alerts**: Alerts include runbook links ## Components ### Metrics Collects time-series metrics. Retention is 15 days local, 1 year in object storage. **Technology:** Prometheus with Thanos **Scaling:** Federated with long-term storage ### Logging Centralized log aggregation. Supports structured JSON logs with label indexing. **Technology:** Loki with Grafana **Scaling:** Distributed with sharding ### Tracing Distributed request tracing. Trace retention is 7 days. **Technology:** Jaeger with OpenTelemetry **Scaling:** Sampling at 10% ### Alerting Routes alerts to on-call engineers. Supports escalation policies and silencing. **Technology:** Alertmanager with PagerDuty **Scaling:** HA pair ## Data Flow Application \u2192 Collector \u2192 Storage \u2192 Query \u2192 Dashboard/Alert ## Performance Characteristics - **Latency P50:** N/A - **Latency P99:** N/A - **Throughput:** 1M metrics/second, 100GB logs/day",
              "score": null
            }
          ],
          "latency_ms": 3.5168289996363455,
          "query_id": "arch_037",
          "dimension": "original",
          "query": "Explain the Authentication Service"
        },
        {
          "key_facts": [
            "bcrypt",
            "cost factor 12",
            "RS256",
            "refresh tokens"
          ],
          "found_facts": [
            "bcrypt",
            "cost factor 12",
            "RS256",
            "refresh tokens"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "arch_authentication_service"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "arch_authentication_service",
              "chunk_id": "arch_authentication_service_fix_0",
              "content": "# Authentication Service Architecture ## Overview The Authentication Service handles user identity, session management, and token issuance. Supports OAuth 2.0, SAML, and API key authentication. ## Design Principles - **Zero Knowledge**: Passwords never stored in plaintext - **Token Rotation**: Refresh tokens rotate on each use ## Components ### Identity Provider Manages user accounts and credentials. Password hashing uses bcrypt with cost factor 12. **Technology:** Custom Go service **Scaling:** Horizontal stateless ### Token Service Issues and validates access tokens. Token lifetime is 1 hour with 7-day refresh tokens. **Technology:** JWT with RS256 **Scaling:** Horizontal stateless ### SSO Gateway Integrates with external identity providers. Supports Google, Microsoft, Okta, and custom SAML. **Technology:** Dex OIDC proxy **Scaling:** Active-passive ## Data Flow Credentials \u2192 Validation \u2192 Token Issue \u2192 Token Refresh \u2192 Logout ## Performance Characteristics - **Latency P50:** 50ms - **Latency P99:** 200ms - **Throughput:** 5000 auth/second",
              "score": null
            },
            {
              "doc_id": "adr_003",
              "chunk_id": "adr_003_fix_0",
              "content": "# ADR-003: Implement OAuth 2.0 with JWT for Authentication **Status:** Accepted **Date:** 2024-05-08 ## Context We need a secure authentication system that supports API keys, user sessions, and third-party integrations. ## Options Considered ### Option 1: OAuth 2.0 with JWT Industry standard token-based auth. **Pros:** - Industry standard - Stateless validation - Supports refresh tokens - SSO compatible **Cons:** - Token size overhead - Cannot revoke individual tokens - Complexity ### Option 2: Session-based auth Traditional server-side sessions. **Pros:** - Simple implementation - Easy revocation - Smaller payloads **Cons:** - Requires session store - Not suitable for APIs - Scaling challenges ### Option 3: API keys only Simple key-based authentication. **Pros:** - Very simple - Low overhead - Easy to implement **Cons:** - No user context - Hard to rotate - No SSO support ## Decision We will implement OAuth 2.0 with JWT tokens. Access tokens expire after 1 hour, refresh tokens after 7 days. ## Consequences - Implement token rotation on refresh - Support API keys for machine-to-machine auth - Add MFA for admin accounts",
              "score": null
            },
            {
              "doc_id": "arch_api_gateway",
              "chunk_id": "arch_api_gateway_fix_0",
              "content": "# API Gateway Architecture ## Overview The API Gateway handles all incoming HTTP requests, providing authentication, rate limiting, and request routing. Built on Kong with custom plugins for CloudFlow-specific functionality. ## Design Principles - **Zero Trust**: Every request is authenticated and authorized - **Defense in Depth**: Multiple layers of security validation ## Components ### Kong Gateway Primary ingress controller. Handles TLS termination, request routing, and plugin execution. **Technology:** Kong 3.4 on Kubernetes **Scaling:** Horizontal with HPA ### Auth Plugin Validates JWT tokens, API keys, and OAuth2 flows. Token validation latency under 5ms. **Technology:** Custom Lua plugin **Scaling:** Stateless ### Rate Limiter Enforces per-user and per-IP rate limits. Default limit is 100 requests per minute. **Technology:** Redis-backed sliding window **Scaling:** Shared Redis cluster ## Data Flow Client \u2192 TLS \u2192 Kong \u2192 Auth \u2192 Rate Limit \u2192 Backend Services ## Performance Characteristics - **Latency P50:** 12ms - **Latency P99:** 45ms - **Throughput:** 50000 requests/second",
              "score": null
            },
            {
              "doc_id": "adr_007",
              "chunk_id": "adr_007_fix_0",
              "content": "# ADR-007: Implement Rate Limiting at API Gateway **Status:** Accepted **Date:** 2024-09-03 ## Context We need to protect our services from abuse and ensure fair resource usage across customers. ## Options Considered ### Option 1: Sliding window at gateway Rate limit at API gateway level. **Pros:** - Centralized enforcement - Low latency - Easy configuration **Cons:** - Gateway becomes bottleneck - Coarse granularity - Redis dependency ### Option 2: Per-service limiting Each service implements its own limits. **Pros:** - Fine-grained control - Service autonomy - No single point of failure **Cons:** - Inconsistent implementation - Higher complexity - Distributed state ### Option 3: Token bucket Token-based rate limiting. **Pros:** - Allows bursts - Smooth rate limiting - Well understood **Cons:** - More complex - Harder to explain to users - State management ## Decision We will implement sliding window rate limiting at the Kong API Gateway using Redis. ## Consequences - Default limit: 100 requests per minute - Implement per-plan limits - Return rate limit headers - Add retry-after on 429",
              "score": null
            },
            {
              "doc_id": "arch_notification_service",
              "chunk_id": "arch_notification_service_fix_0",
              "content": "# Notification Service Architecture ## Overview The Notification Service delivers alerts and updates through multiple channels including email, Slack, webhooks, and mobile push notifications. ## Design Principles - **Delivery Guarantee**: At-least-once delivery with deduplication - **Channel Abstraction**: Unified API regardless of delivery channel ## Components ### Dispatcher Routes notifications to appropriate channel handlers. Supports priority queues for urgent notifications. **Technology:** Go microservice **Scaling:** Horizontal with queue partitioning ### Email Provider Sends transactional emails. Maximum 10000 emails per hour per account. **Technology:** SendGrid integration **Scaling:** API rate limited ### Slack Integration Posts messages to Slack channels. Supports message formatting and interactive components. **Technology:** Slack Bolt SDK **Scaling:** Per-workspace rate limits ## Data Flow Event \u2192 Template \u2192 Dispatcher \u2192 Channel \u2192 Delivery Confirmation ## Performance Characteristics - **Latency P50:** 200ms - **Latency P99:** 2000ms - **Throughput:** 10000 notifications/minute",
              "score": null
            }
          ],
          "latency_ms": 4.237831999489572,
          "query_id": "arch_037",
          "dimension": "synonym",
          "query": "How is the identity management and token issuance system designed?"
        },
        {
          "key_facts": [
            "bcrypt",
            "cost factor 12",
            "RS256",
            "refresh tokens"
          ],
          "found_facts": [
            "bcrypt",
            "cost factor 12",
            "RS256",
            "refresh tokens"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "arch_authentication_service"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "arch_authentication_service",
              "chunk_id": "arch_authentication_service_fix_0",
              "content": "# Authentication Service Architecture ## Overview The Authentication Service handles user identity, session management, and token issuance. Supports OAuth 2.0, SAML, and API key authentication. ## Design Principles - **Zero Knowledge**: Passwords never stored in plaintext - **Token Rotation**: Refresh tokens rotate on each use ## Components ### Identity Provider Manages user accounts and credentials. Password hashing uses bcrypt with cost factor 12. **Technology:** Custom Go service **Scaling:** Horizontal stateless ### Token Service Issues and validates access tokens. Token lifetime is 1 hour with 7-day refresh tokens. **Technology:** JWT with RS256 **Scaling:** Horizontal stateless ### SSO Gateway Integrates with external identity providers. Supports Google, Microsoft, Okta, and custom SAML. **Technology:** Dex OIDC proxy **Scaling:** Active-passive ## Data Flow Credentials \u2192 Validation \u2192 Token Issue \u2192 Token Refresh \u2192 Logout ## Performance Characteristics - **Latency P50:** 50ms - **Latency P99:** 200ms - **Throughput:** 5000 auth/second",
              "score": null
            },
            {
              "doc_id": "adr_003",
              "chunk_id": "adr_003_fix_0",
              "content": "# ADR-003: Implement OAuth 2.0 with JWT for Authentication **Status:** Accepted **Date:** 2024-05-08 ## Context We need a secure authentication system that supports API keys, user sessions, and third-party integrations. ## Options Considered ### Option 1: OAuth 2.0 with JWT Industry standard token-based auth. **Pros:** - Industry standard - Stateless validation - Supports refresh tokens - SSO compatible **Cons:** - Token size overhead - Cannot revoke individual tokens - Complexity ### Option 2: Session-based auth Traditional server-side sessions. **Pros:** - Simple implementation - Easy revocation - Smaller payloads **Cons:** - Requires session store - Not suitable for APIs - Scaling challenges ### Option 3: API keys only Simple key-based authentication. **Pros:** - Very simple - Low overhead - Easy to implement **Cons:** - No user context - Hard to rotate - No SSO support ## Decision We will implement OAuth 2.0 with JWT tokens. Access tokens expire after 1 hour, refresh tokens after 7 days. ## Consequences - Implement token rotation on refresh - Support API keys for machine-to-machine auth - Add MFA for admin accounts",
              "score": null
            },
            {
              "doc_id": "arch_monitoring_stack",
              "chunk_id": "arch_monitoring_stack_fix_0",
              "content": "# Monitoring Stack Architecture ## Overview The Monitoring Stack provides observability into system health, performance, and errors. Includes metrics, logs, traces, and alerting. ## Design Principles - **Observability**: Every component emits metrics and traces - **Actionable Alerts**: Alerts include runbook links ## Components ### Metrics Collects time-series metrics. Retention is 15 days local, 1 year in object storage. **Technology:** Prometheus with Thanos **Scaling:** Federated with long-term storage ### Logging Centralized log aggregation. Supports structured JSON logs with label indexing. **Technology:** Loki with Grafana **Scaling:** Distributed with sharding ### Tracing Distributed request tracing. Trace retention is 7 days. **Technology:** Jaeger with OpenTelemetry **Scaling:** Sampling at 10% ### Alerting Routes alerts to on-call engineers. Supports escalation policies and silencing. **Technology:** Alertmanager with PagerDuty **Scaling:** HA pair ## Data Flow Application \u2192 Collector \u2192 Storage \u2192 Query \u2192 Dashboard/Alert ## Performance Characteristics - **Latency P50:** N/A - **Latency P99:** N/A - **Throughput:** 1M metrics/second, 100GB logs/day",
              "score": null
            },
            {
              "doc_id": "runbook_database_connection_exhaustion",
              "chunk_id": "runbook_database_connection_exhaustion_fix_0",
              "content": "# Runbook: Database Connection Exhaustion ## Severity **Level:** P1 - Critical **Response Time:** 15 minutes ## Symptoms - Connection pool at 100% - Queries timing out - 'too many connections' errors in logs ## Diagnosis ### Step 1 Check active connections: `SELECT count(*) FROM pg_stat_activity` ### Step 2 Identify long-running queries: `SELECT pid, now() - pg_stat_activity.query_start AS duration, query FROM pg_stat_activity WHERE state != 'idle' ORDER BY duration DESC` ### Step 3 Check PgBouncer stats: `SHOW POOLS` ### Step 4 Verify connection limits: `SHOW max_connections` ## Resolution ### Step 1 Kill long-running queries: `SELECT pg_terminate_backend(pid) FROM pg_stat_activity WHERE duration > interval '5 minutes'` ### Step 2 Increase PgBouncer pool size temporarily: update pgbouncer.ini ### Step 3 Restart affected services to release connections: `kubectl rollout restart deployment/api` ### Step 4 Scale down non-critical services to free connections ## Escalation If connections remain exhausted, escalate to DBA. Consider enabling connection queueing. ## Post-Incident - Document timeline in incident log - Schedule post-mortem within 48 hours - Update runbook if new patterns discovered",
              "score": null
            },
            {
              "doc_id": "arch_api_gateway",
              "chunk_id": "arch_api_gateway_fix_0",
              "content": "# API Gateway Architecture ## Overview The API Gateway handles all incoming HTTP requests, providing authentication, rate limiting, and request routing. Built on Kong with custom plugins for CloudFlow-specific functionality. ## Design Principles - **Zero Trust**: Every request is authenticated and authorized - **Defense in Depth**: Multiple layers of security validation ## Components ### Kong Gateway Primary ingress controller. Handles TLS termination, request routing, and plugin execution. **Technology:** Kong 3.4 on Kubernetes **Scaling:** Horizontal with HPA ### Auth Plugin Validates JWT tokens, API keys, and OAuth2 flows. Token validation latency under 5ms. **Technology:** Custom Lua plugin **Scaling:** Stateless ### Rate Limiter Enforces per-user and per-IP rate limits. Default limit is 100 requests per minute. **Technology:** Redis-backed sliding window **Scaling:** Shared Redis cluster ## Data Flow Client \u2192 TLS \u2192 Kong \u2192 Auth \u2192 Rate Limit \u2192 Backend Services ## Performance Characteristics - **Latency P50:** 12ms - **Latency P99:** 45ms - **Throughput:** 50000 requests/second",
              "score": null
            }
          ],
          "latency_ms": 3.760482999496162,
          "query_id": "arch_037",
          "dimension": "problem",
          "query": "Users report getting logged out unexpectedly - need to understand token lifecycle and refresh mechanism"
        },
        {
          "key_facts": [
            "bcrypt",
            "cost factor 12",
            "RS256",
            "refresh tokens"
          ],
          "found_facts": [
            "bcrypt",
            "cost factor 12",
            "RS256",
            "refresh tokens"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "arch_authentication_service"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "arch_authentication_service",
              "chunk_id": "arch_authentication_service_fix_0",
              "content": "# Authentication Service Architecture ## Overview The Authentication Service handles user identity, session management, and token issuance. Supports OAuth 2.0, SAML, and API key authentication. ## Design Principles - **Zero Knowledge**: Passwords never stored in plaintext - **Token Rotation**: Refresh tokens rotate on each use ## Components ### Identity Provider Manages user accounts and credentials. Password hashing uses bcrypt with cost factor 12. **Technology:** Custom Go service **Scaling:** Horizontal stateless ### Token Service Issues and validates access tokens. Token lifetime is 1 hour with 7-day refresh tokens. **Technology:** JWT with RS256 **Scaling:** Horizontal stateless ### SSO Gateway Integrates with external identity providers. Supports Google, Microsoft, Okta, and custom SAML. **Technology:** Dex OIDC proxy **Scaling:** Active-passive ## Data Flow Credentials \u2192 Validation \u2192 Token Issue \u2192 Token Refresh \u2192 Logout ## Performance Characteristics - **Latency P50:** 50ms - **Latency P99:** 200ms - **Throughput:** 5000 auth/second",
              "score": null
            },
            {
              "doc_id": "adr_003",
              "chunk_id": "adr_003_fix_0",
              "content": "# ADR-003: Implement OAuth 2.0 with JWT for Authentication **Status:** Accepted **Date:** 2024-05-08 ## Context We need a secure authentication system that supports API keys, user sessions, and third-party integrations. ## Options Considered ### Option 1: OAuth 2.0 with JWT Industry standard token-based auth. **Pros:** - Industry standard - Stateless validation - Supports refresh tokens - SSO compatible **Cons:** - Token size overhead - Cannot revoke individual tokens - Complexity ### Option 2: Session-based auth Traditional server-side sessions. **Pros:** - Simple implementation - Easy revocation - Smaller payloads **Cons:** - Requires session store - Not suitable for APIs - Scaling challenges ### Option 3: API keys only Simple key-based authentication. **Pros:** - Very simple - Low overhead - Easy to implement **Cons:** - No user context - Hard to rotate - No SSO support ## Decision We will implement OAuth 2.0 with JWT tokens. Access tokens expire after 1 hour, refresh tokens after 7 days. ## Consequences - Implement token rotation on refresh - Support API keys for machine-to-machine auth - Add MFA for admin accounts",
              "score": null
            },
            {
              "doc_id": "howto_configure_single_sign_on",
              "chunk_id": "howto_configure_single_sign_on_fix_0",
              "content": "# How to Configure Single Sign-On ## Prerequisites - Admin access - Enterprise plan - Identity provider access ## Steps ### Step 1: Choose provider CloudFlow supports SAML 2.0, OpenID Connect, and OAuth 2.0 providers. ### Step 2: Get metadata Download CloudFlow's SAML metadata for your IdP configuration. ```bash cloudflow sso metadata --format xml > cloudflow-metadata.xml ``` ### Step 3: Configure IdP Add CloudFlow as a service provider in your identity provider. > **Note:** ACS URL: https://auth.cloudflow.io/saml/callback ### Step 4: Upload IdP metadata Provide your IdP's metadata to CloudFlow. ```bash cloudflow sso configure --idp-metadata ./idp-metadata.xml ``` ### Step 5: Test login Verify SSO login works correctly. ```bash cloudflow sso test ``` ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            },
            {
              "doc_id": "arch_api_gateway",
              "chunk_id": "arch_api_gateway_fix_0",
              "content": "# API Gateway Architecture ## Overview The API Gateway handles all incoming HTTP requests, providing authentication, rate limiting, and request routing. Built on Kong with custom plugins for CloudFlow-specific functionality. ## Design Principles - **Zero Trust**: Every request is authenticated and authorized - **Defense in Depth**: Multiple layers of security validation ## Components ### Kong Gateway Primary ingress controller. Handles TLS termination, request routing, and plugin execution. **Technology:** Kong 3.4 on Kubernetes **Scaling:** Horizontal with HPA ### Auth Plugin Validates JWT tokens, API keys, and OAuth2 flows. Token validation latency under 5ms. **Technology:** Custom Lua plugin **Scaling:** Stateless ### Rate Limiter Enforces per-user and per-IP rate limits. Default limit is 100 requests per minute. **Technology:** Redis-backed sliding window **Scaling:** Shared Redis cluster ## Data Flow Client \u2192 TLS \u2192 Kong \u2192 Auth \u2192 Rate Limit \u2192 Backend Services ## Performance Characteristics - **Latency P50:** 12ms - **Latency P99:** 45ms - **Throughput:** 50000 requests/second",
              "score": null
            },
            {
              "doc_id": "adr_007",
              "chunk_id": "adr_007_fix_0",
              "content": "# ADR-007: Implement Rate Limiting at API Gateway **Status:** Accepted **Date:** 2024-09-03 ## Context We need to protect our services from abuse and ensure fair resource usage across customers. ## Options Considered ### Option 1: Sliding window at gateway Rate limit at API gateway level. **Pros:** - Centralized enforcement - Low latency - Easy configuration **Cons:** - Gateway becomes bottleneck - Coarse granularity - Redis dependency ### Option 2: Per-service limiting Each service implements its own limits. **Pros:** - Fine-grained control - Service autonomy - No single point of failure **Cons:** - Inconsistent implementation - Higher complexity - Distributed state ### Option 3: Token bucket Token-based rate limiting. **Pros:** - Allows bursts - Smooth rate limiting - Well understood **Cons:** - More complex - Harder to explain to users - State management ## Decision We will implement sliding window rate limiting at the Kong API Gateway using Redis. ## Consequences - Default limit: 100 requests per minute - Implement per-plan limits - Return rate limit headers - Add retry-after on 429",
              "score": null
            }
          ],
          "latency_ms": 3.5401330005697673,
          "query_id": "arch_037",
          "dimension": "casual",
          "query": "auth service design"
        },
        {
          "key_facts": [
            "bcrypt",
            "cost factor 12",
            "RS256",
            "refresh tokens"
          ],
          "found_facts": [
            "bcrypt",
            "cost factor 12",
            "RS256",
            "refresh tokens"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "arch_authentication_service"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "howto_configure_single_sign_on",
              "chunk_id": "howto_configure_single_sign_on_fix_0",
              "content": "# How to Configure Single Sign-On ## Prerequisites - Admin access - Enterprise plan - Identity provider access ## Steps ### Step 1: Choose provider CloudFlow supports SAML 2.0, OpenID Connect, and OAuth 2.0 providers. ### Step 2: Get metadata Download CloudFlow's SAML metadata for your IdP configuration. ```bash cloudflow sso metadata --format xml > cloudflow-metadata.xml ``` ### Step 3: Configure IdP Add CloudFlow as a service provider in your identity provider. > **Note:** ACS URL: https://auth.cloudflow.io/saml/callback ### Step 4: Upload IdP metadata Provide your IdP's metadata to CloudFlow. ```bash cloudflow sso configure --idp-metadata ./idp-metadata.xml ``` ### Step 5: Test login Verify SSO login works correctly. ```bash cloudflow sso test ``` ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            },
            {
              "doc_id": "arch_authentication_service",
              "chunk_id": "arch_authentication_service_fix_0",
              "content": "# Authentication Service Architecture ## Overview The Authentication Service handles user identity, session management, and token issuance. Supports OAuth 2.0, SAML, and API key authentication. ## Design Principles - **Zero Knowledge**: Passwords never stored in plaintext - **Token Rotation**: Refresh tokens rotate on each use ## Components ### Identity Provider Manages user accounts and credentials. Password hashing uses bcrypt with cost factor 12. **Technology:** Custom Go service **Scaling:** Horizontal stateless ### Token Service Issues and validates access tokens. Token lifetime is 1 hour with 7-day refresh tokens. **Technology:** JWT with RS256 **Scaling:** Horizontal stateless ### SSO Gateway Integrates with external identity providers. Supports Google, Microsoft, Okta, and custom SAML. **Technology:** Dex OIDC proxy **Scaling:** Active-passive ## Data Flow Credentials \u2192 Validation \u2192 Token Issue \u2192 Token Refresh \u2192 Logout ## Performance Characteristics - **Latency P50:** 50ms - **Latency P99:** 200ms - **Throughput:** 5000 auth/second",
              "score": null
            },
            {
              "doc_id": "howto_configure_custom_domain",
              "chunk_id": "howto_configure_custom_domain_fix_0",
              "content": "# How to Configure Custom Domain ## Prerequisites - Domain with DNS access - CloudFlow Pro plan or higher ## Steps ### Step 1: Verify domain ownership Add a TXT record to verify domain ownership. ```bash cloudflow domains verify --domain app.example.com ``` ### Step 2: Configure DNS Add a CNAME record pointing to your CloudFlow endpoint. > **Note:** CNAME should point to ingress.cloudflow.io ### Step 3: Enable SSL CloudFlow automatically provisions SSL certificates via Let's Encrypt. ```bash cloudflow domains ssl --domain app.example.com ``` ### Step 4: Set as primary Make this the primary domain for your application. ```bash cloudflow domains set-primary --domain app.example.com ``` ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            },
            {
              "doc_id": "howto_set_up_data_encryption",
              "chunk_id": "howto_set_up_data_encryption_fix_0",
              "content": "# How to Set Up Data Encryption ## Prerequisites - CloudFlow Enterprise plan ## Steps ### Step 1: Enable encryption at rest All data is encrypted at rest using AES-256 by default. > **Note:** Encryption is automatic for all plans ### Step 2: Configure KMS Bring your own encryption keys using AWS KMS or GCP KMS. ```bash cloudflow encryption configure --kms-key arn:aws:kms:... ``` ### Step 3: Enable field-level encryption Encrypt sensitive fields within your workflow data. ```bash cloudflow encryption add-field --path $.user.ssn --key-alias sensitive-data ``` ### Step 4: Verify encryption Confirm encryption is active for your workspace. ```bash cloudflow encryption status ``` ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            },
            {
              "doc_id": "arch_api_gateway",
              "chunk_id": "arch_api_gateway_fix_0",
              "content": "# API Gateway Architecture ## Overview The API Gateway handles all incoming HTTP requests, providing authentication, rate limiting, and request routing. Built on Kong with custom plugins for CloudFlow-specific functionality. ## Design Principles - **Zero Trust**: Every request is authenticated and authorized - **Defense in Depth**: Multiple layers of security validation ## Components ### Kong Gateway Primary ingress controller. Handles TLS termination, request routing, and plugin execution. **Technology:** Kong 3.4 on Kubernetes **Scaling:** Horizontal with HPA ### Auth Plugin Validates JWT tokens, API keys, and OAuth2 flows. Token validation latency under 5ms. **Technology:** Custom Lua plugin **Scaling:** Stateless ### Rate Limiter Enforces per-user and per-IP rate limits. Default limit is 100 requests per minute. **Technology:** Redis-backed sliding window **Scaling:** Shared Redis cluster ## Data Flow Client \u2192 TLS \u2192 Kong \u2192 Auth \u2192 Rate Limit \u2192 Backend Services ## Performance Characteristics - **Latency P50:** 12ms - **Latency P99:** 45ms - **Throughput:** 50000 requests/second",
              "score": null
            }
          ],
          "latency_ms": 4.115174000617117,
          "query_id": "arch_037",
          "dimension": "contextual",
          "query": "Integrating with our corporate SSO and need to understand how CloudFlow handles OAuth, SAML, and password hashing"
        },
        {
          "key_facts": [
            "bcrypt",
            "cost factor 12",
            "RS256",
            "refresh tokens"
          ],
          "found_facts": [
            "bcrypt",
            "cost factor 12",
            "RS256",
            "refresh tokens"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "arch_authentication_service"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "arch_authentication_service",
              "chunk_id": "arch_authentication_service_fix_0",
              "content": "# Authentication Service Architecture ## Overview The Authentication Service handles user identity, session management, and token issuance. Supports OAuth 2.0, SAML, and API key authentication. ## Design Principles - **Zero Knowledge**: Passwords never stored in plaintext - **Token Rotation**: Refresh tokens rotate on each use ## Components ### Identity Provider Manages user accounts and credentials. Password hashing uses bcrypt with cost factor 12. **Technology:** Custom Go service **Scaling:** Horizontal stateless ### Token Service Issues and validates access tokens. Token lifetime is 1 hour with 7-day refresh tokens. **Technology:** JWT with RS256 **Scaling:** Horizontal stateless ### SSO Gateway Integrates with external identity providers. Supports Google, Microsoft, Okta, and custom SAML. **Technology:** Dex OIDC proxy **Scaling:** Active-passive ## Data Flow Credentials \u2192 Validation \u2192 Token Issue \u2192 Token Refresh \u2192 Logout ## Performance Characteristics - **Latency P50:** 50ms - **Latency P99:** 200ms - **Throughput:** 5000 auth/second",
              "score": null
            },
            {
              "doc_id": "adr_003",
              "chunk_id": "adr_003_fix_0",
              "content": "# ADR-003: Implement OAuth 2.0 with JWT for Authentication **Status:** Accepted **Date:** 2024-05-08 ## Context We need a secure authentication system that supports API keys, user sessions, and third-party integrations. ## Options Considered ### Option 1: OAuth 2.0 with JWT Industry standard token-based auth. **Pros:** - Industry standard - Stateless validation - Supports refresh tokens - SSO compatible **Cons:** - Token size overhead - Cannot revoke individual tokens - Complexity ### Option 2: Session-based auth Traditional server-side sessions. **Pros:** - Simple implementation - Easy revocation - Smaller payloads **Cons:** - Requires session store - Not suitable for APIs - Scaling challenges ### Option 3: API keys only Simple key-based authentication. **Pros:** - Very simple - Low overhead - Easy to implement **Cons:** - No user context - Hard to rotate - No SSO support ## Decision We will implement OAuth 2.0 with JWT tokens. Access tokens expire after 1 hour, refresh tokens after 7 days. ## Consequences - Implement token rotation on refresh - Support API keys for machine-to-machine auth - Add MFA for admin accounts",
              "score": null
            },
            {
              "doc_id": "howto_configure_single_sign_on",
              "chunk_id": "howto_configure_single_sign_on_fix_0",
              "content": "# How to Configure Single Sign-On ## Prerequisites - Admin access - Enterprise plan - Identity provider access ## Steps ### Step 1: Choose provider CloudFlow supports SAML 2.0, OpenID Connect, and OAuth 2.0 providers. ### Step 2: Get metadata Download CloudFlow's SAML metadata for your IdP configuration. ```bash cloudflow sso metadata --format xml > cloudflow-metadata.xml ``` ### Step 3: Configure IdP Add CloudFlow as a service provider in your identity provider. > **Note:** ACS URL: https://auth.cloudflow.io/saml/callback ### Step 4: Upload IdP metadata Provide your IdP's metadata to CloudFlow. ```bash cloudflow sso configure --idp-metadata ./idp-metadata.xml ``` ### Step 5: Test login Verify SSO login works correctly. ```bash cloudflow sso test ``` ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            },
            {
              "doc_id": "arch_api_gateway",
              "chunk_id": "arch_api_gateway_fix_0",
              "content": "# API Gateway Architecture ## Overview The API Gateway handles all incoming HTTP requests, providing authentication, rate limiting, and request routing. Built on Kong with custom plugins for CloudFlow-specific functionality. ## Design Principles - **Zero Trust**: Every request is authenticated and authorized - **Defense in Depth**: Multiple layers of security validation ## Components ### Kong Gateway Primary ingress controller. Handles TLS termination, request routing, and plugin execution. **Technology:** Kong 3.4 on Kubernetes **Scaling:** Horizontal with HPA ### Auth Plugin Validates JWT tokens, API keys, and OAuth2 flows. Token validation latency under 5ms. **Technology:** Custom Lua plugin **Scaling:** Stateless ### Rate Limiter Enforces per-user and per-IP rate limits. Default limit is 100 requests per minute. **Technology:** Redis-backed sliding window **Scaling:** Shared Redis cluster ## Data Flow Client \u2192 TLS \u2192 Kong \u2192 Auth \u2192 Rate Limit \u2192 Backend Services ## Performance Characteristics - **Latency P50:** 12ms - **Latency P99:** 45ms - **Throughput:** 50000 requests/second",
              "score": null
            },
            {
              "doc_id": "api_secrets",
              "chunk_id": "api_secrets_fix_0",
              "content": "# Secrets API Reference ## Overview The Secrets API provides programmatic access to secrets functionality. Base URL: `https://api.cloudflow.io/v1/secrets` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /secrets List secret names (values not returned). **Parameters:** **Response:** - Success: `200` - Returns array of secret metadata - Error: `400` - Bad Request **Example:** ```json {\"secrets\": [{\"name\": \"DATABASE_URL\", \"created_at\": \"2024-01-01\", \"updated_at\": \"2024-01-10\"}]} ``` ### PUT /secrets/{name} Create or update a secret. **Parameters:** - `value` (string): Secret value (encrypted at rest) **Response:** - Success: `200` - Secret saved - Error: `400` - Bad Request **Example:** ```json {\"name\": \"DATABASE_URL\", \"updated_at\": \"2024-01-15T10:00:00Z\"} ``` ### DELETE /secrets/{name} Delete a secret. **Parameters:** **Response:** - Success: `204` - Secret deleted - Error: `400` - Bad Request **Example:** ```json {} ```",
              "score": null
            }
          ],
          "latency_ms": 3.619220000473433,
          "query_id": "arch_037",
          "dimension": "negation",
          "query": "I don't get how password hashing and JWT token signing work in the auth service"
        },
        {
          "key_facts": [
            "GET /workflows",
            "limit",
            "status"
          ],
          "found_facts": [
            "GET /workflows",
            "limit",
            "status"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "api_workflows"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "api_workflows",
              "chunk_id": "api_workflows_fix_0",
              "content": "# Workflows API Reference ## Overview The Workflows API provides programmatic access to workflows functionality. Base URL: `https://api.cloudflow.io/v1/workflows` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /workflows List all workflows for the authenticated user. **Parameters:** - `limit` (integer): Maximum number of results (default: 50, max: 200) - `status` (string): Filter by status: active, paused, archived **Response:** - Success: `200` - Returns array of workflow objects - Error: `400` - Bad Request **Example:** ```json {\"workflows\": [{\"id\": \"wf_123\", \"name\": \"Daily Report\", \"status\": \"active\"}]} ``` ### POST /workflows Create a new workflow. **Parameters:** - `name` (string): Workflow name (required) - `definition` (object): Workflow definition in JSON format **Response:** - Success: `201` - Workflow created successfully - Error: `400` - Bad Request **Example:** ```json {\"id\": \"wf_456\", \"name\": \"New Workflow\", \"created_at\": \"2024-01-15T10:00:00Z\"} ``` ### DELETE /workflows/{id} Delete a workflow permanently. **Parameters:** - `id` (string): Workflow ID (required) **Response:** - Success: `204` - Workflow deleted - Error: `404` - Workflow not found **Example:** ```json {} ```",
              "score": null
            },
            {
              "doc_id": "api_metrics",
              "chunk_id": "api_metrics_fix_0",
              "content": "# Metrics API Reference ## Overview The Metrics API provides programmatic access to metrics functionality. Base URL: `https://api.cloudflow.io/v1/metrics` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /metrics/workflows Get workflow performance metrics. **Parameters:** - `period` (string): Time period: hour, day, week, month - `workflow_id` (string): Filter by workflow ID **Response:** - Success: `200` - Returns metrics data - Error: `400` - Bad Request **Example:** ```json {\"success_rate\": 0.985, \"avg_duration_ms\": 1250, \"p99_duration_ms\": 5000, \"total_executions\": 10000} ```",
              "score": null
            },
            {
              "doc_id": "api_executions",
              "chunk_id": "api_executions_fix_0",
              "content": "# Executions API Reference ## Overview The Executions API provides programmatic access to executions functionality. Base URL: `https://api.cloudflow.io/v1/executions` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /executions List workflow executions. **Parameters:** - `workflow_id` (string): Filter by workflow ID - `since` (datetime): Filter executions after this timestamp **Response:** - Success: `200` - Returns array of execution objects - Error: `400` - Bad Request **Example:** ```json {\"executions\": [{\"id\": \"ex_789\", \"status\": \"completed\", \"duration_ms\": 1523}]} ``` ### POST /executions/{id}/cancel Cancel a running execution. **Parameters:** - `id` (string): Execution ID - `reason` (string): Cancellation reason (optional) **Response:** - Success: `200` - Execution cancelled - Error: `400` - Bad Request **Example:** ```json {\"id\": \"ex_789\", \"status\": \"cancelled\", \"cancelled_at\": \"2024-01-15T10:05:00Z\"} ```",
              "score": null
            },
            {
              "doc_id": "api_audit",
              "chunk_id": "api_audit_fix_0",
              "content": "# Audit API Reference ## Overview The Audit API provides programmatic access to audit functionality. Base URL: `https://api.cloudflow.io/v1/audit` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /audit/logs Retrieve audit logs for compliance. **Parameters:** - `start_date` (date): Start date for log range - `end_date` (date): End date for log range - `actor` (string): Filter by user ID **Response:** - Success: `200` - Returns array of audit log entries - Error: `400` - Bad Request **Example:** ```json {\"logs\": [{\"timestamp\": \"2024-01-15T10:00:00Z\", \"actor\": \"usr_001\", \"action\": \"workflow.created\", \"resource\": \"wf_123\"}]} ```",
              "score": null
            },
            {
              "doc_id": "api_webhooks",
              "chunk_id": "api_webhooks_fix_0",
              "content": "# Webhooks API Reference ## Overview The Webhooks API provides programmatic access to webhooks functionality. Base URL: `https://api.cloudflow.io/v1/webhooks` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /webhooks List configured webhooks. **Parameters:** - `active` (boolean): Filter by active status **Response:** - Success: `200` - Returns array of webhook objects - Error: `400` - Bad Request **Example:** ```json {\"webhooks\": [{\"id\": \"wh_01\", \"url\": \"https://example.com/hook\", \"events\": [\"workflow.completed\"]}]} ``` ### POST /webhooks Create a new webhook endpoint. **Parameters:** - `url` (string): Webhook URL (must be HTTPS) - `events` (array): Events to subscribe to - `secret` (string): Signing secret for verification **Response:** - Success: `201` - Webhook created - Error: `400` - Bad Request **Example:** ```json {\"id\": \"wh_02\", \"url\": \"https://example.com/new-hook\", \"secret\": \"whsec_...\"} ```",
              "score": null
            }
          ],
          "latency_ms": 4.132325999307795,
          "query_id": "api_038",
          "dimension": "original",
          "query": "How do I list workflows via API?"
        },
        {
          "key_facts": [
            "GET /workflows",
            "limit",
            "status"
          ],
          "found_facts": [
            "GET /workflows",
            "limit",
            "status"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "api_workflows"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "api_workflows",
              "chunk_id": "api_workflows_fix_0",
              "content": "# Workflows API Reference ## Overview The Workflows API provides programmatic access to workflows functionality. Base URL: `https://api.cloudflow.io/v1/workflows` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /workflows List all workflows for the authenticated user. **Parameters:** - `limit` (integer): Maximum number of results (default: 50, max: 200) - `status` (string): Filter by status: active, paused, archived **Response:** - Success: `200` - Returns array of workflow objects - Error: `400` - Bad Request **Example:** ```json {\"workflows\": [{\"id\": \"wf_123\", \"name\": \"Daily Report\", \"status\": \"active\"}]} ``` ### POST /workflows Create a new workflow. **Parameters:** - `name` (string): Workflow name (required) - `definition` (object): Workflow definition in JSON format **Response:** - Success: `201` - Workflow created successfully - Error: `400` - Bad Request **Example:** ```json {\"id\": \"wf_456\", \"name\": \"New Workflow\", \"created_at\": \"2024-01-15T10:00:00Z\"} ``` ### DELETE /workflows/{id} Delete a workflow permanently. **Parameters:** - `id` (string): Workflow ID (required) **Response:** - Success: `204` - Workflow deleted - Error: `404` - Workflow not found **Example:** ```json {} ```",
              "score": null
            },
            {
              "doc_id": "api_metrics",
              "chunk_id": "api_metrics_fix_0",
              "content": "# Metrics API Reference ## Overview The Metrics API provides programmatic access to metrics functionality. Base URL: `https://api.cloudflow.io/v1/metrics` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /metrics/workflows Get workflow performance metrics. **Parameters:** - `period` (string): Time period: hour, day, week, month - `workflow_id` (string): Filter by workflow ID **Response:** - Success: `200` - Returns metrics data - Error: `400` - Bad Request **Example:** ```json {\"success_rate\": 0.985, \"avg_duration_ms\": 1250, \"p99_duration_ms\": 5000, \"total_executions\": 10000} ```",
              "score": null
            },
            {
              "doc_id": "api_executions",
              "chunk_id": "api_executions_fix_0",
              "content": "# Executions API Reference ## Overview The Executions API provides programmatic access to executions functionality. Base URL: `https://api.cloudflow.io/v1/executions` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /executions List workflow executions. **Parameters:** - `workflow_id` (string): Filter by workflow ID - `since` (datetime): Filter executions after this timestamp **Response:** - Success: `200` - Returns array of execution objects - Error: `400` - Bad Request **Example:** ```json {\"executions\": [{\"id\": \"ex_789\", \"status\": \"completed\", \"duration_ms\": 1523}]} ``` ### POST /executions/{id}/cancel Cancel a running execution. **Parameters:** - `id` (string): Execution ID - `reason` (string): Cancellation reason (optional) **Response:** - Success: `200` - Execution cancelled - Error: `400` - Bad Request **Example:** ```json {\"id\": \"ex_789\", \"status\": \"cancelled\", \"cancelled_at\": \"2024-01-15T10:05:00Z\"} ```",
              "score": null
            },
            {
              "doc_id": "api_integrations",
              "chunk_id": "api_integrations_fix_0",
              "content": "# Integrations API Reference ## Overview The Integrations API provides programmatic access to integrations functionality. Base URL: `https://api.cloudflow.io/v1/integrations` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /integrations List available integrations. **Parameters:** - `category` (string): Filter by category: storage, notification, database **Response:** - Success: `200` - Returns array of integration objects - Error: `400` - Bad Request **Example:** ```json {\"integrations\": [{\"id\": \"int_s3\", \"name\": \"Amazon S3\", \"category\": \"storage\", \"status\": \"connected\"}]} ``` ### POST /integrations/{id}/connect Connect an integration. **Parameters:** - `credentials` (object): Integration-specific credentials **Response:** - Success: `200` - Integration connected - Error: `400` - Bad Request **Example:** ```json {\"id\": \"int_s3\", \"status\": \"connected\", \"connected_at\": \"2024-01-15T10:00:00Z\"} ```",
              "score": null
            },
            {
              "doc_id": "api_webhooks",
              "chunk_id": "api_webhooks_fix_0",
              "content": "# Webhooks API Reference ## Overview The Webhooks API provides programmatic access to webhooks functionality. Base URL: `https://api.cloudflow.io/v1/webhooks` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /webhooks List configured webhooks. **Parameters:** - `active` (boolean): Filter by active status **Response:** - Success: `200` - Returns array of webhook objects - Error: `400` - Bad Request **Example:** ```json {\"webhooks\": [{\"id\": \"wh_01\", \"url\": \"https://example.com/hook\", \"events\": [\"workflow.completed\"]}]} ``` ### POST /webhooks Create a new webhook endpoint. **Parameters:** - `url` (string): Webhook URL (must be HTTPS) - `events` (array): Events to subscribe to - `secret` (string): Signing secret for verification **Response:** - Success: `201` - Webhook created - Error: `400` - Bad Request **Example:** ```json {\"id\": \"wh_02\", \"url\": \"https://example.com/new-hook\", \"secret\": \"whsec_...\"} ```",
              "score": null
            }
          ],
          "latency_ms": 3.8931199997023214,
          "query_id": "api_038",
          "dimension": "synonym",
          "query": "I need to fetch all my workflows through the REST endpoint"
        },
        {
          "key_facts": [
            "GET /workflows",
            "limit",
            "status"
          ],
          "found_facts": [
            "GET /workflows",
            "limit",
            "status"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "api_workflows"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "api_workflows",
              "chunk_id": "api_workflows_fix_0",
              "content": "# Workflows API Reference ## Overview The Workflows API provides programmatic access to workflows functionality. Base URL: `https://api.cloudflow.io/v1/workflows` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /workflows List all workflows for the authenticated user. **Parameters:** - `limit` (integer): Maximum number of results (default: 50, max: 200) - `status` (string): Filter by status: active, paused, archived **Response:** - Success: `200` - Returns array of workflow objects - Error: `400` - Bad Request **Example:** ```json {\"workflows\": [{\"id\": \"wf_123\", \"name\": \"Daily Report\", \"status\": \"active\"}]} ``` ### POST /workflows Create a new workflow. **Parameters:** - `name` (string): Workflow name (required) - `definition` (object): Workflow definition in JSON format **Response:** - Success: `201` - Workflow created successfully - Error: `400` - Bad Request **Example:** ```json {\"id\": \"wf_456\", \"name\": \"New Workflow\", \"created_at\": \"2024-01-15T10:00:00Z\"} ``` ### DELETE /workflows/{id} Delete a workflow permanently. **Parameters:** - `id` (string): Workflow ID (required) **Response:** - Success: `204` - Workflow deleted - Error: `404` - Workflow not found **Example:** ```json {} ```",
              "score": null
            },
            {
              "doc_id": "api_metrics",
              "chunk_id": "api_metrics_fix_0",
              "content": "# Metrics API Reference ## Overview The Metrics API provides programmatic access to metrics functionality. Base URL: `https://api.cloudflow.io/v1/metrics` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /metrics/workflows Get workflow performance metrics. **Parameters:** - `period` (string): Time period: hour, day, week, month - `workflow_id` (string): Filter by workflow ID **Response:** - Success: `200` - Returns metrics data - Error: `400` - Bad Request **Example:** ```json {\"success_rate\": 0.985, \"avg_duration_ms\": 1250, \"p99_duration_ms\": 5000, \"total_executions\": 10000} ```",
              "score": null
            },
            {
              "doc_id": "api_executions",
              "chunk_id": "api_executions_fix_0",
              "content": "# Executions API Reference ## Overview The Executions API provides programmatic access to executions functionality. Base URL: `https://api.cloudflow.io/v1/executions` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /executions List workflow executions. **Parameters:** - `workflow_id` (string): Filter by workflow ID - `since` (datetime): Filter executions after this timestamp **Response:** - Success: `200` - Returns array of execution objects - Error: `400` - Bad Request **Example:** ```json {\"executions\": [{\"id\": \"ex_789\", \"status\": \"completed\", \"duration_ms\": 1523}]} ``` ### POST /executions/{id}/cancel Cancel a running execution. **Parameters:** - `id` (string): Execution ID - `reason` (string): Cancellation reason (optional) **Response:** - Success: `200` - Execution cancelled - Error: `400` - Bad Request **Example:** ```json {\"id\": \"ex_789\", \"status\": \"cancelled\", \"cancelled_at\": \"2024-01-15T10:05:00Z\"} ```",
              "score": null
            },
            {
              "doc_id": "api_teams",
              "chunk_id": "api_teams_fix_0",
              "content": "# Teams API Reference ## Overview The Teams API provides programmatic access to teams functionality. Base URL: `https://api.cloudflow.io/v1/teams` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /teams List teams the user belongs to. **Parameters:** **Response:** - Success: `200` - Returns array of team objects - Error: `400` - Bad Request **Example:** ```json {\"teams\": [{\"id\": \"team_01\", \"name\": \"Engineering\", \"member_count\": 15}]} ``` ### POST /teams/{id}/members Add a member to a team. **Parameters:** - `user_id` (string): User ID to add - `role` (string): Role: owner, admin, member, viewer **Response:** - Success: `201` - Member added - Error: `400` - Bad Request **Example:** ```json {\"team_id\": \"team_01\", \"user_id\": \"usr_002\", \"role\": \"member\"} ```",
              "score": null
            },
            {
              "doc_id": "api_audit",
              "chunk_id": "api_audit_fix_0",
              "content": "# Audit API Reference ## Overview The Audit API provides programmatic access to audit functionality. Base URL: `https://api.cloudflow.io/v1/audit` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /audit/logs Retrieve audit logs for compliance. **Parameters:** - `start_date` (date): Start date for log range - `end_date` (date): End date for log range - `actor` (string): Filter by user ID **Response:** - Success: `200` - Returns array of audit log entries - Error: `400` - Bad Request **Example:** ```json {\"logs\": [{\"timestamp\": \"2024-01-15T10:00:00Z\", \"actor\": \"usr_001\", \"action\": \"workflow.created\", \"resource\": \"wf_123\"}]} ```",
              "score": null
            }
          ],
          "latency_ms": 3.540853998856619,
          "query_id": "api_038",
          "dimension": "problem",
          "query": "my GET request to /workflows returns empty array even though I have workflows"
        },
        {
          "key_facts": [
            "GET /workflows",
            "limit",
            "status"
          ],
          "found_facts": [
            "GET /workflows",
            "limit",
            "status"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "api_workflows"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "api_workflows",
              "chunk_id": "api_workflows_fix_0",
              "content": "# Workflows API Reference ## Overview The Workflows API provides programmatic access to workflows functionality. Base URL: `https://api.cloudflow.io/v1/workflows` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /workflows List all workflows for the authenticated user. **Parameters:** - `limit` (integer): Maximum number of results (default: 50, max: 200) - `status` (string): Filter by status: active, paused, archived **Response:** - Success: `200` - Returns array of workflow objects - Error: `400` - Bad Request **Example:** ```json {\"workflows\": [{\"id\": \"wf_123\", \"name\": \"Daily Report\", \"status\": \"active\"}]} ``` ### POST /workflows Create a new workflow. **Parameters:** - `name` (string): Workflow name (required) - `definition` (object): Workflow definition in JSON format **Response:** - Success: `201` - Workflow created successfully - Error: `400` - Bad Request **Example:** ```json {\"id\": \"wf_456\", \"name\": \"New Workflow\", \"created_at\": \"2024-01-15T10:00:00Z\"} ``` ### DELETE /workflows/{id} Delete a workflow permanently. **Parameters:** - `id` (string): Workflow ID (required) **Response:** - Success: `204` - Workflow deleted - Error: `404` - Workflow not found **Example:** ```json {} ```",
              "score": null
            },
            {
              "doc_id": "api_metrics",
              "chunk_id": "api_metrics_fix_0",
              "content": "# Metrics API Reference ## Overview The Metrics API provides programmatic access to metrics functionality. Base URL: `https://api.cloudflow.io/v1/metrics` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /metrics/workflows Get workflow performance metrics. **Parameters:** - `period` (string): Time period: hour, day, week, month - `workflow_id` (string): Filter by workflow ID **Response:** - Success: `200` - Returns metrics data - Error: `400` - Bad Request **Example:** ```json {\"success_rate\": 0.985, \"avg_duration_ms\": 1250, \"p99_duration_ms\": 5000, \"total_executions\": 10000} ```",
              "score": null
            },
            {
              "doc_id": "api_executions",
              "chunk_id": "api_executions_fix_0",
              "content": "# Executions API Reference ## Overview The Executions API provides programmatic access to executions functionality. Base URL: `https://api.cloudflow.io/v1/executions` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /executions List workflow executions. **Parameters:** - `workflow_id` (string): Filter by workflow ID - `since` (datetime): Filter executions after this timestamp **Response:** - Success: `200` - Returns array of execution objects - Error: `400` - Bad Request **Example:** ```json {\"executions\": [{\"id\": \"ex_789\", \"status\": \"completed\", \"duration_ms\": 1523}]} ``` ### POST /executions/{id}/cancel Cancel a running execution. **Parameters:** - `id` (string): Execution ID - `reason` (string): Cancellation reason (optional) **Response:** - Success: `200` - Execution cancelled - Error: `400` - Bad Request **Example:** ```json {\"id\": \"ex_789\", \"status\": \"cancelled\", \"cancelled_at\": \"2024-01-15T10:05:00Z\"} ```",
              "score": null
            },
            {
              "doc_id": "release_3_2_0",
              "chunk_id": "release_3_2_0_fix_0",
              "content": "# Release Notes - v3.2.0 **Release Date:** 2024-01-15 ## New Features - Workflow versioning with rollback support - New Python SDK with async support - Bulk operations API for workflows - Custom retry policies per step ## Bug Fixes - Fixed race condition in concurrent execution handler - Resolved memory leak in long-running workflows - Fixed timezone handling in scheduled triggers ## Breaking Changes - **BREAKING:** API v1 endpoints deprecated, will be removed in v4.0 - **BREAKING:** Minimum Python version now 3.9 ## Upgrade Instructions ```bash cloudflow upgrade --version 3.2.0 ``` ## Known Issues None at this time.",
              "score": null
            },
            {
              "doc_id": "arch_workflow_engine",
              "chunk_id": "arch_workflow_engine_fix_0",
              "content": "# Workflow Engine Architecture ## Overview The Workflow Engine is the core component responsible for executing user-defined workflows. It uses an event-driven architecture with support for parallel execution, conditional branching, and automatic retries. ## Design Principles - **Idempotency**: All operations can be safely retried without side effects - **Eventual Consistency**: State converges to consistent view within 5 seconds - **Fault Tolerance**: System continues operating despite component failures ## Components ### Scheduler Manages workflow scheduling and triggers. Supports cron expressions, webhooks, and event-based triggers. **Technology:** Custom Go service **Scaling:** Horizontal with leader election ### Executor Runs individual workflow steps in isolated containers. Maximum execution time is 30 minutes per step. **Technology:** Kubernetes Jobs **Scaling:** Auto-scaling based on queue depth ### State Store Persists workflow state, execution history, and step results. Uses optimistic locking for concurrent updates. **Technology:** PostgreSQL with JSONB **Scaling:** Primary-replica with read replicas ## Data Flow Triggers \u2192 Scheduler \u2192 Job Queue \u2192 Executor \u2192 State Store \u2192 Webhooks/Notifications ## Performance Characteristics - **Latency P50:** 45ms - **Latency P99:** 250ms - **Throughput:** 5000 executions/minute",
              "score": null
            }
          ],
          "latency_ms": 3.34747399938351,
          "query_id": "api_038",
          "dimension": "casual",
          "query": "list workflows endpoint"
        },
        {
          "key_facts": [
            "GET /workflows",
            "limit",
            "status"
          ],
          "found_facts": [
            "GET /workflows",
            "limit",
            "status"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "api_workflows"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "api_workflows",
              "chunk_id": "api_workflows_fix_0",
              "content": "# Workflows API Reference ## Overview The Workflows API provides programmatic access to workflows functionality. Base URL: `https://api.cloudflow.io/v1/workflows` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /workflows List all workflows for the authenticated user. **Parameters:** - `limit` (integer): Maximum number of results (default: 50, max: 200) - `status` (string): Filter by status: active, paused, archived **Response:** - Success: `200` - Returns array of workflow objects - Error: `400` - Bad Request **Example:** ```json {\"workflows\": [{\"id\": \"wf_123\", \"name\": \"Daily Report\", \"status\": \"active\"}]} ``` ### POST /workflows Create a new workflow. **Parameters:** - `name` (string): Workflow name (required) - `definition` (object): Workflow definition in JSON format **Response:** - Success: `201` - Workflow created successfully - Error: `400` - Bad Request **Example:** ```json {\"id\": \"wf_456\", \"name\": \"New Workflow\", \"created_at\": \"2024-01-15T10:00:00Z\"} ``` ### DELETE /workflows/{id} Delete a workflow permanently. **Parameters:** - `id` (string): Workflow ID (required) **Response:** - Success: `204` - Workflow deleted - Error: `404` - Workflow not found **Example:** ```json {} ```",
              "score": null
            },
            {
              "doc_id": "api_metrics",
              "chunk_id": "api_metrics_fix_0",
              "content": "# Metrics API Reference ## Overview The Metrics API provides programmatic access to metrics functionality. Base URL: `https://api.cloudflow.io/v1/metrics` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /metrics/workflows Get workflow performance metrics. **Parameters:** - `period` (string): Time period: hour, day, week, month - `workflow_id` (string): Filter by workflow ID **Response:** - Success: `200` - Returns metrics data - Error: `400` - Bad Request **Example:** ```json {\"success_rate\": 0.985, \"avg_duration_ms\": 1250, \"p99_duration_ms\": 5000, \"total_executions\": 10000} ```",
              "score": null
            },
            {
              "doc_id": "api_audit",
              "chunk_id": "api_audit_fix_0",
              "content": "# Audit API Reference ## Overview The Audit API provides programmatic access to audit functionality. Base URL: `https://api.cloudflow.io/v1/audit` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /audit/logs Retrieve audit logs for compliance. **Parameters:** - `start_date` (date): Start date for log range - `end_date` (date): End date for log range - `actor` (string): Filter by user ID **Response:** - Success: `200` - Returns array of audit log entries - Error: `400` - Bad Request **Example:** ```json {\"logs\": [{\"timestamp\": \"2024-01-15T10:00:00Z\", \"actor\": \"usr_001\", \"action\": \"workflow.created\", \"resource\": \"wf_123\"}]} ```",
              "score": null
            },
            {
              "doc_id": "api_executions",
              "chunk_id": "api_executions_fix_0",
              "content": "# Executions API Reference ## Overview The Executions API provides programmatic access to executions functionality. Base URL: `https://api.cloudflow.io/v1/executions` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /executions List workflow executions. **Parameters:** - `workflow_id` (string): Filter by workflow ID - `since` (datetime): Filter executions after this timestamp **Response:** - Success: `200` - Returns array of execution objects - Error: `400` - Bad Request **Example:** ```json {\"executions\": [{\"id\": \"ex_789\", \"status\": \"completed\", \"duration_ms\": 1523}]} ``` ### POST /executions/{id}/cancel Cancel a running execution. **Parameters:** - `id` (string): Execution ID - `reason` (string): Cancellation reason (optional) **Response:** - Success: `200` - Execution cancelled - Error: `400` - Bad Request **Example:** ```json {\"id\": \"ex_789\", \"status\": \"cancelled\", \"cancelled_at\": \"2024-01-15T10:05:00Z\"} ```",
              "score": null
            },
            {
              "doc_id": "api_webhooks",
              "chunk_id": "api_webhooks_fix_0",
              "content": "# Webhooks API Reference ## Overview The Webhooks API provides programmatic access to webhooks functionality. Base URL: `https://api.cloudflow.io/v1/webhooks` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /webhooks List configured webhooks. **Parameters:** - `active` (boolean): Filter by active status **Response:** - Success: `200` - Returns array of webhook objects - Error: `400` - Bad Request **Example:** ```json {\"webhooks\": [{\"id\": \"wh_01\", \"url\": \"https://example.com/hook\", \"events\": [\"workflow.completed\"]}]} ``` ### POST /webhooks Create a new webhook endpoint. **Parameters:** - `url` (string): Webhook URL (must be HTTPS) - `events` (array): Events to subscribe to - `secret` (string): Signing secret for verification **Response:** - Success: `201` - Webhook created - Error: `400` - Bad Request **Example:** ```json {\"id\": \"wh_02\", \"url\": \"https://example.com/new-hook\", \"secret\": \"whsec_...\"} ```",
              "score": null
            }
          ],
          "latency_ms": 3.429226000662311,
          "query_id": "api_038",
          "dimension": "contextual",
          "query": "building a dashboard that shows all user workflows, need the API call to retrieve them"
        },
        {
          "key_facts": [
            "GET /workflows",
            "limit",
            "status"
          ],
          "found_facts": [
            "GET /workflows",
            "limit",
            "status"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "api_workflows"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "api_workflows",
              "chunk_id": "api_workflows_fix_0",
              "content": "# Workflows API Reference ## Overview The Workflows API provides programmatic access to workflows functionality. Base URL: `https://api.cloudflow.io/v1/workflows` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /workflows List all workflows for the authenticated user. **Parameters:** - `limit` (integer): Maximum number of results (default: 50, max: 200) - `status` (string): Filter by status: active, paused, archived **Response:** - Success: `200` - Returns array of workflow objects - Error: `400` - Bad Request **Example:** ```json {\"workflows\": [{\"id\": \"wf_123\", \"name\": \"Daily Report\", \"status\": \"active\"}]} ``` ### POST /workflows Create a new workflow. **Parameters:** - `name` (string): Workflow name (required) - `definition` (object): Workflow definition in JSON format **Response:** - Success: `201` - Workflow created successfully - Error: `400` - Bad Request **Example:** ```json {\"id\": \"wf_456\", \"name\": \"New Workflow\", \"created_at\": \"2024-01-15T10:00:00Z\"} ``` ### DELETE /workflows/{id} Delete a workflow permanently. **Parameters:** - `id` (string): Workflow ID (required) **Response:** - Success: `204` - Workflow deleted - Error: `404` - Workflow not found **Example:** ```json {} ```",
              "score": null
            },
            {
              "doc_id": "api_metrics",
              "chunk_id": "api_metrics_fix_0",
              "content": "# Metrics API Reference ## Overview The Metrics API provides programmatic access to metrics functionality. Base URL: `https://api.cloudflow.io/v1/metrics` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /metrics/workflows Get workflow performance metrics. **Parameters:** - `period` (string): Time period: hour, day, week, month - `workflow_id` (string): Filter by workflow ID **Response:** - Success: `200` - Returns metrics data - Error: `400` - Bad Request **Example:** ```json {\"success_rate\": 0.985, \"avg_duration_ms\": 1250, \"p99_duration_ms\": 5000, \"total_executions\": 10000} ```",
              "score": null
            },
            {
              "doc_id": "api_executions",
              "chunk_id": "api_executions_fix_0",
              "content": "# Executions API Reference ## Overview The Executions API provides programmatic access to executions functionality. Base URL: `https://api.cloudflow.io/v1/executions` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /executions List workflow executions. **Parameters:** - `workflow_id` (string): Filter by workflow ID - `since` (datetime): Filter executions after this timestamp **Response:** - Success: `200` - Returns array of execution objects - Error: `400` - Bad Request **Example:** ```json {\"executions\": [{\"id\": \"ex_789\", \"status\": \"completed\", \"duration_ms\": 1523}]} ``` ### POST /executions/{id}/cancel Cancel a running execution. **Parameters:** - `id` (string): Execution ID - `reason` (string): Cancellation reason (optional) **Response:** - Success: `200` - Execution cancelled - Error: `400` - Bad Request **Example:** ```json {\"id\": \"ex_789\", \"status\": \"cancelled\", \"cancelled_at\": \"2024-01-15T10:05:00Z\"} ```",
              "score": null
            },
            {
              "doc_id": "api_webhooks",
              "chunk_id": "api_webhooks_fix_0",
              "content": "# Webhooks API Reference ## Overview The Webhooks API provides programmatic access to webhooks functionality. Base URL: `https://api.cloudflow.io/v1/webhooks` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /webhooks List configured webhooks. **Parameters:** - `active` (boolean): Filter by active status **Response:** - Success: `200` - Returns array of webhook objects - Error: `400` - Bad Request **Example:** ```json {\"webhooks\": [{\"id\": \"wh_01\", \"url\": \"https://example.com/hook\", \"events\": [\"workflow.completed\"]}]} ``` ### POST /webhooks Create a new webhook endpoint. **Parameters:** - `url` (string): Webhook URL (must be HTTPS) - `events` (array): Events to subscribe to - `secret` (string): Signing secret for verification **Response:** - Success: `201` - Webhook created - Error: `400` - Bad Request **Example:** ```json {\"id\": \"wh_02\", \"url\": \"https://example.com/new-hook\", \"secret\": \"whsec_...\"} ```",
              "score": null
            },
            {
              "doc_id": "api_audit",
              "chunk_id": "api_audit_fix_0",
              "content": "# Audit API Reference ## Overview The Audit API provides programmatic access to audit functionality. Base URL: `https://api.cloudflow.io/v1/audit` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /audit/logs Retrieve audit logs for compliance. **Parameters:** - `start_date` (date): Start date for log range - `end_date` (date): End date for log range - `actor` (string): Filter by user ID **Response:** - Success: `200` - Returns array of audit log entries - Error: `400` - Bad Request **Example:** ```json {\"logs\": [{\"timestamp\": \"2024-01-15T10:00:00Z\", \"actor\": \"usr_001\", \"action\": \"workflow.created\", \"resource\": \"wf_123\"}]} ```",
              "score": null
            }
          ],
          "latency_ms": 4.726843000753433,
          "query_id": "api_038",
          "dimension": "negation",
          "query": "why can't I see my workflows when calling the API"
        },
        {
          "key_facts": [
            "POST /workflows",
            "201",
            "name",
            "definition"
          ],
          "found_facts": [
            "POST /workflows",
            "201",
            "name",
            "definition"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "api_workflows"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "api_workflows",
              "chunk_id": "api_workflows_fix_0",
              "content": "# Workflows API Reference ## Overview The Workflows API provides programmatic access to workflows functionality. Base URL: `https://api.cloudflow.io/v1/workflows` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /workflows List all workflows for the authenticated user. **Parameters:** - `limit` (integer): Maximum number of results (default: 50, max: 200) - `status` (string): Filter by status: active, paused, archived **Response:** - Success: `200` - Returns array of workflow objects - Error: `400` - Bad Request **Example:** ```json {\"workflows\": [{\"id\": \"wf_123\", \"name\": \"Daily Report\", \"status\": \"active\"}]} ``` ### POST /workflows Create a new workflow. **Parameters:** - `name` (string): Workflow name (required) - `definition` (object): Workflow definition in JSON format **Response:** - Success: `201` - Workflow created successfully - Error: `400` - Bad Request **Example:** ```json {\"id\": \"wf_456\", \"name\": \"New Workflow\", \"created_at\": \"2024-01-15T10:00:00Z\"} ``` ### DELETE /workflows/{id} Delete a workflow permanently. **Parameters:** - `id` (string): Workflow ID (required) **Response:** - Success: `204` - Workflow deleted - Error: `404` - Workflow not found **Example:** ```json {} ```",
              "score": null
            },
            {
              "doc_id": "api_metrics",
              "chunk_id": "api_metrics_fix_0",
              "content": "# Metrics API Reference ## Overview The Metrics API provides programmatic access to metrics functionality. Base URL: `https://api.cloudflow.io/v1/metrics` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /metrics/workflows Get workflow performance metrics. **Parameters:** - `period` (string): Time period: hour, day, week, month - `workflow_id` (string): Filter by workflow ID **Response:** - Success: `200` - Returns metrics data - Error: `400` - Bad Request **Example:** ```json {\"success_rate\": 0.985, \"avg_duration_ms\": 1250, \"p99_duration_ms\": 5000, \"total_executions\": 10000} ```",
              "score": null
            },
            {
              "doc_id": "api_executions",
              "chunk_id": "api_executions_fix_0",
              "content": "# Executions API Reference ## Overview The Executions API provides programmatic access to executions functionality. Base URL: `https://api.cloudflow.io/v1/executions` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /executions List workflow executions. **Parameters:** - `workflow_id` (string): Filter by workflow ID - `since` (datetime): Filter executions after this timestamp **Response:** - Success: `200` - Returns array of execution objects - Error: `400` - Bad Request **Example:** ```json {\"executions\": [{\"id\": \"ex_789\", \"status\": \"completed\", \"duration_ms\": 1523}]} ``` ### POST /executions/{id}/cancel Cancel a running execution. **Parameters:** - `id` (string): Execution ID - `reason` (string): Cancellation reason (optional) **Response:** - Success: `200` - Execution cancelled - Error: `400` - Bad Request **Example:** ```json {\"id\": \"ex_789\", \"status\": \"cancelled\", \"cancelled_at\": \"2024-01-15T10:05:00Z\"} ```",
              "score": null
            },
            {
              "doc_id": "howto_create_api_integration",
              "chunk_id": "howto_create_api_integration_fix_0",
              "content": "# How to Create API Integration ## Prerequisites - CloudFlow account ## Steps ### Step 1: Generate API key Create an API key for programmatic access. ```bash cloudflow apikeys create --name 'CI Integration' --scopes workflows:read,workflows:execute ``` ### Step 2: Store key securely Never commit API keys to source control. Use environment variables or secret management. > **Note:** Rotate keys every 90 days ### Step 3: Test authentication Verify the API key works correctly. ```bash curl -H 'Authorization: Bearer cf_...' https://api.cloudflow.io/v1/me ``` ### Step 4: Implement rate limiting Handle rate limit responses (HTTP 429) with exponential backoff. > **Note:** Default limit is 100 requests per minute ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            },
            {
              "doc_id": "howto_set_up_ci_cd_pipeline",
              "chunk_id": "howto_set_up_ci_cd_pipeline_fix_0",
              "content": "# How to Set Up CI/CD Pipeline ## Prerequisites - GitHub account - CloudFlow API key - Repository admin access ## Steps ### Step 1: Create GitHub Actions workflow Create a new file at `.github/workflows/cloudflow.yml` with the deployment configuration. ```bash mkdir -p .github/workflows && touch .github/workflows/cloudflow.yml ``` ### Step 2: Configure secrets Add your CloudFlow API key to GitHub secrets. ```bash gh secret set CLOUDFLOW_API_KEY ``` ### Step 3: Add deployment step Add the CloudFlow deployment action to your workflow. > **Note:** Use the official cloudflow/deploy-action@v2 ### Step 4: Test the pipeline Push a commit to trigger the workflow. ```bash git push origin main ``` ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            }
          ],
          "latency_ms": 4.951569999320782,
          "query_id": "api_039",
          "dimension": "original",
          "query": "How do I create a workflow via API?"
        },
        {
          "key_facts": [
            "POST /workflows",
            "201",
            "name",
            "definition"
          ],
          "found_facts": [
            "POST /workflows",
            "201",
            "name",
            "definition"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "api_workflows"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "api_workflows",
              "chunk_id": "api_workflows_fix_0",
              "content": "# Workflows API Reference ## Overview The Workflows API provides programmatic access to workflows functionality. Base URL: `https://api.cloudflow.io/v1/workflows` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /workflows List all workflows for the authenticated user. **Parameters:** - `limit` (integer): Maximum number of results (default: 50, max: 200) - `status` (string): Filter by status: active, paused, archived **Response:** - Success: `200` - Returns array of workflow objects - Error: `400` - Bad Request **Example:** ```json {\"workflows\": [{\"id\": \"wf_123\", \"name\": \"Daily Report\", \"status\": \"active\"}]} ``` ### POST /workflows Create a new workflow. **Parameters:** - `name` (string): Workflow name (required) - `definition` (object): Workflow definition in JSON format **Response:** - Success: `201` - Workflow created successfully - Error: `400` - Bad Request **Example:** ```json {\"id\": \"wf_456\", \"name\": \"New Workflow\", \"created_at\": \"2024-01-15T10:00:00Z\"} ``` ### DELETE /workflows/{id} Delete a workflow permanently. **Parameters:** - `id` (string): Workflow ID (required) **Response:** - Success: `204` - Workflow deleted - Error: `404` - Workflow not found **Example:** ```json {} ```",
              "score": null
            },
            {
              "doc_id": "api_integrations",
              "chunk_id": "api_integrations_fix_0",
              "content": "# Integrations API Reference ## Overview The Integrations API provides programmatic access to integrations functionality. Base URL: `https://api.cloudflow.io/v1/integrations` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /integrations List available integrations. **Parameters:** - `category` (string): Filter by category: storage, notification, database **Response:** - Success: `200` - Returns array of integration objects - Error: `400` - Bad Request **Example:** ```json {\"integrations\": [{\"id\": \"int_s3\", \"name\": \"Amazon S3\", \"category\": \"storage\", \"status\": \"connected\"}]} ``` ### POST /integrations/{id}/connect Connect an integration. **Parameters:** - `credentials` (object): Integration-specific credentials **Response:** - Success: `200` - Integration connected - Error: `400` - Bad Request **Example:** ```json {\"id\": \"int_s3\", \"status\": \"connected\", \"connected_at\": \"2024-01-15T10:00:00Z\"} ```",
              "score": null
            },
            {
              "doc_id": "api_executions",
              "chunk_id": "api_executions_fix_0",
              "content": "# Executions API Reference ## Overview The Executions API provides programmatic access to executions functionality. Base URL: `https://api.cloudflow.io/v1/executions` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /executions List workflow executions. **Parameters:** - `workflow_id` (string): Filter by workflow ID - `since` (datetime): Filter executions after this timestamp **Response:** - Success: `200` - Returns array of execution objects - Error: `400` - Bad Request **Example:** ```json {\"executions\": [{\"id\": \"ex_789\", \"status\": \"completed\", \"duration_ms\": 1523}]} ``` ### POST /executions/{id}/cancel Cancel a running execution. **Parameters:** - `id` (string): Execution ID - `reason` (string): Cancellation reason (optional) **Response:** - Success: `200` - Execution cancelled - Error: `400` - Bad Request **Example:** ```json {\"id\": \"ex_789\", \"status\": \"cancelled\", \"cancelled_at\": \"2024-01-15T10:05:00Z\"} ```",
              "score": null
            },
            {
              "doc_id": "api_webhooks",
              "chunk_id": "api_webhooks_fix_0",
              "content": "# Webhooks API Reference ## Overview The Webhooks API provides programmatic access to webhooks functionality. Base URL: `https://api.cloudflow.io/v1/webhooks` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /webhooks List configured webhooks. **Parameters:** - `active` (boolean): Filter by active status **Response:** - Success: `200` - Returns array of webhook objects - Error: `400` - Bad Request **Example:** ```json {\"webhooks\": [{\"id\": \"wh_01\", \"url\": \"https://example.com/hook\", \"events\": [\"workflow.completed\"]}]} ``` ### POST /webhooks Create a new webhook endpoint. **Parameters:** - `url` (string): Webhook URL (must be HTTPS) - `events` (array): Events to subscribe to - `secret` (string): Signing secret for verification **Response:** - Success: `201` - Webhook created - Error: `400` - Bad Request **Example:** ```json {\"id\": \"wh_02\", \"url\": \"https://example.com/new-hook\", \"secret\": \"whsec_...\"} ```",
              "score": null
            },
            {
              "doc_id": "api_metrics",
              "chunk_id": "api_metrics_fix_0",
              "content": "# Metrics API Reference ## Overview The Metrics API provides programmatic access to metrics functionality. Base URL: `https://api.cloudflow.io/v1/metrics` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /metrics/workflows Get workflow performance metrics. **Parameters:** - `period` (string): Time period: hour, day, week, month - `workflow_id` (string): Filter by workflow ID **Response:** - Success: `200` - Returns metrics data - Error: `400` - Bad Request **Example:** ```json {\"success_rate\": 0.985, \"avg_duration_ms\": 1250, \"p99_duration_ms\": 5000, \"total_executions\": 10000} ```",
              "score": null
            }
          ],
          "latency_ms": 3.917235000699293,
          "query_id": "api_039",
          "dimension": "synonym",
          "query": "I need to programmatically add a new workflow using the REST interface"
        },
        {
          "key_facts": [
            "POST /workflows",
            "201",
            "name",
            "definition"
          ],
          "found_facts": [
            "POST /workflows",
            "201",
            "name",
            "definition"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "api_workflows"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "api_workflows",
              "chunk_id": "api_workflows_fix_0",
              "content": "# Workflows API Reference ## Overview The Workflows API provides programmatic access to workflows functionality. Base URL: `https://api.cloudflow.io/v1/workflows` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /workflows List all workflows for the authenticated user. **Parameters:** - `limit` (integer): Maximum number of results (default: 50, max: 200) - `status` (string): Filter by status: active, paused, archived **Response:** - Success: `200` - Returns array of workflow objects - Error: `400` - Bad Request **Example:** ```json {\"workflows\": [{\"id\": \"wf_123\", \"name\": \"Daily Report\", \"status\": \"active\"}]} ``` ### POST /workflows Create a new workflow. **Parameters:** - `name` (string): Workflow name (required) - `definition` (object): Workflow definition in JSON format **Response:** - Success: `201` - Workflow created successfully - Error: `400` - Bad Request **Example:** ```json {\"id\": \"wf_456\", \"name\": \"New Workflow\", \"created_at\": \"2024-01-15T10:00:00Z\"} ``` ### DELETE /workflows/{id} Delete a workflow permanently. **Parameters:** - `id` (string): Workflow ID (required) **Response:** - Success: `204` - Workflow deleted - Error: `404` - Workflow not found **Example:** ```json {} ```",
              "score": null
            },
            {
              "doc_id": "release_3_2_0",
              "chunk_id": "release_3_2_0_fix_0",
              "content": "# Release Notes - v3.2.0 **Release Date:** 2024-01-15 ## New Features - Workflow versioning with rollback support - New Python SDK with async support - Bulk operations API for workflows - Custom retry policies per step ## Bug Fixes - Fixed race condition in concurrent execution handler - Resolved memory leak in long-running workflows - Fixed timezone handling in scheduled triggers ## Breaking Changes - **BREAKING:** API v1 endpoints deprecated, will be removed in v4.0 - **BREAKING:** Minimum Python version now 3.9 ## Upgrade Instructions ```bash cloudflow upgrade --version 3.2.0 ``` ## Known Issues None at this time.",
              "score": null
            },
            {
              "doc_id": "api_integrations",
              "chunk_id": "api_integrations_fix_0",
              "content": "# Integrations API Reference ## Overview The Integrations API provides programmatic access to integrations functionality. Base URL: `https://api.cloudflow.io/v1/integrations` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /integrations List available integrations. **Parameters:** - `category` (string): Filter by category: storage, notification, database **Response:** - Success: `200` - Returns array of integration objects - Error: `400` - Bad Request **Example:** ```json {\"integrations\": [{\"id\": \"int_s3\", \"name\": \"Amazon S3\", \"category\": \"storage\", \"status\": \"connected\"}]} ``` ### POST /integrations/{id}/connect Connect an integration. **Parameters:** - `credentials` (object): Integration-specific credentials **Response:** - Success: `200` - Integration connected - Error: `400` - Bad Request **Example:** ```json {\"id\": \"int_s3\", \"status\": \"connected\", \"connected_at\": \"2024-01-15T10:00:00Z\"} ```",
              "score": null
            },
            {
              "doc_id": "config_workflow_engine",
              "chunk_id": "config_workflow_engine_fix_0",
              "content": "# Workflow Engine Configuration Reference ## Overview This document describes all configuration options for Workflow Engine. ## Environment Variables | Variable | Type | Default | Description | |----------|------|---------|-------------| | `WORKFLOW_MAX_STEPS` | integer | 100 | Maximum steps per workflow | | `WORKFLOW_TIMEOUT_SECONDS` | integer | 3600 | Maximum workflow execution time | | `WORKFLOW_RETRY_LIMIT` | integer | 3 | Default retry attempts for failed steps | | `WORKFLOW_CONCURRENT_LIMIT` | integer | 10 | Maximum concurrent executions per workflow | | `EXECUTOR_POOL_SIZE` | integer | 50 | Number of executor workers | | `EXECUTOR_MEMORY_LIMIT` | string | 512Mi | Memory limit per executor | ## Configuration File Configuration can also be provided via `workflow engine.yaml`: ```yaml workflow_max_steps: 100 workflow_timeout_seconds: 3600 workflow_retry_limit: 3 workflow_concurrent_limit: 10 executor_pool_size: 50 ``` ## Validation Configuration is validated at startup. Invalid configuration will prevent the service from starting.",
              "score": null
            },
            {
              "doc_id": "api_metrics",
              "chunk_id": "api_metrics_fix_0",
              "content": "# Metrics API Reference ## Overview The Metrics API provides programmatic access to metrics functionality. Base URL: `https://api.cloudflow.io/v1/metrics` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /metrics/workflows Get workflow performance metrics. **Parameters:** - `period` (string): Time period: hour, day, week, month - `workflow_id` (string): Filter by workflow ID **Response:** - Success: `200` - Returns metrics data - Error: `400` - Bad Request **Example:** ```json {\"success_rate\": 0.985, \"avg_duration_ms\": 1250, \"p99_duration_ms\": 5000, \"total_executions\": 10000} ```",
              "score": null
            }
          ],
          "latency_ms": 3.5448410017124843,
          "query_id": "api_039",
          "dimension": "problem",
          "query": "getting 400 error when trying to POST a new workflow, what fields are required"
        },
        {
          "key_facts": [
            "POST /workflows",
            "201",
            "name",
            "definition"
          ],
          "found_facts": [
            "POST /workflows",
            "201",
            "name",
            "definition"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "api_workflows"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "api_executions",
              "chunk_id": "api_executions_fix_0",
              "content": "# Executions API Reference ## Overview The Executions API provides programmatic access to executions functionality. Base URL: `https://api.cloudflow.io/v1/executions` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /executions List workflow executions. **Parameters:** - `workflow_id` (string): Filter by workflow ID - `since` (datetime): Filter executions after this timestamp **Response:** - Success: `200` - Returns array of execution objects - Error: `400` - Bad Request **Example:** ```json {\"executions\": [{\"id\": \"ex_789\", \"status\": \"completed\", \"duration_ms\": 1523}]} ``` ### POST /executions/{id}/cancel Cancel a running execution. **Parameters:** - `id` (string): Execution ID - `reason` (string): Cancellation reason (optional) **Response:** - Success: `200` - Execution cancelled - Error: `400` - Bad Request **Example:** ```json {\"id\": \"ex_789\", \"status\": \"cancelled\", \"cancelled_at\": \"2024-01-15T10:05:00Z\"} ```",
              "score": null
            },
            {
              "doc_id": "api_workflows",
              "chunk_id": "api_workflows_fix_0",
              "content": "# Workflows API Reference ## Overview The Workflows API provides programmatic access to workflows functionality. Base URL: `https://api.cloudflow.io/v1/workflows` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /workflows List all workflows for the authenticated user. **Parameters:** - `limit` (integer): Maximum number of results (default: 50, max: 200) - `status` (string): Filter by status: active, paused, archived **Response:** - Success: `200` - Returns array of workflow objects - Error: `400` - Bad Request **Example:** ```json {\"workflows\": [{\"id\": \"wf_123\", \"name\": \"Daily Report\", \"status\": \"active\"}]} ``` ### POST /workflows Create a new workflow. **Parameters:** - `name` (string): Workflow name (required) - `definition` (object): Workflow definition in JSON format **Response:** - Success: `201` - Workflow created successfully - Error: `400` - Bad Request **Example:** ```json {\"id\": \"wf_456\", \"name\": \"New Workflow\", \"created_at\": \"2024-01-15T10:00:00Z\"} ``` ### DELETE /workflows/{id} Delete a workflow permanently. **Parameters:** - `id` (string): Workflow ID (required) **Response:** - Success: `204` - Workflow deleted - Error: `404` - Workflow not found **Example:** ```json {} ```",
              "score": null
            },
            {
              "doc_id": "api_metrics",
              "chunk_id": "api_metrics_fix_0",
              "content": "# Metrics API Reference ## Overview The Metrics API provides programmatic access to metrics functionality. Base URL: `https://api.cloudflow.io/v1/metrics` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /metrics/workflows Get workflow performance metrics. **Parameters:** - `period` (string): Time period: hour, day, week, month - `workflow_id` (string): Filter by workflow ID **Response:** - Success: `200` - Returns metrics data - Error: `400` - Bad Request **Example:** ```json {\"success_rate\": 0.985, \"avg_duration_ms\": 1250, \"p99_duration_ms\": 5000, \"total_executions\": 10000} ```",
              "score": null
            },
            {
              "doc_id": "api_integrations",
              "chunk_id": "api_integrations_fix_0",
              "content": "# Integrations API Reference ## Overview The Integrations API provides programmatic access to integrations functionality. Base URL: `https://api.cloudflow.io/v1/integrations` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /integrations List available integrations. **Parameters:** - `category` (string): Filter by category: storage, notification, database **Response:** - Success: `200` - Returns array of integration objects - Error: `400` - Bad Request **Example:** ```json {\"integrations\": [{\"id\": \"int_s3\", \"name\": \"Amazon S3\", \"category\": \"storage\", \"status\": \"connected\"}]} ``` ### POST /integrations/{id}/connect Connect an integration. **Parameters:** - `credentials` (object): Integration-specific credentials **Response:** - Success: `200` - Integration connected - Error: `400` - Bad Request **Example:** ```json {\"id\": \"int_s3\", \"status\": \"connected\", \"connected_at\": \"2024-01-15T10:00:00Z\"} ```",
              "score": null
            },
            {
              "doc_id": "api_webhooks",
              "chunk_id": "api_webhooks_fix_0",
              "content": "# Webhooks API Reference ## Overview The Webhooks API provides programmatic access to webhooks functionality. Base URL: `https://api.cloudflow.io/v1/webhooks` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /webhooks List configured webhooks. **Parameters:** - `active` (boolean): Filter by active status **Response:** - Success: `200` - Returns array of webhook objects - Error: `400` - Bad Request **Example:** ```json {\"webhooks\": [{\"id\": \"wh_01\", \"url\": \"https://example.com/hook\", \"events\": [\"workflow.completed\"]}]} ``` ### POST /webhooks Create a new webhook endpoint. **Parameters:** - `url` (string): Webhook URL (must be HTTPS) - `events` (array): Events to subscribe to - `secret` (string): Signing secret for verification **Response:** - Success: `201` - Webhook created - Error: `400` - Bad Request **Example:** ```json {\"id\": \"wh_02\", \"url\": \"https://example.com/new-hook\", \"secret\": \"whsec_...\"} ```",
              "score": null
            }
          ],
          "latency_ms": 4.131593999773031,
          "query_id": "api_039",
          "dimension": "casual",
          "query": "POST workflows endpoint params"
        },
        {
          "key_facts": [
            "POST /workflows",
            "201",
            "name",
            "definition"
          ],
          "found_facts": [
            "POST /workflows",
            "201",
            "name",
            "definition"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "api_workflows"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "api_workflows",
              "chunk_id": "api_workflows_fix_0",
              "content": "# Workflows API Reference ## Overview The Workflows API provides programmatic access to workflows functionality. Base URL: `https://api.cloudflow.io/v1/workflows` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /workflows List all workflows for the authenticated user. **Parameters:** - `limit` (integer): Maximum number of results (default: 50, max: 200) - `status` (string): Filter by status: active, paused, archived **Response:** - Success: `200` - Returns array of workflow objects - Error: `400` - Bad Request **Example:** ```json {\"workflows\": [{\"id\": \"wf_123\", \"name\": \"Daily Report\", \"status\": \"active\"}]} ``` ### POST /workflows Create a new workflow. **Parameters:** - `name` (string): Workflow name (required) - `definition` (object): Workflow definition in JSON format **Response:** - Success: `201` - Workflow created successfully - Error: `400` - Bad Request **Example:** ```json {\"id\": \"wf_456\", \"name\": \"New Workflow\", \"created_at\": \"2024-01-15T10:00:00Z\"} ``` ### DELETE /workflows/{id} Delete a workflow permanently. **Parameters:** - `id` (string): Workflow ID (required) **Response:** - Success: `204` - Workflow deleted - Error: `404` - Workflow not found **Example:** ```json {} ```",
              "score": null
            },
            {
              "doc_id": "howto_create_api_integration",
              "chunk_id": "howto_create_api_integration_fix_0",
              "content": "# How to Create API Integration ## Prerequisites - CloudFlow account ## Steps ### Step 1: Generate API key Create an API key for programmatic access. ```bash cloudflow apikeys create --name 'CI Integration' --scopes workflows:read,workflows:execute ``` ### Step 2: Store key securely Never commit API keys to source control. Use environment variables or secret management. > **Note:** Rotate keys every 90 days ### Step 3: Test authentication Verify the API key works correctly. ```bash curl -H 'Authorization: Bearer cf_...' https://api.cloudflow.io/v1/me ``` ### Step 4: Implement rate limiting Handle rate limit responses (HTTP 429) with exponential backoff. > **Note:** Default limit is 100 requests per minute ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            },
            {
              "doc_id": "howto_set_up_ci_cd_pipeline",
              "chunk_id": "howto_set_up_ci_cd_pipeline_fix_0",
              "content": "# How to Set Up CI/CD Pipeline ## Prerequisites - GitHub account - CloudFlow API key - Repository admin access ## Steps ### Step 1: Create GitHub Actions workflow Create a new file at `.github/workflows/cloudflow.yml` with the deployment configuration. ```bash mkdir -p .github/workflows && touch .github/workflows/cloudflow.yml ``` ### Step 2: Configure secrets Add your CloudFlow API key to GitHub secrets. ```bash gh secret set CLOUDFLOW_API_KEY ``` ### Step 3: Add deployment step Add the CloudFlow deployment action to your workflow. > **Note:** Use the official cloudflow/deploy-action@v2 ### Step 4: Test the pipeline Push a commit to trigger the workflow. ```bash git push origin main ``` ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            },
            {
              "doc_id": "api_metrics",
              "chunk_id": "api_metrics_fix_0",
              "content": "# Metrics API Reference ## Overview The Metrics API provides programmatic access to metrics functionality. Base URL: `https://api.cloudflow.io/v1/metrics` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /metrics/workflows Get workflow performance metrics. **Parameters:** - `period` (string): Time period: hour, day, week, month - `workflow_id` (string): Filter by workflow ID **Response:** - Success: `200` - Returns metrics data - Error: `400` - Bad Request **Example:** ```json {\"success_rate\": 0.985, \"avg_duration_ms\": 1250, \"p99_duration_ms\": 5000, \"total_executions\": 10000} ```",
              "score": null
            },
            {
              "doc_id": "api_users",
              "chunk_id": "api_users_fix_0",
              "content": "# Users API Reference ## Overview The Users API provides programmatic access to users functionality. Base URL: `https://api.cloudflow.io/v1/users` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /users/me Get current user profile. **Parameters:** **Response:** - Success: `200` - Returns user object - Error: `400` - Bad Request **Example:** ```json {\"id\": \"usr_001\", \"email\": \"user@example.com\", \"role\": \"admin\", \"created_at\": \"2023-06-01\"} ``` ### PATCH /users/me Update current user profile. **Parameters:** - `name` (string): Display name - `timezone` (string): Timezone (e.g., America/New_York) **Response:** - Success: `200` - User updated - Error: `400` - Bad Request **Example:** ```json {\"id\": \"usr_001\", \"name\": \"John Doe\", \"timezone\": \"America/New_York\"} ```",
              "score": null
            }
          ],
          "latency_ms": 4.3072210009995615,
          "query_id": "api_039",
          "dimension": "contextual",
          "query": "automating workflow provisioning for new customers, need to know how to create workflows via API"
        },
        {
          "key_facts": [
            "POST /workflows",
            "201",
            "name",
            "definition"
          ],
          "found_facts": [
            "POST /workflows",
            "201",
            "name",
            "definition"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "api_workflows"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "api_workflows",
              "chunk_id": "api_workflows_fix_0",
              "content": "# Workflows API Reference ## Overview The Workflows API provides programmatic access to workflows functionality. Base URL: `https://api.cloudflow.io/v1/workflows` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /workflows List all workflows for the authenticated user. **Parameters:** - `limit` (integer): Maximum number of results (default: 50, max: 200) - `status` (string): Filter by status: active, paused, archived **Response:** - Success: `200` - Returns array of workflow objects - Error: `400` - Bad Request **Example:** ```json {\"workflows\": [{\"id\": \"wf_123\", \"name\": \"Daily Report\", \"status\": \"active\"}]} ``` ### POST /workflows Create a new workflow. **Parameters:** - `name` (string): Workflow name (required) - `definition` (object): Workflow definition in JSON format **Response:** - Success: `201` - Workflow created successfully - Error: `400` - Bad Request **Example:** ```json {\"id\": \"wf_456\", \"name\": \"New Workflow\", \"created_at\": \"2024-01-15T10:00:00Z\"} ``` ### DELETE /workflows/{id} Delete a workflow permanently. **Parameters:** - `id` (string): Workflow ID (required) **Response:** - Success: `204` - Workflow deleted - Error: `404` - Workflow not found **Example:** ```json {} ```",
              "score": null
            },
            {
              "doc_id": "api_teams",
              "chunk_id": "api_teams_fix_0",
              "content": "# Teams API Reference ## Overview The Teams API provides programmatic access to teams functionality. Base URL: `https://api.cloudflow.io/v1/teams` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /teams List teams the user belongs to. **Parameters:** **Response:** - Success: `200` - Returns array of team objects - Error: `400` - Bad Request **Example:** ```json {\"teams\": [{\"id\": \"team_01\", \"name\": \"Engineering\", \"member_count\": 15}]} ``` ### POST /teams/{id}/members Add a member to a team. **Parameters:** - `user_id` (string): User ID to add - `role` (string): Role: owner, admin, member, viewer **Response:** - Success: `201` - Member added - Error: `400` - Bad Request **Example:** ```json {\"team_id\": \"team_01\", \"user_id\": \"usr_002\", \"role\": \"member\"} ```",
              "score": null
            },
            {
              "doc_id": "api_webhooks",
              "chunk_id": "api_webhooks_fix_0",
              "content": "# Webhooks API Reference ## Overview The Webhooks API provides programmatic access to webhooks functionality. Base URL: `https://api.cloudflow.io/v1/webhooks` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /webhooks List configured webhooks. **Parameters:** - `active` (boolean): Filter by active status **Response:** - Success: `200` - Returns array of webhook objects - Error: `400` - Bad Request **Example:** ```json {\"webhooks\": [{\"id\": \"wh_01\", \"url\": \"https://example.com/hook\", \"events\": [\"workflow.completed\"]}]} ``` ### POST /webhooks Create a new webhook endpoint. **Parameters:** - `url` (string): Webhook URL (must be HTTPS) - `events` (array): Events to subscribe to - `secret` (string): Signing secret for verification **Response:** - Success: `201` - Webhook created - Error: `400` - Bad Request **Example:** ```json {\"id\": \"wh_02\", \"url\": \"https://example.com/new-hook\", \"secret\": \"whsec_...\"} ```",
              "score": null
            },
            {
              "doc_id": "api_metrics",
              "chunk_id": "api_metrics_fix_0",
              "content": "# Metrics API Reference ## Overview The Metrics API provides programmatic access to metrics functionality. Base URL: `https://api.cloudflow.io/v1/metrics` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /metrics/workflows Get workflow performance metrics. **Parameters:** - `period` (string): Time period: hour, day, week, month - `workflow_id` (string): Filter by workflow ID **Response:** - Success: `200` - Returns metrics data - Error: `400` - Bad Request **Example:** ```json {\"success_rate\": 0.985, \"avg_duration_ms\": 1250, \"p99_duration_ms\": 5000, \"total_executions\": 10000} ```",
              "score": null
            },
            {
              "doc_id": "api_integrations",
              "chunk_id": "api_integrations_fix_0",
              "content": "# Integrations API Reference ## Overview The Integrations API provides programmatic access to integrations functionality. Base URL: `https://api.cloudflow.io/v1/integrations` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /integrations List available integrations. **Parameters:** - `category` (string): Filter by category: storage, notification, database **Response:** - Success: `200` - Returns array of integration objects - Error: `400` - Bad Request **Example:** ```json {\"integrations\": [{\"id\": \"int_s3\", \"name\": \"Amazon S3\", \"category\": \"storage\", \"status\": \"connected\"}]} ``` ### POST /integrations/{id}/connect Connect an integration. **Parameters:** - `credentials` (object): Integration-specific credentials **Response:** - Success: `200` - Integration connected - Error: `400` - Bad Request **Example:** ```json {\"id\": \"int_s3\", \"status\": \"connected\", \"connected_at\": \"2024-01-15T10:00:00Z\"} ```",
              "score": null
            }
          ],
          "latency_ms": 3.4135070000047563,
          "query_id": "api_039",
          "dimension": "negation",
          "query": "why isn't my workflow creation request working"
        },
        {
          "key_facts": [
            "POST /executions/{id}/cancel",
            "reason"
          ],
          "found_facts": [
            "POST /executions/{id}/cancel",
            "reason"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "api_executions"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "api_executions",
              "chunk_id": "api_executions_fix_0",
              "content": "# Executions API Reference ## Overview The Executions API provides programmatic access to executions functionality. Base URL: `https://api.cloudflow.io/v1/executions` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /executions List workflow executions. **Parameters:** - `workflow_id` (string): Filter by workflow ID - `since` (datetime): Filter executions after this timestamp **Response:** - Success: `200` - Returns array of execution objects - Error: `400` - Bad Request **Example:** ```json {\"executions\": [{\"id\": \"ex_789\", \"status\": \"completed\", \"duration_ms\": 1523}]} ``` ### POST /executions/{id}/cancel Cancel a running execution. **Parameters:** - `id` (string): Execution ID - `reason` (string): Cancellation reason (optional) **Response:** - Success: `200` - Execution cancelled - Error: `400` - Bad Request **Example:** ```json {\"id\": \"ex_789\", \"status\": \"cancelled\", \"cancelled_at\": \"2024-01-15T10:05:00Z\"} ```",
              "score": null
            },
            {
              "doc_id": "howto_debug_failed_execution",
              "chunk_id": "howto_debug_failed_execution_fix_0",
              "content": "# How to Debug Failed Execution ## Prerequisites - Failed execution ID ## Steps ### Step 1: Get execution details Retrieve the full execution record including step outputs. ```bash cloudflow executions get ex_123 --include-steps ``` ### Step 2: View step logs Check logs for the failed step. ```bash cloudflow executions logs ex_123 --step 3 ``` ### Step 3: Inspect input/output Review the data passed between steps. ```bash cloudflow executions inspect ex_123 --step 3 ``` ### Step 4: Enable debug mode Re-run with verbose logging enabled. ```bash cloudflow workflows execute wf_123 --debug ``` ### Step 5: Check error patterns Look for similar failures in recent executions. ```bash cloudflow executions list --status failed --since 24h ``` ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            },
            {
              "doc_id": "runbook_database_connection_exhaustion",
              "chunk_id": "runbook_database_connection_exhaustion_fix_0",
              "content": "# Runbook: Database Connection Exhaustion ## Severity **Level:** P1 - Critical **Response Time:** 15 minutes ## Symptoms - Connection pool at 100% - Queries timing out - 'too many connections' errors in logs ## Diagnosis ### Step 1 Check active connections: `SELECT count(*) FROM pg_stat_activity` ### Step 2 Identify long-running queries: `SELECT pid, now() - pg_stat_activity.query_start AS duration, query FROM pg_stat_activity WHERE state != 'idle' ORDER BY duration DESC` ### Step 3 Check PgBouncer stats: `SHOW POOLS` ### Step 4 Verify connection limits: `SHOW max_connections` ## Resolution ### Step 1 Kill long-running queries: `SELECT pg_terminate_backend(pid) FROM pg_stat_activity WHERE duration > interval '5 minutes'` ### Step 2 Increase PgBouncer pool size temporarily: update pgbouncer.ini ### Step 3 Restart affected services to release connections: `kubectl rollout restart deployment/api` ### Step 4 Scale down non-critical services to free connections ## Escalation If connections remain exhausted, escalate to DBA. Consider enabling connection queueing. ## Post-Incident - Document timeline in incident log - Schedule post-mortem within 48 hours - Update runbook if new patterns discovered",
              "score": null
            },
            {
              "doc_id": "runbook_out_of_memory_oom_kills",
              "chunk_id": "runbook_out_of_memory_oom_kills_fix_0",
              "content": "# Runbook: Out of Memory (OOM) Kills ## Severity **Level:** P3 - Medium **Response Time:** 15 minutes ## Symptoms - Pods restarting with OOMKilled status - Memory usage at limit - Application slowdown before restart ## Diagnosis ### Step 1 Check OOM events: `kubectl get events -n production --field-selector reason=OOMKilled` ### Step 2 Review memory usage: `kubectl top pods -n production --sort-by=memory` ### Step 3 Check container limits: `kubectl describe pod <pod-name> -n production | grep -A5 Limits` ### Step 4 Analyze heap dumps if available: `cloudflow debug heapdump --pod <pod-name>` ## Resolution ### Step 1 Increase memory limit: update deployment resources.limits.memory ### Step 2 Add memory request equal to limit to guarantee allocation ### Step 3 Enable JVM heap dump on OOM: add -XX:+HeapDumpOnOutOfMemoryError ### Step 4 Investigate memory leaks with profiling tools ### Step 5 Consider horizontal scaling instead of vertical ## Escalation If OOMs continue after limit increase, escalate to development team for memory leak investigation. ## Post-Incident - Document timeline in incident log - Schedule post-mortem within 48 hours - Update runbook if new patterns discovered",
              "score": null
            },
            {
              "doc_id": "howto_implement_retry_logic",
              "chunk_id": "howto_implement_retry_logic_fix_0",
              "content": "# How to Implement Retry Logic ## Prerequisites - Existing workflow ## Steps ### Step 1: Define retry policy Add retry configuration to your workflow step. > **Note:** Maximum 5 retry attempts allowed ### Step 2: Configure backoff Set exponential backoff with base delay of 1 second. The formula is: delay = base * 2^attempt. ### Step 3: Add timeout Set maximum execution time to prevent infinite loops. Default timeout is 30 seconds. ### Step 4: Handle final failure Configure on_failure handler to notify or escalate when all retries are exhausted. ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            }
          ],
          "latency_ms": 3.400872999918647,
          "query_id": "api_040",
          "dimension": "original",
          "query": "How do I cancel a running execution?"
        },
        {
          "key_facts": [
            "POST /executions/{id}/cancel",
            "reason"
          ],
          "found_facts": [
            "POST /executions/{id}/cancel",
            "reason"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "api_executions"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "api_executions",
              "chunk_id": "api_executions_fix_0",
              "content": "# Executions API Reference ## Overview The Executions API provides programmatic access to executions functionality. Base URL: `https://api.cloudflow.io/v1/executions` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /executions List workflow executions. **Parameters:** - `workflow_id` (string): Filter by workflow ID - `since` (datetime): Filter executions after this timestamp **Response:** - Success: `200` - Returns array of execution objects - Error: `400` - Bad Request **Example:** ```json {\"executions\": [{\"id\": \"ex_789\", \"status\": \"completed\", \"duration_ms\": 1523}]} ``` ### POST /executions/{id}/cancel Cancel a running execution. **Parameters:** - `id` (string): Execution ID - `reason` (string): Cancellation reason (optional) **Response:** - Success: `200` - Execution cancelled - Error: `400` - Bad Request **Example:** ```json {\"id\": \"ex_789\", \"status\": \"cancelled\", \"cancelled_at\": \"2024-01-15T10:05:00Z\"} ```",
              "score": null
            },
            {
              "doc_id": "api_workflows",
              "chunk_id": "api_workflows_fix_0",
              "content": "# Workflows API Reference ## Overview The Workflows API provides programmatic access to workflows functionality. Base URL: `https://api.cloudflow.io/v1/workflows` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /workflows List all workflows for the authenticated user. **Parameters:** - `limit` (integer): Maximum number of results (default: 50, max: 200) - `status` (string): Filter by status: active, paused, archived **Response:** - Success: `200` - Returns array of workflow objects - Error: `400` - Bad Request **Example:** ```json {\"workflows\": [{\"id\": \"wf_123\", \"name\": \"Daily Report\", \"status\": \"active\"}]} ``` ### POST /workflows Create a new workflow. **Parameters:** - `name` (string): Workflow name (required) - `definition` (object): Workflow definition in JSON format **Response:** - Success: `201` - Workflow created successfully - Error: `400` - Bad Request **Example:** ```json {\"id\": \"wf_456\", \"name\": \"New Workflow\", \"created_at\": \"2024-01-15T10:00:00Z\"} ``` ### DELETE /workflows/{id} Delete a workflow permanently. **Parameters:** - `id` (string): Workflow ID (required) **Response:** - Success: `204` - Workflow deleted - Error: `404` - Workflow not found **Example:** ```json {} ```",
              "score": null
            },
            {
              "doc_id": "api_metrics",
              "chunk_id": "api_metrics_fix_0",
              "content": "# Metrics API Reference ## Overview The Metrics API provides programmatic access to metrics functionality. Base URL: `https://api.cloudflow.io/v1/metrics` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /metrics/workflows Get workflow performance metrics. **Parameters:** - `period` (string): Time period: hour, day, week, month - `workflow_id` (string): Filter by workflow ID **Response:** - Success: `200` - Returns metrics data - Error: `400` - Bad Request **Example:** ```json {\"success_rate\": 0.985, \"avg_duration_ms\": 1250, \"p99_duration_ms\": 5000, \"total_executions\": 10000} ```",
              "score": null
            },
            {
              "doc_id": "howto_create_api_integration",
              "chunk_id": "howto_create_api_integration_fix_0",
              "content": "# How to Create API Integration ## Prerequisites - CloudFlow account ## Steps ### Step 1: Generate API key Create an API key for programmatic access. ```bash cloudflow apikeys create --name 'CI Integration' --scopes workflows:read,workflows:execute ``` ### Step 2: Store key securely Never commit API keys to source control. Use environment variables or secret management. > **Note:** Rotate keys every 90 days ### Step 3: Test authentication Verify the API key works correctly. ```bash curl -H 'Authorization: Bearer cf_...' https://api.cloudflow.io/v1/me ``` ### Step 4: Implement rate limiting Handle rate limit responses (HTTP 429) with exponential backoff. > **Note:** Default limit is 100 requests per minute ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            },
            {
              "doc_id": "api_audit",
              "chunk_id": "api_audit_fix_0",
              "content": "# Audit API Reference ## Overview The Audit API provides programmatic access to audit functionality. Base URL: `https://api.cloudflow.io/v1/audit` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /audit/logs Retrieve audit logs for compliance. **Parameters:** - `start_date` (date): Start date for log range - `end_date` (date): End date for log range - `actor` (string): Filter by user ID **Response:** - Success: `200` - Returns array of audit log entries - Error: `400` - Bad Request **Example:** ```json {\"logs\": [{\"timestamp\": \"2024-01-15T10:00:00Z\", \"actor\": \"usr_001\", \"action\": \"workflow.created\", \"resource\": \"wf_123\"}]} ```",
              "score": null
            }
          ],
          "latency_ms": 3.5675429990078555,
          "query_id": "api_040",
          "dimension": "synonym",
          "query": "I need to stop an in-progress workflow execution through the API"
        },
        {
          "key_facts": [
            "POST /executions/{id}/cancel",
            "reason"
          ],
          "found_facts": [
            "POST /executions/{id}/cancel",
            "reason"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "api_executions"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "api_workflows",
              "chunk_id": "api_workflows_fix_0",
              "content": "# Workflows API Reference ## Overview The Workflows API provides programmatic access to workflows functionality. Base URL: `https://api.cloudflow.io/v1/workflows` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /workflows List all workflows for the authenticated user. **Parameters:** - `limit` (integer): Maximum number of results (default: 50, max: 200) - `status` (string): Filter by status: active, paused, archived **Response:** - Success: `200` - Returns array of workflow objects - Error: `400` - Bad Request **Example:** ```json {\"workflows\": [{\"id\": \"wf_123\", \"name\": \"Daily Report\", \"status\": \"active\"}]} ``` ### POST /workflows Create a new workflow. **Parameters:** - `name` (string): Workflow name (required) - `definition` (object): Workflow definition in JSON format **Response:** - Success: `201` - Workflow created successfully - Error: `400` - Bad Request **Example:** ```json {\"id\": \"wf_456\", \"name\": \"New Workflow\", \"created_at\": \"2024-01-15T10:00:00Z\"} ``` ### DELETE /workflows/{id} Delete a workflow permanently. **Parameters:** - `id` (string): Workflow ID (required) **Response:** - Success: `204` - Workflow deleted - Error: `404` - Workflow not found **Example:** ```json {} ```",
              "score": null
            },
            {
              "doc_id": "api_executions",
              "chunk_id": "api_executions_fix_0",
              "content": "# Executions API Reference ## Overview The Executions API provides programmatic access to executions functionality. Base URL: `https://api.cloudflow.io/v1/executions` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /executions List workflow executions. **Parameters:** - `workflow_id` (string): Filter by workflow ID - `since` (datetime): Filter executions after this timestamp **Response:** - Success: `200` - Returns array of execution objects - Error: `400` - Bad Request **Example:** ```json {\"executions\": [{\"id\": \"ex_789\", \"status\": \"completed\", \"duration_ms\": 1523}]} ``` ### POST /executions/{id}/cancel Cancel a running execution. **Parameters:** - `id` (string): Execution ID - `reason` (string): Cancellation reason (optional) **Response:** - Success: `200` - Execution cancelled - Error: `400` - Bad Request **Example:** ```json {\"id\": \"ex_789\", \"status\": \"cancelled\", \"cancelled_at\": \"2024-01-15T10:05:00Z\"} ```",
              "score": null
            },
            {
              "doc_id": "api_metrics",
              "chunk_id": "api_metrics_fix_0",
              "content": "# Metrics API Reference ## Overview The Metrics API provides programmatic access to metrics functionality. Base URL: `https://api.cloudflow.io/v1/metrics` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /metrics/workflows Get workflow performance metrics. **Parameters:** - `period` (string): Time period: hour, day, week, month - `workflow_id` (string): Filter by workflow ID **Response:** - Success: `200` - Returns metrics data - Error: `400` - Bad Request **Example:** ```json {\"success_rate\": 0.985, \"avg_duration_ms\": 1250, \"p99_duration_ms\": 5000, \"total_executions\": 10000} ```",
              "score": null
            },
            {
              "doc_id": "howto_implement_retry_logic",
              "chunk_id": "howto_implement_retry_logic_fix_0",
              "content": "# How to Implement Retry Logic ## Prerequisites - Existing workflow ## Steps ### Step 1: Define retry policy Add retry configuration to your workflow step. > **Note:** Maximum 5 retry attempts allowed ### Step 2: Configure backoff Set exponential backoff with base delay of 1 second. The formula is: delay = base * 2^attempt. ### Step 3: Add timeout Set maximum execution time to prevent infinite loops. Default timeout is 30 seconds. ### Step 4: Handle final failure Configure on_failure handler to notify or escalate when all retries are exhausted. ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            },
            {
              "doc_id": "api_audit",
              "chunk_id": "api_audit_fix_0",
              "content": "# Audit API Reference ## Overview The Audit API provides programmatic access to audit functionality. Base URL: `https://api.cloudflow.io/v1/audit` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /audit/logs Retrieve audit logs for compliance. **Parameters:** - `start_date` (date): Start date for log range - `end_date` (date): End date for log range - `actor` (string): Filter by user ID **Response:** - Success: `200` - Returns array of audit log entries - Error: `400` - Bad Request **Example:** ```json {\"logs\": [{\"timestamp\": \"2024-01-15T10:00:00Z\", \"actor\": \"usr_001\", \"action\": \"workflow.created\", \"resource\": \"wf_123\"}]} ```",
              "score": null
            }
          ],
          "latency_ms": 3.4306180004932685,
          "query_id": "api_040",
          "dimension": "problem",
          "query": "workflow stuck running for hours, how do I terminate it via API"
        },
        {
          "key_facts": [
            "POST /executions/{id}/cancel",
            "reason"
          ],
          "found_facts": [
            "POST /executions/{id}/cancel",
            "reason"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "api_executions"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "api_executions",
              "chunk_id": "api_executions_fix_0",
              "content": "# Executions API Reference ## Overview The Executions API provides programmatic access to executions functionality. Base URL: `https://api.cloudflow.io/v1/executions` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /executions List workflow executions. **Parameters:** - `workflow_id` (string): Filter by workflow ID - `since` (datetime): Filter executions after this timestamp **Response:** - Success: `200` - Returns array of execution objects - Error: `400` - Bad Request **Example:** ```json {\"executions\": [{\"id\": \"ex_789\", \"status\": \"completed\", \"duration_ms\": 1523}]} ``` ### POST /executions/{id}/cancel Cancel a running execution. **Parameters:** - `id` (string): Execution ID - `reason` (string): Cancellation reason (optional) **Response:** - Success: `200` - Execution cancelled - Error: `400` - Bad Request **Example:** ```json {\"id\": \"ex_789\", \"status\": \"cancelled\", \"cancelled_at\": \"2024-01-15T10:05:00Z\"} ```",
              "score": null
            },
            {
              "doc_id": "howto_implement_retry_logic",
              "chunk_id": "howto_implement_retry_logic_fix_0",
              "content": "# How to Implement Retry Logic ## Prerequisites - Existing workflow ## Steps ### Step 1: Define retry policy Add retry configuration to your workflow step. > **Note:** Maximum 5 retry attempts allowed ### Step 2: Configure backoff Set exponential backoff with base delay of 1 second. The formula is: delay = base * 2^attempt. ### Step 3: Add timeout Set maximum execution time to prevent infinite loops. Default timeout is 30 seconds. ### Step 4: Handle final failure Configure on_failure handler to notify or escalate when all retries are exhausted. ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            },
            {
              "doc_id": "howto_implement_workflow_versioning",
              "chunk_id": "howto_implement_workflow_versioning_fix_0",
              "content": "# How to Implement Workflow Versioning ## Prerequisites - Existing workflow ## Steps ### Step 1: Enable versioning Turn on versioning for your workflow. This is required for the feature. ```bash cloudflow workflows update wf_123 --enable-versioning ``` ### Step 2: Create new version Save changes as a new version instead of overwriting. ```bash cloudflow workflows publish wf_123 --version 2.0.0 ``` ### Step 3: Set active version Choose which version handles new executions. ```bash cloudflow workflows activate wf_123 --version 2.0.0 ``` ### Step 4: Roll back if needed Quickly revert to a previous version. ```bash cloudflow workflows rollback wf_123 --to-version 1.0.0 ``` ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            },
            {
              "doc_id": "howto_debug_failed_execution",
              "chunk_id": "howto_debug_failed_execution_fix_0",
              "content": "# How to Debug Failed Execution ## Prerequisites - Failed execution ID ## Steps ### Step 1: Get execution details Retrieve the full execution record including step outputs. ```bash cloudflow executions get ex_123 --include-steps ``` ### Step 2: View step logs Check logs for the failed step. ```bash cloudflow executions logs ex_123 --step 3 ``` ### Step 3: Inspect input/output Review the data passed between steps. ```bash cloudflow executions inspect ex_123 --step 3 ``` ### Step 4: Enable debug mode Re-run with verbose logging enabled. ```bash cloudflow workflows execute wf_123 --debug ``` ### Step 5: Check error patterns Look for similar failures in recent executions. ```bash cloudflow executions list --status failed --since 24h ``` ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            },
            {
              "doc_id": "runbook_database_connection_exhaustion",
              "chunk_id": "runbook_database_connection_exhaustion_fix_0",
              "content": "# Runbook: Database Connection Exhaustion ## Severity **Level:** P1 - Critical **Response Time:** 15 minutes ## Symptoms - Connection pool at 100% - Queries timing out - 'too many connections' errors in logs ## Diagnosis ### Step 1 Check active connections: `SELECT count(*) FROM pg_stat_activity` ### Step 2 Identify long-running queries: `SELECT pid, now() - pg_stat_activity.query_start AS duration, query FROM pg_stat_activity WHERE state != 'idle' ORDER BY duration DESC` ### Step 3 Check PgBouncer stats: `SHOW POOLS` ### Step 4 Verify connection limits: `SHOW max_connections` ## Resolution ### Step 1 Kill long-running queries: `SELECT pg_terminate_backend(pid) FROM pg_stat_activity WHERE duration > interval '5 minutes'` ### Step 2 Increase PgBouncer pool size temporarily: update pgbouncer.ini ### Step 3 Restart affected services to release connections: `kubectl rollout restart deployment/api` ### Step 4 Scale down non-critical services to free connections ## Escalation If connections remain exhausted, escalate to DBA. Consider enabling connection queueing. ## Post-Incident - Document timeline in incident log - Schedule post-mortem within 48 hours - Update runbook if new patterns discovered",
              "score": null
            }
          ],
          "latency_ms": 3.5996930000692373,
          "query_id": "api_040",
          "dimension": "casual",
          "query": "cancel execution endpoint"
        },
        {
          "key_facts": [
            "POST /executions/{id}/cancel",
            "reason"
          ],
          "found_facts": [
            "POST /executions/{id}/cancel",
            "reason"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "api_executions"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "api_executions",
              "chunk_id": "api_executions_fix_0",
              "content": "# Executions API Reference ## Overview The Executions API provides programmatic access to executions functionality. Base URL: `https://api.cloudflow.io/v1/executions` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /executions List workflow executions. **Parameters:** - `workflow_id` (string): Filter by workflow ID - `since` (datetime): Filter executions after this timestamp **Response:** - Success: `200` - Returns array of execution objects - Error: `400` - Bad Request **Example:** ```json {\"executions\": [{\"id\": \"ex_789\", \"status\": \"completed\", \"duration_ms\": 1523}]} ``` ### POST /executions/{id}/cancel Cancel a running execution. **Parameters:** - `id` (string): Execution ID - `reason` (string): Cancellation reason (optional) **Response:** - Success: `200` - Execution cancelled - Error: `400` - Bad Request **Example:** ```json {\"id\": \"ex_789\", \"status\": \"cancelled\", \"cancelled_at\": \"2024-01-15T10:05:00Z\"} ```",
              "score": null
            },
            {
              "doc_id": "arch_workflow_engine",
              "chunk_id": "arch_workflow_engine_fix_0",
              "content": "# Workflow Engine Architecture ## Overview The Workflow Engine is the core component responsible for executing user-defined workflows. It uses an event-driven architecture with support for parallel execution, conditional branching, and automatic retries. ## Design Principles - **Idempotency**: All operations can be safely retried without side effects - **Eventual Consistency**: State converges to consistent view within 5 seconds - **Fault Tolerance**: System continues operating despite component failures ## Components ### Scheduler Manages workflow scheduling and triggers. Supports cron expressions, webhooks, and event-based triggers. **Technology:** Custom Go service **Scaling:** Horizontal with leader election ### Executor Runs individual workflow steps in isolated containers. Maximum execution time is 30 minutes per step. **Technology:** Kubernetes Jobs **Scaling:** Auto-scaling based on queue depth ### State Store Persists workflow state, execution history, and step results. Uses optimistic locking for concurrent updates. **Technology:** PostgreSQL with JSONB **Scaling:** Primary-replica with read replicas ## Data Flow Triggers \u2192 Scheduler \u2192 Job Queue \u2192 Executor \u2192 State Store \u2192 Webhooks/Notifications ## Performance Characteristics - **Latency P50:** 45ms - **Latency P99:** 250ms - **Throughput:** 5000 executions/minute",
              "score": null
            },
            {
              "doc_id": "adr_005",
              "chunk_id": "adr_005_fix_0",
              "content": "# ADR-005: Adopt Event-Driven Architecture for Workflow Execution **Status:** Accepted **Date:** 2024-12-04 ## Context We need an architecture that supports asynchronous workflow execution, step dependencies, and failure recovery. ## Options Considered ### Option 1: Event-driven with Kafka Publish-subscribe messaging. **Pros:** - Loose coupling - Scalability - Event replay - Audit trail **Cons:** - Eventual consistency - Operational complexity - Message ordering challenges ### Option 2: Request-response Synchronous API calls. **Pros:** - Simple mental model - Immediate feedback - Easy debugging **Cons:** - Tight coupling - Cascading failures - Timeout issues ### Option 3: Polling-based Workers poll for tasks. **Pros:** - Simple implementation - Easy rate limiting - No message broker **Cons:** - Polling overhead - Higher latency - Inefficient at scale ## Decision We will use an event-driven architecture with Kafka for workflow step execution. ## Consequences - Implement idempotent consumers - Use dead letter queues for failed messages - Monitor consumer lag",
              "score": null
            },
            {
              "doc_id": "api_workflows",
              "chunk_id": "api_workflows_fix_0",
              "content": "# Workflows API Reference ## Overview The Workflows API provides programmatic access to workflows functionality. Base URL: `https://api.cloudflow.io/v1/workflows` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /workflows List all workflows for the authenticated user. **Parameters:** - `limit` (integer): Maximum number of results (default: 50, max: 200) - `status` (string): Filter by status: active, paused, archived **Response:** - Success: `200` - Returns array of workflow objects - Error: `400` - Bad Request **Example:** ```json {\"workflows\": [{\"id\": \"wf_123\", \"name\": \"Daily Report\", \"status\": \"active\"}]} ``` ### POST /workflows Create a new workflow. **Parameters:** - `name` (string): Workflow name (required) - `definition` (object): Workflow definition in JSON format **Response:** - Success: `201` - Workflow created successfully - Error: `400` - Bad Request **Example:** ```json {\"id\": \"wf_456\", \"name\": \"New Workflow\", \"created_at\": \"2024-01-15T10:00:00Z\"} ``` ### DELETE /workflows/{id} Delete a workflow permanently. **Parameters:** - `id` (string): Workflow ID (required) **Response:** - Success: `204` - Workflow deleted - Error: `404` - Workflow not found **Example:** ```json {} ```",
              "score": null
            },
            {
              "doc_id": "howto_implement_retry_logic",
              "chunk_id": "howto_implement_retry_logic_fix_0",
              "content": "# How to Implement Retry Logic ## Prerequisites - Existing workflow ## Steps ### Step 1: Define retry policy Add retry configuration to your workflow step. > **Note:** Maximum 5 retry attempts allowed ### Step 2: Configure backoff Set exponential backoff with base delay of 1 second. The formula is: delay = base * 2^attempt. ### Step 3: Add timeout Set maximum execution time to prevent infinite loops. Default timeout is 30 seconds. ### Step 4: Handle final failure Configure on_failure handler to notify or escalate when all retries are exhausted. ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            }
          ],
          "latency_ms": 3.9221639999595936,
          "query_id": "api_040",
          "dimension": "contextual",
          "query": "building a kill switch for runaway workflows, what's the API to abort executions"
        },
        {
          "key_facts": [
            "POST /executions/{id}/cancel",
            "reason"
          ],
          "found_facts": [
            "POST /executions/{id}/cancel",
            "reason"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "api_executions"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "runbook_database_connection_exhaustion",
              "chunk_id": "runbook_database_connection_exhaustion_fix_0",
              "content": "# Runbook: Database Connection Exhaustion ## Severity **Level:** P1 - Critical **Response Time:** 15 minutes ## Symptoms - Connection pool at 100% - Queries timing out - 'too many connections' errors in logs ## Diagnosis ### Step 1 Check active connections: `SELECT count(*) FROM pg_stat_activity` ### Step 2 Identify long-running queries: `SELECT pid, now() - pg_stat_activity.query_start AS duration, query FROM pg_stat_activity WHERE state != 'idle' ORDER BY duration DESC` ### Step 3 Check PgBouncer stats: `SHOW POOLS` ### Step 4 Verify connection limits: `SHOW max_connections` ## Resolution ### Step 1 Kill long-running queries: `SELECT pg_terminate_backend(pid) FROM pg_stat_activity WHERE duration > interval '5 minutes'` ### Step 2 Increase PgBouncer pool size temporarily: update pgbouncer.ini ### Step 3 Restart affected services to release connections: `kubectl rollout restart deployment/api` ### Step 4 Scale down non-critical services to free connections ## Escalation If connections remain exhausted, escalate to DBA. Consider enabling connection queueing. ## Post-Incident - Document timeline in incident log - Schedule post-mortem within 48 hours - Update runbook if new patterns discovered",
              "score": null
            },
            {
              "doc_id": "runbook_out_of_memory_oom_kills",
              "chunk_id": "runbook_out_of_memory_oom_kills_fix_0",
              "content": "# Runbook: Out of Memory (OOM) Kills ## Severity **Level:** P3 - Medium **Response Time:** 15 minutes ## Symptoms - Pods restarting with OOMKilled status - Memory usage at limit - Application slowdown before restart ## Diagnosis ### Step 1 Check OOM events: `kubectl get events -n production --field-selector reason=OOMKilled` ### Step 2 Review memory usage: `kubectl top pods -n production --sort-by=memory` ### Step 3 Check container limits: `kubectl describe pod <pod-name> -n production | grep -A5 Limits` ### Step 4 Analyze heap dumps if available: `cloudflow debug heapdump --pod <pod-name>` ## Resolution ### Step 1 Increase memory limit: update deployment resources.limits.memory ### Step 2 Add memory request equal to limit to guarantee allocation ### Step 3 Enable JVM heap dump on OOM: add -XX:+HeapDumpOnOutOfMemoryError ### Step 4 Investigate memory leaks with profiling tools ### Step 5 Consider horizontal scaling instead of vertical ## Escalation If OOMs continue after limit increase, escalate to development team for memory leak investigation. ## Post-Incident - Document timeline in incident log - Schedule post-mortem within 48 hours - Update runbook if new patterns discovered",
              "score": null
            },
            {
              "doc_id": "howto_debug_failed_execution",
              "chunk_id": "howto_debug_failed_execution_fix_0",
              "content": "# How to Debug Failed Execution ## Prerequisites - Failed execution ID ## Steps ### Step 1: Get execution details Retrieve the full execution record including step outputs. ```bash cloudflow executions get ex_123 --include-steps ``` ### Step 2: View step logs Check logs for the failed step. ```bash cloudflow executions logs ex_123 --step 3 ``` ### Step 3: Inspect input/output Review the data passed between steps. ```bash cloudflow executions inspect ex_123 --step 3 ``` ### Step 4: Enable debug mode Re-run with verbose logging enabled. ```bash cloudflow workflows execute wf_123 --debug ``` ### Step 5: Check error patterns Look for similar failures in recent executions. ```bash cloudflow executions list --status failed --since 24h ``` ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            },
            {
              "doc_id": "api_executions",
              "chunk_id": "api_executions_fix_0",
              "content": "# Executions API Reference ## Overview The Executions API provides programmatic access to executions functionality. Base URL: `https://api.cloudflow.io/v1/executions` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /executions List workflow executions. **Parameters:** - `workflow_id` (string): Filter by workflow ID - `since` (datetime): Filter executions after this timestamp **Response:** - Success: `200` - Returns array of execution objects - Error: `400` - Bad Request **Example:** ```json {\"executions\": [{\"id\": \"ex_789\", \"status\": \"completed\", \"duration_ms\": 1523}]} ``` ### POST /executions/{id}/cancel Cancel a running execution. **Parameters:** - `id` (string): Execution ID - `reason` (string): Cancellation reason (optional) **Response:** - Success: `200` - Execution cancelled - Error: `400` - Bad Request **Example:** ```json {\"id\": \"ex_789\", \"status\": \"cancelled\", \"cancelled_at\": \"2024-01-15T10:05:00Z\"} ```",
              "score": null
            },
            {
              "doc_id": "runbook_kafka_consumer_lag",
              "chunk_id": "runbook_kafka_consumer_lag_fix_0",
              "content": "# Runbook: Kafka Consumer Lag ## Severity **Level:** P1 - Critical **Response Time:** 1 hour ## Symptoms - Consumer lag > 10000 messages - Processing delays reported - Alerts from lag monitor ## Diagnosis ### Step 1 Check consumer group lag: `kafka-consumer-groups.sh --bootstrap-server kafka:9092 --describe --group workflow-executor` ### Step 2 Verify consumer health: `kubectl get pods -l app=workflow-executor -n production` ### Step 3 Check for processing errors: `kubectl logs -l app=workflow-executor --tail=100 -n production | grep ERROR` ### Step 4 Monitor throughput: `cloudflow metrics get kafka.consumer.records_per_second` ## Resolution ### Step 1 Scale up consumers: `kubectl scale deployment/workflow-executor --replicas=10 -n production` ### Step 2 Reset offset if messages are stale: `kafka-consumer-groups.sh --bootstrap-server kafka:9092 --group workflow-executor --reset-offsets --to-latest --execute --topic workflows` ### Step 3 Pause producer if backpressure needed: `cloudflow workflows pause-triggers` ### Step 4 Increase partition count for parallelism (requires coordination) ## Escalation If lag persists > 30 minutes, page platform team. May need Kafka cluster scaling. ## Post-Incident - Document timeline in incident log - Schedule post-mortem within 48 hours - Update runbook if new patterns discovered",
              "score": null
            }
          ],
          "latency_ms": 3.480200999547378,
          "query_id": "api_040",
          "dimension": "negation",
          "query": "why can't I stop this execution that's taking forever"
        },
        {
          "key_facts": [
            "GET /users/me",
            "email",
            "role"
          ],
          "found_facts": [
            "GET /users/me",
            "email",
            "role"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "api_users"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "api_users",
              "chunk_id": "api_users_fix_0",
              "content": "# Users API Reference ## Overview The Users API provides programmatic access to users functionality. Base URL: `https://api.cloudflow.io/v1/users` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /users/me Get current user profile. **Parameters:** **Response:** - Success: `200` - Returns user object - Error: `400` - Bad Request **Example:** ```json {\"id\": \"usr_001\", \"email\": \"user@example.com\", \"role\": \"admin\", \"created_at\": \"2023-06-01\"} ``` ### PATCH /users/me Update current user profile. **Parameters:** - `name` (string): Display name - `timezone` (string): Timezone (e.g., America/New_York) **Response:** - Success: `200` - User updated - Error: `400` - Bad Request **Example:** ```json {\"id\": \"usr_001\", \"name\": \"John Doe\", \"timezone\": \"America/New_York\"} ```",
              "score": null
            },
            {
              "doc_id": "api_audit",
              "chunk_id": "api_audit_fix_0",
              "content": "# Audit API Reference ## Overview The Audit API provides programmatic access to audit functionality. Base URL: `https://api.cloudflow.io/v1/audit` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /audit/logs Retrieve audit logs for compliance. **Parameters:** - `start_date` (date): Start date for log range - `end_date` (date): End date for log range - `actor` (string): Filter by user ID **Response:** - Success: `200` - Returns array of audit log entries - Error: `400` - Bad Request **Example:** ```json {\"logs\": [{\"timestamp\": \"2024-01-15T10:00:00Z\", \"actor\": \"usr_001\", \"action\": \"workflow.created\", \"resource\": \"wf_123\"}]} ```",
              "score": null
            },
            {
              "doc_id": "api_metrics",
              "chunk_id": "api_metrics_fix_0",
              "content": "# Metrics API Reference ## Overview The Metrics API provides programmatic access to metrics functionality. Base URL: `https://api.cloudflow.io/v1/metrics` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /metrics/workflows Get workflow performance metrics. **Parameters:** - `period` (string): Time period: hour, day, week, month - `workflow_id` (string): Filter by workflow ID **Response:** - Success: `200` - Returns metrics data - Error: `400` - Bad Request **Example:** ```json {\"success_rate\": 0.985, \"avg_duration_ms\": 1250, \"p99_duration_ms\": 5000, \"total_executions\": 10000} ```",
              "score": null
            },
            {
              "doc_id": "api_secrets",
              "chunk_id": "api_secrets_fix_0",
              "content": "# Secrets API Reference ## Overview The Secrets API provides programmatic access to secrets functionality. Base URL: `https://api.cloudflow.io/v1/secrets` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /secrets List secret names (values not returned). **Parameters:** **Response:** - Success: `200` - Returns array of secret metadata - Error: `400` - Bad Request **Example:** ```json {\"secrets\": [{\"name\": \"DATABASE_URL\", \"created_at\": \"2024-01-01\", \"updated_at\": \"2024-01-10\"}]} ``` ### PUT /secrets/{name} Create or update a secret. **Parameters:** - `value` (string): Secret value (encrypted at rest) **Response:** - Success: `200` - Secret saved - Error: `400` - Bad Request **Example:** ```json {\"name\": \"DATABASE_URL\", \"updated_at\": \"2024-01-15T10:00:00Z\"} ``` ### DELETE /secrets/{name} Delete a secret. **Parameters:** **Response:** - Success: `204` - Secret deleted - Error: `400` - Bad Request **Example:** ```json {} ```",
              "score": null
            },
            {
              "doc_id": "howto_configure_single_sign_on",
              "chunk_id": "howto_configure_single_sign_on_fix_0",
              "content": "# How to Configure Single Sign-On ## Prerequisites - Admin access - Enterprise plan - Identity provider access ## Steps ### Step 1: Choose provider CloudFlow supports SAML 2.0, OpenID Connect, and OAuth 2.0 providers. ### Step 2: Get metadata Download CloudFlow's SAML metadata for your IdP configuration. ```bash cloudflow sso metadata --format xml > cloudflow-metadata.xml ``` ### Step 3: Configure IdP Add CloudFlow as a service provider in your identity provider. > **Note:** ACS URL: https://auth.cloudflow.io/saml/callback ### Step 4: Upload IdP metadata Provide your IdP's metadata to CloudFlow. ```bash cloudflow sso configure --idp-metadata ./idp-metadata.xml ``` ### Step 5: Test login Verify SSO login works correctly. ```bash cloudflow sso test ``` ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            }
          ],
          "latency_ms": 3.9962619994184934,
          "query_id": "api_041",
          "dimension": "original",
          "query": "How do I get current user profile?"
        },
        {
          "key_facts": [
            "GET /users/me",
            "email",
            "role"
          ],
          "found_facts": [
            "GET /users/me",
            "email",
            "role"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "api_users"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "api_audit",
              "chunk_id": "api_audit_fix_0",
              "content": "# Audit API Reference ## Overview The Audit API provides programmatic access to audit functionality. Base URL: `https://api.cloudflow.io/v1/audit` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /audit/logs Retrieve audit logs for compliance. **Parameters:** - `start_date` (date): Start date for log range - `end_date` (date): End date for log range - `actor` (string): Filter by user ID **Response:** - Success: `200` - Returns array of audit log entries - Error: `400` - Bad Request **Example:** ```json {\"logs\": [{\"timestamp\": \"2024-01-15T10:00:00Z\", \"actor\": \"usr_001\", \"action\": \"workflow.created\", \"resource\": \"wf_123\"}]} ```",
              "score": null
            },
            {
              "doc_id": "api_users",
              "chunk_id": "api_users_fix_0",
              "content": "# Users API Reference ## Overview The Users API provides programmatic access to users functionality. Base URL: `https://api.cloudflow.io/v1/users` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /users/me Get current user profile. **Parameters:** **Response:** - Success: `200` - Returns user object - Error: `400` - Bad Request **Example:** ```json {\"id\": \"usr_001\", \"email\": \"user@example.com\", \"role\": \"admin\", \"created_at\": \"2023-06-01\"} ``` ### PATCH /users/me Update current user profile. **Parameters:** - `name` (string): Display name - `timezone` (string): Timezone (e.g., America/New_York) **Response:** - Success: `200` - User updated - Error: `400` - Bad Request **Example:** ```json {\"id\": \"usr_001\", \"name\": \"John Doe\", \"timezone\": \"America/New_York\"} ```",
              "score": null
            },
            {
              "doc_id": "api_workflows",
              "chunk_id": "api_workflows_fix_0",
              "content": "# Workflows API Reference ## Overview The Workflows API provides programmatic access to workflows functionality. Base URL: `https://api.cloudflow.io/v1/workflows` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /workflows List all workflows for the authenticated user. **Parameters:** - `limit` (integer): Maximum number of results (default: 50, max: 200) - `status` (string): Filter by status: active, paused, archived **Response:** - Success: `200` - Returns array of workflow objects - Error: `400` - Bad Request **Example:** ```json {\"workflows\": [{\"id\": \"wf_123\", \"name\": \"Daily Report\", \"status\": \"active\"}]} ``` ### POST /workflows Create a new workflow. **Parameters:** - `name` (string): Workflow name (required) - `definition` (object): Workflow definition in JSON format **Response:** - Success: `201` - Workflow created successfully - Error: `400` - Bad Request **Example:** ```json {\"id\": \"wf_456\", \"name\": \"New Workflow\", \"created_at\": \"2024-01-15T10:00:00Z\"} ``` ### DELETE /workflows/{id} Delete a workflow permanently. **Parameters:** - `id` (string): Workflow ID (required) **Response:** - Success: `204` - Workflow deleted - Error: `404` - Workflow not found **Example:** ```json {} ```",
              "score": null
            },
            {
              "doc_id": "api_teams",
              "chunk_id": "api_teams_fix_0",
              "content": "# Teams API Reference ## Overview The Teams API provides programmatic access to teams functionality. Base URL: `https://api.cloudflow.io/v1/teams` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /teams List teams the user belongs to. **Parameters:** **Response:** - Success: `200` - Returns array of team objects - Error: `400` - Bad Request **Example:** ```json {\"teams\": [{\"id\": \"team_01\", \"name\": \"Engineering\", \"member_count\": 15}]} ``` ### POST /teams/{id}/members Add a member to a team. **Parameters:** - `user_id` (string): User ID to add - `role` (string): Role: owner, admin, member, viewer **Response:** - Success: `201` - Member added - Error: `400` - Bad Request **Example:** ```json {\"team_id\": \"team_01\", \"user_id\": \"usr_002\", \"role\": \"member\"} ```",
              "score": null
            },
            {
              "doc_id": "api_metrics",
              "chunk_id": "api_metrics_fix_0",
              "content": "# Metrics API Reference ## Overview The Metrics API provides programmatic access to metrics functionality. Base URL: `https://api.cloudflow.io/v1/metrics` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /metrics/workflows Get workflow performance metrics. **Parameters:** - `period` (string): Time period: hour, day, week, month - `workflow_id` (string): Filter by workflow ID **Response:** - Success: `200` - Returns metrics data - Error: `400` - Bad Request **Example:** ```json {\"success_rate\": 0.985, \"avg_duration_ms\": 1250, \"p99_duration_ms\": 5000, \"total_executions\": 10000} ```",
              "score": null
            }
          ],
          "latency_ms": 3.5036539993598126,
          "query_id": "api_041",
          "dimension": "synonym",
          "query": "I need to retrieve the authenticated user's info from the API"
        },
        {
          "key_facts": [
            "GET /users/me",
            "email",
            "role"
          ],
          "found_facts": [
            "GET /users/me",
            "email",
            "role"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "api_users"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "api_users",
              "chunk_id": "api_users_fix_0",
              "content": "# Users API Reference ## Overview The Users API provides programmatic access to users functionality. Base URL: `https://api.cloudflow.io/v1/users` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /users/me Get current user profile. **Parameters:** **Response:** - Success: `200` - Returns user object - Error: `400` - Bad Request **Example:** ```json {\"id\": \"usr_001\", \"email\": \"user@example.com\", \"role\": \"admin\", \"created_at\": \"2023-06-01\"} ``` ### PATCH /users/me Update current user profile. **Parameters:** - `name` (string): Display name - `timezone` (string): Timezone (e.g., America/New_York) **Response:** - Success: `200` - User updated - Error: `400` - Bad Request **Example:** ```json {\"id\": \"usr_001\", \"name\": \"John Doe\", \"timezone\": \"America/New_York\"} ```",
              "score": null
            },
            {
              "doc_id": "api_secrets",
              "chunk_id": "api_secrets_fix_0",
              "content": "# Secrets API Reference ## Overview The Secrets API provides programmatic access to secrets functionality. Base URL: `https://api.cloudflow.io/v1/secrets` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /secrets List secret names (values not returned). **Parameters:** **Response:** - Success: `200` - Returns array of secret metadata - Error: `400` - Bad Request **Example:** ```json {\"secrets\": [{\"name\": \"DATABASE_URL\", \"created_at\": \"2024-01-01\", \"updated_at\": \"2024-01-10\"}]} ``` ### PUT /secrets/{name} Create or update a secret. **Parameters:** - `value` (string): Secret value (encrypted at rest) **Response:** - Success: `200` - Secret saved - Error: `400` - Bad Request **Example:** ```json {\"name\": \"DATABASE_URL\", \"updated_at\": \"2024-01-15T10:00:00Z\"} ``` ### DELETE /secrets/{name} Delete a secret. **Parameters:** **Response:** - Success: `204` - Secret deleted - Error: `400` - Bad Request **Example:** ```json {} ```",
              "score": null
            },
            {
              "doc_id": "api_teams",
              "chunk_id": "api_teams_fix_0",
              "content": "# Teams API Reference ## Overview The Teams API provides programmatic access to teams functionality. Base URL: `https://api.cloudflow.io/v1/teams` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /teams List teams the user belongs to. **Parameters:** **Response:** - Success: `200` - Returns array of team objects - Error: `400` - Bad Request **Example:** ```json {\"teams\": [{\"id\": \"team_01\", \"name\": \"Engineering\", \"member_count\": 15}]} ``` ### POST /teams/{id}/members Add a member to a team. **Parameters:** - `user_id` (string): User ID to add - `role` (string): Role: owner, admin, member, viewer **Response:** - Success: `201` - Member added - Error: `400` - Bad Request **Example:** ```json {\"team_id\": \"team_01\", \"user_id\": \"usr_002\", \"role\": \"member\"} ```",
              "score": null
            },
            {
              "doc_id": "api_webhooks",
              "chunk_id": "api_webhooks_fix_0",
              "content": "# Webhooks API Reference ## Overview The Webhooks API provides programmatic access to webhooks functionality. Base URL: `https://api.cloudflow.io/v1/webhooks` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /webhooks List configured webhooks. **Parameters:** - `active` (boolean): Filter by active status **Response:** - Success: `200` - Returns array of webhook objects - Error: `400` - Bad Request **Example:** ```json {\"webhooks\": [{\"id\": \"wh_01\", \"url\": \"https://example.com/hook\", \"events\": [\"workflow.completed\"]}]} ``` ### POST /webhooks Create a new webhook endpoint. **Parameters:** - `url` (string): Webhook URL (must be HTTPS) - `events` (array): Events to subscribe to - `secret` (string): Signing secret for verification **Response:** - Success: `201` - Webhook created - Error: `400` - Bad Request **Example:** ```json {\"id\": \"wh_02\", \"url\": \"https://example.com/new-hook\", \"secret\": \"whsec_...\"} ```",
              "score": null
            },
            {
              "doc_id": "arch_authentication_service",
              "chunk_id": "arch_authentication_service_fix_0",
              "content": "# Authentication Service Architecture ## Overview The Authentication Service handles user identity, session management, and token issuance. Supports OAuth 2.0, SAML, and API key authentication. ## Design Principles - **Zero Knowledge**: Passwords never stored in plaintext - **Token Rotation**: Refresh tokens rotate on each use ## Components ### Identity Provider Manages user accounts and credentials. Password hashing uses bcrypt with cost factor 12. **Technology:** Custom Go service **Scaling:** Horizontal stateless ### Token Service Issues and validates access tokens. Token lifetime is 1 hour with 7-day refresh tokens. **Technology:** JWT with RS256 **Scaling:** Horizontal stateless ### SSO Gateway Integrates with external identity providers. Supports Google, Microsoft, Okta, and custom SAML. **Technology:** Dex OIDC proxy **Scaling:** Active-passive ## Data Flow Credentials \u2192 Validation \u2192 Token Issue \u2192 Token Refresh \u2192 Logout ## Performance Characteristics - **Latency P50:** 50ms - **Latency P99:** 200ms - **Throughput:** 5000 auth/second",
              "score": null
            }
          ],
          "latency_ms": 3.6975559996790253,
          "query_id": "api_041",
          "dimension": "problem",
          "query": "getting 401 when calling users endpoint, how do I fetch my own profile"
        },
        {
          "key_facts": [
            "GET /users/me",
            "email",
            "role"
          ],
          "found_facts": [
            "GET /users/me",
            "email",
            "role"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "api_users"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "api_users",
              "chunk_id": "api_users_fix_0",
              "content": "# Users API Reference ## Overview The Users API provides programmatic access to users functionality. Base URL: `https://api.cloudflow.io/v1/users` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /users/me Get current user profile. **Parameters:** **Response:** - Success: `200` - Returns user object - Error: `400` - Bad Request **Example:** ```json {\"id\": \"usr_001\", \"email\": \"user@example.com\", \"role\": \"admin\", \"created_at\": \"2023-06-01\"} ``` ### PATCH /users/me Update current user profile. **Parameters:** - `name` (string): Display name - `timezone` (string): Timezone (e.g., America/New_York) **Response:** - Success: `200` - User updated - Error: `400` - Bad Request **Example:** ```json {\"id\": \"usr_001\", \"name\": \"John Doe\", \"timezone\": \"America/New_York\"} ```",
              "score": null
            },
            {
              "doc_id": "arch_authentication_service",
              "chunk_id": "arch_authentication_service_fix_0",
              "content": "# Authentication Service Architecture ## Overview The Authentication Service handles user identity, session management, and token issuance. Supports OAuth 2.0, SAML, and API key authentication. ## Design Principles - **Zero Knowledge**: Passwords never stored in plaintext - **Token Rotation**: Refresh tokens rotate on each use ## Components ### Identity Provider Manages user accounts and credentials. Password hashing uses bcrypt with cost factor 12. **Technology:** Custom Go service **Scaling:** Horizontal stateless ### Token Service Issues and validates access tokens. Token lifetime is 1 hour with 7-day refresh tokens. **Technology:** JWT with RS256 **Scaling:** Horizontal stateless ### SSO Gateway Integrates with external identity providers. Supports Google, Microsoft, Okta, and custom SAML. **Technology:** Dex OIDC proxy **Scaling:** Active-passive ## Data Flow Credentials \u2192 Validation \u2192 Token Issue \u2192 Token Refresh \u2192 Logout ## Performance Characteristics - **Latency P50:** 50ms - **Latency P99:** 200ms - **Throughput:** 5000 auth/second",
              "score": null
            },
            {
              "doc_id": "arch_monitoring_stack",
              "chunk_id": "arch_monitoring_stack_fix_0",
              "content": "# Monitoring Stack Architecture ## Overview The Monitoring Stack provides observability into system health, performance, and errors. Includes metrics, logs, traces, and alerting. ## Design Principles - **Observability**: Every component emits metrics and traces - **Actionable Alerts**: Alerts include runbook links ## Components ### Metrics Collects time-series metrics. Retention is 15 days local, 1 year in object storage. **Technology:** Prometheus with Thanos **Scaling:** Federated with long-term storage ### Logging Centralized log aggregation. Supports structured JSON logs with label indexing. **Technology:** Loki with Grafana **Scaling:** Distributed with sharding ### Tracing Distributed request tracing. Trace retention is 7 days. **Technology:** Jaeger with OpenTelemetry **Scaling:** Sampling at 10% ### Alerting Routes alerts to on-call engineers. Supports escalation policies and silencing. **Technology:** Alertmanager with PagerDuty **Scaling:** HA pair ## Data Flow Application \u2192 Collector \u2192 Storage \u2192 Query \u2192 Dashboard/Alert ## Performance Characteristics - **Latency P50:** N/A - **Latency P99:** N/A - **Throughput:** 1M metrics/second, 100GB logs/day",
              "score": null
            },
            {
              "doc_id": "arch_notification_service",
              "chunk_id": "arch_notification_service_fix_0",
              "content": "# Notification Service Architecture ## Overview The Notification Service delivers alerts and updates through multiple channels including email, Slack, webhooks, and mobile push notifications. ## Design Principles - **Delivery Guarantee**: At-least-once delivery with deduplication - **Channel Abstraction**: Unified API regardless of delivery channel ## Components ### Dispatcher Routes notifications to appropriate channel handlers. Supports priority queues for urgent notifications. **Technology:** Go microservice **Scaling:** Horizontal with queue partitioning ### Email Provider Sends transactional emails. Maximum 10000 emails per hour per account. **Technology:** SendGrid integration **Scaling:** API rate limited ### Slack Integration Posts messages to Slack channels. Supports message formatting and interactive components. **Technology:** Slack Bolt SDK **Scaling:** Per-workspace rate limits ## Data Flow Event \u2192 Template \u2192 Dispatcher \u2192 Channel \u2192 Delivery Confirmation ## Performance Characteristics - **Latency P50:** 200ms - **Latency P99:** 2000ms - **Throughput:** 10000 notifications/minute",
              "score": null
            },
            {
              "doc_id": "adr_003",
              "chunk_id": "adr_003_fix_0",
              "content": "# ADR-003: Implement OAuth 2.0 with JWT for Authentication **Status:** Accepted **Date:** 2024-05-08 ## Context We need a secure authentication system that supports API keys, user sessions, and third-party integrations. ## Options Considered ### Option 1: OAuth 2.0 with JWT Industry standard token-based auth. **Pros:** - Industry standard - Stateless validation - Supports refresh tokens - SSO compatible **Cons:** - Token size overhead - Cannot revoke individual tokens - Complexity ### Option 2: Session-based auth Traditional server-side sessions. **Pros:** - Simple implementation - Easy revocation - Smaller payloads **Cons:** - Requires session store - Not suitable for APIs - Scaling challenges ### Option 3: API keys only Simple key-based authentication. **Pros:** - Very simple - Low overhead - Easy to implement **Cons:** - No user context - Hard to rotate - No SSO support ## Decision We will implement OAuth 2.0 with JWT tokens. Access tokens expire after 1 hour, refresh tokens after 7 days. ## Consequences - Implement token rotation on refresh - Support API keys for machine-to-machine auth - Add MFA for admin accounts",
              "score": null
            }
          ],
          "latency_ms": 3.7189760005276185,
          "query_id": "api_041",
          "dimension": "casual",
          "query": "users me endpoint"
        },
        {
          "key_facts": [
            "GET /users/me",
            "email",
            "role"
          ],
          "found_facts": [
            "GET /users/me",
            "email",
            "role"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "api_users"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "api_users",
              "chunk_id": "api_users_fix_0",
              "content": "# Users API Reference ## Overview The Users API provides programmatic access to users functionality. Base URL: `https://api.cloudflow.io/v1/users` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /users/me Get current user profile. **Parameters:** **Response:** - Success: `200` - Returns user object - Error: `400` - Bad Request **Example:** ```json {\"id\": \"usr_001\", \"email\": \"user@example.com\", \"role\": \"admin\", \"created_at\": \"2023-06-01\"} ``` ### PATCH /users/me Update current user profile. **Parameters:** - `name` (string): Display name - `timezone` (string): Timezone (e.g., America/New_York) **Response:** - Success: `200` - User updated - Error: `400` - Bad Request **Example:** ```json {\"id\": \"usr_001\", \"name\": \"John Doe\", \"timezone\": \"America/New_York\"} ```",
              "score": null
            },
            {
              "doc_id": "api_audit",
              "chunk_id": "api_audit_fix_0",
              "content": "# Audit API Reference ## Overview The Audit API provides programmatic access to audit functionality. Base URL: `https://api.cloudflow.io/v1/audit` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /audit/logs Retrieve audit logs for compliance. **Parameters:** - `start_date` (date): Start date for log range - `end_date` (date): End date for log range - `actor` (string): Filter by user ID **Response:** - Success: `200` - Returns array of audit log entries - Error: `400` - Bad Request **Example:** ```json {\"logs\": [{\"timestamp\": \"2024-01-15T10:00:00Z\", \"actor\": \"usr_001\", \"action\": \"workflow.created\", \"resource\": \"wf_123\"}]} ```",
              "score": null
            },
            {
              "doc_id": "api_teams",
              "chunk_id": "api_teams_fix_0",
              "content": "# Teams API Reference ## Overview The Teams API provides programmatic access to teams functionality. Base URL: `https://api.cloudflow.io/v1/teams` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /teams List teams the user belongs to. **Parameters:** **Response:** - Success: `200` - Returns array of team objects - Error: `400` - Bad Request **Example:** ```json {\"teams\": [{\"id\": \"team_01\", \"name\": \"Engineering\", \"member_count\": 15}]} ``` ### POST /teams/{id}/members Add a member to a team. **Parameters:** - `user_id` (string): User ID to add - `role` (string): Role: owner, admin, member, viewer **Response:** - Success: `201` - Member added - Error: `400` - Bad Request **Example:** ```json {\"team_id\": \"team_01\", \"user_id\": \"usr_002\", \"role\": \"member\"} ```",
              "score": null
            },
            {
              "doc_id": "api_webhooks",
              "chunk_id": "api_webhooks_fix_0",
              "content": "# Webhooks API Reference ## Overview The Webhooks API provides programmatic access to webhooks functionality. Base URL: `https://api.cloudflow.io/v1/webhooks` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /webhooks List configured webhooks. **Parameters:** - `active` (boolean): Filter by active status **Response:** - Success: `200` - Returns array of webhook objects - Error: `400` - Bad Request **Example:** ```json {\"webhooks\": [{\"id\": \"wh_01\", \"url\": \"https://example.com/hook\", \"events\": [\"workflow.completed\"]}]} ``` ### POST /webhooks Create a new webhook endpoint. **Parameters:** - `url` (string): Webhook URL (must be HTTPS) - `events` (array): Events to subscribe to - `secret` (string): Signing secret for verification **Response:** - Success: `201` - Webhook created - Error: `400` - Bad Request **Example:** ```json {\"id\": \"wh_02\", \"url\": \"https://example.com/new-hook\", \"secret\": \"whsec_...\"} ```",
              "score": null
            },
            {
              "doc_id": "api_workflows",
              "chunk_id": "api_workflows_fix_0",
              "content": "# Workflows API Reference ## Overview The Workflows API provides programmatic access to workflows functionality. Base URL: `https://api.cloudflow.io/v1/workflows` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /workflows List all workflows for the authenticated user. **Parameters:** - `limit` (integer): Maximum number of results (default: 50, max: 200) - `status` (string): Filter by status: active, paused, archived **Response:** - Success: `200` - Returns array of workflow objects - Error: `400` - Bad Request **Example:** ```json {\"workflows\": [{\"id\": \"wf_123\", \"name\": \"Daily Report\", \"status\": \"active\"}]} ``` ### POST /workflows Create a new workflow. **Parameters:** - `name` (string): Workflow name (required) - `definition` (object): Workflow definition in JSON format **Response:** - Success: `201` - Workflow created successfully - Error: `400` - Bad Request **Example:** ```json {\"id\": \"wf_456\", \"name\": \"New Workflow\", \"created_at\": \"2024-01-15T10:00:00Z\"} ``` ### DELETE /workflows/{id} Delete a workflow permanently. **Parameters:** - `id` (string): Workflow ID (required) **Response:** - Success: `204` - Workflow deleted - Error: `404` - Workflow not found **Example:** ```json {} ```",
              "score": null
            }
          ],
          "latency_ms": 3.6485140008153394,
          "query_id": "api_041",
          "dimension": "contextual",
          "query": "building a profile page, need to pull the logged-in user's data from the API"
        },
        {
          "key_facts": [
            "GET /users/me",
            "email",
            "role"
          ],
          "found_facts": [
            "GET /users/me",
            "email",
            "role"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "api_users"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "api_users",
              "chunk_id": "api_users_fix_0",
              "content": "# Users API Reference ## Overview The Users API provides programmatic access to users functionality. Base URL: `https://api.cloudflow.io/v1/users` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /users/me Get current user profile. **Parameters:** **Response:** - Success: `200` - Returns user object - Error: `400` - Bad Request **Example:** ```json {\"id\": \"usr_001\", \"email\": \"user@example.com\", \"role\": \"admin\", \"created_at\": \"2023-06-01\"} ``` ### PATCH /users/me Update current user profile. **Parameters:** - `name` (string): Display name - `timezone` (string): Timezone (e.g., America/New_York) **Response:** - Success: `200` - User updated - Error: `400` - Bad Request **Example:** ```json {\"id\": \"usr_001\", \"name\": \"John Doe\", \"timezone\": \"America/New_York\"} ```",
              "score": null
            },
            {
              "doc_id": "api_teams",
              "chunk_id": "api_teams_fix_0",
              "content": "# Teams API Reference ## Overview The Teams API provides programmatic access to teams functionality. Base URL: `https://api.cloudflow.io/v1/teams` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /teams List teams the user belongs to. **Parameters:** **Response:** - Success: `200` - Returns array of team objects - Error: `400` - Bad Request **Example:** ```json {\"teams\": [{\"id\": \"team_01\", \"name\": \"Engineering\", \"member_count\": 15}]} ``` ### POST /teams/{id}/members Add a member to a team. **Parameters:** - `user_id` (string): User ID to add - `role` (string): Role: owner, admin, member, viewer **Response:** - Success: `201` - Member added - Error: `400` - Bad Request **Example:** ```json {\"team_id\": \"team_01\", \"user_id\": \"usr_002\", \"role\": \"member\"} ```",
              "score": null
            },
            {
              "doc_id": "api_audit",
              "chunk_id": "api_audit_fix_0",
              "content": "# Audit API Reference ## Overview The Audit API provides programmatic access to audit functionality. Base URL: `https://api.cloudflow.io/v1/audit` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /audit/logs Retrieve audit logs for compliance. **Parameters:** - `start_date` (date): Start date for log range - `end_date` (date): End date for log range - `actor` (string): Filter by user ID **Response:** - Success: `200` - Returns array of audit log entries - Error: `400` - Bad Request **Example:** ```json {\"logs\": [{\"timestamp\": \"2024-01-15T10:00:00Z\", \"actor\": \"usr_001\", \"action\": \"workflow.created\", \"resource\": \"wf_123\"}]} ```",
              "score": null
            },
            {
              "doc_id": "api_webhooks",
              "chunk_id": "api_webhooks_fix_0",
              "content": "# Webhooks API Reference ## Overview The Webhooks API provides programmatic access to webhooks functionality. Base URL: `https://api.cloudflow.io/v1/webhooks` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /webhooks List configured webhooks. **Parameters:** - `active` (boolean): Filter by active status **Response:** - Success: `200` - Returns array of webhook objects - Error: `400` - Bad Request **Example:** ```json {\"webhooks\": [{\"id\": \"wh_01\", \"url\": \"https://example.com/hook\", \"events\": [\"workflow.completed\"]}]} ``` ### POST /webhooks Create a new webhook endpoint. **Parameters:** - `url` (string): Webhook URL (must be HTTPS) - `events` (array): Events to subscribe to - `secret` (string): Signing secret for verification **Response:** - Success: `201` - Webhook created - Error: `400` - Bad Request **Example:** ```json {\"id\": \"wh_02\", \"url\": \"https://example.com/new-hook\", \"secret\": \"whsec_...\"} ```",
              "score": null
            },
            {
              "doc_id": "api_workflows",
              "chunk_id": "api_workflows_fix_0",
              "content": "# Workflows API Reference ## Overview The Workflows API provides programmatic access to workflows functionality. Base URL: `https://api.cloudflow.io/v1/workflows` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /workflows List all workflows for the authenticated user. **Parameters:** - `limit` (integer): Maximum number of results (default: 50, max: 200) - `status` (string): Filter by status: active, paused, archived **Response:** - Success: `200` - Returns array of workflow objects - Error: `400` - Bad Request **Example:** ```json {\"workflows\": [{\"id\": \"wf_123\", \"name\": \"Daily Report\", \"status\": \"active\"}]} ``` ### POST /workflows Create a new workflow. **Parameters:** - `name` (string): Workflow name (required) - `definition` (object): Workflow definition in JSON format **Response:** - Success: `201` - Workflow created successfully - Error: `400` - Bad Request **Example:** ```json {\"id\": \"wf_456\", \"name\": \"New Workflow\", \"created_at\": \"2024-01-15T10:00:00Z\"} ``` ### DELETE /workflows/{id} Delete a workflow permanently. **Parameters:** - `id` (string): Workflow ID (required) **Response:** - Success: `204` - Workflow deleted - Error: `404` - Workflow not found **Example:** ```json {} ```",
              "score": null
            }
          ],
          "latency_ms": 3.694330000143964,
          "query_id": "api_041",
          "dimension": "negation",
          "query": "why doesn't the API return my user details"
        },
        {
          "key_facts": [
            "POST /teams/{id}/members",
            "user_id",
            "role"
          ],
          "found_facts": [
            "POST /teams/{id}/members",
            "user_id",
            "role"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "api_teams"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "api_teams",
              "chunk_id": "api_teams_fix_0",
              "content": "# Teams API Reference ## Overview The Teams API provides programmatic access to teams functionality. Base URL: `https://api.cloudflow.io/v1/teams` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /teams List teams the user belongs to. **Parameters:** **Response:** - Success: `200` - Returns array of team objects - Error: `400` - Bad Request **Example:** ```json {\"teams\": [{\"id\": \"team_01\", \"name\": \"Engineering\", \"member_count\": 15}]} ``` ### POST /teams/{id}/members Add a member to a team. **Parameters:** - `user_id` (string): User ID to add - `role` (string): Role: owner, admin, member, viewer **Response:** - Success: `201` - Member added - Error: `400` - Bad Request **Example:** ```json {\"team_id\": \"team_01\", \"user_id\": \"usr_002\", \"role\": \"member\"} ```",
              "score": null
            },
            {
              "doc_id": "api_users",
              "chunk_id": "api_users_fix_0",
              "content": "# Users API Reference ## Overview The Users API provides programmatic access to users functionality. Base URL: `https://api.cloudflow.io/v1/users` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /users/me Get current user profile. **Parameters:** **Response:** - Success: `200` - Returns user object - Error: `400` - Bad Request **Example:** ```json {\"id\": \"usr_001\", \"email\": \"user@example.com\", \"role\": \"admin\", \"created_at\": \"2023-06-01\"} ``` ### PATCH /users/me Update current user profile. **Parameters:** - `name` (string): Display name - `timezone` (string): Timezone (e.g., America/New_York) **Response:** - Success: `200` - User updated - Error: `400` - Bad Request **Example:** ```json {\"id\": \"usr_001\", \"name\": \"John Doe\", \"timezone\": \"America/New_York\"} ```",
              "score": null
            },
            {
              "doc_id": "howto_configure_single_sign_on",
              "chunk_id": "howto_configure_single_sign_on_fix_0",
              "content": "# How to Configure Single Sign-On ## Prerequisites - Admin access - Enterprise plan - Identity provider access ## Steps ### Step 1: Choose provider CloudFlow supports SAML 2.0, OpenID Connect, and OAuth 2.0 providers. ### Step 2: Get metadata Download CloudFlow's SAML metadata for your IdP configuration. ```bash cloudflow sso metadata --format xml > cloudflow-metadata.xml ``` ### Step 3: Configure IdP Add CloudFlow as a service provider in your identity provider. > **Note:** ACS URL: https://auth.cloudflow.io/saml/callback ### Step 4: Upload IdP metadata Provide your IdP's metadata to CloudFlow. ```bash cloudflow sso configure --idp-metadata ./idp-metadata.xml ``` ### Step 5: Test login Verify SSO login works correctly. ```bash cloudflow sso test ``` ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            },
            {
              "doc_id": "howto_set_up_ci_cd_pipeline",
              "chunk_id": "howto_set_up_ci_cd_pipeline_fix_0",
              "content": "# How to Set Up CI/CD Pipeline ## Prerequisites - GitHub account - CloudFlow API key - Repository admin access ## Steps ### Step 1: Create GitHub Actions workflow Create a new file at `.github/workflows/cloudflow.yml` with the deployment configuration. ```bash mkdir -p .github/workflows && touch .github/workflows/cloudflow.yml ``` ### Step 2: Configure secrets Add your CloudFlow API key to GitHub secrets. ```bash gh secret set CLOUDFLOW_API_KEY ``` ### Step 3: Add deployment step Add the CloudFlow deployment action to your workflow. > **Note:** Use the official cloudflow/deploy-action@v2 ### Step 4: Test the pipeline Push a commit to trigger the workflow. ```bash git push origin main ``` ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            },
            {
              "doc_id": "runbook_api_gateway_5xx_errors",
              "chunk_id": "runbook_api_gateway_5xx_errors_fix_0",
              "content": "# Runbook: API Gateway 5xx Errors ## Severity **Level:** P3 - Medium **Response Time:** 1 hour ## Symptoms - Error rate > 1% - 5xx responses in access logs - Downstream service failures ## Diagnosis ### Step 1 Check Kong error logs: `kubectl logs -l app=kong -n kong --tail=100 | grep -E '5[0-9]{2}'` ### Step 2 Verify backend health: `kubectl get pods -n production -o wide` ### Step 3 Check service endpoints: `kubectl get endpoints -n production` ### Step 4 Test backend directly: `kubectl exec -it kong-xxx -n kong -- curl http://api.production.svc:8080/health` ## Resolution ### Step 1 Restart unhealthy backends: `kubectl rollout restart deployment/api -n production` ### Step 2 Enable circuit breaker if cascading: update Kong circuit-breaker plugin ### Step 3 Scale up backends: `kubectl scale deployment/api --replicas=5 -n production` ### Step 4 Check for resource exhaustion on backend pods ### Step 5 Verify network policies allow traffic ## Escalation If 5xx persists > 15 minutes, page on-call. May indicate infrastructure issue. ## Post-Incident - Document timeline in incident log - Schedule post-mortem within 48 hours - Update runbook if new patterns discovered",
              "score": null
            }
          ],
          "latency_ms": 3.4173739986727014,
          "query_id": "api_042",
          "dimension": "original",
          "query": "How do I add a team member?"
        },
        {
          "key_facts": [
            "POST /teams/{id}/members",
            "user_id",
            "role"
          ],
          "found_facts": [
            "POST /teams/{id}/members",
            "user_id",
            "role"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "api_teams"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "api_teams",
              "chunk_id": "api_teams_fix_0",
              "content": "# Teams API Reference ## Overview The Teams API provides programmatic access to teams functionality. Base URL: `https://api.cloudflow.io/v1/teams` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /teams List teams the user belongs to. **Parameters:** **Response:** - Success: `200` - Returns array of team objects - Error: `400` - Bad Request **Example:** ```json {\"teams\": [{\"id\": \"team_01\", \"name\": \"Engineering\", \"member_count\": 15}]} ``` ### POST /teams/{id}/members Add a member to a team. **Parameters:** - `user_id` (string): User ID to add - `role` (string): Role: owner, admin, member, viewer **Response:** - Success: `201` - Member added - Error: `400` - Bad Request **Example:** ```json {\"team_id\": \"team_01\", \"user_id\": \"usr_002\", \"role\": \"member\"} ```",
              "score": null
            },
            {
              "doc_id": "api_users",
              "chunk_id": "api_users_fix_0",
              "content": "# Users API Reference ## Overview The Users API provides programmatic access to users functionality. Base URL: `https://api.cloudflow.io/v1/users` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /users/me Get current user profile. **Parameters:** **Response:** - Success: `200` - Returns user object - Error: `400` - Bad Request **Example:** ```json {\"id\": \"usr_001\", \"email\": \"user@example.com\", \"role\": \"admin\", \"created_at\": \"2023-06-01\"} ``` ### PATCH /users/me Update current user profile. **Parameters:** - `name` (string): Display name - `timezone` (string): Timezone (e.g., America/New_York) **Response:** - Success: `200` - User updated - Error: `400` - Bad Request **Example:** ```json {\"id\": \"usr_001\", \"name\": \"John Doe\", \"timezone\": \"America/New_York\"} ```",
              "score": null
            },
            {
              "doc_id": "api_webhooks",
              "chunk_id": "api_webhooks_fix_0",
              "content": "# Webhooks API Reference ## Overview The Webhooks API provides programmatic access to webhooks functionality. Base URL: `https://api.cloudflow.io/v1/webhooks` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /webhooks List configured webhooks. **Parameters:** - `active` (boolean): Filter by active status **Response:** - Success: `200` - Returns array of webhook objects - Error: `400` - Bad Request **Example:** ```json {\"webhooks\": [{\"id\": \"wh_01\", \"url\": \"https://example.com/hook\", \"events\": [\"workflow.completed\"]}]} ``` ### POST /webhooks Create a new webhook endpoint. **Parameters:** - `url` (string): Webhook URL (must be HTTPS) - `events` (array): Events to subscribe to - `secret` (string): Signing secret for verification **Response:** - Success: `201` - Webhook created - Error: `400` - Bad Request **Example:** ```json {\"id\": \"wh_02\", \"url\": \"https://example.com/new-hook\", \"secret\": \"whsec_...\"} ```",
              "score": null
            },
            {
              "doc_id": "api_audit",
              "chunk_id": "api_audit_fix_0",
              "content": "# Audit API Reference ## Overview The Audit API provides programmatic access to audit functionality. Base URL: `https://api.cloudflow.io/v1/audit` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /audit/logs Retrieve audit logs for compliance. **Parameters:** - `start_date` (date): Start date for log range - `end_date` (date): End date for log range - `actor` (string): Filter by user ID **Response:** - Success: `200` - Returns array of audit log entries - Error: `400` - Bad Request **Example:** ```json {\"logs\": [{\"timestamp\": \"2024-01-15T10:00:00Z\", \"actor\": \"usr_001\", \"action\": \"workflow.created\", \"resource\": \"wf_123\"}]} ```",
              "score": null
            },
            {
              "doc_id": "howto_create_api_integration",
              "chunk_id": "howto_create_api_integration_fix_0",
              "content": "# How to Create API Integration ## Prerequisites - CloudFlow account ## Steps ### Step 1: Generate API key Create an API key for programmatic access. ```bash cloudflow apikeys create --name 'CI Integration' --scopes workflows:read,workflows:execute ``` ### Step 2: Store key securely Never commit API keys to source control. Use environment variables or secret management. > **Note:** Rotate keys every 90 days ### Step 3: Test authentication Verify the API key works correctly. ```bash curl -H 'Authorization: Bearer cf_...' https://api.cloudflow.io/v1/me ``` ### Step 4: Implement rate limiting Handle rate limit responses (HTTP 429) with exponential backoff. > **Note:** Default limit is 100 requests per minute ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            }
          ],
          "latency_ms": 3.3703459994285367,
          "query_id": "api_042",
          "dimension": "synonym",
          "query": "I need to invite a user to a team via the API"
        },
        {
          "key_facts": [
            "POST /teams/{id}/members",
            "user_id",
            "role"
          ],
          "found_facts": [
            "POST /teams/{id}/members",
            "user_id",
            "role"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "api_teams"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "api_teams",
              "chunk_id": "api_teams_fix_0",
              "content": "# Teams API Reference ## Overview The Teams API provides programmatic access to teams functionality. Base URL: `https://api.cloudflow.io/v1/teams` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /teams List teams the user belongs to. **Parameters:** **Response:** - Success: `200` - Returns array of team objects - Error: `400` - Bad Request **Example:** ```json {\"teams\": [{\"id\": \"team_01\", \"name\": \"Engineering\", \"member_count\": 15}]} ``` ### POST /teams/{id}/members Add a member to a team. **Parameters:** - `user_id` (string): User ID to add - `role` (string): Role: owner, admin, member, viewer **Response:** - Success: `201` - Member added - Error: `400` - Bad Request **Example:** ```json {\"team_id\": \"team_01\", \"user_id\": \"usr_002\", \"role\": \"member\"} ```",
              "score": null
            },
            {
              "doc_id": "api_webhooks",
              "chunk_id": "api_webhooks_fix_0",
              "content": "# Webhooks API Reference ## Overview The Webhooks API provides programmatic access to webhooks functionality. Base URL: `https://api.cloudflow.io/v1/webhooks` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /webhooks List configured webhooks. **Parameters:** - `active` (boolean): Filter by active status **Response:** - Success: `200` - Returns array of webhook objects - Error: `400` - Bad Request **Example:** ```json {\"webhooks\": [{\"id\": \"wh_01\", \"url\": \"https://example.com/hook\", \"events\": [\"workflow.completed\"]}]} ``` ### POST /webhooks Create a new webhook endpoint. **Parameters:** - `url` (string): Webhook URL (must be HTTPS) - `events` (array): Events to subscribe to - `secret` (string): Signing secret for verification **Response:** - Success: `201` - Webhook created - Error: `400` - Bad Request **Example:** ```json {\"id\": \"wh_02\", \"url\": \"https://example.com/new-hook\", \"secret\": \"whsec_...\"} ```",
              "score": null
            },
            {
              "doc_id": "api_metrics",
              "chunk_id": "api_metrics_fix_0",
              "content": "# Metrics API Reference ## Overview The Metrics API provides programmatic access to metrics functionality. Base URL: `https://api.cloudflow.io/v1/metrics` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /metrics/workflows Get workflow performance metrics. **Parameters:** - `period` (string): Time period: hour, day, week, month - `workflow_id` (string): Filter by workflow ID **Response:** - Success: `200` - Returns metrics data - Error: `400` - Bad Request **Example:** ```json {\"success_rate\": 0.985, \"avg_duration_ms\": 1250, \"p99_duration_ms\": 5000, \"total_executions\": 10000} ```",
              "score": null
            },
            {
              "doc_id": "config_api_gateway",
              "chunk_id": "config_api_gateway_fix_0",
              "content": "# API Gateway Configuration Reference ## Overview This document describes all configuration options for API Gateway. ## Environment Variables | Variable | Type | Default | Description | |----------|------|---------|-------------| | `GATEWAY_PORT` | integer | 8000 | Gateway listen port | | `GATEWAY_RATE_LIMIT` | integer | 100 | Requests per minute per user | | `GATEWAY_TIMEOUT_MS` | integer | 30000 | Request timeout in milliseconds | | `GATEWAY_MAX_BODY_SIZE` | string | 10MB | Maximum request body size | | `GATEWAY_CORS_ORIGINS` | string | * | Allowed CORS origins | | `GATEWAY_SSL_ENABLED` | boolean | true | Enable TLS termination | ## Configuration File Configuration can also be provided via `api gateway.yaml`: ```yaml gateway_port: 8000 gateway_rate_limit: 100 gateway_timeout_ms: 30000 gateway_max_body_size: 10MB gateway_cors_origins: * ``` ## Validation Configuration is validated at startup. Invalid configuration will prevent the service from starting.",
              "score": null
            },
            {
              "doc_id": "config_workflow_engine",
              "chunk_id": "config_workflow_engine_fix_0",
              "content": "# Workflow Engine Configuration Reference ## Overview This document describes all configuration options for Workflow Engine. ## Environment Variables | Variable | Type | Default | Description | |----------|------|---------|-------------| | `WORKFLOW_MAX_STEPS` | integer | 100 | Maximum steps per workflow | | `WORKFLOW_TIMEOUT_SECONDS` | integer | 3600 | Maximum workflow execution time | | `WORKFLOW_RETRY_LIMIT` | integer | 3 | Default retry attempts for failed steps | | `WORKFLOW_CONCURRENT_LIMIT` | integer | 10 | Maximum concurrent executions per workflow | | `EXECUTOR_POOL_SIZE` | integer | 50 | Number of executor workers | | `EXECUTOR_MEMORY_LIMIT` | string | 512Mi | Memory limit per executor | ## Configuration File Configuration can also be provided via `workflow engine.yaml`: ```yaml workflow_max_steps: 100 workflow_timeout_seconds: 3600 workflow_retry_limit: 3 workflow_concurrent_limit: 10 executor_pool_size: 50 ``` ## Validation Configuration is validated at startup. Invalid configuration will prevent the service from starting.",
              "score": null
            }
          ],
          "latency_ms": 3.65541799874336,
          "query_id": "api_042",
          "dimension": "problem",
          "query": "getting 400 when trying to add someone to my team, what parameters does the endpoint need"
        },
        {
          "key_facts": [
            "POST /teams/{id}/members",
            "user_id",
            "role"
          ],
          "found_facts": [
            "POST /teams/{id}/members",
            "user_id",
            "role"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "api_teams"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "api_teams",
              "chunk_id": "api_teams_fix_0",
              "content": "# Teams API Reference ## Overview The Teams API provides programmatic access to teams functionality. Base URL: `https://api.cloudflow.io/v1/teams` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /teams List teams the user belongs to. **Parameters:** **Response:** - Success: `200` - Returns array of team objects - Error: `400` - Bad Request **Example:** ```json {\"teams\": [{\"id\": \"team_01\", \"name\": \"Engineering\", \"member_count\": 15}]} ``` ### POST /teams/{id}/members Add a member to a team. **Parameters:** - `user_id` (string): User ID to add - `role` (string): Role: owner, admin, member, viewer **Response:** - Success: `201` - Member added - Error: `400` - Bad Request **Example:** ```json {\"team_id\": \"team_01\", \"user_id\": \"usr_002\", \"role\": \"member\"} ```",
              "score": null
            },
            {
              "doc_id": "arch_notification_service",
              "chunk_id": "arch_notification_service_fix_0",
              "content": "# Notification Service Architecture ## Overview The Notification Service delivers alerts and updates through multiple channels including email, Slack, webhooks, and mobile push notifications. ## Design Principles - **Delivery Guarantee**: At-least-once delivery with deduplication - **Channel Abstraction**: Unified API regardless of delivery channel ## Components ### Dispatcher Routes notifications to appropriate channel handlers. Supports priority queues for urgent notifications. **Technology:** Go microservice **Scaling:** Horizontal with queue partitioning ### Email Provider Sends transactional emails. Maximum 10000 emails per hour per account. **Technology:** SendGrid integration **Scaling:** API rate limited ### Slack Integration Posts messages to Slack channels. Supports message formatting and interactive components. **Technology:** Slack Bolt SDK **Scaling:** Per-workspace rate limits ## Data Flow Event \u2192 Template \u2192 Dispatcher \u2192 Channel \u2192 Delivery Confirmation ## Performance Characteristics - **Latency P50:** 200ms - **Latency P99:** 2000ms - **Throughput:** 10000 notifications/minute",
              "score": null
            },
            {
              "doc_id": "arch_workflow_engine",
              "chunk_id": "arch_workflow_engine_fix_0",
              "content": "# Workflow Engine Architecture ## Overview The Workflow Engine is the core component responsible for executing user-defined workflows. It uses an event-driven architecture with support for parallel execution, conditional branching, and automatic retries. ## Design Principles - **Idempotency**: All operations can be safely retried without side effects - **Eventual Consistency**: State converges to consistent view within 5 seconds - **Fault Tolerance**: System continues operating despite component failures ## Components ### Scheduler Manages workflow scheduling and triggers. Supports cron expressions, webhooks, and event-based triggers. **Technology:** Custom Go service **Scaling:** Horizontal with leader election ### Executor Runs individual workflow steps in isolated containers. Maximum execution time is 30 minutes per step. **Technology:** Kubernetes Jobs **Scaling:** Auto-scaling based on queue depth ### State Store Persists workflow state, execution history, and step results. Uses optimistic locking for concurrent updates. **Technology:** PostgreSQL with JSONB **Scaling:** Primary-replica with read replicas ## Data Flow Triggers \u2192 Scheduler \u2192 Job Queue \u2192 Executor \u2192 State Store \u2192 Webhooks/Notifications ## Performance Characteristics - **Latency P50:** 45ms - **Latency P99:** 250ms - **Throughput:** 5000 executions/minute",
              "score": null
            },
            {
              "doc_id": "adr_005",
              "chunk_id": "adr_005_fix_0",
              "content": "# ADR-005: Adopt Event-Driven Architecture for Workflow Execution **Status:** Accepted **Date:** 2024-12-04 ## Context We need an architecture that supports asynchronous workflow execution, step dependencies, and failure recovery. ## Options Considered ### Option 1: Event-driven with Kafka Publish-subscribe messaging. **Pros:** - Loose coupling - Scalability - Event replay - Audit trail **Cons:** - Eventual consistency - Operational complexity - Message ordering challenges ### Option 2: Request-response Synchronous API calls. **Pros:** - Simple mental model - Immediate feedback - Easy debugging **Cons:** - Tight coupling - Cascading failures - Timeout issues ### Option 3: Polling-based Workers poll for tasks. **Pros:** - Simple implementation - Easy rate limiting - No message broker **Cons:** - Polling overhead - Higher latency - Inefficient at scale ## Decision We will use an event-driven architecture with Kafka for workflow step execution. ## Consequences - Implement idempotent consumers - Use dead letter queues for failed messages - Monitor consumer lag",
              "score": null
            },
            {
              "doc_id": "api_users",
              "chunk_id": "api_users_fix_0",
              "content": "# Users API Reference ## Overview The Users API provides programmatic access to users functionality. Base URL: `https://api.cloudflow.io/v1/users` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /users/me Get current user profile. **Parameters:** **Response:** - Success: `200` - Returns user object - Error: `400` - Bad Request **Example:** ```json {\"id\": \"usr_001\", \"email\": \"user@example.com\", \"role\": \"admin\", \"created_at\": \"2023-06-01\"} ``` ### PATCH /users/me Update current user profile. **Parameters:** - `name` (string): Display name - `timezone` (string): Timezone (e.g., America/New_York) **Response:** - Success: `200` - User updated - Error: `400` - Bad Request **Example:** ```json {\"id\": \"usr_001\", \"name\": \"John Doe\", \"timezone\": \"America/New_York\"} ```",
              "score": null
            }
          ],
          "latency_ms": 4.564651000691811,
          "query_id": "api_042",
          "dimension": "casual",
          "query": "team members POST endpoint"
        },
        {
          "key_facts": [
            "POST /teams/{id}/members",
            "user_id",
            "role"
          ],
          "found_facts": [
            "POST /teams/{id}/members",
            "user_id",
            "role"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "api_teams"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "api_teams",
              "chunk_id": "api_teams_fix_0",
              "content": "# Teams API Reference ## Overview The Teams API provides programmatic access to teams functionality. Base URL: `https://api.cloudflow.io/v1/teams` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /teams List teams the user belongs to. **Parameters:** **Response:** - Success: `200` - Returns array of team objects - Error: `400` - Bad Request **Example:** ```json {\"teams\": [{\"id\": \"team_01\", \"name\": \"Engineering\", \"member_count\": 15}]} ``` ### POST /teams/{id}/members Add a member to a team. **Parameters:** - `user_id` (string): User ID to add - `role` (string): Role: owner, admin, member, viewer **Response:** - Success: `201` - Member added - Error: `400` - Bad Request **Example:** ```json {\"team_id\": \"team_01\", \"user_id\": \"usr_002\", \"role\": \"member\"} ```",
              "score": null
            },
            {
              "doc_id": "api_users",
              "chunk_id": "api_users_fix_0",
              "content": "# Users API Reference ## Overview The Users API provides programmatic access to users functionality. Base URL: `https://api.cloudflow.io/v1/users` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /users/me Get current user profile. **Parameters:** **Response:** - Success: `200` - Returns user object - Error: `400` - Bad Request **Example:** ```json {\"id\": \"usr_001\", \"email\": \"user@example.com\", \"role\": \"admin\", \"created_at\": \"2023-06-01\"} ``` ### PATCH /users/me Update current user profile. **Parameters:** - `name` (string): Display name - `timezone` (string): Timezone (e.g., America/New_York) **Response:** - Success: `200` - User updated - Error: `400` - Bad Request **Example:** ```json {\"id\": \"usr_001\", \"name\": \"John Doe\", \"timezone\": \"America/New_York\"} ```",
              "score": null
            },
            {
              "doc_id": "howto_set_up_ci_cd_pipeline",
              "chunk_id": "howto_set_up_ci_cd_pipeline_fix_0",
              "content": "# How to Set Up CI/CD Pipeline ## Prerequisites - GitHub account - CloudFlow API key - Repository admin access ## Steps ### Step 1: Create GitHub Actions workflow Create a new file at `.github/workflows/cloudflow.yml` with the deployment configuration. ```bash mkdir -p .github/workflows && touch .github/workflows/cloudflow.yml ``` ### Step 2: Configure secrets Add your CloudFlow API key to GitHub secrets. ```bash gh secret set CLOUDFLOW_API_KEY ``` ### Step 3: Add deployment step Add the CloudFlow deployment action to your workflow. > **Note:** Use the official cloudflow/deploy-action@v2 ### Step 4: Test the pipeline Push a commit to trigger the workflow. ```bash git push origin main ``` ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            },
            {
              "doc_id": "howto_migrate_from_v1_to_v2_api",
              "chunk_id": "howto_migrate_from_v1_to_v2_api_fix_0",
              "content": "# How to Migrate from v1 to v2 API ## Prerequisites - Existing v1 integration - v2 API access enabled ## Steps ### Step 1: Review breaking changes The v2 API changes authentication from API keys to OAuth 2.0. Review the migration guide. ### Step 2: Update client library Upgrade to the latest SDK version. ```bash npm install @cloudflow/sdk@latest ``` ### Step 3: Update authentication Replace API key authentication with OAuth 2.0 client credentials flow. > **Note:** Generate new credentials in the dashboard ### Step 4: Test in staging Run your integration tests against the v2 staging endpoint. ```bash CLOUDFLOW_API_URL=https://api-v2-staging.cloudflow.io npm test ``` ### Step 5: Switch production Update production configuration to use v2 API. ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            },
            {
              "doc_id": "api_webhooks",
              "chunk_id": "api_webhooks_fix_0",
              "content": "# Webhooks API Reference ## Overview The Webhooks API provides programmatic access to webhooks functionality. Base URL: `https://api.cloudflow.io/v1/webhooks` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /webhooks List configured webhooks. **Parameters:** - `active` (boolean): Filter by active status **Response:** - Success: `200` - Returns array of webhook objects - Error: `400` - Bad Request **Example:** ```json {\"webhooks\": [{\"id\": \"wh_01\", \"url\": \"https://example.com/hook\", \"events\": [\"workflow.completed\"]}]} ``` ### POST /webhooks Create a new webhook endpoint. **Parameters:** - `url` (string): Webhook URL (must be HTTPS) - `events` (array): Events to subscribe to - `secret` (string): Signing secret for verification **Response:** - Success: `201` - Webhook created - Error: `400` - Bad Request **Example:** ```json {\"id\": \"wh_02\", \"url\": \"https://example.com/new-hook\", \"secret\": \"whsec_...\"} ```",
              "score": null
            }
          ],
          "latency_ms": 3.775179999138345,
          "query_id": "api_042",
          "dimension": "contextual",
          "query": "automating team onboarding, need the API call to add new members to a team"
        },
        {
          "key_facts": [
            "POST /teams/{id}/members",
            "user_id",
            "role"
          ],
          "found_facts": [
            "POST /teams/{id}/members",
            "user_id",
            "role"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "api_teams"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "api_teams",
              "chunk_id": "api_teams_fix_0",
              "content": "# Teams API Reference ## Overview The Teams API provides programmatic access to teams functionality. Base URL: `https://api.cloudflow.io/v1/teams` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /teams List teams the user belongs to. **Parameters:** **Response:** - Success: `200` - Returns array of team objects - Error: `400` - Bad Request **Example:** ```json {\"teams\": [{\"id\": \"team_01\", \"name\": \"Engineering\", \"member_count\": 15}]} ``` ### POST /teams/{id}/members Add a member to a team. **Parameters:** - `user_id` (string): User ID to add - `role` (string): Role: owner, admin, member, viewer **Response:** - Success: `201` - Member added - Error: `400` - Bad Request **Example:** ```json {\"team_id\": \"team_01\", \"user_id\": \"usr_002\", \"role\": \"member\"} ```",
              "score": null
            },
            {
              "doc_id": "api_users",
              "chunk_id": "api_users_fix_0",
              "content": "# Users API Reference ## Overview The Users API provides programmatic access to users functionality. Base URL: `https://api.cloudflow.io/v1/users` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /users/me Get current user profile. **Parameters:** **Response:** - Success: `200` - Returns user object - Error: `400` - Bad Request **Example:** ```json {\"id\": \"usr_001\", \"email\": \"user@example.com\", \"role\": \"admin\", \"created_at\": \"2023-06-01\"} ``` ### PATCH /users/me Update current user profile. **Parameters:** - `name` (string): Display name - `timezone` (string): Timezone (e.g., America/New_York) **Response:** - Success: `200` - User updated - Error: `400` - Bad Request **Example:** ```json {\"id\": \"usr_001\", \"name\": \"John Doe\", \"timezone\": \"America/New_York\"} ```",
              "score": null
            },
            {
              "doc_id": "api_webhooks",
              "chunk_id": "api_webhooks_fix_0",
              "content": "# Webhooks API Reference ## Overview The Webhooks API provides programmatic access to webhooks functionality. Base URL: `https://api.cloudflow.io/v1/webhooks` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /webhooks List configured webhooks. **Parameters:** - `active` (boolean): Filter by active status **Response:** - Success: `200` - Returns array of webhook objects - Error: `400` - Bad Request **Example:** ```json {\"webhooks\": [{\"id\": \"wh_01\", \"url\": \"https://example.com/hook\", \"events\": [\"workflow.completed\"]}]} ``` ### POST /webhooks Create a new webhook endpoint. **Parameters:** - `url` (string): Webhook URL (must be HTTPS) - `events` (array): Events to subscribe to - `secret` (string): Signing secret for verification **Response:** - Success: `201` - Webhook created - Error: `400` - Bad Request **Example:** ```json {\"id\": \"wh_02\", \"url\": \"https://example.com/new-hook\", \"secret\": \"whsec_...\"} ```",
              "score": null
            },
            {
              "doc_id": "api_secrets",
              "chunk_id": "api_secrets_fix_0",
              "content": "# Secrets API Reference ## Overview The Secrets API provides programmatic access to secrets functionality. Base URL: `https://api.cloudflow.io/v1/secrets` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /secrets List secret names (values not returned). **Parameters:** **Response:** - Success: `200` - Returns array of secret metadata - Error: `400` - Bad Request **Example:** ```json {\"secrets\": [{\"name\": \"DATABASE_URL\", \"created_at\": \"2024-01-01\", \"updated_at\": \"2024-01-10\"}]} ``` ### PUT /secrets/{name} Create or update a secret. **Parameters:** - `value` (string): Secret value (encrypted at rest) **Response:** - Success: `200` - Secret saved - Error: `400` - Bad Request **Example:** ```json {\"name\": \"DATABASE_URL\", \"updated_at\": \"2024-01-15T10:00:00Z\"} ``` ### DELETE /secrets/{name} Delete a secret. **Parameters:** **Response:** - Success: `204` - Secret deleted - Error: `400` - Bad Request **Example:** ```json {} ```",
              "score": null
            },
            {
              "doc_id": "api_audit",
              "chunk_id": "api_audit_fix_0",
              "content": "# Audit API Reference ## Overview The Audit API provides programmatic access to audit functionality. Base URL: `https://api.cloudflow.io/v1/audit` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /audit/logs Retrieve audit logs for compliance. **Parameters:** - `start_date` (date): Start date for log range - `end_date` (date): End date for log range - `actor` (string): Filter by user ID **Response:** - Success: `200` - Returns array of audit log entries - Error: `400` - Bad Request **Example:** ```json {\"logs\": [{\"timestamp\": \"2024-01-15T10:00:00Z\", \"actor\": \"usr_001\", \"action\": \"workflow.created\", \"resource\": \"wf_123\"}]} ```",
              "score": null
            }
          ],
          "latency_ms": 3.5121109995088773,
          "query_id": "api_042",
          "dimension": "negation",
          "query": "why can't I add users to my team through the API"
        },
        {
          "key_facts": [
            "POST /webhooks",
            "url",
            "events",
            "secret"
          ],
          "found_facts": [
            "POST /webhooks",
            "url",
            "events",
            "secret"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "api_webhooks"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "api_webhooks",
              "chunk_id": "api_webhooks_fix_0",
              "content": "# Webhooks API Reference ## Overview The Webhooks API provides programmatic access to webhooks functionality. Base URL: `https://api.cloudflow.io/v1/webhooks` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /webhooks List configured webhooks. **Parameters:** - `active` (boolean): Filter by active status **Response:** - Success: `200` - Returns array of webhook objects - Error: `400` - Bad Request **Example:** ```json {\"webhooks\": [{\"id\": \"wh_01\", \"url\": \"https://example.com/hook\", \"events\": [\"workflow.completed\"]}]} ``` ### POST /webhooks Create a new webhook endpoint. **Parameters:** - `url` (string): Webhook URL (must be HTTPS) - `events` (array): Events to subscribe to - `secret` (string): Signing secret for verification **Response:** - Success: `201` - Webhook created - Error: `400` - Bad Request **Example:** ```json {\"id\": \"wh_02\", \"url\": \"https://example.com/new-hook\", \"secret\": \"whsec_...\"} ```",
              "score": null
            },
            {
              "doc_id": "arch_notification_service",
              "chunk_id": "arch_notification_service_fix_0",
              "content": "# Notification Service Architecture ## Overview The Notification Service delivers alerts and updates through multiple channels including email, Slack, webhooks, and mobile push notifications. ## Design Principles - **Delivery Guarantee**: At-least-once delivery with deduplication - **Channel Abstraction**: Unified API regardless of delivery channel ## Components ### Dispatcher Routes notifications to appropriate channel handlers. Supports priority queues for urgent notifications. **Technology:** Go microservice **Scaling:** Horizontal with queue partitioning ### Email Provider Sends transactional emails. Maximum 10000 emails per hour per account. **Technology:** SendGrid integration **Scaling:** API rate limited ### Slack Integration Posts messages to Slack channels. Supports message formatting and interactive components. **Technology:** Slack Bolt SDK **Scaling:** Per-workspace rate limits ## Data Flow Event \u2192 Template \u2192 Dispatcher \u2192 Channel \u2192 Delivery Confirmation ## Performance Characteristics - **Latency P50:** 200ms - **Latency P99:** 2000ms - **Throughput:** 10000 notifications/minute",
              "score": null
            },
            {
              "doc_id": "howto_set_up_monitoring_alerts",
              "chunk_id": "howto_set_up_monitoring_alerts_fix_0",
              "content": "# How to Set Up Monitoring Alerts ## Prerequisites - CloudFlow Pro plan or higher ## Steps ### Step 1: Define alert rules Create alert conditions based on metrics thresholds. ```bash cloudflow alerts create --name 'High Error Rate' --condition 'error_rate > 0.05' ``` ### Step 2: Configure channels Set up notification channels: email, Slack, or PagerDuty. ```bash cloudflow alerts channel add --type slack --webhook https://hooks.slack.com/... ``` ### Step 3: Set severity levels Assign severity to each alert: critical, warning, or info. > **Note:** Critical alerts bypass quiet hours ### Step 4: Test alerts Trigger a test alert to verify routing. ```bash cloudflow alerts test --name 'High Error Rate' ``` ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            },
            {
              "doc_id": "arch_monitoring_stack",
              "chunk_id": "arch_monitoring_stack_fix_0",
              "content": "# Monitoring Stack Architecture ## Overview The Monitoring Stack provides observability into system health, performance, and errors. Includes metrics, logs, traces, and alerting. ## Design Principles - **Observability**: Every component emits metrics and traces - **Actionable Alerts**: Alerts include runbook links ## Components ### Metrics Collects time-series metrics. Retention is 15 days local, 1 year in object storage. **Technology:** Prometheus with Thanos **Scaling:** Federated with long-term storage ### Logging Centralized log aggregation. Supports structured JSON logs with label indexing. **Technology:** Loki with Grafana **Scaling:** Distributed with sharding ### Tracing Distributed request tracing. Trace retention is 7 days. **Technology:** Jaeger with OpenTelemetry **Scaling:** Sampling at 10% ### Alerting Routes alerts to on-call engineers. Supports escalation policies and silencing. **Technology:** Alertmanager with PagerDuty **Scaling:** HA pair ## Data Flow Application \u2192 Collector \u2192 Storage \u2192 Query \u2192 Dashboard/Alert ## Performance Characteristics - **Latency P50:** N/A - **Latency P99:** N/A - **Throughput:** 1M metrics/second, 100GB logs/day",
              "score": null
            },
            {
              "doc_id": "adr_003",
              "chunk_id": "adr_003_fix_0",
              "content": "# ADR-003: Implement OAuth 2.0 with JWT for Authentication **Status:** Accepted **Date:** 2024-05-08 ## Context We need a secure authentication system that supports API keys, user sessions, and third-party integrations. ## Options Considered ### Option 1: OAuth 2.0 with JWT Industry standard token-based auth. **Pros:** - Industry standard - Stateless validation - Supports refresh tokens - SSO compatible **Cons:** - Token size overhead - Cannot revoke individual tokens - Complexity ### Option 2: Session-based auth Traditional server-side sessions. **Pros:** - Simple implementation - Easy revocation - Smaller payloads **Cons:** - Requires session store - Not suitable for APIs - Scaling challenges ### Option 3: API keys only Simple key-based authentication. **Pros:** - Very simple - Low overhead - Easy to implement **Cons:** - No user context - Hard to rotate - No SSO support ## Decision We will implement OAuth 2.0 with JWT tokens. Access tokens expire after 1 hour, refresh tokens after 7 days. ## Consequences - Implement token rotation on refresh - Support API keys for machine-to-machine auth - Add MFA for admin accounts",
              "score": null
            }
          ],
          "latency_ms": 3.3465819997218205,
          "query_id": "api_043",
          "dimension": "original",
          "query": "How do I create a webhook?"
        },
        {
          "key_facts": [
            "POST /webhooks",
            "url",
            "events",
            "secret"
          ],
          "found_facts": [
            "POST /webhooks",
            "url",
            "events",
            "secret"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "api_webhooks"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "api_workflows",
              "chunk_id": "api_workflows_fix_0",
              "content": "# Workflows API Reference ## Overview The Workflows API provides programmatic access to workflows functionality. Base URL: `https://api.cloudflow.io/v1/workflows` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /workflows List all workflows for the authenticated user. **Parameters:** - `limit` (integer): Maximum number of results (default: 50, max: 200) - `status` (string): Filter by status: active, paused, archived **Response:** - Success: `200` - Returns array of workflow objects - Error: `400` - Bad Request **Example:** ```json {\"workflows\": [{\"id\": \"wf_123\", \"name\": \"Daily Report\", \"status\": \"active\"}]} ``` ### POST /workflows Create a new workflow. **Parameters:** - `name` (string): Workflow name (required) - `definition` (object): Workflow definition in JSON format **Response:** - Success: `201` - Workflow created successfully - Error: `400` - Bad Request **Example:** ```json {\"id\": \"wf_456\", \"name\": \"New Workflow\", \"created_at\": \"2024-01-15T10:00:00Z\"} ``` ### DELETE /workflows/{id} Delete a workflow permanently. **Parameters:** - `id` (string): Workflow ID (required) **Response:** - Success: `204` - Workflow deleted - Error: `404` - Workflow not found **Example:** ```json {} ```",
              "score": null
            },
            {
              "doc_id": "api_webhooks",
              "chunk_id": "api_webhooks_fix_0",
              "content": "# Webhooks API Reference ## Overview The Webhooks API provides programmatic access to webhooks functionality. Base URL: `https://api.cloudflow.io/v1/webhooks` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /webhooks List configured webhooks. **Parameters:** - `active` (boolean): Filter by active status **Response:** - Success: `200` - Returns array of webhook objects - Error: `400` - Bad Request **Example:** ```json {\"webhooks\": [{\"id\": \"wh_01\", \"url\": \"https://example.com/hook\", \"events\": [\"workflow.completed\"]}]} ``` ### POST /webhooks Create a new webhook endpoint. **Parameters:** - `url` (string): Webhook URL (must be HTTPS) - `events` (array): Events to subscribe to - `secret` (string): Signing secret for verification **Response:** - Success: `201` - Webhook created - Error: `400` - Bad Request **Example:** ```json {\"id\": \"wh_02\", \"url\": \"https://example.com/new-hook\", \"secret\": \"whsec_...\"} ```",
              "score": null
            },
            {
              "doc_id": "api_executions",
              "chunk_id": "api_executions_fix_0",
              "content": "# Executions API Reference ## Overview The Executions API provides programmatic access to executions functionality. Base URL: `https://api.cloudflow.io/v1/executions` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /executions List workflow executions. **Parameters:** - `workflow_id` (string): Filter by workflow ID - `since` (datetime): Filter executions after this timestamp **Response:** - Success: `200` - Returns array of execution objects - Error: `400` - Bad Request **Example:** ```json {\"executions\": [{\"id\": \"ex_789\", \"status\": \"completed\", \"duration_ms\": 1523}]} ``` ### POST /executions/{id}/cancel Cancel a running execution. **Parameters:** - `id` (string): Execution ID - `reason` (string): Cancellation reason (optional) **Response:** - Success: `200` - Execution cancelled - Error: `400` - Bad Request **Example:** ```json {\"id\": \"ex_789\", \"status\": \"cancelled\", \"cancelled_at\": \"2024-01-15T10:05:00Z\"} ```",
              "score": null
            },
            {
              "doc_id": "api_metrics",
              "chunk_id": "api_metrics_fix_0",
              "content": "# Metrics API Reference ## Overview The Metrics API provides programmatic access to metrics functionality. Base URL: `https://api.cloudflow.io/v1/metrics` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /metrics/workflows Get workflow performance metrics. **Parameters:** - `period` (string): Time period: hour, day, week, month - `workflow_id` (string): Filter by workflow ID **Response:** - Success: `200` - Returns metrics data - Error: `400` - Bad Request **Example:** ```json {\"success_rate\": 0.985, \"avg_duration_ms\": 1250, \"p99_duration_ms\": 5000, \"total_executions\": 10000} ```",
              "score": null
            },
            {
              "doc_id": "api_audit",
              "chunk_id": "api_audit_fix_0",
              "content": "# Audit API Reference ## Overview The Audit API provides programmatic access to audit functionality. Base URL: `https://api.cloudflow.io/v1/audit` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /audit/logs Retrieve audit logs for compliance. **Parameters:** - `start_date` (date): Start date for log range - `end_date` (date): End date for log range - `actor` (string): Filter by user ID **Response:** - Success: `200` - Returns array of audit log entries - Error: `400` - Bad Request **Example:** ```json {\"logs\": [{\"timestamp\": \"2024-01-15T10:00:00Z\", \"actor\": \"usr_001\", \"action\": \"workflow.created\", \"resource\": \"wf_123\"}]} ```",
              "score": null
            }
          ],
          "latency_ms": 3.956788999857963,
          "query_id": "api_043",
          "dimension": "synonym",
          "query": "I need to register a callback URL for workflow events via the API"
        },
        {
          "key_facts": [
            "POST /webhooks",
            "url",
            "events",
            "secret"
          ],
          "found_facts": [
            "POST /webhooks",
            "url",
            "events",
            "secret"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "api_webhooks"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "api_webhooks",
              "chunk_id": "api_webhooks_fix_0",
              "content": "# Webhooks API Reference ## Overview The Webhooks API provides programmatic access to webhooks functionality. Base URL: `https://api.cloudflow.io/v1/webhooks` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /webhooks List configured webhooks. **Parameters:** - `active` (boolean): Filter by active status **Response:** - Success: `200` - Returns array of webhook objects - Error: `400` - Bad Request **Example:** ```json {\"webhooks\": [{\"id\": \"wh_01\", \"url\": \"https://example.com/hook\", \"events\": [\"workflow.completed\"]}]} ``` ### POST /webhooks Create a new webhook endpoint. **Parameters:** - `url` (string): Webhook URL (must be HTTPS) - `events` (array): Events to subscribe to - `secret` (string): Signing secret for verification **Response:** - Success: `201` - Webhook created - Error: `400` - Bad Request **Example:** ```json {\"id\": \"wh_02\", \"url\": \"https://example.com/new-hook\", \"secret\": \"whsec_...\"} ```",
              "score": null
            },
            {
              "doc_id": "release_3_0_0",
              "chunk_id": "release_3_0_0_fix_0",
              "content": "# Release Notes - v3.0.0 **Release Date:** 2023-10-15 ## New Features - Complete UI redesign - New workflow editor with visual builder - Real-time execution monitoring - Team workspaces ## Bug Fixes - Fixed critical security issue in token validation - Resolved data loss bug in failed migrations ## Breaking Changes - **BREAKING:** Authentication API completely rewritten - **BREAKING:** Legacy webhook format no longer supported - **BREAKING:** Minimum Node.js version now 18 ## Upgrade Instructions ```bash cloudflow upgrade --version 3.0.0 ``` ## Known Issues None at this time.",
              "score": null
            },
            {
              "doc_id": "api_integrations",
              "chunk_id": "api_integrations_fix_0",
              "content": "# Integrations API Reference ## Overview The Integrations API provides programmatic access to integrations functionality. Base URL: `https://api.cloudflow.io/v1/integrations` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /integrations List available integrations. **Parameters:** - `category` (string): Filter by category: storage, notification, database **Response:** - Success: `200` - Returns array of integration objects - Error: `400` - Bad Request **Example:** ```json {\"integrations\": [{\"id\": \"int_s3\", \"name\": \"Amazon S3\", \"category\": \"storage\", \"status\": \"connected\"}]} ``` ### POST /integrations/{id}/connect Connect an integration. **Parameters:** - `credentials` (object): Integration-specific credentials **Response:** - Success: `200` - Integration connected - Error: `400` - Bad Request **Example:** ```json {\"id\": \"int_s3\", \"status\": \"connected\", \"connected_at\": \"2024-01-15T10:00:00Z\"} ```",
              "score": null
            },
            {
              "doc_id": "config_api_gateway",
              "chunk_id": "config_api_gateway_fix_0",
              "content": "# API Gateway Configuration Reference ## Overview This document describes all configuration options for API Gateway. ## Environment Variables | Variable | Type | Default | Description | |----------|------|---------|-------------| | `GATEWAY_PORT` | integer | 8000 | Gateway listen port | | `GATEWAY_RATE_LIMIT` | integer | 100 | Requests per minute per user | | `GATEWAY_TIMEOUT_MS` | integer | 30000 | Request timeout in milliseconds | | `GATEWAY_MAX_BODY_SIZE` | string | 10MB | Maximum request body size | | `GATEWAY_CORS_ORIGINS` | string | * | Allowed CORS origins | | `GATEWAY_SSL_ENABLED` | boolean | true | Enable TLS termination | ## Configuration File Configuration can also be provided via `api gateway.yaml`: ```yaml gateway_port: 8000 gateway_rate_limit: 100 gateway_timeout_ms: 30000 gateway_max_body_size: 10MB gateway_cors_origins: * ``` ## Validation Configuration is validated at startup. Invalid configuration will prevent the service from starting.",
              "score": null
            },
            {
              "doc_id": "api_teams",
              "chunk_id": "api_teams_fix_0",
              "content": "# Teams API Reference ## Overview The Teams API provides programmatic access to teams functionality. Base URL: `https://api.cloudflow.io/v1/teams` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /teams List teams the user belongs to. **Parameters:** **Response:** - Success: `200` - Returns array of team objects - Error: `400` - Bad Request **Example:** ```json {\"teams\": [{\"id\": \"team_01\", \"name\": \"Engineering\", \"member_count\": 15}]} ``` ### POST /teams/{id}/members Add a member to a team. **Parameters:** - `user_id` (string): User ID to add - `role` (string): Role: owner, admin, member, viewer **Response:** - Success: `201` - Member added - Error: `400` - Bad Request **Example:** ```json {\"team_id\": \"team_01\", \"user_id\": \"usr_002\", \"role\": \"member\"} ```",
              "score": null
            }
          ],
          "latency_ms": 3.904871999111492,
          "query_id": "api_043",
          "dimension": "problem",
          "query": "webhook creation failing with invalid URL error, what format does it expect"
        },
        {
          "key_facts": [
            "POST /webhooks",
            "url",
            "events",
            "secret"
          ],
          "found_facts": [
            "POST /webhooks",
            "url",
            "events",
            "secret"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "api_webhooks"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "api_webhooks",
              "chunk_id": "api_webhooks_fix_0",
              "content": "# Webhooks API Reference ## Overview The Webhooks API provides programmatic access to webhooks functionality. Base URL: `https://api.cloudflow.io/v1/webhooks` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /webhooks List configured webhooks. **Parameters:** - `active` (boolean): Filter by active status **Response:** - Success: `200` - Returns array of webhook objects - Error: `400` - Bad Request **Example:** ```json {\"webhooks\": [{\"id\": \"wh_01\", \"url\": \"https://example.com/hook\", \"events\": [\"workflow.completed\"]}]} ``` ### POST /webhooks Create a new webhook endpoint. **Parameters:** - `url` (string): Webhook URL (must be HTTPS) - `events` (array): Events to subscribe to - `secret` (string): Signing secret for verification **Response:** - Success: `201` - Webhook created - Error: `400` - Bad Request **Example:** ```json {\"id\": \"wh_02\", \"url\": \"https://example.com/new-hook\", \"secret\": \"whsec_...\"} ```",
              "score": null
            },
            {
              "doc_id": "arch_notification_service",
              "chunk_id": "arch_notification_service_fix_0",
              "content": "# Notification Service Architecture ## Overview The Notification Service delivers alerts and updates through multiple channels including email, Slack, webhooks, and mobile push notifications. ## Design Principles - **Delivery Guarantee**: At-least-once delivery with deduplication - **Channel Abstraction**: Unified API regardless of delivery channel ## Components ### Dispatcher Routes notifications to appropriate channel handlers. Supports priority queues for urgent notifications. **Technology:** Go microservice **Scaling:** Horizontal with queue partitioning ### Email Provider Sends transactional emails. Maximum 10000 emails per hour per account. **Technology:** SendGrid integration **Scaling:** API rate limited ### Slack Integration Posts messages to Slack channels. Supports message formatting and interactive components. **Technology:** Slack Bolt SDK **Scaling:** Per-workspace rate limits ## Data Flow Event \u2192 Template \u2192 Dispatcher \u2192 Channel \u2192 Delivery Confirmation ## Performance Characteristics - **Latency P50:** 200ms - **Latency P99:** 2000ms - **Throughput:** 10000 notifications/minute",
              "score": null
            },
            {
              "doc_id": "arch_api_gateway",
              "chunk_id": "arch_api_gateway_fix_0",
              "content": "# API Gateway Architecture ## Overview The API Gateway handles all incoming HTTP requests, providing authentication, rate limiting, and request routing. Built on Kong with custom plugins for CloudFlow-specific functionality. ## Design Principles - **Zero Trust**: Every request is authenticated and authorized - **Defense in Depth**: Multiple layers of security validation ## Components ### Kong Gateway Primary ingress controller. Handles TLS termination, request routing, and plugin execution. **Technology:** Kong 3.4 on Kubernetes **Scaling:** Horizontal with HPA ### Auth Plugin Validates JWT tokens, API keys, and OAuth2 flows. Token validation latency under 5ms. **Technology:** Custom Lua plugin **Scaling:** Stateless ### Rate Limiter Enforces per-user and per-IP rate limits. Default limit is 100 requests per minute. **Technology:** Redis-backed sliding window **Scaling:** Shared Redis cluster ## Data Flow Client \u2192 TLS \u2192 Kong \u2192 Auth \u2192 Rate Limit \u2192 Backend Services ## Performance Characteristics - **Latency P50:** 12ms - **Latency P99:** 45ms - **Throughput:** 50000 requests/second",
              "score": null
            },
            {
              "doc_id": "config_api_gateway",
              "chunk_id": "config_api_gateway_fix_0",
              "content": "# API Gateway Configuration Reference ## Overview This document describes all configuration options for API Gateway. ## Environment Variables | Variable | Type | Default | Description | |----------|------|---------|-------------| | `GATEWAY_PORT` | integer | 8000 | Gateway listen port | | `GATEWAY_RATE_LIMIT` | integer | 100 | Requests per minute per user | | `GATEWAY_TIMEOUT_MS` | integer | 30000 | Request timeout in milliseconds | | `GATEWAY_MAX_BODY_SIZE` | string | 10MB | Maximum request body size | | `GATEWAY_CORS_ORIGINS` | string | * | Allowed CORS origins | | `GATEWAY_SSL_ENABLED` | boolean | true | Enable TLS termination | ## Configuration File Configuration can also be provided via `api gateway.yaml`: ```yaml gateway_port: 8000 gateway_rate_limit: 100 gateway_timeout_ms: 30000 gateway_max_body_size: 10MB gateway_cors_origins: * ``` ## Validation Configuration is validated at startup. Invalid configuration will prevent the service from starting.",
              "score": null
            },
            {
              "doc_id": "api_integrations",
              "chunk_id": "api_integrations_fix_0",
              "content": "# Integrations API Reference ## Overview The Integrations API provides programmatic access to integrations functionality. Base URL: `https://api.cloudflow.io/v1/integrations` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /integrations List available integrations. **Parameters:** - `category` (string): Filter by category: storage, notification, database **Response:** - Success: `200` - Returns array of integration objects - Error: `400` - Bad Request **Example:** ```json {\"integrations\": [{\"id\": \"int_s3\", \"name\": \"Amazon S3\", \"category\": \"storage\", \"status\": \"connected\"}]} ``` ### POST /integrations/{id}/connect Connect an integration. **Parameters:** - `credentials` (object): Integration-specific credentials **Response:** - Success: `200` - Integration connected - Error: `400` - Bad Request **Example:** ```json {\"id\": \"int_s3\", \"status\": \"connected\", \"connected_at\": \"2024-01-15T10:00:00Z\"} ```",
              "score": null
            }
          ],
          "latency_ms": 3.509976000714232,
          "query_id": "api_043",
          "dimension": "casual",
          "query": "webhooks POST endpoint"
        },
        {
          "key_facts": [
            "POST /webhooks",
            "url",
            "events",
            "secret"
          ],
          "found_facts": [
            "POST /webhooks",
            "url",
            "events",
            "secret"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "api_webhooks"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "howto_set_up_monitoring_alerts",
              "chunk_id": "howto_set_up_monitoring_alerts_fix_0",
              "content": "# How to Set Up Monitoring Alerts ## Prerequisites - CloudFlow Pro plan or higher ## Steps ### Step 1: Define alert rules Create alert conditions based on metrics thresholds. ```bash cloudflow alerts create --name 'High Error Rate' --condition 'error_rate > 0.05' ``` ### Step 2: Configure channels Set up notification channels: email, Slack, or PagerDuty. ```bash cloudflow alerts channel add --type slack --webhook https://hooks.slack.com/... ``` ### Step 3: Set severity levels Assign severity to each alert: critical, warning, or info. > **Note:** Critical alerts bypass quiet hours ### Step 4: Test alerts Trigger a test alert to verify routing. ```bash cloudflow alerts test --name 'High Error Rate' ``` ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            },
            {
              "doc_id": "api_webhooks",
              "chunk_id": "api_webhooks_fix_0",
              "content": "# Webhooks API Reference ## Overview The Webhooks API provides programmatic access to webhooks functionality. Base URL: `https://api.cloudflow.io/v1/webhooks` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /webhooks List configured webhooks. **Parameters:** - `active` (boolean): Filter by active status **Response:** - Success: `200` - Returns array of webhook objects - Error: `400` - Bad Request **Example:** ```json {\"webhooks\": [{\"id\": \"wh_01\", \"url\": \"https://example.com/hook\", \"events\": [\"workflow.completed\"]}]} ``` ### POST /webhooks Create a new webhook endpoint. **Parameters:** - `url` (string): Webhook URL (must be HTTPS) - `events` (array): Events to subscribe to - `secret` (string): Signing secret for verification **Response:** - Success: `201` - Webhook created - Error: `400` - Bad Request **Example:** ```json {\"id\": \"wh_02\", \"url\": \"https://example.com/new-hook\", \"secret\": \"whsec_...\"} ```",
              "score": null
            },
            {
              "doc_id": "arch_notification_service",
              "chunk_id": "arch_notification_service_fix_0",
              "content": "# Notification Service Architecture ## Overview The Notification Service delivers alerts and updates through multiple channels including email, Slack, webhooks, and mobile push notifications. ## Design Principles - **Delivery Guarantee**: At-least-once delivery with deduplication - **Channel Abstraction**: Unified API regardless of delivery channel ## Components ### Dispatcher Routes notifications to appropriate channel handlers. Supports priority queues for urgent notifications. **Technology:** Go microservice **Scaling:** Horizontal with queue partitioning ### Email Provider Sends transactional emails. Maximum 10000 emails per hour per account. **Technology:** SendGrid integration **Scaling:** API rate limited ### Slack Integration Posts messages to Slack channels. Supports message formatting and interactive components. **Technology:** Slack Bolt SDK **Scaling:** Per-workspace rate limits ## Data Flow Event \u2192 Template \u2192 Dispatcher \u2192 Channel \u2192 Delivery Confirmation ## Performance Characteristics - **Latency P50:** 200ms - **Latency P99:** 2000ms - **Throughput:** 10000 notifications/minute",
              "score": null
            },
            {
              "doc_id": "api_workflows",
              "chunk_id": "api_workflows_fix_0",
              "content": "# Workflows API Reference ## Overview The Workflows API provides programmatic access to workflows functionality. Base URL: `https://api.cloudflow.io/v1/workflows` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /workflows List all workflows for the authenticated user. **Parameters:** - `limit` (integer): Maximum number of results (default: 50, max: 200) - `status` (string): Filter by status: active, paused, archived **Response:** - Success: `200` - Returns array of workflow objects - Error: `400` - Bad Request **Example:** ```json {\"workflows\": [{\"id\": \"wf_123\", \"name\": \"Daily Report\", \"status\": \"active\"}]} ``` ### POST /workflows Create a new workflow. **Parameters:** - `name` (string): Workflow name (required) - `definition` (object): Workflow definition in JSON format **Response:** - Success: `201` - Workflow created successfully - Error: `400` - Bad Request **Example:** ```json {\"id\": \"wf_456\", \"name\": \"New Workflow\", \"created_at\": \"2024-01-15T10:00:00Z\"} ``` ### DELETE /workflows/{id} Delete a workflow permanently. **Parameters:** - `id` (string): Workflow ID (required) **Response:** - Success: `204` - Workflow deleted - Error: `404` - Workflow not found **Example:** ```json {} ```",
              "score": null
            },
            {
              "doc_id": "howto_set_up_ci_cd_pipeline",
              "chunk_id": "howto_set_up_ci_cd_pipeline_fix_0",
              "content": "# How to Set Up CI/CD Pipeline ## Prerequisites - GitHub account - CloudFlow API key - Repository admin access ## Steps ### Step 1: Create GitHub Actions workflow Create a new file at `.github/workflows/cloudflow.yml` with the deployment configuration. ```bash mkdir -p .github/workflows && touch .github/workflows/cloudflow.yml ``` ### Step 2: Configure secrets Add your CloudFlow API key to GitHub secrets. ```bash gh secret set CLOUDFLOW_API_KEY ``` ### Step 3: Add deployment step Add the CloudFlow deployment action to your workflow. > **Note:** Use the official cloudflow/deploy-action@v2 ### Step 4: Test the pipeline Push a commit to trigger the workflow. ```bash git push origin main ``` ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            }
          ],
          "latency_ms": 3.4867629983637016,
          "query_id": "api_043",
          "dimension": "contextual",
          "query": "integrating with Slack to notify on workflow completion, need to set up a webhook endpoint"
        },
        {
          "key_facts": [
            "POST /webhooks",
            "url",
            "events",
            "secret"
          ],
          "found_facts": [
            "POST /webhooks",
            "url",
            "events",
            "secret"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "api_webhooks"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "api_webhooks",
              "chunk_id": "api_webhooks_fix_0",
              "content": "# Webhooks API Reference ## Overview The Webhooks API provides programmatic access to webhooks functionality. Base URL: `https://api.cloudflow.io/v1/webhooks` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /webhooks List configured webhooks. **Parameters:** - `active` (boolean): Filter by active status **Response:** - Success: `200` - Returns array of webhook objects - Error: `400` - Bad Request **Example:** ```json {\"webhooks\": [{\"id\": \"wh_01\", \"url\": \"https://example.com/hook\", \"events\": [\"workflow.completed\"]}]} ``` ### POST /webhooks Create a new webhook endpoint. **Parameters:** - `url` (string): Webhook URL (must be HTTPS) - `events` (array): Events to subscribe to - `secret` (string): Signing secret for verification **Response:** - Success: `201` - Webhook created - Error: `400` - Bad Request **Example:** ```json {\"id\": \"wh_02\", \"url\": \"https://example.com/new-hook\", \"secret\": \"whsec_...\"} ```",
              "score": null
            },
            {
              "doc_id": "release_3_0_0",
              "chunk_id": "release_3_0_0_fix_0",
              "content": "# Release Notes - v3.0.0 **Release Date:** 2023-10-15 ## New Features - Complete UI redesign - New workflow editor with visual builder - Real-time execution monitoring - Team workspaces ## Bug Fixes - Fixed critical security issue in token validation - Resolved data loss bug in failed migrations ## Breaking Changes - **BREAKING:** Authentication API completely rewritten - **BREAKING:** Legacy webhook format no longer supported - **BREAKING:** Minimum Node.js version now 18 ## Upgrade Instructions ```bash cloudflow upgrade --version 3.0.0 ``` ## Known Issues None at this time.",
              "score": null
            },
            {
              "doc_id": "howto_set_up_monitoring_alerts",
              "chunk_id": "howto_set_up_monitoring_alerts_fix_0",
              "content": "# How to Set Up Monitoring Alerts ## Prerequisites - CloudFlow Pro plan or higher ## Steps ### Step 1: Define alert rules Create alert conditions based on metrics thresholds. ```bash cloudflow alerts create --name 'High Error Rate' --condition 'error_rate > 0.05' ``` ### Step 2: Configure channels Set up notification channels: email, Slack, or PagerDuty. ```bash cloudflow alerts channel add --type slack --webhook https://hooks.slack.com/... ``` ### Step 3: Set severity levels Assign severity to each alert: critical, warning, or info. > **Note:** Critical alerts bypass quiet hours ### Step 4: Test alerts Trigger a test alert to verify routing. ```bash cloudflow alerts test --name 'High Error Rate' ``` ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            },
            {
              "doc_id": "api_teams",
              "chunk_id": "api_teams_fix_0",
              "content": "# Teams API Reference ## Overview The Teams API provides programmatic access to teams functionality. Base URL: `https://api.cloudflow.io/v1/teams` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /teams List teams the user belongs to. **Parameters:** **Response:** - Success: `200` - Returns array of team objects - Error: `400` - Bad Request **Example:** ```json {\"teams\": [{\"id\": \"team_01\", \"name\": \"Engineering\", \"member_count\": 15}]} ``` ### POST /teams/{id}/members Add a member to a team. **Parameters:** - `user_id` (string): User ID to add - `role` (string): Role: owner, admin, member, viewer **Response:** - Success: `201` - Member added - Error: `400` - Bad Request **Example:** ```json {\"team_id\": \"team_01\", \"user_id\": \"usr_002\", \"role\": \"member\"} ```",
              "score": null
            },
            {
              "doc_id": "runbook_api_gateway_5xx_errors",
              "chunk_id": "runbook_api_gateway_5xx_errors_fix_0",
              "content": "# Runbook: API Gateway 5xx Errors ## Severity **Level:** P3 - Medium **Response Time:** 1 hour ## Symptoms - Error rate > 1% - 5xx responses in access logs - Downstream service failures ## Diagnosis ### Step 1 Check Kong error logs: `kubectl logs -l app=kong -n kong --tail=100 | grep -E '5[0-9]{2}'` ### Step 2 Verify backend health: `kubectl get pods -n production -o wide` ### Step 3 Check service endpoints: `kubectl get endpoints -n production` ### Step 4 Test backend directly: `kubectl exec -it kong-xxx -n kong -- curl http://api.production.svc:8080/health` ## Resolution ### Step 1 Restart unhealthy backends: `kubectl rollout restart deployment/api -n production` ### Step 2 Enable circuit breaker if cascading: update Kong circuit-breaker plugin ### Step 3 Scale up backends: `kubectl scale deployment/api --replicas=5 -n production` ### Step 4 Check for resource exhaustion on backend pods ### Step 5 Verify network policies allow traffic ## Escalation If 5xx persists > 15 minutes, page on-call. May indicate infrastructure issue. ## Post-Incident - Document timeline in incident log - Schedule post-mortem within 48 hours - Update runbook if new patterns discovered",
              "score": null
            }
          ],
          "latency_ms": 3.386938000403461,
          "query_id": "api_043",
          "dimension": "negation",
          "query": "why isn't my webhook getting created, what am I missing"
        },
        {
          "key_facts": [
            "GET /billing/usage",
            "executions",
            "compute_minutes"
          ],
          "found_facts": [
            "GET /billing/usage",
            "executions",
            "compute_minutes"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "api_billing"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "api_billing",
              "chunk_id": "api_billing_fix_0",
              "content": "# Billing API Reference ## Overview The Billing API provides programmatic access to billing functionality. Base URL: `https://api.cloudflow.io/v1/billing` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /billing/usage Get current billing period usage. **Parameters:** - `breakdown` (boolean): Include detailed breakdown by resource **Response:** - Success: `200` - Returns usage summary - Error: `400` - Bad Request **Example:** ```json {\"period\": \"2024-01\", \"executions\": 15000, \"compute_minutes\": 2500, \"storage_gb\": 50} ``` ### GET /billing/invoices List past invoices. **Parameters:** - `year` (integer): Filter by year **Response:** - Success: `200` - Returns array of invoice objects - Error: `400` - Bad Request **Example:** ```json {\"invoices\": [{\"id\": \"inv_202401\", \"amount\": 299.00, \"status\": \"paid\", \"date\": \"2024-01-01\"}]} ```",
              "score": null
            },
            {
              "doc_id": "api_metrics",
              "chunk_id": "api_metrics_fix_0",
              "content": "# Metrics API Reference ## Overview The Metrics API provides programmatic access to metrics functionality. Base URL: `https://api.cloudflow.io/v1/metrics` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /metrics/workflows Get workflow performance metrics. **Parameters:** - `period` (string): Time period: hour, day, week, month - `workflow_id` (string): Filter by workflow ID **Response:** - Success: `200` - Returns metrics data - Error: `400` - Bad Request **Example:** ```json {\"success_rate\": 0.985, \"avg_duration_ms\": 1250, \"p99_duration_ms\": 5000, \"total_executions\": 10000} ```",
              "score": null
            },
            {
              "doc_id": "api_audit",
              "chunk_id": "api_audit_fix_0",
              "content": "# Audit API Reference ## Overview The Audit API provides programmatic access to audit functionality. Base URL: `https://api.cloudflow.io/v1/audit` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /audit/logs Retrieve audit logs for compliance. **Parameters:** - `start_date` (date): Start date for log range - `end_date` (date): End date for log range - `actor` (string): Filter by user ID **Response:** - Success: `200` - Returns array of audit log entries - Error: `400` - Bad Request **Example:** ```json {\"logs\": [{\"timestamp\": \"2024-01-15T10:00:00Z\", \"actor\": \"usr_001\", \"action\": \"workflow.created\", \"resource\": \"wf_123\"}]} ```",
              "score": null
            },
            {
              "doc_id": "arch_authentication_service",
              "chunk_id": "arch_authentication_service_fix_0",
              "content": "# Authentication Service Architecture ## Overview The Authentication Service handles user identity, session management, and token issuance. Supports OAuth 2.0, SAML, and API key authentication. ## Design Principles - **Zero Knowledge**: Passwords never stored in plaintext - **Token Rotation**: Refresh tokens rotate on each use ## Components ### Identity Provider Manages user accounts and credentials. Password hashing uses bcrypt with cost factor 12. **Technology:** Custom Go service **Scaling:** Horizontal stateless ### Token Service Issues and validates access tokens. Token lifetime is 1 hour with 7-day refresh tokens. **Technology:** JWT with RS256 **Scaling:** Horizontal stateless ### SSO Gateway Integrates with external identity providers. Supports Google, Microsoft, Okta, and custom SAML. **Technology:** Dex OIDC proxy **Scaling:** Active-passive ## Data Flow Credentials \u2192 Validation \u2192 Token Issue \u2192 Token Refresh \u2192 Logout ## Performance Characteristics - **Latency P50:** 50ms - **Latency P99:** 200ms - **Throughput:** 5000 auth/second",
              "score": null
            },
            {
              "doc_id": "adr_007",
              "chunk_id": "adr_007_fix_0",
              "content": "# ADR-007: Implement Rate Limiting at API Gateway **Status:** Accepted **Date:** 2024-09-03 ## Context We need to protect our services from abuse and ensure fair resource usage across customers. ## Options Considered ### Option 1: Sliding window at gateway Rate limit at API gateway level. **Pros:** - Centralized enforcement - Low latency - Easy configuration **Cons:** - Gateway becomes bottleneck - Coarse granularity - Redis dependency ### Option 2: Per-service limiting Each service implements its own limits. **Pros:** - Fine-grained control - Service autonomy - No single point of failure **Cons:** - Inconsistent implementation - Higher complexity - Distributed state ### Option 3: Token bucket Token-based rate limiting. **Pros:** - Allows bursts - Smooth rate limiting - Well understood **Cons:** - More complex - Harder to explain to users - State management ## Decision We will implement sliding window rate limiting at the Kong API Gateway using Redis. ## Consequences - Default limit: 100 requests per minute - Implement per-plan limits - Return rate limit headers - Add retry-after on 429",
              "score": null
            }
          ],
          "latency_ms": 3.7204979998932686,
          "query_id": "api_044",
          "dimension": "original",
          "query": "How do I get billing usage?"
        },
        {
          "key_facts": [
            "GET /billing/usage",
            "executions",
            "compute_minutes"
          ],
          "found_facts": [
            "GET /billing/usage",
            "executions",
            "compute_minutes"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "api_billing"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "api_billing",
              "chunk_id": "api_billing_fix_0",
              "content": "# Billing API Reference ## Overview The Billing API provides programmatic access to billing functionality. Base URL: `https://api.cloudflow.io/v1/billing` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /billing/usage Get current billing period usage. **Parameters:** - `breakdown` (boolean): Include detailed breakdown by resource **Response:** - Success: `200` - Returns usage summary - Error: `400` - Bad Request **Example:** ```json {\"period\": \"2024-01\", \"executions\": 15000, \"compute_minutes\": 2500, \"storage_gb\": 50} ``` ### GET /billing/invoices List past invoices. **Parameters:** - `year` (integer): Filter by year **Response:** - Success: `200` - Returns array of invoice objects - Error: `400` - Bad Request **Example:** ```json {\"invoices\": [{\"id\": \"inv_202401\", \"amount\": 299.00, \"status\": \"paid\", \"date\": \"2024-01-01\"}]} ```",
              "score": null
            },
            {
              "doc_id": "api_metrics",
              "chunk_id": "api_metrics_fix_0",
              "content": "# Metrics API Reference ## Overview The Metrics API provides programmatic access to metrics functionality. Base URL: `https://api.cloudflow.io/v1/metrics` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /metrics/workflows Get workflow performance metrics. **Parameters:** - `period` (string): Time period: hour, day, week, month - `workflow_id` (string): Filter by workflow ID **Response:** - Success: `200` - Returns metrics data - Error: `400` - Bad Request **Example:** ```json {\"success_rate\": 0.985, \"avg_duration_ms\": 1250, \"p99_duration_ms\": 5000, \"total_executions\": 10000} ```",
              "score": null
            },
            {
              "doc_id": "api_audit",
              "chunk_id": "api_audit_fix_0",
              "content": "# Audit API Reference ## Overview The Audit API provides programmatic access to audit functionality. Base URL: `https://api.cloudflow.io/v1/audit` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /audit/logs Retrieve audit logs for compliance. **Parameters:** - `start_date` (date): Start date for log range - `end_date` (date): End date for log range - `actor` (string): Filter by user ID **Response:** - Success: `200` - Returns array of audit log entries - Error: `400` - Bad Request **Example:** ```json {\"logs\": [{\"timestamp\": \"2024-01-15T10:00:00Z\", \"actor\": \"usr_001\", \"action\": \"workflow.created\", \"resource\": \"wf_123\"}]} ```",
              "score": null
            },
            {
              "doc_id": "arch_monitoring_stack",
              "chunk_id": "arch_monitoring_stack_fix_0",
              "content": "# Monitoring Stack Architecture ## Overview The Monitoring Stack provides observability into system health, performance, and errors. Includes metrics, logs, traces, and alerting. ## Design Principles - **Observability**: Every component emits metrics and traces - **Actionable Alerts**: Alerts include runbook links ## Components ### Metrics Collects time-series metrics. Retention is 15 days local, 1 year in object storage. **Technology:** Prometheus with Thanos **Scaling:** Federated with long-term storage ### Logging Centralized log aggregation. Supports structured JSON logs with label indexing. **Technology:** Loki with Grafana **Scaling:** Distributed with sharding ### Tracing Distributed request tracing. Trace retention is 7 days. **Technology:** Jaeger with OpenTelemetry **Scaling:** Sampling at 10% ### Alerting Routes alerts to on-call engineers. Supports escalation policies and silencing. **Technology:** Alertmanager with PagerDuty **Scaling:** HA pair ## Data Flow Application \u2192 Collector \u2192 Storage \u2192 Query \u2192 Dashboard/Alert ## Performance Characteristics - **Latency P50:** N/A - **Latency P99:** N/A - **Throughput:** 1M metrics/second, 100GB logs/day",
              "score": null
            },
            {
              "doc_id": "adr_007",
              "chunk_id": "adr_007_fix_0",
              "content": "# ADR-007: Implement Rate Limiting at API Gateway **Status:** Accepted **Date:** 2024-09-03 ## Context We need to protect our services from abuse and ensure fair resource usage across customers. ## Options Considered ### Option 1: Sliding window at gateway Rate limit at API gateway level. **Pros:** - Centralized enforcement - Low latency - Easy configuration **Cons:** - Gateway becomes bottleneck - Coarse granularity - Redis dependency ### Option 2: Per-service limiting Each service implements its own limits. **Pros:** - Fine-grained control - Service autonomy - No single point of failure **Cons:** - Inconsistent implementation - Higher complexity - Distributed state ### Option 3: Token bucket Token-based rate limiting. **Pros:** - Allows bursts - Smooth rate limiting - Well understood **Cons:** - More complex - Harder to explain to users - State management ## Decision We will implement sliding window rate limiting at the Kong API Gateway using Redis. ## Consequences - Default limit: 100 requests per minute - Implement per-plan limits - Return rate limit headers - Add retry-after on 429",
              "score": null
            }
          ],
          "latency_ms": 4.768258999320096,
          "query_id": "api_044",
          "dimension": "synonym",
          "query": "I need to pull our consumption metrics from the billing API"
        },
        {
          "key_facts": [
            "GET /billing/usage",
            "executions",
            "compute_minutes"
          ],
          "found_facts": [
            "GET /billing/usage",
            "executions",
            "compute_minutes"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "api_billing"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "api_metrics",
              "chunk_id": "api_metrics_fix_0",
              "content": "# Metrics API Reference ## Overview The Metrics API provides programmatic access to metrics functionality. Base URL: `https://api.cloudflow.io/v1/metrics` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /metrics/workflows Get workflow performance metrics. **Parameters:** - `period` (string): Time period: hour, day, week, month - `workflow_id` (string): Filter by workflow ID **Response:** - Success: `200` - Returns metrics data - Error: `400` - Bad Request **Example:** ```json {\"success_rate\": 0.985, \"avg_duration_ms\": 1250, \"p99_duration_ms\": 5000, \"total_executions\": 10000} ```",
              "score": null
            },
            {
              "doc_id": "api_billing",
              "chunk_id": "api_billing_fix_0",
              "content": "# Billing API Reference ## Overview The Billing API provides programmatic access to billing functionality. Base URL: `https://api.cloudflow.io/v1/billing` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /billing/usage Get current billing period usage. **Parameters:** - `breakdown` (boolean): Include detailed breakdown by resource **Response:** - Success: `200` - Returns usage summary - Error: `400` - Bad Request **Example:** ```json {\"period\": \"2024-01\", \"executions\": 15000, \"compute_minutes\": 2500, \"storage_gb\": 50} ``` ### GET /billing/invoices List past invoices. **Parameters:** - `year` (integer): Filter by year **Response:** - Success: `200` - Returns array of invoice objects - Error: `400` - Bad Request **Example:** ```json {\"invoices\": [{\"id\": \"inv_202401\", \"amount\": 299.00, \"status\": \"paid\", \"date\": \"2024-01-01\"}]} ```",
              "score": null
            },
            {
              "doc_id": "api_audit",
              "chunk_id": "api_audit_fix_0",
              "content": "# Audit API Reference ## Overview The Audit API provides programmatic access to audit functionality. Base URL: `https://api.cloudflow.io/v1/audit` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /audit/logs Retrieve audit logs for compliance. **Parameters:** - `start_date` (date): Start date for log range - `end_date` (date): End date for log range - `actor` (string): Filter by user ID **Response:** - Success: `200` - Returns array of audit log entries - Error: `400` - Bad Request **Example:** ```json {\"logs\": [{\"timestamp\": \"2024-01-15T10:00:00Z\", \"actor\": \"usr_001\", \"action\": \"workflow.created\", \"resource\": \"wf_123\"}]} ```",
              "score": null
            },
            {
              "doc_id": "arch_monitoring_stack",
              "chunk_id": "arch_monitoring_stack_fix_0",
              "content": "# Monitoring Stack Architecture ## Overview The Monitoring Stack provides observability into system health, performance, and errors. Includes metrics, logs, traces, and alerting. ## Design Principles - **Observability**: Every component emits metrics and traces - **Actionable Alerts**: Alerts include runbook links ## Components ### Metrics Collects time-series metrics. Retention is 15 days local, 1 year in object storage. **Technology:** Prometheus with Thanos **Scaling:** Federated with long-term storage ### Logging Centralized log aggregation. Supports structured JSON logs with label indexing. **Technology:** Loki with Grafana **Scaling:** Distributed with sharding ### Tracing Distributed request tracing. Trace retention is 7 days. **Technology:** Jaeger with OpenTelemetry **Scaling:** Sampling at 10% ### Alerting Routes alerts to on-call engineers. Supports escalation policies and silencing. **Technology:** Alertmanager with PagerDuty **Scaling:** HA pair ## Data Flow Application \u2192 Collector \u2192 Storage \u2192 Query \u2192 Dashboard/Alert ## Performance Characteristics - **Latency P50:** N/A - **Latency P99:** N/A - **Throughput:** 1M metrics/second, 100GB logs/day",
              "score": null
            },
            {
              "doc_id": "api_workflows",
              "chunk_id": "api_workflows_fix_0",
              "content": "# Workflows API Reference ## Overview The Workflows API provides programmatic access to workflows functionality. Base URL: `https://api.cloudflow.io/v1/workflows` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /workflows List all workflows for the authenticated user. **Parameters:** - `limit` (integer): Maximum number of results (default: 50, max: 200) - `status` (string): Filter by status: active, paused, archived **Response:** - Success: `200` - Returns array of workflow objects - Error: `400` - Bad Request **Example:** ```json {\"workflows\": [{\"id\": \"wf_123\", \"name\": \"Daily Report\", \"status\": \"active\"}]} ``` ### POST /workflows Create a new workflow. **Parameters:** - `name` (string): Workflow name (required) - `definition` (object): Workflow definition in JSON format **Response:** - Success: `201` - Workflow created successfully - Error: `400` - Bad Request **Example:** ```json {\"id\": \"wf_456\", \"name\": \"New Workflow\", \"created_at\": \"2024-01-15T10:00:00Z\"} ``` ### DELETE /workflows/{id} Delete a workflow permanently. **Parameters:** - `id` (string): Workflow ID (required) **Response:** - Success: `204` - Workflow deleted - Error: `404` - Workflow not found **Example:** ```json {} ```",
              "score": null
            }
          ],
          "latency_ms": 3.7506240005313884,
          "query_id": "api_044",
          "dimension": "problem",
          "query": "need to track our API costs, where can I find usage data via API"
        },
        {
          "key_facts": [
            "GET /billing/usage",
            "executions",
            "compute_minutes"
          ],
          "found_facts": [
            "GET /billing/usage",
            "executions",
            "compute_minutes"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "api_billing"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "api_billing",
              "chunk_id": "api_billing_fix_0",
              "content": "# Billing API Reference ## Overview The Billing API provides programmatic access to billing functionality. Base URL: `https://api.cloudflow.io/v1/billing` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /billing/usage Get current billing period usage. **Parameters:** - `breakdown` (boolean): Include detailed breakdown by resource **Response:** - Success: `200` - Returns usage summary - Error: `400` - Bad Request **Example:** ```json {\"period\": \"2024-01\", \"executions\": 15000, \"compute_minutes\": 2500, \"storage_gb\": 50} ``` ### GET /billing/invoices List past invoices. **Parameters:** - `year` (integer): Filter by year **Response:** - Success: `200` - Returns array of invoice objects - Error: `400` - Bad Request **Example:** ```json {\"invoices\": [{\"id\": \"inv_202401\", \"amount\": 299.00, \"status\": \"paid\", \"date\": \"2024-01-01\"}]} ```",
              "score": null
            },
            {
              "doc_id": "arch_authentication_service",
              "chunk_id": "arch_authentication_service_fix_0",
              "content": "# Authentication Service Architecture ## Overview The Authentication Service handles user identity, session management, and token issuance. Supports OAuth 2.0, SAML, and API key authentication. ## Design Principles - **Zero Knowledge**: Passwords never stored in plaintext - **Token Rotation**: Refresh tokens rotate on each use ## Components ### Identity Provider Manages user accounts and credentials. Password hashing uses bcrypt with cost factor 12. **Technology:** Custom Go service **Scaling:** Horizontal stateless ### Token Service Issues and validates access tokens. Token lifetime is 1 hour with 7-day refresh tokens. **Technology:** JWT with RS256 **Scaling:** Horizontal stateless ### SSO Gateway Integrates with external identity providers. Supports Google, Microsoft, Okta, and custom SAML. **Technology:** Dex OIDC proxy **Scaling:** Active-passive ## Data Flow Credentials \u2192 Validation \u2192 Token Issue \u2192 Token Refresh \u2192 Logout ## Performance Characteristics - **Latency P50:** 50ms - **Latency P99:** 200ms - **Throughput:** 5000 auth/second",
              "score": null
            },
            {
              "doc_id": "api_metrics",
              "chunk_id": "api_metrics_fix_0",
              "content": "# Metrics API Reference ## Overview The Metrics API provides programmatic access to metrics functionality. Base URL: `https://api.cloudflow.io/v1/metrics` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /metrics/workflows Get workflow performance metrics. **Parameters:** - `period` (string): Time period: hour, day, week, month - `workflow_id` (string): Filter by workflow ID **Response:** - Success: `200` - Returns metrics data - Error: `400` - Bad Request **Example:** ```json {\"success_rate\": 0.985, \"avg_duration_ms\": 1250, \"p99_duration_ms\": 5000, \"total_executions\": 10000} ```",
              "score": null
            },
            {
              "doc_id": "arch_api_gateway",
              "chunk_id": "arch_api_gateway_fix_0",
              "content": "# API Gateway Architecture ## Overview The API Gateway handles all incoming HTTP requests, providing authentication, rate limiting, and request routing. Built on Kong with custom plugins for CloudFlow-specific functionality. ## Design Principles - **Zero Trust**: Every request is authenticated and authorized - **Defense in Depth**: Multiple layers of security validation ## Components ### Kong Gateway Primary ingress controller. Handles TLS termination, request routing, and plugin execution. **Technology:** Kong 3.4 on Kubernetes **Scaling:** Horizontal with HPA ### Auth Plugin Validates JWT tokens, API keys, and OAuth2 flows. Token validation latency under 5ms. **Technology:** Custom Lua plugin **Scaling:** Stateless ### Rate Limiter Enforces per-user and per-IP rate limits. Default limit is 100 requests per minute. **Technology:** Redis-backed sliding window **Scaling:** Shared Redis cluster ## Data Flow Client \u2192 TLS \u2192 Kong \u2192 Auth \u2192 Rate Limit \u2192 Backend Services ## Performance Characteristics - **Latency P50:** 12ms - **Latency P99:** 45ms - **Throughput:** 50000 requests/second",
              "score": null
            },
            {
              "doc_id": "adr_007",
              "chunk_id": "adr_007_fix_0",
              "content": "# ADR-007: Implement Rate Limiting at API Gateway **Status:** Accepted **Date:** 2024-09-03 ## Context We need to protect our services from abuse and ensure fair resource usage across customers. ## Options Considered ### Option 1: Sliding window at gateway Rate limit at API gateway level. **Pros:** - Centralized enforcement - Low latency - Easy configuration **Cons:** - Gateway becomes bottleneck - Coarse granularity - Redis dependency ### Option 2: Per-service limiting Each service implements its own limits. **Pros:** - Fine-grained control - Service autonomy - No single point of failure **Cons:** - Inconsistent implementation - Higher complexity - Distributed state ### Option 3: Token bucket Token-based rate limiting. **Pros:** - Allows bursts - Smooth rate limiting - Well understood **Cons:** - More complex - Harder to explain to users - State management ## Decision We will implement sliding window rate limiting at the Kong API Gateway using Redis. ## Consequences - Default limit: 100 requests per minute - Implement per-plan limits - Return rate limit headers - Add retry-after on 429",
              "score": null
            }
          ],
          "latency_ms": 3.365286998814554,
          "query_id": "api_044",
          "dimension": "casual",
          "query": "billing usage endpoint"
        },
        {
          "key_facts": [
            "GET /billing/usage",
            "executions",
            "compute_minutes"
          ],
          "found_facts": [
            "GET /billing/usage",
            "executions",
            "compute_minutes"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "api_billing"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "api_billing",
              "chunk_id": "api_billing_fix_0",
              "content": "# Billing API Reference ## Overview The Billing API provides programmatic access to billing functionality. Base URL: `https://api.cloudflow.io/v1/billing` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /billing/usage Get current billing period usage. **Parameters:** - `breakdown` (boolean): Include detailed breakdown by resource **Response:** - Success: `200` - Returns usage summary - Error: `400` - Bad Request **Example:** ```json {\"period\": \"2024-01\", \"executions\": 15000, \"compute_minutes\": 2500, \"storage_gb\": 50} ``` ### GET /billing/invoices List past invoices. **Parameters:** - `year` (integer): Filter by year **Response:** - Success: `200` - Returns array of invoice objects - Error: `400` - Bad Request **Example:** ```json {\"invoices\": [{\"id\": \"inv_202401\", \"amount\": 299.00, \"status\": \"paid\", \"date\": \"2024-01-01\"}]} ```",
              "score": null
            },
            {
              "doc_id": "api_metrics",
              "chunk_id": "api_metrics_fix_0",
              "content": "# Metrics API Reference ## Overview The Metrics API provides programmatic access to metrics functionality. Base URL: `https://api.cloudflow.io/v1/metrics` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /metrics/workflows Get workflow performance metrics. **Parameters:** - `period` (string): Time period: hour, day, week, month - `workflow_id` (string): Filter by workflow ID **Response:** - Success: `200` - Returns metrics data - Error: `400` - Bad Request **Example:** ```json {\"success_rate\": 0.985, \"avg_duration_ms\": 1250, \"p99_duration_ms\": 5000, \"total_executions\": 10000} ```",
              "score": null
            },
            {
              "doc_id": "adr_007",
              "chunk_id": "adr_007_fix_0",
              "content": "# ADR-007: Implement Rate Limiting at API Gateway **Status:** Accepted **Date:** 2024-09-03 ## Context We need to protect our services from abuse and ensure fair resource usage across customers. ## Options Considered ### Option 1: Sliding window at gateway Rate limit at API gateway level. **Pros:** - Centralized enforcement - Low latency - Easy configuration **Cons:** - Gateway becomes bottleneck - Coarse granularity - Redis dependency ### Option 2: Per-service limiting Each service implements its own limits. **Pros:** - Fine-grained control - Service autonomy - No single point of failure **Cons:** - Inconsistent implementation - Higher complexity - Distributed state ### Option 3: Token bucket Token-based rate limiting. **Pros:** - Allows bursts - Smooth rate limiting - Well understood **Cons:** - More complex - Harder to explain to users - State management ## Decision We will implement sliding window rate limiting at the Kong API Gateway using Redis. ## Consequences - Default limit: 100 requests per minute - Implement per-plan limits - Return rate limit headers - Add retry-after on 429",
              "score": null
            },
            {
              "doc_id": "config_api_gateway",
              "chunk_id": "config_api_gateway_fix_0",
              "content": "# API Gateway Configuration Reference ## Overview This document describes all configuration options for API Gateway. ## Environment Variables | Variable | Type | Default | Description | |----------|------|---------|-------------| | `GATEWAY_PORT` | integer | 8000 | Gateway listen port | | `GATEWAY_RATE_LIMIT` | integer | 100 | Requests per minute per user | | `GATEWAY_TIMEOUT_MS` | integer | 30000 | Request timeout in milliseconds | | `GATEWAY_MAX_BODY_SIZE` | string | 10MB | Maximum request body size | | `GATEWAY_CORS_ORIGINS` | string | * | Allowed CORS origins | | `GATEWAY_SSL_ENABLED` | boolean | true | Enable TLS termination | ## Configuration File Configuration can also be provided via `api gateway.yaml`: ```yaml gateway_port: 8000 gateway_rate_limit: 100 gateway_timeout_ms: 30000 gateway_max_body_size: 10MB gateway_cors_origins: * ``` ## Validation Configuration is validated at startup. Invalid configuration will prevent the service from starting.",
              "score": null
            },
            {
              "doc_id": "adr_004",
              "chunk_id": "adr_004_fix_0",
              "content": "# ADR-004: Use Redis for Caching and Session Storage **Status:** Accepted **Date:** 2024-04-05 ## Context We need a fast data store for caching API responses, session data, and rate limiting counters. ## Options Considered ### Option 1: Redis Cluster In-memory data structure store. **Pros:** - Sub-millisecond latency - Rich data structures - Pub/sub support - Clustering **Cons:** - Memory cost - Persistence complexity - Cluster management ### Option 2: Memcached Simple key-value cache. **Pros:** - Simple operation - Multi-threaded - Lower memory overhead **Cons:** - No persistence - Limited data types - No clustering ### Option 3: Application-level cache In-process caching only. **Pros:** - No external dependency - Fastest possible - Simple **Cons:** - Not shared across pods - Memory pressure - Cold starts ## Decision We will use Redis Cluster for caching with a 6-node cluster for high availability. ## Consequences - Implement cache-aside pattern - Set appropriate TTLs - Monitor memory usage - Plan for cache stampede prevention",
              "score": null
            }
          ],
          "latency_ms": 4.108901999643422,
          "query_id": "api_044",
          "dimension": "contextual",
          "query": "building an internal cost dashboard, need to fetch monthly usage via API"
        },
        {
          "key_facts": [
            "GET /billing/usage",
            "executions",
            "compute_minutes"
          ],
          "found_facts": [
            "GET /billing/usage",
            "executions",
            "compute_minutes"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "api_billing"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "api_metrics",
              "chunk_id": "api_metrics_fix_0",
              "content": "# Metrics API Reference ## Overview The Metrics API provides programmatic access to metrics functionality. Base URL: `https://api.cloudflow.io/v1/metrics` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /metrics/workflows Get workflow performance metrics. **Parameters:** - `period` (string): Time period: hour, day, week, month - `workflow_id` (string): Filter by workflow ID **Response:** - Success: `200` - Returns metrics data - Error: `400` - Bad Request **Example:** ```json {\"success_rate\": 0.985, \"avg_duration_ms\": 1250, \"p99_duration_ms\": 5000, \"total_executions\": 10000} ```",
              "score": null
            },
            {
              "doc_id": "api_billing",
              "chunk_id": "api_billing_fix_0",
              "content": "# Billing API Reference ## Overview The Billing API provides programmatic access to billing functionality. Base URL: `https://api.cloudflow.io/v1/billing` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /billing/usage Get current billing period usage. **Parameters:** - `breakdown` (boolean): Include detailed breakdown by resource **Response:** - Success: `200` - Returns usage summary - Error: `400` - Bad Request **Example:** ```json {\"period\": \"2024-01\", \"executions\": 15000, \"compute_minutes\": 2500, \"storage_gb\": 50} ``` ### GET /billing/invoices List past invoices. **Parameters:** - `year` (integer): Filter by year **Response:** - Success: `200` - Returns array of invoice objects - Error: `400` - Bad Request **Example:** ```json {\"invoices\": [{\"id\": \"inv_202401\", \"amount\": 299.00, \"status\": \"paid\", \"date\": \"2024-01-01\"}]} ```",
              "score": null
            },
            {
              "doc_id": "api_audit",
              "chunk_id": "api_audit_fix_0",
              "content": "# Audit API Reference ## Overview The Audit API provides programmatic access to audit functionality. Base URL: `https://api.cloudflow.io/v1/audit` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /audit/logs Retrieve audit logs for compliance. **Parameters:** - `start_date` (date): Start date for log range - `end_date` (date): End date for log range - `actor` (string): Filter by user ID **Response:** - Success: `200` - Returns array of audit log entries - Error: `400` - Bad Request **Example:** ```json {\"logs\": [{\"timestamp\": \"2024-01-15T10:00:00Z\", \"actor\": \"usr_001\", \"action\": \"workflow.created\", \"resource\": \"wf_123\"}]} ```",
              "score": null
            },
            {
              "doc_id": "api_workflows",
              "chunk_id": "api_workflows_fix_0",
              "content": "# Workflows API Reference ## Overview The Workflows API provides programmatic access to workflows functionality. Base URL: `https://api.cloudflow.io/v1/workflows` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /workflows List all workflows for the authenticated user. **Parameters:** - `limit` (integer): Maximum number of results (default: 50, max: 200) - `status` (string): Filter by status: active, paused, archived **Response:** - Success: `200` - Returns array of workflow objects - Error: `400` - Bad Request **Example:** ```json {\"workflows\": [{\"id\": \"wf_123\", \"name\": \"Daily Report\", \"status\": \"active\"}]} ``` ### POST /workflows Create a new workflow. **Parameters:** - `name` (string): Workflow name (required) - `definition` (object): Workflow definition in JSON format **Response:** - Success: `201` - Workflow created successfully - Error: `400` - Bad Request **Example:** ```json {\"id\": \"wf_456\", \"name\": \"New Workflow\", \"created_at\": \"2024-01-15T10:00:00Z\"} ``` ### DELETE /workflows/{id} Delete a workflow permanently. **Parameters:** - `id` (string): Workflow ID (required) **Response:** - Success: `204` - Workflow deleted - Error: `404` - Workflow not found **Example:** ```json {} ```",
              "score": null
            },
            {
              "doc_id": "api_webhooks",
              "chunk_id": "api_webhooks_fix_0",
              "content": "# Webhooks API Reference ## Overview The Webhooks API provides programmatic access to webhooks functionality. Base URL: `https://api.cloudflow.io/v1/webhooks` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /webhooks List configured webhooks. **Parameters:** - `active` (boolean): Filter by active status **Response:** - Success: `200` - Returns array of webhook objects - Error: `400` - Bad Request **Example:** ```json {\"webhooks\": [{\"id\": \"wh_01\", \"url\": \"https://example.com/hook\", \"events\": [\"workflow.completed\"]}]} ``` ### POST /webhooks Create a new webhook endpoint. **Parameters:** - `url` (string): Webhook URL (must be HTTPS) - `events` (array): Events to subscribe to - `secret` (string): Signing secret for verification **Response:** - Success: `201` - Webhook created - Error: `400` - Bad Request **Example:** ```json {\"id\": \"wh_02\", \"url\": \"https://example.com/new-hook\", \"secret\": \"whsec_...\"} ```",
              "score": null
            }
          ],
          "latency_ms": 3.7629179987561656,
          "query_id": "api_044",
          "dimension": "negation",
          "query": "why can't I find our usage metrics in the API response"
        },
        {
          "key_facts": [
            "PUT /secrets/{name}",
            "DELETE /secrets/{name}",
            "encrypted at rest"
          ],
          "found_facts": [
            "PUT /secrets/{name}",
            "DELETE /secrets/{name}",
            "encrypted at rest"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "api_secrets"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "api_secrets",
              "chunk_id": "api_secrets_fix_0",
              "content": "# Secrets API Reference ## Overview The Secrets API provides programmatic access to secrets functionality. Base URL: `https://api.cloudflow.io/v1/secrets` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /secrets List secret names (values not returned). **Parameters:** **Response:** - Success: `200` - Returns array of secret metadata - Error: `400` - Bad Request **Example:** ```json {\"secrets\": [{\"name\": \"DATABASE_URL\", \"created_at\": \"2024-01-01\", \"updated_at\": \"2024-01-10\"}]} ``` ### PUT /secrets/{name} Create or update a secret. **Parameters:** - `value` (string): Secret value (encrypted at rest) **Response:** - Success: `200` - Secret saved - Error: `400` - Bad Request **Example:** ```json {\"name\": \"DATABASE_URL\", \"updated_at\": \"2024-01-15T10:00:00Z\"} ``` ### DELETE /secrets/{name} Delete a secret. **Parameters:** **Response:** - Success: `204` - Secret deleted - Error: `400` - Bad Request **Example:** ```json {} ```",
              "score": null
            },
            {
              "doc_id": "howto_set_up_data_encryption",
              "chunk_id": "howto_set_up_data_encryption_fix_0",
              "content": "# How to Set Up Data Encryption ## Prerequisites - CloudFlow Enterprise plan ## Steps ### Step 1: Enable encryption at rest All data is encrypted at rest using AES-256 by default. > **Note:** Encryption is automatic for all plans ### Step 2: Configure KMS Bring your own encryption keys using AWS KMS or GCP KMS. ```bash cloudflow encryption configure --kms-key arn:aws:kms:... ``` ### Step 3: Enable field-level encryption Encrypt sensitive fields within your workflow data. ```bash cloudflow encryption add-field --path $.user.ssn --key-alias sensitive-data ``` ### Step 4: Verify encryption Confirm encryption is active for your workspace. ```bash cloudflow encryption status ``` ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            },
            {
              "doc_id": "howto_set_up_ci_cd_pipeline",
              "chunk_id": "howto_set_up_ci_cd_pipeline_fix_0",
              "content": "# How to Set Up CI/CD Pipeline ## Prerequisites - GitHub account - CloudFlow API key - Repository admin access ## Steps ### Step 1: Create GitHub Actions workflow Create a new file at `.github/workflows/cloudflow.yml` with the deployment configuration. ```bash mkdir -p .github/workflows && touch .github/workflows/cloudflow.yml ``` ### Step 2: Configure secrets Add your CloudFlow API key to GitHub secrets. ```bash gh secret set CLOUDFLOW_API_KEY ``` ### Step 3: Add deployment step Add the CloudFlow deployment action to your workflow. > **Note:** Use the official cloudflow/deploy-action@v2 ### Step 4: Test the pipeline Push a commit to trigger the workflow. ```bash git push origin main ``` ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            },
            {
              "doc_id": "howto_create_api_integration",
              "chunk_id": "howto_create_api_integration_fix_0",
              "content": "# How to Create API Integration ## Prerequisites - CloudFlow account ## Steps ### Step 1: Generate API key Create an API key for programmatic access. ```bash cloudflow apikeys create --name 'CI Integration' --scopes workflows:read,workflows:execute ``` ### Step 2: Store key securely Never commit API keys to source control. Use environment variables or secret management. > **Note:** Rotate keys every 90 days ### Step 3: Test authentication Verify the API key works correctly. ```bash curl -H 'Authorization: Bearer cf_...' https://api.cloudflow.io/v1/me ``` ### Step 4: Implement rate limiting Handle rate limit responses (HTTP 429) with exponential backoff. > **Note:** Default limit is 100 requests per minute ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            },
            {
              "doc_id": "config_cache",
              "chunk_id": "config_cache_fix_0",
              "content": "# Cache Configuration Reference ## Overview This document describes all configuration options for Cache. ## Environment Variables | Variable | Type | Default | Description | |----------|------|---------|-------------| | `REDIS_URL` | string | required | Redis connection string | | `REDIS_POOL_SIZE` | integer | 10 | Connection pool size | | `CACHE_TTL_SECONDS` | integer | 300 | Default cache TTL | | `CACHE_PREFIX` | string | cf: | Key prefix for namespacing | | `CACHE_COMPRESSION` | boolean | true | Enable value compression | | `CACHE_MAX_SIZE_MB` | integer | 100 | Maximum cache size per pod | ## Configuration File Configuration can also be provided via `cache.yaml`: ```yaml redis_url: required redis_pool_size: 10 cache_ttl_seconds: 300 cache_prefix: cf: cache_compression: true ``` ## Validation Configuration is validated at startup. Invalid configuration will prevent the service from starting.",
              "score": null
            }
          ],
          "latency_ms": 4.034712999782641,
          "query_id": "api_045",
          "dimension": "original",
          "query": "How do I manage secrets?"
        },
        {
          "key_facts": [
            "PUT /secrets/{name}",
            "DELETE /secrets/{name}",
            "encrypted at rest"
          ],
          "found_facts": [
            "PUT /secrets/{name}",
            "DELETE /secrets/{name}",
            "encrypted at rest"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "api_secrets"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "api_secrets",
              "chunk_id": "api_secrets_fix_0",
              "content": "# Secrets API Reference ## Overview The Secrets API provides programmatic access to secrets functionality. Base URL: `https://api.cloudflow.io/v1/secrets` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /secrets List secret names (values not returned). **Parameters:** **Response:** - Success: `200` - Returns array of secret metadata - Error: `400` - Bad Request **Example:** ```json {\"secrets\": [{\"name\": \"DATABASE_URL\", \"created_at\": \"2024-01-01\", \"updated_at\": \"2024-01-10\"}]} ``` ### PUT /secrets/{name} Create or update a secret. **Parameters:** - `value` (string): Secret value (encrypted at rest) **Response:** - Success: `200` - Secret saved - Error: `400` - Bad Request **Example:** ```json {\"name\": \"DATABASE_URL\", \"updated_at\": \"2024-01-15T10:00:00Z\"} ``` ### DELETE /secrets/{name} Delete a secret. **Parameters:** **Response:** - Success: `204` - Secret deleted - Error: `400` - Bad Request **Example:** ```json {} ```",
              "score": null
            },
            {
              "doc_id": "api_users",
              "chunk_id": "api_users_fix_0",
              "content": "# Users API Reference ## Overview The Users API provides programmatic access to users functionality. Base URL: `https://api.cloudflow.io/v1/users` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /users/me Get current user profile. **Parameters:** **Response:** - Success: `200` - Returns user object - Error: `400` - Bad Request **Example:** ```json {\"id\": \"usr_001\", \"email\": \"user@example.com\", \"role\": \"admin\", \"created_at\": \"2023-06-01\"} ``` ### PATCH /users/me Update current user profile. **Parameters:** - `name` (string): Display name - `timezone` (string): Timezone (e.g., America/New_York) **Response:** - Success: `200` - User updated - Error: `400` - Bad Request **Example:** ```json {\"id\": \"usr_001\", \"name\": \"John Doe\", \"timezone\": \"America/New_York\"} ```",
              "score": null
            },
            {
              "doc_id": "howto_set_up_ci_cd_pipeline",
              "chunk_id": "howto_set_up_ci_cd_pipeline_fix_0",
              "content": "# How to Set Up CI/CD Pipeline ## Prerequisites - GitHub account - CloudFlow API key - Repository admin access ## Steps ### Step 1: Create GitHub Actions workflow Create a new file at `.github/workflows/cloudflow.yml` with the deployment configuration. ```bash mkdir -p .github/workflows && touch .github/workflows/cloudflow.yml ``` ### Step 2: Configure secrets Add your CloudFlow API key to GitHub secrets. ```bash gh secret set CLOUDFLOW_API_KEY ``` ### Step 3: Add deployment step Add the CloudFlow deployment action to your workflow. > **Note:** Use the official cloudflow/deploy-action@v2 ### Step 4: Test the pipeline Push a commit to trigger the workflow. ```bash git push origin main ``` ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            },
            {
              "doc_id": "howto_create_api_integration",
              "chunk_id": "howto_create_api_integration_fix_0",
              "content": "# How to Create API Integration ## Prerequisites - CloudFlow account ## Steps ### Step 1: Generate API key Create an API key for programmatic access. ```bash cloudflow apikeys create --name 'CI Integration' --scopes workflows:read,workflows:execute ``` ### Step 2: Store key securely Never commit API keys to source control. Use environment variables or secret management. > **Note:** Rotate keys every 90 days ### Step 3: Test authentication Verify the API key works correctly. ```bash curl -H 'Authorization: Bearer cf_...' https://api.cloudflow.io/v1/me ``` ### Step 4: Implement rate limiting Handle rate limit responses (HTTP 429) with exponential backoff. > **Note:** Default limit is 100 requests per minute ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            },
            {
              "doc_id": "howto_migrate_from_v1_to_v2_api",
              "chunk_id": "howto_migrate_from_v1_to_v2_api_fix_0",
              "content": "# How to Migrate from v1 to v2 API ## Prerequisites - Existing v1 integration - v2 API access enabled ## Steps ### Step 1: Review breaking changes The v2 API changes authentication from API keys to OAuth 2.0. Review the migration guide. ### Step 2: Update client library Upgrade to the latest SDK version. ```bash npm install @cloudflow/sdk@latest ``` ### Step 3: Update authentication Replace API key authentication with OAuth 2.0 client credentials flow. > **Note:** Generate new credentials in the dashboard ### Step 4: Test in staging Run your integration tests against the v2 staging endpoint. ```bash CLOUDFLOW_API_URL=https://api-v2-staging.cloudflow.io npm test ``` ### Step 5: Switch production Update production configuration to use v2 API. ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            }
          ],
          "latency_ms": 3.6946710006304784,
          "query_id": "api_045",
          "dimension": "synonym",
          "query": "I need to store and update environment variables via the secrets API"
        },
        {
          "key_facts": [
            "PUT /secrets/{name}",
            "DELETE /secrets/{name}",
            "encrypted at rest"
          ],
          "found_facts": [
            "PUT /secrets/{name}",
            "DELETE /secrets/{name}",
            "encrypted at rest"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "api_secrets"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "api_secrets",
              "chunk_id": "api_secrets_fix_0",
              "content": "# Secrets API Reference ## Overview The Secrets API provides programmatic access to secrets functionality. Base URL: `https://api.cloudflow.io/v1/secrets` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /secrets List secret names (values not returned). **Parameters:** **Response:** - Success: `200` - Returns array of secret metadata - Error: `400` - Bad Request **Example:** ```json {\"secrets\": [{\"name\": \"DATABASE_URL\", \"created_at\": \"2024-01-01\", \"updated_at\": \"2024-01-10\"}]} ``` ### PUT /secrets/{name} Create or update a secret. **Parameters:** - `value` (string): Secret value (encrypted at rest) **Response:** - Success: `200` - Secret saved - Error: `400` - Bad Request **Example:** ```json {\"name\": \"DATABASE_URL\", \"updated_at\": \"2024-01-15T10:00:00Z\"} ``` ### DELETE /secrets/{name} Delete a secret. **Parameters:** **Response:** - Success: `204` - Secret deleted - Error: `400` - Bad Request **Example:** ```json {} ```",
              "score": null
            },
            {
              "doc_id": "arch_authentication_service",
              "chunk_id": "arch_authentication_service_fix_0",
              "content": "# Authentication Service Architecture ## Overview The Authentication Service handles user identity, session management, and token issuance. Supports OAuth 2.0, SAML, and API key authentication. ## Design Principles - **Zero Knowledge**: Passwords never stored in plaintext - **Token Rotation**: Refresh tokens rotate on each use ## Components ### Identity Provider Manages user accounts and credentials. Password hashing uses bcrypt with cost factor 12. **Technology:** Custom Go service **Scaling:** Horizontal stateless ### Token Service Issues and validates access tokens. Token lifetime is 1 hour with 7-day refresh tokens. **Technology:** JWT with RS256 **Scaling:** Horizontal stateless ### SSO Gateway Integrates with external identity providers. Supports Google, Microsoft, Okta, and custom SAML. **Technology:** Dex OIDC proxy **Scaling:** Active-passive ## Data Flow Credentials \u2192 Validation \u2192 Token Issue \u2192 Token Refresh \u2192 Logout ## Performance Characteristics - **Latency P50:** 50ms - **Latency P99:** 200ms - **Throughput:** 5000 auth/second",
              "score": null
            },
            {
              "doc_id": "howto_create_api_integration",
              "chunk_id": "howto_create_api_integration_fix_0",
              "content": "# How to Create API Integration ## Prerequisites - CloudFlow account ## Steps ### Step 1: Generate API key Create an API key for programmatic access. ```bash cloudflow apikeys create --name 'CI Integration' --scopes workflows:read,workflows:execute ``` ### Step 2: Store key securely Never commit API keys to source control. Use environment variables or secret management. > **Note:** Rotate keys every 90 days ### Step 3: Test authentication Verify the API key works correctly. ```bash curl -H 'Authorization: Bearer cf_...' https://api.cloudflow.io/v1/me ``` ### Step 4: Implement rate limiting Handle rate limit responses (HTTP 429) with exponential backoff. > **Note:** Default limit is 100 requests per minute ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            },
            {
              "doc_id": "config_database",
              "chunk_id": "config_database_fix_0",
              "content": "# Database Configuration Reference ## Overview This document describes all configuration options for Database. ## Environment Variables | Variable | Type | Default | Description | |----------|------|---------|-------------| | `DATABASE_URL` | string | required | PostgreSQL connection string | | `DATABASE_POOL_MIN` | integer | 5 | Minimum connection pool size | | `DATABASE_POOL_MAX` | integer | 20 | Maximum connection pool size | | `DATABASE_STATEMENT_TIMEOUT` | integer | 30000 | Query timeout in milliseconds | | `DATABASE_SSL_MODE` | string | require | SSL mode: disable, require, verify-full | | `DATABASE_MIGRATION_AUTO` | boolean | false | Run migrations on startup | ## Configuration File Configuration can also be provided via `database.yaml`: ```yaml database_url: required database_pool_min: 5 database_pool_max: 20 database_statement_timeout: 30000 database_ssl_mode: require ``` ## Validation Configuration is validated at startup. Invalid configuration will prevent the service from starting.",
              "score": null
            },
            {
              "doc_id": "api_users",
              "chunk_id": "api_users_fix_0",
              "content": "# Users API Reference ## Overview The Users API provides programmatic access to users functionality. Base URL: `https://api.cloudflow.io/v1/users` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /users/me Get current user profile. **Parameters:** **Response:** - Success: `200` - Returns user object - Error: `400` - Bad Request **Example:** ```json {\"id\": \"usr_001\", \"email\": \"user@example.com\", \"role\": \"admin\", \"created_at\": \"2023-06-01\"} ``` ### PATCH /users/me Update current user profile. **Parameters:** - `name` (string): Display name - `timezone` (string): Timezone (e.g., America/New_York) **Response:** - Success: `200` - User updated - Error: `400` - Bad Request **Example:** ```json {\"id\": \"usr_001\", \"name\": \"John Doe\", \"timezone\": \"America/New_York\"} ```",
              "score": null
            }
          ],
          "latency_ms": 3.6643639996327693,
          "query_id": "api_045",
          "dimension": "problem",
          "query": "need to rotate database credentials, what's the API for updating secrets"
        },
        {
          "key_facts": [
            "PUT /secrets/{name}",
            "DELETE /secrets/{name}",
            "encrypted at rest"
          ],
          "found_facts": [
            "PUT /secrets/{name}",
            "DELETE /secrets/{name}",
            "encrypted at rest"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "api_secrets"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "api_secrets",
              "chunk_id": "api_secrets_fix_0",
              "content": "# Secrets API Reference ## Overview The Secrets API provides programmatic access to secrets functionality. Base URL: `https://api.cloudflow.io/v1/secrets` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /secrets List secret names (values not returned). **Parameters:** **Response:** - Success: `200` - Returns array of secret metadata - Error: `400` - Bad Request **Example:** ```json {\"secrets\": [{\"name\": \"DATABASE_URL\", \"created_at\": \"2024-01-01\", \"updated_at\": \"2024-01-10\"}]} ``` ### PUT /secrets/{name} Create or update a secret. **Parameters:** - `value` (string): Secret value (encrypted at rest) **Response:** - Success: `200` - Secret saved - Error: `400` - Bad Request **Example:** ```json {\"name\": \"DATABASE_URL\", \"updated_at\": \"2024-01-15T10:00:00Z\"} ``` ### DELETE /secrets/{name} Delete a secret. **Parameters:** **Response:** - Success: `204` - Secret deleted - Error: `400` - Bad Request **Example:** ```json {} ```",
              "score": null
            },
            {
              "doc_id": "arch_authentication_service",
              "chunk_id": "arch_authentication_service_fix_0",
              "content": "# Authentication Service Architecture ## Overview The Authentication Service handles user identity, session management, and token issuance. Supports OAuth 2.0, SAML, and API key authentication. ## Design Principles - **Zero Knowledge**: Passwords never stored in plaintext - **Token Rotation**: Refresh tokens rotate on each use ## Components ### Identity Provider Manages user accounts and credentials. Password hashing uses bcrypt with cost factor 12. **Technology:** Custom Go service **Scaling:** Horizontal stateless ### Token Service Issues and validates access tokens. Token lifetime is 1 hour with 7-day refresh tokens. **Technology:** JWT with RS256 **Scaling:** Horizontal stateless ### SSO Gateway Integrates with external identity providers. Supports Google, Microsoft, Okta, and custom SAML. **Technology:** Dex OIDC proxy **Scaling:** Active-passive ## Data Flow Credentials \u2192 Validation \u2192 Token Issue \u2192 Token Refresh \u2192 Logout ## Performance Characteristics - **Latency P50:** 50ms - **Latency P99:** 200ms - **Throughput:** 5000 auth/second",
              "score": null
            },
            {
              "doc_id": "adr_003",
              "chunk_id": "adr_003_fix_0",
              "content": "# ADR-003: Implement OAuth 2.0 with JWT for Authentication **Status:** Accepted **Date:** 2024-05-08 ## Context We need a secure authentication system that supports API keys, user sessions, and third-party integrations. ## Options Considered ### Option 1: OAuth 2.0 with JWT Industry standard token-based auth. **Pros:** - Industry standard - Stateless validation - Supports refresh tokens - SSO compatible **Cons:** - Token size overhead - Cannot revoke individual tokens - Complexity ### Option 2: Session-based auth Traditional server-side sessions. **Pros:** - Simple implementation - Easy revocation - Smaller payloads **Cons:** - Requires session store - Not suitable for APIs - Scaling challenges ### Option 3: API keys only Simple key-based authentication. **Pros:** - Very simple - Low overhead - Easy to implement **Cons:** - No user context - Hard to rotate - No SSO support ## Decision We will implement OAuth 2.0 with JWT tokens. Access tokens expire after 1 hour, refresh tokens after 7 days. ## Consequences - Implement token rotation on refresh - Support API keys for machine-to-machine auth - Add MFA for admin accounts",
              "score": null
            },
            {
              "doc_id": "arch_api_gateway",
              "chunk_id": "arch_api_gateway_fix_0",
              "content": "# API Gateway Architecture ## Overview The API Gateway handles all incoming HTTP requests, providing authentication, rate limiting, and request routing. Built on Kong with custom plugins for CloudFlow-specific functionality. ## Design Principles - **Zero Trust**: Every request is authenticated and authorized - **Defense in Depth**: Multiple layers of security validation ## Components ### Kong Gateway Primary ingress controller. Handles TLS termination, request routing, and plugin execution. **Technology:** Kong 3.4 on Kubernetes **Scaling:** Horizontal with HPA ### Auth Plugin Validates JWT tokens, API keys, and OAuth2 flows. Token validation latency under 5ms. **Technology:** Custom Lua plugin **Scaling:** Stateless ### Rate Limiter Enforces per-user and per-IP rate limits. Default limit is 100 requests per minute. **Technology:** Redis-backed sliding window **Scaling:** Shared Redis cluster ## Data Flow Client \u2192 TLS \u2192 Kong \u2192 Auth \u2192 Rate Limit \u2192 Backend Services ## Performance Characteristics - **Latency P50:** 12ms - **Latency P99:** 45ms - **Throughput:** 50000 requests/second",
              "score": null
            },
            {
              "doc_id": "config_api_gateway",
              "chunk_id": "config_api_gateway_fix_0",
              "content": "# API Gateway Configuration Reference ## Overview This document describes all configuration options for API Gateway. ## Environment Variables | Variable | Type | Default | Description | |----------|------|---------|-------------| | `GATEWAY_PORT` | integer | 8000 | Gateway listen port | | `GATEWAY_RATE_LIMIT` | integer | 100 | Requests per minute per user | | `GATEWAY_TIMEOUT_MS` | integer | 30000 | Request timeout in milliseconds | | `GATEWAY_MAX_BODY_SIZE` | string | 10MB | Maximum request body size | | `GATEWAY_CORS_ORIGINS` | string | * | Allowed CORS origins | | `GATEWAY_SSL_ENABLED` | boolean | true | Enable TLS termination | ## Configuration File Configuration can also be provided via `api gateway.yaml`: ```yaml gateway_port: 8000 gateway_rate_limit: 100 gateway_timeout_ms: 30000 gateway_max_body_size: 10MB gateway_cors_origins: * ``` ## Validation Configuration is validated at startup. Invalid configuration will prevent the service from starting.",
              "score": null
            }
          ],
          "latency_ms": 3.5042359995713923,
          "query_id": "api_045",
          "dimension": "casual",
          "query": "secrets CRUD endpoints"
        },
        {
          "key_facts": [
            "PUT /secrets/{name}",
            "DELETE /secrets/{name}",
            "encrypted at rest"
          ],
          "found_facts": [
            "PUT /secrets/{name}",
            "DELETE /secrets/{name}",
            "encrypted at rest"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "api_secrets"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "api_secrets",
              "chunk_id": "api_secrets_fix_0",
              "content": "# Secrets API Reference ## Overview The Secrets API provides programmatic access to secrets functionality. Base URL: `https://api.cloudflow.io/v1/secrets` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /secrets List secret names (values not returned). **Parameters:** **Response:** - Success: `200` - Returns array of secret metadata - Error: `400` - Bad Request **Example:** ```json {\"secrets\": [{\"name\": \"DATABASE_URL\", \"created_at\": \"2024-01-01\", \"updated_at\": \"2024-01-10\"}]} ``` ### PUT /secrets/{name} Create or update a secret. **Parameters:** - `value` (string): Secret value (encrypted at rest) **Response:** - Success: `200` - Secret saved - Error: `400` - Bad Request **Example:** ```json {\"name\": \"DATABASE_URL\", \"updated_at\": \"2024-01-15T10:00:00Z\"} ``` ### DELETE /secrets/{name} Delete a secret. **Parameters:** **Response:** - Success: `204` - Secret deleted - Error: `400` - Bad Request **Example:** ```json {} ```",
              "score": null
            },
            {
              "doc_id": "howto_create_api_integration",
              "chunk_id": "howto_create_api_integration_fix_0",
              "content": "# How to Create API Integration ## Prerequisites - CloudFlow account ## Steps ### Step 1: Generate API key Create an API key for programmatic access. ```bash cloudflow apikeys create --name 'CI Integration' --scopes workflows:read,workflows:execute ``` ### Step 2: Store key securely Never commit API keys to source control. Use environment variables or secret management. > **Note:** Rotate keys every 90 days ### Step 3: Test authentication Verify the API key works correctly. ```bash curl -H 'Authorization: Bearer cf_...' https://api.cloudflow.io/v1/me ``` ### Step 4: Implement rate limiting Handle rate limit responses (HTTP 429) with exponential backoff. > **Note:** Default limit is 100 requests per minute ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            },
            {
              "doc_id": "howto_set_up_ci_cd_pipeline",
              "chunk_id": "howto_set_up_ci_cd_pipeline_fix_0",
              "content": "# How to Set Up CI/CD Pipeline ## Prerequisites - GitHub account - CloudFlow API key - Repository admin access ## Steps ### Step 1: Create GitHub Actions workflow Create a new file at `.github/workflows/cloudflow.yml` with the deployment configuration. ```bash mkdir -p .github/workflows && touch .github/workflows/cloudflow.yml ``` ### Step 2: Configure secrets Add your CloudFlow API key to GitHub secrets. ```bash gh secret set CLOUDFLOW_API_KEY ``` ### Step 3: Add deployment step Add the CloudFlow deployment action to your workflow. > **Note:** Use the official cloudflow/deploy-action@v2 ### Step 4: Test the pipeline Push a commit to trigger the workflow. ```bash git push origin main ``` ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            },
            {
              "doc_id": "api_workflows",
              "chunk_id": "api_workflows_fix_0",
              "content": "# Workflows API Reference ## Overview The Workflows API provides programmatic access to workflows functionality. Base URL: `https://api.cloudflow.io/v1/workflows` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /workflows List all workflows for the authenticated user. **Parameters:** - `limit` (integer): Maximum number of results (default: 50, max: 200) - `status` (string): Filter by status: active, paused, archived **Response:** - Success: `200` - Returns array of workflow objects - Error: `400` - Bad Request **Example:** ```json {\"workflows\": [{\"id\": \"wf_123\", \"name\": \"Daily Report\", \"status\": \"active\"}]} ``` ### POST /workflows Create a new workflow. **Parameters:** - `name` (string): Workflow name (required) - `definition` (object): Workflow definition in JSON format **Response:** - Success: `201` - Workflow created successfully - Error: `400` - Bad Request **Example:** ```json {\"id\": \"wf_456\", \"name\": \"New Workflow\", \"created_at\": \"2024-01-15T10:00:00Z\"} ``` ### DELETE /workflows/{id} Delete a workflow permanently. **Parameters:** - `id` (string): Workflow ID (required) **Response:** - Success: `204` - Workflow deleted - Error: `404` - Workflow not found **Example:** ```json {} ```",
              "score": null
            },
            {
              "doc_id": "howto_set_up_data_encryption",
              "chunk_id": "howto_set_up_data_encryption_fix_0",
              "content": "# How to Set Up Data Encryption ## Prerequisites - CloudFlow Enterprise plan ## Steps ### Step 1: Enable encryption at rest All data is encrypted at rest using AES-256 by default. > **Note:** Encryption is automatic for all plans ### Step 2: Configure KMS Bring your own encryption keys using AWS KMS or GCP KMS. ```bash cloudflow encryption configure --kms-key arn:aws:kms:... ``` ### Step 3: Enable field-level encryption Encrypt sensitive fields within your workflow data. ```bash cloudflow encryption add-field --path $.user.ssn --key-alias sensitive-data ``` ### Step 4: Verify encryption Confirm encryption is active for your workspace. ```bash cloudflow encryption status ``` ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            }
          ],
          "latency_ms": 3.7018040002294583,
          "query_id": "api_045",
          "dimension": "contextual",
          "query": "automating secret rotation in CI/CD, need to PUT and DELETE secrets via API"
        },
        {
          "key_facts": [
            "PUT /secrets/{name}",
            "DELETE /secrets/{name}",
            "encrypted at rest"
          ],
          "found_facts": [
            "PUT /secrets/{name}",
            "DELETE /secrets/{name}",
            "encrypted at rest"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "api_secrets"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "api_secrets",
              "chunk_id": "api_secrets_fix_0",
              "content": "# Secrets API Reference ## Overview The Secrets API provides programmatic access to secrets functionality. Base URL: `https://api.cloudflow.io/v1/secrets` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /secrets List secret names (values not returned). **Parameters:** **Response:** - Success: `200` - Returns array of secret metadata - Error: `400` - Bad Request **Example:** ```json {\"secrets\": [{\"name\": \"DATABASE_URL\", \"created_at\": \"2024-01-01\", \"updated_at\": \"2024-01-10\"}]} ``` ### PUT /secrets/{name} Create or update a secret. **Parameters:** - `value` (string): Secret value (encrypted at rest) **Response:** - Success: `200` - Secret saved - Error: `400` - Bad Request **Example:** ```json {\"name\": \"DATABASE_URL\", \"updated_at\": \"2024-01-15T10:00:00Z\"} ``` ### DELETE /secrets/{name} Delete a secret. **Parameters:** **Response:** - Success: `204` - Secret deleted - Error: `400` - Bad Request **Example:** ```json {} ```",
              "score": null
            },
            {
              "doc_id": "api_users",
              "chunk_id": "api_users_fix_0",
              "content": "# Users API Reference ## Overview The Users API provides programmatic access to users functionality. Base URL: `https://api.cloudflow.io/v1/users` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /users/me Get current user profile. **Parameters:** **Response:** - Success: `200` - Returns user object - Error: `400` - Bad Request **Example:** ```json {\"id\": \"usr_001\", \"email\": \"user@example.com\", \"role\": \"admin\", \"created_at\": \"2023-06-01\"} ``` ### PATCH /users/me Update current user profile. **Parameters:** - `name` (string): Display name - `timezone` (string): Timezone (e.g., America/New_York) **Response:** - Success: `200` - User updated - Error: `400` - Bad Request **Example:** ```json {\"id\": \"usr_001\", \"name\": \"John Doe\", \"timezone\": \"America/New_York\"} ```",
              "score": null
            },
            {
              "doc_id": "api_executions",
              "chunk_id": "api_executions_fix_0",
              "content": "# Executions API Reference ## Overview The Executions API provides programmatic access to executions functionality. Base URL: `https://api.cloudflow.io/v1/executions` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /executions List workflow executions. **Parameters:** - `workflow_id` (string): Filter by workflow ID - `since` (datetime): Filter executions after this timestamp **Response:** - Success: `200` - Returns array of execution objects - Error: `400` - Bad Request **Example:** ```json {\"executions\": [{\"id\": \"ex_789\", \"status\": \"completed\", \"duration_ms\": 1523}]} ``` ### POST /executions/{id}/cancel Cancel a running execution. **Parameters:** - `id` (string): Execution ID - `reason` (string): Cancellation reason (optional) **Response:** - Success: `200` - Execution cancelled - Error: `400` - Bad Request **Example:** ```json {\"id\": \"ex_789\", \"status\": \"cancelled\", \"cancelled_at\": \"2024-01-15T10:05:00Z\"} ```",
              "score": null
            },
            {
              "doc_id": "api_workflows",
              "chunk_id": "api_workflows_fix_0",
              "content": "# Workflows API Reference ## Overview The Workflows API provides programmatic access to workflows functionality. Base URL: `https://api.cloudflow.io/v1/workflows` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /workflows List all workflows for the authenticated user. **Parameters:** - `limit` (integer): Maximum number of results (default: 50, max: 200) - `status` (string): Filter by status: active, paused, archived **Response:** - Success: `200` - Returns array of workflow objects - Error: `400` - Bad Request **Example:** ```json {\"workflows\": [{\"id\": \"wf_123\", \"name\": \"Daily Report\", \"status\": \"active\"}]} ``` ### POST /workflows Create a new workflow. **Parameters:** - `name` (string): Workflow name (required) - `definition` (object): Workflow definition in JSON format **Response:** - Success: `201` - Workflow created successfully - Error: `400` - Bad Request **Example:** ```json {\"id\": \"wf_456\", \"name\": \"New Workflow\", \"created_at\": \"2024-01-15T10:00:00Z\"} ``` ### DELETE /workflows/{id} Delete a workflow permanently. **Parameters:** - `id` (string): Workflow ID (required) **Response:** - Success: `204` - Workflow deleted - Error: `404` - Workflow not found **Example:** ```json {} ```",
              "score": null
            },
            {
              "doc_id": "api_integrations",
              "chunk_id": "api_integrations_fix_0",
              "content": "# Integrations API Reference ## Overview The Integrations API provides programmatic access to integrations functionality. Base URL: `https://api.cloudflow.io/v1/integrations` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /integrations List available integrations. **Parameters:** - `category` (string): Filter by category: storage, notification, database **Response:** - Success: `200` - Returns array of integration objects - Error: `400` - Bad Request **Example:** ```json {\"integrations\": [{\"id\": \"int_s3\", \"name\": \"Amazon S3\", \"category\": \"storage\", \"status\": \"connected\"}]} ``` ### POST /integrations/{id}/connect Connect an integration. **Parameters:** - `credentials` (object): Integration-specific credentials **Response:** - Success: `200` - Integration connected - Error: `400` - Bad Request **Example:** ```json {\"id\": \"int_s3\", \"status\": \"connected\", \"connected_at\": \"2024-01-15T10:00:00Z\"} ```",
              "score": null
            }
          ],
          "latency_ms": 3.541435000443016,
          "query_id": "api_045",
          "dimension": "negation",
          "query": "why can't I delete old secrets from the API"
        },
        {
          "key_facts": [
            "Workflow versioning",
            "Python SDK",
            "async support",
            "v4.0"
          ],
          "found_facts": [
            "Workflow versioning",
            "Python SDK",
            "async support",
            "v4.0"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "release_3_2_0"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "release_3_1_0",
              "chunk_id": "release_3_1_0_fix_0",
              "content": "# Release Notes - v3.1.0 **Release Date:** 2023-12-01 ## New Features - GraphQL API beta release - Workflow templates marketplace - Enhanced audit logging - SSO support for Okta ## Bug Fixes - Fixed webhook delivery retry logic - Resolved issue with large payload handling - Fixed dashboard loading performance ## Upgrade Instructions ```bash cloudflow upgrade --version 3.1.0 ``` ## Known Issues None at this time.",
              "score": null
            },
            {
              "doc_id": "release_2_9_0",
              "chunk_id": "release_2_9_0_fix_0",
              "content": "# Release Notes - v2.9.0 **Release Date:** 2023-08-01 ## New Features - Parallel step execution - Conditional branching in workflows - Slack integration improvements - Performance dashboard ## Bug Fixes - Fixed issue with special characters in secrets - Resolved timeout in large file uploads ## Upgrade Instructions ```bash cloudflow upgrade --version 2.9.0 ``` ## Known Issues None at this time.",
              "score": null
            },
            {
              "doc_id": "release_3_0_0",
              "chunk_id": "release_3_0_0_fix_0",
              "content": "# Release Notes - v3.0.0 **Release Date:** 2023-10-15 ## New Features - Complete UI redesign - New workflow editor with visual builder - Real-time execution monitoring - Team workspaces ## Bug Fixes - Fixed critical security issue in token validation - Resolved data loss bug in failed migrations ## Breaking Changes - **BREAKING:** Authentication API completely rewritten - **BREAKING:** Legacy webhook format no longer supported - **BREAKING:** Minimum Node.js version now 18 ## Upgrade Instructions ```bash cloudflow upgrade --version 3.0.0 ``` ## Known Issues None at this time.",
              "score": null
            },
            {
              "doc_id": "release_3_2_0",
              "chunk_id": "release_3_2_0_fix_0",
              "content": "# Release Notes - v3.2.0 **Release Date:** 2024-01-15 ## New Features - Workflow versioning with rollback support - New Python SDK with async support - Bulk operations API for workflows - Custom retry policies per step ## Bug Fixes - Fixed race condition in concurrent execution handler - Resolved memory leak in long-running workflows - Fixed timezone handling in scheduled triggers ## Breaking Changes - **BREAKING:** API v1 endpoints deprecated, will be removed in v4.0 - **BREAKING:** Minimum Python version now 3.9 ## Upgrade Instructions ```bash cloudflow upgrade --version 3.2.0 ``` ## Known Issues None at this time.",
              "score": null
            },
            {
              "doc_id": "howto_implement_workflow_versioning",
              "chunk_id": "howto_implement_workflow_versioning_fix_0",
              "content": "# How to Implement Workflow Versioning ## Prerequisites - Existing workflow ## Steps ### Step 1: Enable versioning Turn on versioning for your workflow. This is required for the feature. ```bash cloudflow workflows update wf_123 --enable-versioning ``` ### Step 2: Create new version Save changes as a new version instead of overwriting. ```bash cloudflow workflows publish wf_123 --version 2.0.0 ``` ### Step 3: Set active version Choose which version handles new executions. ```bash cloudflow workflows activate wf_123 --version 2.0.0 ``` ### Step 4: Roll back if needed Quickly revert to a previous version. ```bash cloudflow workflows rollback wf_123 --to-version 1.0.0 ``` ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            }
          ],
          "latency_ms": 3.5637469991343096,
          "query_id": "release_046",
          "dimension": "original",
          "query": "What's new in version 3.2.0?"
        },
        {
          "key_facts": [
            "Workflow versioning",
            "Python SDK",
            "async support",
            "v4.0"
          ],
          "found_facts": [
            "Workflow versioning",
            "Python SDK",
            "async support",
            "v4.0"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "release_3_2_0"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "release_3_1_0",
              "chunk_id": "release_3_1_0_fix_0",
              "content": "# Release Notes - v3.1.0 **Release Date:** 2023-12-01 ## New Features - GraphQL API beta release - Workflow templates marketplace - Enhanced audit logging - SSO support for Okta ## Bug Fixes - Fixed webhook delivery retry logic - Resolved issue with large payload handling - Fixed dashboard loading performance ## Upgrade Instructions ```bash cloudflow upgrade --version 3.1.0 ``` ## Known Issues None at this time.",
              "score": null
            },
            {
              "doc_id": "release_2_9_0",
              "chunk_id": "release_2_9_0_fix_0",
              "content": "# Release Notes - v2.9.0 **Release Date:** 2023-08-01 ## New Features - Parallel step execution - Conditional branching in workflows - Slack integration improvements - Performance dashboard ## Bug Fixes - Fixed issue with special characters in secrets - Resolved timeout in large file uploads ## Upgrade Instructions ```bash cloudflow upgrade --version 2.9.0 ``` ## Known Issues None at this time.",
              "score": null
            },
            {
              "doc_id": "release_3_0_0",
              "chunk_id": "release_3_0_0_fix_0",
              "content": "# Release Notes - v3.0.0 **Release Date:** 2023-10-15 ## New Features - Complete UI redesign - New workflow editor with visual builder - Real-time execution monitoring - Team workspaces ## Bug Fixes - Fixed critical security issue in token validation - Resolved data loss bug in failed migrations ## Breaking Changes - **BREAKING:** Authentication API completely rewritten - **BREAKING:** Legacy webhook format no longer supported - **BREAKING:** Minimum Node.js version now 18 ## Upgrade Instructions ```bash cloudflow upgrade --version 3.0.0 ``` ## Known Issues None at this time.",
              "score": null
            },
            {
              "doc_id": "arch_monitoring_stack",
              "chunk_id": "arch_monitoring_stack_fix_0",
              "content": "# Monitoring Stack Architecture ## Overview The Monitoring Stack provides observability into system health, performance, and errors. Includes metrics, logs, traces, and alerting. ## Design Principles - **Observability**: Every component emits metrics and traces - **Actionable Alerts**: Alerts include runbook links ## Components ### Metrics Collects time-series metrics. Retention is 15 days local, 1 year in object storage. **Technology:** Prometheus with Thanos **Scaling:** Federated with long-term storage ### Logging Centralized log aggregation. Supports structured JSON logs with label indexing. **Technology:** Loki with Grafana **Scaling:** Distributed with sharding ### Tracing Distributed request tracing. Trace retention is 7 days. **Technology:** Jaeger with OpenTelemetry **Scaling:** Sampling at 10% ### Alerting Routes alerts to on-call engineers. Supports escalation policies and silencing. **Technology:** Alertmanager with PagerDuty **Scaling:** HA pair ## Data Flow Application \u2192 Collector \u2192 Storage \u2192 Query \u2192 Dashboard/Alert ## Performance Characteristics - **Latency P50:** N/A - **Latency P99:** N/A - **Throughput:** 1M metrics/second, 100GB logs/day",
              "score": null
            },
            {
              "doc_id": "release_3_2_0",
              "chunk_id": "release_3_2_0_fix_0",
              "content": "# Release Notes - v3.2.0 **Release Date:** 2024-01-15 ## New Features - Workflow versioning with rollback support - New Python SDK with async support - Bulk operations API for workflows - Custom retry policies per step ## Bug Fixes - Fixed race condition in concurrent execution handler - Resolved memory leak in long-running workflows - Fixed timezone handling in scheduled triggers ## Breaking Changes - **BREAKING:** API v1 endpoints deprecated, will be removed in v4.0 - **BREAKING:** Minimum Python version now 3.9 ## Upgrade Instructions ```bash cloudflow upgrade --version 3.2.0 ``` ## Known Issues None at this time.",
              "score": null
            }
          ],
          "latency_ms": 3.6761459996341728,
          "query_id": "release_046",
          "dimension": "synonym",
          "query": "What changes and updates are included in the v3.2.0 changelog?"
        },
        {
          "key_facts": [
            "Workflow versioning",
            "Python SDK",
            "async support",
            "v4.0"
          ],
          "found_facts": [
            "Workflow versioning",
            "Python SDK",
            "async support",
            "v4.0"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "release_3_2_0"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "release_3_1_0",
              "chunk_id": "release_3_1_0_fix_0",
              "content": "# Release Notes - v3.1.0 **Release Date:** 2023-12-01 ## New Features - GraphQL API beta release - Workflow templates marketplace - Enhanced audit logging - SSO support for Okta ## Bug Fixes - Fixed webhook delivery retry logic - Resolved issue with large payload handling - Fixed dashboard loading performance ## Upgrade Instructions ```bash cloudflow upgrade --version 3.1.0 ``` ## Known Issues None at this time.",
              "score": null
            },
            {
              "doc_id": "release_3_2_0",
              "chunk_id": "release_3_2_0_fix_0",
              "content": "# Release Notes - v3.2.0 **Release Date:** 2024-01-15 ## New Features - Workflow versioning with rollback support - New Python SDK with async support - Bulk operations API for workflows - Custom retry policies per step ## Bug Fixes - Fixed race condition in concurrent execution handler - Resolved memory leak in long-running workflows - Fixed timezone handling in scheduled triggers ## Breaking Changes - **BREAKING:** API v1 endpoints deprecated, will be removed in v4.0 - **BREAKING:** Minimum Python version now 3.9 ## Upgrade Instructions ```bash cloudflow upgrade --version 3.2.0 ``` ## Known Issues None at this time.",
              "score": null
            },
            {
              "doc_id": "release_3_0_0",
              "chunk_id": "release_3_0_0_fix_0",
              "content": "# Release Notes - v3.0.0 **Release Date:** 2023-10-15 ## New Features - Complete UI redesign - New workflow editor with visual builder - Real-time execution monitoring - Team workspaces ## Bug Fixes - Fixed critical security issue in token validation - Resolved data loss bug in failed migrations ## Breaking Changes - **BREAKING:** Authentication API completely rewritten - **BREAKING:** Legacy webhook format no longer supported - **BREAKING:** Minimum Node.js version now 18 ## Upgrade Instructions ```bash cloudflow upgrade --version 3.0.0 ``` ## Known Issues None at this time.",
              "score": null
            },
            {
              "doc_id": "release_2_9_0",
              "chunk_id": "release_2_9_0_fix_0",
              "content": "# Release Notes - v2.9.0 **Release Date:** 2023-08-01 ## New Features - Parallel step execution - Conditional branching in workflows - Slack integration improvements - Performance dashboard ## Bug Fixes - Fixed issue with special characters in secrets - Resolved timeout in large file uploads ## Upgrade Instructions ```bash cloudflow upgrade --version 2.9.0 ``` ## Known Issues None at this time.",
              "score": null
            },
            {
              "doc_id": "howto_migrate_from_v1_to_v2_api",
              "chunk_id": "howto_migrate_from_v1_to_v2_api_fix_0",
              "content": "# How to Migrate from v1 to v2 API ## Prerequisites - Existing v1 integration - v2 API access enabled ## Steps ### Step 1: Review breaking changes The v2 API changes authentication from API keys to OAuth 2.0. Review the migration guide. ### Step 2: Update client library Upgrade to the latest SDK version. ```bash npm install @cloudflow/sdk@latest ``` ### Step 3: Update authentication Replace API key authentication with OAuth 2.0 client credentials flow. > **Note:** Generate new credentials in the dashboard ### Step 4: Test in staging Run your integration tests against the v2 staging endpoint. ```bash CLOUDFLOW_API_URL=https://api-v2-staging.cloudflow.io npm test ``` ### Step 5: Switch production Update production configuration to use v2 API. ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            }
          ],
          "latency_ms": 3.6002649994770763,
          "query_id": "release_046",
          "dimension": "problem",
          "query": "I upgraded to 3.2.0 and now the API v1 calls are showing deprecation warnings, what happened?"
        },
        {
          "key_facts": [
            "Workflow versioning",
            "Python SDK",
            "async support",
            "v4.0"
          ],
          "found_facts": [
            "Workflow versioning",
            "Python SDK",
            "async support",
            "v4.0"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "release_3_2_0"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "release_3_1_0",
              "chunk_id": "release_3_1_0_fix_0",
              "content": "# Release Notes - v3.1.0 **Release Date:** 2023-12-01 ## New Features - GraphQL API beta release - Workflow templates marketplace - Enhanced audit logging - SSO support for Okta ## Bug Fixes - Fixed webhook delivery retry logic - Resolved issue with large payload handling - Fixed dashboard loading performance ## Upgrade Instructions ```bash cloudflow upgrade --version 3.1.0 ``` ## Known Issues None at this time.",
              "score": null
            },
            {
              "doc_id": "release_2_9_0",
              "chunk_id": "release_2_9_0_fix_0",
              "content": "# Release Notes - v2.9.0 **Release Date:** 2023-08-01 ## New Features - Parallel step execution - Conditional branching in workflows - Slack integration improvements - Performance dashboard ## Bug Fixes - Fixed issue with special characters in secrets - Resolved timeout in large file uploads ## Upgrade Instructions ```bash cloudflow upgrade --version 2.9.0 ``` ## Known Issues None at this time.",
              "score": null
            },
            {
              "doc_id": "release_3_0_0",
              "chunk_id": "release_3_0_0_fix_0",
              "content": "# Release Notes - v3.0.0 **Release Date:** 2023-10-15 ## New Features - Complete UI redesign - New workflow editor with visual builder - Real-time execution monitoring - Team workspaces ## Bug Fixes - Fixed critical security issue in token validation - Resolved data loss bug in failed migrations ## Breaking Changes - **BREAKING:** Authentication API completely rewritten - **BREAKING:** Legacy webhook format no longer supported - **BREAKING:** Minimum Node.js version now 18 ## Upgrade Instructions ```bash cloudflow upgrade --version 3.0.0 ``` ## Known Issues None at this time.",
              "score": null
            },
            {
              "doc_id": "release_3_2_0",
              "chunk_id": "release_3_2_0_fix_0",
              "content": "# Release Notes - v3.2.0 **Release Date:** 2024-01-15 ## New Features - Workflow versioning with rollback support - New Python SDK with async support - Bulk operations API for workflows - Custom retry policies per step ## Bug Fixes - Fixed race condition in concurrent execution handler - Resolved memory leak in long-running workflows - Fixed timezone handling in scheduled triggers ## Breaking Changes - **BREAKING:** API v1 endpoints deprecated, will be removed in v4.0 - **BREAKING:** Minimum Python version now 3.9 ## Upgrade Instructions ```bash cloudflow upgrade --version 3.2.0 ``` ## Known Issues None at this time.",
              "score": null
            },
            {
              "doc_id": "adr_008",
              "chunk_id": "adr_008_fix_0",
              "content": "# ADR-008: Use S3 for Object Storage **Status:** Accepted **Date:** 2024-10-14 ## Context We need storage for workflow artifacts, execution outputs, and user uploads. ## Options Considered ### Option 1: Amazon S3 Object storage service. **Pros:** - 99.999999999% durability - Lifecycle policies - Event notifications - CDN integration **Cons:** - AWS lock-in - Eventual consistency for overwrites - Cost at scale ### Option 2: Self-hosted MinIO S3-compatible object storage. **Pros:** - No vendor lock-in - S3 compatible - On-premises option **Cons:** - Operational burden - Lower durability guarantees - Scaling complexity ### Option 3: Google Cloud Storage GCP object storage. **Pros:** - Strong consistency - Good performance - Unified billing **Cons:** - GCP lock-in - Fewer features than S3 - Smaller ecosystem ## Decision We will use Amazon S3 for object storage with intelligent tiering for cost optimization. ## Consequences - Enable versioning for critical buckets - Implement lifecycle policies - Use pre-signed URLs for uploads - Enable server-side encryption",
              "score": null
            }
          ],
          "latency_ms": 3.384962999916752,
          "query_id": "release_046",
          "dimension": "casual",
          "query": "3.2.0 new features"
        },
        {
          "key_facts": [
            "Workflow versioning",
            "Python SDK",
            "async support",
            "v4.0"
          ],
          "found_facts": [
            "Workflow versioning",
            "Python SDK",
            "async support",
            "v4.0"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "release_3_2_0"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "release_3_1_0",
              "chunk_id": "release_3_1_0_fix_0",
              "content": "# Release Notes - v3.1.0 **Release Date:** 2023-12-01 ## New Features - GraphQL API beta release - Workflow templates marketplace - Enhanced audit logging - SSO support for Okta ## Bug Fixes - Fixed webhook delivery retry logic - Resolved issue with large payload handling - Fixed dashboard loading performance ## Upgrade Instructions ```bash cloudflow upgrade --version 3.1.0 ``` ## Known Issues None at this time.",
              "score": null
            },
            {
              "doc_id": "release_3_0_0",
              "chunk_id": "release_3_0_0_fix_0",
              "content": "# Release Notes - v3.0.0 **Release Date:** 2023-10-15 ## New Features - Complete UI redesign - New workflow editor with visual builder - Real-time execution monitoring - Team workspaces ## Bug Fixes - Fixed critical security issue in token validation - Resolved data loss bug in failed migrations ## Breaking Changes - **BREAKING:** Authentication API completely rewritten - **BREAKING:** Legacy webhook format no longer supported - **BREAKING:** Minimum Node.js version now 18 ## Upgrade Instructions ```bash cloudflow upgrade --version 3.0.0 ``` ## Known Issues None at this time.",
              "score": null
            },
            {
              "doc_id": "release_3_2_0",
              "chunk_id": "release_3_2_0_fix_0",
              "content": "# Release Notes - v3.2.0 **Release Date:** 2024-01-15 ## New Features - Workflow versioning with rollback support - New Python SDK with async support - Bulk operations API for workflows - Custom retry policies per step ## Bug Fixes - Fixed race condition in concurrent execution handler - Resolved memory leak in long-running workflows - Fixed timezone handling in scheduled triggers ## Breaking Changes - **BREAKING:** API v1 endpoints deprecated, will be removed in v4.0 - **BREAKING:** Minimum Python version now 3.9 ## Upgrade Instructions ```bash cloudflow upgrade --version 3.2.0 ``` ## Known Issues None at this time.",
              "score": null
            },
            {
              "doc_id": "release_2_9_0",
              "chunk_id": "release_2_9_0_fix_0",
              "content": "# Release Notes - v2.9.0 **Release Date:** 2023-08-01 ## New Features - Parallel step execution - Conditional branching in workflows - Slack integration improvements - Performance dashboard ## Bug Fixes - Fixed issue with special characters in secrets - Resolved timeout in large file uploads ## Upgrade Instructions ```bash cloudflow upgrade --version 2.9.0 ``` ## Known Issues None at this time.",
              "score": null
            },
            {
              "doc_id": "arch_workflow_engine",
              "chunk_id": "arch_workflow_engine_fix_0",
              "content": "# Workflow Engine Architecture ## Overview The Workflow Engine is the core component responsible for executing user-defined workflows. It uses an event-driven architecture with support for parallel execution, conditional branching, and automatic retries. ## Design Principles - **Idempotency**: All operations can be safely retried without side effects - **Eventual Consistency**: State converges to consistent view within 5 seconds - **Fault Tolerance**: System continues operating despite component failures ## Components ### Scheduler Manages workflow scheduling and triggers. Supports cron expressions, webhooks, and event-based triggers. **Technology:** Custom Go service **Scaling:** Horizontal with leader election ### Executor Runs individual workflow steps in isolated containers. Maximum execution time is 30 minutes per step. **Technology:** Kubernetes Jobs **Scaling:** Auto-scaling based on queue depth ### State Store Persists workflow state, execution history, and step results. Uses optimistic locking for concurrent updates. **Technology:** PostgreSQL with JSONB **Scaling:** Primary-replica with read replicas ## Data Flow Triggers \u2192 Scheduler \u2192 Job Queue \u2192 Executor \u2192 State Store \u2192 Webhooks/Notifications ## Performance Characteristics - **Latency P50:** 45ms - **Latency P99:** 250ms - **Throughput:** 5000 executions/minute",
              "score": null
            }
          ],
          "latency_ms": 3.528612000081921,
          "query_id": "release_046",
          "dimension": "contextual",
          "query": "We're evaluating upgrading from 3.1 to 3.2.0, what new capabilities did they add like workflow versioning or SDK improvements?"
        },
        {
          "key_facts": [
            "Workflow versioning",
            "Python SDK",
            "async support",
            "v4.0"
          ],
          "found_facts": [
            "Workflow versioning",
            "Python SDK",
            "async support",
            "v4.0"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "release_3_2_0"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "release_3_2_0",
              "chunk_id": "release_3_2_0_fix_0",
              "content": "# Release Notes - v3.2.0 **Release Date:** 2024-01-15 ## New Features - Workflow versioning with rollback support - New Python SDK with async support - Bulk operations API for workflows - Custom retry policies per step ## Bug Fixes - Fixed race condition in concurrent execution handler - Resolved memory leak in long-running workflows - Fixed timezone handling in scheduled triggers ## Breaking Changes - **BREAKING:** API v1 endpoints deprecated, will be removed in v4.0 - **BREAKING:** Minimum Python version now 3.9 ## Upgrade Instructions ```bash cloudflow upgrade --version 3.2.0 ``` ## Known Issues None at this time.",
              "score": null
            },
            {
              "doc_id": "howto_implement_workflow_versioning",
              "chunk_id": "howto_implement_workflow_versioning_fix_0",
              "content": "# How to Implement Workflow Versioning ## Prerequisites - Existing workflow ## Steps ### Step 1: Enable versioning Turn on versioning for your workflow. This is required for the feature. ```bash cloudflow workflows update wf_123 --enable-versioning ``` ### Step 2: Create new version Save changes as a new version instead of overwriting. ```bash cloudflow workflows publish wf_123 --version 2.0.0 ``` ### Step 3: Set active version Choose which version handles new executions. ```bash cloudflow workflows activate wf_123 --version 2.0.0 ``` ### Step 4: Roll back if needed Quickly revert to a previous version. ```bash cloudflow workflows rollback wf_123 --to-version 1.0.0 ``` ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            },
            {
              "doc_id": "release_3_1_0",
              "chunk_id": "release_3_1_0_fix_0",
              "content": "# Release Notes - v3.1.0 **Release Date:** 2023-12-01 ## New Features - GraphQL API beta release - Workflow templates marketplace - Enhanced audit logging - SSO support for Okta ## Bug Fixes - Fixed webhook delivery retry logic - Resolved issue with large payload handling - Fixed dashboard loading performance ## Upgrade Instructions ```bash cloudflow upgrade --version 3.1.0 ``` ## Known Issues None at this time.",
              "score": null
            },
            {
              "doc_id": "release_3_0_0",
              "chunk_id": "release_3_0_0_fix_0",
              "content": "# Release Notes - v3.0.0 **Release Date:** 2023-10-15 ## New Features - Complete UI redesign - New workflow editor with visual builder - Real-time execution monitoring - Team workspaces ## Bug Fixes - Fixed critical security issue in token validation - Resolved data loss bug in failed migrations ## Breaking Changes - **BREAKING:** Authentication API completely rewritten - **BREAKING:** Legacy webhook format no longer supported - **BREAKING:** Minimum Node.js version now 18 ## Upgrade Instructions ```bash cloudflow upgrade --version 3.0.0 ``` ## Known Issues None at this time.",
              "score": null
            },
            {
              "doc_id": "release_2_9_0",
              "chunk_id": "release_2_9_0_fix_0",
              "content": "# Release Notes - v2.9.0 **Release Date:** 2023-08-01 ## New Features - Parallel step execution - Conditional branching in workflows - Slack integration improvements - Performance dashboard ## Bug Fixes - Fixed issue with special characters in secrets - Resolved timeout in large file uploads ## Upgrade Instructions ```bash cloudflow upgrade --version 2.9.0 ``` ## Known Issues None at this time.",
              "score": null
            }
          ],
          "latency_ms": 4.496363000725978,
          "query_id": "release_046",
          "dimension": "negation",
          "query": "Why doesn't version 3.1 have workflow rollback support?"
        },
        {
          "key_facts": [
            "critical security issue",
            "token validation"
          ],
          "found_facts": [
            "critical security issue",
            "token validation"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "release_3_0_0"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "release_3_1_0",
              "chunk_id": "release_3_1_0_fix_0",
              "content": "# Release Notes - v3.1.0 **Release Date:** 2023-12-01 ## New Features - GraphQL API beta release - Workflow templates marketplace - Enhanced audit logging - SSO support for Okta ## Bug Fixes - Fixed webhook delivery retry logic - Resolved issue with large payload handling - Fixed dashboard loading performance ## Upgrade Instructions ```bash cloudflow upgrade --version 3.1.0 ``` ## Known Issues None at this time.",
              "score": null
            },
            {
              "doc_id": "release_3_0_0",
              "chunk_id": "release_3_0_0_fix_0",
              "content": "# Release Notes - v3.0.0 **Release Date:** 2023-10-15 ## New Features - Complete UI redesign - New workflow editor with visual builder - Real-time execution monitoring - Team workspaces ## Bug Fixes - Fixed critical security issue in token validation - Resolved data loss bug in failed migrations ## Breaking Changes - **BREAKING:** Authentication API completely rewritten - **BREAKING:** Legacy webhook format no longer supported - **BREAKING:** Minimum Node.js version now 18 ## Upgrade Instructions ```bash cloudflow upgrade --version 3.0.0 ``` ## Known Issues None at this time.",
              "score": null
            },
            {
              "doc_id": "release_3_2_0",
              "chunk_id": "release_3_2_0_fix_0",
              "content": "# Release Notes - v3.2.0 **Release Date:** 2024-01-15 ## New Features - Workflow versioning with rollback support - New Python SDK with async support - Bulk operations API for workflows - Custom retry policies per step ## Bug Fixes - Fixed race condition in concurrent execution handler - Resolved memory leak in long-running workflows - Fixed timezone handling in scheduled triggers ## Breaking Changes - **BREAKING:** API v1 endpoints deprecated, will be removed in v4.0 - **BREAKING:** Minimum Python version now 3.9 ## Upgrade Instructions ```bash cloudflow upgrade --version 3.2.0 ``` ## Known Issues None at this time.",
              "score": null
            },
            {
              "doc_id": "release_2_9_0",
              "chunk_id": "release_2_9_0_fix_0",
              "content": "# Release Notes - v2.9.0 **Release Date:** 2023-08-01 ## New Features - Parallel step execution - Conditional branching in workflows - Slack integration improvements - Performance dashboard ## Bug Fixes - Fixed issue with special characters in secrets - Resolved timeout in large file uploads ## Upgrade Instructions ```bash cloudflow upgrade --version 2.9.0 ``` ## Known Issues None at this time.",
              "score": null
            },
            {
              "doc_id": "arch_authentication_service",
              "chunk_id": "arch_authentication_service_fix_0",
              "content": "# Authentication Service Architecture ## Overview The Authentication Service handles user identity, session management, and token issuance. Supports OAuth 2.0, SAML, and API key authentication. ## Design Principles - **Zero Knowledge**: Passwords never stored in plaintext - **Token Rotation**: Refresh tokens rotate on each use ## Components ### Identity Provider Manages user accounts and credentials. Password hashing uses bcrypt with cost factor 12. **Technology:** Custom Go service **Scaling:** Horizontal stateless ### Token Service Issues and validates access tokens. Token lifetime is 1 hour with 7-day refresh tokens. **Technology:** JWT with RS256 **Scaling:** Horizontal stateless ### SSO Gateway Integrates with external identity providers. Supports Google, Microsoft, Okta, and custom SAML. **Technology:** Dex OIDC proxy **Scaling:** Active-passive ## Data Flow Credentials \u2192 Validation \u2192 Token Issue \u2192 Token Refresh \u2192 Logout ## Performance Characteristics - **Latency P50:** 50ms - **Latency P99:** 200ms - **Throughput:** 5000 auth/second",
              "score": null
            }
          ],
          "latency_ms": 3.8470040017273277,
          "query_id": "release_047",
          "dimension": "original",
          "query": "What security fixes were in v3.0.0?"
        },
        {
          "key_facts": [
            "critical security issue",
            "token validation"
          ],
          "found_facts": [
            "critical security issue",
            "token validation"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "release_3_0_0"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "release_3_1_0",
              "chunk_id": "release_3_1_0_fix_0",
              "content": "# Release Notes - v3.1.0 **Release Date:** 2023-12-01 ## New Features - GraphQL API beta release - Workflow templates marketplace - Enhanced audit logging - SSO support for Okta ## Bug Fixes - Fixed webhook delivery retry logic - Resolved issue with large payload handling - Fixed dashboard loading performance ## Upgrade Instructions ```bash cloudflow upgrade --version 3.1.0 ``` ## Known Issues None at this time.",
              "score": null
            },
            {
              "doc_id": "release_2_9_0",
              "chunk_id": "release_2_9_0_fix_0",
              "content": "# Release Notes - v2.9.0 **Release Date:** 2023-08-01 ## New Features - Parallel step execution - Conditional branching in workflows - Slack integration improvements - Performance dashboard ## Bug Fixes - Fixed issue with special characters in secrets - Resolved timeout in large file uploads ## Upgrade Instructions ```bash cloudflow upgrade --version 2.9.0 ``` ## Known Issues None at this time.",
              "score": null
            },
            {
              "doc_id": "release_3_0_0",
              "chunk_id": "release_3_0_0_fix_0",
              "content": "# Release Notes - v3.0.0 **Release Date:** 2023-10-15 ## New Features - Complete UI redesign - New workflow editor with visual builder - Real-time execution monitoring - Team workspaces ## Bug Fixes - Fixed critical security issue in token validation - Resolved data loss bug in failed migrations ## Breaking Changes - **BREAKING:** Authentication API completely rewritten - **BREAKING:** Legacy webhook format no longer supported - **BREAKING:** Minimum Node.js version now 18 ## Upgrade Instructions ```bash cloudflow upgrade --version 3.0.0 ``` ## Known Issues None at this time.",
              "score": null
            },
            {
              "doc_id": "release_3_2_0",
              "chunk_id": "release_3_2_0_fix_0",
              "content": "# Release Notes - v3.2.0 **Release Date:** 2024-01-15 ## New Features - Workflow versioning with rollback support - New Python SDK with async support - Bulk operations API for workflows - Custom retry policies per step ## Bug Fixes - Fixed race condition in concurrent execution handler - Resolved memory leak in long-running workflows - Fixed timezone handling in scheduled triggers ## Breaking Changes - **BREAKING:** API v1 endpoints deprecated, will be removed in v4.0 - **BREAKING:** Minimum Python version now 3.9 ## Upgrade Instructions ```bash cloudflow upgrade --version 3.2.0 ``` ## Known Issues None at this time.",
              "score": null
            },
            {
              "doc_id": "howto_implement_workflow_versioning",
              "chunk_id": "howto_implement_workflow_versioning_fix_0",
              "content": "# How to Implement Workflow Versioning ## Prerequisites - Existing workflow ## Steps ### Step 1: Enable versioning Turn on versioning for your workflow. This is required for the feature. ```bash cloudflow workflows update wf_123 --enable-versioning ``` ### Step 2: Create new version Save changes as a new version instead of overwriting. ```bash cloudflow workflows publish wf_123 --version 2.0.0 ``` ### Step 3: Set active version Choose which version handles new executions. ```bash cloudflow workflows activate wf_123 --version 2.0.0 ``` ### Step 4: Roll back if needed Quickly revert to a previous version. ```bash cloudflow workflows rollback wf_123 --to-version 1.0.0 ``` ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            }
          ],
          "latency_ms": 3.4807119991455693,
          "query_id": "release_047",
          "dimension": "synonym",
          "query": "What security patches and vulnerability fixes were shipped in release 3.0.0?"
        },
        {
          "key_facts": [
            "critical security issue",
            "token validation"
          ],
          "found_facts": [
            "critical security issue",
            "token validation"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "release_3_0_0"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "arch_authentication_service",
              "chunk_id": "arch_authentication_service_fix_0",
              "content": "# Authentication Service Architecture ## Overview The Authentication Service handles user identity, session management, and token issuance. Supports OAuth 2.0, SAML, and API key authentication. ## Design Principles - **Zero Knowledge**: Passwords never stored in plaintext - **Token Rotation**: Refresh tokens rotate on each use ## Components ### Identity Provider Manages user accounts and credentials. Password hashing uses bcrypt with cost factor 12. **Technology:** Custom Go service **Scaling:** Horizontal stateless ### Token Service Issues and validates access tokens. Token lifetime is 1 hour with 7-day refresh tokens. **Technology:** JWT with RS256 **Scaling:** Horizontal stateless ### SSO Gateway Integrates with external identity providers. Supports Google, Microsoft, Okta, and custom SAML. **Technology:** Dex OIDC proxy **Scaling:** Active-passive ## Data Flow Credentials \u2192 Validation \u2192 Token Issue \u2192 Token Refresh \u2192 Logout ## Performance Characteristics - **Latency P50:** 50ms - **Latency P99:** 200ms - **Throughput:** 5000 auth/second",
              "score": null
            },
            {
              "doc_id": "adr_003",
              "chunk_id": "adr_003_fix_0",
              "content": "# ADR-003: Implement OAuth 2.0 with JWT for Authentication **Status:** Accepted **Date:** 2024-05-08 ## Context We need a secure authentication system that supports API keys, user sessions, and third-party integrations. ## Options Considered ### Option 1: OAuth 2.0 with JWT Industry standard token-based auth. **Pros:** - Industry standard - Stateless validation - Supports refresh tokens - SSO compatible **Cons:** - Token size overhead - Cannot revoke individual tokens - Complexity ### Option 2: Session-based auth Traditional server-side sessions. **Pros:** - Simple implementation - Easy revocation - Smaller payloads **Cons:** - Requires session store - Not suitable for APIs - Scaling challenges ### Option 3: API keys only Simple key-based authentication. **Pros:** - Very simple - Low overhead - Easy to implement **Cons:** - No user context - Hard to rotate - No SSO support ## Decision We will implement OAuth 2.0 with JWT tokens. Access tokens expire after 1 hour, refresh tokens after 7 days. ## Consequences - Implement token rotation on refresh - Support API keys for machine-to-machine auth - Add MFA for admin accounts",
              "score": null
            },
            {
              "doc_id": "arch_api_gateway",
              "chunk_id": "arch_api_gateway_fix_0",
              "content": "# API Gateway Architecture ## Overview The API Gateway handles all incoming HTTP requests, providing authentication, rate limiting, and request routing. Built on Kong with custom plugins for CloudFlow-specific functionality. ## Design Principles - **Zero Trust**: Every request is authenticated and authorized - **Defense in Depth**: Multiple layers of security validation ## Components ### Kong Gateway Primary ingress controller. Handles TLS termination, request routing, and plugin execution. **Technology:** Kong 3.4 on Kubernetes **Scaling:** Horizontal with HPA ### Auth Plugin Validates JWT tokens, API keys, and OAuth2 flows. Token validation latency under 5ms. **Technology:** Custom Lua plugin **Scaling:** Stateless ### Rate Limiter Enforces per-user and per-IP rate limits. Default limit is 100 requests per minute. **Technology:** Redis-backed sliding window **Scaling:** Shared Redis cluster ## Data Flow Client \u2192 TLS \u2192 Kong \u2192 Auth \u2192 Rate Limit \u2192 Backend Services ## Performance Characteristics - **Latency P50:** 12ms - **Latency P99:** 45ms - **Throughput:** 50000 requests/second",
              "score": null
            },
            {
              "doc_id": "release_3_0_0",
              "chunk_id": "release_3_0_0_fix_0",
              "content": "# Release Notes - v3.0.0 **Release Date:** 2023-10-15 ## New Features - Complete UI redesign - New workflow editor with visual builder - Real-time execution monitoring - Team workspaces ## Bug Fixes - Fixed critical security issue in token validation - Resolved data loss bug in failed migrations ## Breaking Changes - **BREAKING:** Authentication API completely rewritten - **BREAKING:** Legacy webhook format no longer supported - **BREAKING:** Minimum Node.js version now 18 ## Upgrade Instructions ```bash cloudflow upgrade --version 3.0.0 ``` ## Known Issues None at this time.",
              "score": null
            },
            {
              "doc_id": "release_3_1_0",
              "chunk_id": "release_3_1_0_fix_0",
              "content": "# Release Notes - v3.1.0 **Release Date:** 2023-12-01 ## New Features - GraphQL API beta release - Workflow templates marketplace - Enhanced audit logging - SSO support for Okta ## Bug Fixes - Fixed webhook delivery retry logic - Resolved issue with large payload handling - Fixed dashboard loading performance ## Upgrade Instructions ```bash cloudflow upgrade --version 3.1.0 ``` ## Known Issues None at this time.",
              "score": null
            }
          ],
          "latency_ms": 3.4462980001990218,
          "query_id": "release_047",
          "dimension": "problem",
          "query": "Our security team flagged a token validation issue, was this addressed in any recent updates?"
        },
        {
          "key_facts": [
            "critical security issue",
            "token validation"
          ],
          "found_facts": [
            "critical security issue",
            "token validation"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "release_3_0_0"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "release_3_1_0",
              "chunk_id": "release_3_1_0_fix_0",
              "content": "# Release Notes - v3.1.0 **Release Date:** 2023-12-01 ## New Features - GraphQL API beta release - Workflow templates marketplace - Enhanced audit logging - SSO support for Okta ## Bug Fixes - Fixed webhook delivery retry logic - Resolved issue with large payload handling - Fixed dashboard loading performance ## Upgrade Instructions ```bash cloudflow upgrade --version 3.1.0 ``` ## Known Issues None at this time.",
              "score": null
            },
            {
              "doc_id": "release_3_0_0",
              "chunk_id": "release_3_0_0_fix_0",
              "content": "# Release Notes - v3.0.0 **Release Date:** 2023-10-15 ## New Features - Complete UI redesign - New workflow editor with visual builder - Real-time execution monitoring - Team workspaces ## Bug Fixes - Fixed critical security issue in token validation - Resolved data loss bug in failed migrations ## Breaking Changes - **BREAKING:** Authentication API completely rewritten - **BREAKING:** Legacy webhook format no longer supported - **BREAKING:** Minimum Node.js version now 18 ## Upgrade Instructions ```bash cloudflow upgrade --version 3.0.0 ``` ## Known Issues None at this time.",
              "score": null
            },
            {
              "doc_id": "release_3_2_0",
              "chunk_id": "release_3_2_0_fix_0",
              "content": "# Release Notes - v3.2.0 **Release Date:** 2024-01-15 ## New Features - Workflow versioning with rollback support - New Python SDK with async support - Bulk operations API for workflows - Custom retry policies per step ## Bug Fixes - Fixed race condition in concurrent execution handler - Resolved memory leak in long-running workflows - Fixed timezone handling in scheduled triggers ## Breaking Changes - **BREAKING:** API v1 endpoints deprecated, will be removed in v4.0 - **BREAKING:** Minimum Python version now 3.9 ## Upgrade Instructions ```bash cloudflow upgrade --version 3.2.0 ``` ## Known Issues None at this time.",
              "score": null
            },
            {
              "doc_id": "arch_authentication_service",
              "chunk_id": "arch_authentication_service_fix_0",
              "content": "# Authentication Service Architecture ## Overview The Authentication Service handles user identity, session management, and token issuance. Supports OAuth 2.0, SAML, and API key authentication. ## Design Principles - **Zero Knowledge**: Passwords never stored in plaintext - **Token Rotation**: Refresh tokens rotate on each use ## Components ### Identity Provider Manages user accounts and credentials. Password hashing uses bcrypt with cost factor 12. **Technology:** Custom Go service **Scaling:** Horizontal stateless ### Token Service Issues and validates access tokens. Token lifetime is 1 hour with 7-day refresh tokens. **Technology:** JWT with RS256 **Scaling:** Horizontal stateless ### SSO Gateway Integrates with external identity providers. Supports Google, Microsoft, Okta, and custom SAML. **Technology:** Dex OIDC proxy **Scaling:** Active-passive ## Data Flow Credentials \u2192 Validation \u2192 Token Issue \u2192 Token Refresh \u2192 Logout ## Performance Characteristics - **Latency P50:** 50ms - **Latency P99:** 200ms - **Throughput:** 5000 auth/second",
              "score": null
            },
            {
              "doc_id": "release_2_9_0",
              "chunk_id": "release_2_9_0_fix_0",
              "content": "# Release Notes - v2.9.0 **Release Date:** 2023-08-01 ## New Features - Parallel step execution - Conditional branching in workflows - Slack integration improvements - Performance dashboard ## Bug Fixes - Fixed issue with special characters in secrets - Resolved timeout in large file uploads ## Upgrade Instructions ```bash cloudflow upgrade --version 2.9.0 ``` ## Known Issues None at this time.",
              "score": null
            }
          ],
          "latency_ms": 3.4927439992316067,
          "query_id": "release_047",
          "dimension": "casual",
          "query": "v3.0.0 security fixes"
        },
        {
          "key_facts": [
            "critical security issue",
            "token validation"
          ],
          "found_facts": [
            "critical security issue",
            "token validation"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "release_3_0_0"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "release_3_1_0",
              "chunk_id": "release_3_1_0_fix_0",
              "content": "# Release Notes - v3.1.0 **Release Date:** 2023-12-01 ## New Features - GraphQL API beta release - Workflow templates marketplace - Enhanced audit logging - SSO support for Okta ## Bug Fixes - Fixed webhook delivery retry logic - Resolved issue with large payload handling - Fixed dashboard loading performance ## Upgrade Instructions ```bash cloudflow upgrade --version 3.1.0 ``` ## Known Issues None at this time.",
              "score": null
            },
            {
              "doc_id": "release_2_9_0",
              "chunk_id": "release_2_9_0_fix_0",
              "content": "# Release Notes - v2.9.0 **Release Date:** 2023-08-01 ## New Features - Parallel step execution - Conditional branching in workflows - Slack integration improvements - Performance dashboard ## Bug Fixes - Fixed issue with special characters in secrets - Resolved timeout in large file uploads ## Upgrade Instructions ```bash cloudflow upgrade --version 2.9.0 ``` ## Known Issues None at this time.",
              "score": null
            },
            {
              "doc_id": "runbook_api_gateway_5xx_errors",
              "chunk_id": "runbook_api_gateway_5xx_errors_fix_0",
              "content": "# Runbook: API Gateway 5xx Errors ## Severity **Level:** P3 - Medium **Response Time:** 1 hour ## Symptoms - Error rate > 1% - 5xx responses in access logs - Downstream service failures ## Diagnosis ### Step 1 Check Kong error logs: `kubectl logs -l app=kong -n kong --tail=100 | grep -E '5[0-9]{2}'` ### Step 2 Verify backend health: `kubectl get pods -n production -o wide` ### Step 3 Check service endpoints: `kubectl get endpoints -n production` ### Step 4 Test backend directly: `kubectl exec -it kong-xxx -n kong -- curl http://api.production.svc:8080/health` ## Resolution ### Step 1 Restart unhealthy backends: `kubectl rollout restart deployment/api -n production` ### Step 2 Enable circuit breaker if cascading: update Kong circuit-breaker plugin ### Step 3 Scale up backends: `kubectl scale deployment/api --replicas=5 -n production` ### Step 4 Check for resource exhaustion on backend pods ### Step 5 Verify network policies allow traffic ## Escalation If 5xx persists > 15 minutes, page on-call. May indicate infrastructure issue. ## Post-Incident - Document timeline in incident log - Schedule post-mortem within 48 hours - Update runbook if new patterns discovered",
              "score": null
            },
            {
              "doc_id": "release_3_0_0",
              "chunk_id": "release_3_0_0_fix_0",
              "content": "# Release Notes - v3.0.0 **Release Date:** 2023-10-15 ## New Features - Complete UI redesign - New workflow editor with visual builder - Real-time execution monitoring - Team workspaces ## Bug Fixes - Fixed critical security issue in token validation - Resolved data loss bug in failed migrations ## Breaking Changes - **BREAKING:** Authentication API completely rewritten - **BREAKING:** Legacy webhook format no longer supported - **BREAKING:** Minimum Node.js version now 18 ## Upgrade Instructions ```bash cloudflow upgrade --version 3.0.0 ``` ## Known Issues None at this time.",
              "score": null
            },
            {
              "doc_id": "howto_implement_workflow_versioning",
              "chunk_id": "howto_implement_workflow_versioning_fix_0",
              "content": "# How to Implement Workflow Versioning ## Prerequisites - Existing workflow ## Steps ### Step 1: Enable versioning Turn on versioning for your workflow. This is required for the feature. ```bash cloudflow workflows update wf_123 --enable-versioning ``` ### Step 2: Create new version Save changes as a new version instead of overwriting. ```bash cloudflow workflows publish wf_123 --version 2.0.0 ``` ### Step 3: Set active version Choose which version handles new executions. ```bash cloudflow workflows activate wf_123 --version 2.0.0 ``` ### Step 4: Roll back if needed Quickly revert to a previous version. ```bash cloudflow workflows rollback wf_123 --to-version 1.0.0 ``` ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            }
          ],
          "latency_ms": 4.869108001003042,
          "query_id": "release_047",
          "dimension": "contextual",
          "query": "Running a security audit and need to verify if the token validation vulnerability was fixed, which version has that patch?"
        },
        {
          "key_facts": [
            "critical security issue",
            "token validation"
          ],
          "found_facts": [
            "token validation"
          ],
          "missed_facts": [
            "critical security issue"
          ],
          "coverage": 0.5,
          "expected_docs": [
            "release_3_0_0"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "arch_authentication_service",
              "chunk_id": "arch_authentication_service_fix_0",
              "content": "# Authentication Service Architecture ## Overview The Authentication Service handles user identity, session management, and token issuance. Supports OAuth 2.0, SAML, and API key authentication. ## Design Principles - **Zero Knowledge**: Passwords never stored in plaintext - **Token Rotation**: Refresh tokens rotate on each use ## Components ### Identity Provider Manages user accounts and credentials. Password hashing uses bcrypt with cost factor 12. **Technology:** Custom Go service **Scaling:** Horizontal stateless ### Token Service Issues and validates access tokens. Token lifetime is 1 hour with 7-day refresh tokens. **Technology:** JWT with RS256 **Scaling:** Horizontal stateless ### SSO Gateway Integrates with external identity providers. Supports Google, Microsoft, Okta, and custom SAML. **Technology:** Dex OIDC proxy **Scaling:** Active-passive ## Data Flow Credentials \u2192 Validation \u2192 Token Issue \u2192 Token Refresh \u2192 Logout ## Performance Characteristics - **Latency P50:** 50ms - **Latency P99:** 200ms - **Throughput:** 5000 auth/second",
              "score": null
            },
            {
              "doc_id": "adr_003",
              "chunk_id": "adr_003_fix_0",
              "content": "# ADR-003: Implement OAuth 2.0 with JWT for Authentication **Status:** Accepted **Date:** 2024-05-08 ## Context We need a secure authentication system that supports API keys, user sessions, and third-party integrations. ## Options Considered ### Option 1: OAuth 2.0 with JWT Industry standard token-based auth. **Pros:** - Industry standard - Stateless validation - Supports refresh tokens - SSO compatible **Cons:** - Token size overhead - Cannot revoke individual tokens - Complexity ### Option 2: Session-based auth Traditional server-side sessions. **Pros:** - Simple implementation - Easy revocation - Smaller payloads **Cons:** - Requires session store - Not suitable for APIs - Scaling challenges ### Option 3: API keys only Simple key-based authentication. **Pros:** - Very simple - Low overhead - Easy to implement **Cons:** - No user context - Hard to rotate - No SSO support ## Decision We will implement OAuth 2.0 with JWT tokens. Access tokens expire after 1 hour, refresh tokens after 7 days. ## Consequences - Implement token rotation on refresh - Support API keys for machine-to-machine auth - Add MFA for admin accounts",
              "score": null
            },
            {
              "doc_id": "release_2_9_0",
              "chunk_id": "release_2_9_0_fix_0",
              "content": "# Release Notes - v2.9.0 **Release Date:** 2023-08-01 ## New Features - Parallel step execution - Conditional branching in workflows - Slack integration improvements - Performance dashboard ## Bug Fixes - Fixed issue with special characters in secrets - Resolved timeout in large file uploads ## Upgrade Instructions ```bash cloudflow upgrade --version 2.9.0 ``` ## Known Issues None at this time.",
              "score": null
            },
            {
              "doc_id": "arch_api_gateway",
              "chunk_id": "arch_api_gateway_fix_0",
              "content": "# API Gateway Architecture ## Overview The API Gateway handles all incoming HTTP requests, providing authentication, rate limiting, and request routing. Built on Kong with custom plugins for CloudFlow-specific functionality. ## Design Principles - **Zero Trust**: Every request is authenticated and authorized - **Defense in Depth**: Multiple layers of security validation ## Components ### Kong Gateway Primary ingress controller. Handles TLS termination, request routing, and plugin execution. **Technology:** Kong 3.4 on Kubernetes **Scaling:** Horizontal with HPA ### Auth Plugin Validates JWT tokens, API keys, and OAuth2 flows. Token validation latency under 5ms. **Technology:** Custom Lua plugin **Scaling:** Stateless ### Rate Limiter Enforces per-user and per-IP rate limits. Default limit is 100 requests per minute. **Technology:** Redis-backed sliding window **Scaling:** Shared Redis cluster ## Data Flow Client \u2192 TLS \u2192 Kong \u2192 Auth \u2192 Rate Limit \u2192 Backend Services ## Performance Characteristics - **Latency P50:** 12ms - **Latency P99:** 45ms - **Throughput:** 50000 requests/second",
              "score": null
            },
            {
              "doc_id": "release_3_1_0",
              "chunk_id": "release_3_1_0_fix_0",
              "content": "# Release Notes - v3.1.0 **Release Date:** 2023-12-01 ## New Features - GraphQL API beta release - Workflow templates marketplace - Enhanced audit logging - SSO support for Okta ## Bug Fixes - Fixed webhook delivery retry logic - Resolved issue with large payload handling - Fixed dashboard loading performance ## Upgrade Instructions ```bash cloudflow upgrade --version 3.1.0 ``` ## Known Issues None at this time.",
              "score": null
            }
          ],
          "latency_ms": 3.950316999180359,
          "query_id": "release_047",
          "dimension": "negation",
          "query": "Is there a known security vulnerability in token validation that hasn't been patched?"
        },
        {
          "key_facts": [
            "Authentication API",
            "Legacy webhook format",
            "Node.js version now 18"
          ],
          "found_facts": [
            "Authentication API",
            "Legacy webhook format",
            "Node.js version now 18"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "release_3_0_0"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "release_3_1_0",
              "chunk_id": "release_3_1_0_fix_0",
              "content": "# Release Notes - v3.1.0 **Release Date:** 2023-12-01 ## New Features - GraphQL API beta release - Workflow templates marketplace - Enhanced audit logging - SSO support for Okta ## Bug Fixes - Fixed webhook delivery retry logic - Resolved issue with large payload handling - Fixed dashboard loading performance ## Upgrade Instructions ```bash cloudflow upgrade --version 3.1.0 ``` ## Known Issues None at this time.",
              "score": null
            },
            {
              "doc_id": "release_2_9_0",
              "chunk_id": "release_2_9_0_fix_0",
              "content": "# Release Notes - v2.9.0 **Release Date:** 2023-08-01 ## New Features - Parallel step execution - Conditional branching in workflows - Slack integration improvements - Performance dashboard ## Bug Fixes - Fixed issue with special characters in secrets - Resolved timeout in large file uploads ## Upgrade Instructions ```bash cloudflow upgrade --version 2.9.0 ``` ## Known Issues None at this time.",
              "score": null
            },
            {
              "doc_id": "release_3_0_0",
              "chunk_id": "release_3_0_0_fix_0",
              "content": "# Release Notes - v3.0.0 **Release Date:** 2023-10-15 ## New Features - Complete UI redesign - New workflow editor with visual builder - Real-time execution monitoring - Team workspaces ## Bug Fixes - Fixed critical security issue in token validation - Resolved data loss bug in failed migrations ## Breaking Changes - **BREAKING:** Authentication API completely rewritten - **BREAKING:** Legacy webhook format no longer supported - **BREAKING:** Minimum Node.js version now 18 ## Upgrade Instructions ```bash cloudflow upgrade --version 3.0.0 ``` ## Known Issues None at this time.",
              "score": null
            },
            {
              "doc_id": "release_3_2_0",
              "chunk_id": "release_3_2_0_fix_0",
              "content": "# Release Notes - v3.2.0 **Release Date:** 2024-01-15 ## New Features - Workflow versioning with rollback support - New Python SDK with async support - Bulk operations API for workflows - Custom retry policies per step ## Bug Fixes - Fixed race condition in concurrent execution handler - Resolved memory leak in long-running workflows - Fixed timezone handling in scheduled triggers ## Breaking Changes - **BREAKING:** API v1 endpoints deprecated, will be removed in v4.0 - **BREAKING:** Minimum Python version now 3.9 ## Upgrade Instructions ```bash cloudflow upgrade --version 3.2.0 ``` ## Known Issues None at this time.",
              "score": null
            },
            {
              "doc_id": "howto_implement_workflow_versioning",
              "chunk_id": "howto_implement_workflow_versioning_fix_0",
              "content": "# How to Implement Workflow Versioning ## Prerequisites - Existing workflow ## Steps ### Step 1: Enable versioning Turn on versioning for your workflow. This is required for the feature. ```bash cloudflow workflows update wf_123 --enable-versioning ``` ### Step 2: Create new version Save changes as a new version instead of overwriting. ```bash cloudflow workflows publish wf_123 --version 2.0.0 ``` ### Step 3: Set active version Choose which version handles new executions. ```bash cloudflow workflows activate wf_123 --version 2.0.0 ``` ### Step 4: Roll back if needed Quickly revert to a previous version. ```bash cloudflow workflows rollback wf_123 --to-version 1.0.0 ``` ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            }
          ],
          "latency_ms": 3.6110350010858383,
          "query_id": "release_048",
          "dimension": "original",
          "query": "What breaking changes are in v3.0.0?"
        },
        {
          "key_facts": [
            "Authentication API",
            "Legacy webhook format",
            "Node.js version now 18"
          ],
          "found_facts": [
            "Authentication API",
            "Legacy webhook format",
            "Node.js version now 18"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "release_3_0_0"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "release_3_1_0",
              "chunk_id": "release_3_1_0_fix_0",
              "content": "# Release Notes - v3.1.0 **Release Date:** 2023-12-01 ## New Features - GraphQL API beta release - Workflow templates marketplace - Enhanced audit logging - SSO support for Okta ## Bug Fixes - Fixed webhook delivery retry logic - Resolved issue with large payload handling - Fixed dashboard loading performance ## Upgrade Instructions ```bash cloudflow upgrade --version 3.1.0 ``` ## Known Issues None at this time.",
              "score": null
            },
            {
              "doc_id": "release_2_9_0",
              "chunk_id": "release_2_9_0_fix_0",
              "content": "# Release Notes - v2.9.0 **Release Date:** 2023-08-01 ## New Features - Parallel step execution - Conditional branching in workflows - Slack integration improvements - Performance dashboard ## Bug Fixes - Fixed issue with special characters in secrets - Resolved timeout in large file uploads ## Upgrade Instructions ```bash cloudflow upgrade --version 2.9.0 ``` ## Known Issues None at this time.",
              "score": null
            },
            {
              "doc_id": "release_3_0_0",
              "chunk_id": "release_3_0_0_fix_0",
              "content": "# Release Notes - v3.0.0 **Release Date:** 2023-10-15 ## New Features - Complete UI redesign - New workflow editor with visual builder - Real-time execution monitoring - Team workspaces ## Bug Fixes - Fixed critical security issue in token validation - Resolved data loss bug in failed migrations ## Breaking Changes - **BREAKING:** Authentication API completely rewritten - **BREAKING:** Legacy webhook format no longer supported - **BREAKING:** Minimum Node.js version now 18 ## Upgrade Instructions ```bash cloudflow upgrade --version 3.0.0 ``` ## Known Issues None at this time.",
              "score": null
            },
            {
              "doc_id": "release_3_2_0",
              "chunk_id": "release_3_2_0_fix_0",
              "content": "# Release Notes - v3.2.0 **Release Date:** 2024-01-15 ## New Features - Workflow versioning with rollback support - New Python SDK with async support - Bulk operations API for workflows - Custom retry policies per step ## Bug Fixes - Fixed race condition in concurrent execution handler - Resolved memory leak in long-running workflows - Fixed timezone handling in scheduled triggers ## Breaking Changes - **BREAKING:** API v1 endpoints deprecated, will be removed in v4.0 - **BREAKING:** Minimum Python version now 3.9 ## Upgrade Instructions ```bash cloudflow upgrade --version 3.2.0 ``` ## Known Issues None at this time.",
              "score": null
            },
            {
              "doc_id": "howto_implement_workflow_versioning",
              "chunk_id": "howto_implement_workflow_versioning_fix_0",
              "content": "# How to Implement Workflow Versioning ## Prerequisites - Existing workflow ## Steps ### Step 1: Enable versioning Turn on versioning for your workflow. This is required for the feature. ```bash cloudflow workflows update wf_123 --enable-versioning ``` ### Step 2: Create new version Save changes as a new version instead of overwriting. ```bash cloudflow workflows publish wf_123 --version 2.0.0 ``` ### Step 3: Set active version Choose which version handles new executions. ```bash cloudflow workflows activate wf_123 --version 2.0.0 ``` ### Step 4: Roll back if needed Quickly revert to a previous version. ```bash cloudflow workflows rollback wf_123 --to-version 1.0.0 ``` ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            }
          ],
          "latency_ms": 3.484560000288184,
          "query_id": "release_048",
          "dimension": "synonym",
          "query": "What backwards-incompatible changes and migration requirements are in version 3.0.0?"
        },
        {
          "key_facts": [
            "Authentication API",
            "Legacy webhook format",
            "Node.js version now 18"
          ],
          "found_facts": [
            "Authentication API",
            "Legacy webhook format",
            "Node.js version now 18"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "release_3_0_0"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "api_webhooks",
              "chunk_id": "api_webhooks_fix_0",
              "content": "# Webhooks API Reference ## Overview The Webhooks API provides programmatic access to webhooks functionality. Base URL: `https://api.cloudflow.io/v1/webhooks` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /webhooks List configured webhooks. **Parameters:** - `active` (boolean): Filter by active status **Response:** - Success: `200` - Returns array of webhook objects - Error: `400` - Bad Request **Example:** ```json {\"webhooks\": [{\"id\": \"wh_01\", \"url\": \"https://example.com/hook\", \"events\": [\"workflow.completed\"]}]} ``` ### POST /webhooks Create a new webhook endpoint. **Parameters:** - `url` (string): Webhook URL (must be HTTPS) - `events` (array): Events to subscribe to - `secret` (string): Signing secret for verification **Response:** - Success: `201` - Webhook created - Error: `400` - Bad Request **Example:** ```json {\"id\": \"wh_02\", \"url\": \"https://example.com/new-hook\", \"secret\": \"whsec_...\"} ```",
              "score": null
            },
            {
              "doc_id": "release_3_0_0",
              "chunk_id": "release_3_0_0_fix_0",
              "content": "# Release Notes - v3.0.0 **Release Date:** 2023-10-15 ## New Features - Complete UI redesign - New workflow editor with visual builder - Real-time execution monitoring - Team workspaces ## Bug Fixes - Fixed critical security issue in token validation - Resolved data loss bug in failed migrations ## Breaking Changes - **BREAKING:** Authentication API completely rewritten - **BREAKING:** Legacy webhook format no longer supported - **BREAKING:** Minimum Node.js version now 18 ## Upgrade Instructions ```bash cloudflow upgrade --version 3.0.0 ``` ## Known Issues None at this time.",
              "score": null
            },
            {
              "doc_id": "release_3_1_0",
              "chunk_id": "release_3_1_0_fix_0",
              "content": "# Release Notes - v3.1.0 **Release Date:** 2023-12-01 ## New Features - GraphQL API beta release - Workflow templates marketplace - Enhanced audit logging - SSO support for Okta ## Bug Fixes - Fixed webhook delivery retry logic - Resolved issue with large payload handling - Fixed dashboard loading performance ## Upgrade Instructions ```bash cloudflow upgrade --version 3.1.0 ``` ## Known Issues None at this time.",
              "score": null
            },
            {
              "doc_id": "release_3_2_0",
              "chunk_id": "release_3_2_0_fix_0",
              "content": "# Release Notes - v3.2.0 **Release Date:** 2024-01-15 ## New Features - Workflow versioning with rollback support - New Python SDK with async support - Bulk operations API for workflows - Custom retry policies per step ## Bug Fixes - Fixed race condition in concurrent execution handler - Resolved memory leak in long-running workflows - Fixed timezone handling in scheduled triggers ## Breaking Changes - **BREAKING:** API v1 endpoints deprecated, will be removed in v4.0 - **BREAKING:** Minimum Python version now 3.9 ## Upgrade Instructions ```bash cloudflow upgrade --version 3.2.0 ``` ## Known Issues None at this time.",
              "score": null
            },
            {
              "doc_id": "arch_authentication_service",
              "chunk_id": "arch_authentication_service_fix_0",
              "content": "# Authentication Service Architecture ## Overview The Authentication Service handles user identity, session management, and token issuance. Supports OAuth 2.0, SAML, and API key authentication. ## Design Principles - **Zero Knowledge**: Passwords never stored in plaintext - **Token Rotation**: Refresh tokens rotate on each use ## Components ### Identity Provider Manages user accounts and credentials. Password hashing uses bcrypt with cost factor 12. **Technology:** Custom Go service **Scaling:** Horizontal stateless ### Token Service Issues and validates access tokens. Token lifetime is 1 hour with 7-day refresh tokens. **Technology:** JWT with RS256 **Scaling:** Horizontal stateless ### SSO Gateway Integrates with external identity providers. Supports Google, Microsoft, Okta, and custom SAML. **Technology:** Dex OIDC proxy **Scaling:** Active-passive ## Data Flow Credentials \u2192 Validation \u2192 Token Issue \u2192 Token Refresh \u2192 Logout ## Performance Characteristics - **Latency P50:** 50ms - **Latency P99:** 200ms - **Throughput:** 5000 auth/second",
              "score": null
            }
          ],
          "latency_ms": 3.6548759999277536,
          "query_id": "release_048",
          "dimension": "problem",
          "query": "After upgrading to 3.0.0 our webhooks stopped working and auth is broken, what changed?"
        },
        {
          "key_facts": [
            "Authentication API",
            "Legacy webhook format",
            "Node.js version now 18"
          ],
          "found_facts": [
            "Authentication API",
            "Legacy webhook format",
            "Node.js version now 18"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "release_3_0_0"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "release_3_0_0",
              "chunk_id": "release_3_0_0_fix_0",
              "content": "# Release Notes - v3.0.0 **Release Date:** 2023-10-15 ## New Features - Complete UI redesign - New workflow editor with visual builder - Real-time execution monitoring - Team workspaces ## Bug Fixes - Fixed critical security issue in token validation - Resolved data loss bug in failed migrations ## Breaking Changes - **BREAKING:** Authentication API completely rewritten - **BREAKING:** Legacy webhook format no longer supported - **BREAKING:** Minimum Node.js version now 18 ## Upgrade Instructions ```bash cloudflow upgrade --version 3.0.0 ``` ## Known Issues None at this time.",
              "score": null
            },
            {
              "doc_id": "release_3_1_0",
              "chunk_id": "release_3_1_0_fix_0",
              "content": "# Release Notes - v3.1.0 **Release Date:** 2023-12-01 ## New Features - GraphQL API beta release - Workflow templates marketplace - Enhanced audit logging - SSO support for Okta ## Bug Fixes - Fixed webhook delivery retry logic - Resolved issue with large payload handling - Fixed dashboard loading performance ## Upgrade Instructions ```bash cloudflow upgrade --version 3.1.0 ``` ## Known Issues None at this time.",
              "score": null
            },
            {
              "doc_id": "release_3_2_0",
              "chunk_id": "release_3_2_0_fix_0",
              "content": "# Release Notes - v3.2.0 **Release Date:** 2024-01-15 ## New Features - Workflow versioning with rollback support - New Python SDK with async support - Bulk operations API for workflows - Custom retry policies per step ## Bug Fixes - Fixed race condition in concurrent execution handler - Resolved memory leak in long-running workflows - Fixed timezone handling in scheduled triggers ## Breaking Changes - **BREAKING:** API v1 endpoints deprecated, will be removed in v4.0 - **BREAKING:** Minimum Python version now 3.9 ## Upgrade Instructions ```bash cloudflow upgrade --version 3.2.0 ``` ## Known Issues None at this time.",
              "score": null
            },
            {
              "doc_id": "howto_migrate_from_v1_to_v2_api",
              "chunk_id": "howto_migrate_from_v1_to_v2_api_fix_0",
              "content": "# How to Migrate from v1 to v2 API ## Prerequisites - Existing v1 integration - v2 API access enabled ## Steps ### Step 1: Review breaking changes The v2 API changes authentication from API keys to OAuth 2.0. Review the migration guide. ### Step 2: Update client library Upgrade to the latest SDK version. ```bash npm install @cloudflow/sdk@latest ``` ### Step 3: Update authentication Replace API key authentication with OAuth 2.0 client credentials flow. > **Note:** Generate new credentials in the dashboard ### Step 4: Test in staging Run your integration tests against the v2 staging endpoint. ```bash CLOUDFLOW_API_URL=https://api-v2-staging.cloudflow.io npm test ``` ### Step 5: Switch production Update production configuration to use v2 API. ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            },
            {
              "doc_id": "release_2_9_0",
              "chunk_id": "release_2_9_0_fix_0",
              "content": "# Release Notes - v2.9.0 **Release Date:** 2023-08-01 ## New Features - Parallel step execution - Conditional branching in workflows - Slack integration improvements - Performance dashboard ## Bug Fixes - Fixed issue with special characters in secrets - Resolved timeout in large file uploads ## Upgrade Instructions ```bash cloudflow upgrade --version 2.9.0 ``` ## Known Issues None at this time.",
              "score": null
            }
          ],
          "latency_ms": 3.689010000016424,
          "query_id": "release_048",
          "dimension": "casual",
          "query": "v3.0 breaking changes nodejs"
        },
        {
          "key_facts": [
            "Authentication API",
            "Legacy webhook format",
            "Node.js version now 18"
          ],
          "found_facts": [
            "Authentication API",
            "Legacy webhook format",
            "Node.js version now 18"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "release_3_0_0"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "release_3_1_0",
              "chunk_id": "release_3_1_0_fix_0",
              "content": "# Release Notes - v3.1.0 **Release Date:** 2023-12-01 ## New Features - GraphQL API beta release - Workflow templates marketplace - Enhanced audit logging - SSO support for Okta ## Bug Fixes - Fixed webhook delivery retry logic - Resolved issue with large payload handling - Fixed dashboard loading performance ## Upgrade Instructions ```bash cloudflow upgrade --version 3.1.0 ``` ## Known Issues None at this time.",
              "score": null
            },
            {
              "doc_id": "release_3_0_0",
              "chunk_id": "release_3_0_0_fix_0",
              "content": "# Release Notes - v3.0.0 **Release Date:** 2023-10-15 ## New Features - Complete UI redesign - New workflow editor with visual builder - Real-time execution monitoring - Team workspaces ## Bug Fixes - Fixed critical security issue in token validation - Resolved data loss bug in failed migrations ## Breaking Changes - **BREAKING:** Authentication API completely rewritten - **BREAKING:** Legacy webhook format no longer supported - **BREAKING:** Minimum Node.js version now 18 ## Upgrade Instructions ```bash cloudflow upgrade --version 3.0.0 ``` ## Known Issues None at this time.",
              "score": null
            },
            {
              "doc_id": "release_2_9_0",
              "chunk_id": "release_2_9_0_fix_0",
              "content": "# Release Notes - v2.9.0 **Release Date:** 2023-08-01 ## New Features - Parallel step execution - Conditional branching in workflows - Slack integration improvements - Performance dashboard ## Bug Fixes - Fixed issue with special characters in secrets - Resolved timeout in large file uploads ## Upgrade Instructions ```bash cloudflow upgrade --version 2.9.0 ``` ## Known Issues None at this time.",
              "score": null
            },
            {
              "doc_id": "release_3_2_0",
              "chunk_id": "release_3_2_0_fix_0",
              "content": "# Release Notes - v3.2.0 **Release Date:** 2024-01-15 ## New Features - Workflow versioning with rollback support - New Python SDK with async support - Bulk operations API for workflows - Custom retry policies per step ## Bug Fixes - Fixed race condition in concurrent execution handler - Resolved memory leak in long-running workflows - Fixed timezone handling in scheduled triggers ## Breaking Changes - **BREAKING:** API v1 endpoints deprecated, will be removed in v4.0 - **BREAKING:** Minimum Python version now 3.9 ## Upgrade Instructions ```bash cloudflow upgrade --version 3.2.0 ``` ## Known Issues None at this time.",
              "score": null
            },
            {
              "doc_id": "howto_implement_workflow_versioning",
              "chunk_id": "howto_implement_workflow_versioning_fix_0",
              "content": "# How to Implement Workflow Versioning ## Prerequisites - Existing workflow ## Steps ### Step 1: Enable versioning Turn on versioning for your workflow. This is required for the feature. ```bash cloudflow workflows update wf_123 --enable-versioning ``` ### Step 2: Create new version Save changes as a new version instead of overwriting. ```bash cloudflow workflows publish wf_123 --version 2.0.0 ``` ### Step 3: Set active version Choose which version handles new executions. ```bash cloudflow workflows activate wf_123 --version 2.0.0 ``` ### Step 4: Roll back if needed Quickly revert to a previous version. ```bash cloudflow workflows rollback wf_123 --to-version 1.0.0 ``` ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            }
          ],
          "latency_ms": 4.093552999620442,
          "query_id": "release_048",
          "dimension": "contextual",
          "query": "Planning to upgrade from 2.x to 3.0.0, what will break and do I need to update our Node.js version?"
        },
        {
          "key_facts": [
            "Authentication API",
            "Legacy webhook format",
            "Node.js version now 18"
          ],
          "found_facts": [
            "Authentication API",
            "Legacy webhook format",
            "Node.js version now 18"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "release_3_0_0"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "api_webhooks",
              "chunk_id": "api_webhooks_fix_0",
              "content": "# Webhooks API Reference ## Overview The Webhooks API provides programmatic access to webhooks functionality. Base URL: `https://api.cloudflow.io/v1/webhooks` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /webhooks List configured webhooks. **Parameters:** - `active` (boolean): Filter by active status **Response:** - Success: `200` - Returns array of webhook objects - Error: `400` - Bad Request **Example:** ```json {\"webhooks\": [{\"id\": \"wh_01\", \"url\": \"https://example.com/hook\", \"events\": [\"workflow.completed\"]}]} ``` ### POST /webhooks Create a new webhook endpoint. **Parameters:** - `url` (string): Webhook URL (must be HTTPS) - `events` (array): Events to subscribe to - `secret` (string): Signing secret for verification **Response:** - Success: `201` - Webhook created - Error: `400` - Bad Request **Example:** ```json {\"id\": \"wh_02\", \"url\": \"https://example.com/new-hook\", \"secret\": \"whsec_...\"} ```",
              "score": null
            },
            {
              "doc_id": "release_3_0_0",
              "chunk_id": "release_3_0_0_fix_0",
              "content": "# Release Notes - v3.0.0 **Release Date:** 2023-10-15 ## New Features - Complete UI redesign - New workflow editor with visual builder - Real-time execution monitoring - Team workspaces ## Bug Fixes - Fixed critical security issue in token validation - Resolved data loss bug in failed migrations ## Breaking Changes - **BREAKING:** Authentication API completely rewritten - **BREAKING:** Legacy webhook format no longer supported - **BREAKING:** Minimum Node.js version now 18 ## Upgrade Instructions ```bash cloudflow upgrade --version 3.0.0 ``` ## Known Issues None at this time.",
              "score": null
            },
            {
              "doc_id": "release_3_1_0",
              "chunk_id": "release_3_1_0_fix_0",
              "content": "# Release Notes - v3.1.0 **Release Date:** 2023-12-01 ## New Features - GraphQL API beta release - Workflow templates marketplace - Enhanced audit logging - SSO support for Okta ## Bug Fixes - Fixed webhook delivery retry logic - Resolved issue with large payload handling - Fixed dashboard loading performance ## Upgrade Instructions ```bash cloudflow upgrade --version 3.1.0 ``` ## Known Issues None at this time.",
              "score": null
            },
            {
              "doc_id": "release_3_2_0",
              "chunk_id": "release_3_2_0_fix_0",
              "content": "# Release Notes - v3.2.0 **Release Date:** 2024-01-15 ## New Features - Workflow versioning with rollback support - New Python SDK with async support - Bulk operations API for workflows - Custom retry policies per step ## Bug Fixes - Fixed race condition in concurrent execution handler - Resolved memory leak in long-running workflows - Fixed timezone handling in scheduled triggers ## Breaking Changes - **BREAKING:** API v1 endpoints deprecated, will be removed in v4.0 - **BREAKING:** Minimum Python version now 3.9 ## Upgrade Instructions ```bash cloudflow upgrade --version 3.2.0 ``` ## Known Issues None at this time.",
              "score": null
            },
            {
              "doc_id": "runbook_api_gateway_5xx_errors",
              "chunk_id": "runbook_api_gateway_5xx_errors_fix_0",
              "content": "# Runbook: API Gateway 5xx Errors ## Severity **Level:** P3 - Medium **Response Time:** 1 hour ## Symptoms - Error rate > 1% - 5xx responses in access logs - Downstream service failures ## Diagnosis ### Step 1 Check Kong error logs: `kubectl logs -l app=kong -n kong --tail=100 | grep -E '5[0-9]{2}'` ### Step 2 Verify backend health: `kubectl get pods -n production -o wide` ### Step 3 Check service endpoints: `kubectl get endpoints -n production` ### Step 4 Test backend directly: `kubectl exec -it kong-xxx -n kong -- curl http://api.production.svc:8080/health` ## Resolution ### Step 1 Restart unhealthy backends: `kubectl rollout restart deployment/api -n production` ### Step 2 Enable circuit breaker if cascading: update Kong circuit-breaker plugin ### Step 3 Scale up backends: `kubectl scale deployment/api --replicas=5 -n production` ### Step 4 Check for resource exhaustion on backend pods ### Step 5 Verify network policies allow traffic ## Escalation If 5xx persists > 15 minutes, page on-call. May indicate infrastructure issue. ## Post-Incident - Document timeline in incident log - Schedule post-mortem within 48 hours - Update runbook if new patterns discovered",
              "score": null
            }
          ],
          "latency_ms": 3.471363999778987,
          "query_id": "release_048",
          "dimension": "negation",
          "query": "Why did the legacy webhook format stop working after updating?"
        },
        {
          "key_facts": [
            "GraphQL API beta",
            "2023-12-01"
          ],
          "found_facts": [
            "GraphQL API beta",
            "2023-12-01"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "release_3_1_0"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "release_3_1_0",
              "chunk_id": "release_3_1_0_fix_0",
              "content": "# Release Notes - v3.1.0 **Release Date:** 2023-12-01 ## New Features - GraphQL API beta release - Workflow templates marketplace - Enhanced audit logging - SSO support for Okta ## Bug Fixes - Fixed webhook delivery retry logic - Resolved issue with large payload handling - Fixed dashboard loading performance ## Upgrade Instructions ```bash cloudflow upgrade --version 3.1.0 ``` ## Known Issues None at this time.",
              "score": null
            },
            {
              "doc_id": "adr_001",
              "chunk_id": "adr_001_fix_0",
              "content": "# ADR-001: Use PostgreSQL for Primary Database **Status:** Accepted **Date:** 2024-11-04 ## Context We need a database to store workflow definitions, execution state, and user data. The database must support ACID transactions, JSON queries, and scale to millions of records. ## Options Considered ### Option 1: PostgreSQL Mature relational database with JSONB support. **Pros:** - ACID compliance - Rich JSONB queries - Team expertise - Proven at scale **Cons:** - Manual sharding for extreme scale - Requires connection pooling ### Option 2: MongoDB Document database with native JSON. **Pros:** - Flexible schema - Built-in sharding - Good for documents **Cons:** - Weaker consistency guarantees - Complex transactions - Higher operational cost ### Option 3: CockroachDB Distributed SQL database. **Pros:** - Automatic sharding - Strong consistency - PostgreSQL compatible **Cons:** - Higher latency - More expensive - Less mature ecosystem ## Decision We will use PostgreSQL as our primary database with JSONB for flexible workflow definitions. ## Consequences - Need PgBouncer for connection pooling - Must plan for sharding at 100M+ records - Requires regular VACUUM maintenance",
              "score": null
            },
            {
              "doc_id": "config_api_gateway",
              "chunk_id": "config_api_gateway_fix_0",
              "content": "# API Gateway Configuration Reference ## Overview This document describes all configuration options for API Gateway. ## Environment Variables | Variable | Type | Default | Description | |----------|------|---------|-------------| | `GATEWAY_PORT` | integer | 8000 | Gateway listen port | | `GATEWAY_RATE_LIMIT` | integer | 100 | Requests per minute per user | | `GATEWAY_TIMEOUT_MS` | integer | 30000 | Request timeout in milliseconds | | `GATEWAY_MAX_BODY_SIZE` | string | 10MB | Maximum request body size | | `GATEWAY_CORS_ORIGINS` | string | * | Allowed CORS origins | | `GATEWAY_SSL_ENABLED` | boolean | true | Enable TLS termination | ## Configuration File Configuration can also be provided via `api gateway.yaml`: ```yaml gateway_port: 8000 gateway_rate_limit: 100 gateway_timeout_ms: 30000 gateway_max_body_size: 10MB gateway_cors_origins: * ``` ## Validation Configuration is validated at startup. Invalid configuration will prevent the service from starting.",
              "score": null
            },
            {
              "doc_id": "api_metrics",
              "chunk_id": "api_metrics_fix_0",
              "content": "# Metrics API Reference ## Overview The Metrics API provides programmatic access to metrics functionality. Base URL: `https://api.cloudflow.io/v1/metrics` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /metrics/workflows Get workflow performance metrics. **Parameters:** - `period` (string): Time period: hour, day, week, month - `workflow_id` (string): Filter by workflow ID **Response:** - Success: `200` - Returns metrics data - Error: `400` - Bad Request **Example:** ```json {\"success_rate\": 0.985, \"avg_duration_ms\": 1250, \"p99_duration_ms\": 5000, \"total_executions\": 10000} ```",
              "score": null
            },
            {
              "doc_id": "release_3_0_0",
              "chunk_id": "release_3_0_0_fix_0",
              "content": "# Release Notes - v3.0.0 **Release Date:** 2023-10-15 ## New Features - Complete UI redesign - New workflow editor with visual builder - Real-time execution monitoring - Team workspaces ## Bug Fixes - Fixed critical security issue in token validation - Resolved data loss bug in failed migrations ## Breaking Changes - **BREAKING:** Authentication API completely rewritten - **BREAKING:** Legacy webhook format no longer supported - **BREAKING:** Minimum Node.js version now 18 ## Upgrade Instructions ```bash cloudflow upgrade --version 3.0.0 ``` ## Known Issues None at this time.",
              "score": null
            }
          ],
          "latency_ms": 3.488887999992585,
          "query_id": "release_049",
          "dimension": "original",
          "query": "When was GraphQL API released?"
        },
        {
          "key_facts": [
            "GraphQL API beta",
            "2023-12-01"
          ],
          "found_facts": [
            "GraphQL API beta",
            "2023-12-01"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "release_3_1_0"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "release_3_1_0",
              "chunk_id": "release_3_1_0_fix_0",
              "content": "# Release Notes - v3.1.0 **Release Date:** 2023-12-01 ## New Features - GraphQL API beta release - Workflow templates marketplace - Enhanced audit logging - SSO support for Okta ## Bug Fixes - Fixed webhook delivery retry logic - Resolved issue with large payload handling - Fixed dashboard loading performance ## Upgrade Instructions ```bash cloudflow upgrade --version 3.1.0 ``` ## Known Issues None at this time.",
              "score": null
            },
            {
              "doc_id": "adr_001",
              "chunk_id": "adr_001_fix_0",
              "content": "# ADR-001: Use PostgreSQL for Primary Database **Status:** Accepted **Date:** 2024-11-04 ## Context We need a database to store workflow definitions, execution state, and user data. The database must support ACID transactions, JSON queries, and scale to millions of records. ## Options Considered ### Option 1: PostgreSQL Mature relational database with JSONB support. **Pros:** - ACID compliance - Rich JSONB queries - Team expertise - Proven at scale **Cons:** - Manual sharding for extreme scale - Requires connection pooling ### Option 2: MongoDB Document database with native JSON. **Pros:** - Flexible schema - Built-in sharding - Good for documents **Cons:** - Weaker consistency guarantees - Complex transactions - Higher operational cost ### Option 3: CockroachDB Distributed SQL database. **Pros:** - Automatic sharding - Strong consistency - PostgreSQL compatible **Cons:** - Higher latency - More expensive - Less mature ecosystem ## Decision We will use PostgreSQL as our primary database with JSONB for flexible workflow definitions. ## Consequences - Need PgBouncer for connection pooling - Must plan for sharding at 100M+ records - Requires regular VACUUM maintenance",
              "score": null
            },
            {
              "doc_id": "arch_monitoring_stack",
              "chunk_id": "arch_monitoring_stack_fix_0",
              "content": "# Monitoring Stack Architecture ## Overview The Monitoring Stack provides observability into system health, performance, and errors. Includes metrics, logs, traces, and alerting. ## Design Principles - **Observability**: Every component emits metrics and traces - **Actionable Alerts**: Alerts include runbook links ## Components ### Metrics Collects time-series metrics. Retention is 15 days local, 1 year in object storage. **Technology:** Prometheus with Thanos **Scaling:** Federated with long-term storage ### Logging Centralized log aggregation. Supports structured JSON logs with label indexing. **Technology:** Loki with Grafana **Scaling:** Distributed with sharding ### Tracing Distributed request tracing. Trace retention is 7 days. **Technology:** Jaeger with OpenTelemetry **Scaling:** Sampling at 10% ### Alerting Routes alerts to on-call engineers. Supports escalation policies and silencing. **Technology:** Alertmanager with PagerDuty **Scaling:** HA pair ## Data Flow Application \u2192 Collector \u2192 Storage \u2192 Query \u2192 Dashboard/Alert ## Performance Characteristics - **Latency P50:** N/A - **Latency P99:** N/A - **Throughput:** 1M metrics/second, 100GB logs/day",
              "score": null
            },
            {
              "doc_id": "release_3_0_0",
              "chunk_id": "release_3_0_0_fix_0",
              "content": "# Release Notes - v3.0.0 **Release Date:** 2023-10-15 ## New Features - Complete UI redesign - New workflow editor with visual builder - Real-time execution monitoring - Team workspaces ## Bug Fixes - Fixed critical security issue in token validation - Resolved data loss bug in failed migrations ## Breaking Changes - **BREAKING:** Authentication API completely rewritten - **BREAKING:** Legacy webhook format no longer supported - **BREAKING:** Minimum Node.js version now 18 ## Upgrade Instructions ```bash cloudflow upgrade --version 3.0.0 ``` ## Known Issues None at this time.",
              "score": null
            },
            {
              "doc_id": "config_api_gateway",
              "chunk_id": "config_api_gateway_fix_0",
              "content": "# API Gateway Configuration Reference ## Overview This document describes all configuration options for API Gateway. ## Environment Variables | Variable | Type | Default | Description | |----------|------|---------|-------------| | `GATEWAY_PORT` | integer | 8000 | Gateway listen port | | `GATEWAY_RATE_LIMIT` | integer | 100 | Requests per minute per user | | `GATEWAY_TIMEOUT_MS` | integer | 30000 | Request timeout in milliseconds | | `GATEWAY_MAX_BODY_SIZE` | string | 10MB | Maximum request body size | | `GATEWAY_CORS_ORIGINS` | string | * | Allowed CORS origins | | `GATEWAY_SSL_ENABLED` | boolean | true | Enable TLS termination | ## Configuration File Configuration can also be provided via `api gateway.yaml`: ```yaml gateway_port: 8000 gateway_rate_limit: 100 gateway_timeout_ms: 30000 gateway_max_body_size: 10MB gateway_cors_origins: * ``` ## Validation Configuration is validated at startup. Invalid configuration will prevent the service from starting.",
              "score": null
            }
          ],
          "latency_ms": 3.7034469987702323,
          "query_id": "release_049",
          "dimension": "synonym",
          "query": "What version introduced the GraphQL endpoint and when was it launched?"
        },
        {
          "key_facts": [
            "GraphQL API beta",
            "2023-12-01"
          ],
          "found_facts": [
            "GraphQL API beta",
            "2023-12-01"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "release_3_1_0"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "release_3_1_0",
              "chunk_id": "release_3_1_0_fix_0",
              "content": "# Release Notes - v3.1.0 **Release Date:** 2023-12-01 ## New Features - GraphQL API beta release - Workflow templates marketplace - Enhanced audit logging - SSO support for Okta ## Bug Fixes - Fixed webhook delivery retry logic - Resolved issue with large payload handling - Fixed dashboard loading performance ## Upgrade Instructions ```bash cloudflow upgrade --version 3.1.0 ``` ## Known Issues None at this time.",
              "score": null
            },
            {
              "doc_id": "config_api_gateway",
              "chunk_id": "config_api_gateway_fix_0",
              "content": "# API Gateway Configuration Reference ## Overview This document describes all configuration options for API Gateway. ## Environment Variables | Variable | Type | Default | Description | |----------|------|---------|-------------| | `GATEWAY_PORT` | integer | 8000 | Gateway listen port | | `GATEWAY_RATE_LIMIT` | integer | 100 | Requests per minute per user | | `GATEWAY_TIMEOUT_MS` | integer | 30000 | Request timeout in milliseconds | | `GATEWAY_MAX_BODY_SIZE` | string | 10MB | Maximum request body size | | `GATEWAY_CORS_ORIGINS` | string | * | Allowed CORS origins | | `GATEWAY_SSL_ENABLED` | boolean | true | Enable TLS termination | ## Configuration File Configuration can also be provided via `api gateway.yaml`: ```yaml gateway_port: 8000 gateway_rate_limit: 100 gateway_timeout_ms: 30000 gateway_max_body_size: 10MB gateway_cors_origins: * ``` ## Validation Configuration is validated at startup. Invalid configuration will prevent the service from starting.",
              "score": null
            },
            {
              "doc_id": "api_metrics",
              "chunk_id": "api_metrics_fix_0",
              "content": "# Metrics API Reference ## Overview The Metrics API provides programmatic access to metrics functionality. Base URL: `https://api.cloudflow.io/v1/metrics` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /metrics/workflows Get workflow performance metrics. **Parameters:** - `period` (string): Time period: hour, day, week, month - `workflow_id` (string): Filter by workflow ID **Response:** - Success: `200` - Returns metrics data - Error: `400` - Bad Request **Example:** ```json {\"success_rate\": 0.985, \"avg_duration_ms\": 1250, \"p99_duration_ms\": 5000, \"total_executions\": 10000} ```",
              "score": null
            },
            {
              "doc_id": "release_3_0_0",
              "chunk_id": "release_3_0_0_fix_0",
              "content": "# Release Notes - v3.0.0 **Release Date:** 2023-10-15 ## New Features - Complete UI redesign - New workflow editor with visual builder - Real-time execution monitoring - Team workspaces ## Bug Fixes - Fixed critical security issue in token validation - Resolved data loss bug in failed migrations ## Breaking Changes - **BREAKING:** Authentication API completely rewritten - **BREAKING:** Legacy webhook format no longer supported - **BREAKING:** Minimum Node.js version now 18 ## Upgrade Instructions ```bash cloudflow upgrade --version 3.0.0 ``` ## Known Issues None at this time.",
              "score": null
            },
            {
              "doc_id": "api_integrations",
              "chunk_id": "api_integrations_fix_0",
              "content": "# Integrations API Reference ## Overview The Integrations API provides programmatic access to integrations functionality. Base URL: `https://api.cloudflow.io/v1/integrations` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /integrations List available integrations. **Parameters:** - `category` (string): Filter by category: storage, notification, database **Response:** - Success: `200` - Returns array of integration objects - Error: `400` - Bad Request **Example:** ```json {\"integrations\": [{\"id\": \"int_s3\", \"name\": \"Amazon S3\", \"category\": \"storage\", \"status\": \"connected\"}]} ``` ### POST /integrations/{id}/connect Connect an integration. **Parameters:** - `credentials` (object): Integration-specific credentials **Response:** - Success: `200` - Integration connected - Error: `400` - Bad Request **Example:** ```json {\"id\": \"int_s3\", \"status\": \"connected\", \"connected_at\": \"2024-01-15T10:00:00Z\"} ```",
              "score": null
            }
          ],
          "latency_ms": 3.9647630001127254,
          "query_id": "release_049",
          "dimension": "problem",
          "query": "I can't find the GraphQL API in my current version, when was it added?"
        },
        {
          "key_facts": [
            "GraphQL API beta",
            "2023-12-01"
          ],
          "found_facts": [
            "GraphQL API beta",
            "2023-12-01"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "release_3_1_0"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "release_3_1_0",
              "chunk_id": "release_3_1_0_fix_0",
              "content": "# Release Notes - v3.1.0 **Release Date:** 2023-12-01 ## New Features - GraphQL API beta release - Workflow templates marketplace - Enhanced audit logging - SSO support for Okta ## Bug Fixes - Fixed webhook delivery retry logic - Resolved issue with large payload handling - Fixed dashboard loading performance ## Upgrade Instructions ```bash cloudflow upgrade --version 3.1.0 ``` ## Known Issues None at this time.",
              "score": null
            },
            {
              "doc_id": "release_3_2_0",
              "chunk_id": "release_3_2_0_fix_0",
              "content": "# Release Notes - v3.2.0 **Release Date:** 2024-01-15 ## New Features - Workflow versioning with rollback support - New Python SDK with async support - Bulk operations API for workflows - Custom retry policies per step ## Bug Fixes - Fixed race condition in concurrent execution handler - Resolved memory leak in long-running workflows - Fixed timezone handling in scheduled triggers ## Breaking Changes - **BREAKING:** API v1 endpoints deprecated, will be removed in v4.0 - **BREAKING:** Minimum Python version now 3.9 ## Upgrade Instructions ```bash cloudflow upgrade --version 3.2.0 ``` ## Known Issues None at this time.",
              "score": null
            },
            {
              "doc_id": "release_3_0_0",
              "chunk_id": "release_3_0_0_fix_0",
              "content": "# Release Notes - v3.0.0 **Release Date:** 2023-10-15 ## New Features - Complete UI redesign - New workflow editor with visual builder - Real-time execution monitoring - Team workspaces ## Bug Fixes - Fixed critical security issue in token validation - Resolved data loss bug in failed migrations ## Breaking Changes - **BREAKING:** Authentication API completely rewritten - **BREAKING:** Legacy webhook format no longer supported - **BREAKING:** Minimum Node.js version now 18 ## Upgrade Instructions ```bash cloudflow upgrade --version 3.0.0 ``` ## Known Issues None at this time.",
              "score": null
            },
            {
              "doc_id": "api_secrets",
              "chunk_id": "api_secrets_fix_0",
              "content": "# Secrets API Reference ## Overview The Secrets API provides programmatic access to secrets functionality. Base URL: `https://api.cloudflow.io/v1/secrets` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /secrets List secret names (values not returned). **Parameters:** **Response:** - Success: `200` - Returns array of secret metadata - Error: `400` - Bad Request **Example:** ```json {\"secrets\": [{\"name\": \"DATABASE_URL\", \"created_at\": \"2024-01-01\", \"updated_at\": \"2024-01-10\"}]} ``` ### PUT /secrets/{name} Create or update a secret. **Parameters:** - `value` (string): Secret value (encrypted at rest) **Response:** - Success: `200` - Secret saved - Error: `400` - Bad Request **Example:** ```json {\"name\": \"DATABASE_URL\", \"updated_at\": \"2024-01-15T10:00:00Z\"} ``` ### DELETE /secrets/{name} Delete a secret. **Parameters:** **Response:** - Success: `204` - Secret deleted - Error: `400` - Bad Request **Example:** ```json {} ```",
              "score": null
            },
            {
              "doc_id": "release_2_9_0",
              "chunk_id": "release_2_9_0_fix_0",
              "content": "# Release Notes - v2.9.0 **Release Date:** 2023-08-01 ## New Features - Parallel step execution - Conditional branching in workflows - Slack integration improvements - Performance dashboard ## Bug Fixes - Fixed issue with special characters in secrets - Resolved timeout in large file uploads ## Upgrade Instructions ```bash cloudflow upgrade --version 2.9.0 ``` ## Known Issues None at this time.",
              "score": null
            }
          ],
          "latency_ms": 3.7652820010407595,
          "query_id": "release_049",
          "dimension": "casual",
          "query": "graphql api release date cloudflow"
        },
        {
          "key_facts": [
            "GraphQL API beta",
            "2023-12-01"
          ],
          "found_facts": [
            "GraphQL API beta",
            "2023-12-01"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "release_3_1_0"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "release_3_1_0",
              "chunk_id": "release_3_1_0_fix_0",
              "content": "# Release Notes - v3.1.0 **Release Date:** 2023-12-01 ## New Features - GraphQL API beta release - Workflow templates marketplace - Enhanced audit logging - SSO support for Okta ## Bug Fixes - Fixed webhook delivery retry logic - Resolved issue with large payload handling - Fixed dashboard loading performance ## Upgrade Instructions ```bash cloudflow upgrade --version 3.1.0 ``` ## Known Issues None at this time.",
              "score": null
            },
            {
              "doc_id": "howto_migrate_from_v1_to_v2_api",
              "chunk_id": "howto_migrate_from_v1_to_v2_api_fix_0",
              "content": "# How to Migrate from v1 to v2 API ## Prerequisites - Existing v1 integration - v2 API access enabled ## Steps ### Step 1: Review breaking changes The v2 API changes authentication from API keys to OAuth 2.0. Review the migration guide. ### Step 2: Update client library Upgrade to the latest SDK version. ```bash npm install @cloudflow/sdk@latest ``` ### Step 3: Update authentication Replace API key authentication with OAuth 2.0 client credentials flow. > **Note:** Generate new credentials in the dashboard ### Step 4: Test in staging Run your integration tests against the v2 staging endpoint. ```bash CLOUDFLOW_API_URL=https://api-v2-staging.cloudflow.io npm test ``` ### Step 5: Switch production Update production configuration to use v2 API. ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            },
            {
              "doc_id": "release_3_0_0",
              "chunk_id": "release_3_0_0_fix_0",
              "content": "# Release Notes - v3.0.0 **Release Date:** 2023-10-15 ## New Features - Complete UI redesign - New workflow editor with visual builder - Real-time execution monitoring - Team workspaces ## Bug Fixes - Fixed critical security issue in token validation - Resolved data loss bug in failed migrations ## Breaking Changes - **BREAKING:** Authentication API completely rewritten - **BREAKING:** Legacy webhook format no longer supported - **BREAKING:** Minimum Node.js version now 18 ## Upgrade Instructions ```bash cloudflow upgrade --version 3.0.0 ``` ## Known Issues None at this time.",
              "score": null
            },
            {
              "doc_id": "api_integrations",
              "chunk_id": "api_integrations_fix_0",
              "content": "# Integrations API Reference ## Overview The Integrations API provides programmatic access to integrations functionality. Base URL: `https://api.cloudflow.io/v1/integrations` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /integrations List available integrations. **Parameters:** - `category` (string): Filter by category: storage, notification, database **Response:** - Success: `200` - Returns array of integration objects - Error: `400` - Bad Request **Example:** ```json {\"integrations\": [{\"id\": \"int_s3\", \"name\": \"Amazon S3\", \"category\": \"storage\", \"status\": \"connected\"}]} ``` ### POST /integrations/{id}/connect Connect an integration. **Parameters:** - `credentials` (object): Integration-specific credentials **Response:** - Success: `200` - Integration connected - Error: `400` - Bad Request **Example:** ```json {\"id\": \"int_s3\", \"status\": \"connected\", \"connected_at\": \"2024-01-15T10:00:00Z\"} ```",
              "score": null
            },
            {
              "doc_id": "release_3_2_0",
              "chunk_id": "release_3_2_0_fix_0",
              "content": "# Release Notes - v3.2.0 **Release Date:** 2024-01-15 ## New Features - Workflow versioning with rollback support - New Python SDK with async support - Bulk operations API for workflows - Custom retry policies per step ## Bug Fixes - Fixed race condition in concurrent execution handler - Resolved memory leak in long-running workflows - Fixed timezone handling in scheduled triggers ## Breaking Changes - **BREAKING:** API v1 endpoints deprecated, will be removed in v4.0 - **BREAKING:** Minimum Python version now 3.9 ## Upgrade Instructions ```bash cloudflow upgrade --version 3.2.0 ``` ## Known Issues None at this time.",
              "score": null
            }
          ],
          "latency_ms": 3.5710200008907123,
          "query_id": "release_049",
          "dimension": "contextual",
          "query": "We want to migrate from REST to GraphQL, which CloudFlow version do we need to be on to use the GraphQL API?"
        },
        {
          "key_facts": [
            "GraphQL API beta",
            "2023-12-01"
          ],
          "found_facts": [
            "GraphQL API beta",
            "2023-12-01"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "release_3_1_0"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "release_3_1_0",
              "chunk_id": "release_3_1_0_fix_0",
              "content": "# Release Notes - v3.1.0 **Release Date:** 2023-12-01 ## New Features - GraphQL API beta release - Workflow templates marketplace - Enhanced audit logging - SSO support for Okta ## Bug Fixes - Fixed webhook delivery retry logic - Resolved issue with large payload handling - Fixed dashboard loading performance ## Upgrade Instructions ```bash cloudflow upgrade --version 3.1.0 ``` ## Known Issues None at this time.",
              "score": null
            },
            {
              "doc_id": "release_3_0_0",
              "chunk_id": "release_3_0_0_fix_0",
              "content": "# Release Notes - v3.0.0 **Release Date:** 2023-10-15 ## New Features - Complete UI redesign - New workflow editor with visual builder - Real-time execution monitoring - Team workspaces ## Bug Fixes - Fixed critical security issue in token validation - Resolved data loss bug in failed migrations ## Breaking Changes - **BREAKING:** Authentication API completely rewritten - **BREAKING:** Legacy webhook format no longer supported - **BREAKING:** Minimum Node.js version now 18 ## Upgrade Instructions ```bash cloudflow upgrade --version 3.0.0 ``` ## Known Issues None at this time.",
              "score": null
            },
            {
              "doc_id": "release_3_2_0",
              "chunk_id": "release_3_2_0_fix_0",
              "content": "# Release Notes - v3.2.0 **Release Date:** 2024-01-15 ## New Features - Workflow versioning with rollback support - New Python SDK with async support - Bulk operations API for workflows - Custom retry policies per step ## Bug Fixes - Fixed race condition in concurrent execution handler - Resolved memory leak in long-running workflows - Fixed timezone handling in scheduled triggers ## Breaking Changes - **BREAKING:** API v1 endpoints deprecated, will be removed in v4.0 - **BREAKING:** Minimum Python version now 3.9 ## Upgrade Instructions ```bash cloudflow upgrade --version 3.2.0 ``` ## Known Issues None at this time.",
              "score": null
            },
            {
              "doc_id": "adr_001",
              "chunk_id": "adr_001_fix_0",
              "content": "# ADR-001: Use PostgreSQL for Primary Database **Status:** Accepted **Date:** 2024-11-04 ## Context We need a database to store workflow definitions, execution state, and user data. The database must support ACID transactions, JSON queries, and scale to millions of records. ## Options Considered ### Option 1: PostgreSQL Mature relational database with JSONB support. **Pros:** - ACID compliance - Rich JSONB queries - Team expertise - Proven at scale **Cons:** - Manual sharding for extreme scale - Requires connection pooling ### Option 2: MongoDB Document database with native JSON. **Pros:** - Flexible schema - Built-in sharding - Good for documents **Cons:** - Weaker consistency guarantees - Complex transactions - Higher operational cost ### Option 3: CockroachDB Distributed SQL database. **Pros:** - Automatic sharding - Strong consistency - PostgreSQL compatible **Cons:** - Higher latency - More expensive - Less mature ecosystem ## Decision We will use PostgreSQL as our primary database with JSONB for flexible workflow definitions. ## Consequences - Need PgBouncer for connection pooling - Must plan for sharding at 100M+ records - Requires regular VACUUM maintenance",
              "score": null
            },
            {
              "doc_id": "arch_data_pipeline",
              "chunk_id": "arch_data_pipeline_fix_0",
              "content": "# Data Pipeline Architecture ## Overview The Data Pipeline processes and transforms data between workflow steps. It supports batch processing up to 10GB and streaming for real-time use cases. ## Design Principles - **Exactly-Once Processing**: Each record is processed exactly once - **Backpressure Handling**: System gracefully handles varying load ## Components ### Stream Processor Handles real-time data streams. Maximum throughput of 100000 messages per second. **Technology:** Apache Kafka with Flink **Scaling:** Partition-based scaling ### Batch Processor Processes large batch jobs. Supports data formats: JSON, CSV, Parquet, Avro. **Technology:** Apache Spark on Kubernetes **Scaling:** Dynamic resource allocation ### Data Lake Long-term data storage with time-travel queries. Retention configurable up to 7 years. **Technology:** S3 with Iceberg tables **Scaling:** Unlimited with tiered storage ## Data Flow Input \u2192 Validation \u2192 Transform \u2192 Enrich \u2192 Output ## Performance Characteristics - **Latency P50:** 100ms (streaming), 5min (batch) - **Latency P99:** 500ms (streaming), 30min (batch) - **Throughput:** 100000 messages/second (streaming)",
              "score": null
            }
          ],
          "latency_ms": 4.647163999834447,
          "query_id": "release_049",
          "dimension": "negation",
          "query": "Why doesn't v3.0.0 have GraphQL support?"
        },
        {
          "key_facts": [
            "PostgreSQL",
            "Redis Cluster",
            "S3",
            "Ristretto",
            "CloudFront",
            "Iceberg"
          ],
          "found_facts": [
            "Redis Cluster",
            "S3",
            "Ristretto",
            "CloudFront",
            "Iceberg"
          ],
          "missed_facts": [
            "PostgreSQL"
          ],
          "coverage": 0.8333333333333334,
          "expected_docs": [
            "adr_001",
            "adr_004",
            "adr_008",
            "arch_caching_layer",
            "arch_data_pipeline"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "adr_004",
              "chunk_id": "adr_004_fix_0",
              "content": "# ADR-004: Use Redis for Caching and Session Storage **Status:** Accepted **Date:** 2024-04-05 ## Context We need a fast data store for caching API responses, session data, and rate limiting counters. ## Options Considered ### Option 1: Redis Cluster In-memory data structure store. **Pros:** - Sub-millisecond latency - Rich data structures - Pub/sub support - Clustering **Cons:** - Memory cost - Persistence complexity - Cluster management ### Option 2: Memcached Simple key-value cache. **Pros:** - Simple operation - Multi-threaded - Lower memory overhead **Cons:** - No persistence - Limited data types - No clustering ### Option 3: Application-level cache In-process caching only. **Pros:** - No external dependency - Fastest possible - Simple **Cons:** - Not shared across pods - Memory pressure - Cold starts ## Decision We will use Redis Cluster for caching with a 6-node cluster for high availability. ## Consequences - Implement cache-aside pattern - Set appropriate TTLs - Monitor memory usage - Plan for cache stampede prevention",
              "score": null
            },
            {
              "doc_id": "arch_caching_layer",
              "chunk_id": "arch_caching_layer_fix_0",
              "content": "# Caching Layer Architecture ## Overview The Caching Layer reduces latency and database load by caching frequently accessed data. Uses a multi-tier strategy with local and distributed caches. ## Design Principles - **Cache Invalidation**: Explicit invalidation on writes - **Graceful Degradation**: System works without cache ## Components ### Local Cache First-level cache with 100ms TTL. Reduces Redis round-trips for hot data. **Technology:** Ristretto in-memory cache **Scaling:** Per-pod memory allocation ### Distributed Cache Second-level cache with configurable TTL. Maximum key size 512MB. **Technology:** Redis Cluster 7.x **Scaling:** 6-node cluster with sharding ### CDN Caches static assets and API responses. Cache hit ratio target 95%. **Technology:** CloudFront with Lambda@Edge **Scaling:** Global edge locations ## Data Flow Request \u2192 Local Cache \u2192 Redis \u2192 Database \u2192 Cache Population ## Performance Characteristics - **Latency P50:** 1ms (hit), 50ms (miss) - **Latency P99:** 5ms (hit), 200ms (miss) - **Throughput:** 100000 reads/second",
              "score": null
            },
            {
              "doc_id": "arch_data_pipeline",
              "chunk_id": "arch_data_pipeline_fix_0",
              "content": "# Data Pipeline Architecture ## Overview The Data Pipeline processes and transforms data between workflow steps. It supports batch processing up to 10GB and streaming for real-time use cases. ## Design Principles - **Exactly-Once Processing**: Each record is processed exactly once - **Backpressure Handling**: System gracefully handles varying load ## Components ### Stream Processor Handles real-time data streams. Maximum throughput of 100000 messages per second. **Technology:** Apache Kafka with Flink **Scaling:** Partition-based scaling ### Batch Processor Processes large batch jobs. Supports data formats: JSON, CSV, Parquet, Avro. **Technology:** Apache Spark on Kubernetes **Scaling:** Dynamic resource allocation ### Data Lake Long-term data storage with time-travel queries. Retention configurable up to 7 years. **Technology:** S3 with Iceberg tables **Scaling:** Unlimited with tiered storage ## Data Flow Input \u2192 Validation \u2192 Transform \u2192 Enrich \u2192 Output ## Performance Characteristics - **Latency P50:** 100ms (streaming), 5min (batch) - **Latency P99:** 500ms (streaming), 30min (batch) - **Throughput:** 100000 messages/second (streaming)",
              "score": null
            },
            {
              "doc_id": "adr_008",
              "chunk_id": "adr_008_fix_0",
              "content": "# ADR-008: Use S3 for Object Storage **Status:** Accepted **Date:** 2024-10-14 ## Context We need storage for workflow artifacts, execution outputs, and user uploads. ## Options Considered ### Option 1: Amazon S3 Object storage service. **Pros:** - 99.999999999% durability - Lifecycle policies - Event notifications - CDN integration **Cons:** - AWS lock-in - Eventual consistency for overwrites - Cost at scale ### Option 2: Self-hosted MinIO S3-compatible object storage. **Pros:** - No vendor lock-in - S3 compatible - On-premises option **Cons:** - Operational burden - Lower durability guarantees - Scaling complexity ### Option 3: Google Cloud Storage GCP object storage. **Pros:** - Strong consistency - Good performance - Unified billing **Cons:** - GCP lock-in - Fewer features than S3 - Smaller ecosystem ## Decision We will use Amazon S3 for object storage with intelligent tiering for cost optimization. ## Consequences - Enable versioning for critical buckets - Implement lifecycle policies - Use pre-signed URLs for uploads - Enable server-side encryption",
              "score": null
            },
            {
              "doc_id": "arch_monitoring_stack",
              "chunk_id": "arch_monitoring_stack_fix_0",
              "content": "# Monitoring Stack Architecture ## Overview The Monitoring Stack provides observability into system health, performance, and errors. Includes metrics, logs, traces, and alerting. ## Design Principles - **Observability**: Every component emits metrics and traces - **Actionable Alerts**: Alerts include runbook links ## Components ### Metrics Collects time-series metrics. Retention is 15 days local, 1 year in object storage. **Technology:** Prometheus with Thanos **Scaling:** Federated with long-term storage ### Logging Centralized log aggregation. Supports structured JSON logs with label indexing. **Technology:** Loki with Grafana **Scaling:** Distributed with sharding ### Tracing Distributed request tracing. Trace retention is 7 days. **Technology:** Jaeger with OpenTelemetry **Scaling:** Sampling at 10% ### Alerting Routes alerts to on-call engineers. Supports escalation policies and silencing. **Technology:** Alertmanager with PagerDuty **Scaling:** HA pair ## Data Flow Application \u2192 Collector \u2192 Storage \u2192 Query \u2192 Dashboard/Alert ## Performance Characteristics - **Latency P50:** N/A - **Latency P99:** N/A - **Throughput:** 1M metrics/second, 100GB logs/day",
              "score": null
            }
          ],
          "latency_ms": 4.017040999315213,
          "query_id": "complex_050",
          "dimension": "original",
          "query": "What are all the technologies used for data storage and caching?"
        },
        {
          "key_facts": [
            "PostgreSQL",
            "Redis Cluster",
            "S3",
            "Ristretto",
            "CloudFront",
            "Iceberg"
          ],
          "found_facts": [
            "PostgreSQL",
            "Redis Cluster",
            "S3",
            "Ristretto",
            "CloudFront",
            "Iceberg"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "adr_001",
            "adr_004",
            "adr_008",
            "arch_caching_layer",
            "arch_data_pipeline"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "arch_data_pipeline",
              "chunk_id": "arch_data_pipeline_fix_0",
              "content": "# Data Pipeline Architecture ## Overview The Data Pipeline processes and transforms data between workflow steps. It supports batch processing up to 10GB and streaming for real-time use cases. ## Design Principles - **Exactly-Once Processing**: Each record is processed exactly once - **Backpressure Handling**: System gracefully handles varying load ## Components ### Stream Processor Handles real-time data streams. Maximum throughput of 100000 messages per second. **Technology:** Apache Kafka with Flink **Scaling:** Partition-based scaling ### Batch Processor Processes large batch jobs. Supports data formats: JSON, CSV, Parquet, Avro. **Technology:** Apache Spark on Kubernetes **Scaling:** Dynamic resource allocation ### Data Lake Long-term data storage with time-travel queries. Retention configurable up to 7 years. **Technology:** S3 with Iceberg tables **Scaling:** Unlimited with tiered storage ## Data Flow Input \u2192 Validation \u2192 Transform \u2192 Enrich \u2192 Output ## Performance Characteristics - **Latency P50:** 100ms (streaming), 5min (batch) - **Latency P99:** 500ms (streaming), 30min (batch) - **Throughput:** 100000 messages/second (streaming)",
              "score": null
            },
            {
              "doc_id": "adr_004",
              "chunk_id": "adr_004_fix_0",
              "content": "# ADR-004: Use Redis for Caching and Session Storage **Status:** Accepted **Date:** 2024-04-05 ## Context We need a fast data store for caching API responses, session data, and rate limiting counters. ## Options Considered ### Option 1: Redis Cluster In-memory data structure store. **Pros:** - Sub-millisecond latency - Rich data structures - Pub/sub support - Clustering **Cons:** - Memory cost - Persistence complexity - Cluster management ### Option 2: Memcached Simple key-value cache. **Pros:** - Simple operation - Multi-threaded - Lower memory overhead **Cons:** - No persistence - Limited data types - No clustering ### Option 3: Application-level cache In-process caching only. **Pros:** - No external dependency - Fastest possible - Simple **Cons:** - Not shared across pods - Memory pressure - Cold starts ## Decision We will use Redis Cluster for caching with a 6-node cluster for high availability. ## Consequences - Implement cache-aside pattern - Set appropriate TTLs - Monitor memory usage - Plan for cache stampede prevention",
              "score": null
            },
            {
              "doc_id": "adr_008",
              "chunk_id": "adr_008_fix_0",
              "content": "# ADR-008: Use S3 for Object Storage **Status:** Accepted **Date:** 2024-10-14 ## Context We need storage for workflow artifacts, execution outputs, and user uploads. ## Options Considered ### Option 1: Amazon S3 Object storage service. **Pros:** - 99.999999999% durability - Lifecycle policies - Event notifications - CDN integration **Cons:** - AWS lock-in - Eventual consistency for overwrites - Cost at scale ### Option 2: Self-hosted MinIO S3-compatible object storage. **Pros:** - No vendor lock-in - S3 compatible - On-premises option **Cons:** - Operational burden - Lower durability guarantees - Scaling complexity ### Option 3: Google Cloud Storage GCP object storage. **Pros:** - Strong consistency - Good performance - Unified billing **Cons:** - GCP lock-in - Fewer features than S3 - Smaller ecosystem ## Decision We will use Amazon S3 for object storage with intelligent tiering for cost optimization. ## Consequences - Enable versioning for critical buckets - Implement lifecycle policies - Use pre-signed URLs for uploads - Enable server-side encryption",
              "score": null
            },
            {
              "doc_id": "arch_caching_layer",
              "chunk_id": "arch_caching_layer_fix_0",
              "content": "# Caching Layer Architecture ## Overview The Caching Layer reduces latency and database load by caching frequently accessed data. Uses a multi-tier strategy with local and distributed caches. ## Design Principles - **Cache Invalidation**: Explicit invalidation on writes - **Graceful Degradation**: System works without cache ## Components ### Local Cache First-level cache with 100ms TTL. Reduces Redis round-trips for hot data. **Technology:** Ristretto in-memory cache **Scaling:** Per-pod memory allocation ### Distributed Cache Second-level cache with configurable TTL. Maximum key size 512MB. **Technology:** Redis Cluster 7.x **Scaling:** 6-node cluster with sharding ### CDN Caches static assets and API responses. Cache hit ratio target 95%. **Technology:** CloudFront with Lambda@Edge **Scaling:** Global edge locations ## Data Flow Request \u2192 Local Cache \u2192 Redis \u2192 Database \u2192 Cache Population ## Performance Characteristics - **Latency P50:** 1ms (hit), 50ms (miss) - **Latency P99:** 5ms (hit), 200ms (miss) - **Throughput:** 100000 reads/second",
              "score": null
            },
            {
              "doc_id": "arch_workflow_engine",
              "chunk_id": "arch_workflow_engine_fix_0",
              "content": "# Workflow Engine Architecture ## Overview The Workflow Engine is the core component responsible for executing user-defined workflows. It uses an event-driven architecture with support for parallel execution, conditional branching, and automatic retries. ## Design Principles - **Idempotency**: All operations can be safely retried without side effects - **Eventual Consistency**: State converges to consistent view within 5 seconds - **Fault Tolerance**: System continues operating despite component failures ## Components ### Scheduler Manages workflow scheduling and triggers. Supports cron expressions, webhooks, and event-based triggers. **Technology:** Custom Go service **Scaling:** Horizontal with leader election ### Executor Runs individual workflow steps in isolated containers. Maximum execution time is 30 minutes per step. **Technology:** Kubernetes Jobs **Scaling:** Auto-scaling based on queue depth ### State Store Persists workflow state, execution history, and step results. Uses optimistic locking for concurrent updates. **Technology:** PostgreSQL with JSONB **Scaling:** Primary-replica with read replicas ## Data Flow Triggers \u2192 Scheduler \u2192 Job Queue \u2192 Executor \u2192 State Store \u2192 Webhooks/Notifications ## Performance Characteristics - **Latency P50:** 45ms - **Latency P99:** 250ms - **Throughput:** 5000 executions/minute",
              "score": null
            }
          ],
          "latency_ms": 3.957448998335167,
          "query_id": "complex_050",
          "dimension": "synonym",
          "query": "what data stores, databases, and cache solutions does cloudflow use for persistence and memory caching?"
        },
        {
          "key_facts": [
            "PostgreSQL",
            "Redis Cluster",
            "S3",
            "Ristretto",
            "CloudFront",
            "Iceberg"
          ],
          "found_facts": [
            "Redis Cluster",
            "S3",
            "Ristretto",
            "CloudFront",
            "Iceberg"
          ],
          "missed_facts": [
            "PostgreSQL"
          ],
          "coverage": 0.8333333333333334,
          "expected_docs": [
            "adr_001",
            "adr_004",
            "adr_008",
            "arch_caching_layer",
            "arch_data_pipeline"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "arch_caching_layer",
              "chunk_id": "arch_caching_layer_fix_0",
              "content": "# Caching Layer Architecture ## Overview The Caching Layer reduces latency and database load by caching frequently accessed data. Uses a multi-tier strategy with local and distributed caches. ## Design Principles - **Cache Invalidation**: Explicit invalidation on writes - **Graceful Degradation**: System works without cache ## Components ### Local Cache First-level cache with 100ms TTL. Reduces Redis round-trips for hot data. **Technology:** Ristretto in-memory cache **Scaling:** Per-pod memory allocation ### Distributed Cache Second-level cache with configurable TTL. Maximum key size 512MB. **Technology:** Redis Cluster 7.x **Scaling:** 6-node cluster with sharding ### CDN Caches static assets and API responses. Cache hit ratio target 95%. **Technology:** CloudFront with Lambda@Edge **Scaling:** Global edge locations ## Data Flow Request \u2192 Local Cache \u2192 Redis \u2192 Database \u2192 Cache Population ## Performance Characteristics - **Latency P50:** 1ms (hit), 50ms (miss) - **Latency P99:** 5ms (hit), 200ms (miss) - **Throughput:** 100000 reads/second",
              "score": null
            },
            {
              "doc_id": "adr_004",
              "chunk_id": "adr_004_fix_0",
              "content": "# ADR-004: Use Redis for Caching and Session Storage **Status:** Accepted **Date:** 2024-04-05 ## Context We need a fast data store for caching API responses, session data, and rate limiting counters. ## Options Considered ### Option 1: Redis Cluster In-memory data structure store. **Pros:** - Sub-millisecond latency - Rich data structures - Pub/sub support - Clustering **Cons:** - Memory cost - Persistence complexity - Cluster management ### Option 2: Memcached Simple key-value cache. **Pros:** - Simple operation - Multi-threaded - Lower memory overhead **Cons:** - No persistence - Limited data types - No clustering ### Option 3: Application-level cache In-process caching only. **Pros:** - No external dependency - Fastest possible - Simple **Cons:** - Not shared across pods - Memory pressure - Cold starts ## Decision We will use Redis Cluster for caching with a 6-node cluster for high availability. ## Consequences - Implement cache-aside pattern - Set appropriate TTLs - Monitor memory usage - Plan for cache stampede prevention",
              "score": null
            },
            {
              "doc_id": "arch_data_pipeline",
              "chunk_id": "arch_data_pipeline_fix_0",
              "content": "# Data Pipeline Architecture ## Overview The Data Pipeline processes and transforms data between workflow steps. It supports batch processing up to 10GB and streaming for real-time use cases. ## Design Principles - **Exactly-Once Processing**: Each record is processed exactly once - **Backpressure Handling**: System gracefully handles varying load ## Components ### Stream Processor Handles real-time data streams. Maximum throughput of 100000 messages per second. **Technology:** Apache Kafka with Flink **Scaling:** Partition-based scaling ### Batch Processor Processes large batch jobs. Supports data formats: JSON, CSV, Parquet, Avro. **Technology:** Apache Spark on Kubernetes **Scaling:** Dynamic resource allocation ### Data Lake Long-term data storage with time-travel queries. Retention configurable up to 7 years. **Technology:** S3 with Iceberg tables **Scaling:** Unlimited with tiered storage ## Data Flow Input \u2192 Validation \u2192 Transform \u2192 Enrich \u2192 Output ## Performance Characteristics - **Latency P50:** 100ms (streaming), 5min (batch) - **Latency P99:** 500ms (streaming), 30min (batch) - **Throughput:** 100000 messages/second (streaming)",
              "score": null
            },
            {
              "doc_id": "arch_search_service",
              "chunk_id": "arch_search_service_fix_0",
              "content": "# Search Service Architecture ## Overview The Search Service provides full-text and semantic search across workflows, executions, and documentation. Built on Elasticsearch with custom analyzers for technical content. ## Design Principles - **Relevance First**: Results ranked by relevance, not recency - **Typo Tolerance**: Handles misspellings and variations ## Components ### Indexer Indexes documents in near real-time. Typical indexing latency is 500ms. **Technology:** Custom Go service **Scaling:** Queue-based with backpressure ### Query Engine Executes search queries. Supports filters, facets, and aggregations. **Technology:** Elasticsearch 8.x **Scaling:** 3-node cluster with replicas ### Suggestion Service Provides autocomplete suggestions. Returns suggestions within 20ms. **Technology:** Completion suggester **Scaling:** In-memory FST ## Data Flow Document \u2192 Analyzer \u2192 Index \u2192 Query \u2192 Rank \u2192 Results ## Performance Characteristics - **Latency P50:** 25ms - **Latency P99:** 100ms - **Throughput:** 2000 queries/second",
              "score": null
            },
            {
              "doc_id": "arch_monitoring_stack",
              "chunk_id": "arch_monitoring_stack_fix_0",
              "content": "# Monitoring Stack Architecture ## Overview The Monitoring Stack provides observability into system health, performance, and errors. Includes metrics, logs, traces, and alerting. ## Design Principles - **Observability**: Every component emits metrics and traces - **Actionable Alerts**: Alerts include runbook links ## Components ### Metrics Collects time-series metrics. Retention is 15 days local, 1 year in object storage. **Technology:** Prometheus with Thanos **Scaling:** Federated with long-term storage ### Logging Centralized log aggregation. Supports structured JSON logs with label indexing. **Technology:** Loki with Grafana **Scaling:** Distributed with sharding ### Tracing Distributed request tracing. Trace retention is 7 days. **Technology:** Jaeger with OpenTelemetry **Scaling:** Sampling at 10% ### Alerting Routes alerts to on-call engineers. Supports escalation policies and silencing. **Technology:** Alertmanager with PagerDuty **Scaling:** HA pair ## Data Flow Application \u2192 Collector \u2192 Storage \u2192 Query \u2192 Dashboard/Alert ## Performance Characteristics - **Latency P50:** N/A - **Latency P99:** N/A - **Throughput:** 1M metrics/second, 100GB logs/day",
              "score": null
            }
          ],
          "latency_ms": 4.9567099995329045,
          "query_id": "complex_050",
          "dimension": "problem",
          "query": "we're hitting slow queries and cache misses everywhere - what's our current data layer stack and where is data actually stored?"
        },
        {
          "key_facts": [
            "PostgreSQL",
            "Redis Cluster",
            "S3",
            "Ristretto",
            "CloudFront",
            "Iceberg"
          ],
          "found_facts": [
            "Redis Cluster",
            "S3",
            "Ristretto",
            "CloudFront",
            "Iceberg"
          ],
          "missed_facts": [
            "PostgreSQL"
          ],
          "coverage": 0.8333333333333334,
          "expected_docs": [
            "adr_001",
            "adr_004",
            "adr_008",
            "arch_caching_layer",
            "arch_data_pipeline"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "adr_008",
              "chunk_id": "adr_008_fix_0",
              "content": "# ADR-008: Use S3 for Object Storage **Status:** Accepted **Date:** 2024-10-14 ## Context We need storage for workflow artifacts, execution outputs, and user uploads. ## Options Considered ### Option 1: Amazon S3 Object storage service. **Pros:** - 99.999999999% durability - Lifecycle policies - Event notifications - CDN integration **Cons:** - AWS lock-in - Eventual consistency for overwrites - Cost at scale ### Option 2: Self-hosted MinIO S3-compatible object storage. **Pros:** - No vendor lock-in - S3 compatible - On-premises option **Cons:** - Operational burden - Lower durability guarantees - Scaling complexity ### Option 3: Google Cloud Storage GCP object storage. **Pros:** - Strong consistency - Good performance - Unified billing **Cons:** - GCP lock-in - Fewer features than S3 - Smaller ecosystem ## Decision We will use Amazon S3 for object storage with intelligent tiering for cost optimization. ## Consequences - Enable versioning for critical buckets - Implement lifecycle policies - Use pre-signed URLs for uploads - Enable server-side encryption",
              "score": null
            },
            {
              "doc_id": "adr_004",
              "chunk_id": "adr_004_fix_0",
              "content": "# ADR-004: Use Redis for Caching and Session Storage **Status:** Accepted **Date:** 2024-04-05 ## Context We need a fast data store for caching API responses, session data, and rate limiting counters. ## Options Considered ### Option 1: Redis Cluster In-memory data structure store. **Pros:** - Sub-millisecond latency - Rich data structures - Pub/sub support - Clustering **Cons:** - Memory cost - Persistence complexity - Cluster management ### Option 2: Memcached Simple key-value cache. **Pros:** - Simple operation - Multi-threaded - Lower memory overhead **Cons:** - No persistence - Limited data types - No clustering ### Option 3: Application-level cache In-process caching only. **Pros:** - No external dependency - Fastest possible - Simple **Cons:** - Not shared across pods - Memory pressure - Cold starts ## Decision We will use Redis Cluster for caching with a 6-node cluster for high availability. ## Consequences - Implement cache-aside pattern - Set appropriate TTLs - Monitor memory usage - Plan for cache stampede prevention",
              "score": null
            },
            {
              "doc_id": "arch_caching_layer",
              "chunk_id": "arch_caching_layer_fix_0",
              "content": "# Caching Layer Architecture ## Overview The Caching Layer reduces latency and database load by caching frequently accessed data. Uses a multi-tier strategy with local and distributed caches. ## Design Principles - **Cache Invalidation**: Explicit invalidation on writes - **Graceful Degradation**: System works without cache ## Components ### Local Cache First-level cache with 100ms TTL. Reduces Redis round-trips for hot data. **Technology:** Ristretto in-memory cache **Scaling:** Per-pod memory allocation ### Distributed Cache Second-level cache with configurable TTL. Maximum key size 512MB. **Technology:** Redis Cluster 7.x **Scaling:** 6-node cluster with sharding ### CDN Caches static assets and API responses. Cache hit ratio target 95%. **Technology:** CloudFront with Lambda@Edge **Scaling:** Global edge locations ## Data Flow Request \u2192 Local Cache \u2192 Redis \u2192 Database \u2192 Cache Population ## Performance Characteristics - **Latency P50:** 1ms (hit), 50ms (miss) - **Latency P99:** 5ms (hit), 200ms (miss) - **Throughput:** 100000 reads/second",
              "score": null
            },
            {
              "doc_id": "arch_data_pipeline",
              "chunk_id": "arch_data_pipeline_fix_0",
              "content": "# Data Pipeline Architecture ## Overview The Data Pipeline processes and transforms data between workflow steps. It supports batch processing up to 10GB and streaming for real-time use cases. ## Design Principles - **Exactly-Once Processing**: Each record is processed exactly once - **Backpressure Handling**: System gracefully handles varying load ## Components ### Stream Processor Handles real-time data streams. Maximum throughput of 100000 messages per second. **Technology:** Apache Kafka with Flink **Scaling:** Partition-based scaling ### Batch Processor Processes large batch jobs. Supports data formats: JSON, CSV, Parquet, Avro. **Technology:** Apache Spark on Kubernetes **Scaling:** Dynamic resource allocation ### Data Lake Long-term data storage with time-travel queries. Retention configurable up to 7 years. **Technology:** S3 with Iceberg tables **Scaling:** Unlimited with tiered storage ## Data Flow Input \u2192 Validation \u2192 Transform \u2192 Enrich \u2192 Output ## Performance Characteristics - **Latency P50:** 100ms (streaming), 5min (batch) - **Latency P99:** 500ms (streaming), 30min (batch) - **Throughput:** 100000 messages/second (streaming)",
              "score": null
            },
            {
              "doc_id": "adr_006",
              "chunk_id": "adr_006_fix_0",
              "content": "# ADR-006: Use Terraform for Infrastructure as Code **Status:** Accepted **Date:** 2024-11-24 ## Context We need a way to manage cloud infrastructure consistently across environments. ## Options Considered ### Option 1: Terraform Multi-cloud IaC tool. **Pros:** - Multi-cloud support - Large community - State management - Plan before apply **Cons:** - State file management - Learning curve - HCL limitations ### Option 2: AWS CloudFormation AWS-native IaC. **Pros:** - Native AWS support - No state file - Drift detection **Cons:** - AWS only - Verbose YAML/JSON - Slower development ### Option 3: Pulumi IaC with real programming languages. **Pros:** - Real languages - Type safety - Reusable components **Cons:** - Smaller community - Vendor lock-in - Commercial features ## Decision We will use Terraform with S3 backend for state and DynamoDB for locking. ## Consequences - Implement module structure - Use workspaces for environments - Automate with Atlantis",
              "score": null
            }
          ],
          "latency_ms": 4.1499380004097475,
          "query_id": "complex_050",
          "dimension": "casual",
          "query": "cloudflow storage and caching stack overview"
        },
        {
          "key_facts": [
            "PostgreSQL",
            "Redis Cluster",
            "S3",
            "Ristretto",
            "CloudFront",
            "Iceberg"
          ],
          "found_facts": [
            "Redis Cluster",
            "S3",
            "Ristretto",
            "CloudFront",
            "Iceberg"
          ],
          "missed_facts": [
            "PostgreSQL"
          ],
          "coverage": 0.8333333333333334,
          "expected_docs": [
            "adr_001",
            "adr_004",
            "adr_008",
            "arch_caching_layer",
            "arch_data_pipeline"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "arch_monitoring_stack",
              "chunk_id": "arch_monitoring_stack_fix_0",
              "content": "# Monitoring Stack Architecture ## Overview The Monitoring Stack provides observability into system health, performance, and errors. Includes metrics, logs, traces, and alerting. ## Design Principles - **Observability**: Every component emits metrics and traces - **Actionable Alerts**: Alerts include runbook links ## Components ### Metrics Collects time-series metrics. Retention is 15 days local, 1 year in object storage. **Technology:** Prometheus with Thanos **Scaling:** Federated with long-term storage ### Logging Centralized log aggregation. Supports structured JSON logs with label indexing. **Technology:** Loki with Grafana **Scaling:** Distributed with sharding ### Tracing Distributed request tracing. Trace retention is 7 days. **Technology:** Jaeger with OpenTelemetry **Scaling:** Sampling at 10% ### Alerting Routes alerts to on-call engineers. Supports escalation policies and silencing. **Technology:** Alertmanager with PagerDuty **Scaling:** HA pair ## Data Flow Application \u2192 Collector \u2192 Storage \u2192 Query \u2192 Dashboard/Alert ## Performance Characteristics - **Latency P50:** N/A - **Latency P99:** N/A - **Throughput:** 1M metrics/second, 100GB logs/day",
              "score": null
            },
            {
              "doc_id": "adr_008",
              "chunk_id": "adr_008_fix_0",
              "content": "# ADR-008: Use S3 for Object Storage **Status:** Accepted **Date:** 2024-10-14 ## Context We need storage for workflow artifacts, execution outputs, and user uploads. ## Options Considered ### Option 1: Amazon S3 Object storage service. **Pros:** - 99.999999999% durability - Lifecycle policies - Event notifications - CDN integration **Cons:** - AWS lock-in - Eventual consistency for overwrites - Cost at scale ### Option 2: Self-hosted MinIO S3-compatible object storage. **Pros:** - No vendor lock-in - S3 compatible - On-premises option **Cons:** - Operational burden - Lower durability guarantees - Scaling complexity ### Option 3: Google Cloud Storage GCP object storage. **Pros:** - Strong consistency - Good performance - Unified billing **Cons:** - GCP lock-in - Fewer features than S3 - Smaller ecosystem ## Decision We will use Amazon S3 for object storage with intelligent tiering for cost optimization. ## Consequences - Enable versioning for critical buckets - Implement lifecycle policies - Use pre-signed URLs for uploads - Enable server-side encryption",
              "score": null
            },
            {
              "doc_id": "adr_004",
              "chunk_id": "adr_004_fix_0",
              "content": "# ADR-004: Use Redis for Caching and Session Storage **Status:** Accepted **Date:** 2024-04-05 ## Context We need a fast data store for caching API responses, session data, and rate limiting counters. ## Options Considered ### Option 1: Redis Cluster In-memory data structure store. **Pros:** - Sub-millisecond latency - Rich data structures - Pub/sub support - Clustering **Cons:** - Memory cost - Persistence complexity - Cluster management ### Option 2: Memcached Simple key-value cache. **Pros:** - Simple operation - Multi-threaded - Lower memory overhead **Cons:** - No persistence - Limited data types - No clustering ### Option 3: Application-level cache In-process caching only. **Pros:** - No external dependency - Fastest possible - Simple **Cons:** - Not shared across pods - Memory pressure - Cold starts ## Decision We will use Redis Cluster for caching with a 6-node cluster for high availability. ## Consequences - Implement cache-aside pattern - Set appropriate TTLs - Monitor memory usage - Plan for cache stampede prevention",
              "score": null
            },
            {
              "doc_id": "arch_data_pipeline",
              "chunk_id": "arch_data_pipeline_fix_0",
              "content": "# Data Pipeline Architecture ## Overview The Data Pipeline processes and transforms data between workflow steps. It supports batch processing up to 10GB and streaming for real-time use cases. ## Design Principles - **Exactly-Once Processing**: Each record is processed exactly once - **Backpressure Handling**: System gracefully handles varying load ## Components ### Stream Processor Handles real-time data streams. Maximum throughput of 100000 messages per second. **Technology:** Apache Kafka with Flink **Scaling:** Partition-based scaling ### Batch Processor Processes large batch jobs. Supports data formats: JSON, CSV, Parquet, Avro. **Technology:** Apache Spark on Kubernetes **Scaling:** Dynamic resource allocation ### Data Lake Long-term data storage with time-travel queries. Retention configurable up to 7 years. **Technology:** S3 with Iceberg tables **Scaling:** Unlimited with tiered storage ## Data Flow Input \u2192 Validation \u2192 Transform \u2192 Enrich \u2192 Output ## Performance Characteristics - **Latency P50:** 100ms (streaming), 5min (batch) - **Latency P99:** 500ms (streaming), 30min (batch) - **Throughput:** 100000 messages/second (streaming)",
              "score": null
            },
            {
              "doc_id": "arch_caching_layer",
              "chunk_id": "arch_caching_layer_fix_0",
              "content": "# Caching Layer Architecture ## Overview The Caching Layer reduces latency and database load by caching frequently accessed data. Uses a multi-tier strategy with local and distributed caches. ## Design Principles - **Cache Invalidation**: Explicit invalidation on writes - **Graceful Degradation**: System works without cache ## Components ### Local Cache First-level cache with 100ms TTL. Reduces Redis round-trips for hot data. **Technology:** Ristretto in-memory cache **Scaling:** Per-pod memory allocation ### Distributed Cache Second-level cache with configurable TTL. Maximum key size 512MB. **Technology:** Redis Cluster 7.x **Scaling:** 6-node cluster with sharding ### CDN Caches static assets and API responses. Cache hit ratio target 95%. **Technology:** CloudFront with Lambda@Edge **Scaling:** Global edge locations ## Data Flow Request \u2192 Local Cache \u2192 Redis \u2192 Database \u2192 Cache Population ## Performance Characteristics - **Latency P50:** 1ms (hit), 50ms (miss) - **Latency P99:** 5ms (hit), 200ms (miss) - **Throughput:** 100000 reads/second",
              "score": null
            }
          ],
          "latency_ms": 3.5742860000027576,
          "query_id": "complex_050",
          "dimension": "contextual",
          "query": "planning our disaster recovery strategy - need a complete inventory of all storage and caching technologies so we know what to back up and replicate"
        },
        {
          "key_facts": [
            "PostgreSQL",
            "Redis Cluster",
            "S3",
            "Ristretto",
            "CloudFront",
            "Iceberg"
          ],
          "found_facts": [
            "PostgreSQL",
            "Redis Cluster",
            "S3",
            "Ristretto",
            "CloudFront",
            "Iceberg"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "adr_001",
            "adr_004",
            "adr_008",
            "arch_caching_layer",
            "arch_data_pipeline"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "adr_004",
              "chunk_id": "adr_004_fix_0",
              "content": "# ADR-004: Use Redis for Caching and Session Storage **Status:** Accepted **Date:** 2024-04-05 ## Context We need a fast data store for caching API responses, session data, and rate limiting counters. ## Options Considered ### Option 1: Redis Cluster In-memory data structure store. **Pros:** - Sub-millisecond latency - Rich data structures - Pub/sub support - Clustering **Cons:** - Memory cost - Persistence complexity - Cluster management ### Option 2: Memcached Simple key-value cache. **Pros:** - Simple operation - Multi-threaded - Lower memory overhead **Cons:** - No persistence - Limited data types - No clustering ### Option 3: Application-level cache In-process caching only. **Pros:** - No external dependency - Fastest possible - Simple **Cons:** - Not shared across pods - Memory pressure - Cold starts ## Decision We will use Redis Cluster for caching with a 6-node cluster for high availability. ## Consequences - Implement cache-aside pattern - Set appropriate TTLs - Monitor memory usage - Plan for cache stampede prevention",
              "score": null
            },
            {
              "doc_id": "arch_caching_layer",
              "chunk_id": "arch_caching_layer_fix_0",
              "content": "# Caching Layer Architecture ## Overview The Caching Layer reduces latency and database load by caching frequently accessed data. Uses a multi-tier strategy with local and distributed caches. ## Design Principles - **Cache Invalidation**: Explicit invalidation on writes - **Graceful Degradation**: System works without cache ## Components ### Local Cache First-level cache with 100ms TTL. Reduces Redis round-trips for hot data. **Technology:** Ristretto in-memory cache **Scaling:** Per-pod memory allocation ### Distributed Cache Second-level cache with configurable TTL. Maximum key size 512MB. **Technology:** Redis Cluster 7.x **Scaling:** 6-node cluster with sharding ### CDN Caches static assets and API responses. Cache hit ratio target 95%. **Technology:** CloudFront with Lambda@Edge **Scaling:** Global edge locations ## Data Flow Request \u2192 Local Cache \u2192 Redis \u2192 Database \u2192 Cache Population ## Performance Characteristics - **Latency P50:** 1ms (hit), 50ms (miss) - **Latency P99:** 5ms (hit), 200ms (miss) - **Throughput:** 100000 reads/second",
              "score": null
            },
            {
              "doc_id": "arch_data_pipeline",
              "chunk_id": "arch_data_pipeline_fix_0",
              "content": "# Data Pipeline Architecture ## Overview The Data Pipeline processes and transforms data between workflow steps. It supports batch processing up to 10GB and streaming for real-time use cases. ## Design Principles - **Exactly-Once Processing**: Each record is processed exactly once - **Backpressure Handling**: System gracefully handles varying load ## Components ### Stream Processor Handles real-time data streams. Maximum throughput of 100000 messages per second. **Technology:** Apache Kafka with Flink **Scaling:** Partition-based scaling ### Batch Processor Processes large batch jobs. Supports data formats: JSON, CSV, Parquet, Avro. **Technology:** Apache Spark on Kubernetes **Scaling:** Dynamic resource allocation ### Data Lake Long-term data storage with time-travel queries. Retention configurable up to 7 years. **Technology:** S3 with Iceberg tables **Scaling:** Unlimited with tiered storage ## Data Flow Input \u2192 Validation \u2192 Transform \u2192 Enrich \u2192 Output ## Performance Characteristics - **Latency P50:** 100ms (streaming), 5min (batch) - **Latency P99:** 500ms (streaming), 30min (batch) - **Throughput:** 100000 messages/second (streaming)",
              "score": null
            },
            {
              "doc_id": "adr_001",
              "chunk_id": "adr_001_fix_0",
              "content": "# ADR-001: Use PostgreSQL for Primary Database **Status:** Accepted **Date:** 2024-11-04 ## Context We need a database to store workflow definitions, execution state, and user data. The database must support ACID transactions, JSON queries, and scale to millions of records. ## Options Considered ### Option 1: PostgreSQL Mature relational database with JSONB support. **Pros:** - ACID compliance - Rich JSONB queries - Team expertise - Proven at scale **Cons:** - Manual sharding for extreme scale - Requires connection pooling ### Option 2: MongoDB Document database with native JSON. **Pros:** - Flexible schema - Built-in sharding - Good for documents **Cons:** - Weaker consistency guarantees - Complex transactions - Higher operational cost ### Option 3: CockroachDB Distributed SQL database. **Pros:** - Automatic sharding - Strong consistency - PostgreSQL compatible **Cons:** - Higher latency - More expensive - Less mature ecosystem ## Decision We will use PostgreSQL as our primary database with JSONB for flexible workflow definitions. ## Consequences - Need PgBouncer for connection pooling - Must plan for sharding at 100M+ records - Requires regular VACUUM maintenance",
              "score": null
            },
            {
              "doc_id": "adr_008",
              "chunk_id": "adr_008_fix_0",
              "content": "# ADR-008: Use S3 for Object Storage **Status:** Accepted **Date:** 2024-10-14 ## Context We need storage for workflow artifacts, execution outputs, and user uploads. ## Options Considered ### Option 1: Amazon S3 Object storage service. **Pros:** - 99.999999999% durability - Lifecycle policies - Event notifications - CDN integration **Cons:** - AWS lock-in - Eventual consistency for overwrites - Cost at scale ### Option 2: Self-hosted MinIO S3-compatible object storage. **Pros:** - No vendor lock-in - S3 compatible - On-premises option **Cons:** - Operational burden - Lower durability guarantees - Scaling complexity ### Option 3: Google Cloud Storage GCP object storage. **Pros:** - Strong consistency - Good performance - Unified billing **Cons:** - GCP lock-in - Fewer features than S3 - Smaller ecosystem ## Decision We will use Amazon S3 for object storage with intelligent tiering for cost optimization. ## Consequences - Enable versioning for critical buckets - Implement lifecycle policies - Use pre-signed URLs for uploads - Enable server-side encryption",
              "score": null
            }
          ],
          "latency_ms": 3.4893279989773873,
          "query_id": "complex_050",
          "dimension": "negation",
          "query": "do we even have a proper caching strategy? what's actually persisting data vs just caching it in memory?"
        },
        {
          "key_facts": [
            "OAuth 2.0",
            "JWT",
            "bcrypt",
            "AES-256",
            "TLS",
            "Zero Trust"
          ],
          "found_facts": [
            "JWT",
            "AES-256",
            "TLS",
            "Zero Trust"
          ],
          "missed_facts": [
            "OAuth 2.0",
            "bcrypt"
          ],
          "coverage": 0.6666666666666666,
          "expected_docs": [
            "adr_003",
            "arch_authentication_service",
            "arch_api_gateway",
            "howto_set_up_data_encryption"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "howto_set_up_data_encryption",
              "chunk_id": "howto_set_up_data_encryption_fix_0",
              "content": "# How to Set Up Data Encryption ## Prerequisites - CloudFlow Enterprise plan ## Steps ### Step 1: Enable encryption at rest All data is encrypted at rest using AES-256 by default. > **Note:** Encryption is automatic for all plans ### Step 2: Configure KMS Bring your own encryption keys using AWS KMS or GCP KMS. ```bash cloudflow encryption configure --kms-key arn:aws:kms:... ``` ### Step 3: Enable field-level encryption Encrypt sensitive fields within your workflow data. ```bash cloudflow encryption add-field --path $.user.ssn --key-alias sensitive-data ``` ### Step 4: Verify encryption Confirm encryption is active for your workspace. ```bash cloudflow encryption status ``` ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            },
            {
              "doc_id": "howto_set_up_database_connection",
              "chunk_id": "howto_set_up_database_connection_fix_0",
              "content": "# How to Set Up Database Connection ## Prerequisites - Database with network access to CloudFlow - Connection credentials ## Steps ### Step 1: Add connection secret Store your database connection string as a secret. ```bash cloudflow secrets set DATABASE_URL postgresql://user:pass@host:5432/db ``` ### Step 2: Configure connection pool Set pool size based on your plan limits. Free tier allows 5 connections, Pro allows 20. > **Note:** Pool exhaustion will queue requests ### Step 3: Test connection Verify the database is accessible from CloudFlow. ```bash cloudflow db test --secret DATABASE_URL ``` ### Step 4: Enable SSL For production, always require SSL connections. ```bash cloudflow db configure --require-ssl ``` ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            },
            {
              "doc_id": "howto_create_api_integration",
              "chunk_id": "howto_create_api_integration_fix_0",
              "content": "# How to Create API Integration ## Prerequisites - CloudFlow account ## Steps ### Step 1: Generate API key Create an API key for programmatic access. ```bash cloudflow apikeys create --name 'CI Integration' --scopes workflows:read,workflows:execute ``` ### Step 2: Store key securely Never commit API keys to source control. Use environment variables or secret management. > **Note:** Rotate keys every 90 days ### Step 3: Test authentication Verify the API key works correctly. ```bash curl -H 'Authorization: Bearer cf_...' https://api.cloudflow.io/v1/me ``` ### Step 4: Implement rate limiting Handle rate limit responses (HTTP 429) with exponential backoff. > **Note:** Default limit is 100 requests per minute ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            },
            {
              "doc_id": "api_secrets",
              "chunk_id": "api_secrets_fix_0",
              "content": "# Secrets API Reference ## Overview The Secrets API provides programmatic access to secrets functionality. Base URL: `https://api.cloudflow.io/v1/secrets` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /secrets List secret names (values not returned). **Parameters:** **Response:** - Success: `200` - Returns array of secret metadata - Error: `400` - Bad Request **Example:** ```json {\"secrets\": [{\"name\": \"DATABASE_URL\", \"created_at\": \"2024-01-01\", \"updated_at\": \"2024-01-10\"}]} ``` ### PUT /secrets/{name} Create or update a secret. **Parameters:** - `value` (string): Secret value (encrypted at rest) **Response:** - Success: `200` - Secret saved - Error: `400` - Bad Request **Example:** ```json {\"name\": \"DATABASE_URL\", \"updated_at\": \"2024-01-15T10:00:00Z\"} ``` ### DELETE /secrets/{name} Delete a secret. **Parameters:** **Response:** - Success: `204` - Secret deleted - Error: `400` - Bad Request **Example:** ```json {} ```",
              "score": null
            },
            {
              "doc_id": "arch_api_gateway",
              "chunk_id": "arch_api_gateway_fix_0",
              "content": "# API Gateway Architecture ## Overview The API Gateway handles all incoming HTTP requests, providing authentication, rate limiting, and request routing. Built on Kong with custom plugins for CloudFlow-specific functionality. ## Design Principles - **Zero Trust**: Every request is authenticated and authorized - **Defense in Depth**: Multiple layers of security validation ## Components ### Kong Gateway Primary ingress controller. Handles TLS termination, request routing, and plugin execution. **Technology:** Kong 3.4 on Kubernetes **Scaling:** Horizontal with HPA ### Auth Plugin Validates JWT tokens, API keys, and OAuth2 flows. Token validation latency under 5ms. **Technology:** Custom Lua plugin **Scaling:** Stateless ### Rate Limiter Enforces per-user and per-IP rate limits. Default limit is 100 requests per minute. **Technology:** Redis-backed sliding window **Scaling:** Shared Redis cluster ## Data Flow Client \u2192 TLS \u2192 Kong \u2192 Auth \u2192 Rate Limit \u2192 Backend Services ## Performance Characteristics - **Latency P50:** 12ms - **Latency P99:** 45ms - **Throughput:** 50000 requests/second",
              "score": null
            }
          ],
          "latency_ms": 3.613589999076794,
          "query_id": "complex_051",
          "dimension": "original",
          "query": "How does CloudFlow ensure security across the stack?"
        },
        {
          "key_facts": [
            "OAuth 2.0",
            "JWT",
            "bcrypt",
            "AES-256",
            "TLS",
            "Zero Trust"
          ],
          "found_facts": [
            "JWT",
            "AES-256",
            "TLS",
            "Zero Trust"
          ],
          "missed_facts": [
            "OAuth 2.0",
            "bcrypt"
          ],
          "coverage": 0.6666666666666666,
          "expected_docs": [
            "adr_003",
            "arch_authentication_service",
            "arch_api_gateway",
            "howto_set_up_data_encryption"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "arch_api_gateway",
              "chunk_id": "arch_api_gateway_fix_0",
              "content": "# API Gateway Architecture ## Overview The API Gateway handles all incoming HTTP requests, providing authentication, rate limiting, and request routing. Built on Kong with custom plugins for CloudFlow-specific functionality. ## Design Principles - **Zero Trust**: Every request is authenticated and authorized - **Defense in Depth**: Multiple layers of security validation ## Components ### Kong Gateway Primary ingress controller. Handles TLS termination, request routing, and plugin execution. **Technology:** Kong 3.4 on Kubernetes **Scaling:** Horizontal with HPA ### Auth Plugin Validates JWT tokens, API keys, and OAuth2 flows. Token validation latency under 5ms. **Technology:** Custom Lua plugin **Scaling:** Stateless ### Rate Limiter Enforces per-user and per-IP rate limits. Default limit is 100 requests per minute. **Technology:** Redis-backed sliding window **Scaling:** Shared Redis cluster ## Data Flow Client \u2192 TLS \u2192 Kong \u2192 Auth \u2192 Rate Limit \u2192 Backend Services ## Performance Characteristics - **Latency P50:** 12ms - **Latency P99:** 45ms - **Throughput:** 50000 requests/second",
              "score": null
            },
            {
              "doc_id": "howto_set_up_data_encryption",
              "chunk_id": "howto_set_up_data_encryption_fix_0",
              "content": "# How to Set Up Data Encryption ## Prerequisites - CloudFlow Enterprise plan ## Steps ### Step 1: Enable encryption at rest All data is encrypted at rest using AES-256 by default. > **Note:** Encryption is automatic for all plans ### Step 2: Configure KMS Bring your own encryption keys using AWS KMS or GCP KMS. ```bash cloudflow encryption configure --kms-key arn:aws:kms:... ``` ### Step 3: Enable field-level encryption Encrypt sensitive fields within your workflow data. ```bash cloudflow encryption add-field --path $.user.ssn --key-alias sensitive-data ``` ### Step 4: Verify encryption Confirm encryption is active for your workspace. ```bash cloudflow encryption status ``` ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            },
            {
              "doc_id": "howto_create_api_integration",
              "chunk_id": "howto_create_api_integration_fix_0",
              "content": "# How to Create API Integration ## Prerequisites - CloudFlow account ## Steps ### Step 1: Generate API key Create an API key for programmatic access. ```bash cloudflow apikeys create --name 'CI Integration' --scopes workflows:read,workflows:execute ``` ### Step 2: Store key securely Never commit API keys to source control. Use environment variables or secret management. > **Note:** Rotate keys every 90 days ### Step 3: Test authentication Verify the API key works correctly. ```bash curl -H 'Authorization: Bearer cf_...' https://api.cloudflow.io/v1/me ``` ### Step 4: Implement rate limiting Handle rate limit responses (HTTP 429) with exponential backoff. > **Note:** Default limit is 100 requests per minute ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            },
            {
              "doc_id": "arch_workflow_engine",
              "chunk_id": "arch_workflow_engine_fix_0",
              "content": "# Workflow Engine Architecture ## Overview The Workflow Engine is the core component responsible for executing user-defined workflows. It uses an event-driven architecture with support for parallel execution, conditional branching, and automatic retries. ## Design Principles - **Idempotency**: All operations can be safely retried without side effects - **Eventual Consistency**: State converges to consistent view within 5 seconds - **Fault Tolerance**: System continues operating despite component failures ## Components ### Scheduler Manages workflow scheduling and triggers. Supports cron expressions, webhooks, and event-based triggers. **Technology:** Custom Go service **Scaling:** Horizontal with leader election ### Executor Runs individual workflow steps in isolated containers. Maximum execution time is 30 minutes per step. **Technology:** Kubernetes Jobs **Scaling:** Auto-scaling based on queue depth ### State Store Persists workflow state, execution history, and step results. Uses optimistic locking for concurrent updates. **Technology:** PostgreSQL with JSONB **Scaling:** Primary-replica with read replicas ## Data Flow Triggers \u2192 Scheduler \u2192 Job Queue \u2192 Executor \u2192 State Store \u2192 Webhooks/Notifications ## Performance Characteristics - **Latency P50:** 45ms - **Latency P99:** 250ms - **Throughput:** 5000 executions/minute",
              "score": null
            },
            {
              "doc_id": "howto_set_up_database_connection",
              "chunk_id": "howto_set_up_database_connection_fix_0",
              "content": "# How to Set Up Database Connection ## Prerequisites - Database with network access to CloudFlow - Connection credentials ## Steps ### Step 1: Add connection secret Store your database connection string as a secret. ```bash cloudflow secrets set DATABASE_URL postgresql://user:pass@host:5432/db ``` ### Step 2: Configure connection pool Set pool size based on your plan limits. Free tier allows 5 connections, Pro allows 20. > **Note:** Pool exhaustion will queue requests ### Step 3: Test connection Verify the database is accessible from CloudFlow. ```bash cloudflow db test --secret DATABASE_URL ``` ### Step 4: Enable SSL For production, always require SSL connections. ```bash cloudflow db configure --require-ssl ``` ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            }
          ],
          "latency_ms": 4.397750000862288,
          "query_id": "complex_051",
          "dimension": "synonym",
          "query": "what security controls and protection mechanisms are implemented throughout the cloudflow architecture?"
        },
        {
          "key_facts": [
            "OAuth 2.0",
            "JWT",
            "bcrypt",
            "AES-256",
            "TLS",
            "Zero Trust"
          ],
          "found_facts": [
            "OAuth 2.0",
            "JWT",
            "bcrypt",
            "AES-256"
          ],
          "missed_facts": [
            "TLS",
            "Zero Trust"
          ],
          "coverage": 0.6666666666666666,
          "expected_docs": [
            "adr_003",
            "arch_authentication_service",
            "arch_api_gateway",
            "howto_set_up_data_encryption"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "api_audit",
              "chunk_id": "api_audit_fix_0",
              "content": "# Audit API Reference ## Overview The Audit API provides programmatic access to audit functionality. Base URL: `https://api.cloudflow.io/v1/audit` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /audit/logs Retrieve audit logs for compliance. **Parameters:** - `start_date` (date): Start date for log range - `end_date` (date): End date for log range - `actor` (string): Filter by user ID **Response:** - Success: `200` - Returns array of audit log entries - Error: `400` - Bad Request **Example:** ```json {\"logs\": [{\"timestamp\": \"2024-01-15T10:00:00Z\", \"actor\": \"usr_001\", \"action\": \"workflow.created\", \"resource\": \"wf_123\"}]} ```",
              "score": null
            },
            {
              "doc_id": "arch_monitoring_stack",
              "chunk_id": "arch_monitoring_stack_fix_0",
              "content": "# Monitoring Stack Architecture ## Overview The Monitoring Stack provides observability into system health, performance, and errors. Includes metrics, logs, traces, and alerting. ## Design Principles - **Observability**: Every component emits metrics and traces - **Actionable Alerts**: Alerts include runbook links ## Components ### Metrics Collects time-series metrics. Retention is 15 days local, 1 year in object storage. **Technology:** Prometheus with Thanos **Scaling:** Federated with long-term storage ### Logging Centralized log aggregation. Supports structured JSON logs with label indexing. **Technology:** Loki with Grafana **Scaling:** Distributed with sharding ### Tracing Distributed request tracing. Trace retention is 7 days. **Technology:** Jaeger with OpenTelemetry **Scaling:** Sampling at 10% ### Alerting Routes alerts to on-call engineers. Supports escalation policies and silencing. **Technology:** Alertmanager with PagerDuty **Scaling:** HA pair ## Data Flow Application \u2192 Collector \u2192 Storage \u2192 Query \u2192 Dashboard/Alert ## Performance Characteristics - **Latency P50:** N/A - **Latency P99:** N/A - **Throughput:** 1M metrics/second, 100GB logs/day",
              "score": null
            },
            {
              "doc_id": "arch_authentication_service",
              "chunk_id": "arch_authentication_service_fix_0",
              "content": "# Authentication Service Architecture ## Overview The Authentication Service handles user identity, session management, and token issuance. Supports OAuth 2.0, SAML, and API key authentication. ## Design Principles - **Zero Knowledge**: Passwords never stored in plaintext - **Token Rotation**: Refresh tokens rotate on each use ## Components ### Identity Provider Manages user accounts and credentials. Password hashing uses bcrypt with cost factor 12. **Technology:** Custom Go service **Scaling:** Horizontal stateless ### Token Service Issues and validates access tokens. Token lifetime is 1 hour with 7-day refresh tokens. **Technology:** JWT with RS256 **Scaling:** Horizontal stateless ### SSO Gateway Integrates with external identity providers. Supports Google, Microsoft, Okta, and custom SAML. **Technology:** Dex OIDC proxy **Scaling:** Active-passive ## Data Flow Credentials \u2192 Validation \u2192 Token Issue \u2192 Token Refresh \u2192 Logout ## Performance Characteristics - **Latency P50:** 50ms - **Latency P99:** 200ms - **Throughput:** 5000 auth/second",
              "score": null
            },
            {
              "doc_id": "howto_set_up_data_encryption",
              "chunk_id": "howto_set_up_data_encryption_fix_0",
              "content": "# How to Set Up Data Encryption ## Prerequisites - CloudFlow Enterprise plan ## Steps ### Step 1: Enable encryption at rest All data is encrypted at rest using AES-256 by default. > **Note:** Encryption is automatic for all plans ### Step 2: Configure KMS Bring your own encryption keys using AWS KMS or GCP KMS. ```bash cloudflow encryption configure --kms-key arn:aws:kms:... ``` ### Step 3: Enable field-level encryption Encrypt sensitive fields within your workflow data. ```bash cloudflow encryption add-field --path $.user.ssn --key-alias sensitive-data ``` ### Step 4: Verify encryption Confirm encryption is active for your workspace. ```bash cloudflow encryption status ``` ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            },
            {
              "doc_id": "adr_003",
              "chunk_id": "adr_003_fix_0",
              "content": "# ADR-003: Implement OAuth 2.0 with JWT for Authentication **Status:** Accepted **Date:** 2024-05-08 ## Context We need a secure authentication system that supports API keys, user sessions, and third-party integrations. ## Options Considered ### Option 1: OAuth 2.0 with JWT Industry standard token-based auth. **Pros:** - Industry standard - Stateless validation - Supports refresh tokens - SSO compatible **Cons:** - Token size overhead - Cannot revoke individual tokens - Complexity ### Option 2: Session-based auth Traditional server-side sessions. **Pros:** - Simple implementation - Easy revocation - Smaller payloads **Cons:** - Requires session store - Not suitable for APIs - Scaling challenges ### Option 3: API keys only Simple key-based authentication. **Pros:** - Very simple - Low overhead - Easy to implement **Cons:** - No user context - Hard to rotate - No SSO support ## Decision We will implement OAuth 2.0 with JWT tokens. Access tokens expire after 1 hour, refresh tokens after 7 days. ## Consequences - Implement token rotation on refresh - Support API keys for machine-to-machine auth - Add MFA for admin accounts",
              "score": null
            }
          ],
          "latency_ms": 3.8541769990843022,
          "query_id": "complex_051",
          "dimension": "problem",
          "query": "our security audit is asking about auth, encryption, and data protection - where do I find what security measures are in place at each layer?"
        },
        {
          "key_facts": [
            "OAuth 2.0",
            "JWT",
            "bcrypt",
            "AES-256",
            "TLS",
            "Zero Trust"
          ],
          "found_facts": [
            "OAuth 2.0",
            "JWT",
            "AES-256",
            "TLS",
            "Zero Trust"
          ],
          "missed_facts": [
            "bcrypt"
          ],
          "coverage": 0.8333333333333334,
          "expected_docs": [
            "adr_003",
            "arch_authentication_service",
            "arch_api_gateway",
            "howto_set_up_data_encryption"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "howto_set_up_data_encryption",
              "chunk_id": "howto_set_up_data_encryption_fix_0",
              "content": "# How to Set Up Data Encryption ## Prerequisites - CloudFlow Enterprise plan ## Steps ### Step 1: Enable encryption at rest All data is encrypted at rest using AES-256 by default. > **Note:** Encryption is automatic for all plans ### Step 2: Configure KMS Bring your own encryption keys using AWS KMS or GCP KMS. ```bash cloudflow encryption configure --kms-key arn:aws:kms:... ``` ### Step 3: Enable field-level encryption Encrypt sensitive fields within your workflow data. ```bash cloudflow encryption add-field --path $.user.ssn --key-alias sensitive-data ``` ### Step 4: Verify encryption Confirm encryption is active for your workspace. ```bash cloudflow encryption status ``` ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            },
            {
              "doc_id": "howto_create_api_integration",
              "chunk_id": "howto_create_api_integration_fix_0",
              "content": "# How to Create API Integration ## Prerequisites - CloudFlow account ## Steps ### Step 1: Generate API key Create an API key for programmatic access. ```bash cloudflow apikeys create --name 'CI Integration' --scopes workflows:read,workflows:execute ``` ### Step 2: Store key securely Never commit API keys to source control. Use environment variables or secret management. > **Note:** Rotate keys every 90 days ### Step 3: Test authentication Verify the API key works correctly. ```bash curl -H 'Authorization: Bearer cf_...' https://api.cloudflow.io/v1/me ``` ### Step 4: Implement rate limiting Handle rate limit responses (HTTP 429) with exponential backoff. > **Note:** Default limit is 100 requests per minute ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            },
            {
              "doc_id": "howto_configure_single_sign_on",
              "chunk_id": "howto_configure_single_sign_on_fix_0",
              "content": "# How to Configure Single Sign-On ## Prerequisites - Admin access - Enterprise plan - Identity provider access ## Steps ### Step 1: Choose provider CloudFlow supports SAML 2.0, OpenID Connect, and OAuth 2.0 providers. ### Step 2: Get metadata Download CloudFlow's SAML metadata for your IdP configuration. ```bash cloudflow sso metadata --format xml > cloudflow-metadata.xml ``` ### Step 3: Configure IdP Add CloudFlow as a service provider in your identity provider. > **Note:** ACS URL: https://auth.cloudflow.io/saml/callback ### Step 4: Upload IdP metadata Provide your IdP's metadata to CloudFlow. ```bash cloudflow sso configure --idp-metadata ./idp-metadata.xml ``` ### Step 5: Test login Verify SSO login works correctly. ```bash cloudflow sso test ``` ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            },
            {
              "doc_id": "api_secrets",
              "chunk_id": "api_secrets_fix_0",
              "content": "# Secrets API Reference ## Overview The Secrets API provides programmatic access to secrets functionality. Base URL: `https://api.cloudflow.io/v1/secrets` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /secrets List secret names (values not returned). **Parameters:** **Response:** - Success: `200` - Returns array of secret metadata - Error: `400` - Bad Request **Example:** ```json {\"secrets\": [{\"name\": \"DATABASE_URL\", \"created_at\": \"2024-01-01\", \"updated_at\": \"2024-01-10\"}]} ``` ### PUT /secrets/{name} Create or update a secret. **Parameters:** - `value` (string): Secret value (encrypted at rest) **Response:** - Success: `200` - Secret saved - Error: `400` - Bad Request **Example:** ```json {\"name\": \"DATABASE_URL\", \"updated_at\": \"2024-01-15T10:00:00Z\"} ``` ### DELETE /secrets/{name} Delete a secret. **Parameters:** **Response:** - Success: `204` - Secret deleted - Error: `400` - Bad Request **Example:** ```json {} ```",
              "score": null
            },
            {
              "doc_id": "arch_api_gateway",
              "chunk_id": "arch_api_gateway_fix_0",
              "content": "# API Gateway Architecture ## Overview The API Gateway handles all incoming HTTP requests, providing authentication, rate limiting, and request routing. Built on Kong with custom plugins for CloudFlow-specific functionality. ## Design Principles - **Zero Trust**: Every request is authenticated and authorized - **Defense in Depth**: Multiple layers of security validation ## Components ### Kong Gateway Primary ingress controller. Handles TLS termination, request routing, and plugin execution. **Technology:** Kong 3.4 on Kubernetes **Scaling:** Horizontal with HPA ### Auth Plugin Validates JWT tokens, API keys, and OAuth2 flows. Token validation latency under 5ms. **Technology:** Custom Lua plugin **Scaling:** Stateless ### Rate Limiter Enforces per-user and per-IP rate limits. Default limit is 100 requests per minute. **Technology:** Redis-backed sliding window **Scaling:** Shared Redis cluster ## Data Flow Client \u2192 TLS \u2192 Kong \u2192 Auth \u2192 Rate Limit \u2192 Backend Services ## Performance Characteristics - **Latency P50:** 12ms - **Latency P99:** 45ms - **Throughput:** 50000 requests/second",
              "score": null
            }
          ],
          "latency_ms": 4.310386999350158,
          "query_id": "complex_051",
          "dimension": "casual",
          "query": "cloudflow security - auth encryption tls etc full stack overview"
        },
        {
          "key_facts": [
            "OAuth 2.0",
            "JWT",
            "bcrypt",
            "AES-256",
            "TLS",
            "Zero Trust"
          ],
          "found_facts": [
            "OAuth 2.0",
            "JWT",
            "bcrypt",
            "TLS",
            "Zero Trust"
          ],
          "missed_facts": [
            "AES-256"
          ],
          "coverage": 0.8333333333333334,
          "expected_docs": [
            "adr_003",
            "arch_authentication_service",
            "arch_api_gateway",
            "howto_set_up_data_encryption"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "arch_authentication_service",
              "chunk_id": "arch_authentication_service_fix_0",
              "content": "# Authentication Service Architecture ## Overview The Authentication Service handles user identity, session management, and token issuance. Supports OAuth 2.0, SAML, and API key authentication. ## Design Principles - **Zero Knowledge**: Passwords never stored in plaintext - **Token Rotation**: Refresh tokens rotate on each use ## Components ### Identity Provider Manages user accounts and credentials. Password hashing uses bcrypt with cost factor 12. **Technology:** Custom Go service **Scaling:** Horizontal stateless ### Token Service Issues and validates access tokens. Token lifetime is 1 hour with 7-day refresh tokens. **Technology:** JWT with RS256 **Scaling:** Horizontal stateless ### SSO Gateway Integrates with external identity providers. Supports Google, Microsoft, Okta, and custom SAML. **Technology:** Dex OIDC proxy **Scaling:** Active-passive ## Data Flow Credentials \u2192 Validation \u2192 Token Issue \u2192 Token Refresh \u2192 Logout ## Performance Characteristics - **Latency P50:** 50ms - **Latency P99:** 200ms - **Throughput:** 5000 auth/second",
              "score": null
            },
            {
              "doc_id": "adr_003",
              "chunk_id": "adr_003_fix_0",
              "content": "# ADR-003: Implement OAuth 2.0 with JWT for Authentication **Status:** Accepted **Date:** 2024-05-08 ## Context We need a secure authentication system that supports API keys, user sessions, and third-party integrations. ## Options Considered ### Option 1: OAuth 2.0 with JWT Industry standard token-based auth. **Pros:** - Industry standard - Stateless validation - Supports refresh tokens - SSO compatible **Cons:** - Token size overhead - Cannot revoke individual tokens - Complexity ### Option 2: Session-based auth Traditional server-side sessions. **Pros:** - Simple implementation - Easy revocation - Smaller payloads **Cons:** - Requires session store - Not suitable for APIs - Scaling challenges ### Option 3: API keys only Simple key-based authentication. **Pros:** - Very simple - Low overhead - Easy to implement **Cons:** - No user context - Hard to rotate - No SSO support ## Decision We will implement OAuth 2.0 with JWT tokens. Access tokens expire after 1 hour, refresh tokens after 7 days. ## Consequences - Implement token rotation on refresh - Support API keys for machine-to-machine auth - Add MFA for admin accounts",
              "score": null
            },
            {
              "doc_id": "arch_monitoring_stack",
              "chunk_id": "arch_monitoring_stack_fix_0",
              "content": "# Monitoring Stack Architecture ## Overview The Monitoring Stack provides observability into system health, performance, and errors. Includes metrics, logs, traces, and alerting. ## Design Principles - **Observability**: Every component emits metrics and traces - **Actionable Alerts**: Alerts include runbook links ## Components ### Metrics Collects time-series metrics. Retention is 15 days local, 1 year in object storage. **Technology:** Prometheus with Thanos **Scaling:** Federated with long-term storage ### Logging Centralized log aggregation. Supports structured JSON logs with label indexing. **Technology:** Loki with Grafana **Scaling:** Distributed with sharding ### Tracing Distributed request tracing. Trace retention is 7 days. **Technology:** Jaeger with OpenTelemetry **Scaling:** Sampling at 10% ### Alerting Routes alerts to on-call engineers. Supports escalation policies and silencing. **Technology:** Alertmanager with PagerDuty **Scaling:** HA pair ## Data Flow Application \u2192 Collector \u2192 Storage \u2192 Query \u2192 Dashboard/Alert ## Performance Characteristics - **Latency P50:** N/A - **Latency P99:** N/A - **Throughput:** 1M metrics/second, 100GB logs/day",
              "score": null
            },
            {
              "doc_id": "config_api_gateway",
              "chunk_id": "config_api_gateway_fix_0",
              "content": "# API Gateway Configuration Reference ## Overview This document describes all configuration options for API Gateway. ## Environment Variables | Variable | Type | Default | Description | |----------|------|---------|-------------| | `GATEWAY_PORT` | integer | 8000 | Gateway listen port | | `GATEWAY_RATE_LIMIT` | integer | 100 | Requests per minute per user | | `GATEWAY_TIMEOUT_MS` | integer | 30000 | Request timeout in milliseconds | | `GATEWAY_MAX_BODY_SIZE` | string | 10MB | Maximum request body size | | `GATEWAY_CORS_ORIGINS` | string | * | Allowed CORS origins | | `GATEWAY_SSL_ENABLED` | boolean | true | Enable TLS termination | ## Configuration File Configuration can also be provided via `api gateway.yaml`: ```yaml gateway_port: 8000 gateway_rate_limit: 100 gateway_timeout_ms: 30000 gateway_max_body_size: 10MB gateway_cors_origins: * ``` ## Validation Configuration is validated at startup. Invalid configuration will prevent the service from starting.",
              "score": null
            },
            {
              "doc_id": "arch_api_gateway",
              "chunk_id": "arch_api_gateway_fix_0",
              "content": "# API Gateway Architecture ## Overview The API Gateway handles all incoming HTTP requests, providing authentication, rate limiting, and request routing. Built on Kong with custom plugins for CloudFlow-specific functionality. ## Design Principles - **Zero Trust**: Every request is authenticated and authorized - **Defense in Depth**: Multiple layers of security validation ## Components ### Kong Gateway Primary ingress controller. Handles TLS termination, request routing, and plugin execution. **Technology:** Kong 3.4 on Kubernetes **Scaling:** Horizontal with HPA ### Auth Plugin Validates JWT tokens, API keys, and OAuth2 flows. Token validation latency under 5ms. **Technology:** Custom Lua plugin **Scaling:** Stateless ### Rate Limiter Enforces per-user and per-IP rate limits. Default limit is 100 requests per minute. **Technology:** Redis-backed sliding window **Scaling:** Shared Redis cluster ## Data Flow Client \u2192 TLS \u2192 Kong \u2192 Auth \u2192 Rate Limit \u2192 Backend Services ## Performance Characteristics - **Latency P50:** 12ms - **Latency P99:** 45ms - **Throughput:** 50000 requests/second",
              "score": null
            }
          ],
          "latency_ms": 4.319613999541616,
          "query_id": "complex_051",
          "dimension": "contextual",
          "query": "preparing for SOC 2 compliance review and need to document our security posture - what authentication, encryption, and network security is implemented across gateway, services, and storage?"
        },
        {
          "key_facts": [
            "OAuth 2.0",
            "JWT",
            "bcrypt",
            "AES-256",
            "TLS",
            "Zero Trust"
          ],
          "found_facts": [
            "OAuth 2.0",
            "JWT",
            "bcrypt",
            "AES-256"
          ],
          "missed_facts": [
            "TLS",
            "Zero Trust"
          ],
          "coverage": 0.6666666666666666,
          "expected_docs": [
            "adr_003",
            "arch_authentication_service",
            "arch_api_gateway",
            "howto_set_up_data_encryption"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "howto_set_up_data_encryption",
              "chunk_id": "howto_set_up_data_encryption_fix_0",
              "content": "# How to Set Up Data Encryption ## Prerequisites - CloudFlow Enterprise plan ## Steps ### Step 1: Enable encryption at rest All data is encrypted at rest using AES-256 by default. > **Note:** Encryption is automatic for all plans ### Step 2: Configure KMS Bring your own encryption keys using AWS KMS or GCP KMS. ```bash cloudflow encryption configure --kms-key arn:aws:kms:... ``` ### Step 3: Enable field-level encryption Encrypt sensitive fields within your workflow data. ```bash cloudflow encryption add-field --path $.user.ssn --key-alias sensitive-data ``` ### Step 4: Verify encryption Confirm encryption is active for your workspace. ```bash cloudflow encryption status ``` ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            },
            {
              "doc_id": "arch_authentication_service",
              "chunk_id": "arch_authentication_service_fix_0",
              "content": "# Authentication Service Architecture ## Overview The Authentication Service handles user identity, session management, and token issuance. Supports OAuth 2.0, SAML, and API key authentication. ## Design Principles - **Zero Knowledge**: Passwords never stored in plaintext - **Token Rotation**: Refresh tokens rotate on each use ## Components ### Identity Provider Manages user accounts and credentials. Password hashing uses bcrypt with cost factor 12. **Technology:** Custom Go service **Scaling:** Horizontal stateless ### Token Service Issues and validates access tokens. Token lifetime is 1 hour with 7-day refresh tokens. **Technology:** JWT with RS256 **Scaling:** Horizontal stateless ### SSO Gateway Integrates with external identity providers. Supports Google, Microsoft, Okta, and custom SAML. **Technology:** Dex OIDC proxy **Scaling:** Active-passive ## Data Flow Credentials \u2192 Validation \u2192 Token Issue \u2192 Token Refresh \u2192 Logout ## Performance Characteristics - **Latency P50:** 50ms - **Latency P99:** 200ms - **Throughput:** 5000 auth/second",
              "score": null
            },
            {
              "doc_id": "adr_003",
              "chunk_id": "adr_003_fix_0",
              "content": "# ADR-003: Implement OAuth 2.0 with JWT for Authentication **Status:** Accepted **Date:** 2024-05-08 ## Context We need a secure authentication system that supports API keys, user sessions, and third-party integrations. ## Options Considered ### Option 1: OAuth 2.0 with JWT Industry standard token-based auth. **Pros:** - Industry standard - Stateless validation - Supports refresh tokens - SSO compatible **Cons:** - Token size overhead - Cannot revoke individual tokens - Complexity ### Option 2: Session-based auth Traditional server-side sessions. **Pros:** - Simple implementation - Easy revocation - Smaller payloads **Cons:** - Requires session store - Not suitable for APIs - Scaling challenges ### Option 3: API keys only Simple key-based authentication. **Pros:** - Very simple - Low overhead - Easy to implement **Cons:** - No user context - Hard to rotate - No SSO support ## Decision We will implement OAuth 2.0 with JWT tokens. Access tokens expire after 1 hour, refresh tokens after 7 days. ## Consequences - Implement token rotation on refresh - Support API keys for machine-to-machine auth - Add MFA for admin accounts",
              "score": null
            },
            {
              "doc_id": "api_secrets",
              "chunk_id": "api_secrets_fix_0",
              "content": "# Secrets API Reference ## Overview The Secrets API provides programmatic access to secrets functionality. Base URL: `https://api.cloudflow.io/v1/secrets` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /secrets List secret names (values not returned). **Parameters:** **Response:** - Success: `200` - Returns array of secret metadata - Error: `400` - Bad Request **Example:** ```json {\"secrets\": [{\"name\": \"DATABASE_URL\", \"created_at\": \"2024-01-01\", \"updated_at\": \"2024-01-10\"}]} ``` ### PUT /secrets/{name} Create or update a secret. **Parameters:** - `value` (string): Secret value (encrypted at rest) **Response:** - Success: `200` - Secret saved - Error: `400` - Bad Request **Example:** ```json {\"name\": \"DATABASE_URL\", \"updated_at\": \"2024-01-15T10:00:00Z\"} ``` ### DELETE /secrets/{name} Delete a secret. **Parameters:** **Response:** - Success: `204` - Secret deleted - Error: `400` - Bad Request **Example:** ```json {} ```",
              "score": null
            },
            {
              "doc_id": "arch_data_pipeline",
              "chunk_id": "arch_data_pipeline_fix_0",
              "content": "# Data Pipeline Architecture ## Overview The Data Pipeline processes and transforms data between workflow steps. It supports batch processing up to 10GB and streaming for real-time use cases. ## Design Principles - **Exactly-Once Processing**: Each record is processed exactly once - **Backpressure Handling**: System gracefully handles varying load ## Components ### Stream Processor Handles real-time data streams. Maximum throughput of 100000 messages per second. **Technology:** Apache Kafka with Flink **Scaling:** Partition-based scaling ### Batch Processor Processes large batch jobs. Supports data formats: JSON, CSV, Parquet, Avro. **Technology:** Apache Spark on Kubernetes **Scaling:** Dynamic resource allocation ### Data Lake Long-term data storage with time-travel queries. Retention configurable up to 7 years. **Technology:** S3 with Iceberg tables **Scaling:** Unlimited with tiered storage ## Data Flow Input \u2192 Validation \u2192 Transform \u2192 Enrich \u2192 Output ## Performance Characteristics - **Latency P50:** 100ms (streaming), 5min (batch) - **Latency P99:** 500ms (streaming), 30min (batch) - **Throughput:** 100000 messages/second (streaming)",
              "score": null
            }
          ],
          "latency_ms": 5.048281000199495,
          "query_id": "complex_051",
          "dimension": "negation",
          "query": "is our data even encrypted? and what about authentication - is it properly secured end-to-end or are there gaps?"
        },
        {
          "key_facts": [
            "DATABASE_URL",
            "REDIS_URL",
            "GATEWAY_SSL_ENABLED",
            "DATABASE_SSL_MODE"
          ],
          "found_facts": [
            "DATABASE_URL",
            "REDIS_URL",
            "DATABASE_SSL_MODE"
          ],
          "missed_facts": [
            "GATEWAY_SSL_ENABLED"
          ],
          "coverage": 0.75,
          "expected_docs": [
            "config_workflow_engine",
            "config_api_gateway",
            "config_database",
            "config_cache"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "config_cache",
              "chunk_id": "config_cache_fix_0",
              "content": "# Cache Configuration Reference ## Overview This document describes all configuration options for Cache. ## Environment Variables | Variable | Type | Default | Description | |----------|------|---------|-------------| | `REDIS_URL` | string | required | Redis connection string | | `REDIS_POOL_SIZE` | integer | 10 | Connection pool size | | `CACHE_TTL_SECONDS` | integer | 300 | Default cache TTL | | `CACHE_PREFIX` | string | cf: | Key prefix for namespacing | | `CACHE_COMPRESSION` | boolean | true | Enable value compression | | `CACHE_MAX_SIZE_MB` | integer | 100 | Maximum cache size per pod | ## Configuration File Configuration can also be provided via `cache.yaml`: ```yaml redis_url: required redis_pool_size: 10 cache_ttl_seconds: 300 cache_prefix: cf: cache_compression: true ``` ## Validation Configuration is validated at startup. Invalid configuration will prevent the service from starting.",
              "score": null
            },
            {
              "doc_id": "config_database",
              "chunk_id": "config_database_fix_0",
              "content": "# Database Configuration Reference ## Overview This document describes all configuration options for Database. ## Environment Variables | Variable | Type | Default | Description | |----------|------|---------|-------------| | `DATABASE_URL` | string | required | PostgreSQL connection string | | `DATABASE_POOL_MIN` | integer | 5 | Minimum connection pool size | | `DATABASE_POOL_MAX` | integer | 20 | Maximum connection pool size | | `DATABASE_STATEMENT_TIMEOUT` | integer | 30000 | Query timeout in milliseconds | | `DATABASE_SSL_MODE` | string | require | SSL mode: disable, require, verify-full | | `DATABASE_MIGRATION_AUTO` | boolean | false | Run migrations on startup | ## Configuration File Configuration can also be provided via `database.yaml`: ```yaml database_url: required database_pool_min: 5 database_pool_max: 20 database_statement_timeout: 30000 database_ssl_mode: require ``` ## Validation Configuration is validated at startup. Invalid configuration will prevent the service from starting.",
              "score": null
            },
            {
              "doc_id": "adr_002",
              "chunk_id": "adr_002_fix_0",
              "content": "# ADR-002: Adopt Kubernetes for Container Orchestration **Status:** Accepted **Date:** 2024-01-24 ## Context We need a platform to deploy and scale our microservices. The platform must support auto-scaling, rolling deployments, and multi-region deployment. ## Options Considered ### Option 1: Kubernetes (EKS) Industry-standard container orchestration. **Pros:** - Industry standard - Rich ecosystem - Cloud agnostic - Auto-scaling **Cons:** - Operational complexity - Learning curve - Resource overhead ### Option 2: AWS ECS Amazon's native container service. **Pros:** - Simple operation - Native AWS integration - Lower overhead **Cons:** - AWS lock-in - Less flexible - Smaller community ### Option 3: Serverless (Lambda) Function-as-a-service approach. **Pros:** - Zero infrastructure - Pay per use - Auto-scaling **Cons:** - Cold starts - 15-minute limit - Vendor lock-in ## Decision We will use Kubernetes on EKS for container orchestration with Helm for deployments. ## Consequences - Need platform team expertise - Implement GitOps with ArgoCD - Use managed node groups for easier operations",
              "score": null
            },
            {
              "doc_id": "config_workflow_engine",
              "chunk_id": "config_workflow_engine_fix_0",
              "content": "# Workflow Engine Configuration Reference ## Overview This document describes all configuration options for Workflow Engine. ## Environment Variables | Variable | Type | Default | Description | |----------|------|---------|-------------| | `WORKFLOW_MAX_STEPS` | integer | 100 | Maximum steps per workflow | | `WORKFLOW_TIMEOUT_SECONDS` | integer | 3600 | Maximum workflow execution time | | `WORKFLOW_RETRY_LIMIT` | integer | 3 | Default retry attempts for failed steps | | `WORKFLOW_CONCURRENT_LIMIT` | integer | 10 | Maximum concurrent executions per workflow | | `EXECUTOR_POOL_SIZE` | integer | 50 | Number of executor workers | | `EXECUTOR_MEMORY_LIMIT` | string | 512Mi | Memory limit per executor | ## Configuration File Configuration can also be provided via `workflow engine.yaml`: ```yaml workflow_max_steps: 100 workflow_timeout_seconds: 3600 workflow_retry_limit: 3 workflow_concurrent_limit: 10 executor_pool_size: 50 ``` ## Validation Configuration is validated at startup. Invalid configuration will prevent the service from starting.",
              "score": null
            },
            {
              "doc_id": "adr_006",
              "chunk_id": "adr_006_fix_0",
              "content": "# ADR-006: Use Terraform for Infrastructure as Code **Status:** Accepted **Date:** 2024-11-24 ## Context We need a way to manage cloud infrastructure consistently across environments. ## Options Considered ### Option 1: Terraform Multi-cloud IaC tool. **Pros:** - Multi-cloud support - Large community - State management - Plan before apply **Cons:** - State file management - Learning curve - HCL limitations ### Option 2: AWS CloudFormation AWS-native IaC. **Pros:** - Native AWS support - No state file - Drift detection **Cons:** - AWS only - Verbose YAML/JSON - Slower development ### Option 3: Pulumi IaC with real programming languages. **Pros:** - Real languages - Type safety - Reusable components **Cons:** - Smaller community - Vendor lock-in - Commercial features ## Decision We will use Terraform with S3 backend for state and DynamoDB for locking. ## Consequences - Implement module structure - Use workspaces for environments - Automate with Atlantis",
              "score": null
            }
          ],
          "latency_ms": 3.72582799900556,
          "query_id": "complex_052",
          "dimension": "original",
          "query": "What configuration is needed for production deployment?"
        },
        {
          "key_facts": [
            "DATABASE_URL",
            "REDIS_URL",
            "GATEWAY_SSL_ENABLED",
            "DATABASE_SSL_MODE"
          ],
          "found_facts": [
            "DATABASE_URL"
          ],
          "missed_facts": [
            "REDIS_URL",
            "GATEWAY_SSL_ENABLED",
            "DATABASE_SSL_MODE"
          ],
          "coverage": 0.25,
          "expected_docs": [
            "config_workflow_engine",
            "config_api_gateway",
            "config_database",
            "config_cache"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "howto_configure_custom_domain",
              "chunk_id": "howto_configure_custom_domain_fix_0",
              "content": "# How to Configure Custom Domain ## Prerequisites - Domain with DNS access - CloudFlow Pro plan or higher ## Steps ### Step 1: Verify domain ownership Add a TXT record to verify domain ownership. ```bash cloudflow domains verify --domain app.example.com ``` ### Step 2: Configure DNS Add a CNAME record pointing to your CloudFlow endpoint. > **Note:** CNAME should point to ingress.cloudflow.io ### Step 3: Enable SSL CloudFlow automatically provisions SSL certificates via Let's Encrypt. ```bash cloudflow domains ssl --domain app.example.com ``` ### Step 4: Set as primary Make this the primary domain for your application. ```bash cloudflow domains set-primary --domain app.example.com ``` ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            },
            {
              "doc_id": "howto_set_up_ci_cd_pipeline",
              "chunk_id": "howto_set_up_ci_cd_pipeline_fix_0",
              "content": "# How to Set Up CI/CD Pipeline ## Prerequisites - GitHub account - CloudFlow API key - Repository admin access ## Steps ### Step 1: Create GitHub Actions workflow Create a new file at `.github/workflows/cloudflow.yml` with the deployment configuration. ```bash mkdir -p .github/workflows && touch .github/workflows/cloudflow.yml ``` ### Step 2: Configure secrets Add your CloudFlow API key to GitHub secrets. ```bash gh secret set CLOUDFLOW_API_KEY ``` ### Step 3: Add deployment step Add the CloudFlow deployment action to your workflow. > **Note:** Use the official cloudflow/deploy-action@v2 ### Step 4: Test the pipeline Push a commit to trigger the workflow. ```bash git push origin main ``` ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            },
            {
              "doc_id": "howto_migrate_from_v1_to_v2_api",
              "chunk_id": "howto_migrate_from_v1_to_v2_api_fix_0",
              "content": "# How to Migrate from v1 to v2 API ## Prerequisites - Existing v1 integration - v2 API access enabled ## Steps ### Step 1: Review breaking changes The v2 API changes authentication from API keys to OAuth 2.0. Review the migration guide. ### Step 2: Update client library Upgrade to the latest SDK version. ```bash npm install @cloudflow/sdk@latest ``` ### Step 3: Update authentication Replace API key authentication with OAuth 2.0 client credentials flow. > **Note:** Generate new credentials in the dashboard ### Step 4: Test in staging Run your integration tests against the v2 staging endpoint. ```bash CLOUDFLOW_API_URL=https://api-v2-staging.cloudflow.io npm test ``` ### Step 5: Switch production Update production configuration to use v2 API. ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            },
            {
              "doc_id": "howto_set_up_database_connection",
              "chunk_id": "howto_set_up_database_connection_fix_0",
              "content": "# How to Set Up Database Connection ## Prerequisites - Database with network access to CloudFlow - Connection credentials ## Steps ### Step 1: Add connection secret Store your database connection string as a secret. ```bash cloudflow secrets set DATABASE_URL postgresql://user:pass@host:5432/db ``` ### Step 2: Configure connection pool Set pool size based on your plan limits. Free tier allows 5 connections, Pro allows 20. > **Note:** Pool exhaustion will queue requests ### Step 3: Test connection Verify the database is accessible from CloudFlow. ```bash cloudflow db test --secret DATABASE_URL ``` ### Step 4: Enable SSL For production, always require SSL connections. ```bash cloudflow db configure --require-ssl ``` ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            },
            {
              "doc_id": "howto_configure_single_sign_on",
              "chunk_id": "howto_configure_single_sign_on_fix_0",
              "content": "# How to Configure Single Sign-On ## Prerequisites - Admin access - Enterprise plan - Identity provider access ## Steps ### Step 1: Choose provider CloudFlow supports SAML 2.0, OpenID Connect, and OAuth 2.0 providers. ### Step 2: Get metadata Download CloudFlow's SAML metadata for your IdP configuration. ```bash cloudflow sso metadata --format xml > cloudflow-metadata.xml ``` ### Step 3: Configure IdP Add CloudFlow as a service provider in your identity provider. > **Note:** ACS URL: https://auth.cloudflow.io/saml/callback ### Step 4: Upload IdP metadata Provide your IdP's metadata to CloudFlow. ```bash cloudflow sso configure --idp-metadata ./idp-metadata.xml ``` ### Step 5: Test login Verify SSO login works correctly. ```bash cloudflow sso test ``` ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            }
          ],
          "latency_ms": 3.627144000347471,
          "query_id": "complex_052",
          "dimension": "synonym",
          "query": "what env vars and settings are required to deploy cloudflow to a prod environment?"
        },
        {
          "key_facts": [
            "DATABASE_URL",
            "REDIS_URL",
            "GATEWAY_SSL_ENABLED",
            "DATABASE_SSL_MODE"
          ],
          "found_facts": [
            "DATABASE_URL",
            "GATEWAY_SSL_ENABLED",
            "DATABASE_SSL_MODE"
          ],
          "missed_facts": [
            "REDIS_URL"
          ],
          "coverage": 0.75,
          "expected_docs": [
            "config_workflow_engine",
            "config_api_gateway",
            "config_database",
            "config_cache"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "config_database",
              "chunk_id": "config_database_fix_0",
              "content": "# Database Configuration Reference ## Overview This document describes all configuration options for Database. ## Environment Variables | Variable | Type | Default | Description | |----------|------|---------|-------------| | `DATABASE_URL` | string | required | PostgreSQL connection string | | `DATABASE_POOL_MIN` | integer | 5 | Minimum connection pool size | | `DATABASE_POOL_MAX` | integer | 20 | Maximum connection pool size | | `DATABASE_STATEMENT_TIMEOUT` | integer | 30000 | Query timeout in milliseconds | | `DATABASE_SSL_MODE` | string | require | SSL mode: disable, require, verify-full | | `DATABASE_MIGRATION_AUTO` | boolean | false | Run migrations on startup | ## Configuration File Configuration can also be provided via `database.yaml`: ```yaml database_url: required database_pool_min: 5 database_pool_max: 20 database_statement_timeout: 30000 database_ssl_mode: require ``` ## Validation Configuration is validated at startup. Invalid configuration will prevent the service from starting.",
              "score": null
            },
            {
              "doc_id": "runbook_ssl_certificate_expiry",
              "chunk_id": "runbook_ssl_certificate_expiry_fix_0",
              "content": "# Runbook: SSL Certificate Expiry ## Severity **Level:** P3 - Medium **Response Time:** 15 minutes ## Symptoms - Certificate expiring within 7 days - SSL handshake failures - Browser security warnings ## Diagnosis ### Step 1 Check certificate expiry: `echo | openssl s_client -connect api.cloudflow.io:443 2>/dev/null | openssl x509 -noout -dates` ### Step 2 Verify cert-manager status: `kubectl get certificates -n production` ### Step 3 Check cert-manager logs: `kubectl logs -l app=cert-manager -n cert-manager --tail=50` ### Step 4 Verify DNS is correct: `dig +short api.cloudflow.io` ## Resolution ### Step 1 Trigger manual renewal: `kubectl delete certificate api-cert -n production && kubectl apply -f certificate.yaml` ### Step 2 Check rate limits: Let's Encrypt has 50 certs/domain/week limit ### Step 3 Use staging issuer for testing: switch to letsencrypt-staging ClusterIssuer ### Step 4 If Let's Encrypt fails, manually upload certificate ## Escalation If certificate cannot be renewed, escalate to security team immediately. Consider using backup certificate. ## Post-Incident - Document timeline in incident log - Schedule post-mortem within 48 hours - Update runbook if new patterns discovered",
              "score": null
            },
            {
              "doc_id": "howto_configure_single_sign_on",
              "chunk_id": "howto_configure_single_sign_on_fix_0",
              "content": "# How to Configure Single Sign-On ## Prerequisites - Admin access - Enterprise plan - Identity provider access ## Steps ### Step 1: Choose provider CloudFlow supports SAML 2.0, OpenID Connect, and OAuth 2.0 providers. ### Step 2: Get metadata Download CloudFlow's SAML metadata for your IdP configuration. ```bash cloudflow sso metadata --format xml > cloudflow-metadata.xml ``` ### Step 3: Configure IdP Add CloudFlow as a service provider in your identity provider. > **Note:** ACS URL: https://auth.cloudflow.io/saml/callback ### Step 4: Upload IdP metadata Provide your IdP's metadata to CloudFlow. ```bash cloudflow sso configure --idp-metadata ./idp-metadata.xml ``` ### Step 5: Test login Verify SSO login works correctly. ```bash cloudflow sso test ``` ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            },
            {
              "doc_id": "howto_set_up_database_connection",
              "chunk_id": "howto_set_up_database_connection_fix_0",
              "content": "# How to Set Up Database Connection ## Prerequisites - Database with network access to CloudFlow - Connection credentials ## Steps ### Step 1: Add connection secret Store your database connection string as a secret. ```bash cloudflow secrets set DATABASE_URL postgresql://user:pass@host:5432/db ``` ### Step 2: Configure connection pool Set pool size based on your plan limits. Free tier allows 5 connections, Pro allows 20. > **Note:** Pool exhaustion will queue requests ### Step 3: Test connection Verify the database is accessible from CloudFlow. ```bash cloudflow db test --secret DATABASE_URL ``` ### Step 4: Enable SSL For production, always require SSL connections. ```bash cloudflow db configure --require-ssl ``` ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            },
            {
              "doc_id": "config_api_gateway",
              "chunk_id": "config_api_gateway_fix_0",
              "content": "# API Gateway Configuration Reference ## Overview This document describes all configuration options for API Gateway. ## Environment Variables | Variable | Type | Default | Description | |----------|------|---------|-------------| | `GATEWAY_PORT` | integer | 8000 | Gateway listen port | | `GATEWAY_RATE_LIMIT` | integer | 100 | Requests per minute per user | | `GATEWAY_TIMEOUT_MS` | integer | 30000 | Request timeout in milliseconds | | `GATEWAY_MAX_BODY_SIZE` | string | 10MB | Maximum request body size | | `GATEWAY_CORS_ORIGINS` | string | * | Allowed CORS origins | | `GATEWAY_SSL_ENABLED` | boolean | true | Enable TLS termination | ## Configuration File Configuration can also be provided via `api gateway.yaml`: ```yaml gateway_port: 8000 gateway_rate_limit: 100 gateway_timeout_ms: 30000 gateway_max_body_size: 10MB gateway_cors_origins: * ``` ## Validation Configuration is validated at startup. Invalid configuration will prevent the service from starting.",
              "score": null
            }
          ],
          "latency_ms": 3.5609609985840507,
          "query_id": "complex_052",
          "dimension": "problem",
          "query": "deploying to prod but getting connection failures and ssl errors - what are all the required config settings I might be missing?"
        },
        {
          "key_facts": [
            "DATABASE_URL",
            "REDIS_URL",
            "GATEWAY_SSL_ENABLED",
            "DATABASE_SSL_MODE"
          ],
          "found_facts": [
            "DATABASE_URL",
            "REDIS_URL",
            "GATEWAY_SSL_ENABLED",
            "DATABASE_SSL_MODE"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "config_workflow_engine",
            "config_api_gateway",
            "config_database",
            "config_cache"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "config_database",
              "chunk_id": "config_database_fix_0",
              "content": "# Database Configuration Reference ## Overview This document describes all configuration options for Database. ## Environment Variables | Variable | Type | Default | Description | |----------|------|---------|-------------| | `DATABASE_URL` | string | required | PostgreSQL connection string | | `DATABASE_POOL_MIN` | integer | 5 | Minimum connection pool size | | `DATABASE_POOL_MAX` | integer | 20 | Maximum connection pool size | | `DATABASE_STATEMENT_TIMEOUT` | integer | 30000 | Query timeout in milliseconds | | `DATABASE_SSL_MODE` | string | require | SSL mode: disable, require, verify-full | | `DATABASE_MIGRATION_AUTO` | boolean | false | Run migrations on startup | ## Configuration File Configuration can also be provided via `database.yaml`: ```yaml database_url: required database_pool_min: 5 database_pool_max: 20 database_statement_timeout: 30000 database_ssl_mode: require ``` ## Validation Configuration is validated at startup. Invalid configuration will prevent the service from starting.",
              "score": null
            },
            {
              "doc_id": "config_api_gateway",
              "chunk_id": "config_api_gateway_fix_0",
              "content": "# API Gateway Configuration Reference ## Overview This document describes all configuration options for API Gateway. ## Environment Variables | Variable | Type | Default | Description | |----------|------|---------|-------------| | `GATEWAY_PORT` | integer | 8000 | Gateway listen port | | `GATEWAY_RATE_LIMIT` | integer | 100 | Requests per minute per user | | `GATEWAY_TIMEOUT_MS` | integer | 30000 | Request timeout in milliseconds | | `GATEWAY_MAX_BODY_SIZE` | string | 10MB | Maximum request body size | | `GATEWAY_CORS_ORIGINS` | string | * | Allowed CORS origins | | `GATEWAY_SSL_ENABLED` | boolean | true | Enable TLS termination | ## Configuration File Configuration can also be provided via `api gateway.yaml`: ```yaml gateway_port: 8000 gateway_rate_limit: 100 gateway_timeout_ms: 30000 gateway_max_body_size: 10MB gateway_cors_origins: * ``` ## Validation Configuration is validated at startup. Invalid configuration will prevent the service from starting.",
              "score": null
            },
            {
              "doc_id": "howto_set_up_database_connection",
              "chunk_id": "howto_set_up_database_connection_fix_0",
              "content": "# How to Set Up Database Connection ## Prerequisites - Database with network access to CloudFlow - Connection credentials ## Steps ### Step 1: Add connection secret Store your database connection string as a secret. ```bash cloudflow secrets set DATABASE_URL postgresql://user:pass@host:5432/db ``` ### Step 2: Configure connection pool Set pool size based on your plan limits. Free tier allows 5 connections, Pro allows 20. > **Note:** Pool exhaustion will queue requests ### Step 3: Test connection Verify the database is accessible from CloudFlow. ```bash cloudflow db test --secret DATABASE_URL ``` ### Step 4: Enable SSL For production, always require SSL connections. ```bash cloudflow db configure --require-ssl ``` ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            },
            {
              "doc_id": "config_cache",
              "chunk_id": "config_cache_fix_0",
              "content": "# Cache Configuration Reference ## Overview This document describes all configuration options for Cache. ## Environment Variables | Variable | Type | Default | Description | |----------|------|---------|-------------| | `REDIS_URL` | string | required | Redis connection string | | `REDIS_POOL_SIZE` | integer | 10 | Connection pool size | | `CACHE_TTL_SECONDS` | integer | 300 | Default cache TTL | | `CACHE_PREFIX` | string | cf: | Key prefix for namespacing | | `CACHE_COMPRESSION` | boolean | true | Enable value compression | | `CACHE_MAX_SIZE_MB` | integer | 100 | Maximum cache size per pod | ## Configuration File Configuration can also be provided via `cache.yaml`: ```yaml redis_url: required redis_pool_size: 10 cache_ttl_seconds: 300 cache_prefix: cf: cache_compression: true ``` ## Validation Configuration is validated at startup. Invalid configuration will prevent the service from starting.",
              "score": null
            },
            {
              "doc_id": "howto_configure_single_sign_on",
              "chunk_id": "howto_configure_single_sign_on_fix_0",
              "content": "# How to Configure Single Sign-On ## Prerequisites - Admin access - Enterprise plan - Identity provider access ## Steps ### Step 1: Choose provider CloudFlow supports SAML 2.0, OpenID Connect, and OAuth 2.0 providers. ### Step 2: Get metadata Download CloudFlow's SAML metadata for your IdP configuration. ```bash cloudflow sso metadata --format xml > cloudflow-metadata.xml ``` ### Step 3: Configure IdP Add CloudFlow as a service provider in your identity provider. > **Note:** ACS URL: https://auth.cloudflow.io/saml/callback ### Step 4: Upload IdP metadata Provide your IdP's metadata to CloudFlow. ```bash cloudflow sso configure --idp-metadata ./idp-metadata.xml ``` ### Step 5: Test login Verify SSO login works correctly. ```bash cloudflow sso test ``` ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            }
          ],
          "latency_ms": 3.898359998856904,
          "query_id": "complex_052",
          "dimension": "casual",
          "query": "prod config checklist - db redis gateway ssl required settings"
        },
        {
          "key_facts": [
            "DATABASE_URL",
            "REDIS_URL",
            "GATEWAY_SSL_ENABLED",
            "DATABASE_SSL_MODE"
          ],
          "found_facts": [
            "DATABASE_URL",
            "REDIS_URL",
            "DATABASE_SSL_MODE"
          ],
          "missed_facts": [
            "GATEWAY_SSL_ENABLED"
          ],
          "coverage": 0.75,
          "expected_docs": [
            "config_workflow_engine",
            "config_api_gateway",
            "config_database",
            "config_cache"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "config_database",
              "chunk_id": "config_database_fix_0",
              "content": "# Database Configuration Reference ## Overview This document describes all configuration options for Database. ## Environment Variables | Variable | Type | Default | Description | |----------|------|---------|-------------| | `DATABASE_URL` | string | required | PostgreSQL connection string | | `DATABASE_POOL_MIN` | integer | 5 | Minimum connection pool size | | `DATABASE_POOL_MAX` | integer | 20 | Maximum connection pool size | | `DATABASE_STATEMENT_TIMEOUT` | integer | 30000 | Query timeout in milliseconds | | `DATABASE_SSL_MODE` | string | require | SSL mode: disable, require, verify-full | | `DATABASE_MIGRATION_AUTO` | boolean | false | Run migrations on startup | ## Configuration File Configuration can also be provided via `database.yaml`: ```yaml database_url: required database_pool_min: 5 database_pool_max: 20 database_statement_timeout: 30000 database_ssl_mode: require ``` ## Validation Configuration is validated at startup. Invalid configuration will prevent the service from starting.",
              "score": null
            },
            {
              "doc_id": "adr_004",
              "chunk_id": "adr_004_fix_0",
              "content": "# ADR-004: Use Redis for Caching and Session Storage **Status:** Accepted **Date:** 2024-04-05 ## Context We need a fast data store for caching API responses, session data, and rate limiting counters. ## Options Considered ### Option 1: Redis Cluster In-memory data structure store. **Pros:** - Sub-millisecond latency - Rich data structures - Pub/sub support - Clustering **Cons:** - Memory cost - Persistence complexity - Cluster management ### Option 2: Memcached Simple key-value cache. **Pros:** - Simple operation - Multi-threaded - Lower memory overhead **Cons:** - No persistence - Limited data types - No clustering ### Option 3: Application-level cache In-process caching only. **Pros:** - No external dependency - Fastest possible - Simple **Cons:** - Not shared across pods - Memory pressure - Cold starts ## Decision We will use Redis Cluster for caching with a 6-node cluster for high availability. ## Consequences - Implement cache-aside pattern - Set appropriate TTLs - Monitor memory usage - Plan for cache stampede prevention",
              "score": null
            },
            {
              "doc_id": "config_cache",
              "chunk_id": "config_cache_fix_0",
              "content": "# Cache Configuration Reference ## Overview This document describes all configuration options for Cache. ## Environment Variables | Variable | Type | Default | Description | |----------|------|---------|-------------| | `REDIS_URL` | string | required | Redis connection string | | `REDIS_POOL_SIZE` | integer | 10 | Connection pool size | | `CACHE_TTL_SECONDS` | integer | 300 | Default cache TTL | | `CACHE_PREFIX` | string | cf: | Key prefix for namespacing | | `CACHE_COMPRESSION` | boolean | true | Enable value compression | | `CACHE_MAX_SIZE_MB` | integer | 100 | Maximum cache size per pod | ## Configuration File Configuration can also be provided via `cache.yaml`: ```yaml redis_url: required redis_pool_size: 10 cache_ttl_seconds: 300 cache_prefix: cf: cache_compression: true ``` ## Validation Configuration is validated at startup. Invalid configuration will prevent the service from starting.",
              "score": null
            },
            {
              "doc_id": "arch_caching_layer",
              "chunk_id": "arch_caching_layer_fix_0",
              "content": "# Caching Layer Architecture ## Overview The Caching Layer reduces latency and database load by caching frequently accessed data. Uses a multi-tier strategy with local and distributed caches. ## Design Principles - **Cache Invalidation**: Explicit invalidation on writes - **Graceful Degradation**: System works without cache ## Components ### Local Cache First-level cache with 100ms TTL. Reduces Redis round-trips for hot data. **Technology:** Ristretto in-memory cache **Scaling:** Per-pod memory allocation ### Distributed Cache Second-level cache with configurable TTL. Maximum key size 512MB. **Technology:** Redis Cluster 7.x **Scaling:** 6-node cluster with sharding ### CDN Caches static assets and API responses. Cache hit ratio target 95%. **Technology:** CloudFront with Lambda@Edge **Scaling:** Global edge locations ## Data Flow Request \u2192 Local Cache \u2192 Redis \u2192 Database \u2192 Cache Population ## Performance Characteristics - **Latency P50:** 1ms (hit), 50ms (miss) - **Latency P99:** 5ms (hit), 200ms (miss) - **Throughput:** 100000 reads/second",
              "score": null
            },
            {
              "doc_id": "adr_002",
              "chunk_id": "adr_002_fix_0",
              "content": "# ADR-002: Adopt Kubernetes for Container Orchestration **Status:** Accepted **Date:** 2024-01-24 ## Context We need a platform to deploy and scale our microservices. The platform must support auto-scaling, rolling deployments, and multi-region deployment. ## Options Considered ### Option 1: Kubernetes (EKS) Industry-standard container orchestration. **Pros:** - Industry standard - Rich ecosystem - Cloud agnostic - Auto-scaling **Cons:** - Operational complexity - Learning curve - Resource overhead ### Option 2: AWS ECS Amazon's native container service. **Pros:** - Simple operation - Native AWS integration - Lower overhead **Cons:** - AWS lock-in - Less flexible - Smaller community ### Option 3: Serverless (Lambda) Function-as-a-service approach. **Pros:** - Zero infrastructure - Pay per use - Auto-scaling **Cons:** - Cold starts - 15-minute limit - Vendor lock-in ## Decision We will use Kubernetes on EKS for container orchestration with Helm for deployments. ## Consequences - Need platform team expertise - Implement GitOps with ArgoCD - Use managed node groups for easier operations",
              "score": null
            }
          ],
          "latency_ms": 4.123328000787296,
          "query_id": "complex_052",
          "dimension": "contextual",
          "query": "setting up a new production cluster and need to configure database, cache, and gateway - what are the essential environment variables and security settings?"
        },
        {
          "key_facts": [
            "DATABASE_URL",
            "REDIS_URL",
            "GATEWAY_SSL_ENABLED",
            "DATABASE_SSL_MODE"
          ],
          "found_facts": [],
          "missed_facts": [
            "DATABASE_URL",
            "REDIS_URL",
            "GATEWAY_SSL_ENABLED",
            "DATABASE_SSL_MODE"
          ],
          "coverage": 0.0,
          "expected_docs": [
            "config_workflow_engine",
            "config_api_gateway",
            "config_database",
            "config_cache"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "release_2_9_0",
              "chunk_id": "release_2_9_0_fix_0",
              "content": "# Release Notes - v2.9.0 **Release Date:** 2023-08-01 ## New Features - Parallel step execution - Conditional branching in workflows - Slack integration improvements - Performance dashboard ## Bug Fixes - Fixed issue with special characters in secrets - Resolved timeout in large file uploads ## Upgrade Instructions ```bash cloudflow upgrade --version 2.9.0 ``` ## Known Issues None at this time.",
              "score": null
            },
            {
              "doc_id": "runbook_database_connection_exhaustion",
              "chunk_id": "runbook_database_connection_exhaustion_fix_0",
              "content": "# Runbook: Database Connection Exhaustion ## Severity **Level:** P1 - Critical **Response Time:** 15 minutes ## Symptoms - Connection pool at 100% - Queries timing out - 'too many connections' errors in logs ## Diagnosis ### Step 1 Check active connections: `SELECT count(*) FROM pg_stat_activity` ### Step 2 Identify long-running queries: `SELECT pid, now() - pg_stat_activity.query_start AS duration, query FROM pg_stat_activity WHERE state != 'idle' ORDER BY duration DESC` ### Step 3 Check PgBouncer stats: `SHOW POOLS` ### Step 4 Verify connection limits: `SHOW max_connections` ## Resolution ### Step 1 Kill long-running queries: `SELECT pg_terminate_backend(pid) FROM pg_stat_activity WHERE duration > interval '5 minutes'` ### Step 2 Increase PgBouncer pool size temporarily: update pgbouncer.ini ### Step 3 Restart affected services to release connections: `kubectl rollout restart deployment/api` ### Step 4 Scale down non-critical services to free connections ## Escalation If connections remain exhausted, escalate to DBA. Consider enabling connection queueing. ## Post-Incident - Document timeline in incident log - Schedule post-mortem within 48 hours - Update runbook if new patterns discovered",
              "score": null
            },
            {
              "doc_id": "release_3_0_0",
              "chunk_id": "release_3_0_0_fix_0",
              "content": "# Release Notes - v3.0.0 **Release Date:** 2023-10-15 ## New Features - Complete UI redesign - New workflow editor with visual builder - Real-time execution monitoring - Team workspaces ## Bug Fixes - Fixed critical security issue in token validation - Resolved data loss bug in failed migrations ## Breaking Changes - **BREAKING:** Authentication API completely rewritten - **BREAKING:** Legacy webhook format no longer supported - **BREAKING:** Minimum Node.js version now 18 ## Upgrade Instructions ```bash cloudflow upgrade --version 3.0.0 ``` ## Known Issues None at this time.",
              "score": null
            },
            {
              "doc_id": "runbook_api_gateway_5xx_errors",
              "chunk_id": "runbook_api_gateway_5xx_errors_fix_0",
              "content": "# Runbook: API Gateway 5xx Errors ## Severity **Level:** P3 - Medium **Response Time:** 1 hour ## Symptoms - Error rate > 1% - 5xx responses in access logs - Downstream service failures ## Diagnosis ### Step 1 Check Kong error logs: `kubectl logs -l app=kong -n kong --tail=100 | grep -E '5[0-9]{2}'` ### Step 2 Verify backend health: `kubectl get pods -n production -o wide` ### Step 3 Check service endpoints: `kubectl get endpoints -n production` ### Step 4 Test backend directly: `kubectl exec -it kong-xxx -n kong -- curl http://api.production.svc:8080/health` ## Resolution ### Step 1 Restart unhealthy backends: `kubectl rollout restart deployment/api -n production` ### Step 2 Enable circuit breaker if cascading: update Kong circuit-breaker plugin ### Step 3 Scale up backends: `kubectl scale deployment/api --replicas=5 -n production` ### Step 4 Check for resource exhaustion on backend pods ### Step 5 Verify network policies allow traffic ## Escalation If 5xx persists > 15 minutes, page on-call. May indicate infrastructure issue. ## Post-Incident - Document timeline in incident log - Schedule post-mortem within 48 hours - Update runbook if new patterns discovered",
              "score": null
            },
            {
              "doc_id": "howto_migrate_from_v1_to_v2_api",
              "chunk_id": "howto_migrate_from_v1_to_v2_api_fix_0",
              "content": "# How to Migrate from v1 to v2 API ## Prerequisites - Existing v1 integration - v2 API access enabled ## Steps ### Step 1: Review breaking changes The v2 API changes authentication from API keys to OAuth 2.0. Review the migration guide. ### Step 2: Update client library Upgrade to the latest SDK version. ```bash npm install @cloudflow/sdk@latest ``` ### Step 3: Update authentication Replace API key authentication with OAuth 2.0 client credentials flow. > **Note:** Generate new credentials in the dashboard ### Step 4: Test in staging Run your integration tests against the v2 staging endpoint. ```bash CLOUDFLOW_API_URL=https://api-v2-staging.cloudflow.io npm test ``` ### Step 5: Switch production Update production configuration to use v2 API. ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            }
          ],
          "latency_ms": 3.68987200090487,
          "query_id": "complex_052",
          "dimension": "negation",
          "query": "why does prod keep failing to connect? what production-only configs am I forgetting compared to dev?"
        },
        {
          "key_facts": [
            "Idempotency",
            "retry",
            "exponential backoff",
            "dead letter queues"
          ],
          "found_facts": [
            "retry",
            "exponential backoff"
          ],
          "missed_facts": [
            "Idempotency",
            "dead letter queues"
          ],
          "coverage": 0.5,
          "expected_docs": [
            "arch_workflow_engine",
            "adr_005",
            "howto_implement_retry_logic"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "howto_implement_retry_logic",
              "chunk_id": "howto_implement_retry_logic_fix_0",
              "content": "# How to Implement Retry Logic ## Prerequisites - Existing workflow ## Steps ### Step 1: Define retry policy Add retry configuration to your workflow step. > **Note:** Maximum 5 retry attempts allowed ### Step 2: Configure backoff Set exponential backoff with base delay of 1 second. The formula is: delay = base * 2^attempt. ### Step 3: Add timeout Set maximum execution time to prevent infinite loops. Default timeout is 30 seconds. ### Step 4: Handle final failure Configure on_failure handler to notify or escalate when all retries are exhausted. ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            },
            {
              "doc_id": "howto_debug_failed_execution",
              "chunk_id": "howto_debug_failed_execution_fix_0",
              "content": "# How to Debug Failed Execution ## Prerequisites - Failed execution ID ## Steps ### Step 1: Get execution details Retrieve the full execution record including step outputs. ```bash cloudflow executions get ex_123 --include-steps ``` ### Step 2: View step logs Check logs for the failed step. ```bash cloudflow executions logs ex_123 --step 3 ``` ### Step 3: Inspect input/output Review the data passed between steps. ```bash cloudflow executions inspect ex_123 --step 3 ``` ### Step 4: Enable debug mode Re-run with verbose logging enabled. ```bash cloudflow workflows execute wf_123 --debug ``` ### Step 5: Check error patterns Look for similar failures in recent executions. ```bash cloudflow executions list --status failed --since 24h ``` ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            },
            {
              "doc_id": "release_3_2_0",
              "chunk_id": "release_3_2_0_fix_0",
              "content": "# Release Notes - v3.2.0 **Release Date:** 2024-01-15 ## New Features - Workflow versioning with rollback support - New Python SDK with async support - Bulk operations API for workflows - Custom retry policies per step ## Bug Fixes - Fixed race condition in concurrent execution handler - Resolved memory leak in long-running workflows - Fixed timezone handling in scheduled triggers ## Breaking Changes - **BREAKING:** API v1 endpoints deprecated, will be removed in v4.0 - **BREAKING:** Minimum Python version now 3.9 ## Upgrade Instructions ```bash cloudflow upgrade --version 3.2.0 ``` ## Known Issues None at this time.",
              "score": null
            },
            {
              "doc_id": "release_3_0_0",
              "chunk_id": "release_3_0_0_fix_0",
              "content": "# Release Notes - v3.0.0 **Release Date:** 2023-10-15 ## New Features - Complete UI redesign - New workflow editor with visual builder - Real-time execution monitoring - Team workspaces ## Bug Fixes - Fixed critical security issue in token validation - Resolved data loss bug in failed migrations ## Breaking Changes - **BREAKING:** Authentication API completely rewritten - **BREAKING:** Legacy webhook format no longer supported - **BREAKING:** Minimum Node.js version now 18 ## Upgrade Instructions ```bash cloudflow upgrade --version 3.0.0 ``` ## Known Issues None at this time.",
              "score": null
            },
            {
              "doc_id": "release_3_1_0",
              "chunk_id": "release_3_1_0_fix_0",
              "content": "# Release Notes - v3.1.0 **Release Date:** 2023-12-01 ## New Features - GraphQL API beta release - Workflow templates marketplace - Enhanced audit logging - SSO support for Okta ## Bug Fixes - Fixed webhook delivery retry logic - Resolved issue with large payload handling - Fixed dashboard loading performance ## Upgrade Instructions ```bash cloudflow upgrade --version 3.1.0 ``` ## Known Issues None at this time.",
              "score": null
            }
          ],
          "latency_ms": 3.3351909987686668,
          "query_id": "complex_053",
          "dimension": "original",
          "query": "How does CloudFlow handle failures and recovery?"
        },
        {
          "key_facts": [
            "Idempotency",
            "retry",
            "exponential backoff",
            "dead letter queues"
          ],
          "found_facts": [
            "Idempotency",
            "retry",
            "exponential backoff"
          ],
          "missed_facts": [
            "dead letter queues"
          ],
          "coverage": 0.75,
          "expected_docs": [
            "arch_workflow_engine",
            "adr_005",
            "howto_implement_retry_logic"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "howto_implement_retry_logic",
              "chunk_id": "howto_implement_retry_logic_fix_0",
              "content": "# How to Implement Retry Logic ## Prerequisites - Existing workflow ## Steps ### Step 1: Define retry policy Add retry configuration to your workflow step. > **Note:** Maximum 5 retry attempts allowed ### Step 2: Configure backoff Set exponential backoff with base delay of 1 second. The formula is: delay = base * 2^attempt. ### Step 3: Add timeout Set maximum execution time to prevent infinite loops. Default timeout is 30 seconds. ### Step 4: Handle final failure Configure on_failure handler to notify or escalate when all retries are exhausted. ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            },
            {
              "doc_id": "arch_workflow_engine",
              "chunk_id": "arch_workflow_engine_fix_0",
              "content": "# Workflow Engine Architecture ## Overview The Workflow Engine is the core component responsible for executing user-defined workflows. It uses an event-driven architecture with support for parallel execution, conditional branching, and automatic retries. ## Design Principles - **Idempotency**: All operations can be safely retried without side effects - **Eventual Consistency**: State converges to consistent view within 5 seconds - **Fault Tolerance**: System continues operating despite component failures ## Components ### Scheduler Manages workflow scheduling and triggers. Supports cron expressions, webhooks, and event-based triggers. **Technology:** Custom Go service **Scaling:** Horizontal with leader election ### Executor Runs individual workflow steps in isolated containers. Maximum execution time is 30 minutes per step. **Technology:** Kubernetes Jobs **Scaling:** Auto-scaling based on queue depth ### State Store Persists workflow state, execution history, and step results. Uses optimistic locking for concurrent updates. **Technology:** PostgreSQL with JSONB **Scaling:** Primary-replica with read replicas ## Data Flow Triggers \u2192 Scheduler \u2192 Job Queue \u2192 Executor \u2192 State Store \u2192 Webhooks/Notifications ## Performance Characteristics - **Latency P50:** 45ms - **Latency P99:** 250ms - **Throughput:** 5000 executions/minute",
              "score": null
            },
            {
              "doc_id": "release_3_2_0",
              "chunk_id": "release_3_2_0_fix_0",
              "content": "# Release Notes - v3.2.0 **Release Date:** 2024-01-15 ## New Features - Workflow versioning with rollback support - New Python SDK with async support - Bulk operations API for workflows - Custom retry policies per step ## Bug Fixes - Fixed race condition in concurrent execution handler - Resolved memory leak in long-running workflows - Fixed timezone handling in scheduled triggers ## Breaking Changes - **BREAKING:** API v1 endpoints deprecated, will be removed in v4.0 - **BREAKING:** Minimum Python version now 3.9 ## Upgrade Instructions ```bash cloudflow upgrade --version 3.2.0 ``` ## Known Issues None at this time.",
              "score": null
            },
            {
              "doc_id": "api_workflows",
              "chunk_id": "api_workflows_fix_0",
              "content": "# Workflows API Reference ## Overview The Workflows API provides programmatic access to workflows functionality. Base URL: `https://api.cloudflow.io/v1/workflows` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /workflows List all workflows for the authenticated user. **Parameters:** - `limit` (integer): Maximum number of results (default: 50, max: 200) - `status` (string): Filter by status: active, paused, archived **Response:** - Success: `200` - Returns array of workflow objects - Error: `400` - Bad Request **Example:** ```json {\"workflows\": [{\"id\": \"wf_123\", \"name\": \"Daily Report\", \"status\": \"active\"}]} ``` ### POST /workflows Create a new workflow. **Parameters:** - `name` (string): Workflow name (required) - `definition` (object): Workflow definition in JSON format **Response:** - Success: `201` - Workflow created successfully - Error: `400` - Bad Request **Example:** ```json {\"id\": \"wf_456\", \"name\": \"New Workflow\", \"created_at\": \"2024-01-15T10:00:00Z\"} ``` ### DELETE /workflows/{id} Delete a workflow permanently. **Parameters:** - `id` (string): Workflow ID (required) **Response:** - Success: `204` - Workflow deleted - Error: `404` - Workflow not found **Example:** ```json {} ```",
              "score": null
            },
            {
              "doc_id": "api_executions",
              "chunk_id": "api_executions_fix_0",
              "content": "# Executions API Reference ## Overview The Executions API provides programmatic access to executions functionality. Base URL: `https://api.cloudflow.io/v1/executions` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /executions List workflow executions. **Parameters:** - `workflow_id` (string): Filter by workflow ID - `since` (datetime): Filter executions after this timestamp **Response:** - Success: `200` - Returns array of execution objects - Error: `400` - Bad Request **Example:** ```json {\"executions\": [{\"id\": \"ex_789\", \"status\": \"completed\", \"duration_ms\": 1523}]} ``` ### POST /executions/{id}/cancel Cancel a running execution. **Parameters:** - `id` (string): Execution ID - `reason` (string): Cancellation reason (optional) **Response:** - Success: `200` - Execution cancelled - Error: `400` - Bad Request **Example:** ```json {\"id\": \"ex_789\", \"status\": \"cancelled\", \"cancelled_at\": \"2024-01-15T10:05:00Z\"} ```",
              "score": null
            }
          ],
          "latency_ms": 3.9194789987959666,
          "query_id": "complex_053",
          "dimension": "synonym",
          "query": "what fault tolerance and error recovery mechanisms does cloudflow implement for workflow execution?"
        },
        {
          "key_facts": [
            "Idempotency",
            "retry",
            "exponential backoff",
            "dead letter queues"
          ],
          "found_facts": [
            "Idempotency",
            "retry",
            "exponential backoff",
            "dead letter queues"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "arch_workflow_engine",
            "adr_005",
            "howto_implement_retry_logic"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "howto_implement_retry_logic",
              "chunk_id": "howto_implement_retry_logic_fix_0",
              "content": "# How to Implement Retry Logic ## Prerequisites - Existing workflow ## Steps ### Step 1: Define retry policy Add retry configuration to your workflow step. > **Note:** Maximum 5 retry attempts allowed ### Step 2: Configure backoff Set exponential backoff with base delay of 1 second. The formula is: delay = base * 2^attempt. ### Step 3: Add timeout Set maximum execution time to prevent infinite loops. Default timeout is 30 seconds. ### Step 4: Handle final failure Configure on_failure handler to notify or escalate when all retries are exhausted. ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            },
            {
              "doc_id": "arch_workflow_engine",
              "chunk_id": "arch_workflow_engine_fix_0",
              "content": "# Workflow Engine Architecture ## Overview The Workflow Engine is the core component responsible for executing user-defined workflows. It uses an event-driven architecture with support for parallel execution, conditional branching, and automatic retries. ## Design Principles - **Idempotency**: All operations can be safely retried without side effects - **Eventual Consistency**: State converges to consistent view within 5 seconds - **Fault Tolerance**: System continues operating despite component failures ## Components ### Scheduler Manages workflow scheduling and triggers. Supports cron expressions, webhooks, and event-based triggers. **Technology:** Custom Go service **Scaling:** Horizontal with leader election ### Executor Runs individual workflow steps in isolated containers. Maximum execution time is 30 minutes per step. **Technology:** Kubernetes Jobs **Scaling:** Auto-scaling based on queue depth ### State Store Persists workflow state, execution history, and step results. Uses optimistic locking for concurrent updates. **Technology:** PostgreSQL with JSONB **Scaling:** Primary-replica with read replicas ## Data Flow Triggers \u2192 Scheduler \u2192 Job Queue \u2192 Executor \u2192 State Store \u2192 Webhooks/Notifications ## Performance Characteristics - **Latency P50:** 45ms - **Latency P99:** 250ms - **Throughput:** 5000 executions/minute",
              "score": null
            },
            {
              "doc_id": "adr_005",
              "chunk_id": "adr_005_fix_0",
              "content": "# ADR-005: Adopt Event-Driven Architecture for Workflow Execution **Status:** Accepted **Date:** 2024-12-04 ## Context We need an architecture that supports asynchronous workflow execution, step dependencies, and failure recovery. ## Options Considered ### Option 1: Event-driven with Kafka Publish-subscribe messaging. **Pros:** - Loose coupling - Scalability - Event replay - Audit trail **Cons:** - Eventual consistency - Operational complexity - Message ordering challenges ### Option 2: Request-response Synchronous API calls. **Pros:** - Simple mental model - Immediate feedback - Easy debugging **Cons:** - Tight coupling - Cascading failures - Timeout issues ### Option 3: Polling-based Workers poll for tasks. **Pros:** - Simple implementation - Easy rate limiting - No message broker **Cons:** - Polling overhead - Higher latency - Inefficient at scale ## Decision We will use an event-driven architecture with Kafka for workflow step execution. ## Consequences - Implement idempotent consumers - Use dead letter queues for failed messages - Monitor consumer lag",
              "score": null
            },
            {
              "doc_id": "release_3_2_0",
              "chunk_id": "release_3_2_0_fix_0",
              "content": "# Release Notes - v3.2.0 **Release Date:** 2024-01-15 ## New Features - Workflow versioning with rollback support - New Python SDK with async support - Bulk operations API for workflows - Custom retry policies per step ## Bug Fixes - Fixed race condition in concurrent execution handler - Resolved memory leak in long-running workflows - Fixed timezone handling in scheduled triggers ## Breaking Changes - **BREAKING:** API v1 endpoints deprecated, will be removed in v4.0 - **BREAKING:** Minimum Python version now 3.9 ## Upgrade Instructions ```bash cloudflow upgrade --version 3.2.0 ``` ## Known Issues None at this time.",
              "score": null
            },
            {
              "doc_id": "howto_debug_failed_execution",
              "chunk_id": "howto_debug_failed_execution_fix_0",
              "content": "# How to Debug Failed Execution ## Prerequisites - Failed execution ID ## Steps ### Step 1: Get execution details Retrieve the full execution record including step outputs. ```bash cloudflow executions get ex_123 --include-steps ``` ### Step 2: View step logs Check logs for the failed step. ```bash cloudflow executions logs ex_123 --step 3 ``` ### Step 3: Inspect input/output Review the data passed between steps. ```bash cloudflow executions inspect ex_123 --step 3 ``` ### Step 4: Enable debug mode Re-run with verbose logging enabled. ```bash cloudflow workflows execute wf_123 --debug ``` ### Step 5: Check error patterns Look for similar failures in recent executions. ```bash cloudflow executions list --status failed --since 24h ``` ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            }
          ],
          "latency_ms": 3.8805460008006776,
          "query_id": "complex_053",
          "dimension": "problem",
          "query": "our workflows keep failing and we're seeing duplicate executions - how does the retry and recovery system work and are there dead letter queues?"
        },
        {
          "key_facts": [
            "Idempotency",
            "retry",
            "exponential backoff",
            "dead letter queues"
          ],
          "found_facts": [
            "retry",
            "exponential backoff"
          ],
          "missed_facts": [
            "Idempotency",
            "dead letter queues"
          ],
          "coverage": 0.5,
          "expected_docs": [
            "arch_workflow_engine",
            "adr_005",
            "howto_implement_retry_logic"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "howto_implement_retry_logic",
              "chunk_id": "howto_implement_retry_logic_fix_0",
              "content": "# How to Implement Retry Logic ## Prerequisites - Existing workflow ## Steps ### Step 1: Define retry policy Add retry configuration to your workflow step. > **Note:** Maximum 5 retry attempts allowed ### Step 2: Configure backoff Set exponential backoff with base delay of 1 second. The formula is: delay = base * 2^attempt. ### Step 3: Add timeout Set maximum execution time to prevent infinite loops. Default timeout is 30 seconds. ### Step 4: Handle final failure Configure on_failure handler to notify or escalate when all retries are exhausted. ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            },
            {
              "doc_id": "release_3_2_0",
              "chunk_id": "release_3_2_0_fix_0",
              "content": "# Release Notes - v3.2.0 **Release Date:** 2024-01-15 ## New Features - Workflow versioning with rollback support - New Python SDK with async support - Bulk operations API for workflows - Custom retry policies per step ## Bug Fixes - Fixed race condition in concurrent execution handler - Resolved memory leak in long-running workflows - Fixed timezone handling in scheduled triggers ## Breaking Changes - **BREAKING:** API v1 endpoints deprecated, will be removed in v4.0 - **BREAKING:** Minimum Python version now 3.9 ## Upgrade Instructions ```bash cloudflow upgrade --version 3.2.0 ``` ## Known Issues None at this time.",
              "score": null
            },
            {
              "doc_id": "howto_debug_failed_execution",
              "chunk_id": "howto_debug_failed_execution_fix_0",
              "content": "# How to Debug Failed Execution ## Prerequisites - Failed execution ID ## Steps ### Step 1: Get execution details Retrieve the full execution record including step outputs. ```bash cloudflow executions get ex_123 --include-steps ``` ### Step 2: View step logs Check logs for the failed step. ```bash cloudflow executions logs ex_123 --step 3 ``` ### Step 3: Inspect input/output Review the data passed between steps. ```bash cloudflow executions inspect ex_123 --step 3 ``` ### Step 4: Enable debug mode Re-run with verbose logging enabled. ```bash cloudflow workflows execute wf_123 --debug ``` ### Step 5: Check error patterns Look for similar failures in recent executions. ```bash cloudflow executions list --status failed --since 24h ``` ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            },
            {
              "doc_id": "howto_configure_single_sign_on",
              "chunk_id": "howto_configure_single_sign_on_fix_0",
              "content": "# How to Configure Single Sign-On ## Prerequisites - Admin access - Enterprise plan - Identity provider access ## Steps ### Step 1: Choose provider CloudFlow supports SAML 2.0, OpenID Connect, and OAuth 2.0 providers. ### Step 2: Get metadata Download CloudFlow's SAML metadata for your IdP configuration. ```bash cloudflow sso metadata --format xml > cloudflow-metadata.xml ``` ### Step 3: Configure IdP Add CloudFlow as a service provider in your identity provider. > **Note:** ACS URL: https://auth.cloudflow.io/saml/callback ### Step 4: Upload IdP metadata Provide your IdP's metadata to CloudFlow. ```bash cloudflow sso configure --idp-metadata ./idp-metadata.xml ``` ### Step 5: Test login Verify SSO login works correctly. ```bash cloudflow sso test ``` ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            },
            {
              "doc_id": "howto_create_api_integration",
              "chunk_id": "howto_create_api_integration_fix_0",
              "content": "# How to Create API Integration ## Prerequisites - CloudFlow account ## Steps ### Step 1: Generate API key Create an API key for programmatic access. ```bash cloudflow apikeys create --name 'CI Integration' --scopes workflows:read,workflows:execute ``` ### Step 2: Store key securely Never commit API keys to source control. Use environment variables or secret management. > **Note:** Rotate keys every 90 days ### Step 3: Test authentication Verify the API key works correctly. ```bash curl -H 'Authorization: Bearer cf_...' https://api.cloudflow.io/v1/me ``` ### Step 4: Implement rate limiting Handle rate limit responses (HTTP 429) with exponential backoff. > **Note:** Default limit is 100 requests per minute ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            }
          ],
          "latency_ms": 3.863816000375664,
          "query_id": "complex_053",
          "dimension": "casual",
          "query": "cloudflow failure handling - retries backoff dlq idempotency"
        },
        {
          "key_facts": [
            "Idempotency",
            "retry",
            "exponential backoff",
            "dead letter queues"
          ],
          "found_facts": [
            "Idempotency",
            "retry",
            "exponential backoff",
            "dead letter queues"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "arch_workflow_engine",
            "adr_005",
            "howto_implement_retry_logic"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "howto_implement_retry_logic",
              "chunk_id": "howto_implement_retry_logic_fix_0",
              "content": "# How to Implement Retry Logic ## Prerequisites - Existing workflow ## Steps ### Step 1: Define retry policy Add retry configuration to your workflow step. > **Note:** Maximum 5 retry attempts allowed ### Step 2: Configure backoff Set exponential backoff with base delay of 1 second. The formula is: delay = base * 2^attempt. ### Step 3: Add timeout Set maximum execution time to prevent infinite loops. Default timeout is 30 seconds. ### Step 4: Handle final failure Configure on_failure handler to notify or escalate when all retries are exhausted. ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            },
            {
              "doc_id": "arch_workflow_engine",
              "chunk_id": "arch_workflow_engine_fix_0",
              "content": "# Workflow Engine Architecture ## Overview The Workflow Engine is the core component responsible for executing user-defined workflows. It uses an event-driven architecture with support for parallel execution, conditional branching, and automatic retries. ## Design Principles - **Idempotency**: All operations can be safely retried without side effects - **Eventual Consistency**: State converges to consistent view within 5 seconds - **Fault Tolerance**: System continues operating despite component failures ## Components ### Scheduler Manages workflow scheduling and triggers. Supports cron expressions, webhooks, and event-based triggers. **Technology:** Custom Go service **Scaling:** Horizontal with leader election ### Executor Runs individual workflow steps in isolated containers. Maximum execution time is 30 minutes per step. **Technology:** Kubernetes Jobs **Scaling:** Auto-scaling based on queue depth ### State Store Persists workflow state, execution history, and step results. Uses optimistic locking for concurrent updates. **Technology:** PostgreSQL with JSONB **Scaling:** Primary-replica with read replicas ## Data Flow Triggers \u2192 Scheduler \u2192 Job Queue \u2192 Executor \u2192 State Store \u2192 Webhooks/Notifications ## Performance Characteristics - **Latency P50:** 45ms - **Latency P99:** 250ms - **Throughput:** 5000 executions/minute",
              "score": null
            },
            {
              "doc_id": "adr_005",
              "chunk_id": "adr_005_fix_0",
              "content": "# ADR-005: Adopt Event-Driven Architecture for Workflow Execution **Status:** Accepted **Date:** 2024-12-04 ## Context We need an architecture that supports asynchronous workflow execution, step dependencies, and failure recovery. ## Options Considered ### Option 1: Event-driven with Kafka Publish-subscribe messaging. **Pros:** - Loose coupling - Scalability - Event replay - Audit trail **Cons:** - Eventual consistency - Operational complexity - Message ordering challenges ### Option 2: Request-response Synchronous API calls. **Pros:** - Simple mental model - Immediate feedback - Easy debugging **Cons:** - Tight coupling - Cascading failures - Timeout issues ### Option 3: Polling-based Workers poll for tasks. **Pros:** - Simple implementation - Easy rate limiting - No message broker **Cons:** - Polling overhead - Higher latency - Inefficient at scale ## Decision We will use an event-driven architecture with Kafka for workflow step execution. ## Consequences - Implement idempotent consumers - Use dead letter queues for failed messages - Monitor consumer lag",
              "score": null
            },
            {
              "doc_id": "release_3_2_0",
              "chunk_id": "release_3_2_0_fix_0",
              "content": "# Release Notes - v3.2.0 **Release Date:** 2024-01-15 ## New Features - Workflow versioning with rollback support - New Python SDK with async support - Bulk operations API for workflows - Custom retry policies per step ## Bug Fixes - Fixed race condition in concurrent execution handler - Resolved memory leak in long-running workflows - Fixed timezone handling in scheduled triggers ## Breaking Changes - **BREAKING:** API v1 endpoints deprecated, will be removed in v4.0 - **BREAKING:** Minimum Python version now 3.9 ## Upgrade Instructions ```bash cloudflow upgrade --version 3.2.0 ``` ## Known Issues None at this time.",
              "score": null
            },
            {
              "doc_id": "howto_debug_failed_execution",
              "chunk_id": "howto_debug_failed_execution_fix_0",
              "content": "# How to Debug Failed Execution ## Prerequisites - Failed execution ID ## Steps ### Step 1: Get execution details Retrieve the full execution record including step outputs. ```bash cloudflow executions get ex_123 --include-steps ``` ### Step 2: View step logs Check logs for the failed step. ```bash cloudflow executions logs ex_123 --step 3 ``` ### Step 3: Inspect input/output Review the data passed between steps. ```bash cloudflow executions inspect ex_123 --step 3 ``` ### Step 4: Enable debug mode Re-run with verbose logging enabled. ```bash cloudflow workflows execute wf_123 --debug ``` ### Step 5: Check error patterns Look for similar failures in recent executions. ```bash cloudflow executions list --status failed --since 24h ``` ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            }
          ],
          "latency_ms": 3.7626769990311004,
          "query_id": "complex_053",
          "dimension": "contextual",
          "query": "designing a critical financial workflow that must handle failures gracefully - how do retries, idempotency, and dead letter queues work together in cloudflow?"
        },
        {
          "key_facts": [
            "Idempotency",
            "retry",
            "exponential backoff",
            "dead letter queues"
          ],
          "found_facts": [
            "Idempotency",
            "retry",
            "exponential backoff"
          ],
          "missed_facts": [
            "dead letter queues"
          ],
          "coverage": 0.75,
          "expected_docs": [
            "arch_workflow_engine",
            "adr_005",
            "howto_implement_retry_logic"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "howto_implement_retry_logic",
              "chunk_id": "howto_implement_retry_logic_fix_0",
              "content": "# How to Implement Retry Logic ## Prerequisites - Existing workflow ## Steps ### Step 1: Define retry policy Add retry configuration to your workflow step. > **Note:** Maximum 5 retry attempts allowed ### Step 2: Configure backoff Set exponential backoff with base delay of 1 second. The formula is: delay = base * 2^attempt. ### Step 3: Add timeout Set maximum execution time to prevent infinite loops. Default timeout is 30 seconds. ### Step 4: Handle final failure Configure on_failure handler to notify or escalate when all retries are exhausted. ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            },
            {
              "doc_id": "arch_workflow_engine",
              "chunk_id": "arch_workflow_engine_fix_0",
              "content": "# Workflow Engine Architecture ## Overview The Workflow Engine is the core component responsible for executing user-defined workflows. It uses an event-driven architecture with support for parallel execution, conditional branching, and automatic retries. ## Design Principles - **Idempotency**: All operations can be safely retried without side effects - **Eventual Consistency**: State converges to consistent view within 5 seconds - **Fault Tolerance**: System continues operating despite component failures ## Components ### Scheduler Manages workflow scheduling and triggers. Supports cron expressions, webhooks, and event-based triggers. **Technology:** Custom Go service **Scaling:** Horizontal with leader election ### Executor Runs individual workflow steps in isolated containers. Maximum execution time is 30 minutes per step. **Technology:** Kubernetes Jobs **Scaling:** Auto-scaling based on queue depth ### State Store Persists workflow state, execution history, and step results. Uses optimistic locking for concurrent updates. **Technology:** PostgreSQL with JSONB **Scaling:** Primary-replica with read replicas ## Data Flow Triggers \u2192 Scheduler \u2192 Job Queue \u2192 Executor \u2192 State Store \u2192 Webhooks/Notifications ## Performance Characteristics - **Latency P50:** 45ms - **Latency P99:** 250ms - **Throughput:** 5000 executions/minute",
              "score": null
            },
            {
              "doc_id": "release_3_2_0",
              "chunk_id": "release_3_2_0_fix_0",
              "content": "# Release Notes - v3.2.0 **Release Date:** 2024-01-15 ## New Features - Workflow versioning with rollback support - New Python SDK with async support - Bulk operations API for workflows - Custom retry policies per step ## Bug Fixes - Fixed race condition in concurrent execution handler - Resolved memory leak in long-running workflows - Fixed timezone handling in scheduled triggers ## Breaking Changes - **BREAKING:** API v1 endpoints deprecated, will be removed in v4.0 - **BREAKING:** Minimum Python version now 3.9 ## Upgrade Instructions ```bash cloudflow upgrade --version 3.2.0 ``` ## Known Issues None at this time.",
              "score": null
            },
            {
              "doc_id": "howto_debug_failed_execution",
              "chunk_id": "howto_debug_failed_execution_fix_0",
              "content": "# How to Debug Failed Execution ## Prerequisites - Failed execution ID ## Steps ### Step 1: Get execution details Retrieve the full execution record including step outputs. ```bash cloudflow executions get ex_123 --include-steps ``` ### Step 2: View step logs Check logs for the failed step. ```bash cloudflow executions logs ex_123 --step 3 ``` ### Step 3: Inspect input/output Review the data passed between steps. ```bash cloudflow executions inspect ex_123 --step 3 ``` ### Step 4: Enable debug mode Re-run with verbose logging enabled. ```bash cloudflow workflows execute wf_123 --debug ``` ### Step 5: Check error patterns Look for similar failures in recent executions. ```bash cloudflow executions list --status failed --since 24h ``` ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            },
            {
              "doc_id": "api_workflows",
              "chunk_id": "api_workflows_fix_0",
              "content": "# Workflows API Reference ## Overview The Workflows API provides programmatic access to workflows functionality. Base URL: `https://api.cloudflow.io/v1/workflows` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /workflows List all workflows for the authenticated user. **Parameters:** - `limit` (integer): Maximum number of results (default: 50, max: 200) - `status` (string): Filter by status: active, paused, archived **Response:** - Success: `200` - Returns array of workflow objects - Error: `400` - Bad Request **Example:** ```json {\"workflows\": [{\"id\": \"wf_123\", \"name\": \"Daily Report\", \"status\": \"active\"}]} ``` ### POST /workflows Create a new workflow. **Parameters:** - `name` (string): Workflow name (required) - `definition` (object): Workflow definition in JSON format **Response:** - Success: `201` - Workflow created successfully - Error: `400` - Bad Request **Example:** ```json {\"id\": \"wf_456\", \"name\": \"New Workflow\", \"created_at\": \"2024-01-15T10:00:00Z\"} ``` ### DELETE /workflows/{id} Delete a workflow permanently. **Parameters:** - `id` (string): Workflow ID (required) **Response:** - Success: `204` - Workflow deleted - Error: `404` - Workflow not found **Example:** ```json {} ```",
              "score": null
            }
          ],
          "latency_ms": 3.8489369999297196,
          "query_id": "complex_053",
          "dimension": "negation",
          "query": "why do failed workflows sometimes process twice? is there proper idempotency and what happens when all retries are exhausted?"
        }
      ]
    }
  ]
}