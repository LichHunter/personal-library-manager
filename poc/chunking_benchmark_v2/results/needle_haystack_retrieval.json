{
  "benchmark_run_at": "2026-01-26T20:08:44.941943",
  "needle_doc_id": "tasks_administer-cluster_topology-manager",
  "total_documents": 200,
  "total_chunks": 1030,
  "needle_chunks": 14,
  "index_time_s": 59.7,
  "index_stats": {
    "num_chunks": 1030,
    "embedding_dim": 768,
    "bm25_avg_doc_len": 208.23203883495145,
    "rrf_k": 60,
    "enrichment_time_s": 42.94803023338318,
    "rewrite_timeout_s": 5.0,
    "enricher_stats": {
      "total_processed": 896,
      "total_time_s": 42.72,
      "avg_time_ms": 47.68
    }
  },
  "summary": {
    "total_questions": 20,
    "needle_found_count": 19,
    "needle_found_rate": 95.0,
    "avg_latency_ms": 1022.3
  },
  "results": [
    {
      "question_id": "q_001",
      "question": "My pod keeps getting rejected with a topology affinity error, what's going on?",
      "expected_answer": "This happens with restricted or single-numa-node policies when the Topology Manager cannot find a suitable NUMA node affinity. The pod enters a Terminated state. Use a Deployment or ReplicaSet to trigger redeployment.",
      "difficulty": "medium",
      "section": "restricted policy",
      "latency_ms": 1065.3,
      "needle_found": true,
      "retrieved_chunks": [
        {
          "chunk_id": "tasks_administer-cluster_topology-manager_mdsem_7",
          "doc_id": "tasks_administer-cluster_topology-manager",
          "content": "### `restricted` policy {#policy-restricted}\n\nFor each container in a Pod, the kubelet, with `restricted` topology management policy, calls each\nHint Provider to discover their resource availability. Using this information, the Topology\nManager stores the preferred NUMA Node affinity for that container. If the affinity is not\npreferred, the Topology Manager will reject this pod from the node. This will result in a pod entering a\n`Terminated` state with a pod admission failure.\n\nOnce the pod is in a `Terminated` state, the Kubernetes scheduler will **not** attempt to\nreschedule the pod. It is recommended to use a ReplicaSet or Deployment to trigger a redeployment of\nthe pod. An external control loop could be also implemented to trigger a redeployment of pods that\nhave the `Topology Affinity` error.\n\nIf the pod is admitted, the *Hint Providers* can then use this information when making the\nresource allocation decision.",
          "is_needle": true
        },
        {
          "chunk_id": "contribute_style_diagram-guide_mdsem_9",
          "doc_id": "contribute_style_diagram-guide",
          "content": "### Example 1 - Pod topology spread constraints\n\nFigure 6 shows the diagram appearing in the\n[Pod topology spread constraints](/docs/concepts/scheduling-eviction/topology-spread-constraints/#node-labels)\npage.\n\n{{< mermaid >}}\n    graph TB\n    subgraph \"zoneB\"\n    n3(Node3)\n    n4(Node4)\n    end\n    subgraph \"zoneA\"\n    n1(Node1)\n    n2(Node2)\n    end\n    \n    classDef plain fill:#ddd,stroke:#fff,stroke-width:4px,color:#000;\n    classDef k8s fill:#326ce5,stroke:#fff,stroke-width:4px,color:#fff;\n    classDef cluster fill:#fff,stroke:#bbb,stroke-width:2px,color:#326ce5;\n    class n1,n2,n3,n4 k8s;\n    class zoneA,zoneB cluster;\n\nclick n3 \"https://mermaid-js.github.io/mermaid-live-editor/edit/#eyJjb2RlIjoiZ3JhcGggVEJcbiAgICBzdWJncmFwaCBcInpvbmVCXCJcbiAgICAgICAgbjMoTm9kZTMpXG4gICAgICAgIG40KE5vZGU0KVxuICAgIGVuZFxuICAgIHN1YmdyYXBoIFwiem9uZUFcIlxuICAgICAgICBuMShOb2RlMSlcbiAgICAgICAgbjIoTm9kZTIpXG4gICAgZW5kXG5cbiAgICBjbGFzc0RlZiBwbGFpbiBmaWxsOiNkZGQsc3Ryb2tlOiNmZmYsc3Ryb2tlLXdpZHRoOjRweCxjb2xvcjojMDAwO1xuICAgIGNsYXNzRGVmIGs4cyBmaWxsOiMzMjZjZTUsc3Ryb2tlOiNmZmYsc3Ryb2tlLXdpZHRoOjRweCxjb2xvcjojZmZmO1xuICAgIGNsYXNzRGVmIGNsdXN0ZXIgZmlsbDojZmZmLHN0cm9rZTojYmJiLHN0cm9rZS13aWR0aDoycHgsY29sb3I6IzMyNmNlNTtcbiAgICBjbGFzcyBuMSxuMixuMyxuNCBrOHM7XG4gICAgY2xhc3Mgem9uZUEsem9uZUIgY2x1c3RlcjtcbiIsIm1lcm1haWQiOiJ7XG4gIFwidGhlbWVcIjogXCJkZWZhdWx0XCJcbn0iLCJ1cGRhdGVFZGl0b3IiOmZhbHNlLCJhdXRvU3luYyI6dHJ1ZSwidXBkYXRlRGlhZ3JhbSI6dHJ1ZX0\" _blank\n\nclick n4 \"https://mermaid-js.github.io/mermaid-live-editor/edit/#eyJjb2RlIjoiZ3JhcGggVEJcbiAgICBzdWJncmFwaCBcInpvbmVCXCJcbiAgICAgICAgbjMoTm9kZTMpXG4gICAgICAgIG40KE5vZGU0KVxuICAgIGVuZFxuICAgIHN1YmdyYXBoIFwiem9uZUFcIlxuICAgICAgICBuMShOb2RlMSlcbiAgICAgICAgbjIoTm9kZTIpXG4gICAgZW5kXG5cbiAgICBjbGFzc0RlZiBwbGFpbiBmaWxsOiNkZGQsc3Ryb2tlOiNmZmYsc3Ryb2tlLXdpZHRoOjRweCxjb2xvcjojMDAwO1xuICAgIGNsYXNzRGVmIGs4cyBmaWxsOiMzMjZjZTUsc3Ryb2tlOiNmZmYsc3Ryb2tlLXdpZHRoOjRweCxjb2xvcjojZmZmO1xuICAgIGNsYXNzRGVmIGNsdXN0ZXIgZmlsbDojZmZmLHN0cm9rZTojYmJiLHN0cm9rZS13aWR0aDoycHgsY29sb3I6IzMyNmNlNTtcbiAgICBjbGFzcyBuMSxuMixuMyxuNCBrOHM7XG4gICAgY2xhc3Mgem9uZUEsem9uZUIgY2x1c3RlcjtcbiIsIm1lcm1haWQiOiJ7XG4gIFwidGhlbWVcIjogXCJkZWZhdWx0XCJcbn0iLCJ1cGRhdGVFZGl0b3IiOmZhbHNlLCJhdXRvU3luYyI6dHJ1ZSwidXBkYXRlRGlhZ3JhbSI6dHJ1ZX0\" _blank\n\nclick n1 \"https://mermaid-js.github.io/mermaid-live-editor/edit/#eyJjb2RlIjoiZ3JhcGggVEJcbiAgICBzdWJncmFwaCBcInpvbmVCXCJcbiAgICAgICAgbjMoTm9kZTMpXG4gICAgICAgIG40KE5vZGU0KVxuICAgIGVuZFxuICAgIHN1YmdyYXBoIFwiem9uZUFcIlxuICAgICAgICBuMShOb2RlMSlcbiAgICAgICAgbjIoTm9kZTIpXG4gICAgZW5kXG5cbiAgICBjbGFzc0RlZiBwbGFpbiBmaWxsOiNkZGQsc3Ryb2tlOiNmZmYsc3Ryb2tlLXdpZHRoOjRweCxjb2xvcjojMDAwO1xuICAgIGNsYXNzRGVmIGs4cyBmaWxsOiMzMjZjZTUsc3Ryb2tlOiNmZmYsc3Ryb2tlLXdpZHRoOjRweCxjb2xvcjojZmZmO1xuICAgIGNsYXNzRGVmIGNsdXN0ZXIgZmlsbDojZmZmLHN0cm9rZTojYmJiLHN0cm9rZS13aWR0aDoycHgsY29sb3I6IzMyNmNlNTtcbiAgICBjbGFzcyBuMSxuMixuMyxuNCBrOHM7XG4gICAgY2xhc3Mgem9uZUEsem9uZUIgY2x1c3RlcjtcbiIsIm1lcm1haWQiOiJ7XG4gIFwidGhlbWVcIjogXCJkZWZhdWx0XCJcbn0iLCJ1cGRhdGVFZGl0b3IiOmZhbHNlLCJhdXRvU3luYyI6dHJ1ZSwidXBkYXRlRGlhZ3JhbSI6dHJ1ZX0\" _blank\n\nclick n2 \"https://mermaid-js.github.io/mermaid-live-editor/edit/#eyJjb2RlIjoiZ3JhcGggVEJcbiAgICBzdWJncmFwaCBcInpvbmVCXCJcbiAgICAgICAgbjMoTm9kZTMpXG4gICAgICAgIG40KE5vZGU0KVxuICAgIGVuZFxuICAgIHN1YmdyYXBoIFwiem9uZUFcIlxuICAgICAgICBuMShOb2RlMSlcbiAgICAgICAgbjIoTm9kZTIpXG4gICAgZW5kXG5cbiAgICBjbGFzc0RlZiBwbGFpbiBmaWxsOiNkZGQsc3Ryb2tlOiNmZmYsc3Ryb2tlLXdpZHRoOjRweCxjb2xvcjojMDAwO1xuICAgIGNsYXNzRGVmIGs4cyBmaWxsOiMzMjZjZTUsc3Ryb2tlOiNmZmYsc3Ryb2tlLXdpZHRoOjRweCxjb2xvcjojZmZmO1xuICAgIGNsYXNzRGVmIGNsdXN0ZXIgZmlsbDojZmZmLHN0cm9rZTojYmJiLHN0cm9rZS13aWR0aDoycHgsY29sb3I6IzMyNmNlNTtcbiAgICBjbGFzcyBuMSxuMixuMyxuNCBrOHM7XG4gICAgY2xhc3Mgem9uZUEsem9uZUIgY2x1c3RlcjtcbiIsIm1lcm1haWQiOiJ7XG4gIFwidGhlbWVcIjogXCJkZWZhdWx0XCJcbn0iLCJ1cGRhdGVFZGl0b3IiOmZhbHNlLCJhdXRvU3luYyI6dHJ1ZSwidXBkYXRlRGlhZ3JhbSI6dHJ1ZX0\" _blank\n\n{{< /mermaid >}}\n\nFigure 6. Pod Topology Spread Constraints.\n\nCode block:\n\n```text\ngraph TB\n   subgraph \"zoneB\"\n       n3(Node3)\n       n4(Node4)\n   end\n   subgraph \"zoneA\"\n       n1(Node1)\n       n2(Node2)\n   end\n \n   classDef plain fill:#ddd,stroke:#fff,stroke-width:4px,color:#000;\n   classDef k8s fill:#326ce5,stroke:#fff,stroke-width:4px,color:#fff;\n   classDef cluster fill:#fff,stroke:#bbb,stroke-width:2px,color:#326ce5;\n   class n1,n2,n3,n4 k8s;\n   class zoneA,zoneB cluster;\n```",
          "is_needle": false
        },
        {
          "chunk_id": "tasks_debug_debug-cluster_topology_mdsem_2",
          "doc_id": "tasks_debug_debug-cluster_topology",
          "content": "## Troubleshoot `TopologyAffinityError` {#TopologyAffinityError}\n\nThis error typically occurs in the following situations:\n\n* a node has not enough resources available to satisfy the pod's request\n* the pod's request is rejected due to particular Topology Manager policy constraints\n\nThe error appears in the status of a pod:\n\n```shell\nkubectl get pods\n```\n\n```none\nNAME         READY   STATUS                  RESTARTS   AGE\nguaranteed   0/1     TopologyAffinityError   0          113s\n```\n\nUse `kubectl describe pod <id>` or `kubectl events` to obtain a detailed error message:\n\n```none\nWarning  TopologyAffinityError  10m   kubelet, dell8  Resources cannot be allocated with Topology locality\n```",
          "is_needle": false
        },
        {
          "chunk_id": "tasks_administer-cluster_topology-manager_mdsem_8",
          "doc_id": "tasks_administer-cluster_topology-manager",
          "content": "### `single-numa-node` policy {#policy-single-numa-node}\n\nFor each container in a Pod, the kubelet, with `single-numa-node` topology management policy,\ncalls each Hint Provider to discover their resource availability. Using this information, the\nTopology Manager determines if a single NUMA Node affinity is possible. If it is, Topology\nManager will store this and the *Hint Providers* can then use this information when making the\nresource allocation decision. If, however, this is not possible then the Topology Manager will\nreject the pod from the node. This will result in a pod in a `Terminated` state with a pod\nadmission failure.\n\nOnce the pod is in a `Terminated` state, the Kubernetes scheduler will **not** attempt to\nreschedule the pod. It is recommended to use a Deployment with replicas to trigger a redeployment of\nthe Pod. An external control loop could be also implemented to trigger a redeployment of pods\nthat have the `Topology Affinity` error.",
          "is_needle": true
        },
        {
          "chunk_id": "tasks_debug_debug-cluster_topology_mdsem_1",
          "doc_id": "tasks_debug_debug-cluster_topology",
          "content": "## Sources of troubleshooting information\n\nYou can use the following means to troubleshoot the reason why a pod could not be deployed or\nbecame rejected at a node, in the context of topology management:\n\n- _Pod status_ - indicates topology affinity errors\n- _system logs_ - include valuable information for debugging; for example, about generated hints\n- _kubelet state file_ - the dump of internal state of the Memory Manager\n  (including the _node map_ and _memory maps_)\n- You can use the [device plugin resource API](#device-plugin-resource-api)\n  to retrieve information about the memory reserved for containers",
          "is_needle": false
        }
      ]
    },
    {
      "question_id": "q_002",
      "question": "How do I force all my containers to run on the same NUMA node in k8s?",
      "expected_answer": "Use the pod scope with single-numa-node policy. Set topologyManagerScope to pod in kubelet config and use --topology-manager-policy=single-numa-node",
      "difficulty": "medium",
      "section": "pod scope",
      "latency_ms": 1016.8,
      "needle_found": true,
      "retrieved_chunks": [
        {
          "chunk_id": "tasks_administer-cluster_topology-manager_mdsem_7",
          "doc_id": "tasks_administer-cluster_topology-manager",
          "content": "### `restricted` policy {#policy-restricted}\n\nFor each container in a Pod, the kubelet, with `restricted` topology management policy, calls each\nHint Provider to discover their resource availability. Using this information, the Topology\nManager stores the preferred NUMA Node affinity for that container. If the affinity is not\npreferred, the Topology Manager will reject this pod from the node. This will result in a pod entering a\n`Terminated` state with a pod admission failure.\n\nOnce the pod is in a `Terminated` state, the Kubernetes scheduler will **not** attempt to\nreschedule the pod. It is recommended to use a ReplicaSet or Deployment to trigger a redeployment of\nthe pod. An external control loop could be also implemented to trigger a redeployment of pods that\nhave the `Topology Affinity` error.\n\nIf the pod is admitted, the *Hint Providers* can then use this information when making the\nresource allocation decision.",
          "is_needle": true
        },
        {
          "chunk_id": "tasks_administer-cluster_topology-manager_mdsem_8",
          "doc_id": "tasks_administer-cluster_topology-manager",
          "content": "### `single-numa-node` policy {#policy-single-numa-node}\n\nFor each container in a Pod, the kubelet, with `single-numa-node` topology management policy,\ncalls each Hint Provider to discover their resource availability. Using this information, the\nTopology Manager determines if a single NUMA Node affinity is possible. If it is, Topology\nManager will store this and the *Hint Providers* can then use this information when making the\nresource allocation decision. If, however, this is not possible then the Topology Manager will\nreject the pod from the node. This will result in a pod in a `Terminated` state with a pod\nadmission failure.\n\nOnce the pod is in a `Terminated` state, the Kubernetes scheduler will **not** attempt to\nreschedule the pod. It is recommended to use a Deployment with replicas to trigger a redeployment of\nthe Pod. An external control loop could be also implemented to trigger a redeployment of pods\nthat have the `Topology Affinity` error.",
          "is_needle": true
        },
        {
          "chunk_id": "concepts_scheduling-eviction_kube-scheduler_mdsem_1",
          "doc_id": "concepts_scheduling-eviction_kube-scheduler",
          "content": "## kube-scheduler\n\n[kube-scheduler](/docs/reference/command-line-tools-reference/kube-scheduler/)\nis the default scheduler for Kubernetes and runs as part of the\n{{< glossary_tooltip text=\"control plane\" term_id=\"control-plane\" >}}.\nkube-scheduler is designed so that, if you want and need to, you can\nwrite your own scheduling component and use that instead.\n\nKube-scheduler selects an optimal node to run newly created or not yet\nscheduled (unscheduled) pods. Since containers in pods - and pods themselves -\ncan have different requirements, the scheduler filters out any nodes that\ndon't meet a Pod's specific scheduling needs. Alternatively, the API lets\nyou specify a node for a Pod when you create it, but this is unusual\nand is only done in special cases.\n\nIn a cluster, Nodes that meet the scheduling requirements for a Pod\nare called _feasible_ nodes. If none of the nodes are suitable, the pod\nremains unscheduled until the scheduler is able to place it.\n\nThe scheduler finds feasible Nodes for a Pod and then runs a set of\nfunctions to score the feasible Nodes and picks a Node with the highest\nscore among the feasible ones to run the Pod. The scheduler then notifies\nthe API server about this decision in a process called _binding_.\n\nFactors that need to be taken into account for scheduling decisions include\nindividual and collective resource requirements, hardware / software /\npolicy constraints, affinity and anti-affinity specifications, data\nlocality, inter-workload interference, and so on.",
          "is_needle": false
        },
        {
          "chunk_id": "tasks_administer-cluster_topology-manager_mdsem_4",
          "doc_id": "tasks_administer-cluster_topology-manager",
          "content": "### `pod` scope\n\nTo select the `pod` scope, set `topologyManagerScope` in the\n[kubelet configuration file](/docs/tasks/administer-cluster/kubelet-config-file/) to `pod`.\n\nThis scope allows for grouping all containers in a pod to a common set of NUMA nodes. That is, the\nTopology Manager treats a pod as a whole and attempts to allocate the entire pod (all containers)\nto either a single NUMA node or a common set of NUMA nodes. The following examples illustrate the\nalignments produced by the Topology Manager on different occasions:\n\n* all containers can be and are allocated to a single NUMA node;\n* all containers can be and are allocated to a shared set of NUMA nodes.\n\nThe total amount of particular resource demanded for the entire pod is calculated according to\n[effective requests/limits](/docs/concepts/workloads/pods/init-containers/#resource-sharing-within-containers)\nformula, and thus, this total value is equal to the maximum of:\n\n* the sum of all app container requests,\n* the maximum of init container requests,\n\nfor a resource.\n\nUsing the `pod` scope in tandem with `single-numa-node` Topology Manager policy is specifically\nvaluable for workloads that are latency sensitive or for high-throughput applications that perform\nIPC. By combining both options, you are able to place all containers in a pod onto a single NUMA\nnode; hence, the inter-NUMA communication overhead can be eliminated for that pod.\n\nIn the case of `single-numa-node` policy, a pod is accepted only if a suitable set of NUMA nodes\nis present among possible allocations. Reconsider the example above:\n\n* a set containing only a single NUMA node - it leads to pod being admitted,\n* whereas a set containing more NUMA nodes - it results in pod rejection (because instead of one\n  NUMA node, two or more NUMA nodes are required to satisfy the allocation).\n\nTo recap, the Topology Manager first computes a set of NUMA nodes and then tests it against the Topology\nManager policy, which either leads to the rejection or admission of the pod.",
          "is_needle": true
        },
        {
          "chunk_id": "tasks_administer-cluster_topology-manager_mdsem_11",
          "doc_id": "tasks_administer-cluster_topology-manager",
          "content": "### `max-allowable-numa-nodes` {#policy-option-max-allowable-numa-nodes}\n\nThe `max-allowable-numa-nodes` option is GA since Kubernetes 1.35. In Kubernetes {{< skew currentVersion >}},\nthis policy option is visible by default provided that the `TopologyManagerPolicyOptions`\n[feature gate](/docs/reference/command-line-tools-reference/feature-gates/) is enabled.\n\nThe time to admit a pod is tied to the number of NUMA nodes on the physical machine.\nBy default, Kubernetes does not run a kubelet with the Topology Manager enabled, on any (Kubernetes) node where\nmore than 8 NUMA nodes are detected.\n\n{{< note >}}\nIf you select the `max-allowable-numa-nodes` policy option, nodes with more than 8 NUMA nodes can\nbe allowed to run with the Topology Manager enabled. The Kubernetes project only has limited data on the impact\nof using the Topology Manager on (Kubernetes) nodes with more than 8 NUMA nodes. Because of that\nlack of data, using this policy option with Kubernetes {{< skew currentVersion >}} is **not** recommended and is\nat your own risk.\n{{< /note >}}\n\nYou can enable this option by adding `max-allowable-numa-nodes=true` to the Topology Manager policy options.\n\nSetting a value of `max-allowable-numa-nodes` does not (in and of itself) affect the\nlatency of pod admission, but binding a Pod to a (Kubernetes) node with many NUMA does have an impact.\nFuture, potential improvements to Kubernetes may improve Pod admission performance and the high\nlatency that happens as the number of NUMA nodes increases.",
          "is_needle": true
        }
      ]
    },
    {
      "question_id": "q_003",
      "question": "What's the difference between restricted and best-effort topology policies?",
      "expected_answer": "best-effort admits pods even if preferred NUMA affinity isn't available, while restricted rejects pods that can't get preferred affinity, putting them in Terminated state",
      "difficulty": "medium",
      "section": "Topology manager policies",
      "latency_ms": 968.2,
      "needle_found": true,
      "retrieved_chunks": [
        {
          "chunk_id": "tasks_administer-cluster_topology-manager_mdsem_6",
          "doc_id": "tasks_administer-cluster_topology-manager",
          "content": "### `none` policy {#policy-none}\n\nThis is the default policy and does not perform any topology alignment.\n\n### `best-effort` policy {#policy-best-effort}\n\nFor each container in a Pod, the kubelet, with `best-effort` topology management policy, calls\neach Hint Provider to discover their resource availability. Using this information, the Topology\nManager stores the preferred NUMA Node affinity for that container. If the affinity is not\npreferred, the Topology Manager will store this and admit the pod to the node anyway.\n\nThe *Hint Providers* can then use this information when making the\nresource allocation decision.",
          "is_needle": true
        },
        {
          "chunk_id": "concepts_scheduling-eviction_kube-scheduler_mdsem_3",
          "doc_id": "concepts_scheduling-eviction_kube-scheduler",
          "content": "## {{% heading \"whatsnext\" %}}\n\n* Read about [scheduler performance tuning](/docs/concepts/scheduling-eviction/scheduler-perf-tuning/)\n* Read about [Pod topology spread constraints](/docs/concepts/scheduling-eviction/topology-spread-constraints/)\n* Read the [reference documentation](/docs/reference/command-line-tools-reference/kube-scheduler/) for kube-scheduler\n* Read the [kube-scheduler config (v1)](/docs/reference/config-api/kube-scheduler-config.v1/) reference\n* Learn about [configuring multiple schedulers](/docs/tasks/extend-kubernetes/configure-multiple-schedulers/)\n* Learn about [topology management policies](/docs/tasks/administer-cluster/topology-manager/)\n* Learn about [Pod Overhead](/docs/concepts/scheduling-eviction/pod-overhead/)\n* Learn about scheduling of Pods that use volumes in:\n  * [Volume Topology Support](/docs/concepts/storage/storage-classes/#volume-binding-mode)\n  * [Storage Capacity Tracking](/docs/concepts/storage/storage-capacity/)\n  * [Node-specific Volume Limits](/docs/concepts/storage/storage-limits/)",
          "is_needle": false
        },
        {
          "chunk_id": "tasks_administer-cluster_topology-manager_mdsem_7",
          "doc_id": "tasks_administer-cluster_topology-manager",
          "content": "### `restricted` policy {#policy-restricted}\n\nFor each container in a Pod, the kubelet, with `restricted` topology management policy, calls each\nHint Provider to discover their resource availability. Using this information, the Topology\nManager stores the preferred NUMA Node affinity for that container. If the affinity is not\npreferred, the Topology Manager will reject this pod from the node. This will result in a pod entering a\n`Terminated` state with a pod admission failure.\n\nOnce the pod is in a `Terminated` state, the Kubernetes scheduler will **not** attempt to\nreschedule the pod. It is recommended to use a ReplicaSet or Deployment to trigger a redeployment of\nthe pod. An external control loop could be also implemented to trigger a redeployment of pods that\nhave the `Topology Affinity` error.\n\nIf the pod is admitted, the *Hint Providers* can then use this information when making the\nresource allocation decision.",
          "is_needle": true
        },
        {
          "chunk_id": "tasks_debug_debug-cluster_topology_mdsem_2",
          "doc_id": "tasks_debug_debug-cluster_topology",
          "content": "## Troubleshoot `TopologyAffinityError` {#TopologyAffinityError}\n\nThis error typically occurs in the following situations:\n\n* a node has not enough resources available to satisfy the pod's request\n* the pod's request is rejected due to particular Topology Manager policy constraints\n\nThe error appears in the status of a pod:\n\n```shell\nkubectl get pods\n```\n\n```none\nNAME         READY   STATUS                  RESTARTS   AGE\nguaranteed   0/1     TopologyAffinityError   0          113s\n```\n\nUse `kubectl describe pod <id>` or `kubectl events` to obtain a detailed error message:\n\n```none\nWarning  TopologyAffinityError  10m   kubelet, dell8  Resources cannot be allocated with Topology locality\n```",
          "is_needle": false
        },
        {
          "chunk_id": "tasks_administer-cluster_topology-manager_mdsem_11",
          "doc_id": "tasks_administer-cluster_topology-manager",
          "content": "### `max-allowable-numa-nodes` {#policy-option-max-allowable-numa-nodes}\n\nThe `max-allowable-numa-nodes` option is GA since Kubernetes 1.35. In Kubernetes {{< skew currentVersion >}},\nthis policy option is visible by default provided that the `TopologyManagerPolicyOptions`\n[feature gate](/docs/reference/command-line-tools-reference/feature-gates/) is enabled.\n\nThe time to admit a pod is tied to the number of NUMA nodes on the physical machine.\nBy default, Kubernetes does not run a kubelet with the Topology Manager enabled, on any (Kubernetes) node where\nmore than 8 NUMA nodes are detected.\n\n{{< note >}}\nIf you select the `max-allowable-numa-nodes` policy option, nodes with more than 8 NUMA nodes can\nbe allowed to run with the Topology Manager enabled. The Kubernetes project only has limited data on the impact\nof using the Topology Manager on (Kubernetes) nodes with more than 8 NUMA nodes. Because of that\nlack of data, using this policy option with Kubernetes {{< skew currentVersion >}} is **not** recommended and is\nat your own risk.\n{{< /note >}}\n\nYou can enable this option by adding `max-allowable-numa-nodes=true` to the Topology Manager policy options.\n\nSetting a value of `max-allowable-numa-nodes` does not (in and of itself) affect the\nlatency of pod admission, but binding a Pod to a (Kubernetes) node with many NUMA does have an impact.\nFuture, potential improvements to Kubernetes may improve Pod admission performance and the high\nlatency that happens as the number of NUMA nodes increases.",
          "is_needle": true
        }
      ]
    },
    {
      "question_id": "q_004",
      "question": "Getting error about too many NUMA nodes on my server, what's the limit?",
      "expected_answer": "8 NUMA nodes by default. Use max-allowable-numa-nodes policy option to allow more, but it's not recommended",
      "difficulty": "easy",
      "section": "Known limitations",
      "latency_ms": 971.1,
      "needle_found": true,
      "retrieved_chunks": [
        {
          "chunk_id": "tasks_administer-cluster_topology-manager_mdsem_13",
          "doc_id": "tasks_administer-cluster_topology-manager",
          "content": "## Known limitations\n\n1. The maximum number of NUMA nodes that Topology Manager allows is 8. With more than 8 NUMA nodes,\n   there will be a state explosion when trying to enumerate the possible NUMA affinities and\n   generating their hints. See [`max-allowable-numa-nodes`](#policy-option-max-allowable-numa-nodes)\n   (beta) for more options.\n\n1. The scheduler is not topology-aware, so it is possible to be scheduled on a node and then fail\n   on the node due to the Topology Manager.",
          "is_needle": true
        },
        {
          "chunk_id": "tasks_administer-cluster_topology-manager_mdsem_4",
          "doc_id": "tasks_administer-cluster_topology-manager",
          "content": "### `pod` scope\n\nTo select the `pod` scope, set `topologyManagerScope` in the\n[kubelet configuration file](/docs/tasks/administer-cluster/kubelet-config-file/) to `pod`.\n\nThis scope allows for grouping all containers in a pod to a common set of NUMA nodes. That is, the\nTopology Manager treats a pod as a whole and attempts to allocate the entire pod (all containers)\nto either a single NUMA node or a common set of NUMA nodes. The following examples illustrate the\nalignments produced by the Topology Manager on different occasions:\n\n* all containers can be and are allocated to a single NUMA node;\n* all containers can be and are allocated to a shared set of NUMA nodes.\n\nThe total amount of particular resource demanded for the entire pod is calculated according to\n[effective requests/limits](/docs/concepts/workloads/pods/init-containers/#resource-sharing-within-containers)\nformula, and thus, this total value is equal to the maximum of:\n\n* the sum of all app container requests,\n* the maximum of init container requests,\n\nfor a resource.\n\nUsing the `pod` scope in tandem with `single-numa-node` Topology Manager policy is specifically\nvaluable for workloads that are latency sensitive or for high-throughput applications that perform\nIPC. By combining both options, you are able to place all containers in a pod onto a single NUMA\nnode; hence, the inter-NUMA communication overhead can be eliminated for that pod.\n\nIn the case of `single-numa-node` policy, a pod is accepted only if a suitable set of NUMA nodes\nis present among possible allocations. Reconsider the example above:\n\n* a set containing only a single NUMA node - it leads to pod being admitted,\n* whereas a set containing more NUMA nodes - it results in pod rejection (because instead of one\n  NUMA node, two or more NUMA nodes are required to satisfy the allocation).\n\nTo recap, the Topology Manager first computes a set of NUMA nodes and then tests it against the Topology\nManager policy, which either leads to the rejection or admission of the pod.",
          "is_needle": true
        },
        {
          "chunk_id": "tasks_administer-cluster_topology-manager_mdsem_11",
          "doc_id": "tasks_administer-cluster_topology-manager",
          "content": "### `max-allowable-numa-nodes` {#policy-option-max-allowable-numa-nodes}\n\nThe `max-allowable-numa-nodes` option is GA since Kubernetes 1.35. In Kubernetes {{< skew currentVersion >}},\nthis policy option is visible by default provided that the `TopologyManagerPolicyOptions`\n[feature gate](/docs/reference/command-line-tools-reference/feature-gates/) is enabled.\n\nThe time to admit a pod is tied to the number of NUMA nodes on the physical machine.\nBy default, Kubernetes does not run a kubelet with the Topology Manager enabled, on any (Kubernetes) node where\nmore than 8 NUMA nodes are detected.\n\n{{< note >}}\nIf you select the `max-allowable-numa-nodes` policy option, nodes with more than 8 NUMA nodes can\nbe allowed to run with the Topology Manager enabled. The Kubernetes project only has limited data on the impact\nof using the Topology Manager on (Kubernetes) nodes with more than 8 NUMA nodes. Because of that\nlack of data, using this policy option with Kubernetes {{< skew currentVersion >}} is **not** recommended and is\nat your own risk.\n{{< /note >}}\n\nYou can enable this option by adding `max-allowable-numa-nodes=true` to the Topology Manager policy options.\n\nSetting a value of `max-allowable-numa-nodes` does not (in and of itself) affect the\nlatency of pod admission, but binding a Pod to a (Kubernetes) node with many NUMA does have an impact.\nFuture, potential improvements to Kubernetes may improve Pod admission performance and the high\nlatency that happens as the number of NUMA nodes increases.",
          "is_needle": true
        },
        {
          "chunk_id": "tasks_administer-cluster_topology-manager_mdsem_10",
          "doc_id": "tasks_administer-cluster_topology-manager",
          "content": "### `prefer-closest-numa-nodes` {#policy-option-prefer-closest-numa-nodes}\n\nThe `prefer-closest-numa-nodes` option is GA since Kubernetes 1.32. In Kubernetes {{< skew currentVersion >}}\nthis policy option is visible by default provided that the `TopologyManagerPolicyOptions`\n[feature gate](/docs/reference/command-line-tools-reference/feature-gates/) is enabled.\n\nThe Topology Manager is not aware by default of NUMA distances, and does not take them into account when making\nPod admission decisions. This limitation surfaces in multi-socket, as well as single-socket multi NUMA systems,\nand can cause significant performance degradation in latency-critical execution and high-throughput applications\nif the Topology Manager decides to align resources on non-adjacent NUMA nodes.\n\nIf you specify the `prefer-closest-numa-nodes` policy option, the `best-effort` and `restricted`\npolicies favor sets of NUMA nodes with shorter distance between them when making admission decisions.\n\nYou can enable this option by adding `prefer-closest-numa-nodes=true` to the Topology Manager policy options.\n\nBy default (without this option), the Topology Manager aligns resources on either a single NUMA node or,\nin the case where more than one NUMA node is required, using the minimum number of NUMA nodes.",
          "is_needle": true
        },
        {
          "chunk_id": "tasks_debug_debug-cluster_topology_mdsem_0",
          "doc_id": "tasks_debug_debug-cluster_topology",
          "content": "---\ntitle: Troubleshooting Topology Management\nweight: 60\ncontent_type: task\n---\n\n<!-- overview -->\n\nKubernetes keeps many aspects of how pods execute on nodes abstracted\nfrom the user. This is by design. However, some workloads require\nstronger guarantees in terms of latency and/or performance in order to operate\nacceptably. The `kubelet` provides methods to enable more complex workload\nplacement policies while keeping the abstraction free from explicit placement\ndirectives.\n\nYou can manage topology within nodes. This means helping the kubelet to configure the host operating system so that\nPods and containers are placed on the correct side of inner boundaries, such as _NUMA domains_. (NUMA is an abbreviation\nof _non-uniform memory access_, and refers to an idea that CPUs might be topologically closer to specific regions of\nmemory, due to the physical layout of the hardware components and the way that these are connected).",
          "is_needle": false
        }
      ]
    },
    {
      "question_id": "q_005",
      "question": "How to enable topology manager on Windows nodes?",
      "expected_answer": "Enable the WindowsCPUAndMemoryAffinity feature gate and ensure the container runtime supports it",
      "difficulty": "easy",
      "section": "Windows Support",
      "latency_ms": 1034.2,
      "needle_found": true,
      "retrieved_chunks": [
        {
          "chunk_id": "tasks_administer-cluster_topology-manager_mdsem_2",
          "doc_id": "tasks_administer-cluster_topology-manager",
          "content": "## Windows Support\n\n{{< feature-state feature_gate_name=\"WindowsCPUAndMemoryAffinity\" >}}\n\nThe Topology Manager support can be enabled on Windows by using the `WindowsCPUAndMemoryAffinity` feature gate and\nit requires support in the container runtime.\n\n## Topology manager scopes and policies\n\nThe Topology Manager currently:\n\n- aligns Pods of all QoS classes.\n- aligns the requested resources that Hint Provider provides topology hints for.\n\nIf these conditions are met, the Topology Manager will align the requested resources.\n\nIn order to customize how this alignment is carried out, the Topology Manager provides two\ndistinct options: `scope` and `policy`.\n\nThe `scope` defines the granularity at which you would like resource alignment to be performed,\nfor example, at the `pod` or `container` level. And the `policy` defines the actual policy used to\ncarry out the alignment, for example, `best-effort`, `restricted`, and `single-numa-node`.\nDetails on the various `scopes` and `policies` available today can be found below.\n\n{{< note >}}\nTo align CPU resources with other requested resources in a Pod spec, the CPU Manager should be\nenabled and proper CPU Manager policy should be configured on a Node.\nSee [Control CPU Management Policies on the Node](/docs/tasks/administer-cluster/cpu-management-policies/).\n{{< /note >}}\n\n{{< note >}}\nTo align memory (and hugepages) resources with other requested resources in a Pod spec, the Memory\nManager should be enabled and proper Memory Manager policy should be configured on a Node. Refer to\n[Memory Manager](/docs/tasks/administer-cluster/memory-manager/) documentation.\n{{< /note >}}",
          "is_needle": true
        },
        {
          "chunk_id": "tasks_administer-cluster_topology-manager_mdsem_1",
          "doc_id": "tasks_administer-cluster_topology-manager",
          "content": "## {{% heading \"prerequisites\" %}}\n\n{{< include \"task-tutorial-prereqs.md\" >}} {{< version-check >}}\n\n<!-- steps -->\n\n## How topology manager works\n\nPrior to the introduction of Topology Manager, the CPU and Device Manager in Kubernetes make\nresource allocation decisions independently of each other. This can result in undesirable\nallocations on multiple-socketed systems, and performance/latency sensitive applications will suffer\ndue to these undesirable allocations. Undesirable in this case meaning, for example, CPUs and\ndevices being allocated from different NUMA Nodes, thus incurring additional latency.\n\nThe Topology Manager is a kubelet component, which acts as a source of truth so that other kubelet\ncomponents can make topology aligned resource allocation choices.\n\nThe Topology Manager provides an interface for components, called *Hint Providers*, to send and\nreceive topology information. The Topology Manager has a set of node level policies which are\nexplained below.\n\nThe Topology Manager receives topology information from the *Hint Providers* as a bitmask denoting\nNUMA Nodes available and a preferred allocation indication. The Topology Manager policies perform\na set of operations on the hints provided and converge on the hint determined by the policy to\ngive the optimal result. If an undesirable hint is stored, the preferred field for the hint will be\nset to false. In the current policies preferred is the narrowest preferred mask.\nThe selected hint is stored as part of the Topology Manager. Depending on the policy configured,\nthe pod can be accepted or rejected from the node based on the selected hint.\nThe hint is then stored in the Topology Manager for use by the *Hint Providers* when making the\nresource allocation decisions.\n\nThe flow can be seen in the following diagram.\n\n![topology_manager_flow](/images/docs/topology-manager-flow.png)",
          "is_needle": true
        },
        {
          "chunk_id": "tasks_administer-cluster_topology-manager_mdsem_11",
          "doc_id": "tasks_administer-cluster_topology-manager",
          "content": "### `max-allowable-numa-nodes` {#policy-option-max-allowable-numa-nodes}\n\nThe `max-allowable-numa-nodes` option is GA since Kubernetes 1.35. In Kubernetes {{< skew currentVersion >}},\nthis policy option is visible by default provided that the `TopologyManagerPolicyOptions`\n[feature gate](/docs/reference/command-line-tools-reference/feature-gates/) is enabled.\n\nThe time to admit a pod is tied to the number of NUMA nodes on the physical machine.\nBy default, Kubernetes does not run a kubelet with the Topology Manager enabled, on any (Kubernetes) node where\nmore than 8 NUMA nodes are detected.\n\n{{< note >}}\nIf you select the `max-allowable-numa-nodes` policy option, nodes with more than 8 NUMA nodes can\nbe allowed to run with the Topology Manager enabled. The Kubernetes project only has limited data on the impact\nof using the Topology Manager on (Kubernetes) nodes with more than 8 NUMA nodes. Because of that\nlack of data, using this policy option with Kubernetes {{< skew currentVersion >}} is **not** recommended and is\nat your own risk.\n{{< /note >}}\n\nYou can enable this option by adding `max-allowable-numa-nodes=true` to the Topology Manager policy options.\n\nSetting a value of `max-allowable-numa-nodes` does not (in and of itself) affect the\nlatency of pod admission, but binding a Pod to a (Kubernetes) node with many NUMA does have an impact.\nFuture, potential improvements to Kubernetes may improve Pod admission performance and the high\nlatency that happens as the number of NUMA nodes increases.",
          "is_needle": true
        },
        {
          "chunk_id": "tasks_administer-cluster_topology-manager_mdsem_3",
          "doc_id": "tasks_administer-cluster_topology-manager",
          "content": "## Topology manager scopes\n\nThe Topology Manager can deal with the alignment of resources in a couple of distinct scopes:\n\n* `container` (default)\n* `pod`\n\nEither option can be selected at a time of the kubelet startup, by setting the\n`topologyManagerScope` in the\n[kubelet configuration file](/docs/tasks/administer-cluster/kubelet-config-file/).\n\n### `container` scope\n\nThe `container` scope is used by default. You can also explicitly set the\n`topologyManagerScope` to `container` in the\n[kubelet configuration file](/docs/tasks/administer-cluster/kubelet-config-file/).\n\nWithin this scope, the Topology Manager performs a number of sequential resource alignments, i.e.,\nfor each container (in a pod) a separate alignment is computed. In other words, there is no notion\nof grouping the containers to a specific set of NUMA nodes, for this particular scope. In effect,\nthe Topology Manager performs an arbitrary alignment of individual containers to NUMA nodes.\n\nThe notion of grouping the containers was endorsed and implemented on purpose in the following\nscope, for example the `pod` scope.",
          "is_needle": true
        },
        {
          "chunk_id": "tasks_administer-cluster_topology-manager_mdsem_8",
          "doc_id": "tasks_administer-cluster_topology-manager",
          "content": "### `single-numa-node` policy {#policy-single-numa-node}\n\nFor each container in a Pod, the kubelet, with `single-numa-node` topology management policy,\ncalls each Hint Provider to discover their resource availability. Using this information, the\nTopology Manager determines if a single NUMA Node affinity is possible. If it is, Topology\nManager will store this and the *Hint Providers* can then use this information when making the\nresource allocation decision. If, however, this is not possible then the Topology Manager will\nreject the pod from the node. This will result in a pod in a `Terminated` state with a pod\nadmission failure.\n\nOnce the pod is in a `Terminated` state, the Kubernetes scheduler will **not** attempt to\nreschedule the pod. It is recommended to use a Deployment with replicas to trigger a redeployment of\nthe Pod. An external control loop could be also implemented to trigger a redeployment of pods\nthat have the `Topology Affinity` error.",
          "is_needle": true
        }
      ]
    },
    {
      "question_id": "q_006",
      "question": "Which k8s version made topology manager GA/stable?",
      "expected_answer": "v1.27",
      "difficulty": "easy",
      "section": "overview",
      "latency_ms": 892.2,
      "needle_found": true,
      "retrieved_chunks": [
        {
          "chunk_id": "tasks_administer-cluster_topology-manager_mdsem_8",
          "doc_id": "tasks_administer-cluster_topology-manager",
          "content": "### `single-numa-node` policy {#policy-single-numa-node}\n\nFor each container in a Pod, the kubelet, with `single-numa-node` topology management policy,\ncalls each Hint Provider to discover their resource availability. Using this information, the\nTopology Manager determines if a single NUMA Node affinity is possible. If it is, Topology\nManager will store this and the *Hint Providers* can then use this information when making the\nresource allocation decision. If, however, this is not possible then the Topology Manager will\nreject the pod from the node. This will result in a pod in a `Terminated` state with a pod\nadmission failure.\n\nOnce the pod is in a `Terminated` state, the Kubernetes scheduler will **not** attempt to\nreschedule the pod. It is recommended to use a Deployment with replicas to trigger a redeployment of\nthe Pod. An external control loop could be also implemented to trigger a redeployment of pods\nthat have the `Topology Affinity` error.",
          "is_needle": true
        },
        {
          "chunk_id": "tasks_administer-cluster_topology-manager_mdsem_1",
          "doc_id": "tasks_administer-cluster_topology-manager",
          "content": "## {{% heading \"prerequisites\" %}}\n\n{{< include \"task-tutorial-prereqs.md\" >}} {{< version-check >}}\n\n<!-- steps -->\n\n## How topology manager works\n\nPrior to the introduction of Topology Manager, the CPU and Device Manager in Kubernetes make\nresource allocation decisions independently of each other. This can result in undesirable\nallocations on multiple-socketed systems, and performance/latency sensitive applications will suffer\ndue to these undesirable allocations. Undesirable in this case meaning, for example, CPUs and\ndevices being allocated from different NUMA Nodes, thus incurring additional latency.\n\nThe Topology Manager is a kubelet component, which acts as a source of truth so that other kubelet\ncomponents can make topology aligned resource allocation choices.\n\nThe Topology Manager provides an interface for components, called *Hint Providers*, to send and\nreceive topology information. The Topology Manager has a set of node level policies which are\nexplained below.\n\nThe Topology Manager receives topology information from the *Hint Providers* as a bitmask denoting\nNUMA Nodes available and a preferred allocation indication. The Topology Manager policies perform\na set of operations on the hints provided and converge on the hint determined by the policy to\ngive the optimal result. If an undesirable hint is stored, the preferred field for the hint will be\nset to false. In the current policies preferred is the narrowest preferred mask.\nThe selected hint is stored as part of the Topology Manager. Depending on the policy configured,\nthe pod can be accepted or rejected from the node based on the selected hint.\nThe hint is then stored in the Topology Manager for use by the *Hint Providers* when making the\nresource allocation decisions.\n\nThe flow can be seen in the following diagram.\n\n![topology_manager_flow](/images/docs/topology-manager-flow.png)",
          "is_needle": true
        },
        {
          "chunk_id": "tasks_administer-cluster_topology-manager_mdsem_7",
          "doc_id": "tasks_administer-cluster_topology-manager",
          "content": "### `restricted` policy {#policy-restricted}\n\nFor each container in a Pod, the kubelet, with `restricted` topology management policy, calls each\nHint Provider to discover their resource availability. Using this information, the Topology\nManager stores the preferred NUMA Node affinity for that container. If the affinity is not\npreferred, the Topology Manager will reject this pod from the node. This will result in a pod entering a\n`Terminated` state with a pod admission failure.\n\nOnce the pod is in a `Terminated` state, the Kubernetes scheduler will **not** attempt to\nreschedule the pod. It is recommended to use a ReplicaSet or Deployment to trigger a redeployment of\nthe pod. An external control loop could be also implemented to trigger a redeployment of pods that\nhave the `Topology Affinity` error.\n\nIf the pod is admitted, the *Hint Providers* can then use this information when making the\nresource allocation decision.",
          "is_needle": true
        },
        {
          "chunk_id": "tasks_administer-cluster_topology-manager_mdsem_5",
          "doc_id": "tasks_administer-cluster_topology-manager",
          "content": "## Topology manager policies\n\nThe Topology Manager supports four allocation policies. You can set a policy via a kubelet flag,\n`--topology-manager-policy`. There are four supported policies:\n\n* `none` (default)\n* `best-effort`\n* `restricted`\n* `single-numa-node`\n\n{{< note >}}\nIf the Topology Manager is configured with the **pod** scope, the container, which is considered by\nthe policy, is reflecting requirements of the entire pod, and thus each container from the pod\nwill result with **the same** topology alignment decision.\n{{< /note >}}",
          "is_needle": true
        },
        {
          "chunk_id": "tasks_administer-cluster_topology-manager_mdsem_12",
          "doc_id": "tasks_administer-cluster_topology-manager",
          "content": "## Pod interactions with topology manager policies\n\nConsider the containers in the following Pod manifest:\n\n```yaml\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n```\n\nThis pod runs in the `BestEffort` QoS class because no resource `requests` or `limits` are specified.\n\n```yaml\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n    resources:\n      limits:\n        memory: \"200Mi\"\n      requests:\n        memory: \"100Mi\"\n```\n\nThis pod runs in the `Burstable` QoS class because requests are less than limits.\n\nIf the selected policy is anything other than `none`, the Topology Manager would consider these Pod\nspecifications. The Topology Manager would consult the Hint Providers to get topology hints.\nIn the case of the `static`, the CPU Manager policy would return default topology hint, because\nthese Pods do not explicitly request CPU resources.\n\n```yaml\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n    resources:\n      limits:\n        memory: \"200Mi\"\n        cpu: \"2\"\n        example.com/device: \"1\"\n      requests:\n        memory: \"200Mi\"\n        cpu: \"2\"\n        example.com/device: \"1\"\n```\n\nThis pod with integer CPU request runs in the `Guaranteed` QoS class because `requests` are equal\nto `limits`.\n\n```yaml\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n    resources:\n      limits:\n        memory: \"200Mi\"\n        cpu: \"300m\"\n        example.com/device: \"1\"\n      requests:\n        memory: \"200Mi\"\n        cpu: \"300m\"\n        example.com/device: \"1\"\n```\n\nThis pod with sharing CPU request runs in the `Guaranteed` QoS class because `requests` are equal\nto `limits`.\n\n```yaml\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n    resources:\n      limits:\n        example.com/deviceA: \"1\"\n        example.com/deviceB: \"1\"\n      requests:\n        example.com/deviceA: \"1\"\n        example.com/deviceB: \"1\"\n```\n\nThis pod runs in the `BestEffort` QoS class because there are no CPU and memory requests.\n\nThe Topology Manager would consider the above pods. The Topology Manager would consult the Hint\nProviders, which are CPU and Device Manager to get topology hints for the pods.\n\nIn the case of the `Guaranteed` pod with integer CPU request, the `static` CPU Manager policy\nwould return topology hints relating to the exclusive CPU and the Device Manager would send back\nhints for the requested device.\n\nIn the case of the `Guaranteed` pod with sharing CPU request, the `static` CPU Manager policy\nwould return default topology hint as there is no exclusive CPU request and the Device Manager\nwould send back hints for the requested device.\n\nIn the above two cases of the `Guaranteed` pod, the `none` CPU Manager policy would return default\ntopology hint.\n\nIn the case of the `BestEffort` pod, the `static` CPU Manager policy would send back the default\ntopology hint as there is no CPU request and the Device Manager would send back the hints for each\nof the requested devices.\n\nUsing this information the Topology Manager calculates the optimal hint for the pod and stores\nthis information, which will be used by the Hint Providers when they are making their resource\nassignments.",
          "is_needle": true
        }
      ]
    },
    {
      "question_id": "q_007",
      "question": "What flag do I pass to kubelet for setting topology policy?",
      "expected_answer": "--topology-manager-policy",
      "difficulty": "easy",
      "section": "Topology manager policies",
      "latency_ms": 1122.0,
      "needle_found": true,
      "retrieved_chunks": [
        {
          "chunk_id": "tasks_administer-cluster_topology-manager_mdsem_5",
          "doc_id": "tasks_administer-cluster_topology-manager",
          "content": "## Topology manager policies\n\nThe Topology Manager supports four allocation policies. You can set a policy via a kubelet flag,\n`--topology-manager-policy`. There are four supported policies:\n\n* `none` (default)\n* `best-effort`\n* `restricted`\n* `single-numa-node`\n\n{{< note >}}\nIf the Topology Manager is configured with the **pod** scope, the container, which is considered by\nthe policy, is reflecting requirements of the entire pod, and thus each container from the pod\nwill result with **the same** topology alignment decision.\n{{< /note >}}",
          "is_needle": true
        },
        {
          "chunk_id": "tasks_administer-cluster_topology-manager_mdsem_9",
          "doc_id": "tasks_administer-cluster_topology-manager",
          "content": "## Topology manager policy options\n\nSupport for the Topology Manager policy options requires `TopologyManagerPolicyOptions`\n[feature gate](/docs/reference/command-line-tools-reference/feature-gates/) to be enabled\n(it is enabled by default).\n\nYou can toggle groups of options on and off based upon their maturity level using the following feature gates:\n\n* `TopologyManagerPolicyBetaOptions` default enabled. Enable to show beta-level options.\n* `TopologyManagerPolicyAlphaOptions` default disabled. Enable to show alpha-level options.\n\nYou will still have to enable each option using the `TopologyManagerPolicyOptions` kubelet option.",
          "is_needle": true
        },
        {
          "chunk_id": "tasks_administer-cluster_topology-manager_mdsem_6",
          "doc_id": "tasks_administer-cluster_topology-manager",
          "content": "### `none` policy {#policy-none}\n\nThis is the default policy and does not perform any topology alignment.\n\n### `best-effort` policy {#policy-best-effort}\n\nFor each container in a Pod, the kubelet, with `best-effort` topology management policy, calls\neach Hint Provider to discover their resource availability. Using this information, the Topology\nManager stores the preferred NUMA Node affinity for that container. If the affinity is not\npreferred, the Topology Manager will store this and admit the pod to the node anyway.\n\nThe *Hint Providers* can then use this information when making the\nresource allocation decision.",
          "is_needle": true
        },
        {
          "chunk_id": "tasks_administer-cluster_topology-manager_mdsem_3",
          "doc_id": "tasks_administer-cluster_topology-manager",
          "content": "## Topology manager scopes\n\nThe Topology Manager can deal with the alignment of resources in a couple of distinct scopes:\n\n* `container` (default)\n* `pod`\n\nEither option can be selected at a time of the kubelet startup, by setting the\n`topologyManagerScope` in the\n[kubelet configuration file](/docs/tasks/administer-cluster/kubelet-config-file/).\n\n### `container` scope\n\nThe `container` scope is used by default. You can also explicitly set the\n`topologyManagerScope` to `container` in the\n[kubelet configuration file](/docs/tasks/administer-cluster/kubelet-config-file/).\n\nWithin this scope, the Topology Manager performs a number of sequential resource alignments, i.e.,\nfor each container (in a pod) a separate alignment is computed. In other words, there is no notion\nof grouping the containers to a specific set of NUMA nodes, for this particular scope. In effect,\nthe Topology Manager performs an arbitrary alignment of individual containers to NUMA nodes.\n\nThe notion of grouping the containers was endorsed and implemented on purpose in the following\nscope, for example the `pod` scope.",
          "is_needle": true
        },
        {
          "chunk_id": "tasks_administer-cluster_topology-manager_mdsem_1",
          "doc_id": "tasks_administer-cluster_topology-manager",
          "content": "## {{% heading \"prerequisites\" %}}\n\n{{< include \"task-tutorial-prereqs.md\" >}} {{< version-check >}}\n\n<!-- steps -->\n\n## How topology manager works\n\nPrior to the introduction of Topology Manager, the CPU and Device Manager in Kubernetes make\nresource allocation decisions independently of each other. This can result in undesirable\nallocations on multiple-socketed systems, and performance/latency sensitive applications will suffer\ndue to these undesirable allocations. Undesirable in this case meaning, for example, CPUs and\ndevices being allocated from different NUMA Nodes, thus incurring additional latency.\n\nThe Topology Manager is a kubelet component, which acts as a source of truth so that other kubelet\ncomponents can make topology aligned resource allocation choices.\n\nThe Topology Manager provides an interface for components, called *Hint Providers*, to send and\nreceive topology information. The Topology Manager has a set of node level policies which are\nexplained below.\n\nThe Topology Manager receives topology information from the *Hint Providers* as a bitmask denoting\nNUMA Nodes available and a preferred allocation indication. The Topology Manager policies perform\na set of operations on the hints provided and converge on the hint determined by the policy to\ngive the optimal result. If an undesirable hint is stored, the preferred field for the hint will be\nset to false. In the current policies preferred is the narrowest preferred mask.\nThe selected hint is stored as part of the Topology Manager. Depending on the policy configured,\nthe pod can be accepted or rejected from the node based on the selected hint.\nThe hint is then stored in the Topology Manager for use by the *Hint Providers* when making the\nresource allocation decisions.\n\nThe flow can be seen in the following diagram.\n\n![topology_manager_flow](/images/docs/topology-manager-flow.png)",
          "is_needle": true
        }
      ]
    },
    {
      "question_id": "q_008",
      "question": "My latency-sensitive app is slow, containers seem to be on different NUMA nodes. How to fix?",
      "expected_answer": "Use pod scope with single-numa-node policy to place all containers on a single NUMA node, eliminating inter-NUMA communication overhead",
      "difficulty": "hard",
      "section": "pod scope",
      "latency_ms": 986.3,
      "needle_found": true,
      "retrieved_chunks": [
        {
          "chunk_id": "tasks_administer-cluster_topology-manager_mdsem_4",
          "doc_id": "tasks_administer-cluster_topology-manager",
          "content": "### `pod` scope\n\nTo select the `pod` scope, set `topologyManagerScope` in the\n[kubelet configuration file](/docs/tasks/administer-cluster/kubelet-config-file/) to `pod`.\n\nThis scope allows for grouping all containers in a pod to a common set of NUMA nodes. That is, the\nTopology Manager treats a pod as a whole and attempts to allocate the entire pod (all containers)\nto either a single NUMA node or a common set of NUMA nodes. The following examples illustrate the\nalignments produced by the Topology Manager on different occasions:\n\n* all containers can be and are allocated to a single NUMA node;\n* all containers can be and are allocated to a shared set of NUMA nodes.\n\nThe total amount of particular resource demanded for the entire pod is calculated according to\n[effective requests/limits](/docs/concepts/workloads/pods/init-containers/#resource-sharing-within-containers)\nformula, and thus, this total value is equal to the maximum of:\n\n* the sum of all app container requests,\n* the maximum of init container requests,\n\nfor a resource.\n\nUsing the `pod` scope in tandem with `single-numa-node` Topology Manager policy is specifically\nvaluable for workloads that are latency sensitive or for high-throughput applications that perform\nIPC. By combining both options, you are able to place all containers in a pod onto a single NUMA\nnode; hence, the inter-NUMA communication overhead can be eliminated for that pod.\n\nIn the case of `single-numa-node` policy, a pod is accepted only if a suitable set of NUMA nodes\nis present among possible allocations. Reconsider the example above:\n\n* a set containing only a single NUMA node - it leads to pod being admitted,\n* whereas a set containing more NUMA nodes - it results in pod rejection (because instead of one\n  NUMA node, two or more NUMA nodes are required to satisfy the allocation).\n\nTo recap, the Topology Manager first computes a set of NUMA nodes and then tests it against the Topology\nManager policy, which either leads to the rejection or admission of the pod.",
          "is_needle": true
        },
        {
          "chunk_id": "tasks_administer-cluster_topology-manager_mdsem_7",
          "doc_id": "tasks_administer-cluster_topology-manager",
          "content": "### `restricted` policy {#policy-restricted}\n\nFor each container in a Pod, the kubelet, with `restricted` topology management policy, calls each\nHint Provider to discover their resource availability. Using this information, the Topology\nManager stores the preferred NUMA Node affinity for that container. If the affinity is not\npreferred, the Topology Manager will reject this pod from the node. This will result in a pod entering a\n`Terminated` state with a pod admission failure.\n\nOnce the pod is in a `Terminated` state, the Kubernetes scheduler will **not** attempt to\nreschedule the pod. It is recommended to use a ReplicaSet or Deployment to trigger a redeployment of\nthe pod. An external control loop could be also implemented to trigger a redeployment of pods that\nhave the `Topology Affinity` error.\n\nIf the pod is admitted, the *Hint Providers* can then use this information when making the\nresource allocation decision.",
          "is_needle": true
        },
        {
          "chunk_id": "tasks_administer-cluster_topology-manager_mdsem_8",
          "doc_id": "tasks_administer-cluster_topology-manager",
          "content": "### `single-numa-node` policy {#policy-single-numa-node}\n\nFor each container in a Pod, the kubelet, with `single-numa-node` topology management policy,\ncalls each Hint Provider to discover their resource availability. Using this information, the\nTopology Manager determines if a single NUMA Node affinity is possible. If it is, Topology\nManager will store this and the *Hint Providers* can then use this information when making the\nresource allocation decision. If, however, this is not possible then the Topology Manager will\nreject the pod from the node. This will result in a pod in a `Terminated` state with a pod\nadmission failure.\n\nOnce the pod is in a `Terminated` state, the Kubernetes scheduler will **not** attempt to\nreschedule the pod. It is recommended to use a Deployment with replicas to trigger a redeployment of\nthe Pod. An external control loop could be also implemented to trigger a redeployment of pods\nthat have the `Topology Affinity` error.",
          "is_needle": true
        },
        {
          "chunk_id": "tasks_administer-cluster_topology-manager_mdsem_10",
          "doc_id": "tasks_administer-cluster_topology-manager",
          "content": "### `prefer-closest-numa-nodes` {#policy-option-prefer-closest-numa-nodes}\n\nThe `prefer-closest-numa-nodes` option is GA since Kubernetes 1.32. In Kubernetes {{< skew currentVersion >}}\nthis policy option is visible by default provided that the `TopologyManagerPolicyOptions`\n[feature gate](/docs/reference/command-line-tools-reference/feature-gates/) is enabled.\n\nThe Topology Manager is not aware by default of NUMA distances, and does not take them into account when making\nPod admission decisions. This limitation surfaces in multi-socket, as well as single-socket multi NUMA systems,\nand can cause significant performance degradation in latency-critical execution and high-throughput applications\nif the Topology Manager decides to align resources on non-adjacent NUMA nodes.\n\nIf you specify the `prefer-closest-numa-nodes` policy option, the `best-effort` and `restricted`\npolicies favor sets of NUMA nodes with shorter distance between them when making admission decisions.\n\nYou can enable this option by adding `prefer-closest-numa-nodes=true` to the Topology Manager policy options.\n\nBy default (without this option), the Topology Manager aligns resources on either a single NUMA node or,\nin the case where more than one NUMA node is required, using the minimum number of NUMA nodes.",
          "is_needle": true
        },
        {
          "chunk_id": "tasks_administer-cluster_topology-manager_mdsem_11",
          "doc_id": "tasks_administer-cluster_topology-manager",
          "content": "### `max-allowable-numa-nodes` {#policy-option-max-allowable-numa-nodes}\n\nThe `max-allowable-numa-nodes` option is GA since Kubernetes 1.35. In Kubernetes {{< skew currentVersion >}},\nthis policy option is visible by default provided that the `TopologyManagerPolicyOptions`\n[feature gate](/docs/reference/command-line-tools-reference/feature-gates/) is enabled.\n\nThe time to admit a pod is tied to the number of NUMA nodes on the physical machine.\nBy default, Kubernetes does not run a kubelet with the Topology Manager enabled, on any (Kubernetes) node where\nmore than 8 NUMA nodes are detected.\n\n{{< note >}}\nIf you select the `max-allowable-numa-nodes` policy option, nodes with more than 8 NUMA nodes can\nbe allowed to run with the Topology Manager enabled. The Kubernetes project only has limited data on the impact\nof using the Topology Manager on (Kubernetes) nodes with more than 8 NUMA nodes. Because of that\nlack of data, using this policy option with Kubernetes {{< skew currentVersion >}} is **not** recommended and is\nat your own risk.\n{{< /note >}}\n\nYou can enable this option by adding `max-allowable-numa-nodes=true` to the Topology Manager policy options.\n\nSetting a value of `max-allowable-numa-nodes` does not (in and of itself) affect the\nlatency of pod admission, but binding a Pod to a (Kubernetes) node with many NUMA does have an impact.\nFuture, potential improvements to Kubernetes may improve Pod admission performance and the high\nlatency that happens as the number of NUMA nodes increases.",
          "is_needle": true
        }
      ]
    },
    {
      "question_id": "q_009",
      "question": "Why would I use pod scope instead of container scope for topology?",
      "expected_answer": "Pod scope groups all containers to a common set of NUMA nodes, treating the pod as a whole. Container scope does separate alignment per container with no grouping, which can result in containers on different NUMA nodes",
      "difficulty": "hard",
      "section": "Topology manager scopes",
      "latency_ms": 1022.7,
      "needle_found": true,
      "retrieved_chunks": [
        {
          "chunk_id": "tasks_administer-cluster_topology-manager_mdsem_3",
          "doc_id": "tasks_administer-cluster_topology-manager",
          "content": "## Topology manager scopes\n\nThe Topology Manager can deal with the alignment of resources in a couple of distinct scopes:\n\n* `container` (default)\n* `pod`\n\nEither option can be selected at a time of the kubelet startup, by setting the\n`topologyManagerScope` in the\n[kubelet configuration file](/docs/tasks/administer-cluster/kubelet-config-file/).\n\n### `container` scope\n\nThe `container` scope is used by default. You can also explicitly set the\n`topologyManagerScope` to `container` in the\n[kubelet configuration file](/docs/tasks/administer-cluster/kubelet-config-file/).\n\nWithin this scope, the Topology Manager performs a number of sequential resource alignments, i.e.,\nfor each container (in a pod) a separate alignment is computed. In other words, there is no notion\nof grouping the containers to a specific set of NUMA nodes, for this particular scope. In effect,\nthe Topology Manager performs an arbitrary alignment of individual containers to NUMA nodes.\n\nThe notion of grouping the containers was endorsed and implemented on purpose in the following\nscope, for example the `pod` scope.",
          "is_needle": true
        },
        {
          "chunk_id": "tasks_administer-cluster_topology-manager_mdsem_4",
          "doc_id": "tasks_administer-cluster_topology-manager",
          "content": "### `pod` scope\n\nTo select the `pod` scope, set `topologyManagerScope` in the\n[kubelet configuration file](/docs/tasks/administer-cluster/kubelet-config-file/) to `pod`.\n\nThis scope allows for grouping all containers in a pod to a common set of NUMA nodes. That is, the\nTopology Manager treats a pod as a whole and attempts to allocate the entire pod (all containers)\nto either a single NUMA node or a common set of NUMA nodes. The following examples illustrate the\nalignments produced by the Topology Manager on different occasions:\n\n* all containers can be and are allocated to a single NUMA node;\n* all containers can be and are allocated to a shared set of NUMA nodes.\n\nThe total amount of particular resource demanded for the entire pod is calculated according to\n[effective requests/limits](/docs/concepts/workloads/pods/init-containers/#resource-sharing-within-containers)\nformula, and thus, this total value is equal to the maximum of:\n\n* the sum of all app container requests,\n* the maximum of init container requests,\n\nfor a resource.\n\nUsing the `pod` scope in tandem with `single-numa-node` Topology Manager policy is specifically\nvaluable for workloads that are latency sensitive or for high-throughput applications that perform\nIPC. By combining both options, you are able to place all containers in a pod onto a single NUMA\nnode; hence, the inter-NUMA communication overhead can be eliminated for that pod.\n\nIn the case of `single-numa-node` policy, a pod is accepted only if a suitable set of NUMA nodes\nis present among possible allocations. Reconsider the example above:\n\n* a set containing only a single NUMA node - it leads to pod being admitted,\n* whereas a set containing more NUMA nodes - it results in pod rejection (because instead of one\n  NUMA node, two or more NUMA nodes are required to satisfy the allocation).\n\nTo recap, the Topology Manager first computes a set of NUMA nodes and then tests it against the Topology\nManager policy, which either leads to the rejection or admission of the pod.",
          "is_needle": true
        },
        {
          "chunk_id": "tasks_administer-cluster_topology-manager_mdsem_7",
          "doc_id": "tasks_administer-cluster_topology-manager",
          "content": "### `restricted` policy {#policy-restricted}\n\nFor each container in a Pod, the kubelet, with `restricted` topology management policy, calls each\nHint Provider to discover their resource availability. Using this information, the Topology\nManager stores the preferred NUMA Node affinity for that container. If the affinity is not\npreferred, the Topology Manager will reject this pod from the node. This will result in a pod entering a\n`Terminated` state with a pod admission failure.\n\nOnce the pod is in a `Terminated` state, the Kubernetes scheduler will **not** attempt to\nreschedule the pod. It is recommended to use a ReplicaSet or Deployment to trigger a redeployment of\nthe pod. An external control loop could be also implemented to trigger a redeployment of pods that\nhave the `Topology Affinity` error.\n\nIf the pod is admitted, the *Hint Providers* can then use this information when making the\nresource allocation decision.",
          "is_needle": true
        },
        {
          "chunk_id": "concepts_policy_resource-quotas_mdsem_10",
          "doc_id": "concepts_policy_resource-quotas",
          "content": "## Quota scopes\n\nEach quota can have an associated set of `scopes`. A quota will only measure usage for a resource if it matches\nthe intersection of enumerated scopes.\n\nWhen a scope is added to the quota, it limits the number of resources it supports to those that pertain to the scope.\nResources specified on the quota outside of the allowed set results in a validation error.\n\nKubernetes {{< skew currentVersion >}} supports the following scopes:\n\n| Scope | Description |\n| ----- | ----------- |\n| [`BestEffort`](#quota-scope-best-effort) | Match pods that have best effort quality of service. |\n| [`CrossNamespacePodAffinity`](#cross-namespace-pod-affinity-scope) | Match pods that have cross-namespace pod [(anti)affinity terms](/docs/concepts/scheduling-eviction/assign-pod-node). |\n| [`NotBestEffort`](#quota-scope-non-best-effort) | Match pods that do not have best effort quality of service. |\n| [`NotTerminating`](#quota-scope-non-terminating) | Match pods where `.spec.activeDeadlineSeconds` is `nil` |\n| [`PriorityClass`](#resource-quota-per-priorityclass) | Match pods that references the specified [priority class](/docs/concepts/scheduling-eviction/pod-priority-preemption). |\n| [`Terminating`](#quota-scope-terminating) | Match pods where `.spec.activeDeadlineSeconds` >= `0` |\n| [`VolumeAttributesClass`](#quota-scope-volume-attributes-class) | Match PersistentVolumeClaims that reference the specified [volume attributes class](/docs/concepts/storage/volume-attributes-classes). |\n\nResourceQuotas with a scope set can also have a optional `scopeSelector` field. You define one or more _match expressions_\nthat specify an `operators` and, if relevant, a set of `values` to match. For example:\n\n```yaml\n  scopeSelector:\n    matchExpressions:\n      - scopeName: BestEffort # Match pods that have best effort quality of service\n        operator: Exists # optional; \"Exists\" is implied for BestEffort scope\n```\n\nThe `scopeSelector` supports the following values in the `operator` field:\n\n* `In`\n* `NotIn`\n* `Exists`\n* `DoesNotExist`\n\nIf the `operator` is `In` or `NotIn`, the `values` field must have at least\none value. For example:\n\n```yaml\n  scopeSelector:\n    matchExpressions:\n      - scopeName: PriorityClass\n        operator: In\n        values:\n          - middle\n```\n\nIf the `operator` is `Exists` or `DoesNotExist`, the `values` field must *NOT* be\nspecified.",
          "is_needle": false
        },
        {
          "chunk_id": "concepts_policy_resource-quotas_mdsem_11",
          "doc_id": "concepts_policy_resource-quotas",
          "content": "### Best effort Pods scope {#quota-scope-best-effort}\n\nThis scope only tracks quota consumed by Pods.\nIt only matches pods that have the [best effort](/docs/concepts/workloads/pods/pod-qos/#besteffort)\n[QoS class](/docs/concepts/workloads/pods/pod-qos/).\n\nThe `operator` for a `scopeSelector` must be `Exists`.\n\n### Not-best-effort Pods scope {#quota-scope-non-best-effort}\n\nThis scope only tracks quota consumed by Pods.\nIt only matches pods that have the [Guaranteed](/docs/concepts/workloads/pods/pod-qos/#guaranteed)\nor [Burstable](/docs/concepts/workloads/pods/pod-qos/#burstable)\n[QoS class](/docs/concepts/workloads/pods/pod-qos/).\n\nThe `operator` for a `scopeSelector` must be `Exists`.",
          "is_needle": false
        }
      ]
    },
    {
      "question_id": "q_010",
      "question": "What are hint providers in topology manager?",
      "expected_answer": "Components like CPU Manager and Device Manager that send and receive topology information through the Topology Manager interface",
      "difficulty": "medium",
      "section": "How topology manager works",
      "latency_ms": 827.4,
      "needle_found": true,
      "retrieved_chunks": [
        {
          "chunk_id": "tasks_administer-cluster_topology-manager_mdsem_12",
          "doc_id": "tasks_administer-cluster_topology-manager",
          "content": "## Pod interactions with topology manager policies\n\nConsider the containers in the following Pod manifest:\n\n```yaml\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n```\n\nThis pod runs in the `BestEffort` QoS class because no resource `requests` or `limits` are specified.\n\n```yaml\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n    resources:\n      limits:\n        memory: \"200Mi\"\n      requests:\n        memory: \"100Mi\"\n```\n\nThis pod runs in the `Burstable` QoS class because requests are less than limits.\n\nIf the selected policy is anything other than `none`, the Topology Manager would consider these Pod\nspecifications. The Topology Manager would consult the Hint Providers to get topology hints.\nIn the case of the `static`, the CPU Manager policy would return default topology hint, because\nthese Pods do not explicitly request CPU resources.\n\n```yaml\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n    resources:\n      limits:\n        memory: \"200Mi\"\n        cpu: \"2\"\n        example.com/device: \"1\"\n      requests:\n        memory: \"200Mi\"\n        cpu: \"2\"\n        example.com/device: \"1\"\n```\n\nThis pod with integer CPU request runs in the `Guaranteed` QoS class because `requests` are equal\nto `limits`.\n\n```yaml\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n    resources:\n      limits:\n        memory: \"200Mi\"\n        cpu: \"300m\"\n        example.com/device: \"1\"\n      requests:\n        memory: \"200Mi\"\n        cpu: \"300m\"\n        example.com/device: \"1\"\n```\n\nThis pod with sharing CPU request runs in the `Guaranteed` QoS class because `requests` are equal\nto `limits`.\n\n```yaml\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n    resources:\n      limits:\n        example.com/deviceA: \"1\"\n        example.com/deviceB: \"1\"\n      requests:\n        example.com/deviceA: \"1\"\n        example.com/deviceB: \"1\"\n```\n\nThis pod runs in the `BestEffort` QoS class because there are no CPU and memory requests.\n\nThe Topology Manager would consider the above pods. The Topology Manager would consult the Hint\nProviders, which are CPU and Device Manager to get topology hints for the pods.\n\nIn the case of the `Guaranteed` pod with integer CPU request, the `static` CPU Manager policy\nwould return topology hints relating to the exclusive CPU and the Device Manager would send back\nhints for the requested device.\n\nIn the case of the `Guaranteed` pod with sharing CPU request, the `static` CPU Manager policy\nwould return default topology hint as there is no exclusive CPU request and the Device Manager\nwould send back hints for the requested device.\n\nIn the above two cases of the `Guaranteed` pod, the `none` CPU Manager policy would return default\ntopology hint.\n\nIn the case of the `BestEffort` pod, the `static` CPU Manager policy would send back the default\ntopology hint as there is no CPU request and the Device Manager would send back the hints for each\nof the requested devices.\n\nUsing this information the Topology Manager calculates the optimal hint for the pod and stores\nthis information, which will be used by the Hint Providers when they are making their resource\nassignments.",
          "is_needle": true
        },
        {
          "chunk_id": "tasks_administer-cluster_topology-manager_mdsem_1",
          "doc_id": "tasks_administer-cluster_topology-manager",
          "content": "## {{% heading \"prerequisites\" %}}\n\n{{< include \"task-tutorial-prereqs.md\" >}} {{< version-check >}}\n\n<!-- steps -->\n\n## How topology manager works\n\nPrior to the introduction of Topology Manager, the CPU and Device Manager in Kubernetes make\nresource allocation decisions independently of each other. This can result in undesirable\nallocations on multiple-socketed systems, and performance/latency sensitive applications will suffer\ndue to these undesirable allocations. Undesirable in this case meaning, for example, CPUs and\ndevices being allocated from different NUMA Nodes, thus incurring additional latency.\n\nThe Topology Manager is a kubelet component, which acts as a source of truth so that other kubelet\ncomponents can make topology aligned resource allocation choices.\n\nThe Topology Manager provides an interface for components, called *Hint Providers*, to send and\nreceive topology information. The Topology Manager has a set of node level policies which are\nexplained below.\n\nThe Topology Manager receives topology information from the *Hint Providers* as a bitmask denoting\nNUMA Nodes available and a preferred allocation indication. The Topology Manager policies perform\na set of operations on the hints provided and converge on the hint determined by the policy to\ngive the optimal result. If an undesirable hint is stored, the preferred field for the hint will be\nset to false. In the current policies preferred is the narrowest preferred mask.\nThe selected hint is stored as part of the Topology Manager. Depending on the policy configured,\nthe pod can be accepted or rejected from the node based on the selected hint.\nThe hint is then stored in the Topology Manager for use by the *Hint Providers* when making the\nresource allocation decisions.\n\nThe flow can be seen in the following diagram.\n\n![topology_manager_flow](/images/docs/topology-manager-flow.png)",
          "is_needle": true
        },
        {
          "chunk_id": "tasks_administer-cluster_topology-manager_mdsem_6",
          "doc_id": "tasks_administer-cluster_topology-manager",
          "content": "### `none` policy {#policy-none}\n\nThis is the default policy and does not perform any topology alignment.\n\n### `best-effort` policy {#policy-best-effort}\n\nFor each container in a Pod, the kubelet, with `best-effort` topology management policy, calls\neach Hint Provider to discover their resource availability. Using this information, the Topology\nManager stores the preferred NUMA Node affinity for that container. If the affinity is not\npreferred, the Topology Manager will store this and admit the pod to the node anyway.\n\nThe *Hint Providers* can then use this information when making the\nresource allocation decision.",
          "is_needle": true
        },
        {
          "chunk_id": "tasks_administer-cluster_topology-manager_mdsem_2",
          "doc_id": "tasks_administer-cluster_topology-manager",
          "content": "## Windows Support\n\n{{< feature-state feature_gate_name=\"WindowsCPUAndMemoryAffinity\" >}}\n\nThe Topology Manager support can be enabled on Windows by using the `WindowsCPUAndMemoryAffinity` feature gate and\nit requires support in the container runtime.\n\n## Topology manager scopes and policies\n\nThe Topology Manager currently:\n\n- aligns Pods of all QoS classes.\n- aligns the requested resources that Hint Provider provides topology hints for.\n\nIf these conditions are met, the Topology Manager will align the requested resources.\n\nIn order to customize how this alignment is carried out, the Topology Manager provides two\ndistinct options: `scope` and `policy`.\n\nThe `scope` defines the granularity at which you would like resource alignment to be performed,\nfor example, at the `pod` or `container` level. And the `policy` defines the actual policy used to\ncarry out the alignment, for example, `best-effort`, `restricted`, and `single-numa-node`.\nDetails on the various `scopes` and `policies` available today can be found below.\n\n{{< note >}}\nTo align CPU resources with other requested resources in a Pod spec, the CPU Manager should be\nenabled and proper CPU Manager policy should be configured on a Node.\nSee [Control CPU Management Policies on the Node](/docs/tasks/administer-cluster/cpu-management-policies/).\n{{< /note >}}\n\n{{< note >}}\nTo align memory (and hugepages) resources with other requested resources in a Pod spec, the Memory\nManager should be enabled and proper Memory Manager policy should be configured on a Node. Refer to\n[Memory Manager](/docs/tasks/administer-cluster/memory-manager/) documentation.\n{{< /note >}}",
          "is_needle": true
        },
        {
          "chunk_id": "tasks_administer-cluster_topology-manager_mdsem_8",
          "doc_id": "tasks_administer-cluster_topology-manager",
          "content": "### `single-numa-node` policy {#policy-single-numa-node}\n\nFor each container in a Pod, the kubelet, with `single-numa-node` topology management policy,\ncalls each Hint Provider to discover their resource availability. Using this information, the\nTopology Manager determines if a single NUMA Node affinity is possible. If it is, Topology\nManager will store this and the *Hint Providers* can then use this information when making the\nresource allocation decision. If, however, this is not possible then the Topology Manager will\nreject the pod from the node. This will result in a pod in a `Terminated` state with a pod\nadmission failure.\n\nOnce the pod is in a `Terminated` state, the Kubernetes scheduler will **not** attempt to\nreschedule the pod. It is recommended to use a Deployment with replicas to trigger a redeployment of\nthe Pod. An external control loop could be also implemented to trigger a redeployment of pods\nthat have the `Topology Affinity` error.",
          "is_needle": true
        }
      ]
    },
    {
      "question_id": "q_011",
      "question": "How do I configure kubelet to prefer NUMA nodes that are closer together?",
      "expected_answer": "Add prefer-closest-numa-nodes=true to Topology Manager policy options. This makes best-effort and restricted policies favor NUMA nodes with shorter distance",
      "difficulty": "hard",
      "section": "prefer-closest-numa-nodes",
      "latency_ms": 1032.9,
      "needle_found": true,
      "retrieved_chunks": [
        {
          "chunk_id": "tasks_administer-cluster_topology-manager_mdsem_4",
          "doc_id": "tasks_administer-cluster_topology-manager",
          "content": "### `pod` scope\n\nTo select the `pod` scope, set `topologyManagerScope` in the\n[kubelet configuration file](/docs/tasks/administer-cluster/kubelet-config-file/) to `pod`.\n\nThis scope allows for grouping all containers in a pod to a common set of NUMA nodes. That is, the\nTopology Manager treats a pod as a whole and attempts to allocate the entire pod (all containers)\nto either a single NUMA node or a common set of NUMA nodes. The following examples illustrate the\nalignments produced by the Topology Manager on different occasions:\n\n* all containers can be and are allocated to a single NUMA node;\n* all containers can be and are allocated to a shared set of NUMA nodes.\n\nThe total amount of particular resource demanded for the entire pod is calculated according to\n[effective requests/limits](/docs/concepts/workloads/pods/init-containers/#resource-sharing-within-containers)\nformula, and thus, this total value is equal to the maximum of:\n\n* the sum of all app container requests,\n* the maximum of init container requests,\n\nfor a resource.\n\nUsing the `pod` scope in tandem with `single-numa-node` Topology Manager policy is specifically\nvaluable for workloads that are latency sensitive or for high-throughput applications that perform\nIPC. By combining both options, you are able to place all containers in a pod onto a single NUMA\nnode; hence, the inter-NUMA communication overhead can be eliminated for that pod.\n\nIn the case of `single-numa-node` policy, a pod is accepted only if a suitable set of NUMA nodes\nis present among possible allocations. Reconsider the example above:\n\n* a set containing only a single NUMA node - it leads to pod being admitted,\n* whereas a set containing more NUMA nodes - it results in pod rejection (because instead of one\n  NUMA node, two or more NUMA nodes are required to satisfy the allocation).\n\nTo recap, the Topology Manager first computes a set of NUMA nodes and then tests it against the Topology\nManager policy, which either leads to the rejection or admission of the pod.",
          "is_needle": true
        },
        {
          "chunk_id": "tasks_administer-cluster_topology-manager_mdsem_7",
          "doc_id": "tasks_administer-cluster_topology-manager",
          "content": "### `restricted` policy {#policy-restricted}\n\nFor each container in a Pod, the kubelet, with `restricted` topology management policy, calls each\nHint Provider to discover their resource availability. Using this information, the Topology\nManager stores the preferred NUMA Node affinity for that container. If the affinity is not\npreferred, the Topology Manager will reject this pod from the node. This will result in a pod entering a\n`Terminated` state with a pod admission failure.\n\nOnce the pod is in a `Terminated` state, the Kubernetes scheduler will **not** attempt to\nreschedule the pod. It is recommended to use a ReplicaSet or Deployment to trigger a redeployment of\nthe pod. An external control loop could be also implemented to trigger a redeployment of pods that\nhave the `Topology Affinity` error.\n\nIf the pod is admitted, the *Hint Providers* can then use this information when making the\nresource allocation decision.",
          "is_needle": true
        },
        {
          "chunk_id": "tasks_administer-cluster_topology-manager_mdsem_3",
          "doc_id": "tasks_administer-cluster_topology-manager",
          "content": "## Topology manager scopes\n\nThe Topology Manager can deal with the alignment of resources in a couple of distinct scopes:\n\n* `container` (default)\n* `pod`\n\nEither option can be selected at a time of the kubelet startup, by setting the\n`topologyManagerScope` in the\n[kubelet configuration file](/docs/tasks/administer-cluster/kubelet-config-file/).\n\n### `container` scope\n\nThe `container` scope is used by default. You can also explicitly set the\n`topologyManagerScope` to `container` in the\n[kubelet configuration file](/docs/tasks/administer-cluster/kubelet-config-file/).\n\nWithin this scope, the Topology Manager performs a number of sequential resource alignments, i.e.,\nfor each container (in a pod) a separate alignment is computed. In other words, there is no notion\nof grouping the containers to a specific set of NUMA nodes, for this particular scope. In effect,\nthe Topology Manager performs an arbitrary alignment of individual containers to NUMA nodes.\n\nThe notion of grouping the containers was endorsed and implemented on purpose in the following\nscope, for example the `pod` scope.",
          "is_needle": true
        },
        {
          "chunk_id": "tasks_administer-cluster_topology-manager_mdsem_6",
          "doc_id": "tasks_administer-cluster_topology-manager",
          "content": "### `none` policy {#policy-none}\n\nThis is the default policy and does not perform any topology alignment.\n\n### `best-effort` policy {#policy-best-effort}\n\nFor each container in a Pod, the kubelet, with `best-effort` topology management policy, calls\neach Hint Provider to discover their resource availability. Using this information, the Topology\nManager stores the preferred NUMA Node affinity for that container. If the affinity is not\npreferred, the Topology Manager will store this and admit the pod to the node anyway.\n\nThe *Hint Providers* can then use this information when making the\nresource allocation decision.",
          "is_needle": true
        },
        {
          "chunk_id": "tasks_administer-cluster_topology-manager_mdsem_10",
          "doc_id": "tasks_administer-cluster_topology-manager",
          "content": "### `prefer-closest-numa-nodes` {#policy-option-prefer-closest-numa-nodes}\n\nThe `prefer-closest-numa-nodes` option is GA since Kubernetes 1.32. In Kubernetes {{< skew currentVersion >}}\nthis policy option is visible by default provided that the `TopologyManagerPolicyOptions`\n[feature gate](/docs/reference/command-line-tools-reference/feature-gates/) is enabled.\n\nThe Topology Manager is not aware by default of NUMA distances, and does not take them into account when making\nPod admission decisions. This limitation surfaces in multi-socket, as well as single-socket multi NUMA systems,\nand can cause significant performance degradation in latency-critical execution and high-throughput applications\nif the Topology Manager decides to align resources on non-adjacent NUMA nodes.\n\nIf you specify the `prefer-closest-numa-nodes` policy option, the `best-effort` and `restricted`\npolicies favor sets of NUMA nodes with shorter distance between them when making admission decisions.\n\nYou can enable this option by adding `prefer-closest-numa-nodes=true` to the Topology Manager policy options.\n\nBy default (without this option), the Topology Manager aligns resources on either a single NUMA node or,\nin the case where more than one NUMA node is required, using the minimum number of NUMA nodes.",
          "is_needle": true
        }
      ]
    },
    {
      "question_id": "q_012",
      "question": "When did prefer-closest-numa-nodes become generally available?",
      "expected_answer": "Kubernetes 1.32",
      "difficulty": "medium",
      "section": "prefer-closest-numa-nodes",
      "latency_ms": 1009.7,
      "needle_found": false,
      "retrieved_chunks": [
        {
          "chunk_id": "reference_node_node-status_mdsem_4",
          "doc_id": "reference_node_node-status",
          "content": "## Info\n\nDescribes general information about the node, such as kernel version, Kubernetes\nversion (kubelet and kube-proxy version), container runtime details, and which\noperating system the node uses.\nThe kubelet gathers this information from the node and publishes it into\nthe Kubernetes API.\n\n## Declared features {#declaredfeatures}\n\n{{< feature-state feature_gate_name=\"NodeDeclaredFeatures\" >}}\n\nThis field lists specific Kubernetes features that are currently enabled on the\nnode's kubelet via [feature gates](/docs/reference/command-line-tools-reference/feature-gates/).\nThe features are reported by the kubelet as a list of strings in the\n`.status.declaredFeatures` field of the Node object.\n\nThis field is intended for newer features under active development; features that\nhave graduated and no longer require a feature gate are considered baseline and\nare not declared in this field. This reflects the enablement of Kubernetes\nfeatures, not the underlying operating system or kernel capabilities of the node.\n\nSee [Node Declared Features](/docs/concepts/scheduling-eviction/node-declared-features/)\nfor more details.",
          "is_needle": false
        },
        {
          "chunk_id": "reference_using-api_deprecation-policy_mdsem_2",
          "doc_id": "reference_using-api_deprecation-policy",
          "content": "## Deprecating parts of the API (continued)\n\n**Rule #3: An API version in a given track may not be deprecated in favor of a less stable API version.**\n\n* GA API versions can replace beta and alpha API versions.\n* Beta API versions can replace earlier beta and alpha API versions, but *may not* replace GA API versions.\n* Alpha API versions can replace earlier alpha API versions, but *may not* replace GA or beta API versions.\n\n**Rule #4a: API lifetime is determined by the API stability level**\n\n* GA API versions may be marked as deprecated, but must not be removed within a major version of Kubernetes\n* Beta API versions are deprecated no more than 9 months or 3 minor releases after introduction (whichever is longer),\n  and are no longer served 9 months or 3 minor releases after deprecation (whichever is longer)\n* Alpha API versions may be removed in any release without prior deprecation notice\n\nThis ensures beta API support covers the [maximum supported version skew of 2 releases](/releases/version-skew-policy/),\nand that APIs don't stagnate on unstable beta versions, accumulating production usage that will be\ndisrupted when support for the beta API ends.\n\n{{< note >}}\nThere are no current plans for a major version revision of Kubernetes that removes GA APIs.\n{{< /note >}}\n\n{{< note >}}\nUntil [#52185](https://github.com/kubernetes/kubernetes/issues/52185) is\nresolved, no API versions that have been persisted to storage may be removed.\nServing REST endpoints for those versions may be disabled (subject to the\ndeprecation timelines in this document), but the API server must remain capable\nof decoding/converting previously persisted data from storage.\n{{< /note >}}\n\n**Rule #4b: The \"preferred\" API version and the \"storage version\" for a given\ngroup may not advance until after a release has been made that supports both the\nnew version and the previous version**\n\nUsers must be able to upgrade to a new release of Kubernetes and then roll back\nto a previous release, without converting anything to the new API version or\nsuffering breakages (unless they explicitly used features only available in the\nnewer version). This is particularly evident in the stored representation of\nobjects.\n\nAll of this is best illustrated by examples. Imagine a Kubernetes release,\nversion X, which introduces a new API group. A new Kubernetes release is made\nevery approximately 4 months (3 per year). The following table describes which\nAPI versions are supported in a series of subsequent releases.",
          "is_needle": false
        },
        {
          "chunk_id": "reference_command-line-tools-reference_kube-apiserver_mdsem_15",
          "doc_id": "reference_command-line-tools-reference_kube-apiserver",
          "content": "## {{% heading \"synopsis\" %}} (continued)\n\nTo skip any prefixing, provide the value '-'.</p></td>\n</tr>\n\n<tr>\n<td colspan=\"2\">--peer-advertise-ip string</td>\n</tr>\n<tr>\n<td></td><td style=\"line-height: 130%; word-wrap: break-word;\"><p>If set and the UnknownVersionInteroperabilityProxy feature gate is enabled, this IP will be used by peer kube-apiservers to proxy requests to this kube-apiserver when the request cannot be handled by the peer due to version skew between the kube-apiservers. This flag is only used in clusters configured with multiple kube-apiservers for high availability.</p></td>\n</tr>\n\n<tr>\n<td colspan=\"2\">--peer-advertise-port string</td>\n</tr>\n<tr>\n<td></td><td style=\"line-height: 130%; word-wrap: break-word;\"><p>If set and the UnknownVersionInteroperabilityProxy feature gate is enabled, this port will be used by peer kube-apiservers to proxy requests to this kube-apiserver when the request cannot be handled by the peer due to version skew between the kube-apiservers. This flag is only used in clusters configured with multiple kube-apiservers for high availability.</p></td>\n</tr>\n\n<tr>\n<td colspan=\"2\">--peer-ca-file string</td>\n</tr>\n<tr>\n<td></td><td style=\"line-height: 130%; word-wrap: break-word;\"><p>If set and the UnknownVersionInteroperabilityProxy feature gate is enabled, this file will be used to verify serving certificates of peer kube-apiservers. This flag is only used in clusters configured with multiple kube-apiservers for high availability.</p></td>\n</tr>\n\n<tr>\n<td colspan=\"2\">--permit-address-sharing</td>\n</tr>\n<tr>\n<td></td><td style=\"line-height: 130%; word-wrap: break-word;\"><p>If true, SO_REUSEADDR will be used when binding the port. This allows binding to wildcard IPs like 0.0.0.0 and specific IPs in parallel, and it avoids waiting for the kernel to release sockets in TIME_WAIT state. [default=false]</p></td>\n</tr>\n\n<tr>\n<td colspan=\"2\">--permit-port-sharing</td>\n</tr>\n<tr>\n<td></td><td style=\"line-height: 130%; word-wrap: break-word;\"><p>If true, SO_REUSEPORT will be used when binding the port, which allows more than one instance to bind on the same address and port. [default=false]</p></td>\n</tr>\n\n<tr>\n<td colspan=\"2\">--profiling&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Default: true</td>\n</tr>\n<tr>\n<td></td><td style=\"line-height: 130%; word-wrap: break-word;\"><p>Enable profiling via web interface host:port/debug/pprof/</p></td>\n</tr>\n\n<tr>\n<td colspan=\"2\">--proxy-client-cert-file string</td>\n</tr>\n<tr>\n<td></td><td style=\"line-height: 130%; word-wrap: break-word;\"><p>Client certificate used to prove the identity of the aggregator or kube-apiserver when it must call out during a request. This includes proxying requests to a user api-server and calling out to webhook admission plugins. It is expected that this cert includes a signature from the CA in the --requestheader-client-ca-file flag. That CA is published in the 'extension-apiserver-authentication' configmap in the kube-system namespace. Components receiving calls from kube-aggregator should use that CA to perform their half of the mutual TLS verification.</p></td>\n</tr>",
          "is_needle": false
        },
        {
          "chunk_id": "reference_using-api_deprecation-policy_mdsem_6",
          "doc_id": "reference_using-api_deprecation-policy",
          "content": "### Future work\n\nOver time, Kubernetes will introduce more fine-grained API versions, at which\npoint these rules will be adjusted as needed.\n\n## Deprecating a flag or CLI\n\nThe Kubernetes system is comprised of several different programs cooperating.\nSometimes, a Kubernetes release might remove flags or CLI commands\n(collectively \"CLI elements\") in these programs. The individual programs\nnaturally sort into two main groups - user-facing and admin-facing programs,\nwhich vary slightly in their deprecation policies. Unless a flag is explicitly\nprefixed or documented as \"alpha\" or \"beta\", it is considered GA.\n\nCLI elements are effectively part of the API to the system, but since they are\nnot versioned in the same way as the REST API, the rules for deprecation are as\nfollows:\n\n**Rule #5a: CLI elements of user-facing components (e.g. kubectl) must function\nafter their announced deprecation for no less than:**\n\n* **GA: 12 months or 2 releases (whichever is longer)**\n* **Beta: 3 months or 1 release (whichever is longer)**\n* **Alpha: 0 releases**\n\n**Rule #5b: CLI elements of admin-facing components (e.g. kubelet) must function\nafter their announced deprecation for no less than:**\n\n* **GA: 6 months or 1 release (whichever is longer)**\n* **Beta: 3 months or 1 release (whichever is longer)**\n* **Alpha: 0 releases**\n\n**Rule #5c: Command line interface (CLI) elements cannot be deprecated in favor of\nless stable CLI elements**\n\nSimilar to the Rule #3 for APIs, if an element of a command line interface is being replaced with an\nalternative implementation, such as by renaming an existing element, or by switching to\nuse configuration sourced from a file\ninstead of a command line argument, that recommended alternative must be of\nthe same or higher stability level.\n\n**Rule #6: Deprecated CLI elements must emit warnings (optionally disable)\nwhen used.**",
          "is_needle": false
        },
        {
          "chunk_id": "concepts_workloads_management_mdsem_7",
          "doc_id": "concepts_workloads_management",
          "content": "# don't wait for rollout to finish, just check the status\n\nkubectl rollout status statefulsets/backing-stateful-component --watch=false\n```\n\nYou can also pause, resume or cancel a rollout.\nVisit [`kubectl rollout`](/docs/reference/kubectl/generated/kubectl_rollout/) to learn more.\n\n## Canary deployments\n\n<!--TODO: make a task out of this for canary deployment, ref #42786-->\n\nAnother scenario where multiple labels are needed is to distinguish deployments of different\nreleases or configurations of the same component. It is common practice to deploy a *canary* of a\nnew application release (specified via image tag in the pod template) side by side with the\nprevious release so that the new release can receive live production traffic before fully rolling\nit out.\n\nFor instance, you can use a `track` label to differentiate different releases.\n\nThe primary, stable release would have a `track` label with value as `stable`:\n\n```none\nname: frontend\nreplicas: 3\n...\nlabels:\n   app: guestbook\n   tier: frontend\n   track: stable\n...\nimage: gb-frontend:v3\n```\n\nand then you can create a new release of the guestbook frontend that carries the `track` label\nwith different value (i.e. `canary`), so that two sets of pods would not overlap:\n\n```none\nname: frontend-canary\nreplicas: 1\n...\nlabels:\n   app: guestbook\n   tier: frontend\n   track: canary\n...\nimage: gb-frontend:v4\n```\n\nThe frontend service would span both sets of replicas by selecting the common subset of their\nlabels (i.e. omitting the `track` label), so that the traffic will be redirected to both\napplications:\n\n```yaml\nselector:\n   app: guestbook\n   tier: frontend\n```\n\nYou can tweak the number of replicas of the stable and canary releases to determine the ratio of\neach release that will receive live production traffic (in this case, 3:1).\nOnce you're confident, you can update the stable track to the new application release and remove\nthe canary one.",
          "is_needle": false
        }
      ]
    },
    {
      "question_id": "q_013",
      "question": "What's the default topology manager policy if I don't set anything?",
      "expected_answer": "none - which does not perform any topology alignment",
      "difficulty": "easy",
      "section": "none policy",
      "latency_ms": 848.1,
      "needle_found": true,
      "retrieved_chunks": [
        {
          "chunk_id": "tasks_administer-cluster_topology-manager_mdsem_6",
          "doc_id": "tasks_administer-cluster_topology-manager",
          "content": "### `none` policy {#policy-none}\n\nThis is the default policy and does not perform any topology alignment.\n\n### `best-effort` policy {#policy-best-effort}\n\nFor each container in a Pod, the kubelet, with `best-effort` topology management policy, calls\neach Hint Provider to discover their resource availability. Using this information, the Topology\nManager stores the preferred NUMA Node affinity for that container. If the affinity is not\npreferred, the Topology Manager will store this and admit the pod to the node anyway.\n\nThe *Hint Providers* can then use this information when making the\nresource allocation decision.",
          "is_needle": true
        },
        {
          "chunk_id": "tasks_administer-cluster_topology-manager_mdsem_9",
          "doc_id": "tasks_administer-cluster_topology-manager",
          "content": "## Topology manager policy options\n\nSupport for the Topology Manager policy options requires `TopologyManagerPolicyOptions`\n[feature gate](/docs/reference/command-line-tools-reference/feature-gates/) to be enabled\n(it is enabled by default).\n\nYou can toggle groups of options on and off based upon their maturity level using the following feature gates:\n\n* `TopologyManagerPolicyBetaOptions` default enabled. Enable to show beta-level options.\n* `TopologyManagerPolicyAlphaOptions` default disabled. Enable to show alpha-level options.\n\nYou will still have to enable each option using the `TopologyManagerPolicyOptions` kubelet option.",
          "is_needle": true
        },
        {
          "chunk_id": "tasks_administer-cluster_topology-manager_mdsem_10",
          "doc_id": "tasks_administer-cluster_topology-manager",
          "content": "### `prefer-closest-numa-nodes` {#policy-option-prefer-closest-numa-nodes}\n\nThe `prefer-closest-numa-nodes` option is GA since Kubernetes 1.32. In Kubernetes {{< skew currentVersion >}}\nthis policy option is visible by default provided that the `TopologyManagerPolicyOptions`\n[feature gate](/docs/reference/command-line-tools-reference/feature-gates/) is enabled.\n\nThe Topology Manager is not aware by default of NUMA distances, and does not take them into account when making\nPod admission decisions. This limitation surfaces in multi-socket, as well as single-socket multi NUMA systems,\nand can cause significant performance degradation in latency-critical execution and high-throughput applications\nif the Topology Manager decides to align resources on non-adjacent NUMA nodes.\n\nIf you specify the `prefer-closest-numa-nodes` policy option, the `best-effort` and `restricted`\npolicies favor sets of NUMA nodes with shorter distance between them when making admission decisions.\n\nYou can enable this option by adding `prefer-closest-numa-nodes=true` to the Topology Manager policy options.\n\nBy default (without this option), the Topology Manager aligns resources on either a single NUMA node or,\nin the case where more than one NUMA node is required, using the minimum number of NUMA nodes.",
          "is_needle": true
        },
        {
          "chunk_id": "tasks_administer-cluster_topology-manager_mdsem_5",
          "doc_id": "tasks_administer-cluster_topology-manager",
          "content": "## Topology manager policies\n\nThe Topology Manager supports four allocation policies. You can set a policy via a kubelet flag,\n`--topology-manager-policy`. There are four supported policies:\n\n* `none` (default)\n* `best-effort`\n* `restricted`\n* `single-numa-node`\n\n{{< note >}}\nIf the Topology Manager is configured with the **pod** scope, the container, which is considered by\nthe policy, is reflecting requirements of the entire pod, and thus each container from the pod\nwill result with **the same** topology alignment decision.\n{{< /note >}}",
          "is_needle": true
        },
        {
          "chunk_id": "tasks_administer-cluster_topology-manager_mdsem_1",
          "doc_id": "tasks_administer-cluster_topology-manager",
          "content": "## {{% heading \"prerequisites\" %}}\n\n{{< include \"task-tutorial-prereqs.md\" >}} {{< version-check >}}\n\n<!-- steps -->\n\n## How topology manager works\n\nPrior to the introduction of Topology Manager, the CPU and Device Manager in Kubernetes make\nresource allocation decisions independently of each other. This can result in undesirable\nallocations on multiple-socketed systems, and performance/latency sensitive applications will suffer\ndue to these undesirable allocations. Undesirable in this case meaning, for example, CPUs and\ndevices being allocated from different NUMA Nodes, thus incurring additional latency.\n\nThe Topology Manager is a kubelet component, which acts as a source of truth so that other kubelet\ncomponents can make topology aligned resource allocation choices.\n\nThe Topology Manager provides an interface for components, called *Hint Providers*, to send and\nreceive topology information. The Topology Manager has a set of node level policies which are\nexplained below.\n\nThe Topology Manager receives topology information from the *Hint Providers* as a bitmask denoting\nNUMA Nodes available and a preferred allocation indication. The Topology Manager policies perform\na set of operations on the hints provided and converge on the hint determined by the policy to\ngive the optimal result. If an undesirable hint is stored, the preferred field for the hint will be\nset to false. In the current policies preferred is the narrowest preferred mask.\nThe selected hint is stored as part of the Topology Manager. Depending on the policy configured,\nthe pod can be accepted or rejected from the node based on the selected hint.\nThe hint is then stored in the Topology Manager for use by the *Hint Providers* when making the\nresource allocation decisions.\n\nThe flow can be seen in the following diagram.\n\n![topology_manager_flow](/images/docs/topology-manager-flow.png)",
          "is_needle": true
        }
      ]
    },
    {
      "question_id": "q_014",
      "question": "Pod was scheduled but then failed on the node, says something about topology. Is that a bug?",
      "expected_answer": "No, this is a known limitation. The scheduler is not topology-aware, so it's possible to be scheduled on a node and then fail due to the Topology Manager",
      "difficulty": "hard",
      "section": "Known limitations",
      "latency_ms": 1045.6,
      "needle_found": true,
      "retrieved_chunks": [
        {
          "chunk_id": "tasks_administer-cluster_topology-manager_mdsem_7",
          "doc_id": "tasks_administer-cluster_topology-manager",
          "content": "### `restricted` policy {#policy-restricted}\n\nFor each container in a Pod, the kubelet, with `restricted` topology management policy, calls each\nHint Provider to discover their resource availability. Using this information, the Topology\nManager stores the preferred NUMA Node affinity for that container. If the affinity is not\npreferred, the Topology Manager will reject this pod from the node. This will result in a pod entering a\n`Terminated` state with a pod admission failure.\n\nOnce the pod is in a `Terminated` state, the Kubernetes scheduler will **not** attempt to\nreschedule the pod. It is recommended to use a ReplicaSet or Deployment to trigger a redeployment of\nthe pod. An external control loop could be also implemented to trigger a redeployment of pods that\nhave the `Topology Affinity` error.\n\nIf the pod is admitted, the *Hint Providers* can then use this information when making the\nresource allocation decision.",
          "is_needle": true
        },
        {
          "chunk_id": "tasks_debug_debug-cluster_topology_mdsem_1",
          "doc_id": "tasks_debug_debug-cluster_topology",
          "content": "## Sources of troubleshooting information\n\nYou can use the following means to troubleshoot the reason why a pod could not be deployed or\nbecame rejected at a node, in the context of topology management:\n\n- _Pod status_ - indicates topology affinity errors\n- _system logs_ - include valuable information for debugging; for example, about generated hints\n- _kubelet state file_ - the dump of internal state of the Memory Manager\n  (including the _node map_ and _memory maps_)\n- You can use the [device plugin resource API](#device-plugin-resource-api)\n  to retrieve information about the memory reserved for containers",
          "is_needle": false
        },
        {
          "chunk_id": "tasks_administer-cluster_topology-manager_mdsem_8",
          "doc_id": "tasks_administer-cluster_topology-manager",
          "content": "### `single-numa-node` policy {#policy-single-numa-node}\n\nFor each container in a Pod, the kubelet, with `single-numa-node` topology management policy,\ncalls each Hint Provider to discover their resource availability. Using this information, the\nTopology Manager determines if a single NUMA Node affinity is possible. If it is, Topology\nManager will store this and the *Hint Providers* can then use this information when making the\nresource allocation decision. If, however, this is not possible then the Topology Manager will\nreject the pod from the node. This will result in a pod in a `Terminated` state with a pod\nadmission failure.\n\nOnce the pod is in a `Terminated` state, the Kubernetes scheduler will **not** attempt to\nreschedule the pod. It is recommended to use a Deployment with replicas to trigger a redeployment of\nthe Pod. An external control loop could be also implemented to trigger a redeployment of pods\nthat have the `Topology Affinity` error.",
          "is_needle": true
        },
        {
          "chunk_id": "concepts_scheduling-eviction_kube-scheduler_mdsem_3",
          "doc_id": "concepts_scheduling-eviction_kube-scheduler",
          "content": "## {{% heading \"whatsnext\" %}}\n\n* Read about [scheduler performance tuning](/docs/concepts/scheduling-eviction/scheduler-perf-tuning/)\n* Read about [Pod topology spread constraints](/docs/concepts/scheduling-eviction/topology-spread-constraints/)\n* Read the [reference documentation](/docs/reference/command-line-tools-reference/kube-scheduler/) for kube-scheduler\n* Read the [kube-scheduler config (v1)](/docs/reference/config-api/kube-scheduler-config.v1/) reference\n* Learn about [configuring multiple schedulers](/docs/tasks/extend-kubernetes/configure-multiple-schedulers/)\n* Learn about [topology management policies](/docs/tasks/administer-cluster/topology-manager/)\n* Learn about [Pod Overhead](/docs/concepts/scheduling-eviction/pod-overhead/)\n* Learn about scheduling of Pods that use volumes in:\n  * [Volume Topology Support](/docs/concepts/storage/storage-classes/#volume-binding-mode)\n  * [Storage Capacity Tracking](/docs/concepts/storage/storage-capacity/)\n  * [Node-specific Volume Limits](/docs/concepts/storage/storage-limits/)",
          "is_needle": false
        },
        {
          "chunk_id": "concepts_scheduling-eviction_kube-scheduler_mdsem_1",
          "doc_id": "concepts_scheduling-eviction_kube-scheduler",
          "content": "## kube-scheduler\n\n[kube-scheduler](/docs/reference/command-line-tools-reference/kube-scheduler/)\nis the default scheduler for Kubernetes and runs as part of the\n{{< glossary_tooltip text=\"control plane\" term_id=\"control-plane\" >}}.\nkube-scheduler is designed so that, if you want and need to, you can\nwrite your own scheduling component and use that instead.\n\nKube-scheduler selects an optimal node to run newly created or not yet\nscheduled (unscheduled) pods. Since containers in pods - and pods themselves -\ncan have different requirements, the scheduler filters out any nodes that\ndon't meet a Pod's specific scheduling needs. Alternatively, the API lets\nyou specify a node for a Pod when you create it, but this is unusual\nand is only done in special cases.\n\nIn a cluster, Nodes that meet the scheduling requirements for a Pod\nare called _feasible_ nodes. If none of the nodes are suitable, the pod\nremains unscheduled until the scheduler is able to place it.\n\nThe scheduler finds feasible Nodes for a Pod and then runs a set of\nfunctions to score the feasible Nodes and picks a Node with the highest\nscore among the feasible ones to run the Pod. The scheduler then notifies\nthe API server about this decision in a process called _binding_.\n\nFactors that need to be taken into account for scheduling decisions include\nindividual and collective resource requirements, hardware / software /\npolicy constraints, affinity and anti-affinity specifications, data\nlocality, inter-workload interference, and so on.",
          "is_needle": false
        }
      ]
    },
    {
      "question_id": "q_015",
      "question": "What QoS class does my pod need to be for topology hints to work?",
      "expected_answer": "Topology Manager aligns Pods of all QoS classes (BestEffort, Burstable, Guaranteed)",
      "difficulty": "medium",
      "section": "Topology manager scopes and policies",
      "latency_ms": 1412.1,
      "needle_found": true,
      "retrieved_chunks": [
        {
          "chunk_id": "tasks_administer-cluster_topology-manager_mdsem_12",
          "doc_id": "tasks_administer-cluster_topology-manager",
          "content": "## Pod interactions with topology manager policies\n\nConsider the containers in the following Pod manifest:\n\n```yaml\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n```\n\nThis pod runs in the `BestEffort` QoS class because no resource `requests` or `limits` are specified.\n\n```yaml\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n    resources:\n      limits:\n        memory: \"200Mi\"\n      requests:\n        memory: \"100Mi\"\n```\n\nThis pod runs in the `Burstable` QoS class because requests are less than limits.\n\nIf the selected policy is anything other than `none`, the Topology Manager would consider these Pod\nspecifications. The Topology Manager would consult the Hint Providers to get topology hints.\nIn the case of the `static`, the CPU Manager policy would return default topology hint, because\nthese Pods do not explicitly request CPU resources.\n\n```yaml\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n    resources:\n      limits:\n        memory: \"200Mi\"\n        cpu: \"2\"\n        example.com/device: \"1\"\n      requests:\n        memory: \"200Mi\"\n        cpu: \"2\"\n        example.com/device: \"1\"\n```\n\nThis pod with integer CPU request runs in the `Guaranteed` QoS class because `requests` are equal\nto `limits`.\n\n```yaml\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n    resources:\n      limits:\n        memory: \"200Mi\"\n        cpu: \"300m\"\n        example.com/device: \"1\"\n      requests:\n        memory: \"200Mi\"\n        cpu: \"300m\"\n        example.com/device: \"1\"\n```\n\nThis pod with sharing CPU request runs in the `Guaranteed` QoS class because `requests` are equal\nto `limits`.\n\n```yaml\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n    resources:\n      limits:\n        example.com/deviceA: \"1\"\n        example.com/deviceB: \"1\"\n      requests:\n        example.com/deviceA: \"1\"\n        example.com/deviceB: \"1\"\n```\n\nThis pod runs in the `BestEffort` QoS class because there are no CPU and memory requests.\n\nThe Topology Manager would consider the above pods. The Topology Manager would consult the Hint\nProviders, which are CPU and Device Manager to get topology hints for the pods.\n\nIn the case of the `Guaranteed` pod with integer CPU request, the `static` CPU Manager policy\nwould return topology hints relating to the exclusive CPU and the Device Manager would send back\nhints for the requested device.\n\nIn the case of the `Guaranteed` pod with sharing CPU request, the `static` CPU Manager policy\nwould return default topology hint as there is no exclusive CPU request and the Device Manager\nwould send back hints for the requested device.\n\nIn the above two cases of the `Guaranteed` pod, the `none` CPU Manager policy would return default\ntopology hint.\n\nIn the case of the `BestEffort` pod, the `static` CPU Manager policy would send back the default\ntopology hint as there is no CPU request and the Device Manager would send back the hints for each\nof the requested devices.\n\nUsing this information the Topology Manager calculates the optimal hint for the pod and stores\nthis information, which will be used by the Hint Providers when they are making their resource\nassignments.",
          "is_needle": true
        },
        {
          "chunk_id": "concepts_security_multi-tenancy_mdsem_13",
          "doc_id": "concepts_security_multi-tenancy",
          "content": "### Quality-of-Service (QoS) {#qos}\n\nWhen you\u2019re running a SaaS application, you may want the ability to offer different\nQuality-of-Service (QoS) tiers of service to different tenants. For example, you may have freemium\nservice that comes with fewer performance guarantees and features and a for-fee service tier with\nspecific performance guarantees. Fortunately, there are several Kubernetes constructs that can\nhelp you accomplish this within a shared cluster, including network QoS, storage classes, and pod\npriority and preemption. The idea with each of these is to provide tenants with the quality of\nservice that they paid for. Let\u2019s start by looking at networking QoS.\n\nTypically, all pods on a node share a network interface. Without network QoS, some pods may\nconsume an unfair share of the available bandwidth at the expense of other pods.\nThe Kubernetes [bandwidth plugin](https://www.cni.dev/plugins/current/meta/bandwidth/) creates an\n[extended resource](/docs/concepts/configuration/manage-resources-containers/#extended-resources)\nfor networking that allows you to use Kubernetes resources constructs, i.e. requests/limits, to\napply rate limits to pods by using Linux tc queues.\nBe aware that the plugin is considered experimental as per the\n[Network Plugins](/docs/concepts/extend-kubernetes/compute-storage-net/network-plugins/#support-traffic-shaping)\ndocumentation and should be thoroughly tested before use in production environments.\n\nFor storage QoS, you will likely want to create different storage classes or profiles with\ndifferent performance characteristics. Each storage profile can be associated with a different\ntier of service that is optimized for different workloads such IO, redundancy, or throughput.\nAdditional logic might be necessary to allow the tenant to associate the appropriate storage\nprofile with their workload.\n\nFinally, there\u2019s [pod priority and preemption](/docs/concepts/scheduling-eviction/pod-priority-preemption/)\nwhere you can assign priority values to pods. When scheduling pods, the scheduler will try\nevicting pods with lower priority when there are insufficient resources to schedule pods that are\nassigned a higher priority. If you have a use case where tenants have different service tiers in a\nshared cluster e.g. free and paid, you may want to give higher priority to certain tiers using\nthis feature.",
          "is_needle": false
        },
        {
          "chunk_id": "tasks_debug_debug-cluster_topology_mdsem_1",
          "doc_id": "tasks_debug_debug-cluster_topology",
          "content": "## Sources of troubleshooting information\n\nYou can use the following means to troubleshoot the reason why a pod could not be deployed or\nbecame rejected at a node, in the context of topology management:\n\n- _Pod status_ - indicates topology affinity errors\n- _system logs_ - include valuable information for debugging; for example, about generated hints\n- _kubelet state file_ - the dump of internal state of the Memory Manager\n  (including the _node map_ and _memory maps_)\n- You can use the [device plugin resource API](#device-plugin-resource-api)\n  to retrieve information about the memory reserved for containers",
          "is_needle": false
        },
        {
          "chunk_id": "reference_kubernetes-api_service-resources_endpoint-slice-v1_mdsem_2",
          "doc_id": "reference_kubernetes-api_service-resources_endpoint-slice-v1",
          "content": "## EndpointSlice {#EndpointSlice} (continued)\n\n- **endpoints.conditions.serving** (boolean)\n\nserving indicates that this endpoint is able to receive traffic, according to whatever system is managing the endpoint. For endpoints backed by pods, the EndpointSlice controller will mark the endpoint as serving if the pod's Ready condition is True. A nil value should be interpreted as \"true\".\n\n- **endpoints.conditions.terminating** (boolean)\n\nterminating indicates that this endpoint is terminating. A nil value should be interpreted as \"false\".\n\n- **endpoints.deprecatedTopology** (map[string]string)\n\ndeprecatedTopology contains topology information part of the v1beta1 API. This field is deprecated, and will be removed when the v1beta1 API is removed (no sooner than kubernetes v1.24).  While this field can hold values, it is not writable through the v1 API, and any attempts to write to it will be silently ignored. Topology information can be found in the zone and nodeName fields instead.\n\n- **endpoints.hints** (EndpointHints)\n\nhints contains information associated with how an endpoint should be consumed.\n\n<a name=\"EndpointHints\"></a>\n    *EndpointHints provides hints describing how an endpoint should be consumed.*\n\n- **endpoints.hints.forNodes** ([]ForNode)\n\n*Atomic: will be replaced during a merge*\n      \n      forNodes indicates the node(s) this endpoint should be consumed by when using topology aware routing. May contain a maximum of 8 entries.\n\n<a name=\"ForNode\"></a>\n      *ForNode provides information about which nodes should consume this endpoint.*\n\n- **endpoints.hints.forNodes.name** (string), required\n\nname represents the name of the node.\n\n- **endpoints.hints.forZones** ([]ForZone)\n\n*Atomic: will be replaced during a merge*\n      \n      forZones indicates the zone(s) this endpoint should be consumed by when using topology aware routing. May contain a maximum of 8 entries.\n\n<a name=\"ForZone\"></a>\n      *ForZone provides information about which zones should consume this endpoint.*\n\n- **endpoints.hints.forZones.name** (string), required\n\nname represents the name of the zone.\n\n- **endpoints.hostname** (string)\n\nhostname of this endpoint. This field may be used by consumers of endpoints to distinguish endpoints from each other (e.g. in DNS names). Multiple endpoints which use the same hostname should be considered fungible (e.g. multiple A values in DNS). Must be lowercase and pass DNS Label (RFC 1123) validation.\n\n- **endpoints.nodeName** (string)\n\nnodeName represents the name of the Node hosting this endpoint. This can be used to determine endpoints local to a Node.\n\n- **endpoints.targetRef** (<a href=\"{{< ref \"../common-definitions/object-reference#ObjectReference\" >}}\">ObjectReference</a>)\n\ntargetRef is a reference to a Kubernetes object that represents this endpoint.\n\n- **endpoints.zone** (string)\n\nzone is the name of the Zone this endpoint exists in.\n\n- **ports** ([]EndpointPort)",
          "is_needle": false
        },
        {
          "chunk_id": "contribute_style_diagram-guide_mdsem_9",
          "doc_id": "contribute_style_diagram-guide",
          "content": "### Example 1 - Pod topology spread constraints\n\nFigure 6 shows the diagram appearing in the\n[Pod topology spread constraints](/docs/concepts/scheduling-eviction/topology-spread-constraints/#node-labels)\npage.\n\n{{< mermaid >}}\n    graph TB\n    subgraph \"zoneB\"\n    n3(Node3)\n    n4(Node4)\n    end\n    subgraph \"zoneA\"\n    n1(Node1)\n    n2(Node2)\n    end\n    \n    classDef plain fill:#ddd,stroke:#fff,stroke-width:4px,color:#000;\n    classDef k8s fill:#326ce5,stroke:#fff,stroke-width:4px,color:#fff;\n    classDef cluster fill:#fff,stroke:#bbb,stroke-width:2px,color:#326ce5;\n    class n1,n2,n3,n4 k8s;\n    class zoneA,zoneB cluster;\n\nclick n3 \"https://mermaid-js.github.io/mermaid-live-editor/edit/#eyJjb2RlIjoiZ3JhcGggVEJcbiAgICBzdWJncmFwaCBcInpvbmVCXCJcbiAgICAgICAgbjMoTm9kZTMpXG4gICAgICAgIG40KE5vZGU0KVxuICAgIGVuZFxuICAgIHN1YmdyYXBoIFwiem9uZUFcIlxuICAgICAgICBuMShOb2RlMSlcbiAgICAgICAgbjIoTm9kZTIpXG4gICAgZW5kXG5cbiAgICBjbGFzc0RlZiBwbGFpbiBmaWxsOiNkZGQsc3Ryb2tlOiNmZmYsc3Ryb2tlLXdpZHRoOjRweCxjb2xvcjojMDAwO1xuICAgIGNsYXNzRGVmIGs4cyBmaWxsOiMzMjZjZTUsc3Ryb2tlOiNmZmYsc3Ryb2tlLXdpZHRoOjRweCxjb2xvcjojZmZmO1xuICAgIGNsYXNzRGVmIGNsdXN0ZXIgZmlsbDojZmZmLHN0cm9rZTojYmJiLHN0cm9rZS13aWR0aDoycHgsY29sb3I6IzMyNmNlNTtcbiAgICBjbGFzcyBuMSxuMixuMyxuNCBrOHM7XG4gICAgY2xhc3Mgem9uZUEsem9uZUIgY2x1c3RlcjtcbiIsIm1lcm1haWQiOiJ7XG4gIFwidGhlbWVcIjogXCJkZWZhdWx0XCJcbn0iLCJ1cGRhdGVFZGl0b3IiOmZhbHNlLCJhdXRvU3luYyI6dHJ1ZSwidXBkYXRlRGlhZ3JhbSI6dHJ1ZX0\" _blank\n\nclick n4 \"https://mermaid-js.github.io/mermaid-live-editor/edit/#eyJjb2RlIjoiZ3JhcGggVEJcbiAgICBzdWJncmFwaCBcInpvbmVCXCJcbiAgICAgICAgbjMoTm9kZTMpXG4gICAgICAgIG40KE5vZGU0KVxuICAgIGVuZFxuICAgIHN1YmdyYXBoIFwiem9uZUFcIlxuICAgICAgICBuMShOb2RlMSlcbiAgICAgICAgbjIoTm9kZTIpXG4gICAgZW5kXG5cbiAgICBjbGFzc0RlZiBwbGFpbiBmaWxsOiNkZGQsc3Ryb2tlOiNmZmYsc3Ryb2tlLXdpZHRoOjRweCxjb2xvcjojMDAwO1xuICAgIGNsYXNzRGVmIGs4cyBmaWxsOiMzMjZjZTUsc3Ryb2tlOiNmZmYsc3Ryb2tlLXdpZHRoOjRweCxjb2xvcjojZmZmO1xuICAgIGNsYXNzRGVmIGNsdXN0ZXIgZmlsbDojZmZmLHN0cm9rZTojYmJiLHN0cm9rZS13aWR0aDoycHgsY29sb3I6IzMyNmNlNTtcbiAgICBjbGFzcyBuMSxuMixuMyxuNCBrOHM7XG4gICAgY2xhc3Mgem9uZUEsem9uZUIgY2x1c3RlcjtcbiIsIm1lcm1haWQiOiJ7XG4gIFwidGhlbWVcIjogXCJkZWZhdWx0XCJcbn0iLCJ1cGRhdGVFZGl0b3IiOmZhbHNlLCJhdXRvU3luYyI6dHJ1ZSwidXBkYXRlRGlhZ3JhbSI6dHJ1ZX0\" _blank\n\nclick n1 \"https://mermaid-js.github.io/mermaid-live-editor/edit/#eyJjb2RlIjoiZ3JhcGggVEJcbiAgICBzdWJncmFwaCBcInpvbmVCXCJcbiAgICAgICAgbjMoTm9kZTMpXG4gICAgICAgIG40KE5vZGU0KVxuICAgIGVuZFxuICAgIHN1YmdyYXBoIFwiem9uZUFcIlxuICAgICAgICBuMShOb2RlMSlcbiAgICAgICAgbjIoTm9kZTIpXG4gICAgZW5kXG5cbiAgICBjbGFzc0RlZiBwbGFpbiBmaWxsOiNkZGQsc3Ryb2tlOiNmZmYsc3Ryb2tlLXdpZHRoOjRweCxjb2xvcjojMDAwO1xuICAgIGNsYXNzRGVmIGs4cyBmaWxsOiMzMjZjZTUsc3Ryb2tlOiNmZmYsc3Ryb2tlLXdpZHRoOjRweCxjb2xvcjojZmZmO1xuICAgIGNsYXNzRGVmIGNsdXN0ZXIgZmlsbDojZmZmLHN0cm9rZTojYmJiLHN0cm9rZS13aWR0aDoycHgsY29sb3I6IzMyNmNlNTtcbiAgICBjbGFzcyBuMSxuMixuMyxuNCBrOHM7XG4gICAgY2xhc3Mgem9uZUEsem9uZUIgY2x1c3RlcjtcbiIsIm1lcm1haWQiOiJ7XG4gIFwidGhlbWVcIjogXCJkZWZhdWx0XCJcbn0iLCJ1cGRhdGVFZGl0b3IiOmZhbHNlLCJhdXRvU3luYyI6dHJ1ZSwidXBkYXRlRGlhZ3JhbSI6dHJ1ZX0\" _blank\n\nclick n2 \"https://mermaid-js.github.io/mermaid-live-editor/edit/#eyJjb2RlIjoiZ3JhcGggVEJcbiAgICBzdWJncmFwaCBcInpvbmVCXCJcbiAgICAgICAgbjMoTm9kZTMpXG4gICAgICAgIG40KE5vZGU0KVxuICAgIGVuZFxuICAgIHN1YmdyYXBoIFwiem9uZUFcIlxuICAgICAgICBuMShOb2RlMSlcbiAgICAgICAgbjIoTm9kZTIpXG4gICAgZW5kXG5cbiAgICBjbGFzc0RlZiBwbGFpbiBmaWxsOiNkZGQsc3Ryb2tlOiNmZmYsc3Ryb2tlLXdpZHRoOjRweCxjb2xvcjojMDAwO1xuICAgIGNsYXNzRGVmIGs4cyBmaWxsOiMzMjZjZTUsc3Ryb2tlOiNmZmYsc3Ryb2tlLXdpZHRoOjRweCxjb2xvcjojZmZmO1xuICAgIGNsYXNzRGVmIGNsdXN0ZXIgZmlsbDojZmZmLHN0cm9rZTojYmJiLHN0cm9rZS13aWR0aDoycHgsY29sb3I6IzMyNmNlNTtcbiAgICBjbGFzcyBuMSxuMixuMyxuNCBrOHM7XG4gICAgY2xhc3Mgem9uZUEsem9uZUIgY2x1c3RlcjtcbiIsIm1lcm1haWQiOiJ7XG4gIFwidGhlbWVcIjogXCJkZWZhdWx0XCJcbn0iLCJ1cGRhdGVFZGl0b3IiOmZhbHNlLCJhdXRvU3luYyI6dHJ1ZSwidXBkYXRlRGlhZ3JhbSI6dHJ1ZX0\" _blank\n\n{{< /mermaid >}}\n\nFigure 6. Pod Topology Spread Constraints.\n\nCode block:\n\n```text\ngraph TB\n   subgraph \"zoneB\"\n       n3(Node3)\n       n4(Node4)\n   end\n   subgraph \"zoneA\"\n       n1(Node1)\n       n2(Node2)\n   end\n \n   classDef plain fill:#ddd,stroke:#fff,stroke-width:4px,color:#000;\n   classDef k8s fill:#326ce5,stroke:#fff,stroke-width:4px,color:#fff;\n   classDef cluster fill:#fff,stroke:#bbb,stroke-width:2px,color:#326ce5;\n   class n1,n2,n3,n4 k8s;\n   class zoneA,zoneB cluster;\n```",
          "is_needle": false
        }
      ]
    },
    {
      "question_id": "q_016",
      "question": "How to set topology scope to pod level in kubelet?",
      "expected_answer": "Set topologyManagerScope to pod in the kubelet configuration file",
      "difficulty": "easy",
      "section": "pod scope",
      "latency_ms": 959.8,
      "needle_found": true,
      "retrieved_chunks": [
        {
          "chunk_id": "tasks_administer-cluster_topology-manager_mdsem_3",
          "doc_id": "tasks_administer-cluster_topology-manager",
          "content": "## Topology manager scopes\n\nThe Topology Manager can deal with the alignment of resources in a couple of distinct scopes:\n\n* `container` (default)\n* `pod`\n\nEither option can be selected at a time of the kubelet startup, by setting the\n`topologyManagerScope` in the\n[kubelet configuration file](/docs/tasks/administer-cluster/kubelet-config-file/).\n\n### `container` scope\n\nThe `container` scope is used by default. You can also explicitly set the\n`topologyManagerScope` to `container` in the\n[kubelet configuration file](/docs/tasks/administer-cluster/kubelet-config-file/).\n\nWithin this scope, the Topology Manager performs a number of sequential resource alignments, i.e.,\nfor each container (in a pod) a separate alignment is computed. In other words, there is no notion\nof grouping the containers to a specific set of NUMA nodes, for this particular scope. In effect,\nthe Topology Manager performs an arbitrary alignment of individual containers to NUMA nodes.\n\nThe notion of grouping the containers was endorsed and implemented on purpose in the following\nscope, for example the `pod` scope.",
          "is_needle": true
        },
        {
          "chunk_id": "tasks_administer-cluster_topology-manager_mdsem_4",
          "doc_id": "tasks_administer-cluster_topology-manager",
          "content": "### `pod` scope\n\nTo select the `pod` scope, set `topologyManagerScope` in the\n[kubelet configuration file](/docs/tasks/administer-cluster/kubelet-config-file/) to `pod`.\n\nThis scope allows for grouping all containers in a pod to a common set of NUMA nodes. That is, the\nTopology Manager treats a pod as a whole and attempts to allocate the entire pod (all containers)\nto either a single NUMA node or a common set of NUMA nodes. The following examples illustrate the\nalignments produced by the Topology Manager on different occasions:\n\n* all containers can be and are allocated to a single NUMA node;\n* all containers can be and are allocated to a shared set of NUMA nodes.\n\nThe total amount of particular resource demanded for the entire pod is calculated according to\n[effective requests/limits](/docs/concepts/workloads/pods/init-containers/#resource-sharing-within-containers)\nformula, and thus, this total value is equal to the maximum of:\n\n* the sum of all app container requests,\n* the maximum of init container requests,\n\nfor a resource.\n\nUsing the `pod` scope in tandem with `single-numa-node` Topology Manager policy is specifically\nvaluable for workloads that are latency sensitive or for high-throughput applications that perform\nIPC. By combining both options, you are able to place all containers in a pod onto a single NUMA\nnode; hence, the inter-NUMA communication overhead can be eliminated for that pod.\n\nIn the case of `single-numa-node` policy, a pod is accepted only if a suitable set of NUMA nodes\nis present among possible allocations. Reconsider the example above:\n\n* a set containing only a single NUMA node - it leads to pod being admitted,\n* whereas a set containing more NUMA nodes - it results in pod rejection (because instead of one\n  NUMA node, two or more NUMA nodes are required to satisfy the allocation).\n\nTo recap, the Topology Manager first computes a set of NUMA nodes and then tests it against the Topology\nManager policy, which either leads to the rejection or admission of the pod.",
          "is_needle": true
        },
        {
          "chunk_id": "tasks_administer-cluster_topology-manager_mdsem_5",
          "doc_id": "tasks_administer-cluster_topology-manager",
          "content": "## Topology manager policies\n\nThe Topology Manager supports four allocation policies. You can set a policy via a kubelet flag,\n`--topology-manager-policy`. There are four supported policies:\n\n* `none` (default)\n* `best-effort`\n* `restricted`\n* `single-numa-node`\n\n{{< note >}}\nIf the Topology Manager is configured with the **pod** scope, the container, which is considered by\nthe policy, is reflecting requirements of the entire pod, and thus each container from the pod\nwill result with **the same** topology alignment decision.\n{{< /note >}}",
          "is_needle": true
        },
        {
          "chunk_id": "tasks_administer-cluster_topology-manager_mdsem_7",
          "doc_id": "tasks_administer-cluster_topology-manager",
          "content": "### `restricted` policy {#policy-restricted}\n\nFor each container in a Pod, the kubelet, with `restricted` topology management policy, calls each\nHint Provider to discover their resource availability. Using this information, the Topology\nManager stores the preferred NUMA Node affinity for that container. If the affinity is not\npreferred, the Topology Manager will reject this pod from the node. This will result in a pod entering a\n`Terminated` state with a pod admission failure.\n\nOnce the pod is in a `Terminated` state, the Kubernetes scheduler will **not** attempt to\nreschedule the pod. It is recommended to use a ReplicaSet or Deployment to trigger a redeployment of\nthe pod. An external control loop could be also implemented to trigger a redeployment of pods that\nhave the `Topology Affinity` error.\n\nIf the pod is admitted, the *Hint Providers* can then use this information when making the\nresource allocation decision.",
          "is_needle": true
        },
        {
          "chunk_id": "concepts_services-networking_ingress_mdsem_13",
          "doc_id": "concepts_services-networking_ingress",
          "content": "### IngressClass scope\n\nDepending on your ingress controller, you may be able to use parameters\nthat you set cluster-wide, or just for one namespace.\n\n{{< tabs name=\"tabs_ingressclass_parameter_scope\" >}}\n{{% tab name=\"Cluster\" %}}\nThe default scope for IngressClass parameters is cluster-wide.\n\nIf you set the `.spec.parameters` field and don't set\n`.spec.parameters.scope`, or if you set `.spec.parameters.scope` to\n`Cluster`, then the IngressClass refers to a cluster-scoped resource.\nThe `kind` (in combination the `apiGroup`) of the parameters\nrefers to a cluster-scoped API (possibly a custom resource), and\nthe `name` of the parameters identifies a specific cluster scoped\nresource for that API.\n\nFor example:\n\n```yaml\n---\napiVersion: networking.k8s.io/v1\nkind: IngressClass\nmetadata:\n  name: external-lb-1\nspec:\n  controller: example.com/ingress-controller\n  parameters:\n    # The parameters for this IngressClass are specified in a\n    # ClusterIngressParameter (API group k8s.example.net) named\n    # \"external-config-1\". This definition tells Kubernetes to\n    # look for a cluster-scoped parameter resource.\n    scope: Cluster\n    apiGroup: k8s.example.net\n    kind: ClusterIngressParameter\n    name: external-config-1\n```\n\n{{% /tab %}}\n{{% tab name=\"Namespaced\" %}}\n{{< feature-state for_k8s_version=\"v1.23\" state=\"stable\" >}}\n\nIf you set the `.spec.parameters` field and set\n`.spec.parameters.scope` to `Namespace`, then the IngressClass refers\nto a namespaced-scoped resource. You must also set the `namespace`\nfield within `.spec.parameters` to the namespace that contains\nthe parameters you want to use.\n\nThe `kind` (in combination the `apiGroup`) of the parameters\nrefers to a namespaced API (for example: ConfigMap), and\nthe `name` of the parameters identifies a specific resource\nin the namespace you specified in `namespace`.\n\nNamespace-scoped parameters help the cluster operator delegate control over the\nconfiguration (for example: load balancer settings, API gateway definition)\nthat is used for a workload. If you used a cluster-scoped parameter then either:\n\n- the cluster operator team needs to approve a different team's changes every\n  time there's a new configuration change being applied.\n- the cluster operator must define specific access controls, such as\n  [RBAC](/docs/reference/access-authn-authz/rbac/) roles and bindings, that let\n  the application team make changes to the cluster-scoped parameters resource.\n\nThe IngressClass API itself is always cluster-scoped.\n\nHere is an example of an IngressClass that refers to parameters that are\nnamespaced:\n\n```yaml\n---\napiVersion: networking.k8s.io/v1\nkind: IngressClass\nmetadata:\n  name: external-lb-2\nspec:\n  controller: example.com/ingress-controller\n  parameters:\n    # The parameters for this IngressClass are specified in an\n    # IngressParameter (API group k8s.example.com) named \"external-config\",\n    # that's in the \"external-configuration\" namespace.\n    scope: Namespace\n    apiGroup: k8s.example.com\n    kind: IngressParameter\n    namespace: external-configuration\n    name: external-config\n```\n\n{{% /tab %}}\n{{< /tabs >}}",
          "is_needle": false
        }
      ]
    },
    {
      "question_id": "q_017",
      "question": "What feature gate do I need for topology manager policy options?",
      "expected_answer": "TopologyManagerPolicyOptions (enabled by default)",
      "difficulty": "medium",
      "section": "Topology manager policy options",
      "latency_ms": 863.1,
      "needle_found": true,
      "retrieved_chunks": [
        {
          "chunk_id": "tasks_administer-cluster_topology-manager_mdsem_9",
          "doc_id": "tasks_administer-cluster_topology-manager",
          "content": "## Topology manager policy options\n\nSupport for the Topology Manager policy options requires `TopologyManagerPolicyOptions`\n[feature gate](/docs/reference/command-line-tools-reference/feature-gates/) to be enabled\n(it is enabled by default).\n\nYou can toggle groups of options on and off based upon their maturity level using the following feature gates:\n\n* `TopologyManagerPolicyBetaOptions` default enabled. Enable to show beta-level options.\n* `TopologyManagerPolicyAlphaOptions` default disabled. Enable to show alpha-level options.\n\nYou will still have to enable each option using the `TopologyManagerPolicyOptions` kubelet option.",
          "is_needle": true
        },
        {
          "chunk_id": "tasks_administer-cluster_topology-manager_mdsem_2",
          "doc_id": "tasks_administer-cluster_topology-manager",
          "content": "## Windows Support\n\n{{< feature-state feature_gate_name=\"WindowsCPUAndMemoryAffinity\" >}}\n\nThe Topology Manager support can be enabled on Windows by using the `WindowsCPUAndMemoryAffinity` feature gate and\nit requires support in the container runtime.\n\n## Topology manager scopes and policies\n\nThe Topology Manager currently:\n\n- aligns Pods of all QoS classes.\n- aligns the requested resources that Hint Provider provides topology hints for.\n\nIf these conditions are met, the Topology Manager will align the requested resources.\n\nIn order to customize how this alignment is carried out, the Topology Manager provides two\ndistinct options: `scope` and `policy`.\n\nThe `scope` defines the granularity at which you would like resource alignment to be performed,\nfor example, at the `pod` or `container` level. And the `policy` defines the actual policy used to\ncarry out the alignment, for example, `best-effort`, `restricted`, and `single-numa-node`.\nDetails on the various `scopes` and `policies` available today can be found below.\n\n{{< note >}}\nTo align CPU resources with other requested resources in a Pod spec, the CPU Manager should be\nenabled and proper CPU Manager policy should be configured on a Node.\nSee [Control CPU Management Policies on the Node](/docs/tasks/administer-cluster/cpu-management-policies/).\n{{< /note >}}\n\n{{< note >}}\nTo align memory (and hugepages) resources with other requested resources in a Pod spec, the Memory\nManager should be enabled and proper Memory Manager policy should be configured on a Node. Refer to\n[Memory Manager](/docs/tasks/administer-cluster/memory-manager/) documentation.\n{{< /note >}}",
          "is_needle": true
        },
        {
          "chunk_id": "tasks_administer-cluster_topology-manager_mdsem_5",
          "doc_id": "tasks_administer-cluster_topology-manager",
          "content": "## Topology manager policies\n\nThe Topology Manager supports four allocation policies. You can set a policy via a kubelet flag,\n`--topology-manager-policy`. There are four supported policies:\n\n* `none` (default)\n* `best-effort`\n* `restricted`\n* `single-numa-node`\n\n{{< note >}}\nIf the Topology Manager is configured with the **pod** scope, the container, which is considered by\nthe policy, is reflecting requirements of the entire pod, and thus each container from the pod\nwill result with **the same** topology alignment decision.\n{{< /note >}}",
          "is_needle": true
        },
        {
          "chunk_id": "tasks_administer-cluster_topology-manager_mdsem_6",
          "doc_id": "tasks_administer-cluster_topology-manager",
          "content": "### `none` policy {#policy-none}\n\nThis is the default policy and does not perform any topology alignment.\n\n### `best-effort` policy {#policy-best-effort}\n\nFor each container in a Pod, the kubelet, with `best-effort` topology management policy, calls\neach Hint Provider to discover their resource availability. Using this information, the Topology\nManager stores the preferred NUMA Node affinity for that container. If the affinity is not\npreferred, the Topology Manager will store this and admit the pod to the node anyway.\n\nThe *Hint Providers* can then use this information when making the\nresource allocation decision.",
          "is_needle": true
        },
        {
          "chunk_id": "tasks_administer-cluster_topology-manager_mdsem_10",
          "doc_id": "tasks_administer-cluster_topology-manager",
          "content": "### `prefer-closest-numa-nodes` {#policy-option-prefer-closest-numa-nodes}\n\nThe `prefer-closest-numa-nodes` option is GA since Kubernetes 1.32. In Kubernetes {{< skew currentVersion >}}\nthis policy option is visible by default provided that the `TopologyManagerPolicyOptions`\n[feature gate](/docs/reference/command-line-tools-reference/feature-gates/) is enabled.\n\nThe Topology Manager is not aware by default of NUMA distances, and does not take them into account when making\nPod admission decisions. This limitation surfaces in multi-socket, as well as single-socket multi NUMA systems,\nand can cause significant performance degradation in latency-critical execution and high-throughput applications\nif the Topology Manager decides to align resources on non-adjacent NUMA nodes.\n\nIf you specify the `prefer-closest-numa-nodes` policy option, the `best-effort` and `restricted`\npolicies favor sets of NUMA nodes with shorter distance between them when making admission decisions.\n\nYou can enable this option by adding `prefer-closest-numa-nodes=true` to the Topology Manager policy options.\n\nBy default (without this option), the Topology Manager aligns resources on either a single NUMA node or,\nin the case where more than one NUMA node is required, using the minimum number of NUMA nodes.",
          "is_needle": true
        }
      ]
    },
    {
      "question_id": "q_018",
      "question": "My multi-socket server has GPUs and CPUs, how does k8s coordinate their placement?",
      "expected_answer": "Topology Manager coordinates CPU Manager and Device Manager to make topology-aligned resource allocation decisions, avoiding CPUs and devices being allocated from different NUMA nodes",
      "difficulty": "hard",
      "section": "How topology manager works",
      "latency_ms": 1253.3,
      "needle_found": true,
      "retrieved_chunks": [
        {
          "chunk_id": "concepts_scheduling-eviction_kube-scheduler_mdsem_3",
          "doc_id": "concepts_scheduling-eviction_kube-scheduler",
          "content": "## {{% heading \"whatsnext\" %}}\n\n* Read about [scheduler performance tuning](/docs/concepts/scheduling-eviction/scheduler-perf-tuning/)\n* Read about [Pod topology spread constraints](/docs/concepts/scheduling-eviction/topology-spread-constraints/)\n* Read the [reference documentation](/docs/reference/command-line-tools-reference/kube-scheduler/) for kube-scheduler\n* Read the [kube-scheduler config (v1)](/docs/reference/config-api/kube-scheduler-config.v1/) reference\n* Learn about [configuring multiple schedulers](/docs/tasks/extend-kubernetes/configure-multiple-schedulers/)\n* Learn about [topology management policies](/docs/tasks/administer-cluster/topology-manager/)\n* Learn about [Pod Overhead](/docs/concepts/scheduling-eviction/pod-overhead/)\n* Learn about scheduling of Pods that use volumes in:\n  * [Volume Topology Support](/docs/concepts/storage/storage-classes/#volume-binding-mode)\n  * [Storage Capacity Tracking](/docs/concepts/storage/storage-capacity/)\n  * [Node-specific Volume Limits](/docs/concepts/storage/storage-limits/)",
          "is_needle": false
        },
        {
          "chunk_id": "concepts_scheduling-eviction_kube-scheduler_mdsem_1",
          "doc_id": "concepts_scheduling-eviction_kube-scheduler",
          "content": "## kube-scheduler\n\n[kube-scheduler](/docs/reference/command-line-tools-reference/kube-scheduler/)\nis the default scheduler for Kubernetes and runs as part of the\n{{< glossary_tooltip text=\"control plane\" term_id=\"control-plane\" >}}.\nkube-scheduler is designed so that, if you want and need to, you can\nwrite your own scheduling component and use that instead.\n\nKube-scheduler selects an optimal node to run newly created or not yet\nscheduled (unscheduled) pods. Since containers in pods - and pods themselves -\ncan have different requirements, the scheduler filters out any nodes that\ndon't meet a Pod's specific scheduling needs. Alternatively, the API lets\nyou specify a node for a Pod when you create it, but this is unusual\nand is only done in special cases.\n\nIn a cluster, Nodes that meet the scheduling requirements for a Pod\nare called _feasible_ nodes. If none of the nodes are suitable, the pod\nremains unscheduled until the scheduler is able to place it.\n\nThe scheduler finds feasible Nodes for a Pod and then runs a set of\nfunctions to score the feasible Nodes and picks a Node with the highest\nscore among the feasible ones to run the Pod. The scheduler then notifies\nthe API server about this decision in a process called _binding_.\n\nFactors that need to be taken into account for scheduling decisions include\nindividual and collective resource requirements, hardware / software /\npolicy constraints, affinity and anti-affinity specifications, data\nlocality, inter-workload interference, and so on.",
          "is_needle": false
        },
        {
          "chunk_id": "tasks_administer-cluster_topology-manager_mdsem_12",
          "doc_id": "tasks_administer-cluster_topology-manager",
          "content": "## Pod interactions with topology manager policies\n\nConsider the containers in the following Pod manifest:\n\n```yaml\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n```\n\nThis pod runs in the `BestEffort` QoS class because no resource `requests` or `limits` are specified.\n\n```yaml\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n    resources:\n      limits:\n        memory: \"200Mi\"\n      requests:\n        memory: \"100Mi\"\n```\n\nThis pod runs in the `Burstable` QoS class because requests are less than limits.\n\nIf the selected policy is anything other than `none`, the Topology Manager would consider these Pod\nspecifications. The Topology Manager would consult the Hint Providers to get topology hints.\nIn the case of the `static`, the CPU Manager policy would return default topology hint, because\nthese Pods do not explicitly request CPU resources.\n\n```yaml\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n    resources:\n      limits:\n        memory: \"200Mi\"\n        cpu: \"2\"\n        example.com/device: \"1\"\n      requests:\n        memory: \"200Mi\"\n        cpu: \"2\"\n        example.com/device: \"1\"\n```\n\nThis pod with integer CPU request runs in the `Guaranteed` QoS class because `requests` are equal\nto `limits`.\n\n```yaml\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n    resources:\n      limits:\n        memory: \"200Mi\"\n        cpu: \"300m\"\n        example.com/device: \"1\"\n      requests:\n        memory: \"200Mi\"\n        cpu: \"300m\"\n        example.com/device: \"1\"\n```\n\nThis pod with sharing CPU request runs in the `Guaranteed` QoS class because `requests` are equal\nto `limits`.\n\n```yaml\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n    resources:\n      limits:\n        example.com/deviceA: \"1\"\n        example.com/deviceB: \"1\"\n      requests:\n        example.com/deviceA: \"1\"\n        example.com/deviceB: \"1\"\n```\n\nThis pod runs in the `BestEffort` QoS class because there are no CPU and memory requests.\n\nThe Topology Manager would consider the above pods. The Topology Manager would consult the Hint\nProviders, which are CPU and Device Manager to get topology hints for the pods.\n\nIn the case of the `Guaranteed` pod with integer CPU request, the `static` CPU Manager policy\nwould return topology hints relating to the exclusive CPU and the Device Manager would send back\nhints for the requested device.\n\nIn the case of the `Guaranteed` pod with sharing CPU request, the `static` CPU Manager policy\nwould return default topology hint as there is no exclusive CPU request and the Device Manager\nwould send back hints for the requested device.\n\nIn the above two cases of the `Guaranteed` pod, the `none` CPU Manager policy would return default\ntopology hint.\n\nIn the case of the `BestEffort` pod, the `static` CPU Manager policy would send back the default\ntopology hint as there is no CPU request and the Device Manager would send back the hints for each\nof the requested devices.\n\nUsing this information the Topology Manager calculates the optimal hint for the pod and stores\nthis information, which will be used by the Hint Providers when they are making their resource\nassignments.",
          "is_needle": true
        },
        {
          "chunk_id": "concepts_scheduling-eviction_kube-scheduler_mdsem_0",
          "doc_id": "concepts_scheduling-eviction_kube-scheduler",
          "content": "---\ntitle: Kubernetes Scheduler\ncontent_type: concept\nweight: 10\n---\n\n<!-- overview -->\n\nIn Kubernetes, _scheduling_ refers to making sure that {{< glossary_tooltip text=\"Pods\" term_id=\"pod\" >}}\nare matched to {{< glossary_tooltip text=\"Nodes\" term_id=\"node\" >}} so that\n{{< glossary_tooltip term_id=\"kubelet\" >}} can run them.\n\n<!-- body -->\n\n## Scheduling overview {#scheduling}\n\nA scheduler watches for newly created Pods that have no Node assigned. For\nevery Pod that the scheduler discovers, the scheduler becomes responsible\nfor finding the best Node for that Pod to run on. The scheduler reaches\nthis placement decision taking into account the scheduling principles\ndescribed below.\n\nIf you want to understand why Pods are placed onto a particular Node,\nor if you're planning to implement a custom scheduler yourself, this\npage will help you learn about scheduling.",
          "is_needle": false
        },
        {
          "chunk_id": "tutorials_kubernetes-basics_create-cluster_cluster-intro_mdsem_0",
          "doc_id": "tutorials_kubernetes-basics_create-cluster_cluster-intro",
          "content": "## {{% heading \"objectives\" %}}\n\n* Learn what a Kubernetes cluster is.\n* Learn what Minikube is.\n* Start a Kubernetes cluster on your computer.\n\n## Kubernetes Clusters\n\n{{% alert %}}\n_Kubernetes is a production-grade, open-source platform that orchestrates\nthe placement (scheduling) and execution of application containers\nwithin and across computer clusters._\n{{% /alert %}}\n\n**Kubernetes coordinates a highly available cluster of computers that are connected\nto work as a single unit.** The abstractions in Kubernetes allow you to deploy\ncontainerized applications to a cluster without tying them specifically to individual\nmachines. To make use of this new model of deployment, applications need to be packaged\nin a way that decouples them from individual hosts: they need to be containerized.\nContainerized applications are more flexible and available than in past deployment models,\nwhere applications were installed directly onto specific machines as packages deeply\nintegrated into the host. **Kubernetes automates the distribution and scheduling of\napplication containers across a cluster in a more efficient way.** Kubernetes is an\nopen-source platform and is production-ready.\n\nA Kubernetes cluster consists of two types of resources:\n\n* The **Control Plane** coordinates the cluster\n* **Nodes** are the workers that run applications",
          "is_needle": false
        }
      ]
    },
    {
      "question_id": "q_019",
      "question": "With single-numa-node policy, when exactly does a pod get rejected?",
      "expected_answer": "When a single NUMA Node affinity is not possible - if more than one NUMA node is required to satisfy the allocation, the pod is rejected",
      "difficulty": "hard",
      "section": "single-numa-node policy",
      "latency_ms": 1127.6,
      "needle_found": true,
      "retrieved_chunks": [
        {
          "chunk_id": "tasks_administer-cluster_topology-manager_mdsem_4",
          "doc_id": "tasks_administer-cluster_topology-manager",
          "content": "### `pod` scope\n\nTo select the `pod` scope, set `topologyManagerScope` in the\n[kubelet configuration file](/docs/tasks/administer-cluster/kubelet-config-file/) to `pod`.\n\nThis scope allows for grouping all containers in a pod to a common set of NUMA nodes. That is, the\nTopology Manager treats a pod as a whole and attempts to allocate the entire pod (all containers)\nto either a single NUMA node or a common set of NUMA nodes. The following examples illustrate the\nalignments produced by the Topology Manager on different occasions:\n\n* all containers can be and are allocated to a single NUMA node;\n* all containers can be and are allocated to a shared set of NUMA nodes.\n\nThe total amount of particular resource demanded for the entire pod is calculated according to\n[effective requests/limits](/docs/concepts/workloads/pods/init-containers/#resource-sharing-within-containers)\nformula, and thus, this total value is equal to the maximum of:\n\n* the sum of all app container requests,\n* the maximum of init container requests,\n\nfor a resource.\n\nUsing the `pod` scope in tandem with `single-numa-node` Topology Manager policy is specifically\nvaluable for workloads that are latency sensitive or for high-throughput applications that perform\nIPC. By combining both options, you are able to place all containers in a pod onto a single NUMA\nnode; hence, the inter-NUMA communication overhead can be eliminated for that pod.\n\nIn the case of `single-numa-node` policy, a pod is accepted only if a suitable set of NUMA nodes\nis present among possible allocations. Reconsider the example above:\n\n* a set containing only a single NUMA node - it leads to pod being admitted,\n* whereas a set containing more NUMA nodes - it results in pod rejection (because instead of one\n  NUMA node, two or more NUMA nodes are required to satisfy the allocation).\n\nTo recap, the Topology Manager first computes a set of NUMA nodes and then tests it against the Topology\nManager policy, which either leads to the rejection or admission of the pod.",
          "is_needle": true
        },
        {
          "chunk_id": "tasks_administer-cluster_topology-manager_mdsem_7",
          "doc_id": "tasks_administer-cluster_topology-manager",
          "content": "### `restricted` policy {#policy-restricted}\n\nFor each container in a Pod, the kubelet, with `restricted` topology management policy, calls each\nHint Provider to discover their resource availability. Using this information, the Topology\nManager stores the preferred NUMA Node affinity for that container. If the affinity is not\npreferred, the Topology Manager will reject this pod from the node. This will result in a pod entering a\n`Terminated` state with a pod admission failure.\n\nOnce the pod is in a `Terminated` state, the Kubernetes scheduler will **not** attempt to\nreschedule the pod. It is recommended to use a ReplicaSet or Deployment to trigger a redeployment of\nthe pod. An external control loop could be also implemented to trigger a redeployment of pods that\nhave the `Topology Affinity` error.\n\nIf the pod is admitted, the *Hint Providers* can then use this information when making the\nresource allocation decision.",
          "is_needle": true
        },
        {
          "chunk_id": "tasks_administer-cluster_topology-manager_mdsem_8",
          "doc_id": "tasks_administer-cluster_topology-manager",
          "content": "### `single-numa-node` policy {#policy-single-numa-node}\n\nFor each container in a Pod, the kubelet, with `single-numa-node` topology management policy,\ncalls each Hint Provider to discover their resource availability. Using this information, the\nTopology Manager determines if a single NUMA Node affinity is possible. If it is, Topology\nManager will store this and the *Hint Providers* can then use this information when making the\nresource allocation decision. If, however, this is not possible then the Topology Manager will\nreject the pod from the node. This will result in a pod in a `Terminated` state with a pod\nadmission failure.\n\nOnce the pod is in a `Terminated` state, the Kubernetes scheduler will **not** attempt to\nreschedule the pod. It is recommended to use a Deployment with replicas to trigger a redeployment of\nthe Pod. An external control loop could be also implemented to trigger a redeployment of pods\nthat have the `Topology Affinity` error.",
          "is_needle": true
        },
        {
          "chunk_id": "concepts_scheduling-eviction_kube-scheduler_mdsem_1",
          "doc_id": "concepts_scheduling-eviction_kube-scheduler",
          "content": "## kube-scheduler\n\n[kube-scheduler](/docs/reference/command-line-tools-reference/kube-scheduler/)\nis the default scheduler for Kubernetes and runs as part of the\n{{< glossary_tooltip text=\"control plane\" term_id=\"control-plane\" >}}.\nkube-scheduler is designed so that, if you want and need to, you can\nwrite your own scheduling component and use that instead.\n\nKube-scheduler selects an optimal node to run newly created or not yet\nscheduled (unscheduled) pods. Since containers in pods - and pods themselves -\ncan have different requirements, the scheduler filters out any nodes that\ndon't meet a Pod's specific scheduling needs. Alternatively, the API lets\nyou specify a node for a Pod when you create it, but this is unusual\nand is only done in special cases.\n\nIn a cluster, Nodes that meet the scheduling requirements for a Pod\nare called _feasible_ nodes. If none of the nodes are suitable, the pod\nremains unscheduled until the scheduler is able to place it.\n\nThe scheduler finds feasible Nodes for a Pod and then runs a set of\nfunctions to score the feasible Nodes and picks a Node with the highest\nscore among the feasible ones to run the Pod. The scheduler then notifies\nthe API server about this decision in a process called _binding_.\n\nFactors that need to be taken into account for scheduling decisions include\nindividual and collective resource requirements, hardware / software /\npolicy constraints, affinity and anti-affinity specifications, data\nlocality, inter-workload interference, and so on.",
          "is_needle": false
        },
        {
          "chunk_id": "tasks_administer-cluster_topology-manager_mdsem_11",
          "doc_id": "tasks_administer-cluster_topology-manager",
          "content": "### `max-allowable-numa-nodes` {#policy-option-max-allowable-numa-nodes}\n\nThe `max-allowable-numa-nodes` option is GA since Kubernetes 1.35. In Kubernetes {{< skew currentVersion >}},\nthis policy option is visible by default provided that the `TopologyManagerPolicyOptions`\n[feature gate](/docs/reference/command-line-tools-reference/feature-gates/) is enabled.\n\nThe time to admit a pod is tied to the number of NUMA nodes on the physical machine.\nBy default, Kubernetes does not run a kubelet with the Topology Manager enabled, on any (Kubernetes) node where\nmore than 8 NUMA nodes are detected.\n\n{{< note >}}\nIf you select the `max-allowable-numa-nodes` policy option, nodes with more than 8 NUMA nodes can\nbe allowed to run with the Topology Manager enabled. The Kubernetes project only has limited data on the impact\nof using the Topology Manager on (Kubernetes) nodes with more than 8 NUMA nodes. Because of that\nlack of data, using this policy option with Kubernetes {{< skew currentVersion >}} is **not** recommended and is\nat your own risk.\n{{< /note >}}\n\nYou can enable this option by adding `max-allowable-numa-nodes=true` to the Topology Manager policy options.\n\nSetting a value of `max-allowable-numa-nodes` does not (in and of itself) affect the\nlatency of pod admission, but binding a Pod to a (Kubernetes) node with many NUMA does have an impact.\nFuture, potential improvements to Kubernetes may improve Pod admission performance and the high\nlatency that happens as the number of NUMA nodes increases.",
          "is_needle": true
        }
      ]
    },
    {
      "question_id": "q_020",
      "question": "What happens if topology manager can't find preferred NUMA affinity with best-effort policy?",
      "expected_answer": "The pod is admitted to the node anyway - best-effort stores the non-preferred hint and allows the pod",
      "difficulty": "medium",
      "section": "best-effort policy",
      "latency_ms": 988.4,
      "needle_found": true,
      "retrieved_chunks": [
        {
          "chunk_id": "tasks_administer-cluster_topology-manager_mdsem_6",
          "doc_id": "tasks_administer-cluster_topology-manager",
          "content": "### `none` policy {#policy-none}\n\nThis is the default policy and does not perform any topology alignment.\n\n### `best-effort` policy {#policy-best-effort}\n\nFor each container in a Pod, the kubelet, with `best-effort` topology management policy, calls\neach Hint Provider to discover their resource availability. Using this information, the Topology\nManager stores the preferred NUMA Node affinity for that container. If the affinity is not\npreferred, the Topology Manager will store this and admit the pod to the node anyway.\n\nThe *Hint Providers* can then use this information when making the\nresource allocation decision.",
          "is_needle": true
        },
        {
          "chunk_id": "tasks_administer-cluster_topology-manager_mdsem_10",
          "doc_id": "tasks_administer-cluster_topology-manager",
          "content": "### `prefer-closest-numa-nodes` {#policy-option-prefer-closest-numa-nodes}\n\nThe `prefer-closest-numa-nodes` option is GA since Kubernetes 1.32. In Kubernetes {{< skew currentVersion >}}\nthis policy option is visible by default provided that the `TopologyManagerPolicyOptions`\n[feature gate](/docs/reference/command-line-tools-reference/feature-gates/) is enabled.\n\nThe Topology Manager is not aware by default of NUMA distances, and does not take them into account when making\nPod admission decisions. This limitation surfaces in multi-socket, as well as single-socket multi NUMA systems,\nand can cause significant performance degradation in latency-critical execution and high-throughput applications\nif the Topology Manager decides to align resources on non-adjacent NUMA nodes.\n\nIf you specify the `prefer-closest-numa-nodes` policy option, the `best-effort` and `restricted`\npolicies favor sets of NUMA nodes with shorter distance between them when making admission decisions.\n\nYou can enable this option by adding `prefer-closest-numa-nodes=true` to the Topology Manager policy options.\n\nBy default (without this option), the Topology Manager aligns resources on either a single NUMA node or,\nin the case where more than one NUMA node is required, using the minimum number of NUMA nodes.",
          "is_needle": true
        },
        {
          "chunk_id": "tasks_administer-cluster_topology-manager_mdsem_8",
          "doc_id": "tasks_administer-cluster_topology-manager",
          "content": "### `single-numa-node` policy {#policy-single-numa-node}\n\nFor each container in a Pod, the kubelet, with `single-numa-node` topology management policy,\ncalls each Hint Provider to discover their resource availability. Using this information, the\nTopology Manager determines if a single NUMA Node affinity is possible. If it is, Topology\nManager will store this and the *Hint Providers* can then use this information when making the\nresource allocation decision. If, however, this is not possible then the Topology Manager will\nreject the pod from the node. This will result in a pod in a `Terminated` state with a pod\nadmission failure.\n\nOnce the pod is in a `Terminated` state, the Kubernetes scheduler will **not** attempt to\nreschedule the pod. It is recommended to use a Deployment with replicas to trigger a redeployment of\nthe Pod. An external control loop could be also implemented to trigger a redeployment of pods\nthat have the `Topology Affinity` error.",
          "is_needle": true
        },
        {
          "chunk_id": "tasks_administer-cluster_topology-manager_mdsem_4",
          "doc_id": "tasks_administer-cluster_topology-manager",
          "content": "### `pod` scope\n\nTo select the `pod` scope, set `topologyManagerScope` in the\n[kubelet configuration file](/docs/tasks/administer-cluster/kubelet-config-file/) to `pod`.\n\nThis scope allows for grouping all containers in a pod to a common set of NUMA nodes. That is, the\nTopology Manager treats a pod as a whole and attempts to allocate the entire pod (all containers)\nto either a single NUMA node or a common set of NUMA nodes. The following examples illustrate the\nalignments produced by the Topology Manager on different occasions:\n\n* all containers can be and are allocated to a single NUMA node;\n* all containers can be and are allocated to a shared set of NUMA nodes.\n\nThe total amount of particular resource demanded for the entire pod is calculated according to\n[effective requests/limits](/docs/concepts/workloads/pods/init-containers/#resource-sharing-within-containers)\nformula, and thus, this total value is equal to the maximum of:\n\n* the sum of all app container requests,\n* the maximum of init container requests,\n\nfor a resource.\n\nUsing the `pod` scope in tandem with `single-numa-node` Topology Manager policy is specifically\nvaluable for workloads that are latency sensitive or for high-throughput applications that perform\nIPC. By combining both options, you are able to place all containers in a pod onto a single NUMA\nnode; hence, the inter-NUMA communication overhead can be eliminated for that pod.\n\nIn the case of `single-numa-node` policy, a pod is accepted only if a suitable set of NUMA nodes\nis present among possible allocations. Reconsider the example above:\n\n* a set containing only a single NUMA node - it leads to pod being admitted,\n* whereas a set containing more NUMA nodes - it results in pod rejection (because instead of one\n  NUMA node, two or more NUMA nodes are required to satisfy the allocation).\n\nTo recap, the Topology Manager first computes a set of NUMA nodes and then tests it against the Topology\nManager policy, which either leads to the rejection or admission of the pod.",
          "is_needle": true
        },
        {
          "chunk_id": "tasks_administer-cluster_topology-manager_mdsem_7",
          "doc_id": "tasks_administer-cluster_topology-manager",
          "content": "### `restricted` policy {#policy-restricted}\n\nFor each container in a Pod, the kubelet, with `restricted` topology management policy, calls each\nHint Provider to discover their resource availability. Using this information, the Topology\nManager stores the preferred NUMA Node affinity for that container. If the affinity is not\npreferred, the Topology Manager will reject this pod from the node. This will result in a pod entering a\n`Terminated` state with a pod admission failure.\n\nOnce the pod is in a `Terminated` state, the Kubernetes scheduler will **not** attempt to\nreschedule the pod. It is recommended to use a ReplicaSet or Deployment to trigger a redeployment of\nthe pod. An external control loop could be also implemented to trigger a redeployment of pods that\nhave the `Topology Affinity` error.\n\nIf the pod is admitted, the *Hint Providers* can then use this information when making the\nresource allocation decision.",
          "is_needle": true
        }
      ]
    }
  ]
}