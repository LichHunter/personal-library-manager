{
  "metadata": {
    "timestamp": "2026-01-24_134603",
    "num_documents": 5,
    "num_queries": 20,
    "total_facts": 53,
    "has_human_queries": true
  },
  "evaluations": [
    {
      "strategy": "semantic",
      "chunking": "fixed_200_0pct",
      "embedder": "BAAI/bge-base-en-v1.5",
      "reranker": null,
      "llm": null,
      "k": 5,
      "metric": "exact_match",
      "num_chunks": 125,
      "index_time_s": 2.2163017709990527,
      "aggregate": {
        "original": {
          "coverage": 0.5660377358490566,
          "found": 30,
          "total": 53,
          "avg_latency_ms": 5.914408450007613,
          "p95_latency_ms": 6.189716300650621
        },
        "synonym": {
          "coverage": 0.6037735849056604,
          "found": 32,
          "total": 53,
          "avg_latency_ms": 5.603244049871137,
          "p95_latency_ms": 5.898102551236662
        },
        "problem": {
          "coverage": 0.5283018867924528,
          "found": 28,
          "total": 53,
          "avg_latency_ms": 5.543346600006771,
          "p95_latency_ms": 5.7880431500962
        },
        "casual": {
          "coverage": 0.5849056603773585,
          "found": 31,
          "total": 53,
          "avg_latency_ms": 5.569148649829003,
          "p95_latency_ms": 5.947793399809598
        },
        "contextual": {
          "coverage": 0.5283018867924528,
          "found": 28,
          "total": 53,
          "avg_latency_ms": 5.595763100154727,
          "p95_latency_ms": 5.940699048915121
        },
        "negation": {
          "coverage": 0.41509433962264153,
          "found": 22,
          "total": 53,
          "avg_latency_ms": 5.5861849499706295,
          "p95_latency_ms": 5.830764099573571
        }
      },
      "per_query": [
        {
          "key_facts": [
            "100 requests per minute per authenticated user",
            "20 requests per minute for unauthenticated requests"
          ],
          "found_facts": [
            "100 requests per minute per authenticated user",
            "20 requests per minute for unauthenticated requests"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "api_reference"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_3",
              "content": "## Rate Limiting To ensure fair usage and system stability, CloudFlow enforces rate limits on all API endpoints. **Default Limits:** - 100 requests per minute per authenticated user - 20 requests per minute for unauthenticated requests - Burst allowance: 150 requests in a 10-second window ### Rate Limit Headers Every API response includes rate limit information: ``` X-RateLimit-Limit: 100 X-RateLimit-Remaining: 87 X-RateLimit-Reset: 1640995200 ``` When you exceed the rate limit, you'll receive a `429 Too Many Requests` response: ```json { \"error\": { \"code\": \"rate_limit_exceeded\", \"message\": \"Rate limit exceeded. Please retry after 42 seconds.\", \"retry_after\": 42 } } ``` **Best Practices:** - Monitor `X-RateLimit-Remaining` header values - Implement exponential backoff when receiving 429 responses - Cache responses when appropriate to reduce API calls - Consider upgrading to Enterprise tier for higher limits (1000 req/min) ## Pagination List endpoints return paginated results to optimize performance.",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_17",
              "content": "Retry after 45 seconds.\", \"status\": 429, \"headers\": { \"X-RateLimit-Limit\": \"1000\", \"X-RateLimit-Remaining\": \"0\", \"X-RateLimit-Reset\": \"1706112345\", \"Retry-After\": \"45\" } } ``` #### Rate Limit Tiers CloudFlow enforces the following rate limits per workspace: | Tier | Requests/Minute | Requests/Hour | Concurrent Workflows | |------|-----------------|---------------|----------------------| | Free | 60 | 1,000 | 5 | | Standard | 1,000 | 50,000 | 50 | | Premium | 5,000 | 250,000 | 200 | | Enterprise | Custom | Custom | Unlimited | #### Checking Rate Limit Status ```bash # Check current rate limit status curl -I https://api.cloudflow.io/api/v1/workflows \\ -H \"Authorization: Bearer $CF_ACCESS_TOKEN\" # Extract rate limit headers curl -s -I https://api.cloudflow.io/api/v1/workflows \\ -H \"Authorization: Bearer $CF_ACCESS_TOKEN\" | \\ grep -i \"x-ratelimit\" # Output: # X-RateLimit-Limit: 1000 # X-RateLimit-Remaining: 247 # X-RateLimit-Reset: 1706112400 # Monitor rate limit usage cloudflow metrics query \\ --metric rate_limit_remaining \\ --workspace ws_abc123 \\ --last 1h \\ --interval 1m ``` ####",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_18",
              "content": "Handling Rate Limits in Code **Python example with retry logic:** ```python import time import requests def cloudflow_api_call_with_retry(url, headers, max_retries=3): for attempt in range(max_retries): response = requests.get(url, headers=headers) if response.status_code == 429: retry_after = int(response.headers.get('Retry-After', 60)) print(f\"Rate limited. Waiting {retry_after} seconds...\") time.sleep(retry_after) continue remaining = int(response.headers.get('X-RateLimit-Remaining', 0)) if remaining < 10: print(f\"Warning: Only {remaining} requests remaining\") return response raise Exception(\"Max retries exceeded due to rate limiting\") ``` **Bash script with rate limit checking:** ```bash #!/bin/bash check_rate_limit() { local remaining=$(curl -s -I https://api.cloudflow.io/api/v1/workflows \\ -H \"Authorization: Bearer $CF_ACCESS_TOKEN\" | \\ grep -i \"x-ratelimit-remaining\" | \\ awk '{print $2}' | tr -d '\\r') if [ \"$remaining\" -lt 10 ]; then echo \"Warning: Only $remaining requests remaining\" local reset=$(curl -s -I https://api.cloudflow.io/api/v1/workflows \\ -H \"Authorization: Bearer $CF_ACCESS_TOKEN\" | \\ grep -i \"x-ratelimit-reset\" | \\ awk '{print $2}' | tr -d '\\r') local wait_time=$((reset - $(date +%s))) echo \"Rate limit resets in $wait_time seconds\" sleep",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_25",
              "content": "limiting: 1,000 requests per minute per API key **Workflow Engine**: - Concurrent executions: 8,000 workflows (across 16 pods) - Execution start rate: 500 per second - Completion rate: 450 per second (average 2-second execution time) **Database**: - Read throughput: 50,000 queries per second (across replicas) - Write throughput: 15,000 transactions per second - Connection capacity: 2,000 concurrent connections **Kafka**: - Message ingestion: 100,000 messages per second - Consumer throughput: 80,000 messages per second (aggregated) - End-to-end latency: < 100ms (P99) ### Resource Utilization **CPU Utilization** (Target: 60-70% average): - API Gateway: 55% average, 80% peak - Workflow Engine: 65% average, 85% peak - Auth Service: 40% average, 60% peak **Memory Utilization** (Target: < 80%): - API Gateway: 2.5GB average per pod (4GB allocated) - Workflow Engine: 6GB average per pod (8GB allocated) - Notification Service: 2.8GB average per pod (4GB allocated) **Network Throughput**: - Ingress: 2 Gbps average, 5 Gbps",
              "score": null
            },
            {
              "doc_id": "user_guide",
              "chunk_id": "user_guide_fix_17",
              "content": "### Execution Timeout - **Default**: 3600 seconds (60 minutes) - **Behavior**: Workflows exceeding this timeout are automatically terminated - **Custom Timeouts**: Enterprise plans can request custom timeout limits **Setting Step-Level Timeouts:** ```yaml - id: long_running_task action: http_request config: url: \"https://api.example.com/process\" timeout: 300 # 5 minutes for this specific step ``` ### Execution Limits - **Maximum**: 1000 executions per day (per workflow) - **Rate Limiting**: 100 concurrent executions per workflow - **Burst Limit**: 10 executions per second **What happens when limits are reached:** - New executions are queued automatically - Webhook triggers return HTTP 429 (Too Many Requests) - Scheduled executions are skipped (logged in audit trail) - Email notifications sent to workflow owner **Monitoring Usage:** View real-time metrics in your workflow dashboard: - Executions today: 847 / 1000 - Average duration: 12.3s - Success rate: 98.2% - Current queue depth: 3 ### Data Limits - **Maximum request/response size**: 10MB per",
              "score": null
            }
          ],
          "latency_ms": 12.431469000148354,
          "query_id": "realistic_001",
          "dimension": "original",
          "query": "What is the API rate limit per minute?"
        },
        {
          "key_facts": [
            "100 requests per minute per authenticated user",
            "20 requests per minute for unauthenticated requests"
          ],
          "found_facts": [
            "100 requests per minute per authenticated user",
            "20 requests per minute for unauthenticated requests"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "api_reference"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_25",
              "content": "limiting: 1,000 requests per minute per API key **Workflow Engine**: - Concurrent executions: 8,000 workflows (across 16 pods) - Execution start rate: 500 per second - Completion rate: 450 per second (average 2-second execution time) **Database**: - Read throughput: 50,000 queries per second (across replicas) - Write throughput: 15,000 transactions per second - Connection capacity: 2,000 concurrent connections **Kafka**: - Message ingestion: 100,000 messages per second - Consumer throughput: 80,000 messages per second (aggregated) - End-to-end latency: < 100ms (P99) ### Resource Utilization **CPU Utilization** (Target: 60-70% average): - API Gateway: 55% average, 80% peak - Workflow Engine: 65% average, 85% peak - Auth Service: 40% average, 60% peak **Memory Utilization** (Target: < 80%): - API Gateway: 2.5GB average per pod (4GB allocated) - Workflow Engine: 6GB average per pod (8GB allocated) - Notification Service: 2.8GB average per pod (4GB allocated) **Network Throughput**: - Ingress: 2 Gbps average, 5 Gbps",
              "score": null
            },
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_3",
              "content": "## Rate Limiting To ensure fair usage and system stability, CloudFlow enforces rate limits on all API endpoints. **Default Limits:** - 100 requests per minute per authenticated user - 20 requests per minute for unauthenticated requests - Burst allowance: 150 requests in a 10-second window ### Rate Limit Headers Every API response includes rate limit information: ``` X-RateLimit-Limit: 100 X-RateLimit-Remaining: 87 X-RateLimit-Reset: 1640995200 ``` When you exceed the rate limit, you'll receive a `429 Too Many Requests` response: ```json { \"error\": { \"code\": \"rate_limit_exceeded\", \"message\": \"Rate limit exceeded. Please retry after 42 seconds.\", \"retry_after\": 42 } } ``` **Best Practices:** - Monitor `X-RateLimit-Remaining` header values - Implement exponential backoff when receiving 429 responses - Cache responses when appropriate to reduce API calls - Consider upgrading to Enterprise tier for higher limits (1000 req/min) ## Pagination List endpoints return paginated results to optimize performance.",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_17",
              "content": "Retry after 45 seconds.\", \"status\": 429, \"headers\": { \"X-RateLimit-Limit\": \"1000\", \"X-RateLimit-Remaining\": \"0\", \"X-RateLimit-Reset\": \"1706112345\", \"Retry-After\": \"45\" } } ``` #### Rate Limit Tiers CloudFlow enforces the following rate limits per workspace: | Tier | Requests/Minute | Requests/Hour | Concurrent Workflows | |------|-----------------|---------------|----------------------| | Free | 60 | 1,000 | 5 | | Standard | 1,000 | 50,000 | 50 | | Premium | 5,000 | 250,000 | 200 | | Enterprise | Custom | Custom | Unlimited | #### Checking Rate Limit Status ```bash # Check current rate limit status curl -I https://api.cloudflow.io/api/v1/workflows \\ -H \"Authorization: Bearer $CF_ACCESS_TOKEN\" # Extract rate limit headers curl -s -I https://api.cloudflow.io/api/v1/workflows \\ -H \"Authorization: Bearer $CF_ACCESS_TOKEN\" | \\ grep -i \"x-ratelimit\" # Output: # X-RateLimit-Limit: 1000 # X-RateLimit-Remaining: 247 # X-RateLimit-Reset: 1706112400 # Monitor rate limit usage cloudflow metrics query \\ --metric rate_limit_remaining \\ --workspace ws_abc123 \\ --last 1h \\ --interval 1m ``` ####",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_18",
              "content": "Handling Rate Limits in Code **Python example with retry logic:** ```python import time import requests def cloudflow_api_call_with_retry(url, headers, max_retries=3): for attempt in range(max_retries): response = requests.get(url, headers=headers) if response.status_code == 429: retry_after = int(response.headers.get('Retry-After', 60)) print(f\"Rate limited. Waiting {retry_after} seconds...\") time.sleep(retry_after) continue remaining = int(response.headers.get('X-RateLimit-Remaining', 0)) if remaining < 10: print(f\"Warning: Only {remaining} requests remaining\") return response raise Exception(\"Max retries exceeded due to rate limiting\") ``` **Bash script with rate limit checking:** ```bash #!/bin/bash check_rate_limit() { local remaining=$(curl -s -I https://api.cloudflow.io/api/v1/workflows \\ -H \"Authorization: Bearer $CF_ACCESS_TOKEN\" | \\ grep -i \"x-ratelimit-remaining\" | \\ awk '{print $2}' | tr -d '\\r') if [ \"$remaining\" -lt 10 ]; then echo \"Warning: Only $remaining requests remaining\" local reset=$(curl -s -I https://api.cloudflow.io/api/v1/workflows \\ -H \"Authorization: Bearer $CF_ACCESS_TOKEN\" | \\ grep -i \"x-ratelimit-reset\" | \\ awk '{print $2}' | tr -d '\\r') local wait_time=$((reset - $(date +%s))) echo \"Rate limit resets in $wait_time seconds\" sleep",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_19",
              "content": "$wait_time fi } # Use before API calls check_rate_limit cloudflow workflows execute wf_9k2n4m8p1q ``` #### Optimization Strategies **1. Implement request batching:** ```bash # Batch multiple workflow executions cloudflow workflows execute-batch \\ --workflow-ids \"wf_id1,wf_id2,wf_id3,wf_id4,wf_id5\" \\ --batch-size 5 # This counts as 1 API request instead of 5 ``` **2. Use webhooks instead of polling:** ```bash # Configure webhook for workflow completion cloudflow webhooks create \\ --event workflow.completed \\ --url https://your-service.com/webhooks/cloudflow \\ --secret $WEBHOOK_SECRET # Verify webhook cloudflow webhooks test webhook_abc123 ``` **3.",
              "score": null
            }
          ],
          "latency_ms": 6.602043999009766,
          "query_id": "realistic_001",
          "dimension": "synonym",
          "query": "How many requests can I make per minute to the API?"
        },
        {
          "key_facts": [
            "100 requests per minute per authenticated user",
            "20 requests per minute for unauthenticated requests"
          ],
          "found_facts": [
            "100 requests per minute per authenticated user",
            "20 requests per minute for unauthenticated requests"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "api_reference"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_3",
              "content": "## Rate Limiting To ensure fair usage and system stability, CloudFlow enforces rate limits on all API endpoints. **Default Limits:** - 100 requests per minute per authenticated user - 20 requests per minute for unauthenticated requests - Burst allowance: 150 requests in a 10-second window ### Rate Limit Headers Every API response includes rate limit information: ``` X-RateLimit-Limit: 100 X-RateLimit-Remaining: 87 X-RateLimit-Reset: 1640995200 ``` When you exceed the rate limit, you'll receive a `429 Too Many Requests` response: ```json { \"error\": { \"code\": \"rate_limit_exceeded\", \"message\": \"Rate limit exceeded. Please retry after 42 seconds.\", \"retry_after\": 42 } } ``` **Best Practices:** - Monitor `X-RateLimit-Remaining` header values - Implement exponential backoff when receiving 429 responses - Cache responses when appropriate to reduce API calls - Consider upgrading to Enterprise tier for higher limits (1000 req/min) ## Pagination List endpoints return paginated results to optimize performance.",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_18",
              "content": "Handling Rate Limits in Code **Python example with retry logic:** ```python import time import requests def cloudflow_api_call_with_retry(url, headers, max_retries=3): for attempt in range(max_retries): response = requests.get(url, headers=headers) if response.status_code == 429: retry_after = int(response.headers.get('Retry-After', 60)) print(f\"Rate limited. Waiting {retry_after} seconds...\") time.sleep(retry_after) continue remaining = int(response.headers.get('X-RateLimit-Remaining', 0)) if remaining < 10: print(f\"Warning: Only {remaining} requests remaining\") return response raise Exception(\"Max retries exceeded due to rate limiting\") ``` **Bash script with rate limit checking:** ```bash #!/bin/bash check_rate_limit() { local remaining=$(curl -s -I https://api.cloudflow.io/api/v1/workflows \\ -H \"Authorization: Bearer $CF_ACCESS_TOKEN\" | \\ grep -i \"x-ratelimit-remaining\" | \\ awk '{print $2}' | tr -d '\\r') if [ \"$remaining\" -lt 10 ]; then echo \"Warning: Only $remaining requests remaining\" local reset=$(curl -s -I https://api.cloudflow.io/api/v1/workflows \\ -H \"Authorization: Bearer $CF_ACCESS_TOKEN\" | \\ grep -i \"x-ratelimit-reset\" | \\ awk '{print $2}' | tr -d '\\r') local wait_time=$((reset - $(date +%s))) echo \"Rate limit resets in $wait_time seconds\" sleep",
              "score": null
            },
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_4",
              "content": "Use `limit` and `offset` parameters to navigate through result sets. **Parameters:** - `limit`: Number of items per page (default: 20, max: 100) - `offset`: Number of items to skip (default: 0) **Example Request:** ```bash curl -H \"X-API-Key: YOUR_API_KEY\" \\ \"https://api.cloudflow.io/v2/workflows?limit=50&offset=100\" ``` **Response Structure:** ```json { \"data\": [...], \"pagination\": { \"total\": 347, \"limit\": 50, \"offset\": 100, \"has_more\": true } } ``` **Python Example:** ```python import requests def fetch_all_workflows(api_key): base_url = \"https://api.cloudflow.io/v2/workflows\" headers = {\"X-API-Key\": api_key} all_workflows = [] offset = 0 limit = 100 while True: response = requests.get( base_url, headers=headers, params={\"limit\": limit, \"offset\": offset} ) data = response.json() all_workflows.extend(data['data']) if not data['pagination']['has_more']: break offset += limit return all_workflows ``` ## Endpoints ### Workflows #### List Workflows Retrieve a paginated list of all workflows in your account.",
              "score": null
            },
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_12",
              "content": "Waiting {retry_after} seconds...\") time.sleep(retry_after) continue elif response.status_code >= 500: # Server error - retry with exponential backoff wait_time = 2 ** attempt print(f\"Server error. Retrying in {wait_time} seconds...\") time.sleep(wait_time) continue else: # Client error - don't retry error_data = response.json() raise Exception(f\"API Error: {error_data['error']['message']}\") except requests.exceptions.RequestException as e: print(f\"Request failed: {e}\") if attempt < max_retries - 1: time.sleep(2 ** attempt) else: raise raise Exception(\"Max retries exceeded\") ``` ## Webhooks CloudFlow can send webhook notifications when specific events occur in your workflows.",
              "score": null
            },
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_10",
              "content": "format. ### Error Response Format ```json { \"error\": { \"code\": \"invalid_parameter\", \"message\": \"The 'limit' parameter must be between 1 and 100\", \"field\": \"limit\", \"request_id\": \"req_8k3m9x2p\" } } ``` ### HTTP Status Codes | Status Code | Description | Common Causes | |------------|-------------|---------------| | 400 | Bad Request | Invalid parameters, malformed JSON, validation errors | | 401 | Unauthorized | Missing or invalid authentication credentials | | 403 | Forbidden | Insufficient permissions for requested resource | | 404 | Not Found | Resource does not exist or has been deleted | | 429 | Too Many Requests | Rate limit exceeded, retry after specified period | | 500 | Internal Server Error | Unexpected server error, contact support if persists | | 502 | Bad Gateway | Temporary service issue, retry with exponential backoff | | 503 | Service Unavailable | Scheduled maintenance or temporary outage | ### Error",
              "score": null
            }
          ],
          "latency_ms": 5.772729000455001,
          "query_id": "realistic_001",
          "dimension": "problem",
          "query": "My API calls are getting blocked, what's the limit?"
        },
        {
          "key_facts": [
            "100 requests per minute per authenticated user",
            "20 requests per minute for unauthenticated requests"
          ],
          "found_facts": [
            "100 requests per minute per authenticated user",
            "20 requests per minute for unauthenticated requests"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "api_reference"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_3",
              "content": "## Rate Limiting To ensure fair usage and system stability, CloudFlow enforces rate limits on all API endpoints. **Default Limits:** - 100 requests per minute per authenticated user - 20 requests per minute for unauthenticated requests - Burst allowance: 150 requests in a 10-second window ### Rate Limit Headers Every API response includes rate limit information: ``` X-RateLimit-Limit: 100 X-RateLimit-Remaining: 87 X-RateLimit-Reset: 1640995200 ``` When you exceed the rate limit, you'll receive a `429 Too Many Requests` response: ```json { \"error\": { \"code\": \"rate_limit_exceeded\", \"message\": \"Rate limit exceeded. Please retry after 42 seconds.\", \"retry_after\": 42 } } ``` **Best Practices:** - Monitor `X-RateLimit-Remaining` header values - Implement exponential backoff when receiving 429 responses - Cache responses when appropriate to reduce API calls - Consider upgrading to Enterprise tier for higher limits (1000 req/min) ## Pagination List endpoints return paginated results to optimize performance.",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_18",
              "content": "Handling Rate Limits in Code **Python example with retry logic:** ```python import time import requests def cloudflow_api_call_with_retry(url, headers, max_retries=3): for attempt in range(max_retries): response = requests.get(url, headers=headers) if response.status_code == 429: retry_after = int(response.headers.get('Retry-After', 60)) print(f\"Rate limited. Waiting {retry_after} seconds...\") time.sleep(retry_after) continue remaining = int(response.headers.get('X-RateLimit-Remaining', 0)) if remaining < 10: print(f\"Warning: Only {remaining} requests remaining\") return response raise Exception(\"Max retries exceeded due to rate limiting\") ``` **Bash script with rate limit checking:** ```bash #!/bin/bash check_rate_limit() { local remaining=$(curl -s -I https://api.cloudflow.io/api/v1/workflows \\ -H \"Authorization: Bearer $CF_ACCESS_TOKEN\" | \\ grep -i \"x-ratelimit-remaining\" | \\ awk '{print $2}' | tr -d '\\r') if [ \"$remaining\" -lt 10 ]; then echo \"Warning: Only $remaining requests remaining\" local reset=$(curl -s -I https://api.cloudflow.io/api/v1/workflows \\ -H \"Authorization: Bearer $CF_ACCESS_TOKEN\" | \\ grep -i \"x-ratelimit-reset\" | \\ awk '{print $2}' | tr -d '\\r') local wait_time=$((reset - $(date +%s))) echo \"Rate limit resets in $wait_time seconds\" sleep",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_17",
              "content": "Retry after 45 seconds.\", \"status\": 429, \"headers\": { \"X-RateLimit-Limit\": \"1000\", \"X-RateLimit-Remaining\": \"0\", \"X-RateLimit-Reset\": \"1706112345\", \"Retry-After\": \"45\" } } ``` #### Rate Limit Tiers CloudFlow enforces the following rate limits per workspace: | Tier | Requests/Minute | Requests/Hour | Concurrent Workflows | |------|-----------------|---------------|----------------------| | Free | 60 | 1,000 | 5 | | Standard | 1,000 | 50,000 | 50 | | Premium | 5,000 | 250,000 | 200 | | Enterprise | Custom | Custom | Unlimited | #### Checking Rate Limit Status ```bash # Check current rate limit status curl -I https://api.cloudflow.io/api/v1/workflows \\ -H \"Authorization: Bearer $CF_ACCESS_TOKEN\" # Extract rate limit headers curl -s -I https://api.cloudflow.io/api/v1/workflows \\ -H \"Authorization: Bearer $CF_ACCESS_TOKEN\" | \\ grep -i \"x-ratelimit\" # Output: # X-RateLimit-Limit: 1000 # X-RateLimit-Remaining: 247 # X-RateLimit-Reset: 1706112400 # Monitor rate limit usage cloudflow metrics query \\ --metric rate_limit_remaining \\ --workspace ws_abc123 \\ --last 1h \\ --interval 1m ``` ####",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_25",
              "content": "limiting: 1,000 requests per minute per API key **Workflow Engine**: - Concurrent executions: 8,000 workflows (across 16 pods) - Execution start rate: 500 per second - Completion rate: 450 per second (average 2-second execution time) **Database**: - Read throughput: 50,000 queries per second (across replicas) - Write throughput: 15,000 transactions per second - Connection capacity: 2,000 concurrent connections **Kafka**: - Message ingestion: 100,000 messages per second - Consumer throughput: 80,000 messages per second (aggregated) - End-to-end latency: < 100ms (P99) ### Resource Utilization **CPU Utilization** (Target: 60-70% average): - API Gateway: 55% average, 80% peak - Workflow Engine: 65% average, 85% peak - Auth Service: 40% average, 60% peak **Memory Utilization** (Target: < 80%): - API Gateway: 2.5GB average per pod (4GB allocated) - Workflow Engine: 6GB average per pod (8GB allocated) - Notification Service: 2.8GB average per pod (4GB allocated) **Network Throughput**: - Ingress: 2 Gbps average, 5 Gbps",
              "score": null
            },
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_10",
              "content": "format. ### Error Response Format ```json { \"error\": { \"code\": \"invalid_parameter\", \"message\": \"The 'limit' parameter must be between 1 and 100\", \"field\": \"limit\", \"request_id\": \"req_8k3m9x2p\" } } ``` ### HTTP Status Codes | Status Code | Description | Common Causes | |------------|-------------|---------------| | 400 | Bad Request | Invalid parameters, malformed JSON, validation errors | | 401 | Unauthorized | Missing or invalid authentication credentials | | 403 | Forbidden | Insufficient permissions for requested resource | | 404 | Not Found | Resource does not exist or has been deleted | | 429 | Too Many Requests | Rate limit exceeded, retry after specified period | | 500 | Internal Server Error | Unexpected server error, contact support if persists | | 502 | Bad Gateway | Temporary service issue, retry with exponential backoff | | 503 | Service Unavailable | Scheduled maintenance or temporary outage | ### Error",
              "score": null
            }
          ],
          "latency_ms": 6.187029999637161,
          "query_id": "realistic_001",
          "dimension": "casual",
          "query": "api rate limit"
        },
        {
          "key_facts": [
            "100 requests per minute per authenticated user",
            "20 requests per minute for unauthenticated requests"
          ],
          "found_facts": [
            "100 requests per minute per authenticated user",
            "20 requests per minute for unauthenticated requests"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "api_reference"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_3",
              "content": "## Rate Limiting To ensure fair usage and system stability, CloudFlow enforces rate limits on all API endpoints. **Default Limits:** - 100 requests per minute per authenticated user - 20 requests per minute for unauthenticated requests - Burst allowance: 150 requests in a 10-second window ### Rate Limit Headers Every API response includes rate limit information: ``` X-RateLimit-Limit: 100 X-RateLimit-Remaining: 87 X-RateLimit-Reset: 1640995200 ``` When you exceed the rate limit, you'll receive a `429 Too Many Requests` response: ```json { \"error\": { \"code\": \"rate_limit_exceeded\", \"message\": \"Rate limit exceeded. Please retry after 42 seconds.\", \"retry_after\": 42 } } ``` **Best Practices:** - Monitor `X-RateLimit-Remaining` header values - Implement exponential backoff when receiving 429 responses - Cache responses when appropriate to reduce API calls - Consider upgrading to Enterprise tier for higher limits (1000 req/min) ## Pagination List endpoints return paginated results to optimize performance.",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_17",
              "content": "Retry after 45 seconds.\", \"status\": 429, \"headers\": { \"X-RateLimit-Limit\": \"1000\", \"X-RateLimit-Remaining\": \"0\", \"X-RateLimit-Reset\": \"1706112345\", \"Retry-After\": \"45\" } } ``` #### Rate Limit Tiers CloudFlow enforces the following rate limits per workspace: | Tier | Requests/Minute | Requests/Hour | Concurrent Workflows | |------|-----------------|---------------|----------------------| | Free | 60 | 1,000 | 5 | | Standard | 1,000 | 50,000 | 50 | | Premium | 5,000 | 250,000 | 200 | | Enterprise | Custom | Custom | Unlimited | #### Checking Rate Limit Status ```bash # Check current rate limit status curl -I https://api.cloudflow.io/api/v1/workflows \\ -H \"Authorization: Bearer $CF_ACCESS_TOKEN\" # Extract rate limit headers curl -s -I https://api.cloudflow.io/api/v1/workflows \\ -H \"Authorization: Bearer $CF_ACCESS_TOKEN\" | \\ grep -i \"x-ratelimit\" # Output: # X-RateLimit-Limit: 1000 # X-RateLimit-Remaining: 247 # X-RateLimit-Reset: 1706112400 # Monitor rate limit usage cloudflow metrics query \\ --metric rate_limit_remaining \\ --workspace ws_abc123 \\ --last 1h \\ --interval 1m ``` ####",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_19",
              "content": "$wait_time fi } # Use before API calls check_rate_limit cloudflow workflows execute wf_9k2n4m8p1q ``` #### Optimization Strategies **1. Implement request batching:** ```bash # Batch multiple workflow executions cloudflow workflows execute-batch \\ --workflow-ids \"wf_id1,wf_id2,wf_id3,wf_id4,wf_id5\" \\ --batch-size 5 # This counts as 1 API request instead of 5 ``` **2. Use webhooks instead of polling:** ```bash # Configure webhook for workflow completion cloudflow webhooks create \\ --event workflow.completed \\ --url https://your-service.com/webhooks/cloudflow \\ --secret $WEBHOOK_SECRET # Verify webhook cloudflow webhooks test webhook_abc123 ``` **3.",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_18",
              "content": "Handling Rate Limits in Code **Python example with retry logic:** ```python import time import requests def cloudflow_api_call_with_retry(url, headers, max_retries=3): for attempt in range(max_retries): response = requests.get(url, headers=headers) if response.status_code == 429: retry_after = int(response.headers.get('Retry-After', 60)) print(f\"Rate limited. Waiting {retry_after} seconds...\") time.sleep(retry_after) continue remaining = int(response.headers.get('X-RateLimit-Remaining', 0)) if remaining < 10: print(f\"Warning: Only {remaining} requests remaining\") return response raise Exception(\"Max retries exceeded due to rate limiting\") ``` **Bash script with rate limit checking:** ```bash #!/bin/bash check_rate_limit() { local remaining=$(curl -s -I https://api.cloudflow.io/api/v1/workflows \\ -H \"Authorization: Bearer $CF_ACCESS_TOKEN\" | \\ grep -i \"x-ratelimit-remaining\" | \\ awk '{print $2}' | tr -d '\\r') if [ \"$remaining\" -lt 10 ]; then echo \"Warning: Only $remaining requests remaining\" local reset=$(curl -s -I https://api.cloudflow.io/api/v1/workflows \\ -H \"Authorization: Bearer $CF_ACCESS_TOKEN\" | \\ grep -i \"x-ratelimit-reset\" | \\ awk '{print $2}' | tr -d '\\r') local wait_time=$((reset - $(date +%s))) echo \"Rate limit resets in $wait_time seconds\" sleep",
              "score": null
            },
            {
              "doc_id": "user_guide",
              "chunk_id": "user_guide_fix_18",
              "content": "action - **Maximum execution payload**: 50MB total - **Variable value size**: 1MB per variable ### Enterprise Plan Limits Enterprise customers can request increased limits: - Up to 100 steps per workflow - Up to 10,000 executions per day - Up to 7200 second timeout (2 hours) - Priority execution queue - Dedicated capacity allocation Contact sales@cloudflow.io for Enterprise pricing and custom limits. ## Best Practices Follow these best practices to build reliable, maintainable workflows: ### 1. Use Descriptive Names **Good:** - Workflow: \"Sync Customer Data from Salesforce to Database\" - Step: \"validate_customer_email\" **Bad:** - Workflow: \"Workflow 1\" - Step: \"step3\" ### 2.",
              "score": null
            }
          ],
          "latency_ms": 5.915625000852742,
          "query_id": "realistic_001",
          "dimension": "contextual",
          "query": "I'm building a batch job that calls CloudFlow API repeatedly, what rate limit should I expect?"
        },
        {
          "key_facts": [
            "100 requests per minute per authenticated user",
            "20 requests per minute for unauthenticated requests"
          ],
          "found_facts": [
            "100 requests per minute per authenticated user",
            "20 requests per minute for unauthenticated requests"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "api_reference"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "user_guide",
              "chunk_id": "user_guide_fix_14",
              "content": "### Retry Policies Configure automatic retries for failed actions: ```yaml - id: api_call action: http_request config: url: \"https://api.example.com/data\" retry: max_attempts: 3 backoff_type: \"exponential\" # or \"fixed\", \"linear\" initial_interval: 1000 # milliseconds max_interval: 30000 multiplier: 2.0 retry_on: - timeout - network_error - status: [500, 502, 503, 504] ``` **Backoff Strategies:** - **Fixed**: Wait the same amount of time between retries - Attempt 1: Wait 1s - Attempt 2: Wait 1s - Attempt 3: Wait 1s - **Linear**: Increase wait time by a fixed amount - Attempt 1: Wait 1s - Attempt 2: Wait 2s - Attempt 3: Wait 3s - **Exponential**: Double the wait time with each retry (recommended) - Attempt 1: Wait 1s - Attempt 2: Wait 2s - Attempt 3: Wait 4s **Retry Conditions:** Control which errors trigger retries: - `timeout`: Request timeout - `network_error`: Connection failures - `status`: Specific HTTP status codes - `error_code`: Application-specific error codes ###",
              "score": null
            },
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_12",
              "content": "Waiting {retry_after} seconds...\") time.sleep(retry_after) continue elif response.status_code >= 500: # Server error - retry with exponential backoff wait_time = 2 ** attempt print(f\"Server error. Retrying in {wait_time} seconds...\") time.sleep(wait_time) continue else: # Client error - don't retry error_data = response.json() raise Exception(f\"API Error: {error_data['error']['message']}\") except requests.exceptions.RequestException as e: print(f\"Request failed: {e}\") if attempt < max_retries - 1: time.sleep(2 ** attempt) else: raise raise Exception(\"Max retries exceeded\") ``` ## Webhooks CloudFlow can send webhook notifications when specific events occur in your workflows.",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_18",
              "content": "Handling Rate Limits in Code **Python example with retry logic:** ```python import time import requests def cloudflow_api_call_with_retry(url, headers, max_retries=3): for attempt in range(max_retries): response = requests.get(url, headers=headers) if response.status_code == 429: retry_after = int(response.headers.get('Retry-After', 60)) print(f\"Rate limited. Waiting {retry_after} seconds...\") time.sleep(retry_after) continue remaining = int(response.headers.get('X-RateLimit-Remaining', 0)) if remaining < 10: print(f\"Warning: Only {remaining} requests remaining\") return response raise Exception(\"Max retries exceeded due to rate limiting\") ``` **Bash script with rate limit checking:** ```bash #!/bin/bash check_rate_limit() { local remaining=$(curl -s -I https://api.cloudflow.io/api/v1/workflows \\ -H \"Authorization: Bearer $CF_ACCESS_TOKEN\" | \\ grep -i \"x-ratelimit-remaining\" | \\ awk '{print $2}' | tr -d '\\r') if [ \"$remaining\" -lt 10 ]; then echo \"Warning: Only $remaining requests remaining\" local reset=$(curl -s -I https://api.cloudflow.io/api/v1/workflows \\ -H \"Authorization: Bearer $CF_ACCESS_TOKEN\" | \\ grep -i \"x-ratelimit-reset\" | \\ awk '{print $2}' | tr -d '\\r') local wait_time=$((reset - $(date +%s))) echo \"Rate limit resets in $wait_time seconds\" sleep",
              "score": null
            },
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_3",
              "content": "## Rate Limiting To ensure fair usage and system stability, CloudFlow enforces rate limits on all API endpoints. **Default Limits:** - 100 requests per minute per authenticated user - 20 requests per minute for unauthenticated requests - Burst allowance: 150 requests in a 10-second window ### Rate Limit Headers Every API response includes rate limit information: ``` X-RateLimit-Limit: 100 X-RateLimit-Remaining: 87 X-RateLimit-Reset: 1640995200 ``` When you exceed the rate limit, you'll receive a `429 Too Many Requests` response: ```json { \"error\": { \"code\": \"rate_limit_exceeded\", \"message\": \"Rate limit exceeded. Please retry after 42 seconds.\", \"retry_after\": 42 } } ``` **Best Practices:** - Monitor `X-RateLimit-Remaining` header values - Implement exponential backoff when receiving 429 responses - Cache responses when appropriate to reduce API calls - Consider upgrading to Enterprise tier for higher limits (1000 req/min) ## Pagination List endpoints return paginated results to optimize performance.",
              "score": null
            },
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_10",
              "content": "format. ### Error Response Format ```json { \"error\": { \"code\": \"invalid_parameter\", \"message\": \"The 'limit' parameter must be between 1 and 100\", \"field\": \"limit\", \"request_id\": \"req_8k3m9x2p\" } } ``` ### HTTP Status Codes | Status Code | Description | Common Causes | |------------|-------------|---------------| | 400 | Bad Request | Invalid parameters, malformed JSON, validation errors | | 401 | Unauthorized | Missing or invalid authentication credentials | | 403 | Forbidden | Insufficient permissions for requested resource | | 404 | Not Found | Resource does not exist or has been deleted | | 429 | Too Many Requests | Rate limit exceeded, retry after specified period | | 500 | Internal Server Error | Unexpected server error, contact support if persists | | 502 | Bad Gateway | Temporary service issue, retry with exponential backoff | | 503 | Service Unavailable | Scheduled maintenance or temporary outage | ### Error",
              "score": null
            }
          ],
          "latency_ms": 5.681187998561654,
          "query_id": "realistic_001",
          "dimension": "negation",
          "query": "Why is the API rejecting my requests after 100 calls?"
        },
        {
          "key_facts": [
            "429 Too Many Requests",
            "X-RateLimit-Remaining",
            "Retry-After",
            "Monitor X-RateLimit-Remaining header values",
            "Implement exponential backoff when receiving 429 responses"
          ],
          "found_facts": [
            "429 Too Many Requests"
          ],
          "missed_facts": [
            "X-RateLimit-Remaining",
            "Retry-After",
            "Monitor X-RateLimit-Remaining header values",
            "Implement exponential backoff when receiving 429 responses"
          ],
          "coverage": 0.2,
          "expected_docs": [
            "api_reference",
            "troubleshooting_guide"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_10",
              "content": "format. ### Error Response Format ```json { \"error\": { \"code\": \"invalid_parameter\", \"message\": \"The 'limit' parameter must be between 1 and 100\", \"field\": \"limit\", \"request_id\": \"req_8k3m9x2p\" } } ``` ### HTTP Status Codes | Status Code | Description | Common Causes | |------------|-------------|---------------| | 400 | Bad Request | Invalid parameters, malformed JSON, validation errors | | 401 | Unauthorized | Missing or invalid authentication credentials | | 403 | Forbidden | Insufficient permissions for requested resource | | 404 | Not Found | Resource does not exist or has been deleted | | 429 | Too Many Requests | Rate limit exceeded, retry after specified period | | 500 | Internal Server Error | Unexpected server error, contact support if persists | | 502 | Bad Gateway | Temporary service issue, retry with exponential backoff | | 503 | Service Unavailable | Scheduled maintenance or temporary outage | ### Error",
              "score": null
            },
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_11",
              "content": "Codes CloudFlow returns specific error codes to help you identify and resolve issues: - `invalid_parameter`: One or more request parameters are invalid - `missing_required_field`: Required field is missing from request body - `authentication_failed`: Invalid API key or token - `insufficient_permissions`: User lacks required scope or permission - `resource_not_found`: Requested resource does not exist - `rate_limit_exceeded`: Too many requests, see rate limiting section - `workflow_execution_failed`: Workflow execution encountered an error - `invalid_json`: Request body contains malformed JSON - `duplicate_resource`: Resource with same identifier already exists - `quota_exceeded`: Account quota limit reached ### Error Handling Best Practices **Python Example:** ```python import requests import time def make_api_request(url, headers, max_retries=3): for attempt in range(max_retries): try: response = requests.get(url, headers=headers) if response.status_code == 200: return response.json() elif response.status_code == 429: # Rate limit exceeded retry_after = int(response.headers.get('X-RateLimit-Reset', 60)) print(f\"Rate limited.",
              "score": null
            },
            {
              "doc_id": "user_guide",
              "chunk_id": "user_guide_fix_14",
              "content": "### Retry Policies Configure automatic retries for failed actions: ```yaml - id: api_call action: http_request config: url: \"https://api.example.com/data\" retry: max_attempts: 3 backoff_type: \"exponential\" # or \"fixed\", \"linear\" initial_interval: 1000 # milliseconds max_interval: 30000 multiplier: 2.0 retry_on: - timeout - network_error - status: [500, 502, 503, 504] ``` **Backoff Strategies:** - **Fixed**: Wait the same amount of time between retries - Attempt 1: Wait 1s - Attempt 2: Wait 1s - Attempt 3: Wait 1s - **Linear**: Increase wait time by a fixed amount - Attempt 1: Wait 1s - Attempt 2: Wait 2s - Attempt 3: Wait 3s - **Exponential**: Double the wait time with each retry (recommended) - Attempt 1: Wait 1s - Attempt 2: Wait 2s - Attempt 3: Wait 4s **Retry Conditions:** Control which errors trigger retries: - `timeout`: Request timeout - `network_error`: Connection failures - `status`: Specific HTTP status codes - `error_code`: Application-specific error codes ###",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_16",
              "content": "Data Validation Errors** ``` ValidationError: Field 'customer_id' is required but missing in 234 records ``` **Resolution:** ```bash # Add data quality checks cloudflow workflows update wf_9k2n4m8p1q \\ --step data_ingestion \\ --add-validator required_fields \\ --validator-config '{\"fields\": [\"customer_id\", \"timestamp\", \"amount\"]}' # Configure error handling cloudflow workflows update wf_9k2n4m8p1q \\ --step data_validation \\ --on-error continue \\ --error-threshold 5% # Fail if > 5% of records invalid ``` **2. External API Failures** ``` ExternalAPIError: API request to https://partner-api.example.com failed with status 502 ``` **Resolution:** ```bash # Add circuit breaker cloudflow workflows update wf_9k2n4m8p1q \\ --step external_api_call \\ --circuit-breaker-enabled true \\ --circuit-breaker-threshold 5 \\ --circuit-breaker-timeout 30000 # Configure fallback behavior cloudflow workflows update wf_9k2n4m8p1q \\ --step external_api_call \\ --fallback-action use_cached_data \\ --cache-ttl 3600 ``` --- ## Rate Limiting & Throttling ### 429 Too Many Requests #### Error Response ```json { \"error\": \"rate_limit_exceeded\", \"message\": \"API rate limit exceeded.",
              "score": null
            },
            {
              "doc_id": "user_guide",
              "chunk_id": "user_guide_fix_19",
              "content": "Handle Errors Gracefully Always implement error handling for external API calls and database operations: ```yaml - id: fetch_data action: http_request config: url: \"https://api.example.com/data\" retry: max_attempts: 3 backoff_type: exponential on_error: - id: log_error action: database_query config: query: \"INSERT INTO error_log (workflow_id, error) VALUES ($1, $2)\" parameters: - \"{{workflow.id}}\" - \"{{error.message}}\" ``` ### 3. Use Secrets for Sensitive Data Never hardcode API keys, passwords, or tokens in workflows: **Bad:** ```yaml headers: Authorization: \"Bearer sk_live_abc123xyz789\" ``` **Good:** ```yaml headers: Authorization: \"Bearer {{secrets.API_TOKEN}}\" ``` Store secrets in **Settings** > **Secrets** with encryption at rest. ### 4.",
              "score": null
            }
          ],
          "latency_ms": 5.667531999279163,
          "query_id": "realistic_002",
          "dimension": "original",
          "query": "How do I fix 429 Too Many Requests errors?"
        },
        {
          "key_facts": [
            "429 Too Many Requests",
            "X-RateLimit-Remaining",
            "Retry-After",
            "Monitor X-RateLimit-Remaining header values",
            "Implement exponential backoff when receiving 429 responses"
          ],
          "found_facts": [
            "429 Too Many Requests",
            "X-RateLimit-Remaining",
            "Retry-After",
            "Implement exponential backoff when receiving 429 responses"
          ],
          "missed_facts": [
            "Monitor X-RateLimit-Remaining header values"
          ],
          "coverage": 0.8,
          "expected_docs": [
            "api_reference",
            "troubleshooting_guide"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_18",
              "content": "Handling Rate Limits in Code **Python example with retry logic:** ```python import time import requests def cloudflow_api_call_with_retry(url, headers, max_retries=3): for attempt in range(max_retries): response = requests.get(url, headers=headers) if response.status_code == 429: retry_after = int(response.headers.get('Retry-After', 60)) print(f\"Rate limited. Waiting {retry_after} seconds...\") time.sleep(retry_after) continue remaining = int(response.headers.get('X-RateLimit-Remaining', 0)) if remaining < 10: print(f\"Warning: Only {remaining} requests remaining\") return response raise Exception(\"Max retries exceeded due to rate limiting\") ``` **Bash script with rate limit checking:** ```bash #!/bin/bash check_rate_limit() { local remaining=$(curl -s -I https://api.cloudflow.io/api/v1/workflows \\ -H \"Authorization: Bearer $CF_ACCESS_TOKEN\" | \\ grep -i \"x-ratelimit-remaining\" | \\ awk '{print $2}' | tr -d '\\r') if [ \"$remaining\" -lt 10 ]; then echo \"Warning: Only $remaining requests remaining\" local reset=$(curl -s -I https://api.cloudflow.io/api/v1/workflows \\ -H \"Authorization: Bearer $CF_ACCESS_TOKEN\" | \\ grep -i \"x-ratelimit-reset\" | \\ awk '{print $2}' | tr -d '\\r') local wait_time=$((reset - $(date +%s))) echo \"Rate limit resets in $wait_time seconds\" sleep",
              "score": null
            },
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_3",
              "content": "## Rate Limiting To ensure fair usage and system stability, CloudFlow enforces rate limits on all API endpoints. **Default Limits:** - 100 requests per minute per authenticated user - 20 requests per minute for unauthenticated requests - Burst allowance: 150 requests in a 10-second window ### Rate Limit Headers Every API response includes rate limit information: ``` X-RateLimit-Limit: 100 X-RateLimit-Remaining: 87 X-RateLimit-Reset: 1640995200 ``` When you exceed the rate limit, you'll receive a `429 Too Many Requests` response: ```json { \"error\": { \"code\": \"rate_limit_exceeded\", \"message\": \"Rate limit exceeded. Please retry after 42 seconds.\", \"retry_after\": 42 } } ``` **Best Practices:** - Monitor `X-RateLimit-Remaining` header values - Implement exponential backoff when receiving 429 responses - Cache responses when appropriate to reduce API calls - Consider upgrading to Enterprise tier for higher limits (1000 req/min) ## Pagination List endpoints return paginated results to optimize performance.",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_17",
              "content": "Retry after 45 seconds.\", \"status\": 429, \"headers\": { \"X-RateLimit-Limit\": \"1000\", \"X-RateLimit-Remaining\": \"0\", \"X-RateLimit-Reset\": \"1706112345\", \"Retry-After\": \"45\" } } ``` #### Rate Limit Tiers CloudFlow enforces the following rate limits per workspace: | Tier | Requests/Minute | Requests/Hour | Concurrent Workflows | |------|-----------------|---------------|----------------------| | Free | 60 | 1,000 | 5 | | Standard | 1,000 | 50,000 | 50 | | Premium | 5,000 | 250,000 | 200 | | Enterprise | Custom | Custom | Unlimited | #### Checking Rate Limit Status ```bash # Check current rate limit status curl -I https://api.cloudflow.io/api/v1/workflows \\ -H \"Authorization: Bearer $CF_ACCESS_TOKEN\" # Extract rate limit headers curl -s -I https://api.cloudflow.io/api/v1/workflows \\ -H \"Authorization: Bearer $CF_ACCESS_TOKEN\" | \\ grep -i \"x-ratelimit\" # Output: # X-RateLimit-Limit: 1000 # X-RateLimit-Remaining: 247 # X-RateLimit-Reset: 1706112400 # Monitor rate limit usage cloudflow metrics query \\ --metric rate_limit_remaining \\ --workspace ws_abc123 \\ --last 1h \\ --interval 1m ``` ####",
              "score": null
            },
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_4",
              "content": "Use `limit` and `offset` parameters to navigate through result sets. **Parameters:** - `limit`: Number of items per page (default: 20, max: 100) - `offset`: Number of items to skip (default: 0) **Example Request:** ```bash curl -H \"X-API-Key: YOUR_API_KEY\" \\ \"https://api.cloudflow.io/v2/workflows?limit=50&offset=100\" ``` **Response Structure:** ```json { \"data\": [...], \"pagination\": { \"total\": 347, \"limit\": 50, \"offset\": 100, \"has_more\": true } } ``` **Python Example:** ```python import requests def fetch_all_workflows(api_key): base_url = \"https://api.cloudflow.io/v2/workflows\" headers = {\"X-API-Key\": api_key} all_workflows = [] offset = 0 limit = 100 while True: response = requests.get( base_url, headers=headers, params={\"limit\": limit, \"offset\": offset} ) data = response.json() all_workflows.extend(data['data']) if not data['pagination']['has_more']: break offset += limit return all_workflows ``` ## Endpoints ### Workflows #### List Workflows Retrieve a paginated list of all workflows in your account.",
              "score": null
            },
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_10",
              "content": "format. ### Error Response Format ```json { \"error\": { \"code\": \"invalid_parameter\", \"message\": \"The 'limit' parameter must be between 1 and 100\", \"field\": \"limit\", \"request_id\": \"req_8k3m9x2p\" } } ``` ### HTTP Status Codes | Status Code | Description | Common Causes | |------------|-------------|---------------| | 400 | Bad Request | Invalid parameters, malformed JSON, validation errors | | 401 | Unauthorized | Missing or invalid authentication credentials | | 403 | Forbidden | Insufficient permissions for requested resource | | 404 | Not Found | Resource does not exist or has been deleted | | 429 | Too Many Requests | Rate limit exceeded, retry after specified period | | 500 | Internal Server Error | Unexpected server error, contact support if persists | | 502 | Bad Gateway | Temporary service issue, retry with exponential backoff | | 503 | Service Unavailable | Scheduled maintenance or temporary outage | ### Error",
              "score": null
            }
          ],
          "latency_ms": 5.742561999795726,
          "query_id": "realistic_002",
          "dimension": "synonym",
          "query": "What should I do when I get rate limited?"
        },
        {
          "key_facts": [
            "429 Too Many Requests",
            "X-RateLimit-Remaining",
            "Retry-After",
            "Monitor X-RateLimit-Remaining header values",
            "Implement exponential backoff when receiving 429 responses"
          ],
          "found_facts": [
            "X-RateLimit-Remaining",
            "Retry-After"
          ],
          "missed_facts": [
            "429 Too Many Requests",
            "Monitor X-RateLimit-Remaining header values",
            "Implement exponential backoff when receiving 429 responses"
          ],
          "coverage": 0.4,
          "expected_docs": [
            "api_reference",
            "troubleshooting_guide"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_10",
              "content": "format. ### Error Response Format ```json { \"error\": { \"code\": \"invalid_parameter\", \"message\": \"The 'limit' parameter must be between 1 and 100\", \"field\": \"limit\", \"request_id\": \"req_8k3m9x2p\" } } ``` ### HTTP Status Codes | Status Code | Description | Common Causes | |------------|-------------|---------------| | 400 | Bad Request | Invalid parameters, malformed JSON, validation errors | | 401 | Unauthorized | Missing or invalid authentication credentials | | 403 | Forbidden | Insufficient permissions for requested resource | | 404 | Not Found | Resource does not exist or has been deleted | | 429 | Too Many Requests | Rate limit exceeded, retry after specified period | | 500 | Internal Server Error | Unexpected server error, contact support if persists | | 502 | Bad Gateway | Temporary service issue, retry with exponential backoff | | 503 | Service Unavailable | Scheduled maintenance or temporary outage | ### Error",
              "score": null
            },
            {
              "doc_id": "user_guide",
              "chunk_id": "user_guide_fix_14",
              "content": "### Retry Policies Configure automatic retries for failed actions: ```yaml - id: api_call action: http_request config: url: \"https://api.example.com/data\" retry: max_attempts: 3 backoff_type: \"exponential\" # or \"fixed\", \"linear\" initial_interval: 1000 # milliseconds max_interval: 30000 multiplier: 2.0 retry_on: - timeout - network_error - status: [500, 502, 503, 504] ``` **Backoff Strategies:** - **Fixed**: Wait the same amount of time between retries - Attempt 1: Wait 1s - Attempt 2: Wait 1s - Attempt 3: Wait 1s - **Linear**: Increase wait time by a fixed amount - Attempt 1: Wait 1s - Attempt 2: Wait 2s - Attempt 3: Wait 3s - **Exponential**: Double the wait time with each retry (recommended) - Attempt 1: Wait 1s - Attempt 2: Wait 2s - Attempt 3: Wait 4s **Retry Conditions:** Control which errors trigger retries: - `timeout`: Request timeout - `network_error`: Connection failures - `status`: Specific HTTP status codes - `error_code`: Application-specific error codes ###",
              "score": null
            },
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_11",
              "content": "Codes CloudFlow returns specific error codes to help you identify and resolve issues: - `invalid_parameter`: One or more request parameters are invalid - `missing_required_field`: Required field is missing from request body - `authentication_failed`: Invalid API key or token - `insufficient_permissions`: User lacks required scope or permission - `resource_not_found`: Requested resource does not exist - `rate_limit_exceeded`: Too many requests, see rate limiting section - `workflow_execution_failed`: Workflow execution encountered an error - `invalid_json`: Request body contains malformed JSON - `duplicate_resource`: Resource with same identifier already exists - `quota_exceeded`: Account quota limit reached ### Error Handling Best Practices **Python Example:** ```python import requests import time def make_api_request(url, headers, max_retries=3): for attempt in range(max_retries): try: response = requests.get(url, headers=headers) if response.status_code == 200: return response.json() elif response.status_code == 429: # Rate limit exceeded retry_after = int(response.headers.get('X-RateLimit-Reset', 60)) print(f\"Rate limited.",
              "score": null
            },
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_12",
              "content": "Waiting {retry_after} seconds...\") time.sleep(retry_after) continue elif response.status_code >= 500: # Server error - retry with exponential backoff wait_time = 2 ** attempt print(f\"Server error. Retrying in {wait_time} seconds...\") time.sleep(wait_time) continue else: # Client error - don't retry error_data = response.json() raise Exception(f\"API Error: {error_data['error']['message']}\") except requests.exceptions.RequestException as e: print(f\"Request failed: {e}\") if attempt < max_retries - 1: time.sleep(2 ** attempt) else: raise raise Exception(\"Max retries exceeded\") ``` ## Webhooks CloudFlow can send webhook notifications when specific events occur in your workflows.",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_17",
              "content": "Retry after 45 seconds.\", \"status\": 429, \"headers\": { \"X-RateLimit-Limit\": \"1000\", \"X-RateLimit-Remaining\": \"0\", \"X-RateLimit-Reset\": \"1706112345\", \"Retry-After\": \"45\" } } ``` #### Rate Limit Tiers CloudFlow enforces the following rate limits per workspace: | Tier | Requests/Minute | Requests/Hour | Concurrent Workflows | |------|-----------------|---------------|----------------------| | Free | 60 | 1,000 | 5 | | Standard | 1,000 | 50,000 | 50 | | Premium | 5,000 | 250,000 | 200 | | Enterprise | Custom | Custom | Unlimited | #### Checking Rate Limit Status ```bash # Check current rate limit status curl -I https://api.cloudflow.io/api/v1/workflows \\ -H \"Authorization: Bearer $CF_ACCESS_TOKEN\" # Extract rate limit headers curl -s -I https://api.cloudflow.io/api/v1/workflows \\ -H \"Authorization: Bearer $CF_ACCESS_TOKEN\" | \\ grep -i \"x-ratelimit\" # Output: # X-RateLimit-Limit: 1000 # X-RateLimit-Remaining: 247 # X-RateLimit-Reset: 1706112400 # Monitor rate limit usage cloudflow metrics query \\ --metric rate_limit_remaining \\ --workspace ws_abc123 \\ --last 1h \\ --interval 1m ``` ####",
              "score": null
            }
          ],
          "latency_ms": 5.477517999679549,
          "query_id": "realistic_002",
          "dimension": "problem",
          "query": "My requests keep failing with 429 status code"
        },
        {
          "key_facts": [
            "429 Too Many Requests",
            "X-RateLimit-Remaining",
            "Retry-After",
            "Monitor X-RateLimit-Remaining header values",
            "Implement exponential backoff when receiving 429 responses"
          ],
          "found_facts": [
            "429 Too Many Requests"
          ],
          "missed_facts": [
            "X-RateLimit-Remaining",
            "Retry-After",
            "Monitor X-RateLimit-Remaining header values",
            "Implement exponential backoff when receiving 429 responses"
          ],
          "coverage": 0.2,
          "expected_docs": [
            "api_reference",
            "troubleshooting_guide"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "user_guide",
              "chunk_id": "user_guide_fix_16",
              "content": "fixing issues - Export failed executions for analysis - Set up alerts for DLQ threshold breaches **Accessing the DLQ:** 1. Navigate to **Workflows** > **[Your Workflow]** > **Dead Letter Queue** 2. Filter by error type, date range, or execution ID 3. Click an execution to view full details 4. Click **\"Retry\"** to reprocess with the same input data ### Error Notifications Get notified when workflows fail: ```yaml workflow: notifications: on_failure: - type: email to: \"ops-team@company.com\" - type: slack channel: \"#alerts\" message: \"Workflow {{workflow.name}} failed: {{error.message}}\" on_success_after_retry: - type: slack channel: \"#monitoring\" message: \"Workflow recovered after {{error.attempts}} attempts\" ``` ## Workflow Limits CloudFlow enforces the following limits to ensure platform stability and performance: ### Steps Per Workflow - **Maximum**: 50 steps per workflow - **Recommendation**: Keep workflows focused and modular. If you need more steps, consider splitting into multiple workflows connected via webhooks.",
              "score": null
            },
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_10",
              "content": "format. ### Error Response Format ```json { \"error\": { \"code\": \"invalid_parameter\", \"message\": \"The 'limit' parameter must be between 1 and 100\", \"field\": \"limit\", \"request_id\": \"req_8k3m9x2p\" } } ``` ### HTTP Status Codes | Status Code | Description | Common Causes | |------------|-------------|---------------| | 400 | Bad Request | Invalid parameters, malformed JSON, validation errors | | 401 | Unauthorized | Missing or invalid authentication credentials | | 403 | Forbidden | Insufficient permissions for requested resource | | 404 | Not Found | Resource does not exist or has been deleted | | 429 | Too Many Requests | Rate limit exceeded, retry after specified period | | 500 | Internal Server Error | Unexpected server error, contact support if persists | | 502 | Bad Gateway | Temporary service issue, retry with exponential backoff | | 503 | Service Unavailable | Scheduled maintenance or temporary outage | ### Error",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_2",
              "content": "**Verification:** ```bash # Decode JWT to check expiration echo $CF_ACCESS_TOKEN | cut -d'.' -f2 | base64 -d | jq '.exp' # Compare with current time date +%s ``` **Resolution:** ```bash # Refresh the access token curl -X POST https://api.cloudflow.io/auth/refresh \\ -H \"Content-Type: application/json\" \\ -d '{\"refresh_token\": \"'$CF_REFRESH_TOKEN'\"}' # Update environment variable export CF_ACCESS_TOKEN=\"<new_token>\" ``` **2. Invalid JWT Signature** Error: `JWT signature verification failed` **Causes:** - Token was modified or corrupted - Using wrong signing key - Token generated with different secret **Resolution:** ```bash # Validate token structure cloudflow auth validate --token $CF_ACCESS_TOKEN # Generate new token cloudflow auth login --username <user> --password <pass> # For service accounts cloudflow auth service-account --create --name <sa-name> --scopes \"workflow:read,workflow:write\" ``` **3. Clock Skew Issues** JWT validation fails when system clocks are out of sync (tolerance: \u00b1300 seconds).",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_16",
              "content": "Data Validation Errors** ``` ValidationError: Field 'customer_id' is required but missing in 234 records ``` **Resolution:** ```bash # Add data quality checks cloudflow workflows update wf_9k2n4m8p1q \\ --step data_ingestion \\ --add-validator required_fields \\ --validator-config '{\"fields\": [\"customer_id\", \"timestamp\", \"amount\"]}' # Configure error handling cloudflow workflows update wf_9k2n4m8p1q \\ --step data_validation \\ --on-error continue \\ --error-threshold 5% # Fail if > 5% of records invalid ``` **2. External API Failures** ``` ExternalAPIError: API request to https://partner-api.example.com failed with status 502 ``` **Resolution:** ```bash # Add circuit breaker cloudflow workflows update wf_9k2n4m8p1q \\ --step external_api_call \\ --circuit-breaker-enabled true \\ --circuit-breaker-threshold 5 \\ --circuit-breaker-timeout 30000 # Configure fallback behavior cloudflow workflows update wf_9k2n4m8p1q \\ --step external_api_call \\ --fallback-action use_cached_data \\ --cache-ttl 3600 ``` --- ## Rate Limiting & Throttling ### 429 Too Many Requests #### Error Response ```json { \"error\": \"rate_limit_exceeded\", \"message\": \"API rate limit exceeded.",
              "score": null
            },
            {
              "doc_id": "user_guide",
              "chunk_id": "user_guide_fix_14",
              "content": "### Retry Policies Configure automatic retries for failed actions: ```yaml - id: api_call action: http_request config: url: \"https://api.example.com/data\" retry: max_attempts: 3 backoff_type: \"exponential\" # or \"fixed\", \"linear\" initial_interval: 1000 # milliseconds max_interval: 30000 multiplier: 2.0 retry_on: - timeout - network_error - status: [500, 502, 503, 504] ``` **Backoff Strategies:** - **Fixed**: Wait the same amount of time between retries - Attempt 1: Wait 1s - Attempt 2: Wait 1s - Attempt 3: Wait 1s - **Linear**: Increase wait time by a fixed amount - Attempt 1: Wait 1s - Attempt 2: Wait 2s - Attempt 3: Wait 3s - **Exponential**: Double the wait time with each retry (recommended) - Attempt 1: Wait 1s - Attempt 2: Wait 2s - Attempt 3: Wait 4s **Retry Conditions:** Control which errors trigger retries: - `timeout`: Request timeout - `network_error`: Connection failures - `status`: Specific HTTP status codes - `error_code`: Application-specific error codes ###",
              "score": null
            }
          ],
          "latency_ms": 5.38966500062088,
          "query_id": "realistic_002",
          "dimension": "casual",
          "query": "429 error fix"
        },
        {
          "key_facts": [
            "429 Too Many Requests",
            "X-RateLimit-Remaining",
            "Retry-After",
            "Monitor X-RateLimit-Remaining header values",
            "Implement exponential backoff when receiving 429 responses"
          ],
          "found_facts": [
            "429 Too Many Requests",
            "X-RateLimit-Remaining",
            "Retry-After",
            "Implement exponential backoff when receiving 429 responses"
          ],
          "missed_facts": [
            "Monitor X-RateLimit-Remaining header values"
          ],
          "coverage": 0.8,
          "expected_docs": [
            "api_reference",
            "troubleshooting_guide"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_3",
              "content": "## Rate Limiting To ensure fair usage and system stability, CloudFlow enforces rate limits on all API endpoints. **Default Limits:** - 100 requests per minute per authenticated user - 20 requests per minute for unauthenticated requests - Burst allowance: 150 requests in a 10-second window ### Rate Limit Headers Every API response includes rate limit information: ``` X-RateLimit-Limit: 100 X-RateLimit-Remaining: 87 X-RateLimit-Reset: 1640995200 ``` When you exceed the rate limit, you'll receive a `429 Too Many Requests` response: ```json { \"error\": { \"code\": \"rate_limit_exceeded\", \"message\": \"Rate limit exceeded. Please retry after 42 seconds.\", \"retry_after\": 42 } } ``` **Best Practices:** - Monitor `X-RateLimit-Remaining` header values - Implement exponential backoff when receiving 429 responses - Cache responses when appropriate to reduce API calls - Consider upgrading to Enterprise tier for higher limits (1000 req/min) ## Pagination List endpoints return paginated results to optimize performance.",
              "score": null
            },
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_0",
              "content": "# CloudFlow API Reference Version 2.1.0 | Last Updated: January 2026 ## Overview The CloudFlow API is a RESTful service that enables developers to programmatically manage cloud workflows, data pipelines, and automation tasks. This documentation provides comprehensive details on authentication, endpoints, request/response formats, error handling, and best practices. **Base URL:** `https://api.cloudflow.io/v2` **API Status:** https://status.cloudflow.io ## Authentication CloudFlow supports three authentication methods to suit different use cases and security requirements. ### API Keys API keys provide simple authentication for server-to-server communication. Include your API key in the request header: ```bash curl -H \"X-API-Key: cf_live_a1b2c3d4e5f6g7h8i9j0\" \\ https://api.cloudflow.io/v2/workflows ``` **Security Notes:** - Never expose API keys in client-side code - Rotate keys every 90 days - Use separate keys for development and production environments ### OAuth 2.0 OAuth 2.0 is recommended for applications that access CloudFlow on behalf of users. We support the Authorization Code flow with PKCE.",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_0",
              "content": "# CloudFlow Platform Troubleshooting Guide **Version:** 3.2.1 **Last Updated:** January 2026 **Audience:** Platform Engineers, DevOps, Support Teams ## Table of Contents 1. [Overview](#overview) 2. [Authentication & Authorization Issues](#authentication--authorization-issues) 3. [Performance Problems](#performance-problems) 4. [Database Connection Issues](#database-connection-issues) 5. [Workflow Execution Failures](#workflow-execution-failures) 6. [Rate Limiting & Throttling](#rate-limiting--throttling) 7. [Log Analysis & Debugging](#log-analysis--debugging) 8. [Escalation Procedures](#escalation-procedures) --- ## Overview This guide provides comprehensive troubleshooting steps for common CloudFlow platform issues encountered in production environments. Each section includes error symptoms, root cause analysis, resolution steps, and preventive measures.",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_26",
              "content": "Major functionality impaired affecting multiple users - **Examples:** - Workflow execution success rate < 90% - Significant performance degradation (p95 latency > 10s) - Rate limiting affecting large customer segment - **Response Time:** < 1 hour - **Escalation:** Create incident ticket and notify on-call #### SEV-3: Medium (P3) - **Definition:** Partial functionality degraded affecting some users - **Examples:** - Intermittent failures for specific workflow types - Minor performance issues - Non-critical feature unavailable - **Response Time:** < 4 hours - **Escalation:** Create ticket, normal business hours support #### SEV-4: Low (P4) - **Definition:** Minor issues with minimal user impact - **Examples:** - Cosmetic issues - Documentation errors - Feature requests - **Response Time:** < 2 business days - **Escalation:** Standard support ticket ### Escalation Steps #### Step 1: Initial Assessment (0-5 minutes) ```bash # Run health check cloudflow health check --comprehensive # Check status page curl https://status.cloudflow.io/api/v1/status.json # Review recent",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_18",
              "content": "Handling Rate Limits in Code **Python example with retry logic:** ```python import time import requests def cloudflow_api_call_with_retry(url, headers, max_retries=3): for attempt in range(max_retries): response = requests.get(url, headers=headers) if response.status_code == 429: retry_after = int(response.headers.get('Retry-After', 60)) print(f\"Rate limited. Waiting {retry_after} seconds...\") time.sleep(retry_after) continue remaining = int(response.headers.get('X-RateLimit-Remaining', 0)) if remaining < 10: print(f\"Warning: Only {remaining} requests remaining\") return response raise Exception(\"Max retries exceeded due to rate limiting\") ``` **Bash script with rate limit checking:** ```bash #!/bin/bash check_rate_limit() { local remaining=$(curl -s -I https://api.cloudflow.io/api/v1/workflows \\ -H \"Authorization: Bearer $CF_ACCESS_TOKEN\" | \\ grep -i \"x-ratelimit-remaining\" | \\ awk '{print $2}' | tr -d '\\r') if [ \"$remaining\" -lt 10 ]; then echo \"Warning: Only $remaining requests remaining\" local reset=$(curl -s -I https://api.cloudflow.io/api/v1/workflows \\ -H \"Authorization: Bearer $CF_ACCESS_TOKEN\" | \\ grep -i \"x-ratelimit-reset\" | \\ awk '{print $2}' | tr -d '\\r') local wait_time=$((reset - $(date +%s))) echo \"Rate limit resets in $wait_time seconds\" sleep",
              "score": null
            }
          ],
          "latency_ms": 5.902871000216692,
          "query_id": "realistic_002",
          "dimension": "contextual",
          "query": "I'm getting throttled by CloudFlow API, how can I handle this in my application?"
        },
        {
          "key_facts": [
            "429 Too Many Requests",
            "X-RateLimit-Remaining",
            "Retry-After",
            "Monitor X-RateLimit-Remaining header values",
            "Implement exponential backoff when receiving 429 responses"
          ],
          "found_facts": [
            "429 Too Many Requests",
            "X-RateLimit-Remaining",
            "Retry-After",
            "Implement exponential backoff when receiving 429 responses"
          ],
          "missed_facts": [
            "Monitor X-RateLimit-Remaining header values"
          ],
          "coverage": 0.8,
          "expected_docs": [
            "api_reference",
            "troubleshooting_guide"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_3",
              "content": "## Rate Limiting To ensure fair usage and system stability, CloudFlow enforces rate limits on all API endpoints. **Default Limits:** - 100 requests per minute per authenticated user - 20 requests per minute for unauthenticated requests - Burst allowance: 150 requests in a 10-second window ### Rate Limit Headers Every API response includes rate limit information: ``` X-RateLimit-Limit: 100 X-RateLimit-Remaining: 87 X-RateLimit-Reset: 1640995200 ``` When you exceed the rate limit, you'll receive a `429 Too Many Requests` response: ```json { \"error\": { \"code\": \"rate_limit_exceeded\", \"message\": \"Rate limit exceeded. Please retry after 42 seconds.\", \"retry_after\": 42 } } ``` **Best Practices:** - Monitor `X-RateLimit-Remaining` header values - Implement exponential backoff when receiving 429 responses - Cache responses when appropriate to reduce API calls - Consider upgrading to Enterprise tier for higher limits (1000 req/min) ## Pagination List endpoints return paginated results to optimize performance.",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_18",
              "content": "Handling Rate Limits in Code **Python example with retry logic:** ```python import time import requests def cloudflow_api_call_with_retry(url, headers, max_retries=3): for attempt in range(max_retries): response = requests.get(url, headers=headers) if response.status_code == 429: retry_after = int(response.headers.get('Retry-After', 60)) print(f\"Rate limited. Waiting {retry_after} seconds...\") time.sleep(retry_after) continue remaining = int(response.headers.get('X-RateLimit-Remaining', 0)) if remaining < 10: print(f\"Warning: Only {remaining} requests remaining\") return response raise Exception(\"Max retries exceeded due to rate limiting\") ``` **Bash script with rate limit checking:** ```bash #!/bin/bash check_rate_limit() { local remaining=$(curl -s -I https://api.cloudflow.io/api/v1/workflows \\ -H \"Authorization: Bearer $CF_ACCESS_TOKEN\" | \\ grep -i \"x-ratelimit-remaining\" | \\ awk '{print $2}' | tr -d '\\r') if [ \"$remaining\" -lt 10 ]; then echo \"Warning: Only $remaining requests remaining\" local reset=$(curl -s -I https://api.cloudflow.io/api/v1/workflows \\ -H \"Authorization: Bearer $CF_ACCESS_TOKEN\" | \\ grep -i \"x-ratelimit-reset\" | \\ awk '{print $2}' | tr -d '\\r') local wait_time=$((reset - $(date +%s))) echo \"Rate limit resets in $wait_time seconds\" sleep",
              "score": null
            },
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_10",
              "content": "format. ### Error Response Format ```json { \"error\": { \"code\": \"invalid_parameter\", \"message\": \"The 'limit' parameter must be between 1 and 100\", \"field\": \"limit\", \"request_id\": \"req_8k3m9x2p\" } } ``` ### HTTP Status Codes | Status Code | Description | Common Causes | |------------|-------------|---------------| | 400 | Bad Request | Invalid parameters, malformed JSON, validation errors | | 401 | Unauthorized | Missing or invalid authentication credentials | | 403 | Forbidden | Insufficient permissions for requested resource | | 404 | Not Found | Resource does not exist or has been deleted | | 429 | Too Many Requests | Rate limit exceeded, retry after specified period | | 500 | Internal Server Error | Unexpected server error, contact support if persists | | 502 | Bad Gateway | Temporary service issue, retry with exponential backoff | | 503 | Service Unavailable | Scheduled maintenance or temporary outage | ### Error",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_17",
              "content": "Retry after 45 seconds.\", \"status\": 429, \"headers\": { \"X-RateLimit-Limit\": \"1000\", \"X-RateLimit-Remaining\": \"0\", \"X-RateLimit-Reset\": \"1706112345\", \"Retry-After\": \"45\" } } ``` #### Rate Limit Tiers CloudFlow enforces the following rate limits per workspace: | Tier | Requests/Minute | Requests/Hour | Concurrent Workflows | |------|-----------------|---------------|----------------------| | Free | 60 | 1,000 | 5 | | Standard | 1,000 | 50,000 | 50 | | Premium | 5,000 | 250,000 | 200 | | Enterprise | Custom | Custom | Unlimited | #### Checking Rate Limit Status ```bash # Check current rate limit status curl -I https://api.cloudflow.io/api/v1/workflows \\ -H \"Authorization: Bearer $CF_ACCESS_TOKEN\" # Extract rate limit headers curl -s -I https://api.cloudflow.io/api/v1/workflows \\ -H \"Authorization: Bearer $CF_ACCESS_TOKEN\" | \\ grep -i \"x-ratelimit\" # Output: # X-RateLimit-Limit: 1000 # X-RateLimit-Remaining: 247 # X-RateLimit-Reset: 1706112400 # Monitor rate limit usage cloudflow metrics query \\ --metric rate_limit_remaining \\ --workspace ws_abc123 \\ --last 1h \\ --interval 1m ``` ####",
              "score": null
            },
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_11",
              "content": "Codes CloudFlow returns specific error codes to help you identify and resolve issues: - `invalid_parameter`: One or more request parameters are invalid - `missing_required_field`: Required field is missing from request body - `authentication_failed`: Invalid API key or token - `insufficient_permissions`: User lacks required scope or permission - `resource_not_found`: Requested resource does not exist - `rate_limit_exceeded`: Too many requests, see rate limiting section - `workflow_execution_failed`: Workflow execution encountered an error - `invalid_json`: Request body contains malformed JSON - `duplicate_resource`: Resource with same identifier already exists - `quota_exceeded`: Account quota limit reached ### Error Handling Best Practices **Python Example:** ```python import requests import time def make_api_request(url, headers, max_retries=3): for attempt in range(max_retries): try: response = requests.get(url, headers=headers) if response.status_code == 200: return response.json() elif response.status_code == 429: # Rate limit exceeded retry_after = int(response.headers.get('X-RateLimit-Reset', 60)) print(f\"Rate limited.",
              "score": null
            }
          ],
          "latency_ms": 5.630483999993885,
          "query_id": "realistic_002",
          "dimension": "negation",
          "query": "Why am I being blocked with rate limit errors?"
        },
        {
          "key_facts": [
            "3600 seconds",
            "max 3600 seconds from iat",
            "All tokens expire after 3600 seconds"
          ],
          "found_facts": [
            "3600 seconds",
            "All tokens expire after 3600 seconds"
          ],
          "missed_facts": [
            "max 3600 seconds from iat"
          ],
          "coverage": 0.6666666666666666,
          "expected_docs": [
            "api_reference"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_2",
              "content": "JWTs must include the following claims: - `iss` (issuer): Your application identifier - `sub` (subject): User or service account ID - `aud` (audience): `https://api.cloudflow.io` - `exp` (expiration): Unix timestamp (max 3600 seconds from `iat`) - `iat` (issued at): Unix timestamp - `scope`: Space-separated list of requested scopes Example JWT header: ```python import jwt import time from cryptography.hazmat.primitives import serialization from cryptography.hazmat.backends import default_backend # Load your private key with open('private_key.pem', 'rb') as key_file: private_key = serialization.load_pem_private_key( key_file.read(), password=None, backend=default_backend() ) # Create JWT payload payload = { 'iss': 'your-app-id', 'sub': 'user-12345', 'aud': 'https://api.cloudflow.io', 'exp': int(time.time()) + 3600, 'iat': int(time.time()), 'scope': 'workflows:read workflows:write' } # Generate token token = jwt.encode(payload, private_key, algorithm='RS256') # Use in API request headers = {'Authorization': f'Bearer {token}'} ``` **Token Expiration:** All tokens expire after 3600 seconds (1 hour). Implement token refresh logic in your application.",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_2",
              "content": "**Verification:** ```bash # Decode JWT to check expiration echo $CF_ACCESS_TOKEN | cut -d'.' -f2 | base64 -d | jq '.exp' # Compare with current time date +%s ``` **Resolution:** ```bash # Refresh the access token curl -X POST https://api.cloudflow.io/auth/refresh \\ -H \"Content-Type: application/json\" \\ -d '{\"refresh_token\": \"'$CF_REFRESH_TOKEN'\"}' # Update environment variable export CF_ACCESS_TOKEN=\"<new_token>\" ``` **2. Invalid JWT Signature** Error: `JWT signature verification failed` **Causes:** - Token was modified or corrupted - Using wrong signing key - Token generated with different secret **Resolution:** ```bash # Validate token structure cloudflow auth validate --token $CF_ACCESS_TOKEN # Generate new token cloudflow auth login --username <user> --password <pass> # For service accounts cloudflow auth service-account --create --name <sa-name> --scopes \"workflow:read,workflow:write\" ``` **3. Clock Skew Issues** JWT validation fails when system clocks are out of sync (tolerance: \u00b1300 seconds).",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_4",
              "content": "authorization service handling user identity, token generation, and permission validation. **Technology**: Go with gRPC for internal communication, REST for external **Replicas**: 8 pods (production), auto-scaling 6-12 **Resource Allocation**: 1 vCPU, 2GB RAM per pod **Key Responsibilities**: - User authentication via multiple providers (OAuth2, SAML, local credentials) - JWT token generation and validation (RS256 algorithm) - Role-based access control (RBAC) with fine-grained permissions - Session management with Redis-backed storage - API key generation and validation for service accounts - MFA enforcement for administrative operations **Authentication Flow**: ``` Client Request \u2192 API Gateway \u2192 Auth Service \u2502 \u251c\u2500 Validate credentials \u251c\u2500 Check MFA if required \u251c\u2500 Generate JWT (15min expiry) \u251c\u2500 Generate refresh token (7 days) \u2514\u2500 Store session in Redis ``` **Token Structure**: - Access Token: JWT with 15-minute expiry - Refresh Token: Opaque token with 7-day expiry, stored in PostgreSQL - Claims: user_id, email, roles[], permissions[], tenant_id **Security Features**: -",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_3",
              "content": "**Diagnosis:** ```bash # Check system time timedatectl status # Compare with NTP server ntpdate -q pool.ntp.org # Check JWT issued time vs current time jwt_iat=$(echo $CF_ACCESS_TOKEN | cut -d'.' -f2 | base64 -d | jq '.iat') current_time=$(date +%s) skew=$((current_time - jwt_iat)) echo \"Clock skew: $skew seconds\" ``` **Resolution:** ```bash # Sync with NTP server sudo ntpdate -s pool.ntp.org # Enable automatic time sync sudo timedatectl set-ntp true # Restart CloudFlow client cloudflow restart ``` **4. Insufficient Permissions** Error: `User does not have required permissions for this operation` **Check user roles:** ```bash cloudflow auth whoami --verbose # Expected output: # User: john.doe@company.com # Roles: developer, workflow-admin # Scopes: workflow:*, data:read, metrics:read ``` **Request permission elevation:** ```bash # Submit access request cloudflow auth request-access \\ --resource \"workflow:production:deploy\" \\ --justification \"Deploy critical hotfix for TICKET-1234\" # Check pending approvals cloudflow auth list-requests --status pending ``` ### 403 Forbidden Errors #### RBAC Policy Violations",
              "score": null
            },
            {
              "doc_id": "user_guide",
              "chunk_id": "user_guide_fix_17",
              "content": "### Execution Timeout - **Default**: 3600 seconds (60 minutes) - **Behavior**: Workflows exceeding this timeout are automatically terminated - **Custom Timeouts**: Enterprise plans can request custom timeout limits **Setting Step-Level Timeouts:** ```yaml - id: long_running_task action: http_request config: url: \"https://api.example.com/process\" timeout: 300 # 5 minutes for this specific step ``` ### Execution Limits - **Maximum**: 1000 executions per day (per workflow) - **Rate Limiting**: 100 concurrent executions per workflow - **Burst Limit**: 10 executions per second **What happens when limits are reached:** - New executions are queued automatically - Webhook triggers return HTTP 429 (Too Many Requests) - Scheduled executions are skipped (logged in audit trail) - Email notifications sent to workflow owner **Monitoring Usage:** View real-time metrics in your workflow dashboard: - Executions today: 847 / 1000 - Average duration: 12.3s - Success rate: 98.2% - Current queue depth: 3 ### Data Limits - **Maximum request/response size**: 10MB per",
              "score": null
            }
          ],
          "latency_ms": 5.495090999829699,
          "query_id": "realistic_003",
          "dimension": "original",
          "query": "What is the JWT token expiration time?"
        },
        {
          "key_facts": [
            "3600 seconds",
            "max 3600 seconds from iat",
            "All tokens expire after 3600 seconds"
          ],
          "found_facts": [
            "3600 seconds",
            "All tokens expire after 3600 seconds"
          ],
          "missed_facts": [
            "max 3600 seconds from iat"
          ],
          "coverage": 0.6666666666666666,
          "expected_docs": [
            "api_reference"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_4",
              "content": "authorization service handling user identity, token generation, and permission validation. **Technology**: Go with gRPC for internal communication, REST for external **Replicas**: 8 pods (production), auto-scaling 6-12 **Resource Allocation**: 1 vCPU, 2GB RAM per pod **Key Responsibilities**: - User authentication via multiple providers (OAuth2, SAML, local credentials) - JWT token generation and validation (RS256 algorithm) - Role-based access control (RBAC) with fine-grained permissions - Session management with Redis-backed storage - API key generation and validation for service accounts - MFA enforcement for administrative operations **Authentication Flow**: ``` Client Request \u2192 API Gateway \u2192 Auth Service \u2502 \u251c\u2500 Validate credentials \u251c\u2500 Check MFA if required \u251c\u2500 Generate JWT (15min expiry) \u251c\u2500 Generate refresh token (7 days) \u2514\u2500 Store session in Redis ``` **Token Structure**: - Access Token: JWT with 15-minute expiry - Refresh Token: Opaque token with 7-day expiry, stored in PostgreSQL - Claims: user_id, email, roles[], permissions[], tenant_id **Security Features**: -",
              "score": null
            },
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_2",
              "content": "JWTs must include the following claims: - `iss` (issuer): Your application identifier - `sub` (subject): User or service account ID - `aud` (audience): `https://api.cloudflow.io` - `exp` (expiration): Unix timestamp (max 3600 seconds from `iat`) - `iat` (issued at): Unix timestamp - `scope`: Space-separated list of requested scopes Example JWT header: ```python import jwt import time from cryptography.hazmat.primitives import serialization from cryptography.hazmat.backends import default_backend # Load your private key with open('private_key.pem', 'rb') as key_file: private_key = serialization.load_pem_private_key( key_file.read(), password=None, backend=default_backend() ) # Create JWT payload payload = { 'iss': 'your-app-id', 'sub': 'user-12345', 'aud': 'https://api.cloudflow.io', 'exp': int(time.time()) + 3600, 'iat': int(time.time()), 'scope': 'workflows:read workflows:write' } # Generate token token = jwt.encode(payload, private_key, algorithm='RS256') # Use in API request headers = {'Authorization': f'Bearer {token}'} ``` **Token Expiration:** All tokens expire after 3600 seconds (1 hour). Implement token refresh logic in your application.",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_5",
              "content": "Password hashing: Argon2id with 64MB memory, 4 iterations - Token rotation on refresh to prevent replay attacks - Brute force protection: 5 failed attempts \u2192 15-minute lockout - Secrets stored in HashiCorp Vault, rotated every 90 days **Performance Targets**: - Token validation: < 10ms (P99) - Token generation: < 50ms (P99) - Throughput: 5,000 RPS for validation operations --- ### Workflow Engine **Purpose**: Core orchestration service that executes workflow definitions, manages state transitions, and coordinates task execution across distributed systems.",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_31",
              "content": "DNS propagation slower than expected (resolved) - Success criteria: Met all recovery objectives ### Business Continuity **Communication Plan**: - Status page: status.cloudflow.com (updated every 15 minutes during incident) - Customer notifications: Email + SMS for all P1 incidents - Internal escalation: PagerDuty \u2192 Incident Commander \u2192 Engineering Manager \u2192 CTO **Data Retention Policy**: - Active data: 12 months in hot storage (PostgreSQL) - Archived data: 7 years in cold storage (S3 Glacier) - Audit logs: 7 years (compliance requirement) - Backup retention: 30 days standard, 90 days monthly snapshots --- ## Appendix ### Service Dependency Matrix | Service | Depends On | Critical Path | |---------|-----------|---------------| | API Gateway | Auth Service, Redis | Yes | | Auth Service | PostgreSQL, Redis, Vault | Yes | | Workflow Engine | PostgreSQL, Kafka, Redis | Yes | | Scheduler | PostgreSQL, Redis, Kafka | No | | Notification Service | Kafka,",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_22",
              "content": "- Token revocation: Blacklist in Redis for compromised tokens **Permission Model**: ``` User \u2192 Roles \u2192 Permissions \u2198 \u2197 Tenants (Multi-tenancy isolation) ``` Example permissions: - `workflow:read` - View workflows - `workflow:write` - Create/update workflows - `workflow:execute` - Trigger workflow execution - `workflow:delete` - Delete workflows - `admin:*` - All administrative operations **API Key Management**: - Format: `cfk_live_<32-char-random>` (production), `cfk_test_<32-char-random>` (sandbox) - Hashing: SHA-256 before storage in PostgreSQL - Scoping: API keys can be scoped to specific workflows or operations - Rate limits: Configurable per API key (default: 1000 RPM) ### Secrets Management **HashiCorp Vault Integration**: - Dynamic database credentials: Generated on-demand, 1-hour TTL - Encryption keys: Transit secrets engine for encryption-as-a-service - API keys for external services: Stored with versioning - Rotation policy: Automated rotation every 90 days with notification **Secret Access Pattern**: ``` Service starts \u2192 Vault authentication (Kubernetes service account) \u2502 \u25bc Request secret lease (1-hour TTL)",
              "score": null
            }
          ],
          "latency_ms": 5.70715599860705,
          "query_id": "realistic_003",
          "dimension": "synonym",
          "query": "How long do access tokens last?"
        },
        {
          "key_facts": [
            "3600 seconds",
            "max 3600 seconds from iat",
            "All tokens expire after 3600 seconds"
          ],
          "found_facts": [
            "3600 seconds",
            "All tokens expire after 3600 seconds"
          ],
          "missed_facts": [
            "max 3600 seconds from iat"
          ],
          "coverage": 0.6666666666666666,
          "expected_docs": [
            "api_reference"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_2",
              "content": "**Verification:** ```bash # Decode JWT to check expiration echo $CF_ACCESS_TOKEN | cut -d'.' -f2 | base64 -d | jq '.exp' # Compare with current time date +%s ``` **Resolution:** ```bash # Refresh the access token curl -X POST https://api.cloudflow.io/auth/refresh \\ -H \"Content-Type: application/json\" \\ -d '{\"refresh_token\": \"'$CF_REFRESH_TOKEN'\"}' # Update environment variable export CF_ACCESS_TOKEN=\"<new_token>\" ``` **2. Invalid JWT Signature** Error: `JWT signature verification failed` **Causes:** - Token was modified or corrupted - Using wrong signing key - Token generated with different secret **Resolution:** ```bash # Validate token structure cloudflow auth validate --token $CF_ACCESS_TOKEN # Generate new token cloudflow auth login --username <user> --password <pass> # For service accounts cloudflow auth service-account --create --name <sa-name> --scopes \"workflow:read,workflow:write\" ``` **3. Clock Skew Issues** JWT validation fails when system clocks are out of sync (tolerance: \u00b1300 seconds).",
              "score": null
            },
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_2",
              "content": "JWTs must include the following claims: - `iss` (issuer): Your application identifier - `sub` (subject): User or service account ID - `aud` (audience): `https://api.cloudflow.io` - `exp` (expiration): Unix timestamp (max 3600 seconds from `iat`) - `iat` (issued at): Unix timestamp - `scope`: Space-separated list of requested scopes Example JWT header: ```python import jwt import time from cryptography.hazmat.primitives import serialization from cryptography.hazmat.backends import default_backend # Load your private key with open('private_key.pem', 'rb') as key_file: private_key = serialization.load_pem_private_key( key_file.read(), password=None, backend=default_backend() ) # Create JWT payload payload = { 'iss': 'your-app-id', 'sub': 'user-12345', 'aud': 'https://api.cloudflow.io', 'exp': int(time.time()) + 3600, 'iat': int(time.time()), 'scope': 'workflows:read workflows:write' } # Generate token token = jwt.encode(payload, private_key, algorithm='RS256') # Use in API request headers = {'Authorization': f'Bearer {token}'} ``` **Token Expiration:** All tokens expire after 3600 seconds (1 hour). Implement token refresh logic in your application.",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_4",
              "content": "authorization service handling user identity, token generation, and permission validation. **Technology**: Go with gRPC for internal communication, REST for external **Replicas**: 8 pods (production), auto-scaling 6-12 **Resource Allocation**: 1 vCPU, 2GB RAM per pod **Key Responsibilities**: - User authentication via multiple providers (OAuth2, SAML, local credentials) - JWT token generation and validation (RS256 algorithm) - Role-based access control (RBAC) with fine-grained permissions - Session management with Redis-backed storage - API key generation and validation for service accounts - MFA enforcement for administrative operations **Authentication Flow**: ``` Client Request \u2192 API Gateway \u2192 Auth Service \u2502 \u251c\u2500 Validate credentials \u251c\u2500 Check MFA if required \u251c\u2500 Generate JWT (15min expiry) \u251c\u2500 Generate refresh token (7 days) \u2514\u2500 Store session in Redis ``` **Token Structure**: - Access Token: JWT with 15-minute expiry - Refresh Token: Opaque token with 7-day expiry, stored in PostgreSQL - Claims: user_id, email, roles[], permissions[], tenant_id **Security Features**: -",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_3",
              "content": "**Diagnosis:** ```bash # Check system time timedatectl status # Compare with NTP server ntpdate -q pool.ntp.org # Check JWT issued time vs current time jwt_iat=$(echo $CF_ACCESS_TOKEN | cut -d'.' -f2 | base64 -d | jq '.iat') current_time=$(date +%s) skew=$((current_time - jwt_iat)) echo \"Clock skew: $skew seconds\" ``` **Resolution:** ```bash # Sync with NTP server sudo ntpdate -s pool.ntp.org # Enable automatic time sync sudo timedatectl set-ntp true # Restart CloudFlow client cloudflow restart ``` **4. Insufficient Permissions** Error: `User does not have required permissions for this operation` **Check user roles:** ```bash cloudflow auth whoami --verbose # Expected output: # User: john.doe@company.com # Roles: developer, workflow-admin # Scopes: workflow:*, data:read, metrics:read ``` **Request permission elevation:** ```bash # Submit access request cloudflow auth request-access \\ --resource \"workflow:production:deploy\" \\ --justification \"Deploy critical hotfix for TICKET-1234\" # Check pending approvals cloudflow auth list-requests --status pending ``` ### 403 Forbidden Errors #### RBAC Policy Violations",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_1",
              "content": "### Quick Diagnostic Checklist Before diving into specific issues, perform these initial checks: - Verify service health: `cloudflow status --all` - Check API connectivity: `curl -I https://api.cloudflow.io/health` - Review recent deployments: `kubectl get deployments -n cloudflow --sort-by=.metadata.creationTimestamp` - Inspect platform metrics: `cloudflow metrics --last 1h` --- ## Authentication & Authorization Issues ### 401 Unauthorized Errors #### Symptoms - API requests return `401 Unauthorized` - Error message: `Authentication credentials were not provided or are invalid` - Frontend displays \"Session expired\" message #### Common Causes **1. Token Expiration** CloudFlow access tokens expire after 3600 seconds (1 hour) by default. Refresh tokens are valid for 30 days.",
              "score": null
            }
          ],
          "latency_ms": 5.297421999785001,
          "query_id": "realistic_003",
          "dimension": "problem",
          "query": "My authentication keeps expiring after an hour"
        },
        {
          "key_facts": [
            "3600 seconds",
            "max 3600 seconds from iat",
            "All tokens expire after 3600 seconds"
          ],
          "found_facts": [
            "3600 seconds",
            "All tokens expire after 3600 seconds"
          ],
          "missed_facts": [
            "max 3600 seconds from iat"
          ],
          "coverage": 0.6666666666666666,
          "expected_docs": [
            "api_reference"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_2",
              "content": "JWTs must include the following claims: - `iss` (issuer): Your application identifier - `sub` (subject): User or service account ID - `aud` (audience): `https://api.cloudflow.io` - `exp` (expiration): Unix timestamp (max 3600 seconds from `iat`) - `iat` (issued at): Unix timestamp - `scope`: Space-separated list of requested scopes Example JWT header: ```python import jwt import time from cryptography.hazmat.primitives import serialization from cryptography.hazmat.backends import default_backend # Load your private key with open('private_key.pem', 'rb') as key_file: private_key = serialization.load_pem_private_key( key_file.read(), password=None, backend=default_backend() ) # Create JWT payload payload = { 'iss': 'your-app-id', 'sub': 'user-12345', 'aud': 'https://api.cloudflow.io', 'exp': int(time.time()) + 3600, 'iat': int(time.time()), 'scope': 'workflows:read workflows:write' } # Generate token token = jwt.encode(payload, private_key, algorithm='RS256') # Use in API request headers = {'Authorization': f'Bearer {token}'} ``` **Token Expiration:** All tokens expire after 3600 seconds (1 hour). Implement token refresh logic in your application.",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_22",
              "content": "- Token revocation: Blacklist in Redis for compromised tokens **Permission Model**: ``` User \u2192 Roles \u2192 Permissions \u2198 \u2197 Tenants (Multi-tenancy isolation) ``` Example permissions: - `workflow:read` - View workflows - `workflow:write` - Create/update workflows - `workflow:execute` - Trigger workflow execution - `workflow:delete` - Delete workflows - `admin:*` - All administrative operations **API Key Management**: - Format: `cfk_live_<32-char-random>` (production), `cfk_test_<32-char-random>` (sandbox) - Hashing: SHA-256 before storage in PostgreSQL - Scoping: API keys can be scoped to specific workflows or operations - Rate limits: Configurable per API key (default: 1000 RPM) ### Secrets Management **HashiCorp Vault Integration**: - Dynamic database credentials: Generated on-demand, 1-hour TTL - Encryption keys: Transit secrets engine for encryption-as-a-service - API keys for external services: Stored with versioning - Rotation policy: Automated rotation every 90 days with notification **Secret Access Pattern**: ``` Service starts \u2192 Vault authentication (Kubernetes service account) \u2502 \u25bc Request secret lease (1-hour TTL)",
              "score": null
            },
            {
              "doc_id": "user_guide",
              "chunk_id": "user_guide_fix_17",
              "content": "### Execution Timeout - **Default**: 3600 seconds (60 minutes) - **Behavior**: Workflows exceeding this timeout are automatically terminated - **Custom Timeouts**: Enterprise plans can request custom timeout limits **Setting Step-Level Timeouts:** ```yaml - id: long_running_task action: http_request config: url: \"https://api.example.com/process\" timeout: 300 # 5 minutes for this specific step ``` ### Execution Limits - **Maximum**: 1000 executions per day (per workflow) - **Rate Limiting**: 100 concurrent executions per workflow - **Burst Limit**: 10 executions per second **What happens when limits are reached:** - New executions are queued automatically - Webhook triggers return HTTP 429 (Too Many Requests) - Scheduled executions are skipped (logged in audit trail) - Email notifications sent to workflow owner **Monitoring Usage:** View real-time metrics in your workflow dashboard: - Executions today: 847 / 1000 - Average duration: 12.3s - Success rate: 98.2% - Current queue depth: 3 ### Data Limits - **Maximum request/response size**: 10MB per",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_16",
              "content": "(created, started, completed, failed) - Retention: 7 days - Message rate: 5,000/sec peak - Consumer groups: workflow-engine, analytics-pipeline notifications.email (16 partitions): - Email notification triggers - Retention: 3 days - Message rate: 2,000/sec peak - Consumer groups: notification-service notifications.sms (8 partitions): - SMS notification triggers - Retention: 3 days - Message rate: 500/sec peak - Consumer groups: notification-service audit.logs (24 partitions): - Security and compliance audit events - Retention: 90 days (compliance requirement) - Message rate: 3,000/sec peak - Consumer groups: audit-processor, security-monitor dead-letter-queue (8 partitions): - Failed message processing - Retention: 30 days - Manual intervention required - Consumer groups: ops-team-alerts ``` **Producer Configuration**: - Acknowledgment: `acks=all` (wait for all in-sync replicas) - Compression: LZ4 (reduces network bandwidth by 60%) - Batching: 100ms linger time, 100KB batch size - Idempotence: Enabled to prevent duplicates **Consumer Configuration**: - Auto-commit: Disabled (manual commit after processing) - Offset reset: Earliest (replay from",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_2",
              "content": "**Verification:** ```bash # Decode JWT to check expiration echo $CF_ACCESS_TOKEN | cut -d'.' -f2 | base64 -d | jq '.exp' # Compare with current time date +%s ``` **Resolution:** ```bash # Refresh the access token curl -X POST https://api.cloudflow.io/auth/refresh \\ -H \"Content-Type: application/json\" \\ -d '{\"refresh_token\": \"'$CF_REFRESH_TOKEN'\"}' # Update environment variable export CF_ACCESS_TOKEN=\"<new_token>\" ``` **2. Invalid JWT Signature** Error: `JWT signature verification failed` **Causes:** - Token was modified or corrupted - Using wrong signing key - Token generated with different secret **Resolution:** ```bash # Validate token structure cloudflow auth validate --token $CF_ACCESS_TOKEN # Generate new token cloudflow auth login --username <user> --password <pass> # For service accounts cloudflow auth service-account --create --name <sa-name> --scopes \"workflow:read,workflow:write\" ``` **3. Clock Skew Issues** JWT validation fails when system clocks are out of sync (tolerance: \u00b1300 seconds).",
              "score": null
            }
          ],
          "latency_ms": 5.440939999971306,
          "query_id": "realistic_003",
          "dimension": "casual",
          "query": "token expiry time"
        },
        {
          "key_facts": [
            "3600 seconds",
            "max 3600 seconds from iat",
            "All tokens expire after 3600 seconds"
          ],
          "found_facts": [
            "3600 seconds",
            "All tokens expire after 3600 seconds"
          ],
          "missed_facts": [
            "max 3600 seconds from iat"
          ],
          "coverage": 0.6666666666666666,
          "expected_docs": [
            "api_reference"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_2",
              "content": "JWTs must include the following claims: - `iss` (issuer): Your application identifier - `sub` (subject): User or service account ID - `aud` (audience): `https://api.cloudflow.io` - `exp` (expiration): Unix timestamp (max 3600 seconds from `iat`) - `iat` (issued at): Unix timestamp - `scope`: Space-separated list of requested scopes Example JWT header: ```python import jwt import time from cryptography.hazmat.primitives import serialization from cryptography.hazmat.backends import default_backend # Load your private key with open('private_key.pem', 'rb') as key_file: private_key = serialization.load_pem_private_key( key_file.read(), password=None, backend=default_backend() ) # Create JWT payload payload = { 'iss': 'your-app-id', 'sub': 'user-12345', 'aud': 'https://api.cloudflow.io', 'exp': int(time.time()) + 3600, 'iat': int(time.time()), 'scope': 'workflows:read workflows:write' } # Generate token token = jwt.encode(payload, private_key, algorithm='RS256') # Use in API request headers = {'Authorization': f'Bearer {token}'} ``` **Token Expiration:** All tokens expire after 3600 seconds (1 hour). Implement token refresh logic in your application.",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_2",
              "content": "**Verification:** ```bash # Decode JWT to check expiration echo $CF_ACCESS_TOKEN | cut -d'.' -f2 | base64 -d | jq '.exp' # Compare with current time date +%s ``` **Resolution:** ```bash # Refresh the access token curl -X POST https://api.cloudflow.io/auth/refresh \\ -H \"Content-Type: application/json\" \\ -d '{\"refresh_token\": \"'$CF_REFRESH_TOKEN'\"}' # Update environment variable export CF_ACCESS_TOKEN=\"<new_token>\" ``` **2. Invalid JWT Signature** Error: `JWT signature verification failed` **Causes:** - Token was modified or corrupted - Using wrong signing key - Token generated with different secret **Resolution:** ```bash # Validate token structure cloudflow auth validate --token $CF_ACCESS_TOKEN # Generate new token cloudflow auth login --username <user> --password <pass> # For service accounts cloudflow auth service-account --create --name <sa-name> --scopes \"workflow:read,workflow:write\" ``` **3. Clock Skew Issues** JWT validation fails when system clocks are out of sync (tolerance: \u00b1300 seconds).",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_4",
              "content": "authorization service handling user identity, token generation, and permission validation. **Technology**: Go with gRPC for internal communication, REST for external **Replicas**: 8 pods (production), auto-scaling 6-12 **Resource Allocation**: 1 vCPU, 2GB RAM per pod **Key Responsibilities**: - User authentication via multiple providers (OAuth2, SAML, local credentials) - JWT token generation and validation (RS256 algorithm) - Role-based access control (RBAC) with fine-grained permissions - Session management with Redis-backed storage - API key generation and validation for service accounts - MFA enforcement for administrative operations **Authentication Flow**: ``` Client Request \u2192 API Gateway \u2192 Auth Service \u2502 \u251c\u2500 Validate credentials \u251c\u2500 Check MFA if required \u251c\u2500 Generate JWT (15min expiry) \u251c\u2500 Generate refresh token (7 days) \u2514\u2500 Store session in Redis ``` **Token Structure**: - Access Token: JWT with 15-minute expiry - Refresh Token: Opaque token with 7-day expiry, stored in PostgreSQL - Claims: user_id, email, roles[], permissions[], tenant_id **Security Features**: -",
              "score": null
            },
            {
              "doc_id": "user_guide",
              "chunk_id": "user_guide_fix_25",
              "content": "- id: process_payment action: http_request config: method: POST url: \"https://api.accounting.com/payments\" body: amount: \"{{trigger.body.amount}}\" recipient: \"{{trigger.body.requester}}\" ``` ### Pattern 4: Data Synchronization Keep two systems in sync bidirectionally: ```yaml name: \"Sync Customers: CRM to Database\" trigger: type: event source: \"salesforce\" event: \"customer.updated\" steps: - id: fetch_customer action: http_request config: url: \"https://api.salesforce.com/customers/{{trigger.customer_id}}\" headers: Authorization: \"Bearer {{secrets.SALESFORCE_TOKEN}}\" - id: check_existing action: database_query config: query: \"SELECT id, last_updated FROM customers WHERE salesforce_id = $1\" parameters: - \"{{trigger.customer_id}}\" - id: update_or_insert action: database_query config: query: | INSERT INTO customers (salesforce_id, name, email, phone, last_updated) VALUES ($1, $2, $3, $4, NOW()) ON CONFLICT (salesforce_id) DO UPDATE SET name = $2, email = $3, phone = $4, last_updated = NOW() parameters: - \"{{trigger.customer_id}}\" - \"{{steps.fetch_customer.body.name}}\" - \"{{steps.fetch_customer.body.email}}\" - \"{{steps.fetch_customer.body.phone}}\" - id: log_sync action: database_query config: query: | INSERT INTO sync_log (source, target, record_id, synced_at) VALUES ('salesforce', 'postgresql', $1, NOW()) parameters: - \"{{trigger.customer_id}}\" ``` ### Pattern 5: Error",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_22",
              "content": "- Token revocation: Blacklist in Redis for compromised tokens **Permission Model**: ``` User \u2192 Roles \u2192 Permissions \u2198 \u2197 Tenants (Multi-tenancy isolation) ``` Example permissions: - `workflow:read` - View workflows - `workflow:write` - Create/update workflows - `workflow:execute` - Trigger workflow execution - `workflow:delete` - Delete workflows - `admin:*` - All administrative operations **API Key Management**: - Format: `cfk_live_<32-char-random>` (production), `cfk_test_<32-char-random>` (sandbox) - Hashing: SHA-256 before storage in PostgreSQL - Scoping: API keys can be scoped to specific workflows or operations - Rate limits: Configurable per API key (default: 1000 RPM) ### Secrets Management **HashiCorp Vault Integration**: - Dynamic database credentials: Generated on-demand, 1-hour TTL - Encryption keys: Transit secrets engine for encryption-as-a-service - API keys for external services: Stored with versioning - Rotation policy: Automated rotation every 90 days with notification **Secret Access Pattern**: ``` Service starts \u2192 Vault authentication (Kubernetes service account) \u2502 \u25bc Request secret lease (1-hour TTL)",
              "score": null
            }
          ],
          "latency_ms": 5.425412000477081,
          "query_id": "realistic_003",
          "dimension": "contextual",
          "query": "I need to implement token refresh logic, when do JWT tokens expire?"
        },
        {
          "key_facts": [
            "3600 seconds",
            "max 3600 seconds from iat",
            "All tokens expire after 3600 seconds"
          ],
          "found_facts": [
            "3600 seconds"
          ],
          "missed_facts": [
            "max 3600 seconds from iat",
            "All tokens expire after 3600 seconds"
          ],
          "coverage": 0.3333333333333333,
          "expected_docs": [
            "api_reference"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_12",
              "content": "Waiting {retry_after} seconds...\") time.sleep(retry_after) continue elif response.status_code >= 500: # Server error - retry with exponential backoff wait_time = 2 ** attempt print(f\"Server error. Retrying in {wait_time} seconds...\") time.sleep(wait_time) continue else: # Client error - don't retry error_data = response.json() raise Exception(f\"API Error: {error_data['error']['message']}\") except requests.exceptions.RequestException as e: print(f\"Request failed: {e}\") if attempt < max_retries - 1: time.sleep(2 ** attempt) else: raise raise Exception(\"Max retries exceeded\") ``` ## Webhooks CloudFlow can send webhook notifications when specific events occur in your workflows.",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_2",
              "content": "**Verification:** ```bash # Decode JWT to check expiration echo $CF_ACCESS_TOKEN | cut -d'.' -f2 | base64 -d | jq '.exp' # Compare with current time date +%s ``` **Resolution:** ```bash # Refresh the access token curl -X POST https://api.cloudflow.io/auth/refresh \\ -H \"Content-Type: application/json\" \\ -d '{\"refresh_token\": \"'$CF_REFRESH_TOKEN'\"}' # Update environment variable export CF_ACCESS_TOKEN=\"<new_token>\" ``` **2. Invalid JWT Signature** Error: `JWT signature verification failed` **Causes:** - Token was modified or corrupted - Using wrong signing key - Token generated with different secret **Resolution:** ```bash # Validate token structure cloudflow auth validate --token $CF_ACCESS_TOKEN # Generate new token cloudflow auth login --username <user> --password <pass> # For service accounts cloudflow auth service-account --create --name <sa-name> --scopes \"workflow:read,workflow:write\" ``` **3. Clock Skew Issues** JWT validation fails when system clocks are out of sync (tolerance: \u00b1300 seconds).",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_3",
              "content": "**Diagnosis:** ```bash # Check system time timedatectl status # Compare with NTP server ntpdate -q pool.ntp.org # Check JWT issued time vs current time jwt_iat=$(echo $CF_ACCESS_TOKEN | cut -d'.' -f2 | base64 -d | jq '.iat') current_time=$(date +%s) skew=$((current_time - jwt_iat)) echo \"Clock skew: $skew seconds\" ``` **Resolution:** ```bash # Sync with NTP server sudo ntpdate -s pool.ntp.org # Enable automatic time sync sudo timedatectl set-ntp true # Restart CloudFlow client cloudflow restart ``` **4. Insufficient Permissions** Error: `User does not have required permissions for this operation` **Check user roles:** ```bash cloudflow auth whoami --verbose # Expected output: # User: john.doe@company.com # Roles: developer, workflow-admin # Scopes: workflow:*, data:read, metrics:read ``` **Request permission elevation:** ```bash # Submit access request cloudflow auth request-access \\ --resource \"workflow:production:deploy\" \\ --justification \"Deploy critical hotfix for TICKET-1234\" # Check pending approvals cloudflow auth list-requests --status pending ``` ### 403 Forbidden Errors #### RBAC Policy Violations",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_17",
              "content": "Retry after 45 seconds.\", \"status\": 429, \"headers\": { \"X-RateLimit-Limit\": \"1000\", \"X-RateLimit-Remaining\": \"0\", \"X-RateLimit-Reset\": \"1706112345\", \"Retry-After\": \"45\" } } ``` #### Rate Limit Tiers CloudFlow enforces the following rate limits per workspace: | Tier | Requests/Minute | Requests/Hour | Concurrent Workflows | |------|-----------------|---------------|----------------------| | Free | 60 | 1,000 | 5 | | Standard | 1,000 | 50,000 | 50 | | Premium | 5,000 | 250,000 | 200 | | Enterprise | Custom | Custom | Unlimited | #### Checking Rate Limit Status ```bash # Check current rate limit status curl -I https://api.cloudflow.io/api/v1/workflows \\ -H \"Authorization: Bearer $CF_ACCESS_TOKEN\" # Extract rate limit headers curl -s -I https://api.cloudflow.io/api/v1/workflows \\ -H \"Authorization: Bearer $CF_ACCESS_TOKEN\" | \\ grep -i \"x-ratelimit\" # Output: # X-RateLimit-Limit: 1000 # X-RateLimit-Remaining: 247 # X-RateLimit-Reset: 1706112400 # Monitor rate limit usage cloudflow metrics query \\ --metric rate_limit_remaining \\ --workspace ws_abc123 \\ --last 1h \\ --interval 1m ``` ####",
              "score": null
            },
            {
              "doc_id": "user_guide",
              "chunk_id": "user_guide_fix_17",
              "content": "### Execution Timeout - **Default**: 3600 seconds (60 minutes) - **Behavior**: Workflows exceeding this timeout are automatically terminated - **Custom Timeouts**: Enterprise plans can request custom timeout limits **Setting Step-Level Timeouts:** ```yaml - id: long_running_task action: http_request config: url: \"https://api.example.com/process\" timeout: 300 # 5 minutes for this specific step ``` ### Execution Limits - **Maximum**: 1000 executions per day (per workflow) - **Rate Limiting**: 100 concurrent executions per workflow - **Burst Limit**: 10 executions per second **What happens when limits are reached:** - New executions are queued automatically - Webhook triggers return HTTP 429 (Too Many Requests) - Scheduled executions are skipped (logged in audit trail) - Email notifications sent to workflow owner **Monitoring Usage:** View real-time metrics in your workflow dashboard: - Executions today: 847 / 1000 - Average duration: 12.3s - Success rate: 98.2% - Current queue depth: 3 ### Data Limits - **Maximum request/response size**: 10MB per",
              "score": null
            }
          ],
          "latency_ms": 5.63164600134769,
          "query_id": "realistic_003",
          "dimension": "negation",
          "query": "Why does my token stop working after 3600 seconds?"
        },
        {
          "key_facts": [
            "PostgreSQL 15.4",
            "Redis 7.2",
            "Apache Kafka 3.6"
          ],
          "found_facts": [],
          "missed_facts": [
            "PostgreSQL 15.4",
            "Redis 7.2",
            "Apache Kafka 3.6"
          ],
          "coverage": 0.0,
          "expected_docs": [
            "architecture_overview"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_0",
              "content": "# CloudFlow Platform - System Architecture Overview **Document Version:** 2.3.1 **Last Updated:** January 15, 2026 **Owner:** Platform Architecture Team **Status:** Production ## Executive Summary CloudFlow is a distributed, cloud-native workflow automation platform designed to orchestrate complex business processes at scale. The platform processes over 2.5 million workflow executions daily with an average P99 latency of 180ms for API operations and 4.2 seconds for workflow execution. This document provides a comprehensive overview of the system architecture, including microservices design, data flow patterns, infrastructure decisions, and operational considerations. ## Table of Contents 1. [High-Level Architecture](#high-level-architecture) 2. [Microservices Breakdown](#microservices-breakdown) 3. [Data Flow Architecture](#data-flow-architecture) 4. [Database Architecture](#database-architecture) 5. [Message Queue Patterns](#message-queue-patterns) 6. [Caching Strategy](#caching-strategy) 7. [Security Architecture](#security-architecture) 8. [Performance Characteristics](#performance-characteristics) 9. [Disaster Recovery](#disaster-recovery) --- ## High-Level Architecture CloudFlow follows a microservices architecture pattern deployed across multiple availability zones in AWS.",
              "score": null
            },
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_0",
              "content": "# CloudFlow API Reference Version 2.1.0 | Last Updated: January 2026 ## Overview The CloudFlow API is a RESTful service that enables developers to programmatically manage cloud workflows, data pipelines, and automation tasks. This documentation provides comprehensive details on authentication, endpoints, request/response formats, error handling, and best practices. **Base URL:** `https://api.cloudflow.io/v2` **API Status:** https://status.cloudflow.io ## Authentication CloudFlow supports three authentication methods to suit different use cases and security requirements. ### API Keys API keys provide simple authentication for server-to-server communication. Include your API key in the request header: ```bash curl -H \"X-API-Key: cf_live_a1b2c3d4e5f6g7h8i9j0\" \\ https://api.cloudflow.io/v2/workflows ``` **Security Notes:** - Never expose API keys in client-side code - Rotate keys every 90 days - Use separate keys for development and production environments ### OAuth 2.0 OAuth 2.0 is recommended for applications that access CloudFlow on behalf of users. We support the Authorization Code flow with PKCE.",
              "score": null
            },
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_1",
              "content": "### Architecture Summary CloudFlow consists of the following components: - **API Server**: REST API for workflow management (Node.js/Express) - **Worker Service**: Background job processor (Node.js) - **Scheduler**: Cron-based task scheduler (Node.js) - **PostgreSQL**: Primary data store (version 14) - **Redis**: Cache and message queue (version 7.0) - **PgBouncer**: Database connection pooler ### Deployment Model - **Namespace**: `cloudflow-prod` - **Cluster**: AWS EKS 1.28 - **Region**: us-east-1 (primary), us-west-2 (disaster recovery) - **High Availability**: Multi-AZ deployment across 3 availability zones --- ## Prerequisites Before beginning the deployment process, ensure you have the following: ### Required Tools - `kubectl` (v1.28 or later) - `helm` (v3.12 or later) - `aws-cli` (v2.13 or later) - `eksctl` (v0.165 or later) - `terraform` (v1.6 or later) - for infrastructure provisioning ### Access Requirements - AWS account with appropriate IAM permissions - EKS cluster admin access - Container registry access (ECR) - Domain name and SSL certificates -",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_12",
              "content": "database tier. #### Permanent Solutions **Option 1: Upgrade database tier** ```bash # Check available tiers cloudflow db tiers list # Upgrade to higher tier (supports 200 connections) cloudflow db upgrade --tier standard-plus --confirm # Monitor migration progress cloudflow db migration status ``` **Option 2: Implement aggressive connection reuse** ```bash # Reduce connection lifetime cloudflow config set db.pool.max_lifetime 600 # 10 minutes # Enable prepared statement caching cloudflow config set db.prepared_statements.cache true cloudflow config set db.prepared_statements.max_size 250 # Reduce idle connection timeout cloudflow config set db.pool.idle_timeout 120 # 2 minutes ``` --- ## Workflow Execution Failures ### Timeout Errors (3600 second limit) #### Error Message ``` WorkflowExecutionError: Workflow exceeded maximum execution time of 3600 seconds Status: TIMEOUT Workflow ID: wf_9k2n4m8p1q Execution ID: exec_7h3j6k9m2n ``` #### Analysis ```bash # Get workflow execution details cloudflow workflows executions get exec_7h3j6k9m2n --verbose # Check step-by-step breakdown cloudflow workflows executions steps exec_7h3j6k9m2n # Sample output: #",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_4",
              "content": "CloudFlow uses role-based access control (RBAC) with the following hierarchy: - `viewer` - Read-only access - `developer` - Create and modify workflows (non-production) - `operator` - Execute workflows, view logs - `admin` - Full access to workspace - `platform-admin` - Cross-workspace administration **Verify resource permissions:** ```bash # Check effective permissions cloudflow rbac check \\ --user john.doe@company.com \\ --resource workflow:prod-pipeline \\ --action execute # List all policies affecting user cloudflow rbac policies --user john.doe@company.com --verbose ``` --- ## Performance Problems ### Slow Query Performance #### Symptoms - API response times > 5000ms - Database query latency warnings in logs - CloudFlow UI becomes unresponsive - Timeout errors: `Request exceeded maximum duration of 30000ms` #### Diagnosis Steps **1.",
              "score": null
            }
          ],
          "latency_ms": 5.77351999891107,
          "query_id": "realistic_004",
          "dimension": "original",
          "query": "What database technology does CloudFlow use?"
        },
        {
          "key_facts": [
            "PostgreSQL 15.4",
            "Redis 7.2",
            "Apache Kafka 3.6"
          ],
          "found_facts": [],
          "missed_facts": [
            "PostgreSQL 15.4",
            "Redis 7.2",
            "Apache Kafka 3.6"
          ],
          "coverage": 0.0,
          "expected_docs": [
            "architecture_overview"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_0",
              "content": "# CloudFlow Platform - System Architecture Overview **Document Version:** 2.3.1 **Last Updated:** January 15, 2026 **Owner:** Platform Architecture Team **Status:** Production ## Executive Summary CloudFlow is a distributed, cloud-native workflow automation platform designed to orchestrate complex business processes at scale. The platform processes over 2.5 million workflow executions daily with an average P99 latency of 180ms for API operations and 4.2 seconds for workflow execution. This document provides a comprehensive overview of the system architecture, including microservices design, data flow patterns, infrastructure decisions, and operational considerations. ## Table of Contents 1. [High-Level Architecture](#high-level-architecture) 2. [Microservices Breakdown](#microservices-breakdown) 3. [Data Flow Architecture](#data-flow-architecture) 4. [Database Architecture](#database-architecture) 5. [Message Queue Patterns](#message-queue-patterns) 6. [Caching Strategy](#caching-strategy) 7. [Security Architecture](#security-architecture) 8. [Performance Characteristics](#performance-characteristics) 9. [Disaster Recovery](#disaster-recovery) --- ## High-Level Architecture CloudFlow follows a microservices architecture pattern deployed across multiple availability zones in AWS.",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_4",
              "content": "CloudFlow uses role-based access control (RBAC) with the following hierarchy: - `viewer` - Read-only access - `developer` - Create and modify workflows (non-production) - `operator` - Execute workflows, view logs - `admin` - Full access to workspace - `platform-admin` - Cross-workspace administration **Verify resource permissions:** ```bash # Check effective permissions cloudflow rbac check \\ --user john.doe@company.com \\ --resource workflow:prod-pipeline \\ --action execute # List all policies affecting user cloudflow rbac policies --user john.doe@company.com --verbose ``` --- ## Performance Problems ### Slow Query Performance #### Symptoms - API response times > 5000ms - Database query latency warnings in logs - CloudFlow UI becomes unresponsive - Timeout errors: `Request exceeded maximum duration of 30000ms` #### Diagnosis Steps **1.",
              "score": null
            },
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_0",
              "content": "# CloudFlow API Reference Version 2.1.0 | Last Updated: January 2026 ## Overview The CloudFlow API is a RESTful service that enables developers to programmatically manage cloud workflows, data pipelines, and automation tasks. This documentation provides comprehensive details on authentication, endpoints, request/response formats, error handling, and best practices. **Base URL:** `https://api.cloudflow.io/v2` **API Status:** https://status.cloudflow.io ## Authentication CloudFlow supports three authentication methods to suit different use cases and security requirements. ### API Keys API keys provide simple authentication for server-to-server communication. Include your API key in the request header: ```bash curl -H \"X-API-Key: cf_live_a1b2c3d4e5f6g7h8i9j0\" \\ https://api.cloudflow.io/v2/workflows ``` **Security Notes:** - Never expose API keys in client-side code - Rotate keys every 90 days - Use separate keys for development and production environments ### OAuth 2.0 OAuth 2.0 is recommended for applications that access CloudFlow on behalf of users. We support the Authorization Code flow with PKCE.",
              "score": null
            },
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_1",
              "content": "### Architecture Summary CloudFlow consists of the following components: - **API Server**: REST API for workflow management (Node.js/Express) - **Worker Service**: Background job processor (Node.js) - **Scheduler**: Cron-based task scheduler (Node.js) - **PostgreSQL**: Primary data store (version 14) - **Redis**: Cache and message queue (version 7.0) - **PgBouncer**: Database connection pooler ### Deployment Model - **Namespace**: `cloudflow-prod` - **Cluster**: AWS EKS 1.28 - **Region**: us-east-1 (primary), us-west-2 (disaster recovery) - **High Availability**: Multi-AZ deployment across 3 availability zones --- ## Prerequisites Before beginning the deployment process, ensure you have the following: ### Required Tools - `kubectl` (v1.28 or later) - `helm` (v3.12 or later) - `aws-cli` (v2.13 or later) - `eksctl` (v0.165 or later) - `terraform` (v1.6 or later) - for infrastructure provisioning ### Access Requirements - AWS account with appropriate IAM permissions - EKS cluster admin access - Container registry access (ECR) - Domain name and SSL certificates -",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_12",
              "content": "database tier. #### Permanent Solutions **Option 1: Upgrade database tier** ```bash # Check available tiers cloudflow db tiers list # Upgrade to higher tier (supports 200 connections) cloudflow db upgrade --tier standard-plus --confirm # Monitor migration progress cloudflow db migration status ``` **Option 2: Implement aggressive connection reuse** ```bash # Reduce connection lifetime cloudflow config set db.pool.max_lifetime 600 # 10 minutes # Enable prepared statement caching cloudflow config set db.prepared_statements.cache true cloudflow config set db.prepared_statements.max_size 250 # Reduce idle connection timeout cloudflow config set db.pool.idle_timeout 120 # 2 minutes ``` --- ## Workflow Execution Failures ### Timeout Errors (3600 second limit) #### Error Message ``` WorkflowExecutionError: Workflow exceeded maximum execution time of 3600 seconds Status: TIMEOUT Workflow ID: wf_9k2n4m8p1q Execution ID: exec_7h3j6k9m2n ``` #### Analysis ```bash # Get workflow execution details cloudflow workflows executions get exec_7h3j6k9m2n --verbose # Check step-by-step breakdown cloudflow workflows executions steps exec_7h3j6k9m2n # Sample output: #",
              "score": null
            }
          ],
          "latency_ms": 5.500241000845563,
          "query_id": "realistic_004",
          "dimension": "synonym",
          "query": "Which database system powers CloudFlow?"
        },
        {
          "key_facts": [
            "PostgreSQL 15.4",
            "Redis 7.2",
            "Apache Kafka 3.6"
          ],
          "found_facts": [],
          "missed_facts": [
            "PostgreSQL 15.4",
            "Redis 7.2",
            "Apache Kafka 3.6"
          ],
          "coverage": 0.0,
          "expected_docs": [
            "architecture_overview"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_1",
              "content": "The platform is designed with the following core principles: - **Scalability**: Horizontal scaling for all services with auto-scaling groups - **Resilience**: Circuit breakers, bulkheads, and graceful degradation - **Observability**: Distributed tracing, centralized logging, and comprehensive metrics - **Security**: Zero-trust network architecture with mTLS encryption ### Architecture Diagram (Conceptual) ``` \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 Load Balancer (ALB) \u2502 \u2502 (TLS Termination - 443) \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u25bc \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 API Gateway Layer \u2502 \u2502 (Rate Limiting, Auth, Request Routing) \u2502 \u2514\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u2502 \u2502 \u2502 \u2502 \u25bc \u25bc \u25bc \u25bc \u25bc \u250c\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502Auth\u2502 \u2502Workflow\u2502 \u2502Scheduler\u2502 \u2502Notification\u2502 \u2502User Service\u2502 \u2502Svc \u2502 \u2502 Engine \u2502 \u2502 Service \u2502 \u2502 Service \u2502 \u2502 \u2502 \u2514\u2500\u252c\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u2502 \u2502 \u2502 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 \u2502 \u2502 \u25bc \u25bc \u25bc \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502PostgreSQL\u2502 \u2502 Redis \u2502 \u2502 Kafka \u2502 \u2502 Cluster \u2502 \u2502 Cluster \u2502 \u2502 Cluster \u2502",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_21",
              "content": "for hot data. --- ## Security Architecture ### Network Security **Zero-Trust Network Model**: - All service-to-service communication encrypted with mTLS - Certificate rotation: Every 90 days (automated via cert-manager) - Certificate authority: Internal PKI with HashiCorp Vault - Network segmentation: Private subnets for services, public subnet for ALB only **Service Mesh (Istio)**: ``` Service A \u2192 Envoy Sidecar (mTLS client cert) \u2502 Encrypted channel \u2502 Envoy Sidecar (mTLS server cert) \u2192 Service B ``` **Firewall Rules**: - Security groups: Deny all by default, explicit allow rules - Ingress: Only ALB can reach API Gateway (port 8080) - Egress: Services can only reach specific dependencies - Database access: Limited to application subnets only ### Authentication & Authorization **JWT Token Validation**: - Algorithm: RS256 (asymmetric signing) - Key rotation: Every 30 days with 7-day overlap period - Public key distribution: JWKS endpoint cached in Redis - Validation: Signature, expiry, issuer, audience claims",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_13",
              "content": "Core Tables: - users (5M rows): User accounts and profiles - workflows (2M rows): Workflow definitions - workflow_executions (500M rows, partitioned): Execution history - workflow_steps (2B rows, partitioned): Individual step records - schedules (100K rows): Scheduled workflow triggers - notifications (1B rows, partitioned): Notification delivery log Partitioning Strategy: - workflow_executions: Monthly partitions by created_at - workflow_steps: Monthly partitions by created_at - notifications: Weekly partitions by created_at - Automatic partition creation via cron job - Retention policy: Drop partitions older than 12 months ``` **Indexing Strategy**: - Primary keys: UUIDs with B-tree indexes - Frequently queried columns: Compound indexes (e.g., user_id + status + created_at) - JSONB columns: GIN indexes for workflow definitions - Full-text search: GiST indexes on description fields **Connection Pooling**: - PgBouncer in transaction mode - Pool size: 100 connections per service - Max client connections: 2000 - Connection timeout: 30 seconds **Performance Characteristics**: - Read query P99:",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_0",
              "content": "# CloudFlow Platform - System Architecture Overview **Document Version:** 2.3.1 **Last Updated:** January 15, 2026 **Owner:** Platform Architecture Team **Status:** Production ## Executive Summary CloudFlow is a distributed, cloud-native workflow automation platform designed to orchestrate complex business processes at scale. The platform processes over 2.5 million workflow executions daily with an average P99 latency of 180ms for API operations and 4.2 seconds for workflow execution. This document provides a comprehensive overview of the system architecture, including microservices design, data flow patterns, infrastructure decisions, and operational considerations. ## Table of Contents 1. [High-Level Architecture](#high-level-architecture) 2. [Microservices Breakdown](#microservices-breakdown) 3. [Data Flow Architecture](#data-flow-architecture) 4. [Database Architecture](#database-architecture) 5. [Message Queue Patterns](#message-queue-patterns) 6. [Caching Strategy](#caching-strategy) 7. [Security Architecture](#security-architecture) 8. [Performance Characteristics](#performance-characteristics) 9. [Disaster Recovery](#disaster-recovery) --- ## High-Level Architecture CloudFlow follows a microservices architecture pattern deployed across multiple availability zones in AWS.",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_31",
              "content": "DNS propagation slower than expected (resolved) - Success criteria: Met all recovery objectives ### Business Continuity **Communication Plan**: - Status page: status.cloudflow.com (updated every 15 minutes during incident) - Customer notifications: Email + SMS for all P1 incidents - Internal escalation: PagerDuty \u2192 Incident Commander \u2192 Engineering Manager \u2192 CTO **Data Retention Policy**: - Active data: 12 months in hot storage (PostgreSQL) - Archived data: 7 years in cold storage (S3 Glacier) - Audit logs: 7 years (compliance requirement) - Backup retention: 30 days standard, 90 days monthly snapshots --- ## Appendix ### Service Dependency Matrix | Service | Depends On | Critical Path | |---------|-----------|---------------| | API Gateway | Auth Service, Redis | Yes | | Auth Service | PostgreSQL, Redis, Vault | Yes | | Workflow Engine | PostgreSQL, Kafka, Redis | Yes | | Scheduler | PostgreSQL, Redis, Kafka | No | | Notification Service | Kafka,",
              "score": null
            }
          ],
          "latency_ms": 5.5328319995169295,
          "query_id": "realistic_004",
          "dimension": "problem",
          "query": "I need to understand the data storage architecture"
        },
        {
          "key_facts": [
            "PostgreSQL 15.4",
            "Redis 7.2",
            "Apache Kafka 3.6"
          ],
          "found_facts": [],
          "missed_facts": [
            "PostgreSQL 15.4",
            "Redis 7.2",
            "Apache Kafka 3.6"
          ],
          "coverage": 0.0,
          "expected_docs": [
            "architecture_overview"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_7",
              "content": "\"retry\": {\"max_attempts\": 3, \"backoff\": \"exponential\"} } }, { \"id\": \"create-account\", \"type\": \"database_operation\", \"depends_on\": [\"validate-customer\"] } ] } ``` **State Management**: - Workflow states: PENDING, RUNNING, COMPLETED, FAILED, CANCELLED - Step states: QUEUED, EXECUTING, SUCCESS, FAILED, SKIPPED - State transitions stored in PostgreSQL with event sourcing pattern - Checkpointing every 10 steps for long-running workflows **Performance Targets**: - Simple workflow (3-5 steps): < 2 seconds (P99) - Complex workflow (20+ steps): < 5 seconds (P99) - Throughput: 500 concurrent workflow executions per pod --- ### Scheduler Service **Purpose**: Time-based workflow triggering system supporting cron-like schedules and one-time delayed executions.",
              "score": null
            },
            {
              "doc_id": "user_guide",
              "chunk_id": "user_guide_fix_21",
              "content": "Keep Workflows Modular Break complex workflows into smaller, reusable components: - Use sub-workflows for repeated logic - Trigger child workflows via webhooks - Share common configurations via templates ### 8. Test Thoroughly Before activating a workflow: 1. Use test mode with sample data 2. Verify all actions execute correctly 3. Test error handling paths 4. Review execution logs 5. Start with a limited scope (e.g., test channel, small dataset) ### 9. Document Your Workflows Add descriptions to workflows and steps: ```yaml workflow: name: \"Daily Sales Report\" description: | Generates a daily sales report and distributes it to the sales team. Runs at 8:00 AM EST Monday-Friday. Data source: PostgreSQL sales database Recipients: sales-team@company.com steps: - id: fetch_sales name: \"Fetch Yesterday's Sales\" description: \"Query database for all completed orders from previous day\" ``` ### 10.",
              "score": null
            },
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_18",
              "content": "#### Database Connection Pool Exhausted **Symptoms**: \"Too many connections\" errors **Diagnosis**: ```bash # Check PgBouncer stats kubectl exec -it pgbouncer-xxx -n cloudflow-prod -- \\ psql -p 5432 -U pgbouncer pgbouncer -c \"SHOW POOLS;\" ``` **Resolution**: - Increase `max_db_connections` in PgBouncer - Optimize application connection usage - Add more PostgreSQL replicas ### Support Contacts - **On-call Engineer**: pagerduty.com/cloudflow-oncall - **Slack Channel**: #cloudflow-operations - **Documentation**: https://docs.cloudflow.io - **Status Page**: https://status.cloudflow.io --- ## Appendix ### Useful Commands ```bash # View all resources in namespace kubectl get all -n cloudflow-prod # Check resource usage kubectl top pods -n cloudflow-prod kubectl top nodes # View logs kubectl logs -f deployment/cloudflow-api -n cloudflow-prod # Execute command in pod kubectl exec -it <pod-name> -n cloudflow-prod -- /bin/sh # Port forward for local testing kubectl port-forward svc/cloudflow-api 8080:80 -n cloudflow-prod # Rollback deployment helm rollback cloudflow -n cloudflow-prod # Update deployment helm upgrade cloudflow cloudflow/cloudflow \\ --namespace cloudflow-prod",
              "score": null
            },
            {
              "doc_id": "user_guide",
              "chunk_id": "user_guide_fix_6",
              "content": "| SELECT * FROM orders WHERE customer_id = $1 AND status = $2 ORDER BY created_at DESC LIMIT 10 parameters: - \"{{trigger.customer_id}}\" - \"pending\" ``` **MongoDB:** ```yaml - id: find_documents action: mongodb_query config: connection: \"{{secrets.MONGO_URI}}\" database: \"production\" collection: \"users\" operation: find filter: email: \"{{trigger.email}}\" options: limit: 1 ``` **Important Notes:** - Always use parameterized queries to prevent SQL injection - Connection strings should be stored in secrets - Query timeout is 30 seconds by default - Maximum result set size is 10MB ### Email Notifications Send emails via SMTP or integrated email providers: **Configuration:** ```yaml - id: send_notification action: email config: provider: \"smtp\" # or \"sendgrid\", \"mailgun\", \"ses\" from: \"notifications@cloudflow.io\" to: \"{{trigger.recipient}}\" cc: \"manager@company.com\" subject: \"Alert: {{alert_type}}\" body: | Hello {{user.name}}, This is an automated notification about {{event_description}}.",
              "score": null
            },
            {
              "doc_id": "user_guide",
              "chunk_id": "user_guide_fix_26",
              "content": "Aggregation and Alerting Aggregate errors and send smart alerts: ```yaml name: \"Application Error Monitor\" schedule: cron: \"*/5 * * * *\" timezone: \"UTC\" steps: - id: fetch_recent_errors action: database_query config: query: | SELECT error_type, COUNT(*) as count, MAX(created_at) as last_seen FROM error_logs WHERE created_at > NOW() - INTERVAL '5 minutes' AND alerted = false GROUP BY error_type HAVING COUNT(*) > 5 - id: format_alert action: javascript condition: \"{{length(steps.fetch_recent_errors.rows) > 0}}\" code: | const errors = input.rows; let message = \"\u26a0\ufe0f Error Alert\\n\\n\"; for (const error of errors) { message += `\u2022 ${error.error_type}: ${error.count} occurrences (last: ${error.last_seen})\\n`; } return { message, total_errors: errors.reduce((sum, e) => sum + e.count, 0) }; - id: send_alert action: slack_message condition: \"{{steps.format_alert.output.total_errors > 10}}\" config: channel: \"#incidents\" text: \"{{steps.format_alert.output.message}}\" priority: \"high\" - id: mark_alerted action: database_query config: query: | UPDATE error_logs SET alerted = true WHERE created_at > NOW() - INTERVAL '5 minutes' ``` --- ##",
              "score": null
            }
          ],
          "latency_ms": 5.428096001196536,
          "query_id": "realistic_004",
          "dimension": "casual",
          "query": "database stack"
        },
        {
          "key_facts": [
            "PostgreSQL 15.4",
            "Redis 7.2",
            "Apache Kafka 3.6"
          ],
          "found_facts": [],
          "missed_facts": [
            "PostgreSQL 15.4",
            "Redis 7.2",
            "Apache Kafka 3.6"
          ],
          "coverage": 0.0,
          "expected_docs": [
            "architecture_overview"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_32",
              "content": "PostgreSQL, SendGrid, Twilio | No | ### Capacity Planning **Current Capacity** (Jan 2026): - Daily workflow executions: 2.5M - Active users: 150,000 - API requests per day: 50M - Database size: 1.2TB - Kafka throughput: 100K msg/sec peak **6-Month Projection** (Jul 2026): - Daily workflow executions: 4M (+60%) - Active users: 225,000 (+50%) - API requests per day: 80M (+60%) - Database size: 2TB (+67%) - Required scaling: +30% compute, +50% storage ### Contact Information - **Architecture Team**: architecture@cloudflow.internal - **On-Call Engineer**: PagerDuty escalation - **Security Team**: security@cloudflow.internal - **Documentation**: https://wiki.cloudflow.internal/architecture --- **Document Revision History**: - v2.3.1 (Jan 15, 2026): Updated performance metrics, added DR test results - v2.3.0 (Dec 1, 2025): Added multi-AZ deployment details - v2.2.0 (Oct 15, 2025): Security architecture overhaul - v2.1.0 (Sep 1, 2025): Initial Kafka migration documentation - v2.0.0 (Jul 1, 2025): Complete microservices rewrite",
              "score": null
            },
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_1",
              "content": "### Architecture Summary CloudFlow consists of the following components: - **API Server**: REST API for workflow management (Node.js/Express) - **Worker Service**: Background job processor (Node.js) - **Scheduler**: Cron-based task scheduler (Node.js) - **PostgreSQL**: Primary data store (version 14) - **Redis**: Cache and message queue (version 7.0) - **PgBouncer**: Database connection pooler ### Deployment Model - **Namespace**: `cloudflow-prod` - **Cluster**: AWS EKS 1.28 - **Region**: us-east-1 (primary), us-west-2 (disaster recovery) - **High Availability**: Multi-AZ deployment across 3 availability zones --- ## Prerequisites Before beginning the deployment process, ensure you have the following: ### Required Tools - `kubectl` (v1.28 or later) - `helm` (v3.12 or later) - `aws-cli` (v2.13 or later) - `eksctl` (v0.165 or later) - `terraform` (v1.6 or later) - for infrastructure provisioning ### Access Requirements - AWS account with appropriate IAM permissions - EKS cluster admin access - Container registry access (ECR) - Domain name and SSL certificates -",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_0",
              "content": "# CloudFlow Platform - System Architecture Overview **Document Version:** 2.3.1 **Last Updated:** January 15, 2026 **Owner:** Platform Architecture Team **Status:** Production ## Executive Summary CloudFlow is a distributed, cloud-native workflow automation platform designed to orchestrate complex business processes at scale. The platform processes over 2.5 million workflow executions daily with an average P99 latency of 180ms for API operations and 4.2 seconds for workflow execution. This document provides a comprehensive overview of the system architecture, including microservices design, data flow patterns, infrastructure decisions, and operational considerations. ## Table of Contents 1. [High-Level Architecture](#high-level-architecture) 2. [Microservices Breakdown](#microservices-breakdown) 3. [Data Flow Architecture](#data-flow-architecture) 4. [Database Architecture](#database-architecture) 5. [Message Queue Patterns](#message-queue-patterns) 6. [Caching Strategy](#caching-strategy) 7. [Security Architecture](#security-architecture) 8. [Performance Characteristics](#performance-characteristics) 9. [Disaster Recovery](#disaster-recovery) --- ## High-Level Architecture CloudFlow follows a microservices architecture pattern deployed across multiple availability zones in AWS.",
              "score": null
            },
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_0",
              "content": "# CloudFlow Platform - Deployment and Operations Guide **Version:** 2.4.0 **Last Updated:** January 2026 **Target Environment:** Production (AWS EKS) ## Table of Contents 1. [Overview](#overview) 2. [Prerequisites](#prerequisites) 3. [Infrastructure Setup](#infrastructure-setup) 4. [Kubernetes Deployment](#kubernetes-deployment) 5. [Database Configuration](#database-configuration) 6. [Monitoring and Observability](#monitoring-and-observability) 7. [Backup and Disaster Recovery](#backup-and-disaster-recovery) 8. [Scaling and Performance](#scaling-and-performance) 9. [Security and Compliance](#security-and-compliance) 10. [Troubleshooting](#troubleshooting) --- ## Overview CloudFlow is a cloud-native workflow orchestration platform designed for high-availability production environments. This guide provides comprehensive instructions for deploying and operating CloudFlow on Amazon EKS (Elastic Kubernetes Service).",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_20",
              "content": "Leverage caching:** ```bash # Enable client-side caching export CLOUDFLOW_CACHE_ENABLED=true export CLOUDFLOW_CACHE_TTL=300 # Cache workflow metadata cloudflow workflows list --use-cache --cache-ttl 600 ``` --- ## Log Analysis & Debugging ### Accessing CloudFlow Logs #### Kubernetes Deployments ```bash # List all CloudFlow pods kubectl get pods -n cloudflow # Tail logs from API server kubectl logs -f -n cloudflow deployment/cloudflow-api --tail=100 # Get logs from specific pod kubectl logs -n cloudflow cloudflow-api-7d4f6b8c9d-x7k2m # Get logs from previous crashed pod kubectl logs -n cloudflow cloudflow-api-7d4f6b8c9d-x7k2m --previous # Get logs from all pods in deployment kubectl logs -n cloudflow deployment/cloudflow-api --all-containers=true # Stream logs from multiple pods kubectl logs -n cloudflow -l app=cloudflow-api -f --max-log-requests=10 ``` #### Log Levels CloudFlow supports the following log levels: - `TRACE` - Very detailed debugging information - `DEBUG` - Detailed debugging information - `INFO` - Informational messages (default) - `WARN` - Warning messages - `ERROR` - Error messages",
              "score": null
            }
          ],
          "latency_ms": 5.434498001704924,
          "query_id": "realistic_004",
          "dimension": "contextual",
          "query": "For capacity planning, what databases does CloudFlow rely on?"
        },
        {
          "key_facts": [
            "PostgreSQL 15.4",
            "Redis 7.2",
            "Apache Kafka 3.6"
          ],
          "found_facts": [],
          "missed_facts": [
            "PostgreSQL 15.4",
            "Redis 7.2",
            "Apache Kafka 3.6"
          ],
          "coverage": 0.0,
          "expected_docs": [
            "architecture_overview"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_0",
              "content": "# CloudFlow API Reference Version 2.1.0 | Last Updated: January 2026 ## Overview The CloudFlow API is a RESTful service that enables developers to programmatically manage cloud workflows, data pipelines, and automation tasks. This documentation provides comprehensive details on authentication, endpoints, request/response formats, error handling, and best practices. **Base URL:** `https://api.cloudflow.io/v2` **API Status:** https://status.cloudflow.io ## Authentication CloudFlow supports three authentication methods to suit different use cases and security requirements. ### API Keys API keys provide simple authentication for server-to-server communication. Include your API key in the request header: ```bash curl -H \"X-API-Key: cf_live_a1b2c3d4e5f6g7h8i9j0\" \\ https://api.cloudflow.io/v2/workflows ``` **Security Notes:** - Never expose API keys in client-side code - Rotate keys every 90 days - Use separate keys for development and production environments ### OAuth 2.0 OAuth 2.0 is recommended for applications that access CloudFlow on behalf of users. We support the Authorization Code flow with PKCE.",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_22",
              "content": "\"connection.*pool|could not connect|database.*timeout\" # Find slow queries kubectl logs -n cloudflow deployment/cloudflow-api | \\ grep \"slow query\" | \\ awk '{print $NF}' | \\ sort -n | \\ tail -n 20 # Find deadlock errors kubectl logs -n cloudflow deployment/cloudflow-api | \\ grep -i \"deadlock detected\" ``` #### Workflow Execution Errors ```bash # Find workflow timeout errors kubectl logs -n cloudflow deployment/cloudflow-workflow-engine | \\ grep -E \"timeout|exceeded.*3600\" # Find workflow retry attempts kubectl logs -n cloudflow deployment/cloudflow-workflow-engine | \\ grep -E \"retry attempt [0-9]|retrying in\" # Find workflow failures by ID kubectl logs -n cloudflow deployment/cloudflow-workflow-engine | \\ grep \"exec_7h3j6k9m2n\" ``` #### Rate Limiting ```bash # Find rate limit events kubectl logs -n cloudflow deployment/cloudflow-api | \\ grep -E \"429|rate.*limit|throttle\" # Count rate limit errors by hour kubectl logs -n cloudflow deployment/cloudflow-api --since=24h | \\ grep \"rate_limit_exceeded\" | \\ awk '{print $1}' | \\ cut -d'T' -f1-2 | \\ sort",
              "score": null
            },
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_4",
              "content": "verbosity | `info`, `debug`, `warn`, `error` | No (default: `info`) | | `API_PORT` | API server port | `3000` | No (default: `3000`) | | `WORKER_CONCURRENCY` | Number of concurrent workers | `10` | No (default: `5`) | | `SESSION_SECRET` | Session encryption key | `<generated-secret-256-bit>` | Yes | | `AWS_REGION` | AWS region for services | `us-east-1` | Yes | | `S3_BUCKET` | S3 bucket for file storage | `cloudflow-prod-storage` | Yes | | `SMTP_HOST` | Email server hostname | `smtp.sendgrid.net` | No | | `SMTP_PORT` | Email server port | `587` | No | | `METRICS_ENABLED` | Enable Prometheus metrics | `true` | No (default: `false`) | Create a Kubernetes secret for sensitive variables: ```bash kubectl create secret generic cloudflow-secrets \\ --namespace cloudflow-prod \\ --from-literal=DATABASE_URL=\"postgresql://cloudflow:$(cat db-password.txt)@pgbouncer.cloudflow-prod.svc.cluster.local:5432/cloudflow\" \\ --from-literal=REDIS_URL=\"redis://redis-master.cloudflow-prod.svc.cluster.local:6379\" \\ --from-literal=JWT_SECRET=\"$(openssl rand -base64 32)\" \\ --from-literal=SESSION_SECRET=\"$(openssl rand -base64 32)\" ``` ### Helm Chart Deployment Add the CloudFlow Helm repository:",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_12",
              "content": "database tier. #### Permanent Solutions **Option 1: Upgrade database tier** ```bash # Check available tiers cloudflow db tiers list # Upgrade to higher tier (supports 200 connections) cloudflow db upgrade --tier standard-plus --confirm # Monitor migration progress cloudflow db migration status ``` **Option 2: Implement aggressive connection reuse** ```bash # Reduce connection lifetime cloudflow config set db.pool.max_lifetime 600 # 10 minutes # Enable prepared statement caching cloudflow config set db.prepared_statements.cache true cloudflow config set db.prepared_statements.max_size 250 # Reduce idle connection timeout cloudflow config set db.pool.idle_timeout 120 # 2 minutes ``` --- ## Workflow Execution Failures ### Timeout Errors (3600 second limit) #### Error Message ``` WorkflowExecutionError: Workflow exceeded maximum execution time of 3600 seconds Status: TIMEOUT Workflow ID: wf_9k2n4m8p1q Execution ID: exec_7h3j6k9m2n ``` #### Analysis ```bash # Get workflow execution details cloudflow workflows executions get exec_7h3j6k9m2n --verbose # Check step-by-step breakdown cloudflow workflows executions steps exec_7h3j6k9m2n # Sample output: #",
              "score": null
            },
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_1",
              "content": "### Architecture Summary CloudFlow consists of the following components: - **API Server**: REST API for workflow management (Node.js/Express) - **Worker Service**: Background job processor (Node.js) - **Scheduler**: Cron-based task scheduler (Node.js) - **PostgreSQL**: Primary data store (version 14) - **Redis**: Cache and message queue (version 7.0) - **PgBouncer**: Database connection pooler ### Deployment Model - **Namespace**: `cloudflow-prod` - **Cluster**: AWS EKS 1.28 - **Region**: us-east-1 (primary), us-west-2 (disaster recovery) - **High Availability**: Multi-AZ deployment across 3 availability zones --- ## Prerequisites Before beginning the deployment process, ensure you have the following: ### Required Tools - `kubectl` (v1.28 or later) - `helm` (v3.12 or later) - `aws-cli` (v2.13 or later) - `eksctl` (v0.165 or later) - `terraform` (v1.6 or later) - for infrastructure provisioning ### Access Requirements - AWS account with appropriate IAM permissions - EKS cluster admin access - Container registry access (ECR) - Domain name and SSL certificates -",
              "score": null
            }
          ],
          "latency_ms": 5.61307099997066,
          "query_id": "realistic_004",
          "dimension": "negation",
          "query": "Is CloudFlow using MySQL or something else?"
        },
        {
          "key_facts": [
            "cloudflow-prod"
          ],
          "found_facts": [
            "cloudflow-prod"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "deployment_guide"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_17",
              "content": "namespace level: ```bash kubectl label namespace cloudflow-prod \\ pod-security.kubernetes.io/enforce=restricted \\ pod-security.kubernetes.io/audit=restricted \\ pod-security.kubernetes.io/warn=restricted ``` ### Secrets Management Use AWS Secrets Manager for sensitive credentials: ```bash # Install External Secrets Operator helm install external-secrets external-secrets/external-secrets \\ --namespace external-secrets-system \\ --create-namespace # Create SecretStore kubectl apply -f - <<EOF apiVersion: external-secrets.io/v1beta1 kind: SecretStore metadata: name: aws-secrets-manager namespace: cloudflow-prod spec: provider: aws: service: SecretsManager region: us-east-1 auth: jwt: serviceAccountRef: name: external-secrets-sa EOF ``` --- ## Troubleshooting ### Common Issues #### Pods in CrashLoopBackOff **Symptoms**: Pods continuously restart **Diagnosis**: ```bash kubectl logs -n cloudflow-prod <pod-name> --previous kubectl describe pod -n cloudflow-prod <pod-name> ``` **Common Causes**: - Database connection failure - Invalid environment variables - Insufficient resources #### High Memory Usage **Symptoms**: Pods being OOMKilled **Diagnosis**: ```bash kubectl top pods -n cloudflow-prod ``` **Resolution**: - Increase memory limits in deployment - Check for memory leaks in application logs - Review heap dump for Node.js processes",
              "score": null
            },
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_0",
              "content": "# CloudFlow Platform - Deployment and Operations Guide **Version:** 2.4.0 **Last Updated:** January 2026 **Target Environment:** Production (AWS EKS) ## Table of Contents 1. [Overview](#overview) 2. [Prerequisites](#prerequisites) 3. [Infrastructure Setup](#infrastructure-setup) 4. [Kubernetes Deployment](#kubernetes-deployment) 5. [Database Configuration](#database-configuration) 6. [Monitoring and Observability](#monitoring-and-observability) 7. [Backup and Disaster Recovery](#backup-and-disaster-recovery) 8. [Scaling and Performance](#scaling-and-performance) 9. [Security and Compliance](#security-and-compliance) 10. [Troubleshooting](#troubleshooting) --- ## Overview CloudFlow is a cloud-native workflow orchestration platform designed for high-availability production environments. This guide provides comprehensive instructions for deploying and operating CloudFlow on Amazon EKS (Elastic Kubernetes Service).",
              "score": null
            },
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_3",
              "content": "-f cluster-config.yaml ``` ### Storage Configuration Install the EBS CSI driver for persistent volumes: ```bash eksctl create addon --name aws-ebs-csi-driver \\ --cluster cloudflow-production \\ --service-account-role-arn arn:aws:iam::ACCOUNT_ID:role/AmazonEKS_EBS_CSI_DriverRole \\ --force ``` --- ## Kubernetes Deployment ### Namespace Setup Create the CloudFlow namespace and configure resource quotas: ```yaml # namespace.yaml apiVersion: v1 kind: Namespace metadata: name: cloudflow-prod labels: name: cloudflow-prod environment: production --- apiVersion: v1 kind: ResourceQuota metadata: name: cloudflow-quota namespace: cloudflow-prod spec: hard: requests.cpu: \"50\" requests.memory: 100Gi persistentvolumeclaims: \"10\" services.loadbalancers: \"2\" ``` Apply the namespace configuration: ```bash kubectl apply -f namespace.yaml ``` ### Environment Configuration CloudFlow requires the following environment variables: | Variable | Description | Example | Required | |----------|-------------|---------|----------| | `DATABASE_URL` | PostgreSQL connection string | `postgresql://user:pass@host:5432/cloudflow` | Yes | | `REDIS_URL` | Redis connection string | `redis://redis-master.cloudflow-prod.svc.cluster.local:6379` | Yes | | `JWT_SECRET` | Secret key for JWT token signing | `<generated-secret-256-bit>` | Yes | | `LOG_LEVEL` | Application log",
              "score": null
            },
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_5",
              "content": "```bash helm repo add cloudflow https://charts.cloudflow.io helm repo update ``` Create a values file for production configuration: ```yaml # values-production.yaml replicaCount: 3 image: repository: 123456789012.dkr.ecr.us-east-1.amazonaws.com/cloudflow tag: \"2.4.0\" pullPolicy: IfNotPresent resources: limits: cpu: 2000m memory: 4Gi requests: cpu: 2000m memory: 4Gi autoscaling: enabled: true minReplicas: 3 maxReplicas: 10 targetCPUUtilizationPercentage: 70 targetMemoryUtilizationPercentage: 80 service: type: ClusterIP port: 80 targetPort: 3000 annotations: service.beta.kubernetes.io/aws-load-balancer-type: \"nlb\" ingress: enabled: true className: nginx annotations: cert-manager.io/cluster-issuer: \"letsencrypt-prod\" nginx.ingress.kubernetes.io/ssl-redirect: \"true\" hosts: - host: api.cloudflow.io paths: - path: / pathType: Prefix tls: - secretName: cloudflow-tls hosts: - api.cloudflow.io healthCheck: enabled: true livenessProbe: path: /health initialDelaySeconds: 30 periodSeconds: 10 timeoutSeconds: 5 failureThreshold: 3 readinessProbe: path: /ready initialDelaySeconds: 10 periodSeconds: 5 timeoutSeconds: 3 failureThreshold: 3 env: - name: NODE_ENV value: \"production\" - name: LOG_LEVEL value: \"info\" - name: API_PORT value: \"3000\" - name: WORKER_CONCURRENCY value: \"10\" - name: AWS_REGION value: \"us-east-1\" - name: METRICS_ENABLED value: \"true\" envFrom: - secretRef: name: cloudflow-secrets podAnnotations:",
              "score": null
            },
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_4",
              "content": "verbosity | `info`, `debug`, `warn`, `error` | No (default: `info`) | | `API_PORT` | API server port | `3000` | No (default: `3000`) | | `WORKER_CONCURRENCY` | Number of concurrent workers | `10` | No (default: `5`) | | `SESSION_SECRET` | Session encryption key | `<generated-secret-256-bit>` | Yes | | `AWS_REGION` | AWS region for services | `us-east-1` | Yes | | `S3_BUCKET` | S3 bucket for file storage | `cloudflow-prod-storage` | Yes | | `SMTP_HOST` | Email server hostname | `smtp.sendgrid.net` | No | | `SMTP_PORT` | Email server port | `587` | No | | `METRICS_ENABLED` | Enable Prometheus metrics | `true` | No (default: `false`) | Create a Kubernetes secret for sensitive variables: ```bash kubectl create secret generic cloudflow-secrets \\ --namespace cloudflow-prod \\ --from-literal=DATABASE_URL=\"postgresql://cloudflow:$(cat db-password.txt)@pgbouncer.cloudflow-prod.svc.cluster.local:5432/cloudflow\" \\ --from-literal=REDIS_URL=\"redis://redis-master.cloudflow-prod.svc.cluster.local:6379\" \\ --from-literal=JWT_SECRET=\"$(openssl rand -base64 32)\" \\ --from-literal=SESSION_SECRET=\"$(openssl rand -base64 32)\" ``` ### Helm Chart Deployment Add the CloudFlow Helm repository:",
              "score": null
            }
          ],
          "latency_ms": 5.531689999770606,
          "query_id": "realistic_005",
          "dimension": "original",
          "query": "What is the Kubernetes namespace for production deployment?"
        },
        {
          "key_facts": [
            "cloudflow-prod"
          ],
          "found_facts": [
            "cloudflow-prod"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "deployment_guide"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_17",
              "content": "namespace level: ```bash kubectl label namespace cloudflow-prod \\ pod-security.kubernetes.io/enforce=restricted \\ pod-security.kubernetes.io/audit=restricted \\ pod-security.kubernetes.io/warn=restricted ``` ### Secrets Management Use AWS Secrets Manager for sensitive credentials: ```bash # Install External Secrets Operator helm install external-secrets external-secrets/external-secrets \\ --namespace external-secrets-system \\ --create-namespace # Create SecretStore kubectl apply -f - <<EOF apiVersion: external-secrets.io/v1beta1 kind: SecretStore metadata: name: aws-secrets-manager namespace: cloudflow-prod spec: provider: aws: service: SecretsManager region: us-east-1 auth: jwt: serviceAccountRef: name: external-secrets-sa EOF ``` --- ## Troubleshooting ### Common Issues #### Pods in CrashLoopBackOff **Symptoms**: Pods continuously restart **Diagnosis**: ```bash kubectl logs -n cloudflow-prod <pod-name> --previous kubectl describe pod -n cloudflow-prod <pod-name> ``` **Common Causes**: - Database connection failure - Invalid environment variables - Insufficient resources #### High Memory Usage **Symptoms**: Pods being OOMKilled **Diagnosis**: ```bash kubectl top pods -n cloudflow-prod ``` **Resolution**: - Increase memory limits in deployment - Check for memory leaks in application logs - Review heap dump for Node.js processes",
              "score": null
            },
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_3",
              "content": "-f cluster-config.yaml ``` ### Storage Configuration Install the EBS CSI driver for persistent volumes: ```bash eksctl create addon --name aws-ebs-csi-driver \\ --cluster cloudflow-production \\ --service-account-role-arn arn:aws:iam::ACCOUNT_ID:role/AmazonEKS_EBS_CSI_DriverRole \\ --force ``` --- ## Kubernetes Deployment ### Namespace Setup Create the CloudFlow namespace and configure resource quotas: ```yaml # namespace.yaml apiVersion: v1 kind: Namespace metadata: name: cloudflow-prod labels: name: cloudflow-prod environment: production --- apiVersion: v1 kind: ResourceQuota metadata: name: cloudflow-quota namespace: cloudflow-prod spec: hard: requests.cpu: \"50\" requests.memory: 100Gi persistentvolumeclaims: \"10\" services.loadbalancers: \"2\" ``` Apply the namespace configuration: ```bash kubectl apply -f namespace.yaml ``` ### Environment Configuration CloudFlow requires the following environment variables: | Variable | Description | Example | Required | |----------|-------------|---------|----------| | `DATABASE_URL` | PostgreSQL connection string | `postgresql://user:pass@host:5432/cloudflow` | Yes | | `REDIS_URL` | Redis connection string | `redis://redis-master.cloudflow-prod.svc.cluster.local:6379` | Yes | | `JWT_SECRET` | Secret key for JWT token signing | `<generated-secret-256-bit>` | Yes | | `LOG_LEVEL` | Application log",
              "score": null
            },
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_0",
              "content": "# CloudFlow API Reference Version 2.1.0 | Last Updated: January 2026 ## Overview The CloudFlow API is a RESTful service that enables developers to programmatically manage cloud workflows, data pipelines, and automation tasks. This documentation provides comprehensive details on authentication, endpoints, request/response formats, error handling, and best practices. **Base URL:** `https://api.cloudflow.io/v2` **API Status:** https://status.cloudflow.io ## Authentication CloudFlow supports three authentication methods to suit different use cases and security requirements. ### API Keys API keys provide simple authentication for server-to-server communication. Include your API key in the request header: ```bash curl -H \"X-API-Key: cf_live_a1b2c3d4e5f6g7h8i9j0\" \\ https://api.cloudflow.io/v2/workflows ``` **Security Notes:** - Never expose API keys in client-side code - Rotate keys every 90 days - Use separate keys for development and production environments ### OAuth 2.0 OAuth 2.0 is recommended for applications that access CloudFlow on behalf of users. We support the Authorization Code flow with PKCE.",
              "score": null
            },
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_5",
              "content": "```bash helm repo add cloudflow https://charts.cloudflow.io helm repo update ``` Create a values file for production configuration: ```yaml # values-production.yaml replicaCount: 3 image: repository: 123456789012.dkr.ecr.us-east-1.amazonaws.com/cloudflow tag: \"2.4.0\" pullPolicy: IfNotPresent resources: limits: cpu: 2000m memory: 4Gi requests: cpu: 2000m memory: 4Gi autoscaling: enabled: true minReplicas: 3 maxReplicas: 10 targetCPUUtilizationPercentage: 70 targetMemoryUtilizationPercentage: 80 service: type: ClusterIP port: 80 targetPort: 3000 annotations: service.beta.kubernetes.io/aws-load-balancer-type: \"nlb\" ingress: enabled: true className: nginx annotations: cert-manager.io/cluster-issuer: \"letsencrypt-prod\" nginx.ingress.kubernetes.io/ssl-redirect: \"true\" hosts: - host: api.cloudflow.io paths: - path: / pathType: Prefix tls: - secretName: cloudflow-tls hosts: - api.cloudflow.io healthCheck: enabled: true livenessProbe: path: /health initialDelaySeconds: 30 periodSeconds: 10 timeoutSeconds: 5 failureThreshold: 3 readinessProbe: path: /ready initialDelaySeconds: 10 periodSeconds: 5 timeoutSeconds: 3 failureThreshold: 3 env: - name: NODE_ENV value: \"production\" - name: LOG_LEVEL value: \"info\" - name: API_PORT value: \"3000\" - name: WORKER_CONCURRENCY value: \"10\" - name: AWS_REGION value: \"us-east-1\" - name: METRICS_ENABLED value: \"true\" envFrom: - secretRef: name: cloudflow-secrets podAnnotations:",
              "score": null
            },
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_4",
              "content": "verbosity | `info`, `debug`, `warn`, `error` | No (default: `info`) | | `API_PORT` | API server port | `3000` | No (default: `3000`) | | `WORKER_CONCURRENCY` | Number of concurrent workers | `10` | No (default: `5`) | | `SESSION_SECRET` | Session encryption key | `<generated-secret-256-bit>` | Yes | | `AWS_REGION` | AWS region for services | `us-east-1` | Yes | | `S3_BUCKET` | S3 bucket for file storage | `cloudflow-prod-storage` | Yes | | `SMTP_HOST` | Email server hostname | `smtp.sendgrid.net` | No | | `SMTP_PORT` | Email server port | `587` | No | | `METRICS_ENABLED` | Enable Prometheus metrics | `true` | No (default: `false`) | Create a Kubernetes secret for sensitive variables: ```bash kubectl create secret generic cloudflow-secrets \\ --namespace cloudflow-prod \\ --from-literal=DATABASE_URL=\"postgresql://cloudflow:$(cat db-password.txt)@pgbouncer.cloudflow-prod.svc.cluster.local:5432/cloudflow\" \\ --from-literal=REDIS_URL=\"redis://redis-master.cloudflow-prod.svc.cluster.local:6379\" \\ --from-literal=JWT_SECRET=\"$(openssl rand -base64 32)\" \\ --from-literal=SESSION_SECRET=\"$(openssl rand -base64 32)\" ``` ### Helm Chart Deployment Add the CloudFlow Helm repository:",
              "score": null
            }
          ],
          "latency_ms": 5.861053001353866,
          "query_id": "realistic_005",
          "dimension": "synonym",
          "query": "Which namespace is used for CloudFlow production?"
        },
        {
          "key_facts": [
            "cloudflow-prod"
          ],
          "found_facts": [
            "cloudflow-prod"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "deployment_guide"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_17",
              "content": "namespace level: ```bash kubectl label namespace cloudflow-prod \\ pod-security.kubernetes.io/enforce=restricted \\ pod-security.kubernetes.io/audit=restricted \\ pod-security.kubernetes.io/warn=restricted ``` ### Secrets Management Use AWS Secrets Manager for sensitive credentials: ```bash # Install External Secrets Operator helm install external-secrets external-secrets/external-secrets \\ --namespace external-secrets-system \\ --create-namespace # Create SecretStore kubectl apply -f - <<EOF apiVersion: external-secrets.io/v1beta1 kind: SecretStore metadata: name: aws-secrets-manager namespace: cloudflow-prod spec: provider: aws: service: SecretsManager region: us-east-1 auth: jwt: serviceAccountRef: name: external-secrets-sa EOF ``` --- ## Troubleshooting ### Common Issues #### Pods in CrashLoopBackOff **Symptoms**: Pods continuously restart **Diagnosis**: ```bash kubectl logs -n cloudflow-prod <pod-name> --previous kubectl describe pod -n cloudflow-prod <pod-name> ``` **Common Causes**: - Database connection failure - Invalid environment variables - Insufficient resources #### High Memory Usage **Symptoms**: Pods being OOMKilled **Diagnosis**: ```bash kubectl top pods -n cloudflow-prod ``` **Resolution**: - Increase memory limits in deployment - Check for memory leaks in application logs - Review heap dump for Node.js processes",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_24",
              "content": "```bash # Extract correlation ID from error CORRELATION_ID=\"corr_8h4j9k2m5n\" # Trace request across all services for pod in $(kubectl get pods -n cloudflow -l tier=backend -o name); do echo \"=== $pod ===\" kubectl logs -n cloudflow $pod | grep $CORRELATION_ID done # Export full trace to file cloudflow debug trace $CORRELATION_ID --output trace-$CORRELATION_ID.json ``` ### Debugging Commands #### Enable Debug Mode for Workflow Execution ```bash # Execute workflow with debug logging cloudflow workflows execute wf_9k2n4m8p1q \\ --debug \\ --log-level TRACE \\ --output-logs /tmp/workflow-debug.log # Enable step-by-step execution cloudflow workflows execute wf_9k2n4m8p1q \\ --step-mode interactive \\ --breakpoint-on-error # Capture full execution context cloudflow workflows execute wf_9k2n4m8p1q \\ --capture-context \\ --context-output /tmp/execution-context.json ``` #### Database Query Debugging ```bash # Enable query logging cloudflow db config set log_statement all cloudflow db config set log_duration on cloudflow db config set log_min_duration_statement 1000 # Log queries > 1s # Capture query plan for slow endpoint cloudflow",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_20",
              "content": "Leverage caching:** ```bash # Enable client-side caching export CLOUDFLOW_CACHE_ENABLED=true export CLOUDFLOW_CACHE_TTL=300 # Cache workflow metadata cloudflow workflows list --use-cache --cache-ttl 600 ``` --- ## Log Analysis & Debugging ### Accessing CloudFlow Logs #### Kubernetes Deployments ```bash # List all CloudFlow pods kubectl get pods -n cloudflow # Tail logs from API server kubectl logs -f -n cloudflow deployment/cloudflow-api --tail=100 # Get logs from specific pod kubectl logs -n cloudflow cloudflow-api-7d4f6b8c9d-x7k2m # Get logs from previous crashed pod kubectl logs -n cloudflow cloudflow-api-7d4f6b8c9d-x7k2m --previous # Get logs from all pods in deployment kubectl logs -n cloudflow deployment/cloudflow-api --all-containers=true # Stream logs from multiple pods kubectl logs -n cloudflow -l app=cloudflow-api -f --max-log-requests=10 ``` #### Log Levels CloudFlow supports the following log levels: - `TRACE` - Very detailed debugging information - `DEBUG` - Detailed debugging information - `INFO` - Informational messages (default) - `WARN` - Warning messages - `ERROR` - Error messages",
              "score": null
            },
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_7",
              "content": "**Endpoint:** `GET /workflows/{workflow_id}` **Example Request:** ```bash curl -H \"X-API-Key: YOUR_API_KEY\" \\ https://api.cloudflow.io/v2/workflows/wf_8x7k2m9p ``` **Example Response (200 OK):** ```json { \"id\": \"wf_8x7k2m9p\", \"name\": \"Data Processing Pipeline\", \"description\": \"Processes customer data every hour\", \"status\": \"active\", \"trigger\": { \"type\": \"schedule\", \"cron\": \"0 * * * *\", \"timezone\": \"UTC\" }, \"steps\": [ { \"id\": \"step_1\", \"type\": \"fetch_data\", \"source\": \"customer_db\", \"query\": \"SELECT * FROM customers WHERE updated_at > :last_run\" }, { \"id\": \"step_2\", \"type\": \"transform\", \"script_id\": \"scr_transform_v3\" } ], \"metrics\": { \"total_runs\": 1247, \"success_rate\": 98.7, \"avg_duration_ms\": 3420, \"last_error\": null }, \"created_at\": \"2026-01-15T10:30:00Z\", \"updated_at\": \"2026-01-23T14:22:00Z\" } ``` #### Update Workflow Modify an existing workflow's configuration. **Endpoint:** `PATCH /workflows/{workflow_id}` **Example Request:** ```bash curl -X PATCH https://api.cloudflow.io/v2/workflows/wf_8x7k2m9p \\ -H \"X-API-Key: YOUR_API_KEY\" \\ -H \"Content-Type: application/json\" \\ -d '{\"status\": \"paused\"}' ``` **Example Response (200 OK):** ```json { \"id\": \"wf_8x7k2m9p\", \"status\": \"paused\", \"updated_at\": \"2026-01-24T10:30:00Z\" } ``` #### Delete Workflow Permanently delete a workflow.",
              "score": null
            },
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_13",
              "content": "**Supported Events:** - `workflow.started` - `workflow.completed` - `workflow.failed` - `pipeline.completed` - `pipeline.failed` **Webhook Payload Example:** ```json { \"event\": \"workflow.completed\", \"timestamp\": \"2026-01-24T10:45:00Z\", \"data\": { \"workflow_id\": \"wf_8x7k2m9p\", \"execution_id\": \"exec_9k3m7n2q\", \"status\": \"completed\", \"duration_ms\": 3420, \"records_processed\": 1247 } } ``` Configure webhooks in your account settings or via the API: ```bash curl -X POST https://api.cloudflow.io/v2/webhooks \\ -H \"X-API-Key: YOUR_API_KEY\" \\ -H \"Content-Type: application/json\" \\ -d '{ \"url\": \"https://your-app.com/cloudflow-webhook\", \"events\": [\"workflow.completed\", \"workflow.failed\"], \"secret\": \"whsec_your_webhook_secret\" }' ``` ## Support For additional help and resources: - **Documentation:** https://docs.cloudflow.io - **API Status:** https://status.cloudflow.io - **Support Email:** support@cloudflow.io - **Community Forum:** https://community.cloudflow.io Enterprise customers have access to 24/7 priority support via phone and dedicated Slack channels.",
              "score": null
            }
          ],
          "latency_ms": 5.60243099971558,
          "query_id": "realistic_005",
          "dimension": "problem",
          "query": "I can't find the production pods"
        },
        {
          "key_facts": [
            "cloudflow-prod"
          ],
          "found_facts": [
            "cloudflow-prod"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "deployment_guide"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_17",
              "content": "namespace level: ```bash kubectl label namespace cloudflow-prod \\ pod-security.kubernetes.io/enforce=restricted \\ pod-security.kubernetes.io/audit=restricted \\ pod-security.kubernetes.io/warn=restricted ``` ### Secrets Management Use AWS Secrets Manager for sensitive credentials: ```bash # Install External Secrets Operator helm install external-secrets external-secrets/external-secrets \\ --namespace external-secrets-system \\ --create-namespace # Create SecretStore kubectl apply -f - <<EOF apiVersion: external-secrets.io/v1beta1 kind: SecretStore metadata: name: aws-secrets-manager namespace: cloudflow-prod spec: provider: aws: service: SecretsManager region: us-east-1 auth: jwt: serviceAccountRef: name: external-secrets-sa EOF ``` --- ## Troubleshooting ### Common Issues #### Pods in CrashLoopBackOff **Symptoms**: Pods continuously restart **Diagnosis**: ```bash kubectl logs -n cloudflow-prod <pod-name> --previous kubectl describe pod -n cloudflow-prod <pod-name> ``` **Common Causes**: - Database connection failure - Invalid environment variables - Insufficient resources #### High Memory Usage **Symptoms**: Pods being OOMKilled **Diagnosis**: ```bash kubectl top pods -n cloudflow-prod ``` **Resolution**: - Increase memory limits in deployment - Check for memory leaks in application logs - Review heap dump for Node.js processes",
              "score": null
            },
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_3",
              "content": "-f cluster-config.yaml ``` ### Storage Configuration Install the EBS CSI driver for persistent volumes: ```bash eksctl create addon --name aws-ebs-csi-driver \\ --cluster cloudflow-production \\ --service-account-role-arn arn:aws:iam::ACCOUNT_ID:role/AmazonEKS_EBS_CSI_DriverRole \\ --force ``` --- ## Kubernetes Deployment ### Namespace Setup Create the CloudFlow namespace and configure resource quotas: ```yaml # namespace.yaml apiVersion: v1 kind: Namespace metadata: name: cloudflow-prod labels: name: cloudflow-prod environment: production --- apiVersion: v1 kind: ResourceQuota metadata: name: cloudflow-quota namespace: cloudflow-prod spec: hard: requests.cpu: \"50\" requests.memory: 100Gi persistentvolumeclaims: \"10\" services.loadbalancers: \"2\" ``` Apply the namespace configuration: ```bash kubectl apply -f namespace.yaml ``` ### Environment Configuration CloudFlow requires the following environment variables: | Variable | Description | Example | Required | |----------|-------------|---------|----------| | `DATABASE_URL` | PostgreSQL connection string | `postgresql://user:pass@host:5432/cloudflow` | Yes | | `REDIS_URL` | Redis connection string | `redis://redis-master.cloudflow-prod.svc.cluster.local:6379` | Yes | | `JWT_SECRET` | Secret key for JWT token signing | `<generated-secret-256-bit>` | Yes | | `LOG_LEVEL` | Application log",
              "score": null
            },
            {
              "doc_id": "user_guide",
              "chunk_id": "user_guide_fix_22",
              "content": "Version Control For critical workflows: - Export YAML definitions regularly - Store in version control (Git) - Use pull requests for changes - Tag releases with semantic versioning ## Common Workflow Patterns Here are proven patterns for common automation scenarios: ### Pattern 1: Form to Database Capture form submissions and store in database: ```yaml name: \"Contact Form to Database\" trigger: type: webhook method: POST steps: - id: validate_submission action: javascript code: | if (!input.email || !input.message) { throw new Error(\"Email and message required\"); } return input; - id: check_duplicate action: database_query config: connection: \"{{secrets.DB_CONNECTION}}\" query: \"SELECT id FROM contacts WHERE email = $1 AND created_at > NOW() - INTERVAL '1 hour'\" parameters: - \"{{trigger.body.email}}\" - id: insert_contact action: database_query condition: \"{{length(steps.check_duplicate.rows) == 0}}\" config: connection: \"{{secrets.DB_CONNECTION}}\" query: | INSERT INTO contacts (name, email, message, source, created_at) VALUES ($1, $2, $3, $4, NOW()) parameters: - \"{{trigger.body.name}}\" - \"{{trigger.body.email}}\" - \"{{trigger.body.message}}\" -",
              "score": null
            },
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_4",
              "content": "verbosity | `info`, `debug`, `warn`, `error` | No (default: `info`) | | `API_PORT` | API server port | `3000` | No (default: `3000`) | | `WORKER_CONCURRENCY` | Number of concurrent workers | `10` | No (default: `5`) | | `SESSION_SECRET` | Session encryption key | `<generated-secret-256-bit>` | Yes | | `AWS_REGION` | AWS region for services | `us-east-1` | Yes | | `S3_BUCKET` | S3 bucket for file storage | `cloudflow-prod-storage` | Yes | | `SMTP_HOST` | Email server hostname | `smtp.sendgrid.net` | No | | `SMTP_PORT` | Email server port | `587` | No | | `METRICS_ENABLED` | Enable Prometheus metrics | `true` | No (default: `false`) | Create a Kubernetes secret for sensitive variables: ```bash kubectl create secret generic cloudflow-secrets \\ --namespace cloudflow-prod \\ --from-literal=DATABASE_URL=\"postgresql://cloudflow:$(cat db-password.txt)@pgbouncer.cloudflow-prod.svc.cluster.local:5432/cloudflow\" \\ --from-literal=REDIS_URL=\"redis://redis-master.cloudflow-prod.svc.cluster.local:6379\" \\ --from-literal=JWT_SECRET=\"$(openssl rand -base64 32)\" \\ --from-literal=SESSION_SECRET=\"$(openssl rand -base64 32)\" ``` ### Helm Chart Deployment Add the CloudFlow Helm repository:",
              "score": null
            },
            {
              "doc_id": "user_guide",
              "chunk_id": "user_guide_fix_21",
              "content": "Keep Workflows Modular Break complex workflows into smaller, reusable components: - Use sub-workflows for repeated logic - Trigger child workflows via webhooks - Share common configurations via templates ### 8. Test Thoroughly Before activating a workflow: 1. Use test mode with sample data 2. Verify all actions execute correctly 3. Test error handling paths 4. Review execution logs 5. Start with a limited scope (e.g., test channel, small dataset) ### 9. Document Your Workflows Add descriptions to workflows and steps: ```yaml workflow: name: \"Daily Sales Report\" description: | Generates a daily sales report and distributes it to the sales team. Runs at 8:00 AM EST Monday-Friday. Data source: PostgreSQL sales database Recipients: sales-team@company.com steps: - id: fetch_sales name: \"Fetch Yesterday's Sales\" description: \"Query database for all completed orders from previous day\" ``` ### 10.",
              "score": null
            }
          ],
          "latency_ms": 5.540165000638808,
          "query_id": "realistic_005",
          "dimension": "casual",
          "query": "prod namespace"
        },
        {
          "key_facts": [
            "cloudflow-prod"
          ],
          "found_facts": [
            "cloudflow-prod"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "deployment_guide"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_17",
              "content": "namespace level: ```bash kubectl label namespace cloudflow-prod \\ pod-security.kubernetes.io/enforce=restricted \\ pod-security.kubernetes.io/audit=restricted \\ pod-security.kubernetes.io/warn=restricted ``` ### Secrets Management Use AWS Secrets Manager for sensitive credentials: ```bash # Install External Secrets Operator helm install external-secrets external-secrets/external-secrets \\ --namespace external-secrets-system \\ --create-namespace # Create SecretStore kubectl apply -f - <<EOF apiVersion: external-secrets.io/v1beta1 kind: SecretStore metadata: name: aws-secrets-manager namespace: cloudflow-prod spec: provider: aws: service: SecretsManager region: us-east-1 auth: jwt: serviceAccountRef: name: external-secrets-sa EOF ``` --- ## Troubleshooting ### Common Issues #### Pods in CrashLoopBackOff **Symptoms**: Pods continuously restart **Diagnosis**: ```bash kubectl logs -n cloudflow-prod <pod-name> --previous kubectl describe pod -n cloudflow-prod <pod-name> ``` **Common Causes**: - Database connection failure - Invalid environment variables - Insufficient resources #### High Memory Usage **Symptoms**: Pods being OOMKilled **Diagnosis**: ```bash kubectl top pods -n cloudflow-prod ``` **Resolution**: - Increase memory limits in deployment - Check for memory leaks in application logs - Review heap dump for Node.js processes",
              "score": null
            },
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_3",
              "content": "-f cluster-config.yaml ``` ### Storage Configuration Install the EBS CSI driver for persistent volumes: ```bash eksctl create addon --name aws-ebs-csi-driver \\ --cluster cloudflow-production \\ --service-account-role-arn arn:aws:iam::ACCOUNT_ID:role/AmazonEKS_EBS_CSI_DriverRole \\ --force ``` --- ## Kubernetes Deployment ### Namespace Setup Create the CloudFlow namespace and configure resource quotas: ```yaml # namespace.yaml apiVersion: v1 kind: Namespace metadata: name: cloudflow-prod labels: name: cloudflow-prod environment: production --- apiVersion: v1 kind: ResourceQuota metadata: name: cloudflow-quota namespace: cloudflow-prod spec: hard: requests.cpu: \"50\" requests.memory: 100Gi persistentvolumeclaims: \"10\" services.loadbalancers: \"2\" ``` Apply the namespace configuration: ```bash kubectl apply -f namespace.yaml ``` ### Environment Configuration CloudFlow requires the following environment variables: | Variable | Description | Example | Required | |----------|-------------|---------|----------| | `DATABASE_URL` | PostgreSQL connection string | `postgresql://user:pass@host:5432/cloudflow` | Yes | | `REDIS_URL` | Redis connection string | `redis://redis-master.cloudflow-prod.svc.cluster.local:6379` | Yes | | `JWT_SECRET` | Secret key for JWT token signing | `<generated-secret-256-bit>` | Yes | | `LOG_LEVEL` | Application log",
              "score": null
            },
            {
              "doc_id": "user_guide",
              "chunk_id": "user_guide_fix_22",
              "content": "Version Control For critical workflows: - Export YAML definitions regularly - Store in version control (Git) - Use pull requests for changes - Tag releases with semantic versioning ## Common Workflow Patterns Here are proven patterns for common automation scenarios: ### Pattern 1: Form to Database Capture form submissions and store in database: ```yaml name: \"Contact Form to Database\" trigger: type: webhook method: POST steps: - id: validate_submission action: javascript code: | if (!input.email || !input.message) { throw new Error(\"Email and message required\"); } return input; - id: check_duplicate action: database_query config: connection: \"{{secrets.DB_CONNECTION}}\" query: \"SELECT id FROM contacts WHERE email = $1 AND created_at > NOW() - INTERVAL '1 hour'\" parameters: - \"{{trigger.body.email}}\" - id: insert_contact action: database_query condition: \"{{length(steps.check_duplicate.rows) == 0}}\" config: connection: \"{{secrets.DB_CONNECTION}}\" query: | INSERT INTO contacts (name, email, message, source, created_at) VALUES ($1, $2, $3, $4, NOW()) parameters: - \"{{trigger.body.name}}\" - \"{{trigger.body.email}}\" - \"{{trigger.body.message}}\" -",
              "score": null
            },
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_0",
              "content": "# CloudFlow Platform - Deployment and Operations Guide **Version:** 2.4.0 **Last Updated:** January 2026 **Target Environment:** Production (AWS EKS) ## Table of Contents 1. [Overview](#overview) 2. [Prerequisites](#prerequisites) 3. [Infrastructure Setup](#infrastructure-setup) 4. [Kubernetes Deployment](#kubernetes-deployment) 5. [Database Configuration](#database-configuration) 6. [Monitoring and Observability](#monitoring-and-observability) 7. [Backup and Disaster Recovery](#backup-and-disaster-recovery) 8. [Scaling and Performance](#scaling-and-performance) 9. [Security and Compliance](#security-and-compliance) 10. [Troubleshooting](#troubleshooting) --- ## Overview CloudFlow is a cloud-native workflow orchestration platform designed for high-availability production environments. This guide provides comprehensive instructions for deploying and operating CloudFlow on Amazon EKS (Elastic Kubernetes Service).",
              "score": null
            },
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_5",
              "content": "```bash helm repo add cloudflow https://charts.cloudflow.io helm repo update ``` Create a values file for production configuration: ```yaml # values-production.yaml replicaCount: 3 image: repository: 123456789012.dkr.ecr.us-east-1.amazonaws.com/cloudflow tag: \"2.4.0\" pullPolicy: IfNotPresent resources: limits: cpu: 2000m memory: 4Gi requests: cpu: 2000m memory: 4Gi autoscaling: enabled: true minReplicas: 3 maxReplicas: 10 targetCPUUtilizationPercentage: 70 targetMemoryUtilizationPercentage: 80 service: type: ClusterIP port: 80 targetPort: 3000 annotations: service.beta.kubernetes.io/aws-load-balancer-type: \"nlb\" ingress: enabled: true className: nginx annotations: cert-manager.io/cluster-issuer: \"letsencrypt-prod\" nginx.ingress.kubernetes.io/ssl-redirect: \"true\" hosts: - host: api.cloudflow.io paths: - path: / pathType: Prefix tls: - secretName: cloudflow-tls hosts: - api.cloudflow.io healthCheck: enabled: true livenessProbe: path: /health initialDelaySeconds: 30 periodSeconds: 10 timeoutSeconds: 5 failureThreshold: 3 readinessProbe: path: /ready initialDelaySeconds: 10 periodSeconds: 5 timeoutSeconds: 3 failureThreshold: 3 env: - name: NODE_ENV value: \"production\" - name: LOG_LEVEL value: \"info\" - name: API_PORT value: \"3000\" - name: WORKER_CONCURRENCY value: \"10\" - name: AWS_REGION value: \"us-east-1\" - name: METRICS_ENABLED value: \"true\" envFrom: - secretRef: name: cloudflow-secrets podAnnotations:",
              "score": null
            }
          ],
          "latency_ms": 5.5591609998373315,
          "query_id": "realistic_005",
          "dimension": "contextual",
          "query": "I need to deploy to production, what namespace should I use?"
        },
        {
          "key_facts": [
            "cloudflow-prod"
          ],
          "found_facts": [
            "cloudflow-prod"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "deployment_guide"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_17",
              "content": "namespace level: ```bash kubectl label namespace cloudflow-prod \\ pod-security.kubernetes.io/enforce=restricted \\ pod-security.kubernetes.io/audit=restricted \\ pod-security.kubernetes.io/warn=restricted ``` ### Secrets Management Use AWS Secrets Manager for sensitive credentials: ```bash # Install External Secrets Operator helm install external-secrets external-secrets/external-secrets \\ --namespace external-secrets-system \\ --create-namespace # Create SecretStore kubectl apply -f - <<EOF apiVersion: external-secrets.io/v1beta1 kind: SecretStore metadata: name: aws-secrets-manager namespace: cloudflow-prod spec: provider: aws: service: SecretsManager region: us-east-1 auth: jwt: serviceAccountRef: name: external-secrets-sa EOF ``` --- ## Troubleshooting ### Common Issues #### Pods in CrashLoopBackOff **Symptoms**: Pods continuously restart **Diagnosis**: ```bash kubectl logs -n cloudflow-prod <pod-name> --previous kubectl describe pod -n cloudflow-prod <pod-name> ``` **Common Causes**: - Database connection failure - Invalid environment variables - Insufficient resources #### High Memory Usage **Symptoms**: Pods being OOMKilled **Diagnosis**: ```bash kubectl top pods -n cloudflow-prod ``` **Resolution**: - Increase memory limits in deployment - Check for memory leaks in application logs - Review heap dump for Node.js processes",
              "score": null
            },
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_3",
              "content": "-f cluster-config.yaml ``` ### Storage Configuration Install the EBS CSI driver for persistent volumes: ```bash eksctl create addon --name aws-ebs-csi-driver \\ --cluster cloudflow-production \\ --service-account-role-arn arn:aws:iam::ACCOUNT_ID:role/AmazonEKS_EBS_CSI_DriverRole \\ --force ``` --- ## Kubernetes Deployment ### Namespace Setup Create the CloudFlow namespace and configure resource quotas: ```yaml # namespace.yaml apiVersion: v1 kind: Namespace metadata: name: cloudflow-prod labels: name: cloudflow-prod environment: production --- apiVersion: v1 kind: ResourceQuota metadata: name: cloudflow-quota namespace: cloudflow-prod spec: hard: requests.cpu: \"50\" requests.memory: 100Gi persistentvolumeclaims: \"10\" services.loadbalancers: \"2\" ``` Apply the namespace configuration: ```bash kubectl apply -f namespace.yaml ``` ### Environment Configuration CloudFlow requires the following environment variables: | Variable | Description | Example | Required | |----------|-------------|---------|----------| | `DATABASE_URL` | PostgreSQL connection string | `postgresql://user:pass@host:5432/cloudflow` | Yes | | `REDIS_URL` | Redis connection string | `redis://redis-master.cloudflow-prod.svc.cluster.local:6379` | Yes | | `JWT_SECRET` | Secret key for JWT token signing | `<generated-secret-256-bit>` | Yes | | `LOG_LEVEL` | Application log",
              "score": null
            },
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_4",
              "content": "verbosity | `info`, `debug`, `warn`, `error` | No (default: `info`) | | `API_PORT` | API server port | `3000` | No (default: `3000`) | | `WORKER_CONCURRENCY` | Number of concurrent workers | `10` | No (default: `5`) | | `SESSION_SECRET` | Session encryption key | `<generated-secret-256-bit>` | Yes | | `AWS_REGION` | AWS region for services | `us-east-1` | Yes | | `S3_BUCKET` | S3 bucket for file storage | `cloudflow-prod-storage` | Yes | | `SMTP_HOST` | Email server hostname | `smtp.sendgrid.net` | No | | `SMTP_PORT` | Email server port | `587` | No | | `METRICS_ENABLED` | Enable Prometheus metrics | `true` | No (default: `false`) | Create a Kubernetes secret for sensitive variables: ```bash kubectl create secret generic cloudflow-secrets \\ --namespace cloudflow-prod \\ --from-literal=DATABASE_URL=\"postgresql://cloudflow:$(cat db-password.txt)@pgbouncer.cloudflow-prod.svc.cluster.local:5432/cloudflow\" \\ --from-literal=REDIS_URL=\"redis://redis-master.cloudflow-prod.svc.cluster.local:6379\" \\ --from-literal=JWT_SECRET=\"$(openssl rand -base64 32)\" \\ --from-literal=SESSION_SECRET=\"$(openssl rand -base64 32)\" ``` ### Helm Chart Deployment Add the CloudFlow Helm repository:",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_29",
              "content": "Root cause analysis # 5. Resolution steps # 6. Action items # 7. Lessons learned ``` --- ## Additional Resources - **CloudFlow Documentation:** https://docs.cloudflow.io - **API Reference:** https://api-docs.cloudflow.io - **Community Forum:** https://community.cloudflow.io - **GitHub Issues:** https://github.com/cloudflow/platform/issues - **Training Portal:** https://training.cloudflow.io - **Monitoring Dashboard:** https://monitoring.cloudflow.io ### Getting Help If this troubleshooting guide doesn't resolve your issue: 1. Search the knowledge base: `cloudflow kb search \"your issue\"` 2. Check community forum for similar issues 3. Contact support with detailed logs and reproduction steps 4. For urgent issues, use emergency escalation procedures **Remember:** Always capture logs, metrics, and reproduction steps before escalating! --- *Last updated: January 24, 2026* *Document version: 3.2.1* *Feedback: docs-feedback@cloudflow.io*",
              "score": null
            },
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_5",
              "content": "```bash helm repo add cloudflow https://charts.cloudflow.io helm repo update ``` Create a values file for production configuration: ```yaml # values-production.yaml replicaCount: 3 image: repository: 123456789012.dkr.ecr.us-east-1.amazonaws.com/cloudflow tag: \"2.4.0\" pullPolicy: IfNotPresent resources: limits: cpu: 2000m memory: 4Gi requests: cpu: 2000m memory: 4Gi autoscaling: enabled: true minReplicas: 3 maxReplicas: 10 targetCPUUtilizationPercentage: 70 targetMemoryUtilizationPercentage: 80 service: type: ClusterIP port: 80 targetPort: 3000 annotations: service.beta.kubernetes.io/aws-load-balancer-type: \"nlb\" ingress: enabled: true className: nginx annotations: cert-manager.io/cluster-issuer: \"letsencrypt-prod\" nginx.ingress.kubernetes.io/ssl-redirect: \"true\" hosts: - host: api.cloudflow.io paths: - path: / pathType: Prefix tls: - secretName: cloudflow-tls hosts: - api.cloudflow.io healthCheck: enabled: true livenessProbe: path: /health initialDelaySeconds: 30 periodSeconds: 10 timeoutSeconds: 5 failureThreshold: 3 readinessProbe: path: /ready initialDelaySeconds: 10 periodSeconds: 5 timeoutSeconds: 3 failureThreshold: 3 env: - name: NODE_ENV value: \"production\" - name: LOG_LEVEL value: \"info\" - name: API_PORT value: \"3000\" - name: WORKER_CONCURRENCY value: \"10\" - name: AWS_REGION value: \"us-east-1\" - name: METRICS_ENABLED value: \"true\" envFrom: - secretRef: name: cloudflow-secrets podAnnotations:",
              "score": null
            }
          ],
          "latency_ms": 5.347856998923817,
          "query_id": "realistic_005",
          "dimension": "negation",
          "query": "Why can't I see resources in the default namespace?"
        },
        {
          "key_facts": [
            "2 vCPU, 4GB RAM per pod"
          ],
          "found_facts": [
            "2 vCPU, 4GB RAM per pod"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "architecture_overview"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_11",
              "content": "Codes CloudFlow returns specific error codes to help you identify and resolve issues: - `invalid_parameter`: One or more request parameters are invalid - `missing_required_field`: Required field is missing from request body - `authentication_failed`: Invalid API key or token - `insufficient_permissions`: User lacks required scope or permission - `resource_not_found`: Requested resource does not exist - `rate_limit_exceeded`: Too many requests, see rate limiting section - `workflow_execution_failed`: Workflow execution encountered an error - `invalid_json`: Request body contains malformed JSON - `duplicate_resource`: Resource with same identifier already exists - `quota_exceeded`: Account quota limit reached ### Error Handling Best Practices **Python Example:** ```python import requests import time def make_api_request(url, headers, max_retries=3): for attempt in range(max_retries): try: response = requests.get(url, headers=headers) if response.status_code == 200: return response.json() elif response.status_code == 429: # Rate limit exceeded retry_after = int(response.headers.get('X-RateLimit-Reset', 60)) print(f\"Rate limited.",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_11",
              "content": "Client \u2192 Load Balancer (TLS termination) 2. Load Balancer \u2192 API Gateway (HTTP/2) 3. API Gateway \u2192 Auth Service (JWT validation via gRPC) 4. API Gateway \u2192 Target Service (HTTP/REST or gRPC) 5. Target Service \u2192 Database/Cache (data retrieval) 6. Response propagates back through the chain ``` **Timeout Configuration**: - Client \u2192 Load Balancer: 60 seconds - Load Balancer \u2192 API Gateway: 55 seconds - API Gateway \u2192 Services: 45 seconds - Service \u2192 Database: 10 seconds ### Asynchronous Event Flow Long-running operations and inter-service communication use event-driven patterns: ``` 1. Service publishes event \u2192 Kafka topic 2. Kafka persists event (replication factor: 3) 3. Consumer groups subscribe to topics 4. Consumers process events with at-least-once delivery 5. State updates written to PostgreSQL 6.",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_25",
              "content": "limiting: 1,000 requests per minute per API key **Workflow Engine**: - Concurrent executions: 8,000 workflows (across 16 pods) - Execution start rate: 500 per second - Completion rate: 450 per second (average 2-second execution time) **Database**: - Read throughput: 50,000 queries per second (across replicas) - Write throughput: 15,000 transactions per second - Connection capacity: 2,000 concurrent connections **Kafka**: - Message ingestion: 100,000 messages per second - Consumer throughput: 80,000 messages per second (aggregated) - End-to-end latency: < 100ms (P99) ### Resource Utilization **CPU Utilization** (Target: 60-70% average): - API Gateway: 55% average, 80% peak - Workflow Engine: 65% average, 85% peak - Auth Service: 40% average, 60% peak **Memory Utilization** (Target: < 80%): - API Gateway: 2.5GB average per pod (4GB allocated) - Workflow Engine: 6GB average per pod (8GB allocated) - Notification Service: 2.8GB average per pod (4GB allocated) **Network Throughput**: - Ingress: 2 Gbps average, 5 Gbps",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_2",
              "content": "\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 ``` ### Technology Stack - **Runtime**: Node.js 20.x (API Gateway, Workflow Engine), Go 1.21 (Auth Service, Scheduler) - **Container Orchestration**: Kubernetes 1.28 (EKS) - **Service Mesh**: Istio 1.20 for service-to-service communication - **Databases**: PostgreSQL 15.4, Redis 7.2 - **Message Broker**: Apache Kafka 3.6 - **Monitoring**: Prometheus, Grafana, Jaeger for distributed tracing - **Secrets Management**: HashiCorp Vault 1.15 --- ## Microservices Breakdown ### API Gateway **Purpose**: Single entry point for all client requests, providing authentication, rate limiting, request routing, and protocol translation.",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_3",
              "content": "**Technology**: Node.js with Express.js framework **Replicas**: 12 pods (production), auto-scaling 8-20 based on CPU **Resource Allocation**: 2 vCPU, 4GB RAM per pod **Key Responsibilities**: - JWT token validation (delegated to Auth Service for initial validation) - Rate limiting: 1000 requests per minute per API key (sliding window) - Request/response transformation and validation using JSON Schema - Routing to downstream services based on URL path patterns - CORS handling for web clients - Request/response logging and correlation ID injection **Critical Endpoints**: - `POST /api/v1/workflows` - Create new workflow - `GET /api/v1/workflows/:id` - Retrieve workflow status - `POST /api/v1/workflows/:id/execute` - Trigger workflow execution - `GET /api/v1/workflows/:id/history` - Get execution history **Dependencies**: - Auth Service (for token validation) - Redis (for rate limiting counters) - All downstream microservices **Performance Targets**: - P50 latency: < 50ms - P99 latency: < 200ms - Throughput: 10,000 RPS sustained --- ### Auth Service **Purpose**: Centralized authentication and",
              "score": null
            }
          ],
          "latency_ms": 5.379354999604402,
          "query_id": "realistic_006",
          "dimension": "original",
          "query": "What are the resource requirements for the API Gateway?"
        },
        {
          "key_facts": [
            "2 vCPU, 4GB RAM per pod"
          ],
          "found_facts": [
            "2 vCPU, 4GB RAM per pod"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "architecture_overview"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_25",
              "content": "limiting: 1,000 requests per minute per API key **Workflow Engine**: - Concurrent executions: 8,000 workflows (across 16 pods) - Execution start rate: 500 per second - Completion rate: 450 per second (average 2-second execution time) **Database**: - Read throughput: 50,000 queries per second (across replicas) - Write throughput: 15,000 transactions per second - Connection capacity: 2,000 concurrent connections **Kafka**: - Message ingestion: 100,000 messages per second - Consumer throughput: 80,000 messages per second (aggregated) - End-to-end latency: < 100ms (P99) ### Resource Utilization **CPU Utilization** (Target: 60-70% average): - API Gateway: 55% average, 80% peak - Workflow Engine: 65% average, 85% peak - Auth Service: 40% average, 60% peak **Memory Utilization** (Target: < 80%): - API Gateway: 2.5GB average per pod (4GB allocated) - Workflow Engine: 6GB average per pod (8GB allocated) - Notification Service: 2.8GB average per pod (4GB allocated) **Network Throughput**: - Ingress: 2 Gbps average, 5 Gbps",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_32",
              "content": "PostgreSQL, SendGrid, Twilio | No | ### Capacity Planning **Current Capacity** (Jan 2026): - Daily workflow executions: 2.5M - Active users: 150,000 - API requests per day: 50M - Database size: 1.2TB - Kafka throughput: 100K msg/sec peak **6-Month Projection** (Jul 2026): - Daily workflow executions: 4M (+60%) - Active users: 225,000 (+50%) - API requests per day: 80M (+60%) - Database size: 2TB (+67%) - Required scaling: +30% compute, +50% storage ### Contact Information - **Architecture Team**: architecture@cloudflow.internal - **On-Call Engineer**: PagerDuty escalation - **Security Team**: security@cloudflow.internal - **Documentation**: https://wiki.cloudflow.internal/architecture --- **Document Revision History**: - v2.3.1 (Jan 15, 2026): Updated performance metrics, added DR test results - v2.3.0 (Dec 1, 2025): Added multi-AZ deployment details - v2.2.0 (Oct 15, 2025): Security architecture overhaul - v2.1.0 (Sep 1, 2025): Initial Kafka migration documentation - v2.0.0 (Jul 1, 2025): Complete microservices rewrite",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_11",
              "content": "Client \u2192 Load Balancer (TLS termination) 2. Load Balancer \u2192 API Gateway (HTTP/2) 3. API Gateway \u2192 Auth Service (JWT validation via gRPC) 4. API Gateway \u2192 Target Service (HTTP/REST or gRPC) 5. Target Service \u2192 Database/Cache (data retrieval) 6. Response propagates back through the chain ``` **Timeout Configuration**: - Client \u2192 Load Balancer: 60 seconds - Load Balancer \u2192 API Gateway: 55 seconds - API Gateway \u2192 Services: 45 seconds - Service \u2192 Database: 10 seconds ### Asynchronous Event Flow Long-running operations and inter-service communication use event-driven patterns: ``` 1. Service publishes event \u2192 Kafka topic 2. Kafka persists event (replication factor: 3) 3. Consumer groups subscribe to topics 4. Consumers process events with at-least-once delivery 5. State updates written to PostgreSQL 6.",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_3",
              "content": "**Technology**: Node.js with Express.js framework **Replicas**: 12 pods (production), auto-scaling 8-20 based on CPU **Resource Allocation**: 2 vCPU, 4GB RAM per pod **Key Responsibilities**: - JWT token validation (delegated to Auth Service for initial validation) - Rate limiting: 1000 requests per minute per API key (sliding window) - Request/response transformation and validation using JSON Schema - Routing to downstream services based on URL path patterns - CORS handling for web clients - Request/response logging and correlation ID injection **Critical Endpoints**: - `POST /api/v1/workflows` - Create new workflow - `GET /api/v1/workflows/:id` - Retrieve workflow status - `POST /api/v1/workflows/:id/execute` - Trigger workflow execution - `GET /api/v1/workflows/:id/history` - Get execution history **Dependencies**: - Auth Service (for token validation) - Redis (for rate limiting counters) - All downstream microservices **Performance Targets**: - P50 latency: < 50ms - P99 latency: < 200ms - Throughput: 10,000 RPS sustained --- ### Auth Service **Purpose**: Centralized authentication and",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_2",
              "content": "\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 ``` ### Technology Stack - **Runtime**: Node.js 20.x (API Gateway, Workflow Engine), Go 1.21 (Auth Service, Scheduler) - **Container Orchestration**: Kubernetes 1.28 (EKS) - **Service Mesh**: Istio 1.20 for service-to-service communication - **Databases**: PostgreSQL 15.4, Redis 7.2 - **Message Broker**: Apache Kafka 3.6 - **Monitoring**: Prometheus, Grafana, Jaeger for distributed tracing - **Secrets Management**: HashiCorp Vault 1.15 --- ## Microservices Breakdown ### API Gateway **Purpose**: Single entry point for all client requests, providing authentication, rate limiting, request routing, and protocol translation.",
              "score": null
            }
          ],
          "latency_ms": 5.500841998582473,
          "query_id": "realistic_006",
          "dimension": "synonym",
          "query": "How much CPU and memory does the API Gateway need?"
        },
        {
          "key_facts": [
            "2 vCPU, 4GB RAM per pod"
          ],
          "found_facts": [],
          "missed_facts": [
            "2 vCPU, 4GB RAM per pod"
          ],
          "coverage": 0.0,
          "expected_docs": [
            "architecture_overview"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_2",
              "content": "\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 ``` ### Technology Stack - **Runtime**: Node.js 20.x (API Gateway, Workflow Engine), Go 1.21 (Auth Service, Scheduler) - **Container Orchestration**: Kubernetes 1.28 (EKS) - **Service Mesh**: Istio 1.20 for service-to-service communication - **Databases**: PostgreSQL 15.4, Redis 7.2 - **Message Broker**: Apache Kafka 3.6 - **Monitoring**: Prometheus, Grafana, Jaeger for distributed tracing - **Secrets Management**: HashiCorp Vault 1.15 --- ## Microservices Breakdown ### API Gateway **Purpose**: Single entry point for all client requests, providing authentication, rate limiting, request routing, and protocol translation.",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_25",
              "content": "limiting: 1,000 requests per minute per API key **Workflow Engine**: - Concurrent executions: 8,000 workflows (across 16 pods) - Execution start rate: 500 per second - Completion rate: 450 per second (average 2-second execution time) **Database**: - Read throughput: 50,000 queries per second (across replicas) - Write throughput: 15,000 transactions per second - Connection capacity: 2,000 concurrent connections **Kafka**: - Message ingestion: 100,000 messages per second - Consumer throughput: 80,000 messages per second (aggregated) - End-to-end latency: < 100ms (P99) ### Resource Utilization **CPU Utilization** (Target: 60-70% average): - API Gateway: 55% average, 80% peak - Workflow Engine: 65% average, 85% peak - Auth Service: 40% average, 60% peak **Memory Utilization** (Target: < 80%): - API Gateway: 2.5GB average per pod (4GB allocated) - Workflow Engine: 6GB average per pod (8GB allocated) - Notification Service: 2.8GB average per pod (4GB allocated) **Network Throughput**: - Ingress: 2 Gbps average, 5 Gbps",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_11",
              "content": "Client \u2192 Load Balancer (TLS termination) 2. Load Balancer \u2192 API Gateway (HTTP/2) 3. API Gateway \u2192 Auth Service (JWT validation via gRPC) 4. API Gateway \u2192 Target Service (HTTP/REST or gRPC) 5. Target Service \u2192 Database/Cache (data retrieval) 6. Response propagates back through the chain ``` **Timeout Configuration**: - Client \u2192 Load Balancer: 60 seconds - Load Balancer \u2192 API Gateway: 55 seconds - API Gateway \u2192 Services: 45 seconds - Service \u2192 Database: 10 seconds ### Asynchronous Event Flow Long-running operations and inter-service communication use event-driven patterns: ``` 1. Service publishes event \u2192 Kafka topic 2. Kafka persists event (replication factor: 3) 3. Consumer groups subscribe to topics 4. Consumers process events with at-least-once delivery 5. State updates written to PostgreSQL 6.",
              "score": null
            },
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_16",
              "content": "}, ], thresholds: { http_req_duration: ['p(95)<500'], }, }; export default function () { let res = http.get('https://api.cloudflow.io/api/workflows'); check(res, { 'status is 200': (r) => r.status === 200 }); sleep(1); } EOF ``` **Performance Targets**: - P95 API latency: < 500ms - P99 API latency: < 1000ms - Error rate: < 0.1% - Throughput: > 1000 requests/second --- ## Security and Compliance ### Network Policies Implement network policies to restrict pod-to-pod communication: ```yaml # network-policy.yaml apiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: cloudflow-network-policy namespace: cloudflow-prod spec: podSelector: matchLabels: app: cloudflow policyTypes: - Ingress - Egress ingress: - from: - namespaceSelector: matchLabels: name: cloudflow-prod - podSelector: matchLabels: app: nginx-ingress ports: - protocol: TCP port: 3000 egress: - to: - podSelector: matchLabels: app: postgresql ports: - protocol: TCP port: 5432 - to: - podSelector: matchLabels: app: redis ports: - protocol: TCP port: 6379 ``` ### Pod Security Standards Enforce Pod Security Standards at the",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_17",
              "content": "beginning on new consumer group) - Max poll records: 500 - Session timeout: 30 seconds --- ## Message Queue Patterns ### Pub/Sub Pattern Used for broadcasting events to multiple interested consumers: ``` Workflow Engine publishes workflow.execution.completed \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u25bc \u25bc \u25bc \u25bc Analytics Notification Audit Logger Billing Service Service Service Service ``` Each service maintains its own consumer group and processes events independently. Failures in one consumer don't affect others. ### Request-Reply Pattern Synchronous communication over async messaging (used sparingly): ``` API Gateway publishes request \u2192 Kafka (reply-to: temp-queue-123) \u2502 \u25bc Service processes request \u2502 \u25bc Service publishes response \u2192 temp-queue-123 \u2502 \u25bc API Gateway receives response ``` Timeout: 5 seconds, fallback to direct HTTP call if no response.",
              "score": null
            }
          ],
          "latency_ms": 5.684032999852207,
          "query_id": "realistic_006",
          "dimension": "problem",
          "query": "The API Gateway pods keep getting OOMKilled"
        },
        {
          "key_facts": [
            "2 vCPU, 4GB RAM per pod"
          ],
          "found_facts": [],
          "missed_facts": [
            "2 vCPU, 4GB RAM per pod"
          ],
          "coverage": 0.0,
          "expected_docs": [
            "architecture_overview"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "user_guide",
              "chunk_id": "user_guide_fix_5",
              "content": "workflows on-demand from the dashboard or via API ## Available Actions CloudFlow provides a comprehensive library of actions to build powerful automations. ### HTTP Requests Make HTTP requests to any API endpoint: **Configuration:** - **Method**: GET, POST, PUT, PATCH, DELETE, HEAD - **URL**: Full endpoint URL (supports variable interpolation) - **Headers**: Custom headers as key-value pairs - **Query Parameters**: URL parameters - **Body**: JSON, form data, or raw text - **Authentication**: Basic Auth, Bearer Token, API Key, OAuth 2.0 **Example:** ```yaml - id: fetch_user action: http_request config: method: GET url: \"https://api.example.com/users/{{user_id}}\" headers: Authorization: \"Bearer {{secrets.API_TOKEN}}\" Content-Type: \"application/json\" timeout: 30 ``` **Response Handling:** Access the response in subsequent steps: - `{{steps.fetch_user.status}}` - HTTP status code - `{{steps.fetch_user.body}}` - Response body - `{{steps.fetch_user.headers}}` - Response headers ### Database Queries Execute queries against supported databases (PostgreSQL, MySQL, MongoDB, Redis): **SQL Databases (PostgreSQL, MySQL):** ```yaml - id: get_orders action: database_query config: connection: \"{{secrets.DB_CONNECTION_STRING}}\" query:",
              "score": null
            },
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_0",
              "content": "# CloudFlow API Reference Version 2.1.0 | Last Updated: January 2026 ## Overview The CloudFlow API is a RESTful service that enables developers to programmatically manage cloud workflows, data pipelines, and automation tasks. This documentation provides comprehensive details on authentication, endpoints, request/response formats, error handling, and best practices. **Base URL:** `https://api.cloudflow.io/v2` **API Status:** https://status.cloudflow.io ## Authentication CloudFlow supports three authentication methods to suit different use cases and security requirements. ### API Keys API keys provide simple authentication for server-to-server communication. Include your API key in the request header: ```bash curl -H \"X-API-Key: cf_live_a1b2c3d4e5f6g7h8i9j0\" \\ https://api.cloudflow.io/v2/workflows ``` **Security Notes:** - Never expose API keys in client-side code - Rotate keys every 90 days - Use separate keys for development and production environments ### OAuth 2.0 OAuth 2.0 is recommended for applications that access CloudFlow on behalf of users. We support the Authorization Code flow with PKCE.",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_2",
              "content": "\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 ``` ### Technology Stack - **Runtime**: Node.js 20.x (API Gateway, Workflow Engine), Go 1.21 (Auth Service, Scheduler) - **Container Orchestration**: Kubernetes 1.28 (EKS) - **Service Mesh**: Istio 1.20 for service-to-service communication - **Databases**: PostgreSQL 15.4, Redis 7.2 - **Message Broker**: Apache Kafka 3.6 - **Monitoring**: Prometheus, Grafana, Jaeger for distributed tracing - **Secrets Management**: HashiCorp Vault 1.15 --- ## Microservices Breakdown ### API Gateway **Purpose**: Single entry point for all client requests, providing authentication, rate limiting, request routing, and protocol translation.",
              "score": null
            },
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_5",
              "content": "**Endpoint:** `GET /workflows` **Query Parameters:** - `status` (optional): Filter by status (`active`, `paused`, `archived`) - `created_after` (optional): ISO 8601 timestamp - `limit` (optional): Items per page (max 100) - `offset` (optional): Pagination offset **Example Request:** ```bash curl -H \"X-API-Key: YOUR_API_KEY\" \\ \"https://api.cloudflow.io/v2/workflows?status=active&limit=25\" ``` **Example Response (200 OK):** ```json { \"data\": [ { \"id\": \"wf_8x7k2m9p\", \"name\": \"Data Processing Pipeline\", \"description\": \"Processes customer data every hour\", \"status\": \"active\", \"trigger\": { \"type\": \"schedule\", \"cron\": \"0 * * * *\" }, \"steps\": 12, \"created_at\": \"2026-01-15T10:30:00Z\", \"updated_at\": \"2026-01-23T14:22:00Z\", \"last_run\": \"2026-01-24T09:00:00Z\" } ], \"pagination\": { \"total\": 1, \"limit\": 25, \"offset\": 0, \"has_more\": false } } ``` #### Create Workflow Create a new workflow with specified configuration.",
              "score": null
            },
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_7",
              "content": "**Endpoint:** `GET /workflows/{workflow_id}` **Example Request:** ```bash curl -H \"X-API-Key: YOUR_API_KEY\" \\ https://api.cloudflow.io/v2/workflows/wf_8x7k2m9p ``` **Example Response (200 OK):** ```json { \"id\": \"wf_8x7k2m9p\", \"name\": \"Data Processing Pipeline\", \"description\": \"Processes customer data every hour\", \"status\": \"active\", \"trigger\": { \"type\": \"schedule\", \"cron\": \"0 * * * *\", \"timezone\": \"UTC\" }, \"steps\": [ { \"id\": \"step_1\", \"type\": \"fetch_data\", \"source\": \"customer_db\", \"query\": \"SELECT * FROM customers WHERE updated_at > :last_run\" }, { \"id\": \"step_2\", \"type\": \"transform\", \"script_id\": \"scr_transform_v3\" } ], \"metrics\": { \"total_runs\": 1247, \"success_rate\": 98.7, \"avg_duration_ms\": 3420, \"last_error\": null }, \"created_at\": \"2026-01-15T10:30:00Z\", \"updated_at\": \"2026-01-23T14:22:00Z\" } ``` #### Update Workflow Modify an existing workflow's configuration. **Endpoint:** `PATCH /workflows/{workflow_id}` **Example Request:** ```bash curl -X PATCH https://api.cloudflow.io/v2/workflows/wf_8x7k2m9p \\ -H \"X-API-Key: YOUR_API_KEY\" \\ -H \"Content-Type: application/json\" \\ -d '{\"status\": \"paused\"}' ``` **Example Response (200 OK):** ```json { \"id\": \"wf_8x7k2m9p\", \"status\": \"paused\", \"updated_at\": \"2026-01-24T10:30:00Z\" } ``` #### Delete Workflow Permanently delete a workflow.",
              "score": null
            }
          ],
          "latency_ms": 5.626255999231944,
          "query_id": "realistic_006",
          "dimension": "casual",
          "query": "api gateway resources"
        },
        {
          "key_facts": [
            "2 vCPU, 4GB RAM per pod"
          ],
          "found_facts": [
            "2 vCPU, 4GB RAM per pod"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "architecture_overview"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_0",
              "content": "# CloudFlow Platform - System Architecture Overview **Document Version:** 2.3.1 **Last Updated:** January 15, 2026 **Owner:** Platform Architecture Team **Status:** Production ## Executive Summary CloudFlow is a distributed, cloud-native workflow automation platform designed to orchestrate complex business processes at scale. The platform processes over 2.5 million workflow executions daily with an average P99 latency of 180ms for API operations and 4.2 seconds for workflow execution. This document provides a comprehensive overview of the system architecture, including microservices design, data flow patterns, infrastructure decisions, and operational considerations. ## Table of Contents 1. [High-Level Architecture](#high-level-architecture) 2. [Microservices Breakdown](#microservices-breakdown) 3. [Data Flow Architecture](#data-flow-architecture) 4. [Database Architecture](#database-architecture) 5. [Message Queue Patterns](#message-queue-patterns) 6. [Caching Strategy](#caching-strategy) 7. [Security Architecture](#security-architecture) 8. [Performance Characteristics](#performance-characteristics) 9. [Disaster Recovery](#disaster-recovery) --- ## High-Level Architecture CloudFlow follows a microservices architecture pattern deployed across multiple availability zones in AWS.",
              "score": null
            },
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_1",
              "content": "### Architecture Summary CloudFlow consists of the following components: - **API Server**: REST API for workflow management (Node.js/Express) - **Worker Service**: Background job processor (Node.js) - **Scheduler**: Cron-based task scheduler (Node.js) - **PostgreSQL**: Primary data store (version 14) - **Redis**: Cache and message queue (version 7.0) - **PgBouncer**: Database connection pooler ### Deployment Model - **Namespace**: `cloudflow-prod` - **Cluster**: AWS EKS 1.28 - **Region**: us-east-1 (primary), us-west-2 (disaster recovery) - **High Availability**: Multi-AZ deployment across 3 availability zones --- ## Prerequisites Before beginning the deployment process, ensure you have the following: ### Required Tools - `kubectl` (v1.28 or later) - `helm` (v3.12 or later) - `aws-cli` (v2.13 or later) - `eksctl` (v0.165 or later) - `terraform` (v1.6 or later) - for infrastructure provisioning ### Access Requirements - AWS account with appropriate IAM permissions - EKS cluster admin access - Container registry access (ECR) - Domain name and SSL certificates -",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_25",
              "content": "limiting: 1,000 requests per minute per API key **Workflow Engine**: - Concurrent executions: 8,000 workflows (across 16 pods) - Execution start rate: 500 per second - Completion rate: 450 per second (average 2-second execution time) **Database**: - Read throughput: 50,000 queries per second (across replicas) - Write throughput: 15,000 transactions per second - Connection capacity: 2,000 concurrent connections **Kafka**: - Message ingestion: 100,000 messages per second - Consumer throughput: 80,000 messages per second (aggregated) - End-to-end latency: < 100ms (P99) ### Resource Utilization **CPU Utilization** (Target: 60-70% average): - API Gateway: 55% average, 80% peak - Workflow Engine: 65% average, 85% peak - Auth Service: 40% average, 60% peak **Memory Utilization** (Target: < 80%): - API Gateway: 2.5GB average per pod (4GB allocated) - Workflow Engine: 6GB average per pod (8GB allocated) - Notification Service: 2.8GB average per pod (4GB allocated) **Network Throughput**: - Ingress: 2 Gbps average, 5 Gbps",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_3",
              "content": "**Technology**: Node.js with Express.js framework **Replicas**: 12 pods (production), auto-scaling 8-20 based on CPU **Resource Allocation**: 2 vCPU, 4GB RAM per pod **Key Responsibilities**: - JWT token validation (delegated to Auth Service for initial validation) - Rate limiting: 1000 requests per minute per API key (sliding window) - Request/response transformation and validation using JSON Schema - Routing to downstream services based on URL path patterns - CORS handling for web clients - Request/response logging and correlation ID injection **Critical Endpoints**: - `POST /api/v1/workflows` - Create new workflow - `GET /api/v1/workflows/:id` - Retrieve workflow status - `POST /api/v1/workflows/:id/execute` - Trigger workflow execution - `GET /api/v1/workflows/:id/history` - Get execution history **Dependencies**: - Auth Service (for token validation) - Redis (for rate limiting counters) - All downstream microservices **Performance Targets**: - P50 latency: < 50ms - P99 latency: < 200ms - Throughput: 10,000 RPS sustained --- ### Auth Service **Purpose**: Centralized authentication and",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_2",
              "content": "\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 ``` ### Technology Stack - **Runtime**: Node.js 20.x (API Gateway, Workflow Engine), Go 1.21 (Auth Service, Scheduler) - **Container Orchestration**: Kubernetes 1.28 (EKS) - **Service Mesh**: Istio 1.20 for service-to-service communication - **Databases**: PostgreSQL 15.4, Redis 7.2 - **Message Broker**: Apache Kafka 3.6 - **Monitoring**: Prometheus, Grafana, Jaeger for distributed tracing - **Secrets Management**: HashiCorp Vault 1.15 --- ## Microservices Breakdown ### API Gateway **Purpose**: Single entry point for all client requests, providing authentication, rate limiting, request routing, and protocol translation.",
              "score": null
            }
          ],
          "latency_ms": 5.583075000686222,
          "query_id": "realistic_006",
          "dimension": "contextual",
          "query": "I'm provisioning infrastructure, what are the compute specs for API Gateway?"
        },
        {
          "key_facts": [
            "2 vCPU, 4GB RAM per pod"
          ],
          "found_facts": [],
          "missed_facts": [
            "2 vCPU, 4GB RAM per pod"
          ],
          "coverage": 0.0,
          "expected_docs": [
            "architecture_overview"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_25",
              "content": "limiting: 1,000 requests per minute per API key **Workflow Engine**: - Concurrent executions: 8,000 workflows (across 16 pods) - Execution start rate: 500 per second - Completion rate: 450 per second (average 2-second execution time) **Database**: - Read throughput: 50,000 queries per second (across replicas) - Write throughput: 15,000 transactions per second - Connection capacity: 2,000 concurrent connections **Kafka**: - Message ingestion: 100,000 messages per second - Consumer throughput: 80,000 messages per second (aggregated) - End-to-end latency: < 100ms (P99) ### Resource Utilization **CPU Utilization** (Target: 60-70% average): - API Gateway: 55% average, 80% peak - Workflow Engine: 65% average, 85% peak - Auth Service: 40% average, 60% peak **Memory Utilization** (Target: < 80%): - API Gateway: 2.5GB average per pod (4GB allocated) - Workflow Engine: 6GB average per pod (8GB allocated) - Notification Service: 2.8GB average per pod (4GB allocated) **Network Throughput**: - Ingress: 2 Gbps average, 5 Gbps",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_11",
              "content": "Client \u2192 Load Balancer (TLS termination) 2. Load Balancer \u2192 API Gateway (HTTP/2) 3. API Gateway \u2192 Auth Service (JWT validation via gRPC) 4. API Gateway \u2192 Target Service (HTTP/REST or gRPC) 5. Target Service \u2192 Database/Cache (data retrieval) 6. Response propagates back through the chain ``` **Timeout Configuration**: - Client \u2192 Load Balancer: 60 seconds - Load Balancer \u2192 API Gateway: 55 seconds - API Gateway \u2192 Services: 45 seconds - Service \u2192 Database: 10 seconds ### Asynchronous Event Flow Long-running operations and inter-service communication use event-driven patterns: ``` 1. Service publishes event \u2192 Kafka topic 2. Kafka persists event (replication factor: 3) 3. Consumer groups subscribe to topics 4. Consumers process events with at-least-once delivery 5. State updates written to PostgreSQL 6.",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_2",
              "content": "\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 ``` ### Technology Stack - **Runtime**: Node.js 20.x (API Gateway, Workflow Engine), Go 1.21 (Auth Service, Scheduler) - **Container Orchestration**: Kubernetes 1.28 (EKS) - **Service Mesh**: Istio 1.20 for service-to-service communication - **Databases**: PostgreSQL 15.4, Redis 7.2 - **Message Broker**: Apache Kafka 3.6 - **Monitoring**: Prometheus, Grafana, Jaeger for distributed tracing - **Secrets Management**: HashiCorp Vault 1.15 --- ## Microservices Breakdown ### API Gateway **Purpose**: Single entry point for all client requests, providing authentication, rate limiting, request routing, and protocol translation.",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_32",
              "content": "PostgreSQL, SendGrid, Twilio | No | ### Capacity Planning **Current Capacity** (Jan 2026): - Daily workflow executions: 2.5M - Active users: 150,000 - API requests per day: 50M - Database size: 1.2TB - Kafka throughput: 100K msg/sec peak **6-Month Projection** (Jul 2026): - Daily workflow executions: 4M (+60%) - Active users: 225,000 (+50%) - API requests per day: 80M (+60%) - Database size: 2TB (+67%) - Required scaling: +30% compute, +50% storage ### Contact Information - **Architecture Team**: architecture@cloudflow.internal - **On-Call Engineer**: PagerDuty escalation - **Security Team**: security@cloudflow.internal - **Documentation**: https://wiki.cloudflow.internal/architecture --- **Document Revision History**: - v2.3.1 (Jan 15, 2026): Updated performance metrics, added DR test results - v2.3.0 (Dec 1, 2025): Added multi-AZ deployment details - v2.2.0 (Oct 15, 2025): Security architecture overhaul - v2.1.0 (Sep 1, 2025): Initial Kafka migration documentation - v2.0.0 (Jul 1, 2025): Complete microservices rewrite",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_24",
              "content": "Operations** (P99 latency): - `GET /workflows/{id}`: < 50ms (cache hit), < 150ms (cache miss) - `POST /workflows`: < 200ms (includes validation and database write) - `POST /workflows/{id}/execute`: < 100ms (async, returns execution ID) - `GET /workflows/{id}/history`: < 300ms (paginated, 50 records per page) **Workflow Execution** (P99 latency): - Simple workflow (< 5 steps): < 2 seconds - Medium workflow (5-15 steps): < 5 seconds - Complex workflow (> 15 steps): < 15 seconds - Parallel execution: Linear scaling up to 10 concurrent branches **Database Operations** (P99 latency): - Simple SELECT: < 5ms - JOIN query: < 20ms - INSERT/UPDATE: < 10ms - Batch operations: < 100ms (batch size: 100 records) **Cache Operations** (P99 latency): - Redis GET: < 2ms - Redis SET: < 3ms - Redis DEL: < 2ms ### Throughput Capacity **API Gateway**: - Sustained: 10,000 requests per second - Peak: 25,000 requests per second (5-minute burst) - Rate",
              "score": null
            }
          ],
          "latency_ms": 5.644179000228178,
          "query_id": "realistic_006",
          "dimension": "negation",
          "query": "Why is 1GB RAM not enough for API Gateway?"
        },
        {
          "key_facts": [
            "/health",
            "/ready"
          ],
          "found_facts": [
            "/health",
            "/ready"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "deployment_guide"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_7",
              "content": "health endpoint curl http://localhost:8080/health # Expected: {\"status\":\"healthy\",\"timestamp\":\"2026-01-24T10:30:00Z\"} # Test readiness endpoint curl http://localhost:8080/ready # Expected: {\"status\":\"ready\",\"dependencies\":{\"database\":\"connected\",\"redis\":\"connected\"}} ``` --- ## Database Configuration ### PostgreSQL Setup CloudFlow uses PostgreSQL 14 as its primary data store. Deploy PostgreSQL using the Bitnami Helm chart: ```yaml # postgres-values.yaml global: postgresql: auth: username: cloudflow database: cloudflow existingSecret: postgres-credentials image: tag: \"14.10.0\" primary: resources: limits: cpu: 4000m memory: 8Gi requests: cpu: 2000m memory: 4Gi persistence: enabled: true size: 100Gi storageClass: gp3 extendedConfiguration: | max_connections = 100 shared_buffers = 2GB effective_cache_size = 6GB maintenance_work_mem = 512MB checkpoint_completion_target = 0.9 wal_buffers = 16MB default_statistics_target = 100 random_page_cost = 1.1 effective_io_concurrency = 200 work_mem = 10MB min_wal_size = 1GB max_wal_size = 4GB readReplicas: replicaCount: 2 resources: limits: cpu: 2000m memory: 4Gi requests: cpu: 1000m memory: 2Gi metrics: enabled: true serviceMonitor: enabled: true ``` Install PostgreSQL: ```bash # Create password secret kubectl create secret generic postgres-credentials \\ --namespace cloudflow-prod \\ --from-literal=postgres-password=\"$(openssl",
              "score": null
            },
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_6",
              "content": "**Endpoint:** `POST /workflows` **Request Body:** ```json { \"name\": \"Email Campaign Automation\", \"description\": \"Sends personalized emails based on user behavior\", \"trigger\": { \"type\": \"webhook\", \"url\": \"https://your-app.com/webhook\" }, \"steps\": [ { \"type\": \"fetch_data\", \"source\": \"users_table\", \"filters\": {\"active\": true} }, { \"type\": \"transform\", \"script\": \"data.map(user => ({ ...user, segment: calculateSegment(user) }))\" }, { \"type\": \"send_email\", \"template_id\": \"tpl_welcome_v2\" } ] } ``` **Example Request:** ```bash curl -X POST https://api.cloudflow.io/v2/workflows \\ -H \"X-API-Key: YOUR_API_KEY\" \\ -H \"Content-Type: application/json\" \\ -d '{ \"name\": \"Email Campaign Automation\", \"trigger\": {\"type\": \"webhook\"}, \"steps\": [{\"type\": \"fetch_data\", \"source\": \"users_table\"}] }' ``` **Example Response (201 Created):** ```json { \"id\": \"wf_9k3m7n2q\", \"name\": \"Email Campaign Automation\", \"status\": \"active\", \"created_at\": \"2026-01-24T10:15:00Z\", \"webhook_url\": \"https://api.cloudflow.io/v2/webhooks/wf_9k3m7n2q/trigger\" } ``` #### Get Workflow Details Retrieve detailed information about a specific workflow.",
              "score": null
            },
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_5",
              "content": "**Endpoint:** `GET /workflows` **Query Parameters:** - `status` (optional): Filter by status (`active`, `paused`, `archived`) - `created_after` (optional): ISO 8601 timestamp - `limit` (optional): Items per page (max 100) - `offset` (optional): Pagination offset **Example Request:** ```bash curl -H \"X-API-Key: YOUR_API_KEY\" \\ \"https://api.cloudflow.io/v2/workflows?status=active&limit=25\" ``` **Example Response (200 OK):** ```json { \"data\": [ { \"id\": \"wf_8x7k2m9p\", \"name\": \"Data Processing Pipeline\", \"description\": \"Processes customer data every hour\", \"status\": \"active\", \"trigger\": { \"type\": \"schedule\", \"cron\": \"0 * * * *\" }, \"steps\": 12, \"created_at\": \"2026-01-15T10:30:00Z\", \"updated_at\": \"2026-01-23T14:22:00Z\", \"last_run\": \"2026-01-24T09:00:00Z\" } ], \"pagination\": { \"total\": 1, \"limit\": 25, \"offset\": 0, \"has_more\": false } } ``` #### Create Workflow Create a new workflow with specified configuration.",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_1",
              "content": "### Quick Diagnostic Checklist Before diving into specific issues, perform these initial checks: - Verify service health: `cloudflow status --all` - Check API connectivity: `curl -I https://api.cloudflow.io/health` - Review recent deployments: `kubectl get deployments -n cloudflow --sort-by=.metadata.creationTimestamp` - Inspect platform metrics: `cloudflow metrics --last 1h` --- ## Authentication & Authorization Issues ### 401 Unauthorized Errors #### Symptoms - API requests return `401 Unauthorized` - Error message: `Authentication credentials were not provided or are invalid` - Frontend displays \"Session expired\" message #### Common Causes **1. Token Expiration** CloudFlow access tokens expire after 3600 seconds (1 hour) by default. Refresh tokens are valid for 30 days.",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_28",
              "content": "< 10 seconds (Sentinel-based failover) - Kafka: < 30 seconds (controller election) - Services: Kubernetes health checks with 10-second liveness probes ### Backup Strategy **Database Backups**: - Automated snapshots: Daily at 02:00 UTC - Retention: 30 days for daily, 90 days for monthly - Cross-region replication: Async replication to us-west-2 (15-minute lag) - Backup verification: Weekly automated restore test in staging environment **Backup Schedule**: ``` Daily: Full snapshot \u2192 S3 (encrypted) Hourly: WAL archives \u2192 S3 (point-in-time recovery) Weekly: Backup validation test Monthly: Long-term archive to Glacier ``` **Configuration Backups**: - Kubernetes manifests: Stored in Git (GitOps with ArgoCD) - Vault secrets: Automated snapshot every 6 hours - Kafka topic configurations: Exported daily to S3 ### Disaster Recovery Procedures **Scenario 1: Single AZ Failure** - Detection: Health checks fail for entire AZ (< 30 seconds) - Action: Traffic automatically routed to healthy AZs by ALB - Recovery time: < 5",
              "score": null
            }
          ],
          "latency_ms": 5.597811999905389,
          "query_id": "realistic_007",
          "dimension": "original",
          "query": "What are the health check endpoints?"
        },
        {
          "key_facts": [
            "/health",
            "/ready"
          ],
          "found_facts": [
            "/health",
            "/ready"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "deployment_guide"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_7",
              "content": "health endpoint curl http://localhost:8080/health # Expected: {\"status\":\"healthy\",\"timestamp\":\"2026-01-24T10:30:00Z\"} # Test readiness endpoint curl http://localhost:8080/ready # Expected: {\"status\":\"ready\",\"dependencies\":{\"database\":\"connected\",\"redis\":\"connected\"}} ``` --- ## Database Configuration ### PostgreSQL Setup CloudFlow uses PostgreSQL 14 as its primary data store. Deploy PostgreSQL using the Bitnami Helm chart: ```yaml # postgres-values.yaml global: postgresql: auth: username: cloudflow database: cloudflow existingSecret: postgres-credentials image: tag: \"14.10.0\" primary: resources: limits: cpu: 4000m memory: 8Gi requests: cpu: 2000m memory: 4Gi persistence: enabled: true size: 100Gi storageClass: gp3 extendedConfiguration: | max_connections = 100 shared_buffers = 2GB effective_cache_size = 6GB maintenance_work_mem = 512MB checkpoint_completion_target = 0.9 wal_buffers = 16MB default_statistics_target = 100 random_page_cost = 1.1 effective_io_concurrency = 200 work_mem = 10MB min_wal_size = 1GB max_wal_size = 4GB readReplicas: replicaCount: 2 resources: limits: cpu: 2000m memory: 4Gi requests: cpu: 1000m memory: 2Gi metrics: enabled: true serviceMonitor: enabled: true ``` Install PostgreSQL: ```bash # Create password secret kubectl create secret generic postgres-credentials \\ --namespace cloudflow-prod \\ --from-literal=postgres-password=\"$(openssl",
              "score": null
            },
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_5",
              "content": "**Endpoint:** `GET /workflows` **Query Parameters:** - `status` (optional): Filter by status (`active`, `paused`, `archived`) - `created_after` (optional): ISO 8601 timestamp - `limit` (optional): Items per page (max 100) - `offset` (optional): Pagination offset **Example Request:** ```bash curl -H \"X-API-Key: YOUR_API_KEY\" \\ \"https://api.cloudflow.io/v2/workflows?status=active&limit=25\" ``` **Example Response (200 OK):** ```json { \"data\": [ { \"id\": \"wf_8x7k2m9p\", \"name\": \"Data Processing Pipeline\", \"description\": \"Processes customer data every hour\", \"status\": \"active\", \"trigger\": { \"type\": \"schedule\", \"cron\": \"0 * * * *\" }, \"steps\": 12, \"created_at\": \"2026-01-15T10:30:00Z\", \"updated_at\": \"2026-01-23T14:22:00Z\", \"last_run\": \"2026-01-24T09:00:00Z\" } ], \"pagination\": { \"total\": 1, \"limit\": 25, \"offset\": 0, \"has_more\": false } } ``` #### Create Workflow Create a new workflow with specified configuration.",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_28",
              "content": "< 10 seconds (Sentinel-based failover) - Kafka: < 30 seconds (controller election) - Services: Kubernetes health checks with 10-second liveness probes ### Backup Strategy **Database Backups**: - Automated snapshots: Daily at 02:00 UTC - Retention: 30 days for daily, 90 days for monthly - Cross-region replication: Async replication to us-west-2 (15-minute lag) - Backup verification: Weekly automated restore test in staging environment **Backup Schedule**: ``` Daily: Full snapshot \u2192 S3 (encrypted) Hourly: WAL archives \u2192 S3 (point-in-time recovery) Weekly: Backup validation test Monthly: Long-term archive to Glacier ``` **Configuration Backups**: - Kubernetes manifests: Stored in Git (GitOps with ArgoCD) - Vault secrets: Automated snapshot every 6 hours - Kafka topic configurations: Exported daily to S3 ### Disaster Recovery Procedures **Scenario 1: Single AZ Failure** - Detection: Health checks fail for entire AZ (< 30 seconds) - Action: Traffic automatically routed to healthy AZs by ALB - Recovery time: < 5",
              "score": null
            },
            {
              "doc_id": "user_guide",
              "chunk_id": "user_guide_fix_23",
              "content": "\"website_form\" - id: notify_sales action: slack_message condition: \"{{steps.insert_contact.affected_rows > 0}}\" config: channel: \"#leads\" text: \"New contact form submission from {{trigger.body.email}}\" ``` ### Pattern 2: API Polling Periodically check an API and take action on new items: ```yaml name: \"Poll GitHub Issues\" schedule: cron: \"*/15 * * * *\" # Every 15 minutes timezone: \"UTC\" steps: - id: get_last_check action: database_query config: query: \"SELECT last_checked_at FROM workflow_state WHERE workflow_id = $1\" parameters: - \"{{workflow.id}}\" - id: fetch_issues action: http_request config: method: GET url: \"https://api.github.com/repos/company/project/issues\" params: since: \"{{steps.get_last_check.rows[0].last_checked_at}}\" state: \"open\" headers: Authorization: \"token {{secrets.GITHUB_TOKEN}}\" - id: process_new_issues action: javascript code: | const issues = input.body; return { count: issues.length, issues: issues.map(i => ({ number: i.number, title: i.title, url: i.html_url })) }; - id: notify_team action: slack_message condition: \"{{steps.process_new_issues.output.count > 0}}\" config: channel: \"#engineering\" text: \"{{steps.process_new_issues.output.count}} new GitHub issues\" - id: update_last_check action: database_query config: query: | UPDATE workflow_state SET last_checked_at = $1",
              "score": null
            },
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_9",
              "content": "\"limit\": 50, \"offset\": 0, \"has_more\": true } } ``` ### Analytics #### Get Workflow Metrics Retrieve performance metrics and analytics for workflows. **Endpoint:** `GET /analytics/workflows/{workflow_id}` **Query Parameters:** - `period` (required): Time period (`1h`, `24h`, `7d`, `30d`) - `metrics` (optional): Comma-separated list of metrics **Available Metrics:** - `execution_count`: Total number of executions - `success_rate`: Percentage of successful executions - `avg_duration`: Average execution duration in milliseconds - `error_count`: Total number of errors - `throughput`: Records processed per hour **Example Request:** ```bash curl -H \"X-API-Key: YOUR_API_KEY\" \\ \"https://api.cloudflow.io/v2/analytics/workflows/wf_8x7k2m9p?period=7d&metrics=execution_count,success_rate,avg_duration\" ``` **Example Response (200 OK):** ```json { \"workflow_id\": \"wf_8x7k2m9p\", \"period\": \"7d\", \"start_date\": \"2026-01-17T10:30:00Z\", \"end_date\": \"2026-01-24T10:30:00Z\", \"metrics\": { \"execution_count\": 168, \"success_rate\": 97.6, \"avg_duration\": 3285, \"error_count\": 4, \"throughput\": 2847 }, \"timeseries\": [ { \"timestamp\": \"2026-01-17T00:00:00Z\", \"execution_count\": 24, \"success_rate\": 100.0 }, { \"timestamp\": \"2026-01-18T00:00:00Z\", \"execution_count\": 24, \"success_rate\": 95.8 } ] } ``` ## Error Handling CloudFlow uses standard HTTP status codes and returns detailed error information in JSON",
              "score": null
            }
          ],
          "latency_ms": 5.517221999980393,
          "query_id": "realistic_007",
          "dimension": "synonym",
          "query": "Which URLs should I use for health monitoring?"
        },
        {
          "key_facts": [
            "/health",
            "/ready"
          ],
          "found_facts": [
            "/health",
            "/ready"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "deployment_guide"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_7",
              "content": "health endpoint curl http://localhost:8080/health # Expected: {\"status\":\"healthy\",\"timestamp\":\"2026-01-24T10:30:00Z\"} # Test readiness endpoint curl http://localhost:8080/ready # Expected: {\"status\":\"ready\",\"dependencies\":{\"database\":\"connected\",\"redis\":\"connected\"}} ``` --- ## Database Configuration ### PostgreSQL Setup CloudFlow uses PostgreSQL 14 as its primary data store. Deploy PostgreSQL using the Bitnami Helm chart: ```yaml # postgres-values.yaml global: postgresql: auth: username: cloudflow database: cloudflow existingSecret: postgres-credentials image: tag: \"14.10.0\" primary: resources: limits: cpu: 4000m memory: 8Gi requests: cpu: 2000m memory: 4Gi persistence: enabled: true size: 100Gi storageClass: gp3 extendedConfiguration: | max_connections = 100 shared_buffers = 2GB effective_cache_size = 6GB maintenance_work_mem = 512MB checkpoint_completion_target = 0.9 wal_buffers = 16MB default_statistics_target = 100 random_page_cost = 1.1 effective_io_concurrency = 200 work_mem = 10MB min_wal_size = 1GB max_wal_size = 4GB readReplicas: replicaCount: 2 resources: limits: cpu: 2000m memory: 4Gi requests: cpu: 1000m memory: 2Gi metrics: enabled: true serviceMonitor: enabled: true ``` Install PostgreSQL: ```bash # Create password secret kubectl create secret generic postgres-credentials \\ --namespace cloudflow-prod \\ --from-literal=postgres-password=\"$(openssl",
              "score": null
            },
            {
              "doc_id": "user_guide",
              "chunk_id": "user_guide_fix_16",
              "content": "fixing issues - Export failed executions for analysis - Set up alerts for DLQ threshold breaches **Accessing the DLQ:** 1. Navigate to **Workflows** > **[Your Workflow]** > **Dead Letter Queue** 2. Filter by error type, date range, or execution ID 3. Click an execution to view full details 4. Click **\"Retry\"** to reprocess with the same input data ### Error Notifications Get notified when workflows fail: ```yaml workflow: notifications: on_failure: - type: email to: \"ops-team@company.com\" - type: slack channel: \"#alerts\" message: \"Workflow {{workflow.name}} failed: {{error.message}}\" on_success_after_retry: - type: slack channel: \"#monitoring\" message: \"Workflow recovered after {{error.attempts}} attempts\" ``` ## Workflow Limits CloudFlow enforces the following limits to ensure platform stability and performance: ### Steps Per Workflow - **Maximum**: 50 steps per workflow - **Recommendation**: Keep workflows focused and modular. If you need more steps, consider splitting into multiple workflows connected via webhooks.",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_29",
              "content": "minutes (no manual intervention) - Data loss: None (multi-AZ replication) **Scenario 2: Database Primary Failure** - Detection: Health check fails for primary database (< 30 seconds) - Action: Automatic promotion of read replica to primary - Recovery time: 30-60 seconds - Data loss: Minimal (< 1 second due to synchronous replication) **Scenario 3: Full Region Failure** - Detection: Multiple health check failures across all AZs (< 2 minutes) - Action: Manual failover to DR region (us-west-2) - Procedure: 1. Update DNS to point to DR region (TTL: 60 seconds) 2. Promote DR database replica to primary 3. Scale up DR region services to production capacity 4. Verify data consistency and integrity 5.",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_1",
              "content": "### Quick Diagnostic Checklist Before diving into specific issues, perform these initial checks: - Verify service health: `cloudflow status --all` - Check API connectivity: `curl -I https://api.cloudflow.io/health` - Review recent deployments: `kubectl get deployments -n cloudflow --sort-by=.metadata.creationTimestamp` - Inspect platform metrics: `cloudflow metrics --last 1h` --- ## Authentication & Authorization Issues ### 401 Unauthorized Errors #### Symptoms - API requests return `401 Unauthorized` - Error message: `Authentication credentials were not provided or are invalid` - Frontend displays \"Session expired\" message #### Common Causes **1. Token Expiration** CloudFlow access tokens expire after 3600 seconds (1 hour) by default. Refresh tokens are valid for 30 days.",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_15",
              "content": "--step data_export \\ --retry-enabled false ``` #### Monitoring Retries ```bash # List failed executions with retry information cloudflow workflows executions list \\ --status FAILED \\ --show-retries \\ --last 7d # Get retry history for specific execution cloudflow workflows executions retries exec_7h3j6k9m2n # Output: # Execution: exec_7h3j6k9m2n # Attempt 1: FAILED - NetworkError: Connection refused (delay: 0ms) # Attempt 2: FAILED - NetworkError: Connection timeout (delay: 1000ms) # Attempt 3: FAILED - NetworkError: Connection timeout (delay: 2000ms) # Attempt 4: FAILED - NetworkError: Connection timeout (delay: 4000ms) # Final Status: FAILED_AFTER_RETRIES ``` ### Workflow Step Failures #### Common Error Patterns **1.",
              "score": null
            }
          ],
          "latency_ms": 5.462491000798764,
          "query_id": "realistic_007",
          "dimension": "problem",
          "query": "My load balancer health checks are failing"
        },
        {
          "key_facts": [
            "/health",
            "/ready"
          ],
          "found_facts": [
            "/health",
            "/ready"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "deployment_guide"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_7",
              "content": "health endpoint curl http://localhost:8080/health # Expected: {\"status\":\"healthy\",\"timestamp\":\"2026-01-24T10:30:00Z\"} # Test readiness endpoint curl http://localhost:8080/ready # Expected: {\"status\":\"ready\",\"dependencies\":{\"database\":\"connected\",\"redis\":\"connected\"}} ``` --- ## Database Configuration ### PostgreSQL Setup CloudFlow uses PostgreSQL 14 as its primary data store. Deploy PostgreSQL using the Bitnami Helm chart: ```yaml # postgres-values.yaml global: postgresql: auth: username: cloudflow database: cloudflow existingSecret: postgres-credentials image: tag: \"14.10.0\" primary: resources: limits: cpu: 4000m memory: 8Gi requests: cpu: 2000m memory: 4Gi persistence: enabled: true size: 100Gi storageClass: gp3 extendedConfiguration: | max_connections = 100 shared_buffers = 2GB effective_cache_size = 6GB maintenance_work_mem = 512MB checkpoint_completion_target = 0.9 wal_buffers = 16MB default_statistics_target = 100 random_page_cost = 1.1 effective_io_concurrency = 200 work_mem = 10MB min_wal_size = 1GB max_wal_size = 4GB readReplicas: replicaCount: 2 resources: limits: cpu: 2000m memory: 4Gi requests: cpu: 1000m memory: 2Gi metrics: enabled: true serviceMonitor: enabled: true ``` Install PostgreSQL: ```bash # Create password secret kubectl create secret generic postgres-credentials \\ --namespace cloudflow-prod \\ --from-literal=postgres-password=\"$(openssl",
              "score": null
            },
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_14",
              "content": "**Verify Service Health**: ```bash kubectl get pods -n cloudflow-prod curl https://api.cloudflow.io/health ``` **Recovery Time Objective (RTO)**: 4 hours **Recovery Point Objective (RPO)**: 24 hours --- ## Scaling and Performance ### Horizontal Pod Autoscaling CloudFlow is configured with Horizontal Pod Autoscaler (HPA) to automatically scale based on resource utilization: ```yaml # hpa.yaml apiVersion: autoscaling/v2 kind: HorizontalPodAutoscaler metadata: name: cloudflow-api-hpa namespace: cloudflow-prod spec: scaleTargetRef: apiVersion: apps/v1 kind: Deployment name: cloudflow-api minReplicas: 3 maxReplicas: 10 metrics: - type: Resource resource: name: cpu target: type: Utilization averageUtilization: 70 - type: Resource resource: name: memory target: type: Utilization averageUtilization: 80 behavior: scaleDown: stabilizationWindowSeconds: 300 policies: - type: Percent value: 50 periodSeconds: 60 scaleUp: stabilizationWindowSeconds: 60 policies: - type: Percent value: 100 periodSeconds: 30 - type: Pods value: 2 periodSeconds: 30 selectPolicy: Max ``` Monitor autoscaling events: ```bash kubectl get hpa -n cloudflow-prod -w kubectl describe hpa cloudflow-api-hpa -n cloudflow-prod ``` ### Performance Optimization #### Connection",
              "score": null
            },
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_16",
              "content": "}, ], thresholds: { http_req_duration: ['p(95)<500'], }, }; export default function () { let res = http.get('https://api.cloudflow.io/api/workflows'); check(res, { 'status is 200': (r) => r.status === 200 }); sleep(1); } EOF ``` **Performance Targets**: - P95 API latency: < 500ms - P99 API latency: < 1000ms - Error rate: < 0.1% - Throughput: > 1000 requests/second --- ## Security and Compliance ### Network Policies Implement network policies to restrict pod-to-pod communication: ```yaml # network-policy.yaml apiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: cloudflow-network-policy namespace: cloudflow-prod spec: podSelector: matchLabels: app: cloudflow policyTypes: - Ingress - Egress ingress: - from: - namespaceSelector: matchLabels: name: cloudflow-prod - podSelector: matchLabels: app: nginx-ingress ports: - protocol: TCP port: 3000 egress: - to: - podSelector: matchLabels: app: postgresql ports: - protocol: TCP port: 5432 - to: - podSelector: matchLabels: app: redis ports: - protocol: TCP port: 6379 ``` ### Pod Security Standards Enforce Pod Security Standards at the",
              "score": null
            },
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_6",
              "content": "**Endpoint:** `POST /workflows` **Request Body:** ```json { \"name\": \"Email Campaign Automation\", \"description\": \"Sends personalized emails based on user behavior\", \"trigger\": { \"type\": \"webhook\", \"url\": \"https://your-app.com/webhook\" }, \"steps\": [ { \"type\": \"fetch_data\", \"source\": \"users_table\", \"filters\": {\"active\": true} }, { \"type\": \"transform\", \"script\": \"data.map(user => ({ ...user, segment: calculateSegment(user) }))\" }, { \"type\": \"send_email\", \"template_id\": \"tpl_welcome_v2\" } ] } ``` **Example Request:** ```bash curl -X POST https://api.cloudflow.io/v2/workflows \\ -H \"X-API-Key: YOUR_API_KEY\" \\ -H \"Content-Type: application/json\" \\ -d '{ \"name\": \"Email Campaign Automation\", \"trigger\": {\"type\": \"webhook\"}, \"steps\": [{\"type\": \"fetch_data\", \"source\": \"users_table\"}] }' ``` **Example Response (201 Created):** ```json { \"id\": \"wf_9k3m7n2q\", \"name\": \"Email Campaign Automation\", \"status\": \"active\", \"created_at\": \"2026-01-24T10:15:00Z\", \"webhook_url\": \"https://api.cloudflow.io/v2/webhooks/wf_9k3m7n2q/trigger\" } ``` #### Get Workflow Details Retrieve detailed information about a specific workflow.",
              "score": null
            },
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_5",
              "content": "**Endpoint:** `GET /workflows` **Query Parameters:** - `status` (optional): Filter by status (`active`, `paused`, `archived`) - `created_after` (optional): ISO 8601 timestamp - `limit` (optional): Items per page (max 100) - `offset` (optional): Pagination offset **Example Request:** ```bash curl -H \"X-API-Key: YOUR_API_KEY\" \\ \"https://api.cloudflow.io/v2/workflows?status=active&limit=25\" ``` **Example Response (200 OK):** ```json { \"data\": [ { \"id\": \"wf_8x7k2m9p\", \"name\": \"Data Processing Pipeline\", \"description\": \"Processes customer data every hour\", \"status\": \"active\", \"trigger\": { \"type\": \"schedule\", \"cron\": \"0 * * * *\" }, \"steps\": 12, \"created_at\": \"2026-01-15T10:30:00Z\", \"updated_at\": \"2026-01-23T14:22:00Z\", \"last_run\": \"2026-01-24T09:00:00Z\" } ], \"pagination\": { \"total\": 1, \"limit\": 25, \"offset\": 0, \"has_more\": false } } ``` #### Create Workflow Create a new workflow with specified configuration.",
              "score": null
            }
          ],
          "latency_ms": 5.792726000436232,
          "query_id": "realistic_007",
          "dimension": "casual",
          "query": "health endpoints"
        },
        {
          "key_facts": [
            "/health",
            "/ready"
          ],
          "found_facts": [
            "/health",
            "/ready"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "deployment_guide"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_7",
              "content": "health endpoint curl http://localhost:8080/health # Expected: {\"status\":\"healthy\",\"timestamp\":\"2026-01-24T10:30:00Z\"} # Test readiness endpoint curl http://localhost:8080/ready # Expected: {\"status\":\"ready\",\"dependencies\":{\"database\":\"connected\",\"redis\":\"connected\"}} ``` --- ## Database Configuration ### PostgreSQL Setup CloudFlow uses PostgreSQL 14 as its primary data store. Deploy PostgreSQL using the Bitnami Helm chart: ```yaml # postgres-values.yaml global: postgresql: auth: username: cloudflow database: cloudflow existingSecret: postgres-credentials image: tag: \"14.10.0\" primary: resources: limits: cpu: 4000m memory: 8Gi requests: cpu: 2000m memory: 4Gi persistence: enabled: true size: 100Gi storageClass: gp3 extendedConfiguration: | max_connections = 100 shared_buffers = 2GB effective_cache_size = 6GB maintenance_work_mem = 512MB checkpoint_completion_target = 0.9 wal_buffers = 16MB default_statistics_target = 100 random_page_cost = 1.1 effective_io_concurrency = 200 work_mem = 10MB min_wal_size = 1GB max_wal_size = 4GB readReplicas: replicaCount: 2 resources: limits: cpu: 2000m memory: 4Gi requests: cpu: 1000m memory: 2Gi metrics: enabled: true serviceMonitor: enabled: true ``` Install PostgreSQL: ```bash # Create password secret kubectl create secret generic postgres-credentials \\ --namespace cloudflow-prod \\ --from-literal=postgres-password=\"$(openssl",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_1",
              "content": "### Quick Diagnostic Checklist Before diving into specific issues, perform these initial checks: - Verify service health: `cloudflow status --all` - Check API connectivity: `curl -I https://api.cloudflow.io/health` - Review recent deployments: `kubectl get deployments -n cloudflow --sort-by=.metadata.creationTimestamp` - Inspect platform metrics: `cloudflow metrics --last 1h` --- ## Authentication & Authorization Issues ### 401 Unauthorized Errors #### Symptoms - API requests return `401 Unauthorized` - Error message: `Authentication credentials were not provided or are invalid` - Frontend displays \"Session expired\" message #### Common Causes **1. Token Expiration** CloudFlow access tokens expire after 3600 seconds (1 hour) by default. Refresh tokens are valid for 30 days.",
              "score": null
            },
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_14",
              "content": "**Verify Service Health**: ```bash kubectl get pods -n cloudflow-prod curl https://api.cloudflow.io/health ``` **Recovery Time Objective (RTO)**: 4 hours **Recovery Point Objective (RPO)**: 24 hours --- ## Scaling and Performance ### Horizontal Pod Autoscaling CloudFlow is configured with Horizontal Pod Autoscaler (HPA) to automatically scale based on resource utilization: ```yaml # hpa.yaml apiVersion: autoscaling/v2 kind: HorizontalPodAutoscaler metadata: name: cloudflow-api-hpa namespace: cloudflow-prod spec: scaleTargetRef: apiVersion: apps/v1 kind: Deployment name: cloudflow-api minReplicas: 3 maxReplicas: 10 metrics: - type: Resource resource: name: cpu target: type: Utilization averageUtilization: 70 - type: Resource resource: name: memory target: type: Utilization averageUtilization: 80 behavior: scaleDown: stabilizationWindowSeconds: 300 policies: - type: Percent value: 50 periodSeconds: 60 scaleUp: stabilizationWindowSeconds: 60 policies: - type: Percent value: 100 periodSeconds: 30 - type: Pods value: 2 periodSeconds: 30 selectPolicy: Max ``` Monitor autoscaling events: ```bash kubectl get hpa -n cloudflow-prod -w kubectl describe hpa cloudflow-api-hpa -n cloudflow-prod ``` ### Performance Optimization #### Connection",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_26",
              "content": "peak - Egress: 1.5 Gbps average, 4 Gbps peak - Internal (service mesh): 8 Gbps average ### Monitoring & Alerting **Key Metrics** (Monitored via Prometheus): - Request rate (per service, per endpoint) - Error rate (4xx, 5xx responses) - Latency percentiles (P50, P95, P99) - Resource utilization (CPU, memory, disk) - Cache hit/miss ratios - Database connection pool saturation - Kafka consumer lag **Alerts** (via PagerDuty): - P1 (Immediate): API error rate > 1%, database connection failure - P2 (< 15 min): P99 latency > 500ms, cache hit rate < 80% - P3 (< 1 hour): Resource utilization > 85%, replica count < minimum **SLI/SLO/SLA**: - SLI: API success rate (non-5xx responses) - SLO: 99.9% uptime per month (43 minutes downtime allowance) - SLA: 99.5% uptime guarantee (customer-facing SLA with credits) --- ## Disaster Recovery ### Recovery Objectives **RPO (Recovery Point Objective): 1 hour** - Maximum acceptable data loss: 1",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_27",
              "content": "changes cloudflow audit log --last 2h --event-type \"deployment,configuration\" # Check metrics dashboard cloudflow metrics dashboard --incident-mode ``` #### Step 2: Gather Information (5-15 minutes) Create incident document with: - Incident timestamp and duration - Affected services and endpoints - Error rates and user impact - Recent changes or deployments - Relevant log excerpts - Correlation IDs for failed requests ```bash # Generate incident report cloudflow debug incident-report \\ --start \"2026-01-24T10:30:00Z\" \\ --end \"2026-01-24T11:00:00Z\" \\ --output incident-report.md # Capture system snapshot cloudflow debug snapshot --output snapshot-$(date +%Y%m%d-%H%M%S).tar.gz ``` #### Step 3: Escalate Based on Severity **For SEV-1 (Critical):** ```bash # Page on-call engineer cloudflow incident create \\ --severity SEV-1 \\ --title \"Complete API outage\" \\ --description \"All API requests returning 503\" \\ --page-oncall # Notify status page cloudflow status update \\ --status \"major_outage\" \\ --message \"We are investigating a complete service outage\" # Create war room cloudflow incident war-room create incident-2024012401",
              "score": null
            }
          ],
          "latency_ms": 6.038493000232847,
          "query_id": "realistic_007",
          "dimension": "contextual",
          "query": "Setting up monitoring, what endpoints indicate service health?"
        },
        {
          "key_facts": [
            "/health",
            "/ready"
          ],
          "found_facts": [
            "/health",
            "/ready"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "deployment_guide"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_7",
              "content": "health endpoint curl http://localhost:8080/health # Expected: {\"status\":\"healthy\",\"timestamp\":\"2026-01-24T10:30:00Z\"} # Test readiness endpoint curl http://localhost:8080/ready # Expected: {\"status\":\"ready\",\"dependencies\":{\"database\":\"connected\",\"redis\":\"connected\"}} ``` --- ## Database Configuration ### PostgreSQL Setup CloudFlow uses PostgreSQL 14 as its primary data store. Deploy PostgreSQL using the Bitnami Helm chart: ```yaml # postgres-values.yaml global: postgresql: auth: username: cloudflow database: cloudflow existingSecret: postgres-credentials image: tag: \"14.10.0\" primary: resources: limits: cpu: 4000m memory: 8Gi requests: cpu: 2000m memory: 4Gi persistence: enabled: true size: 100Gi storageClass: gp3 extendedConfiguration: | max_connections = 100 shared_buffers = 2GB effective_cache_size = 6GB maintenance_work_mem = 512MB checkpoint_completion_target = 0.9 wal_buffers = 16MB default_statistics_target = 100 random_page_cost = 1.1 effective_io_concurrency = 200 work_mem = 10MB min_wal_size = 1GB max_wal_size = 4GB readReplicas: replicaCount: 2 resources: limits: cpu: 2000m memory: 4Gi requests: cpu: 1000m memory: 2Gi metrics: enabled: true serviceMonitor: enabled: true ``` Install PostgreSQL: ```bash # Create password secret kubectl create secret generic postgres-credentials \\ --namespace cloudflow-prod \\ --from-literal=postgres-password=\"$(openssl",
              "score": null
            },
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_5",
              "content": "**Endpoint:** `GET /workflows` **Query Parameters:** - `status` (optional): Filter by status (`active`, `paused`, `archived`) - `created_after` (optional): ISO 8601 timestamp - `limit` (optional): Items per page (max 100) - `offset` (optional): Pagination offset **Example Request:** ```bash curl -H \"X-API-Key: YOUR_API_KEY\" \\ \"https://api.cloudflow.io/v2/workflows?status=active&limit=25\" ``` **Example Response (200 OK):** ```json { \"data\": [ { \"id\": \"wf_8x7k2m9p\", \"name\": \"Data Processing Pipeline\", \"description\": \"Processes customer data every hour\", \"status\": \"active\", \"trigger\": { \"type\": \"schedule\", \"cron\": \"0 * * * *\" }, \"steps\": 12, \"created_at\": \"2026-01-15T10:30:00Z\", \"updated_at\": \"2026-01-23T14:22:00Z\", \"last_run\": \"2026-01-24T09:00:00Z\" } ], \"pagination\": { \"total\": 1, \"limit\": 25, \"offset\": 0, \"has_more\": false } } ``` #### Create Workflow Create a new workflow with specified configuration.",
              "score": null
            },
            {
              "doc_id": "user_guide",
              "chunk_id": "user_guide_fix_23",
              "content": "\"website_form\" - id: notify_sales action: slack_message condition: \"{{steps.insert_contact.affected_rows > 0}}\" config: channel: \"#leads\" text: \"New contact form submission from {{trigger.body.email}}\" ``` ### Pattern 2: API Polling Periodically check an API and take action on new items: ```yaml name: \"Poll GitHub Issues\" schedule: cron: \"*/15 * * * *\" # Every 15 minutes timezone: \"UTC\" steps: - id: get_last_check action: database_query config: query: \"SELECT last_checked_at FROM workflow_state WHERE workflow_id = $1\" parameters: - \"{{workflow.id}}\" - id: fetch_issues action: http_request config: method: GET url: \"https://api.github.com/repos/company/project/issues\" params: since: \"{{steps.get_last_check.rows[0].last_checked_at}}\" state: \"open\" headers: Authorization: \"token {{secrets.GITHUB_TOKEN}}\" - id: process_new_issues action: javascript code: | const issues = input.body; return { count: issues.length, issues: issues.map(i => ({ number: i.number, title: i.title, url: i.html_url })) }; - id: notify_team action: slack_message condition: \"{{steps.process_new_issues.output.count > 0}}\" config: channel: \"#engineering\" text: \"{{steps.process_new_issues.output.count}} new GitHub issues\" - id: update_last_check action: database_query config: query: | UPDATE workflow_state SET last_checked_at = $1",
              "score": null
            },
            {
              "doc_id": "user_guide",
              "chunk_id": "user_guide_fix_10",
              "content": "Status: {{if(steps.check_inventory.in_stock, \"\u2713 Available\", \"\u2717 Out of Stock\")}} ``` **Date calculations:** ``` Due Date: {{format_date(add_days(now(), 7), \"MMMM DD, YYYY\")}} ``` **Dynamic URLs:** ``` https://api.example.com/v1/users/{{user_id}}/orders?status={{status}}&limit={{default(limit, 10)}} ``` ## Scheduling CloudFlow supports powerful scheduling options for recurring workflows. ### Cron Syntax Use standard cron expressions to define schedules: ``` * * * * * \u252c \u252c \u252c \u252c \u252c \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2514\u2500\u2500\u2500 Day of Week (0-6, Sunday=0) \u2502 \u2502 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Month (1-12) \u2502 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Day of Month (1-31) \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Hour (0-23) \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Minute (0-59) ``` **Common Cron Patterns:** | Pattern | Description | |---------|-------------| | `*/5 * * * *` | Every 5 minutes | | `0 * * * *` | Every hour at minute 0 | | `0 9 * * *` | Daily at 9:00 AM | | `0 9 * * 1` | Every Monday at 9:00 AM",
              "score": null
            },
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_7",
              "content": "**Endpoint:** `GET /workflows/{workflow_id}` **Example Request:** ```bash curl -H \"X-API-Key: YOUR_API_KEY\" \\ https://api.cloudflow.io/v2/workflows/wf_8x7k2m9p ``` **Example Response (200 OK):** ```json { \"id\": \"wf_8x7k2m9p\", \"name\": \"Data Processing Pipeline\", \"description\": \"Processes customer data every hour\", \"status\": \"active\", \"trigger\": { \"type\": \"schedule\", \"cron\": \"0 * * * *\", \"timezone\": \"UTC\" }, \"steps\": [ { \"id\": \"step_1\", \"type\": \"fetch_data\", \"source\": \"customer_db\", \"query\": \"SELECT * FROM customers WHERE updated_at > :last_run\" }, { \"id\": \"step_2\", \"type\": \"transform\", \"script_id\": \"scr_transform_v3\" } ], \"metrics\": { \"total_runs\": 1247, \"success_rate\": 98.7, \"avg_duration_ms\": 3420, \"last_error\": null }, \"created_at\": \"2026-01-15T10:30:00Z\", \"updated_at\": \"2026-01-23T14:22:00Z\" } ``` #### Update Workflow Modify an existing workflow's configuration. **Endpoint:** `PATCH /workflows/{workflow_id}` **Example Request:** ```bash curl -X PATCH https://api.cloudflow.io/v2/workflows/wf_8x7k2m9p \\ -H \"X-API-Key: YOUR_API_KEY\" \\ -H \"Content-Type: application/json\" \\ -d '{\"status\": \"paused\"}' ``` **Example Response (200 OK):** ```json { \"id\": \"wf_8x7k2m9p\", \"status\": \"paused\", \"updated_at\": \"2026-01-24T10:30:00Z\" } ``` #### Delete Workflow Permanently delete a workflow.",
              "score": null
            }
          ],
          "latency_ms": 5.510018998393207,
          "query_id": "realistic_007",
          "dimension": "negation",
          "query": "Why doesn't /status return health information?"
        },
        {
          "key_facts": [
            "minReplicas: 3",
            "maxReplicas: 10",
            "targetCPUUtilizationPercentage: 70"
          ],
          "found_facts": [
            "minReplicas: 3",
            "maxReplicas: 10"
          ],
          "missed_facts": [
            "targetCPUUtilizationPercentage: 70"
          ],
          "coverage": 0.6666666666666666,
          "expected_docs": [
            "deployment_guide"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_14",
              "content": "**Verify Service Health**: ```bash kubectl get pods -n cloudflow-prod curl https://api.cloudflow.io/health ``` **Recovery Time Objective (RTO)**: 4 hours **Recovery Point Objective (RPO)**: 24 hours --- ## Scaling and Performance ### Horizontal Pod Autoscaling CloudFlow is configured with Horizontal Pod Autoscaler (HPA) to automatically scale based on resource utilization: ```yaml # hpa.yaml apiVersion: autoscaling/v2 kind: HorizontalPodAutoscaler metadata: name: cloudflow-api-hpa namespace: cloudflow-prod spec: scaleTargetRef: apiVersion: apps/v1 kind: Deployment name: cloudflow-api minReplicas: 3 maxReplicas: 10 metrics: - type: Resource resource: name: cpu target: type: Utilization averageUtilization: 70 - type: Resource resource: name: memory target: type: Utilization averageUtilization: 80 behavior: scaleDown: stabilizationWindowSeconds: 300 policies: - type: Percent value: 50 periodSeconds: 60 scaleUp: stabilizationWindowSeconds: 60 policies: - type: Percent value: 100 periodSeconds: 30 - type: Pods value: 2 periodSeconds: 30 selectPolicy: Max ``` Monitor autoscaling events: ```bash kubectl get hpa -n cloudflow-prod -w kubectl describe hpa cloudflow-api-hpa -n cloudflow-prod ``` ### Performance Optimization #### Connection",
              "score": null
            },
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_10",
              "content": "The metrics include: - **HTTP Metrics**: Request rate, latency, error rate - **Database Metrics**: Connection pool status, query duration - **Worker Metrics**: Job queue size, processing time, failure rate - **System Metrics**: CPU usage, memory usage, heap statistics Example metrics exposed: ``` # HELP cloudflow_http_requests_total Total number of HTTP requests # TYPE cloudflow_http_requests_total counter cloudflow_http_requests_total{method=\"GET\",route=\"/api/workflows\",status=\"200\"} 15420 # HELP cloudflow_http_request_duration_seconds HTTP request duration in seconds # TYPE cloudflow_http_request_duration_seconds histogram cloudflow_http_request_duration_seconds_bucket{method=\"POST\",route=\"/api/workflows\",le=\"0.1\"} 8234 # HELP cloudflow_worker_jobs_processed_total Total number of jobs processed # TYPE cloudflow_worker_jobs_processed_total counter cloudflow_worker_jobs_processed_total{status=\"success\"} 45230 # HELP cloudflow_database_connections Current database connections # TYPE cloudflow_database_connections gauge cloudflow_database_connections{state=\"active\"} 23 ``` ### Grafana Dashboards Access Grafana to view CloudFlow dashboards: ```bash kubectl port-forward -n monitoring svc/prometheus-grafana 3000:80 ``` Import the CloudFlow dashboard (ID: 15847) or use the provided JSON template. Key dashboard panels include: 1. **API Performance**: Request rate, P95/P99 latency, error rate 2. **Resource Usage**: CPU, memory, disk I/O per pod 3.",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_14",
              "content": "< 20ms - Write query P99: < 50ms - Transaction throughput: 15,000 TPS - Replication lag: < 500ms --- ### Redis Caching Layer **Cluster Configuration**: - Redis Cluster with 6 nodes (3 primary + 3 replica) - Instance type: cache.r6g.xlarge (4 vCPU, 26GB RAM) - Total memory: 78GB usable cache space - Persistence: RDB snapshots every 5 minutes + AOF - Eviction policy: allkeys-lru **Cache Usage Patterns**: 1. **Session Storage**: - Key pattern: `session:{user_id}` - TTL: 15 minutes (aligned with JWT expiry) - Data: User session state, preferences - Invalidation: On logout or password change 2. **Rate Limiting Counters**: - Key pattern: `ratelimit:{api_key}:{window}` - TTL: 60 seconds (sliding window) - Data: Request count per window - Algorithm: Token bucket with Redis INCR 3. **Compiled Templates**: - Key pattern: `template:{template_id}:{version}` - TTL: 1 hour - Data: Compiled Handlebars template - Invalidation: On template update 4.",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_25",
              "content": "limiting: 1,000 requests per minute per API key **Workflow Engine**: - Concurrent executions: 8,000 workflows (across 16 pods) - Execution start rate: 500 per second - Completion rate: 450 per second (average 2-second execution time) **Database**: - Read throughput: 50,000 queries per second (across replicas) - Write throughput: 15,000 transactions per second - Connection capacity: 2,000 concurrent connections **Kafka**: - Message ingestion: 100,000 messages per second - Consumer throughput: 80,000 messages per second (aggregated) - End-to-end latency: < 100ms (P99) ### Resource Utilization **CPU Utilization** (Target: 60-70% average): - API Gateway: 55% average, 80% peak - Workflow Engine: 65% average, 85% peak - Auth Service: 40% average, 60% peak **Memory Utilization** (Target: < 80%): - API Gateway: 2.5GB average per pod (4GB allocated) - Workflow Engine: 6GB average per pod (8GB allocated) - Notification Service: 2.8GB average per pod (4GB allocated) **Network Throughput**: - Ingress: 2 Gbps average, 5 Gbps",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_32",
              "content": "PostgreSQL, SendGrid, Twilio | No | ### Capacity Planning **Current Capacity** (Jan 2026): - Daily workflow executions: 2.5M - Active users: 150,000 - API requests per day: 50M - Database size: 1.2TB - Kafka throughput: 100K msg/sec peak **6-Month Projection** (Jul 2026): - Daily workflow executions: 4M (+60%) - Active users: 225,000 (+50%) - API requests per day: 80M (+60%) - Database size: 2TB (+67%) - Required scaling: +30% compute, +50% storage ### Contact Information - **Architecture Team**: architecture@cloudflow.internal - **On-Call Engineer**: PagerDuty escalation - **Security Team**: security@cloudflow.internal - **Documentation**: https://wiki.cloudflow.internal/architecture --- **Document Revision History**: - v2.3.1 (Jan 15, 2026): Updated performance metrics, added DR test results - v2.3.0 (Dec 1, 2025): Added multi-AZ deployment details - v2.2.0 (Oct 15, 2025): Security architecture overhaul - v2.1.0 (Sep 1, 2025): Initial Kafka migration documentation - v2.0.0 (Jul 1, 2025): Complete microservices rewrite",
              "score": null
            }
          ],
          "latency_ms": 5.623600000035367,
          "query_id": "realistic_008",
          "dimension": "original",
          "query": "What are the HPA scaling parameters?"
        },
        {
          "key_facts": [
            "minReplicas: 3",
            "maxReplicas: 10",
            "targetCPUUtilizationPercentage: 70"
          ],
          "found_facts": [
            "minReplicas: 3",
            "maxReplicas: 10"
          ],
          "missed_facts": [
            "targetCPUUtilizationPercentage: 70"
          ],
          "coverage": 0.6666666666666666,
          "expected_docs": [
            "deployment_guide"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_14",
              "content": "**Verify Service Health**: ```bash kubectl get pods -n cloudflow-prod curl https://api.cloudflow.io/health ``` **Recovery Time Objective (RTO)**: 4 hours **Recovery Point Objective (RPO)**: 24 hours --- ## Scaling and Performance ### Horizontal Pod Autoscaling CloudFlow is configured with Horizontal Pod Autoscaler (HPA) to automatically scale based on resource utilization: ```yaml # hpa.yaml apiVersion: autoscaling/v2 kind: HorizontalPodAutoscaler metadata: name: cloudflow-api-hpa namespace: cloudflow-prod spec: scaleTargetRef: apiVersion: apps/v1 kind: Deployment name: cloudflow-api minReplicas: 3 maxReplicas: 10 metrics: - type: Resource resource: name: cpu target: type: Utilization averageUtilization: 70 - type: Resource resource: name: memory target: type: Utilization averageUtilization: 80 behavior: scaleDown: stabilizationWindowSeconds: 300 policies: - type: Percent value: 50 periodSeconds: 60 scaleUp: stabilizationWindowSeconds: 60 policies: - type: Percent value: 100 periodSeconds: 30 - type: Pods value: 2 periodSeconds: 30 selectPolicy: Max ``` Monitor autoscaling events: ```bash kubectl get hpa -n cloudflow-prod -w kubectl describe hpa cloudflow-api-hpa -n cloudflow-prod ``` ### Performance Optimization #### Connection",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_1",
              "content": "The platform is designed with the following core principles: - **Scalability**: Horizontal scaling for all services with auto-scaling groups - **Resilience**: Circuit breakers, bulkheads, and graceful degradation - **Observability**: Distributed tracing, centralized logging, and comprehensive metrics - **Security**: Zero-trust network architecture with mTLS encryption ### Architecture Diagram (Conceptual) ``` \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 Load Balancer (ALB) \u2502 \u2502 (TLS Termination - 443) \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u25bc \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 API Gateway Layer \u2502 \u2502 (Rate Limiting, Auth, Request Routing) \u2502 \u2514\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u2502 \u2502 \u2502 \u2502 \u25bc \u25bc \u25bc \u25bc \u25bc \u250c\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502Auth\u2502 \u2502Workflow\u2502 \u2502Scheduler\u2502 \u2502Notification\u2502 \u2502User Service\u2502 \u2502Svc \u2502 \u2502 Engine \u2502 \u2502 Service \u2502 \u2502 Service \u2502 \u2502 \u2502 \u2514\u2500\u252c\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u2502 \u2502 \u2502 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 \u2502 \u2502 \u25bc \u25bc \u25bc \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502PostgreSQL\u2502 \u2502 Redis \u2502 \u2502 Kafka \u2502 \u2502 Cluster \u2502 \u2502 Cluster \u2502 \u2502 Cluster \u2502",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_6",
              "content": "**Technology**: Node.js with TypeScript, Bull queue library **Replicas**: 16 pods (production), auto-scaling 12-24 **Resource Allocation**: 4 vCPU, 8GB RAM per pod **Key Responsibilities**: - Parse and validate workflow definitions (JSON-based DSL) - Execute workflow steps with state machine pattern - Handle retries with exponential backoff (max 3 retries, backoff: 2^n seconds) - Coordinate parallel and sequential task execution - Manage workflow state persistence and recovery - Support for conditional branching and loops - Sub-workflow invocation and composition **Workflow Execution Model**: ``` Workflow Submitted \u2192 Validation \u2192 Queue in Kafka \u2502 \u25bc Workflow Engine picks up \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u25bc \u25bc \u25bc [Task 1 Exec] [Task 2 Exec] [Task 3 Exec] \u2502 \u2502 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u25bc State Update \u2192 PostgreSQL \u2502 \u25bc Publish Event \u2192 Kafka ``` **Workflow DSL Example**: ```json { \"workflow_id\": \"customer-onboarding-v2\", \"version\": \"2.1.0\", \"steps\": [ { \"id\": \"validate-customer\", \"type\": \"http_request\", \"config\": { \"url\": \"https://validation.internal/verify\", \"method\": \"POST\", \"timeout\": 5000,",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_8",
              "content": "**Technology**: Go with distributed locking via Redis **Replicas**: 4 pods (production), active-passive with leader election **Resource Allocation**: 2 vCPU, 4GB RAM per pod **Key Responsibilities**: - Parse and validate cron expressions (extended format supporting seconds) - Maintain schedule registry in PostgreSQL - Distributed scheduling with leader election (one active scheduler) - Missed execution handling with configurable catch-up policy - Schedule conflict detection and resolution - Time zone support for international schedules **Scheduling Architecture**: ``` PostgreSQL Schedules Table \u2502 \u25bc Leader Scheduler (elected via Redis) \u2502 \u251c\u2500 Scan for due schedules (every 10 seconds) \u251c\u2500 Acquire lock per schedule (prevents duplicates) \u251c\u2500 Publish to Kafka \u2192 workflow.events topic \u2514\u2500 Update last_run timestamp ``` **Schedule Types**: - **Cron-based**: `0 */5 * * * *` (every 5 minutes) - **One-time**: Specific timestamp for delayed execution - **Interval-based**: Every N seconds/minutes/hours **Reliability Features**: - Leader election using Redis with 30-second lease - Heartbeat mechanism",
              "score": null
            },
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_6",
              "content": "prometheus.io/scrape: \"true\" prometheus.io/port: \"9090\" prometheus.io/path: \"/metrics\" podSecurityContext: runAsNonRoot: true runAsUser: 1000 fsGroup: 1000 securityContext: allowPrivilegeEscalation: false capabilities: drop: - ALL readOnlyRootFilesystem: true persistence: enabled: true storageClass: gp3 accessMode: ReadWriteOnce size: 50Gi redis: enabled: true architecture: replication auth: enabled: true master: persistence: size: 20Gi replica: replicaCount: 2 persistence: size: 20Gi ``` Deploy CloudFlow using Helm: ```bash helm install cloudflow cloudflow/cloudflow \\ --namespace cloudflow-prod \\ --values values-production.yaml \\ --wait \\ --timeout 10m ``` ### Deployment Verification Verify the deployment status: ```bash # Check pod status kubectl get pods -n cloudflow-prod # Expected output: # NAME READY STATUS RESTARTS AGE # cloudflow-api-7d8f9c5b6d-4xk2p 1/1 Running 0 2m # cloudflow-api-7d8f9c5b6d-9hj5m 1/1 Running 0 2m # cloudflow-api-7d8f9c5b6d-tn8wq 1/1 Running 0 2m # Check service endpoints kubectl get svc -n cloudflow-prod # Check ingress kubectl get ingress -n cloudflow-prod ``` Test health endpoints: ```bash # Port forward for testing kubectl port-forward -n cloudflow-prod svc/cloudflow-api 8080:80 # Test",
              "score": null
            }
          ],
          "latency_ms": 5.378484000175376,
          "query_id": "realistic_008",
          "dimension": "synonym",
          "query": "How does horizontal pod autoscaling work?"
        },
        {
          "key_facts": [
            "minReplicas: 3",
            "maxReplicas: 10",
            "targetCPUUtilizationPercentage: 70"
          ],
          "found_facts": [
            "minReplicas: 3",
            "maxReplicas: 10"
          ],
          "missed_facts": [
            "targetCPUUtilizationPercentage: 70"
          ],
          "coverage": 0.6666666666666666,
          "expected_docs": [
            "deployment_guide"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_16",
              "content": "}, ], thresholds: { http_req_duration: ['p(95)<500'], }, }; export default function () { let res = http.get('https://api.cloudflow.io/api/workflows'); check(res, { 'status is 200': (r) => r.status === 200 }); sleep(1); } EOF ``` **Performance Targets**: - P95 API latency: < 500ms - P99 API latency: < 1000ms - Error rate: < 0.1% - Throughput: > 1000 requests/second --- ## Security and Compliance ### Network Policies Implement network policies to restrict pod-to-pod communication: ```yaml # network-policy.yaml apiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: cloudflow-network-policy namespace: cloudflow-prod spec: podSelector: matchLabels: app: cloudflow policyTypes: - Ingress - Egress ingress: - from: - namespaceSelector: matchLabels: name: cloudflow-prod - podSelector: matchLabels: app: nginx-ingress ports: - protocol: TCP port: 3000 egress: - to: - podSelector: matchLabels: app: postgresql ports: - protocol: TCP port: 5432 - to: - podSelector: matchLabels: app: redis ports: - protocol: TCP port: 6379 ``` ### Pod Security Standards Enforce Pod Security Standards at the",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_9",
              "content": "to detect leader failure (5-second interval) - Automatic failover to standby scheduler (< 10 seconds) - Schedule versioning to handle updates during execution - Missed execution policy: SKIP, RUN_ONCE, or RUN_ALL **Performance Targets**: - Schedule evaluation: < 100ms per cycle - Accuracy: \u00b1 1 second for schedule triggers - Capacity: 100,000 active schedules --- ### Notification Service **Purpose**: Multi-channel notification delivery system supporting email, SMS, webhooks, and in-app notifications. **Technology**: Node.js with worker pool pattern **Replicas**: 8 pods (production), auto-scaling 6-16 **Resource Allocation**: 2 vCPU, 4GB RAM per pod **Key Responsibilities**: - Consume notification events from Kafka - Template rendering with Handlebars (cached in Redis) - Multi-channel delivery (Email via SendGrid, SMS via Twilio, Webhooks) - Retry logic with dead letter queue for failed deliveries - Delivery status tracking and analytics - User preference management (opt-in/opt-out) **Notification Flow**: ``` Event Source \u2192 Kafka (notifications.email topic) \u2502 \u25bc Notification Service Consumer",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_25",
              "content": "limiting: 1,000 requests per minute per API key **Workflow Engine**: - Concurrent executions: 8,000 workflows (across 16 pods) - Execution start rate: 500 per second - Completion rate: 450 per second (average 2-second execution time) **Database**: - Read throughput: 50,000 queries per second (across replicas) - Write throughput: 15,000 transactions per second - Connection capacity: 2,000 concurrent connections **Kafka**: - Message ingestion: 100,000 messages per second - Consumer throughput: 80,000 messages per second (aggregated) - End-to-end latency: < 100ms (P99) ### Resource Utilization **CPU Utilization** (Target: 60-70% average): - API Gateway: 55% average, 80% peak - Workflow Engine: 65% average, 85% peak - Auth Service: 40% average, 60% peak **Memory Utilization** (Target: < 80%): - API Gateway: 2.5GB average per pod (4GB allocated) - Workflow Engine: 6GB average per pod (8GB allocated) - Notification Service: 2.8GB average per pod (4GB allocated) **Network Throughput**: - Ingress: 2 Gbps average, 5 Gbps",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_7",
              "content": "# - Business Logic: 32ms (13%) # - Response Serialization: 12ms (5%) ``` **Network latency issues:** ```bash # Test connectivity to CloudFlow API time curl -w \"@curl-format.txt\" -o /dev/null -s https://api.cloudflow.io/health # Create curl-format.txt: cat > curl-format.txt << EOF time_namelookup: %{time_namelookup}s\\n time_connect: %{time_connect}s\\n time_appconnect: %{time_appconnect}s\\n time_pretransfer: %{time_pretransfer}s\\n time_redirect: %{time_redirect}s\\n time_starttransfer: %{time_starttransfer}s\\n ----------\\n time_total: %{time_total}s\\n EOF # Trace route to API endpoint traceroute api.cloudflow.io # Check DNS resolution time dig api.cloudflow.io | grep \"Query time\" ``` ### Memory Leaks #### Detection ```bash # Monitor CloudFlow service memory usage kubectl top pods -n cloudflow --sort-by=memory # Get detailed memory metrics for specific pod kubectl exec -n cloudflow deploy/cloudflow-api -- \\ curl localhost:9090/metrics | grep memory # Check for OOMKilled pods kubectl get pods -n cloudflow --field-selector=status.phase=Failed | \\ grep OOMKilled # Review memory limits and requests kubectl describe deployment cloudflow-api -n cloudflow | \\ grep -A 5 \"Limits\\|Requests\" ``` #### Common Causes",
              "score": null
            },
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_14",
              "content": "**Verify Service Health**: ```bash kubectl get pods -n cloudflow-prod curl https://api.cloudflow.io/health ``` **Recovery Time Objective (RTO)**: 4 hours **Recovery Point Objective (RPO)**: 24 hours --- ## Scaling and Performance ### Horizontal Pod Autoscaling CloudFlow is configured with Horizontal Pod Autoscaler (HPA) to automatically scale based on resource utilization: ```yaml # hpa.yaml apiVersion: autoscaling/v2 kind: HorizontalPodAutoscaler metadata: name: cloudflow-api-hpa namespace: cloudflow-prod spec: scaleTargetRef: apiVersion: apps/v1 kind: Deployment name: cloudflow-api minReplicas: 3 maxReplicas: 10 metrics: - type: Resource resource: name: cpu target: type: Utilization averageUtilization: 70 - type: Resource resource: name: memory target: type: Utilization averageUtilization: 80 behavior: scaleDown: stabilizationWindowSeconds: 300 policies: - type: Percent value: 50 periodSeconds: 60 scaleUp: stabilizationWindowSeconds: 60 policies: - type: Percent value: 100 periodSeconds: 30 - type: Pods value: 2 periodSeconds: 30 selectPolicy: Max ``` Monitor autoscaling events: ```bash kubectl get hpa -n cloudflow-prod -w kubectl describe hpa cloudflow-api-hpa -n cloudflow-prod ``` ### Performance Optimization #### Connection",
              "score": null
            }
          ],
          "latency_ms": 5.293897000228753,
          "query_id": "realistic_008",
          "dimension": "problem",
          "query": "Pods aren't scaling up during traffic spikes"
        },
        {
          "key_facts": [
            "minReplicas: 3",
            "maxReplicas: 10",
            "targetCPUUtilizationPercentage: 70"
          ],
          "found_facts": [
            "minReplicas: 3",
            "maxReplicas: 10"
          ],
          "missed_facts": [
            "targetCPUUtilizationPercentage: 70"
          ],
          "coverage": 0.6666666666666666,
          "expected_docs": [
            "deployment_guide"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_14",
              "content": "**Verify Service Health**: ```bash kubectl get pods -n cloudflow-prod curl https://api.cloudflow.io/health ``` **Recovery Time Objective (RTO)**: 4 hours **Recovery Point Objective (RPO)**: 24 hours --- ## Scaling and Performance ### Horizontal Pod Autoscaling CloudFlow is configured with Horizontal Pod Autoscaler (HPA) to automatically scale based on resource utilization: ```yaml # hpa.yaml apiVersion: autoscaling/v2 kind: HorizontalPodAutoscaler metadata: name: cloudflow-api-hpa namespace: cloudflow-prod spec: scaleTargetRef: apiVersion: apps/v1 kind: Deployment name: cloudflow-api minReplicas: 3 maxReplicas: 10 metrics: - type: Resource resource: name: cpu target: type: Utilization averageUtilization: 70 - type: Resource resource: name: memory target: type: Utilization averageUtilization: 80 behavior: scaleDown: stabilizationWindowSeconds: 300 policies: - type: Percent value: 50 periodSeconds: 60 scaleUp: stabilizationWindowSeconds: 60 policies: - type: Percent value: 100 periodSeconds: 30 - type: Pods value: 2 periodSeconds: 30 selectPolicy: Max ``` Monitor autoscaling events: ```bash kubectl get hpa -n cloudflow-prod -w kubectl describe hpa cloudflow-api-hpa -n cloudflow-prod ``` ### Performance Optimization #### Connection",
              "score": null
            },
            {
              "doc_id": "user_guide",
              "chunk_id": "user_guide_fix_22",
              "content": "Version Control For critical workflows: - Export YAML definitions regularly - Store in version control (Git) - Use pull requests for changes - Tag releases with semantic versioning ## Common Workflow Patterns Here are proven patterns for common automation scenarios: ### Pattern 1: Form to Database Capture form submissions and store in database: ```yaml name: \"Contact Form to Database\" trigger: type: webhook method: POST steps: - id: validate_submission action: javascript code: | if (!input.email || !input.message) { throw new Error(\"Email and message required\"); } return input; - id: check_duplicate action: database_query config: connection: \"{{secrets.DB_CONNECTION}}\" query: \"SELECT id FROM contacts WHERE email = $1 AND created_at > NOW() - INTERVAL '1 hour'\" parameters: - \"{{trigger.body.email}}\" - id: insert_contact action: database_query condition: \"{{length(steps.check_duplicate.rows) == 0}}\" config: connection: \"{{secrets.DB_CONNECTION}}\" query: | INSERT INTO contacts (name, email, message, source, created_at) VALUES ($1, $2, $3, $4, NOW()) parameters: - \"{{trigger.body.name}}\" - \"{{trigger.body.email}}\" - \"{{trigger.body.message}}\" -",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_14",
              "content": "Split workflow into smaller workflows:** ```bash # Create sub-workflows cloudflow workflows create data-pipeline-part1 \\ --steps \"data_ingestion,data_validation\" \\ --timeout 1800 cloudflow workflows create data-pipeline-part2 \\ --steps \"data_transformation,data_export\" \\ --timeout 3600 \\ --trigger workflow_completed \\ --trigger-workflow data-pipeline-part1 ``` ### Retry Logic and Exponential Backoff CloudFlow implements automatic retry with exponential backoff for transient failures: - Max retries: 3 - Initial delay: 1 second - Backoff multiplier: 2 - Max delay: 60 seconds #### Retry Sequence ``` Attempt 1: Immediate Attempt 2: Wait 1s (2^0 * 1s) Attempt 3: Wait 2s (2^1 * 1s) Attempt 4: Wait 4s (2^2 * 1s) ``` #### Configuration ```bash # View current retry settings cloudflow workflows get wf_9k2n4m8p1q --format json | jq '.retry_policy' # Customize retry behavior cloudflow workflows update wf_9k2n4m8p1q \\ --retry-max-attempts 5 \\ --retry-initial-delay 2000 \\ --retry-backoff-multiplier 2 \\ --retry-max-delay 120000 \\ --retry-on-errors \"NETWORK_ERROR,TIMEOUT,RATE_LIMIT\" # Disable retry for specific step cloudflow workflows update wf_9k2n4m8p1q \\",
              "score": null
            },
            {
              "doc_id": "user_guide",
              "chunk_id": "user_guide_fix_14",
              "content": "### Retry Policies Configure automatic retries for failed actions: ```yaml - id: api_call action: http_request config: url: \"https://api.example.com/data\" retry: max_attempts: 3 backoff_type: \"exponential\" # or \"fixed\", \"linear\" initial_interval: 1000 # milliseconds max_interval: 30000 multiplier: 2.0 retry_on: - timeout - network_error - status: [500, 502, 503, 504] ``` **Backoff Strategies:** - **Fixed**: Wait the same amount of time between retries - Attempt 1: Wait 1s - Attempt 2: Wait 1s - Attempt 3: Wait 1s - **Linear**: Increase wait time by a fixed amount - Attempt 1: Wait 1s - Attempt 2: Wait 2s - Attempt 3: Wait 3s - **Exponential**: Double the wait time with each retry (recommended) - Attempt 1: Wait 1s - Attempt 2: Wait 2s - Attempt 3: Wait 4s **Retry Conditions:** Control which errors trigger retries: - `timeout`: Request timeout - `network_error`: Connection failures - `status`: Specific HTTP status codes - `error_code`: Application-specific error codes ###",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_8",
              "content": "**1. Workflow Context Accumulation** Large workflow executions may accumulate state in memory. **Solution:** ```bash # Configure context cleanup cloudflow config set workflow.context.max_size_mb 100 cloudflow config set workflow.context.cleanup_threshold 0.8 # Enable context persistence to disk cloudflow config set workflow.context.persistence.enabled true cloudflow config set workflow.context.persistence.backend s3 ``` **2. Connection Pool Leaks** **Diagnosis:** ```bash # Check active connections cloudflow db connections --verbose # Expected output: # Active: 45/100 # Idle: 23 # Waiting: 2 # Average age: 245s ``` **Resolution:** ```bash # Adjust connection pool settings cloudflow config set db.pool.max_connections 100 cloudflow config set db.pool.min_connections 10 cloudflow config set db.pool.idle_timeout 300 cloudflow config set db.pool.max_lifetime 1800 # Force connection pool reset cloudflow db pool reset --confirm ``` **3. Event Stream Buffers** Unbounded event buffers can cause memory exhaustion.",
              "score": null
            }
          ],
          "latency_ms": 5.525867998585454,
          "query_id": "realistic_008",
          "dimension": "casual",
          "query": "autoscaling config"
        },
        {
          "key_facts": [
            "minReplicas: 3",
            "maxReplicas: 10",
            "targetCPUUtilizationPercentage: 70"
          ],
          "found_facts": [],
          "missed_facts": [
            "minReplicas: 3",
            "maxReplicas: 10",
            "targetCPUUtilizationPercentage: 70"
          ],
          "coverage": 0.0,
          "expected_docs": [
            "deployment_guide"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_29",
              "content": "minutes (no manual intervention) - Data loss: None (multi-AZ replication) **Scenario 2: Database Primary Failure** - Detection: Health check fails for primary database (< 30 seconds) - Action: Automatic promotion of read replica to primary - Recovery time: 30-60 seconds - Data loss: Minimal (< 1 second due to synchronous replication) **Scenario 3: Full Region Failure** - Detection: Multiple health check failures across all AZs (< 2 minutes) - Action: Manual failover to DR region (us-west-2) - Procedure: 1. Update DNS to point to DR region (TTL: 60 seconds) 2. Promote DR database replica to primary 3. Scale up DR region services to production capacity 4. Verify data consistency and integrity 5.",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_27",
              "content": "hour of transactions - Achieved through: Continuous database replication + hourly snapshots - Kafka retention: 7 days allows event replay **RTO (Recovery Time Objective): 4 hours** - Maximum acceptable downtime: 4 hours for full system recovery - Includes: Failover, data verification, and service restoration - Automated runbooks reduce RTO to < 2 hours for common scenarios ### High Availability Architecture **Multi-AZ Deployment**: ``` Region: us-east-1 AZ-1a: AZ-1b: AZ-1c: - API Gateway (4) - API Gateway (4) - API Gateway (4) - Workflow Engine (6) - Workflow Engine (5) - Workflow Engine (5) - Auth Service (3) - Auth Service (3) - Auth Service (2) - PostgreSQL Primary - PostgreSQL Replica - PostgreSQL Replica - Redis Primary (2) - Redis Replica (2) - Redis Replica (2) - Kafka Broker (2) - Kafka Broker (2) - Kafka Broker (1) ``` **Automatic Failover**: - Database: 30-60 seconds (automatic promotion of replica) - Redis:",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_3",
              "content": "**Technology**: Node.js with Express.js framework **Replicas**: 12 pods (production), auto-scaling 8-20 based on CPU **Resource Allocation**: 2 vCPU, 4GB RAM per pod **Key Responsibilities**: - JWT token validation (delegated to Auth Service for initial validation) - Rate limiting: 1000 requests per minute per API key (sliding window) - Request/response transformation and validation using JSON Schema - Routing to downstream services based on URL path patterns - CORS handling for web clients - Request/response logging and correlation ID injection **Critical Endpoints**: - `POST /api/v1/workflows` - Create new workflow - `GET /api/v1/workflows/:id` - Retrieve workflow status - `POST /api/v1/workflows/:id/execute` - Trigger workflow execution - `GET /api/v1/workflows/:id/history` - Get execution history **Dependencies**: - Auth Service (for token validation) - Redis (for rate limiting counters) - All downstream microservices **Performance Targets**: - P50 latency: < 50ms - P99 latency: < 200ms - Throughput: 10,000 RPS sustained --- ### Auth Service **Purpose**: Centralized authentication and",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_14",
              "content": "< 20ms - Write query P99: < 50ms - Transaction throughput: 15,000 TPS - Replication lag: < 500ms --- ### Redis Caching Layer **Cluster Configuration**: - Redis Cluster with 6 nodes (3 primary + 3 replica) - Instance type: cache.r6g.xlarge (4 vCPU, 26GB RAM) - Total memory: 78GB usable cache space - Persistence: RDB snapshots every 5 minutes + AOF - Eviction policy: allkeys-lru **Cache Usage Patterns**: 1. **Session Storage**: - Key pattern: `session:{user_id}` - TTL: 15 minutes (aligned with JWT expiry) - Data: User session state, preferences - Invalidation: On logout or password change 2. **Rate Limiting Counters**: - Key pattern: `ratelimit:{api_key}:{window}` - TTL: 60 seconds (sliding window) - Data: Request count per window - Algorithm: Token bucket with Redis INCR 3. **Compiled Templates**: - Key pattern: `template:{template_id}:{version}` - TTL: 1 hour - Data: Compiled Handlebars template - Invalidation: On template update 4.",
              "score": null
            },
            {
              "doc_id": "user_guide",
              "chunk_id": "user_guide_fix_18",
              "content": "action - **Maximum execution payload**: 50MB total - **Variable value size**: 1MB per variable ### Enterprise Plan Limits Enterprise customers can request increased limits: - Up to 100 steps per workflow - Up to 10,000 executions per day - Up to 7200 second timeout (2 hours) - Priority execution queue - Dedicated capacity allocation Contact sales@cloudflow.io for Enterprise pricing and custom limits. ## Best Practices Follow these best practices to build reliable, maintainable workflows: ### 1. Use Descriptive Names **Good:** - Workflow: \"Sync Customer Data from Salesforce to Database\" - Step: \"validate_customer_email\" **Bad:** - Workflow: \"Workflow 1\" - Step: \"step3\" ### 2.",
              "score": null
            }
          ],
          "latency_ms": 5.590859998847009,
          "query_id": "realistic_008",
          "dimension": "contextual",
          "query": "I need to configure autoscaling, what are the min/max replicas and thresholds?"
        },
        {
          "key_facts": [
            "minReplicas: 3",
            "maxReplicas: 10",
            "targetCPUUtilizationPercentage: 70"
          ],
          "found_facts": [],
          "missed_facts": [
            "minReplicas: 3",
            "maxReplicas: 10",
            "targetCPUUtilizationPercentage: 70"
          ],
          "coverage": 0.0,
          "expected_docs": [
            "deployment_guide"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_29",
              "content": "minutes (no manual intervention) - Data loss: None (multi-AZ replication) **Scenario 2: Database Primary Failure** - Detection: Health check fails for primary database (< 30 seconds) - Action: Automatic promotion of read replica to primary - Recovery time: 30-60 seconds - Data loss: Minimal (< 1 second due to synchronous replication) **Scenario 3: Full Region Failure** - Detection: Multiple health check failures across all AZs (< 2 minutes) - Action: Manual failover to DR region (us-west-2) - Procedure: 1. Update DNS to point to DR region (TTL: 60 seconds) 2. Promote DR database replica to primary 3. Scale up DR region services to production capacity 4. Verify data consistency and integrity 5.",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_3",
              "content": "**Technology**: Node.js with Express.js framework **Replicas**: 12 pods (production), auto-scaling 8-20 based on CPU **Resource Allocation**: 2 vCPU, 4GB RAM per pod **Key Responsibilities**: - JWT token validation (delegated to Auth Service for initial validation) - Rate limiting: 1000 requests per minute per API key (sliding window) - Request/response transformation and validation using JSON Schema - Routing to downstream services based on URL path patterns - CORS handling for web clients - Request/response logging and correlation ID injection **Critical Endpoints**: - `POST /api/v1/workflows` - Create new workflow - `GET /api/v1/workflows/:id` - Retrieve workflow status - `POST /api/v1/workflows/:id/execute` - Trigger workflow execution - `GET /api/v1/workflows/:id/history` - Get execution history **Dependencies**: - Auth Service (for token validation) - Redis (for rate limiting counters) - All downstream microservices **Performance Targets**: - P50 latency: < 50ms - P99 latency: < 200ms - Throughput: 10,000 RPS sustained --- ### Auth Service **Purpose**: Centralized authentication and",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_27",
              "content": "hour of transactions - Achieved through: Continuous database replication + hourly snapshots - Kafka retention: 7 days allows event replay **RTO (Recovery Time Objective): 4 hours** - Maximum acceptable downtime: 4 hours for full system recovery - Includes: Failover, data verification, and service restoration - Automated runbooks reduce RTO to < 2 hours for common scenarios ### High Availability Architecture **Multi-AZ Deployment**: ``` Region: us-east-1 AZ-1a: AZ-1b: AZ-1c: - API Gateway (4) - API Gateway (4) - API Gateway (4) - Workflow Engine (6) - Workflow Engine (5) - Workflow Engine (5) - Auth Service (3) - Auth Service (3) - Auth Service (2) - PostgreSQL Primary - PostgreSQL Replica - PostgreSQL Replica - Redis Primary (2) - Redis Replica (2) - Redis Replica (2) - Kafka Broker (2) - Kafka Broker (2) - Kafka Broker (1) ``` **Automatic Failover**: - Database: 30-60 seconds (automatic promotion of replica) - Redis:",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_11",
              "content": "**Add read replicas:** ```bash # Route read-only queries to replicas cloudflow db replicas add --count 2 cloudflow config set db.read_replicas \"replica-1:5432,replica-2:5432\" cloudflow config set db.read_write_split true ``` ### Connection Timeout Errors #### Error Messages - `connection timeout after 30000ms` - `could not connect to database server at 10.0.2.45:5432` - `database server unreachable` #### Troubleshooting Steps ```bash # Test network connectivity telnet cloudflow-db.internal.company.com 5432 # Check DNS resolution nslookup cloudflow-db.internal.company.com # Verify database is accepting connections pg_isready -h cloudflow-db.internal.company.com -p 5432 -U cloudflow # Check firewall rules sudo iptables -L -n | grep 5432 # Test from CloudFlow pod network kubectl run -n cloudflow debug-pod --rm -i --tty \\ --image=postgres:14 -- \\ psql -h cloudflow-db.internal.company.com -U cloudflow -d cloudflow_production # Review database logs for connection rejections kubectl logs -n cloudflow statefulset/cloudflow-db --tail=100 | \\ grep -i \"connection\\|reject\\|authentication\" ``` ### Maximum Connection Limit (100) Reached This is a hard limit in CloudFlow's",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_31",
              "content": "DNS propagation slower than expected (resolved) - Success criteria: Met all recovery objectives ### Business Continuity **Communication Plan**: - Status page: status.cloudflow.com (updated every 15 minutes during incident) - Customer notifications: Email + SMS for all P1 incidents - Internal escalation: PagerDuty \u2192 Incident Commander \u2192 Engineering Manager \u2192 CTO **Data Retention Policy**: - Active data: 12 months in hot storage (PostgreSQL) - Archived data: 7 years in cold storage (S3 Glacier) - Audit logs: 7 years (compliance requirement) - Backup retention: 30 days standard, 90 days monthly snapshots --- ## Appendix ### Service Dependency Matrix | Service | Depends On | Critical Path | |---------|-----------|---------------| | API Gateway | Auth Service, Redis | Yes | | Auth Service | PostgreSQL, Redis, Vault | Yes | | Workflow Engine | PostgreSQL, Kafka, Redis | Yes | | Scheduler | PostgreSQL, Redis, Kafka | No | | Notification Service | Kafka,",
              "score": null
            }
          ],
          "latency_ms": 5.819735999466502,
          "query_id": "realistic_008",
          "dimension": "negation",
          "query": "Why do we have 3 replicas even with low traffic?"
        },
        {
          "key_facts": [
            "P99 latency: < 200ms",
            "average P99 latency of 180ms for API operations"
          ],
          "found_facts": [],
          "missed_facts": [
            "P99 latency: < 200ms",
            "average P99 latency of 180ms for API operations"
          ],
          "coverage": 0.0,
          "expected_docs": [
            "architecture_overview"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_24",
              "content": "Operations** (P99 latency): - `GET /workflows/{id}`: < 50ms (cache hit), < 150ms (cache miss) - `POST /workflows`: < 200ms (includes validation and database write) - `POST /workflows/{id}/execute`: < 100ms (async, returns execution ID) - `GET /workflows/{id}/history`: < 300ms (paginated, 50 records per page) **Workflow Execution** (P99 latency): - Simple workflow (< 5 steps): < 2 seconds - Medium workflow (5-15 steps): < 5 seconds - Complex workflow (> 15 steps): < 15 seconds - Parallel execution: Linear scaling up to 10 concurrent branches **Database Operations** (P99 latency): - Simple SELECT: < 5ms - JOIN query: < 20ms - INSERT/UPDATE: < 10ms - Batch operations: < 100ms (batch size: 100 records) **Cache Operations** (P99 latency): - Redis GET: < 2ms - Redis SET: < 3ms - Redis DEL: < 2ms ### Throughput Capacity **API Gateway**: - Sustained: 10,000 requests per second - Peak: 25,000 requests per second (5-minute burst) - Rate",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_14",
              "content": "< 20ms - Write query P99: < 50ms - Transaction throughput: 15,000 TPS - Replication lag: < 500ms --- ### Redis Caching Layer **Cluster Configuration**: - Redis Cluster with 6 nodes (3 primary + 3 replica) - Instance type: cache.r6g.xlarge (4 vCPU, 26GB RAM) - Total memory: 78GB usable cache space - Persistence: RDB snapshots every 5 minutes + AOF - Eviction policy: allkeys-lru **Cache Usage Patterns**: 1. **Session Storage**: - Key pattern: `session:{user_id}` - TTL: 15 minutes (aligned with JWT expiry) - Data: User session state, preferences - Invalidation: On logout or password change 2. **Rate Limiting Counters**: - Key pattern: `ratelimit:{api_key}:{window}` - TTL: 60 seconds (sliding window) - Data: Request count per window - Algorithm: Token bucket with Redis INCR 3. **Compiled Templates**: - Key pattern: `template:{template_id}:{version}` - TTL: 1 hour - Data: Compiled Handlebars template - Invalidation: On template update 4.",
              "score": null
            },
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_16",
              "content": "}, ], thresholds: { http_req_duration: ['p(95)<500'], }, }; export default function () { let res = http.get('https://api.cloudflow.io/api/workflows'); check(res, { 'status is 200': (r) => r.status === 200 }); sleep(1); } EOF ``` **Performance Targets**: - P95 API latency: < 500ms - P99 API latency: < 1000ms - Error rate: < 0.1% - Throughput: > 1000 requests/second --- ## Security and Compliance ### Network Policies Implement network policies to restrict pod-to-pod communication: ```yaml # network-policy.yaml apiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: cloudflow-network-policy namespace: cloudflow-prod spec: podSelector: matchLabels: app: cloudflow policyTypes: - Ingress - Egress ingress: - from: - namespaceSelector: matchLabels: name: cloudflow-prod - podSelector: matchLabels: app: nginx-ingress ports: - protocol: TCP port: 3000 egress: - to: - podSelector: matchLabels: app: postgresql ports: - protocol: TCP port: 5432 - to: - podSelector: matchLabels: app: redis ports: - protocol: TCP port: 6379 ``` ### Pod Security Standards Enforce Pod Security Standards at the",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_26",
              "content": "Major functionality impaired affecting multiple users - **Examples:** - Workflow execution success rate < 90% - Significant performance degradation (p95 latency > 10s) - Rate limiting affecting large customer segment - **Response Time:** < 1 hour - **Escalation:** Create incident ticket and notify on-call #### SEV-3: Medium (P3) - **Definition:** Partial functionality degraded affecting some users - **Examples:** - Intermittent failures for specific workflow types - Minor performance issues - Non-critical feature unavailable - **Response Time:** < 4 hours - **Escalation:** Create ticket, normal business hours support #### SEV-4: Low (P4) - **Definition:** Minor issues with minimal user impact - **Examples:** - Cosmetic issues - Documentation errors - Feature requests - **Response Time:** < 2 business days - **Escalation:** Standard support ticket ### Escalation Steps #### Step 1: Initial Assessment (0-5 minutes) ```bash # Run health check cloudflow health check --comprehensive # Check status page curl https://status.cloudflow.io/api/v1/status.json # Review recent",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_25",
              "content": "limiting: 1,000 requests per minute per API key **Workflow Engine**: - Concurrent executions: 8,000 workflows (across 16 pods) - Execution start rate: 500 per second - Completion rate: 450 per second (average 2-second execution time) **Database**: - Read throughput: 50,000 queries per second (across replicas) - Write throughput: 15,000 transactions per second - Connection capacity: 2,000 concurrent connections **Kafka**: - Message ingestion: 100,000 messages per second - Consumer throughput: 80,000 messages per second (aggregated) - End-to-end latency: < 100ms (P99) ### Resource Utilization **CPU Utilization** (Target: 60-70% average): - API Gateway: 55% average, 80% peak - Workflow Engine: 65% average, 85% peak - Auth Service: 40% average, 60% peak **Memory Utilization** (Target: < 80%): - API Gateway: 2.5GB average per pod (4GB allocated) - Workflow Engine: 6GB average per pod (8GB allocated) - Notification Service: 2.8GB average per pod (4GB allocated) **Network Throughput**: - Ingress: 2 Gbps average, 5 Gbps",
              "score": null
            }
          ],
          "latency_ms": 5.587974001173279,
          "query_id": "realistic_009",
          "dimension": "original",
          "query": "What is the P99 latency target for API operations?"
        },
        {
          "key_facts": [
            "P99 latency: < 200ms",
            "average P99 latency of 180ms for API operations"
          ],
          "found_facts": [],
          "missed_facts": [
            "P99 latency: < 200ms",
            "average P99 latency of 180ms for API operations"
          ],
          "coverage": 0.0,
          "expected_docs": [
            "architecture_overview"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_9",
              "content": "to detect leader failure (5-second interval) - Automatic failover to standby scheduler (< 10 seconds) - Schedule versioning to handle updates during execution - Missed execution policy: SKIP, RUN_ONCE, or RUN_ALL **Performance Targets**: - Schedule evaluation: < 100ms per cycle - Accuracy: \u00b1 1 second for schedule triggers - Capacity: 100,000 active schedules --- ### Notification Service **Purpose**: Multi-channel notification delivery system supporting email, SMS, webhooks, and in-app notifications. **Technology**: Node.js with worker pool pattern **Replicas**: 8 pods (production), auto-scaling 6-16 **Resource Allocation**: 2 vCPU, 4GB RAM per pod **Key Responsibilities**: - Consume notification events from Kafka - Template rendering with Handlebars (cached in Redis) - Multi-channel delivery (Email via SendGrid, SMS via Twilio, Webhooks) - Retry logic with dead letter queue for failed deliveries - Delivery status tracking and analytics - User preference management (opt-in/opt-out) **Notification Flow**: ``` Event Source \u2192 Kafka (notifications.email topic) \u2502 \u25bc Notification Service Consumer",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_24",
              "content": "Operations** (P99 latency): - `GET /workflows/{id}`: < 50ms (cache hit), < 150ms (cache miss) - `POST /workflows`: < 200ms (includes validation and database write) - `POST /workflows/{id}/execute`: < 100ms (async, returns execution ID) - `GET /workflows/{id}/history`: < 300ms (paginated, 50 records per page) **Workflow Execution** (P99 latency): - Simple workflow (< 5 steps): < 2 seconds - Medium workflow (5-15 steps): < 5 seconds - Complex workflow (> 15 steps): < 15 seconds - Parallel execution: Linear scaling up to 10 concurrent branches **Database Operations** (P99 latency): - Simple SELECT: < 5ms - JOIN query: < 20ms - INSERT/UPDATE: < 10ms - Batch operations: < 100ms (batch size: 100 records) **Cache Operations** (P99 latency): - Redis GET: < 2ms - Redis SET: < 3ms - Redis DEL: < 2ms ### Throughput Capacity **API Gateway**: - Sustained: 10,000 requests per second - Peak: 25,000 requests per second (5-minute burst) - Rate",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_14",
              "content": "< 20ms - Write query P99: < 50ms - Transaction throughput: 15,000 TPS - Replication lag: < 500ms --- ### Redis Caching Layer **Cluster Configuration**: - Redis Cluster with 6 nodes (3 primary + 3 replica) - Instance type: cache.r6g.xlarge (4 vCPU, 26GB RAM) - Total memory: 78GB usable cache space - Persistence: RDB snapshots every 5 minutes + AOF - Eviction policy: allkeys-lru **Cache Usage Patterns**: 1. **Session Storage**: - Key pattern: `session:{user_id}` - TTL: 15 minutes (aligned with JWT expiry) - Data: User session state, preferences - Invalidation: On logout or password change 2. **Rate Limiting Counters**: - Key pattern: `ratelimit:{api_key}:{window}` - TTL: 60 seconds (sliding window) - Data: Request count per window - Algorithm: Token bucket with Redis INCR 3. **Compiled Templates**: - Key pattern: `template:{template_id}:{version}` - TTL: 1 hour - Data: Compiled Handlebars template - Invalidation: On template update 4.",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_29",
              "content": "minutes (no manual intervention) - Data loss: None (multi-AZ replication) **Scenario 2: Database Primary Failure** - Detection: Health check fails for primary database (< 30 seconds) - Action: Automatic promotion of read replica to primary - Recovery time: 30-60 seconds - Data loss: Minimal (< 1 second due to synchronous replication) **Scenario 3: Full Region Failure** - Detection: Multiple health check failures across all AZs (< 2 minutes) - Action: Manual failover to DR region (us-west-2) - Procedure: 1. Update DNS to point to DR region (TTL: 60 seconds) 2. Promote DR database replica to primary 3. Scale up DR region services to production capacity 4. Verify data consistency and integrity 5.",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_30",
              "content": "Update monitoring dashboards - Recovery time: 2-4 hours (includes verification) - Data loss: < 1 hour (last cross-region replication) **Scenario 4: Data Corruption** - Detection: Data validation checks or user report - Action: Point-in-time recovery from WAL archives - Procedure: 1. Identify corruption time window 2. Restore from snapshot prior to corruption 3. Replay WAL logs up to corruption point 4. Verify data integrity 5. Resume normal operations - Recovery time: 1-3 hours depending on data volume - Data loss: None if corruption detected quickly ### Testing & Validation **DR Drill Schedule**: - Monthly: Automated failover test (single AZ failure simulation) - Quarterly: Full DR region failover (non-production hours) - Annually: Complete disaster simulation with all stakeholders **Last DR Test Results** (Dec 15, 2025): - Scenario: Full region failover - Actual RTO: 2 hours 23 minutes (target: 4 hours) - Actual RPO: 42 minutes (target: 1 hour) - Issues identified:",
              "score": null
            }
          ],
          "latency_ms": 5.699911998817697,
          "query_id": "realistic_009",
          "dimension": "synonym",
          "query": "What's the 99th percentile response time target?"
        },
        {
          "key_facts": [
            "P99 latency: < 200ms",
            "average P99 latency of 180ms for API operations"
          ],
          "found_facts": [],
          "missed_facts": [
            "P99 latency: < 200ms",
            "average P99 latency of 180ms for API operations"
          ],
          "coverage": 0.0,
          "expected_docs": [
            "architecture_overview"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_7",
              "content": "# - Business Logic: 32ms (13%) # - Response Serialization: 12ms (5%) ``` **Network latency issues:** ```bash # Test connectivity to CloudFlow API time curl -w \"@curl-format.txt\" -o /dev/null -s https://api.cloudflow.io/health # Create curl-format.txt: cat > curl-format.txt << EOF time_namelookup: %{time_namelookup}s\\n time_connect: %{time_connect}s\\n time_appconnect: %{time_appconnect}s\\n time_pretransfer: %{time_pretransfer}s\\n time_redirect: %{time_redirect}s\\n time_starttransfer: %{time_starttransfer}s\\n ----------\\n time_total: %{time_total}s\\n EOF # Trace route to API endpoint traceroute api.cloudflow.io # Check DNS resolution time dig api.cloudflow.io | grep \"Query time\" ``` ### Memory Leaks #### Detection ```bash # Monitor CloudFlow service memory usage kubectl top pods -n cloudflow --sort-by=memory # Get detailed memory metrics for specific pod kubectl exec -n cloudflow deploy/cloudflow-api -- \\ curl localhost:9090/metrics | grep memory # Check for OOMKilled pods kubectl get pods -n cloudflow --field-selector=status.phase=Failed | \\ grep OOMKilled # Review memory limits and requests kubectl describe deployment cloudflow-api -n cloudflow | \\ grep -A 5 \"Limits\\|Requests\" ``` #### Common Causes",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_24",
              "content": "Operations** (P99 latency): - `GET /workflows/{id}`: < 50ms (cache hit), < 150ms (cache miss) - `POST /workflows`: < 200ms (includes validation and database write) - `POST /workflows/{id}/execute`: < 100ms (async, returns execution ID) - `GET /workflows/{id}/history`: < 300ms (paginated, 50 records per page) **Workflow Execution** (P99 latency): - Simple workflow (< 5 steps): < 2 seconds - Medium workflow (5-15 steps): < 5 seconds - Complex workflow (> 15 steps): < 15 seconds - Parallel execution: Linear scaling up to 10 concurrent branches **Database Operations** (P99 latency): - Simple SELECT: < 5ms - JOIN query: < 20ms - INSERT/UPDATE: < 10ms - Batch operations: < 100ms (batch size: 100 records) **Cache Operations** (P99 latency): - Redis GET: < 2ms - Redis SET: < 3ms - Redis DEL: < 2ms ### Throughput Capacity **API Gateway**: - Sustained: 10,000 requests per second - Peak: 25,000 requests per second (5-minute burst) - Rate",
              "score": null
            },
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_16",
              "content": "}, ], thresholds: { http_req_duration: ['p(95)<500'], }, }; export default function () { let res = http.get('https://api.cloudflow.io/api/workflows'); check(res, { 'status is 200': (r) => r.status === 200 }); sleep(1); } EOF ``` **Performance Targets**: - P95 API latency: < 500ms - P99 API latency: < 1000ms - Error rate: < 0.1% - Throughput: > 1000 requests/second --- ## Security and Compliance ### Network Policies Implement network policies to restrict pod-to-pod communication: ```yaml # network-policy.yaml apiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: cloudflow-network-policy namespace: cloudflow-prod spec: podSelector: matchLabels: app: cloudflow policyTypes: - Ingress - Egress ingress: - from: - namespaceSelector: matchLabels: name: cloudflow-prod - podSelector: matchLabels: app: nginx-ingress ports: - protocol: TCP port: 3000 egress: - to: - podSelector: matchLabels: app: postgresql ports: - protocol: TCP port: 5432 - to: - podSelector: matchLabels: app: redis ports: - protocol: TCP port: 6379 ``` ### Pod Security Standards Enforce Pod Security Standards at the",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_25",
              "content": "limiting: 1,000 requests per minute per API key **Workflow Engine**: - Concurrent executions: 8,000 workflows (across 16 pods) - Execution start rate: 500 per second - Completion rate: 450 per second (average 2-second execution time) **Database**: - Read throughput: 50,000 queries per second (across replicas) - Write throughput: 15,000 transactions per second - Connection capacity: 2,000 concurrent connections **Kafka**: - Message ingestion: 100,000 messages per second - Consumer throughput: 80,000 messages per second (aggregated) - End-to-end latency: < 100ms (P99) ### Resource Utilization **CPU Utilization** (Target: 60-70% average): - API Gateway: 55% average, 80% peak - Workflow Engine: 65% average, 85% peak - Auth Service: 40% average, 60% peak **Memory Utilization** (Target: < 80%): - API Gateway: 2.5GB average per pod (4GB allocated) - Workflow Engine: 6GB average per pod (8GB allocated) - Notification Service: 2.8GB average per pod (4GB allocated) **Network Throughput**: - Ingress: 2 Gbps average, 5 Gbps",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_26",
              "content": "Major functionality impaired affecting multiple users - **Examples:** - Workflow execution success rate < 90% - Significant performance degradation (p95 latency > 10s) - Rate limiting affecting large customer segment - **Response Time:** < 1 hour - **Escalation:** Create incident ticket and notify on-call #### SEV-3: Medium (P3) - **Definition:** Partial functionality degraded affecting some users - **Examples:** - Intermittent failures for specific workflow types - Minor performance issues - Non-critical feature unavailable - **Response Time:** < 4 hours - **Escalation:** Create ticket, normal business hours support #### SEV-4: Low (P4) - **Definition:** Minor issues with minimal user impact - **Examples:** - Cosmetic issues - Documentation errors - Feature requests - **Response Time:** < 2 business days - **Escalation:** Standard support ticket ### Escalation Steps #### Step 1: Initial Assessment (0-5 minutes) ```bash # Run health check cloudflow health check --comprehensive # Check status page curl https://status.cloudflow.io/api/v1/status.json # Review recent",
              "score": null
            }
          ],
          "latency_ms": 5.6807170003594365,
          "query_id": "realistic_009",
          "dimension": "problem",
          "query": "Our API latency is 500ms, is that acceptable?"
        },
        {
          "key_facts": [
            "P99 latency: < 200ms",
            "average P99 latency of 180ms for API operations"
          ],
          "found_facts": [],
          "missed_facts": [
            "P99 latency: < 200ms",
            "average P99 latency of 180ms for API operations"
          ],
          "coverage": 0.0,
          "expected_docs": [
            "architecture_overview"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_16",
              "content": "}, ], thresholds: { http_req_duration: ['p(95)<500'], }, }; export default function () { let res = http.get('https://api.cloudflow.io/api/workflows'); check(res, { 'status is 200': (r) => r.status === 200 }); sleep(1); } EOF ``` **Performance Targets**: - P95 API latency: < 500ms - P99 API latency: < 1000ms - Error rate: < 0.1% - Throughput: > 1000 requests/second --- ## Security and Compliance ### Network Policies Implement network policies to restrict pod-to-pod communication: ```yaml # network-policy.yaml apiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: cloudflow-network-policy namespace: cloudflow-prod spec: podSelector: matchLabels: app: cloudflow policyTypes: - Ingress - Egress ingress: - from: - namespaceSelector: matchLabels: name: cloudflow-prod - podSelector: matchLabels: app: nginx-ingress ports: - protocol: TCP port: 3000 egress: - to: - podSelector: matchLabels: app: postgresql ports: - protocol: TCP port: 5432 - to: - podSelector: matchLabels: app: redis ports: - protocol: TCP port: 6379 ``` ### Pod Security Standards Enforce Pod Security Standards at the",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_7",
              "content": "# - Business Logic: 32ms (13%) # - Response Serialization: 12ms (5%) ``` **Network latency issues:** ```bash # Test connectivity to CloudFlow API time curl -w \"@curl-format.txt\" -o /dev/null -s https://api.cloudflow.io/health # Create curl-format.txt: cat > curl-format.txt << EOF time_namelookup: %{time_namelookup}s\\n time_connect: %{time_connect}s\\n time_appconnect: %{time_appconnect}s\\n time_pretransfer: %{time_pretransfer}s\\n time_redirect: %{time_redirect}s\\n time_starttransfer: %{time_starttransfer}s\\n ----------\\n time_total: %{time_total}s\\n EOF # Trace route to API endpoint traceroute api.cloudflow.io # Check DNS resolution time dig api.cloudflow.io | grep \"Query time\" ``` ### Memory Leaks #### Detection ```bash # Monitor CloudFlow service memory usage kubectl top pods -n cloudflow --sort-by=memory # Get detailed memory metrics for specific pod kubectl exec -n cloudflow deploy/cloudflow-api -- \\ curl localhost:9090/metrics | grep memory # Check for OOMKilled pods kubectl get pods -n cloudflow --field-selector=status.phase=Failed | \\ grep OOMKilled # Review memory limits and requests kubectl describe deployment cloudflow-api -n cloudflow | \\ grep -A 5 \"Limits\\|Requests\" ``` #### Common Causes",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_24",
              "content": "Operations** (P99 latency): - `GET /workflows/{id}`: < 50ms (cache hit), < 150ms (cache miss) - `POST /workflows`: < 200ms (includes validation and database write) - `POST /workflows/{id}/execute`: < 100ms (async, returns execution ID) - `GET /workflows/{id}/history`: < 300ms (paginated, 50 records per page) **Workflow Execution** (P99 latency): - Simple workflow (< 5 steps): < 2 seconds - Medium workflow (5-15 steps): < 5 seconds - Complex workflow (> 15 steps): < 15 seconds - Parallel execution: Linear scaling up to 10 concurrent branches **Database Operations** (P99 latency): - Simple SELECT: < 5ms - JOIN query: < 20ms - INSERT/UPDATE: < 10ms - Batch operations: < 100ms (batch size: 100 records) **Cache Operations** (P99 latency): - Redis GET: < 2ms - Redis SET: < 3ms - Redis DEL: < 2ms ### Throughput Capacity **API Gateway**: - Sustained: 10,000 requests per second - Peak: 25,000 requests per second (5-minute burst) - Rate",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_25",
              "content": "limiting: 1,000 requests per minute per API key **Workflow Engine**: - Concurrent executions: 8,000 workflows (across 16 pods) - Execution start rate: 500 per second - Completion rate: 450 per second (average 2-second execution time) **Database**: - Read throughput: 50,000 queries per second (across replicas) - Write throughput: 15,000 transactions per second - Connection capacity: 2,000 concurrent connections **Kafka**: - Message ingestion: 100,000 messages per second - Consumer throughput: 80,000 messages per second (aggregated) - End-to-end latency: < 100ms (P99) ### Resource Utilization **CPU Utilization** (Target: 60-70% average): - API Gateway: 55% average, 80% peak - Workflow Engine: 65% average, 85% peak - Auth Service: 40% average, 60% peak **Memory Utilization** (Target: < 80%): - API Gateway: 2.5GB average per pod (4GB allocated) - Workflow Engine: 6GB average per pod (8GB allocated) - Notification Service: 2.8GB average per pod (4GB allocated) **Network Throughput**: - Ingress: 2 Gbps average, 5 Gbps",
              "score": null
            },
            {
              "doc_id": "user_guide",
              "chunk_id": "user_guide_fix_14",
              "content": "### Retry Policies Configure automatic retries for failed actions: ```yaml - id: api_call action: http_request config: url: \"https://api.example.com/data\" retry: max_attempts: 3 backoff_type: \"exponential\" # or \"fixed\", \"linear\" initial_interval: 1000 # milliseconds max_interval: 30000 multiplier: 2.0 retry_on: - timeout - network_error - status: [500, 502, 503, 504] ``` **Backoff Strategies:** - **Fixed**: Wait the same amount of time between retries - Attempt 1: Wait 1s - Attempt 2: Wait 1s - Attempt 3: Wait 1s - **Linear**: Increase wait time by a fixed amount - Attempt 1: Wait 1s - Attempt 2: Wait 2s - Attempt 3: Wait 3s - **Exponential**: Double the wait time with each retry (recommended) - Attempt 1: Wait 1s - Attempt 2: Wait 2s - Attempt 3: Wait 4s **Retry Conditions:** Control which errors trigger retries: - `timeout`: Request timeout - `network_error`: Connection failures - `status`: Specific HTTP status codes - `error_code`: Application-specific error codes ###",
              "score": null
            }
          ],
          "latency_ms": 5.464854999445379,
          "query_id": "realistic_009",
          "dimension": "casual",
          "query": "api latency target"
        },
        {
          "key_facts": [
            "P99 latency: < 200ms",
            "average P99 latency of 180ms for API operations"
          ],
          "found_facts": [
            "average P99 latency of 180ms for API operations"
          ],
          "missed_facts": [
            "P99 latency: < 200ms"
          ],
          "coverage": 0.5,
          "expected_docs": [
            "architecture_overview"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_7",
              "content": "# - Business Logic: 32ms (13%) # - Response Serialization: 12ms (5%) ``` **Network latency issues:** ```bash # Test connectivity to CloudFlow API time curl -w \"@curl-format.txt\" -o /dev/null -s https://api.cloudflow.io/health # Create curl-format.txt: cat > curl-format.txt << EOF time_namelookup: %{time_namelookup}s\\n time_connect: %{time_connect}s\\n time_appconnect: %{time_appconnect}s\\n time_pretransfer: %{time_pretransfer}s\\n time_redirect: %{time_redirect}s\\n time_starttransfer: %{time_starttransfer}s\\n ----------\\n time_total: %{time_total}s\\n EOF # Trace route to API endpoint traceroute api.cloudflow.io # Check DNS resolution time dig api.cloudflow.io | grep \"Query time\" ``` ### Memory Leaks #### Detection ```bash # Monitor CloudFlow service memory usage kubectl top pods -n cloudflow --sort-by=memory # Get detailed memory metrics for specific pod kubectl exec -n cloudflow deploy/cloudflow-api -- \\ curl localhost:9090/metrics | grep memory # Check for OOMKilled pods kubectl get pods -n cloudflow --field-selector=status.phase=Failed | \\ grep OOMKilled # Review memory limits and requests kubectl describe deployment cloudflow-api -n cloudflow | \\ grep -A 5 \"Limits\\|Requests\" ``` #### Common Causes",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_0",
              "content": "# CloudFlow Platform - System Architecture Overview **Document Version:** 2.3.1 **Last Updated:** January 15, 2026 **Owner:** Platform Architecture Team **Status:** Production ## Executive Summary CloudFlow is a distributed, cloud-native workflow automation platform designed to orchestrate complex business processes at scale. The platform processes over 2.5 million workflow executions daily with an average P99 latency of 180ms for API operations and 4.2 seconds for workflow execution. This document provides a comprehensive overview of the system architecture, including microservices design, data flow patterns, infrastructure decisions, and operational considerations. ## Table of Contents 1. [High-Level Architecture](#high-level-architecture) 2. [Microservices Breakdown](#microservices-breakdown) 3. [Data Flow Architecture](#data-flow-architecture) 4. [Database Architecture](#database-architecture) 5. [Message Queue Patterns](#message-queue-patterns) 6. [Caching Strategy](#caching-strategy) 7. [Security Architecture](#security-architecture) 8. [Performance Characteristics](#performance-characteristics) 9. [Disaster Recovery](#disaster-recovery) --- ## High-Level Architecture CloudFlow follows a microservices architecture pattern deployed across multiple availability zones in AWS.",
              "score": null
            },
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_4",
              "content": "verbosity | `info`, `debug`, `warn`, `error` | No (default: `info`) | | `API_PORT` | API server port | `3000` | No (default: `3000`) | | `WORKER_CONCURRENCY` | Number of concurrent workers | `10` | No (default: `5`) | | `SESSION_SECRET` | Session encryption key | `<generated-secret-256-bit>` | Yes | | `AWS_REGION` | AWS region for services | `us-east-1` | Yes | | `S3_BUCKET` | S3 bucket for file storage | `cloudflow-prod-storage` | Yes | | `SMTP_HOST` | Email server hostname | `smtp.sendgrid.net` | No | | `SMTP_PORT` | Email server port | `587` | No | | `METRICS_ENABLED` | Enable Prometheus metrics | `true` | No (default: `false`) | Create a Kubernetes secret for sensitive variables: ```bash kubectl create secret generic cloudflow-secrets \\ --namespace cloudflow-prod \\ --from-literal=DATABASE_URL=\"postgresql://cloudflow:$(cat db-password.txt)@pgbouncer.cloudflow-prod.svc.cluster.local:5432/cloudflow\" \\ --from-literal=REDIS_URL=\"redis://redis-master.cloudflow-prod.svc.cluster.local:6379\" \\ --from-literal=JWT_SECRET=\"$(openssl rand -base64 32)\" \\ --from-literal=SESSION_SECRET=\"$(openssl rand -base64 32)\" ``` ### Helm Chart Deployment Add the CloudFlow Helm repository:",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_26",
              "content": "Major functionality impaired affecting multiple users - **Examples:** - Workflow execution success rate < 90% - Significant performance degradation (p95 latency > 10s) - Rate limiting affecting large customer segment - **Response Time:** < 1 hour - **Escalation:** Create incident ticket and notify on-call #### SEV-3: Medium (P3) - **Definition:** Partial functionality degraded affecting some users - **Examples:** - Intermittent failures for specific workflow types - Minor performance issues - Non-critical feature unavailable - **Response Time:** < 4 hours - **Escalation:** Create ticket, normal business hours support #### SEV-4: Low (P4) - **Definition:** Minor issues with minimal user impact - **Examples:** - Cosmetic issues - Documentation errors - Feature requests - **Response Time:** < 2 business days - **Escalation:** Standard support ticket ### Escalation Steps #### Step 1: Initial Assessment (0-5 minutes) ```bash # Run health check cloudflow health check --comprehensive # Check status page curl https://status.cloudflow.io/api/v1/status.json # Review recent",
              "score": null
            },
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_3",
              "content": "## Rate Limiting To ensure fair usage and system stability, CloudFlow enforces rate limits on all API endpoints. **Default Limits:** - 100 requests per minute per authenticated user - 20 requests per minute for unauthenticated requests - Burst allowance: 150 requests in a 10-second window ### Rate Limit Headers Every API response includes rate limit information: ``` X-RateLimit-Limit: 100 X-RateLimit-Remaining: 87 X-RateLimit-Reset: 1640995200 ``` When you exceed the rate limit, you'll receive a `429 Too Many Requests` response: ```json { \"error\": { \"code\": \"rate_limit_exceeded\", \"message\": \"Rate limit exceeded. Please retry after 42 seconds.\", \"retry_after\": 42 } } ``` **Best Practices:** - Monitor `X-RateLimit-Remaining` header values - Implement exponential backoff when receiving 429 responses - Cache responses when appropriate to reduce API calls - Consider upgrading to Enterprise tier for higher limits (1000 req/min) ## Pagination List endpoints return paginated results to optimize performance.",
              "score": null
            }
          ],
          "latency_ms": 5.470284000693937,
          "query_id": "realistic_009",
          "dimension": "contextual",
          "query": "Setting SLOs for our service, what P99 latency does CloudFlow guarantee?"
        },
        {
          "key_facts": [
            "P99 latency: < 200ms",
            "average P99 latency of 180ms for API operations"
          ],
          "found_facts": [],
          "missed_facts": [
            "P99 latency: < 200ms",
            "average P99 latency of 180ms for API operations"
          ],
          "coverage": 0.0,
          "expected_docs": [
            "architecture_overview"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_26",
              "content": "peak - Egress: 1.5 Gbps average, 4 Gbps peak - Internal (service mesh): 8 Gbps average ### Monitoring & Alerting **Key Metrics** (Monitored via Prometheus): - Request rate (per service, per endpoint) - Error rate (4xx, 5xx responses) - Latency percentiles (P50, P95, P99) - Resource utilization (CPU, memory, disk) - Cache hit/miss ratios - Database connection pool saturation - Kafka consumer lag **Alerts** (via PagerDuty): - P1 (Immediate): API error rate > 1%, database connection failure - P2 (< 15 min): P99 latency > 500ms, cache hit rate < 80% - P3 (< 1 hour): Resource utilization > 85%, replica count < minimum **SLI/SLO/SLA**: - SLI: API success rate (non-5xx responses) - SLO: 99.9% uptime per month (43 minutes downtime allowance) - SLA: 99.5% uptime guarantee (customer-facing SLA with credits) --- ## Disaster Recovery ### Recovery Objectives **RPO (Recovery Point Objective): 1 hour** - Maximum acceptable data loss: 1",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_7",
              "content": "# - Business Logic: 32ms (13%) # - Response Serialization: 12ms (5%) ``` **Network latency issues:** ```bash # Test connectivity to CloudFlow API time curl -w \"@curl-format.txt\" -o /dev/null -s https://api.cloudflow.io/health # Create curl-format.txt: cat > curl-format.txt << EOF time_namelookup: %{time_namelookup}s\\n time_connect: %{time_connect}s\\n time_appconnect: %{time_appconnect}s\\n time_pretransfer: %{time_pretransfer}s\\n time_redirect: %{time_redirect}s\\n time_starttransfer: %{time_starttransfer}s\\n ----------\\n time_total: %{time_total}s\\n EOF # Trace route to API endpoint traceroute api.cloudflow.io # Check DNS resolution time dig api.cloudflow.io | grep \"Query time\" ``` ### Memory Leaks #### Detection ```bash # Monitor CloudFlow service memory usage kubectl top pods -n cloudflow --sort-by=memory # Get detailed memory metrics for specific pod kubectl exec -n cloudflow deploy/cloudflow-api -- \\ curl localhost:9090/metrics | grep memory # Check for OOMKilled pods kubectl get pods -n cloudflow --field-selector=status.phase=Failed | \\ grep OOMKilled # Review memory limits and requests kubectl describe deployment cloudflow-api -n cloudflow | \\ grep -A 5 \"Limits\\|Requests\" ``` #### Common Causes",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_9",
              "content": "to detect leader failure (5-second interval) - Automatic failover to standby scheduler (< 10 seconds) - Schedule versioning to handle updates during execution - Missed execution policy: SKIP, RUN_ONCE, or RUN_ALL **Performance Targets**: - Schedule evaluation: < 100ms per cycle - Accuracy: \u00b1 1 second for schedule triggers - Capacity: 100,000 active schedules --- ### Notification Service **Purpose**: Multi-channel notification delivery system supporting email, SMS, webhooks, and in-app notifications. **Technology**: Node.js with worker pool pattern **Replicas**: 8 pods (production), auto-scaling 6-16 **Resource Allocation**: 2 vCPU, 4GB RAM per pod **Key Responsibilities**: - Consume notification events from Kafka - Template rendering with Handlebars (cached in Redis) - Multi-channel delivery (Email via SendGrid, SMS via Twilio, Webhooks) - Retry logic with dead letter queue for failed deliveries - Delivery status tracking and analytics - User preference management (opt-in/opt-out) **Notification Flow**: ``` Event Source \u2192 Kafka (notifications.email topic) \u2502 \u25bc Notification Service Consumer",
              "score": null
            },
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_12",
              "content": "Waiting {retry_after} seconds...\") time.sleep(retry_after) continue elif response.status_code >= 500: # Server error - retry with exponential backoff wait_time = 2 ** attempt print(f\"Server error. Retrying in {wait_time} seconds...\") time.sleep(wait_time) continue else: # Client error - don't retry error_data = response.json() raise Exception(f\"API Error: {error_data['error']['message']}\") except requests.exceptions.RequestException as e: print(f\"Request failed: {e}\") if attempt < max_retries - 1: time.sleep(2 ** attempt) else: raise raise Exception(\"Max retries exceeded\") ``` ## Webhooks CloudFlow can send webhook notifications when specific events occur in your workflows.",
              "score": null
            },
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_16",
              "content": "}, ], thresholds: { http_req_duration: ['p(95)<500'], }, }; export default function () { let res = http.get('https://api.cloudflow.io/api/workflows'); check(res, { 'status is 200': (r) => r.status === 200 }); sleep(1); } EOF ``` **Performance Targets**: - P95 API latency: < 500ms - P99 API latency: < 1000ms - Error rate: < 0.1% - Throughput: > 1000 requests/second --- ## Security and Compliance ### Network Policies Implement network policies to restrict pod-to-pod communication: ```yaml # network-policy.yaml apiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: cloudflow-network-policy namespace: cloudflow-prod spec: podSelector: matchLabels: app: cloudflow policyTypes: - Ingress - Egress ingress: - from: - namespaceSelector: matchLabels: name: cloudflow-prod - podSelector: matchLabels: app: nginx-ingress ports: - protocol: TCP port: 3000 egress: - to: - podSelector: matchLabels: app: postgresql ports: - protocol: TCP port: 5432 - to: - podSelector: matchLabels: app: redis ports: - protocol: TCP port: 6379 ``` ### Pod Security Standards Enforce Pod Security Standards at the",
              "score": null
            }
          ],
          "latency_ms": 5.446260000098846,
          "query_id": "realistic_009",
          "dimension": "negation",
          "query": "Why are we getting alerts at 200ms latency?"
        },
        {
          "key_facts": [
            "RPO (Recovery Point Objective): 1 hour",
            "RTO (Recovery Time Objective): 4 hours"
          ],
          "found_facts": [
            "RTO (Recovery Time Objective): 4 hours"
          ],
          "missed_facts": [
            "RPO (Recovery Point Objective): 1 hour"
          ],
          "coverage": 0.5,
          "expected_docs": [
            "architecture_overview",
            "deployment_guide"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_30",
              "content": "Update monitoring dashboards - Recovery time: 2-4 hours (includes verification) - Data loss: < 1 hour (last cross-region replication) **Scenario 4: Data Corruption** - Detection: Data validation checks or user report - Action: Point-in-time recovery from WAL archives - Procedure: 1. Identify corruption time window 2. Restore from snapshot prior to corruption 3. Replay WAL logs up to corruption point 4. Verify data integrity 5. Resume normal operations - Recovery time: 1-3 hours depending on data volume - Data loss: None if corruption detected quickly ### Testing & Validation **DR Drill Schedule**: - Monthly: Automated failover test (single AZ failure simulation) - Quarterly: Full DR region failover (non-production hours) - Annually: Complete disaster simulation with all stakeholders **Last DR Test Results** (Dec 15, 2025): - Scenario: Full region failover - Actual RTO: 2 hours 23 minutes (target: 4 hours) - Actual RPO: 42 minutes (target: 1 hour) - Issues identified:",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_27",
              "content": "hour of transactions - Achieved through: Continuous database replication + hourly snapshots - Kafka retention: 7 days allows event replay **RTO (Recovery Time Objective): 4 hours** - Maximum acceptable downtime: 4 hours for full system recovery - Includes: Failover, data verification, and service restoration - Automated runbooks reduce RTO to < 2 hours for common scenarios ### High Availability Architecture **Multi-AZ Deployment**: ``` Region: us-east-1 AZ-1a: AZ-1b: AZ-1c: - API Gateway (4) - API Gateway (4) - API Gateway (4) - Workflow Engine (6) - Workflow Engine (5) - Workflow Engine (5) - Auth Service (3) - Auth Service (3) - Auth Service (2) - PostgreSQL Primary - PostgreSQL Replica - PostgreSQL Replica - Redis Primary (2) - Redis Replica (2) - Redis Replica (2) - Kafka Broker (2) - Kafka Broker (2) - Kafka Broker (1) ``` **Automatic Failover**: - Database: 30-60 seconds (automatic promotion of replica) - Redis:",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_7",
              "content": "\"retry\": {\"max_attempts\": 3, \"backoff\": \"exponential\"} } }, { \"id\": \"create-account\", \"type\": \"database_operation\", \"depends_on\": [\"validate-customer\"] } ] } ``` **State Management**: - Workflow states: PENDING, RUNNING, COMPLETED, FAILED, CANCELLED - Step states: QUEUED, EXECUTING, SUCCESS, FAILED, SKIPPED - State transitions stored in PostgreSQL with event sourcing pattern - Checkpointing every 10 steps for long-running workflows **Performance Targets**: - Simple workflow (3-5 steps): < 2 seconds (P99) - Complex workflow (20+ steps): < 5 seconds (P99) - Throughput: 500 concurrent workflow executions per pod --- ### Scheduler Service **Purpose**: Time-based workflow triggering system supporting cron-like schedules and one-time delayed executions.",
              "score": null
            },
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_14",
              "content": "**Verify Service Health**: ```bash kubectl get pods -n cloudflow-prod curl https://api.cloudflow.io/health ``` **Recovery Time Objective (RTO)**: 4 hours **Recovery Point Objective (RPO)**: 24 hours --- ## Scaling and Performance ### Horizontal Pod Autoscaling CloudFlow is configured with Horizontal Pod Autoscaler (HPA) to automatically scale based on resource utilization: ```yaml # hpa.yaml apiVersion: autoscaling/v2 kind: HorizontalPodAutoscaler metadata: name: cloudflow-api-hpa namespace: cloudflow-prod spec: scaleTargetRef: apiVersion: apps/v1 kind: Deployment name: cloudflow-api minReplicas: 3 maxReplicas: 10 metrics: - type: Resource resource: name: cpu target: type: Utilization averageUtilization: 70 - type: Resource resource: name: memory target: type: Utilization averageUtilization: 80 behavior: scaleDown: stabilizationWindowSeconds: 300 policies: - type: Percent value: 50 periodSeconds: 60 scaleUp: stabilizationWindowSeconds: 60 policies: - type: Percent value: 100 periodSeconds: 30 - type: Pods value: 2 periodSeconds: 30 selectPolicy: Max ``` Monitor autoscaling events: ```bash kubectl get hpa -n cloudflow-prod -w kubectl describe hpa cloudflow-api-hpa -n cloudflow-prod ``` ### Performance Optimization #### Connection",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_28",
              "content": "``` **For SEV-2 (High):** ```bash # Create incident and notify cloudflow incident create \\ --severity SEV-2 \\ --title \"High rate of workflow failures\" \\ --description \"Workflow success rate dropped to 75%\" \\ --notify-oncall # Update status page cloudflow status update \\ --status \"partial_outage\" \\ --message \"We are experiencing elevated error rates\" ``` **For SEV-3/SEV-4:** ```bash # Create support ticket cloudflow support ticket create \\ --priority MEDIUM \\ --subject \"Intermittent timeout errors\" \\ --description \"See attached logs and reproduction steps\" ``` ### Contact Information - **On-call hotline:** +1-888-CLOUDFLOW (24/7) - **Support email:** support@cloudflow.io - **Incident Slack channel:** #cloudflow-incidents - **Status page:** https://status.cloudflow.io - **Documentation:** https://docs.cloudflow.io ### Post-Incident Review After resolving incidents, complete a postmortem: ```bash # Generate postmortem template cloudflow incident postmortem incident-2024012401 \\ --template standard \\ --output postmortem-2024012401.md # Required sections: # 1. Incident summary # 2. Impact assessment # 3. Timeline of events # 4.",
              "score": null
            }
          ],
          "latency_ms": 5.861203000677051,
          "query_id": "realistic_010",
          "dimension": "original",
          "query": "What are the disaster recovery RPO and RTO values?"
        },
        {
          "key_facts": [
            "RPO (Recovery Point Objective): 1 hour",
            "RTO (Recovery Time Objective): 4 hours"
          ],
          "found_facts": [
            "RTO (Recovery Time Objective): 4 hours"
          ],
          "missed_facts": [
            "RPO (Recovery Point Objective): 1 hour"
          ],
          "coverage": 0.5,
          "expected_docs": [
            "architecture_overview",
            "deployment_guide"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_30",
              "content": "Update monitoring dashboards - Recovery time: 2-4 hours (includes verification) - Data loss: < 1 hour (last cross-region replication) **Scenario 4: Data Corruption** - Detection: Data validation checks or user report - Action: Point-in-time recovery from WAL archives - Procedure: 1. Identify corruption time window 2. Restore from snapshot prior to corruption 3. Replay WAL logs up to corruption point 4. Verify data integrity 5. Resume normal operations - Recovery time: 1-3 hours depending on data volume - Data loss: None if corruption detected quickly ### Testing & Validation **DR Drill Schedule**: - Monthly: Automated failover test (single AZ failure simulation) - Quarterly: Full DR region failover (non-production hours) - Annually: Complete disaster simulation with all stakeholders **Last DR Test Results** (Dec 15, 2025): - Scenario: Full region failover - Actual RTO: 2 hours 23 minutes (target: 4 hours) - Actual RPO: 42 minutes (target: 1 hour) - Issues identified:",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_29",
              "content": "minutes (no manual intervention) - Data loss: None (multi-AZ replication) **Scenario 2: Database Primary Failure** - Detection: Health check fails for primary database (< 30 seconds) - Action: Automatic promotion of read replica to primary - Recovery time: 30-60 seconds - Data loss: Minimal (< 1 second due to synchronous replication) **Scenario 3: Full Region Failure** - Detection: Multiple health check failures across all AZs (< 2 minutes) - Action: Manual failover to DR region (us-west-2) - Procedure: 1. Update DNS to point to DR region (TTL: 60 seconds) 2. Promote DR database replica to primary 3. Scale up DR region services to production capacity 4. Verify data consistency and integrity 5.",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_31",
              "content": "DNS propagation slower than expected (resolved) - Success criteria: Met all recovery objectives ### Business Continuity **Communication Plan**: - Status page: status.cloudflow.com (updated every 15 minutes during incident) - Customer notifications: Email + SMS for all P1 incidents - Internal escalation: PagerDuty \u2192 Incident Commander \u2192 Engineering Manager \u2192 CTO **Data Retention Policy**: - Active data: 12 months in hot storage (PostgreSQL) - Archived data: 7 years in cold storage (S3 Glacier) - Audit logs: 7 years (compliance requirement) - Backup retention: 30 days standard, 90 days monthly snapshots --- ## Appendix ### Service Dependency Matrix | Service | Depends On | Critical Path | |---------|-----------|---------------| | API Gateway | Auth Service, Redis | Yes | | Auth Service | PostgreSQL, Redis, Vault | Yes | | Workflow Engine | PostgreSQL, Kafka, Redis | Yes | | Scheduler | PostgreSQL, Redis, Kafka | No | | Notification Service | Kafka,",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_27",
              "content": "hour of transactions - Achieved through: Continuous database replication + hourly snapshots - Kafka retention: 7 days allows event replay **RTO (Recovery Time Objective): 4 hours** - Maximum acceptable downtime: 4 hours for full system recovery - Includes: Failover, data verification, and service restoration - Automated runbooks reduce RTO to < 2 hours for common scenarios ### High Availability Architecture **Multi-AZ Deployment**: ``` Region: us-east-1 AZ-1a: AZ-1b: AZ-1c: - API Gateway (4) - API Gateway (4) - API Gateway (4) - Workflow Engine (6) - Workflow Engine (5) - Workflow Engine (5) - Auth Service (3) - Auth Service (3) - Auth Service (2) - PostgreSQL Primary - PostgreSQL Replica - PostgreSQL Replica - Redis Primary (2) - Redis Replica (2) - Redis Replica (2) - Kafka Broker (2) - Kafka Broker (2) - Kafka Broker (1) ``` **Automatic Failover**: - Database: 30-60 seconds (automatic promotion of replica) - Redis:",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_16",
              "content": "(created, started, completed, failed) - Retention: 7 days - Message rate: 5,000/sec peak - Consumer groups: workflow-engine, analytics-pipeline notifications.email (16 partitions): - Email notification triggers - Retention: 3 days - Message rate: 2,000/sec peak - Consumer groups: notification-service notifications.sms (8 partitions): - SMS notification triggers - Retention: 3 days - Message rate: 500/sec peak - Consumer groups: notification-service audit.logs (24 partitions): - Security and compliance audit events - Retention: 90 days (compliance requirement) - Message rate: 3,000/sec peak - Consumer groups: audit-processor, security-monitor dead-letter-queue (8 partitions): - Failed message processing - Retention: 30 days - Manual intervention required - Consumer groups: ops-team-alerts ``` **Producer Configuration**: - Acknowledgment: `acks=all` (wait for all in-sync replicas) - Compression: LZ4 (reduces network bandwidth by 60%) - Batching: 100ms linger time, 100KB batch size - Idempotence: Enabled to prevent duplicates **Consumer Configuration**: - Auto-commit: Disabled (manual commit after processing) - Offset reset: Earliest (replay from",
              "score": null
            }
          ],
          "latency_ms": 5.565122000916745,
          "query_id": "realistic_010",
          "dimension": "synonym",
          "query": "What's the maximum data loss and recovery time?"
        },
        {
          "key_facts": [
            "RPO (Recovery Point Objective): 1 hour",
            "RTO (Recovery Time Objective): 4 hours"
          ],
          "found_facts": [],
          "missed_facts": [
            "RPO (Recovery Point Objective): 1 hour",
            "RTO (Recovery Time Objective): 4 hours"
          ],
          "coverage": 0.0,
          "expected_docs": [
            "architecture_overview",
            "deployment_guide"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_30",
              "content": "Update monitoring dashboards - Recovery time: 2-4 hours (includes verification) - Data loss: < 1 hour (last cross-region replication) **Scenario 4: Data Corruption** - Detection: Data validation checks or user report - Action: Point-in-time recovery from WAL archives - Procedure: 1. Identify corruption time window 2. Restore from snapshot prior to corruption 3. Replay WAL logs up to corruption point 4. Verify data integrity 5. Resume normal operations - Recovery time: 1-3 hours depending on data volume - Data loss: None if corruption detected quickly ### Testing & Validation **DR Drill Schedule**: - Monthly: Automated failover test (single AZ failure simulation) - Quarterly: Full DR region failover (non-production hours) - Annually: Complete disaster simulation with all stakeholders **Last DR Test Results** (Dec 15, 2025): - Scenario: Full region failover - Actual RTO: 2 hours 23 minutes (target: 4 hours) - Actual RPO: 42 minutes (target: 1 hour) - Issues identified:",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_29",
              "content": "minutes (no manual intervention) - Data loss: None (multi-AZ replication) **Scenario 2: Database Primary Failure** - Detection: Health check fails for primary database (< 30 seconds) - Action: Automatic promotion of read replica to primary - Recovery time: 30-60 seconds - Data loss: Minimal (< 1 second due to synchronous replication) **Scenario 3: Full Region Failure** - Detection: Multiple health check failures across all AZs (< 2 minutes) - Action: Manual failover to DR region (us-west-2) - Procedure: 1. Update DNS to point to DR region (TTL: 60 seconds) 2. Promote DR database replica to primary 3. Scale up DR region services to production capacity 4. Verify data consistency and integrity 5.",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_31",
              "content": "DNS propagation slower than expected (resolved) - Success criteria: Met all recovery objectives ### Business Continuity **Communication Plan**: - Status page: status.cloudflow.com (updated every 15 minutes during incident) - Customer notifications: Email + SMS for all P1 incidents - Internal escalation: PagerDuty \u2192 Incident Commander \u2192 Engineering Manager \u2192 CTO **Data Retention Policy**: - Active data: 12 months in hot storage (PostgreSQL) - Archived data: 7 years in cold storage (S3 Glacier) - Audit logs: 7 years (compliance requirement) - Backup retention: 30 days standard, 90 days monthly snapshots --- ## Appendix ### Service Dependency Matrix | Service | Depends On | Critical Path | |---------|-----------|---------------| | API Gateway | Auth Service, Redis | Yes | | Auth Service | PostgreSQL, Redis, Vault | Yes | | Workflow Engine | PostgreSQL, Kafka, Redis | Yes | | Scheduler | PostgreSQL, Redis, Kafka | No | | Notification Service | Kafka,",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_28",
              "content": "< 10 seconds (Sentinel-based failover) - Kafka: < 30 seconds (controller election) - Services: Kubernetes health checks with 10-second liveness probes ### Backup Strategy **Database Backups**: - Automated snapshots: Daily at 02:00 UTC - Retention: 30 days for daily, 90 days for monthly - Cross-region replication: Async replication to us-west-2 (15-minute lag) - Backup verification: Weekly automated restore test in staging environment **Backup Schedule**: ``` Daily: Full snapshot \u2192 S3 (encrypted) Hourly: WAL archives \u2192 S3 (point-in-time recovery) Weekly: Backup validation test Monthly: Long-term archive to Glacier ``` **Configuration Backups**: - Kubernetes manifests: Stored in Git (GitOps with ArgoCD) - Vault secrets: Automated snapshot every 6 hours - Kafka topic configurations: Exported daily to S3 ### Disaster Recovery Procedures **Scenario 1: Single AZ Failure** - Detection: Health checks fail for entire AZ (< 30 seconds) - Action: Traffic automatically routed to healthy AZs by ALB - Recovery time: < 5",
              "score": null
            },
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_12",
              "content": "Waiting {retry_after} seconds...\") time.sleep(retry_after) continue elif response.status_code >= 500: # Server error - retry with exponential backoff wait_time = 2 ** attempt print(f\"Server error. Retrying in {wait_time} seconds...\") time.sleep(wait_time) continue else: # Client error - don't retry error_data = response.json() raise Exception(f\"API Error: {error_data['error']['message']}\") except requests.exceptions.RequestException as e: print(f\"Request failed: {e}\") if attempt < max_retries - 1: time.sleep(2 ** attempt) else: raise raise Exception(\"Max retries exceeded\") ``` ## Webhooks CloudFlow can send webhook notifications when specific events occur in your workflows.",
              "score": null
            }
          ],
          "latency_ms": 5.467199000122491,
          "query_id": "realistic_010",
          "dimension": "problem",
          "query": "How long will it take to recover from a disaster?"
        },
        {
          "key_facts": [
            "RPO (Recovery Point Objective): 1 hour",
            "RTO (Recovery Time Objective): 4 hours"
          ],
          "found_facts": [
            "RTO (Recovery Time Objective): 4 hours"
          ],
          "missed_facts": [
            "RPO (Recovery Point Objective): 1 hour"
          ],
          "coverage": 0.5,
          "expected_docs": [
            "architecture_overview",
            "deployment_guide"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_30",
              "content": "Update monitoring dashboards - Recovery time: 2-4 hours (includes verification) - Data loss: < 1 hour (last cross-region replication) **Scenario 4: Data Corruption** - Detection: Data validation checks or user report - Action: Point-in-time recovery from WAL archives - Procedure: 1. Identify corruption time window 2. Restore from snapshot prior to corruption 3. Replay WAL logs up to corruption point 4. Verify data integrity 5. Resume normal operations - Recovery time: 1-3 hours depending on data volume - Data loss: None if corruption detected quickly ### Testing & Validation **DR Drill Schedule**: - Monthly: Automated failover test (single AZ failure simulation) - Quarterly: Full DR region failover (non-production hours) - Annually: Complete disaster simulation with all stakeholders **Last DR Test Results** (Dec 15, 2025): - Scenario: Full region failover - Actual RTO: 2 hours 23 minutes (target: 4 hours) - Actual RPO: 42 minutes (target: 1 hour) - Issues identified:",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_27",
              "content": "hour of transactions - Achieved through: Continuous database replication + hourly snapshots - Kafka retention: 7 days allows event replay **RTO (Recovery Time Objective): 4 hours** - Maximum acceptable downtime: 4 hours for full system recovery - Includes: Failover, data verification, and service restoration - Automated runbooks reduce RTO to < 2 hours for common scenarios ### High Availability Architecture **Multi-AZ Deployment**: ``` Region: us-east-1 AZ-1a: AZ-1b: AZ-1c: - API Gateway (4) - API Gateway (4) - API Gateway (4) - Workflow Engine (6) - Workflow Engine (5) - Workflow Engine (5) - Auth Service (3) - Auth Service (3) - Auth Service (2) - PostgreSQL Primary - PostgreSQL Replica - PostgreSQL Replica - Redis Primary (2) - Redis Replica (2) - Redis Replica (2) - Kafka Broker (2) - Kafka Broker (2) - Kafka Broker (1) ``` **Automatic Failover**: - Database: 30-60 seconds (automatic promotion of replica) - Redis:",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_8",
              "content": "**Technology**: Go with distributed locking via Redis **Replicas**: 4 pods (production), active-passive with leader election **Resource Allocation**: 2 vCPU, 4GB RAM per pod **Key Responsibilities**: - Parse and validate cron expressions (extended format supporting seconds) - Maintain schedule registry in PostgreSQL - Distributed scheduling with leader election (one active scheduler) - Missed execution handling with configurable catch-up policy - Schedule conflict detection and resolution - Time zone support for international schedules **Scheduling Architecture**: ``` PostgreSQL Schedules Table \u2502 \u25bc Leader Scheduler (elected via Redis) \u2502 \u251c\u2500 Scan for due schedules (every 10 seconds) \u251c\u2500 Acquire lock per schedule (prevents duplicates) \u251c\u2500 Publish to Kafka \u2192 workflow.events topic \u2514\u2500 Update last_run timestamp ``` **Schedule Types**: - **Cron-based**: `0 */5 * * * *` (every 5 minutes) - **One-time**: Specific timestamp for delayed execution - **Interval-based**: Every N seconds/minutes/hours **Reliability Features**: - Leader election using Redis with 30-second lease - Heartbeat mechanism",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_3",
              "content": "**Technology**: Node.js with Express.js framework **Replicas**: 12 pods (production), auto-scaling 8-20 based on CPU **Resource Allocation**: 2 vCPU, 4GB RAM per pod **Key Responsibilities**: - JWT token validation (delegated to Auth Service for initial validation) - Rate limiting: 1000 requests per minute per API key (sliding window) - Request/response transformation and validation using JSON Schema - Routing to downstream services based on URL path patterns - CORS handling for web clients - Request/response logging and correlation ID injection **Critical Endpoints**: - `POST /api/v1/workflows` - Create new workflow - `GET /api/v1/workflows/:id` - Retrieve workflow status - `POST /api/v1/workflows/:id/execute` - Trigger workflow execution - `GET /api/v1/workflows/:id/history` - Get execution history **Dependencies**: - Auth Service (for token validation) - Redis (for rate limiting counters) - All downstream microservices **Performance Targets**: - P50 latency: < 50ms - P99 latency: < 200ms - Throughput: 10,000 RPS sustained --- ### Auth Service **Purpose**: Centralized authentication and",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_7",
              "content": "\"retry\": {\"max_attempts\": 3, \"backoff\": \"exponential\"} } }, { \"id\": \"create-account\", \"type\": \"database_operation\", \"depends_on\": [\"validate-customer\"] } ] } ``` **State Management**: - Workflow states: PENDING, RUNNING, COMPLETED, FAILED, CANCELLED - Step states: QUEUED, EXECUTING, SUCCESS, FAILED, SKIPPED - State transitions stored in PostgreSQL with event sourcing pattern - Checkpointing every 10 steps for long-running workflows **Performance Targets**: - Simple workflow (3-5 steps): < 2 seconds (P99) - Complex workflow (20+ steps): < 5 seconds (P99) - Throughput: 500 concurrent workflow executions per pod --- ### Scheduler Service **Purpose**: Time-based workflow triggering system supporting cron-like schedules and one-time delayed executions.",
              "score": null
            }
          ],
          "latency_ms": 5.935201999818673,
          "query_id": "realistic_010",
          "dimension": "casual",
          "query": "RPO RTO"
        },
        {
          "key_facts": [
            "RPO (Recovery Point Objective): 1 hour",
            "RTO (Recovery Time Objective): 4 hours"
          ],
          "found_facts": [],
          "missed_facts": [
            "RPO (Recovery Point Objective): 1 hour",
            "RTO (Recovery Time Objective): 4 hours"
          ],
          "coverage": 0.0,
          "expected_docs": [
            "architecture_overview",
            "deployment_guide"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_1",
              "content": "### Architecture Summary CloudFlow consists of the following components: - **API Server**: REST API for workflow management (Node.js/Express) - **Worker Service**: Background job processor (Node.js) - **Scheduler**: Cron-based task scheduler (Node.js) - **PostgreSQL**: Primary data store (version 14) - **Redis**: Cache and message queue (version 7.0) - **PgBouncer**: Database connection pooler ### Deployment Model - **Namespace**: `cloudflow-prod` - **Cluster**: AWS EKS 1.28 - **Region**: us-east-1 (primary), us-west-2 (disaster recovery) - **High Availability**: Multi-AZ deployment across 3 availability zones --- ## Prerequisites Before beginning the deployment process, ensure you have the following: ### Required Tools - `kubectl` (v1.28 or later) - `helm` (v3.12 or later) - `aws-cli` (v2.13 or later) - `eksctl` (v0.165 or later) - `terraform` (v1.6 or later) - for infrastructure provisioning ### Access Requirements - AWS account with appropriate IAM permissions - EKS cluster admin access - Container registry access (ECR) - Domain name and SSL certificates -",
              "score": null
            },
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_0",
              "content": "# CloudFlow Platform - Deployment and Operations Guide **Version:** 2.4.0 **Last Updated:** January 2026 **Target Environment:** Production (AWS EKS) ## Table of Contents 1. [Overview](#overview) 2. [Prerequisites](#prerequisites) 3. [Infrastructure Setup](#infrastructure-setup) 4. [Kubernetes Deployment](#kubernetes-deployment) 5. [Database Configuration](#database-configuration) 6. [Monitoring and Observability](#monitoring-and-observability) 7. [Backup and Disaster Recovery](#backup-and-disaster-recovery) 8. [Scaling and Performance](#scaling-and-performance) 9. [Security and Compliance](#security-and-compliance) 10. [Troubleshooting](#troubleshooting) --- ## Overview CloudFlow is a cloud-native workflow orchestration platform designed for high-availability production environments. This guide provides comprehensive instructions for deploying and operating CloudFlow on Amazon EKS (Elastic Kubernetes Service).",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_0",
              "content": "# CloudFlow Platform - System Architecture Overview **Document Version:** 2.3.1 **Last Updated:** January 15, 2026 **Owner:** Platform Architecture Team **Status:** Production ## Executive Summary CloudFlow is a distributed, cloud-native workflow automation platform designed to orchestrate complex business processes at scale. The platform processes over 2.5 million workflow executions daily with an average P99 latency of 180ms for API operations and 4.2 seconds for workflow execution. This document provides a comprehensive overview of the system architecture, including microservices design, data flow patterns, infrastructure decisions, and operational considerations. ## Table of Contents 1. [High-Level Architecture](#high-level-architecture) 2. [Microservices Breakdown](#microservices-breakdown) 3. [Data Flow Architecture](#data-flow-architecture) 4. [Database Architecture](#database-architecture) 5. [Message Queue Patterns](#message-queue-patterns) 6. [Caching Strategy](#caching-strategy) 7. [Security Architecture](#security-architecture) 8. [Performance Characteristics](#performance-characteristics) 9. [Disaster Recovery](#disaster-recovery) --- ## High-Level Architecture CloudFlow follows a microservices architecture pattern deployed across multiple availability zones in AWS.",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_31",
              "content": "DNS propagation slower than expected (resolved) - Success criteria: Met all recovery objectives ### Business Continuity **Communication Plan**: - Status page: status.cloudflow.com (updated every 15 minutes during incident) - Customer notifications: Email + SMS for all P1 incidents - Internal escalation: PagerDuty \u2192 Incident Commander \u2192 Engineering Manager \u2192 CTO **Data Retention Policy**: - Active data: 12 months in hot storage (PostgreSQL) - Archived data: 7 years in cold storage (S3 Glacier) - Audit logs: 7 years (compliance requirement) - Backup retention: 30 days standard, 90 days monthly snapshots --- ## Appendix ### Service Dependency Matrix | Service | Depends On | Critical Path | |---------|-----------|---------------| | API Gateway | Auth Service, Redis | Yes | | Auth Service | PostgreSQL, Redis, Vault | Yes | | Workflow Engine | PostgreSQL, Kafka, Redis | Yes | | Scheduler | PostgreSQL, Redis, Kafka | No | | Notification Service | Kafka,",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_14",
              "content": "Split workflow into smaller workflows:** ```bash # Create sub-workflows cloudflow workflows create data-pipeline-part1 \\ --steps \"data_ingestion,data_validation\" \\ --timeout 1800 cloudflow workflows create data-pipeline-part2 \\ --steps \"data_transformation,data_export\" \\ --timeout 3600 \\ --trigger workflow_completed \\ --trigger-workflow data-pipeline-part1 ``` ### Retry Logic and Exponential Backoff CloudFlow implements automatic retry with exponential backoff for transient failures: - Max retries: 3 - Initial delay: 1 second - Backoff multiplier: 2 - Max delay: 60 seconds #### Retry Sequence ``` Attempt 1: Immediate Attempt 2: Wait 1s (2^0 * 1s) Attempt 3: Wait 2s (2^1 * 1s) Attempt 4: Wait 4s (2^2 * 1s) ``` #### Configuration ```bash # View current retry settings cloudflow workflows get wf_9k2n4m8p1q --format json | jq '.retry_policy' # Customize retry behavior cloudflow workflows update wf_9k2n4m8p1q \\ --retry-max-attempts 5 \\ --retry-initial-delay 2000 \\ --retry-backoff-multiplier 2 \\ --retry-max-delay 120000 \\ --retry-on-errors \"NETWORK_ERROR,TIMEOUT,RATE_LIMIT\" # Disable retry for specific step cloudflow workflows update wf_9k2n4m8p1q \\",
              "score": null
            }
          ],
          "latency_ms": 5.646924000757281,
          "query_id": "realistic_010",
          "dimension": "contextual",
          "query": "For our business continuity plan, what are CloudFlow's recovery objectives?"
        },
        {
          "key_facts": [
            "RPO (Recovery Point Objective): 1 hour",
            "RTO (Recovery Time Objective): 4 hours"
          ],
          "found_facts": [],
          "missed_facts": [
            "RPO (Recovery Point Objective): 1 hour",
            "RTO (Recovery Time Objective): 4 hours"
          ],
          "coverage": 0.0,
          "expected_docs": [
            "architecture_overview",
            "deployment_guide"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_31",
              "content": "DNS propagation slower than expected (resolved) - Success criteria: Met all recovery objectives ### Business Continuity **Communication Plan**: - Status page: status.cloudflow.com (updated every 15 minutes during incident) - Customer notifications: Email + SMS for all P1 incidents - Internal escalation: PagerDuty \u2192 Incident Commander \u2192 Engineering Manager \u2192 CTO **Data Retention Policy**: - Active data: 12 months in hot storage (PostgreSQL) - Archived data: 7 years in cold storage (S3 Glacier) - Audit logs: 7 years (compliance requirement) - Backup retention: 30 days standard, 90 days monthly snapshots --- ## Appendix ### Service Dependency Matrix | Service | Depends On | Critical Path | |---------|-----------|---------------| | API Gateway | Auth Service, Redis | Yes | | Auth Service | PostgreSQL, Redis, Vault | Yes | | Workflow Engine | PostgreSQL, Kafka, Redis | Yes | | Scheduler | PostgreSQL, Redis, Kafka | No | | Notification Service | Kafka,",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_29",
              "content": "minutes (no manual intervention) - Data loss: None (multi-AZ replication) **Scenario 2: Database Primary Failure** - Detection: Health check fails for primary database (< 30 seconds) - Action: Automatic promotion of read replica to primary - Recovery time: 30-60 seconds - Data loss: Minimal (< 1 second due to synchronous replication) **Scenario 3: Full Region Failure** - Detection: Multiple health check failures across all AZs (< 2 minutes) - Action: Manual failover to DR region (us-west-2) - Procedure: 1. Update DNS to point to DR region (TTL: 60 seconds) 2. Promote DR database replica to primary 3. Scale up DR region services to production capacity 4. Verify data consistency and integrity 5.",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_21",
              "content": "for hot data. --- ## Security Architecture ### Network Security **Zero-Trust Network Model**: - All service-to-service communication encrypted with mTLS - Certificate rotation: Every 90 days (automated via cert-manager) - Certificate authority: Internal PKI with HashiCorp Vault - Network segmentation: Private subnets for services, public subnet for ALB only **Service Mesh (Istio)**: ``` Service A \u2192 Envoy Sidecar (mTLS client cert) \u2502 Encrypted channel \u2502 Envoy Sidecar (mTLS server cert) \u2192 Service B ``` **Firewall Rules**: - Security groups: Deny all by default, explicit allow rules - Ingress: Only ALB can reach API Gateway (port 8080) - Egress: Services can only reach specific dependencies - Database access: Limited to application subnets only ### Authentication & Authorization **JWT Token Validation**: - Algorithm: RS256 (asymmetric signing) - Key rotation: Every 30 days with 7-day overlap period - Public key distribution: JWKS endpoint cached in Redis - Validation: Signature, expiry, issuer, audience claims",
              "score": null
            },
            {
              "doc_id": "user_guide",
              "chunk_id": "user_guide_fix_16",
              "content": "fixing issues - Export failed executions for analysis - Set up alerts for DLQ threshold breaches **Accessing the DLQ:** 1. Navigate to **Workflows** > **[Your Workflow]** > **Dead Letter Queue** 2. Filter by error type, date range, or execution ID 3. Click an execution to view full details 4. Click **\"Retry\"** to reprocess with the same input data ### Error Notifications Get notified when workflows fail: ```yaml workflow: notifications: on_failure: - type: email to: \"ops-team@company.com\" - type: slack channel: \"#alerts\" message: \"Workflow {{workflow.name}} failed: {{error.message}}\" on_success_after_retry: - type: slack channel: \"#monitoring\" message: \"Workflow recovered after {{error.attempts}} attempts\" ``` ## Workflow Limits CloudFlow enforces the following limits to ensure platform stability and performance: ### Steps Per Workflow - **Maximum**: 50 steps per workflow - **Recommendation**: Keep workflows focused and modular. If you need more steps, consider splitting into multiple workflows connected via webhooks.",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_30",
              "content": "Update monitoring dashboards - Recovery time: 2-4 hours (includes verification) - Data loss: < 1 hour (last cross-region replication) **Scenario 4: Data Corruption** - Detection: Data validation checks or user report - Action: Point-in-time recovery from WAL archives - Procedure: 1. Identify corruption time window 2. Restore from snapshot prior to corruption 3. Replay WAL logs up to corruption point 4. Verify data integrity 5. Resume normal operations - Recovery time: 1-3 hours depending on data volume - Data loss: None if corruption detected quickly ### Testing & Validation **DR Drill Schedule**: - Monthly: Automated failover test (single AZ failure simulation) - Quarterly: Full DR region failover (non-production hours) - Annually: Complete disaster simulation with all stakeholders **Last DR Test Results** (Dec 15, 2025): - Scenario: Full region failover - Actual RTO: 2 hours 23 minutes (target: 4 hours) - Actual RPO: 42 minutes (target: 1 hour) - Issues identified:",
              "score": null
            }
          ],
          "latency_ms": 5.651241999657941,
          "query_id": "realistic_010",
          "dimension": "negation",
          "query": "Why can't we guarantee zero data loss?"
        },
        {
          "key_facts": [
            "3600 seconds",
            "exceeded maximum execution time of 3600 seconds"
          ],
          "found_facts": [
            "3600 seconds"
          ],
          "missed_facts": [
            "exceeded maximum execution time of 3600 seconds"
          ],
          "coverage": 0.5,
          "expected_docs": [
            "troubleshooting_guide",
            "user_guide"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "user_guide",
              "chunk_id": "user_guide_fix_17",
              "content": "### Execution Timeout - **Default**: 3600 seconds (60 minutes) - **Behavior**: Workflows exceeding this timeout are automatically terminated - **Custom Timeouts**: Enterprise plans can request custom timeout limits **Setting Step-Level Timeouts:** ```yaml - id: long_running_task action: http_request config: url: \"https://api.example.com/process\" timeout: 300 # 5 minutes for this specific step ``` ### Execution Limits - **Maximum**: 1000 executions per day (per workflow) - **Rate Limiting**: 100 concurrent executions per workflow - **Burst Limit**: 10 executions per second **What happens when limits are reached:** - New executions are queued automatically - Webhook triggers return HTTP 429 (Too Many Requests) - Scheduled executions are skipped (logged in audit trail) - Email notifications sent to workflow owner **Monitoring Usage:** View real-time metrics in your workflow dashboard: - Executions today: 847 / 1000 - Average duration: 12.3s - Success rate: 98.2% - Current queue depth: 3 ### Data Limits - **Maximum request/response size**: 10MB per",
              "score": null
            },
            {
              "doc_id": "user_guide",
              "chunk_id": "user_guide_fix_18",
              "content": "action - **Maximum execution payload**: 50MB total - **Variable value size**: 1MB per variable ### Enterprise Plan Limits Enterprise customers can request increased limits: - Up to 100 steps per workflow - Up to 10,000 executions per day - Up to 7200 second timeout (2 hours) - Priority execution queue - Dedicated capacity allocation Contact sales@cloudflow.io for Enterprise pricing and custom limits. ## Best Practices Follow these best practices to build reliable, maintainable workflows: ### 1. Use Descriptive Names **Good:** - Workflow: \"Sync Customer Data from Salesforce to Database\" - Step: \"validate_customer_email\" **Bad:** - Workflow: \"Workflow 1\" - Step: \"step3\" ### 2.",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_24",
              "content": "Operations** (P99 latency): - `GET /workflows/{id}`: < 50ms (cache hit), < 150ms (cache miss) - `POST /workflows`: < 200ms (includes validation and database write) - `POST /workflows/{id}/execute`: < 100ms (async, returns execution ID) - `GET /workflows/{id}/history`: < 300ms (paginated, 50 records per page) **Workflow Execution** (P99 latency): - Simple workflow (< 5 steps): < 2 seconds - Medium workflow (5-15 steps): < 5 seconds - Complex workflow (> 15 steps): < 15 seconds - Parallel execution: Linear scaling up to 10 concurrent branches **Database Operations** (P99 latency): - Simple SELECT: < 5ms - JOIN query: < 20ms - INSERT/UPDATE: < 10ms - Batch operations: < 100ms (batch size: 100 records) **Cache Operations** (P99 latency): - Redis GET: < 2ms - Redis SET: < 3ms - Redis DEL: < 2ms ### Throughput Capacity **API Gateway**: - Sustained: 10,000 requests per second - Peak: 25,000 requests per second (5-minute burst) - Rate",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_14",
              "content": "Split workflow into smaller workflows:** ```bash # Create sub-workflows cloudflow workflows create data-pipeline-part1 \\ --steps \"data_ingestion,data_validation\" \\ --timeout 1800 cloudflow workflows create data-pipeline-part2 \\ --steps \"data_transformation,data_export\" \\ --timeout 3600 \\ --trigger workflow_completed \\ --trigger-workflow data-pipeline-part1 ``` ### Retry Logic and Exponential Backoff CloudFlow implements automatic retry with exponential backoff for transient failures: - Max retries: 3 - Initial delay: 1 second - Backoff multiplier: 2 - Max delay: 60 seconds #### Retry Sequence ``` Attempt 1: Immediate Attempt 2: Wait 1s (2^0 * 1s) Attempt 3: Wait 2s (2^1 * 1s) Attempt 4: Wait 4s (2^2 * 1s) ``` #### Configuration ```bash # View current retry settings cloudflow workflows get wf_9k2n4m8p1q --format json | jq '.retry_policy' # Customize retry behavior cloudflow workflows update wf_9k2n4m8p1q \\ --retry-max-attempts 5 \\ --retry-initial-delay 2000 \\ --retry-backoff-multiplier 2 \\ --retry-max-delay 120000 \\ --retry-on-errors \"NETWORK_ERROR,TIMEOUT,RATE_LIMIT\" # Disable retry for specific step cloudflow workflows update wf_9k2n4m8p1q \\",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_7",
              "content": "\"retry\": {\"max_attempts\": 3, \"backoff\": \"exponential\"} } }, { \"id\": \"create-account\", \"type\": \"database_operation\", \"depends_on\": [\"validate-customer\"] } ] } ``` **State Management**: - Workflow states: PENDING, RUNNING, COMPLETED, FAILED, CANCELLED - Step states: QUEUED, EXECUTING, SUCCESS, FAILED, SKIPPED - State transitions stored in PostgreSQL with event sourcing pattern - Checkpointing every 10 steps for long-running workflows **Performance Targets**: - Simple workflow (3-5 steps): < 2 seconds (P99) - Complex workflow (20+ steps): < 5 seconds (P99) - Throughput: 500 concurrent workflow executions per pod --- ### Scheduler Service **Purpose**: Time-based workflow triggering system supporting cron-like schedules and one-time delayed executions.",
              "score": null
            }
          ],
          "latency_ms": 5.5750600004103035,
          "query_id": "realistic_011",
          "dimension": "original",
          "query": "What is the maximum workflow execution timeout?"
        },
        {
          "key_facts": [
            "3600 seconds",
            "exceeded maximum execution time of 3600 seconds"
          ],
          "found_facts": [
            "3600 seconds"
          ],
          "missed_facts": [
            "exceeded maximum execution time of 3600 seconds"
          ],
          "coverage": 0.5,
          "expected_docs": [
            "troubleshooting_guide",
            "user_guide"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "user_guide",
              "chunk_id": "user_guide_fix_17",
              "content": "### Execution Timeout - **Default**: 3600 seconds (60 minutes) - **Behavior**: Workflows exceeding this timeout are automatically terminated - **Custom Timeouts**: Enterprise plans can request custom timeout limits **Setting Step-Level Timeouts:** ```yaml - id: long_running_task action: http_request config: url: \"https://api.example.com/process\" timeout: 300 # 5 minutes for this specific step ``` ### Execution Limits - **Maximum**: 1000 executions per day (per workflow) - **Rate Limiting**: 100 concurrent executions per workflow - **Burst Limit**: 10 executions per second **What happens when limits are reached:** - New executions are queued automatically - Webhook triggers return HTTP 429 (Too Many Requests) - Scheduled executions are skipped (logged in audit trail) - Email notifications sent to workflow owner **Monitoring Usage:** View real-time metrics in your workflow dashboard: - Executions today: 847 / 1000 - Average duration: 12.3s - Success rate: 98.2% - Current queue depth: 3 ### Data Limits - **Maximum request/response size**: 10MB per",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_14",
              "content": "Split workflow into smaller workflows:** ```bash # Create sub-workflows cloudflow workflows create data-pipeline-part1 \\ --steps \"data_ingestion,data_validation\" \\ --timeout 1800 cloudflow workflows create data-pipeline-part2 \\ --steps \"data_transformation,data_export\" \\ --timeout 3600 \\ --trigger workflow_completed \\ --trigger-workflow data-pipeline-part1 ``` ### Retry Logic and Exponential Backoff CloudFlow implements automatic retry with exponential backoff for transient failures: - Max retries: 3 - Initial delay: 1 second - Backoff multiplier: 2 - Max delay: 60 seconds #### Retry Sequence ``` Attempt 1: Immediate Attempt 2: Wait 1s (2^0 * 1s) Attempt 3: Wait 2s (2^1 * 1s) Attempt 4: Wait 4s (2^2 * 1s) ``` #### Configuration ```bash # View current retry settings cloudflow workflows get wf_9k2n4m8p1q --format json | jq '.retry_policy' # Customize retry behavior cloudflow workflows update wf_9k2n4m8p1q \\ --retry-max-attempts 5 \\ --retry-initial-delay 2000 \\ --retry-backoff-multiplier 2 \\ --retry-max-delay 120000 \\ --retry-on-errors \"NETWORK_ERROR,TIMEOUT,RATE_LIMIT\" # Disable retry for specific step cloudflow workflows update wf_9k2n4m8p1q \\",
              "score": null
            },
            {
              "doc_id": "user_guide",
              "chunk_id": "user_guide_fix_24",
              "content": "WHERE workflow_id = $2 parameters: - \"{{now()}}\" - \"{{workflow.id}}\" ``` ### Pattern 3: Multi-Step Approval Implement approval workflows with timeouts: ```yaml name: \"Expense Approval Workflow\" trigger: type: webhook method: POST steps: - id: create_approval_request action: database_query config: query: | INSERT INTO approvals (expense_id, amount, requester, status, created_at) VALUES ($1, $2, $3, 'pending', NOW()) RETURNING id parameters: - \"{{trigger.body.expense_id}}\" - \"{{trigger.body.amount}}\" - \"{{trigger.body.requester}}\" - id: notify_manager action: email config: to: \"{{trigger.body.manager_email}}\" subject: \"Expense Approval Required: ${{trigger.body.amount}}\" body: | An expense requires your approval: Amount: ${{trigger.body.amount}} Requester: {{trigger.body.requester}} Description: {{trigger.body.description}} Approve: https://app.company.com/approve/{{steps.create_approval_request.rows[0].id}} Reject: https://app.company.com/reject/{{steps.create_approval_request.rows[0].id}} - id: wait_for_approval action: wait_for_webhook config: timeout: 86400 # 24 hours webhook_path: \"/approval/{{steps.create_approval_request.rows[0].id}}\" - id: process_approval action: javascript code: | if (input.status === 'approved') { return { approved: true }; } else { throw new Error('Expense rejected'); } on_error: - id: notify_rejection action: email config: to: \"{{trigger.body.requester}}\" subject: \"Expense Rejected\" body: \"Your expense request has been rejected.\"",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_13",
              "content": "Step 1: data_ingestion - Duration: 245s - Status: SUCCESS # Step 2: data_validation - Duration: 123s - Status: SUCCESS # Step 3: data_transformation - Duration: 3189s - Status: TIMEOUT # Step 4: data_export - Duration: 0s - Status: SKIPPED # Identify bottleneck step cloudflow workflows analyze exec_7h3j6k9m2n --show-bottlenecks ``` #### Solutions **1. Increase workflow timeout (if justified):** ```bash # Update workflow configuration cloudflow workflows update wf_9k2n4m8p1q \\ --timeout 7200 \\ --reason \"Large dataset processing requires extended time\" # Verify update cloudflow workflows get wf_9k2n4m8p1q | grep timeout ``` **2. Optimize slow steps:** ```bash # Enable parallel processing cloudflow workflows update wf_9k2n4m8p1q \\ --step data_transformation \\ --parallel-workers 8 \\ --batch-size 1000 # Add checkpointing for long operations cloudflow workflows update wf_9k2n4m8p1q \\ --step data_transformation \\ --enable-checkpointing \\ --checkpoint-interval 300 ``` **3.",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_7",
              "content": "\"retry\": {\"max_attempts\": 3, \"backoff\": \"exponential\"} } }, { \"id\": \"create-account\", \"type\": \"database_operation\", \"depends_on\": [\"validate-customer\"] } ] } ``` **State Management**: - Workflow states: PENDING, RUNNING, COMPLETED, FAILED, CANCELLED - Step states: QUEUED, EXECUTING, SUCCESS, FAILED, SKIPPED - State transitions stored in PostgreSQL with event sourcing pattern - Checkpointing every 10 steps for long-running workflows **Performance Targets**: - Simple workflow (3-5 steps): < 2 seconds (P99) - Complex workflow (20+ steps): < 5 seconds (P99) - Throughput: 500 concurrent workflow executions per pod --- ### Scheduler Service **Purpose**: Time-based workflow triggering system supporting cron-like schedules and one-time delayed executions.",
              "score": null
            }
          ],
          "latency_ms": 5.784570001196698,
          "query_id": "realistic_011",
          "dimension": "synonym",
          "query": "How long can a workflow run before timing out?"
        },
        {
          "key_facts": [
            "3600 seconds",
            "exceeded maximum execution time of 3600 seconds"
          ],
          "found_facts": [
            "3600 seconds"
          ],
          "missed_facts": [
            "exceeded maximum execution time of 3600 seconds"
          ],
          "coverage": 0.5,
          "expected_docs": [
            "troubleshooting_guide",
            "user_guide"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "user_guide",
              "chunk_id": "user_guide_fix_17",
              "content": "### Execution Timeout - **Default**: 3600 seconds (60 minutes) - **Behavior**: Workflows exceeding this timeout are automatically terminated - **Custom Timeouts**: Enterprise plans can request custom timeout limits **Setting Step-Level Timeouts:** ```yaml - id: long_running_task action: http_request config: url: \"https://api.example.com/process\" timeout: 300 # 5 minutes for this specific step ``` ### Execution Limits - **Maximum**: 1000 executions per day (per workflow) - **Rate Limiting**: 100 concurrent executions per workflow - **Burst Limit**: 10 executions per second **What happens when limits are reached:** - New executions are queued automatically - Webhook triggers return HTTP 429 (Too Many Requests) - Scheduled executions are skipped (logged in audit trail) - Email notifications sent to workflow owner **Monitoring Usage:** View real-time metrics in your workflow dashboard: - Executions today: 847 / 1000 - Average duration: 12.3s - Success rate: 98.2% - Current queue depth: 3 ### Data Limits - **Maximum request/response size**: 10MB per",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_14",
              "content": "Split workflow into smaller workflows:** ```bash # Create sub-workflows cloudflow workflows create data-pipeline-part1 \\ --steps \"data_ingestion,data_validation\" \\ --timeout 1800 cloudflow workflows create data-pipeline-part2 \\ --steps \"data_transformation,data_export\" \\ --timeout 3600 \\ --trigger workflow_completed \\ --trigger-workflow data-pipeline-part1 ``` ### Retry Logic and Exponential Backoff CloudFlow implements automatic retry with exponential backoff for transient failures: - Max retries: 3 - Initial delay: 1 second - Backoff multiplier: 2 - Max delay: 60 seconds #### Retry Sequence ``` Attempt 1: Immediate Attempt 2: Wait 1s (2^0 * 1s) Attempt 3: Wait 2s (2^1 * 1s) Attempt 4: Wait 4s (2^2 * 1s) ``` #### Configuration ```bash # View current retry settings cloudflow workflows get wf_9k2n4m8p1q --format json | jq '.retry_policy' # Customize retry behavior cloudflow workflows update wf_9k2n4m8p1q \\ --retry-max-attempts 5 \\ --retry-initial-delay 2000 \\ --retry-backoff-multiplier 2 \\ --retry-max-delay 120000 \\ --retry-on-errors \"NETWORK_ERROR,TIMEOUT,RATE_LIMIT\" # Disable retry for specific step cloudflow workflows update wf_9k2n4m8p1q \\",
              "score": null
            },
            {
              "doc_id": "user_guide",
              "chunk_id": "user_guide_fix_16",
              "content": "fixing issues - Export failed executions for analysis - Set up alerts for DLQ threshold breaches **Accessing the DLQ:** 1. Navigate to **Workflows** > **[Your Workflow]** > **Dead Letter Queue** 2. Filter by error type, date range, or execution ID 3. Click an execution to view full details 4. Click **\"Retry\"** to reprocess with the same input data ### Error Notifications Get notified when workflows fail: ```yaml workflow: notifications: on_failure: - type: email to: \"ops-team@company.com\" - type: slack channel: \"#alerts\" message: \"Workflow {{workflow.name}} failed: {{error.message}}\" on_success_after_retry: - type: slack channel: \"#monitoring\" message: \"Workflow recovered after {{error.attempts}} attempts\" ``` ## Workflow Limits CloudFlow enforces the following limits to ensure platform stability and performance: ### Steps Per Workflow - **Maximum**: 50 steps per workflow - **Recommendation**: Keep workflows focused and modular. If you need more steps, consider splitting into multiple workflows connected via webhooks.",
              "score": null
            },
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_8",
              "content": "**Endpoint:** `DELETE /workflows/{workflow_id}` **Example Request:** ```bash curl -X DELETE https://api.cloudflow.io/v2/workflows/wf_8x7k2m9p \\ -H \"X-API-Key: YOUR_API_KEY\" ``` **Example Response (204 No Content)** ### Pipelines #### List Pipeline Executions Retrieve execution history for data pipelines. **Endpoint:** `GET /pipelines/{pipeline_id}/executions` **Query Parameters:** - `status` (optional): Filter by status (`running`, `completed`, `failed`) - `start_date` (optional): ISO 8601 timestamp - `end_date` (optional): ISO 8601 timestamp - `limit` (optional): Items per page (max 100) - `offset` (optional): Pagination offset **Example Request:** ```python import requests api_key = \"YOUR_API_KEY\" pipeline_id = \"pipe_4x9k2m\" url = f\"https://api.cloudflow.io/v2/pipelines/{pipeline_id}/executions\" headers = {\"X-API-Key\": api_key} params = { \"status\": \"completed\", \"start_date\": \"2026-01-20T00:00:00Z\", \"limit\": 50 } response = requests.get(url, headers=headers, params=params) executions = response.json() for execution in executions['data']: print(f\"Execution {execution['id']}: {execution['status']} - {execution['duration_ms']}ms\") ``` **Example Response (200 OK):** ```json { \"data\": [ { \"id\": \"exec_7m3k9x2p\", \"pipeline_id\": \"pipe_4x9k2m\", \"status\": \"completed\", \"started_at\": \"2026-01-23T15:00:00Z\", \"completed_at\": \"2026-01-23T15:03:42Z\", \"duration_ms\": 222000, \"records_processed\": 15420, \"records_failed\": 3, \"error_rate\": 0.02 } ], \"pagination\": { \"total\": 156,",
              "score": null
            },
            {
              "doc_id": "user_guide",
              "chunk_id": "user_guide_fix_12",
              "content": "To account for your local timezone: **Option 1: Convert to UTC** If you want a workflow to run at 9:00 AM EST (UTC-5), schedule it for 14:00 UTC: ``` 0 14 * * * # 9:00 AM EST = 14:00 UTC ``` **Option 2: Use Timezone Configuration** Specify a timezone in your workflow configuration: ```yaml schedule: cron: \"0 9 * * *\" timezone: \"America/New_York\" # IANA timezone identifier ``` **Supported Timezones:** CloudFlow supports all IANA timezone identifiers, including: - `America/New_York` (Eastern Time) - `America/Chicago` (Central Time) - `America/Los_Angeles` (Pacific Time) - `Europe/London` (GMT/BST) - `Asia/Tokyo` (Japan Standard Time) - `Australia/Sydney` (Australian Eastern Time) **Daylight Saving Time:** When using timezone configuration, CloudFlow automatically handles DST transitions. A workflow scheduled for 9:00 AM will run at 9:00 AM local time regardless of DST changes. ### Schedule Management **Creating a Schedule:** 1. Open your workflow in the editor 2.",
              "score": null
            }
          ],
          "latency_ms": 5.690445001164335,
          "query_id": "realistic_011",
          "dimension": "problem",
          "query": "My workflow is being killed after an hour"
        },
        {
          "key_facts": [
            "3600 seconds",
            "exceeded maximum execution time of 3600 seconds"
          ],
          "found_facts": [
            "3600 seconds"
          ],
          "missed_facts": [
            "exceeded maximum execution time of 3600 seconds"
          ],
          "coverage": 0.5,
          "expected_docs": [
            "troubleshooting_guide",
            "user_guide"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "user_guide",
              "chunk_id": "user_guide_fix_17",
              "content": "### Execution Timeout - **Default**: 3600 seconds (60 minutes) - **Behavior**: Workflows exceeding this timeout are automatically terminated - **Custom Timeouts**: Enterprise plans can request custom timeout limits **Setting Step-Level Timeouts:** ```yaml - id: long_running_task action: http_request config: url: \"https://api.example.com/process\" timeout: 300 # 5 minutes for this specific step ``` ### Execution Limits - **Maximum**: 1000 executions per day (per workflow) - **Rate Limiting**: 100 concurrent executions per workflow - **Burst Limit**: 10 executions per second **What happens when limits are reached:** - New executions are queued automatically - Webhook triggers return HTTP 429 (Too Many Requests) - Scheduled executions are skipped (logged in audit trail) - Email notifications sent to workflow owner **Monitoring Usage:** View real-time metrics in your workflow dashboard: - Executions today: 847 / 1000 - Average duration: 12.3s - Success rate: 98.2% - Current queue depth: 3 ### Data Limits - **Maximum request/response size**: 10MB per",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_14",
              "content": "Split workflow into smaller workflows:** ```bash # Create sub-workflows cloudflow workflows create data-pipeline-part1 \\ --steps \"data_ingestion,data_validation\" \\ --timeout 1800 cloudflow workflows create data-pipeline-part2 \\ --steps \"data_transformation,data_export\" \\ --timeout 3600 \\ --trigger workflow_completed \\ --trigger-workflow data-pipeline-part1 ``` ### Retry Logic and Exponential Backoff CloudFlow implements automatic retry with exponential backoff for transient failures: - Max retries: 3 - Initial delay: 1 second - Backoff multiplier: 2 - Max delay: 60 seconds #### Retry Sequence ``` Attempt 1: Immediate Attempt 2: Wait 1s (2^0 * 1s) Attempt 3: Wait 2s (2^1 * 1s) Attempt 4: Wait 4s (2^2 * 1s) ``` #### Configuration ```bash # View current retry settings cloudflow workflows get wf_9k2n4m8p1q --format json | jq '.retry_policy' # Customize retry behavior cloudflow workflows update wf_9k2n4m8p1q \\ --retry-max-attempts 5 \\ --retry-initial-delay 2000 \\ --retry-backoff-multiplier 2 \\ --retry-max-delay 120000 \\ --retry-on-errors \"NETWORK_ERROR,TIMEOUT,RATE_LIMIT\" # Disable retry for specific step cloudflow workflows update wf_9k2n4m8p1q \\",
              "score": null
            },
            {
              "doc_id": "user_guide",
              "chunk_id": "user_guide_fix_24",
              "content": "WHERE workflow_id = $2 parameters: - \"{{now()}}\" - \"{{workflow.id}}\" ``` ### Pattern 3: Multi-Step Approval Implement approval workflows with timeouts: ```yaml name: \"Expense Approval Workflow\" trigger: type: webhook method: POST steps: - id: create_approval_request action: database_query config: query: | INSERT INTO approvals (expense_id, amount, requester, status, created_at) VALUES ($1, $2, $3, 'pending', NOW()) RETURNING id parameters: - \"{{trigger.body.expense_id}}\" - \"{{trigger.body.amount}}\" - \"{{trigger.body.requester}}\" - id: notify_manager action: email config: to: \"{{trigger.body.manager_email}}\" subject: \"Expense Approval Required: ${{trigger.body.amount}}\" body: | An expense requires your approval: Amount: ${{trigger.body.amount}} Requester: {{trigger.body.requester}} Description: {{trigger.body.description}} Approve: https://app.company.com/approve/{{steps.create_approval_request.rows[0].id}} Reject: https://app.company.com/reject/{{steps.create_approval_request.rows[0].id}} - id: wait_for_approval action: wait_for_webhook config: timeout: 86400 # 24 hours webhook_path: \"/approval/{{steps.create_approval_request.rows[0].id}}\" - id: process_approval action: javascript code: | if (input.status === 'approved') { return { approved: true }; } else { throw new Error('Expense rejected'); } on_error: - id: notify_rejection action: email config: to: \"{{trigger.body.requester}}\" subject: \"Expense Rejected\" body: \"Your expense request has been rejected.\"",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_22",
              "content": "\"connection.*pool|could not connect|database.*timeout\" # Find slow queries kubectl logs -n cloudflow deployment/cloudflow-api | \\ grep \"slow query\" | \\ awk '{print $NF}' | \\ sort -n | \\ tail -n 20 # Find deadlock errors kubectl logs -n cloudflow deployment/cloudflow-api | \\ grep -i \"deadlock detected\" ``` #### Workflow Execution Errors ```bash # Find workflow timeout errors kubectl logs -n cloudflow deployment/cloudflow-workflow-engine | \\ grep -E \"timeout|exceeded.*3600\" # Find workflow retry attempts kubectl logs -n cloudflow deployment/cloudflow-workflow-engine | \\ grep -E \"retry attempt [0-9]|retrying in\" # Find workflow failures by ID kubectl logs -n cloudflow deployment/cloudflow-workflow-engine | \\ grep \"exec_7h3j6k9m2n\" ``` #### Rate Limiting ```bash # Find rate limit events kubectl logs -n cloudflow deployment/cloudflow-api | \\ grep -E \"429|rate.*limit|throttle\" # Count rate limit errors by hour kubectl logs -n cloudflow deployment/cloudflow-api --since=24h | \\ grep \"rate_limit_exceeded\" | \\ awk '{print $1}' | \\ cut -d'T' -f1-2 | \\ sort",
              "score": null
            },
            {
              "doc_id": "user_guide",
              "chunk_id": "user_guide_fix_16",
              "content": "fixing issues - Export failed executions for analysis - Set up alerts for DLQ threshold breaches **Accessing the DLQ:** 1. Navigate to **Workflows** > **[Your Workflow]** > **Dead Letter Queue** 2. Filter by error type, date range, or execution ID 3. Click an execution to view full details 4. Click **\"Retry\"** to reprocess with the same input data ### Error Notifications Get notified when workflows fail: ```yaml workflow: notifications: on_failure: - type: email to: \"ops-team@company.com\" - type: slack channel: \"#alerts\" message: \"Workflow {{workflow.name}} failed: {{error.message}}\" on_success_after_retry: - type: slack channel: \"#monitoring\" message: \"Workflow recovered after {{error.attempts}} attempts\" ``` ## Workflow Limits CloudFlow enforces the following limits to ensure platform stability and performance: ### Steps Per Workflow - **Maximum**: 50 steps per workflow - **Recommendation**: Keep workflows focused and modular. If you need more steps, consider splitting into multiple workflows connected via webhooks.",
              "score": null
            }
          ],
          "latency_ms": 5.591069999354659,
          "query_id": "realistic_011",
          "dimension": "casual",
          "query": "workflow timeout"
        },
        {
          "key_facts": [
            "3600 seconds",
            "exceeded maximum execution time of 3600 seconds"
          ],
          "found_facts": [
            "3600 seconds"
          ],
          "missed_facts": [
            "exceeded maximum execution time of 3600 seconds"
          ],
          "coverage": 0.5,
          "expected_docs": [
            "troubleshooting_guide",
            "user_guide"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_13",
              "content": "Step 1: data_ingestion - Duration: 245s - Status: SUCCESS # Step 2: data_validation - Duration: 123s - Status: SUCCESS # Step 3: data_transformation - Duration: 3189s - Status: TIMEOUT # Step 4: data_export - Duration: 0s - Status: SKIPPED # Identify bottleneck step cloudflow workflows analyze exec_7h3j6k9m2n --show-bottlenecks ``` #### Solutions **1. Increase workflow timeout (if justified):** ```bash # Update workflow configuration cloudflow workflows update wf_9k2n4m8p1q \\ --timeout 7200 \\ --reason \"Large dataset processing requires extended time\" # Verify update cloudflow workflows get wf_9k2n4m8p1q | grep timeout ``` **2. Optimize slow steps:** ```bash # Enable parallel processing cloudflow workflows update wf_9k2n4m8p1q \\ --step data_transformation \\ --parallel-workers 8 \\ --batch-size 1000 # Add checkpointing for long operations cloudflow workflows update wf_9k2n4m8p1q \\ --step data_transformation \\ --enable-checkpointing \\ --checkpoint-interval 300 ``` **3.",
              "score": null
            },
            {
              "doc_id": "user_guide",
              "chunk_id": "user_guide_fix_18",
              "content": "action - **Maximum execution payload**: 50MB total - **Variable value size**: 1MB per variable ### Enterprise Plan Limits Enterprise customers can request increased limits: - Up to 100 steps per workflow - Up to 10,000 executions per day - Up to 7200 second timeout (2 hours) - Priority execution queue - Dedicated capacity allocation Contact sales@cloudflow.io for Enterprise pricing and custom limits. ## Best Practices Follow these best practices to build reliable, maintainable workflows: ### 1. Use Descriptive Names **Good:** - Workflow: \"Sync Customer Data from Salesforce to Database\" - Step: \"validate_customer_email\" **Bad:** - Workflow: \"Workflow 1\" - Step: \"step3\" ### 2.",
              "score": null
            },
            {
              "doc_id": "user_guide",
              "chunk_id": "user_guide_fix_17",
              "content": "### Execution Timeout - **Default**: 3600 seconds (60 minutes) - **Behavior**: Workflows exceeding this timeout are automatically terminated - **Custom Timeouts**: Enterprise plans can request custom timeout limits **Setting Step-Level Timeouts:** ```yaml - id: long_running_task action: http_request config: url: \"https://api.example.com/process\" timeout: 300 # 5 minutes for this specific step ``` ### Execution Limits - **Maximum**: 1000 executions per day (per workflow) - **Rate Limiting**: 100 concurrent executions per workflow - **Burst Limit**: 10 executions per second **What happens when limits are reached:** - New executions are queued automatically - Webhook triggers return HTTP 429 (Too Many Requests) - Scheduled executions are skipped (logged in audit trail) - Email notifications sent to workflow owner **Monitoring Usage:** View real-time metrics in your workflow dashboard: - Executions today: 847 / 1000 - Average duration: 12.3s - Success rate: 98.2% - Current queue depth: 3 ### Data Limits - **Maximum request/response size**: 10MB per",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_24",
              "content": "Operations** (P99 latency): - `GET /workflows/{id}`: < 50ms (cache hit), < 150ms (cache miss) - `POST /workflows`: < 200ms (includes validation and database write) - `POST /workflows/{id}/execute`: < 100ms (async, returns execution ID) - `GET /workflows/{id}/history`: < 300ms (paginated, 50 records per page) **Workflow Execution** (P99 latency): - Simple workflow (< 5 steps): < 2 seconds - Medium workflow (5-15 steps): < 5 seconds - Complex workflow (> 15 steps): < 15 seconds - Parallel execution: Linear scaling up to 10 concurrent branches **Database Operations** (P99 latency): - Simple SELECT: < 5ms - JOIN query: < 20ms - INSERT/UPDATE: < 10ms - Batch operations: < 100ms (batch size: 100 records) **Cache Operations** (P99 latency): - Redis GET: < 2ms - Redis SET: < 3ms - Redis DEL: < 2ms ### Throughput Capacity **API Gateway**: - Sustained: 10,000 requests per second - Peak: 25,000 requests per second (5-minute burst) - Rate",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_14",
              "content": "Split workflow into smaller workflows:** ```bash # Create sub-workflows cloudflow workflows create data-pipeline-part1 \\ --steps \"data_ingestion,data_validation\" \\ --timeout 1800 cloudflow workflows create data-pipeline-part2 \\ --steps \"data_transformation,data_export\" \\ --timeout 3600 \\ --trigger workflow_completed \\ --trigger-workflow data-pipeline-part1 ``` ### Retry Logic and Exponential Backoff CloudFlow implements automatic retry with exponential backoff for transient failures: - Max retries: 3 - Initial delay: 1 second - Backoff multiplier: 2 - Max delay: 60 seconds #### Retry Sequence ``` Attempt 1: Immediate Attempt 2: Wait 1s (2^0 * 1s) Attempt 3: Wait 2s (2^1 * 1s) Attempt 4: Wait 4s (2^2 * 1s) ``` #### Configuration ```bash # View current retry settings cloudflow workflows get wf_9k2n4m8p1q --format json | jq '.retry_policy' # Customize retry behavior cloudflow workflows update wf_9k2n4m8p1q \\ --retry-max-attempts 5 \\ --retry-initial-delay 2000 \\ --retry-backoff-multiplier 2 \\ --retry-max-delay 120000 \\ --retry-on-errors \"NETWORK_ERROR,TIMEOUT,RATE_LIMIT\" # Disable retry for specific step cloudflow workflows update wf_9k2n4m8p1q \\",
              "score": null
            }
          ],
          "latency_ms": 5.5722939996485366,
          "query_id": "realistic_011",
          "dimension": "contextual",
          "query": "I have a long-running data processing workflow, what's the time limit?"
        },
        {
          "key_facts": [
            "3600 seconds",
            "exceeded maximum execution time of 3600 seconds"
          ],
          "found_facts": [
            "3600 seconds"
          ],
          "missed_facts": [
            "exceeded maximum execution time of 3600 seconds"
          ],
          "coverage": 0.5,
          "expected_docs": [
            "troubleshooting_guide",
            "user_guide"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "user_guide",
              "chunk_id": "user_guide_fix_16",
              "content": "fixing issues - Export failed executions for analysis - Set up alerts for DLQ threshold breaches **Accessing the DLQ:** 1. Navigate to **Workflows** > **[Your Workflow]** > **Dead Letter Queue** 2. Filter by error type, date range, or execution ID 3. Click an execution to view full details 4. Click **\"Retry\"** to reprocess with the same input data ### Error Notifications Get notified when workflows fail: ```yaml workflow: notifications: on_failure: - type: email to: \"ops-team@company.com\" - type: slack channel: \"#alerts\" message: \"Workflow {{workflow.name}} failed: {{error.message}}\" on_success_after_retry: - type: slack channel: \"#monitoring\" message: \"Workflow recovered after {{error.attempts}} attempts\" ``` ## Workflow Limits CloudFlow enforces the following limits to ensure platform stability and performance: ### Steps Per Workflow - **Maximum**: 50 steps per workflow - **Recommendation**: Keep workflows focused and modular. If you need more steps, consider splitting into multiple workflows connected via webhooks.",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_14",
              "content": "Split workflow into smaller workflows:** ```bash # Create sub-workflows cloudflow workflows create data-pipeline-part1 \\ --steps \"data_ingestion,data_validation\" \\ --timeout 1800 cloudflow workflows create data-pipeline-part2 \\ --steps \"data_transformation,data_export\" \\ --timeout 3600 \\ --trigger workflow_completed \\ --trigger-workflow data-pipeline-part1 ``` ### Retry Logic and Exponential Backoff CloudFlow implements automatic retry with exponential backoff for transient failures: - Max retries: 3 - Initial delay: 1 second - Backoff multiplier: 2 - Max delay: 60 seconds #### Retry Sequence ``` Attempt 1: Immediate Attempt 2: Wait 1s (2^0 * 1s) Attempt 3: Wait 2s (2^1 * 1s) Attempt 4: Wait 4s (2^2 * 1s) ``` #### Configuration ```bash # View current retry settings cloudflow workflows get wf_9k2n4m8p1q --format json | jq '.retry_policy' # Customize retry behavior cloudflow workflows update wf_9k2n4m8p1q \\ --retry-max-attempts 5 \\ --retry-initial-delay 2000 \\ --retry-backoff-multiplier 2 \\ --retry-max-delay 120000 \\ --retry-on-errors \"NETWORK_ERROR,TIMEOUT,RATE_LIMIT\" # Disable retry for specific step cloudflow workflows update wf_9k2n4m8p1q \\",
              "score": null
            },
            {
              "doc_id": "user_guide",
              "chunk_id": "user_guide_fix_17",
              "content": "### Execution Timeout - **Default**: 3600 seconds (60 minutes) - **Behavior**: Workflows exceeding this timeout are automatically terminated - **Custom Timeouts**: Enterprise plans can request custom timeout limits **Setting Step-Level Timeouts:** ```yaml - id: long_running_task action: http_request config: url: \"https://api.example.com/process\" timeout: 300 # 5 minutes for this specific step ``` ### Execution Limits - **Maximum**: 1000 executions per day (per workflow) - **Rate Limiting**: 100 concurrent executions per workflow - **Burst Limit**: 10 executions per second **What happens when limits are reached:** - New executions are queued automatically - Webhook triggers return HTTP 429 (Too Many Requests) - Scheduled executions are skipped (logged in audit trail) - Email notifications sent to workflow owner **Monitoring Usage:** View real-time metrics in your workflow dashboard: - Executions today: 847 / 1000 - Average duration: 12.3s - Success rate: 98.2% - Current queue depth: 3 ### Data Limits - **Maximum request/response size**: 10MB per",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_15",
              "content": "--step data_export \\ --retry-enabled false ``` #### Monitoring Retries ```bash # List failed executions with retry information cloudflow workflows executions list \\ --status FAILED \\ --show-retries \\ --last 7d # Get retry history for specific execution cloudflow workflows executions retries exec_7h3j6k9m2n # Output: # Execution: exec_7h3j6k9m2n # Attempt 1: FAILED - NetworkError: Connection refused (delay: 0ms) # Attempt 2: FAILED - NetworkError: Connection timeout (delay: 1000ms) # Attempt 3: FAILED - NetworkError: Connection timeout (delay: 2000ms) # Attempt 4: FAILED - NetworkError: Connection timeout (delay: 4000ms) # Final Status: FAILED_AFTER_RETRIES ``` ### Workflow Step Failures #### Common Error Patterns **1.",
              "score": null
            },
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_12",
              "content": "Waiting {retry_after} seconds...\") time.sleep(retry_after) continue elif response.status_code >= 500: # Server error - retry with exponential backoff wait_time = 2 ** attempt print(f\"Server error. Retrying in {wait_time} seconds...\") time.sleep(wait_time) continue else: # Client error - don't retry error_data = response.json() raise Exception(f\"API Error: {error_data['error']['message']}\") except requests.exceptions.RequestException as e: print(f\"Request failed: {e}\") if attempt < max_retries - 1: time.sleep(2 ** attempt) else: raise raise Exception(\"Max retries exceeded\") ``` ## Webhooks CloudFlow can send webhook notifications when specific events occur in your workflows.",
              "score": null
            }
          ],
          "latency_ms": 5.406717000369099,
          "query_id": "realistic_011",
          "dimension": "negation",
          "query": "Why did my workflow fail after 3600 seconds?"
        },
        {
          "key_facts": [
            "RS256",
            "RS256 algorithm",
            "RS256 signing algorithm"
          ],
          "found_facts": [
            "RS256",
            "RS256 algorithm"
          ],
          "missed_facts": [
            "RS256 signing algorithm"
          ],
          "coverage": 0.6666666666666666,
          "expected_docs": [
            "api_reference"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_2",
              "content": "JWTs must include the following claims: - `iss` (issuer): Your application identifier - `sub` (subject): User or service account ID - `aud` (audience): `https://api.cloudflow.io` - `exp` (expiration): Unix timestamp (max 3600 seconds from `iat`) - `iat` (issued at): Unix timestamp - `scope`: Space-separated list of requested scopes Example JWT header: ```python import jwt import time from cryptography.hazmat.primitives import serialization from cryptography.hazmat.backends import default_backend # Load your private key with open('private_key.pem', 'rb') as key_file: private_key = serialization.load_pem_private_key( key_file.read(), password=None, backend=default_backend() ) # Create JWT payload payload = { 'iss': 'your-app-id', 'sub': 'user-12345', 'aud': 'https://api.cloudflow.io', 'exp': int(time.time()) + 3600, 'iat': int(time.time()), 'scope': 'workflows:read workflows:write' } # Generate token token = jwt.encode(payload, private_key, algorithm='RS256') # Use in API request headers = {'Authorization': f'Bearer {token}'} ``` **Token Expiration:** All tokens expire after 3600 seconds (1 hour). Implement token refresh logic in your application.",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_2",
              "content": "**Verification:** ```bash # Decode JWT to check expiration echo $CF_ACCESS_TOKEN | cut -d'.' -f2 | base64 -d | jq '.exp' # Compare with current time date +%s ``` **Resolution:** ```bash # Refresh the access token curl -X POST https://api.cloudflow.io/auth/refresh \\ -H \"Content-Type: application/json\" \\ -d '{\"refresh_token\": \"'$CF_REFRESH_TOKEN'\"}' # Update environment variable export CF_ACCESS_TOKEN=\"<new_token>\" ``` **2. Invalid JWT Signature** Error: `JWT signature verification failed` **Causes:** - Token was modified or corrupted - Using wrong signing key - Token generated with different secret **Resolution:** ```bash # Validate token structure cloudflow auth validate --token $CF_ACCESS_TOKEN # Generate new token cloudflow auth login --username <user> --password <pass> # For service accounts cloudflow auth service-account --create --name <sa-name> --scopes \"workflow:read,workflow:write\" ``` **3. Clock Skew Issues** JWT validation fails when system clocks are out of sync (tolerance: \u00b1300 seconds).",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_4",
              "content": "authorization service handling user identity, token generation, and permission validation. **Technology**: Go with gRPC for internal communication, REST for external **Replicas**: 8 pods (production), auto-scaling 6-12 **Resource Allocation**: 1 vCPU, 2GB RAM per pod **Key Responsibilities**: - User authentication via multiple providers (OAuth2, SAML, local credentials) - JWT token generation and validation (RS256 algorithm) - Role-based access control (RBAC) with fine-grained permissions - Session management with Redis-backed storage - API key generation and validation for service accounts - MFA enforcement for administrative operations **Authentication Flow**: ``` Client Request \u2192 API Gateway \u2192 Auth Service \u2502 \u251c\u2500 Validate credentials \u251c\u2500 Check MFA if required \u251c\u2500 Generate JWT (15min expiry) \u251c\u2500 Generate refresh token (7 days) \u2514\u2500 Store session in Redis ``` **Token Structure**: - Access Token: JWT with 15-minute expiry - Refresh Token: Opaque token with 7-day expiry, stored in PostgreSQL - Claims: user_id, email, roles[], permissions[], tenant_id **Security Features**: -",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_3",
              "content": "**Technology**: Node.js with Express.js framework **Replicas**: 12 pods (production), auto-scaling 8-20 based on CPU **Resource Allocation**: 2 vCPU, 4GB RAM per pod **Key Responsibilities**: - JWT token validation (delegated to Auth Service for initial validation) - Rate limiting: 1000 requests per minute per API key (sliding window) - Request/response transformation and validation using JSON Schema - Routing to downstream services based on URL path patterns - CORS handling for web clients - Request/response logging and correlation ID injection **Critical Endpoints**: - `POST /api/v1/workflows` - Create new workflow - `GET /api/v1/workflows/:id` - Retrieve workflow status - `POST /api/v1/workflows/:id/execute` - Trigger workflow execution - `GET /api/v1/workflows/:id/history` - Get execution history **Dependencies**: - Auth Service (for token validation) - Redis (for rate limiting counters) - All downstream microservices **Performance Targets**: - P50 latency: < 50ms - P99 latency: < 200ms - Throughput: 10,000 RPS sustained --- ### Auth Service **Purpose**: Centralized authentication and",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_22",
              "content": "- Token revocation: Blacklist in Redis for compromised tokens **Permission Model**: ``` User \u2192 Roles \u2192 Permissions \u2198 \u2197 Tenants (Multi-tenancy isolation) ``` Example permissions: - `workflow:read` - View workflows - `workflow:write` - Create/update workflows - `workflow:execute` - Trigger workflow execution - `workflow:delete` - Delete workflows - `admin:*` - All administrative operations **API Key Management**: - Format: `cfk_live_<32-char-random>` (production), `cfk_test_<32-char-random>` (sandbox) - Hashing: SHA-256 before storage in PostgreSQL - Scoping: API keys can be scoped to specific workflows or operations - Rate limits: Configurable per API key (default: 1000 RPM) ### Secrets Management **HashiCorp Vault Integration**: - Dynamic database credentials: Generated on-demand, 1-hour TTL - Encryption keys: Transit secrets engine for encryption-as-a-service - API keys for external services: Stored with versioning - Rotation policy: Automated rotation every 90 days with notification **Secret Access Pattern**: ``` Service starts \u2192 Vault authentication (Kubernetes service account) \u2502 \u25bc Request secret lease (1-hour TTL)",
              "score": null
            }
          ],
          "latency_ms": 5.427485000836896,
          "query_id": "realistic_012",
          "dimension": "original",
          "query": "What JWT algorithm is used for token signing?"
        },
        {
          "key_facts": [
            "RS256",
            "RS256 algorithm",
            "RS256 signing algorithm"
          ],
          "found_facts": [
            "RS256",
            "RS256 signing algorithm"
          ],
          "missed_facts": [
            "RS256 algorithm"
          ],
          "coverage": 0.6666666666666666,
          "expected_docs": [
            "api_reference"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_0",
              "content": "# CloudFlow API Reference Version 2.1.0 | Last Updated: January 2026 ## Overview The CloudFlow API is a RESTful service that enables developers to programmatically manage cloud workflows, data pipelines, and automation tasks. This documentation provides comprehensive details on authentication, endpoints, request/response formats, error handling, and best practices. **Base URL:** `https://api.cloudflow.io/v2` **API Status:** https://status.cloudflow.io ## Authentication CloudFlow supports three authentication methods to suit different use cases and security requirements. ### API Keys API keys provide simple authentication for server-to-server communication. Include your API key in the request header: ```bash curl -H \"X-API-Key: cf_live_a1b2c3d4e5f6g7h8i9j0\" \\ https://api.cloudflow.io/v2/workflows ``` **Security Notes:** - Never expose API keys in client-side code - Rotate keys every 90 days - Use separate keys for development and production environments ### OAuth 2.0 OAuth 2.0 is recommended for applications that access CloudFlow on behalf of users. We support the Authorization Code flow with PKCE.",
              "score": null
            },
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_2",
              "content": "JWTs must include the following claims: - `iss` (issuer): Your application identifier - `sub` (subject): User or service account ID - `aud` (audience): `https://api.cloudflow.io` - `exp` (expiration): Unix timestamp (max 3600 seconds from `iat`) - `iat` (issued at): Unix timestamp - `scope`: Space-separated list of requested scopes Example JWT header: ```python import jwt import time from cryptography.hazmat.primitives import serialization from cryptography.hazmat.backends import default_backend # Load your private key with open('private_key.pem', 'rb') as key_file: private_key = serialization.load_pem_private_key( key_file.read(), password=None, backend=default_backend() ) # Create JWT payload payload = { 'iss': 'your-app-id', 'sub': 'user-12345', 'aud': 'https://api.cloudflow.io', 'exp': int(time.time()) + 3600, 'iat': int(time.time()), 'scope': 'workflows:read workflows:write' } # Generate token token = jwt.encode(payload, private_key, algorithm='RS256') # Use in API request headers = {'Authorization': f'Bearer {token}'} ``` **Token Expiration:** All tokens expire after 3600 seconds (1 hour). Implement token refresh logic in your application.",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_2",
              "content": "**Verification:** ```bash # Decode JWT to check expiration echo $CF_ACCESS_TOKEN | cut -d'.' -f2 | base64 -d | jq '.exp' # Compare with current time date +%s ``` **Resolution:** ```bash # Refresh the access token curl -X POST https://api.cloudflow.io/auth/refresh \\ -H \"Content-Type: application/json\" \\ -d '{\"refresh_token\": \"'$CF_REFRESH_TOKEN'\"}' # Update environment variable export CF_ACCESS_TOKEN=\"<new_token>\" ``` **2. Invalid JWT Signature** Error: `JWT signature verification failed` **Causes:** - Token was modified or corrupted - Using wrong signing key - Token generated with different secret **Resolution:** ```bash # Validate token structure cloudflow auth validate --token $CF_ACCESS_TOKEN # Generate new token cloudflow auth login --username <user> --password <pass> # For service accounts cloudflow auth service-account --create --name <sa-name> --scopes \"workflow:read,workflow:write\" ``` **3. Clock Skew Issues** JWT validation fails when system clocks are out of sync (tolerance: \u00b1300 seconds).",
              "score": null
            },
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_1",
              "content": "**Authorization Endpoint:** `https://auth.cloudflow.io/oauth/authorize` **Token Endpoint:** `https://auth.cloudflow.io/oauth/token` **Supported Scopes:** - `workflows:read` - Read workflow configurations - `workflows:write` - Create and modify workflows - `pipelines:read` - Read pipeline data - `pipelines:write` - Create and manage pipelines - `analytics:read` - Access analytics and metrics - `admin:full` - Full administrative access Example authorization request: ```bash curl -X POST https://auth.cloudflow.io/oauth/token \\ -H \"Content-Type: application/x-www-form-urlencoded\" \\ -d \"grant_type=authorization_code\" \\ -d \"code=AUTH_CODE_HERE\" \\ -d \"client_id=YOUR_CLIENT_ID\" \\ -d \"client_secret=YOUR_CLIENT_SECRET\" \\ -d \"redirect_uri=https://yourapp.com/callback\" ``` ### JWT Tokens For advanced use cases, CloudFlow supports JSON Web Tokens (JWT) with RS256 signing algorithm.",
              "score": null
            },
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_4",
              "content": "verbosity | `info`, `debug`, `warn`, `error` | No (default: `info`) | | `API_PORT` | API server port | `3000` | No (default: `3000`) | | `WORKER_CONCURRENCY` | Number of concurrent workers | `10` | No (default: `5`) | | `SESSION_SECRET` | Session encryption key | `<generated-secret-256-bit>` | Yes | | `AWS_REGION` | AWS region for services | `us-east-1` | Yes | | `S3_BUCKET` | S3 bucket for file storage | `cloudflow-prod-storage` | Yes | | `SMTP_HOST` | Email server hostname | `smtp.sendgrid.net` | No | | `SMTP_PORT` | Email server port | `587` | No | | `METRICS_ENABLED` | Enable Prometheus metrics | `true` | No (default: `false`) | Create a Kubernetes secret for sensitive variables: ```bash kubectl create secret generic cloudflow-secrets \\ --namespace cloudflow-prod \\ --from-literal=DATABASE_URL=\"postgresql://cloudflow:$(cat db-password.txt)@pgbouncer.cloudflow-prod.svc.cluster.local:5432/cloudflow\" \\ --from-literal=REDIS_URL=\"redis://redis-master.cloudflow-prod.svc.cluster.local:6379\" \\ --from-literal=JWT_SECRET=\"$(openssl rand -base64 32)\" \\ --from-literal=SESSION_SECRET=\"$(openssl rand -base64 32)\" ``` ### Helm Chart Deployment Add the CloudFlow Helm repository:",
              "score": null
            }
          ],
          "latency_ms": 5.496432999279932,
          "query_id": "realistic_012",
          "dimension": "synonym",
          "query": "Which signing algorithm does CloudFlow use for JWTs?"
        },
        {
          "key_facts": [
            "RS256",
            "RS256 algorithm",
            "RS256 signing algorithm"
          ],
          "found_facts": [
            "RS256"
          ],
          "missed_facts": [
            "RS256 algorithm",
            "RS256 signing algorithm"
          ],
          "coverage": 0.3333333333333333,
          "expected_docs": [
            "api_reference"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_2",
              "content": "**Verification:** ```bash # Decode JWT to check expiration echo $CF_ACCESS_TOKEN | cut -d'.' -f2 | base64 -d | jq '.exp' # Compare with current time date +%s ``` **Resolution:** ```bash # Refresh the access token curl -X POST https://api.cloudflow.io/auth/refresh \\ -H \"Content-Type: application/json\" \\ -d '{\"refresh_token\": \"'$CF_REFRESH_TOKEN'\"}' # Update environment variable export CF_ACCESS_TOKEN=\"<new_token>\" ``` **2. Invalid JWT Signature** Error: `JWT signature verification failed` **Causes:** - Token was modified or corrupted - Using wrong signing key - Token generated with different secret **Resolution:** ```bash # Validate token structure cloudflow auth validate --token $CF_ACCESS_TOKEN # Generate new token cloudflow auth login --username <user> --password <pass> # For service accounts cloudflow auth service-account --create --name <sa-name> --scopes \"workflow:read,workflow:write\" ``` **3. Clock Skew Issues** JWT validation fails when system clocks are out of sync (tolerance: \u00b1300 seconds).",
              "score": null
            },
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_2",
              "content": "JWTs must include the following claims: - `iss` (issuer): Your application identifier - `sub` (subject): User or service account ID - `aud` (audience): `https://api.cloudflow.io` - `exp` (expiration): Unix timestamp (max 3600 seconds from `iat`) - `iat` (issued at): Unix timestamp - `scope`: Space-separated list of requested scopes Example JWT header: ```python import jwt import time from cryptography.hazmat.primitives import serialization from cryptography.hazmat.backends import default_backend # Load your private key with open('private_key.pem', 'rb') as key_file: private_key = serialization.load_pem_private_key( key_file.read(), password=None, backend=default_backend() ) # Create JWT payload payload = { 'iss': 'your-app-id', 'sub': 'user-12345', 'aud': 'https://api.cloudflow.io', 'exp': int(time.time()) + 3600, 'iat': int(time.time()), 'scope': 'workflows:read workflows:write' } # Generate token token = jwt.encode(payload, private_key, algorithm='RS256') # Use in API request headers = {'Authorization': f'Bearer {token}'} ``` **Token Expiration:** All tokens expire after 3600 seconds (1 hour). Implement token refresh logic in your application.",
              "score": null
            },
            {
              "doc_id": "user_guide",
              "chunk_id": "user_guide_fix_20",
              "content": "Validate Input Data Always validate trigger data before processing: ```yaml - id: validate_input action: javascript code: | const required_fields = ['email', 'name', 'order_id']; for (const field of required_fields) { if (!input[field]) { throw new Error(`Missing required field: ${field}`); } } // Validate email format const email_regex = /^[^\\s@]+@[^\\s@]+\\.[^\\s@]+$/; if (!email_regex.test(input.email)) { throw new Error('Invalid email format'); } return { validated: true }; ``` ### 5. Use Idempotency Keys For operations that shouldn't be repeated (payments, record creation), use idempotency keys: ```yaml - id: create_charge action: http_request config: url: \"https://api.stripe.com/v1/charges\" method: POST headers: Idempotency-Key: \"{{workflow.id}}-{{execution.id}}\" body: amount: \"{{amount}}\" ``` ### 6. Monitor and Log Add logging steps for important workflow milestones: ```yaml - id: log_start action: database_query config: query: \"INSERT INTO workflow_audit (execution_id, step, timestamp) VALUES ($1, $2, $3)\" parameters: - \"{{execution.id}}\" - \"workflow_started\" - \"{{now()}}\" ``` ### 7.",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_16",
              "content": "Data Validation Errors** ``` ValidationError: Field 'customer_id' is required but missing in 234 records ``` **Resolution:** ```bash # Add data quality checks cloudflow workflows update wf_9k2n4m8p1q \\ --step data_ingestion \\ --add-validator required_fields \\ --validator-config '{\"fields\": [\"customer_id\", \"timestamp\", \"amount\"]}' # Configure error handling cloudflow workflows update wf_9k2n4m8p1q \\ --step data_validation \\ --on-error continue \\ --error-threshold 5% # Fail if > 5% of records invalid ``` **2. External API Failures** ``` ExternalAPIError: API request to https://partner-api.example.com failed with status 502 ``` **Resolution:** ```bash # Add circuit breaker cloudflow workflows update wf_9k2n4m8p1q \\ --step external_api_call \\ --circuit-breaker-enabled true \\ --circuit-breaker-threshold 5 \\ --circuit-breaker-timeout 30000 # Configure fallback behavior cloudflow workflows update wf_9k2n4m8p1q \\ --step external_api_call \\ --fallback-action use_cached_data \\ --cache-ttl 3600 ``` --- ## Rate Limiting & Throttling ### 429 Too Many Requests #### Error Response ```json { \"error\": \"rate_limit_exceeded\", \"message\": \"API rate limit exceeded.",
              "score": null
            },
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_11",
              "content": "Codes CloudFlow returns specific error codes to help you identify and resolve issues: - `invalid_parameter`: One or more request parameters are invalid - `missing_required_field`: Required field is missing from request body - `authentication_failed`: Invalid API key or token - `insufficient_permissions`: User lacks required scope or permission - `resource_not_found`: Requested resource does not exist - `rate_limit_exceeded`: Too many requests, see rate limiting section - `workflow_execution_failed`: Workflow execution encountered an error - `invalid_json`: Request body contains malformed JSON - `duplicate_resource`: Resource with same identifier already exists - `quota_exceeded`: Account quota limit reached ### Error Handling Best Practices **Python Example:** ```python import requests import time def make_api_request(url, headers, max_retries=3): for attempt in range(max_retries): try: response = requests.get(url, headers=headers) if response.status_code == 200: return response.json() elif response.status_code == 429: # Rate limit exceeded retry_after = int(response.headers.get('X-RateLimit-Reset', 60)) print(f\"Rate limited.",
              "score": null
            }
          ],
          "latency_ms": 5.782307000117726,
          "query_id": "realistic_012",
          "dimension": "problem",
          "query": "My JWT validation is failing with algorithm mismatch"
        },
        {
          "key_facts": [
            "RS256",
            "RS256 algorithm",
            "RS256 signing algorithm"
          ],
          "found_facts": [
            "RS256",
            "RS256 algorithm"
          ],
          "missed_facts": [
            "RS256 signing algorithm"
          ],
          "coverage": 0.6666666666666666,
          "expected_docs": [
            "api_reference"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_2",
              "content": "JWTs must include the following claims: - `iss` (issuer): Your application identifier - `sub` (subject): User or service account ID - `aud` (audience): `https://api.cloudflow.io` - `exp` (expiration): Unix timestamp (max 3600 seconds from `iat`) - `iat` (issued at): Unix timestamp - `scope`: Space-separated list of requested scopes Example JWT header: ```python import jwt import time from cryptography.hazmat.primitives import serialization from cryptography.hazmat.backends import default_backend # Load your private key with open('private_key.pem', 'rb') as key_file: private_key = serialization.load_pem_private_key( key_file.read(), password=None, backend=default_backend() ) # Create JWT payload payload = { 'iss': 'your-app-id', 'sub': 'user-12345', 'aud': 'https://api.cloudflow.io', 'exp': int(time.time()) + 3600, 'iat': int(time.time()), 'scope': 'workflows:read workflows:write' } # Generate token token = jwt.encode(payload, private_key, algorithm='RS256') # Use in API request headers = {'Authorization': f'Bearer {token}'} ``` **Token Expiration:** All tokens expire after 3600 seconds (1 hour). Implement token refresh logic in your application.",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_3",
              "content": "**Technology**: Node.js with Express.js framework **Replicas**: 12 pods (production), auto-scaling 8-20 based on CPU **Resource Allocation**: 2 vCPU, 4GB RAM per pod **Key Responsibilities**: - JWT token validation (delegated to Auth Service for initial validation) - Rate limiting: 1000 requests per minute per API key (sliding window) - Request/response transformation and validation using JSON Schema - Routing to downstream services based on URL path patterns - CORS handling for web clients - Request/response logging and correlation ID injection **Critical Endpoints**: - `POST /api/v1/workflows` - Create new workflow - `GET /api/v1/workflows/:id` - Retrieve workflow status - `POST /api/v1/workflows/:id/execute` - Trigger workflow execution - `GET /api/v1/workflows/:id/history` - Get execution history **Dependencies**: - Auth Service (for token validation) - Redis (for rate limiting counters) - All downstream microservices **Performance Targets**: - P50 latency: < 50ms - P99 latency: < 200ms - Throughput: 10,000 RPS sustained --- ### Auth Service **Purpose**: Centralized authentication and",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_4",
              "content": "authorization service handling user identity, token generation, and permission validation. **Technology**: Go with gRPC for internal communication, REST for external **Replicas**: 8 pods (production), auto-scaling 6-12 **Resource Allocation**: 1 vCPU, 2GB RAM per pod **Key Responsibilities**: - User authentication via multiple providers (OAuth2, SAML, local credentials) - JWT token generation and validation (RS256 algorithm) - Role-based access control (RBAC) with fine-grained permissions - Session management with Redis-backed storage - API key generation and validation for service accounts - MFA enforcement for administrative operations **Authentication Flow**: ``` Client Request \u2192 API Gateway \u2192 Auth Service \u2502 \u251c\u2500 Validate credentials \u251c\u2500 Check MFA if required \u251c\u2500 Generate JWT (15min expiry) \u251c\u2500 Generate refresh token (7 days) \u2514\u2500 Store session in Redis ``` **Token Structure**: - Access Token: JWT with 15-minute expiry - Refresh Token: Opaque token with 7-day expiry, stored in PostgreSQL - Claims: user_id, email, roles[], permissions[], tenant_id **Security Features**: -",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_6",
              "content": "**Technology**: Node.js with TypeScript, Bull queue library **Replicas**: 16 pods (production), auto-scaling 12-24 **Resource Allocation**: 4 vCPU, 8GB RAM per pod **Key Responsibilities**: - Parse and validate workflow definitions (JSON-based DSL) - Execute workflow steps with state machine pattern - Handle retries with exponential backoff (max 3 retries, backoff: 2^n seconds) - Coordinate parallel and sequential task execution - Manage workflow state persistence and recovery - Support for conditional branching and loops - Sub-workflow invocation and composition **Workflow Execution Model**: ``` Workflow Submitted \u2192 Validation \u2192 Queue in Kafka \u2502 \u25bc Workflow Engine picks up \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u25bc \u25bc \u25bc [Task 1 Exec] [Task 2 Exec] [Task 3 Exec] \u2502 \u2502 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u25bc State Update \u2192 PostgreSQL \u2502 \u25bc Publish Event \u2192 Kafka ``` **Workflow DSL Example**: ```json { \"workflow_id\": \"customer-onboarding-v2\", \"version\": \"2.1.0\", \"steps\": [ { \"id\": \"validate-customer\", \"type\": \"http_request\", \"config\": { \"url\": \"https://validation.internal/verify\", \"method\": \"POST\", \"timeout\": 5000,",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_2",
              "content": "**Verification:** ```bash # Decode JWT to check expiration echo $CF_ACCESS_TOKEN | cut -d'.' -f2 | base64 -d | jq '.exp' # Compare with current time date +%s ``` **Resolution:** ```bash # Refresh the access token curl -X POST https://api.cloudflow.io/auth/refresh \\ -H \"Content-Type: application/json\" \\ -d '{\"refresh_token\": \"'$CF_REFRESH_TOKEN'\"}' # Update environment variable export CF_ACCESS_TOKEN=\"<new_token>\" ``` **2. Invalid JWT Signature** Error: `JWT signature verification failed` **Causes:** - Token was modified or corrupted - Using wrong signing key - Token generated with different secret **Resolution:** ```bash # Validate token structure cloudflow auth validate --token $CF_ACCESS_TOKEN # Generate new token cloudflow auth login --username <user> --password <pass> # For service accounts cloudflow auth service-account --create --name <sa-name> --scopes \"workflow:read,workflow:write\" ``` **3. Clock Skew Issues** JWT validation fails when system clocks are out of sync (tolerance: \u00b1300 seconds).",
              "score": null
            }
          ],
          "latency_ms": 5.747762001192314,
          "query_id": "realistic_012",
          "dimension": "casual",
          "query": "jwt algorithm"
        },
        {
          "key_facts": [
            "RS256",
            "RS256 algorithm",
            "RS256 signing algorithm"
          ],
          "found_facts": [
            "RS256",
            "RS256 algorithm"
          ],
          "missed_facts": [
            "RS256 signing algorithm"
          ],
          "coverage": 0.6666666666666666,
          "expected_docs": [
            "api_reference"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_2",
              "content": "JWTs must include the following claims: - `iss` (issuer): Your application identifier - `sub` (subject): User or service account ID - `aud` (audience): `https://api.cloudflow.io` - `exp` (expiration): Unix timestamp (max 3600 seconds from `iat`) - `iat` (issued at): Unix timestamp - `scope`: Space-separated list of requested scopes Example JWT header: ```python import jwt import time from cryptography.hazmat.primitives import serialization from cryptography.hazmat.backends import default_backend # Load your private key with open('private_key.pem', 'rb') as key_file: private_key = serialization.load_pem_private_key( key_file.read(), password=None, backend=default_backend() ) # Create JWT payload payload = { 'iss': 'your-app-id', 'sub': 'user-12345', 'aud': 'https://api.cloudflow.io', 'exp': int(time.time()) + 3600, 'iat': int(time.time()), 'scope': 'workflows:read workflows:write' } # Generate token token = jwt.encode(payload, private_key, algorithm='RS256') # Use in API request headers = {'Authorization': f'Bearer {token}'} ``` **Token Expiration:** All tokens expire after 3600 seconds (1 hour). Implement token refresh logic in your application.",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_3",
              "content": "**Technology**: Node.js with Express.js framework **Replicas**: 12 pods (production), auto-scaling 8-20 based on CPU **Resource Allocation**: 2 vCPU, 4GB RAM per pod **Key Responsibilities**: - JWT token validation (delegated to Auth Service for initial validation) - Rate limiting: 1000 requests per minute per API key (sliding window) - Request/response transformation and validation using JSON Schema - Routing to downstream services based on URL path patterns - CORS handling for web clients - Request/response logging and correlation ID injection **Critical Endpoints**: - `POST /api/v1/workflows` - Create new workflow - `GET /api/v1/workflows/:id` - Retrieve workflow status - `POST /api/v1/workflows/:id/execute` - Trigger workflow execution - `GET /api/v1/workflows/:id/history` - Get execution history **Dependencies**: - Auth Service (for token validation) - Redis (for rate limiting counters) - All downstream microservices **Performance Targets**: - P50 latency: < 50ms - P99 latency: < 200ms - Throughput: 10,000 RPS sustained --- ### Auth Service **Purpose**: Centralized authentication and",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_2",
              "content": "**Verification:** ```bash # Decode JWT to check expiration echo $CF_ACCESS_TOKEN | cut -d'.' -f2 | base64 -d | jq '.exp' # Compare with current time date +%s ``` **Resolution:** ```bash # Refresh the access token curl -X POST https://api.cloudflow.io/auth/refresh \\ -H \"Content-Type: application/json\" \\ -d '{\"refresh_token\": \"'$CF_REFRESH_TOKEN'\"}' # Update environment variable export CF_ACCESS_TOKEN=\"<new_token>\" ``` **2. Invalid JWT Signature** Error: `JWT signature verification failed` **Causes:** - Token was modified or corrupted - Using wrong signing key - Token generated with different secret **Resolution:** ```bash # Validate token structure cloudflow auth validate --token $CF_ACCESS_TOKEN # Generate new token cloudflow auth login --username <user> --password <pass> # For service accounts cloudflow auth service-account --create --name <sa-name> --scopes \"workflow:read,workflow:write\" ``` **3. Clock Skew Issues** JWT validation fails when system clocks are out of sync (tolerance: \u00b1300 seconds).",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_4",
              "content": "authorization service handling user identity, token generation, and permission validation. **Technology**: Go with gRPC for internal communication, REST for external **Replicas**: 8 pods (production), auto-scaling 6-12 **Resource Allocation**: 1 vCPU, 2GB RAM per pod **Key Responsibilities**: - User authentication via multiple providers (OAuth2, SAML, local credentials) - JWT token generation and validation (RS256 algorithm) - Role-based access control (RBAC) with fine-grained permissions - Session management with Redis-backed storage - API key generation and validation for service accounts - MFA enforcement for administrative operations **Authentication Flow**: ``` Client Request \u2192 API Gateway \u2192 Auth Service \u2502 \u251c\u2500 Validate credentials \u251c\u2500 Check MFA if required \u251c\u2500 Generate JWT (15min expiry) \u251c\u2500 Generate refresh token (7 days) \u2514\u2500 Store session in Redis ``` **Token Structure**: - Access Token: JWT with 15-minute expiry - Refresh Token: Opaque token with 7-day expiry, stored in PostgreSQL - Claims: user_id, email, roles[], permissions[], tenant_id **Security Features**: -",
              "score": null
            },
            {
              "doc_id": "user_guide",
              "chunk_id": "user_guide_fix_20",
              "content": "Validate Input Data Always validate trigger data before processing: ```yaml - id: validate_input action: javascript code: | const required_fields = ['email', 'name', 'order_id']; for (const field of required_fields) { if (!input[field]) { throw new Error(`Missing required field: ${field}`); } } // Validate email format const email_regex = /^[^\\s@]+@[^\\s@]+\\.[^\\s@]+$/; if (!email_regex.test(input.email)) { throw new Error('Invalid email format'); } return { validated: true }; ``` ### 5. Use Idempotency Keys For operations that shouldn't be repeated (payments, record creation), use idempotency keys: ```yaml - id: create_charge action: http_request config: url: \"https://api.stripe.com/v1/charges\" method: POST headers: Idempotency-Key: \"{{workflow.id}}-{{execution.id}}\" body: amount: \"{{amount}}\" ``` ### 6. Monitor and Log Add logging steps for important workflow milestones: ```yaml - id: log_start action: database_query config: query: \"INSERT INTO workflow_audit (execution_id, step, timestamp) VALUES ($1, $2, $3)\" parameters: - \"{{execution.id}}\" - \"workflow_started\" - \"{{now()}}\" ``` ### 7.",
              "score": null
            }
          ],
          "latency_ms": 5.57141299941577,
          "query_id": "realistic_012",
          "dimension": "contextual",
          "query": "I'm implementing JWT verification, what algorithm should I expect?"
        },
        {
          "key_facts": [
            "RS256",
            "RS256 algorithm",
            "RS256 signing algorithm"
          ],
          "found_facts": [
            "RS256"
          ],
          "missed_facts": [
            "RS256 algorithm",
            "RS256 signing algorithm"
          ],
          "coverage": 0.3333333333333333,
          "expected_docs": [
            "api_reference"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_2",
              "content": "**Verification:** ```bash # Decode JWT to check expiration echo $CF_ACCESS_TOKEN | cut -d'.' -f2 | base64 -d | jq '.exp' # Compare with current time date +%s ``` **Resolution:** ```bash # Refresh the access token curl -X POST https://api.cloudflow.io/auth/refresh \\ -H \"Content-Type: application/json\" \\ -d '{\"refresh_token\": \"'$CF_REFRESH_TOKEN'\"}' # Update environment variable export CF_ACCESS_TOKEN=\"<new_token>\" ``` **2. Invalid JWT Signature** Error: `JWT signature verification failed` **Causes:** - Token was modified or corrupted - Using wrong signing key - Token generated with different secret **Resolution:** ```bash # Validate token structure cloudflow auth validate --token $CF_ACCESS_TOKEN # Generate new token cloudflow auth login --username <user> --password <pass> # For service accounts cloudflow auth service-account --create --name <sa-name> --scopes \"workflow:read,workflow:write\" ``` **3. Clock Skew Issues** JWT validation fails when system clocks are out of sync (tolerance: \u00b1300 seconds).",
              "score": null
            },
            {
              "doc_id": "user_guide",
              "chunk_id": "user_guide_fix_20",
              "content": "Validate Input Data Always validate trigger data before processing: ```yaml - id: validate_input action: javascript code: | const required_fields = ['email', 'name', 'order_id']; for (const field of required_fields) { if (!input[field]) { throw new Error(`Missing required field: ${field}`); } } // Validate email format const email_regex = /^[^\\s@]+@[^\\s@]+\\.[^\\s@]+$/; if (!email_regex.test(input.email)) { throw new Error('Invalid email format'); } return { validated: true }; ``` ### 5. Use Idempotency Keys For operations that shouldn't be repeated (payments, record creation), use idempotency keys: ```yaml - id: create_charge action: http_request config: url: \"https://api.stripe.com/v1/charges\" method: POST headers: Idempotency-Key: \"{{workflow.id}}-{{execution.id}}\" body: amount: \"{{amount}}\" ``` ### 6. Monitor and Log Add logging steps for important workflow milestones: ```yaml - id: log_start action: database_query config: query: \"INSERT INTO workflow_audit (execution_id, step, timestamp) VALUES ($1, $2, $3)\" parameters: - \"{{execution.id}}\" - \"workflow_started\" - \"{{now()}}\" ``` ### 7.",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_16",
              "content": "Data Validation Errors** ``` ValidationError: Field 'customer_id' is required but missing in 234 records ``` **Resolution:** ```bash # Add data quality checks cloudflow workflows update wf_9k2n4m8p1q \\ --step data_ingestion \\ --add-validator required_fields \\ --validator-config '{\"fields\": [\"customer_id\", \"timestamp\", \"amount\"]}' # Configure error handling cloudflow workflows update wf_9k2n4m8p1q \\ --step data_validation \\ --on-error continue \\ --error-threshold 5% # Fail if > 5% of records invalid ``` **2. External API Failures** ``` ExternalAPIError: API request to https://partner-api.example.com failed with status 502 ``` **Resolution:** ```bash # Add circuit breaker cloudflow workflows update wf_9k2n4m8p1q \\ --step external_api_call \\ --circuit-breaker-enabled true \\ --circuit-breaker-threshold 5 \\ --circuit-breaker-timeout 30000 # Configure fallback behavior cloudflow workflows update wf_9k2n4m8p1q \\ --step external_api_call \\ --fallback-action use_cached_data \\ --cache-ttl 3600 ``` --- ## Rate Limiting & Throttling ### 429 Too Many Requests #### Error Response ```json { \"error\": \"rate_limit_exceeded\", \"message\": \"API rate limit exceeded.",
              "score": null
            },
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_2",
              "content": "JWTs must include the following claims: - `iss` (issuer): Your application identifier - `sub` (subject): User or service account ID - `aud` (audience): `https://api.cloudflow.io` - `exp` (expiration): Unix timestamp (max 3600 seconds from `iat`) - `iat` (issued at): Unix timestamp - `scope`: Space-separated list of requested scopes Example JWT header: ```python import jwt import time from cryptography.hazmat.primitives import serialization from cryptography.hazmat.backends import default_backend # Load your private key with open('private_key.pem', 'rb') as key_file: private_key = serialization.load_pem_private_key( key_file.read(), password=None, backend=default_backend() ) # Create JWT payload payload = { 'iss': 'your-app-id', 'sub': 'user-12345', 'aud': 'https://api.cloudflow.io', 'exp': int(time.time()) + 3600, 'iat': int(time.time()), 'scope': 'workflows:read workflows:write' } # Generate token token = jwt.encode(payload, private_key, algorithm='RS256') # Use in API request headers = {'Authorization': f'Bearer {token}'} ``` **Token Expiration:** All tokens expire after 3600 seconds (1 hour). Implement token refresh logic in your application.",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_5",
              "content": "Password hashing: Argon2id with 64MB memory, 4 iterations - Token rotation on refresh to prevent replay attacks - Brute force protection: 5 failed attempts \u2192 15-minute lockout - Secrets stored in HashiCorp Vault, rotated every 90 days **Performance Targets**: - Token validation: < 10ms (P99) - Token generation: < 50ms (P99) - Throughput: 5,000 RPS for validation operations --- ### Workflow Engine **Purpose**: Core orchestration service that executes workflow definitions, manages state transitions, and coordinates task execution across distributed systems.",
              "score": null
            }
          ],
          "latency_ms": 5.575540999416262,
          "query_id": "realistic_012",
          "dimension": "negation",
          "query": "Why doesn't HS256 work for token validation?"
        },
        {
          "key_facts": [
            "TTL: 1 hour",
            "Workflow Definitions: TTL: 1 hour"
          ],
          "found_facts": [
            "TTL: 1 hour"
          ],
          "missed_facts": [
            "Workflow Definitions: TTL: 1 hour"
          ],
          "coverage": 0.5,
          "expected_docs": [
            "architecture_overview"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_19",
              "content": "Store result in cache with TTL c. Return data to application ``` **Implementation Example** (Workflow Definition): ```javascript async function getWorkflowDefinition(workflowId) { const cacheKey = `workflow:def:${workflowId}`; // Step 1: Check cache let workflow = await redis.get(cacheKey); if (workflow) { metrics.increment('cache.hit.workflow_def'); return JSON.parse(workflow); } // Step 2: Cache miss - query database metrics.increment('cache.miss.workflow_def'); workflow = await db.query( 'SELECT * FROM workflows WHERE id = $1', [workflowId] ); // Step 3: Store in cache (1 hour TTL) await redis.setex(cacheKey, 3600, JSON.stringify(workflow)); return workflow; } ``` ### Write-Through Pattern Used for critical data where cache consistency is paramount: ``` 1. Application writes to database 2. Database transaction commits 3. Application updates cache 4. Return success to client ``` Applied to: User profiles, authentication sessions, system configuration.",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_20",
              "content": "### Cache Invalidation Strategies **Time-Based Expiration (TTL)**: - Short-lived data: 5-15 minutes (session tokens, rate limit counters) - Medium-lived data: 1 hour (workflow definitions, templates) - Long-lived data: 24 hours (static configuration) **Event-Based Invalidation**: ``` Database Update Event \u2192 Kafka (cache.invalidation topic) \u2502 \u25bc All service instances consume event \u2502 \u25bc Redis DEL for affected keys ``` **Pattern-Based Invalidation**: - Use Redis SCAN + DEL for wildcard patterns - Example: Invalidate all user caches: `user:*:{user_id}` **Cache Warming**: - Scheduled job runs every hour to pre-populate frequently accessed data - Targets: Top 1000 workflows, system configuration, active user profiles - Reduces cache miss rate during traffic spikes ### Multi-Level Caching Application implements L1 (in-memory) and L2 (Redis) caching: ``` Request \u2192 L1 Cache (Node.js Map, 1000 entry LRU) \u2502 MISS \u2502 \u25bc L2 Cache (Redis, distributed) \u2502 MISS \u2502 \u25bc Database (PostgreSQL) ``` L1 cache reduces Redis network calls by 40%",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_24",
              "content": "Operations** (P99 latency): - `GET /workflows/{id}`: < 50ms (cache hit), < 150ms (cache miss) - `POST /workflows`: < 200ms (includes validation and database write) - `POST /workflows/{id}/execute`: < 100ms (async, returns execution ID) - `GET /workflows/{id}/history`: < 300ms (paginated, 50 records per page) **Workflow Execution** (P99 latency): - Simple workflow (< 5 steps): < 2 seconds - Medium workflow (5-15 steps): < 5 seconds - Complex workflow (> 15 steps): < 15 seconds - Parallel execution: Linear scaling up to 10 concurrent branches **Database Operations** (P99 latency): - Simple SELECT: < 5ms - JOIN query: < 20ms - INSERT/UPDATE: < 10ms - Batch operations: < 100ms (batch size: 100 records) **Cache Operations** (P99 latency): - Redis GET: < 2ms - Redis SET: < 3ms - Redis DEL: < 2ms ### Throughput Capacity **API Gateway**: - Sustained: 10,000 requests per second - Peak: 25,000 requests per second (5-minute burst) - Rate",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_15",
              "content": "**Workflow Definitions**: - Key pattern: `workflow:def:{workflow_id}` - TTL: 1 hour - Data: Parsed workflow JSON - Invalidation: On workflow update or manual flush 5. **User Profiles**: - Key pattern: `user:profile:{user_id}` - TTL: 30 minutes - Data: User metadata (name, email, roles) - Invalidation: On profile update **Cache Hit Rates** (Production Metrics): - Session lookups: 98.5% - Workflow definitions: 94.2% - Template cache: 99.1% - User profiles: 91.8% **Performance Characteristics**: - GET operation P99: < 2ms - SET operation P99: < 3ms - Throughput: 100,000 operations per second - Network latency: < 1ms (same AZ) --- ### Kafka Event Streaming **Cluster Configuration**: - 5 broker nodes (distributed across 3 AZs) - Instance type: kafka.m5.2xlarge (8 vCPU, 32GB RAM) - Storage: 10TB per broker (gp3 SSD) - ZooKeeper ensemble: 3 nodes for cluster coordination - Replication factor: 3 (min in-sync replicas: 2) **Topic Architecture**: ``` workflow.events (32 partitions): - Workflow lifecycle events",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_14",
              "content": "< 20ms - Write query P99: < 50ms - Transaction throughput: 15,000 TPS - Replication lag: < 500ms --- ### Redis Caching Layer **Cluster Configuration**: - Redis Cluster with 6 nodes (3 primary + 3 replica) - Instance type: cache.r6g.xlarge (4 vCPU, 26GB RAM) - Total memory: 78GB usable cache space - Persistence: RDB snapshots every 5 minutes + AOF - Eviction policy: allkeys-lru **Cache Usage Patterns**: 1. **Session Storage**: - Key pattern: `session:{user_id}` - TTL: 15 minutes (aligned with JWT expiry) - Data: User session state, preferences - Invalidation: On logout or password change 2. **Rate Limiting Counters**: - Key pattern: `ratelimit:{api_key}:{window}` - TTL: 60 seconds (sliding window) - Data: Request count per window - Algorithm: Token bucket with Redis INCR 3. **Compiled Templates**: - Key pattern: `template:{template_id}:{version}` - TTL: 1 hour - Data: Compiled Handlebars template - Invalidation: On template update 4.",
              "score": null
            }
          ],
          "latency_ms": 5.372222000005422,
          "query_id": "realistic_013",
          "dimension": "original",
          "query": "What is the Redis cache TTL for workflow definitions?"
        },
        {
          "key_facts": [
            "TTL: 1 hour",
            "Workflow Definitions: TTL: 1 hour"
          ],
          "found_facts": [
            "TTL: 1 hour"
          ],
          "missed_facts": [
            "Workflow Definitions: TTL: 1 hour"
          ],
          "coverage": 0.5,
          "expected_docs": [
            "architecture_overview"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_15",
              "content": "**Workflow Definitions**: - Key pattern: `workflow:def:{workflow_id}` - TTL: 1 hour - Data: Parsed workflow JSON - Invalidation: On workflow update or manual flush 5. **User Profiles**: - Key pattern: `user:profile:{user_id}` - TTL: 30 minutes - Data: User metadata (name, email, roles) - Invalidation: On profile update **Cache Hit Rates** (Production Metrics): - Session lookups: 98.5% - Workflow definitions: 94.2% - Template cache: 99.1% - User profiles: 91.8% **Performance Characteristics**: - GET operation P99: < 2ms - SET operation P99: < 3ms - Throughput: 100,000 operations per second - Network latency: < 1ms (same AZ) --- ### Kafka Event Streaming **Cluster Configuration**: - 5 broker nodes (distributed across 3 AZs) - Instance type: kafka.m5.2xlarge (8 vCPU, 32GB RAM) - Storage: 10TB per broker (gp3 SSD) - ZooKeeper ensemble: 3 nodes for cluster coordination - Replication factor: 3 (min in-sync replicas: 2) **Topic Architecture**: ``` workflow.events (32 partitions): - Workflow lifecycle events",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_19",
              "content": "Store result in cache with TTL c. Return data to application ``` **Implementation Example** (Workflow Definition): ```javascript async function getWorkflowDefinition(workflowId) { const cacheKey = `workflow:def:${workflowId}`; // Step 1: Check cache let workflow = await redis.get(cacheKey); if (workflow) { metrics.increment('cache.hit.workflow_def'); return JSON.parse(workflow); } // Step 2: Cache miss - query database metrics.increment('cache.miss.workflow_def'); workflow = await db.query( 'SELECT * FROM workflows WHERE id = $1', [workflowId] ); // Step 3: Store in cache (1 hour TTL) await redis.setex(cacheKey, 3600, JSON.stringify(workflow)); return workflow; } ``` ### Write-Through Pattern Used for critical data where cache consistency is paramount: ``` 1. Application writes to database 2. Database transaction commits 3. Application updates cache 4. Return success to client ``` Applied to: User profiles, authentication sessions, system configuration.",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_24",
              "content": "Operations** (P99 latency): - `GET /workflows/{id}`: < 50ms (cache hit), < 150ms (cache miss) - `POST /workflows`: < 200ms (includes validation and database write) - `POST /workflows/{id}/execute`: < 100ms (async, returns execution ID) - `GET /workflows/{id}/history`: < 300ms (paginated, 50 records per page) **Workflow Execution** (P99 latency): - Simple workflow (< 5 steps): < 2 seconds - Medium workflow (5-15 steps): < 5 seconds - Complex workflow (> 15 steps): < 15 seconds - Parallel execution: Linear scaling up to 10 concurrent branches **Database Operations** (P99 latency): - Simple SELECT: < 5ms - JOIN query: < 20ms - INSERT/UPDATE: < 10ms - Batch operations: < 100ms (batch size: 100 records) **Cache Operations** (P99 latency): - Redis GET: < 2ms - Redis SET: < 3ms - Redis DEL: < 2ms ### Throughput Capacity **API Gateway**: - Sustained: 10,000 requests per second - Peak: 25,000 requests per second (5-minute burst) - Rate",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_13",
              "content": "Core Tables: - users (5M rows): User accounts and profiles - workflows (2M rows): Workflow definitions - workflow_executions (500M rows, partitioned): Execution history - workflow_steps (2B rows, partitioned): Individual step records - schedules (100K rows): Scheduled workflow triggers - notifications (1B rows, partitioned): Notification delivery log Partitioning Strategy: - workflow_executions: Monthly partitions by created_at - workflow_steps: Monthly partitions by created_at - notifications: Weekly partitions by created_at - Automatic partition creation via cron job - Retention policy: Drop partitions older than 12 months ``` **Indexing Strategy**: - Primary keys: UUIDs with B-tree indexes - Frequently queried columns: Compound indexes (e.g., user_id + status + created_at) - JSONB columns: GIN indexes for workflow definitions - Full-text search: GiST indexes on description fields **Connection Pooling**: - PgBouncer in transaction mode - Pool size: 100 connections per service - Max client connections: 2000 - Connection timeout: 30 seconds **Performance Characteristics**: - Read query P99:",
              "score": null
            },
            {
              "doc_id": "user_guide",
              "chunk_id": "user_guide_fix_17",
              "content": "### Execution Timeout - **Default**: 3600 seconds (60 minutes) - **Behavior**: Workflows exceeding this timeout are automatically terminated - **Custom Timeouts**: Enterprise plans can request custom timeout limits **Setting Step-Level Timeouts:** ```yaml - id: long_running_task action: http_request config: url: \"https://api.example.com/process\" timeout: 300 # 5 minutes for this specific step ``` ### Execution Limits - **Maximum**: 1000 executions per day (per workflow) - **Rate Limiting**: 100 concurrent executions per workflow - **Burst Limit**: 10 executions per second **What happens when limits are reached:** - New executions are queued automatically - Webhook triggers return HTTP 429 (Too Many Requests) - Scheduled executions are skipped (logged in audit trail) - Email notifications sent to workflow owner **Monitoring Usage:** View real-time metrics in your workflow dashboard: - Executions today: 847 / 1000 - Average duration: 12.3s - Success rate: 98.2% - Current queue depth: 3 ### Data Limits - **Maximum request/response size**: 10MB per",
              "score": null
            }
          ],
          "latency_ms": 5.307362000166904,
          "query_id": "realistic_013",
          "dimension": "synonym",
          "query": "How long are workflow definitions cached?"
        },
        {
          "key_facts": [
            "TTL: 1 hour",
            "Workflow Definitions: TTL: 1 hour"
          ],
          "found_facts": [],
          "missed_facts": [
            "TTL: 1 hour",
            "Workflow Definitions: TTL: 1 hour"
          ],
          "coverage": 0.0,
          "expected_docs": [
            "architecture_overview"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "user_guide",
              "chunk_id": "user_guide_fix_21",
              "content": "Keep Workflows Modular Break complex workflows into smaller, reusable components: - Use sub-workflows for repeated logic - Trigger child workflows via webhooks - Share common configurations via templates ### 8. Test Thoroughly Before activating a workflow: 1. Use test mode with sample data 2. Verify all actions execute correctly 3. Test error handling paths 4. Review execution logs 5. Start with a limited scope (e.g., test channel, small dataset) ### 9. Document Your Workflows Add descriptions to workflows and steps: ```yaml workflow: name: \"Daily Sales Report\" description: | Generates a daily sales report and distributes it to the sales team. Runs at 8:00 AM EST Monday-Friday. Data source: PostgreSQL sales database Recipients: sales-team@company.com steps: - id: fetch_sales name: \"Fetch Yesterday's Sales\" description: \"Query database for all completed orders from previous day\" ``` ### 10.",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_24",
              "content": "Operations** (P99 latency): - `GET /workflows/{id}`: < 50ms (cache hit), < 150ms (cache miss) - `POST /workflows`: < 200ms (includes validation and database write) - `POST /workflows/{id}/execute`: < 100ms (async, returns execution ID) - `GET /workflows/{id}/history`: < 300ms (paginated, 50 records per page) **Workflow Execution** (P99 latency): - Simple workflow (< 5 steps): < 2 seconds - Medium workflow (5-15 steps): < 5 seconds - Complex workflow (> 15 steps): < 15 seconds - Parallel execution: Linear scaling up to 10 concurrent branches **Database Operations** (P99 latency): - Simple SELECT: < 5ms - JOIN query: < 20ms - INSERT/UPDATE: < 10ms - Batch operations: < 100ms (batch size: 100 records) **Cache Operations** (P99 latency): - Redis GET: < 2ms - Redis SET: < 3ms - Redis DEL: < 2ms ### Throughput Capacity **API Gateway**: - Sustained: 10,000 requests per second - Peak: 25,000 requests per second (5-minute burst) - Rate",
              "score": null
            },
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_7",
              "content": "**Endpoint:** `GET /workflows/{workflow_id}` **Example Request:** ```bash curl -H \"X-API-Key: YOUR_API_KEY\" \\ https://api.cloudflow.io/v2/workflows/wf_8x7k2m9p ``` **Example Response (200 OK):** ```json { \"id\": \"wf_8x7k2m9p\", \"name\": \"Data Processing Pipeline\", \"description\": \"Processes customer data every hour\", \"status\": \"active\", \"trigger\": { \"type\": \"schedule\", \"cron\": \"0 * * * *\", \"timezone\": \"UTC\" }, \"steps\": [ { \"id\": \"step_1\", \"type\": \"fetch_data\", \"source\": \"customer_db\", \"query\": \"SELECT * FROM customers WHERE updated_at > :last_run\" }, { \"id\": \"step_2\", \"type\": \"transform\", \"script_id\": \"scr_transform_v3\" } ], \"metrics\": { \"total_runs\": 1247, \"success_rate\": 98.7, \"avg_duration_ms\": 3420, \"last_error\": null }, \"created_at\": \"2026-01-15T10:30:00Z\", \"updated_at\": \"2026-01-23T14:22:00Z\" } ``` #### Update Workflow Modify an existing workflow's configuration. **Endpoint:** `PATCH /workflows/{workflow_id}` **Example Request:** ```bash curl -X PATCH https://api.cloudflow.io/v2/workflows/wf_8x7k2m9p \\ -H \"X-API-Key: YOUR_API_KEY\" \\ -H \"Content-Type: application/json\" \\ -d '{\"status\": \"paused\"}' ``` **Example Response (200 OK):** ```json { \"id\": \"wf_8x7k2m9p\", \"status\": \"paused\", \"updated_at\": \"2026-01-24T10:30:00Z\" } ``` #### Delete Workflow Permanently delete a workflow.",
              "score": null
            },
            {
              "doc_id": "user_guide",
              "chunk_id": "user_guide_fix_16",
              "content": "fixing issues - Export failed executions for analysis - Set up alerts for DLQ threshold breaches **Accessing the DLQ:** 1. Navigate to **Workflows** > **[Your Workflow]** > **Dead Letter Queue** 2. Filter by error type, date range, or execution ID 3. Click an execution to view full details 4. Click **\"Retry\"** to reprocess with the same input data ### Error Notifications Get notified when workflows fail: ```yaml workflow: notifications: on_failure: - type: email to: \"ops-team@company.com\" - type: slack channel: \"#alerts\" message: \"Workflow {{workflow.name}} failed: {{error.message}}\" on_success_after_retry: - type: slack channel: \"#monitoring\" message: \"Workflow recovered after {{error.attempts}} attempts\" ``` ## Workflow Limits CloudFlow enforces the following limits to ensure platform stability and performance: ### Steps Per Workflow - **Maximum**: 50 steps per workflow - **Recommendation**: Keep workflows focused and modular. If you need more steps, consider splitting into multiple workflows connected via webhooks.",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_6",
              "content": "Optimize Queries** Common optimization techniques: **Add missing indexes:** ```sql -- Index for workflow lookup by workspace CREATE INDEX CONCURRENTLY idx_workflows_workspace_created ON workflows(workspace_id, created_at DESC); -- Index for execution status queries CREATE INDEX CONCURRENTLY idx_executions_status_created ON executions(workflow_id, status, created_at DESC) WHERE status IN ('running', 'pending'); -- Composite index for common filter combinations CREATE INDEX CONCURRENTLY idx_executions_workspace_status ON executions(workspace_id, status, created_at) INCLUDE (error_message, retry_count); ``` **Use query result caching:** ```bash # Enable query cache for workspace metadata cloudflow config set cache.workspace.ttl 3600 # Configure Redis cache backend cloudflow config set cache.backend redis cloudflow config set cache.redis.host redis.cloudflow.svc.cluster.local cloudflow config set cache.redis.port 6379 ``` ### High API Latency #### Latency Breakdown Analysis ```bash # Generate latency report cloudflow metrics latency-report --endpoint \"/api/v1/workflows\" --last 24h # Sample output: # Endpoint: POST /api/v1/workflows/execute # P50: 245ms | P95: 1823ms | P99: 4521ms # Breakdown: # - Auth: 45ms (18%) # - DB Query: 156ms (64%)",
              "score": null
            }
          ],
          "latency_ms": 5.400304000431788,
          "query_id": "realistic_013",
          "dimension": "problem",
          "query": "My workflow updates aren't reflecting immediately"
        },
        {
          "key_facts": [
            "TTL: 1 hour",
            "Workflow Definitions: TTL: 1 hour"
          ],
          "found_facts": [],
          "missed_facts": [
            "TTL: 1 hour",
            "Workflow Definitions: TTL: 1 hour"
          ],
          "coverage": 0.0,
          "expected_docs": [
            "architecture_overview"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_19",
              "content": "Store result in cache with TTL c. Return data to application ``` **Implementation Example** (Workflow Definition): ```javascript async function getWorkflowDefinition(workflowId) { const cacheKey = `workflow:def:${workflowId}`; // Step 1: Check cache let workflow = await redis.get(cacheKey); if (workflow) { metrics.increment('cache.hit.workflow_def'); return JSON.parse(workflow); } // Step 2: Cache miss - query database metrics.increment('cache.miss.workflow_def'); workflow = await db.query( 'SELECT * FROM workflows WHERE id = $1', [workflowId] ); // Step 3: Store in cache (1 hour TTL) await redis.setex(cacheKey, 3600, JSON.stringify(workflow)); return workflow; } ``` ### Write-Through Pattern Used for critical data where cache consistency is paramount: ``` 1. Application writes to database 2. Database transaction commits 3. Application updates cache 4. Return success to client ``` Applied to: User profiles, authentication sessions, system configuration.",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_20",
              "content": "### Cache Invalidation Strategies **Time-Based Expiration (TTL)**: - Short-lived data: 5-15 minutes (session tokens, rate limit counters) - Medium-lived data: 1 hour (workflow definitions, templates) - Long-lived data: 24 hours (static configuration) **Event-Based Invalidation**: ``` Database Update Event \u2192 Kafka (cache.invalidation topic) \u2502 \u25bc All service instances consume event \u2502 \u25bc Redis DEL for affected keys ``` **Pattern-Based Invalidation**: - Use Redis SCAN + DEL for wildcard patterns - Example: Invalidate all user caches: `user:*:{user_id}` **Cache Warming**: - Scheduled job runs every hour to pre-populate frequently accessed data - Targets: Top 1000 workflows, system configuration, active user profiles - Reduces cache miss rate during traffic spikes ### Multi-Level Caching Application implements L1 (in-memory) and L2 (Redis) caching: ``` Request \u2192 L1 Cache (Node.js Map, 1000 entry LRU) \u2502 MISS \u2502 \u25bc L2 Cache (Redis, distributed) \u2502 MISS \u2502 \u25bc Database (PostgreSQL) ``` L1 cache reduces Redis network calls by 40%",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_18",
              "content": "### Saga Pattern Distributed transaction management for multi-service workflows: ``` Step 1: Create Order \u2192 SUCCESS \u2192 Step 2: Reserve Inventory \u2502 FAILURE \u2502 \u25bc Compensating Transaction \u2502 \u25bc Cancel Order (Rollback) ``` Orchestrated by Workflow Engine with compensation logic defined in workflow DSL. ### Dead Letter Queue (DLQ) Failed messages after all retry attempts are routed to DLQ: ``` Message processing fails (3 retries with exponential backoff) \u2502 \u25bc Publish to dead-letter-queue topic \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u25bc \u25bc Alert to PagerDuty Store in PostgreSQL for analysis \u2502 \u2502 \u25bc \u25bc Manual investigation Automated pattern detection ``` DLQ Processing SLA: < 4 hours for critical events, < 24 hours for non-critical. --- ## Caching Strategy ### Cache-Aside Pattern (Lazy Loading) Primary caching pattern used across all services: ``` 1. Application checks cache (Redis GET) 2. If HIT \u2192 Return cached data 3. If MISS: a. Query database (PostgreSQL) b.",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_24",
              "content": "Operations** (P99 latency): - `GET /workflows/{id}`: < 50ms (cache hit), < 150ms (cache miss) - `POST /workflows`: < 200ms (includes validation and database write) - `POST /workflows/{id}/execute`: < 100ms (async, returns execution ID) - `GET /workflows/{id}/history`: < 300ms (paginated, 50 records per page) **Workflow Execution** (P99 latency): - Simple workflow (< 5 steps): < 2 seconds - Medium workflow (5-15 steps): < 5 seconds - Complex workflow (> 15 steps): < 15 seconds - Parallel execution: Linear scaling up to 10 concurrent branches **Database Operations** (P99 latency): - Simple SELECT: < 5ms - JOIN query: < 20ms - INSERT/UPDATE: < 10ms - Batch operations: < 100ms (batch size: 100 records) **Cache Operations** (P99 latency): - Redis GET: < 2ms - Redis SET: < 3ms - Redis DEL: < 2ms ### Throughput Capacity **API Gateway**: - Sustained: 10,000 requests per second - Peak: 25,000 requests per second (5-minute burst) - Rate",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_20",
              "content": "Leverage caching:** ```bash # Enable client-side caching export CLOUDFLOW_CACHE_ENABLED=true export CLOUDFLOW_CACHE_TTL=300 # Cache workflow metadata cloudflow workflows list --use-cache --cache-ttl 600 ``` --- ## Log Analysis & Debugging ### Accessing CloudFlow Logs #### Kubernetes Deployments ```bash # List all CloudFlow pods kubectl get pods -n cloudflow # Tail logs from API server kubectl logs -f -n cloudflow deployment/cloudflow-api --tail=100 # Get logs from specific pod kubectl logs -n cloudflow cloudflow-api-7d4f6b8c9d-x7k2m # Get logs from previous crashed pod kubectl logs -n cloudflow cloudflow-api-7d4f6b8c9d-x7k2m --previous # Get logs from all pods in deployment kubectl logs -n cloudflow deployment/cloudflow-api --all-containers=true # Stream logs from multiple pods kubectl logs -n cloudflow -l app=cloudflow-api -f --max-log-requests=10 ``` #### Log Levels CloudFlow supports the following log levels: - `TRACE` - Very detailed debugging information - `DEBUG` - Detailed debugging information - `INFO` - Informational messages (default) - `WARN` - Warning messages - `ERROR` - Error messages",
              "score": null
            }
          ],
          "latency_ms": 5.415101999460603,
          "query_id": "realistic_013",
          "dimension": "casual",
          "query": "cache ttl workflows"
        },
        {
          "key_facts": [
            "TTL: 1 hour",
            "Workflow Definitions: TTL: 1 hour"
          ],
          "found_facts": [],
          "missed_facts": [
            "TTL: 1 hour",
            "Workflow Definitions: TTL: 1 hour"
          ],
          "coverage": 0.0,
          "expected_docs": [
            "architecture_overview"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_19",
              "content": "Store result in cache with TTL c. Return data to application ``` **Implementation Example** (Workflow Definition): ```javascript async function getWorkflowDefinition(workflowId) { const cacheKey = `workflow:def:${workflowId}`; // Step 1: Check cache let workflow = await redis.get(cacheKey); if (workflow) { metrics.increment('cache.hit.workflow_def'); return JSON.parse(workflow); } // Step 2: Cache miss - query database metrics.increment('cache.miss.workflow_def'); workflow = await db.query( 'SELECT * FROM workflows WHERE id = $1', [workflowId] ); // Step 3: Store in cache (1 hour TTL) await redis.setex(cacheKey, 3600, JSON.stringify(workflow)); return workflow; } ``` ### Write-Through Pattern Used for critical data where cache consistency is paramount: ``` 1. Application writes to database 2. Database transaction commits 3. Application updates cache 4. Return success to client ``` Applied to: User profiles, authentication sessions, system configuration.",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_24",
              "content": "Operations** (P99 latency): - `GET /workflows/{id}`: < 50ms (cache hit), < 150ms (cache miss) - `POST /workflows`: < 200ms (includes validation and database write) - `POST /workflows/{id}/execute`: < 100ms (async, returns execution ID) - `GET /workflows/{id}/history`: < 300ms (paginated, 50 records per page) **Workflow Execution** (P99 latency): - Simple workflow (< 5 steps): < 2 seconds - Medium workflow (5-15 steps): < 5 seconds - Complex workflow (> 15 steps): < 15 seconds - Parallel execution: Linear scaling up to 10 concurrent branches **Database Operations** (P99 latency): - Simple SELECT: < 5ms - JOIN query: < 20ms - INSERT/UPDATE: < 10ms - Batch operations: < 100ms (batch size: 100 records) **Cache Operations** (P99 latency): - Redis GET: < 2ms - Redis SET: < 3ms - Redis DEL: < 2ms ### Throughput Capacity **API Gateway**: - Sustained: 10,000 requests per second - Peak: 25,000 requests per second (5-minute burst) - Rate",
              "score": null
            },
            {
              "doc_id": "user_guide",
              "chunk_id": "user_guide_fix_17",
              "content": "### Execution Timeout - **Default**: 3600 seconds (60 minutes) - **Behavior**: Workflows exceeding this timeout are automatically terminated - **Custom Timeouts**: Enterprise plans can request custom timeout limits **Setting Step-Level Timeouts:** ```yaml - id: long_running_task action: http_request config: url: \"https://api.example.com/process\" timeout: 300 # 5 minutes for this specific step ``` ### Execution Limits - **Maximum**: 1000 executions per day (per workflow) - **Rate Limiting**: 100 concurrent executions per workflow - **Burst Limit**: 10 executions per second **What happens when limits are reached:** - New executions are queued automatically - Webhook triggers return HTTP 429 (Too Many Requests) - Scheduled executions are skipped (logged in audit trail) - Email notifications sent to workflow owner **Monitoring Usage:** View real-time metrics in your workflow dashboard: - Executions today: 847 / 1000 - Average duration: 12.3s - Success rate: 98.2% - Current queue depth: 3 ### Data Limits - **Maximum request/response size**: 10MB per",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_6",
              "content": "Optimize Queries** Common optimization techniques: **Add missing indexes:** ```sql -- Index for workflow lookup by workspace CREATE INDEX CONCURRENTLY idx_workflows_workspace_created ON workflows(workspace_id, created_at DESC); -- Index for execution status queries CREATE INDEX CONCURRENTLY idx_executions_status_created ON executions(workflow_id, status, created_at DESC) WHERE status IN ('running', 'pending'); -- Composite index for common filter combinations CREATE INDEX CONCURRENTLY idx_executions_workspace_status ON executions(workspace_id, status, created_at) INCLUDE (error_message, retry_count); ``` **Use query result caching:** ```bash # Enable query cache for workspace metadata cloudflow config set cache.workspace.ttl 3600 # Configure Redis cache backend cloudflow config set cache.backend redis cloudflow config set cache.redis.host redis.cloudflow.svc.cluster.local cloudflow config set cache.redis.port 6379 ``` ### High API Latency #### Latency Breakdown Analysis ```bash # Generate latency report cloudflow metrics latency-report --endpoint \"/api/v1/workflows\" --last 24h # Sample output: # Endpoint: POST /api/v1/workflows/execute # P50: 245ms | P95: 1823ms | P99: 4521ms # Breakdown: # - Auth: 45ms (18%) # - DB Query: 156ms (64%)",
              "score": null
            },
            {
              "doc_id": "user_guide",
              "chunk_id": "user_guide_fix_24",
              "content": "WHERE workflow_id = $2 parameters: - \"{{now()}}\" - \"{{workflow.id}}\" ``` ### Pattern 3: Multi-Step Approval Implement approval workflows with timeouts: ```yaml name: \"Expense Approval Workflow\" trigger: type: webhook method: POST steps: - id: create_approval_request action: database_query config: query: | INSERT INTO approvals (expense_id, amount, requester, status, created_at) VALUES ($1, $2, $3, 'pending', NOW()) RETURNING id parameters: - \"{{trigger.body.expense_id}}\" - \"{{trigger.body.amount}}\" - \"{{trigger.body.requester}}\" - id: notify_manager action: email config: to: \"{{trigger.body.manager_email}}\" subject: \"Expense Approval Required: ${{trigger.body.amount}}\" body: | An expense requires your approval: Amount: ${{trigger.body.amount}} Requester: {{trigger.body.requester}} Description: {{trigger.body.description}} Approve: https://app.company.com/approve/{{steps.create_approval_request.rows[0].id}} Reject: https://app.company.com/reject/{{steps.create_approval_request.rows[0].id}} - id: wait_for_approval action: wait_for_webhook config: timeout: 86400 # 24 hours webhook_path: \"/approval/{{steps.create_approval_request.rows[0].id}}\" - id: process_approval action: javascript code: | if (input.status === 'approved') { return { approved: true }; } else { throw new Error('Expense rejected'); } on_error: - id: notify_rejection action: email config: to: \"{{trigger.body.requester}}\" subject: \"Expense Rejected\" body: \"Your expense request has been rejected.\"",
              "score": null
            }
          ],
          "latency_ms": 5.935551998845767,
          "query_id": "realistic_013",
          "dimension": "contextual",
          "query": "After updating a workflow, how long until the cache expires?"
        },
        {
          "key_facts": [
            "TTL: 1 hour",
            "Workflow Definitions: TTL: 1 hour"
          ],
          "found_facts": [],
          "missed_facts": [
            "TTL: 1 hour",
            "Workflow Definitions: TTL: 1 hour"
          ],
          "coverage": 0.0,
          "expected_docs": [
            "architecture_overview"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "user_guide",
              "chunk_id": "user_guide_fix_12",
              "content": "To account for your local timezone: **Option 1: Convert to UTC** If you want a workflow to run at 9:00 AM EST (UTC-5), schedule it for 14:00 UTC: ``` 0 14 * * * # 9:00 AM EST = 14:00 UTC ``` **Option 2: Use Timezone Configuration** Specify a timezone in your workflow configuration: ```yaml schedule: cron: \"0 9 * * *\" timezone: \"America/New_York\" # IANA timezone identifier ``` **Supported Timezones:** CloudFlow supports all IANA timezone identifiers, including: - `America/New_York` (Eastern Time) - `America/Chicago` (Central Time) - `America/Los_Angeles` (Pacific Time) - `Europe/London` (GMT/BST) - `Asia/Tokyo` (Japan Standard Time) - `Australia/Sydney` (Australian Eastern Time) **Daylight Saving Time:** When using timezone configuration, CloudFlow automatically handles DST transitions. A workflow scheduled for 9:00 AM will run at 9:00 AM local time regardless of DST changes. ### Schedule Management **Creating a Schedule:** 1. Open your workflow in the editor 2.",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_24",
              "content": "Operations** (P99 latency): - `GET /workflows/{id}`: < 50ms (cache hit), < 150ms (cache miss) - `POST /workflows`: < 200ms (includes validation and database write) - `POST /workflows/{id}/execute`: < 100ms (async, returns execution ID) - `GET /workflows/{id}/history`: < 300ms (paginated, 50 records per page) **Workflow Execution** (P99 latency): - Simple workflow (< 5 steps): < 2 seconds - Medium workflow (5-15 steps): < 5 seconds - Complex workflow (> 15 steps): < 15 seconds - Parallel execution: Linear scaling up to 10 concurrent branches **Database Operations** (P99 latency): - Simple SELECT: < 5ms - JOIN query: < 20ms - INSERT/UPDATE: < 10ms - Batch operations: < 100ms (batch size: 100 records) **Cache Operations** (P99 latency): - Redis GET: < 2ms - Redis SET: < 3ms - Redis DEL: < 2ms ### Throughput Capacity **API Gateway**: - Sustained: 10,000 requests per second - Peak: 25,000 requests per second (5-minute burst) - Rate",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_7",
              "content": "# - Business Logic: 32ms (13%) # - Response Serialization: 12ms (5%) ``` **Network latency issues:** ```bash # Test connectivity to CloudFlow API time curl -w \"@curl-format.txt\" -o /dev/null -s https://api.cloudflow.io/health # Create curl-format.txt: cat > curl-format.txt << EOF time_namelookup: %{time_namelookup}s\\n time_connect: %{time_connect}s\\n time_appconnect: %{time_appconnect}s\\n time_pretransfer: %{time_pretransfer}s\\n time_redirect: %{time_redirect}s\\n time_starttransfer: %{time_starttransfer}s\\n ----------\\n time_total: %{time_total}s\\n EOF # Trace route to API endpoint traceroute api.cloudflow.io # Check DNS resolution time dig api.cloudflow.io | grep \"Query time\" ``` ### Memory Leaks #### Detection ```bash # Monitor CloudFlow service memory usage kubectl top pods -n cloudflow --sort-by=memory # Get detailed memory metrics for specific pod kubectl exec -n cloudflow deploy/cloudflow-api -- \\ curl localhost:9090/metrics | grep memory # Check for OOMKilled pods kubectl get pods -n cloudflow --field-selector=status.phase=Failed | \\ grep OOMKilled # Review memory limits and requests kubectl describe deployment cloudflow-api -n cloudflow | \\ grep -A 5 \"Limits\\|Requests\" ``` #### Common Causes",
              "score": null
            },
            {
              "doc_id": "user_guide",
              "chunk_id": "user_guide_fix_17",
              "content": "### Execution Timeout - **Default**: 3600 seconds (60 minutes) - **Behavior**: Workflows exceeding this timeout are automatically terminated - **Custom Timeouts**: Enterprise plans can request custom timeout limits **Setting Step-Level Timeouts:** ```yaml - id: long_running_task action: http_request config: url: \"https://api.example.com/process\" timeout: 300 # 5 minutes for this specific step ``` ### Execution Limits - **Maximum**: 1000 executions per day (per workflow) - **Rate Limiting**: 100 concurrent executions per workflow - **Burst Limit**: 10 executions per second **What happens when limits are reached:** - New executions are queued automatically - Webhook triggers return HTTP 429 (Too Many Requests) - Scheduled executions are skipped (logged in audit trail) - Email notifications sent to workflow owner **Monitoring Usage:** View real-time metrics in your workflow dashboard: - Executions today: 847 / 1000 - Average duration: 12.3s - Success rate: 98.2% - Current queue depth: 3 ### Data Limits - **Maximum request/response size**: 10MB per",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_13",
              "content": "Step 1: data_ingestion - Duration: 245s - Status: SUCCESS # Step 2: data_validation - Duration: 123s - Status: SUCCESS # Step 3: data_transformation - Duration: 3189s - Status: TIMEOUT # Step 4: data_export - Duration: 0s - Status: SKIPPED # Identify bottleneck step cloudflow workflows analyze exec_7h3j6k9m2n --show-bottlenecks ``` #### Solutions **1. Increase workflow timeout (if justified):** ```bash # Update workflow configuration cloudflow workflows update wf_9k2n4m8p1q \\ --timeout 7200 \\ --reason \"Large dataset processing requires extended time\" # Verify update cloudflow workflows get wf_9k2n4m8p1q | grep timeout ``` **2. Optimize slow steps:** ```bash # Enable parallel processing cloudflow workflows update wf_9k2n4m8p1q \\ --step data_transformation \\ --parallel-workers 8 \\ --batch-size 1000 # Add checkpointing for long operations cloudflow workflows update wf_9k2n4m8p1q \\ --step data_transformation \\ --enable-checkpointing \\ --checkpoint-interval 300 ``` **3.",
              "score": null
            }
          ],
          "latency_ms": 5.47331999950984,
          "query_id": "realistic_013",
          "dimension": "negation",
          "query": "Why are changes taking an hour to appear?"
        },
        {
          "key_facts": [
            "Prometheus",
            "Grafana",
            "Jaeger for distributed tracing"
          ],
          "found_facts": [],
          "missed_facts": [
            "Prometheus",
            "Grafana",
            "Jaeger for distributed tracing"
          ],
          "coverage": 0.0,
          "expected_docs": [
            "architecture_overview",
            "deployment_guide"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_0",
              "content": "# CloudFlow API Reference Version 2.1.0 | Last Updated: January 2026 ## Overview The CloudFlow API is a RESTful service that enables developers to programmatically manage cloud workflows, data pipelines, and automation tasks. This documentation provides comprehensive details on authentication, endpoints, request/response formats, error handling, and best practices. **Base URL:** `https://api.cloudflow.io/v2` **API Status:** https://status.cloudflow.io ## Authentication CloudFlow supports three authentication methods to suit different use cases and security requirements. ### API Keys API keys provide simple authentication for server-to-server communication. Include your API key in the request header: ```bash curl -H \"X-API-Key: cf_live_a1b2c3d4e5f6g7h8i9j0\" \\ https://api.cloudflow.io/v2/workflows ``` **Security Notes:** - Never expose API keys in client-side code - Rotate keys every 90 days - Use separate keys for development and production environments ### OAuth 2.0 OAuth 2.0 is recommended for applications that access CloudFlow on behalf of users. We support the Authorization Code flow with PKCE.",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_0",
              "content": "# CloudFlow Platform - System Architecture Overview **Document Version:** 2.3.1 **Last Updated:** January 15, 2026 **Owner:** Platform Architecture Team **Status:** Production ## Executive Summary CloudFlow is a distributed, cloud-native workflow automation platform designed to orchestrate complex business processes at scale. The platform processes over 2.5 million workflow executions daily with an average P99 latency of 180ms for API operations and 4.2 seconds for workflow execution. This document provides a comprehensive overview of the system architecture, including microservices design, data flow patterns, infrastructure decisions, and operational considerations. ## Table of Contents 1. [High-Level Architecture](#high-level-architecture) 2. [Microservices Breakdown](#microservices-breakdown) 3. [Data Flow Architecture](#data-flow-architecture) 4. [Database Architecture](#database-architecture) 5. [Message Queue Patterns](#message-queue-patterns) 6. [Caching Strategy](#caching-strategy) 7. [Security Architecture](#security-architecture) 8. [Performance Characteristics](#performance-characteristics) 9. [Disaster Recovery](#disaster-recovery) --- ## High-Level Architecture CloudFlow follows a microservices architecture pattern deployed across multiple availability zones in AWS.",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_4",
              "content": "CloudFlow uses role-based access control (RBAC) with the following hierarchy: - `viewer` - Read-only access - `developer` - Create and modify workflows (non-production) - `operator` - Execute workflows, view logs - `admin` - Full access to workspace - `platform-admin` - Cross-workspace administration **Verify resource permissions:** ```bash # Check effective permissions cloudflow rbac check \\ --user john.doe@company.com \\ --resource workflow:prod-pipeline \\ --action execute # List all policies affecting user cloudflow rbac policies --user john.doe@company.com --verbose ``` --- ## Performance Problems ### Slow Query Performance #### Symptoms - API response times > 5000ms - Database query latency warnings in logs - CloudFlow UI becomes unresponsive - Timeout errors: `Request exceeded maximum duration of 30000ms` #### Diagnosis Steps **1.",
              "score": null
            },
            {
              "doc_id": "user_guide",
              "chunk_id": "user_guide_fix_0",
              "content": "# CloudFlow User Guide Welcome to CloudFlow, the modern workflow automation platform that helps you connect your apps, automate repetitive tasks, and build powerful integrations without writing code. ## Table of Contents 1. [Getting Started](#getting-started) 2. [Workflow Creation](#workflow-creation) 3. [Available Actions](#available-actions) 4. [Variables and Expressions](#variables-and-expressions) 5. [Scheduling](#scheduling) 6. [Error Handling](#error-handling) 7. [Workflow Limits](#workflow-limits) 8. [Best Practices](#best-practices) 9. [Common Workflow Patterns](#common-workflow-patterns) ## Getting Started ### Account Setup Creating your CloudFlow account takes just a few minutes: 1. Navigate to https://app.cloudflow.io/signup 2. Enter your email address and create a strong password 3. Verify your email address by clicking the link sent to your inbox 4. Complete your profile by providing your organization name and use case 5. Choose your plan (Free, Professional, or Enterprise) Once your account is created, you'll be redirected to your CloudFlow dashboard where you can start building workflows immediately.",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_1",
              "content": "### Quick Diagnostic Checklist Before diving into specific issues, perform these initial checks: - Verify service health: `cloudflow status --all` - Check API connectivity: `curl -I https://api.cloudflow.io/health` - Review recent deployments: `kubectl get deployments -n cloudflow --sort-by=.metadata.creationTimestamp` - Inspect platform metrics: `cloudflow metrics --last 1h` --- ## Authentication & Authorization Issues ### 401 Unauthorized Errors #### Symptoms - API requests return `401 Unauthorized` - Error message: `Authentication credentials were not provided or are invalid` - Frontend displays \"Session expired\" message #### Common Causes **1. Token Expiration** CloudFlow access tokens expire after 3600 seconds (1 hour) by default. Refresh tokens are valid for 30 days.",
              "score": null
            }
          ],
          "latency_ms": 5.40422199992463,
          "query_id": "realistic_014",
          "dimension": "original",
          "query": "What monitoring tools does CloudFlow use?"
        },
        {
          "key_facts": [
            "Prometheus",
            "Grafana",
            "Jaeger for distributed tracing"
          ],
          "found_facts": [
            "Prometheus",
            "Grafana",
            "Jaeger for distributed tracing"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "architecture_overview",
            "deployment_guide"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_1",
              "content": "The platform is designed with the following core principles: - **Scalability**: Horizontal scaling for all services with auto-scaling groups - **Resilience**: Circuit breakers, bulkheads, and graceful degradation - **Observability**: Distributed tracing, centralized logging, and comprehensive metrics - **Security**: Zero-trust network architecture with mTLS encryption ### Architecture Diagram (Conceptual) ``` \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 Load Balancer (ALB) \u2502 \u2502 (TLS Termination - 443) \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u25bc \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 API Gateway Layer \u2502 \u2502 (Rate Limiting, Auth, Request Routing) \u2502 \u2514\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u2502 \u2502 \u2502 \u2502 \u25bc \u25bc \u25bc \u25bc \u25bc \u250c\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502Auth\u2502 \u2502Workflow\u2502 \u2502Scheduler\u2502 \u2502Notification\u2502 \u2502User Service\u2502 \u2502Svc \u2502 \u2502 Engine \u2502 \u2502 Service \u2502 \u2502 Service \u2502 \u2502 \u2502 \u2514\u2500\u252c\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u2502 \u2502 \u2502 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 \u2502 \u2502 \u25bc \u25bc \u25bc \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502PostgreSQL\u2502 \u2502 Redis \u2502 \u2502 Kafka \u2502 \u2502 Cluster \u2502 \u2502 Cluster \u2502 \u2502 Cluster \u2502",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_0",
              "content": "# CloudFlow Platform - System Architecture Overview **Document Version:** 2.3.1 **Last Updated:** January 15, 2026 **Owner:** Platform Architecture Team **Status:** Production ## Executive Summary CloudFlow is a distributed, cloud-native workflow automation platform designed to orchestrate complex business processes at scale. The platform processes over 2.5 million workflow executions daily with an average P99 latency of 180ms for API operations and 4.2 seconds for workflow execution. This document provides a comprehensive overview of the system architecture, including microservices design, data flow patterns, infrastructure decisions, and operational considerations. ## Table of Contents 1. [High-Level Architecture](#high-level-architecture) 2. [Microservices Breakdown](#microservices-breakdown) 3. [Data Flow Architecture](#data-flow-architecture) 4. [Database Architecture](#database-architecture) 5. [Message Queue Patterns](#message-queue-patterns) 6. [Caching Strategy](#caching-strategy) 7. [Security Architecture](#security-architecture) 8. [Performance Characteristics](#performance-characteristics) 9. [Disaster Recovery](#disaster-recovery) --- ## High-Level Architecture CloudFlow follows a microservices architecture pattern deployed across multiple availability zones in AWS.",
              "score": null
            },
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_0",
              "content": "# CloudFlow Platform - Deployment and Operations Guide **Version:** 2.4.0 **Last Updated:** January 2026 **Target Environment:** Production (AWS EKS) ## Table of Contents 1. [Overview](#overview) 2. [Prerequisites](#prerequisites) 3. [Infrastructure Setup](#infrastructure-setup) 4. [Kubernetes Deployment](#kubernetes-deployment) 5. [Database Configuration](#database-configuration) 6. [Monitoring and Observability](#monitoring-and-observability) 7. [Backup and Disaster Recovery](#backup-and-disaster-recovery) 8. [Scaling and Performance](#scaling-and-performance) 9. [Security and Compliance](#security-and-compliance) 10. [Troubleshooting](#troubleshooting) --- ## Overview CloudFlow is a cloud-native workflow orchestration platform designed for high-availability production environments. This guide provides comprehensive instructions for deploying and operating CloudFlow on Amazon EKS (Elastic Kubernetes Service).",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_8",
              "content": "**Technology**: Go with distributed locking via Redis **Replicas**: 4 pods (production), active-passive with leader election **Resource Allocation**: 2 vCPU, 4GB RAM per pod **Key Responsibilities**: - Parse and validate cron expressions (extended format supporting seconds) - Maintain schedule registry in PostgreSQL - Distributed scheduling with leader election (one active scheduler) - Missed execution handling with configurable catch-up policy - Schedule conflict detection and resolution - Time zone support for international schedules **Scheduling Architecture**: ``` PostgreSQL Schedules Table \u2502 \u25bc Leader Scheduler (elected via Redis) \u2502 \u251c\u2500 Scan for due schedules (every 10 seconds) \u251c\u2500 Acquire lock per schedule (prevents duplicates) \u251c\u2500 Publish to Kafka \u2192 workflow.events topic \u2514\u2500 Update last_run timestamp ``` **Schedule Types**: - **Cron-based**: `0 */5 * * * *` (every 5 minutes) - **One-time**: Specific timestamp for delayed execution - **Interval-based**: Every N seconds/minutes/hours **Reliability Features**: - Leader election using Redis with 30-second lease - Heartbeat mechanism",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_2",
              "content": "\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 ``` ### Technology Stack - **Runtime**: Node.js 20.x (API Gateway, Workflow Engine), Go 1.21 (Auth Service, Scheduler) - **Container Orchestration**: Kubernetes 1.28 (EKS) - **Service Mesh**: Istio 1.20 for service-to-service communication - **Databases**: PostgreSQL 15.4, Redis 7.2 - **Message Broker**: Apache Kafka 3.6 - **Monitoring**: Prometheus, Grafana, Jaeger for distributed tracing - **Secrets Management**: HashiCorp Vault 1.15 --- ## Microservices Breakdown ### API Gateway **Purpose**: Single entry point for all client requests, providing authentication, rate limiting, request routing, and protocol translation.",
              "score": null
            }
          ],
          "latency_ms": 5.481985999722383,
          "query_id": "realistic_014",
          "dimension": "synonym",
          "query": "Which observability platform is integrated?"
        },
        {
          "key_facts": [
            "Prometheus",
            "Grafana",
            "Jaeger for distributed tracing"
          ],
          "found_facts": [
            "Prometheus",
            "Grafana"
          ],
          "missed_facts": [
            "Jaeger for distributed tracing"
          ],
          "coverage": 0.6666666666666666,
          "expected_docs": [
            "architecture_overview",
            "deployment_guide"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_27",
              "content": "changes cloudflow audit log --last 2h --event-type \"deployment,configuration\" # Check metrics dashboard cloudflow metrics dashboard --incident-mode ``` #### Step 2: Gather Information (5-15 minutes) Create incident document with: - Incident timestamp and duration - Affected services and endpoints - Error rates and user impact - Recent changes or deployments - Relevant log excerpts - Correlation IDs for failed requests ```bash # Generate incident report cloudflow debug incident-report \\ --start \"2026-01-24T10:30:00Z\" \\ --end \"2026-01-24T11:00:00Z\" \\ --output incident-report.md # Capture system snapshot cloudflow debug snapshot --output snapshot-$(date +%Y%m%d-%H%M%S).tar.gz ``` #### Step 3: Escalate Based on Severity **For SEV-1 (Critical):** ```bash # Page on-call engineer cloudflow incident create \\ --severity SEV-1 \\ --title \"Complete API outage\" \\ --description \"All API requests returning 503\" \\ --page-oncall # Notify status page cloudflow status update \\ --status \"major_outage\" \\ --message \"We are investigating a complete service outage\" # Create war room cloudflow incident war-room create incident-2024012401",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_20",
              "content": "Leverage caching:** ```bash # Enable client-side caching export CLOUDFLOW_CACHE_ENABLED=true export CLOUDFLOW_CACHE_TTL=300 # Cache workflow metadata cloudflow workflows list --use-cache --cache-ttl 600 ``` --- ## Log Analysis & Debugging ### Accessing CloudFlow Logs #### Kubernetes Deployments ```bash # List all CloudFlow pods kubectl get pods -n cloudflow # Tail logs from API server kubectl logs -f -n cloudflow deployment/cloudflow-api --tail=100 # Get logs from specific pod kubectl logs -n cloudflow cloudflow-api-7d4f6b8c9d-x7k2m # Get logs from previous crashed pod kubectl logs -n cloudflow cloudflow-api-7d4f6b8c9d-x7k2m --previous # Get logs from all pods in deployment kubectl logs -n cloudflow deployment/cloudflow-api --all-containers=true # Stream logs from multiple pods kubectl logs -n cloudflow -l app=cloudflow-api -f --max-log-requests=10 ``` #### Log Levels CloudFlow supports the following log levels: - `TRACE` - Very detailed debugging information - `DEBUG` - Detailed debugging information - `INFO` - Informational messages (default) - `WARN` - Warning messages - `ERROR` - Error messages",
              "score": null
            },
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_10",
              "content": "The metrics include: - **HTTP Metrics**: Request rate, latency, error rate - **Database Metrics**: Connection pool status, query duration - **Worker Metrics**: Job queue size, processing time, failure rate - **System Metrics**: CPU usage, memory usage, heap statistics Example metrics exposed: ``` # HELP cloudflow_http_requests_total Total number of HTTP requests # TYPE cloudflow_http_requests_total counter cloudflow_http_requests_total{method=\"GET\",route=\"/api/workflows\",status=\"200\"} 15420 # HELP cloudflow_http_request_duration_seconds HTTP request duration in seconds # TYPE cloudflow_http_request_duration_seconds histogram cloudflow_http_request_duration_seconds_bucket{method=\"POST\",route=\"/api/workflows\",le=\"0.1\"} 8234 # HELP cloudflow_worker_jobs_processed_total Total number of jobs processed # TYPE cloudflow_worker_jobs_processed_total counter cloudflow_worker_jobs_processed_total{status=\"success\"} 45230 # HELP cloudflow_database_connections Current database connections # TYPE cloudflow_database_connections gauge cloudflow_database_connections{state=\"active\"} 23 ``` ### Grafana Dashboards Access Grafana to view CloudFlow dashboards: ```bash kubectl port-forward -n monitoring svc/prometheus-grafana 3000:80 ``` Import the CloudFlow dashboard (ID: 15847) or use the provided JSON template. Key dashboard panels include: 1. **API Performance**: Request rate, P95/P99 latency, error rate 2. **Resource Usage**: CPU, memory, disk I/O per pod 3.",
              "score": null
            },
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_4",
              "content": "verbosity | `info`, `debug`, `warn`, `error` | No (default: `info`) | | `API_PORT` | API server port | `3000` | No (default: `3000`) | | `WORKER_CONCURRENCY` | Number of concurrent workers | `10` | No (default: `5`) | | `SESSION_SECRET` | Session encryption key | `<generated-secret-256-bit>` | Yes | | `AWS_REGION` | AWS region for services | `us-east-1` | Yes | | `S3_BUCKET` | S3 bucket for file storage | `cloudflow-prod-storage` | Yes | | `SMTP_HOST` | Email server hostname | `smtp.sendgrid.net` | No | | `SMTP_PORT` | Email server port | `587` | No | | `METRICS_ENABLED` | Enable Prometheus metrics | `true` | No (default: `false`) | Create a Kubernetes secret for sensitive variables: ```bash kubectl create secret generic cloudflow-secrets \\ --namespace cloudflow-prod \\ --from-literal=DATABASE_URL=\"postgresql://cloudflow:$(cat db-password.txt)@pgbouncer.cloudflow-prod.svc.cluster.local:5432/cloudflow\" \\ --from-literal=REDIS_URL=\"redis://redis-master.cloudflow-prod.svc.cluster.local:6379\" \\ --from-literal=JWT_SECRET=\"$(openssl rand -base64 32)\" \\ --from-literal=SESSION_SECRET=\"$(openssl rand -base64 32)\" ``` ### Helm Chart Deployment Add the CloudFlow Helm repository:",
              "score": null
            },
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_9",
              "content": "\"limit\": 50, \"offset\": 0, \"has_more\": true } } ``` ### Analytics #### Get Workflow Metrics Retrieve performance metrics and analytics for workflows. **Endpoint:** `GET /analytics/workflows/{workflow_id}` **Query Parameters:** - `period` (required): Time period (`1h`, `24h`, `7d`, `30d`) - `metrics` (optional): Comma-separated list of metrics **Available Metrics:** - `execution_count`: Total number of executions - `success_rate`: Percentage of successful executions - `avg_duration`: Average execution duration in milliseconds - `error_count`: Total number of errors - `throughput`: Records processed per hour **Example Request:** ```bash curl -H \"X-API-Key: YOUR_API_KEY\" \\ \"https://api.cloudflow.io/v2/analytics/workflows/wf_8x7k2m9p?period=7d&metrics=execution_count,success_rate,avg_duration\" ``` **Example Response (200 OK):** ```json { \"workflow_id\": \"wf_8x7k2m9p\", \"period\": \"7d\", \"start_date\": \"2026-01-17T10:30:00Z\", \"end_date\": \"2026-01-24T10:30:00Z\", \"metrics\": { \"execution_count\": 168, \"success_rate\": 97.6, \"avg_duration\": 3285, \"error_count\": 4, \"throughput\": 2847 }, \"timeseries\": [ { \"timestamp\": \"2026-01-17T00:00:00Z\", \"execution_count\": 24, \"success_rate\": 100.0 }, { \"timestamp\": \"2026-01-18T00:00:00Z\", \"execution_count\": 24, \"success_rate\": 95.8 } ] } ``` ## Error Handling CloudFlow uses standard HTTP status codes and returns detailed error information in JSON",
              "score": null
            }
          ],
          "latency_ms": 5.312571000104072,
          "query_id": "realistic_014",
          "dimension": "problem",
          "query": "Where can I view CloudFlow metrics and logs?"
        },
        {
          "key_facts": [
            "Prometheus",
            "Grafana",
            "Jaeger for distributed tracing"
          ],
          "found_facts": [
            "Prometheus",
            "Grafana",
            "Jaeger for distributed tracing"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "architecture_overview",
            "deployment_guide"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "user_guide",
              "chunk_id": "user_guide_fix_26",
              "content": "Aggregation and Alerting Aggregate errors and send smart alerts: ```yaml name: \"Application Error Monitor\" schedule: cron: \"*/5 * * * *\" timezone: \"UTC\" steps: - id: fetch_recent_errors action: database_query config: query: | SELECT error_type, COUNT(*) as count, MAX(created_at) as last_seen FROM error_logs WHERE created_at > NOW() - INTERVAL '5 minutes' AND alerted = false GROUP BY error_type HAVING COUNT(*) > 5 - id: format_alert action: javascript condition: \"{{length(steps.fetch_recent_errors.rows) > 0}}\" code: | const errors = input.rows; let message = \"\u26a0\ufe0f Error Alert\\n\\n\"; for (const error of errors) { message += `\u2022 ${error.error_type}: ${error.count} occurrences (last: ${error.last_seen})\\n`; } return { message, total_errors: errors.reduce((sum, e) => sum + e.count, 0) }; - id: send_alert action: slack_message condition: \"{{steps.format_alert.output.total_errors > 10}}\" config: channel: \"#incidents\" text: \"{{steps.format_alert.output.message}}\" priority: \"high\" - id: mark_alerted action: database_query config: query: | UPDATE error_logs SET alerted = true WHERE created_at > NOW() - INTERVAL '5 minutes' ``` --- ##",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_9",
              "content": "to detect leader failure (5-second interval) - Automatic failover to standby scheduler (< 10 seconds) - Schedule versioning to handle updates during execution - Missed execution policy: SKIP, RUN_ONCE, or RUN_ALL **Performance Targets**: - Schedule evaluation: < 100ms per cycle - Accuracy: \u00b1 1 second for schedule triggers - Capacity: 100,000 active schedules --- ### Notification Service **Purpose**: Multi-channel notification delivery system supporting email, SMS, webhooks, and in-app notifications. **Technology**: Node.js with worker pool pattern **Replicas**: 8 pods (production), auto-scaling 6-16 **Resource Allocation**: 2 vCPU, 4GB RAM per pod **Key Responsibilities**: - Consume notification events from Kafka - Template rendering with Handlebars (cached in Redis) - Multi-channel delivery (Email via SendGrid, SMS via Twilio, Webhooks) - Retry logic with dead letter queue for failed deliveries - Delivery status tracking and analytics - User preference management (opt-in/opt-out) **Notification Flow**: ``` Event Source \u2192 Kafka (notifications.email topic) \u2502 \u25bc Notification Service Consumer",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_2",
              "content": "\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 ``` ### Technology Stack - **Runtime**: Node.js 20.x (API Gateway, Workflow Engine), Go 1.21 (Auth Service, Scheduler) - **Container Orchestration**: Kubernetes 1.28 (EKS) - **Service Mesh**: Istio 1.20 for service-to-service communication - **Databases**: PostgreSQL 15.4, Redis 7.2 - **Message Broker**: Apache Kafka 3.6 - **Monitoring**: Prometheus, Grafana, Jaeger for distributed tracing - **Secrets Management**: HashiCorp Vault 1.15 --- ## Microservices Breakdown ### API Gateway **Purpose**: Single entry point for all client requests, providing authentication, rate limiting, request routing, and protocol translation.",
              "score": null
            },
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_12",
              "content": "Waiting {retry_after} seconds...\") time.sleep(retry_after) continue elif response.status_code >= 500: # Server error - retry with exponential backoff wait_time = 2 ** attempt print(f\"Server error. Retrying in {wait_time} seconds...\") time.sleep(wait_time) continue else: # Client error - don't retry error_data = response.json() raise Exception(f\"API Error: {error_data['error']['message']}\") except requests.exceptions.RequestException as e: print(f\"Request failed: {e}\") if attempt < max_retries - 1: time.sleep(2 ** attempt) else: raise raise Exception(\"Max retries exceeded\") ``` ## Webhooks CloudFlow can send webhook notifications when specific events occur in your workflows.",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_29",
              "content": "Root cause analysis # 5. Resolution steps # 6. Action items # 7. Lessons learned ``` --- ## Additional Resources - **CloudFlow Documentation:** https://docs.cloudflow.io - **API Reference:** https://api-docs.cloudflow.io - **Community Forum:** https://community.cloudflow.io - **GitHub Issues:** https://github.com/cloudflow/platform/issues - **Training Portal:** https://training.cloudflow.io - **Monitoring Dashboard:** https://monitoring.cloudflow.io ### Getting Help If this troubleshooting guide doesn't resolve your issue: 1. Search the knowledge base: `cloudflow kb search \"your issue\"` 2. Check community forum for similar issues 3. Contact support with detailed logs and reproduction steps 4. For urgent issues, use emergency escalation procedures **Remember:** Always capture logs, metrics, and reproduction steps before escalating! --- *Last updated: January 24, 2026* *Document version: 3.2.1* *Feedback: docs-feedback@cloudflow.io*",
              "score": null
            }
          ],
          "latency_ms": 5.372732999603613,
          "query_id": "realistic_014",
          "dimension": "casual",
          "query": "monitoring stack"
        },
        {
          "key_facts": [
            "Prometheus",
            "Grafana",
            "Jaeger for distributed tracing"
          ],
          "found_facts": [],
          "missed_facts": [
            "Prometheus",
            "Grafana",
            "Jaeger for distributed tracing"
          ],
          "coverage": 0.0,
          "expected_docs": [
            "architecture_overview",
            "deployment_guide"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_9",
              "content": "to detect leader failure (5-second interval) - Automatic failover to standby scheduler (< 10 seconds) - Schedule versioning to handle updates during execution - Missed execution policy: SKIP, RUN_ONCE, or RUN_ALL **Performance Targets**: - Schedule evaluation: < 100ms per cycle - Accuracy: \u00b1 1 second for schedule triggers - Capacity: 100,000 active schedules --- ### Notification Service **Purpose**: Multi-channel notification delivery system supporting email, SMS, webhooks, and in-app notifications. **Technology**: Node.js with worker pool pattern **Replicas**: 8 pods (production), auto-scaling 6-16 **Resource Allocation**: 2 vCPU, 4GB RAM per pod **Key Responsibilities**: - Consume notification events from Kafka - Template rendering with Handlebars (cached in Redis) - Multi-channel delivery (Email via SendGrid, SMS via Twilio, Webhooks) - Retry logic with dead letter queue for failed deliveries - Delivery status tracking and analytics - User preference management (opt-in/opt-out) **Notification Flow**: ``` Event Source \u2192 Kafka (notifications.email topic) \u2502 \u25bc Notification Service Consumer",
              "score": null
            },
            {
              "doc_id": "user_guide",
              "chunk_id": "user_guide_fix_26",
              "content": "Aggregation and Alerting Aggregate errors and send smart alerts: ```yaml name: \"Application Error Monitor\" schedule: cron: \"*/5 * * * *\" timezone: \"UTC\" steps: - id: fetch_recent_errors action: database_query config: query: | SELECT error_type, COUNT(*) as count, MAX(created_at) as last_seen FROM error_logs WHERE created_at > NOW() - INTERVAL '5 minutes' AND alerted = false GROUP BY error_type HAVING COUNT(*) > 5 - id: format_alert action: javascript condition: \"{{length(steps.fetch_recent_errors.rows) > 0}}\" code: | const errors = input.rows; let message = \"\u26a0\ufe0f Error Alert\\n\\n\"; for (const error of errors) { message += `\u2022 ${error.error_type}: ${error.count} occurrences (last: ${error.last_seen})\\n`; } return { message, total_errors: errors.reduce((sum, e) => sum + e.count, 0) }; - id: send_alert action: slack_message condition: \"{{steps.format_alert.output.total_errors > 10}}\" config: channel: \"#incidents\" text: \"{{steps.format_alert.output.message}}\" priority: \"high\" - id: mark_alerted action: database_query config: query: | UPDATE error_logs SET alerted = true WHERE created_at > NOW() - INTERVAL '5 minutes' ``` --- ##",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_27",
              "content": "changes cloudflow audit log --last 2h --event-type \"deployment,configuration\" # Check metrics dashboard cloudflow metrics dashboard --incident-mode ``` #### Step 2: Gather Information (5-15 minutes) Create incident document with: - Incident timestamp and duration - Affected services and endpoints - Error rates and user impact - Recent changes or deployments - Relevant log excerpts - Correlation IDs for failed requests ```bash # Generate incident report cloudflow debug incident-report \\ --start \"2026-01-24T10:30:00Z\" \\ --end \"2026-01-24T11:00:00Z\" \\ --output incident-report.md # Capture system snapshot cloudflow debug snapshot --output snapshot-$(date +%Y%m%d-%H%M%S).tar.gz ``` #### Step 3: Escalate Based on Severity **For SEV-1 (Critical):** ```bash # Page on-call engineer cloudflow incident create \\ --severity SEV-1 \\ --title \"Complete API outage\" \\ --description \"All API requests returning 503\" \\ --page-oncall # Notify status page cloudflow status update \\ --status \"major_outage\" \\ --message \"We are investigating a complete service outage\" # Create war room cloudflow incident war-room create incident-2024012401",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_30",
              "content": "Update monitoring dashboards - Recovery time: 2-4 hours (includes verification) - Data loss: < 1 hour (last cross-region replication) **Scenario 4: Data Corruption** - Detection: Data validation checks or user report - Action: Point-in-time recovery from WAL archives - Procedure: 1. Identify corruption time window 2. Restore from snapshot prior to corruption 3. Replay WAL logs up to corruption point 4. Verify data integrity 5. Resume normal operations - Recovery time: 1-3 hours depending on data volume - Data loss: None if corruption detected quickly ### Testing & Validation **DR Drill Schedule**: - Monthly: Automated failover test (single AZ failure simulation) - Quarterly: Full DR region failover (non-production hours) - Annually: Complete disaster simulation with all stakeholders **Last DR Test Results** (Dec 15, 2025): - Scenario: Full region failover - Actual RTO: 2 hours 23 minutes (target: 4 hours) - Actual RPO: 42 minutes (target: 1 hour) - Issues identified:",
              "score": null
            },
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_5",
              "content": "**Endpoint:** `GET /workflows` **Query Parameters:** - `status` (optional): Filter by status (`active`, `paused`, `archived`) - `created_after` (optional): ISO 8601 timestamp - `limit` (optional): Items per page (max 100) - `offset` (optional): Pagination offset **Example Request:** ```bash curl -H \"X-API-Key: YOUR_API_KEY\" \\ \"https://api.cloudflow.io/v2/workflows?status=active&limit=25\" ``` **Example Response (200 OK):** ```json { \"data\": [ { \"id\": \"wf_8x7k2m9p\", \"name\": \"Data Processing Pipeline\", \"description\": \"Processes customer data every hour\", \"status\": \"active\", \"trigger\": { \"type\": \"schedule\", \"cron\": \"0 * * * *\" }, \"steps\": 12, \"created_at\": \"2026-01-15T10:30:00Z\", \"updated_at\": \"2026-01-23T14:22:00Z\", \"last_run\": \"2026-01-24T09:00:00Z\" } ], \"pagination\": { \"total\": 1, \"limit\": 25, \"offset\": 0, \"has_more\": false } } ``` #### Create Workflow Create a new workflow with specified configuration.",
              "score": null
            }
          ],
          "latency_ms": 5.354920000172569,
          "query_id": "realistic_014",
          "dimension": "contextual",
          "query": "I need to set up dashboards, what monitoring systems are available?"
        },
        {
          "key_facts": [
            "Prometheus",
            "Grafana",
            "Jaeger for distributed tracing"
          ],
          "found_facts": [
            "Prometheus",
            "Grafana"
          ],
          "missed_facts": [
            "Jaeger for distributed tracing"
          ],
          "coverage": 0.6666666666666666,
          "expected_docs": [
            "architecture_overview",
            "deployment_guide"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_9",
              "content": "\"limit\": 50, \"offset\": 0, \"has_more\": true } } ``` ### Analytics #### Get Workflow Metrics Retrieve performance metrics and analytics for workflows. **Endpoint:** `GET /analytics/workflows/{workflow_id}` **Query Parameters:** - `period` (required): Time period (`1h`, `24h`, `7d`, `30d`) - `metrics` (optional): Comma-separated list of metrics **Available Metrics:** - `execution_count`: Total number of executions - `success_rate`: Percentage of successful executions - `avg_duration`: Average execution duration in milliseconds - `error_count`: Total number of errors - `throughput`: Records processed per hour **Example Request:** ```bash curl -H \"X-API-Key: YOUR_API_KEY\" \\ \"https://api.cloudflow.io/v2/analytics/workflows/wf_8x7k2m9p?period=7d&metrics=execution_count,success_rate,avg_duration\" ``` **Example Response (200 OK):** ```json { \"workflow_id\": \"wf_8x7k2m9p\", \"period\": \"7d\", \"start_date\": \"2026-01-17T10:30:00Z\", \"end_date\": \"2026-01-24T10:30:00Z\", \"metrics\": { \"execution_count\": 168, \"success_rate\": 97.6, \"avg_duration\": 3285, \"error_count\": 4, \"throughput\": 2847 }, \"timeseries\": [ { \"timestamp\": \"2026-01-17T00:00:00Z\", \"execution_count\": 24, \"success_rate\": 100.0 }, { \"timestamp\": \"2026-01-18T00:00:00Z\", \"execution_count\": 24, \"success_rate\": 95.8 } ] } ``` ## Error Handling CloudFlow uses standard HTTP status codes and returns detailed error information in JSON",
              "score": null
            },
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_10",
              "content": "The metrics include: - **HTTP Metrics**: Request rate, latency, error rate - **Database Metrics**: Connection pool status, query duration - **Worker Metrics**: Job queue size, processing time, failure rate - **System Metrics**: CPU usage, memory usage, heap statistics Example metrics exposed: ``` # HELP cloudflow_http_requests_total Total number of HTTP requests # TYPE cloudflow_http_requests_total counter cloudflow_http_requests_total{method=\"GET\",route=\"/api/workflows\",status=\"200\"} 15420 # HELP cloudflow_http_request_duration_seconds HTTP request duration in seconds # TYPE cloudflow_http_request_duration_seconds histogram cloudflow_http_request_duration_seconds_bucket{method=\"POST\",route=\"/api/workflows\",le=\"0.1\"} 8234 # HELP cloudflow_worker_jobs_processed_total Total number of jobs processed # TYPE cloudflow_worker_jobs_processed_total counter cloudflow_worker_jobs_processed_total{status=\"success\"} 45230 # HELP cloudflow_database_connections Current database connections # TYPE cloudflow_database_connections gauge cloudflow_database_connections{state=\"active\"} 23 ``` ### Grafana Dashboards Access Grafana to view CloudFlow dashboards: ```bash kubectl port-forward -n monitoring svc/prometheus-grafana 3000:80 ``` Import the CloudFlow dashboard (ID: 15847) or use the provided JSON template. Key dashboard panels include: 1. **API Performance**: Request rate, P95/P99 latency, error rate 2. **Resource Usage**: CPU, memory, disk I/O per pod 3.",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_5",
              "content": "Identify Slow Queries** ```bash # Query CloudFlow metrics cloudflow metrics query --metric api_request_duration_ms \\ --filter \"p95 > 5000\" \\ --last 1h # Check database slow query log kubectl logs -n cloudflow deploy/cloudflow-db-primary | \\ grep \"slow query\" | \\ tail -n 50 # Analyze query patterns cloudflow db analyze-queries --min-duration 5000 --limit 20 ``` **2. Review Query Execution Plans** ```sql -- Connect to CloudFlow database cloudflow db connect --readonly -- Explain slow query EXPLAIN ANALYZE SELECT w.*, e.status, e.error_message FROM workflows w LEFT JOIN executions e ON w.id = e.workflow_id WHERE w.workspace_id = 'ws_abc123' AND e.created_at > NOW() - INTERVAL '7 days' ORDER BY e.created_at DESC; -- Check for missing indexes SELECT schemaname, tablename, indexname, indexdef FROM pg_indexes WHERE tablename IN ('workflows', 'executions', 'workflow_steps') ORDER BY tablename, indexname; ``` **3.",
              "score": null
            },
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_6",
              "content": "prometheus.io/scrape: \"true\" prometheus.io/port: \"9090\" prometheus.io/path: \"/metrics\" podSecurityContext: runAsNonRoot: true runAsUser: 1000 fsGroup: 1000 securityContext: allowPrivilegeEscalation: false capabilities: drop: - ALL readOnlyRootFilesystem: true persistence: enabled: true storageClass: gp3 accessMode: ReadWriteOnce size: 50Gi redis: enabled: true architecture: replication auth: enabled: true master: persistence: size: 20Gi replica: replicaCount: 2 persistence: size: 20Gi ``` Deploy CloudFlow using Helm: ```bash helm install cloudflow cloudflow/cloudflow \\ --namespace cloudflow-prod \\ --values values-production.yaml \\ --wait \\ --timeout 10m ``` ### Deployment Verification Verify the deployment status: ```bash # Check pod status kubectl get pods -n cloudflow-prod # Expected output: # NAME READY STATUS RESTARTS AGE # cloudflow-api-7d8f9c5b6d-4xk2p 1/1 Running 0 2m # cloudflow-api-7d8f9c5b6d-9hj5m 1/1 Running 0 2m # cloudflow-api-7d8f9c5b6d-tn8wq 1/1 Running 0 2m # Check service endpoints kubectl get svc -n cloudflow-prod # Check ingress kubectl get ingress -n cloudflow-prod ``` Test health endpoints: ```bash # Port forward for testing kubectl port-forward -n cloudflow-prod svc/cloudflow-api 8080:80 # Test",
              "score": null
            },
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_4",
              "content": "verbosity | `info`, `debug`, `warn`, `error` | No (default: `info`) | | `API_PORT` | API server port | `3000` | No (default: `3000`) | | `WORKER_CONCURRENCY` | Number of concurrent workers | `10` | No (default: `5`) | | `SESSION_SECRET` | Session encryption key | `<generated-secret-256-bit>` | Yes | | `AWS_REGION` | AWS region for services | `us-east-1` | Yes | | `S3_BUCKET` | S3 bucket for file storage | `cloudflow-prod-storage` | Yes | | `SMTP_HOST` | Email server hostname | `smtp.sendgrid.net` | No | | `SMTP_PORT` | Email server port | `587` | No | | `METRICS_ENABLED` | Enable Prometheus metrics | `true` | No (default: `false`) | Create a Kubernetes secret for sensitive variables: ```bash kubectl create secret generic cloudflow-secrets \\ --namespace cloudflow-prod \\ --from-literal=DATABASE_URL=\"postgresql://cloudflow:$(cat db-password.txt)@pgbouncer.cloudflow-prod.svc.cluster.local:5432/cloudflow\" \\ --from-literal=REDIS_URL=\"redis://redis-master.cloudflow-prod.svc.cluster.local:6379\" \\ --from-literal=JWT_SECRET=\"$(openssl rand -base64 32)\" \\ --from-literal=SESSION_SECRET=\"$(openssl rand -base64 32)\" ``` ### Helm Chart Deployment Add the CloudFlow Helm repository:",
              "score": null
            }
          ],
          "latency_ms": 5.755015999966417,
          "query_id": "realistic_014",
          "dimension": "negation",
          "query": "Why can't I see metrics in Datadog?"
        },
        {
          "key_facts": [
            "could not obtain connection from pool within 5000ms",
            "connection pool exhausted (100/100 connections in use)",
            "PgBouncer",
            "max_db_connections = 100"
          ],
          "found_facts": [
            "could not obtain connection from pool within 5000ms",
            "connection pool exhausted (100/100 connections in use)",
            "PgBouncer"
          ],
          "missed_facts": [
            "max_db_connections = 100"
          ],
          "coverage": 0.75,
          "expected_docs": [
            "troubleshooting_guide",
            "deployment_guide"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_18",
              "content": "#### Database Connection Pool Exhausted **Symptoms**: \"Too many connections\" errors **Diagnosis**: ```bash # Check PgBouncer stats kubectl exec -it pgbouncer-xxx -n cloudflow-prod -- \\ psql -p 5432 -U pgbouncer pgbouncer -c \"SHOW POOLS;\" ``` **Resolution**: - Increase `max_db_connections` in PgBouncer - Optimize application connection usage - Add more PostgreSQL replicas ### Support Contacts - **On-call Engineer**: pagerduty.com/cloudflow-oncall - **Slack Channel**: #cloudflow-operations - **Documentation**: https://docs.cloudflow.io - **Status Page**: https://status.cloudflow.io --- ## Appendix ### Useful Commands ```bash # View all resources in namespace kubectl get all -n cloudflow-prod # Check resource usage kubectl top pods -n cloudflow-prod kubectl top nodes # View logs kubectl logs -f deployment/cloudflow-api -n cloudflow-prod # Execute command in pod kubectl exec -it <pod-name> -n cloudflow-prod -- /bin/sh # Port forward for local testing kubectl port-forward svc/cloudflow-api 8080:80 -n cloudflow-prod # Rollback deployment helm rollback cloudflow -n cloudflow-prod # Update deployment helm upgrade cloudflow cloudflow/cloudflow \\ --namespace cloudflow-prod",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_22",
              "content": "\"connection.*pool|could not connect|database.*timeout\" # Find slow queries kubectl logs -n cloudflow deployment/cloudflow-api | \\ grep \"slow query\" | \\ awk '{print $NF}' | \\ sort -n | \\ tail -n 20 # Find deadlock errors kubectl logs -n cloudflow deployment/cloudflow-api | \\ grep -i \"deadlock detected\" ``` #### Workflow Execution Errors ```bash # Find workflow timeout errors kubectl logs -n cloudflow deployment/cloudflow-workflow-engine | \\ grep -E \"timeout|exceeded.*3600\" # Find workflow retry attempts kubectl logs -n cloudflow deployment/cloudflow-workflow-engine | \\ grep -E \"retry attempt [0-9]|retrying in\" # Find workflow failures by ID kubectl logs -n cloudflow deployment/cloudflow-workflow-engine | \\ grep \"exec_7h3j6k9m2n\" ``` #### Rate Limiting ```bash # Find rate limit events kubectl logs -n cloudflow deployment/cloudflow-api | \\ grep -E \"429|rate.*limit|throttle\" # Count rate limit errors by hour kubectl logs -n cloudflow deployment/cloudflow-api --since=24h | \\ grep \"rate_limit_exceeded\" | \\ awk '{print $1}' | \\ cut -d'T' -f1-2 | \\ sort",
              "score": null
            },
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_15",
              "content": "Pooling Ensure optimal database connection pooling settings in PgBouncer: - **Pool Mode**: Transaction (optimal for microservices) - **Default Pool Size**: 25 connections per user - **Max DB Connections**: 100 (matches PostgreSQL `max_connections`) #### Redis Caching Implement Redis caching for frequently accessed data: ```javascript // Example caching strategy const CACHE_TTL = { workflows: 300, // 5 minutes userSessions: 3600, // 1 hour apiResults: 60 // 1 minute }; ``` #### Load Testing Perform regular load testing to validate scaling configuration: ```bash # Install k6 for load testing kubectl apply -f - <<EOF apiVersion: v1 kind: ConfigMap metadata: name: loadtest-script namespace: cloudflow-prod data: script.js: | import http from 'k6/http'; import { check, sleep } from 'k6'; export let options = { stages: [ { duration: '2m', target: 100 }, { duration: '5m', target: 100 }, { duration: '2m', target: 200 }, { duration: '5m', target: 200 }, { duration: '2m', target: 0",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_9",
              "content": "```bash # Configure event buffer limits cloudflow config set events.buffer.max_size 10000 cloudflow config set events.buffer.overflow_strategy drop_oldest # Enable event streaming to external sink cloudflow config set events.sink.type kafka cloudflow config set events.sink.kafka.brokers \"kafka-1:9092,kafka-2:9092\" cloudflow config set events.sink.kafka.topic cloudflow-events ``` --- ## Database Connection Issues ### Connection Pool Exhaustion #### Symptoms - Error: `could not obtain connection from pool within 5000ms` - Error: `connection pool exhausted (100/100 connections in use)` - API requests fail with `503 Service Unavailable` - Database CPU usage normal, but connection count at maximum #### Investigation ```bash # Check current connection pool status cloudflow db pool status --detailed # Output example: # Pool Statistics: # Total Connections: 100/100 (100%) # Active: 87 # Idle: 13 # Waiting Requests: 45 # Average Wait Time: 3420ms # Max Wait Time: 8234ms # Identify long-running queries cloudflow db queries --status running --min-duration 30000 # Check connection distribution by client SELECT",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_10",
              "content": "application_name, state, COUNT(*) as conn_count, AVG(EXTRACT(EPOCH FROM (NOW() - state_change))) as avg_duration_sec FROM pg_stat_activity WHERE datname = 'cloudflow_production' GROUP BY application_name, state ORDER BY conn_count DESC; ``` #### Resolution **Immediate mitigation:** ```bash # Temporarily increase connection limit (requires database restart) cloudflow db config set max_connections 150 # Kill idle connections older than 5 minutes cloudflow db connections kill --idle-timeout 300 # Restart connection pool without downtime kubectl rollout restart deployment/cloudflow-api -n cloudflow kubectl rollout status deployment/cloudflow-api -n cloudflow ``` **Long-term solutions:** 1. **Implement connection pooling optimization:** ```bash # Use PgBouncer for connection pooling kubectl apply -f cloudflow-pgbouncer.yaml # Configure CloudFlow to use PgBouncer cloudflow config set db.host pgbouncer.cloudflow.svc.cluster.local cloudflow config set db.port 6432 cloudflow config set db.pool.mode transaction ``` 2.",
              "score": null
            }
          ],
          "latency_ms": 5.7802730007097125,
          "query_id": "realistic_015",
          "dimension": "original",
          "query": "How do I diagnose database connection pool exhaustion?"
        },
        {
          "key_facts": [
            "could not obtain connection from pool within 5000ms",
            "connection pool exhausted (100/100 connections in use)",
            "PgBouncer",
            "max_db_connections = 100"
          ],
          "found_facts": [
            "PgBouncer"
          ],
          "missed_facts": [
            "could not obtain connection from pool within 5000ms",
            "connection pool exhausted (100/100 connections in use)",
            "max_db_connections = 100"
          ],
          "coverage": 0.25,
          "expected_docs": [
            "troubleshooting_guide",
            "deployment_guide"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_18",
              "content": "#### Database Connection Pool Exhausted **Symptoms**: \"Too many connections\" errors **Diagnosis**: ```bash # Check PgBouncer stats kubectl exec -it pgbouncer-xxx -n cloudflow-prod -- \\ psql -p 5432 -U pgbouncer pgbouncer -c \"SHOW POOLS;\" ``` **Resolution**: - Increase `max_db_connections` in PgBouncer - Optimize application connection usage - Add more PostgreSQL replicas ### Support Contacts - **On-call Engineer**: pagerduty.com/cloudflow-oncall - **Slack Channel**: #cloudflow-operations - **Documentation**: https://docs.cloudflow.io - **Status Page**: https://status.cloudflow.io --- ## Appendix ### Useful Commands ```bash # View all resources in namespace kubectl get all -n cloudflow-prod # Check resource usage kubectl top pods -n cloudflow-prod kubectl top nodes # View logs kubectl logs -f deployment/cloudflow-api -n cloudflow-prod # Execute command in pod kubectl exec -it <pod-name> -n cloudflow-prod -- /bin/sh # Port forward for local testing kubectl port-forward svc/cloudflow-api 8080:80 -n cloudflow-prod # Rollback deployment helm rollback cloudflow -n cloudflow-prod # Update deployment helm upgrade cloudflow cloudflow/cloudflow \\ --namespace cloudflow-prod",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_22",
              "content": "\"connection.*pool|could not connect|database.*timeout\" # Find slow queries kubectl logs -n cloudflow deployment/cloudflow-api | \\ grep \"slow query\" | \\ awk '{print $NF}' | \\ sort -n | \\ tail -n 20 # Find deadlock errors kubectl logs -n cloudflow deployment/cloudflow-api | \\ grep -i \"deadlock detected\" ``` #### Workflow Execution Errors ```bash # Find workflow timeout errors kubectl logs -n cloudflow deployment/cloudflow-workflow-engine | \\ grep -E \"timeout|exceeded.*3600\" # Find workflow retry attempts kubectl logs -n cloudflow deployment/cloudflow-workflow-engine | \\ grep -E \"retry attempt [0-9]|retrying in\" # Find workflow failures by ID kubectl logs -n cloudflow deployment/cloudflow-workflow-engine | \\ grep \"exec_7h3j6k9m2n\" ``` #### Rate Limiting ```bash # Find rate limit events kubectl logs -n cloudflow deployment/cloudflow-api | \\ grep -E \"429|rate.*limit|throttle\" # Count rate limit errors by hour kubectl logs -n cloudflow deployment/cloudflow-api --since=24h | \\ grep \"rate_limit_exceeded\" | \\ awk '{print $1}' | \\ cut -d'T' -f1-2 | \\ sort",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_11",
              "content": "**Add read replicas:** ```bash # Route read-only queries to replicas cloudflow db replicas add --count 2 cloudflow config set db.read_replicas \"replica-1:5432,replica-2:5432\" cloudflow config set db.read_write_split true ``` ### Connection Timeout Errors #### Error Messages - `connection timeout after 30000ms` - `could not connect to database server at 10.0.2.45:5432` - `database server unreachable` #### Troubleshooting Steps ```bash # Test network connectivity telnet cloudflow-db.internal.company.com 5432 # Check DNS resolution nslookup cloudflow-db.internal.company.com # Verify database is accepting connections pg_isready -h cloudflow-db.internal.company.com -p 5432 -U cloudflow # Check firewall rules sudo iptables -L -n | grep 5432 # Test from CloudFlow pod network kubectl run -n cloudflow debug-pod --rm -i --tty \\ --image=postgres:14 -- \\ psql -h cloudflow-db.internal.company.com -U cloudflow -d cloudflow_production # Review database logs for connection rejections kubectl logs -n cloudflow statefulset/cloudflow-db --tail=100 | \\ grep -i \"connection\\|reject\\|authentication\" ``` ### Maximum Connection Limit (100) Reached This is a hard limit in CloudFlow's",
              "score": null
            },
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_15",
              "content": "Pooling Ensure optimal database connection pooling settings in PgBouncer: - **Pool Mode**: Transaction (optimal for microservices) - **Default Pool Size**: 25 connections per user - **Max DB Connections**: 100 (matches PostgreSQL `max_connections`) #### Redis Caching Implement Redis caching for frequently accessed data: ```javascript // Example caching strategy const CACHE_TTL = { workflows: 300, // 5 minutes userSessions: 3600, // 1 hour apiResults: 60 // 1 minute }; ``` #### Load Testing Perform regular load testing to validate scaling configuration: ```bash # Install k6 for load testing kubectl apply -f - <<EOF apiVersion: v1 kind: ConfigMap metadata: name: loadtest-script namespace: cloudflow-prod data: script.js: | import http from 'k6/http'; import { check, sleep } from 'k6'; export let options = { stages: [ { duration: '2m', target: 100 }, { duration: '5m', target: 100 }, { duration: '2m', target: 200 }, { duration: '5m', target: 200 }, { duration: '2m', target: 0",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_29",
              "content": "minutes (no manual intervention) - Data loss: None (multi-AZ replication) **Scenario 2: Database Primary Failure** - Detection: Health check fails for primary database (< 30 seconds) - Action: Automatic promotion of read replica to primary - Recovery time: 30-60 seconds - Data loss: Minimal (< 1 second due to synchronous replication) **Scenario 3: Full Region Failure** - Detection: Multiple health check failures across all AZs (< 2 minutes) - Action: Manual failover to DR region (us-west-2) - Procedure: 1. Update DNS to point to DR region (TTL: 60 seconds) 2. Promote DR database replica to primary 3. Scale up DR region services to production capacity 4. Verify data consistency and integrity 5.",
              "score": null
            }
          ],
          "latency_ms": 5.430430999695091,
          "query_id": "realistic_015",
          "dimension": "synonym",
          "query": "What should I check when I run out of database connections?"
        },
        {
          "key_facts": [
            "could not obtain connection from pool within 5000ms",
            "connection pool exhausted (100/100 connections in use)",
            "PgBouncer",
            "max_db_connections = 100"
          ],
          "found_facts": [
            "could not obtain connection from pool within 5000ms",
            "connection pool exhausted (100/100 connections in use)",
            "PgBouncer"
          ],
          "missed_facts": [
            "max_db_connections = 100"
          ],
          "coverage": 0.75,
          "expected_docs": [
            "troubleshooting_guide",
            "deployment_guide"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_22",
              "content": "\"connection.*pool|could not connect|database.*timeout\" # Find slow queries kubectl logs -n cloudflow deployment/cloudflow-api | \\ grep \"slow query\" | \\ awk '{print $NF}' | \\ sort -n | \\ tail -n 20 # Find deadlock errors kubectl logs -n cloudflow deployment/cloudflow-api | \\ grep -i \"deadlock detected\" ``` #### Workflow Execution Errors ```bash # Find workflow timeout errors kubectl logs -n cloudflow deployment/cloudflow-workflow-engine | \\ grep -E \"timeout|exceeded.*3600\" # Find workflow retry attempts kubectl logs -n cloudflow deployment/cloudflow-workflow-engine | \\ grep -E \"retry attempt [0-9]|retrying in\" # Find workflow failures by ID kubectl logs -n cloudflow deployment/cloudflow-workflow-engine | \\ grep \"exec_7h3j6k9m2n\" ``` #### Rate Limiting ```bash # Find rate limit events kubectl logs -n cloudflow deployment/cloudflow-api | \\ grep -E \"429|rate.*limit|throttle\" # Count rate limit errors by hour kubectl logs -n cloudflow deployment/cloudflow-api --since=24h | \\ grep \"rate_limit_exceeded\" | \\ awk '{print $1}' | \\ cut -d'T' -f1-2 | \\ sort",
              "score": null
            },
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_15",
              "content": "Pooling Ensure optimal database connection pooling settings in PgBouncer: - **Pool Mode**: Transaction (optimal for microservices) - **Default Pool Size**: 25 connections per user - **Max DB Connections**: 100 (matches PostgreSQL `max_connections`) #### Redis Caching Implement Redis caching for frequently accessed data: ```javascript // Example caching strategy const CACHE_TTL = { workflows: 300, // 5 minutes userSessions: 3600, // 1 hour apiResults: 60 // 1 minute }; ``` #### Load Testing Perform regular load testing to validate scaling configuration: ```bash # Install k6 for load testing kubectl apply -f - <<EOF apiVersion: v1 kind: ConfigMap metadata: name: loadtest-script namespace: cloudflow-prod data: script.js: | import http from 'k6/http'; import { check, sleep } from 'k6'; export let options = { stages: [ { duration: '2m', target: 100 }, { duration: '5m', target: 100 }, { duration: '2m', target: 200 }, { duration: '5m', target: 200 }, { duration: '2m', target: 0",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_9",
              "content": "```bash # Configure event buffer limits cloudflow config set events.buffer.max_size 10000 cloudflow config set events.buffer.overflow_strategy drop_oldest # Enable event streaming to external sink cloudflow config set events.sink.type kafka cloudflow config set events.sink.kafka.brokers \"kafka-1:9092,kafka-2:9092\" cloudflow config set events.sink.kafka.topic cloudflow-events ``` --- ## Database Connection Issues ### Connection Pool Exhaustion #### Symptoms - Error: `could not obtain connection from pool within 5000ms` - Error: `connection pool exhausted (100/100 connections in use)` - API requests fail with `503 Service Unavailable` - Database CPU usage normal, but connection count at maximum #### Investigation ```bash # Check current connection pool status cloudflow db pool status --detailed # Output example: # Pool Statistics: # Total Connections: 100/100 (100%) # Active: 87 # Idle: 13 # Waiting Requests: 45 # Average Wait Time: 3420ms # Max Wait Time: 8234ms # Identify long-running queries cloudflow db queries --status running --min-duration 30000 # Check connection distribution by client SELECT",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_11",
              "content": "**Add read replicas:** ```bash # Route read-only queries to replicas cloudflow db replicas add --count 2 cloudflow config set db.read_replicas \"replica-1:5432,replica-2:5432\" cloudflow config set db.read_write_split true ``` ### Connection Timeout Errors #### Error Messages - `connection timeout after 30000ms` - `could not connect to database server at 10.0.2.45:5432` - `database server unreachable` #### Troubleshooting Steps ```bash # Test network connectivity telnet cloudflow-db.internal.company.com 5432 # Check DNS resolution nslookup cloudflow-db.internal.company.com # Verify database is accepting connections pg_isready -h cloudflow-db.internal.company.com -p 5432 -U cloudflow # Check firewall rules sudo iptables -L -n | grep 5432 # Test from CloudFlow pod network kubectl run -n cloudflow debug-pod --rm -i --tty \\ --image=postgres:14 -- \\ psql -h cloudflow-db.internal.company.com -U cloudflow -d cloudflow_production # Review database logs for connection rejections kubectl logs -n cloudflow statefulset/cloudflow-db --tail=100 | \\ grep -i \"connection\\|reject\\|authentication\" ``` ### Maximum Connection Limit (100) Reached This is a hard limit in CloudFlow's",
              "score": null
            },
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_18",
              "content": "#### Database Connection Pool Exhausted **Symptoms**: \"Too many connections\" errors **Diagnosis**: ```bash # Check PgBouncer stats kubectl exec -it pgbouncer-xxx -n cloudflow-prod -- \\ psql -p 5432 -U pgbouncer pgbouncer -c \"SHOW POOLS;\" ``` **Resolution**: - Increase `max_db_connections` in PgBouncer - Optimize application connection usage - Add more PostgreSQL replicas ### Support Contacts - **On-call Engineer**: pagerduty.com/cloudflow-oncall - **Slack Channel**: #cloudflow-operations - **Documentation**: https://docs.cloudflow.io - **Status Page**: https://status.cloudflow.io --- ## Appendix ### Useful Commands ```bash # View all resources in namespace kubectl get all -n cloudflow-prod # Check resource usage kubectl top pods -n cloudflow-prod kubectl top nodes # View logs kubectl logs -f deployment/cloudflow-api -n cloudflow-prod # Execute command in pod kubectl exec -it <pod-name> -n cloudflow-prod -- /bin/sh # Port forward for local testing kubectl port-forward svc/cloudflow-api 8080:80 -n cloudflow-prod # Rollback deployment helm rollback cloudflow -n cloudflow-prod # Update deployment helm upgrade cloudflow cloudflow/cloudflow \\ --namespace cloudflow-prod",
              "score": null
            }
          ],
          "latency_ms": 5.673312998624169,
          "query_id": "realistic_015",
          "dimension": "problem",
          "query": "Getting 'could not obtain connection from pool' errors"
        },
        {
          "key_facts": [
            "could not obtain connection from pool within 5000ms",
            "connection pool exhausted (100/100 connections in use)",
            "PgBouncer",
            "max_db_connections = 100"
          ],
          "found_facts": [
            "could not obtain connection from pool within 5000ms",
            "connection pool exhausted (100/100 connections in use)",
            "PgBouncer"
          ],
          "missed_facts": [
            "max_db_connections = 100"
          ],
          "coverage": 0.75,
          "expected_docs": [
            "troubleshooting_guide",
            "deployment_guide"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_15",
              "content": "Pooling Ensure optimal database connection pooling settings in PgBouncer: - **Pool Mode**: Transaction (optimal for microservices) - **Default Pool Size**: 25 connections per user - **Max DB Connections**: 100 (matches PostgreSQL `max_connections`) #### Redis Caching Implement Redis caching for frequently accessed data: ```javascript // Example caching strategy const CACHE_TTL = { workflows: 300, // 5 minutes userSessions: 3600, // 1 hour apiResults: 60 // 1 minute }; ``` #### Load Testing Perform regular load testing to validate scaling configuration: ```bash # Install k6 for load testing kubectl apply -f - <<EOF apiVersion: v1 kind: ConfigMap metadata: name: loadtest-script namespace: cloudflow-prod data: script.js: | import http from 'k6/http'; import { check, sleep } from 'k6'; export let options = { stages: [ { duration: '2m', target: 100 }, { duration: '5m', target: 100 }, { duration: '2m', target: 200 }, { duration: '5m', target: 200 }, { duration: '2m', target: 0",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_9",
              "content": "```bash # Configure event buffer limits cloudflow config set events.buffer.max_size 10000 cloudflow config set events.buffer.overflow_strategy drop_oldest # Enable event streaming to external sink cloudflow config set events.sink.type kafka cloudflow config set events.sink.kafka.brokers \"kafka-1:9092,kafka-2:9092\" cloudflow config set events.sink.kafka.topic cloudflow-events ``` --- ## Database Connection Issues ### Connection Pool Exhaustion #### Symptoms - Error: `could not obtain connection from pool within 5000ms` - Error: `connection pool exhausted (100/100 connections in use)` - API requests fail with `503 Service Unavailable` - Database CPU usage normal, but connection count at maximum #### Investigation ```bash # Check current connection pool status cloudflow db pool status --detailed # Output example: # Pool Statistics: # Total Connections: 100/100 (100%) # Active: 87 # Idle: 13 # Waiting Requests: 45 # Average Wait Time: 3420ms # Max Wait Time: 8234ms # Identify long-running queries cloudflow db queries --status running --min-duration 30000 # Check connection distribution by client SELECT",
              "score": null
            },
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_18",
              "content": "#### Database Connection Pool Exhausted **Symptoms**: \"Too many connections\" errors **Diagnosis**: ```bash # Check PgBouncer stats kubectl exec -it pgbouncer-xxx -n cloudflow-prod -- \\ psql -p 5432 -U pgbouncer pgbouncer -c \"SHOW POOLS;\" ``` **Resolution**: - Increase `max_db_connections` in PgBouncer - Optimize application connection usage - Add more PostgreSQL replicas ### Support Contacts - **On-call Engineer**: pagerduty.com/cloudflow-oncall - **Slack Channel**: #cloudflow-operations - **Documentation**: https://docs.cloudflow.io - **Status Page**: https://status.cloudflow.io --- ## Appendix ### Useful Commands ```bash # View all resources in namespace kubectl get all -n cloudflow-prod # Check resource usage kubectl top pods -n cloudflow-prod kubectl top nodes # View logs kubectl logs -f deployment/cloudflow-api -n cloudflow-prod # Execute command in pod kubectl exec -it <pod-name> -n cloudflow-prod -- /bin/sh # Port forward for local testing kubectl port-forward svc/cloudflow-api 8080:80 -n cloudflow-prod # Rollback deployment helm rollback cloudflow -n cloudflow-prod # Update deployment helm upgrade cloudflow cloudflow/cloudflow \\ --namespace cloudflow-prod",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_10",
              "content": "application_name, state, COUNT(*) as conn_count, AVG(EXTRACT(EPOCH FROM (NOW() - state_change))) as avg_duration_sec FROM pg_stat_activity WHERE datname = 'cloudflow_production' GROUP BY application_name, state ORDER BY conn_count DESC; ``` #### Resolution **Immediate mitigation:** ```bash # Temporarily increase connection limit (requires database restart) cloudflow db config set max_connections 150 # Kill idle connections older than 5 minutes cloudflow db connections kill --idle-timeout 300 # Restart connection pool without downtime kubectl rollout restart deployment/cloudflow-api -n cloudflow kubectl rollout status deployment/cloudflow-api -n cloudflow ``` **Long-term solutions:** 1. **Implement connection pooling optimization:** ```bash # Use PgBouncer for connection pooling kubectl apply -f cloudflow-pgbouncer.yaml # Configure CloudFlow to use PgBouncer cloudflow config set db.host pgbouncer.cloudflow.svc.cluster.local cloudflow config set db.port 6432 cloudflow config set db.pool.mode transaction ``` 2.",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_22",
              "content": "\"connection.*pool|could not connect|database.*timeout\" # Find slow queries kubectl logs -n cloudflow deployment/cloudflow-api | \\ grep \"slow query\" | \\ awk '{print $NF}' | \\ sort -n | \\ tail -n 20 # Find deadlock errors kubectl logs -n cloudflow deployment/cloudflow-api | \\ grep -i \"deadlock detected\" ``` #### Workflow Execution Errors ```bash # Find workflow timeout errors kubectl logs -n cloudflow deployment/cloudflow-workflow-engine | \\ grep -E \"timeout|exceeded.*3600\" # Find workflow retry attempts kubectl logs -n cloudflow deployment/cloudflow-workflow-engine | \\ grep -E \"retry attempt [0-9]|retrying in\" # Find workflow failures by ID kubectl logs -n cloudflow deployment/cloudflow-workflow-engine | \\ grep \"exec_7h3j6k9m2n\" ``` #### Rate Limiting ```bash # Find rate limit events kubectl logs -n cloudflow deployment/cloudflow-api | \\ grep -E \"429|rate.*limit|throttle\" # Count rate limit errors by hour kubectl logs -n cloudflow deployment/cloudflow-api --since=24h | \\ grep \"rate_limit_exceeded\" | \\ awk '{print $1}' | \\ cut -d'T' -f1-2 | \\ sort",
              "score": null
            }
          ],
          "latency_ms": 5.495561999850906,
          "query_id": "realistic_015",
          "dimension": "casual",
          "query": "connection pool full"
        },
        {
          "key_facts": [
            "could not obtain connection from pool within 5000ms",
            "connection pool exhausted (100/100 connections in use)",
            "PgBouncer",
            "max_db_connections = 100"
          ],
          "found_facts": [
            "PgBouncer"
          ],
          "missed_facts": [
            "could not obtain connection from pool within 5000ms",
            "connection pool exhausted (100/100 connections in use)",
            "max_db_connections = 100"
          ],
          "coverage": 0.25,
          "expected_docs": [
            "troubleshooting_guide",
            "deployment_guide"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_18",
              "content": "#### Database Connection Pool Exhausted **Symptoms**: \"Too many connections\" errors **Diagnosis**: ```bash # Check PgBouncer stats kubectl exec -it pgbouncer-xxx -n cloudflow-prod -- \\ psql -p 5432 -U pgbouncer pgbouncer -c \"SHOW POOLS;\" ``` **Resolution**: - Increase `max_db_connections` in PgBouncer - Optimize application connection usage - Add more PostgreSQL replicas ### Support Contacts - **On-call Engineer**: pagerduty.com/cloudflow-oncall - **Slack Channel**: #cloudflow-operations - **Documentation**: https://docs.cloudflow.io - **Status Page**: https://status.cloudflow.io --- ## Appendix ### Useful Commands ```bash # View all resources in namespace kubectl get all -n cloudflow-prod # Check resource usage kubectl top pods -n cloudflow-prod kubectl top nodes # View logs kubectl logs -f deployment/cloudflow-api -n cloudflow-prod # Execute command in pod kubectl exec -it <pod-name> -n cloudflow-prod -- /bin/sh # Port forward for local testing kubectl port-forward svc/cloudflow-api 8080:80 -n cloudflow-prod # Rollback deployment helm rollback cloudflow -n cloudflow-prod # Update deployment helm upgrade cloudflow cloudflow/cloudflow \\ --namespace cloudflow-prod",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_22",
              "content": "\"connection.*pool|could not connect|database.*timeout\" # Find slow queries kubectl logs -n cloudflow deployment/cloudflow-api | \\ grep \"slow query\" | \\ awk '{print $NF}' | \\ sort -n | \\ tail -n 20 # Find deadlock errors kubectl logs -n cloudflow deployment/cloudflow-api | \\ grep -i \"deadlock detected\" ``` #### Workflow Execution Errors ```bash # Find workflow timeout errors kubectl logs -n cloudflow deployment/cloudflow-workflow-engine | \\ grep -E \"timeout|exceeded.*3600\" # Find workflow retry attempts kubectl logs -n cloudflow deployment/cloudflow-workflow-engine | \\ grep -E \"retry attempt [0-9]|retrying in\" # Find workflow failures by ID kubectl logs -n cloudflow deployment/cloudflow-workflow-engine | \\ grep \"exec_7h3j6k9m2n\" ``` #### Rate Limiting ```bash # Find rate limit events kubectl logs -n cloudflow deployment/cloudflow-api | \\ grep -E \"429|rate.*limit|throttle\" # Count rate limit errors by hour kubectl logs -n cloudflow deployment/cloudflow-api --since=24h | \\ grep \"rate_limit_exceeded\" | \\ awk '{print $1}' | \\ cut -d'T' -f1-2 | \\ sort",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_11",
              "content": "**Add read replicas:** ```bash # Route read-only queries to replicas cloudflow db replicas add --count 2 cloudflow config set db.read_replicas \"replica-1:5432,replica-2:5432\" cloudflow config set db.read_write_split true ``` ### Connection Timeout Errors #### Error Messages - `connection timeout after 30000ms` - `could not connect to database server at 10.0.2.45:5432` - `database server unreachable` #### Troubleshooting Steps ```bash # Test network connectivity telnet cloudflow-db.internal.company.com 5432 # Check DNS resolution nslookup cloudflow-db.internal.company.com # Verify database is accepting connections pg_isready -h cloudflow-db.internal.company.com -p 5432 -U cloudflow # Check firewall rules sudo iptables -L -n | grep 5432 # Test from CloudFlow pod network kubectl run -n cloudflow debug-pod --rm -i --tty \\ --image=postgres:14 -- \\ psql -h cloudflow-db.internal.company.com -U cloudflow -d cloudflow_production # Review database logs for connection rejections kubectl logs -n cloudflow statefulset/cloudflow-db --tail=100 | \\ grep -i \"connection\\|reject\\|authentication\" ``` ### Maximum Connection Limit (100) Reached This is a hard limit in CloudFlow's",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_0",
              "content": "# CloudFlow Platform Troubleshooting Guide **Version:** 3.2.1 **Last Updated:** January 2026 **Audience:** Platform Engineers, DevOps, Support Teams ## Table of Contents 1. [Overview](#overview) 2. [Authentication & Authorization Issues](#authentication--authorization-issues) 3. [Performance Problems](#performance-problems) 4. [Database Connection Issues](#database-connection-issues) 5. [Workflow Execution Failures](#workflow-execution-failures) 6. [Rate Limiting & Throttling](#rate-limiting--throttling) 7. [Log Analysis & Debugging](#log-analysis--debugging) 8. [Escalation Procedures](#escalation-procedures) --- ## Overview This guide provides comprehensive troubleshooting steps for common CloudFlow platform issues encountered in production environments. Each section includes error symptoms, root cause analysis, resolution steps, and preventive measures.",
              "score": null
            },
            {
              "doc_id": "user_guide",
              "chunk_id": "user_guide_fix_16",
              "content": "fixing issues - Export failed executions for analysis - Set up alerts for DLQ threshold breaches **Accessing the DLQ:** 1. Navigate to **Workflows** > **[Your Workflow]** > **Dead Letter Queue** 2. Filter by error type, date range, or execution ID 3. Click an execution to view full details 4. Click **\"Retry\"** to reprocess with the same input data ### Error Notifications Get notified when workflows fail: ```yaml workflow: notifications: on_failure: - type: email to: \"ops-team@company.com\" - type: slack channel: \"#alerts\" message: \"Workflow {{workflow.name}} failed: {{error.message}}\" on_success_after_retry: - type: slack channel: \"#monitoring\" message: \"Workflow recovered after {{error.attempts}} attempts\" ``` ## Workflow Limits CloudFlow enforces the following limits to ensure platform stability and performance: ### Steps Per Workflow - **Maximum**: 50 steps per workflow - **Recommendation**: Keep workflows focused and modular. If you need more steps, consider splitting into multiple workflows connected via webhooks.",
              "score": null
            }
          ],
          "latency_ms": 5.335955000191461,
          "query_id": "realistic_015",
          "dimension": "contextual",
          "query": "My app is failing with connection pool errors, how do I troubleshoot?"
        },
        {
          "key_facts": [
            "could not obtain connection from pool within 5000ms",
            "connection pool exhausted (100/100 connections in use)",
            "PgBouncer",
            "max_db_connections = 100"
          ],
          "found_facts": [
            "could not obtain connection from pool within 5000ms",
            "connection pool exhausted (100/100 connections in use)",
            "PgBouncer"
          ],
          "missed_facts": [
            "max_db_connections = 100"
          ],
          "coverage": 0.75,
          "expected_docs": [
            "troubleshooting_guide",
            "deployment_guide"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_22",
              "content": "\"connection.*pool|could not connect|database.*timeout\" # Find slow queries kubectl logs -n cloudflow deployment/cloudflow-api | \\ grep \"slow query\" | \\ awk '{print $NF}' | \\ sort -n | \\ tail -n 20 # Find deadlock errors kubectl logs -n cloudflow deployment/cloudflow-api | \\ grep -i \"deadlock detected\" ``` #### Workflow Execution Errors ```bash # Find workflow timeout errors kubectl logs -n cloudflow deployment/cloudflow-workflow-engine | \\ grep -E \"timeout|exceeded.*3600\" # Find workflow retry attempts kubectl logs -n cloudflow deployment/cloudflow-workflow-engine | \\ grep -E \"retry attempt [0-9]|retrying in\" # Find workflow failures by ID kubectl logs -n cloudflow deployment/cloudflow-workflow-engine | \\ grep \"exec_7h3j6k9m2n\" ``` #### Rate Limiting ```bash # Find rate limit events kubectl logs -n cloudflow deployment/cloudflow-api | \\ grep -E \"429|rate.*limit|throttle\" # Count rate limit errors by hour kubectl logs -n cloudflow deployment/cloudflow-api --since=24h | \\ grep \"rate_limit_exceeded\" | \\ awk '{print $1}' | \\ cut -d'T' -f1-2 | \\ sort",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_12",
              "content": "database tier. #### Permanent Solutions **Option 1: Upgrade database tier** ```bash # Check available tiers cloudflow db tiers list # Upgrade to higher tier (supports 200 connections) cloudflow db upgrade --tier standard-plus --confirm # Monitor migration progress cloudflow db migration status ``` **Option 2: Implement aggressive connection reuse** ```bash # Reduce connection lifetime cloudflow config set db.pool.max_lifetime 600 # 10 minutes # Enable prepared statement caching cloudflow config set db.prepared_statements.cache true cloudflow config set db.prepared_statements.max_size 250 # Reduce idle connection timeout cloudflow config set db.pool.idle_timeout 120 # 2 minutes ``` --- ## Workflow Execution Failures ### Timeout Errors (3600 second limit) #### Error Message ``` WorkflowExecutionError: Workflow exceeded maximum execution time of 3600 seconds Status: TIMEOUT Workflow ID: wf_9k2n4m8p1q Execution ID: exec_7h3j6k9m2n ``` #### Analysis ```bash # Get workflow execution details cloudflow workflows executions get exec_7h3j6k9m2n --verbose # Check step-by-step breakdown cloudflow workflows executions steps exec_7h3j6k9m2n # Sample output: #",
              "score": null
            },
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_15",
              "content": "Pooling Ensure optimal database connection pooling settings in PgBouncer: - **Pool Mode**: Transaction (optimal for microservices) - **Default Pool Size**: 25 connections per user - **Max DB Connections**: 100 (matches PostgreSQL `max_connections`) #### Redis Caching Implement Redis caching for frequently accessed data: ```javascript // Example caching strategy const CACHE_TTL = { workflows: 300, // 5 minutes userSessions: 3600, // 1 hour apiResults: 60 // 1 minute }; ``` #### Load Testing Perform regular load testing to validate scaling configuration: ```bash # Install k6 for load testing kubectl apply -f - <<EOF apiVersion: v1 kind: ConfigMap metadata: name: loadtest-script namespace: cloudflow-prod data: script.js: | import http from 'k6/http'; import { check, sleep } from 'k6'; export let options = { stages: [ { duration: '2m', target: 100 }, { duration: '5m', target: 100 }, { duration: '2m', target: 200 }, { duration: '5m', target: 200 }, { duration: '2m', target: 0",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_9",
              "content": "```bash # Configure event buffer limits cloudflow config set events.buffer.max_size 10000 cloudflow config set events.buffer.overflow_strategy drop_oldest # Enable event streaming to external sink cloudflow config set events.sink.type kafka cloudflow config set events.sink.kafka.brokers \"kafka-1:9092,kafka-2:9092\" cloudflow config set events.sink.kafka.topic cloudflow-events ``` --- ## Database Connection Issues ### Connection Pool Exhaustion #### Symptoms - Error: `could not obtain connection from pool within 5000ms` - Error: `connection pool exhausted (100/100 connections in use)` - API requests fail with `503 Service Unavailable` - Database CPU usage normal, but connection count at maximum #### Investigation ```bash # Check current connection pool status cloudflow db pool status --detailed # Output example: # Pool Statistics: # Total Connections: 100/100 (100%) # Active: 87 # Idle: 13 # Waiting Requests: 45 # Average Wait Time: 3420ms # Max Wait Time: 8234ms # Identify long-running queries cloudflow db queries --status running --min-duration 30000 # Check connection distribution by client SELECT",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_11",
              "content": "**Add read replicas:** ```bash # Route read-only queries to replicas cloudflow db replicas add --count 2 cloudflow config set db.read_replicas \"replica-1:5432,replica-2:5432\" cloudflow config set db.read_write_split true ``` ### Connection Timeout Errors #### Error Messages - `connection timeout after 30000ms` - `could not connect to database server at 10.0.2.45:5432` - `database server unreachable` #### Troubleshooting Steps ```bash # Test network connectivity telnet cloudflow-db.internal.company.com 5432 # Check DNS resolution nslookup cloudflow-db.internal.company.com # Verify database is accepting connections pg_isready -h cloudflow-db.internal.company.com -p 5432 -U cloudflow # Check firewall rules sudo iptables -L -n | grep 5432 # Test from CloudFlow pod network kubectl run -n cloudflow debug-pod --rm -i --tty \\ --image=postgres:14 -- \\ psql -h cloudflow-db.internal.company.com -U cloudflow -d cloudflow_production # Review database logs for connection rejections kubectl logs -n cloudflow statefulset/cloudflow-db --tail=100 | \\ grep -i \"connection\\|reject\\|authentication\" ``` ### Maximum Connection Limit (100) Reached This is a hard limit in CloudFlow's",
              "score": null
            }
          ],
          "latency_ms": 5.48224700105493,
          "query_id": "realistic_015",
          "dimension": "negation",
          "query": "Why can't I get a database connection even though CPU is low?"
        },
        {
          "key_facts": [
            "OAuth 2.0",
            "API keys",
            "JWT tokens"
          ],
          "found_facts": [
            "OAuth 2.0",
            "API keys",
            "JWT tokens"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "api_reference"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "user_guide",
              "chunk_id": "user_guide_fix_19",
              "content": "Handle Errors Gracefully Always implement error handling for external API calls and database operations: ```yaml - id: fetch_data action: http_request config: url: \"https://api.example.com/data\" retry: max_attempts: 3 backoff_type: exponential on_error: - id: log_error action: database_query config: query: \"INSERT INTO error_log (workflow_id, error) VALUES ($1, $2)\" parameters: - \"{{workflow.id}}\" - \"{{error.message}}\" ``` ### 3. Use Secrets for Sensitive Data Never hardcode API keys, passwords, or tokens in workflows: **Bad:** ```yaml headers: Authorization: \"Bearer sk_live_abc123xyz789\" ``` **Good:** ```yaml headers: Authorization: \"Bearer {{secrets.API_TOKEN}}\" ``` Store secrets in **Settings** > **Secrets** with encryption at rest. ### 4.",
              "score": null
            },
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_0",
              "content": "# CloudFlow API Reference Version 2.1.0 | Last Updated: January 2026 ## Overview The CloudFlow API is a RESTful service that enables developers to programmatically manage cloud workflows, data pipelines, and automation tasks. This documentation provides comprehensive details on authentication, endpoints, request/response formats, error handling, and best practices. **Base URL:** `https://api.cloudflow.io/v2` **API Status:** https://status.cloudflow.io ## Authentication CloudFlow supports three authentication methods to suit different use cases and security requirements. ### API Keys API keys provide simple authentication for server-to-server communication. Include your API key in the request header: ```bash curl -H \"X-API-Key: cf_live_a1b2c3d4e5f6g7h8i9j0\" \\ https://api.cloudflow.io/v2/workflows ``` **Security Notes:** - Never expose API keys in client-side code - Rotate keys every 90 days - Use separate keys for development and production environments ### OAuth 2.0 OAuth 2.0 is recommended for applications that access CloudFlow on behalf of users. We support the Authorization Code flow with PKCE.",
              "score": null
            },
            {
              "doc_id": "user_guide",
              "chunk_id": "user_guide_fix_5",
              "content": "workflows on-demand from the dashboard or via API ## Available Actions CloudFlow provides a comprehensive library of actions to build powerful automations. ### HTTP Requests Make HTTP requests to any API endpoint: **Configuration:** - **Method**: GET, POST, PUT, PATCH, DELETE, HEAD - **URL**: Full endpoint URL (supports variable interpolation) - **Headers**: Custom headers as key-value pairs - **Query Parameters**: URL parameters - **Body**: JSON, form data, or raw text - **Authentication**: Basic Auth, Bearer Token, API Key, OAuth 2.0 **Example:** ```yaml - id: fetch_user action: http_request config: method: GET url: \"https://api.example.com/users/{{user_id}}\" headers: Authorization: \"Bearer {{secrets.API_TOKEN}}\" Content-Type: \"application/json\" timeout: 30 ``` **Response Handling:** Access the response in subsequent steps: - `{{steps.fetch_user.status}}` - HTTP status code - `{{steps.fetch_user.body}}` - Response body - `{{steps.fetch_user.headers}}` - Response headers ### Database Queries Execute queries against supported databases (PostgreSQL, MySQL, MongoDB, Redis): **SQL Databases (PostgreSQL, MySQL):** ```yaml - id: get_orders action: database_query config: connection: \"{{secrets.DB_CONNECTION_STRING}}\" query:",
              "score": null
            },
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_2",
              "content": "JWTs must include the following claims: - `iss` (issuer): Your application identifier - `sub` (subject): User or service account ID - `aud` (audience): `https://api.cloudflow.io` - `exp` (expiration): Unix timestamp (max 3600 seconds from `iat`) - `iat` (issued at): Unix timestamp - `scope`: Space-separated list of requested scopes Example JWT header: ```python import jwt import time from cryptography.hazmat.primitives import serialization from cryptography.hazmat.backends import default_backend # Load your private key with open('private_key.pem', 'rb') as key_file: private_key = serialization.load_pem_private_key( key_file.read(), password=None, backend=default_backend() ) # Create JWT payload payload = { 'iss': 'your-app-id', 'sub': 'user-12345', 'aud': 'https://api.cloudflow.io', 'exp': int(time.time()) + 3600, 'iat': int(time.time()), 'scope': 'workflows:read workflows:write' } # Generate token token = jwt.encode(payload, private_key, algorithm='RS256') # Use in API request headers = {'Authorization': f'Bearer {token}'} ``` **Token Expiration:** All tokens expire after 3600 seconds (1 hour). Implement token refresh logic in your application.",
              "score": null
            },
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_1",
              "content": "**Authorization Endpoint:** `https://auth.cloudflow.io/oauth/authorize` **Token Endpoint:** `https://auth.cloudflow.io/oauth/token` **Supported Scopes:** - `workflows:read` - Read workflow configurations - `workflows:write` - Create and modify workflows - `pipelines:read` - Read pipeline data - `pipelines:write` - Create and manage pipelines - `analytics:read` - Access analytics and metrics - `admin:full` - Full administrative access Example authorization request: ```bash curl -X POST https://auth.cloudflow.io/oauth/token \\ -H \"Content-Type: application/x-www-form-urlencoded\" \\ -d \"grant_type=authorization_code\" \\ -d \"code=AUTH_CODE_HERE\" \\ -d \"client_id=YOUR_CLIENT_ID\" \\ -d \"client_secret=YOUR_CLIENT_SECRET\" \\ -d \"redirect_uri=https://yourapp.com/callback\" ``` ### JWT Tokens For advanced use cases, CloudFlow supports JSON Web Tokens (JWT) with RS256 signing algorithm.",
              "score": null
            }
          ],
          "latency_ms": 5.44608999916818,
          "query_id": "realistic_016",
          "dimension": "original",
          "query": "How do I handle API authentication?"
        },
        {
          "key_facts": [
            "OAuth 2.0",
            "API keys",
            "JWT tokens"
          ],
          "found_facts": [
            "OAuth 2.0",
            "API keys",
            "JWT tokens"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "api_reference"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_1",
              "content": "**Authorization Endpoint:** `https://auth.cloudflow.io/oauth/authorize` **Token Endpoint:** `https://auth.cloudflow.io/oauth/token` **Supported Scopes:** - `workflows:read` - Read workflow configurations - `workflows:write` - Create and modify workflows - `pipelines:read` - Read pipeline data - `pipelines:write` - Create and manage pipelines - `analytics:read` - Access analytics and metrics - `admin:full` - Full administrative access Example authorization request: ```bash curl -X POST https://auth.cloudflow.io/oauth/token \\ -H \"Content-Type: application/x-www-form-urlencoded\" \\ -d \"grant_type=authorization_code\" \\ -d \"code=AUTH_CODE_HERE\" \\ -d \"client_id=YOUR_CLIENT_ID\" \\ -d \"client_secret=YOUR_CLIENT_SECRET\" \\ -d \"redirect_uri=https://yourapp.com/callback\" ``` ### JWT Tokens For advanced use cases, CloudFlow supports JSON Web Tokens (JWT) with RS256 signing algorithm.",
              "score": null
            },
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_13",
              "content": "**Supported Events:** - `workflow.started` - `workflow.completed` - `workflow.failed` - `pipeline.completed` - `pipeline.failed` **Webhook Payload Example:** ```json { \"event\": \"workflow.completed\", \"timestamp\": \"2026-01-24T10:45:00Z\", \"data\": { \"workflow_id\": \"wf_8x7k2m9p\", \"execution_id\": \"exec_9k3m7n2q\", \"status\": \"completed\", \"duration_ms\": 3420, \"records_processed\": 1247 } } ``` Configure webhooks in your account settings or via the API: ```bash curl -X POST https://api.cloudflow.io/v2/webhooks \\ -H \"X-API-Key: YOUR_API_KEY\" \\ -H \"Content-Type: application/json\" \\ -d '{ \"url\": \"https://your-app.com/cloudflow-webhook\", \"events\": [\"workflow.completed\", \"workflow.failed\"], \"secret\": \"whsec_your_webhook_secret\" }' ``` ## Support For additional help and resources: - **Documentation:** https://docs.cloudflow.io - **API Status:** https://status.cloudflow.io - **Support Email:** support@cloudflow.io - **Community Forum:** https://community.cloudflow.io Enterprise customers have access to 24/7 priority support via phone and dedicated Slack channels.",
              "score": null
            },
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_0",
              "content": "# CloudFlow API Reference Version 2.1.0 | Last Updated: January 2026 ## Overview The CloudFlow API is a RESTful service that enables developers to programmatically manage cloud workflows, data pipelines, and automation tasks. This documentation provides comprehensive details on authentication, endpoints, request/response formats, error handling, and best practices. **Base URL:** `https://api.cloudflow.io/v2` **API Status:** https://status.cloudflow.io ## Authentication CloudFlow supports three authentication methods to suit different use cases and security requirements. ### API Keys API keys provide simple authentication for server-to-server communication. Include your API key in the request header: ```bash curl -H \"X-API-Key: cf_live_a1b2c3d4e5f6g7h8i9j0\" \\ https://api.cloudflow.io/v2/workflows ``` **Security Notes:** - Never expose API keys in client-side code - Rotate keys every 90 days - Use separate keys for development and production environments ### OAuth 2.0 OAuth 2.0 is recommended for applications that access CloudFlow on behalf of users. We support the Authorization Code flow with PKCE.",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_2",
              "content": "**Verification:** ```bash # Decode JWT to check expiration echo $CF_ACCESS_TOKEN | cut -d'.' -f2 | base64 -d | jq '.exp' # Compare with current time date +%s ``` **Resolution:** ```bash # Refresh the access token curl -X POST https://api.cloudflow.io/auth/refresh \\ -H \"Content-Type: application/json\" \\ -d '{\"refresh_token\": \"'$CF_REFRESH_TOKEN'\"}' # Update environment variable export CF_ACCESS_TOKEN=\"<new_token>\" ``` **2. Invalid JWT Signature** Error: `JWT signature verification failed` **Causes:** - Token was modified or corrupted - Using wrong signing key - Token generated with different secret **Resolution:** ```bash # Validate token structure cloudflow auth validate --token $CF_ACCESS_TOKEN # Generate new token cloudflow auth login --username <user> --password <pass> # For service accounts cloudflow auth service-account --create --name <sa-name> --scopes \"workflow:read,workflow:write\" ``` **3. Clock Skew Issues** JWT validation fails when system clocks are out of sync (tolerance: \u00b1300 seconds).",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_4",
              "content": "authorization service handling user identity, token generation, and permission validation. **Technology**: Go with gRPC for internal communication, REST for external **Replicas**: 8 pods (production), auto-scaling 6-12 **Resource Allocation**: 1 vCPU, 2GB RAM per pod **Key Responsibilities**: - User authentication via multiple providers (OAuth2, SAML, local credentials) - JWT token generation and validation (RS256 algorithm) - Role-based access control (RBAC) with fine-grained permissions - Session management with Redis-backed storage - API key generation and validation for service accounts - MFA enforcement for administrative operations **Authentication Flow**: ``` Client Request \u2192 API Gateway \u2192 Auth Service \u2502 \u251c\u2500 Validate credentials \u251c\u2500 Check MFA if required \u251c\u2500 Generate JWT (15min expiry) \u251c\u2500 Generate refresh token (7 days) \u2514\u2500 Store session in Redis ``` **Token Structure**: - Access Token: JWT with 15-minute expiry - Refresh Token: Opaque token with 7-day expiry, stored in PostgreSQL - Claims: user_id, email, roles[], permissions[], tenant_id **Security Features**: -",
              "score": null
            }
          ],
          "latency_ms": 5.807203000586014,
          "query_id": "realistic_016",
          "dimension": "synonym",
          "query": "What authentication methods are supported?"
        },
        {
          "key_facts": [
            "OAuth 2.0",
            "API keys",
            "JWT tokens"
          ],
          "found_facts": [
            "OAuth 2.0",
            "API keys"
          ],
          "missed_facts": [
            "JWT tokens"
          ],
          "coverage": 0.6666666666666666,
          "expected_docs": [
            "api_reference"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_11",
              "content": "Codes CloudFlow returns specific error codes to help you identify and resolve issues: - `invalid_parameter`: One or more request parameters are invalid - `missing_required_field`: Required field is missing from request body - `authentication_failed`: Invalid API key or token - `insufficient_permissions`: User lacks required scope or permission - `resource_not_found`: Requested resource does not exist - `rate_limit_exceeded`: Too many requests, see rate limiting section - `workflow_execution_failed`: Workflow execution encountered an error - `invalid_json`: Request body contains malformed JSON - `duplicate_resource`: Resource with same identifier already exists - `quota_exceeded`: Account quota limit reached ### Error Handling Best Practices **Python Example:** ```python import requests import time def make_api_request(url, headers, max_retries=3): for attempt in range(max_retries): try: response = requests.get(url, headers=headers) if response.status_code == 200: return response.json() elif response.status_code == 429: # Rate limit exceeded retry_after = int(response.headers.get('X-RateLimit-Reset', 60)) print(f\"Rate limited.",
              "score": null
            },
            {
              "doc_id": "user_guide",
              "chunk_id": "user_guide_fix_19",
              "content": "Handle Errors Gracefully Always implement error handling for external API calls and database operations: ```yaml - id: fetch_data action: http_request config: url: \"https://api.example.com/data\" retry: max_attempts: 3 backoff_type: exponential on_error: - id: log_error action: database_query config: query: \"INSERT INTO error_log (workflow_id, error) VALUES ($1, $2)\" parameters: - \"{{workflow.id}}\" - \"{{error.message}}\" ``` ### 3. Use Secrets for Sensitive Data Never hardcode API keys, passwords, or tokens in workflows: **Bad:** ```yaml headers: Authorization: \"Bearer sk_live_abc123xyz789\" ``` **Good:** ```yaml headers: Authorization: \"Bearer {{secrets.API_TOKEN}}\" ``` Store secrets in **Settings** > **Secrets** with encryption at rest. ### 4.",
              "score": null
            },
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_7",
              "content": "**Endpoint:** `GET /workflows/{workflow_id}` **Example Request:** ```bash curl -H \"X-API-Key: YOUR_API_KEY\" \\ https://api.cloudflow.io/v2/workflows/wf_8x7k2m9p ``` **Example Response (200 OK):** ```json { \"id\": \"wf_8x7k2m9p\", \"name\": \"Data Processing Pipeline\", \"description\": \"Processes customer data every hour\", \"status\": \"active\", \"trigger\": { \"type\": \"schedule\", \"cron\": \"0 * * * *\", \"timezone\": \"UTC\" }, \"steps\": [ { \"id\": \"step_1\", \"type\": \"fetch_data\", \"source\": \"customer_db\", \"query\": \"SELECT * FROM customers WHERE updated_at > :last_run\" }, { \"id\": \"step_2\", \"type\": \"transform\", \"script_id\": \"scr_transform_v3\" } ], \"metrics\": { \"total_runs\": 1247, \"success_rate\": 98.7, \"avg_duration_ms\": 3420, \"last_error\": null }, \"created_at\": \"2026-01-15T10:30:00Z\", \"updated_at\": \"2026-01-23T14:22:00Z\" } ``` #### Update Workflow Modify an existing workflow's configuration. **Endpoint:** `PATCH /workflows/{workflow_id}` **Example Request:** ```bash curl -X PATCH https://api.cloudflow.io/v2/workflows/wf_8x7k2m9p \\ -H \"X-API-Key: YOUR_API_KEY\" \\ -H \"Content-Type: application/json\" \\ -d '{\"status\": \"paused\"}' ``` **Example Response (200 OK):** ```json { \"id\": \"wf_8x7k2m9p\", \"status\": \"paused\", \"updated_at\": \"2026-01-24T10:30:00Z\" } ``` #### Delete Workflow Permanently delete a workflow.",
              "score": null
            },
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_10",
              "content": "format. ### Error Response Format ```json { \"error\": { \"code\": \"invalid_parameter\", \"message\": \"The 'limit' parameter must be between 1 and 100\", \"field\": \"limit\", \"request_id\": \"req_8k3m9x2p\" } } ``` ### HTTP Status Codes | Status Code | Description | Common Causes | |------------|-------------|---------------| | 400 | Bad Request | Invalid parameters, malformed JSON, validation errors | | 401 | Unauthorized | Missing or invalid authentication credentials | | 403 | Forbidden | Insufficient permissions for requested resource | | 404 | Not Found | Resource does not exist or has been deleted | | 429 | Too Many Requests | Rate limit exceeded, retry after specified period | | 500 | Internal Server Error | Unexpected server error, contact support if persists | | 502 | Bad Gateway | Temporary service issue, retry with exponential backoff | | 503 | Service Unavailable | Scheduled maintenance or temporary outage | ### Error",
              "score": null
            },
            {
              "doc_id": "user_guide",
              "chunk_id": "user_guide_fix_5",
              "content": "workflows on-demand from the dashboard or via API ## Available Actions CloudFlow provides a comprehensive library of actions to build powerful automations. ### HTTP Requests Make HTTP requests to any API endpoint: **Configuration:** - **Method**: GET, POST, PUT, PATCH, DELETE, HEAD - **URL**: Full endpoint URL (supports variable interpolation) - **Headers**: Custom headers as key-value pairs - **Query Parameters**: URL parameters - **Body**: JSON, form data, or raw text - **Authentication**: Basic Auth, Bearer Token, API Key, OAuth 2.0 **Example:** ```yaml - id: fetch_user action: http_request config: method: GET url: \"https://api.example.com/users/{{user_id}}\" headers: Authorization: \"Bearer {{secrets.API_TOKEN}}\" Content-Type: \"application/json\" timeout: 30 ``` **Response Handling:** Access the response in subsequent steps: - `{{steps.fetch_user.status}}` - HTTP status code - `{{steps.fetch_user.body}}` - Response body - `{{steps.fetch_user.headers}}` - Response headers ### Database Queries Execute queries against supported databases (PostgreSQL, MySQL, MongoDB, Redis): **SQL Databases (PostgreSQL, MySQL):** ```yaml - id: get_orders action: database_query config: connection: \"{{secrets.DB_CONNECTION_STRING}}\" query:",
              "score": null
            }
          ],
          "latency_ms": 5.56649400095921,
          "query_id": "realistic_016",
          "dimension": "problem",
          "query": "My API requests are getting 401 errors"
        },
        {
          "key_facts": [
            "OAuth 2.0",
            "API keys",
            "JWT tokens"
          ],
          "found_facts": [
            "OAuth 2.0",
            "JWT tokens"
          ],
          "missed_facts": [
            "API keys"
          ],
          "coverage": 0.6666666666666666,
          "expected_docs": [
            "api_reference"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_4",
              "content": "authorization service handling user identity, token generation, and permission validation. **Technology**: Go with gRPC for internal communication, REST for external **Replicas**: 8 pods (production), auto-scaling 6-12 **Resource Allocation**: 1 vCPU, 2GB RAM per pod **Key Responsibilities**: - User authentication via multiple providers (OAuth2, SAML, local credentials) - JWT token generation and validation (RS256 algorithm) - Role-based access control (RBAC) with fine-grained permissions - Session management with Redis-backed storage - API key generation and validation for service accounts - MFA enforcement for administrative operations **Authentication Flow**: ``` Client Request \u2192 API Gateway \u2192 Auth Service \u2502 \u251c\u2500 Validate credentials \u251c\u2500 Check MFA if required \u251c\u2500 Generate JWT (15min expiry) \u251c\u2500 Generate refresh token (7 days) \u2514\u2500 Store session in Redis ``` **Token Structure**: - Access Token: JWT with 15-minute expiry - Refresh Token: Opaque token with 7-day expiry, stored in PostgreSQL - Claims: user_id, email, roles[], permissions[], tenant_id **Security Features**: -",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_5",
              "content": "Password hashing: Argon2id with 64MB memory, 4 iterations - Token rotation on refresh to prevent replay attacks - Brute force protection: 5 failed attempts \u2192 15-minute lockout - Secrets stored in HashiCorp Vault, rotated every 90 days **Performance Targets**: - Token validation: < 10ms (P99) - Token generation: < 50ms (P99) - Throughput: 5,000 RPS for validation operations --- ### Workflow Engine **Purpose**: Core orchestration service that executes workflow definitions, manages state transitions, and coordinates task execution across distributed systems.",
              "score": null
            },
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_6",
              "content": "**Endpoint:** `POST /workflows` **Request Body:** ```json { \"name\": \"Email Campaign Automation\", \"description\": \"Sends personalized emails based on user behavior\", \"trigger\": { \"type\": \"webhook\", \"url\": \"https://your-app.com/webhook\" }, \"steps\": [ { \"type\": \"fetch_data\", \"source\": \"users_table\", \"filters\": {\"active\": true} }, { \"type\": \"transform\", \"script\": \"data.map(user => ({ ...user, segment: calculateSegment(user) }))\" }, { \"type\": \"send_email\", \"template_id\": \"tpl_welcome_v2\" } ] } ``` **Example Request:** ```bash curl -X POST https://api.cloudflow.io/v2/workflows \\ -H \"X-API-Key: YOUR_API_KEY\" \\ -H \"Content-Type: application/json\" \\ -d '{ \"name\": \"Email Campaign Automation\", \"trigger\": {\"type\": \"webhook\"}, \"steps\": [{\"type\": \"fetch_data\", \"source\": \"users_table\"}] }' ``` **Example Response (201 Created):** ```json { \"id\": \"wf_9k3m7n2q\", \"name\": \"Email Campaign Automation\", \"status\": \"active\", \"created_at\": \"2026-01-24T10:15:00Z\", \"webhook_url\": \"https://api.cloudflow.io/v2/webhooks/wf_9k3m7n2q/trigger\" } ``` #### Get Workflow Details Retrieve detailed information about a specific workflow.",
              "score": null
            },
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_1",
              "content": "**Authorization Endpoint:** `https://auth.cloudflow.io/oauth/authorize` **Token Endpoint:** `https://auth.cloudflow.io/oauth/token` **Supported Scopes:** - `workflows:read` - Read workflow configurations - `workflows:write` - Create and modify workflows - `pipelines:read` - Read pipeline data - `pipelines:write` - Create and manage pipelines - `analytics:read` - Access analytics and metrics - `admin:full` - Full administrative access Example authorization request: ```bash curl -X POST https://auth.cloudflow.io/oauth/token \\ -H \"Content-Type: application/x-www-form-urlencoded\" \\ -d \"grant_type=authorization_code\" \\ -d \"code=AUTH_CODE_HERE\" \\ -d \"client_id=YOUR_CLIENT_ID\" \\ -d \"client_secret=YOUR_CLIENT_SECRET\" \\ -d \"redirect_uri=https://yourapp.com/callback\" ``` ### JWT Tokens For advanced use cases, CloudFlow supports JSON Web Tokens (JWT) with RS256 signing algorithm.",
              "score": null
            },
            {
              "doc_id": "user_guide",
              "chunk_id": "user_guide_fix_5",
              "content": "workflows on-demand from the dashboard or via API ## Available Actions CloudFlow provides a comprehensive library of actions to build powerful automations. ### HTTP Requests Make HTTP requests to any API endpoint: **Configuration:** - **Method**: GET, POST, PUT, PATCH, DELETE, HEAD - **URL**: Full endpoint URL (supports variable interpolation) - **Headers**: Custom headers as key-value pairs - **Query Parameters**: URL parameters - **Body**: JSON, form data, or raw text - **Authentication**: Basic Auth, Bearer Token, API Key, OAuth 2.0 **Example:** ```yaml - id: fetch_user action: http_request config: method: GET url: \"https://api.example.com/users/{{user_id}}\" headers: Authorization: \"Bearer {{secrets.API_TOKEN}}\" Content-Type: \"application/json\" timeout: 30 ``` **Response Handling:** Access the response in subsequent steps: - `{{steps.fetch_user.status}}` - HTTP status code - `{{steps.fetch_user.body}}` - Response body - `{{steps.fetch_user.headers}}` - Response headers ### Database Queries Execute queries against supported databases (PostgreSQL, MySQL, MongoDB, Redis): **SQL Databases (PostgreSQL, MySQL):** ```yaml - id: get_orders action: database_query config: connection: \"{{secrets.DB_CONNECTION_STRING}}\" query:",
              "score": null
            }
          ],
          "latency_ms": 5.525567001313902,
          "query_id": "realistic_016",
          "dimension": "casual",
          "query": "auth methods"
        },
        {
          "key_facts": [
            "OAuth 2.0",
            "API keys",
            "JWT tokens"
          ],
          "found_facts": [
            "OAuth 2.0",
            "API keys",
            "JWT tokens"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "api_reference"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_0",
              "content": "# CloudFlow API Reference Version 2.1.0 | Last Updated: January 2026 ## Overview The CloudFlow API is a RESTful service that enables developers to programmatically manage cloud workflows, data pipelines, and automation tasks. This documentation provides comprehensive details on authentication, endpoints, request/response formats, error handling, and best practices. **Base URL:** `https://api.cloudflow.io/v2` **API Status:** https://status.cloudflow.io ## Authentication CloudFlow supports three authentication methods to suit different use cases and security requirements. ### API Keys API keys provide simple authentication for server-to-server communication. Include your API key in the request header: ```bash curl -H \"X-API-Key: cf_live_a1b2c3d4e5f6g7h8i9j0\" \\ https://api.cloudflow.io/v2/workflows ``` **Security Notes:** - Never expose API keys in client-side code - Rotate keys every 90 days - Use separate keys for development and production environments ### OAuth 2.0 OAuth 2.0 is recommended for applications that access CloudFlow on behalf of users. We support the Authorization Code flow with PKCE.",
              "score": null
            },
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_1",
              "content": "**Authorization Endpoint:** `https://auth.cloudflow.io/oauth/authorize` **Token Endpoint:** `https://auth.cloudflow.io/oauth/token` **Supported Scopes:** - `workflows:read` - Read workflow configurations - `workflows:write` - Create and modify workflows - `pipelines:read` - Read pipeline data - `pipelines:write` - Create and manage pipelines - `analytics:read` - Access analytics and metrics - `admin:full` - Full administrative access Example authorization request: ```bash curl -X POST https://auth.cloudflow.io/oauth/token \\ -H \"Content-Type: application/x-www-form-urlencoded\" \\ -d \"grant_type=authorization_code\" \\ -d \"code=AUTH_CODE_HERE\" \\ -d \"client_id=YOUR_CLIENT_ID\" \\ -d \"client_secret=YOUR_CLIENT_SECRET\" \\ -d \"redirect_uri=https://yourapp.com/callback\" ``` ### JWT Tokens For advanced use cases, CloudFlow supports JSON Web Tokens (JWT) with RS256 signing algorithm.",
              "score": null
            },
            {
              "doc_id": "user_guide",
              "chunk_id": "user_guide_fix_0",
              "content": "# CloudFlow User Guide Welcome to CloudFlow, the modern workflow automation platform that helps you connect your apps, automate repetitive tasks, and build powerful integrations without writing code. ## Table of Contents 1. [Getting Started](#getting-started) 2. [Workflow Creation](#workflow-creation) 3. [Available Actions](#available-actions) 4. [Variables and Expressions](#variables-and-expressions) 5. [Scheduling](#scheduling) 6. [Error Handling](#error-handling) 7. [Workflow Limits](#workflow-limits) 8. [Best Practices](#best-practices) 9. [Common Workflow Patterns](#common-workflow-patterns) ## Getting Started ### Account Setup Creating your CloudFlow account takes just a few minutes: 1. Navigate to https://app.cloudflow.io/signup 2. Enter your email address and create a strong password 3. Verify your email address by clicking the link sent to your inbox 4. Complete your profile by providing your organization name and use case 5. Choose your plan (Free, Professional, or Enterprise) Once your account is created, you'll be redirected to your CloudFlow dashboard where you can start building workflows immediately.",
              "score": null
            },
            {
              "doc_id": "user_guide",
              "chunk_id": "user_guide_fix_5",
              "content": "workflows on-demand from the dashboard or via API ## Available Actions CloudFlow provides a comprehensive library of actions to build powerful automations. ### HTTP Requests Make HTTP requests to any API endpoint: **Configuration:** - **Method**: GET, POST, PUT, PATCH, DELETE, HEAD - **URL**: Full endpoint URL (supports variable interpolation) - **Headers**: Custom headers as key-value pairs - **Query Parameters**: URL parameters - **Body**: JSON, form data, or raw text - **Authentication**: Basic Auth, Bearer Token, API Key, OAuth 2.0 **Example:** ```yaml - id: fetch_user action: http_request config: method: GET url: \"https://api.example.com/users/{{user_id}}\" headers: Authorization: \"Bearer {{secrets.API_TOKEN}}\" Content-Type: \"application/json\" timeout: 30 ``` **Response Handling:** Access the response in subsequent steps: - `{{steps.fetch_user.status}}` - HTTP status code - `{{steps.fetch_user.body}}` - Response body - `{{steps.fetch_user.headers}}` - Response headers ### Database Queries Execute queries against supported databases (PostgreSQL, MySQL, MongoDB, Redis): **SQL Databases (PostgreSQL, MySQL):** ```yaml - id: get_orders action: database_query config: connection: \"{{secrets.DB_CONNECTION_STRING}}\" query:",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_4",
              "content": "CloudFlow uses role-based access control (RBAC) with the following hierarchy: - `viewer` - Read-only access - `developer` - Create and modify workflows (non-production) - `operator` - Execute workflows, view logs - `admin` - Full access to workspace - `platform-admin` - Cross-workspace administration **Verify resource permissions:** ```bash # Check effective permissions cloudflow rbac check \\ --user john.doe@company.com \\ --resource workflow:prod-pipeline \\ --action execute # List all policies affecting user cloudflow rbac policies --user john.doe@company.com --verbose ``` --- ## Performance Problems ### Slow Query Performance #### Symptoms - API response times > 5000ms - Database query latency warnings in logs - CloudFlow UI becomes unresponsive - Timeout errors: `Request exceeded maximum duration of 30000ms` #### Diagnosis Steps **1.",
              "score": null
            }
          ],
          "latency_ms": 5.585339000390377,
          "query_id": "realistic_016",
          "dimension": "contextual",
          "query": "I'm integrating with CloudFlow API, what authentication options do I have?"
        },
        {
          "key_facts": [
            "OAuth 2.0",
            "API keys",
            "JWT tokens"
          ],
          "found_facts": [
            "JWT tokens"
          ],
          "missed_facts": [
            "OAuth 2.0",
            "API keys"
          ],
          "coverage": 0.3333333333333333,
          "expected_docs": [
            "api_reference"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_2",
              "content": "**Verification:** ```bash # Decode JWT to check expiration echo $CF_ACCESS_TOKEN | cut -d'.' -f2 | base64 -d | jq '.exp' # Compare with current time date +%s ``` **Resolution:** ```bash # Refresh the access token curl -X POST https://api.cloudflow.io/auth/refresh \\ -H \"Content-Type: application/json\" \\ -d '{\"refresh_token\": \"'$CF_REFRESH_TOKEN'\"}' # Update environment variable export CF_ACCESS_TOKEN=\"<new_token>\" ``` **2. Invalid JWT Signature** Error: `JWT signature verification failed` **Causes:** - Token was modified or corrupted - Using wrong signing key - Token generated with different secret **Resolution:** ```bash # Validate token structure cloudflow auth validate --token $CF_ACCESS_TOKEN # Generate new token cloudflow auth login --username <user> --password <pass> # For service accounts cloudflow auth service-account --create --name <sa-name> --scopes \"workflow:read,workflow:write\" ``` **3. Clock Skew Issues** JWT validation fails when system clocks are out of sync (tolerance: \u00b1300 seconds).",
              "score": null
            },
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_1",
              "content": "**Authorization Endpoint:** `https://auth.cloudflow.io/oauth/authorize` **Token Endpoint:** `https://auth.cloudflow.io/oauth/token` **Supported Scopes:** - `workflows:read` - Read workflow configurations - `workflows:write` - Create and modify workflows - `pipelines:read` - Read pipeline data - `pipelines:write` - Create and manage pipelines - `analytics:read` - Access analytics and metrics - `admin:full` - Full administrative access Example authorization request: ```bash curl -X POST https://auth.cloudflow.io/oauth/token \\ -H \"Content-Type: application/x-www-form-urlencoded\" \\ -d \"grant_type=authorization_code\" \\ -d \"code=AUTH_CODE_HERE\" \\ -d \"client_id=YOUR_CLIENT_ID\" \\ -d \"client_secret=YOUR_CLIENT_SECRET\" \\ -d \"redirect_uri=https://yourapp.com/callback\" ``` ### JWT Tokens For advanced use cases, CloudFlow supports JSON Web Tokens (JWT) with RS256 signing algorithm.",
              "score": null
            },
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_6",
              "content": "**Endpoint:** `POST /workflows` **Request Body:** ```json { \"name\": \"Email Campaign Automation\", \"description\": \"Sends personalized emails based on user behavior\", \"trigger\": { \"type\": \"webhook\", \"url\": \"https://your-app.com/webhook\" }, \"steps\": [ { \"type\": \"fetch_data\", \"source\": \"users_table\", \"filters\": {\"active\": true} }, { \"type\": \"transform\", \"script\": \"data.map(user => ({ ...user, segment: calculateSegment(user) }))\" }, { \"type\": \"send_email\", \"template_id\": \"tpl_welcome_v2\" } ] } ``` **Example Request:** ```bash curl -X POST https://api.cloudflow.io/v2/workflows \\ -H \"X-API-Key: YOUR_API_KEY\" \\ -H \"Content-Type: application/json\" \\ -d '{ \"name\": \"Email Campaign Automation\", \"trigger\": {\"type\": \"webhook\"}, \"steps\": [{\"type\": \"fetch_data\", \"source\": \"users_table\"}] }' ``` **Example Response (201 Created):** ```json { \"id\": \"wf_9k3m7n2q\", \"name\": \"Email Campaign Automation\", \"status\": \"active\", \"created_at\": \"2026-01-24T10:15:00Z\", \"webhook_url\": \"https://api.cloudflow.io/v2/webhooks/wf_9k3m7n2q/trigger\" } ``` #### Get Workflow Details Retrieve detailed information about a specific workflow.",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_1",
              "content": "### Quick Diagnostic Checklist Before diving into specific issues, perform these initial checks: - Verify service health: `cloudflow status --all` - Check API connectivity: `curl -I https://api.cloudflow.io/health` - Review recent deployments: `kubectl get deployments -n cloudflow --sort-by=.metadata.creationTimestamp` - Inspect platform metrics: `cloudflow metrics --last 1h` --- ## Authentication & Authorization Issues ### 401 Unauthorized Errors #### Symptoms - API requests return `401 Unauthorized` - Error message: `Authentication credentials were not provided or are invalid` - Frontend displays \"Session expired\" message #### Common Causes **1. Token Expiration** CloudFlow access tokens expire after 3600 seconds (1 hour) by default. Refresh tokens are valid for 30 days.",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_3",
              "content": "**Diagnosis:** ```bash # Check system time timedatectl status # Compare with NTP server ntpdate -q pool.ntp.org # Check JWT issued time vs current time jwt_iat=$(echo $CF_ACCESS_TOKEN | cut -d'.' -f2 | base64 -d | jq '.iat') current_time=$(date +%s) skew=$((current_time - jwt_iat)) echo \"Clock skew: $skew seconds\" ``` **Resolution:** ```bash # Sync with NTP server sudo ntpdate -s pool.ntp.org # Enable automatic time sync sudo timedatectl set-ntp true # Restart CloudFlow client cloudflow restart ``` **4. Insufficient Permissions** Error: `User does not have required permissions for this operation` **Check user roles:** ```bash cloudflow auth whoami --verbose # Expected output: # User: john.doe@company.com # Roles: developer, workflow-admin # Scopes: workflow:*, data:read, metrics:read ``` **Request permission elevation:** ```bash # Submit access request cloudflow auth request-access \\ --resource \"workflow:production:deploy\" \\ --justification \"Deploy critical hotfix for TICKET-1234\" # Check pending approvals cloudflow auth list-requests --status pending ``` ### 403 Forbidden Errors #### RBAC Policy Violations",
              "score": null
            }
          ],
          "latency_ms": 5.3546689996437635,
          "query_id": "realistic_016",
          "dimension": "negation",
          "query": "Why isn't basic auth working?"
        },
        {
          "key_facts": [
            "PgBouncer",
            "connection pooling",
            "max_db_connections = 100",
            "default_pool_size = 25",
            "pool_mode = transaction"
          ],
          "found_facts": [
            "PgBouncer",
            "connection pooling",
            "max_db_connections = 100",
            "default_pool_size = 25",
            "pool_mode = transaction"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "deployment_guide",
            "architecture_overview"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_8",
              "content": "rand -base64 32)\" \\ --from-literal=password=\"$(openssl rand -base64 32)\" # Install PostgreSQL helm install postgresql bitnami/postgresql \\ --namespace cloudflow-prod \\ --values postgres-values.yaml \\ --wait ``` ### PgBouncer Configuration Deploy PgBouncer for connection pooling to handle up to 100 connections efficiently: ```yaml # pgbouncer.yaml apiVersion: v1 kind: ConfigMap metadata: name: pgbouncer-config namespace: cloudflow-prod data: pgbouncer.ini: | [databases] cloudflow = host=postgresql.cloudflow-prod.svc.cluster.local port=5432 dbname=cloudflow [pgbouncer] listen_addr = 0.0.0.0 listen_port = 5432 auth_type = md5 auth_file = /etc/pgbouncer/userlist.txt pool_mode = transaction max_client_conn = 1000 default_pool_size = 25 reserve_pool_size = 5 reserve_pool_timeout = 3 max_db_connections = 100 max_user_connections = 100 server_lifetime = 3600 server_idle_timeout = 600 log_connections = 1 log_disconnections = 1 log_pooler_errors = 1 --- apiVersion: apps/v1 kind: Deployment metadata: name: pgbouncer namespace: cloudflow-prod spec: replicas: 2 selector: matchLabels: app: pgbouncer template: metadata: labels: app: pgbouncer spec: containers: - name: pgbouncer image: pgbouncer/pgbouncer:1.21.0 ports: - containerPort: 5432 resources: requests: cpu: 500m memory: 512Mi limits: cpu:",
              "score": null
            },
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_9",
              "content": "1000m memory: 1Gi volumeMounts: - name: config mountPath: /etc/pgbouncer volumes: - name: config configMap: name: pgbouncer-config --- apiVersion: v1 kind: Service metadata: name: pgbouncer namespace: cloudflow-prod spec: selector: app: pgbouncer ports: - port: 5432 targetPort: 5432 type: ClusterIP ``` Apply the PgBouncer configuration: ```bash kubectl apply -f pgbouncer.yaml ``` ### Database Migrations Run database migrations before deploying new versions: ```bash # Create migration job kubectl create job cloudflow-migrate-$(date +%s) \\ --namespace cloudflow-prod \\ --image=123456789012.dkr.ecr.us-east-1.amazonaws.com/cloudflow:2.4.0 \\ -- npm run migrate # Monitor migration progress kubectl logs -f job/cloudflow-migrate-<timestamp> -n cloudflow-prod ``` --- ## Monitoring and Observability ### Prometheus Setup Install Prometheus using the kube-prometheus-stack: ```bash helm install prometheus prometheus-community/kube-prometheus-stack \\ --namespace monitoring \\ --create-namespace \\ --set prometheus.prometheusSpec.serviceMonitorSelectorNilUsesHelmValues=false \\ --set prometheus.prometheusSpec.retention=30d \\ --set prometheus.prometheusSpec.storageSpec.volumeClaimTemplate.spec.resources.requests.storage=100Gi ``` CloudFlow exposes Prometheus metrics on port 9090 at the `/metrics` endpoint.",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_10",
              "content": "application_name, state, COUNT(*) as conn_count, AVG(EXTRACT(EPOCH FROM (NOW() - state_change))) as avg_duration_sec FROM pg_stat_activity WHERE datname = 'cloudflow_production' GROUP BY application_name, state ORDER BY conn_count DESC; ``` #### Resolution **Immediate mitigation:** ```bash # Temporarily increase connection limit (requires database restart) cloudflow db config set max_connections 150 # Kill idle connections older than 5 minutes cloudflow db connections kill --idle-timeout 300 # Restart connection pool without downtime kubectl rollout restart deployment/cloudflow-api -n cloudflow kubectl rollout status deployment/cloudflow-api -n cloudflow ``` **Long-term solutions:** 1. **Implement connection pooling optimization:** ```bash # Use PgBouncer for connection pooling kubectl apply -f cloudflow-pgbouncer.yaml # Configure CloudFlow to use PgBouncer cloudflow config set db.host pgbouncer.cloudflow.svc.cluster.local cloudflow config set db.port 6432 cloudflow config set db.pool.mode transaction ``` 2.",
              "score": null
            },
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_18",
              "content": "#### Database Connection Pool Exhausted **Symptoms**: \"Too many connections\" errors **Diagnosis**: ```bash # Check PgBouncer stats kubectl exec -it pgbouncer-xxx -n cloudflow-prod -- \\ psql -p 5432 -U pgbouncer pgbouncer -c \"SHOW POOLS;\" ``` **Resolution**: - Increase `max_db_connections` in PgBouncer - Optimize application connection usage - Add more PostgreSQL replicas ### Support Contacts - **On-call Engineer**: pagerduty.com/cloudflow-oncall - **Slack Channel**: #cloudflow-operations - **Documentation**: https://docs.cloudflow.io - **Status Page**: https://status.cloudflow.io --- ## Appendix ### Useful Commands ```bash # View all resources in namespace kubectl get all -n cloudflow-prod # Check resource usage kubectl top pods -n cloudflow-prod kubectl top nodes # View logs kubectl logs -f deployment/cloudflow-api -n cloudflow-prod # Execute command in pod kubectl exec -it <pod-name> -n cloudflow-prod -- /bin/sh # Port forward for local testing kubectl port-forward svc/cloudflow-api 8080:80 -n cloudflow-prod # Rollback deployment helm rollback cloudflow -n cloudflow-prod # Update deployment helm upgrade cloudflow cloudflow/cloudflow \\ --namespace cloudflow-prod",
              "score": null
            },
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_15",
              "content": "Pooling Ensure optimal database connection pooling settings in PgBouncer: - **Pool Mode**: Transaction (optimal for microservices) - **Default Pool Size**: 25 connections per user - **Max DB Connections**: 100 (matches PostgreSQL `max_connections`) #### Redis Caching Implement Redis caching for frequently accessed data: ```javascript // Example caching strategy const CACHE_TTL = { workflows: 300, // 5 minutes userSessions: 3600, // 1 hour apiResults: 60 // 1 minute }; ``` #### Load Testing Perform regular load testing to validate scaling configuration: ```bash # Install k6 for load testing kubectl apply -f - <<EOF apiVersion: v1 kind: ConfigMap metadata: name: loadtest-script namespace: cloudflow-prod data: script.js: | import http from 'k6/http'; import { check, sleep } from 'k6'; export let options = { stages: [ { duration: '2m', target: 100 }, { duration: '5m', target: 100 }, { duration: '2m', target: 200 }, { duration: '5m', target: 200 }, { duration: '2m', target: 0",
              "score": null
            }
          ],
          "latency_ms": 5.435529999886057,
          "query_id": "realistic_017",
          "dimension": "original",
          "query": "What is PgBouncer and why is it used in CloudFlow?"
        },
        {
          "key_facts": [
            "PgBouncer",
            "connection pooling",
            "max_db_connections = 100",
            "default_pool_size = 25",
            "pool_mode = transaction"
          ],
          "found_facts": [
            "PgBouncer",
            "connection pooling"
          ],
          "missed_facts": [
            "max_db_connections = 100",
            "default_pool_size = 25",
            "pool_mode = transaction"
          ],
          "coverage": 0.4,
          "expected_docs": [
            "deployment_guide",
            "architecture_overview"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_15",
              "content": "Pooling Ensure optimal database connection pooling settings in PgBouncer: - **Pool Mode**: Transaction (optimal for microservices) - **Default Pool Size**: 25 connections per user - **Max DB Connections**: 100 (matches PostgreSQL `max_connections`) #### Redis Caching Implement Redis caching for frequently accessed data: ```javascript // Example caching strategy const CACHE_TTL = { workflows: 300, // 5 minutes userSessions: 3600, // 1 hour apiResults: 60 // 1 minute }; ``` #### Load Testing Perform regular load testing to validate scaling configuration: ```bash # Install k6 for load testing kubectl apply -f - <<EOF apiVersion: v1 kind: ConfigMap metadata: name: loadtest-script namespace: cloudflow-prod data: script.js: | import http from 'k6/http'; import { check, sleep } from 'k6'; export let options = { stages: [ { duration: '2m', target: 100 }, { duration: '5m', target: 100 }, { duration: '2m', target: 200 }, { duration: '5m', target: 200 }, { duration: '2m', target: 0",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_10",
              "content": "application_name, state, COUNT(*) as conn_count, AVG(EXTRACT(EPOCH FROM (NOW() - state_change))) as avg_duration_sec FROM pg_stat_activity WHERE datname = 'cloudflow_production' GROUP BY application_name, state ORDER BY conn_count DESC; ``` #### Resolution **Immediate mitigation:** ```bash # Temporarily increase connection limit (requires database restart) cloudflow db config set max_connections 150 # Kill idle connections older than 5 minutes cloudflow db connections kill --idle-timeout 300 # Restart connection pool without downtime kubectl rollout restart deployment/cloudflow-api -n cloudflow kubectl rollout status deployment/cloudflow-api -n cloudflow ``` **Long-term solutions:** 1. **Implement connection pooling optimization:** ```bash # Use PgBouncer for connection pooling kubectl apply -f cloudflow-pgbouncer.yaml # Configure CloudFlow to use PgBouncer cloudflow config set db.host pgbouncer.cloudflow.svc.cluster.local cloudflow config set db.port 6432 cloudflow config set db.pool.mode transaction ``` 2.",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_9",
              "content": "```bash # Configure event buffer limits cloudflow config set events.buffer.max_size 10000 cloudflow config set events.buffer.overflow_strategy drop_oldest # Enable event streaming to external sink cloudflow config set events.sink.type kafka cloudflow config set events.sink.kafka.brokers \"kafka-1:9092,kafka-2:9092\" cloudflow config set events.sink.kafka.topic cloudflow-events ``` --- ## Database Connection Issues ### Connection Pool Exhaustion #### Symptoms - Error: `could not obtain connection from pool within 5000ms` - Error: `connection pool exhausted (100/100 connections in use)` - API requests fail with `503 Service Unavailable` - Database CPU usage normal, but connection count at maximum #### Investigation ```bash # Check current connection pool status cloudflow db pool status --detailed # Output example: # Pool Statistics: # Total Connections: 100/100 (100%) # Active: 87 # Idle: 13 # Waiting Requests: 45 # Average Wait Time: 3420ms # Max Wait Time: 8234ms # Identify long-running queries cloudflow db queries --status running --min-duration 30000 # Check connection distribution by client SELECT",
              "score": null
            },
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_18",
              "content": "#### Database Connection Pool Exhausted **Symptoms**: \"Too many connections\" errors **Diagnosis**: ```bash # Check PgBouncer stats kubectl exec -it pgbouncer-xxx -n cloudflow-prod -- \\ psql -p 5432 -U pgbouncer pgbouncer -c \"SHOW POOLS;\" ``` **Resolution**: - Increase `max_db_connections` in PgBouncer - Optimize application connection usage - Add more PostgreSQL replicas ### Support Contacts - **On-call Engineer**: pagerduty.com/cloudflow-oncall - **Slack Channel**: #cloudflow-operations - **Documentation**: https://docs.cloudflow.io - **Status Page**: https://status.cloudflow.io --- ## Appendix ### Useful Commands ```bash # View all resources in namespace kubectl get all -n cloudflow-prod # Check resource usage kubectl top pods -n cloudflow-prod kubectl top nodes # View logs kubectl logs -f deployment/cloudflow-api -n cloudflow-prod # Execute command in pod kubectl exec -it <pod-name> -n cloudflow-prod -- /bin/sh # Port forward for local testing kubectl port-forward svc/cloudflow-api 8080:80 -n cloudflow-prod # Rollback deployment helm rollback cloudflow -n cloudflow-prod # Update deployment helm upgrade cloudflow cloudflow/cloudflow \\ --namespace cloudflow-prod",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_22",
              "content": "\"connection.*pool|could not connect|database.*timeout\" # Find slow queries kubectl logs -n cloudflow deployment/cloudflow-api | \\ grep \"slow query\" | \\ awk '{print $NF}' | \\ sort -n | \\ tail -n 20 # Find deadlock errors kubectl logs -n cloudflow deployment/cloudflow-api | \\ grep -i \"deadlock detected\" ``` #### Workflow Execution Errors ```bash # Find workflow timeout errors kubectl logs -n cloudflow deployment/cloudflow-workflow-engine | \\ grep -E \"timeout|exceeded.*3600\" # Find workflow retry attempts kubectl logs -n cloudflow deployment/cloudflow-workflow-engine | \\ grep -E \"retry attempt [0-9]|retrying in\" # Find workflow failures by ID kubectl logs -n cloudflow deployment/cloudflow-workflow-engine | \\ grep \"exec_7h3j6k9m2n\" ``` #### Rate Limiting ```bash # Find rate limit events kubectl logs -n cloudflow deployment/cloudflow-api | \\ grep -E \"429|rate.*limit|throttle\" # Count rate limit errors by hour kubectl logs -n cloudflow deployment/cloudflow-api --since=24h | \\ grep \"rate_limit_exceeded\" | \\ awk '{print $1}' | \\ cut -d'T' -f1-2 | \\ sort",
              "score": null
            }
          ],
          "latency_ms": 5.375688999265549,
          "query_id": "realistic_017",
          "dimension": "synonym",
          "query": "What's the purpose of the connection pooler?"
        },
        {
          "key_facts": [
            "PgBouncer",
            "connection pooling",
            "max_db_connections = 100",
            "default_pool_size = 25",
            "pool_mode = transaction"
          ],
          "found_facts": [
            "PgBouncer",
            "connection pooling",
            "max_db_connections = 100",
            "default_pool_size = 25",
            "pool_mode = transaction"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "deployment_guide",
            "architecture_overview"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_15",
              "content": "Pooling Ensure optimal database connection pooling settings in PgBouncer: - **Pool Mode**: Transaction (optimal for microservices) - **Default Pool Size**: 25 connections per user - **Max DB Connections**: 100 (matches PostgreSQL `max_connections`) #### Redis Caching Implement Redis caching for frequently accessed data: ```javascript // Example caching strategy const CACHE_TTL = { workflows: 300, // 5 minutes userSessions: 3600, // 1 hour apiResults: 60 // 1 minute }; ``` #### Load Testing Perform regular load testing to validate scaling configuration: ```bash # Install k6 for load testing kubectl apply -f - <<EOF apiVersion: v1 kind: ConfigMap metadata: name: loadtest-script namespace: cloudflow-prod data: script.js: | import http from 'k6/http'; import { check, sleep } from 'k6'; export let options = { stages: [ { duration: '2m', target: 100 }, { duration: '5m', target: 100 }, { duration: '2m', target: 200 }, { duration: '5m', target: 200 }, { duration: '2m', target: 0",
              "score": null
            },
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_8",
              "content": "rand -base64 32)\" \\ --from-literal=password=\"$(openssl rand -base64 32)\" # Install PostgreSQL helm install postgresql bitnami/postgresql \\ --namespace cloudflow-prod \\ --values postgres-values.yaml \\ --wait ``` ### PgBouncer Configuration Deploy PgBouncer for connection pooling to handle up to 100 connections efficiently: ```yaml # pgbouncer.yaml apiVersion: v1 kind: ConfigMap metadata: name: pgbouncer-config namespace: cloudflow-prod data: pgbouncer.ini: | [databases] cloudflow = host=postgresql.cloudflow-prod.svc.cluster.local port=5432 dbname=cloudflow [pgbouncer] listen_addr = 0.0.0.0 listen_port = 5432 auth_type = md5 auth_file = /etc/pgbouncer/userlist.txt pool_mode = transaction max_client_conn = 1000 default_pool_size = 25 reserve_pool_size = 5 reserve_pool_timeout = 3 max_db_connections = 100 max_user_connections = 100 server_lifetime = 3600 server_idle_timeout = 600 log_connections = 1 log_disconnections = 1 log_pooler_errors = 1 --- apiVersion: apps/v1 kind: Deployment metadata: name: pgbouncer namespace: cloudflow-prod spec: replicas: 2 selector: matchLabels: app: pgbouncer template: metadata: labels: app: pgbouncer spec: containers: - name: pgbouncer image: pgbouncer/pgbouncer:1.21.0 ports: - containerPort: 5432 resources: requests: cpu: 500m memory: 512Mi limits: cpu:",
              "score": null
            },
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_18",
              "content": "#### Database Connection Pool Exhausted **Symptoms**: \"Too many connections\" errors **Diagnosis**: ```bash # Check PgBouncer stats kubectl exec -it pgbouncer-xxx -n cloudflow-prod -- \\ psql -p 5432 -U pgbouncer pgbouncer -c \"SHOW POOLS;\" ``` **Resolution**: - Increase `max_db_connections` in PgBouncer - Optimize application connection usage - Add more PostgreSQL replicas ### Support Contacts - **On-call Engineer**: pagerduty.com/cloudflow-oncall - **Slack Channel**: #cloudflow-operations - **Documentation**: https://docs.cloudflow.io - **Status Page**: https://status.cloudflow.io --- ## Appendix ### Useful Commands ```bash # View all resources in namespace kubectl get all -n cloudflow-prod # Check resource usage kubectl top pods -n cloudflow-prod kubectl top nodes # View logs kubectl logs -f deployment/cloudflow-api -n cloudflow-prod # Execute command in pod kubectl exec -it <pod-name> -n cloudflow-prod -- /bin/sh # Port forward for local testing kubectl port-forward svc/cloudflow-api 8080:80 -n cloudflow-prod # Rollback deployment helm rollback cloudflow -n cloudflow-prod # Update deployment helm upgrade cloudflow cloudflow/cloudflow \\ --namespace cloudflow-prod",
              "score": null
            },
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_9",
              "content": "1000m memory: 1Gi volumeMounts: - name: config mountPath: /etc/pgbouncer volumes: - name: config configMap: name: pgbouncer-config --- apiVersion: v1 kind: Service metadata: name: pgbouncer namespace: cloudflow-prod spec: selector: app: pgbouncer ports: - port: 5432 targetPort: 5432 type: ClusterIP ``` Apply the PgBouncer configuration: ```bash kubectl apply -f pgbouncer.yaml ``` ### Database Migrations Run database migrations before deploying new versions: ```bash # Create migration job kubectl create job cloudflow-migrate-$(date +%s) \\ --namespace cloudflow-prod \\ --image=123456789012.dkr.ecr.us-east-1.amazonaws.com/cloudflow:2.4.0 \\ -- npm run migrate # Monitor migration progress kubectl logs -f job/cloudflow-migrate-<timestamp> -n cloudflow-prod ``` --- ## Monitoring and Observability ### Prometheus Setup Install Prometheus using the kube-prometheus-stack: ```bash helm install prometheus prometheus-community/kube-prometheus-stack \\ --namespace monitoring \\ --create-namespace \\ --set prometheus.prometheusSpec.serviceMonitorSelectorNilUsesHelmValues=false \\ --set prometheus.prometheusSpec.retention=30d \\ --set prometheus.prometheusSpec.storageSpec.volumeClaimTemplate.spec.resources.requests.storage=100Gi ``` CloudFlow exposes Prometheus metrics on port 9090 at the `/metrics` endpoint.",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_10",
              "content": "application_name, state, COUNT(*) as conn_count, AVG(EXTRACT(EPOCH FROM (NOW() - state_change))) as avg_duration_sec FROM pg_stat_activity WHERE datname = 'cloudflow_production' GROUP BY application_name, state ORDER BY conn_count DESC; ``` #### Resolution **Immediate mitigation:** ```bash # Temporarily increase connection limit (requires database restart) cloudflow db config set max_connections 150 # Kill idle connections older than 5 minutes cloudflow db connections kill --idle-timeout 300 # Restart connection pool without downtime kubectl rollout restart deployment/cloudflow-api -n cloudflow kubectl rollout status deployment/cloudflow-api -n cloudflow ``` **Long-term solutions:** 1. **Implement connection pooling optimization:** ```bash # Use PgBouncer for connection pooling kubectl apply -f cloudflow-pgbouncer.yaml # Configure CloudFlow to use PgBouncer cloudflow config set db.host pgbouncer.cloudflow.svc.cluster.local cloudflow config set db.port 6432 cloudflow config set db.pool.mode transaction ``` 2.",
              "score": null
            }
          ],
          "latency_ms": 5.89702999968722,
          "query_id": "realistic_017",
          "dimension": "problem",
          "query": "Should I connect directly to PostgreSQL or through PgBouncer?"
        },
        {
          "key_facts": [
            "PgBouncer",
            "connection pooling",
            "max_db_connections = 100",
            "default_pool_size = 25",
            "pool_mode = transaction"
          ],
          "found_facts": [
            "PgBouncer",
            "connection pooling",
            "max_db_connections = 100",
            "default_pool_size = 25",
            "pool_mode = transaction"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "deployment_guide",
            "architecture_overview"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_8",
              "content": "rand -base64 32)\" \\ --from-literal=password=\"$(openssl rand -base64 32)\" # Install PostgreSQL helm install postgresql bitnami/postgresql \\ --namespace cloudflow-prod \\ --values postgres-values.yaml \\ --wait ``` ### PgBouncer Configuration Deploy PgBouncer for connection pooling to handle up to 100 connections efficiently: ```yaml # pgbouncer.yaml apiVersion: v1 kind: ConfigMap metadata: name: pgbouncer-config namespace: cloudflow-prod data: pgbouncer.ini: | [databases] cloudflow = host=postgresql.cloudflow-prod.svc.cluster.local port=5432 dbname=cloudflow [pgbouncer] listen_addr = 0.0.0.0 listen_port = 5432 auth_type = md5 auth_file = /etc/pgbouncer/userlist.txt pool_mode = transaction max_client_conn = 1000 default_pool_size = 25 reserve_pool_size = 5 reserve_pool_timeout = 3 max_db_connections = 100 max_user_connections = 100 server_lifetime = 3600 server_idle_timeout = 600 log_connections = 1 log_disconnections = 1 log_pooler_errors = 1 --- apiVersion: apps/v1 kind: Deployment metadata: name: pgbouncer namespace: cloudflow-prod spec: replicas: 2 selector: matchLabels: app: pgbouncer template: metadata: labels: app: pgbouncer spec: containers: - name: pgbouncer image: pgbouncer/pgbouncer:1.21.0 ports: - containerPort: 5432 resources: requests: cpu: 500m memory: 512Mi limits: cpu:",
              "score": null
            },
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_15",
              "content": "Pooling Ensure optimal database connection pooling settings in PgBouncer: - **Pool Mode**: Transaction (optimal for microservices) - **Default Pool Size**: 25 connections per user - **Max DB Connections**: 100 (matches PostgreSQL `max_connections`) #### Redis Caching Implement Redis caching for frequently accessed data: ```javascript // Example caching strategy const CACHE_TTL = { workflows: 300, // 5 minutes userSessions: 3600, // 1 hour apiResults: 60 // 1 minute }; ``` #### Load Testing Perform regular load testing to validate scaling configuration: ```bash # Install k6 for load testing kubectl apply -f - <<EOF apiVersion: v1 kind: ConfigMap metadata: name: loadtest-script namespace: cloudflow-prod data: script.js: | import http from 'k6/http'; import { check, sleep } from 'k6'; export let options = { stages: [ { duration: '2m', target: 100 }, { duration: '5m', target: 100 }, { duration: '2m', target: 200 }, { duration: '5m', target: 200 }, { duration: '2m', target: 0",
              "score": null
            },
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_9",
              "content": "1000m memory: 1Gi volumeMounts: - name: config mountPath: /etc/pgbouncer volumes: - name: config configMap: name: pgbouncer-config --- apiVersion: v1 kind: Service metadata: name: pgbouncer namespace: cloudflow-prod spec: selector: app: pgbouncer ports: - port: 5432 targetPort: 5432 type: ClusterIP ``` Apply the PgBouncer configuration: ```bash kubectl apply -f pgbouncer.yaml ``` ### Database Migrations Run database migrations before deploying new versions: ```bash # Create migration job kubectl create job cloudflow-migrate-$(date +%s) \\ --namespace cloudflow-prod \\ --image=123456789012.dkr.ecr.us-east-1.amazonaws.com/cloudflow:2.4.0 \\ -- npm run migrate # Monitor migration progress kubectl logs -f job/cloudflow-migrate-<timestamp> -n cloudflow-prod ``` --- ## Monitoring and Observability ### Prometheus Setup Install Prometheus using the kube-prometheus-stack: ```bash helm install prometheus prometheus-community/kube-prometheus-stack \\ --namespace monitoring \\ --create-namespace \\ --set prometheus.prometheusSpec.serviceMonitorSelectorNilUsesHelmValues=false \\ --set prometheus.prometheusSpec.retention=30d \\ --set prometheus.prometheusSpec.storageSpec.volumeClaimTemplate.spec.resources.requests.storage=100Gi ``` CloudFlow exposes Prometheus metrics on port 9090 at the `/metrics` endpoint.",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_10",
              "content": "application_name, state, COUNT(*) as conn_count, AVG(EXTRACT(EPOCH FROM (NOW() - state_change))) as avg_duration_sec FROM pg_stat_activity WHERE datname = 'cloudflow_production' GROUP BY application_name, state ORDER BY conn_count DESC; ``` #### Resolution **Immediate mitigation:** ```bash # Temporarily increase connection limit (requires database restart) cloudflow db config set max_connections 150 # Kill idle connections older than 5 minutes cloudflow db connections kill --idle-timeout 300 # Restart connection pool without downtime kubectl rollout restart deployment/cloudflow-api -n cloudflow kubectl rollout status deployment/cloudflow-api -n cloudflow ``` **Long-term solutions:** 1. **Implement connection pooling optimization:** ```bash # Use PgBouncer for connection pooling kubectl apply -f cloudflow-pgbouncer.yaml # Configure CloudFlow to use PgBouncer cloudflow config set db.host pgbouncer.cloudflow.svc.cluster.local cloudflow config set db.port 6432 cloudflow config set db.pool.mode transaction ``` 2.",
              "score": null
            },
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_18",
              "content": "#### Database Connection Pool Exhausted **Symptoms**: \"Too many connections\" errors **Diagnosis**: ```bash # Check PgBouncer stats kubectl exec -it pgbouncer-xxx -n cloudflow-prod -- \\ psql -p 5432 -U pgbouncer pgbouncer -c \"SHOW POOLS;\" ``` **Resolution**: - Increase `max_db_connections` in PgBouncer - Optimize application connection usage - Add more PostgreSQL replicas ### Support Contacts - **On-call Engineer**: pagerduty.com/cloudflow-oncall - **Slack Channel**: #cloudflow-operations - **Documentation**: https://docs.cloudflow.io - **Status Page**: https://status.cloudflow.io --- ## Appendix ### Useful Commands ```bash # View all resources in namespace kubectl get all -n cloudflow-prod # Check resource usage kubectl top pods -n cloudflow-prod kubectl top nodes # View logs kubectl logs -f deployment/cloudflow-api -n cloudflow-prod # Execute command in pod kubectl exec -it <pod-name> -n cloudflow-prod -- /bin/sh # Port forward for local testing kubectl port-forward svc/cloudflow-api 8080:80 -n cloudflow-prod # Rollback deployment helm rollback cloudflow -n cloudflow-prod # Update deployment helm upgrade cloudflow cloudflow/cloudflow \\ --namespace cloudflow-prod",
              "score": null
            }
          ],
          "latency_ms": 5.44579899906239,
          "query_id": "realistic_017",
          "dimension": "casual",
          "query": "pgbouncer purpose"
        },
        {
          "key_facts": [
            "PgBouncer",
            "connection pooling",
            "max_db_connections = 100",
            "default_pool_size = 25",
            "pool_mode = transaction"
          ],
          "found_facts": [
            "PgBouncer",
            "connection pooling",
            "max_db_connections = 100",
            "default_pool_size = 25",
            "pool_mode = transaction"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "deployment_guide",
            "architecture_overview"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_15",
              "content": "Pooling Ensure optimal database connection pooling settings in PgBouncer: - **Pool Mode**: Transaction (optimal for microservices) - **Default Pool Size**: 25 connections per user - **Max DB Connections**: 100 (matches PostgreSQL `max_connections`) #### Redis Caching Implement Redis caching for frequently accessed data: ```javascript // Example caching strategy const CACHE_TTL = { workflows: 300, // 5 minutes userSessions: 3600, // 1 hour apiResults: 60 // 1 minute }; ``` #### Load Testing Perform regular load testing to validate scaling configuration: ```bash # Install k6 for load testing kubectl apply -f - <<EOF apiVersion: v1 kind: ConfigMap metadata: name: loadtest-script namespace: cloudflow-prod data: script.js: | import http from 'k6/http'; import { check, sleep } from 'k6'; export let options = { stages: [ { duration: '2m', target: 100 }, { duration: '5m', target: 100 }, { duration: '2m', target: 200 }, { duration: '5m', target: 200 }, { duration: '2m', target: 0",
              "score": null
            },
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_8",
              "content": "rand -base64 32)\" \\ --from-literal=password=\"$(openssl rand -base64 32)\" # Install PostgreSQL helm install postgresql bitnami/postgresql \\ --namespace cloudflow-prod \\ --values postgres-values.yaml \\ --wait ``` ### PgBouncer Configuration Deploy PgBouncer for connection pooling to handle up to 100 connections efficiently: ```yaml # pgbouncer.yaml apiVersion: v1 kind: ConfigMap metadata: name: pgbouncer-config namespace: cloudflow-prod data: pgbouncer.ini: | [databases] cloudflow = host=postgresql.cloudflow-prod.svc.cluster.local port=5432 dbname=cloudflow [pgbouncer] listen_addr = 0.0.0.0 listen_port = 5432 auth_type = md5 auth_file = /etc/pgbouncer/userlist.txt pool_mode = transaction max_client_conn = 1000 default_pool_size = 25 reserve_pool_size = 5 reserve_pool_timeout = 3 max_db_connections = 100 max_user_connections = 100 server_lifetime = 3600 server_idle_timeout = 600 log_connections = 1 log_disconnections = 1 log_pooler_errors = 1 --- apiVersion: apps/v1 kind: Deployment metadata: name: pgbouncer namespace: cloudflow-prod spec: replicas: 2 selector: matchLabels: app: pgbouncer template: metadata: labels: app: pgbouncer spec: containers: - name: pgbouncer image: pgbouncer/pgbouncer:1.21.0 ports: - containerPort: 5432 resources: requests: cpu: 500m memory: 512Mi limits: cpu:",
              "score": null
            },
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_18",
              "content": "#### Database Connection Pool Exhausted **Symptoms**: \"Too many connections\" errors **Diagnosis**: ```bash # Check PgBouncer stats kubectl exec -it pgbouncer-xxx -n cloudflow-prod -- \\ psql -p 5432 -U pgbouncer pgbouncer -c \"SHOW POOLS;\" ``` **Resolution**: - Increase `max_db_connections` in PgBouncer - Optimize application connection usage - Add more PostgreSQL replicas ### Support Contacts - **On-call Engineer**: pagerduty.com/cloudflow-oncall - **Slack Channel**: #cloudflow-operations - **Documentation**: https://docs.cloudflow.io - **Status Page**: https://status.cloudflow.io --- ## Appendix ### Useful Commands ```bash # View all resources in namespace kubectl get all -n cloudflow-prod # Check resource usage kubectl top pods -n cloudflow-prod kubectl top nodes # View logs kubectl logs -f deployment/cloudflow-api -n cloudflow-prod # Execute command in pod kubectl exec -it <pod-name> -n cloudflow-prod -- /bin/sh # Port forward for local testing kubectl port-forward svc/cloudflow-api 8080:80 -n cloudflow-prod # Rollback deployment helm rollback cloudflow -n cloudflow-prod # Update deployment helm upgrade cloudflow cloudflow/cloudflow \\ --namespace cloudflow-prod",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_10",
              "content": "application_name, state, COUNT(*) as conn_count, AVG(EXTRACT(EPOCH FROM (NOW() - state_change))) as avg_duration_sec FROM pg_stat_activity WHERE datname = 'cloudflow_production' GROUP BY application_name, state ORDER BY conn_count DESC; ``` #### Resolution **Immediate mitigation:** ```bash # Temporarily increase connection limit (requires database restart) cloudflow db config set max_connections 150 # Kill idle connections older than 5 minutes cloudflow db connections kill --idle-timeout 300 # Restart connection pool without downtime kubectl rollout restart deployment/cloudflow-api -n cloudflow kubectl rollout status deployment/cloudflow-api -n cloudflow ``` **Long-term solutions:** 1. **Implement connection pooling optimization:** ```bash # Use PgBouncer for connection pooling kubectl apply -f cloudflow-pgbouncer.yaml # Configure CloudFlow to use PgBouncer cloudflow config set db.host pgbouncer.cloudflow.svc.cluster.local cloudflow config set db.port 6432 cloudflow config set db.pool.mode transaction ``` 2.",
              "score": null
            },
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_9",
              "content": "1000m memory: 1Gi volumeMounts: - name: config mountPath: /etc/pgbouncer volumes: - name: config configMap: name: pgbouncer-config --- apiVersion: v1 kind: Service metadata: name: pgbouncer namespace: cloudflow-prod spec: selector: app: pgbouncer ports: - port: 5432 targetPort: 5432 type: ClusterIP ``` Apply the PgBouncer configuration: ```bash kubectl apply -f pgbouncer.yaml ``` ### Database Migrations Run database migrations before deploying new versions: ```bash # Create migration job kubectl create job cloudflow-migrate-$(date +%s) \\ --namespace cloudflow-prod \\ --image=123456789012.dkr.ecr.us-east-1.amazonaws.com/cloudflow:2.4.0 \\ -- npm run migrate # Monitor migration progress kubectl logs -f job/cloudflow-migrate-<timestamp> -n cloudflow-prod ``` --- ## Monitoring and Observability ### Prometheus Setup Install Prometheus using the kube-prometheus-stack: ```bash helm install prometheus prometheus-community/kube-prometheus-stack \\ --namespace monitoring \\ --create-namespace \\ --set prometheus.prometheusSpec.serviceMonitorSelectorNilUsesHelmValues=false \\ --set prometheus.prometheusSpec.retention=30d \\ --set prometheus.prometheusSpec.storageSpec.volumeClaimTemplate.spec.resources.requests.storage=100Gi ``` CloudFlow exposes Prometheus metrics on port 9090 at the `/metrics` endpoint.",
              "score": null
            }
          ],
          "latency_ms": 5.432705000202986,
          "query_id": "realistic_017",
          "dimension": "contextual",
          "query": "Optimizing database connections, what role does PgBouncer play?"
        },
        {
          "key_facts": [
            "PgBouncer",
            "connection pooling",
            "max_db_connections = 100",
            "default_pool_size = 25",
            "pool_mode = transaction"
          ],
          "found_facts": [
            "PgBouncer"
          ],
          "missed_facts": [
            "connection pooling",
            "max_db_connections = 100",
            "default_pool_size = 25",
            "pool_mode = transaction"
          ],
          "coverage": 0.2,
          "expected_docs": [
            "deployment_guide",
            "architecture_overview"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_11",
              "content": "**Add read replicas:** ```bash # Route read-only queries to replicas cloudflow db replicas add --count 2 cloudflow config set db.read_replicas \"replica-1:5432,replica-2:5432\" cloudflow config set db.read_write_split true ``` ### Connection Timeout Errors #### Error Messages - `connection timeout after 30000ms` - `could not connect to database server at 10.0.2.45:5432` - `database server unreachable` #### Troubleshooting Steps ```bash # Test network connectivity telnet cloudflow-db.internal.company.com 5432 # Check DNS resolution nslookup cloudflow-db.internal.company.com # Verify database is accepting connections pg_isready -h cloudflow-db.internal.company.com -p 5432 -U cloudflow # Check firewall rules sudo iptables -L -n | grep 5432 # Test from CloudFlow pod network kubectl run -n cloudflow debug-pod --rm -i --tty \\ --image=postgres:14 -- \\ psql -h cloudflow-db.internal.company.com -U cloudflow -d cloudflow_production # Review database logs for connection rejections kubectl logs -n cloudflow statefulset/cloudflow-db --tail=100 | \\ grep -i \"connection\\|reject\\|authentication\" ``` ### Maximum Connection Limit (100) Reached This is a hard limit in CloudFlow's",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_22",
              "content": "\"connection.*pool|could not connect|database.*timeout\" # Find slow queries kubectl logs -n cloudflow deployment/cloudflow-api | \\ grep \"slow query\" | \\ awk '{print $NF}' | \\ sort -n | \\ tail -n 20 # Find deadlock errors kubectl logs -n cloudflow deployment/cloudflow-api | \\ grep -i \"deadlock detected\" ``` #### Workflow Execution Errors ```bash # Find workflow timeout errors kubectl logs -n cloudflow deployment/cloudflow-workflow-engine | \\ grep -E \"timeout|exceeded.*3600\" # Find workflow retry attempts kubectl logs -n cloudflow deployment/cloudflow-workflow-engine | \\ grep -E \"retry attempt [0-9]|retrying in\" # Find workflow failures by ID kubectl logs -n cloudflow deployment/cloudflow-workflow-engine | \\ grep \"exec_7h3j6k9m2n\" ``` #### Rate Limiting ```bash # Find rate limit events kubectl logs -n cloudflow deployment/cloudflow-api | \\ grep -E \"429|rate.*limit|throttle\" # Count rate limit errors by hour kubectl logs -n cloudflow deployment/cloudflow-api --since=24h | \\ grep \"rate_limit_exceeded\" | \\ awk '{print $1}' | \\ cut -d'T' -f1-2 | \\ sort",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_12",
              "content": "database tier. #### Permanent Solutions **Option 1: Upgrade database tier** ```bash # Check available tiers cloudflow db tiers list # Upgrade to higher tier (supports 200 connections) cloudflow db upgrade --tier standard-plus --confirm # Monitor migration progress cloudflow db migration status ``` **Option 2: Implement aggressive connection reuse** ```bash # Reduce connection lifetime cloudflow config set db.pool.max_lifetime 600 # 10 minutes # Enable prepared statement caching cloudflow config set db.prepared_statements.cache true cloudflow config set db.prepared_statements.max_size 250 # Reduce idle connection timeout cloudflow config set db.pool.idle_timeout 120 # 2 minutes ``` --- ## Workflow Execution Failures ### Timeout Errors (3600 second limit) #### Error Message ``` WorkflowExecutionError: Workflow exceeded maximum execution time of 3600 seconds Status: TIMEOUT Workflow ID: wf_9k2n4m8p1q Execution ID: exec_7h3j6k9m2n ``` #### Analysis ```bash # Get workflow execution details cloudflow workflows executions get exec_7h3j6k9m2n --verbose # Check step-by-step breakdown cloudflow workflows executions steps exec_7h3j6k9m2n # Sample output: #",
              "score": null
            },
            {
              "doc_id": "user_guide",
              "chunk_id": "user_guide_fix_19",
              "content": "Handle Errors Gracefully Always implement error handling for external API calls and database operations: ```yaml - id: fetch_data action: http_request config: url: \"https://api.example.com/data\" retry: max_attempts: 3 backoff_type: exponential on_error: - id: log_error action: database_query config: query: \"INSERT INTO error_log (workflow_id, error) VALUES ($1, $2)\" parameters: - \"{{workflow.id}}\" - \"{{error.message}}\" ``` ### 3. Use Secrets for Sensitive Data Never hardcode API keys, passwords, or tokens in workflows: **Bad:** ```yaml headers: Authorization: \"Bearer sk_live_abc123xyz789\" ``` **Good:** ```yaml headers: Authorization: \"Bearer {{secrets.API_TOKEN}}\" ``` Store secrets in **Settings** > **Secrets** with encryption at rest. ### 4.",
              "score": null
            },
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_18",
              "content": "#### Database Connection Pool Exhausted **Symptoms**: \"Too many connections\" errors **Diagnosis**: ```bash # Check PgBouncer stats kubectl exec -it pgbouncer-xxx -n cloudflow-prod -- \\ psql -p 5432 -U pgbouncer pgbouncer -c \"SHOW POOLS;\" ``` **Resolution**: - Increase `max_db_connections` in PgBouncer - Optimize application connection usage - Add more PostgreSQL replicas ### Support Contacts - **On-call Engineer**: pagerduty.com/cloudflow-oncall - **Slack Channel**: #cloudflow-operations - **Documentation**: https://docs.cloudflow.io - **Status Page**: https://status.cloudflow.io --- ## Appendix ### Useful Commands ```bash # View all resources in namespace kubectl get all -n cloudflow-prod # Check resource usage kubectl top pods -n cloudflow-prod kubectl top nodes # View logs kubectl logs -f deployment/cloudflow-api -n cloudflow-prod # Execute command in pod kubectl exec -it <pod-name> -n cloudflow-prod -- /bin/sh # Port forward for local testing kubectl port-forward svc/cloudflow-api 8080:80 -n cloudflow-prod # Rollback deployment helm rollback cloudflow -n cloudflow-prod # Update deployment helm upgrade cloudflow cloudflow/cloudflow \\ --namespace cloudflow-prod",
              "score": null
            }
          ],
          "latency_ms": 5.741711000155192,
          "query_id": "realistic_017",
          "dimension": "negation",
          "query": "Why can't I connect directly to the database?"
        },
        {
          "key_facts": [
            "max_attempts: 3",
            "backoff_type: exponential",
            "initial_interval: 1000",
            "max retries: 3"
          ],
          "found_facts": [
            "max retries: 3"
          ],
          "missed_facts": [
            "max_attempts: 3",
            "backoff_type: exponential",
            "initial_interval: 1000"
          ],
          "coverage": 0.25,
          "expected_docs": [
            "troubleshooting_guide",
            "user_guide"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "user_guide",
              "chunk_id": "user_guide_fix_16",
              "content": "fixing issues - Export failed executions for analysis - Set up alerts for DLQ threshold breaches **Accessing the DLQ:** 1. Navigate to **Workflows** > **[Your Workflow]** > **Dead Letter Queue** 2. Filter by error type, date range, or execution ID 3. Click an execution to view full details 4. Click **\"Retry\"** to reprocess with the same input data ### Error Notifications Get notified when workflows fail: ```yaml workflow: notifications: on_failure: - type: email to: \"ops-team@company.com\" - type: slack channel: \"#alerts\" message: \"Workflow {{workflow.name}} failed: {{error.message}}\" on_success_after_retry: - type: slack channel: \"#monitoring\" message: \"Workflow recovered after {{error.attempts}} attempts\" ``` ## Workflow Limits CloudFlow enforces the following limits to ensure platform stability and performance: ### Steps Per Workflow - **Maximum**: 50 steps per workflow - **Recommendation**: Keep workflows focused and modular. If you need more steps, consider splitting into multiple workflows connected via webhooks.",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_15",
              "content": "--step data_export \\ --retry-enabled false ``` #### Monitoring Retries ```bash # List failed executions with retry information cloudflow workflows executions list \\ --status FAILED \\ --show-retries \\ --last 7d # Get retry history for specific execution cloudflow workflows executions retries exec_7h3j6k9m2n # Output: # Execution: exec_7h3j6k9m2n # Attempt 1: FAILED - NetworkError: Connection refused (delay: 0ms) # Attempt 2: FAILED - NetworkError: Connection timeout (delay: 1000ms) # Attempt 3: FAILED - NetworkError: Connection timeout (delay: 2000ms) # Attempt 4: FAILED - NetworkError: Connection timeout (delay: 4000ms) # Final Status: FAILED_AFTER_RETRIES ``` ### Workflow Step Failures #### Common Error Patterns **1.",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_7",
              "content": "\"retry\": {\"max_attempts\": 3, \"backoff\": \"exponential\"} } }, { \"id\": \"create-account\", \"type\": \"database_operation\", \"depends_on\": [\"validate-customer\"] } ] } ``` **State Management**: - Workflow states: PENDING, RUNNING, COMPLETED, FAILED, CANCELLED - Step states: QUEUED, EXECUTING, SUCCESS, FAILED, SKIPPED - State transitions stored in PostgreSQL with event sourcing pattern - Checkpointing every 10 steps for long-running workflows **Performance Targets**: - Simple workflow (3-5 steps): < 2 seconds (P99) - Complex workflow (20+ steps): < 5 seconds (P99) - Throughput: 500 concurrent workflow executions per pod --- ### Scheduler Service **Purpose**: Time-based workflow triggering system supporting cron-like schedules and one-time delayed executions.",
              "score": null
            },
            {
              "doc_id": "user_guide",
              "chunk_id": "user_guide_fix_15",
              "content": "Fallback Actions Execute alternative actions when the primary action fails: ```yaml - id: primary_payment action: http_request config: url: \"https://primary-payment-gateway.com/charge\" method: POST body: amount: \"{{amount}}\" on_error: - id: fallback_payment action: http_request config: url: \"https://backup-payment-gateway.com/charge\" method: POST body: amount: \"{{amount}}\" - id: notify_admin action: email config: to: \"admin@company.com\" subject: \"Payment Gateway Failure\" body: \"Primary gateway failed, switched to backup\" ``` **Error Object:** Access error details in fallback actions: ``` {{error.message}} # Error message {{error.code}} # Error code {{error.step_id}} # Failed step ID {{error.timestamp}} # When the error occurred {{error.attempts}} # Number of retry attempts ``` ### Dead Letter Queue When all retries and fallbacks fail, CloudFlow can route failed executions to a Dead Letter Queue (DLQ) for manual review: **Enable DLQ:** ```yaml workflow: error_handling: dead_letter_queue: enabled: true retain_days: 30 ``` **DLQ Features:** - View failed executions in the dashboard - Inspect complete execution context and error details - Retry individual executions after",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_14",
              "content": "Split workflow into smaller workflows:** ```bash # Create sub-workflows cloudflow workflows create data-pipeline-part1 \\ --steps \"data_ingestion,data_validation\" \\ --timeout 1800 cloudflow workflows create data-pipeline-part2 \\ --steps \"data_transformation,data_export\" \\ --timeout 3600 \\ --trigger workflow_completed \\ --trigger-workflow data-pipeline-part1 ``` ### Retry Logic and Exponential Backoff CloudFlow implements automatic retry with exponential backoff for transient failures: - Max retries: 3 - Initial delay: 1 second - Backoff multiplier: 2 - Max delay: 60 seconds #### Retry Sequence ``` Attempt 1: Immediate Attempt 2: Wait 1s (2^0 * 1s) Attempt 3: Wait 2s (2^1 * 1s) Attempt 4: Wait 4s (2^2 * 1s) ``` #### Configuration ```bash # View current retry settings cloudflow workflows get wf_9k2n4m8p1q --format json | jq '.retry_policy' # Customize retry behavior cloudflow workflows update wf_9k2n4m8p1q \\ --retry-max-attempts 5 \\ --retry-initial-delay 2000 \\ --retry-backoff-multiplier 2 \\ --retry-max-delay 120000 \\ --retry-on-errors \"NETWORK_ERROR,TIMEOUT,RATE_LIMIT\" # Disable retry for specific step cloudflow workflows update wf_9k2n4m8p1q \\",
              "score": null
            }
          ],
          "latency_ms": 5.807853000078467,
          "query_id": "realistic_018",
          "dimension": "original",
          "query": "How do I implement retry logic for failed workflow steps?"
        },
        {
          "key_facts": [
            "max_attempts: 3",
            "backoff_type: exponential",
            "initial_interval: 1000",
            "max retries: 3"
          ],
          "found_facts": [
            "max_attempts: 3",
            "initial_interval: 1000"
          ],
          "missed_facts": [
            "backoff_type: exponential",
            "max retries: 3"
          ],
          "coverage": 0.5,
          "expected_docs": [
            "troubleshooting_guide",
            "user_guide"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "user_guide",
              "chunk_id": "user_guide_fix_14",
              "content": "### Retry Policies Configure automatic retries for failed actions: ```yaml - id: api_call action: http_request config: url: \"https://api.example.com/data\" retry: max_attempts: 3 backoff_type: \"exponential\" # or \"fixed\", \"linear\" initial_interval: 1000 # milliseconds max_interval: 30000 multiplier: 2.0 retry_on: - timeout - network_error - status: [500, 502, 503, 504] ``` **Backoff Strategies:** - **Fixed**: Wait the same amount of time between retries - Attempt 1: Wait 1s - Attempt 2: Wait 1s - Attempt 3: Wait 1s - **Linear**: Increase wait time by a fixed amount - Attempt 1: Wait 1s - Attempt 2: Wait 2s - Attempt 3: Wait 3s - **Exponential**: Double the wait time with each retry (recommended) - Attempt 1: Wait 1s - Attempt 2: Wait 2s - Attempt 3: Wait 4s **Retry Conditions:** Control which errors trigger retries: - `timeout`: Request timeout - `network_error`: Connection failures - `status`: Specific HTTP status codes - `error_code`: Application-specific error codes ###",
              "score": null
            },
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_12",
              "content": "Waiting {retry_after} seconds...\") time.sleep(retry_after) continue elif response.status_code >= 500: # Server error - retry with exponential backoff wait_time = 2 ** attempt print(f\"Server error. Retrying in {wait_time} seconds...\") time.sleep(wait_time) continue else: # Client error - don't retry error_data = response.json() raise Exception(f\"API Error: {error_data['error']['message']}\") except requests.exceptions.RequestException as e: print(f\"Request failed: {e}\") if attempt < max_retries - 1: time.sleep(2 ** attempt) else: raise raise Exception(\"Max retries exceeded\") ``` ## Webhooks CloudFlow can send webhook notifications when specific events occur in your workflows.",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_7",
              "content": "\"retry\": {\"max_attempts\": 3, \"backoff\": \"exponential\"} } }, { \"id\": \"create-account\", \"type\": \"database_operation\", \"depends_on\": [\"validate-customer\"] } ] } ``` **State Management**: - Workflow states: PENDING, RUNNING, COMPLETED, FAILED, CANCELLED - Step states: QUEUED, EXECUTING, SUCCESS, FAILED, SKIPPED - State transitions stored in PostgreSQL with event sourcing pattern - Checkpointing every 10 steps for long-running workflows **Performance Targets**: - Simple workflow (3-5 steps): < 2 seconds (P99) - Complex workflow (20+ steps): < 5 seconds (P99) - Throughput: 500 concurrent workflow executions per pod --- ### Scheduler Service **Purpose**: Time-based workflow triggering system supporting cron-like schedules and one-time delayed executions.",
              "score": null
            },
            {
              "doc_id": "user_guide",
              "chunk_id": "user_guide_fix_16",
              "content": "fixing issues - Export failed executions for analysis - Set up alerts for DLQ threshold breaches **Accessing the DLQ:** 1. Navigate to **Workflows** > **[Your Workflow]** > **Dead Letter Queue** 2. Filter by error type, date range, or execution ID 3. Click an execution to view full details 4. Click **\"Retry\"** to reprocess with the same input data ### Error Notifications Get notified when workflows fail: ```yaml workflow: notifications: on_failure: - type: email to: \"ops-team@company.com\" - type: slack channel: \"#alerts\" message: \"Workflow {{workflow.name}} failed: {{error.message}}\" on_success_after_retry: - type: slack channel: \"#monitoring\" message: \"Workflow recovered after {{error.attempts}} attempts\" ``` ## Workflow Limits CloudFlow enforces the following limits to ensure platform stability and performance: ### Steps Per Workflow - **Maximum**: 50 steps per workflow - **Recommendation**: Keep workflows focused and modular. If you need more steps, consider splitting into multiple workflows connected via webhooks.",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_29",
              "content": "minutes (no manual intervention) - Data loss: None (multi-AZ replication) **Scenario 2: Database Primary Failure** - Detection: Health check fails for primary database (< 30 seconds) - Action: Automatic promotion of read replica to primary - Recovery time: 30-60 seconds - Data loss: Minimal (< 1 second due to synchronous replication) **Scenario 3: Full Region Failure** - Detection: Multiple health check failures across all AZs (< 2 minutes) - Action: Manual failover to DR region (us-west-2) - Procedure: 1. Update DNS to point to DR region (TTL: 60 seconds) 2. Promote DR database replica to primary 3. Scale up DR region services to production capacity 4. Verify data consistency and integrity 5.",
              "score": null
            }
          ],
          "latency_ms": 5.551575999561464,
          "query_id": "realistic_018",
          "dimension": "synonym",
          "query": "What's the retry strategy for transient failures?"
        },
        {
          "key_facts": [
            "max_attempts: 3",
            "backoff_type: exponential",
            "initial_interval: 1000",
            "max retries: 3"
          ],
          "found_facts": [
            "max retries: 3"
          ],
          "missed_facts": [
            "max_attempts: 3",
            "backoff_type: exponential",
            "initial_interval: 1000"
          ],
          "coverage": 0.25,
          "expected_docs": [
            "troubleshooting_guide",
            "user_guide"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "user_guide",
              "chunk_id": "user_guide_fix_16",
              "content": "fixing issues - Export failed executions for analysis - Set up alerts for DLQ threshold breaches **Accessing the DLQ:** 1. Navigate to **Workflows** > **[Your Workflow]** > **Dead Letter Queue** 2. Filter by error type, date range, or execution ID 3. Click an execution to view full details 4. Click **\"Retry\"** to reprocess with the same input data ### Error Notifications Get notified when workflows fail: ```yaml workflow: notifications: on_failure: - type: email to: \"ops-team@company.com\" - type: slack channel: \"#alerts\" message: \"Workflow {{workflow.name}} failed: {{error.message}}\" on_success_after_retry: - type: slack channel: \"#monitoring\" message: \"Workflow recovered after {{error.attempts}} attempts\" ``` ## Workflow Limits CloudFlow enforces the following limits to ensure platform stability and performance: ### Steps Per Workflow - **Maximum**: 50 steps per workflow - **Recommendation**: Keep workflows focused and modular. If you need more steps, consider splitting into multiple workflows connected via webhooks.",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_15",
              "content": "--step data_export \\ --retry-enabled false ``` #### Monitoring Retries ```bash # List failed executions with retry information cloudflow workflows executions list \\ --status FAILED \\ --show-retries \\ --last 7d # Get retry history for specific execution cloudflow workflows executions retries exec_7h3j6k9m2n # Output: # Execution: exec_7h3j6k9m2n # Attempt 1: FAILED - NetworkError: Connection refused (delay: 0ms) # Attempt 2: FAILED - NetworkError: Connection timeout (delay: 1000ms) # Attempt 3: FAILED - NetworkError: Connection timeout (delay: 2000ms) # Attempt 4: FAILED - NetworkError: Connection timeout (delay: 4000ms) # Final Status: FAILED_AFTER_RETRIES ``` ### Workflow Step Failures #### Common Error Patterns **1.",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_14",
              "content": "Split workflow into smaller workflows:** ```bash # Create sub-workflows cloudflow workflows create data-pipeline-part1 \\ --steps \"data_ingestion,data_validation\" \\ --timeout 1800 cloudflow workflows create data-pipeline-part2 \\ --steps \"data_transformation,data_export\" \\ --timeout 3600 \\ --trigger workflow_completed \\ --trigger-workflow data-pipeline-part1 ``` ### Retry Logic and Exponential Backoff CloudFlow implements automatic retry with exponential backoff for transient failures: - Max retries: 3 - Initial delay: 1 second - Backoff multiplier: 2 - Max delay: 60 seconds #### Retry Sequence ``` Attempt 1: Immediate Attempt 2: Wait 1s (2^0 * 1s) Attempt 3: Wait 2s (2^1 * 1s) Attempt 4: Wait 4s (2^2 * 1s) ``` #### Configuration ```bash # View current retry settings cloudflow workflows get wf_9k2n4m8p1q --format json | jq '.retry_policy' # Customize retry behavior cloudflow workflows update wf_9k2n4m8p1q \\ --retry-max-attempts 5 \\ --retry-initial-delay 2000 \\ --retry-backoff-multiplier 2 \\ --retry-max-delay 120000 \\ --retry-on-errors \"NETWORK_ERROR,TIMEOUT,RATE_LIMIT\" # Disable retry for specific step cloudflow workflows update wf_9k2n4m8p1q \\",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_16",
              "content": "Data Validation Errors** ``` ValidationError: Field 'customer_id' is required but missing in 234 records ``` **Resolution:** ```bash # Add data quality checks cloudflow workflows update wf_9k2n4m8p1q \\ --step data_ingestion \\ --add-validator required_fields \\ --validator-config '{\"fields\": [\"customer_id\", \"timestamp\", \"amount\"]}' # Configure error handling cloudflow workflows update wf_9k2n4m8p1q \\ --step data_validation \\ --on-error continue \\ --error-threshold 5% # Fail if > 5% of records invalid ``` **2. External API Failures** ``` ExternalAPIError: API request to https://partner-api.example.com failed with status 502 ``` **Resolution:** ```bash # Add circuit breaker cloudflow workflows update wf_9k2n4m8p1q \\ --step external_api_call \\ --circuit-breaker-enabled true \\ --circuit-breaker-threshold 5 \\ --circuit-breaker-timeout 30000 # Configure fallback behavior cloudflow workflows update wf_9k2n4m8p1q \\ --step external_api_call \\ --fallback-action use_cached_data \\ --cache-ttl 3600 ``` --- ## Rate Limiting & Throttling ### 429 Too Many Requests #### Error Response ```json { \"error\": \"rate_limit_exceeded\", \"message\": \"API rate limit exceeded.",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_28",
              "content": "``` **For SEV-2 (High):** ```bash # Create incident and notify cloudflow incident create \\ --severity SEV-2 \\ --title \"High rate of workflow failures\" \\ --description \"Workflow success rate dropped to 75%\" \\ --notify-oncall # Update status page cloudflow status update \\ --status \"partial_outage\" \\ --message \"We are experiencing elevated error rates\" ``` **For SEV-3/SEV-4:** ```bash # Create support ticket cloudflow support ticket create \\ --priority MEDIUM \\ --subject \"Intermittent timeout errors\" \\ --description \"See attached logs and reproduction steps\" ``` ### Contact Information - **On-call hotline:** +1-888-CLOUDFLOW (24/7) - **Support email:** support@cloudflow.io - **Incident Slack channel:** #cloudflow-incidents - **Status page:** https://status.cloudflow.io - **Documentation:** https://docs.cloudflow.io ### Post-Incident Review After resolving incidents, complete a postmortem: ```bash # Generate postmortem template cloudflow incident postmortem incident-2024012401 \\ --template standard \\ --output postmortem-2024012401.md # Required sections: # 1. Incident summary # 2. Impact assessment # 3. Timeline of events # 4.",
              "score": null
            }
          ],
          "latency_ms": 5.488740000146208,
          "query_id": "realistic_018",
          "dimension": "problem",
          "query": "My workflow fails on temporary network errors"
        },
        {
          "key_facts": [
            "max_attempts: 3",
            "backoff_type: exponential",
            "initial_interval: 1000",
            "max retries: 3"
          ],
          "found_facts": [
            "max_attempts: 3",
            "initial_interval: 1000"
          ],
          "missed_facts": [
            "backoff_type: exponential",
            "max retries: 3"
          ],
          "coverage": 0.5,
          "expected_docs": [
            "troubleshooting_guide",
            "user_guide"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "user_guide",
              "chunk_id": "user_guide_fix_14",
              "content": "### Retry Policies Configure automatic retries for failed actions: ```yaml - id: api_call action: http_request config: url: \"https://api.example.com/data\" retry: max_attempts: 3 backoff_type: \"exponential\" # or \"fixed\", \"linear\" initial_interval: 1000 # milliseconds max_interval: 30000 multiplier: 2.0 retry_on: - timeout - network_error - status: [500, 502, 503, 504] ``` **Backoff Strategies:** - **Fixed**: Wait the same amount of time between retries - Attempt 1: Wait 1s - Attempt 2: Wait 1s - Attempt 3: Wait 1s - **Linear**: Increase wait time by a fixed amount - Attempt 1: Wait 1s - Attempt 2: Wait 2s - Attempt 3: Wait 3s - **Exponential**: Double the wait time with each retry (recommended) - Attempt 1: Wait 1s - Attempt 2: Wait 2s - Attempt 3: Wait 4s **Retry Conditions:** Control which errors trigger retries: - `timeout`: Request timeout - `network_error`: Connection failures - `status`: Specific HTTP status codes - `error_code`: Application-specific error codes ###",
              "score": null
            },
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_12",
              "content": "Waiting {retry_after} seconds...\") time.sleep(retry_after) continue elif response.status_code >= 500: # Server error - retry with exponential backoff wait_time = 2 ** attempt print(f\"Server error. Retrying in {wait_time} seconds...\") time.sleep(wait_time) continue else: # Client error - don't retry error_data = response.json() raise Exception(f\"API Error: {error_data['error']['message']}\") except requests.exceptions.RequestException as e: print(f\"Request failed: {e}\") if attempt < max_retries - 1: time.sleep(2 ** attempt) else: raise raise Exception(\"Max retries exceeded\") ``` ## Webhooks CloudFlow can send webhook notifications when specific events occur in your workflows.",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_7",
              "content": "\"retry\": {\"max_attempts\": 3, \"backoff\": \"exponential\"} } }, { \"id\": \"create-account\", \"type\": \"database_operation\", \"depends_on\": [\"validate-customer\"] } ] } ``` **State Management**: - Workflow states: PENDING, RUNNING, COMPLETED, FAILED, CANCELLED - Step states: QUEUED, EXECUTING, SUCCESS, FAILED, SKIPPED - State transitions stored in PostgreSQL with event sourcing pattern - Checkpointing every 10 steps for long-running workflows **Performance Targets**: - Simple workflow (3-5 steps): < 2 seconds (P99) - Complex workflow (20+ steps): < 5 seconds (P99) - Throughput: 500 concurrent workflow executions per pod --- ### Scheduler Service **Purpose**: Time-based workflow triggering system supporting cron-like schedules and one-time delayed executions.",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_15",
              "content": "--step data_export \\ --retry-enabled false ``` #### Monitoring Retries ```bash # List failed executions with retry information cloudflow workflows executions list \\ --status FAILED \\ --show-retries \\ --last 7d # Get retry history for specific execution cloudflow workflows executions retries exec_7h3j6k9m2n # Output: # Execution: exec_7h3j6k9m2n # Attempt 1: FAILED - NetworkError: Connection refused (delay: 0ms) # Attempt 2: FAILED - NetworkError: Connection timeout (delay: 1000ms) # Attempt 3: FAILED - NetworkError: Connection timeout (delay: 2000ms) # Attempt 4: FAILED - NetworkError: Connection timeout (delay: 4000ms) # Final Status: FAILED_AFTER_RETRIES ``` ### Workflow Step Failures #### Common Error Patterns **1.",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_18",
              "content": "Handling Rate Limits in Code **Python example with retry logic:** ```python import time import requests def cloudflow_api_call_with_retry(url, headers, max_retries=3): for attempt in range(max_retries): response = requests.get(url, headers=headers) if response.status_code == 429: retry_after = int(response.headers.get('Retry-After', 60)) print(f\"Rate limited. Waiting {retry_after} seconds...\") time.sleep(retry_after) continue remaining = int(response.headers.get('X-RateLimit-Remaining', 0)) if remaining < 10: print(f\"Warning: Only {remaining} requests remaining\") return response raise Exception(\"Max retries exceeded due to rate limiting\") ``` **Bash script with rate limit checking:** ```bash #!/bin/bash check_rate_limit() { local remaining=$(curl -s -I https://api.cloudflow.io/api/v1/workflows \\ -H \"Authorization: Bearer $CF_ACCESS_TOKEN\" | \\ grep -i \"x-ratelimit-remaining\" | \\ awk '{print $2}' | tr -d '\\r') if [ \"$remaining\" -lt 10 ]; then echo \"Warning: Only $remaining requests remaining\" local reset=$(curl -s -I https://api.cloudflow.io/api/v1/workflows \\ -H \"Authorization: Bearer $CF_ACCESS_TOKEN\" | \\ grep -i \"x-ratelimit-reset\" | \\ awk '{print $2}' | tr -d '\\r') local wait_time=$((reset - $(date +%s))) echo \"Rate limit resets in $wait_time seconds\" sleep",
              "score": null
            }
          ],
          "latency_ms": 5.7687619992066175,
          "query_id": "realistic_018",
          "dimension": "casual",
          "query": "retry config"
        },
        {
          "key_facts": [
            "max_attempts: 3",
            "backoff_type: exponential",
            "initial_interval: 1000",
            "max retries: 3"
          ],
          "found_facts": [
            "max retries: 3"
          ],
          "missed_facts": [
            "max_attempts: 3",
            "backoff_type: exponential",
            "initial_interval: 1000"
          ],
          "coverage": 0.25,
          "expected_docs": [
            "troubleshooting_guide",
            "user_guide"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "user_guide",
              "chunk_id": "user_guide_fix_16",
              "content": "fixing issues - Export failed executions for analysis - Set up alerts for DLQ threshold breaches **Accessing the DLQ:** 1. Navigate to **Workflows** > **[Your Workflow]** > **Dead Letter Queue** 2. Filter by error type, date range, or execution ID 3. Click an execution to view full details 4. Click **\"Retry\"** to reprocess with the same input data ### Error Notifications Get notified when workflows fail: ```yaml workflow: notifications: on_failure: - type: email to: \"ops-team@company.com\" - type: slack channel: \"#alerts\" message: \"Workflow {{workflow.name}} failed: {{error.message}}\" on_success_after_retry: - type: slack channel: \"#monitoring\" message: \"Workflow recovered after {{error.attempts}} attempts\" ``` ## Workflow Limits CloudFlow enforces the following limits to ensure platform stability and performance: ### Steps Per Workflow - **Maximum**: 50 steps per workflow - **Recommendation**: Keep workflows focused and modular. If you need more steps, consider splitting into multiple workflows connected via webhooks.",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_14",
              "content": "Split workflow into smaller workflows:** ```bash # Create sub-workflows cloudflow workflows create data-pipeline-part1 \\ --steps \"data_ingestion,data_validation\" \\ --timeout 1800 cloudflow workflows create data-pipeline-part2 \\ --steps \"data_transformation,data_export\" \\ --timeout 3600 \\ --trigger workflow_completed \\ --trigger-workflow data-pipeline-part1 ``` ### Retry Logic and Exponential Backoff CloudFlow implements automatic retry with exponential backoff for transient failures: - Max retries: 3 - Initial delay: 1 second - Backoff multiplier: 2 - Max delay: 60 seconds #### Retry Sequence ``` Attempt 1: Immediate Attempt 2: Wait 1s (2^0 * 1s) Attempt 3: Wait 2s (2^1 * 1s) Attempt 4: Wait 4s (2^2 * 1s) ``` #### Configuration ```bash # View current retry settings cloudflow workflows get wf_9k2n4m8p1q --format json | jq '.retry_policy' # Customize retry behavior cloudflow workflows update wf_9k2n4m8p1q \\ --retry-max-attempts 5 \\ --retry-initial-delay 2000 \\ --retry-backoff-multiplier 2 \\ --retry-max-delay 120000 \\ --retry-on-errors \"NETWORK_ERROR,TIMEOUT,RATE_LIMIT\" # Disable retry for specific step cloudflow workflows update wf_9k2n4m8p1q \\",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_15",
              "content": "--step data_export \\ --retry-enabled false ``` #### Monitoring Retries ```bash # List failed executions with retry information cloudflow workflows executions list \\ --status FAILED \\ --show-retries \\ --last 7d # Get retry history for specific execution cloudflow workflows executions retries exec_7h3j6k9m2n # Output: # Execution: exec_7h3j6k9m2n # Attempt 1: FAILED - NetworkError: Connection refused (delay: 0ms) # Attempt 2: FAILED - NetworkError: Connection timeout (delay: 1000ms) # Attempt 3: FAILED - NetworkError: Connection timeout (delay: 2000ms) # Attempt 4: FAILED - NetworkError: Connection timeout (delay: 4000ms) # Final Status: FAILED_AFTER_RETRIES ``` ### Workflow Step Failures #### Common Error Patterns **1.",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_7",
              "content": "\"retry\": {\"max_attempts\": 3, \"backoff\": \"exponential\"} } }, { \"id\": \"create-account\", \"type\": \"database_operation\", \"depends_on\": [\"validate-customer\"] } ] } ``` **State Management**: - Workflow states: PENDING, RUNNING, COMPLETED, FAILED, CANCELLED - Step states: QUEUED, EXECUTING, SUCCESS, FAILED, SKIPPED - State transitions stored in PostgreSQL with event sourcing pattern - Checkpointing every 10 steps for long-running workflows **Performance Targets**: - Simple workflow (3-5 steps): < 2 seconds (P99) - Complex workflow (20+ steps): < 5 seconds (P99) - Throughput: 500 concurrent workflow executions per pod --- ### Scheduler Service **Purpose**: Time-based workflow triggering system supporting cron-like schedules and one-time delayed executions.",
              "score": null
            },
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_12",
              "content": "Waiting {retry_after} seconds...\") time.sleep(retry_after) continue elif response.status_code >= 500: # Server error - retry with exponential backoff wait_time = 2 ** attempt print(f\"Server error. Retrying in {wait_time} seconds...\") time.sleep(wait_time) continue else: # Client error - don't retry error_data = response.json() raise Exception(f\"API Error: {error_data['error']['message']}\") except requests.exceptions.RequestException as e: print(f\"Request failed: {e}\") if attempt < max_retries - 1: time.sleep(2 ** attempt) else: raise raise Exception(\"Max retries exceeded\") ``` ## Webhooks CloudFlow can send webhook notifications when specific events occur in your workflows.",
              "score": null
            }
          ],
          "latency_ms": 5.723787000533775,
          "query_id": "realistic_018",
          "dimension": "contextual",
          "query": "I want workflows to automatically retry on errors, what are the options?"
        },
        {
          "key_facts": [
            "max_attempts: 3",
            "backoff_type: exponential",
            "initial_interval: 1000",
            "max retries: 3"
          ],
          "found_facts": [
            "max retries: 3"
          ],
          "missed_facts": [
            "max_attempts: 3",
            "backoff_type: exponential",
            "initial_interval: 1000"
          ],
          "coverage": 0.25,
          "expected_docs": [
            "troubleshooting_guide",
            "user_guide"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "user_guide",
              "chunk_id": "user_guide_fix_16",
              "content": "fixing issues - Export failed executions for analysis - Set up alerts for DLQ threshold breaches **Accessing the DLQ:** 1. Navigate to **Workflows** > **[Your Workflow]** > **Dead Letter Queue** 2. Filter by error type, date range, or execution ID 3. Click an execution to view full details 4. Click **\"Retry\"** to reprocess with the same input data ### Error Notifications Get notified when workflows fail: ```yaml workflow: notifications: on_failure: - type: email to: \"ops-team@company.com\" - type: slack channel: \"#alerts\" message: \"Workflow {{workflow.name}} failed: {{error.message}}\" on_success_after_retry: - type: slack channel: \"#monitoring\" message: \"Workflow recovered after {{error.attempts}} attempts\" ``` ## Workflow Limits CloudFlow enforces the following limits to ensure platform stability and performance: ### Steps Per Workflow - **Maximum**: 50 steps per workflow - **Recommendation**: Keep workflows focused and modular. If you need more steps, consider splitting into multiple workflows connected via webhooks.",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_15",
              "content": "--step data_export \\ --retry-enabled false ``` #### Monitoring Retries ```bash # List failed executions with retry information cloudflow workflows executions list \\ --status FAILED \\ --show-retries \\ --last 7d # Get retry history for specific execution cloudflow workflows executions retries exec_7h3j6k9m2n # Output: # Execution: exec_7h3j6k9m2n # Attempt 1: FAILED - NetworkError: Connection refused (delay: 0ms) # Attempt 2: FAILED - NetworkError: Connection timeout (delay: 1000ms) # Attempt 3: FAILED - NetworkError: Connection timeout (delay: 2000ms) # Attempt 4: FAILED - NetworkError: Connection timeout (delay: 4000ms) # Final Status: FAILED_AFTER_RETRIES ``` ### Workflow Step Failures #### Common Error Patterns **1.",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_7",
              "content": "\"retry\": {\"max_attempts\": 3, \"backoff\": \"exponential\"} } }, { \"id\": \"create-account\", \"type\": \"database_operation\", \"depends_on\": [\"validate-customer\"] } ] } ``` **State Management**: - Workflow states: PENDING, RUNNING, COMPLETED, FAILED, CANCELLED - Step states: QUEUED, EXECUTING, SUCCESS, FAILED, SKIPPED - State transitions stored in PostgreSQL with event sourcing pattern - Checkpointing every 10 steps for long-running workflows **Performance Targets**: - Simple workflow (3-5 steps): < 2 seconds (P99) - Complex workflow (20+ steps): < 5 seconds (P99) - Throughput: 500 concurrent workflow executions per pod --- ### Scheduler Service **Purpose**: Time-based workflow triggering system supporting cron-like schedules and one-time delayed executions.",
              "score": null
            },
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_12",
              "content": "Waiting {retry_after} seconds...\") time.sleep(retry_after) continue elif response.status_code >= 500: # Server error - retry with exponential backoff wait_time = 2 ** attempt print(f\"Server error. Retrying in {wait_time} seconds...\") time.sleep(wait_time) continue else: # Client error - don't retry error_data = response.json() raise Exception(f\"API Error: {error_data['error']['message']}\") except requests.exceptions.RequestException as e: print(f\"Request failed: {e}\") if attempt < max_retries - 1: time.sleep(2 ** attempt) else: raise raise Exception(\"Max retries exceeded\") ``` ## Webhooks CloudFlow can send webhook notifications when specific events occur in your workflows.",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_14",
              "content": "Split workflow into smaller workflows:** ```bash # Create sub-workflows cloudflow workflows create data-pipeline-part1 \\ --steps \"data_ingestion,data_validation\" \\ --timeout 1800 cloudflow workflows create data-pipeline-part2 \\ --steps \"data_transformation,data_export\" \\ --timeout 3600 \\ --trigger workflow_completed \\ --trigger-workflow data-pipeline-part1 ``` ### Retry Logic and Exponential Backoff CloudFlow implements automatic retry with exponential backoff for transient failures: - Max retries: 3 - Initial delay: 1 second - Backoff multiplier: 2 - Max delay: 60 seconds #### Retry Sequence ``` Attempt 1: Immediate Attempt 2: Wait 1s (2^0 * 1s) Attempt 3: Wait 2s (2^1 * 1s) Attempt 4: Wait 4s (2^2 * 1s) ``` #### Configuration ```bash # View current retry settings cloudflow workflows get wf_9k2n4m8p1q --format json | jq '.retry_policy' # Customize retry behavior cloudflow workflows update wf_9k2n4m8p1q \\ --retry-max-attempts 5 \\ --retry-initial-delay 2000 \\ --retry-backoff-multiplier 2 \\ --retry-max-delay 120000 \\ --retry-on-errors \"NETWORK_ERROR,TIMEOUT,RATE_LIMIT\" # Disable retry for specific step cloudflow workflows update wf_9k2n4m8p1q \\",
              "score": null
            }
          ],
          "latency_ms": 5.41367900041223,
          "query_id": "realistic_018",
          "dimension": "negation",
          "query": "Why doesn't my workflow retry after failing?"
        },
        {
          "key_facts": [
            "helm repo add cloudflow https://charts.cloudflow.io"
          ],
          "found_facts": [
            "helm repo add cloudflow https://charts.cloudflow.io"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "deployment_guide"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_5",
              "content": "```bash helm repo add cloudflow https://charts.cloudflow.io helm repo update ``` Create a values file for production configuration: ```yaml # values-production.yaml replicaCount: 3 image: repository: 123456789012.dkr.ecr.us-east-1.amazonaws.com/cloudflow tag: \"2.4.0\" pullPolicy: IfNotPresent resources: limits: cpu: 2000m memory: 4Gi requests: cpu: 2000m memory: 4Gi autoscaling: enabled: true minReplicas: 3 maxReplicas: 10 targetCPUUtilizationPercentage: 70 targetMemoryUtilizationPercentage: 80 service: type: ClusterIP port: 80 targetPort: 3000 annotations: service.beta.kubernetes.io/aws-load-balancer-type: \"nlb\" ingress: enabled: true className: nginx annotations: cert-manager.io/cluster-issuer: \"letsencrypt-prod\" nginx.ingress.kubernetes.io/ssl-redirect: \"true\" hosts: - host: api.cloudflow.io paths: - path: / pathType: Prefix tls: - secretName: cloudflow-tls hosts: - api.cloudflow.io healthCheck: enabled: true livenessProbe: path: /health initialDelaySeconds: 30 periodSeconds: 10 timeoutSeconds: 5 failureThreshold: 3 readinessProbe: path: /ready initialDelaySeconds: 10 periodSeconds: 5 timeoutSeconds: 3 failureThreshold: 3 env: - name: NODE_ENV value: \"production\" - name: LOG_LEVEL value: \"info\" - name: API_PORT value: \"3000\" - name: WORKER_CONCURRENCY value: \"10\" - name: AWS_REGION value: \"us-east-1\" - name: METRICS_ENABLED value: \"true\" envFrom: - secretRef: name: cloudflow-secrets podAnnotations:",
              "score": null
            },
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_0",
              "content": "# CloudFlow Platform - Deployment and Operations Guide **Version:** 2.4.0 **Last Updated:** January 2026 **Target Environment:** Production (AWS EKS) ## Table of Contents 1. [Overview](#overview) 2. [Prerequisites](#prerequisites) 3. [Infrastructure Setup](#infrastructure-setup) 4. [Kubernetes Deployment](#kubernetes-deployment) 5. [Database Configuration](#database-configuration) 6. [Monitoring and Observability](#monitoring-and-observability) 7. [Backup and Disaster Recovery](#backup-and-disaster-recovery) 8. [Scaling and Performance](#scaling-and-performance) 9. [Security and Compliance](#security-and-compliance) 10. [Troubleshooting](#troubleshooting) --- ## Overview CloudFlow is a cloud-native workflow orchestration platform designed for high-availability production environments. This guide provides comprehensive instructions for deploying and operating CloudFlow on Amazon EKS (Elastic Kubernetes Service).",
              "score": null
            },
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_6",
              "content": "prometheus.io/scrape: \"true\" prometheus.io/port: \"9090\" prometheus.io/path: \"/metrics\" podSecurityContext: runAsNonRoot: true runAsUser: 1000 fsGroup: 1000 securityContext: allowPrivilegeEscalation: false capabilities: drop: - ALL readOnlyRootFilesystem: true persistence: enabled: true storageClass: gp3 accessMode: ReadWriteOnce size: 50Gi redis: enabled: true architecture: replication auth: enabled: true master: persistence: size: 20Gi replica: replicaCount: 2 persistence: size: 20Gi ``` Deploy CloudFlow using Helm: ```bash helm install cloudflow cloudflow/cloudflow \\ --namespace cloudflow-prod \\ --values values-production.yaml \\ --wait \\ --timeout 10m ``` ### Deployment Verification Verify the deployment status: ```bash # Check pod status kubectl get pods -n cloudflow-prod # Expected output: # NAME READY STATUS RESTARTS AGE # cloudflow-api-7d8f9c5b6d-4xk2p 1/1 Running 0 2m # cloudflow-api-7d8f9c5b6d-9hj5m 1/1 Running 0 2m # cloudflow-api-7d8f9c5b6d-tn8wq 1/1 Running 0 2m # Check service endpoints kubectl get svc -n cloudflow-prod # Check ingress kubectl get ingress -n cloudflow-prod ``` Test health endpoints: ```bash # Port forward for testing kubectl port-forward -n cloudflow-prod svc/cloudflow-api 8080:80 # Test",
              "score": null
            },
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_1",
              "content": "### Architecture Summary CloudFlow consists of the following components: - **API Server**: REST API for workflow management (Node.js/Express) - **Worker Service**: Background job processor (Node.js) - **Scheduler**: Cron-based task scheduler (Node.js) - **PostgreSQL**: Primary data store (version 14) - **Redis**: Cache and message queue (version 7.0) - **PgBouncer**: Database connection pooler ### Deployment Model - **Namespace**: `cloudflow-prod` - **Cluster**: AWS EKS 1.28 - **Region**: us-east-1 (primary), us-west-2 (disaster recovery) - **High Availability**: Multi-AZ deployment across 3 availability zones --- ## Prerequisites Before beginning the deployment process, ensure you have the following: ### Required Tools - `kubectl` (v1.28 or later) - `helm` (v3.12 or later) - `aws-cli` (v2.13 or later) - `eksctl` (v0.165 or later) - `terraform` (v1.6 or later) - for infrastructure provisioning ### Access Requirements - AWS account with appropriate IAM permissions - EKS cluster admin access - Container registry access (ECR) - Domain name and SSL certificates -",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_27",
              "content": "changes cloudflow audit log --last 2h --event-type \"deployment,configuration\" # Check metrics dashboard cloudflow metrics dashboard --incident-mode ``` #### Step 2: Gather Information (5-15 minutes) Create incident document with: - Incident timestamp and duration - Affected services and endpoints - Error rates and user impact - Recent changes or deployments - Relevant log excerpts - Correlation IDs for failed requests ```bash # Generate incident report cloudflow debug incident-report \\ --start \"2026-01-24T10:30:00Z\" \\ --end \"2026-01-24T11:00:00Z\" \\ --output incident-report.md # Capture system snapshot cloudflow debug snapshot --output snapshot-$(date +%Y%m%d-%H%M%S).tar.gz ``` #### Step 3: Escalate Based on Severity **For SEV-1 (Critical):** ```bash # Page on-call engineer cloudflow incident create \\ --severity SEV-1 \\ --title \"Complete API outage\" \\ --description \"All API requests returning 503\" \\ --page-oncall # Notify status page cloudflow status update \\ --status \"major_outage\" \\ --message \"We are investigating a complete service outage\" # Create war room cloudflow incident war-room create incident-2024012401",
              "score": null
            }
          ],
          "latency_ms": 5.593414000031771,
          "query_id": "realistic_019",
          "dimension": "original",
          "query": "What Helm chart repository should I use for deployment?"
        },
        {
          "key_facts": [
            "helm repo add cloudflow https://charts.cloudflow.io"
          ],
          "found_facts": [
            "helm repo add cloudflow https://charts.cloudflow.io"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "deployment_guide"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_5",
              "content": "```bash helm repo add cloudflow https://charts.cloudflow.io helm repo update ``` Create a values file for production configuration: ```yaml # values-production.yaml replicaCount: 3 image: repository: 123456789012.dkr.ecr.us-east-1.amazonaws.com/cloudflow tag: \"2.4.0\" pullPolicy: IfNotPresent resources: limits: cpu: 2000m memory: 4Gi requests: cpu: 2000m memory: 4Gi autoscaling: enabled: true minReplicas: 3 maxReplicas: 10 targetCPUUtilizationPercentage: 70 targetMemoryUtilizationPercentage: 80 service: type: ClusterIP port: 80 targetPort: 3000 annotations: service.beta.kubernetes.io/aws-load-balancer-type: \"nlb\" ingress: enabled: true className: nginx annotations: cert-manager.io/cluster-issuer: \"letsencrypt-prod\" nginx.ingress.kubernetes.io/ssl-redirect: \"true\" hosts: - host: api.cloudflow.io paths: - path: / pathType: Prefix tls: - secretName: cloudflow-tls hosts: - api.cloudflow.io healthCheck: enabled: true livenessProbe: path: /health initialDelaySeconds: 30 periodSeconds: 10 timeoutSeconds: 5 failureThreshold: 3 readinessProbe: path: /ready initialDelaySeconds: 10 periodSeconds: 5 timeoutSeconds: 3 failureThreshold: 3 env: - name: NODE_ENV value: \"production\" - name: LOG_LEVEL value: \"info\" - name: API_PORT value: \"3000\" - name: WORKER_CONCURRENCY value: \"10\" - name: AWS_REGION value: \"us-east-1\" - name: METRICS_ENABLED value: \"true\" envFrom: - secretRef: name: cloudflow-secrets podAnnotations:",
              "score": null
            },
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_0",
              "content": "# CloudFlow Platform - Deployment and Operations Guide **Version:** 2.4.0 **Last Updated:** January 2026 **Target Environment:** Production (AWS EKS) ## Table of Contents 1. [Overview](#overview) 2. [Prerequisites](#prerequisites) 3. [Infrastructure Setup](#infrastructure-setup) 4. [Kubernetes Deployment](#kubernetes-deployment) 5. [Database Configuration](#database-configuration) 6. [Monitoring and Observability](#monitoring-and-observability) 7. [Backup and Disaster Recovery](#backup-and-disaster-recovery) 8. [Scaling and Performance](#scaling-and-performance) 9. [Security and Compliance](#security-and-compliance) 10. [Troubleshooting](#troubleshooting) --- ## Overview CloudFlow is a cloud-native workflow orchestration platform designed for high-availability production environments. This guide provides comprehensive instructions for deploying and operating CloudFlow on Amazon EKS (Elastic Kubernetes Service).",
              "score": null
            },
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_4",
              "content": "verbosity | `info`, `debug`, `warn`, `error` | No (default: `info`) | | `API_PORT` | API server port | `3000` | No (default: `3000`) | | `WORKER_CONCURRENCY` | Number of concurrent workers | `10` | No (default: `5`) | | `SESSION_SECRET` | Session encryption key | `<generated-secret-256-bit>` | Yes | | `AWS_REGION` | AWS region for services | `us-east-1` | Yes | | `S3_BUCKET` | S3 bucket for file storage | `cloudflow-prod-storage` | Yes | | `SMTP_HOST` | Email server hostname | `smtp.sendgrid.net` | No | | `SMTP_PORT` | Email server port | `587` | No | | `METRICS_ENABLED` | Enable Prometheus metrics | `true` | No (default: `false`) | Create a Kubernetes secret for sensitive variables: ```bash kubectl create secret generic cloudflow-secrets \\ --namespace cloudflow-prod \\ --from-literal=DATABASE_URL=\"postgresql://cloudflow:$(cat db-password.txt)@pgbouncer.cloudflow-prod.svc.cluster.local:5432/cloudflow\" \\ --from-literal=REDIS_URL=\"redis://redis-master.cloudflow-prod.svc.cluster.local:6379\" \\ --from-literal=JWT_SECRET=\"$(openssl rand -base64 32)\" \\ --from-literal=SESSION_SECRET=\"$(openssl rand -base64 32)\" ``` ### Helm Chart Deployment Add the CloudFlow Helm repository:",
              "score": null
            },
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_1",
              "content": "### Architecture Summary CloudFlow consists of the following components: - **API Server**: REST API for workflow management (Node.js/Express) - **Worker Service**: Background job processor (Node.js) - **Scheduler**: Cron-based task scheduler (Node.js) - **PostgreSQL**: Primary data store (version 14) - **Redis**: Cache and message queue (version 7.0) - **PgBouncer**: Database connection pooler ### Deployment Model - **Namespace**: `cloudflow-prod` - **Cluster**: AWS EKS 1.28 - **Region**: us-east-1 (primary), us-west-2 (disaster recovery) - **High Availability**: Multi-AZ deployment across 3 availability zones --- ## Prerequisites Before beginning the deployment process, ensure you have the following: ### Required Tools - `kubectl` (v1.28 or later) - `helm` (v3.12 or later) - `aws-cli` (v2.13 or later) - `eksctl` (v0.165 or later) - `terraform` (v1.6 or later) - for infrastructure provisioning ### Access Requirements - AWS account with appropriate IAM permissions - EKS cluster admin access - Container registry access (ECR) - Domain name and SSL certificates -",
              "score": null
            },
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_6",
              "content": "prometheus.io/scrape: \"true\" prometheus.io/port: \"9090\" prometheus.io/path: \"/metrics\" podSecurityContext: runAsNonRoot: true runAsUser: 1000 fsGroup: 1000 securityContext: allowPrivilegeEscalation: false capabilities: drop: - ALL readOnlyRootFilesystem: true persistence: enabled: true storageClass: gp3 accessMode: ReadWriteOnce size: 50Gi redis: enabled: true architecture: replication auth: enabled: true master: persistence: size: 20Gi replica: replicaCount: 2 persistence: size: 20Gi ``` Deploy CloudFlow using Helm: ```bash helm install cloudflow cloudflow/cloudflow \\ --namespace cloudflow-prod \\ --values values-production.yaml \\ --wait \\ --timeout 10m ``` ### Deployment Verification Verify the deployment status: ```bash # Check pod status kubectl get pods -n cloudflow-prod # Expected output: # NAME READY STATUS RESTARTS AGE # cloudflow-api-7d8f9c5b6d-4xk2p 1/1 Running 0 2m # cloudflow-api-7d8f9c5b6d-9hj5m 1/1 Running 0 2m # cloudflow-api-7d8f9c5b6d-tn8wq 1/1 Running 0 2m # Check service endpoints kubectl get svc -n cloudflow-prod # Check ingress kubectl get ingress -n cloudflow-prod ``` Test health endpoints: ```bash # Port forward for testing kubectl port-forward -n cloudflow-prod svc/cloudflow-api 8080:80 # Test",
              "score": null
            }
          ],
          "latency_ms": 5.342866999853868,
          "query_id": "realistic_019",
          "dimension": "synonym",
          "query": "Where is the CloudFlow Helm chart hosted?"
        },
        {
          "key_facts": [
            "helm repo add cloudflow https://charts.cloudflow.io"
          ],
          "found_facts": [
            "helm repo add cloudflow https://charts.cloudflow.io"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "deployment_guide"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_5",
              "content": "```bash helm repo add cloudflow https://charts.cloudflow.io helm repo update ``` Create a values file for production configuration: ```yaml # values-production.yaml replicaCount: 3 image: repository: 123456789012.dkr.ecr.us-east-1.amazonaws.com/cloudflow tag: \"2.4.0\" pullPolicy: IfNotPresent resources: limits: cpu: 2000m memory: 4Gi requests: cpu: 2000m memory: 4Gi autoscaling: enabled: true minReplicas: 3 maxReplicas: 10 targetCPUUtilizationPercentage: 70 targetMemoryUtilizationPercentage: 80 service: type: ClusterIP port: 80 targetPort: 3000 annotations: service.beta.kubernetes.io/aws-load-balancer-type: \"nlb\" ingress: enabled: true className: nginx annotations: cert-manager.io/cluster-issuer: \"letsencrypt-prod\" nginx.ingress.kubernetes.io/ssl-redirect: \"true\" hosts: - host: api.cloudflow.io paths: - path: / pathType: Prefix tls: - secretName: cloudflow-tls hosts: - api.cloudflow.io healthCheck: enabled: true livenessProbe: path: /health initialDelaySeconds: 30 periodSeconds: 10 timeoutSeconds: 5 failureThreshold: 3 readinessProbe: path: /ready initialDelaySeconds: 10 periodSeconds: 5 timeoutSeconds: 3 failureThreshold: 3 env: - name: NODE_ENV value: \"production\" - name: LOG_LEVEL value: \"info\" - name: API_PORT value: \"3000\" - name: WORKER_CONCURRENCY value: \"10\" - name: AWS_REGION value: \"us-east-1\" - name: METRICS_ENABLED value: \"true\" envFrom: - secretRef: name: cloudflow-secrets podAnnotations:",
              "score": null
            },
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_13",
              "content": "**Supported Events:** - `workflow.started` - `workflow.completed` - `workflow.failed` - `pipeline.completed` - `pipeline.failed` **Webhook Payload Example:** ```json { \"event\": \"workflow.completed\", \"timestamp\": \"2026-01-24T10:45:00Z\", \"data\": { \"workflow_id\": \"wf_8x7k2m9p\", \"execution_id\": \"exec_9k3m7n2q\", \"status\": \"completed\", \"duration_ms\": 3420, \"records_processed\": 1247 } } ``` Configure webhooks in your account settings or via the API: ```bash curl -X POST https://api.cloudflow.io/v2/webhooks \\ -H \"X-API-Key: YOUR_API_KEY\" \\ -H \"Content-Type: application/json\" \\ -d '{ \"url\": \"https://your-app.com/cloudflow-webhook\", \"events\": [\"workflow.completed\", \"workflow.failed\"], \"secret\": \"whsec_your_webhook_secret\" }' ``` ## Support For additional help and resources: - **Documentation:** https://docs.cloudflow.io - **API Status:** https://status.cloudflow.io - **Support Email:** support@cloudflow.io - **Community Forum:** https://community.cloudflow.io Enterprise customers have access to 24/7 priority support via phone and dedicated Slack channels.",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_28",
              "content": "``` **For SEV-2 (High):** ```bash # Create incident and notify cloudflow incident create \\ --severity SEV-2 \\ --title \"High rate of workflow failures\" \\ --description \"Workflow success rate dropped to 75%\" \\ --notify-oncall # Update status page cloudflow status update \\ --status \"partial_outage\" \\ --message \"We are experiencing elevated error rates\" ``` **For SEV-3/SEV-4:** ```bash # Create support ticket cloudflow support ticket create \\ --priority MEDIUM \\ --subject \"Intermittent timeout errors\" \\ --description \"See attached logs and reproduction steps\" ``` ### Contact Information - **On-call hotline:** +1-888-CLOUDFLOW (24/7) - **Support email:** support@cloudflow.io - **Incident Slack channel:** #cloudflow-incidents - **Status page:** https://status.cloudflow.io - **Documentation:** https://docs.cloudflow.io ### Post-Incident Review After resolving incidents, complete a postmortem: ```bash # Generate postmortem template cloudflow incident postmortem incident-2024012401 \\ --template standard \\ --output postmortem-2024012401.md # Required sections: # 1. Incident summary # 2. Impact assessment # 3. Timeline of events # 4.",
              "score": null
            },
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_6",
              "content": "prometheus.io/scrape: \"true\" prometheus.io/port: \"9090\" prometheus.io/path: \"/metrics\" podSecurityContext: runAsNonRoot: true runAsUser: 1000 fsGroup: 1000 securityContext: allowPrivilegeEscalation: false capabilities: drop: - ALL readOnlyRootFilesystem: true persistence: enabled: true storageClass: gp3 accessMode: ReadWriteOnce size: 50Gi redis: enabled: true architecture: replication auth: enabled: true master: persistence: size: 20Gi replica: replicaCount: 2 persistence: size: 20Gi ``` Deploy CloudFlow using Helm: ```bash helm install cloudflow cloudflow/cloudflow \\ --namespace cloudflow-prod \\ --values values-production.yaml \\ --wait \\ --timeout 10m ``` ### Deployment Verification Verify the deployment status: ```bash # Check pod status kubectl get pods -n cloudflow-prod # Expected output: # NAME READY STATUS RESTARTS AGE # cloudflow-api-7d8f9c5b6d-4xk2p 1/1 Running 0 2m # cloudflow-api-7d8f9c5b6d-9hj5m 1/1 Running 0 2m # cloudflow-api-7d8f9c5b6d-tn8wq 1/1 Running 0 2m # Check service endpoints kubectl get svc -n cloudflow-prod # Check ingress kubectl get ingress -n cloudflow-prod ``` Test health endpoints: ```bash # Port forward for testing kubectl port-forward -n cloudflow-prod svc/cloudflow-api 8080:80 # Test",
              "score": null
            },
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_4",
              "content": "verbosity | `info`, `debug`, `warn`, `error` | No (default: `info`) | | `API_PORT` | API server port | `3000` | No (default: `3000`) | | `WORKER_CONCURRENCY` | Number of concurrent workers | `10` | No (default: `5`) | | `SESSION_SECRET` | Session encryption key | `<generated-secret-256-bit>` | Yes | | `AWS_REGION` | AWS region for services | `us-east-1` | Yes | | `S3_BUCKET` | S3 bucket for file storage | `cloudflow-prod-storage` | Yes | | `SMTP_HOST` | Email server hostname | `smtp.sendgrid.net` | No | | `SMTP_PORT` | Email server port | `587` | No | | `METRICS_ENABLED` | Enable Prometheus metrics | `true` | No (default: `false`) | Create a Kubernetes secret for sensitive variables: ```bash kubectl create secret generic cloudflow-secrets \\ --namespace cloudflow-prod \\ --from-literal=DATABASE_URL=\"postgresql://cloudflow:$(cat db-password.txt)@pgbouncer.cloudflow-prod.svc.cluster.local:5432/cloudflow\" \\ --from-literal=REDIS_URL=\"redis://redis-master.cloudflow-prod.svc.cluster.local:6379\" \\ --from-literal=JWT_SECRET=\"$(openssl rand -base64 32)\" \\ --from-literal=SESSION_SECRET=\"$(openssl rand -base64 32)\" ``` ### Helm Chart Deployment Add the CloudFlow Helm repository:",
              "score": null
            }
          ],
          "latency_ms": 5.3028329984954325,
          "query_id": "realistic_019",
          "dimension": "problem",
          "query": "helm repo add is failing, what's the correct URL?"
        },
        {
          "key_facts": [
            "helm repo add cloudflow https://charts.cloudflow.io"
          ],
          "found_facts": [
            "helm repo add cloudflow https://charts.cloudflow.io"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "deployment_guide"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_5",
              "content": "```bash helm repo add cloudflow https://charts.cloudflow.io helm repo update ``` Create a values file for production configuration: ```yaml # values-production.yaml replicaCount: 3 image: repository: 123456789012.dkr.ecr.us-east-1.amazonaws.com/cloudflow tag: \"2.4.0\" pullPolicy: IfNotPresent resources: limits: cpu: 2000m memory: 4Gi requests: cpu: 2000m memory: 4Gi autoscaling: enabled: true minReplicas: 3 maxReplicas: 10 targetCPUUtilizationPercentage: 70 targetMemoryUtilizationPercentage: 80 service: type: ClusterIP port: 80 targetPort: 3000 annotations: service.beta.kubernetes.io/aws-load-balancer-type: \"nlb\" ingress: enabled: true className: nginx annotations: cert-manager.io/cluster-issuer: \"letsencrypt-prod\" nginx.ingress.kubernetes.io/ssl-redirect: \"true\" hosts: - host: api.cloudflow.io paths: - path: / pathType: Prefix tls: - secretName: cloudflow-tls hosts: - api.cloudflow.io healthCheck: enabled: true livenessProbe: path: /health initialDelaySeconds: 30 periodSeconds: 10 timeoutSeconds: 5 failureThreshold: 3 readinessProbe: path: /ready initialDelaySeconds: 10 periodSeconds: 5 timeoutSeconds: 3 failureThreshold: 3 env: - name: NODE_ENV value: \"production\" - name: LOG_LEVEL value: \"info\" - name: API_PORT value: \"3000\" - name: WORKER_CONCURRENCY value: \"10\" - name: AWS_REGION value: \"us-east-1\" - name: METRICS_ENABLED value: \"true\" envFrom: - secretRef: name: cloudflow-secrets podAnnotations:",
              "score": null
            },
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_6",
              "content": "prometheus.io/scrape: \"true\" prometheus.io/port: \"9090\" prometheus.io/path: \"/metrics\" podSecurityContext: runAsNonRoot: true runAsUser: 1000 fsGroup: 1000 securityContext: allowPrivilegeEscalation: false capabilities: drop: - ALL readOnlyRootFilesystem: true persistence: enabled: true storageClass: gp3 accessMode: ReadWriteOnce size: 50Gi redis: enabled: true architecture: replication auth: enabled: true master: persistence: size: 20Gi replica: replicaCount: 2 persistence: size: 20Gi ``` Deploy CloudFlow using Helm: ```bash helm install cloudflow cloudflow/cloudflow \\ --namespace cloudflow-prod \\ --values values-production.yaml \\ --wait \\ --timeout 10m ``` ### Deployment Verification Verify the deployment status: ```bash # Check pod status kubectl get pods -n cloudflow-prod # Expected output: # NAME READY STATUS RESTARTS AGE # cloudflow-api-7d8f9c5b6d-4xk2p 1/1 Running 0 2m # cloudflow-api-7d8f9c5b6d-9hj5m 1/1 Running 0 2m # cloudflow-api-7d8f9c5b6d-tn8wq 1/1 Running 0 2m # Check service endpoints kubectl get svc -n cloudflow-prod # Check ingress kubectl get ingress -n cloudflow-prod ``` Test health endpoints: ```bash # Port forward for testing kubectl port-forward -n cloudflow-prod svc/cloudflow-api 8080:80 # Test",
              "score": null
            },
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_17",
              "content": "namespace level: ```bash kubectl label namespace cloudflow-prod \\ pod-security.kubernetes.io/enforce=restricted \\ pod-security.kubernetes.io/audit=restricted \\ pod-security.kubernetes.io/warn=restricted ``` ### Secrets Management Use AWS Secrets Manager for sensitive credentials: ```bash # Install External Secrets Operator helm install external-secrets external-secrets/external-secrets \\ --namespace external-secrets-system \\ --create-namespace # Create SecretStore kubectl apply -f - <<EOF apiVersion: external-secrets.io/v1beta1 kind: SecretStore metadata: name: aws-secrets-manager namespace: cloudflow-prod spec: provider: aws: service: SecretsManager region: us-east-1 auth: jwt: serviceAccountRef: name: external-secrets-sa EOF ``` --- ## Troubleshooting ### Common Issues #### Pods in CrashLoopBackOff **Symptoms**: Pods continuously restart **Diagnosis**: ```bash kubectl logs -n cloudflow-prod <pod-name> --previous kubectl describe pod -n cloudflow-prod <pod-name> ``` **Common Causes**: - Database connection failure - Invalid environment variables - Insufficient resources #### High Memory Usage **Symptoms**: Pods being OOMKilled **Diagnosis**: ```bash kubectl top pods -n cloudflow-prod ``` **Resolution**: - Increase memory limits in deployment - Check for memory leaks in application logs - Review heap dump for Node.js processes",
              "score": null
            },
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_4",
              "content": "verbosity | `info`, `debug`, `warn`, `error` | No (default: `info`) | | `API_PORT` | API server port | `3000` | No (default: `3000`) | | `WORKER_CONCURRENCY` | Number of concurrent workers | `10` | No (default: `5`) | | `SESSION_SECRET` | Session encryption key | `<generated-secret-256-bit>` | Yes | | `AWS_REGION` | AWS region for services | `us-east-1` | Yes | | `S3_BUCKET` | S3 bucket for file storage | `cloudflow-prod-storage` | Yes | | `SMTP_HOST` | Email server hostname | `smtp.sendgrid.net` | No | | `SMTP_PORT` | Email server port | `587` | No | | `METRICS_ENABLED` | Enable Prometheus metrics | `true` | No (default: `false`) | Create a Kubernetes secret for sensitive variables: ```bash kubectl create secret generic cloudflow-secrets \\ --namespace cloudflow-prod \\ --from-literal=DATABASE_URL=\"postgresql://cloudflow:$(cat db-password.txt)@pgbouncer.cloudflow-prod.svc.cluster.local:5432/cloudflow\" \\ --from-literal=REDIS_URL=\"redis://redis-master.cloudflow-prod.svc.cluster.local:6379\" \\ --from-literal=JWT_SECRET=\"$(openssl rand -base64 32)\" \\ --from-literal=SESSION_SECRET=\"$(openssl rand -base64 32)\" ``` ### Helm Chart Deployment Add the CloudFlow Helm repository:",
              "score": null
            },
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_7",
              "content": "health endpoint curl http://localhost:8080/health # Expected: {\"status\":\"healthy\",\"timestamp\":\"2026-01-24T10:30:00Z\"} # Test readiness endpoint curl http://localhost:8080/ready # Expected: {\"status\":\"ready\",\"dependencies\":{\"database\":\"connected\",\"redis\":\"connected\"}} ``` --- ## Database Configuration ### PostgreSQL Setup CloudFlow uses PostgreSQL 14 as its primary data store. Deploy PostgreSQL using the Bitnami Helm chart: ```yaml # postgres-values.yaml global: postgresql: auth: username: cloudflow database: cloudflow existingSecret: postgres-credentials image: tag: \"14.10.0\" primary: resources: limits: cpu: 4000m memory: 8Gi requests: cpu: 2000m memory: 4Gi persistence: enabled: true size: 100Gi storageClass: gp3 extendedConfiguration: | max_connections = 100 shared_buffers = 2GB effective_cache_size = 6GB maintenance_work_mem = 512MB checkpoint_completion_target = 0.9 wal_buffers = 16MB default_statistics_target = 100 random_page_cost = 1.1 effective_io_concurrency = 200 work_mem = 10MB min_wal_size = 1GB max_wal_size = 4GB readReplicas: replicaCount: 2 resources: limits: cpu: 2000m memory: 4Gi requests: cpu: 1000m memory: 2Gi metrics: enabled: true serviceMonitor: enabled: true ``` Install PostgreSQL: ```bash # Create password secret kubectl create secret generic postgres-credentials \\ --namespace cloudflow-prod \\ --from-literal=postgres-password=\"$(openssl",
              "score": null
            }
          ],
          "latency_ms": 5.418237999037956,
          "query_id": "realistic_019",
          "dimension": "casual",
          "query": "helm repo"
        },
        {
          "key_facts": [
            "helm repo add cloudflow https://charts.cloudflow.io"
          ],
          "found_facts": [
            "helm repo add cloudflow https://charts.cloudflow.io"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "deployment_guide"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_5",
              "content": "```bash helm repo add cloudflow https://charts.cloudflow.io helm repo update ``` Create a values file for production configuration: ```yaml # values-production.yaml replicaCount: 3 image: repository: 123456789012.dkr.ecr.us-east-1.amazonaws.com/cloudflow tag: \"2.4.0\" pullPolicy: IfNotPresent resources: limits: cpu: 2000m memory: 4Gi requests: cpu: 2000m memory: 4Gi autoscaling: enabled: true minReplicas: 3 maxReplicas: 10 targetCPUUtilizationPercentage: 70 targetMemoryUtilizationPercentage: 80 service: type: ClusterIP port: 80 targetPort: 3000 annotations: service.beta.kubernetes.io/aws-load-balancer-type: \"nlb\" ingress: enabled: true className: nginx annotations: cert-manager.io/cluster-issuer: \"letsencrypt-prod\" nginx.ingress.kubernetes.io/ssl-redirect: \"true\" hosts: - host: api.cloudflow.io paths: - path: / pathType: Prefix tls: - secretName: cloudflow-tls hosts: - api.cloudflow.io healthCheck: enabled: true livenessProbe: path: /health initialDelaySeconds: 30 periodSeconds: 10 timeoutSeconds: 5 failureThreshold: 3 readinessProbe: path: /ready initialDelaySeconds: 10 periodSeconds: 5 timeoutSeconds: 3 failureThreshold: 3 env: - name: NODE_ENV value: \"production\" - name: LOG_LEVEL value: \"info\" - name: API_PORT value: \"3000\" - name: WORKER_CONCURRENCY value: \"10\" - name: AWS_REGION value: \"us-east-1\" - name: METRICS_ENABLED value: \"true\" envFrom: - secretRef: name: cloudflow-secrets podAnnotations:",
              "score": null
            },
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_0",
              "content": "# CloudFlow Platform - Deployment and Operations Guide **Version:** 2.4.0 **Last Updated:** January 2026 **Target Environment:** Production (AWS EKS) ## Table of Contents 1. [Overview](#overview) 2. [Prerequisites](#prerequisites) 3. [Infrastructure Setup](#infrastructure-setup) 4. [Kubernetes Deployment](#kubernetes-deployment) 5. [Database Configuration](#database-configuration) 6. [Monitoring and Observability](#monitoring-and-observability) 7. [Backup and Disaster Recovery](#backup-and-disaster-recovery) 8. [Scaling and Performance](#scaling-and-performance) 9. [Security and Compliance](#security-and-compliance) 10. [Troubleshooting](#troubleshooting) --- ## Overview CloudFlow is a cloud-native workflow orchestration platform designed for high-availability production environments. This guide provides comprehensive instructions for deploying and operating CloudFlow on Amazon EKS (Elastic Kubernetes Service).",
              "score": null
            },
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_1",
              "content": "### Architecture Summary CloudFlow consists of the following components: - **API Server**: REST API for workflow management (Node.js/Express) - **Worker Service**: Background job processor (Node.js) - **Scheduler**: Cron-based task scheduler (Node.js) - **PostgreSQL**: Primary data store (version 14) - **Redis**: Cache and message queue (version 7.0) - **PgBouncer**: Database connection pooler ### Deployment Model - **Namespace**: `cloudflow-prod` - **Cluster**: AWS EKS 1.28 - **Region**: us-east-1 (primary), us-west-2 (disaster recovery) - **High Availability**: Multi-AZ deployment across 3 availability zones --- ## Prerequisites Before beginning the deployment process, ensure you have the following: ### Required Tools - `kubectl` (v1.28 or later) - `helm` (v3.12 or later) - `aws-cli` (v2.13 or later) - `eksctl` (v0.165 or later) - `terraform` (v1.6 or later) - for infrastructure provisioning ### Access Requirements - AWS account with appropriate IAM permissions - EKS cluster admin access - Container registry access (ECR) - Domain name and SSL certificates -",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_0",
              "content": "# CloudFlow Platform - System Architecture Overview **Document Version:** 2.3.1 **Last Updated:** January 15, 2026 **Owner:** Platform Architecture Team **Status:** Production ## Executive Summary CloudFlow is a distributed, cloud-native workflow automation platform designed to orchestrate complex business processes at scale. The platform processes over 2.5 million workflow executions daily with an average P99 latency of 180ms for API operations and 4.2 seconds for workflow execution. This document provides a comprehensive overview of the system architecture, including microservices design, data flow patterns, infrastructure decisions, and operational considerations. ## Table of Contents 1. [High-Level Architecture](#high-level-architecture) 2. [Microservices Breakdown](#microservices-breakdown) 3. [Data Flow Architecture](#data-flow-architecture) 4. [Database Architecture](#database-architecture) 5. [Message Queue Patterns](#message-queue-patterns) 6. [Caching Strategy](#caching-strategy) 7. [Security Architecture](#security-architecture) 8. [Performance Characteristics](#performance-characteristics) 9. [Disaster Recovery](#disaster-recovery) --- ## High-Level Architecture CloudFlow follows a microservices architecture pattern deployed across multiple availability zones in AWS.",
              "score": null
            },
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_4",
              "content": "verbosity | `info`, `debug`, `warn`, `error` | No (default: `info`) | | `API_PORT` | API server port | `3000` | No (default: `3000`) | | `WORKER_CONCURRENCY` | Number of concurrent workers | `10` | No (default: `5`) | | `SESSION_SECRET` | Session encryption key | `<generated-secret-256-bit>` | Yes | | `AWS_REGION` | AWS region for services | `us-east-1` | Yes | | `S3_BUCKET` | S3 bucket for file storage | `cloudflow-prod-storage` | Yes | | `SMTP_HOST` | Email server hostname | `smtp.sendgrid.net` | No | | `SMTP_PORT` | Email server port | `587` | No | | `METRICS_ENABLED` | Enable Prometheus metrics | `true` | No (default: `false`) | Create a Kubernetes secret for sensitive variables: ```bash kubectl create secret generic cloudflow-secrets \\ --namespace cloudflow-prod \\ --from-literal=DATABASE_URL=\"postgresql://cloudflow:$(cat db-password.txt)@pgbouncer.cloudflow-prod.svc.cluster.local:5432/cloudflow\" \\ --from-literal=REDIS_URL=\"redis://redis-master.cloudflow-prod.svc.cluster.local:6379\" \\ --from-literal=JWT_SECRET=\"$(openssl rand -base64 32)\" \\ --from-literal=SESSION_SECRET=\"$(openssl rand -base64 32)\" ``` ### Helm Chart Deployment Add the CloudFlow Helm repository:",
              "score": null
            }
          ],
          "latency_ms": 5.431792998933815,
          "query_id": "realistic_019",
          "dimension": "contextual",
          "query": "Setting up deployment pipeline, which Helm repository has CloudFlow charts?"
        },
        {
          "key_facts": [
            "helm repo add cloudflow https://charts.cloudflow.io"
          ],
          "found_facts": [
            "helm repo add cloudflow https://charts.cloudflow.io"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "deployment_guide"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_5",
              "content": "```bash helm repo add cloudflow https://charts.cloudflow.io helm repo update ``` Create a values file for production configuration: ```yaml # values-production.yaml replicaCount: 3 image: repository: 123456789012.dkr.ecr.us-east-1.amazonaws.com/cloudflow tag: \"2.4.0\" pullPolicy: IfNotPresent resources: limits: cpu: 2000m memory: 4Gi requests: cpu: 2000m memory: 4Gi autoscaling: enabled: true minReplicas: 3 maxReplicas: 10 targetCPUUtilizationPercentage: 70 targetMemoryUtilizationPercentage: 80 service: type: ClusterIP port: 80 targetPort: 3000 annotations: service.beta.kubernetes.io/aws-load-balancer-type: \"nlb\" ingress: enabled: true className: nginx annotations: cert-manager.io/cluster-issuer: \"letsencrypt-prod\" nginx.ingress.kubernetes.io/ssl-redirect: \"true\" hosts: - host: api.cloudflow.io paths: - path: / pathType: Prefix tls: - secretName: cloudflow-tls hosts: - api.cloudflow.io healthCheck: enabled: true livenessProbe: path: /health initialDelaySeconds: 30 periodSeconds: 10 timeoutSeconds: 5 failureThreshold: 3 readinessProbe: path: /ready initialDelaySeconds: 10 periodSeconds: 5 timeoutSeconds: 3 failureThreshold: 3 env: - name: NODE_ENV value: \"production\" - name: LOG_LEVEL value: \"info\" - name: API_PORT value: \"3000\" - name: WORKER_CONCURRENCY value: \"10\" - name: AWS_REGION value: \"us-east-1\" - name: METRICS_ENABLED value: \"true\" envFrom: - secretRef: name: cloudflow-secrets podAnnotations:",
              "score": null
            },
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_4",
              "content": "verbosity | `info`, `debug`, `warn`, `error` | No (default: `info`) | | `API_PORT` | API server port | `3000` | No (default: `3000`) | | `WORKER_CONCURRENCY` | Number of concurrent workers | `10` | No (default: `5`) | | `SESSION_SECRET` | Session encryption key | `<generated-secret-256-bit>` | Yes | | `AWS_REGION` | AWS region for services | `us-east-1` | Yes | | `S3_BUCKET` | S3 bucket for file storage | `cloudflow-prod-storage` | Yes | | `SMTP_HOST` | Email server hostname | `smtp.sendgrid.net` | No | | `SMTP_PORT` | Email server port | `587` | No | | `METRICS_ENABLED` | Enable Prometheus metrics | `true` | No (default: `false`) | Create a Kubernetes secret for sensitive variables: ```bash kubectl create secret generic cloudflow-secrets \\ --namespace cloudflow-prod \\ --from-literal=DATABASE_URL=\"postgresql://cloudflow:$(cat db-password.txt)@pgbouncer.cloudflow-prod.svc.cluster.local:5432/cloudflow\" \\ --from-literal=REDIS_URL=\"redis://redis-master.cloudflow-prod.svc.cluster.local:6379\" \\ --from-literal=JWT_SECRET=\"$(openssl rand -base64 32)\" \\ --from-literal=SESSION_SECRET=\"$(openssl rand -base64 32)\" ``` ### Helm Chart Deployment Add the CloudFlow Helm repository:",
              "score": null
            },
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_17",
              "content": "namespace level: ```bash kubectl label namespace cloudflow-prod \\ pod-security.kubernetes.io/enforce=restricted \\ pod-security.kubernetes.io/audit=restricted \\ pod-security.kubernetes.io/warn=restricted ``` ### Secrets Management Use AWS Secrets Manager for sensitive credentials: ```bash # Install External Secrets Operator helm install external-secrets external-secrets/external-secrets \\ --namespace external-secrets-system \\ --create-namespace # Create SecretStore kubectl apply -f - <<EOF apiVersion: external-secrets.io/v1beta1 kind: SecretStore metadata: name: aws-secrets-manager namespace: cloudflow-prod spec: provider: aws: service: SecretsManager region: us-east-1 auth: jwt: serviceAccountRef: name: external-secrets-sa EOF ``` --- ## Troubleshooting ### Common Issues #### Pods in CrashLoopBackOff **Symptoms**: Pods continuously restart **Diagnosis**: ```bash kubectl logs -n cloudflow-prod <pod-name> --previous kubectl describe pod -n cloudflow-prod <pod-name> ``` **Common Causes**: - Database connection failure - Invalid environment variables - Insufficient resources #### High Memory Usage **Symptoms**: Pods being OOMKilled **Diagnosis**: ```bash kubectl top pods -n cloudflow-prod ``` **Resolution**: - Increase memory limits in deployment - Check for memory leaks in application logs - Review heap dump for Node.js processes",
              "score": null
            },
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_6",
              "content": "prometheus.io/scrape: \"true\" prometheus.io/port: \"9090\" prometheus.io/path: \"/metrics\" podSecurityContext: runAsNonRoot: true runAsUser: 1000 fsGroup: 1000 securityContext: allowPrivilegeEscalation: false capabilities: drop: - ALL readOnlyRootFilesystem: true persistence: enabled: true storageClass: gp3 accessMode: ReadWriteOnce size: 50Gi redis: enabled: true architecture: replication auth: enabled: true master: persistence: size: 20Gi replica: replicaCount: 2 persistence: size: 20Gi ``` Deploy CloudFlow using Helm: ```bash helm install cloudflow cloudflow/cloudflow \\ --namespace cloudflow-prod \\ --values values-production.yaml \\ --wait \\ --timeout 10m ``` ### Deployment Verification Verify the deployment status: ```bash # Check pod status kubectl get pods -n cloudflow-prod # Expected output: # NAME READY STATUS RESTARTS AGE # cloudflow-api-7d8f9c5b6d-4xk2p 1/1 Running 0 2m # cloudflow-api-7d8f9c5b6d-9hj5m 1/1 Running 0 2m # cloudflow-api-7d8f9c5b6d-tn8wq 1/1 Running 0 2m # Check service endpoints kubectl get svc -n cloudflow-prod # Check ingress kubectl get ingress -n cloudflow-prod ``` Test health endpoints: ```bash # Port forward for testing kubectl port-forward -n cloudflow-prod svc/cloudflow-api 8080:80 # Test",
              "score": null
            },
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_1",
              "content": "### Architecture Summary CloudFlow consists of the following components: - **API Server**: REST API for workflow management (Node.js/Express) - **Worker Service**: Background job processor (Node.js) - **Scheduler**: Cron-based task scheduler (Node.js) - **PostgreSQL**: Primary data store (version 14) - **Redis**: Cache and message queue (version 7.0) - **PgBouncer**: Database connection pooler ### Deployment Model - **Namespace**: `cloudflow-prod` - **Cluster**: AWS EKS 1.28 - **Region**: us-east-1 (primary), us-west-2 (disaster recovery) - **High Availability**: Multi-AZ deployment across 3 availability zones --- ## Prerequisites Before beginning the deployment process, ensure you have the following: ### Required Tools - `kubectl` (v1.28 or later) - `helm` (v3.12 or later) - `aws-cli` (v2.13 or later) - `eksctl` (v0.165 or later) - `terraform` (v1.6 or later) - for infrastructure provisioning ### Access Requirements - AWS account with appropriate IAM permissions - EKS cluster admin access - Container registry access (ECR) - Domain name and SSL certificates -",
              "score": null
            }
          ],
          "latency_ms": 6.04029800160788,
          "query_id": "realistic_019",
          "dimension": "negation",
          "query": "Why can't I find CloudFlow in the official Helm hub?"
        },
        {
          "key_facts": [
            "minimum scheduling interval is 1 minute",
            "The minimum scheduling interval is **1 minute**"
          ],
          "found_facts": [
            "The minimum scheduling interval is **1 minute**"
          ],
          "missed_facts": [
            "minimum scheduling interval is 1 minute"
          ],
          "coverage": 0.5,
          "expected_docs": [
            "user_guide"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "user_guide",
              "chunk_id": "user_guide_fix_11",
              "content": "| | `0 0 1 * *` | First day of every month at midnight | | `0 0 * * 0` | Every Sunday at midnight | | `0 9-17 * * 1-5` | Every hour from 9 AM to 5 PM, Monday-Friday | | `*/15 9-17 * * 1-5` | Every 15 minutes during business hours | **Important:** The minimum scheduling interval is **1 minute**. Expressions that evaluate to more frequent executions will be rejected. ### Timezone Handling All scheduled workflows run in **UTC by default**.",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_24",
              "content": "Operations** (P99 latency): - `GET /workflows/{id}`: < 50ms (cache hit), < 150ms (cache miss) - `POST /workflows`: < 200ms (includes validation and database write) - `POST /workflows/{id}/execute`: < 100ms (async, returns execution ID) - `GET /workflows/{id}/history`: < 300ms (paginated, 50 records per page) **Workflow Execution** (P99 latency): - Simple workflow (< 5 steps): < 2 seconds - Medium workflow (5-15 steps): < 5 seconds - Complex workflow (> 15 steps): < 15 seconds - Parallel execution: Linear scaling up to 10 concurrent branches **Database Operations** (P99 latency): - Simple SELECT: < 5ms - JOIN query: < 20ms - INSERT/UPDATE: < 10ms - Batch operations: < 100ms (batch size: 100 records) **Cache Operations** (P99 latency): - Redis GET: < 2ms - Redis SET: < 3ms - Redis DEL: < 2ms ### Throughput Capacity **API Gateway**: - Sustained: 10,000 requests per second - Peak: 25,000 requests per second (5-minute burst) - Rate",
              "score": null
            },
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_9",
              "content": "\"limit\": 50, \"offset\": 0, \"has_more\": true } } ``` ### Analytics #### Get Workflow Metrics Retrieve performance metrics and analytics for workflows. **Endpoint:** `GET /analytics/workflows/{workflow_id}` **Query Parameters:** - `period` (required): Time period (`1h`, `24h`, `7d`, `30d`) - `metrics` (optional): Comma-separated list of metrics **Available Metrics:** - `execution_count`: Total number of executions - `success_rate`: Percentage of successful executions - `avg_duration`: Average execution duration in milliseconds - `error_count`: Total number of errors - `throughput`: Records processed per hour **Example Request:** ```bash curl -H \"X-API-Key: YOUR_API_KEY\" \\ \"https://api.cloudflow.io/v2/analytics/workflows/wf_8x7k2m9p?period=7d&metrics=execution_count,success_rate,avg_duration\" ``` **Example Response (200 OK):** ```json { \"workflow_id\": \"wf_8x7k2m9p\", \"period\": \"7d\", \"start_date\": \"2026-01-17T10:30:00Z\", \"end_date\": \"2026-01-24T10:30:00Z\", \"metrics\": { \"execution_count\": 168, \"success_rate\": 97.6, \"avg_duration\": 3285, \"error_count\": 4, \"throughput\": 2847 }, \"timeseries\": [ { \"timestamp\": \"2026-01-17T00:00:00Z\", \"execution_count\": 24, \"success_rate\": 100.0 }, { \"timestamp\": \"2026-01-18T00:00:00Z\", \"execution_count\": 24, \"success_rate\": 95.8 } ] } ``` ## Error Handling CloudFlow uses standard HTTP status codes and returns detailed error information in JSON",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_7",
              "content": "\"retry\": {\"max_attempts\": 3, \"backoff\": \"exponential\"} } }, { \"id\": \"create-account\", \"type\": \"database_operation\", \"depends_on\": [\"validate-customer\"] } ] } ``` **State Management**: - Workflow states: PENDING, RUNNING, COMPLETED, FAILED, CANCELLED - Step states: QUEUED, EXECUTING, SUCCESS, FAILED, SKIPPED - State transitions stored in PostgreSQL with event sourcing pattern - Checkpointing every 10 steps for long-running workflows **Performance Targets**: - Simple workflow (3-5 steps): < 2 seconds (P99) - Complex workflow (20+ steps): < 5 seconds (P99) - Throughput: 500 concurrent workflow executions per pod --- ### Scheduler Service **Purpose**: Time-based workflow triggering system supporting cron-like schedules and one-time delayed executions.",
              "score": null
            },
            {
              "doc_id": "user_guide",
              "chunk_id": "user_guide_fix_12",
              "content": "To account for your local timezone: **Option 1: Convert to UTC** If you want a workflow to run at 9:00 AM EST (UTC-5), schedule it for 14:00 UTC: ``` 0 14 * * * # 9:00 AM EST = 14:00 UTC ``` **Option 2: Use Timezone Configuration** Specify a timezone in your workflow configuration: ```yaml schedule: cron: \"0 9 * * *\" timezone: \"America/New_York\" # IANA timezone identifier ``` **Supported Timezones:** CloudFlow supports all IANA timezone identifiers, including: - `America/New_York` (Eastern Time) - `America/Chicago` (Central Time) - `America/Los_Angeles` (Pacific Time) - `Europe/London` (GMT/BST) - `Asia/Tokyo` (Japan Standard Time) - `Australia/Sydney` (Australian Eastern Time) **Daylight Saving Time:** When using timezone configuration, CloudFlow automatically handles DST transitions. A workflow scheduled for 9:00 AM will run at 9:00 AM local time regardless of DST changes. ### Schedule Management **Creating a Schedule:** 1. Open your workflow in the editor 2.",
              "score": null
            }
          ],
          "latency_ms": 5.496773999766447,
          "query_id": "realistic_020",
          "dimension": "original",
          "query": "What is the minimum scheduling interval for workflows?"
        },
        {
          "key_facts": [
            "minimum scheduling interval is 1 minute",
            "The minimum scheduling interval is **1 minute**"
          ],
          "found_facts": [
            "The minimum scheduling interval is **1 minute**"
          ],
          "missed_facts": [
            "minimum scheduling interval is 1 minute"
          ],
          "coverage": 0.5,
          "expected_docs": [
            "user_guide"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "user_guide",
              "chunk_id": "user_guide_fix_11",
              "content": "| | `0 0 1 * *` | First day of every month at midnight | | `0 0 * * 0` | Every Sunday at midnight | | `0 9-17 * * 1-5` | Every hour from 9 AM to 5 PM, Monday-Friday | | `*/15 9-17 * * 1-5` | Every 15 minutes during business hours | **Important:** The minimum scheduling interval is **1 minute**. Expressions that evaluate to more frequent executions will be rejected. ### Timezone Handling All scheduled workflows run in **UTC by default**.",
              "score": null
            },
            {
              "doc_id": "user_guide",
              "chunk_id": "user_guide_fix_21",
              "content": "Keep Workflows Modular Break complex workflows into smaller, reusable components: - Use sub-workflows for repeated logic - Trigger child workflows via webhooks - Share common configurations via templates ### 8. Test Thoroughly Before activating a workflow: 1. Use test mode with sample data 2. Verify all actions execute correctly 3. Test error handling paths 4. Review execution logs 5. Start with a limited scope (e.g., test channel, small dataset) ### 9. Document Your Workflows Add descriptions to workflows and steps: ```yaml workflow: name: \"Daily Sales Report\" description: | Generates a daily sales report and distributes it to the sales team. Runs at 8:00 AM EST Monday-Friday. Data source: PostgreSQL sales database Recipients: sales-team@company.com steps: - id: fetch_sales name: \"Fetch Yesterday's Sales\" description: \"Query database for all completed orders from previous day\" ``` ### 10.",
              "score": null
            },
            {
              "doc_id": "user_guide",
              "chunk_id": "user_guide_fix_12",
              "content": "To account for your local timezone: **Option 1: Convert to UTC** If you want a workflow to run at 9:00 AM EST (UTC-5), schedule it for 14:00 UTC: ``` 0 14 * * * # 9:00 AM EST = 14:00 UTC ``` **Option 2: Use Timezone Configuration** Specify a timezone in your workflow configuration: ```yaml schedule: cron: \"0 9 * * *\" timezone: \"America/New_York\" # IANA timezone identifier ``` **Supported Timezones:** CloudFlow supports all IANA timezone identifiers, including: - `America/New_York` (Eastern Time) - `America/Chicago` (Central Time) - `America/Los_Angeles` (Pacific Time) - `Europe/London` (GMT/BST) - `Asia/Tokyo` (Japan Standard Time) - `Australia/Sydney` (Australian Eastern Time) **Daylight Saving Time:** When using timezone configuration, CloudFlow automatically handles DST transitions. A workflow scheduled for 9:00 AM will run at 9:00 AM local time regardless of DST changes. ### Schedule Management **Creating a Schedule:** 1. Open your workflow in the editor 2.",
              "score": null
            },
            {
              "doc_id": "user_guide",
              "chunk_id": "user_guide_fix_13",
              "content": "Click the **\"Trigger\"** section 3. Select **\"Schedule\"** as the trigger type 4. Enter your cron expression or use the visual schedule builder 5. Select your timezone 6. Save and activate **Testing Schedules:** Use the built-in schedule calculator to preview upcoming executions: ``` Next 5 executions: 1. 2026-01-24 14:00:00 UTC 2. 2026-01-25 14:00:00 UTC 3. 2026-01-26 14:00:00 UTC 4. 2026-01-27 14:00:00 UTC 5. 2026-01-28 14:00:00 UTC ``` ## Error Handling Robust error handling ensures your workflows are resilient and reliable.",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_7",
              "content": "\"retry\": {\"max_attempts\": 3, \"backoff\": \"exponential\"} } }, { \"id\": \"create-account\", \"type\": \"database_operation\", \"depends_on\": [\"validate-customer\"] } ] } ``` **State Management**: - Workflow states: PENDING, RUNNING, COMPLETED, FAILED, CANCELLED - Step states: QUEUED, EXECUTING, SUCCESS, FAILED, SKIPPED - State transitions stored in PostgreSQL with event sourcing pattern - Checkpointing every 10 steps for long-running workflows **Performance Targets**: - Simple workflow (3-5 steps): < 2 seconds (P99) - Complex workflow (20+ steps): < 5 seconds (P99) - Throughput: 500 concurrent workflow executions per pod --- ### Scheduler Service **Purpose**: Time-based workflow triggering system supporting cron-like schedules and one-time delayed executions.",
              "score": null
            }
          ],
          "latency_ms": 5.412126000010176,
          "query_id": "realistic_020",
          "dimension": "synonym",
          "query": "How frequently can I schedule a workflow?"
        },
        {
          "key_facts": [
            "minimum scheduling interval is 1 minute",
            "The minimum scheduling interval is **1 minute**"
          ],
          "found_facts": [
            "The minimum scheduling interval is **1 minute**"
          ],
          "missed_facts": [
            "minimum scheduling interval is 1 minute"
          ],
          "coverage": 0.5,
          "expected_docs": [
            "user_guide"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "user_guide",
              "chunk_id": "user_guide_fix_11",
              "content": "| | `0 0 1 * *` | First day of every month at midnight | | `0 0 * * 0` | Every Sunday at midnight | | `0 9-17 * * 1-5` | Every hour from 9 AM to 5 PM, Monday-Friday | | `*/15 9-17 * * 1-5` | Every 15 minutes during business hours | **Important:** The minimum scheduling interval is **1 minute**. Expressions that evaluate to more frequent executions will be rejected. ### Timezone Handling All scheduled workflows run in **UTC by default**.",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_9",
              "content": "to detect leader failure (5-second interval) - Automatic failover to standby scheduler (< 10 seconds) - Schedule versioning to handle updates during execution - Missed execution policy: SKIP, RUN_ONCE, or RUN_ALL **Performance Targets**: - Schedule evaluation: < 100ms per cycle - Accuracy: \u00b1 1 second for schedule triggers - Capacity: 100,000 active schedules --- ### Notification Service **Purpose**: Multi-channel notification delivery system supporting email, SMS, webhooks, and in-app notifications. **Technology**: Node.js with worker pool pattern **Replicas**: 8 pods (production), auto-scaling 6-16 **Resource Allocation**: 2 vCPU, 4GB RAM per pod **Key Responsibilities**: - Consume notification events from Kafka - Template rendering with Handlebars (cached in Redis) - Multi-channel delivery (Email via SendGrid, SMS via Twilio, Webhooks) - Retry logic with dead letter queue for failed deliveries - Delivery status tracking and analytics - User preference management (opt-in/opt-out) **Notification Flow**: ``` Event Source \u2192 Kafka (notifications.email topic) \u2502 \u25bc Notification Service Consumer",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_3",
              "content": "**Diagnosis:** ```bash # Check system time timedatectl status # Compare with NTP server ntpdate -q pool.ntp.org # Check JWT issued time vs current time jwt_iat=$(echo $CF_ACCESS_TOKEN | cut -d'.' -f2 | base64 -d | jq '.iat') current_time=$(date +%s) skew=$((current_time - jwt_iat)) echo \"Clock skew: $skew seconds\" ``` **Resolution:** ```bash # Sync with NTP server sudo ntpdate -s pool.ntp.org # Enable automatic time sync sudo timedatectl set-ntp true # Restart CloudFlow client cloudflow restart ``` **4. Insufficient Permissions** Error: `User does not have required permissions for this operation` **Check user roles:** ```bash cloudflow auth whoami --verbose # Expected output: # User: john.doe@company.com # Roles: developer, workflow-admin # Scopes: workflow:*, data:read, metrics:read ``` **Request permission elevation:** ```bash # Submit access request cloudflow auth request-access \\ --resource \"workflow:production:deploy\" \\ --justification \"Deploy critical hotfix for TICKET-1234\" # Check pending approvals cloudflow auth list-requests --status pending ``` ### 403 Forbidden Errors #### RBAC Policy Violations",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_8",
              "content": "**Technology**: Go with distributed locking via Redis **Replicas**: 4 pods (production), active-passive with leader election **Resource Allocation**: 2 vCPU, 4GB RAM per pod **Key Responsibilities**: - Parse and validate cron expressions (extended format supporting seconds) - Maintain schedule registry in PostgreSQL - Distributed scheduling with leader election (one active scheduler) - Missed execution handling with configurable catch-up policy - Schedule conflict detection and resolution - Time zone support for international schedules **Scheduling Architecture**: ``` PostgreSQL Schedules Table \u2502 \u25bc Leader Scheduler (elected via Redis) \u2502 \u251c\u2500 Scan for due schedules (every 10 seconds) \u251c\u2500 Acquire lock per schedule (prevents duplicates) \u251c\u2500 Publish to Kafka \u2192 workflow.events topic \u2514\u2500 Update last_run timestamp ``` **Schedule Types**: - **Cron-based**: `0 */5 * * * *` (every 5 minutes) - **One-time**: Specific timestamp for delayed execution - **Interval-based**: Every N seconds/minutes/hours **Reliability Features**: - Leader election using Redis with 30-second lease - Heartbeat mechanism",
              "score": null
            },
            {
              "doc_id": "user_guide",
              "chunk_id": "user_guide_fix_13",
              "content": "Click the **\"Trigger\"** section 3. Select **\"Schedule\"** as the trigger type 4. Enter your cron expression or use the visual schedule builder 5. Select your timezone 6. Save and activate **Testing Schedules:** Use the built-in schedule calculator to preview upcoming executions: ``` Next 5 executions: 1. 2026-01-24 14:00:00 UTC 2. 2026-01-25 14:00:00 UTC 3. 2026-01-26 14:00:00 UTC 4. 2026-01-27 14:00:00 UTC 5. 2026-01-28 14:00:00 UTC ``` ## Error Handling Robust error handling ensures your workflows are resilient and reliable.",
              "score": null
            }
          ],
          "latency_ms": 5.481625999891548,
          "query_id": "realistic_020",
          "dimension": "problem",
          "query": "My every-30-seconds schedule is being rejected"
        },
        {
          "key_facts": [
            "minimum scheduling interval is 1 minute",
            "The minimum scheduling interval is **1 minute**"
          ],
          "found_facts": [
            "The minimum scheduling interval is **1 minute**"
          ],
          "missed_facts": [
            "minimum scheduling interval is 1 minute"
          ],
          "coverage": 0.5,
          "expected_docs": [
            "user_guide"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "user_guide",
              "chunk_id": "user_guide_fix_11",
              "content": "| | `0 0 1 * *` | First day of every month at midnight | | `0 0 * * 0` | Every Sunday at midnight | | `0 9-17 * * 1-5` | Every hour from 9 AM to 5 PM, Monday-Friday | | `*/15 9-17 * * 1-5` | Every 15 minutes during business hours | **Important:** The minimum scheduling interval is **1 minute**. Expressions that evaluate to more frequent executions will be rejected. ### Timezone Handling All scheduled workflows run in **UTC by default**.",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_8",
              "content": "**Technology**: Go with distributed locking via Redis **Replicas**: 4 pods (production), active-passive with leader election **Resource Allocation**: 2 vCPU, 4GB RAM per pod **Key Responsibilities**: - Parse and validate cron expressions (extended format supporting seconds) - Maintain schedule registry in PostgreSQL - Distributed scheduling with leader election (one active scheduler) - Missed execution handling with configurable catch-up policy - Schedule conflict detection and resolution - Time zone support for international schedules **Scheduling Architecture**: ``` PostgreSQL Schedules Table \u2502 \u25bc Leader Scheduler (elected via Redis) \u2502 \u251c\u2500 Scan for due schedules (every 10 seconds) \u251c\u2500 Acquire lock per schedule (prevents duplicates) \u251c\u2500 Publish to Kafka \u2192 workflow.events topic \u2514\u2500 Update last_run timestamp ``` **Schedule Types**: - **Cron-based**: `0 */5 * * * *` (every 5 minutes) - **One-time**: Specific timestamp for delayed execution - **Interval-based**: Every N seconds/minutes/hours **Reliability Features**: - Leader election using Redis with 30-second lease - Heartbeat mechanism",
              "score": null
            },
            {
              "doc_id": "user_guide",
              "chunk_id": "user_guide_fix_13",
              "content": "Click the **\"Trigger\"** section 3. Select **\"Schedule\"** as the trigger type 4. Enter your cron expression or use the visual schedule builder 5. Select your timezone 6. Save and activate **Testing Schedules:** Use the built-in schedule calculator to preview upcoming executions: ``` Next 5 executions: 1. 2026-01-24 14:00:00 UTC 2. 2026-01-25 14:00:00 UTC 3. 2026-01-26 14:00:00 UTC 4. 2026-01-27 14:00:00 UTC 5. 2026-01-28 14:00:00 UTC ``` ## Error Handling Robust error handling ensures your workflows are resilient and reliable.",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_9",
              "content": "to detect leader failure (5-second interval) - Automatic failover to standby scheduler (< 10 seconds) - Schedule versioning to handle updates during execution - Missed execution policy: SKIP, RUN_ONCE, or RUN_ALL **Performance Targets**: - Schedule evaluation: < 100ms per cycle - Accuracy: \u00b1 1 second for schedule triggers - Capacity: 100,000 active schedules --- ### Notification Service **Purpose**: Multi-channel notification delivery system supporting email, SMS, webhooks, and in-app notifications. **Technology**: Node.js with worker pool pattern **Replicas**: 8 pods (production), auto-scaling 6-16 **Resource Allocation**: 2 vCPU, 4GB RAM per pod **Key Responsibilities**: - Consume notification events from Kafka - Template rendering with Handlebars (cached in Redis) - Multi-channel delivery (Email via SendGrid, SMS via Twilio, Webhooks) - Retry logic with dead letter queue for failed deliveries - Delivery status tracking and analytics - User preference management (opt-in/opt-out) **Notification Flow**: ``` Event Source \u2192 Kafka (notifications.email topic) \u2502 \u25bc Notification Service Consumer",
              "score": null
            },
            {
              "doc_id": "user_guide",
              "chunk_id": "user_guide_fix_10",
              "content": "Status: {{if(steps.check_inventory.in_stock, \"\u2713 Available\", \"\u2717 Out of Stock\")}} ``` **Date calculations:** ``` Due Date: {{format_date(add_days(now(), 7), \"MMMM DD, YYYY\")}} ``` **Dynamic URLs:** ``` https://api.example.com/v1/users/{{user_id}}/orders?status={{status}}&limit={{default(limit, 10)}} ``` ## Scheduling CloudFlow supports powerful scheduling options for recurring workflows. ### Cron Syntax Use standard cron expressions to define schedules: ``` * * * * * \u252c \u252c \u252c \u252c \u252c \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2514\u2500\u2500\u2500 Day of Week (0-6, Sunday=0) \u2502 \u2502 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Month (1-12) \u2502 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Day of Month (1-31) \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Hour (0-23) \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Minute (0-59) ``` **Common Cron Patterns:** | Pattern | Description | |---------|-------------| | `*/5 * * * *` | Every 5 minutes | | `0 * * * *` | Every hour at minute 0 | | `0 9 * * *` | Daily at 9:00 AM | | `0 9 * * 1` | Every Monday at 9:00 AM",
              "score": null
            }
          ],
          "latency_ms": 5.27157499891473,
          "query_id": "realistic_020",
          "dimension": "casual",
          "query": "min schedule interval"
        },
        {
          "key_facts": [
            "minimum scheduling interval is 1 minute",
            "The minimum scheduling interval is **1 minute**"
          ],
          "found_facts": [
            "The minimum scheduling interval is **1 minute**"
          ],
          "missed_facts": [
            "minimum scheduling interval is 1 minute"
          ],
          "coverage": 0.5,
          "expected_docs": [
            "user_guide"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "user_guide",
              "chunk_id": "user_guide_fix_13",
              "content": "Click the **\"Trigger\"** section 3. Select **\"Schedule\"** as the trigger type 4. Enter your cron expression or use the visual schedule builder 5. Select your timezone 6. Save and activate **Testing Schedules:** Use the built-in schedule calculator to preview upcoming executions: ``` Next 5 executions: 1. 2026-01-24 14:00:00 UTC 2. 2026-01-25 14:00:00 UTC 3. 2026-01-26 14:00:00 UTC 4. 2026-01-27 14:00:00 UTC 5. 2026-01-28 14:00:00 UTC ``` ## Error Handling Robust error handling ensures your workflows are resilient and reliable.",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_8",
              "content": "**Technology**: Go with distributed locking via Redis **Replicas**: 4 pods (production), active-passive with leader election **Resource Allocation**: 2 vCPU, 4GB RAM per pod **Key Responsibilities**: - Parse and validate cron expressions (extended format supporting seconds) - Maintain schedule registry in PostgreSQL - Distributed scheduling with leader election (one active scheduler) - Missed execution handling with configurable catch-up policy - Schedule conflict detection and resolution - Time zone support for international schedules **Scheduling Architecture**: ``` PostgreSQL Schedules Table \u2502 \u25bc Leader Scheduler (elected via Redis) \u2502 \u251c\u2500 Scan for due schedules (every 10 seconds) \u251c\u2500 Acquire lock per schedule (prevents duplicates) \u251c\u2500 Publish to Kafka \u2192 workflow.events topic \u2514\u2500 Update last_run timestamp ``` **Schedule Types**: - **Cron-based**: `0 */5 * * * *` (every 5 minutes) - **One-time**: Specific timestamp for delayed execution - **Interval-based**: Every N seconds/minutes/hours **Reliability Features**: - Leader election using Redis with 30-second lease - Heartbeat mechanism",
              "score": null
            },
            {
              "doc_id": "user_guide",
              "chunk_id": "user_guide_fix_11",
              "content": "| | `0 0 1 * *` | First day of every month at midnight | | `0 0 * * 0` | Every Sunday at midnight | | `0 9-17 * * 1-5` | Every hour from 9 AM to 5 PM, Monday-Friday | | `*/15 9-17 * * 1-5` | Every 15 minutes during business hours | **Important:** The minimum scheduling interval is **1 minute**. Expressions that evaluate to more frequent executions will be rejected. ### Timezone Handling All scheduled workflows run in **UTC by default**.",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_9",
              "content": "to detect leader failure (5-second interval) - Automatic failover to standby scheduler (< 10 seconds) - Schedule versioning to handle updates during execution - Missed execution policy: SKIP, RUN_ONCE, or RUN_ALL **Performance Targets**: - Schedule evaluation: < 100ms per cycle - Accuracy: \u00b1 1 second for schedule triggers - Capacity: 100,000 active schedules --- ### Notification Service **Purpose**: Multi-channel notification delivery system supporting email, SMS, webhooks, and in-app notifications. **Technology**: Node.js with worker pool pattern **Replicas**: 8 pods (production), auto-scaling 6-16 **Resource Allocation**: 2 vCPU, 4GB RAM per pod **Key Responsibilities**: - Consume notification events from Kafka - Template rendering with Handlebars (cached in Redis) - Multi-channel delivery (Email via SendGrid, SMS via Twilio, Webhooks) - Retry logic with dead letter queue for failed deliveries - Delivery status tracking and analytics - User preference management (opt-in/opt-out) **Notification Flow**: ``` Event Source \u2192 Kafka (notifications.email topic) \u2502 \u25bc Notification Service Consumer",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_24",
              "content": "Operations** (P99 latency): - `GET /workflows/{id}`: < 50ms (cache hit), < 150ms (cache miss) - `POST /workflows`: < 200ms (includes validation and database write) - `POST /workflows/{id}/execute`: < 100ms (async, returns execution ID) - `GET /workflows/{id}/history`: < 300ms (paginated, 50 records per page) **Workflow Execution** (P99 latency): - Simple workflow (< 5 steps): < 2 seconds - Medium workflow (5-15 steps): < 5 seconds - Complex workflow (> 15 steps): < 15 seconds - Parallel execution: Linear scaling up to 10 concurrent branches **Database Operations** (P99 latency): - Simple SELECT: < 5ms - JOIN query: < 20ms - INSERT/UPDATE: < 10ms - Batch operations: < 100ms (batch size: 100 records) **Cache Operations** (P99 latency): - Redis GET: < 2ms - Redis SET: < 3ms - Redis DEL: < 2ms ### Throughput Capacity **API Gateway**: - Sustained: 10,000 requests per second - Peak: 25,000 requests per second (5-minute burst) - Rate",
              "score": null
            }
          ],
          "latency_ms": 5.404301000453415,
          "query_id": "realistic_020",
          "dimension": "contextual",
          "query": "I need near real-time execution, what's the fastest schedule I can set?"
        },
        {
          "key_facts": [
            "minimum scheduling interval is 1 minute",
            "The minimum scheduling interval is **1 minute**"
          ],
          "found_facts": [
            "The minimum scheduling interval is **1 minute**"
          ],
          "missed_facts": [
            "minimum scheduling interval is 1 minute"
          ],
          "coverage": 0.5,
          "expected_docs": [
            "user_guide"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "user_guide",
              "chunk_id": "user_guide_fix_11",
              "content": "| | `0 0 1 * *` | First day of every month at midnight | | `0 0 * * 0` | Every Sunday at midnight | | `0 9-17 * * 1-5` | Every hour from 9 AM to 5 PM, Monday-Friday | | `*/15 9-17 * * 1-5` | Every 15 minutes during business hours | **Important:** The minimum scheduling interval is **1 minute**. Expressions that evaluate to more frequent executions will be rejected. ### Timezone Handling All scheduled workflows run in **UTC by default**.",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_14",
              "content": "Split workflow into smaller workflows:** ```bash # Create sub-workflows cloudflow workflows create data-pipeline-part1 \\ --steps \"data_ingestion,data_validation\" \\ --timeout 1800 cloudflow workflows create data-pipeline-part2 \\ --steps \"data_transformation,data_export\" \\ --timeout 3600 \\ --trigger workflow_completed \\ --trigger-workflow data-pipeline-part1 ``` ### Retry Logic and Exponential Backoff CloudFlow implements automatic retry with exponential backoff for transient failures: - Max retries: 3 - Initial delay: 1 second - Backoff multiplier: 2 - Max delay: 60 seconds #### Retry Sequence ``` Attempt 1: Immediate Attempt 2: Wait 1s (2^0 * 1s) Attempt 3: Wait 2s (2^1 * 1s) Attempt 4: Wait 4s (2^2 * 1s) ``` #### Configuration ```bash # View current retry settings cloudflow workflows get wf_9k2n4m8p1q --format json | jq '.retry_policy' # Customize retry behavior cloudflow workflows update wf_9k2n4m8p1q \\ --retry-max-attempts 5 \\ --retry-initial-delay 2000 \\ --retry-backoff-multiplier 2 \\ --retry-max-delay 120000 \\ --retry-on-errors \"NETWORK_ERROR,TIMEOUT,RATE_LIMIT\" # Disable retry for specific step cloudflow workflows update wf_9k2n4m8p1q \\",
              "score": null
            },
            {
              "doc_id": "user_guide",
              "chunk_id": "user_guide_fix_17",
              "content": "### Execution Timeout - **Default**: 3600 seconds (60 minutes) - **Behavior**: Workflows exceeding this timeout are automatically terminated - **Custom Timeouts**: Enterprise plans can request custom timeout limits **Setting Step-Level Timeouts:** ```yaml - id: long_running_task action: http_request config: url: \"https://api.example.com/process\" timeout: 300 # 5 minutes for this specific step ``` ### Execution Limits - **Maximum**: 1000 executions per day (per workflow) - **Rate Limiting**: 100 concurrent executions per workflow - **Burst Limit**: 10 executions per second **What happens when limits are reached:** - New executions are queued automatically - Webhook triggers return HTTP 429 (Too Many Requests) - Scheduled executions are skipped (logged in audit trail) - Email notifications sent to workflow owner **Monitoring Usage:** View real-time metrics in your workflow dashboard: - Executions today: 847 / 1000 - Average duration: 12.3s - Success rate: 98.2% - Current queue depth: 3 ### Data Limits - **Maximum request/response size**: 10MB per",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_7",
              "content": "\"retry\": {\"max_attempts\": 3, \"backoff\": \"exponential\"} } }, { \"id\": \"create-account\", \"type\": \"database_operation\", \"depends_on\": [\"validate-customer\"] } ] } ``` **State Management**: - Workflow states: PENDING, RUNNING, COMPLETED, FAILED, CANCELLED - Step states: QUEUED, EXECUTING, SUCCESS, FAILED, SKIPPED - State transitions stored in PostgreSQL with event sourcing pattern - Checkpointing every 10 steps for long-running workflows **Performance Targets**: - Simple workflow (3-5 steps): < 2 seconds (P99) - Complex workflow (20+ steps): < 5 seconds (P99) - Throughput: 500 concurrent workflow executions per pod --- ### Scheduler Service **Purpose**: Time-based workflow triggering system supporting cron-like schedules and one-time delayed executions.",
              "score": null
            },
            {
              "doc_id": "user_guide",
              "chunk_id": "user_guide_fix_12",
              "content": "To account for your local timezone: **Option 1: Convert to UTC** If you want a workflow to run at 9:00 AM EST (UTC-5), schedule it for 14:00 UTC: ``` 0 14 * * * # 9:00 AM EST = 14:00 UTC ``` **Option 2: Use Timezone Configuration** Specify a timezone in your workflow configuration: ```yaml schedule: cron: \"0 9 * * *\" timezone: \"America/New_York\" # IANA timezone identifier ``` **Supported Timezones:** CloudFlow supports all IANA timezone identifiers, including: - `America/New_York` (Eastern Time) - `America/Chicago` (Central Time) - `America/Los_Angeles` (Pacific Time) - `Europe/London` (GMT/BST) - `Asia/Tokyo` (Japan Standard Time) - `Australia/Sydney` (Australian Eastern Time) **Daylight Saving Time:** When using timezone configuration, CloudFlow automatically handles DST transitions. A workflow scheduled for 9:00 AM will run at 9:00 AM local time regardless of DST changes. ### Schedule Management **Creating a Schedule:** 1. Open your workflow in the editor 2.",
              "score": null
            }
          ],
          "latency_ms": 5.504819000634598,
          "query_id": "realistic_020",
          "dimension": "negation",
          "query": "Why can't I schedule workflows every 30 seconds?"
        }
      ]
    }
  ]
}