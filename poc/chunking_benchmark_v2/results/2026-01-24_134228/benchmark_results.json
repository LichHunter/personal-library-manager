{
  "metadata": {
    "timestamp": "2026-01-24_134228",
    "num_documents": 5,
    "num_queries": 20,
    "total_facts": 53,
    "has_human_queries": true
  },
  "evaluations": [
    {
      "strategy": "semantic",
      "chunking": "fixed_100_0pct",
      "embedder": "all-MiniLM-L6-v2",
      "reranker": null,
      "llm": null,
      "k": 5,
      "metric": "exact_match",
      "num_chunks": 241,
      "index_time_s": 0.501951011001438,
      "aggregate": {
        "original": {
          "coverage": 0.6226415094339622,
          "found": 33,
          "total": 53,
          "avg_latency_ms": 3.6409506499694544,
          "p95_latency_ms": 4.178717901049828
        },
        "synonym": {
          "coverage": 0.5283018867924528,
          "found": 28,
          "total": 53,
          "avg_latency_ms": 3.167344050052634,
          "p95_latency_ms": 3.418022850109992
        },
        "problem": {
          "coverage": 0.37735849056603776,
          "found": 20,
          "total": 53,
          "avg_latency_ms": 3.1674201500209165,
          "p95_latency_ms": 3.406302950497775
        },
        "casual": {
          "coverage": 0.5471698113207547,
          "found": 29,
          "total": 53,
          "avg_latency_ms": 3.1397284497870714,
          "p95_latency_ms": 3.399440550674626
        },
        "contextual": {
          "coverage": 0.5471698113207547,
          "found": 29,
          "total": 53,
          "avg_latency_ms": 3.247274949808343,
          "p95_latency_ms": 3.4254673991199525
        },
        "negation": {
          "coverage": 0.39622641509433965,
          "found": 21,
          "total": 53,
          "avg_latency_ms": 3.1659328500609263,
          "p95_latency_ms": 3.3746085995517205
        }
      },
      "per_query": [
        {
          "key_facts": [
            "100 requests per minute per authenticated user",
            "20 requests per minute for unauthenticated requests"
          ],
          "found_facts": [
            "100 requests per minute per authenticated user",
            "20 requests per minute for unauthenticated requests"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "api_reference"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_49",
              "content": "limiting: 1,000 requests per minute per API key **Workflow Engine**: - Concurrent executions: 8,000 workflows (across 16 pods) - Execution start rate: 500 per second - Completion rate: 450 per second (average 2-second execution time) **Database**: - Read throughput: 50,000 queries per second (across replicas) - Write throughput: 15,000 transactions per second - Connection capacity: 2,000 concurrent connections **Kafka**: - Message ingestion: 100,000 messages per second - Consumer throughput: 80,000 messages per second (aggregated) -",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_32",
              "content": "Limiting & Throttling ### 429 Too Many Requests #### Error Response ```json { \"error\": \"rate_limit_exceeded\", \"message\": \"API rate limit exceeded. Retry after 45 seconds.\", \"status\": 429, \"headers\": { \"X-RateLimit-Limit\": \"1000\", \"X-RateLimit-Remaining\": \"0\", \"X-RateLimit-Reset\": \"1706112345\", \"Retry-After\": \"45\" } } ``` #### Rate Limit Tiers CloudFlow enforces the following rate limits per workspace: | Tier | Requests/Minute | Requests/Hour | Concurrent Workflows | |------|-----------------|---------------|----------------------| | Free | 60 | 1,000 | 5 | | Standard | 1,000",
              "score": null
            },
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_5",
              "content": "tokens expire after 3600 seconds (1 hour). Implement token refresh logic in your application. ## Rate Limiting To ensure fair usage and system stability, CloudFlow enforces rate limits on all API endpoints. **Default Limits:** - 100 requests per minute per authenticated user - 20 requests per minute for unauthenticated requests - Burst allowance: 150 requests in a 10-second window ### Rate Limit Headers Every API response includes rate limit information: ``` X-RateLimit-Limit: 100 X-RateLimit-Remaining: 87",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_34",
              "content": "rate limit usage cloudflow metrics query \\ --metric rate_limit_remaining \\ --workspace ws_abc123 \\ --last 1h \\ --interval 1m ``` #### Handling Rate Limits in Code **Python example with retry logic:** ```python import time import requests def cloudflow_api_call_with_retry(url, headers, max_retries=3): for attempt in range(max_retries): response = requests.get(url, headers=headers) if response.status_code == 429: retry_after = int(response.headers.get('Retry-After', 60)) print(f\"Rate limited.",
              "score": null
            },
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_21",
              "content": "JSON - `duplicate_resource`: Resource with same identifier already exists - `quota_exceeded`: Account quota limit reached ### Error Handling Best Practices **Python Example:** ```python import requests import time def make_api_request(url, headers, max_retries=3): for attempt in range(max_retries): try: response = requests.get(url, headers=headers) if response.status_code == 200: return response.json() elif response.status_code == 429: # Rate limit exceeded retry_after = int(response.headers.get('X-RateLimit-Reset', 60)) print(f\"Rate limited.",
              "score": null
            }
          ],
          "latency_ms": 12.262132999239839,
          "query_id": "realistic_001",
          "dimension": "original",
          "query": "What is the API rate limit per minute?"
        },
        {
          "key_facts": [
            "100 requests per minute per authenticated user",
            "20 requests per minute for unauthenticated requests"
          ],
          "found_facts": [
            "100 requests per minute per authenticated user",
            "20 requests per minute for unauthenticated requests"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "api_reference"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_49",
              "content": "limiting: 1,000 requests per minute per API key **Workflow Engine**: - Concurrent executions: 8,000 workflows (across 16 pods) - Execution start rate: 500 per second - Completion rate: 450 per second (average 2-second execution time) **Database**: - Read throughput: 50,000 queries per second (across replicas) - Write throughput: 15,000 transactions per second - Connection capacity: 2,000 concurrent connections **Kafka**: - Message ingestion: 100,000 messages per second - Consumer throughput: 80,000 messages per second (aggregated) -",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_32",
              "content": "Limiting & Throttling ### 429 Too Many Requests #### Error Response ```json { \"error\": \"rate_limit_exceeded\", \"message\": \"API rate limit exceeded. Retry after 45 seconds.\", \"status\": 429, \"headers\": { \"X-RateLimit-Limit\": \"1000\", \"X-RateLimit-Remaining\": \"0\", \"X-RateLimit-Reset\": \"1706112345\", \"Retry-After\": \"45\" } } ``` #### Rate Limit Tiers CloudFlow enforces the following rate limits per workspace: | Tier | Requests/Minute | Requests/Hour | Concurrent Workflows | |------|-----------------|---------------|----------------------| | Free | 60 | 1,000 | 5 | | Standard | 1,000",
              "score": null
            },
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_5",
              "content": "tokens expire after 3600 seconds (1 hour). Implement token refresh logic in your application. ## Rate Limiting To ensure fair usage and system stability, CloudFlow enforces rate limits on all API endpoints. **Default Limits:** - 100 requests per minute per authenticated user - 20 requests per minute for unauthenticated requests - Burst allowance: 150 requests in a 10-second window ### Rate Limit Headers Every API response includes rate limit information: ``` X-RateLimit-Limit: 100 X-RateLimit-Remaining: 87",
              "score": null
            },
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_21",
              "content": "JSON - `duplicate_resource`: Resource with same identifier already exists - `quota_exceeded`: Account quota limit reached ### Error Handling Best Practices **Python Example:** ```python import requests import time def make_api_request(url, headers, max_retries=3): for attempt in range(max_retries): try: response = requests.get(url, headers=headers) if response.status_code == 200: return response.json() elif response.status_code == 429: # Rate limit exceeded retry_after = int(response.headers.get('X-RateLimit-Reset', 60)) print(f\"Rate limited.",
              "score": null
            },
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_6",
              "content": "X-RateLimit-Reset: 1640995200 ``` When you exceed the rate limit, you'll receive a `429 Too Many Requests` response: ```json { \"error\": { \"code\": \"rate_limit_exceeded\", \"message\": \"Rate limit exceeded. Please retry after 42 seconds.\", \"retry_after\": 42 } } ``` **Best Practices:** - Monitor `X-RateLimit-Remaining` header values - Implement exponential backoff when receiving 429 responses - Cache responses when appropriate to reduce API calls - Consider upgrading to Enterprise tier for higher limits (1000 req/min) ## Pagination List",
              "score": null
            }
          ],
          "latency_ms": 3.5694119997060625,
          "query_id": "realistic_001",
          "dimension": "synonym",
          "query": "How many requests can I make per minute to the API?"
        },
        {
          "key_facts": [
            "100 requests per minute per authenticated user",
            "20 requests per minute for unauthenticated requests"
          ],
          "found_facts": [],
          "missed_facts": [
            "100 requests per minute per authenticated user",
            "20 requests per minute for unauthenticated requests"
          ],
          "coverage": 0.0,
          "expected_docs": [
            "api_reference"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_32",
              "content": "Limiting & Throttling ### 429 Too Many Requests #### Error Response ```json { \"error\": \"rate_limit_exceeded\", \"message\": \"API rate limit exceeded. Retry after 45 seconds.\", \"status\": 429, \"headers\": { \"X-RateLimit-Limit\": \"1000\", \"X-RateLimit-Remaining\": \"0\", \"X-RateLimit-Reset\": \"1706112345\", \"Retry-After\": \"45\" } } ``` #### Rate Limit Tiers CloudFlow enforces the following rate limits per workspace: | Tier | Requests/Minute | Requests/Hour | Concurrent Workflows | |------|-----------------|---------------|----------------------| | Free | 60 | 1,000 | 5 | | Standard | 1,000",
              "score": null
            },
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_6",
              "content": "X-RateLimit-Reset: 1640995200 ``` When you exceed the rate limit, you'll receive a `429 Too Many Requests` response: ```json { \"error\": { \"code\": \"rate_limit_exceeded\", \"message\": \"Rate limit exceeded. Please retry after 42 seconds.\", \"retry_after\": 42 } } ``` **Best Practices:** - Monitor `X-RateLimit-Remaining` header values - Implement exponential backoff when receiving 429 responses - Cache responses when appropriate to reduce API calls - Consider upgrading to Enterprise tier for higher limits (1000 req/min) ## Pagination List",
              "score": null
            },
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_18",
              "content": "format. ### Error Response Format ```json { \"error\": { \"code\": \"invalid_parameter\", \"message\": \"The 'limit' parameter must be between 1 and 100\", \"field\": \"limit\", \"request_id\": \"req_8k3m9x2p\" } } ``` ### HTTP Status Codes | Status Code | Description | Common Causes | |------------|-------------|---------------| | 400 | Bad Request | Invalid parameters, malformed JSON, validation errors | | 401 | Unauthorized | Missing or invalid authentication credentials | | 403 | Forbidden | Insufficient permissions for requested",
              "score": null
            },
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_21",
              "content": "JSON - `duplicate_resource`: Resource with same identifier already exists - `quota_exceeded`: Account quota limit reached ### Error Handling Best Practices **Python Example:** ```python import requests import time def make_api_request(url, headers, max_retries=3): for attempt in range(max_retries): try: response = requests.get(url, headers=headers) if response.status_code == 200: return response.json() elif response.status_code == 429: # Rate limit exceeded retry_after = int(response.headers.get('X-RateLimit-Reset', 60)) print(f\"Rate limited.",
              "score": null
            },
            {
              "doc_id": "user_guide",
              "chunk_id": "user_guide_fix_28",
              "content": "2026-01-28 14:00:00 UTC ``` ## Error Handling Robust error handling ensures your workflows are resilient and reliable. ### Retry Policies Configure automatic retries for failed actions: ```yaml - id: api_call action: http_request config: url: \"https://api.example.com/data\" retry: max_attempts: 3 backoff_type: \"exponential\" # or \"fixed\", \"linear\" initial_interval: 1000 # milliseconds max_interval: 30000 multiplier: 2.0 retry_on: - timeout - network_error - status: [500, 502, 503, 504] ``` **Backoff Strategies:** - **Fixed**: Wait the same amount of time between",
              "score": null
            }
          ],
          "latency_ms": 3.5685809998540208,
          "query_id": "realistic_001",
          "dimension": "problem",
          "query": "My API calls are getting blocked, what's the limit?"
        },
        {
          "key_facts": [
            "100 requests per minute per authenticated user",
            "20 requests per minute for unauthenticated requests"
          ],
          "found_facts": [
            "100 requests per minute per authenticated user",
            "20 requests per minute for unauthenticated requests"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "api_reference"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_32",
              "content": "Limiting & Throttling ### 429 Too Many Requests #### Error Response ```json { \"error\": \"rate_limit_exceeded\", \"message\": \"API rate limit exceeded. Retry after 45 seconds.\", \"status\": 429, \"headers\": { \"X-RateLimit-Limit\": \"1000\", \"X-RateLimit-Remaining\": \"0\", \"X-RateLimit-Reset\": \"1706112345\", \"Retry-After\": \"45\" } } ``` #### Rate Limit Tiers CloudFlow enforces the following rate limits per workspace: | Tier | Requests/Minute | Requests/Hour | Concurrent Workflows | |------|-----------------|---------------|----------------------| | Free | 60 | 1,000 | 5 | | Standard | 1,000",
              "score": null
            },
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_21",
              "content": "JSON - `duplicate_resource`: Resource with same identifier already exists - `quota_exceeded`: Account quota limit reached ### Error Handling Best Practices **Python Example:** ```python import requests import time def make_api_request(url, headers, max_retries=3): for attempt in range(max_retries): try: response = requests.get(url, headers=headers) if response.status_code == 200: return response.json() elif response.status_code == 429: # Rate limit exceeded retry_after = int(response.headers.get('X-RateLimit-Reset', 60)) print(f\"Rate limited.",
              "score": null
            },
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_6",
              "content": "X-RateLimit-Reset: 1640995200 ``` When you exceed the rate limit, you'll receive a `429 Too Many Requests` response: ```json { \"error\": { \"code\": \"rate_limit_exceeded\", \"message\": \"Rate limit exceeded. Please retry after 42 seconds.\", \"retry_after\": 42 } } ``` **Best Practices:** - Monitor `X-RateLimit-Remaining` header values - Implement exponential backoff when receiving 429 responses - Cache responses when appropriate to reduce API calls - Consider upgrading to Enterprise tier for higher limits (1000 req/min) ## Pagination List",
              "score": null
            },
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_5",
              "content": "tokens expire after 3600 seconds (1 hour). Implement token refresh logic in your application. ## Rate Limiting To ensure fair usage and system stability, CloudFlow enforces rate limits on all API endpoints. **Default Limits:** - 100 requests per minute per authenticated user - 20 requests per minute for unauthenticated requests - Burst allowance: 150 requests in a 10-second window ### Rate Limit Headers Every API response includes rate limit information: ``` X-RateLimit-Limit: 100 X-RateLimit-Remaining: 87",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_49",
              "content": "limiting: 1,000 requests per minute per API key **Workflow Engine**: - Concurrent executions: 8,000 workflows (across 16 pods) - Execution start rate: 500 per second - Completion rate: 450 per second (average 2-second execution time) **Database**: - Read throughput: 50,000 queries per second (across replicas) - Write throughput: 15,000 transactions per second - Connection capacity: 2,000 concurrent connections **Kafka**: - Message ingestion: 100,000 messages per second - Consumer throughput: 80,000 messages per second (aggregated) -",
              "score": null
            }
          ],
          "latency_ms": 3.3972710007219575,
          "query_id": "realistic_001",
          "dimension": "casual",
          "query": "api rate limit"
        },
        {
          "key_facts": [
            "100 requests per minute per authenticated user",
            "20 requests per minute for unauthenticated requests"
          ],
          "found_facts": [
            "100 requests per minute per authenticated user",
            "20 requests per minute for unauthenticated requests"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "api_reference"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_5",
              "content": "tokens expire after 3600 seconds (1 hour). Implement token refresh logic in your application. ## Rate Limiting To ensure fair usage and system stability, CloudFlow enforces rate limits on all API endpoints. **Default Limits:** - 100 requests per minute per authenticated user - 20 requests per minute for unauthenticated requests - Burst allowance: 150 requests in a 10-second window ### Rate Limit Headers Every API response includes rate limit information: ``` X-RateLimit-Limit: 100 X-RateLimit-Remaining: 87",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_32",
              "content": "Limiting & Throttling ### 429 Too Many Requests #### Error Response ```json { \"error\": \"rate_limit_exceeded\", \"message\": \"API rate limit exceeded. Retry after 45 seconds.\", \"status\": 429, \"headers\": { \"X-RateLimit-Limit\": \"1000\", \"X-RateLimit-Remaining\": \"0\", \"X-RateLimit-Reset\": \"1706112345\", \"Retry-After\": \"45\" } } ``` #### Rate Limit Tiers CloudFlow enforces the following rate limits per workspace: | Tier | Requests/Minute | Requests/Hour | Concurrent Workflows | |------|-----------------|---------------|----------------------| | Free | 60 | 1,000 | 5 | | Standard | 1,000",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_34",
              "content": "rate limit usage cloudflow metrics query \\ --metric rate_limit_remaining \\ --workspace ws_abc123 \\ --last 1h \\ --interval 1m ``` #### Handling Rate Limits in Code **Python example with retry logic:** ```python import time import requests def cloudflow_api_call_with_retry(url, headers, max_retries=3): for attempt in range(max_retries): response = requests.get(url, headers=headers) if response.status_code == 429: retry_after = int(response.headers.get('Retry-After', 60)) print(f\"Rate limited.",
              "score": null
            },
            {
              "doc_id": "user_guide",
              "chunk_id": "user_guide_fix_36",
              "content": "- Success rate: 98.2% - Current queue depth: 3 ### Data Limits - **Maximum request/response size**: 10MB per action - **Maximum execution payload**: 50MB total - **Variable value size**: 1MB per variable ### Enterprise Plan Limits Enterprise customers can request increased limits: - Up to 100 steps per workflow - Up to 10,000 executions per day - Up to 7200 second timeout (2 hours) - Priority execution queue - Dedicated capacity allocation Contact sales@cloudflow.io for",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_63",
              "content": "**6-Month Projection** (Jul 2026): - Daily workflow executions: 4M (+60%) - Active users: 225,000 (+50%) - API requests per day: 80M (+60%) - Database size: 2TB (+67%) - Required scaling: +30% compute, +50% storage ### Contact Information - **Architecture Team**: architecture@cloudflow.internal - **On-Call Engineer**: PagerDuty escalation - **Security Team**: security@cloudflow.internal - **Documentation**: https://wiki.cloudflow.internal/architecture --- **Document Revision History**: - v2.3.1 (Jan 15, 2026): Updated performance metrics, added DR test results - v2.3.0 (Dec 1, 2025): Added",
              "score": null
            }
          ],
          "latency_ms": 3.418029999011196,
          "query_id": "realistic_001",
          "dimension": "contextual",
          "query": "I'm building a batch job that calls CloudFlow API repeatedly, what rate limit should I expect?"
        },
        {
          "key_facts": [
            "100 requests per minute per authenticated user",
            "20 requests per minute for unauthenticated requests"
          ],
          "found_facts": [],
          "missed_facts": [
            "100 requests per minute per authenticated user",
            "20 requests per minute for unauthenticated requests"
          ],
          "coverage": 0.0,
          "expected_docs": [
            "api_reference"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_6",
              "content": "X-RateLimit-Reset: 1640995200 ``` When you exceed the rate limit, you'll receive a `429 Too Many Requests` response: ```json { \"error\": { \"code\": \"rate_limit_exceeded\", \"message\": \"Rate limit exceeded. Please retry after 42 seconds.\", \"retry_after\": 42 } } ``` **Best Practices:** - Monitor `X-RateLimit-Remaining` header values - Implement exponential backoff when receiving 429 responses - Cache responses when appropriate to reduce API calls - Consider upgrading to Enterprise tier for higher limits (1000 req/min) ## Pagination List",
              "score": null
            },
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_21",
              "content": "JSON - `duplicate_resource`: Resource with same identifier already exists - `quota_exceeded`: Account quota limit reached ### Error Handling Best Practices **Python Example:** ```python import requests import time def make_api_request(url, headers, max_retries=3): for attempt in range(max_retries): try: response = requests.get(url, headers=headers) if response.status_code == 200: return response.json() elif response.status_code == 429: # Rate limit exceeded retry_after = int(response.headers.get('X-RateLimit-Reset', 60)) print(f\"Rate limited.",
              "score": null
            },
            {
              "doc_id": "user_guide",
              "chunk_id": "user_guide_fix_28",
              "content": "2026-01-28 14:00:00 UTC ``` ## Error Handling Robust error handling ensures your workflows are resilient and reliable. ### Retry Policies Configure automatic retries for failed actions: ```yaml - id: api_call action: http_request config: url: \"https://api.example.com/data\" retry: max_attempts: 3 backoff_type: \"exponential\" # or \"fixed\", \"linear\" initial_interval: 1000 # milliseconds max_interval: 30000 multiplier: 2.0 retry_on: - timeout - network_error - status: [500, 502, 503, 504] ``` **Backoff Strategies:** - **Fixed**: Wait the same amount of time between",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_32",
              "content": "Limiting & Throttling ### 429 Too Many Requests #### Error Response ```json { \"error\": \"rate_limit_exceeded\", \"message\": \"API rate limit exceeded. Retry after 45 seconds.\", \"status\": 429, \"headers\": { \"X-RateLimit-Limit\": \"1000\", \"X-RateLimit-Remaining\": \"0\", \"X-RateLimit-Reset\": \"1706112345\", \"Retry-After\": \"45\" } } ``` #### Rate Limit Tiers CloudFlow enforces the following rate limits per workspace: | Tier | Requests/Minute | Requests/Hour | Concurrent Workflows | |------|-----------------|---------------|----------------------| | Free | 60 | 1,000 | 5 | | Standard | 1,000",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_18",
              "content": "(100/100 connections in use)` - API requests fail with `503 Service Unavailable` - Database CPU usage normal, but connection count at maximum #### Investigation ```bash # Check current connection pool status cloudflow db pool status --detailed # Output example: # Pool Statistics: # Total Connections: 100/100 (100%) # Active: 87 # Idle: 13 # Waiting Requests: 45 # Average Wait Time: 3420ms # Max Wait Time: 8234ms # Identify long-running queries cloudflow db queries --status",
              "score": null
            }
          ],
          "latency_ms": 3.200434999598656,
          "query_id": "realistic_001",
          "dimension": "negation",
          "query": "Why is the API rejecting my requests after 100 calls?"
        },
        {
          "key_facts": [
            "429 Too Many Requests",
            "X-RateLimit-Remaining",
            "Retry-After",
            "Monitor X-RateLimit-Remaining header values",
            "Implement exponential backoff when receiving 429 responses"
          ],
          "found_facts": [
            "429 Too Many Requests",
            "X-RateLimit-Remaining",
            "Implement exponential backoff when receiving 429 responses"
          ],
          "missed_facts": [
            "Retry-After",
            "Monitor X-RateLimit-Remaining header values"
          ],
          "coverage": 0.6,
          "expected_docs": [
            "api_reference",
            "troubleshooting_guide"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_19",
              "content": "resource | | 404 | Not Found | Resource does not exist or has been deleted | | 429 | Too Many Requests | Rate limit exceeded, retry after specified period | | 500 | Internal Server Error | Unexpected server error, contact support if persists | | 502 | Bad Gateway | Temporary service issue, retry with exponential backoff | | 503 | Service Unavailable | Scheduled maintenance or temporary outage | ### Error",
              "score": null
            },
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_6",
              "content": "X-RateLimit-Reset: 1640995200 ``` When you exceed the rate limit, you'll receive a `429 Too Many Requests` response: ```json { \"error\": { \"code\": \"rate_limit_exceeded\", \"message\": \"Rate limit exceeded. Please retry after 42 seconds.\", \"retry_after\": 42 } } ``` **Best Practices:** - Monitor `X-RateLimit-Remaining` header values - Implement exponential backoff when receiving 429 responses - Cache responses when appropriate to reduce API calls - Consider upgrading to Enterprise tier for higher limits (1000 req/min) ## Pagination List",
              "score": null
            },
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_20",
              "content": "Codes CloudFlow returns specific error codes to help you identify and resolve issues: - `invalid_parameter`: One or more request parameters are invalid - `missing_required_field`: Required field is missing from request body - `authentication_failed`: Invalid API key or token - `insufficient_permissions`: User lacks required scope or permission - `resource_not_found`: Requested resource does not exist - `rate_limit_exceeded`: Too many requests, see rate limiting section - `workflow_execution_failed`: Workflow execution encountered an error - `invalid_json`: Request body contains malformed",
              "score": null
            },
            {
              "doc_id": "user_guide",
              "chunk_id": "user_guide_fix_28",
              "content": "2026-01-28 14:00:00 UTC ``` ## Error Handling Robust error handling ensures your workflows are resilient and reliable. ### Retry Policies Configure automatic retries for failed actions: ```yaml - id: api_call action: http_request config: url: \"https://api.example.com/data\" retry: max_attempts: 3 backoff_type: \"exponential\" # or \"fixed\", \"linear\" initial_interval: 1000 # milliseconds max_interval: 30000 multiplier: 2.0 retry_on: - timeout - network_error - status: [500, 502, 503, 504] ``` **Backoff Strategies:** - **Fixed**: Wait the same amount of time between",
              "score": null
            },
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_18",
              "content": "format. ### Error Response Format ```json { \"error\": { \"code\": \"invalid_parameter\", \"message\": \"The 'limit' parameter must be between 1 and 100\", \"field\": \"limit\", \"request_id\": \"req_8k3m9x2p\" } } ``` ### HTTP Status Codes | Status Code | Description | Common Causes | |------------|-------------|---------------| | 400 | Bad Request | Invalid parameters, malformed JSON, validation errors | | 401 | Unauthorized | Missing or invalid authentication credentials | | 403 | Forbidden | Insufficient permissions for requested",
              "score": null
            }
          ],
          "latency_ms": 3.184915000019828,
          "query_id": "realistic_002",
          "dimension": "original",
          "query": "How do I fix 429 Too Many Requests errors?"
        },
        {
          "key_facts": [
            "429 Too Many Requests",
            "X-RateLimit-Remaining",
            "Retry-After",
            "Monitor X-RateLimit-Remaining header values",
            "Implement exponential backoff when receiving 429 responses"
          ],
          "found_facts": [
            "429 Too Many Requests",
            "X-RateLimit-Remaining",
            "Retry-After",
            "Implement exponential backoff when receiving 429 responses"
          ],
          "missed_facts": [
            "Monitor X-RateLimit-Remaining header values"
          ],
          "coverage": 0.8,
          "expected_docs": [
            "api_reference",
            "troubleshooting_guide"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_6",
              "content": "X-RateLimit-Reset: 1640995200 ``` When you exceed the rate limit, you'll receive a `429 Too Many Requests` response: ```json { \"error\": { \"code\": \"rate_limit_exceeded\", \"message\": \"Rate limit exceeded. Please retry after 42 seconds.\", \"retry_after\": 42 } } ``` **Best Practices:** - Monitor `X-RateLimit-Remaining` header values - Implement exponential backoff when receiving 429 responses - Cache responses when appropriate to reduce API calls - Consider upgrading to Enterprise tier for higher limits (1000 req/min) ## Pagination List",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_32",
              "content": "Limiting & Throttling ### 429 Too Many Requests #### Error Response ```json { \"error\": \"rate_limit_exceeded\", \"message\": \"API rate limit exceeded. Retry after 45 seconds.\", \"status\": 429, \"headers\": { \"X-RateLimit-Limit\": \"1000\", \"X-RateLimit-Remaining\": \"0\", \"X-RateLimit-Reset\": \"1706112345\", \"Retry-After\": \"45\" } } ``` #### Rate Limit Tiers CloudFlow enforces the following rate limits per workspace: | Tier | Requests/Minute | Requests/Hour | Concurrent Workflows | |------|-----------------|---------------|----------------------| | Free | 60 | 1,000 | 5 | | Standard | 1,000",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_33",
              "content": "| 50,000 | 50 | | Premium | 5,000 | 250,000 | 200 | | Enterprise | Custom | Custom | Unlimited | #### Checking Rate Limit Status ```bash # Check current rate limit status curl -I https://api.cloudflow.io/api/v1/workflows \\ -H \"Authorization: Bearer $CF_ACCESS_TOKEN\" # Extract rate limit headers curl -s -I https://api.cloudflow.io/api/v1/workflows \\ -H \"Authorization: Bearer $CF_ACCESS_TOKEN\" | \\ grep -i \"x-ratelimit\" # Output: # X-RateLimit-Limit: 1000 # X-RateLimit-Remaining: 247 # X-RateLimit-Reset: 1706112400 # Monitor",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_34",
              "content": "rate limit usage cloudflow metrics query \\ --metric rate_limit_remaining \\ --workspace ws_abc123 \\ --last 1h \\ --interval 1m ``` #### Handling Rate Limits in Code **Python example with retry logic:** ```python import time import requests def cloudflow_api_call_with_retry(url, headers, max_retries=3): for attempt in range(max_retries): response = requests.get(url, headers=headers) if response.status_code == 429: retry_after = int(response.headers.get('Retry-After', 60)) print(f\"Rate limited.",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_35",
              "content": "Waiting {retry_after} seconds...\") time.sleep(retry_after) continue remaining = int(response.headers.get('X-RateLimit-Remaining', 0)) if remaining < 10: print(f\"Warning: Only {remaining} requests remaining\") return response raise Exception(\"Max retries exceeded due to rate limiting\") ``` **Bash script with rate limit checking:** ```bash #!/bin/bash check_rate_limit() { local remaining=$(curl -s -I https://api.cloudflow.io/api/v1/workflows \\ -H \"Authorization: Bearer $CF_ACCESS_TOKEN\" | \\ grep -i \"x-ratelimit-remaining\" | \\ awk '{print $2}' | tr -d '\\r') if [ \"$remaining\" -lt 10 ]; then echo \"Warning: Only $remaining requests",
              "score": null
            }
          ],
          "latency_ms": 3.4100550001312513,
          "query_id": "realistic_002",
          "dimension": "synonym",
          "query": "What should I do when I get rate limited?"
        },
        {
          "key_facts": [
            "429 Too Many Requests",
            "X-RateLimit-Remaining",
            "Retry-After",
            "Monitor X-RateLimit-Remaining header values",
            "Implement exponential backoff when receiving 429 responses"
          ],
          "found_facts": [],
          "missed_facts": [
            "429 Too Many Requests",
            "X-RateLimit-Remaining",
            "Retry-After",
            "Monitor X-RateLimit-Remaining header values",
            "Implement exponential backoff when receiving 429 responses"
          ],
          "coverage": 0.0,
          "expected_docs": [
            "api_reference",
            "troubleshooting_guide"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_18",
              "content": "format. ### Error Response Format ```json { \"error\": { \"code\": \"invalid_parameter\", \"message\": \"The 'limit' parameter must be between 1 and 100\", \"field\": \"limit\", \"request_id\": \"req_8k3m9x2p\" } } ``` ### HTTP Status Codes | Status Code | Description | Common Causes | |------------|-------------|---------------| | 400 | Bad Request | Invalid parameters, malformed JSON, validation errors | | 401 | Unauthorized | Missing or invalid authentication credentials | | 403 | Forbidden | Insufficient permissions for requested",
              "score": null
            },
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_20",
              "content": "Codes CloudFlow returns specific error codes to help you identify and resolve issues: - `invalid_parameter`: One or more request parameters are invalid - `missing_required_field`: Required field is missing from request body - `authentication_failed`: Invalid API key or token - `insufficient_permissions`: User lacks required scope or permission - `resource_not_found`: Requested resource does not exist - `rate_limit_exceeded`: Too many requests, see rate limiting section - `workflow_execution_failed`: Workflow execution encountered an error - `invalid_json`: Request body contains malformed",
              "score": null
            },
            {
              "doc_id": "user_guide",
              "chunk_id": "user_guide_fix_28",
              "content": "2026-01-28 14:00:00 UTC ``` ## Error Handling Robust error handling ensures your workflows are resilient and reliable. ### Retry Policies Configure automatic retries for failed actions: ```yaml - id: api_call action: http_request config: url: \"https://api.example.com/data\" retry: max_attempts: 3 backoff_type: \"exponential\" # or \"fixed\", \"linear\" initial_interval: 1000 # milliseconds max_interval: 30000 multiplier: 2.0 retry_on: - timeout - network_error - status: [500, 502, 503, 504] ``` **Backoff Strategies:** - **Fixed**: Wait the same amount of time between",
              "score": null
            },
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_19",
              "content": "resource | | 404 | Not Found | Resource does not exist or has been deleted | | 429 | Too Many Requests | Rate limit exceeded, retry after specified period | | 500 | Internal Server Error | Unexpected server error, contact support if persists | | 502 | Bad Gateway | Temporary service issue, retry with exponential backoff | | 503 | Service Unavailable | Scheduled maintenance or temporary outage | ### Error",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_53",
              "content": "\\ --description \"All API requests returning 503\" \\ --page-oncall # Notify status page cloudflow status update \\ --status \"major_outage\" \\ --message \"We are investigating a complete service outage\" # Create war room cloudflow incident war-room create incident-2024012401 ``` **For SEV-2 (High):** ```bash # Create incident and notify cloudflow incident create \\ --severity SEV-2 \\ --title \"High rate of workflow failures\" \\ --description \"Workflow success rate dropped to 75%\" \\ --notify-oncall # Update status page cloudflow",
              "score": null
            }
          ],
          "latency_ms": 3.3818430001701927,
          "query_id": "realistic_002",
          "dimension": "problem",
          "query": "My requests keep failing with 429 status code"
        },
        {
          "key_facts": [
            "429 Too Many Requests",
            "X-RateLimit-Remaining",
            "Retry-After",
            "Monitor X-RateLimit-Remaining header values",
            "Implement exponential backoff when receiving 429 responses"
          ],
          "found_facts": [],
          "missed_facts": [
            "429 Too Many Requests",
            "X-RateLimit-Remaining",
            "Retry-After",
            "Monitor X-RateLimit-Remaining header values",
            "Implement exponential backoff when receiving 429 responses"
          ],
          "coverage": 0.0,
          "expected_docs": [
            "api_reference",
            "troubleshooting_guide"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_19",
              "content": "resource | | 404 | Not Found | Resource does not exist or has been deleted | | 429 | Too Many Requests | Rate limit exceeded, retry after specified period | | 500 | Internal Server Error | Unexpected server error, contact support if persists | | 502 | Bad Gateway | Temporary service issue, retry with exponential backoff | | 503 | Service Unavailable | Scheduled maintenance or temporary outage | ### Error",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_54",
              "content": "status update \\ --status \"partial_outage\" \\ --message \"We are experiencing elevated error rates\" ``` **For SEV-3/SEV-4:** ```bash # Create support ticket cloudflow support ticket create \\ --priority MEDIUM \\ --subject \"Intermittent timeout errors\" \\ --description \"See attached logs and reproduction steps\" ``` ### Contact Information - **On-call hotline:** +1-888-CLOUDFLOW (24/7) - **Support email:** support@cloudflow.io - **Incident Slack channel:** #cloudflow-incidents - **Status page:** https://status.cloudflow.io - **Documentation:** https://docs.cloudflow.io ### Post-Incident Review After resolving incidents, complete a postmortem:",
              "score": null
            },
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_20",
              "content": "Codes CloudFlow returns specific error codes to help you identify and resolve issues: - `invalid_parameter`: One or more request parameters are invalid - `missing_required_field`: Required field is missing from request body - `authentication_failed`: Invalid API key or token - `insufficient_permissions`: User lacks required scope or permission - `resource_not_found`: Requested resource does not exist - `rate_limit_exceeded`: Too many requests, see rate limiting section - `workflow_execution_failed`: Workflow execution encountered an error - `invalid_json`: Request body contains malformed",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_49",
              "content": "errors for > 5 minutes - Database completely unavailable - Authentication system down - Data loss or corruption - **Response Time:** Immediate (< 15 minutes) - **Escalation:** Page on-call engineer immediately #### SEV-2: High (P2) - **Definition:** Major functionality impaired affecting multiple users - **Examples:** - Workflow execution success rate < 90% - Significant performance degradation (p95 latency > 10s) - Rate limiting affecting large customer segment - **Response Time:** < 1 hour - **Escalation:**",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_31",
              "content": "--step data_validation \\ --on-error continue \\ --error-threshold 5% # Fail if > 5% of records invalid ``` **2. External API Failures** ``` ExternalAPIError: API request to https://partner-api.example.com failed with status 502 ``` **Resolution:** ```bash # Add circuit breaker cloudflow workflows update wf_9k2n4m8p1q \\ --step external_api_call \\ --circuit-breaker-enabled true \\ --circuit-breaker-threshold 5 \\ --circuit-breaker-timeout 30000 # Configure fallback behavior cloudflow workflows update wf_9k2n4m8p1q \\ --step external_api_call \\ --fallback-action use_cached_data \\ --cache-ttl 3600 ``` --- ## Rate",
              "score": null
            }
          ],
          "latency_ms": 3.263261998654343,
          "query_id": "realistic_002",
          "dimension": "casual",
          "query": "429 error fix"
        },
        {
          "key_facts": [
            "429 Too Many Requests",
            "X-RateLimit-Remaining",
            "Retry-After",
            "Monitor X-RateLimit-Remaining header values",
            "Implement exponential backoff when receiving 429 responses"
          ],
          "found_facts": [
            "429 Too Many Requests",
            "X-RateLimit-Remaining",
            "Retry-After"
          ],
          "missed_facts": [
            "Monitor X-RateLimit-Remaining header values",
            "Implement exponential backoff when receiving 429 responses"
          ],
          "coverage": 0.6,
          "expected_docs": [
            "api_reference",
            "troubleshooting_guide"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_32",
              "content": "Limiting & Throttling ### 429 Too Many Requests #### Error Response ```json { \"error\": \"rate_limit_exceeded\", \"message\": \"API rate limit exceeded. Retry after 45 seconds.\", \"status\": 429, \"headers\": { \"X-RateLimit-Limit\": \"1000\", \"X-RateLimit-Remaining\": \"0\", \"X-RateLimit-Reset\": \"1706112345\", \"Retry-After\": \"45\" } } ``` #### Rate Limit Tiers CloudFlow enforces the following rate limits per workspace: | Tier | Requests/Minute | Requests/Hour | Concurrent Workflows | |------|-----------------|---------------|----------------------| | Free | 60 | 1,000 | 5 | | Standard | 1,000",
              "score": null
            },
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_5",
              "content": "tokens expire after 3600 seconds (1 hour). Implement token refresh logic in your application. ## Rate Limiting To ensure fair usage and system stability, CloudFlow enforces rate limits on all API endpoints. **Default Limits:** - 100 requests per minute per authenticated user - 20 requests per minute for unauthenticated requests - Burst allowance: 150 requests in a 10-second window ### Rate Limit Headers Every API response includes rate limit information: ``` X-RateLimit-Limit: 100 X-RateLimit-Remaining: 87",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_34",
              "content": "rate limit usage cloudflow metrics query \\ --metric rate_limit_remaining \\ --workspace ws_abc123 \\ --last 1h \\ --interval 1m ``` #### Handling Rate Limits in Code **Python example with retry logic:** ```python import time import requests def cloudflow_api_call_with_retry(url, headers, max_retries=3): for attempt in range(max_retries): response = requests.get(url, headers=headers) if response.status_code == 429: retry_after = int(response.headers.get('Retry-After', 60)) print(f\"Rate limited.",
              "score": null
            },
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_31",
              "content": "}, ], thresholds: { http_req_duration: ['p(95)<500'], }, }; export default function () { let res = http.get('https://api.cloudflow.io/api/workflows'); check(res, { 'status is 200': (r) => r.status === 200 }); sleep(1); } EOF ``` **Performance Targets**: - P95 API latency: < 500ms - P99 API latency: < 1000ms - Error rate: < 0.1% - Throughput: > 1000 requests/second --- ## Security and Compliance ### Network Policies Implement network policies to restrict pod-to-pod communication: ```yaml # network-policy.yaml apiVersion:",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_19",
              "content": "running --min-duration 30000 # Check connection distribution by client SELECT application_name, state, COUNT(*) as conn_count, AVG(EXTRACT(EPOCH FROM (NOW() - state_change))) as avg_duration_sec FROM pg_stat_activity WHERE datname = 'cloudflow_production' GROUP BY application_name, state ORDER BY conn_count DESC; ``` #### Resolution **Immediate mitigation:** ```bash # Temporarily increase connection limit (requires database restart) cloudflow db config set max_connections 150 # Kill idle connections older than 5 minutes cloudflow db connections kill --idle-timeout 300 # Restart connection pool without",
              "score": null
            }
          ],
          "latency_ms": 3.313404999062186,
          "query_id": "realistic_002",
          "dimension": "contextual",
          "query": "I'm getting throttled by CloudFlow API, how can I handle this in my application?"
        },
        {
          "key_facts": [
            "429 Too Many Requests",
            "X-RateLimit-Remaining",
            "Retry-After",
            "Monitor X-RateLimit-Remaining header values",
            "Implement exponential backoff when receiving 429 responses"
          ],
          "found_facts": [
            "429 Too Many Requests",
            "X-RateLimit-Remaining",
            "Retry-After",
            "Implement exponential backoff when receiving 429 responses"
          ],
          "missed_facts": [
            "Monitor X-RateLimit-Remaining header values"
          ],
          "coverage": 0.8,
          "expected_docs": [
            "api_reference",
            "troubleshooting_guide"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_6",
              "content": "X-RateLimit-Reset: 1640995200 ``` When you exceed the rate limit, you'll receive a `429 Too Many Requests` response: ```json { \"error\": { \"code\": \"rate_limit_exceeded\", \"message\": \"Rate limit exceeded. Please retry after 42 seconds.\", \"retry_after\": 42 } } ``` **Best Practices:** - Monitor `X-RateLimit-Remaining` header values - Implement exponential backoff when receiving 429 responses - Cache responses when appropriate to reduce API calls - Consider upgrading to Enterprise tier for higher limits (1000 req/min) ## Pagination List",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_32",
              "content": "Limiting & Throttling ### 429 Too Many Requests #### Error Response ```json { \"error\": \"rate_limit_exceeded\", \"message\": \"API rate limit exceeded. Retry after 45 seconds.\", \"status\": 429, \"headers\": { \"X-RateLimit-Limit\": \"1000\", \"X-RateLimit-Remaining\": \"0\", \"X-RateLimit-Reset\": \"1706112345\", \"Retry-After\": \"45\" } } ``` #### Rate Limit Tiers CloudFlow enforces the following rate limits per workspace: | Tier | Requests/Minute | Requests/Hour | Concurrent Workflows | |------|-----------------|---------------|----------------------| | Free | 60 | 1,000 | 5 | | Standard | 1,000",
              "score": null
            },
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_21",
              "content": "JSON - `duplicate_resource`: Resource with same identifier already exists - `quota_exceeded`: Account quota limit reached ### Error Handling Best Practices **Python Example:** ```python import requests import time def make_api_request(url, headers, max_retries=3): for attempt in range(max_retries): try: response = requests.get(url, headers=headers) if response.status_code == 200: return response.json() elif response.status_code == 429: # Rate limit exceeded retry_after = int(response.headers.get('X-RateLimit-Reset', 60)) print(f\"Rate limited.",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_33",
              "content": "| 50,000 | 50 | | Premium | 5,000 | 250,000 | 200 | | Enterprise | Custom | Custom | Unlimited | #### Checking Rate Limit Status ```bash # Check current rate limit status curl -I https://api.cloudflow.io/api/v1/workflows \\ -H \"Authorization: Bearer $CF_ACCESS_TOKEN\" # Extract rate limit headers curl -s -I https://api.cloudflow.io/api/v1/workflows \\ -H \"Authorization: Bearer $CF_ACCESS_TOKEN\" | \\ grep -i \"x-ratelimit\" # Output: # X-RateLimit-Limit: 1000 # X-RateLimit-Remaining: 247 # X-RateLimit-Reset: 1706112400 # Monitor",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_34",
              "content": "rate limit usage cloudflow metrics query \\ --metric rate_limit_remaining \\ --workspace ws_abc123 \\ --last 1h \\ --interval 1m ``` #### Handling Rate Limits in Code **Python example with retry logic:** ```python import time import requests def cloudflow_api_call_with_retry(url, headers, max_retries=3): for attempt in range(max_retries): response = requests.get(url, headers=headers) if response.status_code == 429: retry_after = int(response.headers.get('Retry-After', 60)) print(f\"Rate limited.",
              "score": null
            }
          ],
          "latency_ms": 3.2859439997992013,
          "query_id": "realistic_002",
          "dimension": "negation",
          "query": "Why am I being blocked with rate limit errors?"
        },
        {
          "key_facts": [
            "3600 seconds",
            "max 3600 seconds from iat",
            "All tokens expire after 3600 seconds"
          ],
          "found_facts": [
            "3600 seconds",
            "All tokens expire after 3600 seconds"
          ],
          "missed_facts": [
            "max 3600 seconds from iat"
          ],
          "coverage": 0.6666666666666666,
          "expected_docs": [
            "api_reference"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_3",
              "content": "**Verification:** ```bash # Decode JWT to check expiration echo $CF_ACCESS_TOKEN | cut -d'.' -f2 | base64 -d | jq '.exp' # Compare with current time date +%s ``` **Resolution:** ```bash # Refresh the access token curl -X POST https://api.cloudflow.io/auth/refresh \\ -H \"Content-Type: application/json\" \\ -d '{\"refresh_token\": \"'$CF_REFRESH_TOKEN'\"}' # Update environment variable export CF_ACCESS_TOKEN=\"<new_token>\" ``` **2.",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_26",
              "content": "5 minutes + AOF - Eviction policy: allkeys-lru **Cache Usage Patterns**: 1. **Session Storage**: - Key pattern: `session:{user_id}` - TTL: 15 minutes (aligned with JWT expiry) - Data: User session state, preferences - Invalidation: On logout or password change 2. **Rate Limiting Counters**: - Key pattern: `ratelimit:{api_key}:{window}` - TTL: 60 seconds (sliding window) - Data: Request count per window - Algorithm: Token bucket with Redis INCR 3.",
              "score": null
            },
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_4",
              "content": "```python import jwt import time from cryptography.hazmat.primitives import serialization from cryptography.hazmat.backends import default_backend # Load your private key with open('private_key.pem', 'rb') as key_file: private_key = serialization.load_pem_private_key( key_file.read(), password=None, backend=default_backend() ) # Create JWT payload payload = { 'iss': 'your-app-id', 'sub': 'user-12345', 'aud': 'https://api.cloudflow.io', 'exp': int(time.time()) + 3600, 'iat': int(time.time()), 'scope': 'workflows:read workflows:write' } # Generate token token = jwt.encode(payload, private_key, algorithm='RS256') # Use in API request headers = {'Authorization': f'Bearer {token}'} ``` **Token Expiration:** All",
              "score": null
            },
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_5",
              "content": "tokens expire after 3600 seconds (1 hour). Implement token refresh logic in your application. ## Rate Limiting To ensure fair usage and system stability, CloudFlow enforces rate limits on all API endpoints. **Default Limits:** - 100 requests per minute per authenticated user - 20 requests per minute for unauthenticated requests - Burst allowance: 150 requests in a 10-second window ### Rate Limit Headers Every API response includes rate limit information: ``` X-RateLimit-Limit: 100 X-RateLimit-Remaining: 87",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_2",
              "content": "Unauthorized` - Error message: `Authentication credentials were not provided or are invalid` - Frontend displays \"Session expired\" message #### Common Causes **1. Token Expiration** CloudFlow access tokens expire after 3600 seconds (1 hour) by default. Refresh tokens are valid for 30 days.",
              "score": null
            }
          ],
          "latency_ms": 3.200605000529322,
          "query_id": "realistic_003",
          "dimension": "original",
          "query": "What is the JWT token expiration time?"
        },
        {
          "key_facts": [
            "3600 seconds",
            "max 3600 seconds from iat",
            "All tokens expire after 3600 seconds"
          ],
          "found_facts": [
            "3600 seconds"
          ],
          "missed_facts": [
            "max 3600 seconds from iat",
            "All tokens expire after 3600 seconds"
          ],
          "coverage": 0.3333333333333333,
          "expected_docs": [
            "api_reference"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_5",
              "content": "tokens expire after 3600 seconds (1 hour). Implement token refresh logic in your application. ## Rate Limiting To ensure fair usage and system stability, CloudFlow enforces rate limits on all API endpoints. **Default Limits:** - 100 requests per minute per authenticated user - 20 requests per minute for unauthenticated requests - Burst allowance: 150 requests in a 10-second window ### Rate Limit Headers Every API response includes rate limit information: ``` X-RateLimit-Limit: 100 X-RateLimit-Remaining: 87",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_2",
              "content": "Unauthorized` - Error message: `Authentication credentials were not provided or are invalid` - Frontend displays \"Session expired\" message #### Common Causes **1. Token Expiration** CloudFlow access tokens expire after 3600 seconds (1 hour) by default. Refresh tokens are valid for 30 days.",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_26",
              "content": "5 minutes + AOF - Eviction policy: allkeys-lru **Cache Usage Patterns**: 1. **Session Storage**: - Key pattern: `session:{user_id}` - TTL: 15 minutes (aligned with JWT expiry) - Data: User session state, preferences - Invalidation: On logout or password change 2. **Rate Limiting Counters**: - Key pattern: `ratelimit:{api_key}:{window}` - TTL: 60 seconds (sliding window) - Data: Request count per window - Algorithm: Token bucket with Redis INCR 3.",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_30",
              "content": "triggers - Retention: 3 days - Message rate: 2,000/sec peak - Consumer groups: notification-service notifications.sms (8 partitions): - SMS notification triggers - Retention: 3 days - Message rate: 500/sec peak - Consumer groups: notification-service audit.logs (24 partitions): - Security and compliance audit events - Retention: 90 days (compliance requirement) - Message rate: 3,000/sec peak - Consumer groups: audit-processor, security-monitor dead-letter-queue (8 partitions): - Failed message processing - Retention: 30 days - Manual intervention required -",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_3",
              "content": "**Verification:** ```bash # Decode JWT to check expiration echo $CF_ACCESS_TOKEN | cut -d'.' -f2 | base64 -d | jq '.exp' # Compare with current time date +%s ``` **Resolution:** ```bash # Refresh the access token curl -X POST https://api.cloudflow.io/auth/refresh \\ -H \"Content-Type: application/json\" \\ -d '{\"refresh_token\": \"'$CF_REFRESH_TOKEN'\"}' # Update environment variable export CF_ACCESS_TOKEN=\"<new_token>\" ``` **2.",
              "score": null
            }
          ],
          "latency_ms": 3.379478999704588,
          "query_id": "realistic_003",
          "dimension": "synonym",
          "query": "How long do access tokens last?"
        },
        {
          "key_facts": [
            "3600 seconds",
            "max 3600 seconds from iat",
            "All tokens expire after 3600 seconds"
          ],
          "found_facts": [
            "3600 seconds"
          ],
          "missed_facts": [
            "max 3600 seconds from iat",
            "All tokens expire after 3600 seconds"
          ],
          "coverage": 0.3333333333333333,
          "expected_docs": [
            "api_reference"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_2",
              "content": "Unauthorized` - Error message: `Authentication credentials were not provided or are invalid` - Frontend displays \"Session expired\" message #### Common Causes **1. Token Expiration** CloudFlow access tokens expire after 3600 seconds (1 hour) by default. Refresh tokens are valid for 30 days.",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_49",
              "content": "errors for > 5 minutes - Database completely unavailable - Authentication system down - Data loss or corruption - **Response Time:** Immediate (< 15 minutes) - **Escalation:** Page on-call engineer immediately #### SEV-2: High (P2) - **Definition:** Major functionality impaired affecting multiple users - **Examples:** - Workflow execution success rate < 90% - Significant performance degradation (p95 latency > 10s) - Rate limiting affecting large customer segment - **Response Time:** < 1 hour - **Escalation:**",
              "score": null
            },
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_5",
              "content": "tokens expire after 3600 seconds (1 hour). Implement token refresh logic in your application. ## Rate Limiting To ensure fair usage and system stability, CloudFlow enforces rate limits on all API endpoints. **Default Limits:** - 100 requests per minute per authenticated user - 20 requests per minute for unauthenticated requests - Burst allowance: 150 requests in a 10-second window ### Rate Limit Headers Every API response includes rate limit information: ``` X-RateLimit-Limit: 100 X-RateLimit-Remaining: 87",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_26",
              "content": "5 minutes + AOF - Eviction policy: allkeys-lru **Cache Usage Patterns**: 1. **Session Storage**: - Key pattern: `session:{user_id}` - TTL: 15 minutes (aligned with JWT expiry) - Data: User session state, preferences - Invalidation: On logout or password change 2. **Rate Limiting Counters**: - Key pattern: `ratelimit:{api_key}:{window}` - TTL: 60 seconds (sliding window) - Data: Request count per window - Algorithm: Token bucket with Redis INCR 3.",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_3",
              "content": "**Verification:** ```bash # Decode JWT to check expiration echo $CF_ACCESS_TOKEN | cut -d'.' -f2 | base64 -d | jq '.exp' # Compare with current time date +%s ``` **Resolution:** ```bash # Refresh the access token curl -X POST https://api.cloudflow.io/auth/refresh \\ -H \"Content-Type: application/json\" \\ -d '{\"refresh_token\": \"'$CF_REFRESH_TOKEN'\"}' # Update environment variable export CF_ACCESS_TOKEN=\"<new_token>\" ``` **2.",
              "score": null
            }
          ],
          "latency_ms": 3.1423459986399394,
          "query_id": "realistic_003",
          "dimension": "problem",
          "query": "My authentication keeps expiring after an hour"
        },
        {
          "key_facts": [
            "3600 seconds",
            "max 3600 seconds from iat",
            "All tokens expire after 3600 seconds"
          ],
          "found_facts": [
            "3600 seconds"
          ],
          "missed_facts": [
            "max 3600 seconds from iat",
            "All tokens expire after 3600 seconds"
          ],
          "coverage": 0.3333333333333333,
          "expected_docs": [
            "api_reference"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_5",
              "content": "tokens expire after 3600 seconds (1 hour). Implement token refresh logic in your application. ## Rate Limiting To ensure fair usage and system stability, CloudFlow enforces rate limits on all API endpoints. **Default Limits:** - 100 requests per minute per authenticated user - 20 requests per minute for unauthenticated requests - Burst allowance: 150 requests in a 10-second window ### Rate Limit Headers Every API response includes rate limit information: ``` X-RateLimit-Limit: 100 X-RateLimit-Remaining: 87",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_3",
              "content": "**Verification:** ```bash # Decode JWT to check expiration echo $CF_ACCESS_TOKEN | cut -d'.' -f2 | base64 -d | jq '.exp' # Compare with current time date +%s ``` **Resolution:** ```bash # Refresh the access token curl -X POST https://api.cloudflow.io/auth/refresh \\ -H \"Content-Type: application/json\" \\ -d '{\"refresh_token\": \"'$CF_REFRESH_TOKEN'\"}' # Update environment variable export CF_ACCESS_TOKEN=\"<new_token>\" ``` **2.",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_2",
              "content": "Unauthorized` - Error message: `Authentication credentials were not provided or are invalid` - Frontend displays \"Session expired\" message #### Common Causes **1. Token Expiration** CloudFlow access tokens expire after 3600 seconds (1 hour) by default. Refresh tokens are valid for 30 days.",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_26",
              "content": "5 minutes + AOF - Eviction policy: allkeys-lru **Cache Usage Patterns**: 1. **Session Storage**: - Key pattern: `session:{user_id}` - TTL: 15 minutes (aligned with JWT expiry) - Data: User session state, preferences - Invalidation: On logout or password change 2. **Rate Limiting Counters**: - Key pattern: `ratelimit:{api_key}:{window}` - TTL: 60 seconds (sliding window) - Data: Request count per window - Algorithm: Token bucket with Redis INCR 3.",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_30",
              "content": "triggers - Retention: 3 days - Message rate: 2,000/sec peak - Consumer groups: notification-service notifications.sms (8 partitions): - SMS notification triggers - Retention: 3 days - Message rate: 500/sec peak - Consumer groups: notification-service audit.logs (24 partitions): - Security and compliance audit events - Retention: 90 days (compliance requirement) - Message rate: 3,000/sec peak - Consumer groups: audit-processor, security-monitor dead-letter-queue (8 partitions): - Failed message processing - Retention: 30 days - Manual intervention required -",
              "score": null
            }
          ],
          "latency_ms": 3.2081190001918003,
          "query_id": "realistic_003",
          "dimension": "casual",
          "query": "token expiry time"
        },
        {
          "key_facts": [
            "3600 seconds",
            "max 3600 seconds from iat",
            "All tokens expire after 3600 seconds"
          ],
          "found_facts": [
            "3600 seconds"
          ],
          "missed_facts": [
            "max 3600 seconds from iat",
            "All tokens expire after 3600 seconds"
          ],
          "coverage": 0.3333333333333333,
          "expected_docs": [
            "api_reference"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_3",
              "content": "**Verification:** ```bash # Decode JWT to check expiration echo $CF_ACCESS_TOKEN | cut -d'.' -f2 | base64 -d | jq '.exp' # Compare with current time date +%s ``` **Resolution:** ```bash # Refresh the access token curl -X POST https://api.cloudflow.io/auth/refresh \\ -H \"Content-Type: application/json\" \\ -d '{\"refresh_token\": \"'$CF_REFRESH_TOKEN'\"}' # Update environment variable export CF_ACCESS_TOKEN=\"<new_token>\" ``` **2.",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_26",
              "content": "5 minutes + AOF - Eviction policy: allkeys-lru **Cache Usage Patterns**: 1. **Session Storage**: - Key pattern: `session:{user_id}` - TTL: 15 minutes (aligned with JWT expiry) - Data: User session state, preferences - Invalidation: On logout or password change 2. **Rate Limiting Counters**: - Key pattern: `ratelimit:{api_key}:{window}` - TTL: 60 seconds (sliding window) - Data: Request count per window - Algorithm: Token bucket with Redis INCR 3.",
              "score": null
            },
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_5",
              "content": "tokens expire after 3600 seconds (1 hour). Implement token refresh logic in your application. ## Rate Limiting To ensure fair usage and system stability, CloudFlow enforces rate limits on all API endpoints. **Default Limits:** - 100 requests per minute per authenticated user - 20 requests per minute for unauthenticated requests - Burst allowance: 150 requests in a 10-second window ### Rate Limit Headers Every API response includes rate limit information: ``` X-RateLimit-Limit: 100 X-RateLimit-Remaining: 87",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_8",
              "content": "Redis-backed storage - API key generation and validation for service accounts - MFA enforcement for administrative operations **Authentication Flow**: ``` Client Request \u2192 API Gateway \u2192 Auth Service \u2502 \u251c\u2500 Validate credentials \u251c\u2500 Check MFA if required \u251c\u2500 Generate JWT (15min expiry) \u251c\u2500 Generate refresh token (7 days) \u2514\u2500 Store session in Redis ``` **Token Structure**: - Access Token: JWT with 15-minute expiry - Refresh Token: Opaque token with 7-day expiry, stored in PostgreSQL -",
              "score": null
            },
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_4",
              "content": "```python import jwt import time from cryptography.hazmat.primitives import serialization from cryptography.hazmat.backends import default_backend # Load your private key with open('private_key.pem', 'rb') as key_file: private_key = serialization.load_pem_private_key( key_file.read(), password=None, backend=default_backend() ) # Create JWT payload payload = { 'iss': 'your-app-id', 'sub': 'user-12345', 'aud': 'https://api.cloudflow.io', 'exp': int(time.time()) + 3600, 'iat': int(time.time()), 'scope': 'workflows:read workflows:write' } # Generate token token = jwt.encode(payload, private_key, algorithm='RS256') # Use in API request headers = {'Authorization': f'Bearer {token}'} ``` **Token Expiration:** All",
              "score": null
            }
          ],
          "latency_ms": 3.1910469988361,
          "query_id": "realistic_003",
          "dimension": "contextual",
          "query": "I need to implement token refresh logic, when do JWT tokens expire?"
        },
        {
          "key_facts": [
            "3600 seconds",
            "max 3600 seconds from iat",
            "All tokens expire after 3600 seconds"
          ],
          "found_facts": [
            "3600 seconds"
          ],
          "missed_facts": [
            "max 3600 seconds from iat",
            "All tokens expire after 3600 seconds"
          ],
          "coverage": 0.3333333333333333,
          "expected_docs": [
            "api_reference"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_5",
              "content": "tokens expire after 3600 seconds (1 hour). Implement token refresh logic in your application. ## Rate Limiting To ensure fair usage and system stability, CloudFlow enforces rate limits on all API endpoints. **Default Limits:** - 100 requests per minute per authenticated user - 20 requests per minute for unauthenticated requests - Burst allowance: 150 requests in a 10-second window ### Rate Limit Headers Every API response includes rate limit information: ``` X-RateLimit-Limit: 100 X-RateLimit-Remaining: 87",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_2",
              "content": "Unauthorized` - Error message: `Authentication credentials were not provided or are invalid` - Frontend displays \"Session expired\" message #### Common Causes **1. Token Expiration** CloudFlow access tokens expire after 3600 seconds (1 hour) by default. Refresh tokens are valid for 30 days.",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_3",
              "content": "**Verification:** ```bash # Decode JWT to check expiration echo $CF_ACCESS_TOKEN | cut -d'.' -f2 | base64 -d | jq '.exp' # Compare with current time date +%s ``` **Resolution:** ```bash # Refresh the access token curl -X POST https://api.cloudflow.io/auth/refresh \\ -H \"Content-Type: application/json\" \\ -d '{\"refresh_token\": \"'$CF_REFRESH_TOKEN'\"}' # Update environment variable export CF_ACCESS_TOKEN=\"<new_token>\" ``` **2.",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_5",
              "content": "Clock Skew Issues** JWT validation fails when system clocks are out of sync (tolerance: \u00b1300 seconds). **Diagnosis:** ```bash # Check system time timedatectl status # Compare with NTP server ntpdate -q pool.ntp.org # Check JWT issued time vs current time jwt_iat=$(echo $CF_ACCESS_TOKEN | cut -d'.' -f2 | base64 -d | jq '.iat') current_time=$(date +%s) skew=$((current_time - jwt_iat)) echo \"Clock skew: $skew seconds\" ``` **Resolution:** ```bash # Sync with NTP server sudo ntpdate -s pool.ntp.org #",
              "score": null
            },
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_4",
              "content": "```python import jwt import time from cryptography.hazmat.primitives import serialization from cryptography.hazmat.backends import default_backend # Load your private key with open('private_key.pem', 'rb') as key_file: private_key = serialization.load_pem_private_key( key_file.read(), password=None, backend=default_backend() ) # Create JWT payload payload = { 'iss': 'your-app-id', 'sub': 'user-12345', 'aud': 'https://api.cloudflow.io', 'exp': int(time.time()) + 3600, 'iat': int(time.time()), 'scope': 'workflows:read workflows:write' } # Generate token token = jwt.encode(payload, private_key, algorithm='RS256') # Use in API request headers = {'Authorization': f'Bearer {token}'} ``` **Token Expiration:** All",
              "score": null
            }
          ],
          "latency_ms": 3.0247370013967156,
          "query_id": "realistic_003",
          "dimension": "negation",
          "query": "Why does my token stop working after 3600 seconds?"
        },
        {
          "key_facts": [
            "PostgreSQL 15.4",
            "Redis 7.2",
            "Apache Kafka 3.6"
          ],
          "found_facts": [],
          "missed_facts": [
            "PostgreSQL 15.4",
            "Redis 7.2",
            "Apache Kafka 3.6"
          ],
          "coverage": 0.0,
          "expected_docs": [
            "architecture_overview"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_0",
              "content": "# CloudFlow Platform - System Architecture Overview **Document Version:** 2.3.1 **Last Updated:** January 15, 2026 **Owner:** Platform Architecture Team **Status:** Production ## Executive Summary CloudFlow is a distributed, cloud-native workflow automation platform designed to orchestrate complex business processes at scale. The platform processes over 2.5 million workflow executions daily with an average P99 latency of 180ms for API operations and 4.2 seconds for workflow execution.",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_63",
              "content": "**6-Month Projection** (Jul 2026): - Daily workflow executions: 4M (+60%) - Active users: 225,000 (+50%) - API requests per day: 80M (+60%) - Database size: 2TB (+67%) - Required scaling: +30% compute, +50% storage ### Contact Information - **Architecture Team**: architecture@cloudflow.internal - **On-Call Engineer**: PagerDuty escalation - **Security Team**: security@cloudflow.internal - **Documentation**: https://wiki.cloudflow.internal/architecture --- **Document Revision History**: - v2.3.1 (Jan 15, 2026): Updated performance metrics, added DR test results - v2.3.0 (Dec 1, 2025): Added",
              "score": null
            },
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_0",
              "content": "# CloudFlow Platform - Deployment and Operations Guide **Version:** 2.4.0 **Last Updated:** January 2026 **Target Environment:** Production (AWS EKS) ## Table of Contents 1. [Overview](#overview) 2. [Prerequisites](#prerequisites) 3. [Infrastructure Setup](#infrastructure-setup) 4. [Kubernetes Deployment](#kubernetes-deployment) 5. [Database Configuration](#database-configuration) 6. [Monitoring and Observability](#monitoring-and-observability) 7. [Backup and Disaster Recovery](#backup-and-disaster-recovery) 8. [Scaling and Performance](#scaling-and-performance) 9. [Security and Compliance](#security-and-compliance) 10. [Troubleshooting](#troubleshooting) --- ## Overview CloudFlow is a cloud-native workflow orchestration platform designed for high-availability production environments.",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_23",
              "content": "database tier. #### Permanent Solutions **Option 1: Upgrade database tier** ```bash # Check available tiers cloudflow db tiers list # Upgrade to higher tier (supports 200 connections) cloudflow db upgrade --tier standard-plus --confirm # Monitor migration progress cloudflow db migration status ``` **Option 2: Implement aggressive connection reuse** ```bash # Reduce connection lifetime cloudflow config set db.pool.max_lifetime 600 # 10 minutes # Enable prepared statement caching cloudflow config set db.prepared_statements.cache true cloudflow config set db.prepared_statements.max_size",
              "score": null
            },
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_0",
              "content": "# CloudFlow API Reference Version 2.1.0 | Last Updated: January 2026 ## Overview The CloudFlow API is a RESTful service that enables developers to programmatically manage cloud workflows, data pipelines, and automation tasks. This documentation provides comprehensive details on authentication, endpoints, request/response formats, error handling, and best practices. **Base URL:** `https://api.cloudflow.io/v2` **API Status:** https://status.cloudflow.io ## Authentication CloudFlow supports three authentication methods to suit different use cases and security requirements.",
              "score": null
            }
          ],
          "latency_ms": 3.1211570003506495,
          "query_id": "realistic_004",
          "dimension": "original",
          "query": "What database technology does CloudFlow use?"
        },
        {
          "key_facts": [
            "PostgreSQL 15.4",
            "Redis 7.2",
            "Apache Kafka 3.6"
          ],
          "found_facts": [],
          "missed_facts": [
            "PostgreSQL 15.4",
            "Redis 7.2",
            "Apache Kafka 3.6"
          ],
          "coverage": 0.0,
          "expected_docs": [
            "architecture_overview"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_0",
              "content": "# CloudFlow Platform - System Architecture Overview **Document Version:** 2.3.1 **Last Updated:** January 15, 2026 **Owner:** Platform Architecture Team **Status:** Production ## Executive Summary CloudFlow is a distributed, cloud-native workflow automation platform designed to orchestrate complex business processes at scale. The platform processes over 2.5 million workflow executions daily with an average P99 latency of 180ms for API operations and 4.2 seconds for workflow execution.",
              "score": null
            },
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_0",
              "content": "# CloudFlow Platform - Deployment and Operations Guide **Version:** 2.4.0 **Last Updated:** January 2026 **Target Environment:** Production (AWS EKS) ## Table of Contents 1. [Overview](#overview) 2. [Prerequisites](#prerequisites) 3. [Infrastructure Setup](#infrastructure-setup) 4. [Kubernetes Deployment](#kubernetes-deployment) 5. [Database Configuration](#database-configuration) 6. [Monitoring and Observability](#monitoring-and-observability) 7. [Backup and Disaster Recovery](#backup-and-disaster-recovery) 8. [Scaling and Performance](#scaling-and-performance) 9. [Security and Compliance](#security-and-compliance) 10. [Troubleshooting](#troubleshooting) --- ## Overview CloudFlow is a cloud-native workflow orchestration platform designed for high-availability production environments.",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_63",
              "content": "**6-Month Projection** (Jul 2026): - Daily workflow executions: 4M (+60%) - Active users: 225,000 (+50%) - API requests per day: 80M (+60%) - Database size: 2TB (+67%) - Required scaling: +30% compute, +50% storage ### Contact Information - **Architecture Team**: architecture@cloudflow.internal - **On-Call Engineer**: PagerDuty escalation - **Security Team**: security@cloudflow.internal - **Documentation**: https://wiki.cloudflow.internal/architecture --- **Document Revision History**: - v2.3.1 (Jan 15, 2026): Updated performance metrics, added DR test results - v2.3.0 (Dec 1, 2025): Added",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_0",
              "content": "# CloudFlow Platform Troubleshooting Guide **Version:** 3.2.1 **Last Updated:** January 2026 **Audience:** Platform Engineers, DevOps, Support Teams ## Table of Contents 1. [Overview](#overview) 2. [Authentication & Authorization Issues](#authentication--authorization-issues) 3. [Performance Problems](#performance-problems) 4. [Database Connection Issues](#database-connection-issues) 5. [Workflow Execution Failures](#workflow-execution-failures) 6. [Rate Limiting & Throttling](#rate-limiting--throttling) 7. [Log Analysis & Debugging](#log-analysis--debugging) 8. [Escalation Procedures](#escalation-procedures) --- ## Overview This guide provides comprehensive troubleshooting steps for common CloudFlow platform issues encountered in production environments.",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_23",
              "content": "database tier. #### Permanent Solutions **Option 1: Upgrade database tier** ```bash # Check available tiers cloudflow db tiers list # Upgrade to higher tier (supports 200 connections) cloudflow db upgrade --tier standard-plus --confirm # Monitor migration progress cloudflow db migration status ``` **Option 2: Implement aggressive connection reuse** ```bash # Reduce connection lifetime cloudflow config set db.pool.max_lifetime 600 # 10 minutes # Enable prepared statement caching cloudflow config set db.prepared_statements.cache true cloudflow config set db.prepared_statements.max_size",
              "score": null
            }
          ],
          "latency_ms": 3.142587000183994,
          "query_id": "realistic_004",
          "dimension": "synonym",
          "query": "Which database system powers CloudFlow?"
        },
        {
          "key_facts": [
            "PostgreSQL 15.4",
            "Redis 7.2",
            "Apache Kafka 3.6"
          ],
          "found_facts": [],
          "missed_facts": [
            "PostgreSQL 15.4",
            "Redis 7.2",
            "Apache Kafka 3.6"
          ],
          "coverage": 0.0,
          "expected_docs": [
            "architecture_overview"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_38",
              "content": "### Write-Through Pattern Used for critical data where cache consistency is paramount: ``` 1. Application writes to database 2. Database transaction commits 3. Application updates cache 4. Return success to client ``` Applied to: User profiles, authentication sessions, system configuration.",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_1",
              "content": "This document provides a comprehensive overview of the system architecture, including microservices design, data flow patterns, infrastructure decisions, and operational considerations. ## Table of Contents 1. [High-Level Architecture](#high-level-architecture) 2. [Microservices Breakdown](#microservices-breakdown) 3. [Data Flow Architecture](#data-flow-architecture) 4. [Database Architecture](#database-architecture) 5. [Message Queue Patterns](#message-queue-patterns) 6. [Caching Strategy](#caching-strategy) 7. [Security Architecture](#security-architecture) 8. [Performance Characteristics](#performance-characteristics) 9. [Disaster Recovery](#disaster-recovery) --- ## High-Level Architecture CloudFlow follows a microservices architecture pattern deployed across multiple availability zones in AWS.",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_23",
              "content": "for high availability - Automated backups: Daily snapshots, 30-day retention - Point-in-time recovery: 5-minute granularity **Database Schema Design**: ``` Core Tables: - users (5M rows): User accounts and profiles - workflows (2M rows): Workflow definitions - workflow_executions (500M rows, partitioned): Execution history - workflow_steps (2B rows, partitioned): Individual step records - schedules (100K rows): Scheduled workflow triggers - notifications (1B rows, partitioned): Notification delivery log Partitioning Strategy: - workflow_executions: Monthly partitions by created_at - workflow_steps:",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_50",
              "content": "End-to-end latency: < 100ms (P99) ### Resource Utilization **CPU Utilization** (Target: 60-70% average): - API Gateway: 55% average, 80% peak - Workflow Engine: 65% average, 85% peak - Auth Service: 40% average, 60% peak **Memory Utilization** (Target: < 80%): - API Gateway: 2.5GB average per pod (4GB allocated) - Workflow Engine: 6GB average per pod (8GB allocated) - Notification Service: 2.8GB average per pod (4GB allocated) **Network Throughput**: - Ingress: 2 Gbps average, 5 Gbps",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_36",
              "content": "--- ## Caching Strategy ### Cache-Aside Pattern (Lazy Loading) Primary caching pattern used across all services: ``` 1. Application checks cache (Redis GET) 2. If HIT \u2192 Return cached data 3. If MISS: a. Query database (PostgreSQL) b. Store result in cache with TTL c.",
              "score": null
            }
          ],
          "latency_ms": 3.035838000869262,
          "query_id": "realistic_004",
          "dimension": "problem",
          "query": "I need to understand the data storage architecture"
        },
        {
          "key_facts": [
            "PostgreSQL 15.4",
            "Redis 7.2",
            "Apache Kafka 3.6"
          ],
          "found_facts": [],
          "missed_facts": [
            "PostgreSQL 15.4",
            "Redis 7.2",
            "Apache Kafka 3.6"
          ],
          "coverage": 0.0,
          "expected_docs": [
            "architecture_overview"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_48",
              "content": "execution: Linear scaling up to 10 concurrent branches **Database Operations** (P99 latency): - Simple SELECT: < 5ms - JOIN query: < 20ms - INSERT/UPDATE: < 10ms - Batch operations: < 100ms (batch size: 100 records) **Cache Operations** (P99 latency): - Redis GET: < 2ms - Redis SET: < 3ms - Redis DEL: < 2ms ### Throughput Capacity **API Gateway**: - Sustained: 10,000 requests per second - Peak: 25,000 requests per second (5-minute burst) - Rate",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_23",
              "content": "for high availability - Automated backups: Daily snapshots, 30-day retention - Point-in-time recovery: 5-minute granularity **Database Schema Design**: ``` Core Tables: - users (5M rows): User accounts and profiles - workflows (2M rows): Workflow definitions - workflow_executions (500M rows, partitioned): Execution history - workflow_steps (2B rows, partitioned): Individual step records - schedules (100K rows): Scheduled workflow triggers - notifications (1B rows, partitioned): Notification delivery log Partitioning Strategy: - workflow_executions: Monthly partitions by created_at - workflow_steps:",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_38",
              "content": "### Write-Through Pattern Used for critical data where cache consistency is paramount: ``` 1. Application writes to database 2. Database transaction commits 3. Application updates cache 4. Return success to client ``` Applied to: User profiles, authentication sessions, system configuration.",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_49",
              "content": "errors for > 5 minutes - Database completely unavailable - Authentication system down - Data loss or corruption - **Response Time:** Immediate (< 15 minutes) - **Escalation:** Page on-call engineer immediately #### SEV-2: High (P2) - **Definition:** Major functionality impaired affecting multiple users - **Examples:** - Workflow execution success rate < 90% - Significant performance degradation (p95 latency > 10s) - Rate limiting affecting large customer segment - **Response Time:** < 1 hour - **Escalation:**",
              "score": null
            },
            {
              "doc_id": "user_guide",
              "chunk_id": "user_guide_fix_37",
              "content": "Enterprise pricing and custom limits. ## Best Practices Follow these best practices to build reliable, maintainable workflows: ### 1. Use Descriptive Names **Good:** - Workflow: \"Sync Customer Data from Salesforce to Database\" - Step: \"validate_customer_email\" **Bad:** - Workflow: \"Workflow 1\" - Step: \"step3\" ### 2.",
              "score": null
            }
          ],
          "latency_ms": 3.1308050001825904,
          "query_id": "realistic_004",
          "dimension": "casual",
          "query": "database stack"
        },
        {
          "key_facts": [
            "PostgreSQL 15.4",
            "Redis 7.2",
            "Apache Kafka 3.6"
          ],
          "found_facts": [],
          "missed_facts": [
            "PostgreSQL 15.4",
            "Redis 7.2",
            "Apache Kafka 3.6"
          ],
          "coverage": 0.0,
          "expected_docs": [
            "architecture_overview"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_63",
              "content": "**6-Month Projection** (Jul 2026): - Daily workflow executions: 4M (+60%) - Active users: 225,000 (+50%) - API requests per day: 80M (+60%) - Database size: 2TB (+67%) - Required scaling: +30% compute, +50% storage ### Contact Information - **Architecture Team**: architecture@cloudflow.internal - **On-Call Engineer**: PagerDuty escalation - **Security Team**: security@cloudflow.internal - **Documentation**: https://wiki.cloudflow.internal/architecture --- **Document Revision History**: - v2.3.1 (Jan 15, 2026): Updated performance metrics, added DR test results - v2.3.0 (Dec 1, 2025): Added",
              "score": null
            },
            {
              "doc_id": "user_guide",
              "chunk_id": "user_guide_fix_36",
              "content": "- Success rate: 98.2% - Current queue depth: 3 ### Data Limits - **Maximum request/response size**: 10MB per action - **Maximum execution payload**: 50MB total - **Variable value size**: 1MB per variable ### Enterprise Plan Limits Enterprise customers can request increased limits: - Up to 100 steps per workflow - Up to 10,000 executions per day - Up to 7200 second timeout (2 hours) - Priority execution queue - Dedicated capacity allocation Contact sales@cloudflow.io for",
              "score": null
            },
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_0",
              "content": "# CloudFlow Platform - Deployment and Operations Guide **Version:** 2.4.0 **Last Updated:** January 2026 **Target Environment:** Production (AWS EKS) ## Table of Contents 1. [Overview](#overview) 2. [Prerequisites](#prerequisites) 3. [Infrastructure Setup](#infrastructure-setup) 4. [Kubernetes Deployment](#kubernetes-deployment) 5. [Database Configuration](#database-configuration) 6. [Monitoring and Observability](#monitoring-and-observability) 7. [Backup and Disaster Recovery](#backup-and-disaster-recovery) 8. [Scaling and Performance](#scaling-and-performance) 9. [Security and Compliance](#security-and-compliance) 10. [Troubleshooting](#troubleshooting) --- ## Overview CloudFlow is a cloud-native workflow orchestration platform designed for high-availability production environments.",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_23",
              "content": "database tier. #### Permanent Solutions **Option 1: Upgrade database tier** ```bash # Check available tiers cloudflow db tiers list # Upgrade to higher tier (supports 200 connections) cloudflow db upgrade --tier standard-plus --confirm # Monitor migration progress cloudflow db migration status ``` **Option 2: Implement aggressive connection reuse** ```bash # Reduce connection lifetime cloudflow config set db.pool.max_lifetime 600 # 10 minutes # Enable prepared statement caching cloudflow config set db.prepared_statements.cache true cloudflow config set db.prepared_statements.max_size",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_0",
              "content": "# CloudFlow Platform - System Architecture Overview **Document Version:** 2.3.1 **Last Updated:** January 15, 2026 **Owner:** Platform Architecture Team **Status:** Production ## Executive Summary CloudFlow is a distributed, cloud-native workflow automation platform designed to orchestrate complex business processes at scale. The platform processes over 2.5 million workflow executions daily with an average P99 latency of 180ms for API operations and 4.2 seconds for workflow execution.",
              "score": null
            }
          ],
          "latency_ms": 3.1157469984464115,
          "query_id": "realistic_004",
          "dimension": "contextual",
          "query": "For capacity planning, what databases does CloudFlow rely on?"
        },
        {
          "key_facts": [
            "PostgreSQL 15.4",
            "Redis 7.2",
            "Apache Kafka 3.6"
          ],
          "found_facts": [],
          "missed_facts": [
            "PostgreSQL 15.4",
            "Redis 7.2",
            "Apache Kafka 3.6"
          ],
          "coverage": 0.0,
          "expected_docs": [
            "architecture_overview"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_0",
              "content": "# CloudFlow Platform - System Architecture Overview **Document Version:** 2.3.1 **Last Updated:** January 15, 2026 **Owner:** Platform Architecture Team **Status:** Production ## Executive Summary CloudFlow is a distributed, cloud-native workflow automation platform designed to orchestrate complex business processes at scale. The platform processes over 2.5 million workflow executions daily with an average P99 latency of 180ms for API operations and 4.2 seconds for workflow execution.",
              "score": null
            },
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_1",
              "content": "This guide provides comprehensive instructions for deploying and operating CloudFlow on Amazon EKS (Elastic Kubernetes Service). ### Architecture Summary CloudFlow consists of the following components: - **API Server**: REST API for workflow management (Node.js/Express) - **Worker Service**: Background job processor (Node.js) - **Scheduler**: Cron-based task scheduler (Node.js) - **PostgreSQL**: Primary data store (version 14) - **Redis**: Cache and message queue (version 7.0) - **PgBouncer**: Database connection pooler ### Deployment Model - **Namespace**: `cloudflow-prod` - **Cluster**:",
              "score": null
            },
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_0",
              "content": "# CloudFlow Platform - Deployment and Operations Guide **Version:** 2.4.0 **Last Updated:** January 2026 **Target Environment:** Production (AWS EKS) ## Table of Contents 1. [Overview](#overview) 2. [Prerequisites](#prerequisites) 3. [Infrastructure Setup](#infrastructure-setup) 4. [Kubernetes Deployment](#kubernetes-deployment) 5. [Database Configuration](#database-configuration) 6. [Monitoring and Observability](#monitoring-and-observability) 7. [Backup and Disaster Recovery](#backup-and-disaster-recovery) 8. [Scaling and Performance](#scaling-and-performance) 9. [Security and Compliance](#security-and-compliance) 10. [Troubleshooting](#troubleshooting) --- ## Overview CloudFlow is a cloud-native workflow orchestration platform designed for high-availability production environments.",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_63",
              "content": "**6-Month Projection** (Jul 2026): - Daily workflow executions: 4M (+60%) - Active users: 225,000 (+50%) - API requests per day: 80M (+60%) - Database size: 2TB (+67%) - Required scaling: +30% compute, +50% storage ### Contact Information - **Architecture Team**: architecture@cloudflow.internal - **On-Call Engineer**: PagerDuty escalation - **Security Team**: security@cloudflow.internal - **Documentation**: https://wiki.cloudflow.internal/architecture --- **Document Revision History**: - v2.3.1 (Jan 15, 2026): Updated performance metrics, added DR test results - v2.3.0 (Dec 1, 2025): Added",
              "score": null
            },
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_0",
              "content": "# CloudFlow API Reference Version 2.1.0 | Last Updated: January 2026 ## Overview The CloudFlow API is a RESTful service that enables developers to programmatically manage cloud workflows, data pipelines, and automation tasks. This documentation provides comprehensive details on authentication, endpoints, request/response formats, error handling, and best practices. **Base URL:** `https://api.cloudflow.io/v2` **API Status:** https://status.cloudflow.io ## Authentication CloudFlow supports three authentication methods to suit different use cases and security requirements.",
              "score": null
            }
          ],
          "latency_ms": 3.026369999133749,
          "query_id": "realistic_004",
          "dimension": "negation",
          "query": "Is CloudFlow using MySQL or something else?"
        },
        {
          "key_facts": [
            "cloudflow-prod"
          ],
          "found_facts": [
            "cloudflow-prod"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "deployment_guide"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_36",
              "content": "# View all resources in namespace kubectl get all -n cloudflow-prod # Check resource usage kubectl top pods -n cloudflow-prod kubectl top nodes # View logs kubectl logs -f deployment/cloudflow-api -n cloudflow-prod # Execute command in pod kubectl exec -it <pod-name> -n cloudflow-prod -- /bin/sh # Port forward for local testing kubectl port-forward svc/cloudflow-api 8080:80 -n cloudflow-prod # Rollback deployment helm rollback cloudflow -n cloudflow-prod # Update deployment helm upgrade cloudflow cloudflow/cloudflow \\ --namespace cloudflow-prod",
              "score": null
            },
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_33",
              "content": "namespace level: ```bash kubectl label namespace cloudflow-prod \\ pod-security.kubernetes.io/enforce=restricted \\ pod-security.kubernetes.io/audit=restricted \\ pod-security.kubernetes.io/warn=restricted ``` ### Secrets Management Use AWS Secrets Manager for sensitive credentials: ```bash # Install External Secrets Operator helm install external-secrets external-secrets/external-secrets \\ --namespace external-secrets-system \\ --create-namespace # Create SecretStore kubectl apply -f - <<EOF apiVersion: external-secrets.io/v1beta1 kind: SecretStore metadata: name: aws-secrets-manager namespace: cloudflow-prod spec: provider: aws: service: SecretsManager region: us-east-1 auth: jwt: serviceAccountRef: name: external-secrets-sa EOF ``` --- ## Troubleshooting ### Common",
              "score": null
            },
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_0",
              "content": "# CloudFlow Platform - Deployment and Operations Guide **Version:** 2.4.0 **Last Updated:** January 2026 **Target Environment:** Production (AWS EKS) ## Table of Contents 1. [Overview](#overview) 2. [Prerequisites](#prerequisites) 3. [Infrastructure Setup](#infrastructure-setup) 4. [Kubernetes Deployment](#kubernetes-deployment) 5. [Database Configuration](#database-configuration) 6. [Monitoring and Observability](#monitoring-and-observability) 7. [Backup and Disaster Recovery](#backup-and-disaster-recovery) 8. [Scaling and Performance](#scaling-and-performance) 9. [Security and Compliance](#security-and-compliance) 10. [Troubleshooting](#troubleshooting) --- ## Overview CloudFlow is a cloud-native workflow orchestration platform designed for high-availability production environments.",
              "score": null
            },
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_6",
              "content": "--- apiVersion: v1 kind: ResourceQuota metadata: name: cloudflow-quota namespace: cloudflow-prod spec: hard: requests.cpu: \"50\" requests.memory: 100Gi persistentvolumeclaims: \"10\" services.loadbalancers: \"2\" ``` Apply the namespace configuration: ```bash kubectl apply -f namespace.yaml ``` ### Environment Configuration CloudFlow requires the following environment variables: | Variable | Description | Example | Required | |----------|-------------|---------|----------| | `DATABASE_URL` | PostgreSQL connection string | `postgresql://user:pass@host:5432/cloudflow` | Yes | | `REDIS_URL` | Redis connection string | `redis://redis-master.cloudflow-prod.svc.cluster.local:6379` | Yes | | `JWT_SECRET` |",
              "score": null
            },
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_1",
              "content": "This guide provides comprehensive instructions for deploying and operating CloudFlow on Amazon EKS (Elastic Kubernetes Service). ### Architecture Summary CloudFlow consists of the following components: - **API Server**: REST API for workflow management (Node.js/Express) - **Worker Service**: Background job processor (Node.js) - **Scheduler**: Cron-based task scheduler (Node.js) - **PostgreSQL**: Primary data store (version 14) - **Redis**: Cache and message queue (version 7.0) - **PgBouncer**: Database connection pooler ### Deployment Model - **Namespace**: `cloudflow-prod` - **Cluster**:",
              "score": null
            }
          ],
          "latency_ms": 3.580762999263243,
          "query_id": "realistic_005",
          "dimension": "original",
          "query": "What is the Kubernetes namespace for production deployment?"
        },
        {
          "key_facts": [
            "cloudflow-prod"
          ],
          "found_facts": [
            "cloudflow-prod"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "deployment_guide"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_0",
              "content": "# CloudFlow Platform - System Architecture Overview **Document Version:** 2.3.1 **Last Updated:** January 15, 2026 **Owner:** Platform Architecture Team **Status:** Production ## Executive Summary CloudFlow is a distributed, cloud-native workflow automation platform designed to orchestrate complex business processes at scale. The platform processes over 2.5 million workflow executions daily with an average P99 latency of 180ms for API operations and 4.2 seconds for workflow execution.",
              "score": null
            },
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_1",
              "content": "This guide provides comprehensive instructions for deploying and operating CloudFlow on Amazon EKS (Elastic Kubernetes Service). ### Architecture Summary CloudFlow consists of the following components: - **API Server**: REST API for workflow management (Node.js/Express) - **Worker Service**: Background job processor (Node.js) - **Scheduler**: Cron-based task scheduler (Node.js) - **PostgreSQL**: Primary data store (version 14) - **Redis**: Cache and message queue (version 7.0) - **PgBouncer**: Database connection pooler ### Deployment Model - **Namespace**: `cloudflow-prod` - **Cluster**:",
              "score": null
            },
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_0",
              "content": "# CloudFlow Platform - Deployment and Operations Guide **Version:** 2.4.0 **Last Updated:** January 2026 **Target Environment:** Production (AWS EKS) ## Table of Contents 1. [Overview](#overview) 2. [Prerequisites](#prerequisites) 3. [Infrastructure Setup](#infrastructure-setup) 4. [Kubernetes Deployment](#kubernetes-deployment) 5. [Database Configuration](#database-configuration) 6. [Monitoring and Observability](#monitoring-and-observability) 7. [Backup and Disaster Recovery](#backup-and-disaster-recovery) 8. [Scaling and Performance](#scaling-and-performance) 9. [Security and Compliance](#security-and-compliance) 10. [Troubleshooting](#troubleshooting) --- ## Overview CloudFlow is a cloud-native workflow orchestration platform designed for high-availability production environments.",
              "score": null
            },
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_6",
              "content": "--- apiVersion: v1 kind: ResourceQuota metadata: name: cloudflow-quota namespace: cloudflow-prod spec: hard: requests.cpu: \"50\" requests.memory: 100Gi persistentvolumeclaims: \"10\" services.loadbalancers: \"2\" ``` Apply the namespace configuration: ```bash kubectl apply -f namespace.yaml ``` ### Environment Configuration CloudFlow requires the following environment variables: | Variable | Description | Example | Required | |----------|-------------|---------|----------| | `DATABASE_URL` | PostgreSQL connection string | `postgresql://user:pass@host:5432/cloudflow` | Yes | | `REDIS_URL` | Redis connection string | `redis://redis-master.cloudflow-prod.svc.cluster.local:6379` | Yes | | `JWT_SECRET` |",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_63",
              "content": "**6-Month Projection** (Jul 2026): - Daily workflow executions: 4M (+60%) - Active users: 225,000 (+50%) - API requests per day: 80M (+60%) - Database size: 2TB (+67%) - Required scaling: +30% compute, +50% storage ### Contact Information - **Architecture Team**: architecture@cloudflow.internal - **On-Call Engineer**: PagerDuty escalation - **Security Team**: security@cloudflow.internal - **Documentation**: https://wiki.cloudflow.internal/architecture --- **Document Revision History**: - v2.3.1 (Jan 15, 2026): Updated performance metrics, added DR test results - v2.3.0 (Dec 1, 2025): Added",
              "score": null
            }
          ],
          "latency_ms": 3.130585000690189,
          "query_id": "realistic_005",
          "dimension": "synonym",
          "query": "Which namespace is used for CloudFlow production?"
        },
        {
          "key_facts": [
            "cloudflow-prod"
          ],
          "found_facts": [
            "cloudflow-prod"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "deployment_guide"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_34",
              "content": "Issues #### Pods in CrashLoopBackOff **Symptoms**: Pods continuously restart **Diagnosis**: ```bash kubectl logs -n cloudflow-prod <pod-name> --previous kubectl describe pod -n cloudflow-prod <pod-name> ``` **Common Causes**: - Database connection failure - Invalid environment variables - Insufficient resources #### High Memory Usage **Symptoms**: Pods being OOMKilled **Diagnosis**: ```bash kubectl top pods -n cloudflow-prod ``` **Resolution**: - Increase memory limits in deployment - Check for memory leaks in application logs - Review heap dump for Node.js processes",
              "score": null
            },
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_36",
              "content": "# View all resources in namespace kubectl get all -n cloudflow-prod # Check resource usage kubectl top pods -n cloudflow-prod kubectl top nodes # View logs kubectl logs -f deployment/cloudflow-api -n cloudflow-prod # Execute command in pod kubectl exec -it <pod-name> -n cloudflow-prod -- /bin/sh # Port forward for local testing kubectl port-forward svc/cloudflow-api 8080:80 -n cloudflow-prod # Rollback deployment helm rollback cloudflow -n cloudflow-prod # Update deployment helm upgrade cloudflow cloudflow/cloudflow \\ --namespace cloudflow-prod",
              "score": null
            },
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_12",
              "content": "\\ --namespace cloudflow-prod \\ --values values-production.yaml \\ --wait \\ --timeout 10m ``` ### Deployment Verification Verify the deployment status: ```bash # Check pod status kubectl get pods -n cloudflow-prod # Expected output: # NAME READY STATUS RESTARTS AGE # cloudflow-api-7d8f9c5b6d-4xk2p 1/1 Running 0 2m # cloudflow-api-7d8f9c5b6d-9hj5m 1/1 Running 0 2m # cloudflow-api-7d8f9c5b6d-tn8wq 1/1 Running 0 2m # Check service endpoints kubectl get svc -n cloudflow-prod # Check ingress kubectl get ingress -n cloudflow-prod ``` Test",
              "score": null
            },
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_21",
              "content": "JSON template. Key dashboard panels include: 1. **API Performance**: Request rate, P95/P99 latency, error rate 2. **Resource Usage**: CPU, memory, disk I/O per pod 3. **Database Health**: Connection pool utilization, query performance 4. **Worker Status**: Queue depth, processing rate, job success/failure ratio 5. **System Overview**: Pod status, replica count, autoscaling events ### Logging with CloudWatch CloudFlow logs are automatically shipped to CloudWatch Logs.",
              "score": null
            },
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_32",
              "content": "networking.k8s.io/v1 kind: NetworkPolicy metadata: name: cloudflow-network-policy namespace: cloudflow-prod spec: podSelector: matchLabels: app: cloudflow policyTypes: - Ingress - Egress ingress: - from: - namespaceSelector: matchLabels: name: cloudflow-prod - podSelector: matchLabels: app: nginx-ingress ports: - protocol: TCP port: 3000 egress: - to: - podSelector: matchLabels: app: postgresql ports: - protocol: TCP port: 5432 - to: - podSelector: matchLabels: app: redis ports: - protocol: TCP port: 6379 ``` ### Pod Security Standards Enforce Pod Security Standards at the",
              "score": null
            }
          ],
          "latency_ms": 3.066616000069189,
          "query_id": "realistic_005",
          "dimension": "problem",
          "query": "I can't find the production pods"
        },
        {
          "key_facts": [
            "cloudflow-prod"
          ],
          "found_facts": [
            "cloudflow-prod"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "deployment_guide"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_36",
              "content": "# View all resources in namespace kubectl get all -n cloudflow-prod # Check resource usage kubectl top pods -n cloudflow-prod kubectl top nodes # View logs kubectl logs -f deployment/cloudflow-api -n cloudflow-prod # Execute command in pod kubectl exec -it <pod-name> -n cloudflow-prod -- /bin/sh # Port forward for local testing kubectl port-forward svc/cloudflow-api 8080:80 -n cloudflow-prod # Rollback deployment helm rollback cloudflow -n cloudflow-prod # Update deployment helm upgrade cloudflow cloudflow/cloudflow \\ --namespace cloudflow-prod",
              "score": null
            },
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_6",
              "content": "--- apiVersion: v1 kind: ResourceQuota metadata: name: cloudflow-quota namespace: cloudflow-prod spec: hard: requests.cpu: \"50\" requests.memory: 100Gi persistentvolumeclaims: \"10\" services.loadbalancers: \"2\" ``` Apply the namespace configuration: ```bash kubectl apply -f namespace.yaml ``` ### Environment Configuration CloudFlow requires the following environment variables: | Variable | Description | Example | Required | |----------|-------------|---------|----------| | `DATABASE_URL` | PostgreSQL connection string | `postgresql://user:pass@host:5432/cloudflow` | Yes | | `REDIS_URL` | Redis connection string | `redis://redis-master.cloudflow-prod.svc.cluster.local:6379` | Yes | | `JWT_SECRET` |",
              "score": null
            },
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_33",
              "content": "namespace level: ```bash kubectl label namespace cloudflow-prod \\ pod-security.kubernetes.io/enforce=restricted \\ pod-security.kubernetes.io/audit=restricted \\ pod-security.kubernetes.io/warn=restricted ``` ### Secrets Management Use AWS Secrets Manager for sensitive credentials: ```bash # Install External Secrets Operator helm install external-secrets external-secrets/external-secrets \\ --namespace external-secrets-system \\ --create-namespace # Create SecretStore kubectl apply -f - <<EOF apiVersion: external-secrets.io/v1beta1 kind: SecretStore metadata: name: aws-secrets-manager namespace: cloudflow-prod spec: provider: aws: service: SecretsManager region: us-east-1 auth: jwt: serviceAccountRef: name: external-secrets-sa EOF ``` --- ## Troubleshooting ### Common",
              "score": null
            },
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_12",
              "content": "\\ --namespace cloudflow-prod \\ --values values-production.yaml \\ --wait \\ --timeout 10m ``` ### Deployment Verification Verify the deployment status: ```bash # Check pod status kubectl get pods -n cloudflow-prod # Expected output: # NAME READY STATUS RESTARTS AGE # cloudflow-api-7d8f9c5b6d-4xk2p 1/1 Running 0 2m # cloudflow-api-7d8f9c5b6d-9hj5m 1/1 Running 0 2m # cloudflow-api-7d8f9c5b6d-tn8wq 1/1 Running 0 2m # Check service endpoints kubectl get svc -n cloudflow-prod # Check ingress kubectl get ingress -n cloudflow-prod ``` Test",
              "score": null
            },
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_17",
              "content": "1 --- apiVersion: apps/v1 kind: Deployment metadata: name: pgbouncer namespace: cloudflow-prod spec: replicas: 2 selector: matchLabels: app: pgbouncer template: metadata: labels: app: pgbouncer spec: containers: - name: pgbouncer image: pgbouncer/pgbouncer:1.21.0 ports: - containerPort: 5432 resources: requests: cpu: 500m memory: 512Mi limits: cpu: 1000m memory: 1Gi volumeMounts: - name: config mountPath: /etc/pgbouncer volumes: - name: config configMap: name: pgbouncer-config --- apiVersion: v1 kind: Service metadata: name: pgbouncer namespace: cloudflow-prod spec: selector: app: pgbouncer ports: - port:",
              "score": null
            }
          ],
          "latency_ms": 2.9922559988335706,
          "query_id": "realistic_005",
          "dimension": "casual",
          "query": "prod namespace"
        },
        {
          "key_facts": [
            "cloudflow-prod"
          ],
          "found_facts": [
            "cloudflow-prod"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "deployment_guide"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_36",
              "content": "# View all resources in namespace kubectl get all -n cloudflow-prod # Check resource usage kubectl top pods -n cloudflow-prod kubectl top nodes # View logs kubectl logs -f deployment/cloudflow-api -n cloudflow-prod # Execute command in pod kubectl exec -it <pod-name> -n cloudflow-prod -- /bin/sh # Port forward for local testing kubectl port-forward svc/cloudflow-api 8080:80 -n cloudflow-prod # Rollback deployment helm rollback cloudflow -n cloudflow-prod # Update deployment helm upgrade cloudflow cloudflow/cloudflow \\ --namespace cloudflow-prod",
              "score": null
            },
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_0",
              "content": "# CloudFlow Platform - Deployment and Operations Guide **Version:** 2.4.0 **Last Updated:** January 2026 **Target Environment:** Production (AWS EKS) ## Table of Contents 1. [Overview](#overview) 2. [Prerequisites](#prerequisites) 3. [Infrastructure Setup](#infrastructure-setup) 4. [Kubernetes Deployment](#kubernetes-deployment) 5. [Database Configuration](#database-configuration) 6. [Monitoring and Observability](#monitoring-and-observability) 7. [Backup and Disaster Recovery](#backup-and-disaster-recovery) 8. [Scaling and Performance](#scaling-and-performance) 9. [Security and Compliance](#security-and-compliance) 10. [Troubleshooting](#troubleshooting) --- ## Overview CloudFlow is a cloud-native workflow orchestration platform designed for high-availability production environments.",
              "score": null
            },
            {
              "doc_id": "user_guide",
              "chunk_id": "user_guide_fix_44",
              "content": "Version Control For critical workflows: - Export YAML definitions regularly - Store in version control (Git) - Use pull requests for changes - Tag releases with semantic versioning ## Common Workflow Patterns Here are proven patterns for common automation scenarios: ### Pattern 1: Form to Database Capture form submissions and store in database: ```yaml name: \"Contact Form to Database\" trigger: type: webhook method: POST steps: - id: validate_submission action: javascript code: | if (!input.email ||",
              "score": null
            },
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_1",
              "content": "This guide provides comprehensive instructions for deploying and operating CloudFlow on Amazon EKS (Elastic Kubernetes Service). ### Architecture Summary CloudFlow consists of the following components: - **API Server**: REST API for workflow management (Node.js/Express) - **Worker Service**: Background job processor (Node.js) - **Scheduler**: Cron-based task scheduler (Node.js) - **PostgreSQL**: Primary data store (version 14) - **Redis**: Cache and message queue (version 7.0) - **PgBouncer**: Database connection pooler ### Deployment Model - **Namespace**: `cloudflow-prod` - **Cluster**:",
              "score": null
            },
            {
              "doc_id": "user_guide",
              "chunk_id": "user_guide_fix_9",
              "content": "``` **Benefits of YAML Definitions:** - Version control friendly (commit to Git) - Easy to share and duplicate workflows - Supports comments and documentation - Can be generated programmatically - Enables infrastructure-as-code practices To import a YAML workflow, click **\"Import\"** > **\"From YAML\"** in your dashboard. ### Triggers Triggers determine when your workflow runs.",
              "score": null
            }
          ],
          "latency_ms": 3.1877010005700868,
          "query_id": "realistic_005",
          "dimension": "contextual",
          "query": "I need to deploy to production, what namespace should I use?"
        },
        {
          "key_facts": [
            "cloudflow-prod"
          ],
          "found_facts": [
            "cloudflow-prod"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "deployment_guide"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_33",
              "content": "namespace level: ```bash kubectl label namespace cloudflow-prod \\ pod-security.kubernetes.io/enforce=restricted \\ pod-security.kubernetes.io/audit=restricted \\ pod-security.kubernetes.io/warn=restricted ``` ### Secrets Management Use AWS Secrets Manager for sensitive credentials: ```bash # Install External Secrets Operator helm install external-secrets external-secrets/external-secrets \\ --namespace external-secrets-system \\ --create-namespace # Create SecretStore kubectl apply -f - <<EOF apiVersion: external-secrets.io/v1beta1 kind: SecretStore metadata: name: aws-secrets-manager namespace: cloudflow-prod spec: provider: aws: service: SecretsManager region: us-east-1 auth: jwt: serviceAccountRef: name: external-secrets-sa EOF ``` --- ## Troubleshooting ### Common",
              "score": null
            },
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_19",
              "content": "resource | | 404 | Not Found | Resource does not exist or has been deleted | | 429 | Too Many Requests | Rate limit exceeded, retry after specified period | | 500 | Internal Server Error | Unexpected server error, contact support if persists | | 502 | Bad Gateway | Temporary service issue, retry with exponential backoff | | 503 | Service Unavailable | Scheduled maintenance or temporary outage | ### Error",
              "score": null
            },
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_6",
              "content": "--- apiVersion: v1 kind: ResourceQuota metadata: name: cloudflow-quota namespace: cloudflow-prod spec: hard: requests.cpu: \"50\" requests.memory: 100Gi persistentvolumeclaims: \"10\" services.loadbalancers: \"2\" ``` Apply the namespace configuration: ```bash kubectl apply -f namespace.yaml ``` ### Environment Configuration CloudFlow requires the following environment variables: | Variable | Description | Example | Required | |----------|-------------|---------|----------| | `DATABASE_URL` | PostgreSQL connection string | `postgresql://user:pass@host:5432/cloudflow` | Yes | | `REDIS_URL` | Redis connection string | `redis://redis-master.cloudflow-prod.svc.cluster.local:6379` | Yes | | `JWT_SECRET` |",
              "score": null
            },
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_36",
              "content": "# View all resources in namespace kubectl get all -n cloudflow-prod # Check resource usage kubectl top pods -n cloudflow-prod kubectl top nodes # View logs kubectl logs -f deployment/cloudflow-api -n cloudflow-prod # Execute command in pod kubectl exec -it <pod-name> -n cloudflow-prod -- /bin/sh # Port forward for local testing kubectl port-forward svc/cloudflow-api 8080:80 -n cloudflow-prod # Rollback deployment helm rollback cloudflow -n cloudflow-prod # Update deployment helm upgrade cloudflow cloudflow/cloudflow \\ --namespace cloudflow-prod",
              "score": null
            },
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_37",
              "content": "\\ --values values-production.yaml ``` ### Maintenance Windows Scheduled maintenance occurs during the following windows: - **Primary**: Sunday 02:00-06:00 UTC - **Secondary**: Wednesday 10:00-12:00 UTC All deployments should target these windows to minimize user impact. --- **Document Version**: 2.4.0 **Maintained by**: Platform Engineering Team **Last Review**: January 24, 2026",
              "score": null
            }
          ],
          "latency_ms": 3.350073000547127,
          "query_id": "realistic_005",
          "dimension": "negation",
          "query": "Why can't I see resources in the default namespace?"
        },
        {
          "key_facts": [
            "2 vCPU, 4GB RAM per pod"
          ],
          "found_facts": [
            "2 vCPU, 4GB RAM per pod"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "architecture_overview"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_5",
              "content": "providing authentication, rate limiting, request routing, and protocol translation. **Technology**: Node.js with Express.js framework **Replicas**: 12 pods (production), auto-scaling 8-20 based on CPU **Resource Allocation**: 2 vCPU, 4GB RAM per pod **Key Responsibilities**: - JWT token validation (delegated to Auth Service for initial validation) - Rate limiting: 1000 requests per minute per API key (sliding window) - Request/response transformation and validation using JSON Schema - Routing to downstream services based on URL path patterns -",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_19",
              "content": "SMS delivery: < 3 seconds (P99) - Throughput: 10,000 notifications per minute --- ## Data Flow Architecture ### Synchronous Request Flow Client requests follow a synchronous path through the API Gateway to backend services: ``` 1. Client \u2192 Load Balancer (TLS termination) 2. Load Balancer \u2192 API Gateway (HTTP/2) 3. API Gateway \u2192 Auth Service (JWT validation via gRPC) 4. API Gateway \u2192 Target Service (HTTP/REST or gRPC) 5.",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_50",
              "content": "End-to-end latency: < 100ms (P99) ### Resource Utilization **CPU Utilization** (Target: 60-70% average): - API Gateway: 55% average, 80% peak - Workflow Engine: 65% average, 85% peak - Auth Service: 40% average, 60% peak **Memory Utilization** (Target: < 80%): - API Gateway: 2.5GB average per pod (4GB allocated) - Workflow Engine: 6GB average per pod (8GB allocated) - Notification Service: 2.8GB average per pod (4GB allocated) **Network Throughput**: - Ingress: 2 Gbps average, 5 Gbps",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_49",
              "content": "limiting: 1,000 requests per minute per API key **Workflow Engine**: - Concurrent executions: 8,000 workflows (across 16 pods) - Execution start rate: 500 per second - Completion rate: 450 per second (average 2-second execution time) **Database**: - Read throughput: 50,000 queries per second (across replicas) - Write throughput: 15,000 transactions per second - Connection capacity: 2,000 concurrent connections **Kafka**: - Message ingestion: 100,000 messages per second - Consumer throughput: 80,000 messages per second (aggregated) -",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_22",
              "content": "API Gateway - User Service profile lookups - Low latency requirement (< 50ms) - Request-response pattern with circuit breaker **Asynchronous (Kafka)**: - Workflow execution events - Notification triggers - Audit log events - Eventual consistency acceptable --- ## Database Architecture ### PostgreSQL Primary Database **Cluster Configuration**: - Primary-replica setup with 1 primary + 2 read replicas - Instance type: db.r6g.2xlarge (8 vCPU, 64GB RAM) - Storage: 2TB gp3 SSD with 12,000 IOPS - Multi-AZ deployment",
              "score": null
            }
          ],
          "latency_ms": 3.0349159987963503,
          "query_id": "realistic_006",
          "dimension": "original",
          "query": "What are the resource requirements for the API Gateway?"
        },
        {
          "key_facts": [
            "2 vCPU, 4GB RAM per pod"
          ],
          "found_facts": [],
          "missed_facts": [
            "2 vCPU, 4GB RAM per pod"
          ],
          "coverage": 0.0,
          "expected_docs": [
            "architecture_overview"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_50",
              "content": "End-to-end latency: < 100ms (P99) ### Resource Utilization **CPU Utilization** (Target: 60-70% average): - API Gateway: 55% average, 80% peak - Workflow Engine: 65% average, 85% peak - Auth Service: 40% average, 60% peak **Memory Utilization** (Target: < 80%): - API Gateway: 2.5GB average per pod (4GB allocated) - Workflow Engine: 6GB average per pod (8GB allocated) - Notification Service: 2.8GB average per pod (4GB allocated) **Network Throughput**: - Ingress: 2 Gbps average, 5 Gbps",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_49",
              "content": "limiting: 1,000 requests per minute per API key **Workflow Engine**: - Concurrent executions: 8,000 workflows (across 16 pods) - Execution start rate: 500 per second - Completion rate: 450 per second (average 2-second execution time) **Database**: - Read throughput: 50,000 queries per second (across replicas) - Write throughput: 15,000 transactions per second - Connection capacity: 2,000 concurrent connections **Kafka**: - Message ingestion: 100,000 messages per second - Consumer throughput: 80,000 messages per second (aggregated) -",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_22",
              "content": "API Gateway - User Service profile lookups - Low latency requirement (< 50ms) - Request-response pattern with circuit breaker **Asynchronous (Kafka)**: - Workflow execution events - Notification triggers - Audit log events - Eventual consistency acceptable --- ## Database Architecture ### PostgreSQL Primary Database **Cluster Configuration**: - Primary-replica setup with 1 primary + 2 read replicas - Instance type: db.r6g.2xlarge (8 vCPU, 64GB RAM) - Storage: 2TB gp3 SSD with 12,000 IOPS - Multi-AZ deployment",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_19",
              "content": "SMS delivery: < 3 seconds (P99) - Throughput: 10,000 notifications per minute --- ## Data Flow Architecture ### Synchronous Request Flow Client requests follow a synchronous path through the API Gateway to backend services: ``` 1. Client \u2192 Load Balancer (TLS termination) 2. Load Balancer \u2192 API Gateway (HTTP/2) 3. API Gateway \u2192 Auth Service (JWT validation via gRPC) 4. API Gateway \u2192 Target Service (HTTP/REST or gRPC) 5.",
              "score": null
            },
            {
              "doc_id": "user_guide",
              "chunk_id": "user_guide_fix_36",
              "content": "- Success rate: 98.2% - Current queue depth: 3 ### Data Limits - **Maximum request/response size**: 10MB per action - **Maximum execution payload**: 50MB total - **Variable value size**: 1MB per variable ### Enterprise Plan Limits Enterprise customers can request increased limits: - Up to 100 steps per workflow - Up to 10,000 executions per day - Up to 7200 second timeout (2 hours) - Priority execution queue - Dedicated capacity allocation Contact sales@cloudflow.io for",
              "score": null
            }
          ],
          "latency_ms": 3.1266870009858394,
          "query_id": "realistic_006",
          "dimension": "synonym",
          "query": "How much CPU and memory does the API Gateway need?"
        },
        {
          "key_facts": [
            "2 vCPU, 4GB RAM per pod"
          ],
          "found_facts": [],
          "missed_facts": [
            "2 vCPU, 4GB RAM per pod"
          ],
          "coverage": 0.0,
          "expected_docs": [
            "architecture_overview"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_34",
              "content": "Issues #### Pods in CrashLoopBackOff **Symptoms**: Pods continuously restart **Diagnosis**: ```bash kubectl logs -n cloudflow-prod <pod-name> --previous kubectl describe pod -n cloudflow-prod <pod-name> ``` **Common Causes**: - Database connection failure - Invalid environment variables - Insufficient resources #### High Memory Usage **Symptoms**: Pods being OOMKilled **Diagnosis**: ```bash kubectl top pods -n cloudflow-prod ``` **Resolution**: - Increase memory limits in deployment - Check for memory leaks in application logs - Review heap dump for Node.js processes",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_4",
              "content": "\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 ``` ### Technology Stack - **Runtime**: Node.js 20.x (API Gateway, Workflow Engine), Go 1.21 (Auth Service, Scheduler) - **Container Orchestration**: Kubernetes 1.28 (EKS) - **Service Mesh**: Istio 1.20 for service-to-service communication - **Databases**: PostgreSQL 15.4, Redis 7.2 - **Message Broker**: Apache Kafka 3.6 - **Monitoring**: Prometheus, Grafana, Jaeger for distributed tracing - **Secrets Management**: HashiCorp Vault 1.15 --- ## Microservices Breakdown ### API Gateway **Purpose**: Single entry point for all client requests,",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_14",
              "content": "### Memory Leaks #### Detection ```bash # Monitor CloudFlow service memory usage kubectl top pods -n cloudflow --sort-by=memory # Get detailed memory metrics for specific pod kubectl exec -n cloudflow deploy/cloudflow-api -- \\ curl localhost:9090/metrics | grep memory # Check for OOMKilled pods kubectl get pods -n cloudflow --field-selector=status.phase=Failed | \\ grep OOMKilled # Review memory limits and requests kubectl describe deployment cloudflow-api -n cloudflow | \\ grep -A 5 \"Limits\\|Requests\" ``` #### Common Causes",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_48",
              "content": "--tty \\ --image=nicolaka/netshoot -- /bin/bash # Inside pod: # Check DNS resolution nslookup api.cloudflow.io # Check connectivity to database nc -zv cloudflow-db.internal.company.com 5432 # Trace route traceroute api.cloudflow.io # Capture packets tcpdump -i any -w /tmp/capture.pcap port 5432 ``` --- ## Escalation Procedures ### Severity Levels CloudFlow incidents are classified into four severity levels: #### SEV-1: Critical (P1) - **Definition:** Complete service outage or severe degradation affecting all users - **Examples:** - API returns 5xx",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_1",
              "content": "Each section includes error symptoms, root cause analysis, resolution steps, and preventive measures. ### Quick Diagnostic Checklist Before diving into specific issues, perform these initial checks: - Verify service health: `cloudflow status --all` - Check API connectivity: `curl -I https://api.cloudflow.io/health` - Review recent deployments: `kubectl get deployments -n cloudflow --sort-by=.metadata.creationTimestamp` - Inspect platform metrics: `cloudflow metrics --last 1h` --- ## Authentication & Authorization Issues ### 401 Unauthorized Errors #### Symptoms - API requests return `401",
              "score": null
            }
          ],
          "latency_ms": 3.1559319995722035,
          "query_id": "realistic_006",
          "dimension": "problem",
          "query": "The API Gateway pods keep getting OOMKilled"
        },
        {
          "key_facts": [
            "2 vCPU, 4GB RAM per pod"
          ],
          "found_facts": [
            "2 vCPU, 4GB RAM per pod"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "architecture_overview"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_0",
              "content": "# CloudFlow API Reference Version 2.1.0 | Last Updated: January 2026 ## Overview The CloudFlow API is a RESTful service that enables developers to programmatically manage cloud workflows, data pipelines, and automation tasks. This documentation provides comprehensive details on authentication, endpoints, request/response formats, error handling, and best practices. **Base URL:** `https://api.cloudflow.io/v2` **API Status:** https://status.cloudflow.io ## Authentication CloudFlow supports three authentication methods to suit different use cases and security requirements.",
              "score": null
            },
            {
              "doc_id": "user_guide",
              "chunk_id": "user_guide_fix_12",
              "content": "### HTTP Requests Make HTTP requests to any API endpoint: **Configuration:** - **Method**: GET, POST, PUT, PATCH, DELETE, HEAD - **URL**: Full endpoint URL (supports variable interpolation) - **Headers**: Custom headers as key-value pairs - **Query Parameters**: URL parameters - **Body**: JSON, form data, or raw text - **Authentication**: Basic Auth, Bearer Token, API Key, OAuth 2.0 **Example:** ```yaml - id: fetch_user action: http_request config: method: GET url: \"https://api.example.com/users/{{user_id}}\" headers: Authorization: \"Bearer {{secrets.API_TOKEN}}\" Content-Type: \"application/json\"",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_19",
              "content": "SMS delivery: < 3 seconds (P99) - Throughput: 10,000 notifications per minute --- ## Data Flow Architecture ### Synchronous Request Flow Client requests follow a synchronous path through the API Gateway to backend services: ``` 1. Client \u2192 Load Balancer (TLS termination) 2. Load Balancer \u2192 API Gateway (HTTP/2) 3. API Gateway \u2192 Auth Service (JWT validation via gRPC) 4. API Gateway \u2192 Target Service (HTTP/REST or gRPC) 5.",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_6",
              "content": "CORS handling for web clients - Request/response logging and correlation ID injection **Critical Endpoints**: - `POST /api/v1/workflows` - Create new workflow - `GET /api/v1/workflows/:id` - Retrieve workflow status - `POST /api/v1/workflows/:id/execute` - Trigger workflow execution - `GET /api/v1/workflows/:id/history` - Get execution history **Dependencies**: - Auth Service (for token validation) - Redis (for rate limiting counters) - All downstream microservices **Performance Targets**: - P50 latency: < 50ms - P99 latency: < 200ms - Throughput: 10,000 RPS",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_5",
              "content": "providing authentication, rate limiting, request routing, and protocol translation. **Technology**: Node.js with Express.js framework **Replicas**: 12 pods (production), auto-scaling 8-20 based on CPU **Resource Allocation**: 2 vCPU, 4GB RAM per pod **Key Responsibilities**: - JWT token validation (delegated to Auth Service for initial validation) - Rate limiting: 1000 requests per minute per API key (sliding window) - Request/response transformation and validation using JSON Schema - Routing to downstream services based on URL path patterns -",
              "score": null
            }
          ],
          "latency_ms": 3.004098998644622,
          "query_id": "realistic_006",
          "dimension": "casual",
          "query": "api gateway resources"
        },
        {
          "key_facts": [
            "2 vCPU, 4GB RAM per pod"
          ],
          "found_facts": [
            "2 vCPU, 4GB RAM per pod"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "architecture_overview"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_50",
              "content": "End-to-end latency: < 100ms (P99) ### Resource Utilization **CPU Utilization** (Target: 60-70% average): - API Gateway: 55% average, 80% peak - Workflow Engine: 65% average, 85% peak - Auth Service: 40% average, 60% peak **Memory Utilization** (Target: < 80%): - API Gateway: 2.5GB average per pod (4GB allocated) - Workflow Engine: 6GB average per pod (8GB allocated) - Notification Service: 2.8GB average per pod (4GB allocated) **Network Throughput**: - Ingress: 2 Gbps average, 5 Gbps",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_22",
              "content": "API Gateway - User Service profile lookups - Low latency requirement (< 50ms) - Request-response pattern with circuit breaker **Asynchronous (Kafka)**: - Workflow execution events - Notification triggers - Audit log events - Eventual consistency acceptable --- ## Database Architecture ### PostgreSQL Primary Database **Cluster Configuration**: - Primary-replica setup with 1 primary + 2 read replicas - Instance type: db.r6g.2xlarge (8 vCPU, 64GB RAM) - Storage: 2TB gp3 SSD with 12,000 IOPS - Multi-AZ deployment",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_5",
              "content": "providing authentication, rate limiting, request routing, and protocol translation. **Technology**: Node.js with Express.js framework **Replicas**: 12 pods (production), auto-scaling 8-20 based on CPU **Resource Allocation**: 2 vCPU, 4GB RAM per pod **Key Responsibilities**: - JWT token validation (delegated to Auth Service for initial validation) - Rate limiting: 1000 requests per minute per API key (sliding window) - Request/response transformation and validation using JSON Schema - Routing to downstream services based on URL path patterns -",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_63",
              "content": "**6-Month Projection** (Jul 2026): - Daily workflow executions: 4M (+60%) - Active users: 225,000 (+50%) - API requests per day: 80M (+60%) - Database size: 2TB (+67%) - Required scaling: +30% compute, +50% storage ### Contact Information - **Architecture Team**: architecture@cloudflow.internal - **On-Call Engineer**: PagerDuty escalation - **Security Team**: security@cloudflow.internal - **Documentation**: https://wiki.cloudflow.internal/architecture --- **Document Revision History**: - v2.3.1 (Jan 15, 2026): Updated performance metrics, added DR test results - v2.3.0 (Dec 1, 2025): Added",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_19",
              "content": "SMS delivery: < 3 seconds (P99) - Throughput: 10,000 notifications per minute --- ## Data Flow Architecture ### Synchronous Request Flow Client requests follow a synchronous path through the API Gateway to backend services: ``` 1. Client \u2192 Load Balancer (TLS termination) 2. Load Balancer \u2192 API Gateway (HTTP/2) 3. API Gateway \u2192 Auth Service (JWT validation via gRPC) 4. API Gateway \u2192 Target Service (HTTP/REST or gRPC) 5.",
              "score": null
            }
          ],
          "latency_ms": 3.1947539991961094,
          "query_id": "realistic_006",
          "dimension": "contextual",
          "query": "I'm provisioning infrastructure, what are the compute specs for API Gateway?"
        },
        {
          "key_facts": [
            "2 vCPU, 4GB RAM per pod"
          ],
          "found_facts": [],
          "missed_facts": [
            "2 vCPU, 4GB RAM per pod"
          ],
          "coverage": 0.0,
          "expected_docs": [
            "architecture_overview"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_50",
              "content": "End-to-end latency: < 100ms (P99) ### Resource Utilization **CPU Utilization** (Target: 60-70% average): - API Gateway: 55% average, 80% peak - Workflow Engine: 65% average, 85% peak - Auth Service: 40% average, 60% peak **Memory Utilization** (Target: < 80%): - API Gateway: 2.5GB average per pod (4GB allocated) - Workflow Engine: 6GB average per pod (8GB allocated) - Notification Service: 2.8GB average per pod (4GB allocated) **Network Throughput**: - Ingress: 2 Gbps average, 5 Gbps",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_22",
              "content": "API Gateway - User Service profile lookups - Low latency requirement (< 50ms) - Request-response pattern with circuit breaker **Asynchronous (Kafka)**: - Workflow execution events - Notification triggers - Audit log events - Eventual consistency acceptable --- ## Database Architecture ### PostgreSQL Primary Database **Cluster Configuration**: - Primary-replica setup with 1 primary + 2 read replicas - Instance type: db.r6g.2xlarge (8 vCPU, 64GB RAM) - Storage: 2TB gp3 SSD with 12,000 IOPS - Multi-AZ deployment",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_49",
              "content": "limiting: 1,000 requests per minute per API key **Workflow Engine**: - Concurrent executions: 8,000 workflows (across 16 pods) - Execution start rate: 500 per second - Completion rate: 450 per second (average 2-second execution time) **Database**: - Read throughput: 50,000 queries per second (across replicas) - Write throughput: 15,000 transactions per second - Connection capacity: 2,000 concurrent connections **Kafka**: - Message ingestion: 100,000 messages per second - Consumer throughput: 80,000 messages per second (aggregated) -",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_32",
              "content": "Limiting & Throttling ### 429 Too Many Requests #### Error Response ```json { \"error\": \"rate_limit_exceeded\", \"message\": \"API rate limit exceeded. Retry after 45 seconds.\", \"status\": 429, \"headers\": { \"X-RateLimit-Limit\": \"1000\", \"X-RateLimit-Remaining\": \"0\", \"X-RateLimit-Reset\": \"1706112345\", \"Retry-After\": \"45\" } } ``` #### Rate Limit Tiers CloudFlow enforces the following rate limits per workspace: | Tier | Requests/Minute | Requests/Hour | Concurrent Workflows | |------|-----------------|---------------|----------------------| | Free | 60 | 1,000 | 5 | | Standard | 1,000",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_14",
              "content": "### Memory Leaks #### Detection ```bash # Monitor CloudFlow service memory usage kubectl top pods -n cloudflow --sort-by=memory # Get detailed memory metrics for specific pod kubectl exec -n cloudflow deploy/cloudflow-api -- \\ curl localhost:9090/metrics | grep memory # Check for OOMKilled pods kubectl get pods -n cloudflow --field-selector=status.phase=Failed | \\ grep OOMKilled # Review memory limits and requests kubectl describe deployment cloudflow-api -n cloudflow | \\ grep -A 5 \"Limits\\|Requests\" ``` #### Common Causes",
              "score": null
            }
          ],
          "latency_ms": 3.0537010006810306,
          "query_id": "realistic_006",
          "dimension": "negation",
          "query": "Why is 1GB RAM not enough for API Gateway?"
        },
        {
          "key_facts": [
            "/health",
            "/ready"
          ],
          "found_facts": [
            "/health"
          ],
          "missed_facts": [
            "/ready"
          ],
          "coverage": 0.5,
          "expected_docs": [
            "deployment_guide"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_57",
              "content": "minutes (no manual intervention) - Data loss: None (multi-AZ replication) **Scenario 2: Database Primary Failure** - Detection: Health check fails for primary database (< 30 seconds) - Action: Automatic promotion of read replica to primary - Recovery time: 30-60 seconds - Data loss: Minimal (< 1 second due to synchronous replication) **Scenario 3: Full Region Failure** - Detection: Multiple health check failures across all AZs (< 2 minutes) - Action: Manual failover to DR region",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_1",
              "content": "Each section includes error symptoms, root cause analysis, resolution steps, and preventive measures. ### Quick Diagnostic Checklist Before diving into specific issues, perform these initial checks: - Verify service health: `cloudflow status --all` - Check API connectivity: `curl -I https://api.cloudflow.io/health` - Review recent deployments: `kubectl get deployments -n cloudflow --sort-by=.metadata.creationTimestamp` - Inspect platform metrics: `cloudflow metrics --last 1h` --- ## Authentication & Authorization Issues ### 401 Unauthorized Errors #### Symptoms - API requests return `401",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_46",
              "content": "Istio service mesh - Database connections: SSL/TLS enforced (sslmode=require) **Data Masking**: - Sensitive fields (email, phone) masked in logs - PII redacted from error messages and stack traces - Audit logs: Full data retention with access controls **Compliance**: - GDPR: User data export and deletion workflows - SOC 2 Type II: Audit logging for all data access - HIPAA: PHI isolation in dedicated tenants with enhanced encryption --- ## Performance Characteristics ### Latency Targets **API",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_51",
              "content": "< 2 business days - **Escalation:** Standard support ticket ### Escalation Steps #### Step 1: Initial Assessment (0-5 minutes) ```bash # Run health check cloudflow health check --comprehensive # Check status page curl https://status.cloudflow.io/api/v1/status.json # Review recent changes cloudflow audit log --last 2h --event-type \"deployment,configuration\" # Check metrics dashboard cloudflow metrics dashboard --incident-mode ``` #### Step 2: Gather Information (5-15 minutes) Create incident document with: - Incident timestamp and duration - Affected services and endpoints",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_6",
              "content": "CORS handling for web clients - Request/response logging and correlation ID injection **Critical Endpoints**: - `POST /api/v1/workflows` - Create new workflow - `GET /api/v1/workflows/:id` - Retrieve workflow status - `POST /api/v1/workflows/:id/execute` - Trigger workflow execution - `GET /api/v1/workflows/:id/history` - Get execution history **Dependencies**: - Auth Service (for token validation) - Redis (for rate limiting counters) - All downstream microservices **Performance Targets**: - P50 latency: < 50ms - P99 latency: < 200ms - Throughput: 10,000 RPS",
              "score": null
            }
          ],
          "latency_ms": 3.0339140012074495,
          "query_id": "realistic_007",
          "dimension": "original",
          "query": "What are the health check endpoints?"
        },
        {
          "key_facts": [
            "/health",
            "/ready"
          ],
          "found_facts": [
            "/health",
            "/ready"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "deployment_guide"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_6",
              "content": "CORS handling for web clients - Request/response logging and correlation ID injection **Critical Endpoints**: - `POST /api/v1/workflows` - Create new workflow - `GET /api/v1/workflows/:id` - Retrieve workflow status - `POST /api/v1/workflows/:id/execute` - Trigger workflow execution - `GET /api/v1/workflows/:id/history` - Get execution history **Dependencies**: - Auth Service (for token validation) - Redis (for rate limiting counters) - All downstream microservices **Performance Targets**: - P50 latency: < 50ms - P99 latency: < 200ms - Throughput: 10,000 RPS",
              "score": null
            },
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_13",
              "content": "health endpoints: ```bash # Port forward for testing kubectl port-forward -n cloudflow-prod svc/cloudflow-api 8080:80 # Test health endpoint curl http://localhost:8080/health # Expected: {\"status\":\"healthy\",\"timestamp\":\"2026-01-24T10:30:00Z\"} # Test readiness endpoint curl http://localhost:8080/ready # Expected: {\"status\":\"ready\",\"dependencies\":{\"database\":\"connected\",\"redis\":\"connected\"}} ``` --- ## Database Configuration ### PostgreSQL Setup CloudFlow uses PostgreSQL 14 as its primary data store.",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_5",
              "content": "providing authentication, rate limiting, request routing, and protocol translation. **Technology**: Node.js with Express.js framework **Replicas**: 12 pods (production), auto-scaling 8-20 based on CPU **Resource Allocation**: 2 vCPU, 4GB RAM per pod **Key Responsibilities**: - JWT token validation (delegated to Auth Service for initial validation) - Rate limiting: 1000 requests per minute per API key (sliding window) - Request/response transformation and validation using JSON Schema - Routing to downstream services based on URL path patterns -",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_57",
              "content": "minutes (no manual intervention) - Data loss: None (multi-AZ replication) **Scenario 2: Database Primary Failure** - Detection: Health check fails for primary database (< 30 seconds) - Action: Automatic promotion of read replica to primary - Recovery time: 30-60 seconds - Data loss: Minimal (< 1 second due to synchronous replication) **Scenario 3: Full Region Failure** - Detection: Multiple health check failures across all AZs (< 2 minutes) - Action: Manual failover to DR region",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_46",
              "content": "Istio service mesh - Database connections: SSL/TLS enforced (sslmode=require) **Data Masking**: - Sensitive fields (email, phone) masked in logs - PII redacted from error messages and stack traces - Audit logs: Full data retention with access controls **Compliance**: - GDPR: User data export and deletion workflows - SOC 2 Type II: Audit logging for all data access - HIPAA: PHI isolation in dedicated tenants with enhanced encryption --- ## Performance Characteristics ### Latency Targets **API",
              "score": null
            }
          ],
          "latency_ms": 3.309838999484782,
          "query_id": "realistic_007",
          "dimension": "synonym",
          "query": "Which URLs should I use for health monitoring?"
        },
        {
          "key_facts": [
            "/health",
            "/ready"
          ],
          "found_facts": [
            "/health"
          ],
          "missed_facts": [
            "/ready"
          ],
          "coverage": 0.5,
          "expected_docs": [
            "deployment_guide"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_1",
              "content": "Each section includes error symptoms, root cause analysis, resolution steps, and preventive measures. ### Quick Diagnostic Checklist Before diving into specific issues, perform these initial checks: - Verify service health: `cloudflow status --all` - Check API connectivity: `curl -I https://api.cloudflow.io/health` - Review recent deployments: `kubectl get deployments -n cloudflow --sort-by=.metadata.creationTimestamp` - Inspect platform metrics: `cloudflow metrics --last 1h` --- ## Authentication & Authorization Issues ### 401 Unauthorized Errors #### Symptoms - API requests return `401",
              "score": null
            },
            {
              "doc_id": "user_guide",
              "chunk_id": "user_guide_fix_30",
              "content": "timeout - `network_error`: Connection failures - `status`: Specific HTTP status codes - `error_code`: Application-specific error codes ### Fallback Actions Execute alternative actions when the primary action fails: ```yaml - id: primary_payment action: http_request config: url: \"https://primary-payment-gateway.com/charge\" method: POST body: amount: \"{{amount}}\" on_error: - id: fallback_payment action: http_request config: url: \"https://backup-payment-gateway.com/charge\" method: POST body: amount: \"{{amount}}\" - id: notify_admin action: email config: to: \"admin@company.com\" subject: \"Payment Gateway Failure\" body: \"Primary gateway failed, switched to backup\" ```",
              "score": null
            },
            {
              "doc_id": "user_guide",
              "chunk_id": "user_guide_fix_28",
              "content": "2026-01-28 14:00:00 UTC ``` ## Error Handling Robust error handling ensures your workflows are resilient and reliable. ### Retry Policies Configure automatic retries for failed actions: ```yaml - id: api_call action: http_request config: url: \"https://api.example.com/data\" retry: max_attempts: 3 backoff_type: \"exponential\" # or \"fixed\", \"linear\" initial_interval: 1000 # milliseconds max_interval: 30000 multiplier: 2.0 retry_on: - timeout - network_error - status: [500, 502, 503, 504] ``` **Backoff Strategies:** - **Fixed**: Wait the same amount of time between",
              "score": null
            },
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_19",
              "content": "resource | | 404 | Not Found | Resource does not exist or has been deleted | | 429 | Too Many Requests | Rate limit exceeded, retry after specified period | | 500 | Internal Server Error | Unexpected server error, contact support if persists | | 502 | Bad Gateway | Temporary service issue, retry with exponential backoff | | 503 | Service Unavailable | Scheduled maintenance or temporary outage | ### Error",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_52",
              "content": "connection failure - P2 (< 15 min): P99 latency > 500ms, cache hit rate < 80% - P3 (< 1 hour): Resource utilization > 85%, replica count < minimum **SLI/SLO/SLA**: - SLI: API success rate (non-5xx responses) - SLO: 99.9% uptime per month (43 minutes downtime allowance) - SLA: 99.5% uptime guarantee (customer-facing SLA with credits) --- ## Disaster Recovery ### Recovery Objectives **RPO (Recovery Point Objective): 1 hour** - Maximum acceptable data loss: 1",
              "score": null
            }
          ],
          "latency_ms": 3.0585810000047786,
          "query_id": "realistic_007",
          "dimension": "problem",
          "query": "My load balancer health checks are failing"
        },
        {
          "key_facts": [
            "/health",
            "/ready"
          ],
          "found_facts": [
            "/health",
            "/ready"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "deployment_guide"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_13",
              "content": "health endpoints: ```bash # Port forward for testing kubectl port-forward -n cloudflow-prod svc/cloudflow-api 8080:80 # Test health endpoint curl http://localhost:8080/health # Expected: {\"status\":\"healthy\",\"timestamp\":\"2026-01-24T10:30:00Z\"} # Test readiness endpoint curl http://localhost:8080/ready # Expected: {\"status\":\"ready\",\"dependencies\":{\"database\":\"connected\",\"redis\":\"connected\"}} ``` --- ## Database Configuration ### PostgreSQL Setup CloudFlow uses PostgreSQL 14 as its primary data store.",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_57",
              "content": "minutes (no manual intervention) - Data loss: None (multi-AZ replication) **Scenario 2: Database Primary Failure** - Detection: Health check fails for primary database (< 30 seconds) - Action: Automatic promotion of read replica to primary - Recovery time: 30-60 seconds - Data loss: Minimal (< 1 second due to synchronous replication) **Scenario 3: Full Region Failure** - Detection: Multiple health check failures across all AZs (< 2 minutes) - Action: Manual failover to DR region",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_1",
              "content": "Each section includes error symptoms, root cause analysis, resolution steps, and preventive measures. ### Quick Diagnostic Checklist Before diving into specific issues, perform these initial checks: - Verify service health: `cloudflow status --all` - Check API connectivity: `curl -I https://api.cloudflow.io/health` - Review recent deployments: `kubectl get deployments -n cloudflow --sort-by=.metadata.creationTimestamp` - Inspect platform metrics: `cloudflow metrics --last 1h` --- ## Authentication & Authorization Issues ### 401 Unauthorized Errors #### Symptoms - API requests return `401",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_51",
              "content": "< 2 business days - **Escalation:** Standard support ticket ### Escalation Steps #### Step 1: Initial Assessment (0-5 minutes) ```bash # Run health check cloudflow health check --comprehensive # Check status page curl https://status.cloudflow.io/api/v1/status.json # Review recent changes cloudflow audit log --last 2h --event-type \"deployment,configuration\" # Check metrics dashboard cloudflow metrics dashboard --incident-mode ``` #### Step 2: Gather Information (5-15 minutes) Create incident document with: - Incident timestamp and duration - Affected services and endpoints",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_46",
              "content": "Istio service mesh - Database connections: SSL/TLS enforced (sslmode=require) **Data Masking**: - Sensitive fields (email, phone) masked in logs - PII redacted from error messages and stack traces - Audit logs: Full data retention with access controls **Compliance**: - GDPR: User data export and deletion workflows - SOC 2 Type II: Audit logging for all data access - HIPAA: PHI isolation in dedicated tenants with enhanced encryption --- ## Performance Characteristics ### Latency Targets **API",
              "score": null
            }
          ],
          "latency_ms": 3.0052710008021677,
          "query_id": "realistic_007",
          "dimension": "casual",
          "query": "health endpoints"
        },
        {
          "key_facts": [
            "/health",
            "/ready"
          ],
          "found_facts": [],
          "missed_facts": [
            "/health",
            "/ready"
          ],
          "coverage": 0.0,
          "expected_docs": [
            "deployment_guide"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_46",
              "content": "Istio service mesh - Database connections: SSL/TLS enforced (sslmode=require) **Data Masking**: - Sensitive fields (email, phone) masked in logs - PII redacted from error messages and stack traces - Audit logs: Full data retention with access controls **Compliance**: - GDPR: User data export and deletion workflows - SOC 2 Type II: Audit logging for all data access - HIPAA: PHI isolation in dedicated tenants with enhanced encryption --- ## Performance Characteristics ### Latency Targets **API",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_51",
              "content": "peak - Egress: 1.5 Gbps average, 4 Gbps peak - Internal (service mesh): 8 Gbps average ### Monitoring & Alerting **Key Metrics** (Monitored via Prometheus): - Request rate (per service, per endpoint) - Error rate (4xx, 5xx responses) - Latency percentiles (P50, P95, P99) - Resource utilization (CPU, memory, disk) - Cache hit/miss ratios - Database connection pool saturation - Kafka consumer lag **Alerts** (via PagerDuty): - P1 (Immediate): API error rate > 1%, database",
              "score": null
            },
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_21",
              "content": "JSON template. Key dashboard panels include: 1. **API Performance**: Request rate, P95/P99 latency, error rate 2. **Resource Usage**: CPU, memory, disk I/O per pod 3. **Database Health**: Connection pool utilization, query performance 4. **Worker Status**: Queue depth, processing rate, job success/failure ratio 5. **System Overview**: Pod status, replica count, autoscaling events ### Logging with CloudWatch CloudFlow logs are automatically shipped to CloudWatch Logs.",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_4",
              "content": "\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 ``` ### Technology Stack - **Runtime**: Node.js 20.x (API Gateway, Workflow Engine), Go 1.21 (Auth Service, Scheduler) - **Container Orchestration**: Kubernetes 1.28 (EKS) - **Service Mesh**: Istio 1.20 for service-to-service communication - **Databases**: PostgreSQL 15.4, Redis 7.2 - **Message Broker**: Apache Kafka 3.6 - **Monitoring**: Prometheus, Grafana, Jaeger for distributed tracing - **Secrets Management**: HashiCorp Vault 1.15 --- ## Microservices Breakdown ### API Gateway **Purpose**: Single entry point for all client requests,",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_51",
              "content": "< 2 business days - **Escalation:** Standard support ticket ### Escalation Steps #### Step 1: Initial Assessment (0-5 minutes) ```bash # Run health check cloudflow health check --comprehensive # Check status page curl https://status.cloudflow.io/api/v1/status.json # Review recent changes cloudflow audit log --last 2h --event-type \"deployment,configuration\" # Check metrics dashboard cloudflow metrics dashboard --incident-mode ``` #### Step 2: Gather Information (5-15 minutes) Create incident document with: - Incident timestamp and duration - Affected services and endpoints",
              "score": null
            }
          ],
          "latency_ms": 3.0300269991130335,
          "query_id": "realistic_007",
          "dimension": "contextual",
          "query": "Setting up monitoring, what endpoints indicate service health?"
        },
        {
          "key_facts": [
            "/health",
            "/ready"
          ],
          "found_facts": [
            "/health",
            "/ready"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "deployment_guide"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_53",
              "content": "\\ --description \"All API requests returning 503\" \\ --page-oncall # Notify status page cloudflow status update \\ --status \"major_outage\" \\ --message \"We are investigating a complete service outage\" # Create war room cloudflow incident war-room create incident-2024012401 ``` **For SEV-2 (High):** ```bash # Create incident and notify cloudflow incident create \\ --severity SEV-2 \\ --title \"High rate of workflow failures\" \\ --description \"Workflow success rate dropped to 75%\" \\ --notify-oncall # Update status page cloudflow",
              "score": null
            },
            {
              "doc_id": "user_guide",
              "chunk_id": "user_guide_fix_13",
              "content": "timeout: 30 ``` **Response Handling:** Access the response in subsequent steps: - `{{steps.fetch_user.status}}` - HTTP status code - `{{steps.fetch_user.body}}` - Response body - `{{steps.fetch_user.headers}}` - Response headers ### Database Queries Execute queries against supported databases (PostgreSQL, MySQL, MongoDB, Redis): **SQL Databases (PostgreSQL, MySQL):** ```yaml - id: get_orders action: database_query config: connection: \"{{secrets.DB_CONNECTION_STRING}}\" query: | SELECT * FROM orders WHERE customer_id = $1 AND status = $2 ORDER BY created_at DESC LIMIT 10 parameters: - \"{{trigger.customer_id}}\"",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_57",
              "content": "minutes (no manual intervention) - Data loss: None (multi-AZ replication) **Scenario 2: Database Primary Failure** - Detection: Health check fails for primary database (< 30 seconds) - Action: Automatic promotion of read replica to primary - Recovery time: 30-60 seconds - Data loss: Minimal (< 1 second due to synchronous replication) **Scenario 3: Full Region Failure** - Detection: Multiple health check failures across all AZs (< 2 minutes) - Action: Manual failover to DR region",
              "score": null
            },
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_13",
              "content": "health endpoints: ```bash # Port forward for testing kubectl port-forward -n cloudflow-prod svc/cloudflow-api 8080:80 # Test health endpoint curl http://localhost:8080/health # Expected: {\"status\":\"healthy\",\"timestamp\":\"2026-01-24T10:30:00Z\"} # Test readiness endpoint curl http://localhost:8080/ready # Expected: {\"status\":\"ready\",\"dependencies\":{\"database\":\"connected\",\"redis\":\"connected\"}} ``` --- ## Database Configuration ### PostgreSQL Setup CloudFlow uses PostgreSQL 14 as its primary data store.",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_1",
              "content": "Each section includes error symptoms, root cause analysis, resolution steps, and preventive measures. ### Quick Diagnostic Checklist Before diving into specific issues, perform these initial checks: - Verify service health: `cloudflow status --all` - Check API connectivity: `curl -I https://api.cloudflow.io/health` - Review recent deployments: `kubectl get deployments -n cloudflow --sort-by=.metadata.creationTimestamp` - Inspect platform metrics: `cloudflow metrics --last 1h` --- ## Authentication & Authorization Issues ### 401 Unauthorized Errors #### Symptoms - API requests return `401",
              "score": null
            }
          ],
          "latency_ms": 3.0294759999378584,
          "query_id": "realistic_007",
          "dimension": "negation",
          "query": "Why doesn't /status return health information?"
        },
        {
          "key_facts": [
            "minReplicas: 3",
            "maxReplicas: 10",
            "targetCPUUtilizationPercentage: 70"
          ],
          "found_facts": [
            "minReplicas: 3",
            "maxReplicas: 10"
          ],
          "missed_facts": [
            "targetCPUUtilizationPercentage: 70"
          ],
          "coverage": 0.6666666666666666,
          "expected_docs": [
            "deployment_guide"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_28",
              "content": "Resource resource: name: cpu target: type: Utilization averageUtilization: 70 - type: Resource resource: name: memory target: type: Utilization averageUtilization: 80 behavior: scaleDown: stabilizationWindowSeconds: 300 policies: - type: Percent value: 50 periodSeconds: 60 scaleUp: stabilizationWindowSeconds: 60 policies: - type: Percent value: 100 periodSeconds: 30 - type: Pods value: 2 periodSeconds: 30 selectPolicy: Max ``` Monitor autoscaling events: ```bash kubectl get hpa -n cloudflow-prod -w kubectl describe hpa cloudflow-api-hpa -n cloudflow-prod ``` ### Performance Optimization #### Connection",
              "score": null
            },
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_27",
              "content": "**Verify Service Health**: ```bash kubectl get pods -n cloudflow-prod curl https://api.cloudflow.io/health ``` **Recovery Time Objective (RTO)**: 4 hours **Recovery Point Objective (RPO)**: 24 hours --- ## Scaling and Performance ### Horizontal Pod Autoscaling CloudFlow is configured with Horizontal Pod Autoscaler (HPA) to automatically scale based on resource utilization: ```yaml # hpa.yaml apiVersion: autoscaling/v2 kind: HorizontalPodAutoscaler metadata: name: cloudflow-api-hpa namespace: cloudflow-prod spec: scaleTargetRef: apiVersion: apps/v1 kind: Deployment name: cloudflow-api minReplicas: 3 maxReplicas: 10 metrics: - type:",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_51",
              "content": "peak - Egress: 1.5 Gbps average, 4 Gbps peak - Internal (service mesh): 8 Gbps average ### Monitoring & Alerting **Key Metrics** (Monitored via Prometheus): - Request rate (per service, per endpoint) - Error rate (4xx, 5xx responses) - Latency percentiles (P50, P95, P99) - Resource utilization (CPU, memory, disk) - Cache hit/miss ratios - Database connection pool saturation - Kafka consumer lag **Alerts** (via PagerDuty): - P1 (Immediate): API error rate > 1%, database",
              "score": null
            },
            {
              "doc_id": "user_guide",
              "chunk_id": "user_guide_fix_36",
              "content": "- Success rate: 98.2% - Current queue depth: 3 ### Data Limits - **Maximum request/response size**: 10MB per action - **Maximum execution payload**: 50MB total - **Variable value size**: 1MB per variable ### Enterprise Plan Limits Enterprise customers can request increased limits: - Up to 100 steps per workflow - Up to 10,000 executions per day - Up to 7200 second timeout (2 hours) - Priority execution queue - Dedicated capacity allocation Contact sales@cloudflow.io for",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_2",
              "content": "The platform is designed with the following core principles: - **Scalability**: Horizontal scaling for all services with auto-scaling groups - **Resilience**: Circuit breakers, bulkheads, and graceful degradation - **Observability**: Distributed tracing, centralized logging, and comprehensive metrics - **Security**: Zero-trust network architecture with mTLS encryption ### Architecture Diagram (Conceptual) ``` \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 Load Balancer (ALB) \u2502 \u2502 (TLS Termination - 443) \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u25bc \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 API Gateway Layer \u2502 \u2502 (Rate Limiting, Auth, Request",
              "score": null
            }
          ],
          "latency_ms": 3.7532750011450844,
          "query_id": "realistic_008",
          "dimension": "original",
          "query": "What are the HPA scaling parameters?"
        },
        {
          "key_facts": [
            "minReplicas: 3",
            "maxReplicas: 10",
            "targetCPUUtilizationPercentage: 70"
          ],
          "found_facts": [
            "minReplicas: 3",
            "maxReplicas: 10"
          ],
          "missed_facts": [
            "targetCPUUtilizationPercentage: 70"
          ],
          "coverage": 0.6666666666666666,
          "expected_docs": [
            "deployment_guide"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_27",
              "content": "**Verify Service Health**: ```bash kubectl get pods -n cloudflow-prod curl https://api.cloudflow.io/health ``` **Recovery Time Objective (RTO)**: 4 hours **Recovery Point Objective (RPO)**: 24 hours --- ## Scaling and Performance ### Horizontal Pod Autoscaling CloudFlow is configured with Horizontal Pod Autoscaler (HPA) to automatically scale based on resource utilization: ```yaml # hpa.yaml apiVersion: autoscaling/v2 kind: HorizontalPodAutoscaler metadata: name: cloudflow-api-hpa namespace: cloudflow-prod spec: scaleTargetRef: apiVersion: apps/v1 kind: Deployment name: cloudflow-api minReplicas: 3 maxReplicas: 10 metrics: - type:",
              "score": null
            },
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_28",
              "content": "Resource resource: name: cpu target: type: Utilization averageUtilization: 70 - type: Resource resource: name: memory target: type: Utilization averageUtilization: 80 behavior: scaleDown: stabilizationWindowSeconds: 300 policies: - type: Percent value: 50 periodSeconds: 60 scaleUp: stabilizationWindowSeconds: 60 policies: - type: Percent value: 100 periodSeconds: 30 - type: Pods value: 2 periodSeconds: 30 selectPolicy: Max ``` Monitor autoscaling events: ```bash kubectl get hpa -n cloudflow-prod -w kubectl describe hpa cloudflow-api-hpa -n cloudflow-prod ``` ### Performance Optimization #### Connection",
              "score": null
            },
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_21",
              "content": "JSON template. Key dashboard panels include: 1. **API Performance**: Request rate, P95/P99 latency, error rate 2. **Resource Usage**: CPU, memory, disk I/O per pod 3. **Database Health**: Connection pool utilization, query performance 4. **Worker Status**: Queue depth, processing rate, job success/failure ratio 5. **System Overview**: Pod status, replica count, autoscaling events ### Logging with CloudWatch CloudFlow logs are automatically shipped to CloudWatch Logs.",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_13",
              "content": "< 2 seconds (P99) - Complex workflow (20+ steps): < 5 seconds (P99) - Throughput: 500 concurrent workflow executions per pod --- ### Scheduler Service **Purpose**: Time-based workflow triggering system supporting cron-like schedules and one-time delayed executions. **Technology**: Go with distributed locking via Redis **Replicas**: 4 pods (production), active-passive with leader election **Resource Allocation**: 2 vCPU, 4GB RAM per pod **Key Responsibilities**: - Parse and validate cron expressions (extended format supporting seconds) - Maintain schedule",
              "score": null
            },
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_30",
              "content": "to validate scaling configuration: ```bash # Install k6 for load testing kubectl apply -f - <<EOF apiVersion: v1 kind: ConfigMap metadata: name: loadtest-script namespace: cloudflow-prod data: script.js: | import http from 'k6/http'; import { check, sleep } from 'k6'; export let options = { stages: [ { duration: '2m', target: 100 }, { duration: '5m', target: 100 }, { duration: '2m', target: 200 }, { duration: '5m', target: 200 }, { duration: '2m', target: 0",
              "score": null
            }
          ],
          "latency_ms": 3.2806039998831693,
          "query_id": "realistic_008",
          "dimension": "synonym",
          "query": "How does horizontal pod autoscaling work?"
        },
        {
          "key_facts": [
            "minReplicas: 3",
            "maxReplicas: 10",
            "targetCPUUtilizationPercentage: 70"
          ],
          "found_facts": [
            "minReplicas: 3",
            "maxReplicas: 10"
          ],
          "missed_facts": [
            "targetCPUUtilizationPercentage: 70"
          ],
          "coverage": 0.6666666666666666,
          "expected_docs": [
            "deployment_guide"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_27",
              "content": "**Verify Service Health**: ```bash kubectl get pods -n cloudflow-prod curl https://api.cloudflow.io/health ``` **Recovery Time Objective (RTO)**: 4 hours **Recovery Point Objective (RPO)**: 24 hours --- ## Scaling and Performance ### Horizontal Pod Autoscaling CloudFlow is configured with Horizontal Pod Autoscaler (HPA) to automatically scale based on resource utilization: ```yaml # hpa.yaml apiVersion: autoscaling/v2 kind: HorizontalPodAutoscaler metadata: name: cloudflow-api-hpa namespace: cloudflow-prod spec: scaleTargetRef: apiVersion: apps/v1 kind: Deployment name: cloudflow-api minReplicas: 3 maxReplicas: 10 metrics: - type:",
              "score": null
            },
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_28",
              "content": "Resource resource: name: cpu target: type: Utilization averageUtilization: 70 - type: Resource resource: name: memory target: type: Utilization averageUtilization: 80 behavior: scaleDown: stabilizationWindowSeconds: 300 policies: - type: Percent value: 50 periodSeconds: 60 scaleUp: stabilizationWindowSeconds: 60 policies: - type: Percent value: 100 periodSeconds: 30 - type: Pods value: 2 periodSeconds: 30 selectPolicy: Max ``` Monitor autoscaling events: ```bash kubectl get hpa -n cloudflow-prod -w kubectl describe hpa cloudflow-api-hpa -n cloudflow-prod ``` ### Performance Optimization #### Connection",
              "score": null
            },
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_34",
              "content": "Issues #### Pods in CrashLoopBackOff **Symptoms**: Pods continuously restart **Diagnosis**: ```bash kubectl logs -n cloudflow-prod <pod-name> --previous kubectl describe pod -n cloudflow-prod <pod-name> ``` **Common Causes**: - Database connection failure - Invalid environment variables - Insufficient resources #### High Memory Usage **Symptoms**: Pods being OOMKilled **Diagnosis**: ```bash kubectl top pods -n cloudflow-prod ``` **Resolution**: - Increase memory limits in deployment - Check for memory leaks in application logs - Review heap dump for Node.js processes",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_50",
              "content": "End-to-end latency: < 100ms (P99) ### Resource Utilization **CPU Utilization** (Target: 60-70% average): - API Gateway: 55% average, 80% peak - Workflow Engine: 65% average, 85% peak - Auth Service: 40% average, 60% peak **Memory Utilization** (Target: < 80%): - API Gateway: 2.5GB average per pod (4GB allocated) - Workflow Engine: 6GB average per pod (8GB allocated) - Notification Service: 2.8GB average per pod (4GB allocated) **Network Throughput**: - Ingress: 2 Gbps average, 5 Gbps",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_14",
              "content": "### Memory Leaks #### Detection ```bash # Monitor CloudFlow service memory usage kubectl top pods -n cloudflow --sort-by=memory # Get detailed memory metrics for specific pod kubectl exec -n cloudflow deploy/cloudflow-api -- \\ curl localhost:9090/metrics | grep memory # Check for OOMKilled pods kubectl get pods -n cloudflow --field-selector=status.phase=Failed | \\ grep OOMKilled # Review memory limits and requests kubectl describe deployment cloudflow-api -n cloudflow | \\ grep -A 5 \"Limits\\|Requests\" ``` #### Common Causes",
              "score": null
            }
          ],
          "latency_ms": 3.149359001326957,
          "query_id": "realistic_008",
          "dimension": "problem",
          "query": "Pods aren't scaling up during traffic spikes"
        },
        {
          "key_facts": [
            "minReplicas: 3",
            "maxReplicas: 10",
            "targetCPUUtilizationPercentage: 70"
          ],
          "found_facts": [
            "minReplicas: 3",
            "maxReplicas: 10"
          ],
          "missed_facts": [
            "targetCPUUtilizationPercentage: 70"
          ],
          "coverage": 0.6666666666666666,
          "expected_docs": [
            "deployment_guide"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_30",
              "content": "to validate scaling configuration: ```bash # Install k6 for load testing kubectl apply -f - <<EOF apiVersion: v1 kind: ConfigMap metadata: name: loadtest-script namespace: cloudflow-prod data: script.js: | import http from 'k6/http'; import { check, sleep } from 'k6'; export let options = { stages: [ { duration: '2m', target: 100 }, { duration: '5m', target: 100 }, { duration: '2m', target: 200 }, { duration: '5m', target: 200 }, { duration: '2m', target: 0",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_2",
              "content": "The platform is designed with the following core principles: - **Scalability**: Horizontal scaling for all services with auto-scaling groups - **Resilience**: Circuit breakers, bulkheads, and graceful degradation - **Observability**: Distributed tracing, centralized logging, and comprehensive metrics - **Security**: Zero-trust network architecture with mTLS encryption ### Architecture Diagram (Conceptual) ``` \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 Load Balancer (ALB) \u2502 \u2502 (TLS Termination - 443) \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u25bc \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 API Gateway Layer \u2502 \u2502 (Rate Limiting, Auth, Request",
              "score": null
            },
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_27",
              "content": "**Verify Service Health**: ```bash kubectl get pods -n cloudflow-prod curl https://api.cloudflow.io/health ``` **Recovery Time Objective (RTO)**: 4 hours **Recovery Point Objective (RPO)**: 24 hours --- ## Scaling and Performance ### Horizontal Pod Autoscaling CloudFlow is configured with Horizontal Pod Autoscaler (HPA) to automatically scale based on resource utilization: ```yaml # hpa.yaml apiVersion: autoscaling/v2 kind: HorizontalPodAutoscaler metadata: name: cloudflow-api-hpa namespace: cloudflow-prod spec: scaleTargetRef: apiVersion: apps/v1 kind: Deployment name: cloudflow-api minReplicas: 3 maxReplicas: 10 metrics: - type:",
              "score": null
            },
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_28",
              "content": "Resource resource: name: cpu target: type: Utilization averageUtilization: 70 - type: Resource resource: name: memory target: type: Utilization averageUtilization: 80 behavior: scaleDown: stabilizationWindowSeconds: 300 policies: - type: Percent value: 50 periodSeconds: 60 scaleUp: stabilizationWindowSeconds: 60 policies: - type: Percent value: 100 periodSeconds: 30 - type: Pods value: 2 periodSeconds: 30 selectPolicy: Max ``` Monitor autoscaling events: ```bash kubectl get hpa -n cloudflow-prod -w kubectl describe hpa cloudflow-api-hpa -n cloudflow-prod ``` ### Performance Optimization #### Connection",
              "score": null
            },
            {
              "doc_id": "user_guide",
              "chunk_id": "user_guide_fix_42",
              "content": "Keep Workflows Modular Break complex workflows into smaller, reusable components: - Use sub-workflows for repeated logic - Trigger child workflows via webhooks - Share common configurations via templates ### 8. Test Thoroughly Before activating a workflow: 1. Use test mode with sample data 2. Verify all actions execute correctly 3. Test error handling paths 4. Review execution logs 5. Start with a limited scope (e.g., test channel, small dataset) ### 9.",
              "score": null
            }
          ],
          "latency_ms": 3.1563819993607467,
          "query_id": "realistic_008",
          "dimension": "casual",
          "query": "autoscaling config"
        },
        {
          "key_facts": [
            "minReplicas: 3",
            "maxReplicas: 10",
            "targetCPUUtilizationPercentage: 70"
          ],
          "found_facts": [
            "minReplicas: 3",
            "maxReplicas: 10"
          ],
          "missed_facts": [
            "targetCPUUtilizationPercentage: 70"
          ],
          "coverage": 0.6666666666666666,
          "expected_docs": [
            "deployment_guide"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_27",
              "content": "**Verify Service Health**: ```bash kubectl get pods -n cloudflow-prod curl https://api.cloudflow.io/health ``` **Recovery Time Objective (RTO)**: 4 hours **Recovery Point Objective (RPO)**: 24 hours --- ## Scaling and Performance ### Horizontal Pod Autoscaling CloudFlow is configured with Horizontal Pod Autoscaler (HPA) to automatically scale based on resource utilization: ```yaml # hpa.yaml apiVersion: autoscaling/v2 kind: HorizontalPodAutoscaler metadata: name: cloudflow-api-hpa namespace: cloudflow-prod spec: scaleTargetRef: apiVersion: apps/v1 kind: Deployment name: cloudflow-api minReplicas: 3 maxReplicas: 10 metrics: - type:",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_2",
              "content": "The platform is designed with the following core principles: - **Scalability**: Horizontal scaling for all services with auto-scaling groups - **Resilience**: Circuit breakers, bulkheads, and graceful degradation - **Observability**: Distributed tracing, centralized logging, and comprehensive metrics - **Security**: Zero-trust network architecture with mTLS encryption ### Architecture Diagram (Conceptual) ``` \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 Load Balancer (ALB) \u2502 \u2502 (TLS Termination - 443) \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u25bc \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 API Gateway Layer \u2502 \u2502 (Rate Limiting, Auth, Request",
              "score": null
            },
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_28",
              "content": "Resource resource: name: cpu target: type: Utilization averageUtilization: 70 - type: Resource resource: name: memory target: type: Utilization averageUtilization: 80 behavior: scaleDown: stabilizationWindowSeconds: 300 policies: - type: Percent value: 50 periodSeconds: 60 scaleUp: stabilizationWindowSeconds: 60 policies: - type: Percent value: 100 periodSeconds: 30 - type: Pods value: 2 periodSeconds: 30 selectPolicy: Max ``` Monitor autoscaling events: ```bash kubectl get hpa -n cloudflow-prod -w kubectl describe hpa cloudflow-api-hpa -n cloudflow-prod ``` ### Performance Optimization #### Connection",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_44",
              "content": "to specific workflows or operations - Rate limits: Configurable per API key (default: 1000 RPM) ### Secrets Management **HashiCorp Vault Integration**: - Dynamic database credentials: Generated on-demand, 1-hour TTL - Encryption keys: Transit secrets engine for encryption-as-a-service - API keys for external services: Stored with versioning - Rotation policy: Automated rotation every 90 days with notification **Secret Access Pattern**: ``` Service starts \u2192 Vault authentication (Kubernetes service account) \u2502 \u25bc Request secret lease (1-hour TTL)",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_29",
              "content": "**Cluster Configuration**: - 5 broker nodes (distributed across 3 AZs) - Instance type: kafka.m5.2xlarge (8 vCPU, 32GB RAM) - Storage: 10TB per broker (gp3 SSD) - ZooKeeper ensemble: 3 nodes for cluster coordination - Replication factor: 3 (min in-sync replicas: 2) **Topic Architecture**: ``` workflow.events (32 partitions): - Workflow lifecycle events (created, started, completed, failed) - Retention: 7 days - Message rate: 5,000/sec peak - Consumer groups: workflow-engine, analytics-pipeline notifications.email (16 partitions): - Email notification",
              "score": null
            }
          ],
          "latency_ms": 3.308826999273151,
          "query_id": "realistic_008",
          "dimension": "contextual",
          "query": "I need to configure autoscaling, what are the min/max replicas and thresholds?"
        },
        {
          "key_facts": [
            "minReplicas: 3",
            "maxReplicas: 10",
            "targetCPUUtilizationPercentage: 70"
          ],
          "found_facts": [],
          "missed_facts": [
            "minReplicas: 3",
            "maxReplicas: 10",
            "targetCPUUtilizationPercentage: 70"
          ],
          "coverage": 0.0,
          "expected_docs": [
            "deployment_guide"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_54",
              "content": "(4) - API Gateway (4) - Workflow Engine (6) - Workflow Engine (5) - Workflow Engine (5) - Auth Service (3) - Auth Service (3) - Auth Service (2) - PostgreSQL Primary - PostgreSQL Replica - PostgreSQL Replica - Redis Primary (2) - Redis Replica (2) - Redis Replica (2) - Kafka Broker (2) - Kafka Broker (2) - Kafka Broker (1) ``` **Automatic Failover**: - Database: 30-60 seconds (automatic promotion of replica) - Redis:",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_22",
              "content": "API Gateway - User Service profile lookups - Low latency requirement (< 50ms) - Request-response pattern with circuit breaker **Asynchronous (Kafka)**: - Workflow execution events - Notification triggers - Audit log events - Eventual consistency acceptable --- ## Database Architecture ### PostgreSQL Primary Database **Cluster Configuration**: - Primary-replica setup with 1 primary + 2 read replicas - Instance type: db.r6g.2xlarge (8 vCPU, 64GB RAM) - Storage: 2TB gp3 SSD with 12,000 IOPS - Multi-AZ deployment",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_29",
              "content": "**Cluster Configuration**: - 5 broker nodes (distributed across 3 AZs) - Instance type: kafka.m5.2xlarge (8 vCPU, 32GB RAM) - Storage: 10TB per broker (gp3 SSD) - ZooKeeper ensemble: 3 nodes for cluster coordination - Replication factor: 3 (min in-sync replicas: 2) **Topic Architecture**: ``` workflow.events (32 partitions): - Workflow lifecycle events (created, started, completed, failed) - Retention: 7 days - Message rate: 5,000/sec peak - Consumer groups: workflow-engine, analytics-pipeline notifications.email (16 partitions): - Email notification",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_25",
              "content": "connections per service - Max client connections: 2000 - Connection timeout: 30 seconds **Performance Characteristics**: - Read query P99: < 20ms - Write query P99: < 50ms - Transaction throughput: 15,000 TPS - Replication lag: < 500ms --- ### Redis Caching Layer **Cluster Configuration**: - Redis Cluster with 6 nodes (3 primary + 3 replica) - Instance type: cache.r6g.xlarge (4 vCPU, 26GB RAM) - Total memory: 78GB usable cache space - Persistence: RDB snapshots every",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_49",
              "content": "limiting: 1,000 requests per minute per API key **Workflow Engine**: - Concurrent executions: 8,000 workflows (across 16 pods) - Execution start rate: 500 per second - Completion rate: 450 per second (average 2-second execution time) **Database**: - Read throughput: 50,000 queries per second (across replicas) - Write throughput: 15,000 transactions per second - Connection capacity: 2,000 concurrent connections **Kafka**: - Message ingestion: 100,000 messages per second - Consumer throughput: 80,000 messages per second (aggregated) -",
              "score": null
            }
          ],
          "latency_ms": 3.401048999876366,
          "query_id": "realistic_008",
          "dimension": "negation",
          "query": "Why do we have 3 replicas even with low traffic?"
        },
        {
          "key_facts": [
            "P99 latency: < 200ms",
            "average P99 latency of 180ms for API operations"
          ],
          "found_facts": [],
          "missed_facts": [
            "P99 latency: < 200ms",
            "average P99 latency of 180ms for API operations"
          ],
          "coverage": 0.0,
          "expected_docs": [
            "architecture_overview"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_50",
              "content": "End-to-end latency: < 100ms (P99) ### Resource Utilization **CPU Utilization** (Target: 60-70% average): - API Gateway: 55% average, 80% peak - Workflow Engine: 65% average, 85% peak - Auth Service: 40% average, 60% peak **Memory Utilization** (Target: < 80%): - API Gateway: 2.5GB average per pod (4GB allocated) - Workflow Engine: 6GB average per pod (8GB allocated) - Notification Service: 2.8GB average per pod (4GB allocated) **Network Throughput**: - Ingress: 2 Gbps average, 5 Gbps",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_19",
              "content": "SMS delivery: < 3 seconds (P99) - Throughput: 10,000 notifications per minute --- ## Data Flow Architecture ### Synchronous Request Flow Client requests follow a synchronous path through the API Gateway to backend services: ``` 1. Client \u2192 Load Balancer (TLS termination) 2. Load Balancer \u2192 API Gateway (HTTP/2) 3. API Gateway \u2192 Auth Service (JWT validation via gRPC) 4. API Gateway \u2192 Target Service (HTTP/REST or gRPC) 5.",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_48",
              "content": "execution: Linear scaling up to 10 concurrent branches **Database Operations** (P99 latency): - Simple SELECT: < 5ms - JOIN query: < 20ms - INSERT/UPDATE: < 10ms - Batch operations: < 100ms (batch size: 100 records) **Cache Operations** (P99 latency): - Redis GET: < 2ms - Redis SET: < 3ms - Redis DEL: < 2ms ### Throughput Capacity **API Gateway**: - Sustained: 10,000 requests per second - Peak: 25,000 requests per second (5-minute burst) - Rate",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_13",
              "content": "< 2 seconds (P99) - Complex workflow (20+ steps): < 5 seconds (P99) - Throughput: 500 concurrent workflow executions per pod --- ### Scheduler Service **Purpose**: Time-based workflow triggering system supporting cron-like schedules and one-time delayed executions. **Technology**: Go with distributed locking via Redis **Replicas**: 4 pods (production), active-passive with leader election **Resource Allocation**: 2 vCPU, 4GB RAM per pod **Key Responsibilities**: - Parse and validate cron expressions (extended format supporting seconds) - Maintain schedule",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_47",
              "content": "Operations** (P99 latency): - `GET /workflows/{id}`: < 50ms (cache hit), < 150ms (cache miss) - `POST /workflows`: < 200ms (includes validation and database write) - `POST /workflows/{id}/execute`: < 100ms (async, returns execution ID) - `GET /workflows/{id}/history`: < 300ms (paginated, 50 records per page) **Workflow Execution** (P99 latency): - Simple workflow (< 5 steps): < 2 seconds - Medium workflow (5-15 steps): < 5 seconds - Complex workflow (> 15 steps): < 15 seconds - Parallel",
              "score": null
            }
          ],
          "latency_ms": 3.1308449997595744,
          "query_id": "realistic_009",
          "dimension": "original",
          "query": "What is the P99 latency target for API operations?"
        },
        {
          "key_facts": [
            "P99 latency: < 200ms",
            "average P99 latency of 180ms for API operations"
          ],
          "found_facts": [],
          "missed_facts": [
            "P99 latency: < 200ms",
            "average P99 latency of 180ms for API operations"
          ],
          "coverage": 0.0,
          "expected_docs": [
            "architecture_overview"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "user_guide",
              "chunk_id": "user_guide_fix_36",
              "content": "- Success rate: 98.2% - Current queue depth: 3 ### Data Limits - **Maximum request/response size**: 10MB per action - **Maximum execution payload**: 50MB total - **Variable value size**: 1MB per variable ### Enterprise Plan Limits Enterprise customers can request increased limits: - Up to 100 steps per workflow - Up to 10,000 executions per day - Up to 7200 second timeout (2 hours) - Priority execution queue - Dedicated capacity allocation Contact sales@cloudflow.io for",
              "score": null
            },
            {
              "doc_id": "user_guide",
              "chunk_id": "user_guide_fix_29",
              "content": "retries - Attempt 1: Wait 1s - Attempt 2: Wait 1s - Attempt 3: Wait 1s - **Linear**: Increase wait time by a fixed amount - Attempt 1: Wait 1s - Attempt 2: Wait 2s - Attempt 3: Wait 3s - **Exponential**: Double the wait time with each retry (recommended) - Attempt 1: Wait 1s - Attempt 2: Wait 2s - Attempt 3: Wait 4s **Retry Conditions:** Control which errors trigger retries: - `timeout`: Request",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_34",
              "content": "rate limit usage cloudflow metrics query \\ --metric rate_limit_remaining \\ --workspace ws_abc123 \\ --last 1h \\ --interval 1m ``` #### Handling Rate Limits in Code **Python example with retry logic:** ```python import time import requests def cloudflow_api_call_with_retry(url, headers, max_retries=3): for attempt in range(max_retries): response = requests.get(url, headers=headers) if response.status_code == 429: retry_after = int(response.headers.get('Retry-After', 60)) print(f\"Rate limited.",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_30",
              "content": "triggers - Retention: 3 days - Message rate: 2,000/sec peak - Consumer groups: notification-service notifications.sms (8 partitions): - SMS notification triggers - Retention: 3 days - Message rate: 500/sec peak - Consumer groups: notification-service audit.logs (24 partitions): - Security and compliance audit events - Retention: 90 days (compliance requirement) - Message rate: 3,000/sec peak - Consumer groups: audit-processor, security-monitor dead-letter-queue (8 partitions): - Failed message processing - Retention: 30 days - Manual intervention required -",
              "score": null
            },
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_16",
              "content": "\"limit\": 50, \"offset\": 0, \"has_more\": true } } ``` ### Analytics #### Get Workflow Metrics Retrieve performance metrics and analytics for workflows. **Endpoint:** `GET /analytics/workflows/{workflow_id}` **Query Parameters:** - `period` (required): Time period (`1h`, `24h`, `7d`, `30d`) - `metrics` (optional): Comma-separated list of metrics **Available Metrics:** - `execution_count`: Total number of executions - `success_rate`: Percentage of successful executions - `avg_duration`: Average execution duration in milliseconds - `error_count`: Total number of errors - `throughput`: Records processed per",
              "score": null
            }
          ],
          "latency_ms": 3.0114820001472253,
          "query_id": "realistic_009",
          "dimension": "synonym",
          "query": "What's the 99th percentile response time target?"
        },
        {
          "key_facts": [
            "P99 latency: < 200ms",
            "average P99 latency of 180ms for API operations"
          ],
          "found_facts": [],
          "missed_facts": [
            "P99 latency: < 200ms",
            "average P99 latency of 180ms for API operations"
          ],
          "coverage": 0.0,
          "expected_docs": [
            "architecture_overview"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_50",
              "content": "End-to-end latency: < 100ms (P99) ### Resource Utilization **CPU Utilization** (Target: 60-70% average): - API Gateway: 55% average, 80% peak - Workflow Engine: 65% average, 85% peak - Auth Service: 40% average, 60% peak **Memory Utilization** (Target: < 80%): - API Gateway: 2.5GB average per pod (4GB allocated) - Workflow Engine: 6GB average per pod (8GB allocated) - Notification Service: 2.8GB average per pod (4GB allocated) **Network Throughput**: - Ingress: 2 Gbps average, 5 Gbps",
              "score": null
            },
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_31",
              "content": "}, ], thresholds: { http_req_duration: ['p(95)<500'], }, }; export default function () { let res = http.get('https://api.cloudflow.io/api/workflows'); check(res, { 'status is 200': (r) => r.status === 200 }); sleep(1); } EOF ``` **Performance Targets**: - P95 API latency: < 500ms - P99 API latency: < 1000ms - Error rate: < 0.1% - Throughput: > 1000 requests/second --- ## Security and Compliance ### Network Policies Implement network policies to restrict pod-to-pod communication: ```yaml # network-policy.yaml apiVersion:",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_22",
              "content": "API Gateway - User Service profile lookups - Low latency requirement (< 50ms) - Request-response pattern with circuit breaker **Asynchronous (Kafka)**: - Workflow execution events - Notification triggers - Audit log events - Eventual consistency acceptable --- ## Database Architecture ### PostgreSQL Primary Database **Cluster Configuration**: - Primary-replica setup with 1 primary + 2 read replicas - Instance type: db.r6g.2xlarge (8 vCPU, 64GB RAM) - Storage: 2TB gp3 SSD with 12,000 IOPS - Multi-AZ deployment",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_49",
              "content": "limiting: 1,000 requests per minute per API key **Workflow Engine**: - Concurrent executions: 8,000 workflows (across 16 pods) - Execution start rate: 500 per second - Completion rate: 450 per second (average 2-second execution time) **Database**: - Read throughput: 50,000 queries per second (across replicas) - Write throughput: 15,000 transactions per second - Connection capacity: 2,000 concurrent connections **Kafka**: - Message ingestion: 100,000 messages per second - Consumer throughput: 80,000 messages per second (aggregated) -",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_19",
              "content": "SMS delivery: < 3 seconds (P99) - Throughput: 10,000 notifications per minute --- ## Data Flow Architecture ### Synchronous Request Flow Client requests follow a synchronous path through the API Gateway to backend services: ``` 1. Client \u2192 Load Balancer (TLS termination) 2. Load Balancer \u2192 API Gateway (HTTP/2) 3. API Gateway \u2192 Auth Service (JWT validation via gRPC) 4. API Gateway \u2192 Target Service (HTTP/REST or gRPC) 5.",
              "score": null
            }
          ],
          "latency_ms": 3.389396999409655,
          "query_id": "realistic_009",
          "dimension": "problem",
          "query": "Our API latency is 500ms, is that acceptable?"
        },
        {
          "key_facts": [
            "P99 latency: < 200ms",
            "average P99 latency of 180ms for API operations"
          ],
          "found_facts": [
            "P99 latency: < 200ms"
          ],
          "missed_facts": [
            "average P99 latency of 180ms for API operations"
          ],
          "coverage": 0.5,
          "expected_docs": [
            "architecture_overview"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_31",
              "content": "}, ], thresholds: { http_req_duration: ['p(95)<500'], }, }; export default function () { let res = http.get('https://api.cloudflow.io/api/workflows'); check(res, { 'status is 200': (r) => r.status === 200 }); sleep(1); } EOF ``` **Performance Targets**: - P95 API latency: < 500ms - P99 API latency: < 1000ms - Error rate: < 0.1% - Throughput: > 1000 requests/second --- ## Security and Compliance ### Network Policies Implement network policies to restrict pod-to-pod communication: ```yaml # network-policy.yaml apiVersion:",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_50",
              "content": "End-to-end latency: < 100ms (P99) ### Resource Utilization **CPU Utilization** (Target: 60-70% average): - API Gateway: 55% average, 80% peak - Workflow Engine: 65% average, 85% peak - Auth Service: 40% average, 60% peak **Memory Utilization** (Target: < 80%): - API Gateway: 2.5GB average per pod (4GB allocated) - Workflow Engine: 6GB average per pod (8GB allocated) - Notification Service: 2.8GB average per pod (4GB allocated) **Network Throughput**: - Ingress: 2 Gbps average, 5 Gbps",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_6",
              "content": "CORS handling for web clients - Request/response logging and correlation ID injection **Critical Endpoints**: - `POST /api/v1/workflows` - Create new workflow - `GET /api/v1/workflows/:id` - Retrieve workflow status - `POST /api/v1/workflows/:id/execute` - Trigger workflow execution - `GET /api/v1/workflows/:id/history` - Get execution history **Dependencies**: - Auth Service (for token validation) - Redis (for rate limiting counters) - All downstream microservices **Performance Targets**: - P50 latency: < 50ms - P99 latency: < 200ms - Throughput: 10,000 RPS",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_22",
              "content": "API Gateway - User Service profile lookups - Low latency requirement (< 50ms) - Request-response pattern with circuit breaker **Asynchronous (Kafka)**: - Workflow execution events - Notification triggers - Audit log events - Eventual consistency acceptable --- ## Database Architecture ### PostgreSQL Primary Database **Cluster Configuration**: - Primary-replica setup with 1 primary + 2 read replicas - Instance type: db.r6g.2xlarge (8 vCPU, 64GB RAM) - Storage: 2TB gp3 SSD with 12,000 IOPS - Multi-AZ deployment",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_19",
              "content": "SMS delivery: < 3 seconds (P99) - Throughput: 10,000 notifications per minute --- ## Data Flow Architecture ### Synchronous Request Flow Client requests follow a synchronous path through the API Gateway to backend services: ``` 1. Client \u2192 Load Balancer (TLS termination) 2. Load Balancer \u2192 API Gateway (HTTP/2) 3. API Gateway \u2192 Auth Service (JWT validation via gRPC) 4. API Gateway \u2192 Target Service (HTTP/REST or gRPC) 5.",
              "score": null
            }
          ],
          "latency_ms": 3.1418149992532562,
          "query_id": "realistic_009",
          "dimension": "casual",
          "query": "api latency target"
        },
        {
          "key_facts": [
            "P99 latency: < 200ms",
            "average P99 latency of 180ms for API operations"
          ],
          "found_facts": [
            "average P99 latency of 180ms for API operations"
          ],
          "missed_facts": [
            "P99 latency: < 200ms"
          ],
          "coverage": 0.5,
          "expected_docs": [
            "architecture_overview"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_31",
              "content": "}, ], thresholds: { http_req_duration: ['p(95)<500'], }, }; export default function () { let res = http.get('https://api.cloudflow.io/api/workflows'); check(res, { 'status is 200': (r) => r.status === 200 }); sleep(1); } EOF ``` **Performance Targets**: - P95 API latency: < 500ms - P99 API latency: < 1000ms - Error rate: < 0.1% - Throughput: > 1000 requests/second --- ## Security and Compliance ### Network Policies Implement network policies to restrict pod-to-pod communication: ```yaml # network-policy.yaml apiVersion:",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_50",
              "content": "End-to-end latency: < 100ms (P99) ### Resource Utilization **CPU Utilization** (Target: 60-70% average): - API Gateway: 55% average, 80% peak - Workflow Engine: 65% average, 85% peak - Auth Service: 40% average, 60% peak **Memory Utilization** (Target: < 80%): - API Gateway: 2.5GB average per pod (4GB allocated) - Workflow Engine: 6GB average per pod (8GB allocated) - Notification Service: 2.8GB average per pod (4GB allocated) **Network Throughput**: - Ingress: 2 Gbps average, 5 Gbps",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_63",
              "content": "**6-Month Projection** (Jul 2026): - Daily workflow executions: 4M (+60%) - Active users: 225,000 (+50%) - API requests per day: 80M (+60%) - Database size: 2TB (+67%) - Required scaling: +30% compute, +50% storage ### Contact Information - **Architecture Team**: architecture@cloudflow.internal - **On-Call Engineer**: PagerDuty escalation - **Security Team**: security@cloudflow.internal - **Documentation**: https://wiki.cloudflow.internal/architecture --- **Document Revision History**: - v2.3.1 (Jan 15, 2026): Updated performance metrics, added DR test results - v2.3.0 (Dec 1, 2025): Added",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_0",
              "content": "# CloudFlow Platform - System Architecture Overview **Document Version:** 2.3.1 **Last Updated:** January 15, 2026 **Owner:** Platform Architecture Team **Status:** Production ## Executive Summary CloudFlow is a distributed, cloud-native workflow automation platform designed to orchestrate complex business processes at scale. The platform processes over 2.5 million workflow executions daily with an average P99 latency of 180ms for API operations and 4.2 seconds for workflow execution.",
              "score": null
            },
            {
              "doc_id": "user_guide",
              "chunk_id": "user_guide_fix_36",
              "content": "- Success rate: 98.2% - Current queue depth: 3 ### Data Limits - **Maximum request/response size**: 10MB per action - **Maximum execution payload**: 50MB total - **Variable value size**: 1MB per variable ### Enterprise Plan Limits Enterprise customers can request increased limits: - Up to 100 steps per workflow - Up to 10,000 executions per day - Up to 7200 second timeout (2 hours) - Priority execution queue - Dedicated capacity allocation Contact sales@cloudflow.io for",
              "score": null
            }
          ],
          "latency_ms": 3.5667780011863215,
          "query_id": "realistic_009",
          "dimension": "contextual",
          "query": "Setting SLOs for our service, what P99 latency does CloudFlow guarantee?"
        },
        {
          "key_facts": [
            "P99 latency: < 200ms",
            "average P99 latency of 180ms for API operations"
          ],
          "found_facts": [],
          "missed_facts": [
            "P99 latency: < 200ms",
            "average P99 latency of 180ms for API operations"
          ],
          "coverage": 0.0,
          "expected_docs": [
            "architecture_overview"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_16",
              "content": "per cycle - Accuracy: \u00b1 1 second for schedule triggers - Capacity: 100,000 active schedules --- ### Notification Service **Purpose**: Multi-channel notification delivery system supporting email, SMS, webhooks, and in-app notifications. **Technology**: Node.js with worker pool pattern **Replicas**: 8 pods (production), auto-scaling 6-16 **Resource Allocation**: 2 vCPU, 4GB RAM per pod **Key Responsibilities**: - Consume notification events from Kafka - Template rendering with Handlebars (cached in Redis) - Multi-channel delivery (Email via SendGrid, SMS via",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_18",
              "content": "IP warming, DKIM/SPF configured - SMS: Twilio with fallback to AWS SNS - Webhook: Signed payloads (HMAC-SHA256), retry 3 times with exponential backoff - In-app: WebSocket connections via Socket.io **Template Management**: - Templates stored in PostgreSQL with versioning - Compiled templates cached in Redis (1-hour TTL) - Support for localization (i18n) with 12 languages - A/B testing capability for email campaigns **Performance Targets**: - Email delivery: < 5 seconds (P99) from event to SendGrid -",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_50",
              "content": "End-to-end latency: < 100ms (P99) ### Resource Utilization **CPU Utilization** (Target: 60-70% average): - API Gateway: 55% average, 80% peak - Workflow Engine: 65% average, 85% peak - Auth Service: 40% average, 60% peak **Memory Utilization** (Target: < 80%): - API Gateway: 2.5GB average per pod (4GB allocated) - Workflow Engine: 6GB average per pod (8GB allocated) - Notification Service: 2.8GB average per pod (4GB allocated) **Network Throughput**: - Ingress: 2 Gbps average, 5 Gbps",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_49",
              "content": "errors for > 5 minutes - Database completely unavailable - Authentication system down - Data loss or corruption - **Response Time:** Immediate (< 15 minutes) - **Escalation:** Page on-call engineer immediately #### SEV-2: High (P2) - **Definition:** Major functionality impaired affecting multiple users - **Examples:** - Workflow execution success rate < 90% - Significant performance degradation (p95 latency > 10s) - Rate limiting affecting large customer segment - **Response Time:** < 1 hour - **Escalation:**",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_51",
              "content": "peak - Egress: 1.5 Gbps average, 4 Gbps peak - Internal (service mesh): 8 Gbps average ### Monitoring & Alerting **Key Metrics** (Monitored via Prometheus): - Request rate (per service, per endpoint) - Error rate (4xx, 5xx responses) - Latency percentiles (P50, P95, P99) - Resource utilization (CPU, memory, disk) - Cache hit/miss ratios - Database connection pool saturation - Kafka consumer lag **Alerts** (via PagerDuty): - P1 (Immediate): API error rate > 1%, database",
              "score": null
            }
          ],
          "latency_ms": 3.101971000432968,
          "query_id": "realistic_009",
          "dimension": "negation",
          "query": "Why are we getting alerts at 200ms latency?"
        },
        {
          "key_facts": [
            "RPO (Recovery Point Objective): 1 hour",
            "RTO (Recovery Time Objective): 4 hours"
          ],
          "found_facts": [
            "RPO (Recovery Point Objective): 1 hour",
            "RTO (Recovery Time Objective): 4 hours"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "architecture_overview",
            "deployment_guide"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_60",
              "content": "simulation with all stakeholders **Last DR Test Results** (Dec 15, 2025): - Scenario: Full region failover - Actual RTO: 2 hours 23 minutes (target: 4 hours) - Actual RPO: 42 minutes (target: 1 hour) - Issues identified: DNS propagation slower than expected (resolved) - Success criteria: Met all recovery objectives ### Business Continuity **Communication Plan**: - Status page: status.cloudflow.com (updated every 15 minutes during incident) - Customer notifications: Email + SMS for all P1 incidents",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_52",
              "content": "connection failure - P2 (< 15 min): P99 latency > 500ms, cache hit rate < 80% - P3 (< 1 hour): Resource utilization > 85%, replica count < minimum **SLI/SLO/SLA**: - SLI: API success rate (non-5xx responses) - SLO: 99.9% uptime per month (43 minutes downtime allowance) - SLA: 99.5% uptime guarantee (customer-facing SLA with credits) --- ## Disaster Recovery ### Recovery Objectives **RPO (Recovery Point Objective): 1 hour** - Maximum acceptable data loss: 1",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_59",
              "content": "- Procedure: 1. Identify corruption time window 2. Restore from snapshot prior to corruption 3. Replay WAL logs up to corruption point 4. Verify data integrity 5. Resume normal operations - Recovery time: 1-3 hours depending on data volume - Data loss: None if corruption detected quickly ### Testing & Validation **DR Drill Schedule**: - Monthly: Automated failover test (single AZ failure simulation) - Quarterly: Full DR region failover (non-production hours) - Annually: Complete disaster",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_56",
              "content": "S3 (point-in-time recovery) Weekly: Backup validation test Monthly: Long-term archive to Glacier ``` **Configuration Backups**: - Kubernetes manifests: Stored in Git (GitOps with ArgoCD) - Vault secrets: Automated snapshot every 6 hours - Kafka topic configurations: Exported daily to S3 ### Disaster Recovery Procedures **Scenario 1: Single AZ Failure** - Detection: Health checks fail for entire AZ (< 30 seconds) - Action: Traffic automatically routed to healthy AZs by ALB - Recovery time: < 5",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_53",
              "content": "hour of transactions - Achieved through: Continuous database replication + hourly snapshots - Kafka retention: 7 days allows event replay **RTO (Recovery Time Objective): 4 hours** - Maximum acceptable downtime: 4 hours for full system recovery - Includes: Failover, data verification, and service restoration - Automated runbooks reduce RTO to < 2 hours for common scenarios ### High Availability Architecture **Multi-AZ Deployment**: ``` Region: us-east-1 AZ-1a: AZ-1b: AZ-1c: - API Gateway (4) - API Gateway",
              "score": null
            }
          ],
          "latency_ms": 3.041508000023896,
          "query_id": "realistic_010",
          "dimension": "original",
          "query": "What are the disaster recovery RPO and RTO values?"
        },
        {
          "key_facts": [
            "RPO (Recovery Point Objective): 1 hour",
            "RTO (Recovery Time Objective): 4 hours"
          ],
          "found_facts": [
            "RPO (Recovery Point Objective): 1 hour",
            "RTO (Recovery Time Objective): 4 hours"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "architecture_overview",
            "deployment_guide"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_57",
              "content": "minutes (no manual intervention) - Data loss: None (multi-AZ replication) **Scenario 2: Database Primary Failure** - Detection: Health check fails for primary database (< 30 seconds) - Action: Automatic promotion of read replica to primary - Recovery time: 30-60 seconds - Data loss: Minimal (< 1 second due to synchronous replication) **Scenario 3: Full Region Failure** - Detection: Multiple health check failures across all AZs (< 2 minutes) - Action: Manual failover to DR region",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_52",
              "content": "connection failure - P2 (< 15 min): P99 latency > 500ms, cache hit rate < 80% - P3 (< 1 hour): Resource utilization > 85%, replica count < minimum **SLI/SLO/SLA**: - SLI: API success rate (non-5xx responses) - SLO: 99.9% uptime per month (43 minutes downtime allowance) - SLA: 99.5% uptime guarantee (customer-facing SLA with credits) --- ## Disaster Recovery ### Recovery Objectives **RPO (Recovery Point Objective): 1 hour** - Maximum acceptable data loss: 1",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_53",
              "content": "hour of transactions - Achieved through: Continuous database replication + hourly snapshots - Kafka retention: 7 days allows event replay **RTO (Recovery Time Objective): 4 hours** - Maximum acceptable downtime: 4 hours for full system recovery - Includes: Failover, data verification, and service restoration - Automated runbooks reduce RTO to < 2 hours for common scenarios ### High Availability Architecture **Multi-AZ Deployment**: ``` Region: us-east-1 AZ-1a: AZ-1b: AZ-1c: - API Gateway (4) - API Gateway",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_49",
              "content": "errors for > 5 minutes - Database completely unavailable - Authentication system down - Data loss or corruption - **Response Time:** Immediate (< 15 minutes) - **Escalation:** Page on-call engineer immediately #### SEV-2: High (P2) - **Definition:** Major functionality impaired affecting multiple users - **Examples:** - Workflow execution success rate < 90% - Significant performance degradation (p95 latency > 10s) - Rate limiting affecting large customer segment - **Response Time:** < 1 hour - **Escalation:**",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_23",
              "content": "for high availability - Automated backups: Daily snapshots, 30-day retention - Point-in-time recovery: 5-minute granularity **Database Schema Design**: ``` Core Tables: - users (5M rows): User accounts and profiles - workflows (2M rows): Workflow definitions - workflow_executions (500M rows, partitioned): Execution history - workflow_steps (2B rows, partitioned): Individual step records - schedules (100K rows): Scheduled workflow triggers - notifications (1B rows, partitioned): Notification delivery log Partitioning Strategy: - workflow_executions: Monthly partitions by created_at - workflow_steps:",
              "score": null
            }
          ],
          "latency_ms": 3.2936380011960864,
          "query_id": "realistic_010",
          "dimension": "synonym",
          "query": "What's the maximum data loss and recovery time?"
        },
        {
          "key_facts": [
            "RPO (Recovery Point Objective): 1 hour",
            "RTO (Recovery Time Objective): 4 hours"
          ],
          "found_facts": [
            "RPO (Recovery Point Objective): 1 hour"
          ],
          "missed_facts": [
            "RTO (Recovery Time Objective): 4 hours"
          ],
          "coverage": 0.5,
          "expected_docs": [
            "architecture_overview",
            "deployment_guide"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_59",
              "content": "- Procedure: 1. Identify corruption time window 2. Restore from snapshot prior to corruption 3. Replay WAL logs up to corruption point 4. Verify data integrity 5. Resume normal operations - Recovery time: 1-3 hours depending on data volume - Data loss: None if corruption detected quickly ### Testing & Validation **DR Drill Schedule**: - Monthly: Automated failover test (single AZ failure simulation) - Quarterly: Full DR region failover (non-production hours) - Annually: Complete disaster",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_56",
              "content": "S3 (point-in-time recovery) Weekly: Backup validation test Monthly: Long-term archive to Glacier ``` **Configuration Backups**: - Kubernetes manifests: Stored in Git (GitOps with ArgoCD) - Vault secrets: Automated snapshot every 6 hours - Kafka topic configurations: Exported daily to S3 ### Disaster Recovery Procedures **Scenario 1: Single AZ Failure** - Detection: Health checks fail for entire AZ (< 30 seconds) - Action: Traffic automatically routed to healthy AZs by ALB - Recovery time: < 5",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_60",
              "content": "simulation with all stakeholders **Last DR Test Results** (Dec 15, 2025): - Scenario: Full region failover - Actual RTO: 2 hours 23 minutes (target: 4 hours) - Actual RPO: 42 minutes (target: 1 hour) - Issues identified: DNS propagation slower than expected (resolved) - Success criteria: Met all recovery objectives ### Business Continuity **Communication Plan**: - Status page: status.cloudflow.com (updated every 15 minutes during incident) - Customer notifications: Email + SMS for all P1 incidents",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_51",
              "content": "< 2 business days - **Escalation:** Standard support ticket ### Escalation Steps #### Step 1: Initial Assessment (0-5 minutes) ```bash # Run health check cloudflow health check --comprehensive # Check status page curl https://status.cloudflow.io/api/v1/status.json # Review recent changes cloudflow audit log --last 2h --event-type \"deployment,configuration\" # Check metrics dashboard cloudflow metrics dashboard --incident-mode ``` #### Step 2: Gather Information (5-15 minutes) Create incident document with: - Incident timestamp and duration - Affected services and endpoints",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_52",
              "content": "connection failure - P2 (< 15 min): P99 latency > 500ms, cache hit rate < 80% - P3 (< 1 hour): Resource utilization > 85%, replica count < minimum **SLI/SLO/SLA**: - SLI: API success rate (non-5xx responses) - SLO: 99.9% uptime per month (43 minutes downtime allowance) - SLA: 99.5% uptime guarantee (customer-facing SLA with credits) --- ## Disaster Recovery ### Recovery Objectives **RPO (Recovery Point Objective): 1 hour** - Maximum acceptable data loss: 1",
              "score": null
            }
          ],
          "latency_ms": 3.127157999188057,
          "query_id": "realistic_010",
          "dimension": "problem",
          "query": "How long will it take to recover from a disaster?"
        },
        {
          "key_facts": [
            "RPO (Recovery Point Objective): 1 hour",
            "RTO (Recovery Time Objective): 4 hours"
          ],
          "found_facts": [
            "RPO (Recovery Point Objective): 1 hour",
            "RTO (Recovery Time Objective): 4 hours"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "architecture_overview",
            "deployment_guide"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_60",
              "content": "simulation with all stakeholders **Last DR Test Results** (Dec 15, 2025): - Scenario: Full region failover - Actual RTO: 2 hours 23 minutes (target: 4 hours) - Actual RPO: 42 minutes (target: 1 hour) - Issues identified: DNS propagation slower than expected (resolved) - Success criteria: Met all recovery objectives ### Business Continuity **Communication Plan**: - Status page: status.cloudflow.com (updated every 15 minutes during incident) - Customer notifications: Email + SMS for all P1 incidents",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_52",
              "content": "connection failure - P2 (< 15 min): P99 latency > 500ms, cache hit rate < 80% - P3 (< 1 hour): Resource utilization > 85%, replica count < minimum **SLI/SLO/SLA**: - SLI: API success rate (non-5xx responses) - SLO: 99.9% uptime per month (43 minutes downtime allowance) - SLA: 99.5% uptime guarantee (customer-facing SLA with credits) --- ## Disaster Recovery ### Recovery Objectives **RPO (Recovery Point Objective): 1 hour** - Maximum acceptable data loss: 1",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_13",
              "content": "< 2 seconds (P99) - Complex workflow (20+ steps): < 5 seconds (P99) - Throughput: 500 concurrent workflow executions per pod --- ### Scheduler Service **Purpose**: Time-based workflow triggering system supporting cron-like schedules and one-time delayed executions. **Technology**: Go with distributed locking via Redis **Replicas**: 4 pods (production), active-passive with leader election **Resource Allocation**: 2 vCPU, 4GB RAM per pod **Key Responsibilities**: - Parse and validate cron expressions (extended format supporting seconds) - Maintain schedule",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_53",
              "content": "hour of transactions - Achieved through: Continuous database replication + hourly snapshots - Kafka retention: 7 days allows event replay **RTO (Recovery Time Objective): 4 hours** - Maximum acceptable downtime: 4 hours for full system recovery - Includes: Failover, data verification, and service restoration - Automated runbooks reduce RTO to < 2 hours for common scenarios ### High Availability Architecture **Multi-AZ Deployment**: ``` Region: us-east-1 AZ-1a: AZ-1b: AZ-1c: - API Gateway (4) - API Gateway",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_15",
              "content": "Types**: - **Cron-based**: `0 */5 * * * *` (every 5 minutes) - **One-time**: Specific timestamp for delayed execution - **Interval-based**: Every N seconds/minutes/hours **Reliability Features**: - Leader election using Redis with 30-second lease - Heartbeat mechanism to detect leader failure (5-second interval) - Automatic failover to standby scheduler (< 10 seconds) - Schedule versioning to handle updates during execution - Missed execution policy: SKIP, RUN_ONCE, or RUN_ALL **Performance Targets**: - Schedule evaluation: < 100ms",
              "score": null
            }
          ],
          "latency_ms": 3.130493998469319,
          "query_id": "realistic_010",
          "dimension": "casual",
          "query": "RPO RTO"
        },
        {
          "key_facts": [
            "RPO (Recovery Point Objective): 1 hour",
            "RTO (Recovery Time Objective): 4 hours"
          ],
          "found_facts": [],
          "missed_facts": [
            "RPO (Recovery Point Objective): 1 hour",
            "RTO (Recovery Time Objective): 4 hours"
          ],
          "coverage": 0.0,
          "expected_docs": [
            "architecture_overview",
            "deployment_guide"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_57",
              "content": "For urgent issues, use emergency escalation procedures **Remember:** Always capture logs, metrics, and reproduction steps before escalating! --- *Last updated: January 24, 2026* *Document version: 3.2.1* *Feedback: docs-feedback@cloudflow.io*",
              "score": null
            },
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_0",
              "content": "# CloudFlow Platform - Deployment and Operations Guide **Version:** 2.4.0 **Last Updated:** January 2026 **Target Environment:** Production (AWS EKS) ## Table of Contents 1. [Overview](#overview) 2. [Prerequisites](#prerequisites) 3. [Infrastructure Setup](#infrastructure-setup) 4. [Kubernetes Deployment](#kubernetes-deployment) 5. [Database Configuration](#database-configuration) 6. [Monitoring and Observability](#monitoring-and-observability) 7. [Backup and Disaster Recovery](#backup-and-disaster-recovery) 8. [Scaling and Performance](#scaling-and-performance) 9. [Security and Compliance](#security-and-compliance) 10. [Troubleshooting](#troubleshooting) --- ## Overview CloudFlow is a cloud-native workflow orchestration platform designed for high-availability production environments.",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_51",
              "content": "< 2 business days - **Escalation:** Standard support ticket ### Escalation Steps #### Step 1: Initial Assessment (0-5 minutes) ```bash # Run health check cloudflow health check --comprehensive # Check status page curl https://status.cloudflow.io/api/v1/status.json # Review recent changes cloudflow audit log --last 2h --event-type \"deployment,configuration\" # Check metrics dashboard cloudflow metrics dashboard --incident-mode ``` #### Step 2: Gather Information (5-15 minutes) Create incident document with: - Incident timestamp and duration - Affected services and endpoints",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_23",
              "content": "for high availability - Automated backups: Daily snapshots, 30-day retention - Point-in-time recovery: 5-minute granularity **Database Schema Design**: ``` Core Tables: - users (5M rows): User accounts and profiles - workflows (2M rows): Workflow definitions - workflow_executions (500M rows, partitioned): Execution history - workflow_steps (2B rows, partitioned): Individual step records - schedules (100K rows): Scheduled workflow triggers - notifications (1B rows, partitioned): Notification delivery log Partitioning Strategy: - workflow_executions: Monthly partitions by created_at - workflow_steps:",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_1",
              "content": "This document provides a comprehensive overview of the system architecture, including microservices design, data flow patterns, infrastructure decisions, and operational considerations. ## Table of Contents 1. [High-Level Architecture](#high-level-architecture) 2. [Microservices Breakdown](#microservices-breakdown) 3. [Data Flow Architecture](#data-flow-architecture) 4. [Database Architecture](#database-architecture) 5. [Message Queue Patterns](#message-queue-patterns) 6. [Caching Strategy](#caching-strategy) 7. [Security Architecture](#security-architecture) 8. [Performance Characteristics](#performance-characteristics) 9. [Disaster Recovery](#disaster-recovery) --- ## High-Level Architecture CloudFlow follows a microservices architecture pattern deployed across multiple availability zones in AWS.",
              "score": null
            }
          ],
          "latency_ms": 3.277417999925092,
          "query_id": "realistic_010",
          "dimension": "contextual",
          "query": "For our business continuity plan, what are CloudFlow's recovery objectives?"
        },
        {
          "key_facts": [
            "RPO (Recovery Point Objective): 1 hour",
            "RTO (Recovery Time Objective): 4 hours"
          ],
          "found_facts": [
            "RPO (Recovery Point Objective): 1 hour"
          ],
          "missed_facts": [
            "RTO (Recovery Time Objective): 4 hours"
          ],
          "coverage": 0.5,
          "expected_docs": [
            "architecture_overview",
            "deployment_guide"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_57",
              "content": "minutes (no manual intervention) - Data loss: None (multi-AZ replication) **Scenario 2: Database Primary Failure** - Detection: Health check fails for primary database (< 30 seconds) - Action: Automatic promotion of read replica to primary - Recovery time: 30-60 seconds - Data loss: Minimal (< 1 second due to synchronous replication) **Scenario 3: Full Region Failure** - Detection: Multiple health check failures across all AZs (< 2 minutes) - Action: Manual failover to DR region",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_52",
              "content": "connection failure - P2 (< 15 min): P99 latency > 500ms, cache hit rate < 80% - P3 (< 1 hour): Resource utilization > 85%, replica count < minimum **SLI/SLO/SLA**: - SLI: API success rate (non-5xx responses) - SLO: 99.9% uptime per month (43 minutes downtime allowance) - SLA: 99.5% uptime guarantee (customer-facing SLA with credits) --- ## Disaster Recovery ### Recovery Objectives **RPO (Recovery Point Objective): 1 hour** - Maximum acceptable data loss: 1",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_17",
              "content": "Twilio, Webhooks) - Retry logic with dead letter queue for failed deliveries - Delivery status tracking and analytics - User preference management (opt-in/opt-out) **Notification Flow**: ``` Event Source \u2192 Kafka (notifications.email topic) \u2502 \u25bc Notification Service Consumer \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u25bc \u25bc \u25bc [Email Queue] [SMS Queue] [Webhook Queue] \u2502 \u2502 \u2502 \u25bc \u25bc \u25bc [SendGrid] [Twilio] [HTTP POST] \u2502 \u2502 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u25bc Delivery Status \u2192 PostgreSQL ``` **Channel Configuration**: - Email: SendGrid with",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_35",
              "content": "### Dead Letter Queue (DLQ) Failed messages after all retry attempts are routed to DLQ: ``` Message processing fails (3 retries with exponential backoff) \u2502 \u25bc Publish to dead-letter-queue topic \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u25bc \u25bc Alert to PagerDuty Store in PostgreSQL for analysis \u2502 \u2502 \u25bc \u25bc Manual investigation Automated pattern detection ``` DLQ Processing SLA: < 4 hours for critical events, < 24 hours for non-critical.",
              "score": null
            },
            {
              "doc_id": "user_guide",
              "chunk_id": "user_guide_fix_37",
              "content": "Enterprise pricing and custom limits. ## Best Practices Follow these best practices to build reliable, maintainable workflows: ### 1. Use Descriptive Names **Good:** - Workflow: \"Sync Customer Data from Salesforce to Database\" - Step: \"validate_customer_email\" **Bad:** - Workflow: \"Workflow 1\" - Step: \"step3\" ### 2.",
              "score": null
            }
          ],
          "latency_ms": 3.0588810004701372,
          "query_id": "realistic_010",
          "dimension": "negation",
          "query": "Why can't we guarantee zero data loss?"
        },
        {
          "key_facts": [
            "3600 seconds",
            "exceeded maximum execution time of 3600 seconds"
          ],
          "found_facts": [
            "3600 seconds",
            "exceeded maximum execution time of 3600 seconds"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "troubleshooting_guide",
            "user_guide"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_24",
              "content": "250 # Reduce idle connection timeout cloudflow config set db.pool.idle_timeout 120 # 2 minutes ``` --- ## Workflow Execution Failures ### Timeout Errors (3600 second limit) #### Error Message ``` WorkflowExecutionError: Workflow exceeded maximum execution time of 3600 seconds Status: TIMEOUT Workflow ID: wf_9k2n4m8p1q Execution ID: exec_7h3j6k9m2n ``` #### Analysis ```bash # Get workflow execution details cloudflow workflows executions get exec_7h3j6k9m2n --verbose # Check step-by-step breakdown cloudflow workflows executions steps exec_7h3j6k9m2n # Sample output: #",
              "score": null
            },
            {
              "doc_id": "user_guide",
              "chunk_id": "user_guide_fix_34",
              "content": "Keep workflows focused and modular. If you need more steps, consider splitting into multiple workflows connected via webhooks. ### Execution Timeout - **Default**: 3600 seconds (60 minutes) - **Behavior**: Workflows exceeding this timeout are automatically terminated - **Custom Timeouts**: Enterprise plans can request custom timeout limits **Setting Step-Level Timeouts:** ```yaml - id: long_running_task action: http_request config: url: \"https://api.example.com/process\" timeout: 300 # 5 minutes for this specific step ``` ### Execution Limits - **Maximum**: 1000 executions",
              "score": null
            },
            {
              "doc_id": "user_guide",
              "chunk_id": "user_guide_fix_36",
              "content": "- Success rate: 98.2% - Current queue depth: 3 ### Data Limits - **Maximum request/response size**: 10MB per action - **Maximum execution payload**: 50MB total - **Variable value size**: 1MB per variable ### Enterprise Plan Limits Enterprise customers can request increased limits: - Up to 100 steps per workflow - Up to 10,000 executions per day - Up to 7200 second timeout (2 hours) - Priority execution queue - Dedicated capacity allocation Contact sales@cloudflow.io for",
              "score": null
            },
            {
              "doc_id": "user_guide",
              "chunk_id": "user_guide_fix_35",
              "content": "per day (per workflow) - **Rate Limiting**: 100 concurrent executions per workflow - **Burst Limit**: 10 executions per second **What happens when limits are reached:** - New executions are queued automatically - Webhook triggers return HTTP 429 (Too Many Requests) - Scheduled executions are skipped (logged in audit trail) - Email notifications sent to workflow owner **Monitoring Usage:** View real-time metrics in your workflow dashboard: - Executions today: 847 / 1000 - Average duration: 12.3s",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_26",
              "content": "Increase workflow timeout (if justified):** ```bash # Update workflow configuration cloudflow workflows update wf_9k2n4m8p1q \\ --timeout 7200 \\ --reason \"Large dataset processing requires extended time\" # Verify update cloudflow workflows get wf_9k2n4m8p1q | grep timeout ``` **2. Optimize slow steps:** ```bash # Enable parallel processing cloudflow workflows update wf_9k2n4m8p1q \\ --step data_transformation \\ --parallel-workers 8 \\ --batch-size 1000 # Add checkpointing for long operations cloudflow workflows update wf_9k2n4m8p1q \\ --step data_transformation \\ --enable-checkpointing \\ --checkpoint-interval",
              "score": null
            }
          ],
          "latency_ms": 3.0823750003037276,
          "query_id": "realistic_011",
          "dimension": "original",
          "query": "What is the maximum workflow execution timeout?"
        },
        {
          "key_facts": [
            "3600 seconds",
            "exceeded maximum execution time of 3600 seconds"
          ],
          "found_facts": [
            "3600 seconds",
            "exceeded maximum execution time of 3600 seconds"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "troubleshooting_guide",
            "user_guide"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "user_guide",
              "chunk_id": "user_guide_fix_34",
              "content": "Keep workflows focused and modular. If you need more steps, consider splitting into multiple workflows connected via webhooks. ### Execution Timeout - **Default**: 3600 seconds (60 minutes) - **Behavior**: Workflows exceeding this timeout are automatically terminated - **Custom Timeouts**: Enterprise plans can request custom timeout limits **Setting Step-Level Timeouts:** ```yaml - id: long_running_task action: http_request config: url: \"https://api.example.com/process\" timeout: 300 # 5 minutes for this specific step ``` ### Execution Limits - **Maximum**: 1000 executions",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_24",
              "content": "250 # Reduce idle connection timeout cloudflow config set db.pool.idle_timeout 120 # 2 minutes ``` --- ## Workflow Execution Failures ### Timeout Errors (3600 second limit) #### Error Message ``` WorkflowExecutionError: Workflow exceeded maximum execution time of 3600 seconds Status: TIMEOUT Workflow ID: wf_9k2n4m8p1q Execution ID: exec_7h3j6k9m2n ``` #### Analysis ```bash # Get workflow execution details cloudflow workflows executions get exec_7h3j6k9m2n --verbose # Check step-by-step breakdown cloudflow workflows executions steps exec_7h3j6k9m2n # Sample output: #",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_25",
              "content": "Step 1: data_ingestion - Duration: 245s - Status: SUCCESS # Step 2: data_validation - Duration: 123s - Status: SUCCESS # Step 3: data_transformation - Duration: 3189s - Status: TIMEOUT # Step 4: data_export - Duration: 0s - Status: SKIPPED # Identify bottleneck step cloudflow workflows analyze exec_7h3j6k9m2n --show-bottlenecks ``` #### Solutions **1.",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_47",
              "content": "Operations** (P99 latency): - `GET /workflows/{id}`: < 50ms (cache hit), < 150ms (cache miss) - `POST /workflows`: < 200ms (includes validation and database write) - `POST /workflows/{id}/execute`: < 100ms (async, returns execution ID) - `GET /workflows/{id}/history`: < 300ms (paginated, 50 records per page) **Workflow Execution** (P99 latency): - Simple workflow (< 5 steps): < 2 seconds - Medium workflow (5-15 steps): < 5 seconds - Complex workflow (> 15 steps): < 15 seconds - Parallel",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_26",
              "content": "Increase workflow timeout (if justified):** ```bash # Update workflow configuration cloudflow workflows update wf_9k2n4m8p1q \\ --timeout 7200 \\ --reason \"Large dataset processing requires extended time\" # Verify update cloudflow workflows get wf_9k2n4m8p1q | grep timeout ``` **2. Optimize slow steps:** ```bash # Enable parallel processing cloudflow workflows update wf_9k2n4m8p1q \\ --step data_transformation \\ --parallel-workers 8 \\ --batch-size 1000 # Add checkpointing for long operations cloudflow workflows update wf_9k2n4m8p1q \\ --step data_transformation \\ --enable-checkpointing \\ --checkpoint-interval",
              "score": null
            }
          ],
          "latency_ms": 3.00950900054886,
          "query_id": "realistic_011",
          "dimension": "synonym",
          "query": "How long can a workflow run before timing out?"
        },
        {
          "key_facts": [
            "3600 seconds",
            "exceeded maximum execution time of 3600 seconds"
          ],
          "found_facts": [
            "3600 seconds",
            "exceeded maximum execution time of 3600 seconds"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "troubleshooting_guide",
            "user_guide"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_24",
              "content": "250 # Reduce idle connection timeout cloudflow config set db.pool.idle_timeout 120 # 2 minutes ``` --- ## Workflow Execution Failures ### Timeout Errors (3600 second limit) #### Error Message ``` WorkflowExecutionError: Workflow exceeded maximum execution time of 3600 seconds Status: TIMEOUT Workflow ID: wf_9k2n4m8p1q Execution ID: exec_7h3j6k9m2n ``` #### Analysis ```bash # Get workflow execution details cloudflow workflows executions get exec_7h3j6k9m2n --verbose # Check step-by-step breakdown cloudflow workflows executions steps exec_7h3j6k9m2n # Sample output: #",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_42",
              "content": "\"connection.*pool|could not connect|database.*timeout\" # Find slow queries kubectl logs -n cloudflow deployment/cloudflow-api | \\ grep \"slow query\" | \\ awk '{print $NF}' | \\ sort -n | \\ tail -n 20 # Find deadlock errors kubectl logs -n cloudflow deployment/cloudflow-api | \\ grep -i \"deadlock detected\" ``` #### Workflow Execution Errors ```bash # Find workflow timeout errors kubectl logs -n cloudflow deployment/cloudflow-workflow-engine | \\ grep -E \"timeout|exceeded.*3600\" # Find workflow retry attempts kubectl logs -n",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_36",
              "content": "remaining\" local reset=$(curl -s -I https://api.cloudflow.io/api/v1/workflows \\ -H \"Authorization: Bearer $CF_ACCESS_TOKEN\" | \\ grep -i \"x-ratelimit-reset\" | \\ awk '{print $2}' | tr -d '\\r') local wait_time=$((reset - $(date +%s))) echo \"Rate limit resets in $wait_time seconds\" sleep $wait_time fi } # Use before API calls check_rate_limit cloudflow workflows execute wf_9k2n4m8p1q ``` #### Optimization Strategies **1.",
              "score": null
            },
            {
              "doc_id": "user_guide",
              "chunk_id": "user_guide_fix_34",
              "content": "Keep workflows focused and modular. If you need more steps, consider splitting into multiple workflows connected via webhooks. ### Execution Timeout - **Default**: 3600 seconds (60 minutes) - **Behavior**: Workflows exceeding this timeout are automatically terminated - **Custom Timeouts**: Enterprise plans can request custom timeout limits **Setting Step-Level Timeouts:** ```yaml - id: long_running_task action: http_request config: url: \"https://api.example.com/process\" timeout: 300 # 5 minutes for this specific step ``` ### Execution Limits - **Maximum**: 1000 executions",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_6",
              "content": "Enable automatic time sync sudo timedatectl set-ntp true # Restart CloudFlow client cloudflow restart ``` **4. Insufficient Permissions** Error: `User does not have required permissions for this operation` **Check user roles:** ```bash cloudflow auth whoami --verbose # Expected output: # User: john.doe@company.com # Roles: developer, workflow-admin # Scopes: workflow:*, data:read, metrics:read ``` **Request permission elevation:** ```bash # Submit access request cloudflow auth request-access \\ --resource \"workflow:production:deploy\" \\ --justification \"Deploy critical hotfix for TICKET-1234\" # Check",
              "score": null
            }
          ],
          "latency_ms": 3.018005001649726,
          "query_id": "realistic_011",
          "dimension": "problem",
          "query": "My workflow is being killed after an hour"
        },
        {
          "key_facts": [
            "3600 seconds",
            "exceeded maximum execution time of 3600 seconds"
          ],
          "found_facts": [
            "3600 seconds",
            "exceeded maximum execution time of 3600 seconds"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "troubleshooting_guide",
            "user_guide"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_24",
              "content": "250 # Reduce idle connection timeout cloudflow config set db.pool.idle_timeout 120 # 2 minutes ``` --- ## Workflow Execution Failures ### Timeout Errors (3600 second limit) #### Error Message ``` WorkflowExecutionError: Workflow exceeded maximum execution time of 3600 seconds Status: TIMEOUT Workflow ID: wf_9k2n4m8p1q Execution ID: exec_7h3j6k9m2n ``` #### Analysis ```bash # Get workflow execution details cloudflow workflows executions get exec_7h3j6k9m2n --verbose # Check step-by-step breakdown cloudflow workflows executions steps exec_7h3j6k9m2n # Sample output: #",
              "score": null
            },
            {
              "doc_id": "user_guide",
              "chunk_id": "user_guide_fix_34",
              "content": "Keep workflows focused and modular. If you need more steps, consider splitting into multiple workflows connected via webhooks. ### Execution Timeout - **Default**: 3600 seconds (60 minutes) - **Behavior**: Workflows exceeding this timeout are automatically terminated - **Custom Timeouts**: Enterprise plans can request custom timeout limits **Setting Step-Level Timeouts:** ```yaml - id: long_running_task action: http_request config: url: \"https://api.example.com/process\" timeout: 300 # 5 minutes for this specific step ``` ### Execution Limits - **Maximum**: 1000 executions",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_28",
              "content": "Sequence ``` Attempt 1: Immediate Attempt 2: Wait 1s (2^0 * 1s) Attempt 3: Wait 2s (2^1 * 1s) Attempt 4: Wait 4s (2^2 * 1s) ``` #### Configuration ```bash # View current retry settings cloudflow workflows get wf_9k2n4m8p1q --format json | jq '.retry_policy' # Customize retry behavior cloudflow workflows update wf_9k2n4m8p1q \\ --retry-max-attempts 5 \\ --retry-initial-delay 2000 \\ --retry-backoff-multiplier 2 \\ --retry-max-delay 120000 \\ --retry-on-errors \"NETWORK_ERROR,TIMEOUT,RATE_LIMIT\" # Disable retry for specific step cloudflow workflows",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_27",
              "content": "300 ``` **3. Split workflow into smaller workflows:** ```bash # Create sub-workflows cloudflow workflows create data-pipeline-part1 \\ --steps \"data_ingestion,data_validation\" \\ --timeout 1800 cloudflow workflows create data-pipeline-part2 \\ --steps \"data_transformation,data_export\" \\ --timeout 3600 \\ --trigger workflow_completed \\ --trigger-workflow data-pipeline-part1 ``` ### Retry Logic and Exponential Backoff CloudFlow implements automatic retry with exponential backoff for transient failures: - Max retries: 3 - Initial delay: 1 second - Backoff multiplier: 2 - Max delay: 60 seconds #### Retry",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_26",
              "content": "Increase workflow timeout (if justified):** ```bash # Update workflow configuration cloudflow workflows update wf_9k2n4m8p1q \\ --timeout 7200 \\ --reason \"Large dataset processing requires extended time\" # Verify update cloudflow workflows get wf_9k2n4m8p1q | grep timeout ``` **2. Optimize slow steps:** ```bash # Enable parallel processing cloudflow workflows update wf_9k2n4m8p1q \\ --step data_transformation \\ --parallel-workers 8 \\ --batch-size 1000 # Add checkpointing for long operations cloudflow workflows update wf_9k2n4m8p1q \\ --step data_transformation \\ --enable-checkpointing \\ --checkpoint-interval",
              "score": null
            }
          ],
          "latency_ms": 3.2242899997072527,
          "query_id": "realistic_011",
          "dimension": "casual",
          "query": "workflow timeout"
        },
        {
          "key_facts": [
            "3600 seconds",
            "exceeded maximum execution time of 3600 seconds"
          ],
          "found_facts": [
            "3600 seconds",
            "exceeded maximum execution time of 3600 seconds"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "troubleshooting_guide",
            "user_guide"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_25",
              "content": "Step 1: data_ingestion - Duration: 245s - Status: SUCCESS # Step 2: data_validation - Duration: 123s - Status: SUCCESS # Step 3: data_transformation - Duration: 3189s - Status: TIMEOUT # Step 4: data_export - Duration: 0s - Status: SKIPPED # Identify bottleneck step cloudflow workflows analyze exec_7h3j6k9m2n --show-bottlenecks ``` #### Solutions **1.",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_26",
              "content": "Increase workflow timeout (if justified):** ```bash # Update workflow configuration cloudflow workflows update wf_9k2n4m8p1q \\ --timeout 7200 \\ --reason \"Large dataset processing requires extended time\" # Verify update cloudflow workflows get wf_9k2n4m8p1q | grep timeout ``` **2. Optimize slow steps:** ```bash # Enable parallel processing cloudflow workflows update wf_9k2n4m8p1q \\ --step data_transformation \\ --parallel-workers 8 \\ --batch-size 1000 # Add checkpointing for long operations cloudflow workflows update wf_9k2n4m8p1q \\ --step data_transformation \\ --enable-checkpointing \\ --checkpoint-interval",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_24",
              "content": "250 # Reduce idle connection timeout cloudflow config set db.pool.idle_timeout 120 # 2 minutes ``` --- ## Workflow Execution Failures ### Timeout Errors (3600 second limit) #### Error Message ``` WorkflowExecutionError: Workflow exceeded maximum execution time of 3600 seconds Status: TIMEOUT Workflow ID: wf_9k2n4m8p1q Execution ID: exec_7h3j6k9m2n ``` #### Analysis ```bash # Get workflow execution details cloudflow workflows executions get exec_7h3j6k9m2n --verbose # Check step-by-step breakdown cloudflow workflows executions steps exec_7h3j6k9m2n # Sample output: #",
              "score": null
            },
            {
              "doc_id": "user_guide",
              "chunk_id": "user_guide_fix_34",
              "content": "Keep workflows focused and modular. If you need more steps, consider splitting into multiple workflows connected via webhooks. ### Execution Timeout - **Default**: 3600 seconds (60 minutes) - **Behavior**: Workflows exceeding this timeout are automatically terminated - **Custom Timeouts**: Enterprise plans can request custom timeout limits **Setting Step-Level Timeouts:** ```yaml - id: long_running_task action: http_request config: url: \"https://api.example.com/process\" timeout: 300 # 5 minutes for this specific step ``` ### Execution Limits - **Maximum**: 1000 executions",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_47",
              "content": "Operations** (P99 latency): - `GET /workflows/{id}`: < 50ms (cache hit), < 150ms (cache miss) - `POST /workflows`: < 200ms (includes validation and database write) - `POST /workflows/{id}/execute`: < 100ms (async, returns execution ID) - `GET /workflows/{id}/history`: < 300ms (paginated, 50 records per page) **Workflow Execution** (P99 latency): - Simple workflow (< 5 steps): < 2 seconds - Medium workflow (5-15 steps): < 5 seconds - Complex workflow (> 15 steps): < 15 seconds - Parallel",
              "score": null
            }
          ],
          "latency_ms": 3.153867000946775,
          "query_id": "realistic_011",
          "dimension": "contextual",
          "query": "I have a long-running data processing workflow, what's the time limit?"
        },
        {
          "key_facts": [
            "3600 seconds",
            "exceeded maximum execution time of 3600 seconds"
          ],
          "found_facts": [
            "3600 seconds",
            "exceeded maximum execution time of 3600 seconds"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "troubleshooting_guide",
            "user_guide"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_24",
              "content": "250 # Reduce idle connection timeout cloudflow config set db.pool.idle_timeout 120 # 2 minutes ``` --- ## Workflow Execution Failures ### Timeout Errors (3600 second limit) #### Error Message ``` WorkflowExecutionError: Workflow exceeded maximum execution time of 3600 seconds Status: TIMEOUT Workflow ID: wf_9k2n4m8p1q Execution ID: exec_7h3j6k9m2n ``` #### Analysis ```bash # Get workflow execution details cloudflow workflows executions get exec_7h3j6k9m2n --verbose # Check step-by-step breakdown cloudflow workflows executions steps exec_7h3j6k9m2n # Sample output: #",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_28",
              "content": "Sequence ``` Attempt 1: Immediate Attempt 2: Wait 1s (2^0 * 1s) Attempt 3: Wait 2s (2^1 * 1s) Attempt 4: Wait 4s (2^2 * 1s) ``` #### Configuration ```bash # View current retry settings cloudflow workflows get wf_9k2n4m8p1q --format json | jq '.retry_policy' # Customize retry behavior cloudflow workflows update wf_9k2n4m8p1q \\ --retry-max-attempts 5 \\ --retry-initial-delay 2000 \\ --retry-backoff-multiplier 2 \\ --retry-max-delay 120000 \\ --retry-on-errors \"NETWORK_ERROR,TIMEOUT,RATE_LIMIT\" # Disable retry for specific step cloudflow workflows",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_49",
              "content": "errors for > 5 minutes - Database completely unavailable - Authentication system down - Data loss or corruption - **Response Time:** Immediate (< 15 minutes) - **Escalation:** Page on-call engineer immediately #### SEV-2: High (P2) - **Definition:** Major functionality impaired affecting multiple users - **Examples:** - Workflow execution success rate < 90% - Significant performance degradation (p95 latency > 10s) - Rate limiting affecting large customer segment - **Response Time:** < 1 hour - **Escalation:**",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_47",
              "content": "Operations** (P99 latency): - `GET /workflows/{id}`: < 50ms (cache hit), < 150ms (cache miss) - `POST /workflows`: < 200ms (includes validation and database write) - `POST /workflows/{id}/execute`: < 100ms (async, returns execution ID) - `GET /workflows/{id}/history`: < 300ms (paginated, 50 records per page) **Workflow Execution** (P99 latency): - Simple workflow (< 5 steps): < 2 seconds - Medium workflow (5-15 steps): < 5 seconds - Complex workflow (> 15 steps): < 15 seconds - Parallel",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_27",
              "content": "300 ``` **3. Split workflow into smaller workflows:** ```bash # Create sub-workflows cloudflow workflows create data-pipeline-part1 \\ --steps \"data_ingestion,data_validation\" \\ --timeout 1800 cloudflow workflows create data-pipeline-part2 \\ --steps \"data_transformation,data_export\" \\ --timeout 3600 \\ --trigger workflow_completed \\ --trigger-workflow data-pipeline-part1 ``` ### Retry Logic and Exponential Backoff CloudFlow implements automatic retry with exponential backoff for transient failures: - Max retries: 3 - Initial delay: 1 second - Backoff multiplier: 2 - Max delay: 60 seconds #### Retry",
              "score": null
            }
          ],
          "latency_ms": 3.0153089992381865,
          "query_id": "realistic_011",
          "dimension": "negation",
          "query": "Why did my workflow fail after 3600 seconds?"
        },
        {
          "key_facts": [
            "RS256",
            "RS256 algorithm",
            "RS256 signing algorithm"
          ],
          "found_facts": [
            "RS256",
            "RS256 signing algorithm"
          ],
          "missed_facts": [
            "RS256 algorithm"
          ],
          "coverage": 0.6666666666666666,
          "expected_docs": [
            "api_reference"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_4",
              "content": "```python import jwt import time from cryptography.hazmat.primitives import serialization from cryptography.hazmat.backends import default_backend # Load your private key with open('private_key.pem', 'rb') as key_file: private_key = serialization.load_pem_private_key( key_file.read(), password=None, backend=default_backend() ) # Create JWT payload payload = { 'iss': 'your-app-id', 'sub': 'user-12345', 'aud': 'https://api.cloudflow.io', 'exp': int(time.time()) + 3600, 'iat': int(time.time()), 'scope': 'workflows:read workflows:write' } # Generate token token = jwt.encode(payload, private_key, algorithm='RS256') # Use in API request headers = {'Authorization': f'Bearer {token}'} ``` **Token Expiration:** All",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_42",
              "content": "Rules**: - Security groups: Deny all by default, explicit allow rules - Ingress: Only ALB can reach API Gateway (port 8080) - Egress: Services can only reach specific dependencies - Database access: Limited to application subnets only ### Authentication & Authorization **JWT Token Validation**: - Algorithm: RS256 (asymmetric signing) - Key rotation: Every 30 days with 7-day overlap period - Public key distribution: JWKS endpoint cached in Redis - Validation: Signature, expiry, issuer, audience claims",
              "score": null
            },
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_7",
              "content": "Secret key for JWT token signing | `<generated-secret-256-bit>` | Yes | | `LOG_LEVEL` | Application log verbosity | `info`, `debug`, `warn`, `error` | No (default: `info`) | | `API_PORT` | API server port | `3000` | No (default: `3000`) | | `WORKER_CONCURRENCY` | Number of concurrent workers | `10` | No (default: `5`) | | `SESSION_SECRET` | Session encryption key | `<generated-secret-256-bit>` | Yes | | `AWS_REGION` | AWS region for services | `us-east-1` |",
              "score": null
            },
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_3",
              "content": "\"client_id=YOUR_CLIENT_ID\" \\ -d \"client_secret=YOUR_CLIENT_SECRET\" \\ -d \"redirect_uri=https://yourapp.com/callback\" ``` ### JWT Tokens For advanced use cases, CloudFlow supports JSON Web Tokens (JWT) with RS256 signing algorithm. JWTs must include the following claims: - `iss` (issuer): Your application identifier - `sub` (subject): User or service account ID - `aud` (audience): `https://api.cloudflow.io` - `exp` (expiration): Unix timestamp (max 3600 seconds from `iat`) - `iat` (issued at): Unix timestamp - `scope`: Space-separated list of requested scopes Example JWT header:",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_4",
              "content": "Invalid JWT Signature** Error: `JWT signature verification failed` **Causes:** - Token was modified or corrupted - Using wrong signing key - Token generated with different secret **Resolution:** ```bash # Validate token structure cloudflow auth validate --token $CF_ACCESS_TOKEN # Generate new token cloudflow auth login --username <user> --password <pass> # For service accounts cloudflow auth service-account --create --name <sa-name> --scopes \"workflow:read,workflow:write\" ``` **3.",
              "score": null
            }
          ],
          "latency_ms": 3.019978999873274,
          "query_id": "realistic_012",
          "dimension": "original",
          "query": "What JWT algorithm is used for token signing?"
        },
        {
          "key_facts": [
            "RS256",
            "RS256 algorithm",
            "RS256 signing algorithm"
          ],
          "found_facts": [
            "RS256",
            "RS256 signing algorithm"
          ],
          "missed_facts": [
            "RS256 algorithm"
          ],
          "coverage": 0.6666666666666666,
          "expected_docs": [
            "api_reference"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_3",
              "content": "\"client_id=YOUR_CLIENT_ID\" \\ -d \"client_secret=YOUR_CLIENT_SECRET\" \\ -d \"redirect_uri=https://yourapp.com/callback\" ``` ### JWT Tokens For advanced use cases, CloudFlow supports JSON Web Tokens (JWT) with RS256 signing algorithm. JWTs must include the following claims: - `iss` (issuer): Your application identifier - `sub` (subject): User or service account ID - `aud` (audience): `https://api.cloudflow.io` - `exp` (expiration): Unix timestamp (max 3600 seconds from `iat`) - `iat` (issued at): Unix timestamp - `scope`: Space-separated list of requested scopes Example JWT header:",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_4",
              "content": "Invalid JWT Signature** Error: `JWT signature verification failed` **Causes:** - Token was modified or corrupted - Using wrong signing key - Token generated with different secret **Resolution:** ```bash # Validate token structure cloudflow auth validate --token $CF_ACCESS_TOKEN # Generate new token cloudflow auth login --username <user> --password <pass> # For service accounts cloudflow auth service-account --create --name <sa-name> --scopes \"workflow:read,workflow:write\" ``` **3.",
              "score": null
            },
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_4",
              "content": "```python import jwt import time from cryptography.hazmat.primitives import serialization from cryptography.hazmat.backends import default_backend # Load your private key with open('private_key.pem', 'rb') as key_file: private_key = serialization.load_pem_private_key( key_file.read(), password=None, backend=default_backend() ) # Create JWT payload payload = { 'iss': 'your-app-id', 'sub': 'user-12345', 'aud': 'https://api.cloudflow.io', 'exp': int(time.time()) + 3600, 'iat': int(time.time()), 'scope': 'workflows:read workflows:write' } # Generate token token = jwt.encode(payload, private_key, algorithm='RS256') # Use in API request headers = {'Authorization': f'Bearer {token}'} ``` **Token Expiration:** All",
              "score": null
            },
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_0",
              "content": "# CloudFlow API Reference Version 2.1.0 | Last Updated: January 2026 ## Overview The CloudFlow API is a RESTful service that enables developers to programmatically manage cloud workflows, data pipelines, and automation tasks. This documentation provides comprehensive details on authentication, endpoints, request/response formats, error handling, and best practices. **Base URL:** `https://api.cloudflow.io/v2` **API Status:** https://status.cloudflow.io ## Authentication CloudFlow supports three authentication methods to suit different use cases and security requirements.",
              "score": null
            },
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_1",
              "content": "### API Keys API keys provide simple authentication for server-to-server communication. Include your API key in the request header: ```bash curl -H \"X-API-Key: cf_live_a1b2c3d4e5f6g7h8i9j0\" \\ https://api.cloudflow.io/v2/workflows ``` **Security Notes:** - Never expose API keys in client-side code - Rotate keys every 90 days - Use separate keys for development and production environments ### OAuth 2.0 OAuth 2.0 is recommended for applications that access CloudFlow on behalf of users.",
              "score": null
            }
          ],
          "latency_ms": 3.0171229991537984,
          "query_id": "realistic_012",
          "dimension": "synonym",
          "query": "Which signing algorithm does CloudFlow use for JWTs?"
        },
        {
          "key_facts": [
            "RS256",
            "RS256 algorithm",
            "RS256 signing algorithm"
          ],
          "found_facts": [
            "RS256"
          ],
          "missed_facts": [
            "RS256 algorithm",
            "RS256 signing algorithm"
          ],
          "coverage": 0.3333333333333333,
          "expected_docs": [
            "api_reference"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_4",
              "content": "Invalid JWT Signature** Error: `JWT signature verification failed` **Causes:** - Token was modified or corrupted - Using wrong signing key - Token generated with different secret **Resolution:** ```bash # Validate token structure cloudflow auth validate --token $CF_ACCESS_TOKEN # Generate new token cloudflow auth login --username <user> --password <pass> # For service accounts cloudflow auth service-account --create --name <sa-name> --scopes \"workflow:read,workflow:write\" ``` **3.",
              "score": null
            },
            {
              "doc_id": "user_guide",
              "chunk_id": "user_guide_fix_40",
              "content": "Validate Input Data Always validate trigger data before processing: ```yaml - id: validate_input action: javascript code: | const required_fields = ['email', 'name', 'order_id']; for (const field of required_fields) { if (!input[field]) { throw new Error(`Missing required field: ${field}`); } } // Validate email format const email_regex = /^[^\\s@]+@[^\\s@]+\\.[^\\s@]+$/; if (!email_regex.test(input.email)) { throw new Error('Invalid email format'); } return { validated: true }; ``` ### 5.",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_5",
              "content": "Clock Skew Issues** JWT validation fails when system clocks are out of sync (tolerance: \u00b1300 seconds). **Diagnosis:** ```bash # Check system time timedatectl status # Compare with NTP server ntpdate -q pool.ntp.org # Check JWT issued time vs current time jwt_iat=$(echo $CF_ACCESS_TOKEN | cut -d'.' -f2 | base64 -d | jq '.iat') current_time=$(date +%s) skew=$((current_time - jwt_iat)) echo \"Clock skew: $skew seconds\" ``` **Resolution:** ```bash # Sync with NTP server sudo ntpdate -s pool.ntp.org #",
              "score": null
            },
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_4",
              "content": "```python import jwt import time from cryptography.hazmat.primitives import serialization from cryptography.hazmat.backends import default_backend # Load your private key with open('private_key.pem', 'rb') as key_file: private_key = serialization.load_pem_private_key( key_file.read(), password=None, backend=default_backend() ) # Create JWT payload payload = { 'iss': 'your-app-id', 'sub': 'user-12345', 'aud': 'https://api.cloudflow.io', 'exp': int(time.time()) + 3600, 'iat': int(time.time()), 'scope': 'workflows:read workflows:write' } # Generate token token = jwt.encode(payload, private_key, algorithm='RS256') # Use in API request headers = {'Authorization': f'Bearer {token}'} ``` **Token Expiration:** All",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_41",
              "content": "Issues #### Authentication Failures ```bash # Find all authentication errors kubectl logs -n cloudflow deployment/cloudflow-api | \\ grep -i \"authentication\\|401\\|unauthorized\" | \\ tail -n 50 # Find JWT validation failures kubectl logs -n cloudflow deployment/cloudflow-api | \\ grep -E \"JWT|token.*invalid|signature.*failed\" # Find clock skew issues kubectl logs -n cloudflow deployment/cloudflow-api | \\ grep -i \"clock skew\\|time.*sync\\|nbf\\|exp\" ``` #### Database Errors ```bash # Find database connection errors kubectl logs -n cloudflow deployment/cloudflow-api | \\ grep -E",
              "score": null
            }
          ],
          "latency_ms": 3.3977620005316567,
          "query_id": "realistic_012",
          "dimension": "problem",
          "query": "My JWT validation is failing with algorithm mismatch"
        },
        {
          "key_facts": [
            "RS256",
            "RS256 algorithm",
            "RS256 signing algorithm"
          ],
          "found_facts": [
            "RS256",
            "RS256 signing algorithm"
          ],
          "missed_facts": [
            "RS256 algorithm"
          ],
          "coverage": 0.6666666666666666,
          "expected_docs": [
            "api_reference"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_4",
              "content": "```python import jwt import time from cryptography.hazmat.primitives import serialization from cryptography.hazmat.backends import default_backend # Load your private key with open('private_key.pem', 'rb') as key_file: private_key = serialization.load_pem_private_key( key_file.read(), password=None, backend=default_backend() ) # Create JWT payload payload = { 'iss': 'your-app-id', 'sub': 'user-12345', 'aud': 'https://api.cloudflow.io', 'exp': int(time.time()) + 3600, 'iat': int(time.time()), 'scope': 'workflows:read workflows:write' } # Generate token token = jwt.encode(payload, private_key, algorithm='RS256') # Use in API request headers = {'Authorization': f'Bearer {token}'} ``` **Token Expiration:** All",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_19",
              "content": "SMS delivery: < 3 seconds (P99) - Throughput: 10,000 notifications per minute --- ## Data Flow Architecture ### Synchronous Request Flow Client requests follow a synchronous path through the API Gateway to backend services: ``` 1. Client \u2192 Load Balancer (TLS termination) 2. Load Balancer \u2192 API Gateway (HTTP/2) 3. API Gateway \u2192 Auth Service (JWT validation via gRPC) 4. API Gateway \u2192 Target Service (HTTP/REST or gRPC) 5.",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_26",
              "content": "5 minutes + AOF - Eviction policy: allkeys-lru **Cache Usage Patterns**: 1. **Session Storage**: - Key pattern: `session:{user_id}` - TTL: 15 minutes (aligned with JWT expiry) - Data: User session state, preferences - Invalidation: On logout or password change 2. **Rate Limiting Counters**: - Key pattern: `ratelimit:{api_key}:{window}` - TTL: 60 seconds (sliding window) - Data: Request count per window - Algorithm: Token bucket with Redis INCR 3.",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_42",
              "content": "Rules**: - Security groups: Deny all by default, explicit allow rules - Ingress: Only ALB can reach API Gateway (port 8080) - Egress: Services can only reach specific dependencies - Database access: Limited to application subnets only ### Authentication & Authorization **JWT Token Validation**: - Algorithm: RS256 (asymmetric signing) - Key rotation: Every 30 days with 7-day overlap period - Public key distribution: JWKS endpoint cached in Redis - Validation: Signature, expiry, issuer, audience claims",
              "score": null
            },
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_3",
              "content": "\"client_id=YOUR_CLIENT_ID\" \\ -d \"client_secret=YOUR_CLIENT_SECRET\" \\ -d \"redirect_uri=https://yourapp.com/callback\" ``` ### JWT Tokens For advanced use cases, CloudFlow supports JSON Web Tokens (JWT) with RS256 signing algorithm. JWTs must include the following claims: - `iss` (issuer): Your application identifier - `sub` (subject): User or service account ID - `aud` (audience): `https://api.cloudflow.io` - `exp` (expiration): Unix timestamp (max 3600 seconds from `iat`) - `iat` (issued at): Unix timestamp - `scope`: Space-separated list of requested scopes Example JWT header:",
              "score": null
            }
          ],
          "latency_ms": 3.4406619997753296,
          "query_id": "realistic_012",
          "dimension": "casual",
          "query": "jwt algorithm"
        },
        {
          "key_facts": [
            "RS256",
            "RS256 algorithm",
            "RS256 signing algorithm"
          ],
          "found_facts": [
            "RS256"
          ],
          "missed_facts": [
            "RS256 algorithm",
            "RS256 signing algorithm"
          ],
          "coverage": 0.3333333333333333,
          "expected_docs": [
            "api_reference"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_4",
              "content": "```python import jwt import time from cryptography.hazmat.primitives import serialization from cryptography.hazmat.backends import default_backend # Load your private key with open('private_key.pem', 'rb') as key_file: private_key = serialization.load_pem_private_key( key_file.read(), password=None, backend=default_backend() ) # Create JWT payload payload = { 'iss': 'your-app-id', 'sub': 'user-12345', 'aud': 'https://api.cloudflow.io', 'exp': int(time.time()) + 3600, 'iat': int(time.time()), 'scope': 'workflows:read workflows:write' } # Generate token token = jwt.encode(payload, private_key, algorithm='RS256') # Use in API request headers = {'Authorization': f'Bearer {token}'} ``` **Token Expiration:** All",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_42",
              "content": "Rules**: - Security groups: Deny all by default, explicit allow rules - Ingress: Only ALB can reach API Gateway (port 8080) - Egress: Services can only reach specific dependencies - Database access: Limited to application subnets only ### Authentication & Authorization **JWT Token Validation**: - Algorithm: RS256 (asymmetric signing) - Key rotation: Every 30 days with 7-day overlap period - Public key distribution: JWKS endpoint cached in Redis - Validation: Signature, expiry, issuer, audience claims",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_19",
              "content": "SMS delivery: < 3 seconds (P99) - Throughput: 10,000 notifications per minute --- ## Data Flow Architecture ### Synchronous Request Flow Client requests follow a synchronous path through the API Gateway to backend services: ``` 1. Client \u2192 Load Balancer (TLS termination) 2. Load Balancer \u2192 API Gateway (HTTP/2) 3. API Gateway \u2192 Auth Service (JWT validation via gRPC) 4. API Gateway \u2192 Target Service (HTTP/REST or gRPC) 5.",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_5",
              "content": "providing authentication, rate limiting, request routing, and protocol translation. **Technology**: Node.js with Express.js framework **Replicas**: 12 pods (production), auto-scaling 8-20 based on CPU **Resource Allocation**: 2 vCPU, 4GB RAM per pod **Key Responsibilities**: - JWT token validation (delegated to Auth Service for initial validation) - Rate limiting: 1000 requests per minute per API key (sliding window) - Request/response transformation and validation using JSON Schema - Routing to downstream services based on URL path patterns -",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_26",
              "content": "5 minutes + AOF - Eviction policy: allkeys-lru **Cache Usage Patterns**: 1. **Session Storage**: - Key pattern: `session:{user_id}` - TTL: 15 minutes (aligned with JWT expiry) - Data: User session state, preferences - Invalidation: On logout or password change 2. **Rate Limiting Counters**: - Key pattern: `ratelimit:{api_key}:{window}` - TTL: 60 seconds (sliding window) - Data: Request count per window - Algorithm: Token bucket with Redis INCR 3.",
              "score": null
            }
          ],
          "latency_ms": 3.3370889996149344,
          "query_id": "realistic_012",
          "dimension": "contextual",
          "query": "I'm implementing JWT verification, what algorithm should I expect?"
        },
        {
          "key_facts": [
            "RS256",
            "RS256 algorithm",
            "RS256 signing algorithm"
          ],
          "found_facts": [
            "RS256",
            "RS256 signing algorithm"
          ],
          "missed_facts": [
            "RS256 algorithm"
          ],
          "coverage": 0.6666666666666666,
          "expected_docs": [
            "api_reference"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_4",
              "content": "Invalid JWT Signature** Error: `JWT signature verification failed` **Causes:** - Token was modified or corrupted - Using wrong signing key - Token generated with different secret **Resolution:** ```bash # Validate token structure cloudflow auth validate --token $CF_ACCESS_TOKEN # Generate new token cloudflow auth login --username <user> --password <pass> # For service accounts cloudflow auth service-account --create --name <sa-name> --scopes \"workflow:read,workflow:write\" ``` **3.",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_8",
              "content": "Redis-backed storage - API key generation and validation for service accounts - MFA enforcement for administrative operations **Authentication Flow**: ``` Client Request \u2192 API Gateway \u2192 Auth Service \u2502 \u251c\u2500 Validate credentials \u251c\u2500 Check MFA if required \u251c\u2500 Generate JWT (15min expiry) \u251c\u2500 Generate refresh token (7 days) \u2514\u2500 Store session in Redis ``` **Token Structure**: - Access Token: JWT with 15-minute expiry - Refresh Token: Opaque token with 7-day expiry, stored in PostgreSQL -",
              "score": null
            },
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_4",
              "content": "```python import jwt import time from cryptography.hazmat.primitives import serialization from cryptography.hazmat.backends import default_backend # Load your private key with open('private_key.pem', 'rb') as key_file: private_key = serialization.load_pem_private_key( key_file.read(), password=None, backend=default_backend() ) # Create JWT payload payload = { 'iss': 'your-app-id', 'sub': 'user-12345', 'aud': 'https://api.cloudflow.io', 'exp': int(time.time()) + 3600, 'iat': int(time.time()), 'scope': 'workflows:read workflows:write' } # Generate token token = jwt.encode(payload, private_key, algorithm='RS256') # Use in API request headers = {'Authorization': f'Bearer {token}'} ``` **Token Expiration:** All",
              "score": null
            },
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_3",
              "content": "\"client_id=YOUR_CLIENT_ID\" \\ -d \"client_secret=YOUR_CLIENT_SECRET\" \\ -d \"redirect_uri=https://yourapp.com/callback\" ``` ### JWT Tokens For advanced use cases, CloudFlow supports JSON Web Tokens (JWT) with RS256 signing algorithm. JWTs must include the following claims: - `iss` (issuer): Your application identifier - `sub` (subject): User or service account ID - `aud` (audience): `https://api.cloudflow.io` - `exp` (expiration): Unix timestamp (max 3600 seconds from `iat`) - `iat` (issued at): Unix timestamp - `scope`: Space-separated list of requested scopes Example JWT header:",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_2",
              "content": "Unauthorized` - Error message: `Authentication credentials were not provided or are invalid` - Frontend displays \"Session expired\" message #### Common Causes **1. Token Expiration** CloudFlow access tokens expire after 3600 seconds (1 hour) by default. Refresh tokens are valid for 30 days.",
              "score": null
            }
          ],
          "latency_ms": 3.1696879996161442,
          "query_id": "realistic_012",
          "dimension": "negation",
          "query": "Why doesn't HS256 work for token validation?"
        },
        {
          "key_facts": [
            "TTL: 1 hour",
            "Workflow Definitions: TTL: 1 hour"
          ],
          "found_facts": [
            "TTL: 1 hour"
          ],
          "missed_facts": [
            "Workflow Definitions: TTL: 1 hour"
          ],
          "coverage": 0.5,
          "expected_docs": [
            "architecture_overview"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_12",
              "content": "cloudflow config set cache.workspace.ttl 3600 # Configure Redis cache backend cloudflow config set cache.backend redis cloudflow config set cache.redis.host redis.cloudflow.svc.cluster.local cloudflow config set cache.redis.port 6379 ``` ### High API Latency #### Latency Breakdown Analysis ```bash # Generate latency report cloudflow metrics latency-report --endpoint \"/api/v1/workflows\" --last 24h # Sample output: # Endpoint: POST /api/v1/workflows/execute # P50: 245ms | P95: 1823ms | P99: 4521ms # Breakdown: # - Auth: 45ms (18%) # - DB Query: 156ms (64%)",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_36",
              "content": "--- ## Caching Strategy ### Cache-Aside Pattern (Lazy Loading) Primary caching pattern used across all services: ``` 1. Application checks cache (Redis GET) 2. If HIT \u2192 Return cached data 3. If MISS: a. Query database (PostgreSQL) b. Store result in cache with TTL c.",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_37",
              "content": "Return data to application ``` **Implementation Example** (Workflow Definition): ```javascript async function getWorkflowDefinition(workflowId) { const cacheKey = `workflow:def:${workflowId}`; // Step 1: Check cache let workflow = await redis.get(cacheKey); if (workflow) { metrics.increment('cache.hit.workflow_def'); return JSON.parse(workflow); } // Step 2: Cache miss - query database metrics.increment('cache.miss.workflow_def'); workflow = await db.query( 'SELECT * FROM workflows WHERE id = $1', [workflowId] ); // Step 3: Store in cache (1 hour TTL) await redis.setex(cacheKey, 3600, JSON.stringify(workflow)); return workflow; } ```",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_27",
              "content": "**Compiled Templates**: - Key pattern: `template:{template_id}:{version}` - TTL: 1 hour - Data: Compiled Handlebars template - Invalidation: On template update 4. **Workflow Definitions**: - Key pattern: `workflow:def:{workflow_id}` - TTL: 1 hour - Data: Parsed workflow JSON - Invalidation: On workflow update or manual flush 5.",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_40",
              "content": "**Cache Warming**: - Scheduled job runs every hour to pre-populate frequently accessed data - Targets: Top 1000 workflows, system configuration, active user profiles - Reduces cache miss rate during traffic spikes ### Multi-Level Caching Application implements L1 (in-memory) and L2 (Redis) caching: ``` Request \u2192 L1 Cache (Node.js Map, 1000 entry LRU) \u2502 MISS \u2502 \u25bc L2 Cache (Redis, distributed) \u2502 MISS \u2502 \u25bc Database (PostgreSQL) ``` L1 cache reduces Redis network calls by 40%",
              "score": null
            }
          ],
          "latency_ms": 3.2151619998330716,
          "query_id": "realistic_013",
          "dimension": "original",
          "query": "What is the Redis cache TTL for workflow definitions?"
        },
        {
          "key_facts": [
            "TTL: 1 hour",
            "Workflow Definitions: TTL: 1 hour"
          ],
          "found_facts": [
            "TTL: 1 hour"
          ],
          "missed_facts": [
            "Workflow Definitions: TTL: 1 hour"
          ],
          "coverage": 0.5,
          "expected_docs": [
            "architecture_overview"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_27",
              "content": "**Compiled Templates**: - Key pattern: `template:{template_id}:{version}` - TTL: 1 hour - Data: Compiled Handlebars template - Invalidation: On template update 4. **Workflow Definitions**: - Key pattern: `workflow:def:{workflow_id}` - TTL: 1 hour - Data: Parsed workflow JSON - Invalidation: On workflow update or manual flush 5.",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_15",
              "content": "**1. Workflow Context Accumulation** Large workflow executions may accumulate state in memory. **Solution:** ```bash # Configure context cleanup cloudflow config set workflow.context.max_size_mb 100 cloudflow config set workflow.context.cleanup_threshold 0.8 # Enable context persistence to disk cloudflow config set workflow.context.persistence.enabled true cloudflow config set workflow.context.persistence.backend s3 ``` **2.",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_47",
              "content": "Operations** (P99 latency): - `GET /workflows/{id}`: < 50ms (cache hit), < 150ms (cache miss) - `POST /workflows`: < 200ms (includes validation and database write) - `POST /workflows/{id}/execute`: < 100ms (async, returns execution ID) - `GET /workflows/{id}/history`: < 300ms (paginated, 50 records per page) **Workflow Execution** (P99 latency): - Simple workflow (< 5 steps): < 2 seconds - Medium workflow (5-15 steps): < 5 seconds - Complex workflow (> 15 steps): < 15 seconds - Parallel",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_12",
              "content": "cloudflow config set cache.workspace.ttl 3600 # Configure Redis cache backend cloudflow config set cache.backend redis cloudflow config set cache.redis.host redis.cloudflow.svc.cluster.local cloudflow config set cache.redis.port 6379 ``` ### High API Latency #### Latency Breakdown Analysis ```bash # Generate latency report cloudflow metrics latency-report --endpoint \"/api/v1/workflows\" --last 24h # Sample output: # Endpoint: POST /api/v1/workflows/execute # P50: 245ms | P95: 1823ms | P99: 4521ms # Breakdown: # - Auth: 45ms (18%) # - DB Query: 156ms (64%)",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_63",
              "content": "**6-Month Projection** (Jul 2026): - Daily workflow executions: 4M (+60%) - Active users: 225,000 (+50%) - API requests per day: 80M (+60%) - Database size: 2TB (+67%) - Required scaling: +30% compute, +50% storage ### Contact Information - **Architecture Team**: architecture@cloudflow.internal - **On-Call Engineer**: PagerDuty escalation - **Security Team**: security@cloudflow.internal - **Documentation**: https://wiki.cloudflow.internal/architecture --- **Document Revision History**: - v2.3.1 (Jan 15, 2026): Updated performance metrics, added DR test results - v2.3.0 (Dec 1, 2025): Added",
              "score": null
            }
          ],
          "latency_ms": 3.1223299993143883,
          "query_id": "realistic_013",
          "dimension": "synonym",
          "query": "How long are workflow definitions cached?"
        },
        {
          "key_facts": [
            "TTL: 1 hour",
            "Workflow Definitions: TTL: 1 hour"
          ],
          "found_facts": [],
          "missed_facts": [
            "TTL: 1 hour",
            "Workflow Definitions: TTL: 1 hour"
          ],
          "coverage": 0.0,
          "expected_docs": [
            "architecture_overview"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_47",
              "content": "Operations** (P99 latency): - `GET /workflows/{id}`: < 50ms (cache hit), < 150ms (cache miss) - `POST /workflows`: < 200ms (includes validation and database write) - `POST /workflows/{id}/execute`: < 100ms (async, returns execution ID) - `GET /workflows/{id}/history`: < 300ms (paginated, 50 records per page) **Workflow Execution** (P99 latency): - Simple workflow (< 5 steps): < 2 seconds - Medium workflow (5-15 steps): < 5 seconds - Complex workflow (> 15 steps): < 15 seconds - Parallel",
              "score": null
            },
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_13",
              "content": "\"transform\", \"script_id\": \"scr_transform_v3\" } ], \"metrics\": { \"total_runs\": 1247, \"success_rate\": 98.7, \"avg_duration_ms\": 3420, \"last_error\": null }, \"created_at\": \"2026-01-15T10:30:00Z\", \"updated_at\": \"2026-01-23T14:22:00Z\" } ``` #### Update Workflow Modify an existing workflow's configuration. **Endpoint:** `PATCH /workflows/{workflow_id}` **Example Request:** ```bash curl -X PATCH https://api.cloudflow.io/v2/workflows/wf_8x7k2m9p \\ -H \"X-API-Key: YOUR_API_KEY\" \\ -H \"Content-Type: application/json\" \\ -d '{\"status\": \"paused\"}' ``` **Example Response (200 OK):** ```json { \"id\": \"wf_8x7k2m9p\", \"status\": \"paused\", \"updated_at\": \"2026-01-24T10:30:00Z\" } ``` #### Delete Workflow Permanently delete a workflow.",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_29",
              "content": "update wf_9k2n4m8p1q \\ --step data_export \\ --retry-enabled false ``` #### Monitoring Retries ```bash # List failed executions with retry information cloudflow workflows executions list \\ --status FAILED \\ --show-retries \\ --last 7d # Get retry history for specific execution cloudflow workflows executions retries exec_7h3j6k9m2n # Output: # Execution: exec_7h3j6k9m2n # Attempt 1: FAILED - NetworkError: Connection refused (delay: 0ms) # Attempt 2: FAILED - NetworkError: Connection timeout (delay: 1000ms) # Attempt 3: FAILED - NetworkError:",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_10",
              "content": "Review Query Execution Plans** ```sql -- Connect to CloudFlow database cloudflow db connect --readonly -- Explain slow query EXPLAIN ANALYZE SELECT w.*, e.status, e.error_message FROM workflows w LEFT JOIN executions e ON w.id = e.workflow_id WHERE w.workspace_id = 'ws_abc123' AND e.created_at > NOW() - INTERVAL '7 days' ORDER BY e.created_at DESC; -- Check for missing indexes SELECT schemaname, tablename, indexname, indexdef FROM pg_indexes WHERE tablename IN ('workflows', 'executions', 'workflow_steps') ORDER BY tablename, indexname; ``` **3.",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_21",
              "content": "Consumer groups subscribe to topics 4. Consumers process events with at-least-once delivery 5. State updates written to PostgreSQL 6. Success/failure events published back to Kafka ``` **Event Schema**: ```json { \"event_id\": \"uuid-v4\", \"event_type\": \"workflow.execution.completed\", \"timestamp\": \"2026-01-15T10:30:00.000Z\", \"correlation_id\": \"request-trace-id\", \"payload\": { \"workflow_id\": \"wf-12345\", \"execution_id\": \"exec-67890\", \"status\": \"COMPLETED\", \"duration_ms\": 4230 }, \"metadata\": { \"source_service\": \"workflow-engine\", \"schema_version\": \"1.0\" } } ``` ### Inter-Service Communication Services communicate using two primary patterns: **Synchronous (gRPC)**: - Auth Service validation calls from",
              "score": null
            }
          ],
          "latency_ms": 3.124452998235938,
          "query_id": "realistic_013",
          "dimension": "problem",
          "query": "My workflow updates aren't reflecting immediately"
        },
        {
          "key_facts": [
            "TTL: 1 hour",
            "Workflow Definitions: TTL: 1 hour"
          ],
          "found_facts": [
            "TTL: 1 hour"
          ],
          "missed_facts": [
            "Workflow Definitions: TTL: 1 hour"
          ],
          "coverage": 0.5,
          "expected_docs": [
            "architecture_overview"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_36",
              "content": "--- ## Caching Strategy ### Cache-Aside Pattern (Lazy Loading) Primary caching pattern used across all services: ``` 1. Application checks cache (Redis GET) 2. If HIT \u2192 Return cached data 3. If MISS: a. Query database (PostgreSQL) b. Store result in cache with TTL c.",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_47",
              "content": "Operations** (P99 latency): - `GET /workflows/{id}`: < 50ms (cache hit), < 150ms (cache miss) - `POST /workflows`: < 200ms (includes validation and database write) - `POST /workflows/{id}/execute`: < 100ms (async, returns execution ID) - `GET /workflows/{id}/history`: < 300ms (paginated, 50 records per page) **Workflow Execution** (P99 latency): - Simple workflow (< 5 steps): < 2 seconds - Medium workflow (5-15 steps): < 5 seconds - Complex workflow (> 15 steps): < 15 seconds - Parallel",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_38",
              "content": "### Write-Through Pattern Used for critical data where cache consistency is paramount: ``` 1. Application writes to database 2. Database transaction commits 3. Application updates cache 4. Return success to client ``` Applied to: User profiles, authentication sessions, system configuration.",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_27",
              "content": "**Compiled Templates**: - Key pattern: `template:{template_id}:{version}` - TTL: 1 hour - Data: Compiled Handlebars template - Invalidation: On template update 4. **Workflow Definitions**: - Key pattern: `workflow:def:{workflow_id}` - TTL: 1 hour - Data: Parsed workflow JSON - Invalidation: On workflow update or manual flush 5.",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_40",
              "content": "**Cache Warming**: - Scheduled job runs every hour to pre-populate frequently accessed data - Targets: Top 1000 workflows, system configuration, active user profiles - Reduces cache miss rate during traffic spikes ### Multi-Level Caching Application implements L1 (in-memory) and L2 (Redis) caching: ``` Request \u2192 L1 Cache (Node.js Map, 1000 entry LRU) \u2502 MISS \u2502 \u25bc L2 Cache (Redis, distributed) \u2502 MISS \u2502 \u25bc Database (PostgreSQL) ``` L1 cache reduces Redis network calls by 40%",
              "score": null
            }
          ],
          "latency_ms": 3.3182540009875083,
          "query_id": "realistic_013",
          "dimension": "casual",
          "query": "cache ttl workflows"
        },
        {
          "key_facts": [
            "TTL: 1 hour",
            "Workflow Definitions: TTL: 1 hour"
          ],
          "found_facts": [],
          "missed_facts": [
            "TTL: 1 hour",
            "Workflow Definitions: TTL: 1 hour"
          ],
          "coverage": 0.0,
          "expected_docs": [
            "architecture_overview"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_12",
              "content": "cloudflow config set cache.workspace.ttl 3600 # Configure Redis cache backend cloudflow config set cache.backend redis cloudflow config set cache.redis.host redis.cloudflow.svc.cluster.local cloudflow config set cache.redis.port 6379 ``` ### High API Latency #### Latency Breakdown Analysis ```bash # Generate latency report cloudflow metrics latency-report --endpoint \"/api/v1/workflows\" --last 24h # Sample output: # Endpoint: POST /api/v1/workflows/execute # P50: 245ms | P95: 1823ms | P99: 4521ms # Breakdown: # - Auth: 45ms (18%) # - DB Query: 156ms (64%)",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_37",
              "content": "Return data to application ``` **Implementation Example** (Workflow Definition): ```javascript async function getWorkflowDefinition(workflowId) { const cacheKey = `workflow:def:${workflowId}`; // Step 1: Check cache let workflow = await redis.get(cacheKey); if (workflow) { metrics.increment('cache.hit.workflow_def'); return JSON.parse(workflow); } // Step 2: Cache miss - query database metrics.increment('cache.miss.workflow_def'); workflow = await db.query( 'SELECT * FROM workflows WHERE id = $1', [workflowId] ); // Step 3: Store in cache (1 hour TTL) await redis.setex(cacheKey, 3600, JSON.stringify(workflow)); return workflow; } ```",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_39",
              "content": "### Cache Invalidation Strategies **Time-Based Expiration (TTL)**: - Short-lived data: 5-15 minutes (session tokens, rate limit counters) - Medium-lived data: 1 hour (workflow definitions, templates) - Long-lived data: 24 hours (static configuration) **Event-Based Invalidation**: ``` Database Update Event \u2192 Kafka (cache.invalidation topic) \u2502 \u25bc All service instances consume event \u2502 \u25bc Redis DEL for affected keys ``` **Pattern-Based Invalidation**: - Use Redis SCAN + DEL for wildcard patterns - Example: Invalidate all user caches: `user:*:{user_id}`",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_40",
              "content": "**Cache Warming**: - Scheduled job runs every hour to pre-populate frequently accessed data - Targets: Top 1000 workflows, system configuration, active user profiles - Reduces cache miss rate during traffic spikes ### Multi-Level Caching Application implements L1 (in-memory) and L2 (Redis) caching: ``` Request \u2192 L1 Cache (Node.js Map, 1000 entry LRU) \u2502 MISS \u2502 \u25bc L2 Cache (Redis, distributed) \u2502 MISS \u2502 \u25bc Database (PostgreSQL) ``` L1 cache reduces Redis network calls by 40%",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_47",
              "content": "Operations** (P99 latency): - `GET /workflows/{id}`: < 50ms (cache hit), < 150ms (cache miss) - `POST /workflows`: < 200ms (includes validation and database write) - `POST /workflows/{id}/execute`: < 100ms (async, returns execution ID) - `GET /workflows/{id}/history`: < 300ms (paginated, 50 records per page) **Workflow Execution** (P99 latency): - Simple workflow (< 5 steps): < 2 seconds - Medium workflow (5-15 steps): < 5 seconds - Complex workflow (> 15 steps): < 15 seconds - Parallel",
              "score": null
            }
          ],
          "latency_ms": 3.31505800022569,
          "query_id": "realistic_013",
          "dimension": "contextual",
          "query": "After updating a workflow, how long until the cache expires?"
        },
        {
          "key_facts": [
            "TTL: 1 hour",
            "Workflow Definitions: TTL: 1 hour"
          ],
          "found_facts": [],
          "missed_facts": [
            "TTL: 1 hour",
            "Workflow Definitions: TTL: 1 hour"
          ],
          "coverage": 0.0,
          "expected_docs": [
            "architecture_overview"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "user_guide",
              "chunk_id": "user_guide_fix_26",
              "content": "timezone identifiers, including: - `America/New_York` (Eastern Time) - `America/Chicago` (Central Time) - `America/Los_Angeles` (Pacific Time) - `Europe/London` (GMT/BST) - `Asia/Tokyo` (Japan Standard Time) - `Australia/Sydney` (Australian Eastern Time) **Daylight Saving Time:** When using timezone configuration, CloudFlow automatically handles DST transitions. A workflow scheduled for 9:00 AM will run at 9:00 AM local time regardless of DST changes. ### Schedule Management **Creating a Schedule:** 1. Open your workflow in the editor 2.",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_49",
              "content": "errors for > 5 minutes - Database completely unavailable - Authentication system down - Data loss or corruption - **Response Time:** Immediate (< 15 minutes) - **Escalation:** Page on-call engineer immediately #### SEV-2: High (P2) - **Definition:** Major functionality impaired affecting multiple users - **Examples:** - Workflow execution success rate < 90% - Significant performance degradation (p95 latency > 10s) - Rate limiting affecting large customer segment - **Response Time:** < 1 hour - **Escalation:**",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_40",
              "content": "**Cache Warming**: - Scheduled job runs every hour to pre-populate frequently accessed data - Targets: Top 1000 workflows, system configuration, active user profiles - Reduces cache miss rate during traffic spikes ### Multi-Level Caching Application implements L1 (in-memory) and L2 (Redis) caching: ``` Request \u2192 L1 Cache (Node.js Map, 1000 entry LRU) \u2502 MISS \u2502 \u25bc L2 Cache (Redis, distributed) \u2502 MISS \u2502 \u25bc Database (PostgreSQL) ``` L1 cache reduces Redis network calls by 40%",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_5",
              "content": "Clock Skew Issues** JWT validation fails when system clocks are out of sync (tolerance: \u00b1300 seconds). **Diagnosis:** ```bash # Check system time timedatectl status # Compare with NTP server ntpdate -q pool.ntp.org # Check JWT issued time vs current time jwt_iat=$(echo $CF_ACCESS_TOKEN | cut -d'.' -f2 | base64 -d | jq '.iat') current_time=$(date +%s) skew=$((current_time - jwt_iat)) echo \"Clock skew: $skew seconds\" ``` **Resolution:** ```bash # Sync with NTP server sudo ntpdate -s pool.ntp.org #",
              "score": null
            },
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_37",
              "content": "\\ --values values-production.yaml ``` ### Maintenance Windows Scheduled maintenance occurs during the following windows: - **Primary**: Sunday 02:00-06:00 UTC - **Secondary**: Wednesday 10:00-12:00 UTC All deployments should target these windows to minimize user impact. --- **Document Version**: 2.4.0 **Maintained by**: Platform Engineering Team **Last Review**: January 24, 2026",
              "score": null
            }
          ],
          "latency_ms": 3.1126210014917888,
          "query_id": "realistic_013",
          "dimension": "negation",
          "query": "Why are changes taking an hour to appear?"
        },
        {
          "key_facts": [
            "Prometheus",
            "Grafana",
            "Jaeger for distributed tracing"
          ],
          "found_facts": [
            "Prometheus"
          ],
          "missed_facts": [
            "Grafana",
            "Jaeger for distributed tracing"
          ],
          "coverage": 0.3333333333333333,
          "expected_docs": [
            "architecture_overview",
            "deployment_guide"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_0",
              "content": "# CloudFlow Platform - System Architecture Overview **Document Version:** 2.3.1 **Last Updated:** January 15, 2026 **Owner:** Platform Architecture Team **Status:** Production ## Executive Summary CloudFlow is a distributed, cloud-native workflow automation platform designed to orchestrate complex business processes at scale. The platform processes over 2.5 million workflow executions daily with an average P99 latency of 180ms for API operations and 4.2 seconds for workflow execution.",
              "score": null
            },
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_19",
              "content": "\\ --namespace monitoring \\ --create-namespace \\ --set prometheus.prometheusSpec.serviceMonitorSelectorNilUsesHelmValues=false \\ --set prometheus.prometheusSpec.retention=30d \\ --set prometheus.prometheusSpec.storageSpec.volumeClaimTemplate.spec.resources.requests.storage=100Gi ``` CloudFlow exposes Prometheus metrics on port 9090 at the `/metrics` endpoint. The metrics include: - **HTTP Metrics**: Request rate, latency, error rate - **Database Metrics**: Connection pool status, query duration - **Worker Metrics**: Job queue size, processing time, failure rate - **System Metrics**: CPU usage, memory usage, heap statistics Example metrics exposed: ``` # HELP cloudflow_http_requests_total Total number of HTTP",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_57",
              "content": "For urgent issues, use emergency escalation procedures **Remember:** Always capture logs, metrics, and reproduction steps before escalating! --- *Last updated: January 24, 2026* *Document version: 3.2.1* *Feedback: docs-feedback@cloudflow.io*",
              "score": null
            },
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_0",
              "content": "# CloudFlow Platform - Deployment and Operations Guide **Version:** 2.4.0 **Last Updated:** January 2026 **Target Environment:** Production (AWS EKS) ## Table of Contents 1. [Overview](#overview) 2. [Prerequisites](#prerequisites) 3. [Infrastructure Setup](#infrastructure-setup) 4. [Kubernetes Deployment](#kubernetes-deployment) 5. [Database Configuration](#database-configuration) 6. [Monitoring and Observability](#monitoring-and-observability) 7. [Backup and Disaster Recovery](#backup-and-disaster-recovery) 8. [Scaling and Performance](#scaling-and-performance) 9. [Security and Compliance](#security-and-compliance) 10. [Troubleshooting](#troubleshooting) --- ## Overview CloudFlow is a cloud-native workflow orchestration platform designed for high-availability production environments.",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_63",
              "content": "**6-Month Projection** (Jul 2026): - Daily workflow executions: 4M (+60%) - Active users: 225,000 (+50%) - API requests per day: 80M (+60%) - Database size: 2TB (+67%) - Required scaling: +30% compute, +50% storage ### Contact Information - **Architecture Team**: architecture@cloudflow.internal - **On-Call Engineer**: PagerDuty escalation - **Security Team**: security@cloudflow.internal - **Documentation**: https://wiki.cloudflow.internal/architecture --- **Document Revision History**: - v2.3.1 (Jan 15, 2026): Updated performance metrics, added DR test results - v2.3.0 (Dec 1, 2025): Added",
              "score": null
            }
          ],
          "latency_ms": 3.1079620002856245,
          "query_id": "realistic_014",
          "dimension": "original",
          "query": "What monitoring tools does CloudFlow use?"
        },
        {
          "key_facts": [
            "Prometheus",
            "Grafana",
            "Jaeger for distributed tracing"
          ],
          "found_facts": [],
          "missed_facts": [
            "Prometheus",
            "Grafana",
            "Jaeger for distributed tracing"
          ],
          "coverage": 0.0,
          "expected_docs": [
            "architecture_overview",
            "deployment_guide"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_2",
              "content": "The platform is designed with the following core principles: - **Scalability**: Horizontal scaling for all services with auto-scaling groups - **Resilience**: Circuit breakers, bulkheads, and graceful degradation - **Observability**: Distributed tracing, centralized logging, and comprehensive metrics - **Security**: Zero-trust network architecture with mTLS encryption ### Architecture Diagram (Conceptual) ``` \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 Load Balancer (ALB) \u2502 \u2502 (TLS Termination - 443) \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u25bc \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 API Gateway Layer \u2502 \u2502 (Rate Limiting, Auth, Request",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_7",
              "content": "sustained --- ### Auth Service **Purpose**: Centralized authentication and authorization service handling user identity, token generation, and permission validation. **Technology**: Go with gRPC for internal communication, REST for external **Replicas**: 8 pods (production), auto-scaling 6-12 **Resource Allocation**: 1 vCPU, 2GB RAM per pod **Key Responsibilities**: - User authentication via multiple providers (OAuth2, SAML, local credentials) - JWT token generation and validation (RS256 algorithm) - Role-based access control (RBAC) with fine-grained permissions - Session management with",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_46",
              "content": "Istio service mesh - Database connections: SSL/TLS enforced (sslmode=require) **Data Masking**: - Sensitive fields (email, phone) masked in logs - PII redacted from error messages and stack traces - Audit logs: Full data retention with access controls **Compliance**: - GDPR: User data export and deletion workflows - SOC 2 Type II: Audit logging for all data access - HIPAA: PHI isolation in dedicated tenants with enhanced encryption --- ## Performance Characteristics ### Latency Targets **API",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_10",
              "content": "that executes workflow definitions, manages state transitions, and coordinates task execution across distributed systems. **Technology**: Node.js with TypeScript, Bull queue library **Replicas**: 16 pods (production), auto-scaling 12-24 **Resource Allocation**: 4 vCPU, 8GB RAM per pod **Key Responsibilities**: - Parse and validate workflow definitions (JSON-based DSL) - Execute workflow steps with state machine pattern - Handle retries with exponential backoff (max 3 retries, backoff: 2^n seconds) - Coordinate parallel and sequential task execution - Manage workflow",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_0",
              "content": "# CloudFlow Platform - System Architecture Overview **Document Version:** 2.3.1 **Last Updated:** January 15, 2026 **Owner:** Platform Architecture Team **Status:** Production ## Executive Summary CloudFlow is a distributed, cloud-native workflow automation platform designed to orchestrate complex business processes at scale. The platform processes over 2.5 million workflow executions daily with an average P99 latency of 180ms for API operations and 4.2 seconds for workflow execution.",
              "score": null
            }
          ],
          "latency_ms": 3.036369000255945,
          "query_id": "realistic_014",
          "dimension": "synonym",
          "query": "Which observability platform is integrated?"
        },
        {
          "key_facts": [
            "Prometheus",
            "Grafana",
            "Jaeger for distributed tracing"
          ],
          "found_facts": [
            "Prometheus"
          ],
          "missed_facts": [
            "Grafana",
            "Jaeger for distributed tracing"
          ],
          "coverage": 0.3333333333333333,
          "expected_docs": [
            "architecture_overview",
            "deployment_guide"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_19",
              "content": "\\ --namespace monitoring \\ --create-namespace \\ --set prometheus.prometheusSpec.serviceMonitorSelectorNilUsesHelmValues=false \\ --set prometheus.prometheusSpec.retention=30d \\ --set prometheus.prometheusSpec.storageSpec.volumeClaimTemplate.spec.resources.requests.storage=100Gi ``` CloudFlow exposes Prometheus metrics on port 9090 at the `/metrics` endpoint. The metrics include: - **HTTP Metrics**: Request rate, latency, error rate - **Database Metrics**: Connection pool status, query duration - **Worker Metrics**: Job queue size, processing time, failure rate - **System Metrics**: CPU usage, memory usage, heap statistics Example metrics exposed: ``` # HELP cloudflow_http_requests_total Total number of HTTP",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_57",
              "content": "For urgent issues, use emergency escalation procedures **Remember:** Always capture logs, metrics, and reproduction steps before escalating! --- *Last updated: January 24, 2026* *Document version: 3.2.1* *Feedback: docs-feedback@cloudflow.io*",
              "score": null
            },
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_21",
              "content": "JSON template. Key dashboard panels include: 1. **API Performance**: Request rate, P95/P99 latency, error rate 2. **Resource Usage**: CPU, memory, disk I/O per pod 3. **Database Health**: Connection pool utilization, query performance 4. **Worker Status**: Queue depth, processing rate, job success/failure ratio 5. **System Overview**: Pod status, replica count, autoscaling events ### Logging with CloudWatch CloudFlow logs are automatically shipped to CloudWatch Logs.",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_56",
              "content": "Lessons learned ``` --- ## Additional Resources - **CloudFlow Documentation:** https://docs.cloudflow.io - **API Reference:** https://api-docs.cloudflow.io - **Community Forum:** https://community.cloudflow.io - **GitHub Issues:** https://github.com/cloudflow/platform/issues - **Training Portal:** https://training.cloudflow.io - **Monitoring Dashboard:** https://monitoring.cloudflow.io ### Getting Help If this troubleshooting guide doesn't resolve your issue: 1. Search the knowledge base: `cloudflow kb search \"your issue\"` 2. Check community forum for similar issues 3. Contact support with detailed logs and reproduction steps 4.",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_9",
              "content": "Identify Slow Queries** ```bash # Query CloudFlow metrics cloudflow metrics query --metric api_request_duration_ms \\ --filter \"p95 > 5000\" \\ --last 1h # Check database slow query log kubectl logs -n cloudflow deploy/cloudflow-db-primary | \\ grep \"slow query\" | \\ tail -n 50 # Analyze query patterns cloudflow db analyze-queries --min-duration 5000 --limit 20 ``` **2.",
              "score": null
            }
          ],
          "latency_ms": 3.1960560008883476,
          "query_id": "realistic_014",
          "dimension": "problem",
          "query": "Where can I view CloudFlow metrics and logs?"
        },
        {
          "key_facts": [
            "Prometheus",
            "Grafana",
            "Jaeger for distributed tracing"
          ],
          "found_facts": [
            "Prometheus"
          ],
          "missed_facts": [
            "Grafana",
            "Jaeger for distributed tracing"
          ],
          "coverage": 0.3333333333333333,
          "expected_docs": [
            "architecture_overview",
            "deployment_guide"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_52",
              "content": "- Error rates and user impact - Recent changes or deployments - Relevant log excerpts - Correlation IDs for failed requests ```bash # Generate incident report cloudflow debug incident-report \\ --start \"2026-01-24T10:30:00Z\" \\ --end \"2026-01-24T11:00:00Z\" \\ --output incident-report.md # Capture system snapshot cloudflow debug snapshot --output snapshot-$(date +%Y%m%d-%H%M%S).tar.gz ``` #### Step 3: Escalate Based on Severity **For SEV-1 (Critical):** ```bash # Page on-call engineer cloudflow incident create \\ --severity SEV-1 \\ --title \"Complete API outage\"",
              "score": null
            },
            {
              "doc_id": "user_guide",
              "chunk_id": "user_guide_fix_32",
              "content": "executions in the dashboard - Inspect complete execution context and error details - Retry individual executions after fixing issues - Export failed executions for analysis - Set up alerts for DLQ threshold breaches **Accessing the DLQ:** 1. Navigate to **Workflows** > **[Your Workflow]** > **Dead Letter Queue** 2. Filter by error type, date range, or execution ID 3. Click an execution to view full details 4.",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_51",
              "content": "peak - Egress: 1.5 Gbps average, 4 Gbps peak - Internal (service mesh): 8 Gbps average ### Monitoring & Alerting **Key Metrics** (Monitored via Prometheus): - Request rate (per service, per endpoint) - Error rate (4xx, 5xx responses) - Latency percentiles (P50, P95, P99) - Resource utilization (CPU, memory, disk) - Cache hit/miss ratios - Database connection pool saturation - Kafka consumer lag **Alerts** (via PagerDuty): - P1 (Immediate): API error rate > 1%, database",
              "score": null
            },
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_21",
              "content": "JSON template. Key dashboard panels include: 1. **API Performance**: Request rate, P95/P99 latency, error rate 2. **Resource Usage**: CPU, memory, disk I/O per pod 3. **Database Health**: Connection pool utilization, query performance 4. **Worker Status**: Queue depth, processing rate, job success/failure ratio 5. **System Overview**: Pod status, replica count, autoscaling events ### Logging with CloudWatch CloudFlow logs are automatically shipped to CloudWatch Logs.",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_15",
              "content": "Types**: - **Cron-based**: `0 */5 * * * *` (every 5 minutes) - **One-time**: Specific timestamp for delayed execution - **Interval-based**: Every N seconds/minutes/hours **Reliability Features**: - Leader election using Redis with 30-second lease - Heartbeat mechanism to detect leader failure (5-second interval) - Automatic failover to standby scheduler (< 10 seconds) - Schedule versioning to handle updates during execution - Missed execution policy: SKIP, RUN_ONCE, or RUN_ALL **Performance Targets**: - Schedule evaluation: < 100ms",
              "score": null
            }
          ],
          "latency_ms": 3.0629089997091796,
          "query_id": "realistic_014",
          "dimension": "casual",
          "query": "monitoring stack"
        },
        {
          "key_facts": [
            "Prometheus",
            "Grafana",
            "Jaeger for distributed tracing"
          ],
          "found_facts": [
            "Prometheus",
            "Grafana",
            "Jaeger for distributed tracing"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "architecture_overview",
            "deployment_guide"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_21",
              "content": "JSON template. Key dashboard panels include: 1. **API Performance**: Request rate, P95/P99 latency, error rate 2. **Resource Usage**: CPU, memory, disk I/O per pod 3. **Database Health**: Connection pool utilization, query performance 4. **Worker Status**: Queue depth, processing rate, job success/failure ratio 5. **System Overview**: Pod status, replica count, autoscaling events ### Logging with CloudWatch CloudFlow logs are automatically shipped to CloudWatch Logs.",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_7",
              "content": "sustained --- ### Auth Service **Purpose**: Centralized authentication and authorization service handling user identity, token generation, and permission validation. **Technology**: Go with gRPC for internal communication, REST for external **Replicas**: 8 pods (production), auto-scaling 6-12 **Resource Allocation**: 1 vCPU, 2GB RAM per pod **Key Responsibilities**: - User authentication via multiple providers (OAuth2, SAML, local credentials) - JWT token generation and validation (RS256 algorithm) - Role-based access control (RBAC) with fine-grained permissions - Session management with",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_4",
              "content": "\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 ``` ### Technology Stack - **Runtime**: Node.js 20.x (API Gateway, Workflow Engine), Go 1.21 (Auth Service, Scheduler) - **Container Orchestration**: Kubernetes 1.28 (EKS) - **Service Mesh**: Istio 1.20 for service-to-service communication - **Databases**: PostgreSQL 15.4, Redis 7.2 - **Message Broker**: Apache Kafka 3.6 - **Monitoring**: Prometheus, Grafana, Jaeger for distributed tracing - **Secrets Management**: HashiCorp Vault 1.15 --- ## Microservices Breakdown ### API Gateway **Purpose**: Single entry point for all client requests,",
              "score": null
            },
            {
              "doc_id": "user_guide",
              "chunk_id": "user_guide_fix_11",
              "content": "S3) - Database record created or updated - Form submission (Typeform, Google Forms) - Chat message (Slack, Discord) **Manual Triggers** Start workflows on-demand from the dashboard or via API ## Available Actions CloudFlow provides a comprehensive library of actions to build powerful automations.",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_23",
              "content": "for high availability - Automated backups: Daily snapshots, 30-day retention - Point-in-time recovery: 5-minute granularity **Database Schema Design**: ``` Core Tables: - users (5M rows): User accounts and profiles - workflows (2M rows): Workflow definitions - workflow_executions (500M rows, partitioned): Execution history - workflow_steps (2B rows, partitioned): Individual step records - schedules (100K rows): Scheduled workflow triggers - notifications (1B rows, partitioned): Notification delivery log Partitioning Strategy: - workflow_executions: Monthly partitions by created_at - workflow_steps:",
              "score": null
            }
          ],
          "latency_ms": 3.1530260002909927,
          "query_id": "realistic_014",
          "dimension": "contextual",
          "query": "I need to set up dashboards, what monitoring systems are available?"
        },
        {
          "key_facts": [
            "Prometheus",
            "Grafana",
            "Jaeger for distributed tracing"
          ],
          "found_facts": [
            "Prometheus"
          ],
          "missed_facts": [
            "Grafana",
            "Jaeger for distributed tracing"
          ],
          "coverage": 0.3333333333333333,
          "expected_docs": [
            "architecture_overview",
            "deployment_guide"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_19",
              "content": "\\ --namespace monitoring \\ --create-namespace \\ --set prometheus.prometheusSpec.serviceMonitorSelectorNilUsesHelmValues=false \\ --set prometheus.prometheusSpec.retention=30d \\ --set prometheus.prometheusSpec.storageSpec.volumeClaimTemplate.spec.resources.requests.storage=100Gi ``` CloudFlow exposes Prometheus metrics on port 9090 at the `/metrics` endpoint. The metrics include: - **HTTP Metrics**: Request rate, latency, error rate - **Database Metrics**: Connection pool status, query duration - **Worker Metrics**: Job queue size, processing time, failure rate - **System Metrics**: CPU usage, memory usage, heap statistics Example metrics exposed: ``` # HELP cloudflow_http_requests_total Total number of HTTP",
              "score": null
            },
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_21",
              "content": "JSON template. Key dashboard panels include: 1. **API Performance**: Request rate, P95/P99 latency, error rate 2. **Resource Usage**: CPU, memory, disk I/O per pod 3. **Database Health**: Connection pool utilization, query performance 4. **Worker Status**: Queue depth, processing rate, job success/failure ratio 5. **System Overview**: Pod status, replica count, autoscaling events ### Logging with CloudWatch CloudFlow logs are automatically shipped to CloudWatch Logs.",
              "score": null
            },
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_16",
              "content": "\"limit\": 50, \"offset\": 0, \"has_more\": true } } ``` ### Analytics #### Get Workflow Metrics Retrieve performance metrics and analytics for workflows. **Endpoint:** `GET /analytics/workflows/{workflow_id}` **Query Parameters:** - `period` (required): Time period (`1h`, `24h`, `7d`, `30d`) - `metrics` (optional): Comma-separated list of metrics **Available Metrics:** - `execution_count`: Total number of executions - `success_rate`: Percentage of successful executions - `avg_duration`: Average execution duration in milliseconds - `error_count`: Total number of errors - `throughput`: Records processed per",
              "score": null
            },
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_11",
              "content": "- name: AWS_REGION value: \"us-east-1\" - name: METRICS_ENABLED value: \"true\" envFrom: - secretRef: name: cloudflow-secrets podAnnotations: prometheus.io/scrape: \"true\" prometheus.io/port: \"9090\" prometheus.io/path: \"/metrics\" podSecurityContext: runAsNonRoot: true runAsUser: 1000 fsGroup: 1000 securityContext: allowPrivilegeEscalation: false capabilities: drop: - ALL readOnlyRootFilesystem: true persistence: enabled: true storageClass: gp3 accessMode: ReadWriteOnce size: 50Gi redis: enabled: true architecture: replication auth: enabled: true master: persistence: size: 20Gi replica: replicaCount: 2 persistence: size: 20Gi ``` Deploy CloudFlow using Helm: ```bash helm install cloudflow cloudflow/cloudflow",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_51",
              "content": "peak - Egress: 1.5 Gbps average, 4 Gbps peak - Internal (service mesh): 8 Gbps average ### Monitoring & Alerting **Key Metrics** (Monitored via Prometheus): - Request rate (per service, per endpoint) - Error rate (4xx, 5xx responses) - Latency percentiles (P50, P95, P99) - Resource utilization (CPU, memory, disk) - Cache hit/miss ratios - Database connection pool saturation - Kafka consumer lag **Alerts** (via PagerDuty): - P1 (Immediate): API error rate > 1%, database",
              "score": null
            }
          ],
          "latency_ms": 3.153617000862141,
          "query_id": "realistic_014",
          "dimension": "negation",
          "query": "Why can't I see metrics in Datadog?"
        },
        {
          "key_facts": [
            "could not obtain connection from pool within 5000ms",
            "connection pool exhausted (100/100 connections in use)",
            "PgBouncer",
            "max_db_connections = 100"
          ],
          "found_facts": [
            "PgBouncer"
          ],
          "missed_facts": [
            "could not obtain connection from pool within 5000ms",
            "connection pool exhausted (100/100 connections in use)",
            "max_db_connections = 100"
          ],
          "coverage": 0.25,
          "expected_docs": [
            "troubleshooting_guide",
            "deployment_guide"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_16",
              "content": "Connection Pool Leaks** **Diagnosis:** ```bash # Check active connections cloudflow db connections --verbose # Expected output: # Active: 45/100 # Idle: 23 # Waiting: 2 # Average age: 245s ``` **Resolution:** ```bash # Adjust connection pool settings cloudflow config set db.pool.max_connections 100 cloudflow config set db.pool.min_connections 10 cloudflow config set db.pool.idle_timeout 300 cloudflow config set db.pool.max_lifetime 1800 # Force connection pool reset cloudflow db pool reset --confirm ``` **3.",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_18",
              "content": "(100/100 connections in use)` - API requests fail with `503 Service Unavailable` - Database CPU usage normal, but connection count at maximum #### Investigation ```bash # Check current connection pool status cloudflow db pool status --detailed # Output example: # Pool Statistics: # Total Connections: 100/100 (100%) # Active: 87 # Idle: 13 # Waiting Requests: 45 # Average Wait Time: 3420ms # Max Wait Time: 8234ms # Identify long-running queries cloudflow db queries --status",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_19",
              "content": "running --min-duration 30000 # Check connection distribution by client SELECT application_name, state, COUNT(*) as conn_count, AVG(EXTRACT(EPOCH FROM (NOW() - state_change))) as avg_duration_sec FROM pg_stat_activity WHERE datname = 'cloudflow_production' GROUP BY application_name, state ORDER BY conn_count DESC; ``` #### Resolution **Immediate mitigation:** ```bash # Temporarily increase connection limit (requires database restart) cloudflow db config set max_connections 150 # Kill idle connections older than 5 minutes cloudflow db connections kill --idle-timeout 300 # Restart connection pool without",
              "score": null
            },
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_29",
              "content": "Pooling Ensure optimal database connection pooling settings in PgBouncer: - **Pool Mode**: Transaction (optimal for microservices) - **Default Pool Size**: 25 connections per user - **Max DB Connections**: 100 (matches PostgreSQL `max_connections`) #### Redis Caching Implement Redis caching for frequently accessed data: ```javascript // Example caching strategy const CACHE_TTL = { workflows: 300, // 5 minutes userSessions: 3600, // 1 hour apiResults: 60 // 1 minute }; ``` #### Load Testing Perform regular load testing",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_49",
              "content": "errors for > 5 minutes - Database completely unavailable - Authentication system down - Data loss or corruption - **Response Time:** Immediate (< 15 minutes) - **Escalation:** Page on-call engineer immediately #### SEV-2: High (P2) - **Definition:** Major functionality impaired affecting multiple users - **Examples:** - Workflow execution success rate < 90% - Significant performance degradation (p95 latency > 10s) - Rate limiting affecting large customer segment - **Response Time:** < 1 hour - **Escalation:**",
              "score": null
            }
          ],
          "latency_ms": 3.0427310011873487,
          "query_id": "realistic_015",
          "dimension": "original",
          "query": "How do I diagnose database connection pool exhaustion?"
        },
        {
          "key_facts": [
            "could not obtain connection from pool within 5000ms",
            "connection pool exhausted (100/100 connections in use)",
            "PgBouncer",
            "max_db_connections = 100"
          ],
          "found_facts": [],
          "missed_facts": [
            "could not obtain connection from pool within 5000ms",
            "connection pool exhausted (100/100 connections in use)",
            "PgBouncer",
            "max_db_connections = 100"
          ],
          "coverage": 0.0,
          "expected_docs": [
            "troubleshooting_guide",
            "deployment_guide"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_16",
              "content": "Connection Pool Leaks** **Diagnosis:** ```bash # Check active connections cloudflow db connections --verbose # Expected output: # Active: 45/100 # Idle: 23 # Waiting: 2 # Average age: 245s ``` **Resolution:** ```bash # Adjust connection pool settings cloudflow config set db.pool.max_connections 100 cloudflow config set db.pool.min_connections 10 cloudflow config set db.pool.idle_timeout 300 cloudflow config set db.pool.max_lifetime 1800 # Force connection pool reset cloudflow db pool reset --confirm ``` **3.",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_19",
              "content": "running --min-duration 30000 # Check connection distribution by client SELECT application_name, state, COUNT(*) as conn_count, AVG(EXTRACT(EPOCH FROM (NOW() - state_change))) as avg_duration_sec FROM pg_stat_activity WHERE datname = 'cloudflow_production' GROUP BY application_name, state ORDER BY conn_count DESC; ``` #### Resolution **Immediate mitigation:** ```bash # Temporarily increase connection limit (requires database restart) cloudflow db config set max_connections 150 # Kill idle connections older than 5 minutes cloudflow db connections kill --idle-timeout 300 # Restart connection pool without",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_49",
              "content": "errors for > 5 minutes - Database completely unavailable - Authentication system down - Data loss or corruption - **Response Time:** Immediate (< 15 minutes) - **Escalation:** Page on-call engineer immediately #### SEV-2: High (P2) - **Definition:** Major functionality impaired affecting multiple users - **Examples:** - Workflow execution success rate < 90% - Significant performance degradation (p95 latency > 10s) - Rate limiting affecting large customer segment - **Response Time:** < 1 hour - **Escalation:**",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_18",
              "content": "(100/100 connections in use)` - API requests fail with `503 Service Unavailable` - Database CPU usage normal, but connection count at maximum #### Investigation ```bash # Check current connection pool status cloudflow db pool status --detailed # Output example: # Pool Statistics: # Total Connections: 100/100 (100%) # Active: 87 # Idle: 13 # Waiting Requests: 45 # Average Wait Time: 3420ms # Max Wait Time: 8234ms # Identify long-running queries cloudflow db queries --status",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_23",
              "content": "database tier. #### Permanent Solutions **Option 1: Upgrade database tier** ```bash # Check available tiers cloudflow db tiers list # Upgrade to higher tier (supports 200 connections) cloudflow db upgrade --tier standard-plus --confirm # Monitor migration progress cloudflow db migration status ``` **Option 2: Implement aggressive connection reuse** ```bash # Reduce connection lifetime cloudflow config set db.pool.max_lifetime 600 # 10 minutes # Enable prepared statement caching cloudflow config set db.prepared_statements.cache true cloudflow config set db.prepared_statements.max_size",
              "score": null
            }
          ],
          "latency_ms": 3.1086229992070002,
          "query_id": "realistic_015",
          "dimension": "synonym",
          "query": "What should I check when I run out of database connections?"
        },
        {
          "key_facts": [
            "could not obtain connection from pool within 5000ms",
            "connection pool exhausted (100/100 connections in use)",
            "PgBouncer",
            "max_db_connections = 100"
          ],
          "found_facts": [],
          "missed_facts": [
            "could not obtain connection from pool within 5000ms",
            "connection pool exhausted (100/100 connections in use)",
            "PgBouncer",
            "max_db_connections = 100"
          ],
          "coverage": 0.0,
          "expected_docs": [
            "troubleshooting_guide",
            "deployment_guide"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_16",
              "content": "Connection Pool Leaks** **Diagnosis:** ```bash # Check active connections cloudflow db connections --verbose # Expected output: # Active: 45/100 # Idle: 23 # Waiting: 2 # Average age: 245s ``` **Resolution:** ```bash # Adjust connection pool settings cloudflow config set db.pool.max_connections 100 cloudflow config set db.pool.min_connections 10 cloudflow config set db.pool.idle_timeout 300 cloudflow config set db.pool.max_lifetime 1800 # Force connection pool reset cloudflow db pool reset --confirm ``` **3.",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_18",
              "content": "(100/100 connections in use)` - API requests fail with `503 Service Unavailable` - Database CPU usage normal, but connection count at maximum #### Investigation ```bash # Check current connection pool status cloudflow db pool status --detailed # Output example: # Pool Statistics: # Total Connections: 100/100 (100%) # Active: 87 # Idle: 13 # Waiting Requests: 45 # Average Wait Time: 3420ms # Max Wait Time: 8234ms # Identify long-running queries cloudflow db queries --status",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_52",
              "content": "connection failure - P2 (< 15 min): P99 latency > 500ms, cache hit rate < 80% - P3 (< 1 hour): Resource utilization > 85%, replica count < minimum **SLI/SLO/SLA**: - SLI: API success rate (non-5xx responses) - SLO: 99.9% uptime per month (43 minutes downtime allowance) - SLA: 99.5% uptime guarantee (customer-facing SLA with credits) --- ## Disaster Recovery ### Recovery Objectives **RPO (Recovery Point Objective): 1 hour** - Maximum acceptable data loss: 1",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_21",
              "content": "**Add read replicas:** ```bash # Route read-only queries to replicas cloudflow db replicas add --count 2 cloudflow config set db.read_replicas \"replica-1:5432,replica-2:5432\" cloudflow config set db.read_write_split true ``` ### Connection Timeout Errors #### Error Messages - `connection timeout after 30000ms` - `could not connect to database server at 10.0.2.45:5432` - `database server unreachable` #### Troubleshooting Steps ```bash # Test network connectivity telnet cloudflow-db.internal.company.com 5432 # Check DNS resolution nslookup cloudflow-db.internal.company.com # Verify database is accepting connections",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_30",
              "content": "Connection timeout (delay: 2000ms) # Attempt 4: FAILED - NetworkError: Connection timeout (delay: 4000ms) # Final Status: FAILED_AFTER_RETRIES ``` ### Workflow Step Failures #### Common Error Patterns **1. Data Validation Errors** ``` ValidationError: Field 'customer_id' is required but missing in 234 records ``` **Resolution:** ```bash # Add data quality checks cloudflow workflows update wf_9k2n4m8p1q \\ --step data_ingestion \\ --add-validator required_fields \\ --validator-config '{\"fields\": [\"customer_id\", \"timestamp\", \"amount\"]}' # Configure error handling cloudflow workflows update wf_9k2n4m8p1q \\",
              "score": null
            }
          ],
          "latency_ms": 3.023454000867787,
          "query_id": "realistic_015",
          "dimension": "problem",
          "query": "Getting 'could not obtain connection from pool' errors"
        },
        {
          "key_facts": [
            "could not obtain connection from pool within 5000ms",
            "connection pool exhausted (100/100 connections in use)",
            "PgBouncer",
            "max_db_connections = 100"
          ],
          "found_facts": [
            "could not obtain connection from pool within 5000ms",
            "PgBouncer"
          ],
          "missed_facts": [
            "connection pool exhausted (100/100 connections in use)",
            "max_db_connections = 100"
          ],
          "coverage": 0.5,
          "expected_docs": [
            "troubleshooting_guide",
            "deployment_guide"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_16",
              "content": "Connection Pool Leaks** **Diagnosis:** ```bash # Check active connections cloudflow db connections --verbose # Expected output: # Active: 45/100 # Idle: 23 # Waiting: 2 # Average age: 245s ``` **Resolution:** ```bash # Adjust connection pool settings cloudflow config set db.pool.max_connections 100 cloudflow config set db.pool.min_connections 10 cloudflow config set db.pool.idle_timeout 300 cloudflow config set db.pool.max_lifetime 1800 # Force connection pool reset cloudflow db pool reset --confirm ``` **3.",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_19",
              "content": "running --min-duration 30000 # Check connection distribution by client SELECT application_name, state, COUNT(*) as conn_count, AVG(EXTRACT(EPOCH FROM (NOW() - state_change))) as avg_duration_sec FROM pg_stat_activity WHERE datname = 'cloudflow_production' GROUP BY application_name, state ORDER BY conn_count DESC; ``` #### Resolution **Immediate mitigation:** ```bash # Temporarily increase connection limit (requires database restart) cloudflow db config set max_connections 150 # Kill idle connections older than 5 minutes cloudflow db connections kill --idle-timeout 300 # Restart connection pool without",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_17",
              "content": "Event Stream Buffers** Unbounded event buffers can cause memory exhaustion. ```bash # Configure event buffer limits cloudflow config set events.buffer.max_size 10000 cloudflow config set events.buffer.overflow_strategy drop_oldest # Enable event streaming to external sink cloudflow config set events.sink.type kafka cloudflow config set events.sink.kafka.brokers \"kafka-1:9092,kafka-2:9092\" cloudflow config set events.sink.kafka.topic cloudflow-events ``` --- ## Database Connection Issues ### Connection Pool Exhaustion #### Symptoms - Error: `could not obtain connection from pool within 5000ms` - Error: `connection pool exhausted",
              "score": null
            },
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_29",
              "content": "Pooling Ensure optimal database connection pooling settings in PgBouncer: - **Pool Mode**: Transaction (optimal for microservices) - **Default Pool Size**: 25 connections per user - **Max DB Connections**: 100 (matches PostgreSQL `max_connections`) #### Redis Caching Implement Redis caching for frequently accessed data: ```javascript // Example caching strategy const CACHE_TTL = { workflows: 300, // 5 minutes userSessions: 3600, // 1 hour apiResults: 60 // 1 minute }; ``` #### Load Testing Perform regular load testing",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_18",
              "content": "(100/100 connections in use)` - API requests fail with `503 Service Unavailable` - Database CPU usage normal, but connection count at maximum #### Investigation ```bash # Check current connection pool status cloudflow db pool status --detailed # Output example: # Pool Statistics: # Total Connections: 100/100 (100%) # Active: 87 # Idle: 13 # Waiting Requests: 45 # Average Wait Time: 3420ms # Max Wait Time: 8234ms # Identify long-running queries cloudflow db queries --status",
              "score": null
            }
          ],
          "latency_ms": 2.9858050002076197,
          "query_id": "realistic_015",
          "dimension": "casual",
          "query": "connection pool full"
        },
        {
          "key_facts": [
            "could not obtain connection from pool within 5000ms",
            "connection pool exhausted (100/100 connections in use)",
            "PgBouncer",
            "max_db_connections = 100"
          ],
          "found_facts": [],
          "missed_facts": [
            "could not obtain connection from pool within 5000ms",
            "connection pool exhausted (100/100 connections in use)",
            "PgBouncer",
            "max_db_connections = 100"
          ],
          "coverage": 0.0,
          "expected_docs": [
            "troubleshooting_guide",
            "deployment_guide"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_16",
              "content": "Connection Pool Leaks** **Diagnosis:** ```bash # Check active connections cloudflow db connections --verbose # Expected output: # Active: 45/100 # Idle: 23 # Waiting: 2 # Average age: 245s ``` **Resolution:** ```bash # Adjust connection pool settings cloudflow config set db.pool.max_connections 100 cloudflow config set db.pool.min_connections 10 cloudflow config set db.pool.idle_timeout 300 cloudflow config set db.pool.max_lifetime 1800 # Force connection pool reset cloudflow db pool reset --confirm ``` **3.",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_18",
              "content": "(100/100 connections in use)` - API requests fail with `503 Service Unavailable` - Database CPU usage normal, but connection count at maximum #### Investigation ```bash # Check current connection pool status cloudflow db pool status --detailed # Output example: # Pool Statistics: # Total Connections: 100/100 (100%) # Active: 87 # Idle: 13 # Waiting Requests: 45 # Average Wait Time: 3420ms # Max Wait Time: 8234ms # Identify long-running queries cloudflow db queries --status",
              "score": null
            },
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_19",
              "content": "resource | | 404 | Not Found | Resource does not exist or has been deleted | | 429 | Too Many Requests | Rate limit exceeded, retry after specified period | | 500 | Internal Server Error | Unexpected server error, contact support if persists | | 502 | Bad Gateway | Temporary service issue, retry with exponential backoff | | 503 | Service Unavailable | Scheduled maintenance or temporary outage | ### Error",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_19",
              "content": "running --min-duration 30000 # Check connection distribution by client SELECT application_name, state, COUNT(*) as conn_count, AVG(EXTRACT(EPOCH FROM (NOW() - state_change))) as avg_duration_sec FROM pg_stat_activity WHERE datname = 'cloudflow_production' GROUP BY application_name, state ORDER BY conn_count DESC; ``` #### Resolution **Immediate mitigation:** ```bash # Temporarily increase connection limit (requires database restart) cloudflow db config set max_connections 150 # Kill idle connections older than 5 minutes cloudflow db connections kill --idle-timeout 300 # Restart connection pool without",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_52",
              "content": "connection failure - P2 (< 15 min): P99 latency > 500ms, cache hit rate < 80% - P3 (< 1 hour): Resource utilization > 85%, replica count < minimum **SLI/SLO/SLA**: - SLI: API success rate (non-5xx responses) - SLO: 99.9% uptime per month (43 minutes downtime allowance) - SLA: 99.5% uptime guarantee (customer-facing SLA with credits) --- ## Disaster Recovery ### Recovery Objectives **RPO (Recovery Point Objective): 1 hour** - Maximum acceptable data loss: 1",
              "score": null
            }
          ],
          "latency_ms": 3.2832390006660717,
          "query_id": "realistic_015",
          "dimension": "contextual",
          "query": "My app is failing with connection pool errors, how do I troubleshoot?"
        },
        {
          "key_facts": [
            "could not obtain connection from pool within 5000ms",
            "connection pool exhausted (100/100 connections in use)",
            "PgBouncer",
            "max_db_connections = 100"
          ],
          "found_facts": [],
          "missed_facts": [
            "could not obtain connection from pool within 5000ms",
            "connection pool exhausted (100/100 connections in use)",
            "PgBouncer",
            "max_db_connections = 100"
          ],
          "coverage": 0.0,
          "expected_docs": [
            "troubleshooting_guide",
            "deployment_guide"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_18",
              "content": "(100/100 connections in use)` - API requests fail with `503 Service Unavailable` - Database CPU usage normal, but connection count at maximum #### Investigation ```bash # Check current connection pool status cloudflow db pool status --detailed # Output example: # Pool Statistics: # Total Connections: 100/100 (100%) # Active: 87 # Idle: 13 # Waiting Requests: 45 # Average Wait Time: 3420ms # Max Wait Time: 8234ms # Identify long-running queries cloudflow db queries --status",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_49",
              "content": "errors for > 5 minutes - Database completely unavailable - Authentication system down - Data loss or corruption - **Response Time:** Immediate (< 15 minutes) - **Escalation:** Page on-call engineer immediately #### SEV-2: High (P2) - **Definition:** Major functionality impaired affecting multiple users - **Examples:** - Workflow execution success rate < 90% - Significant performance degradation (p95 latency > 10s) - Rate limiting affecting large customer segment - **Response Time:** < 1 hour - **Escalation:**",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_21",
              "content": "**Add read replicas:** ```bash # Route read-only queries to replicas cloudflow db replicas add --count 2 cloudflow config set db.read_replicas \"replica-1:5432,replica-2:5432\" cloudflow config set db.read_write_split true ``` ### Connection Timeout Errors #### Error Messages - `connection timeout after 30000ms` - `could not connect to database server at 10.0.2.45:5432` - `database server unreachable` #### Troubleshooting Steps ```bash # Test network connectivity telnet cloudflow-db.internal.company.com 5432 # Check DNS resolution nslookup cloudflow-db.internal.company.com # Verify database is accepting connections",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_22",
              "content": "pg_isready -h cloudflow-db.internal.company.com -p 5432 -U cloudflow # Check firewall rules sudo iptables -L -n | grep 5432 # Test from CloudFlow pod network kubectl run -n cloudflow debug-pod --rm -i --tty \\ --image=postgres:14 -- \\ psql -h cloudflow-db.internal.company.com -U cloudflow -d cloudflow_production # Review database logs for connection rejections kubectl logs -n cloudflow statefulset/cloudflow-db --tail=100 | \\ grep -i \"connection\\|reject\\|authentication\" ``` ### Maximum Connection Limit (100) Reached This is a hard limit in CloudFlow's",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_22",
              "content": "API Gateway - User Service profile lookups - Low latency requirement (< 50ms) - Request-response pattern with circuit breaker **Asynchronous (Kafka)**: - Workflow execution events - Notification triggers - Audit log events - Eventual consistency acceptable --- ## Database Architecture ### PostgreSQL Primary Database **Cluster Configuration**: - Primary-replica setup with 1 primary + 2 read replicas - Instance type: db.r6g.2xlarge (8 vCPU, 64GB RAM) - Storage: 2TB gp3 SSD with 12,000 IOPS - Multi-AZ deployment",
              "score": null
            }
          ],
          "latency_ms": 3.268320999268326,
          "query_id": "realistic_015",
          "dimension": "negation",
          "query": "Why can't I get a database connection even though CPU is low?"
        },
        {
          "key_facts": [
            "OAuth 2.0",
            "API keys",
            "JWT tokens"
          ],
          "found_facts": [
            "OAuth 2.0",
            "API keys"
          ],
          "missed_facts": [
            "JWT tokens"
          ],
          "coverage": 0.6666666666666666,
          "expected_docs": [
            "api_reference"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "user_guide",
              "chunk_id": "user_guide_fix_12",
              "content": "### HTTP Requests Make HTTP requests to any API endpoint: **Configuration:** - **Method**: GET, POST, PUT, PATCH, DELETE, HEAD - **URL**: Full endpoint URL (supports variable interpolation) - **Headers**: Custom headers as key-value pairs - **Query Parameters**: URL parameters - **Body**: JSON, form data, or raw text - **Authentication**: Basic Auth, Bearer Token, API Key, OAuth 2.0 **Example:** ```yaml - id: fetch_user action: http_request config: method: GET url: \"https://api.example.com/users/{{user_id}}\" headers: Authorization: \"Bearer {{secrets.API_TOKEN}}\" Content-Type: \"application/json\"",
              "score": null
            },
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_0",
              "content": "# CloudFlow API Reference Version 2.1.0 | Last Updated: January 2026 ## Overview The CloudFlow API is a RESTful service that enables developers to programmatically manage cloud workflows, data pipelines, and automation tasks. This documentation provides comprehensive details on authentication, endpoints, request/response formats, error handling, and best practices. **Base URL:** `https://api.cloudflow.io/v2` **API Status:** https://status.cloudflow.io ## Authentication CloudFlow supports three authentication methods to suit different use cases and security requirements.",
              "score": null
            },
            {
              "doc_id": "user_guide",
              "chunk_id": "user_guide_fix_39",
              "content": "Use Secrets for Sensitive Data Never hardcode API keys, passwords, or tokens in workflows: **Bad:** ```yaml headers: Authorization: \"Bearer sk_live_abc123xyz789\" ``` **Good:** ```yaml headers: Authorization: \"Bearer {{secrets.API_TOKEN}}\" ``` Store secrets in **Settings** > **Secrets** with encryption at rest. ### 4.",
              "score": null
            },
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_1",
              "content": "### API Keys API keys provide simple authentication for server-to-server communication. Include your API key in the request header: ```bash curl -H \"X-API-Key: cf_live_a1b2c3d4e5f6g7h8i9j0\" \\ https://api.cloudflow.io/v2/workflows ``` **Security Notes:** - Never expose API keys in client-side code - Rotate keys every 90 days - Use separate keys for development and production environments ### OAuth 2.0 OAuth 2.0 is recommended for applications that access CloudFlow on behalf of users.",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_7",
              "content": "sustained --- ### Auth Service **Purpose**: Centralized authentication and authorization service handling user identity, token generation, and permission validation. **Technology**: Go with gRPC for internal communication, REST for external **Replicas**: 8 pods (production), auto-scaling 6-12 **Resource Allocation**: 1 vCPU, 2GB RAM per pod **Key Responsibilities**: - User authentication via multiple providers (OAuth2, SAML, local credentials) - JWT token generation and validation (RS256 algorithm) - Role-based access control (RBAC) with fine-grained permissions - Session management with",
              "score": null
            }
          ],
          "latency_ms": 3.038102000573417,
          "query_id": "realistic_016",
          "dimension": "original",
          "query": "How do I handle API authentication?"
        },
        {
          "key_facts": [
            "OAuth 2.0",
            "API keys",
            "JWT tokens"
          ],
          "found_facts": [],
          "missed_facts": [
            "OAuth 2.0",
            "API keys",
            "JWT tokens"
          ],
          "coverage": 0.0,
          "expected_docs": [
            "api_reference"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_7",
              "content": "sustained --- ### Auth Service **Purpose**: Centralized authentication and authorization service handling user identity, token generation, and permission validation. **Technology**: Go with gRPC for internal communication, REST for external **Replicas**: 8 pods (production), auto-scaling 6-12 **Resource Allocation**: 1 vCPU, 2GB RAM per pod **Key Responsibilities**: - User authentication via multiple providers (OAuth2, SAML, local credentials) - JWT token generation and validation (RS256 algorithm) - Role-based access control (RBAC) with fine-grained permissions - Session management with",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_8",
              "content": "Redis-backed storage - API key generation and validation for service accounts - MFA enforcement for administrative operations **Authentication Flow**: ``` Client Request \u2192 API Gateway \u2192 Auth Service \u2502 \u251c\u2500 Validate credentials \u251c\u2500 Check MFA if required \u251c\u2500 Generate JWT (15min expiry) \u251c\u2500 Generate refresh token (7 days) \u2514\u2500 Store session in Redis ``` **Token Structure**: - Access Token: JWT with 15-minute expiry - Refresh Token: Opaque token with 7-day expiry, stored in PostgreSQL -",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_46",
              "content": "Istio service mesh - Database connections: SSL/TLS enforced (sslmode=require) **Data Masking**: - Sensitive fields (email, phone) masked in logs - PII redacted from error messages and stack traces - Audit logs: Full data retention with access controls **Compliance**: - GDPR: User data export and deletion workflows - SOC 2 Type II: Audit logging for all data access - HIPAA: PHI isolation in dedicated tenants with enhanced encryption --- ## Performance Characteristics ### Latency Targets **API",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_42",
              "content": "Rules**: - Security groups: Deny all by default, explicit allow rules - Ingress: Only ALB can reach API Gateway (port 8080) - Egress: Services can only reach specific dependencies - Database access: Limited to application subnets only ### Authentication & Authorization **JWT Token Validation**: - Algorithm: RS256 (asymmetric signing) - Key rotation: Every 30 days with 7-day overlap period - Public key distribution: JWKS endpoint cached in Redis - Validation: Signature, expiry, issuer, audience claims",
              "score": null
            },
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_0",
              "content": "# CloudFlow API Reference Version 2.1.0 | Last Updated: January 2026 ## Overview The CloudFlow API is a RESTful service that enables developers to programmatically manage cloud workflows, data pipelines, and automation tasks. This documentation provides comprehensive details on authentication, endpoints, request/response formats, error handling, and best practices. **Base URL:** `https://api.cloudflow.io/v2` **API Status:** https://status.cloudflow.io ## Authentication CloudFlow supports three authentication methods to suit different use cases and security requirements.",
              "score": null
            }
          ],
          "latency_ms": 3.0221929991967045,
          "query_id": "realistic_016",
          "dimension": "synonym",
          "query": "What authentication methods are supported?"
        },
        {
          "key_facts": [
            "OAuth 2.0",
            "API keys",
            "JWT tokens"
          ],
          "found_facts": [
            "OAuth 2.0"
          ],
          "missed_facts": [
            "API keys",
            "JWT tokens"
          ],
          "coverage": 0.3333333333333333,
          "expected_docs": [
            "api_reference"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_18",
              "content": "format. ### Error Response Format ```json { \"error\": { \"code\": \"invalid_parameter\", \"message\": \"The 'limit' parameter must be between 1 and 100\", \"field\": \"limit\", \"request_id\": \"req_8k3m9x2p\" } } ``` ### HTTP Status Codes | Status Code | Description | Common Causes | |------------|-------------|---------------| | 400 | Bad Request | Invalid parameters, malformed JSON, validation errors | | 401 | Unauthorized | Missing or invalid authentication credentials | | 403 | Forbidden | Insufficient permissions for requested",
              "score": null
            },
            {
              "doc_id": "user_guide",
              "chunk_id": "user_guide_fix_12",
              "content": "### HTTP Requests Make HTTP requests to any API endpoint: **Configuration:** - **Method**: GET, POST, PUT, PATCH, DELETE, HEAD - **URL**: Full endpoint URL (supports variable interpolation) - **Headers**: Custom headers as key-value pairs - **Query Parameters**: URL parameters - **Body**: JSON, form data, or raw text - **Authentication**: Basic Auth, Bearer Token, API Key, OAuth 2.0 **Example:** ```yaml - id: fetch_user action: http_request config: method: GET url: \"https://api.example.com/users/{{user_id}}\" headers: Authorization: \"Bearer {{secrets.API_TOKEN}}\" Content-Type: \"application/json\"",
              "score": null
            },
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_20",
              "content": "Codes CloudFlow returns specific error codes to help you identify and resolve issues: - `invalid_parameter`: One or more request parameters are invalid - `missing_required_field`: Required field is missing from request body - `authentication_failed`: Invalid API key or token - `insufficient_permissions`: User lacks required scope or permission - `resource_not_found`: Requested resource does not exist - `rate_limit_exceeded`: Too many requests, see rate limiting section - `workflow_execution_failed`: Workflow execution encountered an error - `invalid_json`: Request body contains malformed",
              "score": null
            },
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_21",
              "content": "JSON - `duplicate_resource`: Resource with same identifier already exists - `quota_exceeded`: Account quota limit reached ### Error Handling Best Practices **Python Example:** ```python import requests import time def make_api_request(url, headers, max_retries=3): for attempt in range(max_retries): try: response = requests.get(url, headers=headers) if response.status_code == 200: return response.json() elif response.status_code == 429: # Rate limit exceeded retry_after = int(response.headers.get('X-RateLimit-Reset', 60)) print(f\"Rate limited.",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_31",
              "content": "--step data_validation \\ --on-error continue \\ --error-threshold 5% # Fail if > 5% of records invalid ``` **2. External API Failures** ``` ExternalAPIError: API request to https://partner-api.example.com failed with status 502 ``` **Resolution:** ```bash # Add circuit breaker cloudflow workflows update wf_9k2n4m8p1q \\ --step external_api_call \\ --circuit-breaker-enabled true \\ --circuit-breaker-threshold 5 \\ --circuit-breaker-timeout 30000 # Configure fallback behavior cloudflow workflows update wf_9k2n4m8p1q \\ --step external_api_call \\ --fallback-action use_cached_data \\ --cache-ttl 3600 ``` --- ## Rate",
              "score": null
            }
          ],
          "latency_ms": 3.011662998687825,
          "query_id": "realistic_016",
          "dimension": "problem",
          "query": "My API requests are getting 401 errors"
        },
        {
          "key_facts": [
            "OAuth 2.0",
            "API keys",
            "JWT tokens"
          ],
          "found_facts": [],
          "missed_facts": [
            "OAuth 2.0",
            "API keys",
            "JWT tokens"
          ],
          "coverage": 0.0,
          "expected_docs": [
            "api_reference"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_7",
              "content": "sustained --- ### Auth Service **Purpose**: Centralized authentication and authorization service handling user identity, token generation, and permission validation. **Technology**: Go with gRPC for internal communication, REST for external **Replicas**: 8 pods (production), auto-scaling 6-12 **Resource Allocation**: 1 vCPU, 2GB RAM per pod **Key Responsibilities**: - User authentication via multiple providers (OAuth2, SAML, local credentials) - JWT token generation and validation (RS256 algorithm) - Role-based access control (RBAC) with fine-grained permissions - Session management with",
              "score": null
            },
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_0",
              "content": "# CloudFlow API Reference Version 2.1.0 | Last Updated: January 2026 ## Overview The CloudFlow API is a RESTful service that enables developers to programmatically manage cloud workflows, data pipelines, and automation tasks. This documentation provides comprehensive details on authentication, endpoints, request/response formats, error handling, and best practices. **Base URL:** `https://api.cloudflow.io/v2` **API Status:** https://status.cloudflow.io ## Authentication CloudFlow supports three authentication methods to suit different use cases and security requirements.",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_8",
              "content": "Redis-backed storage - API key generation and validation for service accounts - MFA enforcement for administrative operations **Authentication Flow**: ``` Client Request \u2192 API Gateway \u2192 Auth Service \u2502 \u251c\u2500 Validate credentials \u251c\u2500 Check MFA if required \u251c\u2500 Generate JWT (15min expiry) \u251c\u2500 Generate refresh token (7 days) \u2514\u2500 Store session in Redis ``` **Token Structure**: - Access Token: JWT with 15-minute expiry - Refresh Token: Opaque token with 7-day expiry, stored in PostgreSQL -",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_9",
              "content": "Claims: user_id, email, roles[], permissions[], tenant_id **Security Features**: - Password hashing: Argon2id with 64MB memory, 4 iterations - Token rotation on refresh to prevent replay attacks - Brute force protection: 5 failed attempts \u2192 15-minute lockout - Secrets stored in HashiCorp Vault, rotated every 90 days **Performance Targets**: - Token validation: < 10ms (P99) - Token generation: < 50ms (P99) - Throughput: 5,000 RPS for validation operations --- ### Workflow Engine **Purpose**: Core orchestration service",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_42",
              "content": "Rules**: - Security groups: Deny all by default, explicit allow rules - Ingress: Only ALB can reach API Gateway (port 8080) - Egress: Services can only reach specific dependencies - Database access: Limited to application subnets only ### Authentication & Authorization **JWT Token Validation**: - Algorithm: RS256 (asymmetric signing) - Key rotation: Every 30 days with 7-day overlap period - Public key distribution: JWKS endpoint cached in Redis - Validation: Signature, expiry, issuer, audience claims",
              "score": null
            }
          ],
          "latency_ms": 2.999719999934314,
          "query_id": "realistic_016",
          "dimension": "casual",
          "query": "auth methods"
        },
        {
          "key_facts": [
            "OAuth 2.0",
            "API keys",
            "JWT tokens"
          ],
          "found_facts": [
            "OAuth 2.0",
            "API keys",
            "JWT tokens"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "api_reference"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_0",
              "content": "# CloudFlow API Reference Version 2.1.0 | Last Updated: January 2026 ## Overview The CloudFlow API is a RESTful service that enables developers to programmatically manage cloud workflows, data pipelines, and automation tasks. This documentation provides comprehensive details on authentication, endpoints, request/response formats, error handling, and best practices. **Base URL:** `https://api.cloudflow.io/v2` **API Status:** https://status.cloudflow.io ## Authentication CloudFlow supports three authentication methods to suit different use cases and security requirements.",
              "score": null
            },
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_1",
              "content": "### API Keys API keys provide simple authentication for server-to-server communication. Include your API key in the request header: ```bash curl -H \"X-API-Key: cf_live_a1b2c3d4e5f6g7h8i9j0\" \\ https://api.cloudflow.io/v2/workflows ``` **Security Notes:** - Never expose API keys in client-side code - Rotate keys every 90 days - Use separate keys for development and production environments ### OAuth 2.0 OAuth 2.0 is recommended for applications that access CloudFlow on behalf of users.",
              "score": null
            },
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_2",
              "content": "We support the Authorization Code flow with PKCE. **Authorization Endpoint:** `https://auth.cloudflow.io/oauth/authorize` **Token Endpoint:** `https://auth.cloudflow.io/oauth/token` **Supported Scopes:** - `workflows:read` - Read workflow configurations - `workflows:write` - Create and modify workflows - `pipelines:read` - Read pipeline data - `pipelines:write` - Create and manage pipelines - `analytics:read` - Access analytics and metrics - `admin:full` - Full administrative access Example authorization request: ```bash curl -X POST https://auth.cloudflow.io/oauth/token \\ -H \"Content-Type: application/x-www-form-urlencoded\" \\ -d \"grant_type=authorization_code\" \\ -d \"code=AUTH_CODE_HERE\" \\ -d",
              "score": null
            },
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_3",
              "content": "\"client_id=YOUR_CLIENT_ID\" \\ -d \"client_secret=YOUR_CLIENT_SECRET\" \\ -d \"redirect_uri=https://yourapp.com/callback\" ``` ### JWT Tokens For advanced use cases, CloudFlow supports JSON Web Tokens (JWT) with RS256 signing algorithm. JWTs must include the following claims: - `iss` (issuer): Your application identifier - `sub` (subject): User or service account ID - `aud` (audience): `https://api.cloudflow.io` - `exp` (expiration): Unix timestamp (max 3600 seconds from `iat`) - `iat` (issued at): Unix timestamp - `scope`: Space-separated list of requested scopes Example JWT header:",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_2",
              "content": "Unauthorized` - Error message: `Authentication credentials were not provided or are invalid` - Frontend displays \"Session expired\" message #### Common Causes **1. Token Expiration** CloudFlow access tokens expire after 3600 seconds (1 hour) by default. Refresh tokens are valid for 30 days.",
              "score": null
            }
          ],
          "latency_ms": 3.287286999693606,
          "query_id": "realistic_016",
          "dimension": "contextual",
          "query": "I'm integrating with CloudFlow API, what authentication options do I have?"
        },
        {
          "key_facts": [
            "OAuth 2.0",
            "API keys",
            "JWT tokens"
          ],
          "found_facts": [
            "API keys"
          ],
          "missed_facts": [
            "OAuth 2.0",
            "JWT tokens"
          ],
          "coverage": 0.3333333333333333,
          "expected_docs": [
            "api_reference"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_2",
              "content": "We support the Authorization Code flow with PKCE. **Authorization Endpoint:** `https://auth.cloudflow.io/oauth/authorize` **Token Endpoint:** `https://auth.cloudflow.io/oauth/token` **Supported Scopes:** - `workflows:read` - Read workflow configurations - `workflows:write` - Create and modify workflows - `pipelines:read` - Read pipeline data - `pipelines:write` - Create and manage pipelines - `analytics:read` - Access analytics and metrics - `admin:full` - Full administrative access Example authorization request: ```bash curl -X POST https://auth.cloudflow.io/oauth/token \\ -H \"Content-Type: application/x-www-form-urlencoded\" \\ -d \"grant_type=authorization_code\" \\ -d \"code=AUTH_CODE_HERE\" \\ -d",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_7",
              "content": "sustained --- ### Auth Service **Purpose**: Centralized authentication and authorization service handling user identity, token generation, and permission validation. **Technology**: Go with gRPC for internal communication, REST for external **Replicas**: 8 pods (production), auto-scaling 6-12 **Resource Allocation**: 1 vCPU, 2GB RAM per pod **Key Responsibilities**: - User authentication via multiple providers (OAuth2, SAML, local credentials) - JWT token generation and validation (RS256 algorithm) - Role-based access control (RBAC) with fine-grained permissions - Session management with",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_7",
              "content": "pending approvals cloudflow auth list-requests --status pending ``` ### 403 Forbidden Errors #### RBAC Policy Violations CloudFlow uses role-based access control (RBAC) with the following hierarchy: - `viewer` - Read-only access - `developer` - Create and modify workflows (non-production) - `operator` - Execute workflows, view logs - `admin` - Full access to workspace - `platform-admin` - Cross-workspace administration **Verify resource permissions:** ```bash # Check effective permissions cloudflow rbac check \\ --user john.doe@company.com \\ --resource workflow:prod-pipeline",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_2",
              "content": "Unauthorized` - Error message: `Authentication credentials were not provided or are invalid` - Frontend displays \"Session expired\" message #### Common Causes **1. Token Expiration** CloudFlow access tokens expire after 3600 seconds (1 hour) by default. Refresh tokens are valid for 30 days.",
              "score": null
            },
            {
              "doc_id": "user_guide",
              "chunk_id": "user_guide_fix_39",
              "content": "Use Secrets for Sensitive Data Never hardcode API keys, passwords, or tokens in workflows: **Bad:** ```yaml headers: Authorization: \"Bearer sk_live_abc123xyz789\" ``` **Good:** ```yaml headers: Authorization: \"Bearer {{secrets.API_TOKEN}}\" ``` Store secrets in **Settings** > **Secrets** with encryption at rest. ### 4.",
              "score": null
            }
          ],
          "latency_ms": 3.0931349992897594,
          "query_id": "realistic_016",
          "dimension": "negation",
          "query": "Why isn't basic auth working?"
        },
        {
          "key_facts": [
            "PgBouncer",
            "connection pooling",
            "max_db_connections = 100",
            "default_pool_size = 25",
            "pool_mode = transaction"
          ],
          "found_facts": [
            "PgBouncer",
            "connection pooling",
            "max_db_connections = 100",
            "default_pool_size = 25",
            "pool_mode = transaction"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "deployment_guide",
            "architecture_overview"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_17",
              "content": "1 --- apiVersion: apps/v1 kind: Deployment metadata: name: pgbouncer namespace: cloudflow-prod spec: replicas: 2 selector: matchLabels: app: pgbouncer template: metadata: labels: app: pgbouncer spec: containers: - name: pgbouncer image: pgbouncer/pgbouncer:1.21.0 ports: - containerPort: 5432 resources: requests: cpu: 500m memory: 512Mi limits: cpu: 1000m memory: 1Gi volumeMounts: - name: config mountPath: /etc/pgbouncer volumes: - name: config configMap: name: pgbouncer-config --- apiVersion: v1 kind: Service metadata: name: pgbouncer namespace: cloudflow-prod spec: selector: app: pgbouncer ports: - port:",
              "score": null
            },
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_16",
              "content": "handle up to 100 connections efficiently: ```yaml # pgbouncer.yaml apiVersion: v1 kind: ConfigMap metadata: name: pgbouncer-config namespace: cloudflow-prod data: pgbouncer.ini: | [databases] cloudflow = host=postgresql.cloudflow-prod.svc.cluster.local port=5432 dbname=cloudflow [pgbouncer] listen_addr = 0.0.0.0 listen_port = 5432 auth_type = md5 auth_file = /etc/pgbouncer/userlist.txt pool_mode = transaction max_client_conn = 1000 default_pool_size = 25 reserve_pool_size = 5 reserve_pool_timeout = 3 max_db_connections = 100 max_user_connections = 100 server_lifetime = 3600 server_idle_timeout = 600 log_connections = 1 log_disconnections = 1 log_pooler_errors =",
              "score": null
            },
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_35",
              "content": "#### Database Connection Pool Exhausted **Symptoms**: \"Too many connections\" errors **Diagnosis**: ```bash # Check PgBouncer stats kubectl exec -it pgbouncer-xxx -n cloudflow-prod -- \\ psql -p 5432 -U pgbouncer pgbouncer -c \"SHOW POOLS;\" ``` **Resolution**: - Increase `max_db_connections` in PgBouncer - Optimize application connection usage - Add more PostgreSQL replicas ### Support Contacts - **On-call Engineer**: pagerduty.com/cloudflow-oncall - **Slack Channel**: #cloudflow-operations - **Documentation**: https://docs.cloudflow.io - **Status Page**: https://status.cloudflow.io --- ## Appendix ### Useful Commands ```bash",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_20",
              "content": "downtime kubectl rollout restart deployment/cloudflow-api -n cloudflow kubectl rollout status deployment/cloudflow-api -n cloudflow ``` **Long-term solutions:** 1. **Implement connection pooling optimization:** ```bash # Use PgBouncer for connection pooling kubectl apply -f cloudflow-pgbouncer.yaml # Configure CloudFlow to use PgBouncer cloudflow config set db.host pgbouncer.cloudflow.svc.cluster.local cloudflow config set db.port 6432 cloudflow config set db.pool.mode transaction ``` 2.",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_0",
              "content": "# CloudFlow Platform - System Architecture Overview **Document Version:** 2.3.1 **Last Updated:** January 15, 2026 **Owner:** Platform Architecture Team **Status:** Production ## Executive Summary CloudFlow is a distributed, cloud-native workflow automation platform designed to orchestrate complex business processes at scale. The platform processes over 2.5 million workflow executions daily with an average P99 latency of 180ms for API operations and 4.2 seconds for workflow execution.",
              "score": null
            }
          ],
          "latency_ms": 3.2783299993752735,
          "query_id": "realistic_017",
          "dimension": "original",
          "query": "What is PgBouncer and why is it used in CloudFlow?"
        },
        {
          "key_facts": [
            "PgBouncer",
            "connection pooling",
            "max_db_connections = 100",
            "default_pool_size = 25",
            "pool_mode = transaction"
          ],
          "found_facts": [
            "PgBouncer",
            "connection pooling",
            "max_db_connections = 100",
            "default_pool_size = 25",
            "pool_mode = transaction"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "deployment_guide",
            "architecture_overview"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_29",
              "content": "Pooling Ensure optimal database connection pooling settings in PgBouncer: - **Pool Mode**: Transaction (optimal for microservices) - **Default Pool Size**: 25 connections per user - **Max DB Connections**: 100 (matches PostgreSQL `max_connections`) #### Redis Caching Implement Redis caching for frequently accessed data: ```javascript // Example caching strategy const CACHE_TTL = { workflows: 300, // 5 minutes userSessions: 3600, // 1 hour apiResults: 60 // 1 minute }; ``` #### Load Testing Perform regular load testing",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_16",
              "content": "Connection Pool Leaks** **Diagnosis:** ```bash # Check active connections cloudflow db connections --verbose # Expected output: # Active: 45/100 # Idle: 23 # Waiting: 2 # Average age: 245s ``` **Resolution:** ```bash # Adjust connection pool settings cloudflow config set db.pool.max_connections 100 cloudflow config set db.pool.min_connections 10 cloudflow config set db.pool.idle_timeout 300 cloudflow config set db.pool.max_lifetime 1800 # Force connection pool reset cloudflow db pool reset --confirm ``` **3.",
              "score": null
            },
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_16",
              "content": "handle up to 100 connections efficiently: ```yaml # pgbouncer.yaml apiVersion: v1 kind: ConfigMap metadata: name: pgbouncer-config namespace: cloudflow-prod data: pgbouncer.ini: | [databases] cloudflow = host=postgresql.cloudflow-prod.svc.cluster.local port=5432 dbname=cloudflow [pgbouncer] listen_addr = 0.0.0.0 listen_port = 5432 auth_type = md5 auth_file = /etc/pgbouncer/userlist.txt pool_mode = transaction max_client_conn = 1000 default_pool_size = 25 reserve_pool_size = 5 reserve_pool_timeout = 3 max_db_connections = 100 max_user_connections = 100 server_lifetime = 3600 server_idle_timeout = 600 log_connections = 1 log_disconnections = 1 log_pooler_errors =",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_25",
              "content": "connections per service - Max client connections: 2000 - Connection timeout: 30 seconds **Performance Characteristics**: - Read query P99: < 20ms - Write query P99: < 50ms - Transaction throughput: 15,000 TPS - Replication lag: < 500ms --- ### Redis Caching Layer **Cluster Configuration**: - Redis Cluster with 6 nodes (3 primary + 3 replica) - Instance type: cache.r6g.xlarge (4 vCPU, 26GB RAM) - Total memory: 78GB usable cache space - Persistence: RDB snapshots every",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_6",
              "content": "CORS handling for web clients - Request/response logging and correlation ID injection **Critical Endpoints**: - `POST /api/v1/workflows` - Create new workflow - `GET /api/v1/workflows/:id` - Retrieve workflow status - `POST /api/v1/workflows/:id/execute` - Trigger workflow execution - `GET /api/v1/workflows/:id/history` - Get execution history **Dependencies**: - Auth Service (for token validation) - Redis (for rate limiting counters) - All downstream microservices **Performance Targets**: - P50 latency: < 50ms - P99 latency: < 200ms - Throughput: 10,000 RPS",
              "score": null
            }
          ],
          "latency_ms": 3.0419489994528703,
          "query_id": "realistic_017",
          "dimension": "synonym",
          "query": "What's the purpose of the connection pooler?"
        },
        {
          "key_facts": [
            "PgBouncer",
            "connection pooling",
            "max_db_connections = 100",
            "default_pool_size = 25",
            "pool_mode = transaction"
          ],
          "found_facts": [
            "PgBouncer",
            "connection pooling",
            "max_db_connections = 100",
            "default_pool_size = 25",
            "pool_mode = transaction"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "deployment_guide",
            "architecture_overview"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_16",
              "content": "handle up to 100 connections efficiently: ```yaml # pgbouncer.yaml apiVersion: v1 kind: ConfigMap metadata: name: pgbouncer-config namespace: cloudflow-prod data: pgbouncer.ini: | [databases] cloudflow = host=postgresql.cloudflow-prod.svc.cluster.local port=5432 dbname=cloudflow [pgbouncer] listen_addr = 0.0.0.0 listen_port = 5432 auth_type = md5 auth_file = /etc/pgbouncer/userlist.txt pool_mode = transaction max_client_conn = 1000 default_pool_size = 25 reserve_pool_size = 5 reserve_pool_timeout = 3 max_db_connections = 100 max_user_connections = 100 server_lifetime = 3600 server_idle_timeout = 600 log_connections = 1 log_disconnections = 1 log_pooler_errors =",
              "score": null
            },
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_29",
              "content": "Pooling Ensure optimal database connection pooling settings in PgBouncer: - **Pool Mode**: Transaction (optimal for microservices) - **Default Pool Size**: 25 connections per user - **Max DB Connections**: 100 (matches PostgreSQL `max_connections`) #### Redis Caching Implement Redis caching for frequently accessed data: ```javascript // Example caching strategy const CACHE_TTL = { workflows: 300, // 5 minutes userSessions: 3600, // 1 hour apiResults: 60 // 1 minute }; ``` #### Load Testing Perform regular load testing",
              "score": null
            },
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_35",
              "content": "#### Database Connection Pool Exhausted **Symptoms**: \"Too many connections\" errors **Diagnosis**: ```bash # Check PgBouncer stats kubectl exec -it pgbouncer-xxx -n cloudflow-prod -- \\ psql -p 5432 -U pgbouncer pgbouncer -c \"SHOW POOLS;\" ``` **Resolution**: - Increase `max_db_connections` in PgBouncer - Optimize application connection usage - Add more PostgreSQL replicas ### Support Contacts - **On-call Engineer**: pagerduty.com/cloudflow-oncall - **Slack Channel**: #cloudflow-operations - **Documentation**: https://docs.cloudflow.io - **Status Page**: https://status.cloudflow.io --- ## Appendix ### Useful Commands ```bash",
              "score": null
            },
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_17",
              "content": "1 --- apiVersion: apps/v1 kind: Deployment metadata: name: pgbouncer namespace: cloudflow-prod spec: replicas: 2 selector: matchLabels: app: pgbouncer template: metadata: labels: app: pgbouncer spec: containers: - name: pgbouncer image: pgbouncer/pgbouncer:1.21.0 ports: - containerPort: 5432 resources: requests: cpu: 500m memory: 512Mi limits: cpu: 1000m memory: 1Gi volumeMounts: - name: config mountPath: /etc/pgbouncer volumes: - name: config configMap: name: pgbouncer-config --- apiVersion: v1 kind: Service metadata: name: pgbouncer namespace: cloudflow-prod spec: selector: app: pgbouncer ports: - port:",
              "score": null
            },
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_15",
              "content": "1GB max_wal_size = 4GB readReplicas: replicaCount: 2 resources: limits: cpu: 2000m memory: 4Gi requests: cpu: 1000m memory: 2Gi metrics: enabled: true serviceMonitor: enabled: true ``` Install PostgreSQL: ```bash # Create password secret kubectl create secret generic postgres-credentials \\ --namespace cloudflow-prod \\ --from-literal=postgres-password=\"$(openssl rand -base64 32)\" \\ --from-literal=password=\"$(openssl rand -base64 32)\" # Install PostgreSQL helm install postgresql bitnami/postgresql \\ --namespace cloudflow-prod \\ --values postgres-values.yaml \\ --wait ``` ### PgBouncer Configuration Deploy PgBouncer for connection pooling to",
              "score": null
            }
          ],
          "latency_ms": 3.1723930005682632,
          "query_id": "realistic_017",
          "dimension": "problem",
          "query": "Should I connect directly to PostgreSQL or through PgBouncer?"
        },
        {
          "key_facts": [
            "PgBouncer",
            "connection pooling",
            "max_db_connections = 100",
            "default_pool_size = 25",
            "pool_mode = transaction"
          ],
          "found_facts": [
            "PgBouncer",
            "connection pooling",
            "max_db_connections = 100",
            "default_pool_size = 25",
            "pool_mode = transaction"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "deployment_guide",
            "architecture_overview"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_17",
              "content": "1 --- apiVersion: apps/v1 kind: Deployment metadata: name: pgbouncer namespace: cloudflow-prod spec: replicas: 2 selector: matchLabels: app: pgbouncer template: metadata: labels: app: pgbouncer spec: containers: - name: pgbouncer image: pgbouncer/pgbouncer:1.21.0 ports: - containerPort: 5432 resources: requests: cpu: 500m memory: 512Mi limits: cpu: 1000m memory: 1Gi volumeMounts: - name: config mountPath: /etc/pgbouncer volumes: - name: config configMap: name: pgbouncer-config --- apiVersion: v1 kind: Service metadata: name: pgbouncer namespace: cloudflow-prod spec: selector: app: pgbouncer ports: - port:",
              "score": null
            },
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_16",
              "content": "handle up to 100 connections efficiently: ```yaml # pgbouncer.yaml apiVersion: v1 kind: ConfigMap metadata: name: pgbouncer-config namespace: cloudflow-prod data: pgbouncer.ini: | [databases] cloudflow = host=postgresql.cloudflow-prod.svc.cluster.local port=5432 dbname=cloudflow [pgbouncer] listen_addr = 0.0.0.0 listen_port = 5432 auth_type = md5 auth_file = /etc/pgbouncer/userlist.txt pool_mode = transaction max_client_conn = 1000 default_pool_size = 25 reserve_pool_size = 5 reserve_pool_timeout = 3 max_db_connections = 100 max_user_connections = 100 server_lifetime = 3600 server_idle_timeout = 600 log_connections = 1 log_disconnections = 1 log_pooler_errors =",
              "score": null
            },
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_35",
              "content": "#### Database Connection Pool Exhausted **Symptoms**: \"Too many connections\" errors **Diagnosis**: ```bash # Check PgBouncer stats kubectl exec -it pgbouncer-xxx -n cloudflow-prod -- \\ psql -p 5432 -U pgbouncer pgbouncer -c \"SHOW POOLS;\" ``` **Resolution**: - Increase `max_db_connections` in PgBouncer - Optimize application connection usage - Add more PostgreSQL replicas ### Support Contacts - **On-call Engineer**: pagerduty.com/cloudflow-oncall - **Slack Channel**: #cloudflow-operations - **Documentation**: https://docs.cloudflow.io - **Status Page**: https://status.cloudflow.io --- ## Appendix ### Useful Commands ```bash",
              "score": null
            },
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_29",
              "content": "Pooling Ensure optimal database connection pooling settings in PgBouncer: - **Pool Mode**: Transaction (optimal for microservices) - **Default Pool Size**: 25 connections per user - **Max DB Connections**: 100 (matches PostgreSQL `max_connections`) #### Redis Caching Implement Redis caching for frequently accessed data: ```javascript // Example caching strategy const CACHE_TTL = { workflows: 300, // 5 minutes userSessions: 3600, // 1 hour apiResults: 60 // 1 minute }; ``` #### Load Testing Perform regular load testing",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_20",
              "content": "downtime kubectl rollout restart deployment/cloudflow-api -n cloudflow kubectl rollout status deployment/cloudflow-api -n cloudflow ``` **Long-term solutions:** 1. **Implement connection pooling optimization:** ```bash # Use PgBouncer for connection pooling kubectl apply -f cloudflow-pgbouncer.yaml # Configure CloudFlow to use PgBouncer cloudflow config set db.host pgbouncer.cloudflow.svc.cluster.local cloudflow config set db.port 6432 cloudflow config set db.pool.mode transaction ``` 2.",
              "score": null
            }
          ],
          "latency_ms": 3.0696410012751585,
          "query_id": "realistic_017",
          "dimension": "casual",
          "query": "pgbouncer purpose"
        },
        {
          "key_facts": [
            "PgBouncer",
            "connection pooling",
            "max_db_connections = 100",
            "default_pool_size = 25",
            "pool_mode = transaction"
          ],
          "found_facts": [
            "PgBouncer",
            "connection pooling",
            "max_db_connections = 100",
            "default_pool_size = 25",
            "pool_mode = transaction"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "deployment_guide",
            "architecture_overview"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_29",
              "content": "Pooling Ensure optimal database connection pooling settings in PgBouncer: - **Pool Mode**: Transaction (optimal for microservices) - **Default Pool Size**: 25 connections per user - **Max DB Connections**: 100 (matches PostgreSQL `max_connections`) #### Redis Caching Implement Redis caching for frequently accessed data: ```javascript // Example caching strategy const CACHE_TTL = { workflows: 300, // 5 minutes userSessions: 3600, // 1 hour apiResults: 60 // 1 minute }; ``` #### Load Testing Perform regular load testing",
              "score": null
            },
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_16",
              "content": "handle up to 100 connections efficiently: ```yaml # pgbouncer.yaml apiVersion: v1 kind: ConfigMap metadata: name: pgbouncer-config namespace: cloudflow-prod data: pgbouncer.ini: | [databases] cloudflow = host=postgresql.cloudflow-prod.svc.cluster.local port=5432 dbname=cloudflow [pgbouncer] listen_addr = 0.0.0.0 listen_port = 5432 auth_type = md5 auth_file = /etc/pgbouncer/userlist.txt pool_mode = transaction max_client_conn = 1000 default_pool_size = 25 reserve_pool_size = 5 reserve_pool_timeout = 3 max_db_connections = 100 max_user_connections = 100 server_lifetime = 3600 server_idle_timeout = 600 log_connections = 1 log_disconnections = 1 log_pooler_errors =",
              "score": null
            },
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_35",
              "content": "#### Database Connection Pool Exhausted **Symptoms**: \"Too many connections\" errors **Diagnosis**: ```bash # Check PgBouncer stats kubectl exec -it pgbouncer-xxx -n cloudflow-prod -- \\ psql -p 5432 -U pgbouncer pgbouncer -c \"SHOW POOLS;\" ``` **Resolution**: - Increase `max_db_connections` in PgBouncer - Optimize application connection usage - Add more PostgreSQL replicas ### Support Contacts - **On-call Engineer**: pagerduty.com/cloudflow-oncall - **Slack Channel**: #cloudflow-operations - **Documentation**: https://docs.cloudflow.io - **Status Page**: https://status.cloudflow.io --- ## Appendix ### Useful Commands ```bash",
              "score": null
            },
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_17",
              "content": "1 --- apiVersion: apps/v1 kind: Deployment metadata: name: pgbouncer namespace: cloudflow-prod spec: replicas: 2 selector: matchLabels: app: pgbouncer template: metadata: labels: app: pgbouncer spec: containers: - name: pgbouncer image: pgbouncer/pgbouncer:1.21.0 ports: - containerPort: 5432 resources: requests: cpu: 500m memory: 512Mi limits: cpu: 1000m memory: 1Gi volumeMounts: - name: config mountPath: /etc/pgbouncer volumes: - name: config configMap: name: pgbouncer-config --- apiVersion: v1 kind: Service metadata: name: pgbouncer namespace: cloudflow-prod spec: selector: app: pgbouncer ports: - port:",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_22",
              "content": "API Gateway - User Service profile lookups - Low latency requirement (< 50ms) - Request-response pattern with circuit breaker **Asynchronous (Kafka)**: - Workflow execution events - Notification triggers - Audit log events - Eventual consistency acceptable --- ## Database Architecture ### PostgreSQL Primary Database **Cluster Configuration**: - Primary-replica setup with 1 primary + 2 read replicas - Instance type: db.r6g.2xlarge (8 vCPU, 64GB RAM) - Storage: 2TB gp3 SSD with 12,000 IOPS - Multi-AZ deployment",
              "score": null
            }
          ],
          "latency_ms": 3.1819800005905563,
          "query_id": "realistic_017",
          "dimension": "contextual",
          "query": "Optimizing database connections, what role does PgBouncer play?"
        },
        {
          "key_facts": [
            "PgBouncer",
            "connection pooling",
            "max_db_connections = 100",
            "default_pool_size = 25",
            "pool_mode = transaction"
          ],
          "found_facts": [
            "PgBouncer"
          ],
          "missed_facts": [
            "connection pooling",
            "max_db_connections = 100",
            "default_pool_size = 25",
            "pool_mode = transaction"
          ],
          "coverage": 0.2,
          "expected_docs": [
            "deployment_guide",
            "architecture_overview"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_21",
              "content": "**Add read replicas:** ```bash # Route read-only queries to replicas cloudflow db replicas add --count 2 cloudflow config set db.read_replicas \"replica-1:5432,replica-2:5432\" cloudflow config set db.read_write_split true ``` ### Connection Timeout Errors #### Error Messages - `connection timeout after 30000ms` - `could not connect to database server at 10.0.2.45:5432` - `database server unreachable` #### Troubleshooting Steps ```bash # Test network connectivity telnet cloudflow-db.internal.company.com 5432 # Check DNS resolution nslookup cloudflow-db.internal.company.com # Verify database is accepting connections",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_22",
              "content": "pg_isready -h cloudflow-db.internal.company.com -p 5432 -U cloudflow # Check firewall rules sudo iptables -L -n | grep 5432 # Test from CloudFlow pod network kubectl run -n cloudflow debug-pod --rm -i --tty \\ --image=postgres:14 -- \\ psql -h cloudflow-db.internal.company.com -U cloudflow -d cloudflow_production # Review database logs for connection rejections kubectl logs -n cloudflow statefulset/cloudflow-db --tail=100 | \\ grep -i \"connection\\|reject\\|authentication\" ``` ### Maximum Connection Limit (100) Reached This is a hard limit in CloudFlow's",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_49",
              "content": "errors for > 5 minutes - Database completely unavailable - Authentication system down - Data loss or corruption - **Response Time:** Immediate (< 15 minutes) - **Escalation:** Page on-call engineer immediately #### SEV-2: High (P2) - **Definition:** Major functionality impaired affecting multiple users - **Examples:** - Workflow execution success rate < 90% - Significant performance degradation (p95 latency > 10s) - Rate limiting affecting large customer segment - **Response Time:** < 1 hour - **Escalation:**",
              "score": null
            },
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_35",
              "content": "#### Database Connection Pool Exhausted **Symptoms**: \"Too many connections\" errors **Diagnosis**: ```bash # Check PgBouncer stats kubectl exec -it pgbouncer-xxx -n cloudflow-prod -- \\ psql -p 5432 -U pgbouncer pgbouncer -c \"SHOW POOLS;\" ``` **Resolution**: - Increase `max_db_connections` in PgBouncer - Optimize application connection usage - Add more PostgreSQL replicas ### Support Contacts - **On-call Engineer**: pagerduty.com/cloudflow-oncall - **Slack Channel**: #cloudflow-operations - **Documentation**: https://docs.cloudflow.io - **Status Page**: https://status.cloudflow.io --- ## Appendix ### Useful Commands ```bash",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_18",
              "content": "(100/100 connections in use)` - API requests fail with `503 Service Unavailable` - Database CPU usage normal, but connection count at maximum #### Investigation ```bash # Check current connection pool status cloudflow db pool status --detailed # Output example: # Pool Statistics: # Total Connections: 100/100 (100%) # Active: 87 # Idle: 13 # Waiting Requests: 45 # Average Wait Time: 3420ms # Max Wait Time: 8234ms # Identify long-running queries cloudflow db queries --status",
              "score": null
            }
          ],
          "latency_ms": 3.299639000033494,
          "query_id": "realistic_017",
          "dimension": "negation",
          "query": "Why can't I connect directly to the database?"
        },
        {
          "key_facts": [
            "max_attempts: 3",
            "backoff_type: exponential",
            "initial_interval: 1000",
            "max retries: 3"
          ],
          "found_facts": [
            "max_attempts: 3",
            "initial_interval: 1000",
            "max retries: 3"
          ],
          "missed_facts": [
            "backoff_type: exponential"
          ],
          "coverage": 0.75,
          "expected_docs": [
            "troubleshooting_guide",
            "user_guide"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_28",
              "content": "Sequence ``` Attempt 1: Immediate Attempt 2: Wait 1s (2^0 * 1s) Attempt 3: Wait 2s (2^1 * 1s) Attempt 4: Wait 4s (2^2 * 1s) ``` #### Configuration ```bash # View current retry settings cloudflow workflows get wf_9k2n4m8p1q --format json | jq '.retry_policy' # Customize retry behavior cloudflow workflows update wf_9k2n4m8p1q \\ --retry-max-attempts 5 \\ --retry-initial-delay 2000 \\ --retry-backoff-multiplier 2 \\ --retry-max-delay 120000 \\ --retry-on-errors \"NETWORK_ERROR,TIMEOUT,RATE_LIMIT\" # Disable retry for specific step cloudflow workflows",
              "score": null
            },
            {
              "doc_id": "user_guide",
              "chunk_id": "user_guide_fix_33",
              "content": "Click **\"Retry\"** to reprocess with the same input data ### Error Notifications Get notified when workflows fail: ```yaml workflow: notifications: on_failure: - type: email to: \"ops-team@company.com\" - type: slack channel: \"#alerts\" message: \"Workflow {{workflow.name}} failed: {{error.message}}\" on_success_after_retry: - type: slack channel: \"#monitoring\" message: \"Workflow recovered after {{error.attempts}} attempts\" ``` ## Workflow Limits CloudFlow enforces the following limits to ensure platform stability and performance: ### Steps Per Workflow - **Maximum**: 50 steps per workflow - **Recommendation**:",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_27",
              "content": "300 ``` **3. Split workflow into smaller workflows:** ```bash # Create sub-workflows cloudflow workflows create data-pipeline-part1 \\ --steps \"data_ingestion,data_validation\" \\ --timeout 1800 cloudflow workflows create data-pipeline-part2 \\ --steps \"data_transformation,data_export\" \\ --timeout 3600 \\ --trigger workflow_completed \\ --trigger-workflow data-pipeline-part1 ``` ### Retry Logic and Exponential Backoff CloudFlow implements automatic retry with exponential backoff for transient failures: - Max retries: 3 - Initial delay: 1 second - Backoff multiplier: 2 - Max delay: 60 seconds #### Retry",
              "score": null
            },
            {
              "doc_id": "user_guide",
              "chunk_id": "user_guide_fix_31",
              "content": "**Error Object:** Access error details in fallback actions: ``` {{error.message}} # Error message {{error.code}} # Error code {{error.step_id}} # Failed step ID {{error.timestamp}} # When the error occurred {{error.attempts}} # Number of retry attempts ``` ### Dead Letter Queue When all retries and fallbacks fail, CloudFlow can route failed executions to a Dead Letter Queue (DLQ) for manual review: **Enable DLQ:** ```yaml workflow: error_handling: dead_letter_queue: enabled: true retain_days: 30 ``` **DLQ Features:** - View failed",
              "score": null
            },
            {
              "doc_id": "user_guide",
              "chunk_id": "user_guide_fix_28",
              "content": "2026-01-28 14:00:00 UTC ``` ## Error Handling Robust error handling ensures your workflows are resilient and reliable. ### Retry Policies Configure automatic retries for failed actions: ```yaml - id: api_call action: http_request config: url: \"https://api.example.com/data\" retry: max_attempts: 3 backoff_type: \"exponential\" # or \"fixed\", \"linear\" initial_interval: 1000 # milliseconds max_interval: 30000 multiplier: 2.0 retry_on: - timeout - network_error - status: [500, 502, 503, 504] ``` **Backoff Strategies:** - **Fixed**: Wait the same amount of time between",
              "score": null
            }
          ],
          "latency_ms": 3.246860998842749,
          "query_id": "realistic_018",
          "dimension": "original",
          "query": "How do I implement retry logic for failed workflow steps?"
        },
        {
          "key_facts": [
            "max_attempts: 3",
            "backoff_type: exponential",
            "initial_interval: 1000",
            "max retries: 3"
          ],
          "found_facts": [
            "max_attempts: 3",
            "initial_interval: 1000"
          ],
          "missed_facts": [
            "backoff_type: exponential",
            "max retries: 3"
          ],
          "coverage": 0.5,
          "expected_docs": [
            "troubleshooting_guide",
            "user_guide"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "user_guide",
              "chunk_id": "user_guide_fix_29",
              "content": "retries - Attempt 1: Wait 1s - Attempt 2: Wait 1s - Attempt 3: Wait 1s - **Linear**: Increase wait time by a fixed amount - Attempt 1: Wait 1s - Attempt 2: Wait 2s - Attempt 3: Wait 3s - **Exponential**: Double the wait time with each retry (recommended) - Attempt 1: Wait 1s - Attempt 2: Wait 2s - Attempt 3: Wait 4s **Retry Conditions:** Control which errors trigger retries: - `timeout`: Request",
              "score": null
            },
            {
              "doc_id": "user_guide",
              "chunk_id": "user_guide_fix_28",
              "content": "2026-01-28 14:00:00 UTC ``` ## Error Handling Robust error handling ensures your workflows are resilient and reliable. ### Retry Policies Configure automatic retries for failed actions: ```yaml - id: api_call action: http_request config: url: \"https://api.example.com/data\" retry: max_attempts: 3 backoff_type: \"exponential\" # or \"fixed\", \"linear\" initial_interval: 1000 # milliseconds max_interval: 30000 multiplier: 2.0 retry_on: - timeout - network_error - status: [500, 502, 503, 504] ``` **Backoff Strategies:** - **Fixed**: Wait the same amount of time between",
              "score": null
            },
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_22",
              "content": "Waiting {retry_after} seconds...\") time.sleep(retry_after) continue elif response.status_code >= 500: # Server error - retry with exponential backoff wait_time = 2 ** attempt print(f\"Server error. Retrying in {wait_time} seconds...\") time.sleep(wait_time) continue else: # Client error - don't retry error_data = response.json() raise Exception(f\"API Error: {error_data['error']['message']}\") except requests.exceptions.RequestException as e: print(f\"Request failed: {e}\") if attempt < max_retries - 1: time.sleep(2 ** attempt) else: raise raise Exception(\"Max retries exceeded\") ``` ## Webhooks CloudFlow can send webhook notifications when",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_57",
              "content": "minutes (no manual intervention) - Data loss: None (multi-AZ replication) **Scenario 2: Database Primary Failure** - Detection: Health check fails for primary database (< 30 seconds) - Action: Automatic promotion of read replica to primary - Recovery time: 30-60 seconds - Data loss: Minimal (< 1 second due to synchronous replication) **Scenario 3: Full Region Failure** - Detection: Multiple health check failures across all AZs (< 2 minutes) - Action: Manual failover to DR region",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_59",
              "content": "- Procedure: 1. Identify corruption time window 2. Restore from snapshot prior to corruption 3. Replay WAL logs up to corruption point 4. Verify data integrity 5. Resume normal operations - Recovery time: 1-3 hours depending on data volume - Data loss: None if corruption detected quickly ### Testing & Validation **DR Drill Schedule**: - Monthly: Automated failover test (single AZ failure simulation) - Quarterly: Full DR region failover (non-production hours) - Annually: Complete disaster",
              "score": null
            }
          ],
          "latency_ms": 3.026561000297079,
          "query_id": "realistic_018",
          "dimension": "synonym",
          "query": "What's the retry strategy for transient failures?"
        },
        {
          "key_facts": [
            "max_attempts: 3",
            "backoff_type: exponential",
            "initial_interval: 1000",
            "max retries: 3"
          ],
          "found_facts": [
            "max_attempts: 3",
            "initial_interval: 1000"
          ],
          "missed_facts": [
            "backoff_type: exponential",
            "max retries: 3"
          ],
          "coverage": 0.5,
          "expected_docs": [
            "troubleshooting_guide",
            "user_guide"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_29",
              "content": "update wf_9k2n4m8p1q \\ --step data_export \\ --retry-enabled false ``` #### Monitoring Retries ```bash # List failed executions with retry information cloudflow workflows executions list \\ --status FAILED \\ --show-retries \\ --last 7d # Get retry history for specific execution cloudflow workflows executions retries exec_7h3j6k9m2n # Output: # Execution: exec_7h3j6k9m2n # Attempt 1: FAILED - NetworkError: Connection refused (delay: 0ms) # Attempt 2: FAILED - NetworkError: Connection timeout (delay: 1000ms) # Attempt 3: FAILED - NetworkError:",
              "score": null
            },
            {
              "doc_id": "user_guide",
              "chunk_id": "user_guide_fix_33",
              "content": "Click **\"Retry\"** to reprocess with the same input data ### Error Notifications Get notified when workflows fail: ```yaml workflow: notifications: on_failure: - type: email to: \"ops-team@company.com\" - type: slack channel: \"#alerts\" message: \"Workflow {{workflow.name}} failed: {{error.message}}\" on_success_after_retry: - type: slack channel: \"#monitoring\" message: \"Workflow recovered after {{error.attempts}} attempts\" ``` ## Workflow Limits CloudFlow enforces the following limits to ensure platform stability and performance: ### Steps Per Workflow - **Maximum**: 50 steps per workflow - **Recommendation**:",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_28",
              "content": "Sequence ``` Attempt 1: Immediate Attempt 2: Wait 1s (2^0 * 1s) Attempt 3: Wait 2s (2^1 * 1s) Attempt 4: Wait 4s (2^2 * 1s) ``` #### Configuration ```bash # View current retry settings cloudflow workflows get wf_9k2n4m8p1q --format json | jq '.retry_policy' # Customize retry behavior cloudflow workflows update wf_9k2n4m8p1q \\ --retry-max-attempts 5 \\ --retry-initial-delay 2000 \\ --retry-backoff-multiplier 2 \\ --retry-max-delay 120000 \\ --retry-on-errors \"NETWORK_ERROR,TIMEOUT,RATE_LIMIT\" # Disable retry for specific step cloudflow workflows",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_30",
              "content": "Connection timeout (delay: 2000ms) # Attempt 4: FAILED - NetworkError: Connection timeout (delay: 4000ms) # Final Status: FAILED_AFTER_RETRIES ``` ### Workflow Step Failures #### Common Error Patterns **1. Data Validation Errors** ``` ValidationError: Field 'customer_id' is required but missing in 234 records ``` **Resolution:** ```bash # Add data quality checks cloudflow workflows update wf_9k2n4m8p1q \\ --step data_ingestion \\ --add-validator required_fields \\ --validator-config '{\"fields\": [\"customer_id\", \"timestamp\", \"amount\"]}' # Configure error handling cloudflow workflows update wf_9k2n4m8p1q \\",
              "score": null
            },
            {
              "doc_id": "user_guide",
              "chunk_id": "user_guide_fix_28",
              "content": "2026-01-28 14:00:00 UTC ``` ## Error Handling Robust error handling ensures your workflows are resilient and reliable. ### Retry Policies Configure automatic retries for failed actions: ```yaml - id: api_call action: http_request config: url: \"https://api.example.com/data\" retry: max_attempts: 3 backoff_type: \"exponential\" # or \"fixed\", \"linear\" initial_interval: 1000 # milliseconds max_interval: 30000 multiplier: 2.0 retry_on: - timeout - network_error - status: [500, 502, 503, 504] ``` **Backoff Strategies:** - **Fixed**: Wait the same amount of time between",
              "score": null
            }
          ],
          "latency_ms": 3.0242960001487518,
          "query_id": "realistic_018",
          "dimension": "problem",
          "query": "My workflow fails on temporary network errors"
        },
        {
          "key_facts": [
            "max_attempts: 3",
            "backoff_type: exponential",
            "initial_interval: 1000",
            "max retries: 3"
          ],
          "found_facts": [
            "max_attempts: 3",
            "initial_interval: 1000"
          ],
          "missed_facts": [
            "backoff_type: exponential",
            "max retries: 3"
          ],
          "coverage": 0.5,
          "expected_docs": [
            "troubleshooting_guide",
            "user_guide"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "user_guide",
              "chunk_id": "user_guide_fix_28",
              "content": "2026-01-28 14:00:00 UTC ``` ## Error Handling Robust error handling ensures your workflows are resilient and reliable. ### Retry Policies Configure automatic retries for failed actions: ```yaml - id: api_call action: http_request config: url: \"https://api.example.com/data\" retry: max_attempts: 3 backoff_type: \"exponential\" # or \"fixed\", \"linear\" initial_interval: 1000 # milliseconds max_interval: 30000 multiplier: 2.0 retry_on: - timeout - network_error - status: [500, 502, 503, 504] ``` **Backoff Strategies:** - **Fixed**: Wait the same amount of time between",
              "score": null
            },
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_19",
              "content": "resource | | 404 | Not Found | Resource does not exist or has been deleted | | 429 | Too Many Requests | Rate limit exceeded, retry after specified period | | 500 | Internal Server Error | Unexpected server error, contact support if persists | | 502 | Bad Gateway | Temporary service issue, retry with exponential backoff | | 503 | Service Unavailable | Scheduled maintenance or temporary outage | ### Error",
              "score": null
            },
            {
              "doc_id": "user_guide",
              "chunk_id": "user_guide_fix_29",
              "content": "retries - Attempt 1: Wait 1s - Attempt 2: Wait 1s - Attempt 3: Wait 1s - **Linear**: Increase wait time by a fixed amount - Attempt 1: Wait 1s - Attempt 2: Wait 2s - Attempt 3: Wait 3s - **Exponential**: Double the wait time with each retry (recommended) - Attempt 1: Wait 1s - Attempt 2: Wait 2s - Attempt 3: Wait 4s **Retry Conditions:** Control which errors trigger retries: - `timeout`: Request",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_28",
              "content": "Sequence ``` Attempt 1: Immediate Attempt 2: Wait 1s (2^0 * 1s) Attempt 3: Wait 2s (2^1 * 1s) Attempt 4: Wait 4s (2^2 * 1s) ``` #### Configuration ```bash # View current retry settings cloudflow workflows get wf_9k2n4m8p1q --format json | jq '.retry_policy' # Customize retry behavior cloudflow workflows update wf_9k2n4m8p1q \\ --retry-max-attempts 5 \\ --retry-initial-delay 2000 \\ --retry-backoff-multiplier 2 \\ --retry-max-delay 120000 \\ --retry-on-errors \"NETWORK_ERROR,TIMEOUT,RATE_LIMIT\" # Disable retry for specific step cloudflow workflows",
              "score": null
            },
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_22",
              "content": "Waiting {retry_after} seconds...\") time.sleep(retry_after) continue elif response.status_code >= 500: # Server error - retry with exponential backoff wait_time = 2 ** attempt print(f\"Server error. Retrying in {wait_time} seconds...\") time.sleep(wait_time) continue else: # Client error - don't retry error_data = response.json() raise Exception(f\"API Error: {error_data['error']['message']}\") except requests.exceptions.RequestException as e: print(f\"Request failed: {e}\") if attempt < max_retries - 1: time.sleep(2 ** attempt) else: raise raise Exception(\"Max retries exceeded\") ``` ## Webhooks CloudFlow can send webhook notifications when",
              "score": null
            }
          ],
          "latency_ms": 3.2395769994764123,
          "query_id": "realistic_018",
          "dimension": "casual",
          "query": "retry config"
        },
        {
          "key_facts": [
            "max_attempts: 3",
            "backoff_type: exponential",
            "initial_interval: 1000",
            "max retries: 3"
          ],
          "found_facts": [
            "max_attempts: 3",
            "backoff_type: exponential",
            "initial_interval: 1000"
          ],
          "missed_facts": [
            "max retries: 3"
          ],
          "coverage": 0.75,
          "expected_docs": [
            "troubleshooting_guide",
            "user_guide"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "user_guide",
              "chunk_id": "user_guide_fix_28",
              "content": "2026-01-28 14:00:00 UTC ``` ## Error Handling Robust error handling ensures your workflows are resilient and reliable. ### Retry Policies Configure automatic retries for failed actions: ```yaml - id: api_call action: http_request config: url: \"https://api.example.com/data\" retry: max_attempts: 3 backoff_type: \"exponential\" # or \"fixed\", \"linear\" initial_interval: 1000 # milliseconds max_interval: 30000 multiplier: 2.0 retry_on: - timeout - network_error - status: [500, 502, 503, 504] ``` **Backoff Strategies:** - **Fixed**: Wait the same amount of time between",
              "score": null
            },
            {
              "doc_id": "user_guide",
              "chunk_id": "user_guide_fix_38",
              "content": "Handle Errors Gracefully Always implement error handling for external API calls and database operations: ```yaml - id: fetch_data action: http_request config: url: \"https://api.example.com/data\" retry: max_attempts: 3 backoff_type: exponential on_error: - id: log_error action: database_query config: query: \"INSERT INTO error_log (workflow_id, error) VALUES ($1, $2)\" parameters: - \"{{workflow.id}}\" - \"{{error.message}}\" ``` ### 3.",
              "score": null
            },
            {
              "doc_id": "user_guide",
              "chunk_id": "user_guide_fix_33",
              "content": "Click **\"Retry\"** to reprocess with the same input data ### Error Notifications Get notified when workflows fail: ```yaml workflow: notifications: on_failure: - type: email to: \"ops-team@company.com\" - type: slack channel: \"#alerts\" message: \"Workflow {{workflow.name}} failed: {{error.message}}\" on_success_after_retry: - type: slack channel: \"#monitoring\" message: \"Workflow recovered after {{error.attempts}} attempts\" ``` ## Workflow Limits CloudFlow enforces the following limits to ensure platform stability and performance: ### Steps Per Workflow - **Maximum**: 50 steps per workflow - **Recommendation**:",
              "score": null
            },
            {
              "doc_id": "user_guide",
              "chunk_id": "user_guide_fix_31",
              "content": "**Error Object:** Access error details in fallback actions: ``` {{error.message}} # Error message {{error.code}} # Error code {{error.step_id}} # Failed step ID {{error.timestamp}} # When the error occurred {{error.attempts}} # Number of retry attempts ``` ### Dead Letter Queue When all retries and fallbacks fail, CloudFlow can route failed executions to a Dead Letter Queue (DLQ) for manual review: **Enable DLQ:** ```yaml workflow: error_handling: dead_letter_queue: enabled: true retain_days: 30 ``` **DLQ Features:** - View failed",
              "score": null
            },
            {
              "doc_id": "user_guide",
              "chunk_id": "user_guide_fix_29",
              "content": "retries - Attempt 1: Wait 1s - Attempt 2: Wait 1s - Attempt 3: Wait 1s - **Linear**: Increase wait time by a fixed amount - Attempt 1: Wait 1s - Attempt 2: Wait 2s - Attempt 3: Wait 3s - **Exponential**: Double the wait time with each retry (recommended) - Attempt 1: Wait 1s - Attempt 2: Wait 2s - Attempt 3: Wait 4s **Retry Conditions:** Control which errors trigger retries: - `timeout`: Request",
              "score": null
            }
          ],
          "latency_ms": 3.3702820001053624,
          "query_id": "realistic_018",
          "dimension": "contextual",
          "query": "I want workflows to automatically retry on errors, what are the options?"
        },
        {
          "key_facts": [
            "max_attempts: 3",
            "backoff_type: exponential",
            "initial_interval: 1000",
            "max retries: 3"
          ],
          "found_facts": [
            "max_attempts: 3",
            "initial_interval: 1000",
            "max retries: 3"
          ],
          "missed_facts": [
            "backoff_type: exponential"
          ],
          "coverage": 0.75,
          "expected_docs": [
            "troubleshooting_guide",
            "user_guide"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "user_guide",
              "chunk_id": "user_guide_fix_33",
              "content": "Click **\"Retry\"** to reprocess with the same input data ### Error Notifications Get notified when workflows fail: ```yaml workflow: notifications: on_failure: - type: email to: \"ops-team@company.com\" - type: slack channel: \"#alerts\" message: \"Workflow {{workflow.name}} failed: {{error.message}}\" on_success_after_retry: - type: slack channel: \"#monitoring\" message: \"Workflow recovered after {{error.attempts}} attempts\" ``` ## Workflow Limits CloudFlow enforces the following limits to ensure platform stability and performance: ### Steps Per Workflow - **Maximum**: 50 steps per workflow - **Recommendation**:",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_28",
              "content": "Sequence ``` Attempt 1: Immediate Attempt 2: Wait 1s (2^0 * 1s) Attempt 3: Wait 2s (2^1 * 1s) Attempt 4: Wait 4s (2^2 * 1s) ``` #### Configuration ```bash # View current retry settings cloudflow workflows get wf_9k2n4m8p1q --format json | jq '.retry_policy' # Customize retry behavior cloudflow workflows update wf_9k2n4m8p1q \\ --retry-max-attempts 5 \\ --retry-initial-delay 2000 \\ --retry-backoff-multiplier 2 \\ --retry-max-delay 120000 \\ --retry-on-errors \"NETWORK_ERROR,TIMEOUT,RATE_LIMIT\" # Disable retry for specific step cloudflow workflows",
              "score": null
            },
            {
              "doc_id": "user_guide",
              "chunk_id": "user_guide_fix_28",
              "content": "2026-01-28 14:00:00 UTC ``` ## Error Handling Robust error handling ensures your workflows are resilient and reliable. ### Retry Policies Configure automatic retries for failed actions: ```yaml - id: api_call action: http_request config: url: \"https://api.example.com/data\" retry: max_attempts: 3 backoff_type: \"exponential\" # or \"fixed\", \"linear\" initial_interval: 1000 # milliseconds max_interval: 30000 multiplier: 2.0 retry_on: - timeout - network_error - status: [500, 502, 503, 504] ``` **Backoff Strategies:** - **Fixed**: Wait the same amount of time between",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_27",
              "content": "300 ``` **3. Split workflow into smaller workflows:** ```bash # Create sub-workflows cloudflow workflows create data-pipeline-part1 \\ --steps \"data_ingestion,data_validation\" \\ --timeout 1800 cloudflow workflows create data-pipeline-part2 \\ --steps \"data_transformation,data_export\" \\ --timeout 3600 \\ --trigger workflow_completed \\ --trigger-workflow data-pipeline-part1 ``` ### Retry Logic and Exponential Backoff CloudFlow implements automatic retry with exponential backoff for transient failures: - Max retries: 3 - Initial delay: 1 second - Backoff multiplier: 2 - Max delay: 60 seconds #### Retry",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_29",
              "content": "update wf_9k2n4m8p1q \\ --step data_export \\ --retry-enabled false ``` #### Monitoring Retries ```bash # List failed executions with retry information cloudflow workflows executions list \\ --status FAILED \\ --show-retries \\ --last 7d # Get retry history for specific execution cloudflow workflows executions retries exec_7h3j6k9m2n # Output: # Execution: exec_7h3j6k9m2n # Attempt 1: FAILED - NetworkError: Connection refused (delay: 0ms) # Attempt 2: FAILED - NetworkError: Connection timeout (delay: 1000ms) # Attempt 3: FAILED - NetworkError:",
              "score": null
            }
          ],
          "latency_ms": 3.1412349999300204,
          "query_id": "realistic_018",
          "dimension": "negation",
          "query": "Why doesn't my workflow retry after failing?"
        },
        {
          "key_facts": [
            "helm repo add cloudflow https://charts.cloudflow.io"
          ],
          "found_facts": [
            "helm repo add cloudflow https://charts.cloudflow.io"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "deployment_guide"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_9",
              "content": "32)\" \\ --from-literal=SESSION_SECRET=\"$(openssl rand -base64 32)\" ``` ### Helm Chart Deployment Add the CloudFlow Helm repository: ```bash helm repo add cloudflow https://charts.cloudflow.io helm repo update ``` Create a values file for production configuration: ```yaml # values-production.yaml replicaCount: 3 image: repository: 123456789012.dkr.ecr.us-east-1.amazonaws.com/cloudflow tag: \"2.4.0\" pullPolicy: IfNotPresent resources: limits: cpu: 2000m memory: 4Gi requests: cpu: 2000m memory: 4Gi autoscaling: enabled: true minReplicas: 3 maxReplicas: 10 targetCPUUtilizationPercentage: 70 targetMemoryUtilizationPercentage: 80 service: type: ClusterIP port: 80 targetPort: 3000 annotations:",
              "score": null
            },
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_21",
              "content": "JSON template. Key dashboard panels include: 1. **API Performance**: Request rate, P95/P99 latency, error rate 2. **Resource Usage**: CPU, memory, disk I/O per pod 3. **Database Health**: Connection pool utilization, query performance 4. **Worker Status**: Queue depth, processing rate, job success/failure ratio 5. **System Overview**: Pod status, replica count, autoscaling events ### Logging with CloudWatch CloudFlow logs are automatically shipped to CloudWatch Logs.",
              "score": null
            },
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_2",
              "content": "AWS EKS 1.28 - **Region**: us-east-1 (primary), us-west-2 (disaster recovery) - **High Availability**: Multi-AZ deployment across 3 availability zones --- ## Prerequisites Before beginning the deployment process, ensure you have the following: ### Required Tools - `kubectl` (v1.28 or later) - `helm` (v3.12 or later) - `aws-cli` (v2.13 or later) - `eksctl` (v0.165 or later) - `terraform` (v1.6 or later) - for infrastructure provisioning ### Access Requirements - AWS account with appropriate IAM permissions -",
              "score": null
            },
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_14",
              "content": "Deploy PostgreSQL using the Bitnami Helm chart: ```yaml # postgres-values.yaml global: postgresql: auth: username: cloudflow database: cloudflow existingSecret: postgres-credentials image: tag: \"14.10.0\" primary: resources: limits: cpu: 4000m memory: 8Gi requests: cpu: 2000m memory: 4Gi persistence: enabled: true size: 100Gi storageClass: gp3 extendedConfiguration: | max_connections = 100 shared_buffers = 2GB effective_cache_size = 6GB maintenance_work_mem = 512MB checkpoint_completion_target = 0.9 wal_buffers = 16MB default_statistics_target = 100 random_page_cost = 1.1 effective_io_concurrency = 200 work_mem = 10MB min_wal_size =",
              "score": null
            },
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_0",
              "content": "# CloudFlow Platform - Deployment and Operations Guide **Version:** 2.4.0 **Last Updated:** January 2026 **Target Environment:** Production (AWS EKS) ## Table of Contents 1. [Overview](#overview) 2. [Prerequisites](#prerequisites) 3. [Infrastructure Setup](#infrastructure-setup) 4. [Kubernetes Deployment](#kubernetes-deployment) 5. [Database Configuration](#database-configuration) 6. [Monitoring and Observability](#monitoring-and-observability) 7. [Backup and Disaster Recovery](#backup-and-disaster-recovery) 8. [Scaling and Performance](#scaling-and-performance) 9. [Security and Compliance](#security-and-compliance) 10. [Troubleshooting](#troubleshooting) --- ## Overview CloudFlow is a cloud-native workflow orchestration platform designed for high-availability production environments.",
              "score": null
            }
          ],
          "latency_ms": 3.0773759990552207,
          "query_id": "realistic_019",
          "dimension": "original",
          "query": "What Helm chart repository should I use for deployment?"
        },
        {
          "key_facts": [
            "helm repo add cloudflow https://charts.cloudflow.io"
          ],
          "found_facts": [
            "helm repo add cloudflow https://charts.cloudflow.io"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "deployment_guide"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_9",
              "content": "32)\" \\ --from-literal=SESSION_SECRET=\"$(openssl rand -base64 32)\" ``` ### Helm Chart Deployment Add the CloudFlow Helm repository: ```bash helm repo add cloudflow https://charts.cloudflow.io helm repo update ``` Create a values file for production configuration: ```yaml # values-production.yaml replicaCount: 3 image: repository: 123456789012.dkr.ecr.us-east-1.amazonaws.com/cloudflow tag: \"2.4.0\" pullPolicy: IfNotPresent resources: limits: cpu: 2000m memory: 4Gi requests: cpu: 2000m memory: 4Gi autoscaling: enabled: true minReplicas: 3 maxReplicas: 10 targetCPUUtilizationPercentage: 70 targetMemoryUtilizationPercentage: 80 service: type: ClusterIP port: 80 targetPort: 3000 annotations:",
              "score": null
            },
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_24",
              "content": "}' ``` ## Support For additional help and resources: - **Documentation:** https://docs.cloudflow.io - **API Status:** https://status.cloudflow.io - **Support Email:** support@cloudflow.io - **Community Forum:** https://community.cloudflow.io Enterprise customers have access to 24/7 priority support via phone and dedicated Slack channels.",
              "score": null
            },
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_1",
              "content": "This guide provides comprehensive instructions for deploying and operating CloudFlow on Amazon EKS (Elastic Kubernetes Service). ### Architecture Summary CloudFlow consists of the following components: - **API Server**: REST API for workflow management (Node.js/Express) - **Worker Service**: Background job processor (Node.js) - **Scheduler**: Cron-based task scheduler (Node.js) - **PostgreSQL**: Primary data store (version 14) - **Redis**: Cache and message queue (version 7.0) - **PgBouncer**: Database connection pooler ### Deployment Model - **Namespace**: `cloudflow-prod` - **Cluster**:",
              "score": null
            },
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_11",
              "content": "- name: AWS_REGION value: \"us-east-1\" - name: METRICS_ENABLED value: \"true\" envFrom: - secretRef: name: cloudflow-secrets podAnnotations: prometheus.io/scrape: \"true\" prometheus.io/port: \"9090\" prometheus.io/path: \"/metrics\" podSecurityContext: runAsNonRoot: true runAsUser: 1000 fsGroup: 1000 securityContext: allowPrivilegeEscalation: false capabilities: drop: - ALL readOnlyRootFilesystem: true persistence: enabled: true storageClass: gp3 accessMode: ReadWriteOnce size: 50Gi redis: enabled: true architecture: replication auth: enabled: true master: persistence: size: 20Gi replica: replicaCount: 2 persistence: size: 20Gi ``` Deploy CloudFlow using Helm: ```bash helm install cloudflow cloudflow/cloudflow",
              "score": null
            },
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_21",
              "content": "JSON template. Key dashboard panels include: 1. **API Performance**: Request rate, P95/P99 latency, error rate 2. **Resource Usage**: CPU, memory, disk I/O per pod 3. **Database Health**: Connection pool utilization, query performance 4. **Worker Status**: Queue depth, processing rate, job success/failure ratio 5. **System Overview**: Pod status, replica count, autoscaling events ### Logging with CloudWatch CloudFlow logs are automatically shipped to CloudWatch Logs.",
              "score": null
            }
          ],
          "latency_ms": 3.120736000710167,
          "query_id": "realistic_019",
          "dimension": "synonym",
          "query": "Where is the CloudFlow Helm chart hosted?"
        },
        {
          "key_facts": [
            "helm repo add cloudflow https://charts.cloudflow.io"
          ],
          "found_facts": [
            "helm repo add cloudflow https://charts.cloudflow.io"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "deployment_guide"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_9",
              "content": "32)\" \\ --from-literal=SESSION_SECRET=\"$(openssl rand -base64 32)\" ``` ### Helm Chart Deployment Add the CloudFlow Helm repository: ```bash helm repo add cloudflow https://charts.cloudflow.io helm repo update ``` Create a values file for production configuration: ```yaml # values-production.yaml replicaCount: 3 image: repository: 123456789012.dkr.ecr.us-east-1.amazonaws.com/cloudflow tag: \"2.4.0\" pullPolicy: IfNotPresent resources: limits: cpu: 2000m memory: 4Gi requests: cpu: 2000m memory: 4Gi autoscaling: enabled: true minReplicas: 3 maxReplicas: 10 targetCPUUtilizationPercentage: 70 targetMemoryUtilizationPercentage: 80 service: type: ClusterIP port: 80 targetPort: 3000 annotations:",
              "score": null
            },
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_33",
              "content": "namespace level: ```bash kubectl label namespace cloudflow-prod \\ pod-security.kubernetes.io/enforce=restricted \\ pod-security.kubernetes.io/audit=restricted \\ pod-security.kubernetes.io/warn=restricted ``` ### Secrets Management Use AWS Secrets Manager for sensitive credentials: ```bash # Install External Secrets Operator helm install external-secrets external-secrets/external-secrets \\ --namespace external-secrets-system \\ --create-namespace # Create SecretStore kubectl apply -f - <<EOF apiVersion: external-secrets.io/v1beta1 kind: SecretStore metadata: name: aws-secrets-manager namespace: cloudflow-prod spec: provider: aws: service: SecretsManager region: us-east-1 auth: jwt: serviceAccountRef: name: external-secrets-sa EOF ``` --- ## Troubleshooting ### Common",
              "score": null
            },
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_11",
              "content": "- name: AWS_REGION value: \"us-east-1\" - name: METRICS_ENABLED value: \"true\" envFrom: - secretRef: name: cloudflow-secrets podAnnotations: prometheus.io/scrape: \"true\" prometheus.io/port: \"9090\" prometheus.io/path: \"/metrics\" podSecurityContext: runAsNonRoot: true runAsUser: 1000 fsGroup: 1000 securityContext: allowPrivilegeEscalation: false capabilities: drop: - ALL readOnlyRootFilesystem: true persistence: enabled: true storageClass: gp3 accessMode: ReadWriteOnce size: 50Gi redis: enabled: true architecture: replication auth: enabled: true master: persistence: size: 20Gi replica: replicaCount: 2 persistence: size: 20Gi ``` Deploy CloudFlow using Helm: ```bash helm install cloudflow cloudflow/cloudflow",
              "score": null
            },
            {
              "doc_id": "user_guide",
              "chunk_id": "user_guide_fix_12",
              "content": "### HTTP Requests Make HTTP requests to any API endpoint: **Configuration:** - **Method**: GET, POST, PUT, PATCH, DELETE, HEAD - **URL**: Full endpoint URL (supports variable interpolation) - **Headers**: Custom headers as key-value pairs - **Query Parameters**: URL parameters - **Body**: JSON, form data, or raw text - **Authentication**: Basic Auth, Bearer Token, API Key, OAuth 2.0 **Example:** ```yaml - id: fetch_user action: http_request config: method: GET url: \"https://api.example.com/users/{{user_id}}\" headers: Authorization: \"Bearer {{secrets.API_TOKEN}}\" Content-Type: \"application/json\"",
              "score": null
            },
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_10",
              "content": "service.beta.kubernetes.io/aws-load-balancer-type: \"nlb\" ingress: enabled: true className: nginx annotations: cert-manager.io/cluster-issuer: \"letsencrypt-prod\" nginx.ingress.kubernetes.io/ssl-redirect: \"true\" hosts: - host: api.cloudflow.io paths: - path: / pathType: Prefix tls: - secretName: cloudflow-tls hosts: - api.cloudflow.io healthCheck: enabled: true livenessProbe: path: /health initialDelaySeconds: 30 periodSeconds: 10 timeoutSeconds: 5 failureThreshold: 3 readinessProbe: path: /ready initialDelaySeconds: 10 periodSeconds: 5 timeoutSeconds: 3 failureThreshold: 3 env: - name: NODE_ENV value: \"production\" - name: LOG_LEVEL value: \"info\" - name: API_PORT value: \"3000\" - name: WORKER_CONCURRENCY value: \"10\"",
              "score": null
            }
          ],
          "latency_ms": 3.287105999334017,
          "query_id": "realistic_019",
          "dimension": "problem",
          "query": "helm repo add is failing, what's the correct URL?"
        },
        {
          "key_facts": [
            "helm repo add cloudflow https://charts.cloudflow.io"
          ],
          "found_facts": [
            "helm repo add cloudflow https://charts.cloudflow.io"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "deployment_guide"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_9",
              "content": "32)\" \\ --from-literal=SESSION_SECRET=\"$(openssl rand -base64 32)\" ``` ### Helm Chart Deployment Add the CloudFlow Helm repository: ```bash helm repo add cloudflow https://charts.cloudflow.io helm repo update ``` Create a values file for production configuration: ```yaml # values-production.yaml replicaCount: 3 image: repository: 123456789012.dkr.ecr.us-east-1.amazonaws.com/cloudflow tag: \"2.4.0\" pullPolicy: IfNotPresent resources: limits: cpu: 2000m memory: 4Gi requests: cpu: 2000m memory: 4Gi autoscaling: enabled: true minReplicas: 3 maxReplicas: 10 targetCPUUtilizationPercentage: 70 targetMemoryUtilizationPercentage: 80 service: type: ClusterIP port: 80 targetPort: 3000 annotations:",
              "score": null
            },
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_33",
              "content": "namespace level: ```bash kubectl label namespace cloudflow-prod \\ pod-security.kubernetes.io/enforce=restricted \\ pod-security.kubernetes.io/audit=restricted \\ pod-security.kubernetes.io/warn=restricted ``` ### Secrets Management Use AWS Secrets Manager for sensitive credentials: ```bash # Install External Secrets Operator helm install external-secrets external-secrets/external-secrets \\ --namespace external-secrets-system \\ --create-namespace # Create SecretStore kubectl apply -f - <<EOF apiVersion: external-secrets.io/v1beta1 kind: SecretStore metadata: name: aws-secrets-manager namespace: cloudflow-prod spec: provider: aws: service: SecretsManager region: us-east-1 auth: jwt: serviceAccountRef: name: external-secrets-sa EOF ``` --- ## Troubleshooting ### Common",
              "score": null
            },
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_11",
              "content": "- name: AWS_REGION value: \"us-east-1\" - name: METRICS_ENABLED value: \"true\" envFrom: - secretRef: name: cloudflow-secrets podAnnotations: prometheus.io/scrape: \"true\" prometheus.io/port: \"9090\" prometheus.io/path: \"/metrics\" podSecurityContext: runAsNonRoot: true runAsUser: 1000 fsGroup: 1000 securityContext: allowPrivilegeEscalation: false capabilities: drop: - ALL readOnlyRootFilesystem: true persistence: enabled: true storageClass: gp3 accessMode: ReadWriteOnce size: 50Gi redis: enabled: true architecture: replication auth: enabled: true master: persistence: size: 20Gi replica: replicaCount: 2 persistence: size: 20Gi ``` Deploy CloudFlow using Helm: ```bash helm install cloudflow cloudflow/cloudflow",
              "score": null
            },
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_15",
              "content": "1GB max_wal_size = 4GB readReplicas: replicaCount: 2 resources: limits: cpu: 2000m memory: 4Gi requests: cpu: 1000m memory: 2Gi metrics: enabled: true serviceMonitor: enabled: true ``` Install PostgreSQL: ```bash # Create password secret kubectl create secret generic postgres-credentials \\ --namespace cloudflow-prod \\ --from-literal=postgres-password=\"$(openssl rand -base64 32)\" \\ --from-literal=password=\"$(openssl rand -base64 32)\" # Install PostgreSQL helm install postgresql bitnami/postgresql \\ --namespace cloudflow-prod \\ --values postgres-values.yaml \\ --wait ``` ### PgBouncer Configuration Deploy PgBouncer for connection pooling to",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_9",
              "content": "Claims: user_id, email, roles[], permissions[], tenant_id **Security Features**: - Password hashing: Argon2id with 64MB memory, 4 iterations - Token rotation on refresh to prevent replay attacks - Brute force protection: 5 failed attempts \u2192 15-minute lockout - Secrets stored in HashiCorp Vault, rotated every 90 days **Performance Targets**: - Token validation: < 10ms (P99) - Token generation: < 50ms (P99) - Throughput: 5,000 RPS for validation operations --- ### Workflow Engine **Purpose**: Core orchestration service",
              "score": null
            }
          ],
          "latency_ms": 3.031379999811179,
          "query_id": "realistic_019",
          "dimension": "casual",
          "query": "helm repo"
        },
        {
          "key_facts": [
            "helm repo add cloudflow https://charts.cloudflow.io"
          ],
          "found_facts": [
            "helm repo add cloudflow https://charts.cloudflow.io"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "deployment_guide"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_9",
              "content": "32)\" \\ --from-literal=SESSION_SECRET=\"$(openssl rand -base64 32)\" ``` ### Helm Chart Deployment Add the CloudFlow Helm repository: ```bash helm repo add cloudflow https://charts.cloudflow.io helm repo update ``` Create a values file for production configuration: ```yaml # values-production.yaml replicaCount: 3 image: repository: 123456789012.dkr.ecr.us-east-1.amazonaws.com/cloudflow tag: \"2.4.0\" pullPolicy: IfNotPresent resources: limits: cpu: 2000m memory: 4Gi requests: cpu: 2000m memory: 4Gi autoscaling: enabled: true minReplicas: 3 maxReplicas: 10 targetCPUUtilizationPercentage: 70 targetMemoryUtilizationPercentage: 80 service: type: ClusterIP port: 80 targetPort: 3000 annotations:",
              "score": null
            },
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_1",
              "content": "This guide provides comprehensive instructions for deploying and operating CloudFlow on Amazon EKS (Elastic Kubernetes Service). ### Architecture Summary CloudFlow consists of the following components: - **API Server**: REST API for workflow management (Node.js/Express) - **Worker Service**: Background job processor (Node.js) - **Scheduler**: Cron-based task scheduler (Node.js) - **PostgreSQL**: Primary data store (version 14) - **Redis**: Cache and message queue (version 7.0) - **PgBouncer**: Database connection pooler ### Deployment Model - **Namespace**: `cloudflow-prod` - **Cluster**:",
              "score": null
            },
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_0",
              "content": "# CloudFlow Platform - Deployment and Operations Guide **Version:** 2.4.0 **Last Updated:** January 2026 **Target Environment:** Production (AWS EKS) ## Table of Contents 1. [Overview](#overview) 2. [Prerequisites](#prerequisites) 3. [Infrastructure Setup](#infrastructure-setup) 4. [Kubernetes Deployment](#kubernetes-deployment) 5. [Database Configuration](#database-configuration) 6. [Monitoring and Observability](#monitoring-and-observability) 7. [Backup and Disaster Recovery](#backup-and-disaster-recovery) 8. [Scaling and Performance](#scaling-and-performance) 9. [Security and Compliance](#security-and-compliance) 10. [Troubleshooting](#troubleshooting) --- ## Overview CloudFlow is a cloud-native workflow orchestration platform designed for high-availability production environments.",
              "score": null
            },
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_36",
              "content": "# View all resources in namespace kubectl get all -n cloudflow-prod # Check resource usage kubectl top pods -n cloudflow-prod kubectl top nodes # View logs kubectl logs -f deployment/cloudflow-api -n cloudflow-prod # Execute command in pod kubectl exec -it <pod-name> -n cloudflow-prod -- /bin/sh # Port forward for local testing kubectl port-forward svc/cloudflow-api 8080:80 -n cloudflow-prod # Rollback deployment helm rollback cloudflow -n cloudflow-prod # Update deployment helm upgrade cloudflow cloudflow/cloudflow \\ --namespace cloudflow-prod",
              "score": null
            },
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_19",
              "content": "\\ --namespace monitoring \\ --create-namespace \\ --set prometheus.prometheusSpec.serviceMonitorSelectorNilUsesHelmValues=false \\ --set prometheus.prometheusSpec.retention=30d \\ --set prometheus.prometheusSpec.storageSpec.volumeClaimTemplate.spec.resources.requests.storage=100Gi ``` CloudFlow exposes Prometheus metrics on port 9090 at the `/metrics` endpoint. The metrics include: - **HTTP Metrics**: Request rate, latency, error rate - **Database Metrics**: Connection pool status, query duration - **Worker Metrics**: Job queue size, processing time, failure rate - **System Metrics**: CPU usage, memory usage, heap statistics Example metrics exposed: ``` # HELP cloudflow_http_requests_total Total number of HTTP",
              "score": null
            }
          ],
          "latency_ms": 3.1260359992302256,
          "query_id": "realistic_019",
          "dimension": "contextual",
          "query": "Setting up deployment pipeline, which Helm repository has CloudFlow charts?"
        },
        {
          "key_facts": [
            "helm repo add cloudflow https://charts.cloudflow.io"
          ],
          "found_facts": [
            "helm repo add cloudflow https://charts.cloudflow.io"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "deployment_guide"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_9",
              "content": "32)\" \\ --from-literal=SESSION_SECRET=\"$(openssl rand -base64 32)\" ``` ### Helm Chart Deployment Add the CloudFlow Helm repository: ```bash helm repo add cloudflow https://charts.cloudflow.io helm repo update ``` Create a values file for production configuration: ```yaml # values-production.yaml replicaCount: 3 image: repository: 123456789012.dkr.ecr.us-east-1.amazonaws.com/cloudflow tag: \"2.4.0\" pullPolicy: IfNotPresent resources: limits: cpu: 2000m memory: 4Gi requests: cpu: 2000m memory: 4Gi autoscaling: enabled: true minReplicas: 3 maxReplicas: 10 targetCPUUtilizationPercentage: 70 targetMemoryUtilizationPercentage: 80 service: type: ClusterIP port: 80 targetPort: 3000 annotations:",
              "score": null
            },
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_24",
              "content": "}' ``` ## Support For additional help and resources: - **Documentation:** https://docs.cloudflow.io - **API Status:** https://status.cloudflow.io - **Support Email:** support@cloudflow.io - **Community Forum:** https://community.cloudflow.io Enterprise customers have access to 24/7 priority support via phone and dedicated Slack channels.",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_56",
              "content": "Lessons learned ``` --- ## Additional Resources - **CloudFlow Documentation:** https://docs.cloudflow.io - **API Reference:** https://api-docs.cloudflow.io - **Community Forum:** https://community.cloudflow.io - **GitHub Issues:** https://github.com/cloudflow/platform/issues - **Training Portal:** https://training.cloudflow.io - **Monitoring Dashboard:** https://monitoring.cloudflow.io ### Getting Help If this troubleshooting guide doesn't resolve your issue: 1. Search the knowledge base: `cloudflow kb search \"your issue\"` 2. Check community forum for similar issues 3. Contact support with detailed logs and reproduction steps 4.",
              "score": null
            },
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_4",
              "content": "`eksctl` configuration to create the EKS cluster: ```yaml # cluster-config.yaml apiVersion: eksctl.io/v1alpha5 kind: ClusterConfig metadata: name: cloudflow-production region: us-east-1 version: \"1.28\" availabilityZones: - us-east-1a - us-east-1b - us-east-1c vpc: cidr: 10.0.0.0/16 nat: gateway: HighlyAvailable managedNodeGroups: - name: cloudflow-workers instanceType: m5.xlarge minSize: 3 maxSize: 10 desiredCapacity: 5 volumeSize: 100 privateNetworking: true labels: role: worker environment: production tags: Environment: production Application: cloudflow iam: withAddonPolicies: ebs: true efs: true albIngress: true cloudWatch: true cloudWatch: clusterLogging: enableTypes: - api",
              "score": null
            },
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_11",
              "content": "- name: AWS_REGION value: \"us-east-1\" - name: METRICS_ENABLED value: \"true\" envFrom: - secretRef: name: cloudflow-secrets podAnnotations: prometheus.io/scrape: \"true\" prometheus.io/port: \"9090\" prometheus.io/path: \"/metrics\" podSecurityContext: runAsNonRoot: true runAsUser: 1000 fsGroup: 1000 securityContext: allowPrivilegeEscalation: false capabilities: drop: - ALL readOnlyRootFilesystem: true persistence: enabled: true storageClass: gp3 accessMode: ReadWriteOnce size: 50Gi redis: enabled: true architecture: replication auth: enabled: true master: persistence: size: 20Gi replica: replicaCount: 2 persistence: size: 20Gi ``` Deploy CloudFlow using Helm: ```bash helm install cloudflow cloudflow/cloudflow",
              "score": null
            }
          ],
          "latency_ms": 3.1592380000802223,
          "query_id": "realistic_019",
          "dimension": "negation",
          "query": "Why can't I find CloudFlow in the official Helm hub?"
        },
        {
          "key_facts": [
            "minimum scheduling interval is 1 minute",
            "The minimum scheduling interval is **1 minute**"
          ],
          "found_facts": [
            "The minimum scheduling interval is **1 minute**"
          ],
          "missed_facts": [
            "minimum scheduling interval is 1 minute"
          ],
          "coverage": 0.5,
          "expected_docs": [
            "user_guide"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "user_guide",
              "chunk_id": "user_guide_fix_24",
              "content": "hour from 9 AM to 5 PM, Monday-Friday | | `*/15 9-17 * * 1-5` | Every 15 minutes during business hours | **Important:** The minimum scheduling interval is **1 minute**. Expressions that evaluate to more frequent executions will be rejected. ### Timezone Handling All scheduled workflows run in **UTC by default**.",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_13",
              "content": "< 2 seconds (P99) - Complex workflow (20+ steps): < 5 seconds (P99) - Throughput: 500 concurrent workflow executions per pod --- ### Scheduler Service **Purpose**: Time-based workflow triggering system supporting cron-like schedules and one-time delayed executions. **Technology**: Go with distributed locking via Redis **Replicas**: 4 pods (production), active-passive with leader election **Resource Allocation**: 2 vCPU, 4GB RAM per pod **Key Responsibilities**: - Parse and validate cron expressions (extended format supporting seconds) - Maintain schedule",
              "score": null
            },
            {
              "doc_id": "user_guide",
              "chunk_id": "user_guide_fix_0",
              "content": "# CloudFlow User Guide Welcome to CloudFlow, the modern workflow automation platform that helps you connect your apps, automate repetitive tasks, and build powerful integrations without writing code. ## Table of Contents 1. [Getting Started](#getting-started) 2. [Workflow Creation](#workflow-creation) 3. [Available Actions](#available-actions) 4. [Variables and Expressions](#variables-and-expressions) 5. [Scheduling](#scheduling) 6. [Error Handling](#error-handling) 7. [Workflow Limits](#workflow-limits) 8. [Best Practices](#best-practices) 9.",
              "score": null
            },
            {
              "doc_id": "user_guide",
              "chunk_id": "user_guide_fix_25",
              "content": "To account for your local timezone: **Option 1: Convert to UTC** If you want a workflow to run at 9:00 AM EST (UTC-5), schedule it for 14:00 UTC: ``` 0 14 * * * # 9:00 AM EST = 14:00 UTC ``` **Option 2: Use Timezone Configuration** Specify a timezone in your workflow configuration: ```yaml schedule: cron: \"0 9 * * *\" timezone: \"America/New_York\" # IANA timezone identifier ``` **Supported Timezones:** CloudFlow supports all IANA",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_47",
              "content": "Operations** (P99 latency): - `GET /workflows/{id}`: < 50ms (cache hit), < 150ms (cache miss) - `POST /workflows`: < 200ms (includes validation and database write) - `POST /workflows/{id}/execute`: < 100ms (async, returns execution ID) - `GET /workflows/{id}/history`: < 300ms (paginated, 50 records per page) **Workflow Execution** (P99 latency): - Simple workflow (< 5 steps): < 2 seconds - Medium workflow (5-15 steps): < 5 seconds - Complex workflow (> 15 steps): < 15 seconds - Parallel",
              "score": null
            }
          ],
          "latency_ms": 3.366103999724146,
          "query_id": "realistic_020",
          "dimension": "original",
          "query": "What is the minimum scheduling interval for workflows?"
        },
        {
          "key_facts": [
            "minimum scheduling interval is 1 minute",
            "The minimum scheduling interval is **1 minute**"
          ],
          "found_facts": [
            "The minimum scheduling interval is **1 minute**"
          ],
          "missed_facts": [
            "minimum scheduling interval is 1 minute"
          ],
          "coverage": 0.5,
          "expected_docs": [
            "user_guide"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "user_guide",
              "chunk_id": "user_guide_fix_24",
              "content": "hour from 9 AM to 5 PM, Monday-Friday | | `*/15 9-17 * * 1-5` | Every 15 minutes during business hours | **Important:** The minimum scheduling interval is **1 minute**. Expressions that evaluate to more frequent executions will be rejected. ### Timezone Handling All scheduled workflows run in **UTC by default**.",
              "score": null
            },
            {
              "doc_id": "user_guide",
              "chunk_id": "user_guide_fix_0",
              "content": "# CloudFlow User Guide Welcome to CloudFlow, the modern workflow automation platform that helps you connect your apps, automate repetitive tasks, and build powerful integrations without writing code. ## Table of Contents 1. [Getting Started](#getting-started) 2. [Workflow Creation](#workflow-creation) 3. [Available Actions](#available-actions) 4. [Variables and Expressions](#variables-and-expressions) 5. [Scheduling](#scheduling) 6. [Error Handling](#error-handling) 7. [Workflow Limits](#workflow-limits) 8. [Best Practices](#best-practices) 9.",
              "score": null
            },
            {
              "doc_id": "user_guide",
              "chunk_id": "user_guide_fix_22",
              "content": "### Cron Syntax Use standard cron expressions to define schedules: ``` * * * * * \u252c \u252c \u252c \u252c \u252c \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2514\u2500\u2500\u2500 Day of Week (0-6, Sunday=0) \u2502 \u2502 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Month (1-12) \u2502 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Day of Month (1-31) \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Hour (0-23) \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Minute (0-59) ``` **Common Cron Patterns:** | Pattern | Description | |---------|-------------| | `*/5 * * * *` | Every 5",
              "score": null
            },
            {
              "doc_id": "user_guide",
              "chunk_id": "user_guide_fix_35",
              "content": "per day (per workflow) - **Rate Limiting**: 100 concurrent executions per workflow - **Burst Limit**: 10 executions per second **What happens when limits are reached:** - New executions are queued automatically - Webhook triggers return HTTP 429 (Too Many Requests) - Scheduled executions are skipped (logged in audit trail) - Email notifications sent to workflow owner **Monitoring Usage:** View real-time metrics in your workflow dashboard: - Executions today: 847 / 1000 - Average duration: 12.3s",
              "score": null
            },
            {
              "doc_id": "user_guide",
              "chunk_id": "user_guide_fix_25",
              "content": "To account for your local timezone: **Option 1: Convert to UTC** If you want a workflow to run at 9:00 AM EST (UTC-5), schedule it for 14:00 UTC: ``` 0 14 * * * # 9:00 AM EST = 14:00 UTC ``` **Option 2: Use Timezone Configuration** Specify a timezone in your workflow configuration: ```yaml schedule: cron: \"0 9 * * *\" timezone: \"America/New_York\" # IANA timezone identifier ``` **Supported Timezones:** CloudFlow supports all IANA",
              "score": null
            }
          ],
          "latency_ms": 3.187120000802679,
          "query_id": "realistic_020",
          "dimension": "synonym",
          "query": "How frequently can I schedule a workflow?"
        },
        {
          "key_facts": [
            "minimum scheduling interval is 1 minute",
            "The minimum scheduling interval is **1 minute**"
          ],
          "found_facts": [
            "The minimum scheduling interval is **1 minute**"
          ],
          "missed_facts": [
            "minimum scheduling interval is 1 minute"
          ],
          "coverage": 0.5,
          "expected_docs": [
            "user_guide"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "user_guide",
              "chunk_id": "user_guide_fix_24",
              "content": "hour from 9 AM to 5 PM, Monday-Friday | | `*/15 9-17 * * 1-5` | Every 15 minutes during business hours | **Important:** The minimum scheduling interval is **1 minute**. Expressions that evaluate to more frequent executions will be rejected. ### Timezone Handling All scheduled workflows run in **UTC by default**.",
              "score": null
            },
            {
              "doc_id": "user_guide",
              "chunk_id": "user_guide_fix_22",
              "content": "### Cron Syntax Use standard cron expressions to define schedules: ``` * * * * * \u252c \u252c \u252c \u252c \u252c \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2514\u2500\u2500\u2500 Day of Week (0-6, Sunday=0) \u2502 \u2502 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Month (1-12) \u2502 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Day of Month (1-31) \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Hour (0-23) \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Minute (0-59) ``` **Common Cron Patterns:** | Pattern | Description | |---------|-------------| | `*/5 * * * *` | Every 5",
              "score": null
            },
            {
              "doc_id": "user_guide",
              "chunk_id": "user_guide_fix_27",
              "content": "Click the **\"Trigger\"** section 3. Select **\"Schedule\"** as the trigger type 4. Enter your cron expression or use the visual schedule builder 5. Select your timezone 6. Save and activate **Testing Schedules:** Use the built-in schedule calculator to preview upcoming executions: ``` Next 5 executions: 1. 2026-01-24 14:00:00 UTC 2. 2026-01-25 14:00:00 UTC 3. 2026-01-26 14:00:00 UTC 4. 2026-01-27 14:00:00 UTC 5.",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_14",
              "content": "registry in PostgreSQL - Distributed scheduling with leader election (one active scheduler) - Missed execution handling with configurable catch-up policy - Schedule conflict detection and resolution - Time zone support for international schedules **Scheduling Architecture**: ``` PostgreSQL Schedules Table \u2502 \u25bc Leader Scheduler (elected via Redis) \u2502 \u251c\u2500 Scan for due schedules (every 10 seconds) \u251c\u2500 Acquire lock per schedule (prevents duplicates) \u251c\u2500 Publish to Kafka \u2192 workflow.events topic \u2514\u2500 Update last_run timestamp ``` **Schedule",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_15",
              "content": "Types**: - **Cron-based**: `0 */5 * * * *` (every 5 minutes) - **One-time**: Specific timestamp for delayed execution - **Interval-based**: Every N seconds/minutes/hours **Reliability Features**: - Leader election using Redis with 30-second lease - Heartbeat mechanism to detect leader failure (5-second interval) - Automatic failover to standby scheduler (< 10 seconds) - Schedule versioning to handle updates during execution - Missed execution policy: SKIP, RUN_ONCE, or RUN_ALL **Performance Targets**: - Schedule evaluation: < 100ms",
              "score": null
            }
          ],
          "latency_ms": 3.0175640004017623,
          "query_id": "realistic_020",
          "dimension": "problem",
          "query": "My every-30-seconds schedule is being rejected"
        },
        {
          "key_facts": [
            "minimum scheduling interval is 1 minute",
            "The minimum scheduling interval is **1 minute**"
          ],
          "found_facts": [
            "The minimum scheduling interval is **1 minute**"
          ],
          "missed_facts": [
            "minimum scheduling interval is 1 minute"
          ],
          "coverage": 0.5,
          "expected_docs": [
            "user_guide"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "user_guide",
              "chunk_id": "user_guide_fix_24",
              "content": "hour from 9 AM to 5 PM, Monday-Friday | | `*/15 9-17 * * 1-5` | Every 15 minutes during business hours | **Important:** The minimum scheduling interval is **1 minute**. Expressions that evaluate to more frequent executions will be rejected. ### Timezone Handling All scheduled workflows run in **UTC by default**.",
              "score": null
            },
            {
              "doc_id": "user_guide",
              "chunk_id": "user_guide_fix_22",
              "content": "### Cron Syntax Use standard cron expressions to define schedules: ``` * * * * * \u252c \u252c \u252c \u252c \u252c \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2514\u2500\u2500\u2500 Day of Week (0-6, Sunday=0) \u2502 \u2502 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Month (1-12) \u2502 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Day of Month (1-31) \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Hour (0-23) \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Minute (0-59) ``` **Common Cron Patterns:** | Pattern | Description | |---------|-------------| | `*/5 * * * *` | Every 5",
              "score": null
            },
            {
              "doc_id": "user_guide",
              "chunk_id": "user_guide_fix_27",
              "content": "Click the **\"Trigger\"** section 3. Select **\"Schedule\"** as the trigger type 4. Enter your cron expression or use the visual schedule builder 5. Select your timezone 6. Save and activate **Testing Schedules:** Use the built-in schedule calculator to preview upcoming executions: ``` Next 5 executions: 1. 2026-01-24 14:00:00 UTC 2. 2026-01-25 14:00:00 UTC 3. 2026-01-26 14:00:00 UTC 4. 2026-01-27 14:00:00 UTC 5.",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_14",
              "content": "registry in PostgreSQL - Distributed scheduling with leader election (one active scheduler) - Missed execution handling with configurable catch-up policy - Schedule conflict detection and resolution - Time zone support for international schedules **Scheduling Architecture**: ``` PostgreSQL Schedules Table \u2502 \u25bc Leader Scheduler (elected via Redis) \u2502 \u251c\u2500 Scan for due schedules (every 10 seconds) \u251c\u2500 Acquire lock per schedule (prevents duplicates) \u251c\u2500 Publish to Kafka \u2192 workflow.events topic \u2514\u2500 Update last_run timestamp ``` **Schedule",
              "score": null
            },
            {
              "doc_id": "user_guide",
              "chunk_id": "user_guide_fix_23",
              "content": "minutes | | `0 * * * *` | Every hour at minute 0 | | `0 9 * * *` | Daily at 9:00 AM | | `0 9 * * 1` | Every Monday at 9:00 AM | | `0 0 1 * *` | First day of every month at midnight | | `0 0 * * 0` | Every Sunday at midnight | | `0 9-17 * * 1-5` | Every",
              "score": null
            }
          ],
          "latency_ms": 2.992556999743101,
          "query_id": "realistic_020",
          "dimension": "casual",
          "query": "min schedule interval"
        },
        {
          "key_facts": [
            "minimum scheduling interval is 1 minute",
            "The minimum scheduling interval is **1 minute**"
          ],
          "found_facts": [],
          "missed_facts": [
            "minimum scheduling interval is 1 minute",
            "The minimum scheduling interval is **1 minute**"
          ],
          "coverage": 0.0,
          "expected_docs": [
            "user_guide"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_13",
              "content": "< 2 seconds (P99) - Complex workflow (20+ steps): < 5 seconds (P99) - Throughput: 500 concurrent workflow executions per pod --- ### Scheduler Service **Purpose**: Time-based workflow triggering system supporting cron-like schedules and one-time delayed executions. **Technology**: Go with distributed locking via Redis **Replicas**: 4 pods (production), active-passive with leader election **Resource Allocation**: 2 vCPU, 4GB RAM per pod **Key Responsibilities**: - Parse and validate cron expressions (extended format supporting seconds) - Maintain schedule",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_48",
              "content": "execution: Linear scaling up to 10 concurrent branches **Database Operations** (P99 latency): - Simple SELECT: < 5ms - JOIN query: < 20ms - INSERT/UPDATE: < 10ms - Batch operations: < 100ms (batch size: 100 records) **Cache Operations** (P99 latency): - Redis GET: < 2ms - Redis SET: < 3ms - Redis DEL: < 2ms ### Throughput Capacity **API Gateway**: - Sustained: 10,000 requests per second - Peak: 25,000 requests per second (5-minute burst) - Rate",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_15",
              "content": "Types**: - **Cron-based**: `0 */5 * * * *` (every 5 minutes) - **One-time**: Specific timestamp for delayed execution - **Interval-based**: Every N seconds/minutes/hours **Reliability Features**: - Leader election using Redis with 30-second lease - Heartbeat mechanism to detect leader failure (5-second interval) - Automatic failover to standby scheduler (< 10 seconds) - Schedule versioning to handle updates during execution - Missed execution policy: SKIP, RUN_ONCE, or RUN_ALL **Performance Targets**: - Schedule evaluation: < 100ms",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_47",
              "content": "Operations** (P99 latency): - `GET /workflows/{id}`: < 50ms (cache hit), < 150ms (cache miss) - `POST /workflows`: < 200ms (includes validation and database write) - `POST /workflows/{id}/execute`: < 100ms (async, returns execution ID) - `GET /workflows/{id}/history`: < 300ms (paginated, 50 records per page) **Workflow Execution** (P99 latency): - Simple workflow (< 5 steps): < 2 seconds - Medium workflow (5-15 steps): < 5 seconds - Complex workflow (> 15 steps): < 15 seconds - Parallel",
              "score": null
            },
            {
              "doc_id": "user_guide",
              "chunk_id": "user_guide_fix_27",
              "content": "Click the **\"Trigger\"** section 3. Select **\"Schedule\"** as the trigger type 4. Enter your cron expression or use the visual schedule builder 5. Select your timezone 6. Save and activate **Testing Schedules:** Use the built-in schedule calculator to preview upcoming executions: ``` Next 5 executions: 1. 2026-01-24 14:00:00 UTC 2. 2026-01-25 14:00:00 UTC 3. 2026-01-26 14:00:00 UTC 4. 2026-01-27 14:00:00 UTC 5.",
              "score": null
            }
          ],
          "latency_ms": 3.1339010001829593,
          "query_id": "realistic_020",
          "dimension": "contextual",
          "query": "I need near real-time execution, what's the fastest schedule I can set?"
        },
        {
          "key_facts": [
            "minimum scheduling interval is 1 minute",
            "The minimum scheduling interval is **1 minute**"
          ],
          "found_facts": [
            "The minimum scheduling interval is **1 minute**"
          ],
          "missed_facts": [
            "minimum scheduling interval is 1 minute"
          ],
          "coverage": 0.5,
          "expected_docs": [
            "user_guide"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "user_guide",
              "chunk_id": "user_guide_fix_24",
              "content": "hour from 9 AM to 5 PM, Monday-Friday | | `*/15 9-17 * * 1-5` | Every 15 minutes during business hours | **Important:** The minimum scheduling interval is **1 minute**. Expressions that evaluate to more frequent executions will be rejected. ### Timezone Handling All scheduled workflows run in **UTC by default**.",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_36",
              "content": "remaining\" local reset=$(curl -s -I https://api.cloudflow.io/api/v1/workflows \\ -H \"Authorization: Bearer $CF_ACCESS_TOKEN\" | \\ grep -i \"x-ratelimit-reset\" | \\ awk '{print $2}' | tr -d '\\r') local wait_time=$((reset - $(date +%s))) echo \"Rate limit resets in $wait_time seconds\" sleep $wait_time fi } # Use before API calls check_rate_limit cloudflow workflows execute wf_9k2n4m8p1q ``` #### Optimization Strategies **1.",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_35",
              "content": "Waiting {retry_after} seconds...\") time.sleep(retry_after) continue remaining = int(response.headers.get('X-RateLimit-Remaining', 0)) if remaining < 10: print(f\"Warning: Only {remaining} requests remaining\") return response raise Exception(\"Max retries exceeded due to rate limiting\") ``` **Bash script with rate limit checking:** ```bash #!/bin/bash check_rate_limit() { local remaining=$(curl -s -I https://api.cloudflow.io/api/v1/workflows \\ -H \"Authorization: Bearer $CF_ACCESS_TOKEN\" | \\ grep -i \"x-ratelimit-remaining\" | \\ awk '{print $2}' | tr -d '\\r') if [ \"$remaining\" -lt 10 ]; then echo \"Warning: Only $remaining requests",
              "score": null
            },
            {
              "doc_id": "user_guide",
              "chunk_id": "user_guide_fix_25",
              "content": "To account for your local timezone: **Option 1: Convert to UTC** If you want a workflow to run at 9:00 AM EST (UTC-5), schedule it for 14:00 UTC: ``` 0 14 * * * # 9:00 AM EST = 14:00 UTC ``` **Option 2: Use Timezone Configuration** Specify a timezone in your workflow configuration: ```yaml schedule: cron: \"0 9 * * *\" timezone: \"America/New_York\" # IANA timezone identifier ``` **Supported Timezones:** CloudFlow supports all IANA",
              "score": null
            },
            {
              "doc_id": "user_guide",
              "chunk_id": "user_guide_fix_26",
              "content": "timezone identifiers, including: - `America/New_York` (Eastern Time) - `America/Chicago` (Central Time) - `America/Los_Angeles` (Pacific Time) - `Europe/London` (GMT/BST) - `Asia/Tokyo` (Japan Standard Time) - `Australia/Sydney` (Australian Eastern Time) **Daylight Saving Time:** When using timezone configuration, CloudFlow automatically handles DST transitions. A workflow scheduled for 9:00 AM will run at 9:00 AM local time regardless of DST changes. ### Schedule Management **Creating a Schedule:** 1. Open your workflow in the editor 2.",
              "score": null
            }
          ],
          "latency_ms": 3.373216999534634,
          "query_id": "realistic_020",
          "dimension": "negation",
          "query": "Why can't I schedule workflows every 30 seconds?"
        }
      ]
    }
  ]
}