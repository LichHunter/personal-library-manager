{
  "metadata": {
    "timestamp": "2026-01-24_133029",
    "num_documents": 52,
    "num_queries": 53,
    "total_facts": 180,
    "has_human_queries": true
  },
  "evaluations": [
    {
      "strategy": "semantic",
      "chunking": "fixed_100_0pct",
      "embedder": "all-MiniLM-L6-v2",
      "reranker": null,
      "llm": null,
      "k": 5,
      "metric": "exact_match",
      "num_chunks": 126,
      "index_time_s": 0.3380246510005236,
      "aggregate": {
        "original": {
          "coverage": 0.7833333333333333,
          "found": 141,
          "total": 180,
          "avg_latency_ms": 3.723205264080052,
          "p95_latency_ms": 4.4388531998265535
        },
        "synonym": {
          "coverage": 0.7555555555555555,
          "found": 136,
          "total": 180,
          "avg_latency_ms": 3.725234358771902,
          "p95_latency_ms": 4.398403400409734
        },
        "problem": {
          "coverage": 0.7888888888888889,
          "found": 142,
          "total": 180,
          "avg_latency_ms": 3.8182381133151897,
          "p95_latency_ms": 4.337484399729874
        },
        "casual": {
          "coverage": 0.8333333333333334,
          "found": 150,
          "total": 180,
          "avg_latency_ms": 3.6680943206381516,
          "p95_latency_ms": 4.52659659931669
        },
        "contextual": {
          "coverage": 0.7888888888888889,
          "found": 142,
          "total": 180,
          "avg_latency_ms": 3.6986345661510844,
          "p95_latency_ms": 4.228533200148377
        },
        "negation": {
          "coverage": 0.8,
          "found": 144,
          "total": 180,
          "avg_latency_ms": 3.6176184150360124,
          "p95_latency_ms": 4.096282399405026
        }
      },
      "per_query": [
        {
          "key_facts": [
            "100 requests per minute",
            "rate limit"
          ],
          "found_facts": [
            "100 requests per minute",
            "rate limit"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "api_workflows",
            "arch_api_gateway"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "adr_007",
              "chunk_id": "adr_007_fix_0",
              "content": "# ADR-007: Implement Rate Limiting at API Gateway **Status:** Accepted **Date:** 2024-09-03 ## Context We need to protect our services from abuse and ensure fair resource usage across customers. ## Options Considered ### Option 1: Sliding window at gateway Rate limit at API gateway level.",
              "score": null
            },
            {
              "doc_id": "config_api_gateway",
              "chunk_id": "config_api_gateway_fix_0",
              "content": "# API Gateway Configuration Reference ## Overview This document describes all configuration options for API Gateway. ## Environment Variables | Variable | Type | Default | Description | |----------|------|---------|-------------| | `GATEWAY_PORT` | integer | 8000 | Gateway listen port | | `GATEWAY_RATE_LIMIT` | integer | 100 | Requests per minute per user | | `GATEWAY_TIMEOUT_MS` | integer | 30000 | Request timeout in milliseconds | | `GATEWAY_MAX_BODY_SIZE` | string | 10MB | Maximum request body",
              "score": null
            },
            {
              "doc_id": "adr_007",
              "chunk_id": "adr_007_fix_2",
              "content": "**Pros:** - Allows bursts - Smooth rate limiting - Well understood **Cons:** - More complex - Harder to explain to users - State management ## Decision We will implement sliding window rate limiting at the Kong API Gateway using Redis. ## Consequences - Default limit: 100 requests per minute - Implement per-plan limits - Return rate limit headers - Add retry-after on 429",
              "score": null
            },
            {
              "doc_id": "howto_create_api_integration",
              "chunk_id": "howto_create_api_integration_fix_1",
              "content": "```bash curl -H 'Authorization: Bearer cf_...' https://api.cloudflow.io/v1/me ``` ### Step 4: Implement rate limiting Handle rate limit responses (HTTP 429) with exponential backoff. > **Note:** Default limit is 100 requests per minute ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            },
            {
              "doc_id": "arch_api_gateway",
              "chunk_id": "arch_api_gateway_fix_1",
              "content": "**Technology:** Kong 3.4 on Kubernetes **Scaling:** Horizontal with HPA ### Auth Plugin Validates JWT tokens, API keys, and OAuth2 flows. Token validation latency under 5ms. **Technology:** Custom Lua plugin **Scaling:** Stateless ### Rate Limiter Enforces per-user and per-IP rate limits. Default limit is 100 requests per minute.",
              "score": null
            }
          ],
          "latency_ms": 12.756018000800395,
          "query_id": "simple_001",
          "dimension": "original",
          "query": "What is the default rate limit for API requests?"
        },
        {
          "key_facts": [
            "100 requests per minute",
            "rate limit"
          ],
          "found_facts": [
            "100 requests per minute",
            "rate limit"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "api_workflows",
            "arch_api_gateway"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "adr_007",
              "chunk_id": "adr_007_fix_0",
              "content": "# ADR-007: Implement Rate Limiting at API Gateway **Status:** Accepted **Date:** 2024-09-03 ## Context We need to protect our services from abuse and ensure fair resource usage across customers. ## Options Considered ### Option 1: Sliding window at gateway Rate limit at API gateway level.",
              "score": null
            },
            {
              "doc_id": "adr_007",
              "chunk_id": "adr_007_fix_2",
              "content": "**Pros:** - Allows bursts - Smooth rate limiting - Well understood **Cons:** - More complex - Harder to explain to users - State management ## Decision We will implement sliding window rate limiting at the Kong API Gateway using Redis. ## Consequences - Default limit: 100 requests per minute - Implement per-plan limits - Return rate limit headers - Add retry-after on 429",
              "score": null
            },
            {
              "doc_id": "config_api_gateway",
              "chunk_id": "config_api_gateway_fix_0",
              "content": "# API Gateway Configuration Reference ## Overview This document describes all configuration options for API Gateway. ## Environment Variables | Variable | Type | Default | Description | |----------|------|---------|-------------| | `GATEWAY_PORT` | integer | 8000 | Gateway listen port | | `GATEWAY_RATE_LIMIT` | integer | 100 | Requests per minute per user | | `GATEWAY_TIMEOUT_MS` | integer | 30000 | Request timeout in milliseconds | | `GATEWAY_MAX_BODY_SIZE` | string | 10MB | Maximum request body",
              "score": null
            },
            {
              "doc_id": "arch_api_gateway",
              "chunk_id": "arch_api_gateway_fix_1",
              "content": "**Technology:** Kong 3.4 on Kubernetes **Scaling:** Horizontal with HPA ### Auth Plugin Validates JWT tokens, API keys, and OAuth2 flows. Token validation latency under 5ms. **Technology:** Custom Lua plugin **Scaling:** Stateless ### Rate Limiter Enforces per-user and per-IP rate limits. Default limit is 100 requests per minute.",
              "score": null
            },
            {
              "doc_id": "adr_007",
              "chunk_id": "adr_007_fix_1",
              "content": "**Pros:** - Centralized enforcement - Low latency - Easy configuration **Cons:** - Gateway becomes bottleneck - Coarse granularity - Redis dependency ### Option 2: Per-service limiting Each service implements its own limits. **Pros:** - Fine-grained control - Service autonomy - No single point of failure **Cons:** - Inconsistent implementation - Higher complexity - Distributed state ### Option 3: Token bucket Token-based rate limiting.",
              "score": null
            }
          ],
          "latency_ms": 3.907650998371537,
          "query_id": "simple_001",
          "dimension": "synonym",
          "query": "what's the API throttling limit per user"
        },
        {
          "key_facts": [
            "100 requests per minute",
            "rate limit"
          ],
          "found_facts": [
            "100 requests per minute",
            "rate limit"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "api_workflows",
            "arch_api_gateway"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "api_secrets",
              "chunk_id": "api_secrets_fix_2",
              "content": "**Parameters:** **Response:** - Success: `204` - Secret deleted - Error: `400` - Bad Request **Example:** ```json {} ```",
              "score": null
            },
            {
              "doc_id": "api_workflows",
              "chunk_id": "api_workflows_fix_1",
              "content": "**Parameters:** - `limit` (integer): Maximum number of results (default: 50, max: 200) - `status` (string): Filter by status: active, paused, archived **Response:** - Success: `200` - Returns array of workflow objects - Error: `400` - Bad Request **Example:** ```json {\"workflows\": [{\"id\": \"wf_123\", \"name\": \"Daily Report\", \"status\": \"active\"}]} ``` ### POST /workflows Create a new workflow.",
              "score": null
            },
            {
              "doc_id": "api_metrics",
              "chunk_id": "api_metrics_fix_1",
              "content": "**Parameters:** - `period` (string): Time period: hour, day, week, month - `workflow_id` (string): Filter by workflow ID **Response:** - Success: `200` - Returns metrics data - Error: `400` - Bad Request **Example:** ```json {\"success_rate\": 0.985, \"avg_duration_ms\": 1250, \"p99_duration_ms\": 5000, \"total_executions\": 10000} ```",
              "score": null
            },
            {
              "doc_id": "api_executions",
              "chunk_id": "api_executions_fix_2",
              "content": "**Parameters:** - `id` (string): Execution ID - `reason` (string): Cancellation reason (optional) **Response:** - Success: `200` - Execution cancelled - Error: `400` - Bad Request **Example:** ```json {\"id\": \"ex_789\", \"status\": \"cancelled\", \"cancelled_at\": \"2024-01-15T10:05:00Z\"} ```",
              "score": null
            },
            {
              "doc_id": "howto_create_api_integration",
              "chunk_id": "howto_create_api_integration_fix_1",
              "content": "```bash curl -H 'Authorization: Bearer cf_...' https://api.cloudflow.io/v1/me ``` ### Step 4: Implement rate limiting Handle rate limit responses (HTTP 429) with exponential backoff. > **Note:** Default limit is 100 requests per minute ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            }
          ],
          "latency_ms": 3.567747999113635,
          "query_id": "simple_001",
          "dimension": "problem",
          "query": "my requests keep getting rejected with 429 too many requests"
        },
        {
          "key_facts": [
            "100 requests per minute",
            "rate limit"
          ],
          "found_facts": [
            "100 requests per minute",
            "rate limit"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "api_workflows",
            "arch_api_gateway"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "adr_007",
              "chunk_id": "adr_007_fix_0",
              "content": "# ADR-007: Implement Rate Limiting at API Gateway **Status:** Accepted **Date:** 2024-09-03 ## Context We need to protect our services from abuse and ensure fair resource usage across customers. ## Options Considered ### Option 1: Sliding window at gateway Rate limit at API gateway level.",
              "score": null
            },
            {
              "doc_id": "adr_007",
              "chunk_id": "adr_007_fix_2",
              "content": "**Pros:** - Allows bursts - Smooth rate limiting - Well understood **Cons:** - More complex - Harder to explain to users - State management ## Decision We will implement sliding window rate limiting at the Kong API Gateway using Redis. ## Consequences - Default limit: 100 requests per minute - Implement per-plan limits - Return rate limit headers - Add retry-after on 429",
              "score": null
            },
            {
              "doc_id": "config_api_gateway",
              "chunk_id": "config_api_gateway_fix_0",
              "content": "# API Gateway Configuration Reference ## Overview This document describes all configuration options for API Gateway. ## Environment Variables | Variable | Type | Default | Description | |----------|------|---------|-------------| | `GATEWAY_PORT` | integer | 8000 | Gateway listen port | | `GATEWAY_RATE_LIMIT` | integer | 100 | Requests per minute per user | | `GATEWAY_TIMEOUT_MS` | integer | 30000 | Request timeout in milliseconds | | `GATEWAY_MAX_BODY_SIZE` | string | 10MB | Maximum request body",
              "score": null
            },
            {
              "doc_id": "howto_create_api_integration",
              "chunk_id": "howto_create_api_integration_fix_1",
              "content": "```bash curl -H 'Authorization: Bearer cf_...' https://api.cloudflow.io/v1/me ``` ### Step 4: Implement rate limiting Handle rate limit responses (HTTP 429) with exponential backoff. > **Note:** Default limit is 100 requests per minute ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            },
            {
              "doc_id": "adr_007",
              "chunk_id": "adr_007_fix_1",
              "content": "**Pros:** - Centralized enforcement - Low latency - Easy configuration **Cons:** - Gateway becomes bottleneck - Coarse granularity - Redis dependency ### Option 2: Per-service limiting Each service implements its own limits. **Pros:** - Fine-grained control - Service autonomy - No single point of failure **Cons:** - Inconsistent implementation - Higher complexity - Distributed state ### Option 3: Token bucket Token-based rate limiting.",
              "score": null
            }
          ],
          "latency_ms": 3.426063000006252,
          "query_id": "simple_001",
          "dimension": "casual",
          "query": "api rate limit"
        },
        {
          "key_facts": [
            "100 requests per minute",
            "rate limit"
          ],
          "found_facts": [
            "100 requests per minute",
            "rate limit"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "api_workflows",
            "arch_api_gateway"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "config_api_gateway",
              "chunk_id": "config_api_gateway_fix_0",
              "content": "# API Gateway Configuration Reference ## Overview This document describes all configuration options for API Gateway. ## Environment Variables | Variable | Type | Default | Description | |----------|------|---------|-------------| | `GATEWAY_PORT` | integer | 8000 | Gateway listen port | | `GATEWAY_RATE_LIMIT` | integer | 100 | Requests per minute per user | | `GATEWAY_TIMEOUT_MS` | integer | 30000 | Request timeout in milliseconds | | `GATEWAY_MAX_BODY_SIZE` | string | 10MB | Maximum request body",
              "score": null
            },
            {
              "doc_id": "api_metrics",
              "chunk_id": "api_metrics_fix_0",
              "content": "# Metrics API Reference ## Overview The Metrics API provides programmatic access to metrics functionality. Base URL: `https://api.cloudflow.io/v1/metrics` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /metrics/workflows Get workflow performance metrics.",
              "score": null
            },
            {
              "doc_id": "howto_create_api_integration",
              "chunk_id": "howto_create_api_integration_fix_1",
              "content": "```bash curl -H 'Authorization: Bearer cf_...' https://api.cloudflow.io/v1/me ``` ### Step 4: Implement rate limiting Handle rate limit responses (HTTP 429) with exponential backoff. > **Note:** Default limit is 100 requests per minute ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            },
            {
              "doc_id": "api_executions",
              "chunk_id": "api_executions_fix_1",
              "content": "**Parameters:** - `workflow_id` (string): Filter by workflow ID - `since` (datetime): Filter executions after this timestamp **Response:** - Success: `200` - Returns array of execution objects - Error: `400` - Bad Request **Example:** ```json {\"executions\": [{\"id\": \"ex_789\", \"status\": \"completed\", \"duration_ms\": 1523}]} ``` ### POST /executions/{id}/cancel Cancel a running execution.",
              "score": null
            },
            {
              "doc_id": "api_workflows",
              "chunk_id": "api_workflows_fix_1",
              "content": "**Parameters:** - `limit` (integer): Maximum number of results (default: 50, max: 200) - `status` (string): Filter by status: active, paused, archived **Response:** - Success: `200` - Returns array of workflow objects - Error: `400` - Bad Request **Example:** ```json {\"workflows\": [{\"id\": \"wf_123\", \"name\": \"Daily Report\", \"status\": \"active\"}]} ``` ### POST /workflows Create a new workflow.",
              "score": null
            }
          ],
          "latency_ms": 3.8662240003759507,
          "query_id": "simple_001",
          "dimension": "contextual",
          "query": "building a batch sync tool that hits the API, need to know the request cap"
        },
        {
          "key_facts": [
            "100 requests per minute",
            "rate limit"
          ],
          "found_facts": [],
          "missed_facts": [
            "100 requests per minute",
            "rate limit"
          ],
          "coverage": 0.0,
          "expected_docs": [
            "api_workflows",
            "arch_api_gateway"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "api_secrets",
              "chunk_id": "api_secrets_fix_2",
              "content": "**Parameters:** **Response:** - Success: `204` - Secret deleted - Error: `400` - Bad Request **Example:** ```json {} ```",
              "score": null
            },
            {
              "doc_id": "api_users",
              "chunk_id": "api_users_fix_1",
              "content": "**Parameters:** **Response:** - Success: `200` - Returns user object - Error: `400` - Bad Request **Example:** ```json {\"id\": \"usr_001\", \"email\": \"user@example.com\", \"role\": \"admin\", \"created_at\": \"2023-06-01\"} ``` ### PATCH /users/me Update current user profile. **Parameters:** - `name` (string): Display name - `timezone` (string): Timezone (e.g., America/New_York) **Response:** - Success: `200` - User updated - Error: `400` - Bad Request **Example:** ```json {\"id\": \"usr_001\", \"name\": \"John Doe\", \"timezone\": \"America/New_York\"} ```",
              "score": null
            },
            {
              "doc_id": "api_executions",
              "chunk_id": "api_executions_fix_2",
              "content": "**Parameters:** - `id` (string): Execution ID - `reason` (string): Cancellation reason (optional) **Response:** - Success: `200` - Execution cancelled - Error: `400` - Bad Request **Example:** ```json {\"id\": \"ex_789\", \"status\": \"cancelled\", \"cancelled_at\": \"2024-01-15T10:05:00Z\"} ```",
              "score": null
            },
            {
              "doc_id": "config_api_gateway",
              "chunk_id": "config_api_gateway_fix_0",
              "content": "# API Gateway Configuration Reference ## Overview This document describes all configuration options for API Gateway. ## Environment Variables | Variable | Type | Default | Description | |----------|------|---------|-------------| | `GATEWAY_PORT` | integer | 8000 | Gateway listen port | | `GATEWAY_RATE_LIMIT` | integer | 100 | Requests per minute per user | | `GATEWAY_TIMEOUT_MS` | integer | 30000 | Request timeout in milliseconds | | `GATEWAY_MAX_BODY_SIZE` | string | 10MB | Maximum request body",
              "score": null
            },
            {
              "doc_id": "api_integrations",
              "chunk_id": "api_integrations_fix_1",
              "content": "**Parameters:** - `category` (string): Filter by category: storage, notification, database **Response:** - Success: `200` - Returns array of integration objects - Error: `400` - Bad Request **Example:** ```json {\"integrations\": [{\"id\": \"int_s3\", \"name\": \"Amazon S3\", \"category\": \"storage\", \"status\": \"connected\"}]} ``` ### POST /integrations/{id}/connect Connect an integration. **Parameters:** - `credentials` (object): Integration-specific credentials **Response:** - Success: `200` - Integration connected - Error: `400` - Bad Request **Example:** ```json {\"id\": \"int_s3\", \"status\": \"connected\", \"connected_at\": \"2024-01-15T10:00:00Z\"} ```",
              "score": null
            }
          ],
          "latency_ms": 3.615585999796167,
          "query_id": "simple_001",
          "dimension": "negation",
          "query": "why is the API blocking my requests after a while"
        },
        {
          "key_facts": [
            "PostgreSQL",
            "JSONB"
          ],
          "found_facts": [
            "PostgreSQL"
          ],
          "missed_facts": [
            "JSONB"
          ],
          "coverage": 0.5,
          "expected_docs": [
            "adr_001",
            "config_database"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "api_integrations",
              "chunk_id": "api_integrations_fix_0",
              "content": "# Integrations API Reference ## Overview The Integrations API provides programmatic access to integrations functionality. Base URL: `https://api.cloudflow.io/v1/integrations` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /integrations List available integrations.",
              "score": null
            },
            {
              "doc_id": "howto_set_up_database_connection",
              "chunk_id": "howto_set_up_database_connection_fix_0",
              "content": "# How to Set Up Database Connection ## Prerequisites - Database with network access to CloudFlow - Connection credentials ## Steps ### Step 1: Add connection secret Store your database connection string as a secret. ```bash cloudflow secrets set DATABASE_URL postgresql://user:pass@host:5432/db ``` ### Step 2: Configure connection pool Set pool size based on your plan limits. Free tier allows 5 connections, Pro allows 20.",
              "score": null
            },
            {
              "doc_id": "api_webhooks",
              "chunk_id": "api_webhooks_fix_0",
              "content": "# Webhooks API Reference ## Overview The Webhooks API provides programmatic access to webhooks functionality. Base URL: `https://api.cloudflow.io/v1/webhooks` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /webhooks List configured webhooks.",
              "score": null
            },
            {
              "doc_id": "api_secrets",
              "chunk_id": "api_secrets_fix_0",
              "content": "# Secrets API Reference ## Overview The Secrets API provides programmatic access to secrets functionality. Base URL: `https://api.cloudflow.io/v1/secrets` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /secrets List secret names (values not returned).",
              "score": null
            },
            {
              "doc_id": "api_users",
              "chunk_id": "api_users_fix_0",
              "content": "# Users API Reference ## Overview The Users API provides programmatic access to users functionality. Base URL: `https://api.cloudflow.io/v1/users` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /users/me Get current user profile.",
              "score": null
            }
          ],
          "latency_ms": 3.6776319993805373,
          "query_id": "simple_002",
          "dimension": "original",
          "query": "What database does CloudFlow use?"
        },
        {
          "key_facts": [
            "PostgreSQL",
            "JSONB"
          ],
          "found_facts": [
            "PostgreSQL"
          ],
          "missed_facts": [
            "JSONB"
          ],
          "coverage": 0.5,
          "expected_docs": [
            "adr_001",
            "config_database"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "howto_set_up_database_connection",
              "chunk_id": "howto_set_up_database_connection_fix_0",
              "content": "# How to Set Up Database Connection ## Prerequisites - Database with network access to CloudFlow - Connection credentials ## Steps ### Step 1: Add connection secret Store your database connection string as a secret. ```bash cloudflow secrets set DATABASE_URL postgresql://user:pass@host:5432/db ``` ### Step 2: Configure connection pool Set pool size based on your plan limits. Free tier allows 5 connections, Pro allows 20.",
              "score": null
            },
            {
              "doc_id": "api_integrations",
              "chunk_id": "api_integrations_fix_0",
              "content": "# Integrations API Reference ## Overview The Integrations API provides programmatic access to integrations functionality. Base URL: `https://api.cloudflow.io/v1/integrations` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /integrations List available integrations.",
              "score": null
            },
            {
              "doc_id": "api_webhooks",
              "chunk_id": "api_webhooks_fix_0",
              "content": "# Webhooks API Reference ## Overview The Webhooks API provides programmatic access to webhooks functionality. Base URL: `https://api.cloudflow.io/v1/webhooks` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /webhooks List configured webhooks.",
              "score": null
            },
            {
              "doc_id": "api_users",
              "chunk_id": "api_users_fix_0",
              "content": "# Users API Reference ## Overview The Users API provides programmatic access to users functionality. Base URL: `https://api.cloudflow.io/v1/users` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /users/me Get current user profile.",
              "score": null
            },
            {
              "doc_id": "api_secrets",
              "chunk_id": "api_secrets_fix_0",
              "content": "# Secrets API Reference ## Overview The Secrets API provides programmatic access to secrets functionality. Base URL: `https://api.cloudflow.io/v1/secrets` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /secrets List secret names (values not returned).",
              "score": null
            }
          ],
          "latency_ms": 3.3110600015788805,
          "query_id": "simple_002",
          "dimension": "synonym",
          "query": "what's the primary data store in CloudFlow"
        },
        {
          "key_facts": [
            "PostgreSQL",
            "JSONB"
          ],
          "found_facts": [],
          "missed_facts": [
            "PostgreSQL",
            "JSONB"
          ],
          "coverage": 0.0,
          "expected_docs": [
            "adr_001",
            "config_database"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "howto_debug_failed_execution",
              "chunk_id": "howto_debug_failed_execution_fix_0",
              "content": "# How to Debug Failed Execution ## Prerequisites - Failed execution ID ## Steps ### Step 1: Get execution details Retrieve the full execution record including step outputs. ```bash cloudflow executions get ex_123 --include-steps ``` ### Step 2: View step logs Check logs for the failed step. ```bash cloudflow executions logs ex_123 --step 3 ``` ### Step 3: Inspect input/output Review the data passed between steps.",
              "score": null
            },
            {
              "doc_id": "howto_set_up_database_connection",
              "chunk_id": "howto_set_up_database_connection_fix_1",
              "content": "> **Note:** Pool exhaustion will queue requests ### Step 3: Test connection Verify the database is accessible from CloudFlow. ```bash cloudflow db test --secret DATABASE_URL ``` ### Step 4: Enable SSL For production, always require SSL connections. ```bash cloudflow db configure --require-ssl ``` ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            },
            {
              "doc_id": "howto_debug_failed_execution",
              "chunk_id": "howto_debug_failed_execution_fix_1",
              "content": "```bash cloudflow executions inspect ex_123 --step 3 ``` ### Step 4: Enable debug mode Re-run with verbose logging enabled. ```bash cloudflow workflows execute wf_123 --debug ``` ### Step 5: Check error patterns Look for similar failures in recent executions. ```bash cloudflow executions list --status failed --since 24h ``` ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            },
            {
              "doc_id": "api_integrations",
              "chunk_id": "api_integrations_fix_0",
              "content": "# Integrations API Reference ## Overview The Integrations API provides programmatic access to integrations functionality. Base URL: `https://api.cloudflow.io/v1/integrations` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /integrations List available integrations.",
              "score": null
            },
            {
              "doc_id": "api_audit",
              "chunk_id": "api_audit_fix_0",
              "content": "# Audit API Reference ## Overview The Audit API provides programmatic access to audit functionality. Base URL: `https://api.cloudflow.io/v1/audit` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /audit/logs Retrieve audit logs for compliance.",
              "score": null
            }
          ],
          "latency_ms": 3.566985998986638,
          "query_id": "simple_002",
          "dimension": "problem",
          "query": "getting database-specific errors from CloudFlow, what DB does it use"
        },
        {
          "key_facts": [
            "PostgreSQL",
            "JSONB"
          ],
          "found_facts": [
            "PostgreSQL"
          ],
          "missed_facts": [
            "JSONB"
          ],
          "coverage": 0.5,
          "expected_docs": [
            "adr_001",
            "config_database"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "howto_set_up_database_connection",
              "chunk_id": "howto_set_up_database_connection_fix_0",
              "content": "# How to Set Up Database Connection ## Prerequisites - Database with network access to CloudFlow - Connection credentials ## Steps ### Step 1: Add connection secret Store your database connection string as a secret. ```bash cloudflow secrets set DATABASE_URL postgresql://user:pass@host:5432/db ``` ### Step 2: Configure connection pool Set pool size based on your plan limits. Free tier allows 5 connections, Pro allows 20.",
              "score": null
            },
            {
              "doc_id": "howto_set_up_database_connection",
              "chunk_id": "howto_set_up_database_connection_fix_1",
              "content": "> **Note:** Pool exhaustion will queue requests ### Step 3: Test connection Verify the database is accessible from CloudFlow. ```bash cloudflow db test --secret DATABASE_URL ``` ### Step 4: Enable SSL For production, always require SSL connections. ```bash cloudflow db configure --require-ssl ``` ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            },
            {
              "doc_id": "api_integrations",
              "chunk_id": "api_integrations_fix_0",
              "content": "# Integrations API Reference ## Overview The Integrations API provides programmatic access to integrations functionality. Base URL: `https://api.cloudflow.io/v1/integrations` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /integrations List available integrations.",
              "score": null
            },
            {
              "doc_id": "api_webhooks",
              "chunk_id": "api_webhooks_fix_0",
              "content": "# Webhooks API Reference ## Overview The Webhooks API provides programmatic access to webhooks functionality. Base URL: `https://api.cloudflow.io/v1/webhooks` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /webhooks List configured webhooks.",
              "score": null
            },
            {
              "doc_id": "howto_create_api_integration",
              "chunk_id": "howto_create_api_integration_fix_1",
              "content": "```bash curl -H 'Authorization: Bearer cf_...' https://api.cloudflow.io/v1/me ``` ### Step 4: Implement rate limiting Handle rate limit responses (HTTP 429) with exponential backoff. > **Note:** Default limit is 100 requests per minute ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            }
          ],
          "latency_ms": 3.6321679999673506,
          "query_id": "simple_002",
          "dimension": "casual",
          "query": "cloudflow db?"
        },
        {
          "key_facts": [
            "PostgreSQL",
            "JSONB"
          ],
          "found_facts": [
            "PostgreSQL"
          ],
          "missed_facts": [
            "JSONB"
          ],
          "coverage": 0.5,
          "expected_docs": [
            "adr_001",
            "config_database"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "api_integrations",
              "chunk_id": "api_integrations_fix_0",
              "content": "# Integrations API Reference ## Overview The Integrations API provides programmatic access to integrations functionality. Base URL: `https://api.cloudflow.io/v1/integrations` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /integrations List available integrations.",
              "score": null
            },
            {
              "doc_id": "howto_set_up_database_connection",
              "chunk_id": "howto_set_up_database_connection_fix_0",
              "content": "# How to Set Up Database Connection ## Prerequisites - Database with network access to CloudFlow - Connection credentials ## Steps ### Step 1: Add connection secret Store your database connection string as a secret. ```bash cloudflow secrets set DATABASE_URL postgresql://user:pass@host:5432/db ``` ### Step 2: Configure connection pool Set pool size based on your plan limits. Free tier allows 5 connections, Pro allows 20.",
              "score": null
            },
            {
              "doc_id": "arch_data_pipeline",
              "chunk_id": "arch_data_pipeline_fix_0",
              "content": "# Data Pipeline Architecture ## Overview The Data Pipeline processes and transforms data between workflow steps. It supports batch processing up to 10GB and streaming for real-time use cases. ## Design Principles - **Exactly-Once Processing**: Each record is processed exactly once - **Backpressure Handling**: System gracefully handles varying load ## Components ### Stream Processor Handles real-time data streams. Maximum throughput of 100000 messages per second.",
              "score": null
            },
            {
              "doc_id": "howto_set_up_database_connection",
              "chunk_id": "howto_set_up_database_connection_fix_1",
              "content": "> **Note:** Pool exhaustion will queue requests ### Step 3: Test connection Verify the database is accessible from CloudFlow. ```bash cloudflow db test --secret DATABASE_URL ``` ### Step 4: Enable SSL For production, always require SSL connections. ```bash cloudflow db configure --require-ssl ``` ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            },
            {
              "doc_id": "api_secrets",
              "chunk_id": "api_secrets_fix_0",
              "content": "# Secrets API Reference ## Overview The Secrets API provides programmatic access to secrets functionality. Base URL: `https://api.cloudflow.io/v1/secrets` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /secrets List secret names (values not returned).",
              "score": null
            }
          ],
          "latency_ms": 4.580995999276638,
          "query_id": "simple_002",
          "dimension": "contextual",
          "query": "need to write raw queries against CloudFlow's backend, what DB engine is it"
        },
        {
          "key_facts": [
            "PostgreSQL",
            "JSONB"
          ],
          "found_facts": [
            "PostgreSQL"
          ],
          "missed_facts": [
            "JSONB"
          ],
          "coverage": 0.5,
          "expected_docs": [
            "adr_001",
            "config_database"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "howto_set_up_database_connection",
              "chunk_id": "howto_set_up_database_connection_fix_1",
              "content": "> **Note:** Pool exhaustion will queue requests ### Step 3: Test connection Verify the database is accessible from CloudFlow. ```bash cloudflow db test --secret DATABASE_URL ``` ### Step 4: Enable SSL For production, always require SSL connections. ```bash cloudflow db configure --require-ssl ``` ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            },
            {
              "doc_id": "release_3_0_0",
              "chunk_id": "release_3_0_0_fix_1",
              "content": "```bash cloudflow upgrade --version 3.0.0 ``` ## Known Issues None at this time.",
              "score": null
            },
            {
              "doc_id": "howto_migrate_from_v1_to_v2_api",
              "chunk_id": "howto_migrate_from_v1_to_v2_api_fix_1",
              "content": "> **Note:** Generate new credentials in the dashboard ### Step 4: Test in staging Run your integration tests against the v2 staging endpoint. ```bash CLOUDFLOW_API_URL=https://api-v2-staging.cloudflow.io npm test ``` ### Step 5: Switch production Update production configuration to use v2 API. ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            },
            {
              "doc_id": "howto_configure_custom_domain",
              "chunk_id": "howto_configure_custom_domain_fix_1",
              "content": "> **Note:** CNAME should point to ingress.cloudflow.io ### Step 3: Enable SSL CloudFlow automatically provisions SSL certificates via Let's Encrypt. ```bash cloudflow domains ssl --domain app.example.com ``` ### Step 4: Set as primary Make this the primary domain for your application. ```bash cloudflow domains set-primary --domain app.example.com ``` ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            },
            {
              "doc_id": "howto_set_up_database_connection",
              "chunk_id": "howto_set_up_database_connection_fix_0",
              "content": "# How to Set Up Database Connection ## Prerequisites - Database with network access to CloudFlow - Connection credentials ## Steps ### Step 1: Add connection secret Store your database connection string as a secret. ```bash cloudflow secrets set DATABASE_URL postgresql://user:pass@host:5432/db ``` ### Step 2: Configure connection pool Set pool size based on your plan limits. Free tier allows 5 connections, Pro allows 20.",
              "score": null
            }
          ],
          "latency_ms": 3.5439729999779956,
          "query_id": "simple_002",
          "dimension": "negation",
          "query": "is CloudFlow not using a NoSQL database"
        },
        {
          "key_facts": [
            "1 hour",
            "7 days",
            "refresh tokens"
          ],
          "found_facts": [
            "1 hour",
            "7 days",
            "refresh tokens"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "adr_003",
            "arch_authentication_service"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "arch_authentication_service",
              "chunk_id": "arch_authentication_service_fix_1",
              "content": "**Technology:** Custom Go service **Scaling:** Horizontal stateless ### Token Service Issues and validates access tokens. Token lifetime is 1 hour with 7-day refresh tokens. **Technology:** JWT with RS256 **Scaling:** Horizontal stateless ### SSO Gateway Integrates with external identity providers. Supports Google, Microsoft, Okta, and custom SAML.",
              "score": null
            },
            {
              "doc_id": "adr_003",
              "chunk_id": "adr_003_fix_2",
              "content": "**Pros:** - Very simple - Low overhead - Easy to implement **Cons:** - No user context - Hard to rotate - No SSO support ## Decision We will implement OAuth 2.0 with JWT tokens. Access tokens expire after 1 hour, refresh tokens after 7 days. ## Consequences - Implement token rotation on refresh - Support API keys for machine-to-machine auth - Add MFA for admin accounts",
              "score": null
            },
            {
              "doc_id": "arch_authentication_service",
              "chunk_id": "arch_authentication_service_fix_0",
              "content": "# Authentication Service Architecture ## Overview The Authentication Service handles user identity, session management, and token issuance. Supports OAuth 2.0, SAML, and API key authentication. ## Design Principles - **Zero Knowledge**: Passwords never stored in plaintext - **Token Rotation**: Refresh tokens rotate on each use ## Components ### Identity Provider Manages user accounts and credentials. Password hashing uses bcrypt with cost factor 12.",
              "score": null
            },
            {
              "doc_id": "adr_003",
              "chunk_id": "adr_003_fix_1",
              "content": "**Pros:** - Industry standard - Stateless validation - Supports refresh tokens - SSO compatible **Cons:** - Token size overhead - Cannot revoke individual tokens - Complexity ### Option 2: Session-based auth Traditional server-side sessions. **Pros:** - Simple implementation - Easy revocation - Smaller payloads **Cons:** - Requires session store - Not suitable for APIs - Scaling challenges ### Option 3: API keys only Simple key-based authentication.",
              "score": null
            },
            {
              "doc_id": "adr_003",
              "chunk_id": "adr_003_fix_0",
              "content": "# ADR-003: Implement OAuth 2.0 with JWT for Authentication **Status:** Accepted **Date:** 2024-05-08 ## Context We need a secure authentication system that supports API keys, user sessions, and third-party integrations. ## Options Considered ### Option 1: OAuth 2.0 with JWT Industry standard token-based auth.",
              "score": null
            }
          ],
          "latency_ms": 3.342238000186626,
          "query_id": "simple_003",
          "dimension": "original",
          "query": "How long do access tokens last?"
        },
        {
          "key_facts": [
            "1 hour",
            "7 days",
            "refresh tokens"
          ],
          "found_facts": [
            "1 hour",
            "7 days",
            "refresh tokens"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "adr_003",
            "arch_authentication_service"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "arch_authentication_service",
              "chunk_id": "arch_authentication_service_fix_1",
              "content": "**Technology:** Custom Go service **Scaling:** Horizontal stateless ### Token Service Issues and validates access tokens. Token lifetime is 1 hour with 7-day refresh tokens. **Technology:** JWT with RS256 **Scaling:** Horizontal stateless ### SSO Gateway Integrates with external identity providers. Supports Google, Microsoft, Okta, and custom SAML.",
              "score": null
            },
            {
              "doc_id": "adr_003",
              "chunk_id": "adr_003_fix_0",
              "content": "# ADR-003: Implement OAuth 2.0 with JWT for Authentication **Status:** Accepted **Date:** 2024-05-08 ## Context We need a secure authentication system that supports API keys, user sessions, and third-party integrations. ## Options Considered ### Option 1: OAuth 2.0 with JWT Industry standard token-based auth.",
              "score": null
            },
            {
              "doc_id": "adr_003",
              "chunk_id": "adr_003_fix_2",
              "content": "**Pros:** - Very simple - Low overhead - Easy to implement **Cons:** - No user context - Hard to rotate - No SSO support ## Decision We will implement OAuth 2.0 with JWT tokens. Access tokens expire after 1 hour, refresh tokens after 7 days. ## Consequences - Implement token rotation on refresh - Support API keys for machine-to-machine auth - Add MFA for admin accounts",
              "score": null
            },
            {
              "doc_id": "arch_api_gateway",
              "chunk_id": "arch_api_gateway_fix_1",
              "content": "**Technology:** Kong 3.4 on Kubernetes **Scaling:** Horizontal with HPA ### Auth Plugin Validates JWT tokens, API keys, and OAuth2 flows. Token validation latency under 5ms. **Technology:** Custom Lua plugin **Scaling:** Stateless ### Rate Limiter Enforces per-user and per-IP rate limits. Default limit is 100 requests per minute.",
              "score": null
            },
            {
              "doc_id": "adr_003",
              "chunk_id": "adr_003_fix_1",
              "content": "**Pros:** - Industry standard - Stateless validation - Supports refresh tokens - SSO compatible **Cons:** - Token size overhead - Cannot revoke individual tokens - Complexity ### Option 2: Session-based auth Traditional server-side sessions. **Pros:** - Simple implementation - Easy revocation - Smaller payloads **Cons:** - Requires session store - Not suitable for APIs - Scaling challenges ### Option 3: API keys only Simple key-based authentication.",
              "score": null
            }
          ],
          "latency_ms": 3.32442400031141,
          "query_id": "simple_003",
          "dimension": "synonym",
          "query": "what's the JWT token expiration time"
        },
        {
          "key_facts": [
            "1 hour",
            "7 days",
            "refresh tokens"
          ],
          "found_facts": [
            "1 hour",
            "refresh tokens"
          ],
          "missed_facts": [
            "7 days"
          ],
          "coverage": 0.6666666666666666,
          "expected_docs": [
            "adr_003",
            "arch_authentication_service"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "runbook_api_gateway_5xx_errors",
              "chunk_id": "runbook_api_gateway_5xx_errors_fix_2",
              "content": "If 5xx persists > 15 minutes, page on-call. May indicate infrastructure issue. ## Post-Incident - Document timeline in incident log - Schedule post-mortem within 48 hours - Update runbook if new patterns discovered",
              "score": null
            },
            {
              "doc_id": "runbook_database_connection_exhaustion",
              "chunk_id": "runbook_database_connection_exhaustion_fix_0",
              "content": "# Runbook: Database Connection Exhaustion ## Severity **Level:** P1 - Critical **Response Time:** 15 minutes ## Symptoms - Connection pool at 100% - Queries timing out - 'too many connections' errors in logs ## Diagnosis ### Step 1 Check active connections: `SELECT count(*) FROM pg_stat_activity` ### Step 2 Identify long-running queries: `SELECT pid, now() - pg_stat_activity.query_start AS duration, query FROM pg_stat_activity WHERE state != 'idle' ORDER BY duration DESC` ### Step 3 Check PgBouncer stats:",
              "score": null
            },
            {
              "doc_id": "runbook_out_of_memory_oom_kills",
              "chunk_id": "runbook_out_of_memory_oom_kills_fix_2",
              "content": "increase, escalate to development team for memory leak investigation. ## Post-Incident - Document timeline in incident log - Schedule post-mortem within 48 hours - Update runbook if new patterns discovered",
              "score": null
            },
            {
              "doc_id": "runbook_high_cpu_usage",
              "chunk_id": "runbook_high_cpu_usage_fix_2",
              "content": "high after scaling, escalate to platform team. Page on-call if affecting users. ## Post-Incident - Document timeline in incident log - Schedule post-mortem within 48 hours - Update runbook if new patterns discovered",
              "score": null
            },
            {
              "doc_id": "arch_authentication_service",
              "chunk_id": "arch_authentication_service_fix_1",
              "content": "**Technology:** Custom Go service **Scaling:** Horizontal stateless ### Token Service Issues and validates access tokens. Token lifetime is 1 hour with 7-day refresh tokens. **Technology:** JWT with RS256 **Scaling:** Horizontal stateless ### SSO Gateway Integrates with external identity providers. Supports Google, Microsoft, Okta, and custom SAML.",
              "score": null
            }
          ],
          "latency_ms": 3.4311030012759147,
          "query_id": "simple_003",
          "dimension": "problem",
          "query": "users are getting logged out after about an hour, is that expected"
        },
        {
          "key_facts": [
            "1 hour",
            "7 days",
            "refresh tokens"
          ],
          "found_facts": [
            "1 hour",
            "7 days",
            "refresh tokens"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "adr_003",
            "arch_authentication_service"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "arch_authentication_service",
              "chunk_id": "arch_authentication_service_fix_1",
              "content": "**Technology:** Custom Go service **Scaling:** Horizontal stateless ### Token Service Issues and validates access tokens. Token lifetime is 1 hour with 7-day refresh tokens. **Technology:** JWT with RS256 **Scaling:** Horizontal stateless ### SSO Gateway Integrates with external identity providers. Supports Google, Microsoft, Okta, and custom SAML.",
              "score": null
            },
            {
              "doc_id": "adr_003",
              "chunk_id": "adr_003_fix_1",
              "content": "**Pros:** - Industry standard - Stateless validation - Supports refresh tokens - SSO compatible **Cons:** - Token size overhead - Cannot revoke individual tokens - Complexity ### Option 2: Session-based auth Traditional server-side sessions. **Pros:** - Simple implementation - Easy revocation - Smaller payloads **Cons:** - Requires session store - Not suitable for APIs - Scaling challenges ### Option 3: API keys only Simple key-based authentication.",
              "score": null
            },
            {
              "doc_id": "adr_003",
              "chunk_id": "adr_003_fix_0",
              "content": "# ADR-003: Implement OAuth 2.0 with JWT for Authentication **Status:** Accepted **Date:** 2024-05-08 ## Context We need a secure authentication system that supports API keys, user sessions, and third-party integrations. ## Options Considered ### Option 1: OAuth 2.0 with JWT Industry standard token-based auth.",
              "score": null
            },
            {
              "doc_id": "arch_authentication_service",
              "chunk_id": "arch_authentication_service_fix_0",
              "content": "# Authentication Service Architecture ## Overview The Authentication Service handles user identity, session management, and token issuance. Supports OAuth 2.0, SAML, and API key authentication. ## Design Principles - **Zero Knowledge**: Passwords never stored in plaintext - **Token Rotation**: Refresh tokens rotate on each use ## Components ### Identity Provider Manages user accounts and credentials. Password hashing uses bcrypt with cost factor 12.",
              "score": null
            },
            {
              "doc_id": "adr_003",
              "chunk_id": "adr_003_fix_2",
              "content": "**Pros:** - Very simple - Low overhead - Easy to implement **Cons:** - No user context - Hard to rotate - No SSO support ## Decision We will implement OAuth 2.0 with JWT tokens. Access tokens expire after 1 hour, refresh tokens after 7 days. ## Consequences - Implement token rotation on refresh - Support API keys for machine-to-machine auth - Add MFA for admin accounts",
              "score": null
            }
          ],
          "latency_ms": 3.9325479992839973,
          "query_id": "simple_003",
          "dimension": "casual",
          "query": "token expiry"
        },
        {
          "key_facts": [
            "1 hour",
            "7 days",
            "refresh tokens"
          ],
          "found_facts": [
            "1 hour",
            "7 days",
            "refresh tokens"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "adr_003",
            "arch_authentication_service"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "arch_authentication_service",
              "chunk_id": "arch_authentication_service_fix_1",
              "content": "**Technology:** Custom Go service **Scaling:** Horizontal stateless ### Token Service Issues and validates access tokens. Token lifetime is 1 hour with 7-day refresh tokens. **Technology:** JWT with RS256 **Scaling:** Horizontal stateless ### SSO Gateway Integrates with external identity providers. Supports Google, Microsoft, Okta, and custom SAML.",
              "score": null
            },
            {
              "doc_id": "adr_003",
              "chunk_id": "adr_003_fix_1",
              "content": "**Pros:** - Industry standard - Stateless validation - Supports refresh tokens - SSO compatible **Cons:** - Token size overhead - Cannot revoke individual tokens - Complexity ### Option 2: Session-based auth Traditional server-side sessions. **Pros:** - Simple implementation - Easy revocation - Smaller payloads **Cons:** - Requires session store - Not suitable for APIs - Scaling challenges ### Option 3: API keys only Simple key-based authentication.",
              "score": null
            },
            {
              "doc_id": "adr_003",
              "chunk_id": "adr_003_fix_0",
              "content": "# ADR-003: Implement OAuth 2.0 with JWT for Authentication **Status:** Accepted **Date:** 2024-05-08 ## Context We need a secure authentication system that supports API keys, user sessions, and third-party integrations. ## Options Considered ### Option 1: OAuth 2.0 with JWT Industry standard token-based auth.",
              "score": null
            },
            {
              "doc_id": "adr_003",
              "chunk_id": "adr_003_fix_2",
              "content": "**Pros:** - Very simple - Low overhead - Easy to implement **Cons:** - No user context - Hard to rotate - No SSO support ## Decision We will implement OAuth 2.0 with JWT tokens. Access tokens expire after 1 hour, refresh tokens after 7 days. ## Consequences - Implement token rotation on refresh - Support API keys for machine-to-machine auth - Add MFA for admin accounts",
              "score": null
            },
            {
              "doc_id": "adr_007",
              "chunk_id": "adr_007_fix_0",
              "content": "# ADR-007: Implement Rate Limiting at API Gateway **Status:** Accepted **Date:** 2024-09-03 ## Context We need to protect our services from abuse and ensure fair resource usage across customers. ## Options Considered ### Option 1: Sliding window at gateway Rate limit at API gateway level.",
              "score": null
            }
          ],
          "latency_ms": 3.4022390009340597,
          "query_id": "simple_003",
          "dimension": "contextual",
          "query": "implementing token refresh logic, need to know when access tokens expire"
        },
        {
          "key_facts": [
            "1 hour",
            "7 days",
            "refresh tokens"
          ],
          "found_facts": [
            "1 hour",
            "7 days",
            "refresh tokens"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "adr_003",
            "arch_authentication_service"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "arch_authentication_service",
              "chunk_id": "arch_authentication_service_fix_1",
              "content": "**Technology:** Custom Go service **Scaling:** Horizontal stateless ### Token Service Issues and validates access tokens. Token lifetime is 1 hour with 7-day refresh tokens. **Technology:** JWT with RS256 **Scaling:** Horizontal stateless ### SSO Gateway Integrates with external identity providers. Supports Google, Microsoft, Okta, and custom SAML.",
              "score": null
            },
            {
              "doc_id": "runbook_api_gateway_5xx_errors",
              "chunk_id": "runbook_api_gateway_5xx_errors_fix_2",
              "content": "If 5xx persists > 15 minutes, page on-call. May indicate infrastructure issue. ## Post-Incident - Document timeline in incident log - Schedule post-mortem within 48 hours - Update runbook if new patterns discovered",
              "score": null
            },
            {
              "doc_id": "runbook_api_gateway_5xx_errors",
              "chunk_id": "runbook_api_gateway_5xx_errors_fix_0",
              "content": "# Runbook: API Gateway 5xx Errors ## Severity **Level:** P3 - Medium **Response Time:** 1 hour ## Symptoms - Error rate > 1% - 5xx responses in access logs - Downstream service failures ## Diagnosis ### Step 1 Check Kong error logs: `kubectl logs -l app=kong -n kong --tail=100 | grep -E '5[0-9]{2}'` ### Step 2 Verify backend health: `kubectl get pods -n production -o wide` ### Step 3 Check service endpoints: `kubectl get endpoints",
              "score": null
            },
            {
              "doc_id": "runbook_out_of_memory_oom_kills",
              "chunk_id": "runbook_out_of_memory_oom_kills_fix_0",
              "content": "# Runbook: Out of Memory (OOM) Kills ## Severity **Level:** P3 - Medium **Response Time:** 15 minutes ## Symptoms - Pods restarting with OOMKilled status - Memory usage at limit - Application slowdown before restart ## Diagnosis ### Step 1 Check OOM events: `kubectl get events -n production --field-selector reason=OOMKilled` ### Step 2 Review memory usage: `kubectl top pods -n production --sort-by=memory` ### Step 3 Check container limits: `kubectl describe pod <pod-name> -n production |",
              "score": null
            },
            {
              "doc_id": "runbook_ssl_certificate_expiry",
              "chunk_id": "runbook_ssl_certificate_expiry_fix_0",
              "content": "# Runbook: SSL Certificate Expiry ## Severity **Level:** P3 - Medium **Response Time:** 15 minutes ## Symptoms - Certificate expiring within 7 days - SSL handshake failures - Browser security warnings ## Diagnosis ### Step 1 Check certificate expiry: `echo | openssl s_client -connect api.cloudflow.io:443 2>/dev/null | openssl x509 -noout -dates` ### Step 2 Verify cert-manager status: `kubectl get certificates -n production` ### Step 3 Check cert-manager logs: `kubectl logs -l app=cert-manager -n cert-manager --tail=50`",
              "score": null
            }
          ],
          "latency_ms": 3.3270090007135877,
          "query_id": "simple_003",
          "dimension": "negation",
          "query": "why does my token stop working after a while"
        },
        {
          "key_facts": [
            "3600",
            "WORKFLOW_TIMEOUT_SECONDS"
          ],
          "found_facts": [
            "3600",
            "WORKFLOW_TIMEOUT_SECONDS"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "config_workflow_engine"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "config_workflow_engine",
              "chunk_id": "config_workflow_engine_fix_0",
              "content": "# Workflow Engine Configuration Reference ## Overview This document describes all configuration options for Workflow Engine. ## Environment Variables | Variable | Type | Default | Description | |----------|------|---------|-------------| | `WORKFLOW_MAX_STEPS` | integer | 100 | Maximum steps per workflow | | `WORKFLOW_TIMEOUT_SECONDS` | integer | 3600 | Maximum workflow execution time | | `WORKFLOW_RETRY_LIMIT` | integer | 3 | Default retry attempts for failed steps | | `WORKFLOW_CONCURRENT_LIMIT` | integer | 10 | Maximum",
              "score": null
            },
            {
              "doc_id": "config_workflow_engine",
              "chunk_id": "config_workflow_engine_fix_1",
              "content": "concurrent executions per workflow | | `EXECUTOR_POOL_SIZE` | integer | 50 | Number of executor workers | | `EXECUTOR_MEMORY_LIMIT` | string | 512Mi | Memory limit per executor | ## Configuration File Configuration can also be provided via `workflow engine.yaml`: ```yaml workflow_max_steps: 100 workflow_timeout_seconds: 3600 workflow_retry_limit: 3 workflow_concurrent_limit: 10 executor_pool_size: 50 ``` ## Validation Configuration is validated at startup. Invalid configuration will prevent the service from starting.",
              "score": null
            },
            {
              "doc_id": "arch_workflow_engine",
              "chunk_id": "arch_workflow_engine_fix_0",
              "content": "# Workflow Engine Architecture ## Overview The Workflow Engine is the core component responsible for executing user-defined workflows. It uses an event-driven architecture with support for parallel execution, conditional branching, and automatic retries. ## Design Principles - **Idempotency**: All operations can be safely retried without side effects - **Eventual Consistency**: State converges to consistent view within 5 seconds - **Fault Tolerance**: System continues operating despite component failures ## Components ### Scheduler Manages workflow scheduling and",
              "score": null
            },
            {
              "doc_id": "api_executions",
              "chunk_id": "api_executions_fix_1",
              "content": "**Parameters:** - `workflow_id` (string): Filter by workflow ID - `since` (datetime): Filter executions after this timestamp **Response:** - Success: `200` - Returns array of execution objects - Error: `400` - Bad Request **Example:** ```json {\"executions\": [{\"id\": \"ex_789\", \"status\": \"completed\", \"duration_ms\": 1523}]} ``` ### POST /executions/{id}/cancel Cancel a running execution.",
              "score": null
            },
            {
              "doc_id": "arch_notification_service",
              "chunk_id": "arch_notification_service_fix_1",
              "content": "Maximum 10000 emails per hour per account. **Technology:** SendGrid integration **Scaling:** API rate limited ### Slack Integration Posts messages to Slack channels. Supports message formatting and interactive components. **Technology:** Slack Bolt SDK **Scaling:** Per-workspace rate limits ## Data Flow Event \u2192 Template \u2192 Dispatcher \u2192 Channel \u2192 Delivery Confirmation ## Performance Characteristics - **Latency P50:** 200ms - **Latency P99:** 2000ms - **Throughput:** 10000 notifications/minute",
              "score": null
            }
          ],
          "latency_ms": 3.4527839998190757,
          "query_id": "simple_004",
          "dimension": "original",
          "query": "What is the maximum workflow execution time?"
        },
        {
          "key_facts": [
            "3600",
            "WORKFLOW_TIMEOUT_SECONDS"
          ],
          "found_facts": [
            "3600",
            "WORKFLOW_TIMEOUT_SECONDS"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "config_workflow_engine"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "config_workflow_engine",
              "chunk_id": "config_workflow_engine_fix_0",
              "content": "# Workflow Engine Configuration Reference ## Overview This document describes all configuration options for Workflow Engine. ## Environment Variables | Variable | Type | Default | Description | |----------|------|---------|-------------| | `WORKFLOW_MAX_STEPS` | integer | 100 | Maximum steps per workflow | | `WORKFLOW_TIMEOUT_SECONDS` | integer | 3600 | Maximum workflow execution time | | `WORKFLOW_RETRY_LIMIT` | integer | 3 | Default retry attempts for failed steps | | `WORKFLOW_CONCURRENT_LIMIT` | integer | 10 | Maximum",
              "score": null
            },
            {
              "doc_id": "howto_implement_retry_logic",
              "chunk_id": "howto_implement_retry_logic_fix_0",
              "content": "# How to Implement Retry Logic ## Prerequisites - Existing workflow ## Steps ### Step 1: Define retry policy Add retry configuration to your workflow step. > **Note:** Maximum 5 retry attempts allowed ### Step 2: Configure backoff Set exponential backoff with base delay of 1 second. The formula is: delay = base * 2^attempt. ### Step 3: Add timeout Set maximum execution time to prevent infinite loops. Default timeout is 30 seconds.",
              "score": null
            },
            {
              "doc_id": "config_workflow_engine",
              "chunk_id": "config_workflow_engine_fix_1",
              "content": "concurrent executions per workflow | | `EXECUTOR_POOL_SIZE` | integer | 50 | Number of executor workers | | `EXECUTOR_MEMORY_LIMIT` | string | 512Mi | Memory limit per executor | ## Configuration File Configuration can also be provided via `workflow engine.yaml`: ```yaml workflow_max_steps: 100 workflow_timeout_seconds: 3600 workflow_retry_limit: 3 workflow_concurrent_limit: 10 executor_pool_size: 50 ``` ## Validation Configuration is validated at startup. Invalid configuration will prevent the service from starting.",
              "score": null
            },
            {
              "doc_id": "api_workflows",
              "chunk_id": "api_workflows_fix_1",
              "content": "**Parameters:** - `limit` (integer): Maximum number of results (default: 50, max: 200) - `status` (string): Filter by status: active, paused, archived **Response:** - Success: `200` - Returns array of workflow objects - Error: `400` - Bad Request **Example:** ```json {\"workflows\": [{\"id\": \"wf_123\", \"name\": \"Daily Report\", \"status\": \"active\"}]} ``` ### POST /workflows Create a new workflow.",
              "score": null
            },
            {
              "doc_id": "howto_create_api_integration",
              "chunk_id": "howto_create_api_integration_fix_1",
              "content": "```bash curl -H 'Authorization: Bearer cf_...' https://api.cloudflow.io/v1/me ``` ### Step 4: Implement rate limiting Handle rate limit responses (HTTP 429) with exponential backoff. > **Note:** Default limit is 100 requests per minute ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            }
          ],
          "latency_ms": 4.423210999448202,
          "query_id": "simple_004",
          "dimension": "synonym",
          "query": "what's the workflow timeout limit"
        },
        {
          "key_facts": [
            "3600",
            "WORKFLOW_TIMEOUT_SECONDS"
          ],
          "found_facts": [
            "3600",
            "WORKFLOW_TIMEOUT_SECONDS"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "config_workflow_engine"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "howto_debug_failed_execution",
              "chunk_id": "howto_debug_failed_execution_fix_1",
              "content": "```bash cloudflow executions inspect ex_123 --step 3 ``` ### Step 4: Enable debug mode Re-run with verbose logging enabled. ```bash cloudflow workflows execute wf_123 --debug ``` ### Step 5: Check error patterns Look for similar failures in recent executions. ```bash cloudflow executions list --status failed --since 24h ``` ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            },
            {
              "doc_id": "howto_create_scheduled_workflow",
              "chunk_id": "howto_create_scheduled_workflow_fix_1",
              "content": "> **Note:** Use IANA timezone names like America/New_York ### Step 3: Configure overlap handling Choose behavior when previous execution is still running: skip, queue, or cancel. ### Step 4: Enable notifications Get notified on schedule failures. ```bash cloudflow workflows notify --on-failure ``` ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            },
            {
              "doc_id": "howto_implement_retry_logic",
              "chunk_id": "howto_implement_retry_logic_fix_1",
              "content": "### Step 4: Handle final failure Configure on_failure handler to notify or escalate when all retries are exhausted. ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            },
            {
              "doc_id": "config_workflow_engine",
              "chunk_id": "config_workflow_engine_fix_1",
              "content": "concurrent executions per workflow | | `EXECUTOR_POOL_SIZE` | integer | 50 | Number of executor workers | | `EXECUTOR_MEMORY_LIMIT` | string | 512Mi | Memory limit per executor | ## Configuration File Configuration can also be provided via `workflow engine.yaml`: ```yaml workflow_max_steps: 100 workflow_timeout_seconds: 3600 workflow_retry_limit: 3 workflow_concurrent_limit: 10 executor_pool_size: 50 ``` ## Validation Configuration is validated at startup. Invalid configuration will prevent the service from starting.",
              "score": null
            },
            {
              "doc_id": "arch_workflow_engine",
              "chunk_id": "arch_workflow_engine_fix_0",
              "content": "# Workflow Engine Architecture ## Overview The Workflow Engine is the core component responsible for executing user-defined workflows. It uses an event-driven architecture with support for parallel execution, conditional branching, and automatic retries. ## Design Principles - **Idempotency**: All operations can be safely retried without side effects - **Eventual Consistency**: State converges to consistent view within 5 seconds - **Fault Tolerance**: System continues operating despite component failures ## Components ### Scheduler Manages workflow scheduling and",
              "score": null
            }
          ],
          "latency_ms": 4.009471000244957,
          "query_id": "simple_004",
          "dimension": "problem",
          "query": "my long-running workflow keeps getting killed before it finishes"
        },
        {
          "key_facts": [
            "3600",
            "WORKFLOW_TIMEOUT_SECONDS"
          ],
          "found_facts": [
            "3600",
            "WORKFLOW_TIMEOUT_SECONDS"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "config_workflow_engine"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "howto_implement_retry_logic",
              "chunk_id": "howto_implement_retry_logic_fix_0",
              "content": "# How to Implement Retry Logic ## Prerequisites - Existing workflow ## Steps ### Step 1: Define retry policy Add retry configuration to your workflow step. > **Note:** Maximum 5 retry attempts allowed ### Step 2: Configure backoff Set exponential backoff with base delay of 1 second. The formula is: delay = base * 2^attempt. ### Step 3: Add timeout Set maximum execution time to prevent infinite loops. Default timeout is 30 seconds.",
              "score": null
            },
            {
              "doc_id": "howto_create_scheduled_workflow",
              "chunk_id": "howto_create_scheduled_workflow_fix_1",
              "content": "> **Note:** Use IANA timezone names like America/New_York ### Step 3: Configure overlap handling Choose behavior when previous execution is still running: skip, queue, or cancel. ### Step 4: Enable notifications Get notified on schedule failures. ```bash cloudflow workflows notify --on-failure ``` ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            },
            {
              "doc_id": "config_workflow_engine",
              "chunk_id": "config_workflow_engine_fix_0",
              "content": "# Workflow Engine Configuration Reference ## Overview This document describes all configuration options for Workflow Engine. ## Environment Variables | Variable | Type | Default | Description | |----------|------|---------|-------------| | `WORKFLOW_MAX_STEPS` | integer | 100 | Maximum steps per workflow | | `WORKFLOW_TIMEOUT_SECONDS` | integer | 3600 | Maximum workflow execution time | | `WORKFLOW_RETRY_LIMIT` | integer | 3 | Default retry attempts for failed steps | | `WORKFLOW_CONCURRENT_LIMIT` | integer | 10 | Maximum",
              "score": null
            },
            {
              "doc_id": "howto_implement_retry_logic",
              "chunk_id": "howto_implement_retry_logic_fix_1",
              "content": "### Step 4: Handle final failure Configure on_failure handler to notify or escalate when all retries are exhausted. ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            },
            {
              "doc_id": "api_workflows",
              "chunk_id": "api_workflows_fix_1",
              "content": "**Parameters:** - `limit` (integer): Maximum number of results (default: 50, max: 200) - `status` (string): Filter by status: active, paused, archived **Response:** - Success: `200` - Returns array of workflow objects - Error: `400` - Bad Request **Example:** ```json {\"workflows\": [{\"id\": \"wf_123\", \"name\": \"Daily Report\", \"status\": \"active\"}]} ``` ### POST /workflows Create a new workflow.",
              "score": null
            }
          ],
          "latency_ms": 3.4463610008970136,
          "query_id": "simple_004",
          "dimension": "casual",
          "query": "workflow timeout"
        },
        {
          "key_facts": [
            "3600",
            "WORKFLOW_TIMEOUT_SECONDS"
          ],
          "found_facts": [
            "3600",
            "WORKFLOW_TIMEOUT_SECONDS"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "config_workflow_engine"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "config_workflow_engine",
              "chunk_id": "config_workflow_engine_fix_0",
              "content": "# Workflow Engine Configuration Reference ## Overview This document describes all configuration options for Workflow Engine. ## Environment Variables | Variable | Type | Default | Description | |----------|------|---------|-------------| | `WORKFLOW_MAX_STEPS` | integer | 100 | Maximum steps per workflow | | `WORKFLOW_TIMEOUT_SECONDS` | integer | 3600 | Maximum workflow execution time | | `WORKFLOW_RETRY_LIMIT` | integer | 3 | Default retry attempts for failed steps | | `WORKFLOW_CONCURRENT_LIMIT` | integer | 10 | Maximum",
              "score": null
            },
            {
              "doc_id": "arch_data_pipeline",
              "chunk_id": "arch_data_pipeline_fix_0",
              "content": "# Data Pipeline Architecture ## Overview The Data Pipeline processes and transforms data between workflow steps. It supports batch processing up to 10GB and streaming for real-time use cases. ## Design Principles - **Exactly-Once Processing**: Each record is processed exactly once - **Backpressure Handling**: System gracefully handles varying load ## Components ### Stream Processor Handles real-time data streams. Maximum throughput of 100000 messages per second.",
              "score": null
            },
            {
              "doc_id": "arch_data_pipeline",
              "chunk_id": "arch_data_pipeline_fix_2",
              "content": "**Technology:** S3 with Iceberg tables **Scaling:** Unlimited with tiered storage ## Data Flow Input \u2192 Validation \u2192 Transform \u2192 Enrich \u2192 Output ## Performance Characteristics - **Latency P50:** 100ms (streaming), 5min (batch) - **Latency P99:** 500ms (streaming), 30min (batch) - **Throughput:** 100000 messages/second (streaming)",
              "score": null
            },
            {
              "doc_id": "config_workflow_engine",
              "chunk_id": "config_workflow_engine_fix_1",
              "content": "concurrent executions per workflow | | `EXECUTOR_POOL_SIZE` | integer | 50 | Number of executor workers | | `EXECUTOR_MEMORY_LIMIT` | string | 512Mi | Memory limit per executor | ## Configuration File Configuration can also be provided via `workflow engine.yaml`: ```yaml workflow_max_steps: 100 workflow_timeout_seconds: 3600 workflow_retry_limit: 3 workflow_concurrent_limit: 10 executor_pool_size: 50 ``` ## Validation Configuration is validated at startup. Invalid configuration will prevent the service from starting.",
              "score": null
            },
            {
              "doc_id": "arch_workflow_engine",
              "chunk_id": "arch_workflow_engine_fix_0",
              "content": "# Workflow Engine Architecture ## Overview The Workflow Engine is the core component responsible for executing user-defined workflows. It uses an event-driven architecture with support for parallel execution, conditional branching, and automatic retries. ## Design Principles - **Idempotency**: All operations can be safely retried without side effects - **Eventual Consistency**: State converges to consistent view within 5 seconds - **Fault Tolerance**: System continues operating despite component failures ## Components ### Scheduler Manages workflow scheduling and",
              "score": null
            }
          ],
          "latency_ms": 3.4376459989289287,
          "query_id": "simple_004",
          "dimension": "contextual",
          "query": "planning a data migration workflow, need to know the max execution duration"
        },
        {
          "key_facts": [
            "3600",
            "WORKFLOW_TIMEOUT_SECONDS"
          ],
          "found_facts": [],
          "missed_facts": [
            "3600",
            "WORKFLOW_TIMEOUT_SECONDS"
          ],
          "coverage": 0.0,
          "expected_docs": [
            "config_workflow_engine"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "howto_debug_failed_execution",
              "chunk_id": "howto_debug_failed_execution_fix_1",
              "content": "```bash cloudflow executions inspect ex_123 --step 3 ``` ### Step 4: Enable debug mode Re-run with verbose logging enabled. ```bash cloudflow workflows execute wf_123 --debug ``` ### Step 5: Check error patterns Look for similar failures in recent executions. ```bash cloudflow executions list --status failed --since 24h ``` ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            },
            {
              "doc_id": "howto_implement_retry_logic",
              "chunk_id": "howto_implement_retry_logic_fix_1",
              "content": "### Step 4: Handle final failure Configure on_failure handler to notify or escalate when all retries are exhausted. ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            },
            {
              "doc_id": "api_workflows",
              "chunk_id": "api_workflows_fix_1",
              "content": "**Parameters:** - `limit` (integer): Maximum number of results (default: 50, max: 200) - `status` (string): Filter by status: active, paused, archived **Response:** - Success: `200` - Returns array of workflow objects - Error: `400` - Bad Request **Example:** ```json {\"workflows\": [{\"id\": \"wf_123\", \"name\": \"Daily Report\", \"status\": \"active\"}]} ``` ### POST /workflows Create a new workflow.",
              "score": null
            },
            {
              "doc_id": "howto_set_up_database_connection",
              "chunk_id": "howto_set_up_database_connection_fix_1",
              "content": "> **Note:** Pool exhaustion will queue requests ### Step 3: Test connection Verify the database is accessible from CloudFlow. ```bash cloudflow db test --secret DATABASE_URL ``` ### Step 4: Enable SSL For production, always require SSL connections. ```bash cloudflow db configure --require-ssl ``` ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            },
            {
              "doc_id": "howto_debug_failed_execution",
              "chunk_id": "howto_debug_failed_execution_fix_0",
              "content": "# How to Debug Failed Execution ## Prerequisites - Failed execution ID ## Steps ### Step 1: Get execution details Retrieve the full execution record including step outputs. ```bash cloudflow executions get ex_123 --include-steps ``` ### Step 2: View step logs Check logs for the failed step. ```bash cloudflow executions logs ex_123 --step 3 ``` ### Step 3: Inspect input/output Review the data passed between steps.",
              "score": null
            }
          ],
          "latency_ms": 3.265565001129289,
          "query_id": "simple_004",
          "dimension": "negation",
          "query": "why does my workflow fail after running for a long time"
        },
        {
          "key_facts": [
            "Kubernetes",
            "EKS",
            "Helm"
          ],
          "found_facts": [
            "Kubernetes",
            "EKS",
            "Helm"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "adr_002"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "adr_002",
              "chunk_id": "adr_002_fix_0",
              "content": "# ADR-002: Adopt Kubernetes for Container Orchestration **Status:** Accepted **Date:** 2024-01-24 ## Context We need a platform to deploy and scale our microservices. The platform must support auto-scaling, rolling deployments, and multi-region deployment. ## Options Considered ### Option 1: Kubernetes (EKS) Industry-standard container orchestration. **Pros:** - Industry standard - Rich ecosystem - Cloud agnostic - Auto-scaling **Cons:** - Operational complexity - Learning curve - Resource overhead ### Option 2: AWS ECS Amazon's native container service.",
              "score": null
            },
            {
              "doc_id": "api_integrations",
              "chunk_id": "api_integrations_fix_0",
              "content": "# Integrations API Reference ## Overview The Integrations API provides programmatic access to integrations functionality. Base URL: `https://api.cloudflow.io/v1/integrations` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /integrations List available integrations.",
              "score": null
            },
            {
              "doc_id": "runbook_high_cpu_usage",
              "chunk_id": "runbook_high_cpu_usage_fix_1",
              "content": "Step 4 Profile the application: `cloudflow debug profile --pod api-xyz --duration 60s` ## Resolution ### Step 1 Scale up if traffic-related: `kubectl scale deployment/api --replicas=10 -n production` ### Step 2 Restart pods if memory leak suspected: `kubectl rollout restart deployment/api -n production` ### Step 3 Roll back recent deployment if regression: `kubectl rollout undo deployment/api -n production` ### Step 4 Add resource limits if unbounded: update deployment with CPU limits ## Escalation If CPU remains",
              "score": null
            },
            {
              "doc_id": "arch_api_gateway",
              "chunk_id": "arch_api_gateway_fix_0",
              "content": "# API Gateway Architecture ## Overview The API Gateway handles all incoming HTTP requests, providing authentication, rate limiting, and request routing. Built on Kong with custom plugins for CloudFlow-specific functionality. ## Design Principles - **Zero Trust**: Every request is authenticated and authorized - **Defense in Depth**: Multiple layers of security validation ## Components ### Kong Gateway Primary ingress controller. Handles TLS termination, request routing, and plugin execution.",
              "score": null
            },
            {
              "doc_id": "adr_002",
              "chunk_id": "adr_002_fix_1",
              "content": "**Pros:** - Simple operation - Native AWS integration - Lower overhead **Cons:** - AWS lock-in - Less flexible - Smaller community ### Option 3: Serverless (Lambda) Function-as-a-service approach. **Pros:** - Zero infrastructure - Pay per use - Auto-scaling **Cons:** - Cold starts - 15-minute limit - Vendor lock-in ## Decision We will use Kubernetes on EKS for container orchestration with Helm for deployments.",
              "score": null
            }
          ],
          "latency_ms": 3.1710479997855145,
          "query_id": "simple_005",
          "dimension": "original",
          "query": "What container orchestration does CloudFlow use?"
        },
        {
          "key_facts": [
            "Kubernetes",
            "EKS",
            "Helm"
          ],
          "found_facts": [
            "Kubernetes",
            "EKS",
            "Helm"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "adr_002"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "adr_002",
              "chunk_id": "adr_002_fix_0",
              "content": "# ADR-002: Adopt Kubernetes for Container Orchestration **Status:** Accepted **Date:** 2024-01-24 ## Context We need a platform to deploy and scale our microservices. The platform must support auto-scaling, rolling deployments, and multi-region deployment. ## Options Considered ### Option 1: Kubernetes (EKS) Industry-standard container orchestration. **Pros:** - Industry standard - Rich ecosystem - Cloud agnostic - Auto-scaling **Cons:** - Operational complexity - Learning curve - Resource overhead ### Option 2: AWS ECS Amazon's native container service.",
              "score": null
            },
            {
              "doc_id": "api_integrations",
              "chunk_id": "api_integrations_fix_0",
              "content": "# Integrations API Reference ## Overview The Integrations API provides programmatic access to integrations functionality. Base URL: `https://api.cloudflow.io/v1/integrations` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /integrations List available integrations.",
              "score": null
            },
            {
              "doc_id": "runbook_high_cpu_usage",
              "chunk_id": "runbook_high_cpu_usage_fix_1",
              "content": "Step 4 Profile the application: `cloudflow debug profile --pod api-xyz --duration 60s` ## Resolution ### Step 1 Scale up if traffic-related: `kubectl scale deployment/api --replicas=10 -n production` ### Step 2 Restart pods if memory leak suspected: `kubectl rollout restart deployment/api -n production` ### Step 3 Roll back recent deployment if regression: `kubectl rollout undo deployment/api -n production` ### Step 4 Add resource limits if unbounded: update deployment with CPU limits ## Escalation If CPU remains",
              "score": null
            },
            {
              "doc_id": "arch_api_gateway",
              "chunk_id": "arch_api_gateway_fix_0",
              "content": "# API Gateway Architecture ## Overview The API Gateway handles all incoming HTTP requests, providing authentication, rate limiting, and request routing. Built on Kong with custom plugins for CloudFlow-specific functionality. ## Design Principles - **Zero Trust**: Every request is authenticated and authorized - **Defense in Depth**: Multiple layers of security validation ## Components ### Kong Gateway Primary ingress controller. Handles TLS termination, request routing, and plugin execution.",
              "score": null
            },
            {
              "doc_id": "adr_002",
              "chunk_id": "adr_002_fix_1",
              "content": "**Pros:** - Simple operation - Native AWS integration - Lower overhead **Cons:** - AWS lock-in - Less flexible - Smaller community ### Option 3: Serverless (Lambda) Function-as-a-service approach. **Pros:** - Zero infrastructure - Pay per use - Auto-scaling **Cons:** - Cold starts - 15-minute limit - Vendor lock-in ## Decision We will use Kubernetes on EKS for container orchestration with Helm for deployments.",
              "score": null
            }
          ],
          "latency_ms": 3.1691950007370906,
          "query_id": "simple_005",
          "dimension": "synonym",
          "query": "what platform manages CloudFlow's containers"
        },
        {
          "key_facts": [
            "Kubernetes",
            "EKS",
            "Helm"
          ],
          "found_facts": [],
          "missed_facts": [
            "Kubernetes",
            "EKS",
            "Helm"
          ],
          "coverage": 0.0,
          "expected_docs": [
            "adr_002"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "runbook_api_gateway_5xx_errors",
              "chunk_id": "runbook_api_gateway_5xx_errors_fix_1",
              "content": "-n production` ### Step 4 Test backend directly: `kubectl exec -it kong-xxx -n kong -- curl http://api.production.svc:8080/health` ## Resolution ### Step 1 Restart unhealthy backends: `kubectl rollout restart deployment/api -n production` ### Step 2 Enable circuit breaker if cascading: update Kong circuit-breaker plugin ### Step 3 Scale up backends: `kubectl scale deployment/api --replicas=5 -n production` ### Step 4 Check for resource exhaustion on backend pods ### Step 5 Verify network policies allow traffic ## Escalation",
              "score": null
            },
            {
              "doc_id": "runbook_high_cpu_usage",
              "chunk_id": "runbook_high_cpu_usage_fix_0",
              "content": "# Runbook: High CPU Usage ## Severity **Level:** P1 - Critical **Response Time:** 15 minutes ## Symptoms - CPU utilization > 80% sustained - Increased response latency - Autoscaler at maximum replicas ## Diagnosis ### Step 1 Check current CPU usage: `kubectl top pods -n production` ### Step 2 Identify hot pods: `kubectl top pods --sort-by=cpu -n production | head -10` ### Step 3 Check for recent deployments: `kubectl rollout history deployment/api -n production` ###",
              "score": null
            },
            {
              "doc_id": "runbook_high_cpu_usage",
              "chunk_id": "runbook_high_cpu_usage_fix_1",
              "content": "Step 4 Profile the application: `cloudflow debug profile --pod api-xyz --duration 60s` ## Resolution ### Step 1 Scale up if traffic-related: `kubectl scale deployment/api --replicas=10 -n production` ### Step 2 Restart pods if memory leak suspected: `kubectl rollout restart deployment/api -n production` ### Step 3 Roll back recent deployment if regression: `kubectl rollout undo deployment/api -n production` ### Step 4 Add resource limits if unbounded: update deployment with CPU limits ## Escalation If CPU remains",
              "score": null
            },
            {
              "doc_id": "runbook_out_of_memory_oom_kills",
              "chunk_id": "runbook_out_of_memory_oom_kills_fix_0",
              "content": "# Runbook: Out of Memory (OOM) Kills ## Severity **Level:** P3 - Medium **Response Time:** 15 minutes ## Symptoms - Pods restarting with OOMKilled status - Memory usage at limit - Application slowdown before restart ## Diagnosis ### Step 1 Check OOM events: `kubectl get events -n production --field-selector reason=OOMKilled` ### Step 2 Review memory usage: `kubectl top pods -n production --sort-by=memory` ### Step 3 Check container limits: `kubectl describe pod <pod-name> -n production |",
              "score": null
            },
            {
              "doc_id": "runbook_api_gateway_5xx_errors",
              "chunk_id": "runbook_api_gateway_5xx_errors_fix_0",
              "content": "# Runbook: API Gateway 5xx Errors ## Severity **Level:** P3 - Medium **Response Time:** 1 hour ## Symptoms - Error rate > 1% - 5xx responses in access logs - Downstream service failures ## Diagnosis ### Step 1 Check Kong error logs: `kubectl logs -l app=kong -n kong --tail=100 | grep -E '5[0-9]{2}'` ### Step 2 Verify backend health: `kubectl get pods -n production -o wide` ### Step 3 Check service endpoints: `kubectl get endpoints",
              "score": null
            }
          ],
          "latency_ms": 4.363710999314208,
          "query_id": "simple_005",
          "dimension": "problem",
          "query": "seeing kubectl commands in the docs, so it's running on k8s?"
        },
        {
          "key_facts": [
            "Kubernetes",
            "EKS",
            "Helm"
          ],
          "found_facts": [
            "Kubernetes",
            "EKS",
            "Helm"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "adr_002"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "adr_002",
              "chunk_id": "adr_002_fix_1",
              "content": "**Pros:** - Simple operation - Native AWS integration - Lower overhead **Cons:** - AWS lock-in - Less flexible - Smaller community ### Option 3: Serverless (Lambda) Function-as-a-service approach. **Pros:** - Zero infrastructure - Pay per use - Auto-scaling **Cons:** - Cold starts - 15-minute limit - Vendor lock-in ## Decision We will use Kubernetes on EKS for container orchestration with Helm for deployments.",
              "score": null
            },
            {
              "doc_id": "adr_002",
              "chunk_id": "adr_002_fix_0",
              "content": "# ADR-002: Adopt Kubernetes for Container Orchestration **Status:** Accepted **Date:** 2024-01-24 ## Context We need a platform to deploy and scale our microservices. The platform must support auto-scaling, rolling deployments, and multi-region deployment. ## Options Considered ### Option 1: Kubernetes (EKS) Industry-standard container orchestration. **Pros:** - Industry standard - Rich ecosystem - Cloud agnostic - Auto-scaling **Cons:** - Operational complexity - Learning curve - Resource overhead ### Option 2: AWS ECS Amazon's native container service.",
              "score": null
            },
            {
              "doc_id": "adr_006",
              "chunk_id": "adr_006_fix_1",
              "content": "**Pros:** - Native AWS support - No state file - Drift detection **Cons:** - AWS only - Verbose YAML/JSON - Slower development ### Option 3: Pulumi IaC with real programming languages. **Pros:** - Real languages - Type safety - Reusable components **Cons:** - Smaller community - Vendor lock-in - Commercial features ## Decision We will use Terraform with S3 backend for state and DynamoDB for locking.",
              "score": null
            },
            {
              "doc_id": "runbook_high_cpu_usage",
              "chunk_id": "runbook_high_cpu_usage_fix_1",
              "content": "Step 4 Profile the application: `cloudflow debug profile --pod api-xyz --duration 60s` ## Resolution ### Step 1 Scale up if traffic-related: `kubectl scale deployment/api --replicas=10 -n production` ### Step 2 Restart pods if memory leak suspected: `kubectl rollout restart deployment/api -n production` ### Step 3 Roll back recent deployment if regression: `kubectl rollout undo deployment/api -n production` ### Step 4 Add resource limits if unbounded: update deployment with CPU limits ## Escalation If CPU remains",
              "score": null
            },
            {
              "doc_id": "howto_set_up_data_encryption",
              "chunk_id": "howto_set_up_data_encryption_fix_0",
              "content": "# How to Set Up Data Encryption ## Prerequisites - CloudFlow Enterprise plan ## Steps ### Step 1: Enable encryption at rest All data is encrypted at rest using AES-256 by default. > **Note:** Encryption is automatic for all plans ### Step 2: Configure KMS Bring your own encryption keys using AWS KMS or GCP KMS. ```bash cloudflow encryption configure --kms-key arn:aws:kms:...",
              "score": null
            }
          ],
          "latency_ms": 3.4992999990208773,
          "query_id": "simple_005",
          "dimension": "casual",
          "query": "cloudflow k8s or ecs?"
        },
        {
          "key_facts": [
            "Kubernetes",
            "EKS",
            "Helm"
          ],
          "found_facts": [
            "Kubernetes",
            "EKS",
            "Helm"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "adr_002"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "adr_002",
              "chunk_id": "adr_002_fix_0",
              "content": "# ADR-002: Adopt Kubernetes for Container Orchestration **Status:** Accepted **Date:** 2024-01-24 ## Context We need a platform to deploy and scale our microservices. The platform must support auto-scaling, rolling deployments, and multi-region deployment. ## Options Considered ### Option 1: Kubernetes (EKS) Industry-standard container orchestration. **Pros:** - Industry standard - Rich ecosystem - Cloud agnostic - Auto-scaling **Cons:** - Operational complexity - Learning curve - Resource overhead ### Option 2: AWS ECS Amazon's native container service.",
              "score": null
            },
            {
              "doc_id": "adr_007",
              "chunk_id": "adr_007_fix_1",
              "content": "**Pros:** - Centralized enforcement - Low latency - Easy configuration **Cons:** - Gateway becomes bottleneck - Coarse granularity - Redis dependency ### Option 2: Per-service limiting Each service implements its own limits. **Pros:** - Fine-grained control - Service autonomy - No single point of failure **Cons:** - Inconsistent implementation - Higher complexity - Distributed state ### Option 3: Token bucket Token-based rate limiting.",
              "score": null
            },
            {
              "doc_id": "adr_002",
              "chunk_id": "adr_002_fix_1",
              "content": "**Pros:** - Simple operation - Native AWS integration - Lower overhead **Cons:** - AWS lock-in - Less flexible - Smaller community ### Option 3: Serverless (Lambda) Function-as-a-service approach. **Pros:** - Zero infrastructure - Pay per use - Auto-scaling **Cons:** - Cold starts - 15-minute limit - Vendor lock-in ## Decision We will use Kubernetes on EKS for container orchestration with Helm for deployments.",
              "score": null
            },
            {
              "doc_id": "arch_caching_layer",
              "chunk_id": "arch_caching_layer_fix_1",
              "content": "**Technology:** Ristretto in-memory cache **Scaling:** Per-pod memory allocation ### Distributed Cache Second-level cache with configurable TTL. Maximum key size 512MB. **Technology:** Redis Cluster 7.x **Scaling:** 6-node cluster with sharding ### CDN Caches static assets and API responses. Cache hit ratio target 95%.",
              "score": null
            },
            {
              "doc_id": "arch_data_pipeline",
              "chunk_id": "arch_data_pipeline_fix_1",
              "content": "**Technology:** Apache Kafka with Flink **Scaling:** Partition-based scaling ### Batch Processor Processes large batch jobs. Supports data formats: JSON, CSV, Parquet, Avro. **Technology:** Apache Spark on Kubernetes **Scaling:** Dynamic resource allocation ### Data Lake Long-term data storage with time-travel queries. Retention configurable up to 7 years.",
              "score": null
            }
          ],
          "latency_ms": 3.5017949994653463,
          "query_id": "simple_005",
          "dimension": "contextual",
          "query": "need to understand the deployment platform for capacity planning"
        },
        {
          "key_facts": [
            "Kubernetes",
            "EKS",
            "Helm"
          ],
          "found_facts": [],
          "missed_facts": [
            "Kubernetes",
            "EKS",
            "Helm"
          ],
          "coverage": 0.0,
          "expected_docs": [
            "adr_002"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "howto_set_up_database_connection",
              "chunk_id": "howto_set_up_database_connection_fix_1",
              "content": "> **Note:** Pool exhaustion will queue requests ### Step 3: Test connection Verify the database is accessible from CloudFlow. ```bash cloudflow db test --secret DATABASE_URL ``` ### Step 4: Enable SSL For production, always require SSL connections. ```bash cloudflow db configure --require-ssl ``` ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            },
            {
              "doc_id": "release_3_0_0",
              "chunk_id": "release_3_0_0_fix_1",
              "content": "```bash cloudflow upgrade --version 3.0.0 ``` ## Known Issues None at this time.",
              "score": null
            },
            {
              "doc_id": "howto_migrate_from_v1_to_v2_api",
              "chunk_id": "howto_migrate_from_v1_to_v2_api_fix_1",
              "content": "> **Note:** Generate new credentials in the dashboard ### Step 4: Test in staging Run your integration tests against the v2 staging endpoint. ```bash CLOUDFLOW_API_URL=https://api-v2-staging.cloudflow.io npm test ``` ### Step 5: Switch production Update production configuration to use v2 API. ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            },
            {
              "doc_id": "howto_implement_retry_logic",
              "chunk_id": "howto_implement_retry_logic_fix_1",
              "content": "### Step 4: Handle final failure Configure on_failure handler to notify or escalate when all retries are exhausted. ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            },
            {
              "doc_id": "howto_debug_failed_execution",
              "chunk_id": "howto_debug_failed_execution_fix_0",
              "content": "# How to Debug Failed Execution ## Prerequisites - Failed execution ID ## Steps ### Step 1: Get execution details Retrieve the full execution record including step outputs. ```bash cloudflow executions get ex_123 --include-steps ``` ### Step 2: View step logs Check logs for the failed step. ```bash cloudflow executions logs ex_123 --step 3 ``` ### Step 3: Inspect input/output Review the data passed between steps.",
              "score": null
            }
          ],
          "latency_ms": 3.3863200005725957,
          "query_id": "simple_005",
          "dimension": "negation",
          "query": "CloudFlow isn't running on serverless is it"
        },
        {
          "key_facts": [
            "300",
            "CACHE_TTL_SECONDS"
          ],
          "found_facts": [
            "300",
            "CACHE_TTL_SECONDS"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "config_cache"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "adr_004",
              "chunk_id": "adr_004_fix_2",
              "content": "## Consequences - Implement cache-aside pattern - Set appropriate TTLs - Monitor memory usage - Plan for cache stampede prevention",
              "score": null
            },
            {
              "doc_id": "arch_caching_layer",
              "chunk_id": "arch_caching_layer_fix_0",
              "content": "# Caching Layer Architecture ## Overview The Caching Layer reduces latency and database load by caching frequently accessed data. Uses a multi-tier strategy with local and distributed caches. ## Design Principles - **Cache Invalidation**: Explicit invalidation on writes - **Graceful Degradation**: System works without cache ## Components ### Local Cache First-level cache with 100ms TTL. Reduces Redis round-trips for hot data.",
              "score": null
            },
            {
              "doc_id": "config_cache",
              "chunk_id": "config_cache_fix_0",
              "content": "# Cache Configuration Reference ## Overview This document describes all configuration options for Cache. ## Environment Variables | Variable | Type | Default | Description | |----------|------|---------|-------------| | `REDIS_URL` | string | required | Redis connection string | | `REDIS_POOL_SIZE` | integer | 10 | Connection pool size | | `CACHE_TTL_SECONDS` | integer | 300 | Default cache TTL | | `CACHE_PREFIX` | string | cf: | Key prefix for namespacing | | `CACHE_COMPRESSION` |",
              "score": null
            },
            {
              "doc_id": "arch_caching_layer",
              "chunk_id": "arch_caching_layer_fix_1",
              "content": "**Technology:** Ristretto in-memory cache **Scaling:** Per-pod memory allocation ### Distributed Cache Second-level cache with configurable TTL. Maximum key size 512MB. **Technology:** Redis Cluster 7.x **Scaling:** 6-node cluster with sharding ### CDN Caches static assets and API responses. Cache hit ratio target 95%.",
              "score": null
            },
            {
              "doc_id": "adr_004",
              "chunk_id": "adr_004_fix_0",
              "content": "# ADR-004: Use Redis for Caching and Session Storage **Status:** Accepted **Date:** 2024-04-05 ## Context We need a fast data store for caching API responses, session data, and rate limiting counters. ## Options Considered ### Option 1: Redis Cluster In-memory data structure store. **Pros:** - Sub-millisecond latency - Rich data structures - Pub/sub support - Clustering **Cons:** - Memory cost - Persistence complexity - Cluster management ### Option 2: Memcached Simple key-value cache.",
              "score": null
            }
          ],
          "latency_ms": 3.2543540000915527,
          "query_id": "simple_006",
          "dimension": "original",
          "query": "What is the cache TTL?"
        },
        {
          "key_facts": [
            "300",
            "CACHE_TTL_SECONDS"
          ],
          "found_facts": [
            "300",
            "CACHE_TTL_SECONDS"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "config_cache"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "arch_caching_layer",
              "chunk_id": "arch_caching_layer_fix_0",
              "content": "# Caching Layer Architecture ## Overview The Caching Layer reduces latency and database load by caching frequently accessed data. Uses a multi-tier strategy with local and distributed caches. ## Design Principles - **Cache Invalidation**: Explicit invalidation on writes - **Graceful Degradation**: System works without cache ## Components ### Local Cache First-level cache with 100ms TTL. Reduces Redis round-trips for hot data.",
              "score": null
            },
            {
              "doc_id": "adr_004",
              "chunk_id": "adr_004_fix_0",
              "content": "# ADR-004: Use Redis for Caching and Session Storage **Status:** Accepted **Date:** 2024-04-05 ## Context We need a fast data store for caching API responses, session data, and rate limiting counters. ## Options Considered ### Option 1: Redis Cluster In-memory data structure store. **Pros:** - Sub-millisecond latency - Rich data structures - Pub/sub support - Clustering **Cons:** - Memory cost - Persistence complexity - Cluster management ### Option 2: Memcached Simple key-value cache.",
              "score": null
            },
            {
              "doc_id": "runbook_api_gateway_5xx_errors",
              "chunk_id": "runbook_api_gateway_5xx_errors_fix_2",
              "content": "If 5xx persists > 15 minutes, page on-call. May indicate infrastructure issue. ## Post-Incident - Document timeline in incident log - Schedule post-mortem within 48 hours - Update runbook if new patterns discovered",
              "score": null
            },
            {
              "doc_id": "config_cache",
              "chunk_id": "config_cache_fix_0",
              "content": "# Cache Configuration Reference ## Overview This document describes all configuration options for Cache. ## Environment Variables | Variable | Type | Default | Description | |----------|------|---------|-------------| | `REDIS_URL` | string | required | Redis connection string | | `REDIS_POOL_SIZE` | integer | 10 | Connection pool size | | `CACHE_TTL_SECONDS` | integer | 300 | Default cache TTL | | `CACHE_PREFIX` | string | cf: | Key prefix for namespacing | | `CACHE_COMPRESSION` |",
              "score": null
            },
            {
              "doc_id": "adr_004",
              "chunk_id": "adr_004_fix_2",
              "content": "## Consequences - Implement cache-aside pattern - Set appropriate TTLs - Monitor memory usage - Plan for cache stampede prevention",
              "score": null
            }
          ],
          "latency_ms": 3.264082000896451,
          "query_id": "simple_006",
          "dimension": "synonym",
          "query": "how long does cached data stay valid"
        },
        {
          "key_facts": [
            "300",
            "CACHE_TTL_SECONDS"
          ],
          "found_facts": [
            "300",
            "CACHE_TTL_SECONDS"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "config_cache"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "runbook_api_gateway_5xx_errors",
              "chunk_id": "runbook_api_gateway_5xx_errors_fix_2",
              "content": "If 5xx persists > 15 minutes, page on-call. May indicate infrastructure issue. ## Post-Incident - Document timeline in incident log - Schedule post-mortem within 48 hours - Update runbook if new patterns discovered",
              "score": null
            },
            {
              "doc_id": "arch_caching_layer",
              "chunk_id": "arch_caching_layer_fix_0",
              "content": "# Caching Layer Architecture ## Overview The Caching Layer reduces latency and database load by caching frequently accessed data. Uses a multi-tier strategy with local and distributed caches. ## Design Principles - **Cache Invalidation**: Explicit invalidation on writes - **Graceful Degradation**: System works without cache ## Components ### Local Cache First-level cache with 100ms TTL. Reduces Redis round-trips for hot data.",
              "score": null
            },
            {
              "doc_id": "config_cache",
              "chunk_id": "config_cache_fix_0",
              "content": "# Cache Configuration Reference ## Overview This document describes all configuration options for Cache. ## Environment Variables | Variable | Type | Default | Description | |----------|------|---------|-------------| | `REDIS_URL` | string | required | Redis connection string | | `REDIS_POOL_SIZE` | integer | 10 | Connection pool size | | `CACHE_TTL_SECONDS` | integer | 300 | Default cache TTL | | `CACHE_PREFIX` | string | cf: | Key prefix for namespacing | | `CACHE_COMPRESSION` |",
              "score": null
            },
            {
              "doc_id": "runbook_high_cpu_usage",
              "chunk_id": "runbook_high_cpu_usage_fix_2",
              "content": "high after scaling, escalate to platform team. Page on-call if affecting users. ## Post-Incident - Document timeline in incident log - Schedule post-mortem within 48 hours - Update runbook if new patterns discovered",
              "score": null
            },
            {
              "doc_id": "runbook_out_of_memory_oom_kills",
              "chunk_id": "runbook_out_of_memory_oom_kills_fix_2",
              "content": "increase, escalate to development team for memory leak investigation. ## Post-Incident - Document timeline in incident log - Schedule post-mortem within 48 hours - Update runbook if new patterns discovered",
              "score": null
            }
          ],
          "latency_ms": 4.09252600002219,
          "query_id": "simple_006",
          "dimension": "problem",
          "query": "changes aren't showing up immediately, probably cached - how long til it refreshes"
        },
        {
          "key_facts": [
            "300",
            "CACHE_TTL_SECONDS"
          ],
          "found_facts": [
            "300",
            "CACHE_TTL_SECONDS"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "config_cache"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "arch_caching_layer",
              "chunk_id": "arch_caching_layer_fix_0",
              "content": "# Caching Layer Architecture ## Overview The Caching Layer reduces latency and database load by caching frequently accessed data. Uses a multi-tier strategy with local and distributed caches. ## Design Principles - **Cache Invalidation**: Explicit invalidation on writes - **Graceful Degradation**: System works without cache ## Components ### Local Cache First-level cache with 100ms TTL. Reduces Redis round-trips for hot data.",
              "score": null
            },
            {
              "doc_id": "config_cache",
              "chunk_id": "config_cache_fix_0",
              "content": "# Cache Configuration Reference ## Overview This document describes all configuration options for Cache. ## Environment Variables | Variable | Type | Default | Description | |----------|------|---------|-------------| | `REDIS_URL` | string | required | Redis connection string | | `REDIS_POOL_SIZE` | integer | 10 | Connection pool size | | `CACHE_TTL_SECONDS` | integer | 300 | Default cache TTL | | `CACHE_PREFIX` | string | cf: | Key prefix for namespacing | | `CACHE_COMPRESSION` |",
              "score": null
            },
            {
              "doc_id": "adr_004",
              "chunk_id": "adr_004_fix_0",
              "content": "# ADR-004: Use Redis for Caching and Session Storage **Status:** Accepted **Date:** 2024-04-05 ## Context We need a fast data store for caching API responses, session data, and rate limiting counters. ## Options Considered ### Option 1: Redis Cluster In-memory data structure store. **Pros:** - Sub-millisecond latency - Rich data structures - Pub/sub support - Clustering **Cons:** - Memory cost - Persistence complexity - Cluster management ### Option 2: Memcached Simple key-value cache.",
              "score": null
            },
            {
              "doc_id": "adr_004",
              "chunk_id": "adr_004_fix_2",
              "content": "## Consequences - Implement cache-aside pattern - Set appropriate TTLs - Monitor memory usage - Plan for cache stampede prevention",
              "score": null
            },
            {
              "doc_id": "arch_caching_layer",
              "chunk_id": "arch_caching_layer_fix_1",
              "content": "**Technology:** Ristretto in-memory cache **Scaling:** Per-pod memory allocation ### Distributed Cache Second-level cache with configurable TTL. Maximum key size 512MB. **Technology:** Redis Cluster 7.x **Scaling:** 6-node cluster with sharding ### CDN Caches static assets and API responses. Cache hit ratio target 95%.",
              "score": null
            }
          ],
          "latency_ms": 3.5149699997418793,
          "query_id": "simple_006",
          "dimension": "casual",
          "query": "cache expiry time"
        },
        {
          "key_facts": [
            "300",
            "CACHE_TTL_SECONDS"
          ],
          "found_facts": [
            "300",
            "CACHE_TTL_SECONDS"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "config_cache"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "arch_caching_layer",
              "chunk_id": "arch_caching_layer_fix_0",
              "content": "# Caching Layer Architecture ## Overview The Caching Layer reduces latency and database load by caching frequently accessed data. Uses a multi-tier strategy with local and distributed caches. ## Design Principles - **Cache Invalidation**: Explicit invalidation on writes - **Graceful Degradation**: System works without cache ## Components ### Local Cache First-level cache with 100ms TTL. Reduces Redis round-trips for hot data.",
              "score": null
            },
            {
              "doc_id": "config_cache",
              "chunk_id": "config_cache_fix_0",
              "content": "# Cache Configuration Reference ## Overview This document describes all configuration options for Cache. ## Environment Variables | Variable | Type | Default | Description | |----------|------|---------|-------------| | `REDIS_URL` | string | required | Redis connection string | | `REDIS_POOL_SIZE` | integer | 10 | Connection pool size | | `CACHE_TTL_SECONDS` | integer | 300 | Default cache TTL | | `CACHE_PREFIX` | string | cf: | Key prefix for namespacing | | `CACHE_COMPRESSION` |",
              "score": null
            },
            {
              "doc_id": "adr_004",
              "chunk_id": "adr_004_fix_0",
              "content": "# ADR-004: Use Redis for Caching and Session Storage **Status:** Accepted **Date:** 2024-04-05 ## Context We need a fast data store for caching API responses, session data, and rate limiting counters. ## Options Considered ### Option 1: Redis Cluster In-memory data structure store. **Pros:** - Sub-millisecond latency - Rich data structures - Pub/sub support - Clustering **Cons:** - Memory cost - Persistence complexity - Cluster management ### Option 2: Memcached Simple key-value cache.",
              "score": null
            },
            {
              "doc_id": "runbook_api_gateway_5xx_errors",
              "chunk_id": "runbook_api_gateway_5xx_errors_fix_2",
              "content": "If 5xx persists > 15 minutes, page on-call. May indicate infrastructure issue. ## Post-Incident - Document timeline in incident log - Schedule post-mortem within 48 hours - Update runbook if new patterns discovered",
              "score": null
            },
            {
              "doc_id": "adr_004",
              "chunk_id": "adr_004_fix_2",
              "content": "## Consequences - Implement cache-aside pattern - Set appropriate TTLs - Monitor memory usage - Plan for cache stampede prevention",
              "score": null
            }
          ],
          "latency_ms": 3.3572659995115828,
          "query_id": "simple_006",
          "dimension": "contextual",
          "query": "debugging stale data issues, need to know the default cache duration"
        },
        {
          "key_facts": [
            "300",
            "CACHE_TTL_SECONDS"
          ],
          "found_facts": [],
          "missed_facts": [
            "300",
            "CACHE_TTL_SECONDS"
          ],
          "coverage": 0.0,
          "expected_docs": [
            "config_cache"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "runbook_api_gateway_5xx_errors",
              "chunk_id": "runbook_api_gateway_5xx_errors_fix_2",
              "content": "If 5xx persists > 15 minutes, page on-call. May indicate infrastructure issue. ## Post-Incident - Document timeline in incident log - Schedule post-mortem within 48 hours - Update runbook if new patterns discovered",
              "score": null
            },
            {
              "doc_id": "runbook_out_of_memory_oom_kills",
              "chunk_id": "runbook_out_of_memory_oom_kills_fix_2",
              "content": "increase, escalate to development team for memory leak investigation. ## Post-Incident - Document timeline in incident log - Schedule post-mortem within 48 hours - Update runbook if new patterns discovered",
              "score": null
            },
            {
              "doc_id": "api_users",
              "chunk_id": "api_users_fix_1",
              "content": "**Parameters:** **Response:** - Success: `200` - Returns user object - Error: `400` - Bad Request **Example:** ```json {\"id\": \"usr_001\", \"email\": \"user@example.com\", \"role\": \"admin\", \"created_at\": \"2023-06-01\"} ``` ### PATCH /users/me Update current user profile. **Parameters:** - `name` (string): Display name - `timezone` (string): Timezone (e.g., America/New_York) **Response:** - Success: `200` - User updated - Error: `400` - Bad Request **Example:** ```json {\"id\": \"usr_001\", \"name\": \"John Doe\", \"timezone\": \"America/New_York\"} ```",
              "score": null
            },
            {
              "doc_id": "runbook_high_cpu_usage",
              "chunk_id": "runbook_high_cpu_usage_fix_2",
              "content": "high after scaling, escalate to platform team. Page on-call if affecting users. ## Post-Incident - Document timeline in incident log - Schedule post-mortem within 48 hours - Update runbook if new patterns discovered",
              "score": null
            },
            {
              "doc_id": "runbook_database_connection_exhaustion",
              "chunk_id": "runbook_database_connection_exhaustion_fix_2",
              "content": "## Post-Incident - Document timeline in incident log - Schedule post-mortem within 48 hours - Update runbook if new patterns discovered",
              "score": null
            }
          ],
          "latency_ms": 3.180175001034513,
          "query_id": "simple_006",
          "dimension": "negation",
          "query": "why isn't my data updating right away"
        },
        {
          "key_facts": [
            "AES-256",
            "encryption at rest"
          ],
          "found_facts": [
            "AES-256",
            "encryption at rest"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "howto_set_up_data_encryption"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "howto_set_up_data_encryption",
              "chunk_id": "howto_set_up_data_encryption_fix_0",
              "content": "# How to Set Up Data Encryption ## Prerequisites - CloudFlow Enterprise plan ## Steps ### Step 1: Enable encryption at rest All data is encrypted at rest using AES-256 by default. > **Note:** Encryption is automatic for all plans ### Step 2: Configure KMS Bring your own encryption keys using AWS KMS or GCP KMS. ```bash cloudflow encryption configure --kms-key arn:aws:kms:...",
              "score": null
            },
            {
              "doc_id": "api_secrets",
              "chunk_id": "api_secrets_fix_1",
              "content": "**Parameters:** **Response:** - Success: `200` - Returns array of secret metadata - Error: `400` - Bad Request **Example:** ```json {\"secrets\": [{\"name\": \"DATABASE_URL\", \"created_at\": \"2024-01-01\", \"updated_at\": \"2024-01-10\"}]} ``` ### PUT /secrets/{name} Create or update a secret. **Parameters:** - `value` (string): Secret value (encrypted at rest) **Response:** - Success: `200` - Secret saved - Error: `400` - Bad Request **Example:** ```json {\"name\": \"DATABASE_URL\", \"updated_at\": \"2024-01-15T10:00:00Z\"} ``` ### DELETE /secrets/{name} Delete a secret.",
              "score": null
            },
            {
              "doc_id": "howto_set_up_data_encryption",
              "chunk_id": "howto_set_up_data_encryption_fix_1",
              "content": "``` ### Step 3: Enable field-level encryption Encrypt sensitive fields within your workflow data. ```bash cloudflow encryption add-field --path $.user.ssn --key-alias sensitive-data ``` ### Step 4: Verify encryption Confirm encryption is active for your workspace. ```bash cloudflow encryption status ``` ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            },
            {
              "doc_id": "arch_authentication_service",
              "chunk_id": "arch_authentication_service_fix_0",
              "content": "# Authentication Service Architecture ## Overview The Authentication Service handles user identity, session management, and token issuance. Supports OAuth 2.0, SAML, and API key authentication. ## Design Principles - **Zero Knowledge**: Passwords never stored in plaintext - **Token Rotation**: Refresh tokens rotate on each use ## Components ### Identity Provider Manages user accounts and credentials. Password hashing uses bcrypt with cost factor 12.",
              "score": null
            },
            {
              "doc_id": "arch_data_pipeline",
              "chunk_id": "arch_data_pipeline_fix_2",
              "content": "**Technology:** S3 with Iceberg tables **Scaling:** Unlimited with tiered storage ## Data Flow Input \u2192 Validation \u2192 Transform \u2192 Enrich \u2192 Output ## Performance Characteristics - **Latency P50:** 100ms (streaming), 5min (batch) - **Latency P99:** 500ms (streaming), 30min (batch) - **Throughput:** 100000 messages/second (streaming)",
              "score": null
            }
          ],
          "latency_ms": 3.139640000881627,
          "query_id": "simple_007",
          "dimension": "original",
          "query": "What encryption is used for data at rest?"
        },
        {
          "key_facts": [
            "AES-256",
            "encryption at rest"
          ],
          "found_facts": [
            "AES-256",
            "encryption at rest"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "howto_set_up_data_encryption"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "howto_set_up_data_encryption",
              "chunk_id": "howto_set_up_data_encryption_fix_0",
              "content": "# How to Set Up Data Encryption ## Prerequisites - CloudFlow Enterprise plan ## Steps ### Step 1: Enable encryption at rest All data is encrypted at rest using AES-256 by default. > **Note:** Encryption is automatic for all plans ### Step 2: Configure KMS Bring your own encryption keys using AWS KMS or GCP KMS. ```bash cloudflow encryption configure --kms-key arn:aws:kms:...",
              "score": null
            },
            {
              "doc_id": "howto_set_up_data_encryption",
              "chunk_id": "howto_set_up_data_encryption_fix_1",
              "content": "``` ### Step 3: Enable field-level encryption Encrypt sensitive fields within your workflow data. ```bash cloudflow encryption add-field --path $.user.ssn --key-alias sensitive-data ``` ### Step 4: Verify encryption Confirm encryption is active for your workspace. ```bash cloudflow encryption status ``` ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            },
            {
              "doc_id": "api_secrets",
              "chunk_id": "api_secrets_fix_0",
              "content": "# Secrets API Reference ## Overview The Secrets API provides programmatic access to secrets functionality. Base URL: `https://api.cloudflow.io/v1/secrets` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /secrets List secret names (values not returned).",
              "score": null
            },
            {
              "doc_id": "api_integrations",
              "chunk_id": "api_integrations_fix_0",
              "content": "# Integrations API Reference ## Overview The Integrations API provides programmatic access to integrations functionality. Base URL: `https://api.cloudflow.io/v1/integrations` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /integrations List available integrations.",
              "score": null
            },
            {
              "doc_id": "howto_create_api_integration",
              "chunk_id": "howto_create_api_integration_fix_1",
              "content": "```bash curl -H 'Authorization: Bearer cf_...' https://api.cloudflow.io/v1/me ``` ### Step 4: Implement rate limiting Handle rate limit responses (HTTP 429) with exponential backoff. > **Note:** Default limit is 100 requests per minute ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            }
          ],
          "latency_ms": 3.868839001370361,
          "query_id": "simple_007",
          "dimension": "synonym",
          "query": "what cipher does CloudFlow use for stored data"
        },
        {
          "key_facts": [
            "AES-256",
            "encryption at rest"
          ],
          "found_facts": [
            "AES-256",
            "encryption at rest"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "howto_set_up_data_encryption"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "howto_set_up_data_encryption",
              "chunk_id": "howto_set_up_data_encryption_fix_0",
              "content": "# How to Set Up Data Encryption ## Prerequisites - CloudFlow Enterprise plan ## Steps ### Step 1: Enable encryption at rest All data is encrypted at rest using AES-256 by default. > **Note:** Encryption is automatic for all plans ### Step 2: Configure KMS Bring your own encryption keys using AWS KMS or GCP KMS. ```bash cloudflow encryption configure --kms-key arn:aws:kms:...",
              "score": null
            },
            {
              "doc_id": "howto_set_up_data_encryption",
              "chunk_id": "howto_set_up_data_encryption_fix_1",
              "content": "``` ### Step 3: Enable field-level encryption Encrypt sensitive fields within your workflow data. ```bash cloudflow encryption add-field --path $.user.ssn --key-alias sensitive-data ``` ### Step 4: Verify encryption Confirm encryption is active for your workspace. ```bash cloudflow encryption status ``` ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            },
            {
              "doc_id": "adr_008",
              "chunk_id": "adr_008_fix_2",
              "content": "## Consequences - Enable versioning for critical buckets - Implement lifecycle policies - Use pre-signed URLs for uploads - Enable server-side encryption",
              "score": null
            },
            {
              "doc_id": "runbook_ssl_certificate_expiry",
              "chunk_id": "runbook_ssl_certificate_expiry_fix_1",
              "content": "### Step 4 Verify DNS is correct: `dig +short api.cloudflow.io` ## Resolution ### Step 1 Trigger manual renewal: `kubectl delete certificate api-cert -n production && kubectl apply -f certificate.yaml` ### Step 2 Check rate limits: Let's Encrypt has 50 certs/domain/week limit ### Step 3 Use staging issuer for testing: switch to letsencrypt-staging ClusterIssuer ### Step 4 If Let's Encrypt fails, manually upload certificate ## Escalation If certificate cannot be renewed, escalate to security team immediately.",
              "score": null
            },
            {
              "doc_id": "runbook_ssl_certificate_expiry",
              "chunk_id": "runbook_ssl_certificate_expiry_fix_2",
              "content": "Consider using backup certificate. ## Post-Incident - Document timeline in incident log - Schedule post-mortem within 48 hours - Update runbook if new patterns discovered",
              "score": null
            }
          ],
          "latency_ms": 4.320000000006985,
          "query_id": "simple_007",
          "dimension": "problem",
          "query": "compliance team asking about our data encryption standard"
        },
        {
          "key_facts": [
            "AES-256",
            "encryption at rest"
          ],
          "found_facts": [
            "AES-256",
            "encryption at rest"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "howto_set_up_data_encryption"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "howto_set_up_data_encryption",
              "chunk_id": "howto_set_up_data_encryption_fix_0",
              "content": "# How to Set Up Data Encryption ## Prerequisites - CloudFlow Enterprise plan ## Steps ### Step 1: Enable encryption at rest All data is encrypted at rest using AES-256 by default. > **Note:** Encryption is automatic for all plans ### Step 2: Configure KMS Bring your own encryption keys using AWS KMS or GCP KMS. ```bash cloudflow encryption configure --kms-key arn:aws:kms:...",
              "score": null
            },
            {
              "doc_id": "api_secrets",
              "chunk_id": "api_secrets_fix_1",
              "content": "**Parameters:** **Response:** - Success: `200` - Returns array of secret metadata - Error: `400` - Bad Request **Example:** ```json {\"secrets\": [{\"name\": \"DATABASE_URL\", \"created_at\": \"2024-01-01\", \"updated_at\": \"2024-01-10\"}]} ``` ### PUT /secrets/{name} Create or update a secret. **Parameters:** - `value` (string): Secret value (encrypted at rest) **Response:** - Success: `200` - Secret saved - Error: `400` - Bad Request **Example:** ```json {\"name\": \"DATABASE_URL\", \"updated_at\": \"2024-01-15T10:00:00Z\"} ``` ### DELETE /secrets/{name} Delete a secret.",
              "score": null
            },
            {
              "doc_id": "howto_set_up_data_encryption",
              "chunk_id": "howto_set_up_data_encryption_fix_1",
              "content": "``` ### Step 3: Enable field-level encryption Encrypt sensitive fields within your workflow data. ```bash cloudflow encryption add-field --path $.user.ssn --key-alias sensitive-data ``` ### Step 4: Verify encryption Confirm encryption is active for your workspace. ```bash cloudflow encryption status ``` ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            },
            {
              "doc_id": "arch_authentication_service",
              "chunk_id": "arch_authentication_service_fix_0",
              "content": "# Authentication Service Architecture ## Overview The Authentication Service handles user identity, session management, and token issuance. Supports OAuth 2.0, SAML, and API key authentication. ## Design Principles - **Zero Knowledge**: Passwords never stored in plaintext - **Token Rotation**: Refresh tokens rotate on each use ## Components ### Identity Provider Manages user accounts and credentials. Password hashing uses bcrypt with cost factor 12.",
              "score": null
            },
            {
              "doc_id": "runbook_ssl_certificate_expiry",
              "chunk_id": "runbook_ssl_certificate_expiry_fix_1",
              "content": "### Step 4 Verify DNS is correct: `dig +short api.cloudflow.io` ## Resolution ### Step 1 Trigger manual renewal: `kubectl delete certificate api-cert -n production && kubectl apply -f certificate.yaml` ### Step 2 Check rate limits: Let's Encrypt has 50 certs/domain/week limit ### Step 3 Use staging issuer for testing: switch to letsencrypt-staging ClusterIssuer ### Step 4 If Let's Encrypt fails, manually upload certificate ## Escalation If certificate cannot be renewed, escalate to security team immediately.",
              "score": null
            }
          ],
          "latency_ms": 3.3392120003554737,
          "query_id": "simple_007",
          "dimension": "casual",
          "query": "encryption at rest algo"
        },
        {
          "key_facts": [
            "AES-256",
            "encryption at rest"
          ],
          "found_facts": [
            "AES-256",
            "encryption at rest"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "howto_set_up_data_encryption"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "howto_set_up_data_encryption",
              "chunk_id": "howto_set_up_data_encryption_fix_0",
              "content": "# How to Set Up Data Encryption ## Prerequisites - CloudFlow Enterprise plan ## Steps ### Step 1: Enable encryption at rest All data is encrypted at rest using AES-256 by default. > **Note:** Encryption is automatic for all plans ### Step 2: Configure KMS Bring your own encryption keys using AWS KMS or GCP KMS. ```bash cloudflow encryption configure --kms-key arn:aws:kms:...",
              "score": null
            },
            {
              "doc_id": "howto_set_up_data_encryption",
              "chunk_id": "howto_set_up_data_encryption_fix_1",
              "content": "``` ### Step 3: Enable field-level encryption Encrypt sensitive fields within your workflow data. ```bash cloudflow encryption add-field --path $.user.ssn --key-alias sensitive-data ``` ### Step 4: Verify encryption Confirm encryption is active for your workspace. ```bash cloudflow encryption status ``` ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            },
            {
              "doc_id": "adr_008",
              "chunk_id": "adr_008_fix_2",
              "content": "## Consequences - Enable versioning for critical buckets - Implement lifecycle policies - Use pre-signed URLs for uploads - Enable server-side encryption",
              "score": null
            },
            {
              "doc_id": "api_secrets",
              "chunk_id": "api_secrets_fix_1",
              "content": "**Parameters:** **Response:** - Success: `200` - Returns array of secret metadata - Error: `400` - Bad Request **Example:** ```json {\"secrets\": [{\"name\": \"DATABASE_URL\", \"created_at\": \"2024-01-01\", \"updated_at\": \"2024-01-10\"}]} ``` ### PUT /secrets/{name} Create or update a secret. **Parameters:** - `value` (string): Secret value (encrypted at rest) **Response:** - Success: `200` - Secret saved - Error: `400` - Bad Request **Example:** ```json {\"name\": \"DATABASE_URL\", \"updated_at\": \"2024-01-15T10:00:00Z\"} ``` ### DELETE /secrets/{name} Delete a secret.",
              "score": null
            },
            {
              "doc_id": "adr_001",
              "chunk_id": "adr_001_fix_1",
              "content": "**Pros:** - ACID compliance - Rich JSONB queries - Team expertise - Proven at scale **Cons:** - Manual sharding for extreme scale - Requires connection pooling ### Option 2: MongoDB Document database with native JSON. **Pros:** - Flexible schema - Built-in sharding - Good for documents **Cons:** - Weaker consistency guarantees - Complex transactions - Higher operational cost ### Option 3: CockroachDB Distributed SQL database.",
              "score": null
            }
          ],
          "latency_ms": 3.474483999525546,
          "query_id": "simple_007",
          "dimension": "contextual",
          "query": "filling out security questionnaire, need to know the encryption standard for stored data"
        },
        {
          "key_facts": [
            "AES-256",
            "encryption at rest"
          ],
          "found_facts": [
            "AES-256",
            "encryption at rest"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "howto_set_up_data_encryption"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "howto_set_up_data_encryption",
              "chunk_id": "howto_set_up_data_encryption_fix_0",
              "content": "# How to Set Up Data Encryption ## Prerequisites - CloudFlow Enterprise plan ## Steps ### Step 1: Enable encryption at rest All data is encrypted at rest using AES-256 by default. > **Note:** Encryption is automatic for all plans ### Step 2: Configure KMS Bring your own encryption keys using AWS KMS or GCP KMS. ```bash cloudflow encryption configure --kms-key arn:aws:kms:...",
              "score": null
            },
            {
              "doc_id": "howto_set_up_data_encryption",
              "chunk_id": "howto_set_up_data_encryption_fix_1",
              "content": "``` ### Step 3: Enable field-level encryption Encrypt sensitive fields within your workflow data. ```bash cloudflow encryption add-field --path $.user.ssn --key-alias sensitive-data ``` ### Step 4: Verify encryption Confirm encryption is active for your workspace. ```bash cloudflow encryption status ``` ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            },
            {
              "doc_id": "api_secrets",
              "chunk_id": "api_secrets_fix_1",
              "content": "**Parameters:** **Response:** - Success: `200` - Returns array of secret metadata - Error: `400` - Bad Request **Example:** ```json {\"secrets\": [{\"name\": \"DATABASE_URL\", \"created_at\": \"2024-01-01\", \"updated_at\": \"2024-01-10\"}]} ``` ### PUT /secrets/{name} Create or update a secret. **Parameters:** - `value` (string): Secret value (encrypted at rest) **Response:** - Success: `200` - Secret saved - Error: `400` - Bad Request **Example:** ```json {\"name\": \"DATABASE_URL\", \"updated_at\": \"2024-01-15T10:00:00Z\"} ``` ### DELETE /secrets/{name} Delete a secret.",
              "score": null
            },
            {
              "doc_id": "arch_authentication_service",
              "chunk_id": "arch_authentication_service_fix_0",
              "content": "# Authentication Service Architecture ## Overview The Authentication Service handles user identity, session management, and token issuance. Supports OAuth 2.0, SAML, and API key authentication. ## Design Principles - **Zero Knowledge**: Passwords never stored in plaintext - **Token Rotation**: Refresh tokens rotate on each use ## Components ### Identity Provider Manages user accounts and credentials. Password hashing uses bcrypt with cost factor 12.",
              "score": null
            },
            {
              "doc_id": "api_secrets",
              "chunk_id": "api_secrets_fix_0",
              "content": "# Secrets API Reference ## Overview The Secrets API provides programmatic access to secrets functionality. Base URL: `https://api.cloudflow.io/v1/secrets` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /secrets List secret names (values not returned).",
              "score": null
            }
          ],
          "latency_ms": 3.442082999754348,
          "query_id": "simple_007",
          "dimension": "negation",
          "query": "is our stored data actually encrypted or not"
        },
        {
          "key_facts": [
            "100",
            "WORKFLOW_MAX_STEPS"
          ],
          "found_facts": [
            "100",
            "WORKFLOW_MAX_STEPS"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "config_workflow_engine"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "config_workflow_engine",
              "chunk_id": "config_workflow_engine_fix_0",
              "content": "# Workflow Engine Configuration Reference ## Overview This document describes all configuration options for Workflow Engine. ## Environment Variables | Variable | Type | Default | Description | |----------|------|---------|-------------| | `WORKFLOW_MAX_STEPS` | integer | 100 | Maximum steps per workflow | | `WORKFLOW_TIMEOUT_SECONDS` | integer | 3600 | Maximum workflow execution time | | `WORKFLOW_RETRY_LIMIT` | integer | 3 | Default retry attempts for failed steps | | `WORKFLOW_CONCURRENT_LIMIT` | integer | 10 | Maximum",
              "score": null
            },
            {
              "doc_id": "api_workflows",
              "chunk_id": "api_workflows_fix_1",
              "content": "**Parameters:** - `limit` (integer): Maximum number of results (default: 50, max: 200) - `status` (string): Filter by status: active, paused, archived **Response:** - Success: `200` - Returns array of workflow objects - Error: `400` - Bad Request **Example:** ```json {\"workflows\": [{\"id\": \"wf_123\", \"name\": \"Daily Report\", \"status\": \"active\"}]} ``` ### POST /workflows Create a new workflow.",
              "score": null
            },
            {
              "doc_id": "howto_implement_retry_logic",
              "chunk_id": "howto_implement_retry_logic_fix_0",
              "content": "# How to Implement Retry Logic ## Prerequisites - Existing workflow ## Steps ### Step 1: Define retry policy Add retry configuration to your workflow step. > **Note:** Maximum 5 retry attempts allowed ### Step 2: Configure backoff Set exponential backoff with base delay of 1 second. The formula is: delay = base * 2^attempt. ### Step 3: Add timeout Set maximum execution time to prevent infinite loops. Default timeout is 30 seconds.",
              "score": null
            },
            {
              "doc_id": "config_workflow_engine",
              "chunk_id": "config_workflow_engine_fix_1",
              "content": "concurrent executions per workflow | | `EXECUTOR_POOL_SIZE` | integer | 50 | Number of executor workers | | `EXECUTOR_MEMORY_LIMIT` | string | 512Mi | Memory limit per executor | ## Configuration File Configuration can also be provided via `workflow engine.yaml`: ```yaml workflow_max_steps: 100 workflow_timeout_seconds: 3600 workflow_retry_limit: 3 workflow_concurrent_limit: 10 executor_pool_size: 50 ``` ## Validation Configuration is validated at startup. Invalid configuration will prevent the service from starting.",
              "score": null
            },
            {
              "doc_id": "arch_workflow_engine",
              "chunk_id": "arch_workflow_engine_fix_0",
              "content": "# Workflow Engine Architecture ## Overview The Workflow Engine is the core component responsible for executing user-defined workflows. It uses an event-driven architecture with support for parallel execution, conditional branching, and automatic retries. ## Design Principles - **Idempotency**: All operations can be safely retried without side effects - **Eventual Consistency**: State converges to consistent view within 5 seconds - **Fault Tolerance**: System continues operating despite component failures ## Components ### Scheduler Manages workflow scheduling and",
              "score": null
            }
          ],
          "latency_ms": 3.2775269992271205,
          "query_id": "simple_008",
          "dimension": "original",
          "query": "What is the maximum number of steps in a workflow?"
        },
        {
          "key_facts": [
            "100",
            "WORKFLOW_MAX_STEPS"
          ],
          "found_facts": [
            "100",
            "WORKFLOW_MAX_STEPS"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "config_workflow_engine"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "config_workflow_engine",
              "chunk_id": "config_workflow_engine_fix_0",
              "content": "# Workflow Engine Configuration Reference ## Overview This document describes all configuration options for Workflow Engine. ## Environment Variables | Variable | Type | Default | Description | |----------|------|---------|-------------| | `WORKFLOW_MAX_STEPS` | integer | 100 | Maximum steps per workflow | | `WORKFLOW_TIMEOUT_SECONDS` | integer | 3600 | Maximum workflow execution time | | `WORKFLOW_RETRY_LIMIT` | integer | 3 | Default retry attempts for failed steps | | `WORKFLOW_CONCURRENT_LIMIT` | integer | 10 | Maximum",
              "score": null
            },
            {
              "doc_id": "howto_implement_retry_logic",
              "chunk_id": "howto_implement_retry_logic_fix_0",
              "content": "# How to Implement Retry Logic ## Prerequisites - Existing workflow ## Steps ### Step 1: Define retry policy Add retry configuration to your workflow step. > **Note:** Maximum 5 retry attempts allowed ### Step 2: Configure backoff Set exponential backoff with base delay of 1 second. The formula is: delay = base * 2^attempt. ### Step 3: Add timeout Set maximum execution time to prevent infinite loops. Default timeout is 30 seconds.",
              "score": null
            },
            {
              "doc_id": "api_workflows",
              "chunk_id": "api_workflows_fix_1",
              "content": "**Parameters:** - `limit` (integer): Maximum number of results (default: 50, max: 200) - `status` (string): Filter by status: active, paused, archived **Response:** - Success: `200` - Returns array of workflow objects - Error: `400` - Bad Request **Example:** ```json {\"workflows\": [{\"id\": \"wf_123\", \"name\": \"Daily Report\", \"status\": \"active\"}]} ``` ### POST /workflows Create a new workflow.",
              "score": null
            },
            {
              "doc_id": "config_workflow_engine",
              "chunk_id": "config_workflow_engine_fix_1",
              "content": "concurrent executions per workflow | | `EXECUTOR_POOL_SIZE` | integer | 50 | Number of executor workers | | `EXECUTOR_MEMORY_LIMIT` | string | 512Mi | Memory limit per executor | ## Configuration File Configuration can also be provided via `workflow engine.yaml`: ```yaml workflow_max_steps: 100 workflow_timeout_seconds: 3600 workflow_retry_limit: 3 workflow_concurrent_limit: 10 executor_pool_size: 50 ``` ## Validation Configuration is validated at startup. Invalid configuration will prevent the service from starting.",
              "score": null
            },
            {
              "doc_id": "howto_create_api_integration",
              "chunk_id": "howto_create_api_integration_fix_1",
              "content": "```bash curl -H 'Authorization: Bearer cf_...' https://api.cloudflow.io/v1/me ``` ### Step 4: Implement rate limiting Handle rate limit responses (HTTP 429) with exponential backoff. > **Note:** Default limit is 100 requests per minute ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            }
          ],
          "latency_ms": 3.491145000225515,
          "query_id": "simple_008",
          "dimension": "synonym",
          "query": "workflow step count limit"
        },
        {
          "key_facts": [
            "100",
            "WORKFLOW_MAX_STEPS"
          ],
          "found_facts": [
            "100",
            "WORKFLOW_MAX_STEPS"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "config_workflow_engine"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "api_workflows",
              "chunk_id": "api_workflows_fix_2",
              "content": "**Parameters:** - `name` (string): Workflow name (required) - `definition` (object): Workflow definition in JSON format **Response:** - Success: `201` - Workflow created successfully - Error: `400` - Bad Request **Example:** ```json {\"id\": \"wf_456\", \"name\": \"New Workflow\", \"created_at\": \"2024-01-15T10:00:00Z\"} ``` ### DELETE /workflows/{id} Delete a workflow permanently. **Parameters:** - `id` (string): Workflow ID (required) **Response:** - Success: `204` - Workflow deleted - Error: `404` - Workflow not found **Example:** ```json {} ```",
              "score": null
            },
            {
              "doc_id": "howto_debug_failed_execution",
              "chunk_id": "howto_debug_failed_execution_fix_0",
              "content": "# How to Debug Failed Execution ## Prerequisites - Failed execution ID ## Steps ### Step 1: Get execution details Retrieve the full execution record including step outputs. ```bash cloudflow executions get ex_123 --include-steps ``` ### Step 2: View step logs Check logs for the failed step. ```bash cloudflow executions logs ex_123 --step 3 ``` ### Step 3: Inspect input/output Review the data passed between steps.",
              "score": null
            },
            {
              "doc_id": "api_workflows",
              "chunk_id": "api_workflows_fix_1",
              "content": "**Parameters:** - `limit` (integer): Maximum number of results (default: 50, max: 200) - `status` (string): Filter by status: active, paused, archived **Response:** - Success: `200` - Returns array of workflow objects - Error: `400` - Bad Request **Example:** ```json {\"workflows\": [{\"id\": \"wf_123\", \"name\": \"Daily Report\", \"status\": \"active\"}]} ``` ### POST /workflows Create a new workflow.",
              "score": null
            },
            {
              "doc_id": "config_workflow_engine",
              "chunk_id": "config_workflow_engine_fix_0",
              "content": "# Workflow Engine Configuration Reference ## Overview This document describes all configuration options for Workflow Engine. ## Environment Variables | Variable | Type | Default | Description | |----------|------|---------|-------------| | `WORKFLOW_MAX_STEPS` | integer | 100 | Maximum steps per workflow | | `WORKFLOW_TIMEOUT_SECONDS` | integer | 3600 | Maximum workflow execution time | | `WORKFLOW_RETRY_LIMIT` | integer | 3 | Default retry attempts for failed steps | | `WORKFLOW_CONCURRENT_LIMIT` | integer | 10 | Maximum",
              "score": null
            },
            {
              "doc_id": "howto_debug_failed_execution",
              "chunk_id": "howto_debug_failed_execution_fix_1",
              "content": "```bash cloudflow executions inspect ex_123 --step 3 ``` ### Step 4: Enable debug mode Re-run with verbose logging enabled. ```bash cloudflow workflows execute wf_123 --debug ``` ### Step 5: Check error patterns Look for similar failures in recent executions. ```bash cloudflow executions list --status failed --since 24h ``` ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            }
          ],
          "latency_ms": 3.4099530003004475,
          "query_id": "simple_008",
          "dimension": "problem",
          "query": "workflow creation failing with 'too many steps' error"
        },
        {
          "key_facts": [
            "100",
            "WORKFLOW_MAX_STEPS"
          ],
          "found_facts": [
            "100",
            "WORKFLOW_MAX_STEPS"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "config_workflow_engine"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "config_workflow_engine",
              "chunk_id": "config_workflow_engine_fix_0",
              "content": "# Workflow Engine Configuration Reference ## Overview This document describes all configuration options for Workflow Engine. ## Environment Variables | Variable | Type | Default | Description | |----------|------|---------|-------------| | `WORKFLOW_MAX_STEPS` | integer | 100 | Maximum steps per workflow | | `WORKFLOW_TIMEOUT_SECONDS` | integer | 3600 | Maximum workflow execution time | | `WORKFLOW_RETRY_LIMIT` | integer | 3 | Default retry attempts for failed steps | | `WORKFLOW_CONCURRENT_LIMIT` | integer | 10 | Maximum",
              "score": null
            },
            {
              "doc_id": "api_workflows",
              "chunk_id": "api_workflows_fix_1",
              "content": "**Parameters:** - `limit` (integer): Maximum number of results (default: 50, max: 200) - `status` (string): Filter by status: active, paused, archived **Response:** - Success: `200` - Returns array of workflow objects - Error: `400` - Bad Request **Example:** ```json {\"workflows\": [{\"id\": \"wf_123\", \"name\": \"Daily Report\", \"status\": \"active\"}]} ``` ### POST /workflows Create a new workflow.",
              "score": null
            },
            {
              "doc_id": "howto_implement_retry_logic",
              "chunk_id": "howto_implement_retry_logic_fix_0",
              "content": "# How to Implement Retry Logic ## Prerequisites - Existing workflow ## Steps ### Step 1: Define retry policy Add retry configuration to your workflow step. > **Note:** Maximum 5 retry attempts allowed ### Step 2: Configure backoff Set exponential backoff with base delay of 1 second. The formula is: delay = base * 2^attempt. ### Step 3: Add timeout Set maximum execution time to prevent infinite loops. Default timeout is 30 seconds.",
              "score": null
            },
            {
              "doc_id": "config_workflow_engine",
              "chunk_id": "config_workflow_engine_fix_1",
              "content": "concurrent executions per workflow | | `EXECUTOR_POOL_SIZE` | integer | 50 | Number of executor workers | | `EXECUTOR_MEMORY_LIMIT` | string | 512Mi | Memory limit per executor | ## Configuration File Configuration can also be provided via `workflow engine.yaml`: ```yaml workflow_max_steps: 100 workflow_timeout_seconds: 3600 workflow_retry_limit: 3 workflow_concurrent_limit: 10 executor_pool_size: 50 ``` ## Validation Configuration is validated at startup. Invalid configuration will prevent the service from starting.",
              "score": null
            },
            {
              "doc_id": "arch_workflow_engine",
              "chunk_id": "arch_workflow_engine_fix_0",
              "content": "# Workflow Engine Architecture ## Overview The Workflow Engine is the core component responsible for executing user-defined workflows. It uses an event-driven architecture with support for parallel execution, conditional branching, and automatic retries. ## Design Principles - **Idempotency**: All operations can be safely retried without side effects - **Eventual Consistency**: State converges to consistent view within 5 seconds - **Fault Tolerance**: System continues operating despite component failures ## Components ### Scheduler Manages workflow scheduling and",
              "score": null
            }
          ],
          "latency_ms": 3.392171000086819,
          "query_id": "simple_008",
          "dimension": "casual",
          "query": "max steps per workflow"
        },
        {
          "key_facts": [
            "100",
            "WORKFLOW_MAX_STEPS"
          ],
          "found_facts": [
            "100",
            "WORKFLOW_MAX_STEPS"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "config_workflow_engine"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "config_workflow_engine",
              "chunk_id": "config_workflow_engine_fix_0",
              "content": "# Workflow Engine Configuration Reference ## Overview This document describes all configuration options for Workflow Engine. ## Environment Variables | Variable | Type | Default | Description | |----------|------|---------|-------------| | `WORKFLOW_MAX_STEPS` | integer | 100 | Maximum steps per workflow | | `WORKFLOW_TIMEOUT_SECONDS` | integer | 3600 | Maximum workflow execution time | | `WORKFLOW_RETRY_LIMIT` | integer | 3 | Default retry attempts for failed steps | | `WORKFLOW_CONCURRENT_LIMIT` | integer | 10 | Maximum",
              "score": null
            },
            {
              "doc_id": "arch_workflow_engine",
              "chunk_id": "arch_workflow_engine_fix_0",
              "content": "# Workflow Engine Architecture ## Overview The Workflow Engine is the core component responsible for executing user-defined workflows. It uses an event-driven architecture with support for parallel execution, conditional branching, and automatic retries. ## Design Principles - **Idempotency**: All operations can be safely retried without side effects - **Eventual Consistency**: State converges to consistent view within 5 seconds - **Fault Tolerance**: System continues operating despite component failures ## Components ### Scheduler Manages workflow scheduling and",
              "score": null
            },
            {
              "doc_id": "howto_implement_retry_logic",
              "chunk_id": "howto_implement_retry_logic_fix_0",
              "content": "# How to Implement Retry Logic ## Prerequisites - Existing workflow ## Steps ### Step 1: Define retry policy Add retry configuration to your workflow step. > **Note:** Maximum 5 retry attempts allowed ### Step 2: Configure backoff Set exponential backoff with base delay of 1 second. The formula is: delay = base * 2^attempt. ### Step 3: Add timeout Set maximum execution time to prevent infinite loops. Default timeout is 30 seconds.",
              "score": null
            },
            {
              "doc_id": "arch_data_pipeline",
              "chunk_id": "arch_data_pipeline_fix_0",
              "content": "# Data Pipeline Architecture ## Overview The Data Pipeline processes and transforms data between workflow steps. It supports batch processing up to 10GB and streaming for real-time use cases. ## Design Principles - **Exactly-Once Processing**: Each record is processed exactly once - **Backpressure Handling**: System gracefully handles varying load ## Components ### Stream Processor Handles real-time data streams. Maximum throughput of 100000 messages per second.",
              "score": null
            },
            {
              "doc_id": "config_workflow_engine",
              "chunk_id": "config_workflow_engine_fix_1",
              "content": "concurrent executions per workflow | | `EXECUTOR_POOL_SIZE` | integer | 50 | Number of executor workers | | `EXECUTOR_MEMORY_LIMIT` | string | 512Mi | Memory limit per executor | ## Configuration File Configuration can also be provided via `workflow engine.yaml`: ```yaml workflow_max_steps: 100 workflow_timeout_seconds: 3600 workflow_retry_limit: 3 workflow_concurrent_limit: 10 executor_pool_size: 50 ``` ## Validation Configuration is validated at startup. Invalid configuration will prevent the service from starting.",
              "score": null
            }
          ],
          "latency_ms": 3.423027999815531,
          "query_id": "simple_008",
          "dimension": "contextual",
          "query": "designing a complex ETL workflow, need to know if there's a step limit"
        },
        {
          "key_facts": [
            "100",
            "WORKFLOW_MAX_STEPS"
          ],
          "found_facts": [
            "100",
            "WORKFLOW_MAX_STEPS"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "config_workflow_engine"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "config_workflow_engine",
              "chunk_id": "config_workflow_engine_fix_0",
              "content": "# Workflow Engine Configuration Reference ## Overview This document describes all configuration options for Workflow Engine. ## Environment Variables | Variable | Type | Default | Description | |----------|------|---------|-------------| | `WORKFLOW_MAX_STEPS` | integer | 100 | Maximum steps per workflow | | `WORKFLOW_TIMEOUT_SECONDS` | integer | 3600 | Maximum workflow execution time | | `WORKFLOW_RETRY_LIMIT` | integer | 3 | Default retry attempts for failed steps | | `WORKFLOW_CONCURRENT_LIMIT` | integer | 10 | Maximum",
              "score": null
            },
            {
              "doc_id": "api_workflows",
              "chunk_id": "api_workflows_fix_1",
              "content": "**Parameters:** - `limit` (integer): Maximum number of results (default: 50, max: 200) - `status` (string): Filter by status: active, paused, archived **Response:** - Success: `200` - Returns array of workflow objects - Error: `400` - Bad Request **Example:** ```json {\"workflows\": [{\"id\": \"wf_123\", \"name\": \"Daily Report\", \"status\": \"active\"}]} ``` ### POST /workflows Create a new workflow.",
              "score": null
            },
            {
              "doc_id": "howto_debug_failed_execution",
              "chunk_id": "howto_debug_failed_execution_fix_0",
              "content": "# How to Debug Failed Execution ## Prerequisites - Failed execution ID ## Steps ### Step 1: Get execution details Retrieve the full execution record including step outputs. ```bash cloudflow executions get ex_123 --include-steps ``` ### Step 2: View step logs Check logs for the failed step. ```bash cloudflow executions logs ex_123 --step 3 ``` ### Step 3: Inspect input/output Review the data passed between steps.",
              "score": null
            },
            {
              "doc_id": "howto_implement_retry_logic",
              "chunk_id": "howto_implement_retry_logic_fix_0",
              "content": "# How to Implement Retry Logic ## Prerequisites - Existing workflow ## Steps ### Step 1: Define retry policy Add retry configuration to your workflow step. > **Note:** Maximum 5 retry attempts allowed ### Step 2: Configure backoff Set exponential backoff with base delay of 1 second. The formula is: delay = base * 2^attempt. ### Step 3: Add timeout Set maximum execution time to prevent infinite loops. Default timeout is 30 seconds.",
              "score": null
            },
            {
              "doc_id": "howto_implement_workflow_versioning",
              "chunk_id": "howto_implement_workflow_versioning_fix_0",
              "content": "# How to Implement Workflow Versioning ## Prerequisites - Existing workflow ## Steps ### Step 1: Enable versioning Turn on versioning for your workflow. This is required for the feature. ```bash cloudflow workflows update wf_123 --enable-versioning ``` ### Step 2: Create new version Save changes as a new version instead of overwriting. ```bash cloudflow workflows publish wf_123 --version 2.0.0 ``` ### Step 3: Set active version Choose which version handles new executions.",
              "score": null
            }
          ],
          "latency_ms": 3.823233000730397,
          "query_id": "simple_008",
          "dimension": "negation",
          "query": "why can't I add more steps to my workflow"
        },
        {
          "key_facts": [
            "30000",
            "GATEWAY_TIMEOUT_MS"
          ],
          "found_facts": [
            "30000",
            "GATEWAY_TIMEOUT_MS"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "config_api_gateway"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "config_api_gateway",
              "chunk_id": "config_api_gateway_fix_0",
              "content": "# API Gateway Configuration Reference ## Overview This document describes all configuration options for API Gateway. ## Environment Variables | Variable | Type | Default | Description | |----------|------|---------|-------------| | `GATEWAY_PORT` | integer | 8000 | Gateway listen port | | `GATEWAY_RATE_LIMIT` | integer | 100 | Requests per minute per user | | `GATEWAY_TIMEOUT_MS` | integer | 30000 | Request timeout in milliseconds | | `GATEWAY_MAX_BODY_SIZE` | string | 10MB | Maximum request body",
              "score": null
            },
            {
              "doc_id": "adr_007",
              "chunk_id": "adr_007_fix_0",
              "content": "# ADR-007: Implement Rate Limiting at API Gateway **Status:** Accepted **Date:** 2024-09-03 ## Context We need to protect our services from abuse and ensure fair resource usage across customers. ## Options Considered ### Option 1: Sliding window at gateway Rate limit at API gateway level.",
              "score": null
            },
            {
              "doc_id": "arch_api_gateway",
              "chunk_id": "arch_api_gateway_fix_0",
              "content": "# API Gateway Architecture ## Overview The API Gateway handles all incoming HTTP requests, providing authentication, rate limiting, and request routing. Built on Kong with custom plugins for CloudFlow-specific functionality. ## Design Principles - **Zero Trust**: Every request is authenticated and authorized - **Defense in Depth**: Multiple layers of security validation ## Components ### Kong Gateway Primary ingress controller. Handles TLS termination, request routing, and plugin execution.",
              "score": null
            },
            {
              "doc_id": "config_api_gateway",
              "chunk_id": "config_api_gateway_fix_1",
              "content": "size | | `GATEWAY_CORS_ORIGINS` | string | * | Allowed CORS origins | | `GATEWAY_SSL_ENABLED` | boolean | true | Enable TLS termination | ## Configuration File Configuration can also be provided via `api gateway.yaml`: ```yaml gateway_port: 8000 gateway_rate_limit: 100 gateway_timeout_ms: 30000 gateway_max_body_size: 10MB gateway_cors_origins: * ``` ## Validation Configuration is validated at startup. Invalid configuration will prevent the service from starting.",
              "score": null
            },
            {
              "doc_id": "runbook_api_gateway_5xx_errors",
              "chunk_id": "runbook_api_gateway_5xx_errors_fix_0",
              "content": "# Runbook: API Gateway 5xx Errors ## Severity **Level:** P3 - Medium **Response Time:** 1 hour ## Symptoms - Error rate > 1% - 5xx responses in access logs - Downstream service failures ## Diagnosis ### Step 1 Check Kong error logs: `kubectl logs -l app=kong -n kong --tail=100 | grep -E '5[0-9]{2}'` ### Step 2 Verify backend health: `kubectl get pods -n production -o wide` ### Step 3 Check service endpoints: `kubectl get endpoints",
              "score": null
            }
          ],
          "latency_ms": 3.840716000922839,
          "query_id": "simple_009",
          "dimension": "original",
          "query": "What is the API gateway timeout?"
        },
        {
          "key_facts": [
            "30000",
            "GATEWAY_TIMEOUT_MS"
          ],
          "found_facts": [
            "30000",
            "GATEWAY_TIMEOUT_MS"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "config_api_gateway"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "config_api_gateway",
              "chunk_id": "config_api_gateway_fix_0",
              "content": "# API Gateway Configuration Reference ## Overview This document describes all configuration options for API Gateway. ## Environment Variables | Variable | Type | Default | Description | |----------|------|---------|-------------| | `GATEWAY_PORT` | integer | 8000 | Gateway listen port | | `GATEWAY_RATE_LIMIT` | integer | 100 | Requests per minute per user | | `GATEWAY_TIMEOUT_MS` | integer | 30000 | Request timeout in milliseconds | | `GATEWAY_MAX_BODY_SIZE` | string | 10MB | Maximum request body",
              "score": null
            },
            {
              "doc_id": "adr_007",
              "chunk_id": "adr_007_fix_0",
              "content": "# ADR-007: Implement Rate Limiting at API Gateway **Status:** Accepted **Date:** 2024-09-03 ## Context We need to protect our services from abuse and ensure fair resource usage across customers. ## Options Considered ### Option 1: Sliding window at gateway Rate limit at API gateway level.",
              "score": null
            },
            {
              "doc_id": "adr_007",
              "chunk_id": "adr_007_fix_2",
              "content": "**Pros:** - Allows bursts - Smooth rate limiting - Well understood **Cons:** - More complex - Harder to explain to users - State management ## Decision We will implement sliding window rate limiting at the Kong API Gateway using Redis. ## Consequences - Default limit: 100 requests per minute - Implement per-plan limits - Return rate limit headers - Add retry-after on 429",
              "score": null
            },
            {
              "doc_id": "arch_api_gateway",
              "chunk_id": "arch_api_gateway_fix_0",
              "content": "# API Gateway Architecture ## Overview The API Gateway handles all incoming HTTP requests, providing authentication, rate limiting, and request routing. Built on Kong with custom plugins for CloudFlow-specific functionality. ## Design Principles - **Zero Trust**: Every request is authenticated and authorized - **Defense in Depth**: Multiple layers of security validation ## Components ### Kong Gateway Primary ingress controller. Handles TLS termination, request routing, and plugin execution.",
              "score": null
            },
            {
              "doc_id": "config_api_gateway",
              "chunk_id": "config_api_gateway_fix_1",
              "content": "size | | `GATEWAY_CORS_ORIGINS` | string | * | Allowed CORS origins | | `GATEWAY_SSL_ENABLED` | boolean | true | Enable TLS termination | ## Configuration File Configuration can also be provided via `api gateway.yaml`: ```yaml gateway_port: 8000 gateway_rate_limit: 100 gateway_timeout_ms: 30000 gateway_max_body_size: 10MB gateway_cors_origins: * ``` ## Validation Configuration is validated at startup. Invalid configuration will prevent the service from starting.",
              "score": null
            }
          ],
          "latency_ms": 3.8836459989397554,
          "query_id": "simple_009",
          "dimension": "synonym",
          "query": "what's the request timeout at the gateway level"
        },
        {
          "key_facts": [
            "30000",
            "GATEWAY_TIMEOUT_MS"
          ],
          "found_facts": [
            "30000",
            "GATEWAY_TIMEOUT_MS"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "config_api_gateway"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "config_api_gateway",
              "chunk_id": "config_api_gateway_fix_0",
              "content": "# API Gateway Configuration Reference ## Overview This document describes all configuration options for API Gateway. ## Environment Variables | Variable | Type | Default | Description | |----------|------|---------|-------------| | `GATEWAY_PORT` | integer | 8000 | Gateway listen port | | `GATEWAY_RATE_LIMIT` | integer | 100 | Requests per minute per user | | `GATEWAY_TIMEOUT_MS` | integer | 30000 | Request timeout in milliseconds | | `GATEWAY_MAX_BODY_SIZE` | string | 10MB | Maximum request body",
              "score": null
            },
            {
              "doc_id": "adr_007",
              "chunk_id": "adr_007_fix_0",
              "content": "# ADR-007: Implement Rate Limiting at API Gateway **Status:** Accepted **Date:** 2024-09-03 ## Context We need to protect our services from abuse and ensure fair resource usage across customers. ## Options Considered ### Option 1: Sliding window at gateway Rate limit at API gateway level.",
              "score": null
            },
            {
              "doc_id": "runbook_api_gateway_5xx_errors",
              "chunk_id": "runbook_api_gateway_5xx_errors_fix_0",
              "content": "# Runbook: API Gateway 5xx Errors ## Severity **Level:** P3 - Medium **Response Time:** 1 hour ## Symptoms - Error rate > 1% - 5xx responses in access logs - Downstream service failures ## Diagnosis ### Step 1 Check Kong error logs: `kubectl logs -l app=kong -n kong --tail=100 | grep -E '5[0-9]{2}'` ### Step 2 Verify backend health: `kubectl get pods -n production -o wide` ### Step 3 Check service endpoints: `kubectl get endpoints",
              "score": null
            },
            {
              "doc_id": "howto_create_api_integration",
              "chunk_id": "howto_create_api_integration_fix_1",
              "content": "```bash curl -H 'Authorization: Bearer cf_...' https://api.cloudflow.io/v1/me ``` ### Step 4: Implement rate limiting Handle rate limit responses (HTTP 429) with exponential backoff. > **Note:** Default limit is 100 requests per minute ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            },
            {
              "doc_id": "config_api_gateway",
              "chunk_id": "config_api_gateway_fix_1",
              "content": "size | | `GATEWAY_CORS_ORIGINS` | string | * | Allowed CORS origins | | `GATEWAY_SSL_ENABLED` | boolean | true | Enable TLS termination | ## Configuration File Configuration can also be provided via `api gateway.yaml`: ```yaml gateway_port: 8000 gateway_rate_limit: 100 gateway_timeout_ms: 30000 gateway_max_body_size: 10MB gateway_cors_origins: * ``` ## Validation Configuration is validated at startup. Invalid configuration will prevent the service from starting.",
              "score": null
            }
          ],
          "latency_ms": 3.5101100002066232,
          "query_id": "simple_009",
          "dimension": "problem",
          "query": "long-running API calls timing out with 504 gateway timeout"
        },
        {
          "key_facts": [
            "30000",
            "GATEWAY_TIMEOUT_MS"
          ],
          "found_facts": [
            "30000",
            "GATEWAY_TIMEOUT_MS"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "config_api_gateway"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "config_api_gateway",
              "chunk_id": "config_api_gateway_fix_0",
              "content": "# API Gateway Configuration Reference ## Overview This document describes all configuration options for API Gateway. ## Environment Variables | Variable | Type | Default | Description | |----------|------|---------|-------------| | `GATEWAY_PORT` | integer | 8000 | Gateway listen port | | `GATEWAY_RATE_LIMIT` | integer | 100 | Requests per minute per user | | `GATEWAY_TIMEOUT_MS` | integer | 30000 | Request timeout in milliseconds | | `GATEWAY_MAX_BODY_SIZE` | string | 10MB | Maximum request body",
              "score": null
            },
            {
              "doc_id": "config_api_gateway",
              "chunk_id": "config_api_gateway_fix_1",
              "content": "size | | `GATEWAY_CORS_ORIGINS` | string | * | Allowed CORS origins | | `GATEWAY_SSL_ENABLED` | boolean | true | Enable TLS termination | ## Configuration File Configuration can also be provided via `api gateway.yaml`: ```yaml gateway_port: 8000 gateway_rate_limit: 100 gateway_timeout_ms: 30000 gateway_max_body_size: 10MB gateway_cors_origins: * ``` ## Validation Configuration is validated at startup. Invalid configuration will prevent the service from starting.",
              "score": null
            },
            {
              "doc_id": "adr_007",
              "chunk_id": "adr_007_fix_0",
              "content": "# ADR-007: Implement Rate Limiting at API Gateway **Status:** Accepted **Date:** 2024-09-03 ## Context We need to protect our services from abuse and ensure fair resource usage across customers. ## Options Considered ### Option 1: Sliding window at gateway Rate limit at API gateway level.",
              "score": null
            },
            {
              "doc_id": "runbook_api_gateway_5xx_errors",
              "chunk_id": "runbook_api_gateway_5xx_errors_fix_0",
              "content": "# Runbook: API Gateway 5xx Errors ## Severity **Level:** P3 - Medium **Response Time:** 1 hour ## Symptoms - Error rate > 1% - 5xx responses in access logs - Downstream service failures ## Diagnosis ### Step 1 Check Kong error logs: `kubectl logs -l app=kong -n kong --tail=100 | grep -E '5[0-9]{2}'` ### Step 2 Verify backend health: `kubectl get pods -n production -o wide` ### Step 3 Check service endpoints: `kubectl get endpoints",
              "score": null
            },
            {
              "doc_id": "config_database",
              "chunk_id": "config_database_fix_0",
              "content": "# Database Configuration Reference ## Overview This document describes all configuration options for Database. ## Environment Variables | Variable | Type | Default | Description | |----------|------|---------|-------------| | `DATABASE_URL` | string | required | PostgreSQL connection string | | `DATABASE_POOL_MIN` | integer | 5 | Minimum connection pool size | | `DATABASE_POOL_MAX` | integer | 20 | Maximum connection pool size | | `DATABASE_STATEMENT_TIMEOUT` | integer | 30000 | Query timeout in milliseconds | |",
              "score": null
            }
          ],
          "latency_ms": 3.3374579998053377,
          "query_id": "simple_009",
          "dimension": "casual",
          "query": "gateway timeout config"
        },
        {
          "key_facts": [
            "30000",
            "GATEWAY_TIMEOUT_MS"
          ],
          "found_facts": [
            "30000",
            "GATEWAY_TIMEOUT_MS"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "config_api_gateway"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "config_api_gateway",
              "chunk_id": "config_api_gateway_fix_0",
              "content": "# API Gateway Configuration Reference ## Overview This document describes all configuration options for API Gateway. ## Environment Variables | Variable | Type | Default | Description | |----------|------|---------|-------------| | `GATEWAY_PORT` | integer | 8000 | Gateway listen port | | `GATEWAY_RATE_LIMIT` | integer | 100 | Requests per minute per user | | `GATEWAY_TIMEOUT_MS` | integer | 30000 | Request timeout in milliseconds | | `GATEWAY_MAX_BODY_SIZE` | string | 10MB | Maximum request body",
              "score": null
            },
            {
              "doc_id": "adr_007",
              "chunk_id": "adr_007_fix_0",
              "content": "# ADR-007: Implement Rate Limiting at API Gateway **Status:** Accepted **Date:** 2024-09-03 ## Context We need to protect our services from abuse and ensure fair resource usage across customers. ## Options Considered ### Option 1: Sliding window at gateway Rate limit at API gateway level.",
              "score": null
            },
            {
              "doc_id": "adr_007",
              "chunk_id": "adr_007_fix_2",
              "content": "**Pros:** - Allows bursts - Smooth rate limiting - Well understood **Cons:** - More complex - Harder to explain to users - State management ## Decision We will implement sliding window rate limiting at the Kong API Gateway using Redis. ## Consequences - Default limit: 100 requests per minute - Implement per-plan limits - Return rate limit headers - Add retry-after on 429",
              "score": null
            },
            {
              "doc_id": "arch_api_gateway",
              "chunk_id": "arch_api_gateway_fix_0",
              "content": "# API Gateway Architecture ## Overview The API Gateway handles all incoming HTTP requests, providing authentication, rate limiting, and request routing. Built on Kong with custom plugins for CloudFlow-specific functionality. ## Design Principles - **Zero Trust**: Every request is authenticated and authorized - **Defense in Depth**: Multiple layers of security validation ## Components ### Kong Gateway Primary ingress controller. Handles TLS termination, request routing, and plugin execution.",
              "score": null
            },
            {
              "doc_id": "howto_implement_retry_logic",
              "chunk_id": "howto_implement_retry_logic_fix_0",
              "content": "# How to Implement Retry Logic ## Prerequisites - Existing workflow ## Steps ### Step 1: Define retry policy Add retry configuration to your workflow step. > **Note:** Maximum 5 retry attempts allowed ### Step 2: Configure backoff Set exponential backoff with base delay of 1 second. The formula is: delay = base * 2^attempt. ### Step 3: Add timeout Set maximum execution time to prevent infinite loops. Default timeout is 30 seconds.",
              "score": null
            }
          ],
          "latency_ms": 3.647456000180682,
          "query_id": "simple_009",
          "dimension": "contextual",
          "query": "have a slow endpoint that takes 45 seconds, will it hit the gateway timeout"
        },
        {
          "key_facts": [
            "30000",
            "GATEWAY_TIMEOUT_MS"
          ],
          "found_facts": [],
          "missed_facts": [
            "30000",
            "GATEWAY_TIMEOUT_MS"
          ],
          "coverage": 0.0,
          "expected_docs": [
            "config_api_gateway"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "adr_007",
              "chunk_id": "adr_007_fix_2",
              "content": "**Pros:** - Allows bursts - Smooth rate limiting - Well understood **Cons:** - More complex - Harder to explain to users - State management ## Decision We will implement sliding window rate limiting at the Kong API Gateway using Redis. ## Consequences - Default limit: 100 requests per minute - Implement per-plan limits - Return rate limit headers - Add retry-after on 429",
              "score": null
            },
            {
              "doc_id": "howto_create_api_integration",
              "chunk_id": "howto_create_api_integration_fix_1",
              "content": "```bash curl -H 'Authorization: Bearer cf_...' https://api.cloudflow.io/v1/me ``` ### Step 4: Implement rate limiting Handle rate limit responses (HTTP 429) with exponential backoff. > **Note:** Default limit is 100 requests per minute ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            },
            {
              "doc_id": "api_workflows",
              "chunk_id": "api_workflows_fix_1",
              "content": "**Parameters:** - `limit` (integer): Maximum number of results (default: 50, max: 200) - `status` (string): Filter by status: active, paused, archived **Response:** - Success: `200` - Returns array of workflow objects - Error: `400` - Bad Request **Example:** ```json {\"workflows\": [{\"id\": \"wf_123\", \"name\": \"Daily Report\", \"status\": \"active\"}]} ``` ### POST /workflows Create a new workflow.",
              "score": null
            },
            {
              "doc_id": "api_secrets",
              "chunk_id": "api_secrets_fix_2",
              "content": "**Parameters:** **Response:** - Success: `204` - Secret deleted - Error: `400` - Bad Request **Example:** ```json {} ```",
              "score": null
            },
            {
              "doc_id": "runbook_database_connection_exhaustion",
              "chunk_id": "runbook_database_connection_exhaustion_fix_0",
              "content": "# Runbook: Database Connection Exhaustion ## Severity **Level:** P1 - Critical **Response Time:** 15 minutes ## Symptoms - Connection pool at 100% - Queries timing out - 'too many connections' errors in logs ## Diagnosis ### Step 1 Check active connections: `SELECT count(*) FROM pg_stat_activity` ### Step 2 Identify long-running queries: `SELECT pid, now() - pg_stat_activity.query_start AS duration, query FROM pg_stat_activity WHERE state != 'idle' ORDER BY duration DESC` ### Step 3 Check PgBouncer stats:",
              "score": null
            }
          ],
          "latency_ms": 3.433728999880259,
          "query_id": "simple_009",
          "dimension": "negation",
          "query": "why are my slow requests getting cut off"
        },
        {
          "key_facts": [
            "Kafka",
            "event-driven"
          ],
          "found_facts": [
            "Kafka",
            "event-driven"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "adr_005"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "adr_005",
              "chunk_id": "adr_005_fix_1",
              "content": "**Pros:** - Simple mental model - Immediate feedback - Easy debugging **Cons:** - Tight coupling - Cascading failures - Timeout issues ### Option 3: Polling-based Workers poll for tasks. **Pros:** - Simple implementation - Easy rate limiting - No message broker **Cons:** - Polling overhead - Higher latency - Inefficient at scale ## Decision We will use an event-driven architecture with Kafka for workflow step execution.",
              "score": null
            },
            {
              "doc_id": "arch_notification_service",
              "chunk_id": "arch_notification_service_fix_0",
              "content": "# Notification Service Architecture ## Overview The Notification Service delivers alerts and updates through multiple channels including email, Slack, webhooks, and mobile push notifications. ## Design Principles - **Delivery Guarantee**: At-least-once delivery with deduplication - **Channel Abstraction**: Unified API regardless of delivery channel ## Components ### Dispatcher Routes notifications to appropriate channel handlers. Supports priority queues for urgent notifications. **Technology:** Go microservice **Scaling:** Horizontal with queue partitioning ### Email Provider Sends transactional emails.",
              "score": null
            },
            {
              "doc_id": "arch_workflow_engine",
              "chunk_id": "arch_workflow_engine_fix_0",
              "content": "# Workflow Engine Architecture ## Overview The Workflow Engine is the core component responsible for executing user-defined workflows. It uses an event-driven architecture with support for parallel execution, conditional branching, and automatic retries. ## Design Principles - **Idempotency**: All operations can be safely retried without side effects - **Eventual Consistency**: State converges to consistent view within 5 seconds - **Fault Tolerance**: System continues operating despite component failures ## Components ### Scheduler Manages workflow scheduling and",
              "score": null
            },
            {
              "doc_id": "adr_005",
              "chunk_id": "adr_005_fix_0",
              "content": "# ADR-005: Adopt Event-Driven Architecture for Workflow Execution **Status:** Accepted **Date:** 2024-12-04 ## Context We need an architecture that supports asynchronous workflow execution, step dependencies, and failure recovery. ## Options Considered ### Option 1: Event-driven with Kafka Publish-subscribe messaging. **Pros:** - Loose coupling - Scalability - Event replay - Audit trail **Cons:** - Eventual consistency - Operational complexity - Message ordering challenges ### Option 2: Request-response Synchronous API calls.",
              "score": null
            },
            {
              "doc_id": "arch_notification_service",
              "chunk_id": "arch_notification_service_fix_1",
              "content": "Maximum 10000 emails per hour per account. **Technology:** SendGrid integration **Scaling:** API rate limited ### Slack Integration Posts messages to Slack channels. Supports message formatting and interactive components. **Technology:** Slack Bolt SDK **Scaling:** Per-workspace rate limits ## Data Flow Event \u2192 Template \u2192 Dispatcher \u2192 Channel \u2192 Delivery Confirmation ## Performance Characteristics - **Latency P50:** 200ms - **Latency P99:** 2000ms - **Throughput:** 10000 notifications/minute",
              "score": null
            }
          ],
          "latency_ms": 3.353688998686266,
          "query_id": "simple_010",
          "dimension": "original",
          "query": "What message broker is used for workflows?"
        },
        {
          "key_facts": [
            "Kafka",
            "event-driven"
          ],
          "found_facts": [
            "Kafka",
            "event-driven"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "adr_005"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "arch_workflow_engine",
              "chunk_id": "arch_workflow_engine_fix_0",
              "content": "# Workflow Engine Architecture ## Overview The Workflow Engine is the core component responsible for executing user-defined workflows. It uses an event-driven architecture with support for parallel execution, conditional branching, and automatic retries. ## Design Principles - **Idempotency**: All operations can be safely retried without side effects - **Eventual Consistency**: State converges to consistent view within 5 seconds - **Fault Tolerance**: System continues operating despite component failures ## Components ### Scheduler Manages workflow scheduling and",
              "score": null
            },
            {
              "doc_id": "adr_005",
              "chunk_id": "adr_005_fix_0",
              "content": "# ADR-005: Adopt Event-Driven Architecture for Workflow Execution **Status:** Accepted **Date:** 2024-12-04 ## Context We need an architecture that supports asynchronous workflow execution, step dependencies, and failure recovery. ## Options Considered ### Option 1: Event-driven with Kafka Publish-subscribe messaging. **Pros:** - Loose coupling - Scalability - Event replay - Audit trail **Cons:** - Eventual consistency - Operational complexity - Message ordering challenges ### Option 2: Request-response Synchronous API calls.",
              "score": null
            },
            {
              "doc_id": "adr_005",
              "chunk_id": "adr_005_fix_1",
              "content": "**Pros:** - Simple mental model - Immediate feedback - Easy debugging **Cons:** - Tight coupling - Cascading failures - Timeout issues ### Option 3: Polling-based Workers poll for tasks. **Pros:** - Simple implementation - Easy rate limiting - No message broker **Cons:** - Polling overhead - Higher latency - Inefficient at scale ## Decision We will use an event-driven architecture with Kafka for workflow step execution.",
              "score": null
            },
            {
              "doc_id": "arch_notification_service",
              "chunk_id": "arch_notification_service_fix_0",
              "content": "# Notification Service Architecture ## Overview The Notification Service delivers alerts and updates through multiple channels including email, Slack, webhooks, and mobile push notifications. ## Design Principles - **Delivery Guarantee**: At-least-once delivery with deduplication - **Channel Abstraction**: Unified API regardless of delivery channel ## Components ### Dispatcher Routes notifications to appropriate channel handlers. Supports priority queues for urgent notifications. **Technology:** Go microservice **Scaling:** Horizontal with queue partitioning ### Email Provider Sends transactional emails.",
              "score": null
            },
            {
              "doc_id": "arch_workflow_engine",
              "chunk_id": "arch_workflow_engine_fix_1",
              "content": "triggers. Supports cron expressions, webhooks, and event-based triggers. **Technology:** Custom Go service **Scaling:** Horizontal with leader election ### Executor Runs individual workflow steps in isolated containers. Maximum execution time is 30 minutes per step. **Technology:** Kubernetes Jobs **Scaling:** Auto-scaling based on queue depth ### State Store Persists workflow state, execution history, and step results. Uses optimistic locking for concurrent updates.",
              "score": null
            }
          ],
          "latency_ms": 3.238904999307124,
          "query_id": "simple_010",
          "dimension": "synonym",
          "query": "what messaging system handles workflow events"
        },
        {
          "key_facts": [
            "Kafka",
            "event-driven"
          ],
          "found_facts": [
            "Kafka"
          ],
          "missed_facts": [
            "event-driven"
          ],
          "coverage": 0.5,
          "expected_docs": [
            "adr_005"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "runbook_kafka_consumer_lag",
              "chunk_id": "runbook_kafka_consumer_lag_fix_1",
              "content": "grep ERROR` ### Step 4 Monitor throughput: `cloudflow metrics get kafka.consumer.records_per_second` ## Resolution ### Step 1 Scale up consumers: `kubectl scale deployment/workflow-executor --replicas=10 -n production` ### Step 2 Reset offset if messages are stale: `kafka-consumer-groups.sh --bootstrap-server kafka:9092 --group workflow-executor --reset-offsets --to-latest --execute --topic workflows` ### Step 3 Pause producer if backpressure needed: `cloudflow workflows pause-triggers` ### Step 4 Increase partition count for parallelism (requires coordination) ## Escalation If lag persists > 30 minutes, page platform",
              "score": null
            },
            {
              "doc_id": "adr_005",
              "chunk_id": "adr_005_fix_2",
              "content": "## Consequences - Implement idempotent consumers - Use dead letter queues for failed messages - Monitor consumer lag",
              "score": null
            },
            {
              "doc_id": "howto_set_up_monitoring_alerts",
              "chunk_id": "howto_set_up_monitoring_alerts_fix_0",
              "content": "# How to Set Up Monitoring Alerts ## Prerequisites - CloudFlow Pro plan or higher ## Steps ### Step 1: Define alert rules Create alert conditions based on metrics thresholds. ```bash cloudflow alerts create --name 'High Error Rate' --condition 'error_rate > 0.05' ``` ### Step 2: Configure channels Set up notification channels: email, Slack, or PagerDuty. ```bash cloudflow alerts channel add --type slack --webhook https://hooks.slack.com/...",
              "score": null
            },
            {
              "doc_id": "howto_set_up_monitoring_alerts",
              "chunk_id": "howto_set_up_monitoring_alerts_fix_1",
              "content": "``` ### Step 3: Set severity levels Assign severity to each alert: critical, warning, or info. > **Note:** Critical alerts bypass quiet hours ### Step 4: Test alerts Trigger a test alert to verify routing. ```bash cloudflow alerts test --name 'High Error Rate' ``` ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            },
            {
              "doc_id": "arch_monitoring_stack",
              "chunk_id": "arch_monitoring_stack_fix_2",
              "content": "**Technology:** Alertmanager with PagerDuty **Scaling:** HA pair ## Data Flow Application \u2192 Collector \u2192 Storage \u2192 Query \u2192 Dashboard/Alert ## Performance Characteristics - **Latency P50:** N/A - **Latency P99:** N/A - **Throughput:** 1M metrics/second, 100GB logs/day",
              "score": null
            }
          ],
          "latency_ms": 3.3213480000995332,
          "query_id": "simple_010",
          "dimension": "problem",
          "query": "seeing consumer lag alerts, what message queue does CloudFlow use"
        },
        {
          "key_facts": [
            "Kafka",
            "event-driven"
          ],
          "found_facts": [
            "Kafka",
            "event-driven"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "adr_005"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "adr_005",
              "chunk_id": "adr_005_fix_1",
              "content": "**Pros:** - Simple mental model - Immediate feedback - Easy debugging **Cons:** - Tight coupling - Cascading failures - Timeout issues ### Option 3: Polling-based Workers poll for tasks. **Pros:** - Simple implementation - Easy rate limiting - No message broker **Cons:** - Polling overhead - Higher latency - Inefficient at scale ## Decision We will use an event-driven architecture with Kafka for workflow step execution.",
              "score": null
            },
            {
              "doc_id": "adr_005",
              "chunk_id": "adr_005_fix_0",
              "content": "# ADR-005: Adopt Event-Driven Architecture for Workflow Execution **Status:** Accepted **Date:** 2024-12-04 ## Context We need an architecture that supports asynchronous workflow execution, step dependencies, and failure recovery. ## Options Considered ### Option 1: Event-driven with Kafka Publish-subscribe messaging. **Pros:** - Loose coupling - Scalability - Event replay - Audit trail **Cons:** - Eventual consistency - Operational complexity - Message ordering challenges ### Option 2: Request-response Synchronous API calls.",
              "score": null
            },
            {
              "doc_id": "arch_notification_service",
              "chunk_id": "arch_notification_service_fix_0",
              "content": "# Notification Service Architecture ## Overview The Notification Service delivers alerts and updates through multiple channels including email, Slack, webhooks, and mobile push notifications. ## Design Principles - **Delivery Guarantee**: At-least-once delivery with deduplication - **Channel Abstraction**: Unified API regardless of delivery channel ## Components ### Dispatcher Routes notifications to appropriate channel handlers. Supports priority queues for urgent notifications. **Technology:** Go microservice **Scaling:** Horizontal with queue partitioning ### Email Provider Sends transactional emails.",
              "score": null
            },
            {
              "doc_id": "arch_workflow_engine",
              "chunk_id": "arch_workflow_engine_fix_0",
              "content": "# Workflow Engine Architecture ## Overview The Workflow Engine is the core component responsible for executing user-defined workflows. It uses an event-driven architecture with support for parallel execution, conditional branching, and automatic retries. ## Design Principles - **Idempotency**: All operations can be safely retried without side effects - **Eventual Consistency**: State converges to consistent view within 5 seconds - **Fault Tolerance**: System continues operating despite component failures ## Components ### Scheduler Manages workflow scheduling and",
              "score": null
            },
            {
              "doc_id": "api_workflows",
              "chunk_id": "api_workflows_fix_0",
              "content": "# Workflows API Reference ## Overview The Workflows API provides programmatic access to workflows functionality. Base URL: `https://api.cloudflow.io/v1/workflows` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /workflows List all workflows for the authenticated user.",
              "score": null
            }
          ],
          "latency_ms": 4.288820999136078,
          "query_id": "simple_010",
          "dimension": "casual",
          "query": "workflow message broker"
        },
        {
          "key_facts": [
            "Kafka",
            "event-driven"
          ],
          "found_facts": [
            "Kafka",
            "event-driven"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "adr_005"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "arch_workflow_engine",
              "chunk_id": "arch_workflow_engine_fix_2",
              "content": "**Technology:** PostgreSQL with JSONB **Scaling:** Primary-replica with read replicas ## Data Flow Triggers \u2192 Scheduler \u2192 Job Queue \u2192 Executor \u2192 State Store \u2192 Webhooks/Notifications ## Performance Characteristics - **Latency P50:** 45ms - **Latency P99:** 250ms - **Throughput:** 5000 executions/minute",
              "score": null
            },
            {
              "doc_id": "adr_005",
              "chunk_id": "adr_005_fix_1",
              "content": "**Pros:** - Simple mental model - Immediate feedback - Easy debugging **Cons:** - Tight coupling - Cascading failures - Timeout issues ### Option 3: Polling-based Workers poll for tasks. **Pros:** - Simple implementation - Easy rate limiting - No message broker **Cons:** - Polling overhead - Higher latency - Inefficient at scale ## Decision We will use an event-driven architecture with Kafka for workflow step execution.",
              "score": null
            },
            {
              "doc_id": "arch_data_pipeline",
              "chunk_id": "arch_data_pipeline_fix_0",
              "content": "# Data Pipeline Architecture ## Overview The Data Pipeline processes and transforms data between workflow steps. It supports batch processing up to 10GB and streaming for real-time use cases. ## Design Principles - **Exactly-Once Processing**: Each record is processed exactly once - **Backpressure Handling**: System gracefully handles varying load ## Components ### Stream Processor Handles real-time data streams. Maximum throughput of 100000 messages per second.",
              "score": null
            },
            {
              "doc_id": "arch_data_pipeline",
              "chunk_id": "arch_data_pipeline_fix_2",
              "content": "**Technology:** S3 with Iceberg tables **Scaling:** Unlimited with tiered storage ## Data Flow Input \u2192 Validation \u2192 Transform \u2192 Enrich \u2192 Output ## Performance Characteristics - **Latency P50:** 100ms (streaming), 5min (batch) - **Latency P99:** 500ms (streaming), 30min (batch) - **Throughput:** 100000 messages/second (streaming)",
              "score": null
            },
            {
              "doc_id": "arch_monitoring_stack",
              "chunk_id": "arch_monitoring_stack_fix_2",
              "content": "**Technology:** Alertmanager with PagerDuty **Scaling:** HA pair ## Data Flow Application \u2192 Collector \u2192 Storage \u2192 Query \u2192 Dashboard/Alert ## Performance Characteristics - **Latency P50:** N/A - **Latency P99:** N/A - **Throughput:** 1M metrics/second, 100GB logs/day",
              "score": null
            }
          ],
          "latency_ms": 3.484873999695992,
          "query_id": "simple_010",
          "dimension": "contextual",
          "query": "need to monitor workflow throughput, what queue system should I look at"
        },
        {
          "key_facts": [
            "Kafka",
            "event-driven"
          ],
          "found_facts": [
            "Kafka",
            "event-driven"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "adr_005"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "howto_implement_workflow_versioning",
              "chunk_id": "howto_implement_workflow_versioning_fix_1",
              "content": "```bash cloudflow workflows activate wf_123 --version 2.0.0 ``` ### Step 4: Roll back if needed Quickly revert to a previous version. ```bash cloudflow workflows rollback wf_123 --to-version 1.0.0 ``` ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            },
            {
              "doc_id": "howto_create_scheduled_workflow",
              "chunk_id": "howto_create_scheduled_workflow_fix_1",
              "content": "> **Note:** Use IANA timezone names like America/New_York ### Step 3: Configure overlap handling Choose behavior when previous execution is still running: skip, queue, or cancel. ### Step 4: Enable notifications Get notified on schedule failures. ```bash cloudflow workflows notify --on-failure ``` ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            },
            {
              "doc_id": "adr_005",
              "chunk_id": "adr_005_fix_0",
              "content": "# ADR-005: Adopt Event-Driven Architecture for Workflow Execution **Status:** Accepted **Date:** 2024-12-04 ## Context We need an architecture that supports asynchronous workflow execution, step dependencies, and failure recovery. ## Options Considered ### Option 1: Event-driven with Kafka Publish-subscribe messaging. **Pros:** - Loose coupling - Scalability - Event replay - Audit trail **Cons:** - Eventual consistency - Operational complexity - Message ordering challenges ### Option 2: Request-response Synchronous API calls.",
              "score": null
            },
            {
              "doc_id": "howto_debug_failed_execution",
              "chunk_id": "howto_debug_failed_execution_fix_1",
              "content": "```bash cloudflow executions inspect ex_123 --step 3 ``` ### Step 4: Enable debug mode Re-run with verbose logging enabled. ```bash cloudflow workflows execute wf_123 --debug ``` ### Step 5: Check error patterns Look for similar failures in recent executions. ```bash cloudflow executions list --status failed --since 24h ``` ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            },
            {
              "doc_id": "api_executions",
              "chunk_id": "api_executions_fix_1",
              "content": "**Parameters:** - `workflow_id` (string): Filter by workflow ID - `since` (datetime): Filter executions after this timestamp **Response:** - Success: `200` - Returns array of execution objects - Error: `400` - Bad Request **Example:** ```json {\"executions\": [{\"id\": \"ex_789\", \"status\": \"completed\", \"duration_ms\": 1523}]} ``` ### POST /executions/{id}/cancel Cancel a running execution.",
              "score": null
            }
          ],
          "latency_ms": 3.4013580007012933,
          "query_id": "simple_010",
          "dimension": "negation",
          "query": "workflows aren't using synchronous calls right"
        },
        {
          "key_facts": [
            "OAuth 2.0",
            "JWT",
            "bcrypt",
            "RS256",
            "cost factor 12"
          ],
          "found_facts": [
            "OAuth 2.0",
            "JWT",
            "bcrypt",
            "cost factor 12"
          ],
          "missed_facts": [
            "RS256"
          ],
          "coverage": 0.8,
          "expected_docs": [
            "adr_003",
            "arch_authentication_service",
            "arch_api_gateway"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "arch_authentication_service",
              "chunk_id": "arch_authentication_service_fix_0",
              "content": "# Authentication Service Architecture ## Overview The Authentication Service handles user identity, session management, and token issuance. Supports OAuth 2.0, SAML, and API key authentication. ## Design Principles - **Zero Knowledge**: Passwords never stored in plaintext - **Token Rotation**: Refresh tokens rotate on each use ## Components ### Identity Provider Manages user accounts and credentials. Password hashing uses bcrypt with cost factor 12.",
              "score": null
            },
            {
              "doc_id": "adr_003",
              "chunk_id": "adr_003_fix_0",
              "content": "# ADR-003: Implement OAuth 2.0 with JWT for Authentication **Status:** Accepted **Date:** 2024-05-08 ## Context We need a secure authentication system that supports API keys, user sessions, and third-party integrations. ## Options Considered ### Option 1: OAuth 2.0 with JWT Industry standard token-based auth.",
              "score": null
            },
            {
              "doc_id": "adr_003",
              "chunk_id": "adr_003_fix_2",
              "content": "**Pros:** - Very simple - Low overhead - Easy to implement **Cons:** - No user context - Hard to rotate - No SSO support ## Decision We will implement OAuth 2.0 with JWT tokens. Access tokens expire after 1 hour, refresh tokens after 7 days. ## Consequences - Implement token rotation on refresh - Support API keys for machine-to-machine auth - Add MFA for admin accounts",
              "score": null
            },
            {
              "doc_id": "adr_003",
              "chunk_id": "adr_003_fix_1",
              "content": "**Pros:** - Industry standard - Stateless validation - Supports refresh tokens - SSO compatible **Cons:** - Token size overhead - Cannot revoke individual tokens - Complexity ### Option 2: Session-based auth Traditional server-side sessions. **Pros:** - Simple implementation - Easy revocation - Smaller payloads **Cons:** - Requires session store - Not suitable for APIs - Scaling challenges ### Option 3: API keys only Simple key-based authentication.",
              "score": null
            },
            {
              "doc_id": "api_workflows",
              "chunk_id": "api_workflows_fix_0",
              "content": "# Workflows API Reference ## Overview The Workflows API provides programmatic access to workflows functionality. Base URL: `https://api.cloudflow.io/v1/workflows` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /workflows List all workflows for the authenticated user.",
              "score": null
            }
          ],
          "latency_ms": 3.6427270006242907,
          "query_id": "cross_011",
          "dimension": "original",
          "query": "How is authentication implemented across the system?"
        },
        {
          "key_facts": [
            "OAuth 2.0",
            "JWT",
            "bcrypt",
            "RS256",
            "cost factor 12"
          ],
          "found_facts": [
            "OAuth 2.0"
          ],
          "missed_facts": [
            "JWT",
            "bcrypt",
            "RS256",
            "cost factor 12"
          ],
          "coverage": 0.2,
          "expected_docs": [
            "adr_003",
            "arch_authentication_service",
            "arch_api_gateway"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "api_integrations",
              "chunk_id": "api_integrations_fix_0",
              "content": "# Integrations API Reference ## Overview The Integrations API provides programmatic access to integrations functionality. Base URL: `https://api.cloudflow.io/v1/integrations` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /integrations List available integrations.",
              "score": null
            },
            {
              "doc_id": "api_users",
              "chunk_id": "api_users_fix_0",
              "content": "# Users API Reference ## Overview The Users API provides programmatic access to users functionality. Base URL: `https://api.cloudflow.io/v1/users` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /users/me Get current user profile.",
              "score": null
            },
            {
              "doc_id": "howto_configure_single_sign_on",
              "chunk_id": "howto_configure_single_sign_on_fix_0",
              "content": "# How to Configure Single Sign-On ## Prerequisites - Admin access - Enterprise plan - Identity provider access ## Steps ### Step 1: Choose provider CloudFlow supports SAML 2.0, OpenID Connect, and OAuth 2.0 providers. ### Step 2: Get metadata Download CloudFlow's SAML metadata for your IdP configuration. ```bash cloudflow sso metadata --format xml > cloudflow-metadata.xml ``` ### Step 3: Configure IdP Add CloudFlow as a service provider in your identity provider.",
              "score": null
            },
            {
              "doc_id": "api_secrets",
              "chunk_id": "api_secrets_fix_0",
              "content": "# Secrets API Reference ## Overview The Secrets API provides programmatic access to secrets functionality. Base URL: `https://api.cloudflow.io/v1/secrets` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /secrets List secret names (values not returned).",
              "score": null
            },
            {
              "doc_id": "api_webhooks",
              "chunk_id": "api_webhooks_fix_0",
              "content": "# Webhooks API Reference ## Overview The Webhooks API provides programmatic access to webhooks functionality. Base URL: `https://api.cloudflow.io/v1/webhooks` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /webhooks List configured webhooks.",
              "score": null
            }
          ],
          "latency_ms": 3.5302080013934756,
          "query_id": "cross_011",
          "dimension": "synonym",
          "query": "what's the login and authorization setup in cloudflow"
        },
        {
          "key_facts": [
            "OAuth 2.0",
            "JWT",
            "bcrypt",
            "RS256",
            "cost factor 12"
          ],
          "found_facts": [
            "OAuth 2.0",
            "JWT"
          ],
          "missed_facts": [
            "bcrypt",
            "RS256",
            "cost factor 12"
          ],
          "coverage": 0.4,
          "expected_docs": [
            "adr_003",
            "arch_authentication_service",
            "arch_api_gateway"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "api_workflows",
              "chunk_id": "api_workflows_fix_0",
              "content": "# Workflows API Reference ## Overview The Workflows API provides programmatic access to workflows functionality. Base URL: `https://api.cloudflow.io/v1/workflows` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /workflows List all workflows for the authenticated user.",
              "score": null
            },
            {
              "doc_id": "adr_003",
              "chunk_id": "adr_003_fix_0",
              "content": "# ADR-003: Implement OAuth 2.0 with JWT for Authentication **Status:** Accepted **Date:** 2024-05-08 ## Context We need a secure authentication system that supports API keys, user sessions, and third-party integrations. ## Options Considered ### Option 1: OAuth 2.0 with JWT Industry standard token-based auth.",
              "score": null
            },
            {
              "doc_id": "api_users",
              "chunk_id": "api_users_fix_0",
              "content": "# Users API Reference ## Overview The Users API provides programmatic access to users functionality. Base URL: `https://api.cloudflow.io/v1/users` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /users/me Get current user profile.",
              "score": null
            },
            {
              "doc_id": "api_secrets",
              "chunk_id": "api_secrets_fix_2",
              "content": "**Parameters:** **Response:** - Success: `204` - Secret deleted - Error: `400` - Bad Request **Example:** ```json {} ```",
              "score": null
            },
            {
              "doc_id": "api_audit",
              "chunk_id": "api_audit_fix_1",
              "content": "**Parameters:** - `start_date` (date): Start date for log range - `end_date` (date): End date for log range - `actor` (string): Filter by user ID **Response:** - Success: `200` - Returns array of audit log entries - Error: `400` - Bad Request **Example:** ```json {\"logs\": [{\"timestamp\": \"2024-01-15T10:00:00Z\", \"actor\": \"usr_001\", \"action\": \"workflow.created\", \"resource\": \"wf_123\"}]} ```",
              "score": null
            }
          ],
          "latency_ms": 3.3892049996211426,
          "query_id": "cross_011",
          "dimension": "problem",
          "query": "users are getting 401 unauthorized on API calls even with tokens that were working yesterday"
        },
        {
          "key_facts": [
            "OAuth 2.0",
            "JWT",
            "bcrypt",
            "RS256",
            "cost factor 12"
          ],
          "found_facts": [
            "OAuth 2.0",
            "bcrypt",
            "cost factor 12"
          ],
          "missed_facts": [
            "JWT",
            "RS256"
          ],
          "coverage": 0.6,
          "expected_docs": [
            "adr_003",
            "arch_authentication_service",
            "arch_api_gateway"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "arch_authentication_service",
              "chunk_id": "arch_authentication_service_fix_0",
              "content": "# Authentication Service Architecture ## Overview The Authentication Service handles user identity, session management, and token issuance. Supports OAuth 2.0, SAML, and API key authentication. ## Design Principles - **Zero Knowledge**: Passwords never stored in plaintext - **Token Rotation**: Refresh tokens rotate on each use ## Components ### Identity Provider Manages user accounts and credentials. Password hashing uses bcrypt with cost factor 12.",
              "score": null
            },
            {
              "doc_id": "api_users",
              "chunk_id": "api_users_fix_0",
              "content": "# Users API Reference ## Overview The Users API provides programmatic access to users functionality. Base URL: `https://api.cloudflow.io/v1/users` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /users/me Get current user profile.",
              "score": null
            },
            {
              "doc_id": "arch_api_gateway",
              "chunk_id": "arch_api_gateway_fix_0",
              "content": "# API Gateway Architecture ## Overview The API Gateway handles all incoming HTTP requests, providing authentication, rate limiting, and request routing. Built on Kong with custom plugins for CloudFlow-specific functionality. ## Design Principles - **Zero Trust**: Every request is authenticated and authorized - **Defense in Depth**: Multiple layers of security validation ## Components ### Kong Gateway Primary ingress controller. Handles TLS termination, request routing, and plugin execution.",
              "score": null
            },
            {
              "doc_id": "api_secrets",
              "chunk_id": "api_secrets_fix_0",
              "content": "# Secrets API Reference ## Overview The Secrets API provides programmatic access to secrets functionality. Base URL: `https://api.cloudflow.io/v1/secrets` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /secrets List secret names (values not returned).",
              "score": null
            },
            {
              "doc_id": "api_integrations",
              "chunk_id": "api_integrations_fix_0",
              "content": "# Integrations API Reference ## Overview The Integrations API provides programmatic access to integrations functionality. Base URL: `https://api.cloudflow.io/v1/integrations` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /integrations List available integrations.",
              "score": null
            }
          ],
          "latency_ms": 3.211032999388408,
          "query_id": "cross_011",
          "dimension": "casual",
          "query": "auth flow details"
        },
        {
          "key_facts": [
            "OAuth 2.0",
            "JWT",
            "bcrypt",
            "RS256",
            "cost factor 12"
          ],
          "found_facts": [],
          "missed_facts": [
            "OAuth 2.0",
            "JWT",
            "bcrypt",
            "RS256",
            "cost factor 12"
          ],
          "coverage": 0.0,
          "expected_docs": [
            "adr_003",
            "arch_authentication_service",
            "arch_api_gateway"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "api_integrations",
              "chunk_id": "api_integrations_fix_0",
              "content": "# Integrations API Reference ## Overview The Integrations API provides programmatic access to integrations functionality. Base URL: `https://api.cloudflow.io/v1/integrations` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /integrations List available integrations.",
              "score": null
            },
            {
              "doc_id": "api_users",
              "chunk_id": "api_users_fix_0",
              "content": "# Users API Reference ## Overview The Users API provides programmatic access to users functionality. Base URL: `https://api.cloudflow.io/v1/users` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /users/me Get current user profile.",
              "score": null
            },
            {
              "doc_id": "api_workflows",
              "chunk_id": "api_workflows_fix_0",
              "content": "# Workflows API Reference ## Overview The Workflows API provides programmatic access to workflows functionality. Base URL: `https://api.cloudflow.io/v1/workflows` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /workflows List all workflows for the authenticated user.",
              "score": null
            },
            {
              "doc_id": "arch_api_gateway",
              "chunk_id": "arch_api_gateway_fix_0",
              "content": "# API Gateway Architecture ## Overview The API Gateway handles all incoming HTTP requests, providing authentication, rate limiting, and request routing. Built on Kong with custom plugins for CloudFlow-specific functionality. ## Design Principles - **Zero Trust**: Every request is authenticated and authorized - **Defense in Depth**: Multiple layers of security validation ## Components ### Kong Gateway Primary ingress controller. Handles TLS termination, request routing, and plugin execution.",
              "score": null
            },
            {
              "doc_id": "howto_create_api_integration",
              "chunk_id": "howto_create_api_integration_fix_0",
              "content": "# How to Create API Integration ## Prerequisites - CloudFlow account ## Steps ### Step 1: Generate API key Create an API key for programmatic access. ```bash cloudflow apikeys create --name 'CI Integration' --scopes workflows:read,workflows:execute ``` ### Step 2: Store key securely Never commit API keys to source control. Use environment variables or secret management. > **Note:** Rotate keys every 90 days ### Step 3: Test authentication Verify the API key works correctly.",
              "score": null
            }
          ],
          "latency_ms": 3.5767549998126924,
          "query_id": "cross_011",
          "dimension": "contextual",
          "query": "building a third-party integration that needs to call cloudflow APIs on behalf of users, need to understand the full auth chain from credentials to token validation"
        },
        {
          "key_facts": [
            "OAuth 2.0",
            "JWT",
            "bcrypt",
            "RS256",
            "cost factor 12"
          ],
          "found_facts": [
            "OAuth 2.0",
            "JWT",
            "bcrypt",
            "RS256",
            "cost factor 12"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "adr_003",
            "arch_authentication_service",
            "arch_api_gateway"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "adr_003",
              "chunk_id": "adr_003_fix_0",
              "content": "# ADR-003: Implement OAuth 2.0 with JWT for Authentication **Status:** Accepted **Date:** 2024-05-08 ## Context We need a secure authentication system that supports API keys, user sessions, and third-party integrations. ## Options Considered ### Option 1: OAuth 2.0 with JWT Industry standard token-based auth.",
              "score": null
            },
            {
              "doc_id": "adr_003",
              "chunk_id": "adr_003_fix_2",
              "content": "**Pros:** - Very simple - Low overhead - Easy to implement **Cons:** - No user context - Hard to rotate - No SSO support ## Decision We will implement OAuth 2.0 with JWT tokens. Access tokens expire after 1 hour, refresh tokens after 7 days. ## Consequences - Implement token rotation on refresh - Support API keys for machine-to-machine auth - Add MFA for admin accounts",
              "score": null
            },
            {
              "doc_id": "arch_authentication_service",
              "chunk_id": "arch_authentication_service_fix_0",
              "content": "# Authentication Service Architecture ## Overview The Authentication Service handles user identity, session management, and token issuance. Supports OAuth 2.0, SAML, and API key authentication. ## Design Principles - **Zero Knowledge**: Passwords never stored in plaintext - **Token Rotation**: Refresh tokens rotate on each use ## Components ### Identity Provider Manages user accounts and credentials. Password hashing uses bcrypt with cost factor 12.",
              "score": null
            },
            {
              "doc_id": "adr_003",
              "chunk_id": "adr_003_fix_1",
              "content": "**Pros:** - Industry standard - Stateless validation - Supports refresh tokens - SSO compatible **Cons:** - Token size overhead - Cannot revoke individual tokens - Complexity ### Option 2: Session-based auth Traditional server-side sessions. **Pros:** - Simple implementation - Easy revocation - Smaller payloads **Cons:** - Requires session store - Not suitable for APIs - Scaling challenges ### Option 3: API keys only Simple key-based authentication.",
              "score": null
            },
            {
              "doc_id": "arch_authentication_service",
              "chunk_id": "arch_authentication_service_fix_1",
              "content": "**Technology:** Custom Go service **Scaling:** Horizontal stateless ### Token Service Issues and validates access tokens. Token lifetime is 1 hour with 7-day refresh tokens. **Technology:** JWT with RS256 **Scaling:** Horizontal stateless ### SSO Gateway Integrates with external identity providers. Supports Google, Microsoft, Okta, and custom SAML.",
              "score": null
            }
          ],
          "latency_ms": 3.804197998761083,
          "query_id": "cross_011",
          "dimension": "negation",
          "query": "why isn't there a simpler auth mechanism instead of all these JWT and OAuth layers"
        },
        {
          "key_facts": [
            "Prometheus",
            "Thanos",
            "Loki",
            "Grafana",
            "Jaeger",
            "OpenTelemetry"
          ],
          "found_facts": [],
          "missed_facts": [
            "Prometheus",
            "Thanos",
            "Loki",
            "Grafana",
            "Jaeger",
            "OpenTelemetry"
          ],
          "coverage": 0.0,
          "expected_docs": [
            "arch_monitoring_stack"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "api_integrations",
              "chunk_id": "api_integrations_fix_0",
              "content": "# Integrations API Reference ## Overview The Integrations API provides programmatic access to integrations functionality. Base URL: `https://api.cloudflow.io/v1/integrations` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /integrations List available integrations.",
              "score": null
            },
            {
              "doc_id": "api_audit",
              "chunk_id": "api_audit_fix_0",
              "content": "# Audit API Reference ## Overview The Audit API provides programmatic access to audit functionality. Base URL: `https://api.cloudflow.io/v1/audit` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /audit/logs Retrieve audit logs for compliance.",
              "score": null
            },
            {
              "doc_id": "arch_api_gateway",
              "chunk_id": "arch_api_gateway_fix_0",
              "content": "# API Gateway Architecture ## Overview The API Gateway handles all incoming HTTP requests, providing authentication, rate limiting, and request routing. Built on Kong with custom plugins for CloudFlow-specific functionality. ## Design Principles - **Zero Trust**: Every request is authenticated and authorized - **Defense in Depth**: Multiple layers of security validation ## Components ### Kong Gateway Primary ingress controller. Handles TLS termination, request routing, and plugin execution.",
              "score": null
            },
            {
              "doc_id": "api_metrics",
              "chunk_id": "api_metrics_fix_0",
              "content": "# Metrics API Reference ## Overview The Metrics API provides programmatic access to metrics functionality. Base URL: `https://api.cloudflow.io/v1/metrics` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /metrics/workflows Get workflow performance metrics.",
              "score": null
            },
            {
              "doc_id": "howto_implement_retry_logic",
              "chunk_id": "howto_implement_retry_logic_fix_1",
              "content": "### Step 4: Handle final failure Configure on_failure handler to notify or escalate when all retries are exhausted. ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            }
          ],
          "latency_ms": 3.7755550001747906,
          "query_id": "cross_012",
          "dimension": "original",
          "query": "What monitoring and observability tools does CloudFlow use?"
        },
        {
          "key_facts": [
            "Prometheus",
            "Thanos",
            "Loki",
            "Grafana",
            "Jaeger",
            "OpenTelemetry"
          ],
          "found_facts": [
            "Prometheus",
            "Thanos"
          ],
          "missed_facts": [
            "Loki",
            "Grafana",
            "Jaeger",
            "OpenTelemetry"
          ],
          "coverage": 0.3333333333333333,
          "expected_docs": [
            "arch_monitoring_stack"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "api_metrics",
              "chunk_id": "api_metrics_fix_0",
              "content": "# Metrics API Reference ## Overview The Metrics API provides programmatic access to metrics functionality. Base URL: `https://api.cloudflow.io/v1/metrics` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /metrics/workflows Get workflow performance metrics.",
              "score": null
            },
            {
              "doc_id": "arch_monitoring_stack",
              "chunk_id": "arch_monitoring_stack_fix_0",
              "content": "# Monitoring Stack Architecture ## Overview The Monitoring Stack provides observability into system health, performance, and errors. Includes metrics, logs, traces, and alerting. ## Design Principles - **Observability**: Every component emits metrics and traces - **Actionable Alerts**: Alerts include runbook links ## Components ### Metrics Collects time-series metrics. Retention is 15 days local, 1 year in object storage. **Technology:** Prometheus with Thanos **Scaling:** Federated with long-term storage ### Logging Centralized log aggregation.",
              "score": null
            },
            {
              "doc_id": "api_audit",
              "chunk_id": "api_audit_fix_0",
              "content": "# Audit API Reference ## Overview The Audit API provides programmatic access to audit functionality. Base URL: `https://api.cloudflow.io/v1/audit` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /audit/logs Retrieve audit logs for compliance.",
              "score": null
            },
            {
              "doc_id": "arch_api_gateway",
              "chunk_id": "arch_api_gateway_fix_0",
              "content": "# API Gateway Architecture ## Overview The API Gateway handles all incoming HTTP requests, providing authentication, rate limiting, and request routing. Built on Kong with custom plugins for CloudFlow-specific functionality. ## Design Principles - **Zero Trust**: Every request is authenticated and authorized - **Defense in Depth**: Multiple layers of security validation ## Components ### Kong Gateway Primary ingress controller. Handles TLS termination, request routing, and plugin execution.",
              "score": null
            },
            {
              "doc_id": "release_3_1_0",
              "chunk_id": "release_3_1_0_fix_0",
              "content": "# Release Notes - v3.1.0 **Release Date:** 2023-12-01 ## New Features - GraphQL API beta release - Workflow templates marketplace - Enhanced audit logging - SSO support for Okta ## Bug Fixes - Fixed webhook delivery retry logic - Resolved issue with large payload handling - Fixed dashboard loading performance ## Upgrade Instructions ```bash cloudflow upgrade --version 3.1.0 ``` ## Known Issues None at this time.",
              "score": null
            }
          ],
          "latency_ms": 3.7504980009543942,
          "query_id": "cross_012",
          "dimension": "synonym",
          "query": "what's the metrics and logging infrastructure in cloudflow"
        },
        {
          "key_facts": [
            "Prometheus",
            "Thanos",
            "Loki",
            "Grafana",
            "Jaeger",
            "OpenTelemetry"
          ],
          "found_facts": [
            "Prometheus",
            "Thanos"
          ],
          "missed_facts": [
            "Loki",
            "Grafana",
            "Jaeger",
            "OpenTelemetry"
          ],
          "coverage": 0.3333333333333333,
          "expected_docs": [
            "arch_monitoring_stack"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "runbook_out_of_memory_oom_kills",
              "chunk_id": "runbook_out_of_memory_oom_kills_fix_2",
              "content": "increase, escalate to development team for memory leak investigation. ## Post-Incident - Document timeline in incident log - Schedule post-mortem within 48 hours - Update runbook if new patterns discovered",
              "score": null
            },
            {
              "doc_id": "arch_monitoring_stack",
              "chunk_id": "arch_monitoring_stack_fix_0",
              "content": "# Monitoring Stack Architecture ## Overview The Monitoring Stack provides observability into system health, performance, and errors. Includes metrics, logs, traces, and alerting. ## Design Principles - **Observability**: Every component emits metrics and traces - **Actionable Alerts**: Alerts include runbook links ## Components ### Metrics Collects time-series metrics. Retention is 15 days local, 1 year in object storage. **Technology:** Prometheus with Thanos **Scaling:** Federated with long-term storage ### Logging Centralized log aggregation.",
              "score": null
            },
            {
              "doc_id": "howto_debug_failed_execution",
              "chunk_id": "howto_debug_failed_execution_fix_0",
              "content": "# How to Debug Failed Execution ## Prerequisites - Failed execution ID ## Steps ### Step 1: Get execution details Retrieve the full execution record including step outputs. ```bash cloudflow executions get ex_123 --include-steps ``` ### Step 2: View step logs Check logs for the failed step. ```bash cloudflow executions logs ex_123 --step 3 ``` ### Step 3: Inspect input/output Review the data passed between steps.",
              "score": null
            },
            {
              "doc_id": "runbook_api_gateway_5xx_errors",
              "chunk_id": "runbook_api_gateway_5xx_errors_fix_0",
              "content": "# Runbook: API Gateway 5xx Errors ## Severity **Level:** P3 - Medium **Response Time:** 1 hour ## Symptoms - Error rate > 1% - 5xx responses in access logs - Downstream service failures ## Diagnosis ### Step 1 Check Kong error logs: `kubectl logs -l app=kong -n kong --tail=100 | grep -E '5[0-9]{2}'` ### Step 2 Verify backend health: `kubectl get pods -n production -o wide` ### Step 3 Check service endpoints: `kubectl get endpoints",
              "score": null
            },
            {
              "doc_id": "runbook_api_gateway_5xx_errors",
              "chunk_id": "runbook_api_gateway_5xx_errors_fix_1",
              "content": "-n production` ### Step 4 Test backend directly: `kubectl exec -it kong-xxx -n kong -- curl http://api.production.svc:8080/health` ## Resolution ### Step 1 Restart unhealthy backends: `kubectl rollout restart deployment/api -n production` ### Step 2 Enable circuit breaker if cascading: update Kong circuit-breaker plugin ### Step 3 Scale up backends: `kubectl scale deployment/api --replicas=5 -n production` ### Step 4 Check for resource exhaustion on backend pods ### Step 5 Verify network policies allow traffic ## Escalation",
              "score": null
            }
          ],
          "latency_ms": 3.9512319999630563,
          "query_id": "cross_012",
          "dimension": "problem",
          "query": "can't find any dashboards or logs when trying to debug a production issue"
        },
        {
          "key_facts": [
            "Prometheus",
            "Thanos",
            "Loki",
            "Grafana",
            "Jaeger",
            "OpenTelemetry"
          ],
          "found_facts": [
            "Prometheus",
            "Thanos",
            "Loki",
            "Grafana",
            "Jaeger",
            "OpenTelemetry"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "arch_monitoring_stack"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "arch_monitoring_stack",
              "chunk_id": "arch_monitoring_stack_fix_0",
              "content": "# Monitoring Stack Architecture ## Overview The Monitoring Stack provides observability into system health, performance, and errors. Includes metrics, logs, traces, and alerting. ## Design Principles - **Observability**: Every component emits metrics and traces - **Actionable Alerts**: Alerts include runbook links ## Components ### Metrics Collects time-series metrics. Retention is 15 days local, 1 year in object storage. **Technology:** Prometheus with Thanos **Scaling:** Federated with long-term storage ### Logging Centralized log aggregation.",
              "score": null
            },
            {
              "doc_id": "arch_monitoring_stack",
              "chunk_id": "arch_monitoring_stack_fix_2",
              "content": "**Technology:** Alertmanager with PagerDuty **Scaling:** HA pair ## Data Flow Application \u2192 Collector \u2192 Storage \u2192 Query \u2192 Dashboard/Alert ## Performance Characteristics - **Latency P50:** N/A - **Latency P99:** N/A - **Throughput:** 1M metrics/second, 100GB logs/day",
              "score": null
            },
            {
              "doc_id": "arch_monitoring_stack",
              "chunk_id": "arch_monitoring_stack_fix_1",
              "content": "Supports structured JSON logs with label indexing. **Technology:** Loki with Grafana **Scaling:** Distributed with sharding ### Tracing Distributed request tracing. Trace retention is 7 days. **Technology:** Jaeger with OpenTelemetry **Scaling:** Sampling at 10% ### Alerting Routes alerts to on-call engineers. Supports escalation policies and silencing.",
              "score": null
            },
            {
              "doc_id": "runbook_high_cpu_usage",
              "chunk_id": "runbook_high_cpu_usage_fix_2",
              "content": "high after scaling, escalate to platform team. Page on-call if affecting users. ## Post-Incident - Document timeline in incident log - Schedule post-mortem within 48 hours - Update runbook if new patterns discovered",
              "score": null
            },
            {
              "doc_id": "runbook_out_of_memory_oom_kills",
              "chunk_id": "runbook_out_of_memory_oom_kills_fix_1",
              "content": "grep -A5 Limits` ### Step 4 Analyze heap dumps if available: `cloudflow debug heapdump --pod <pod-name>` ## Resolution ### Step 1 Increase memory limit: update deployment resources.limits.memory ### Step 2 Add memory request equal to limit to guarantee allocation ### Step 3 Enable JVM heap dump on OOM: add -XX:+HeapDumpOnOutOfMemoryError ### Step 4 Investigate memory leaks with profiling tools ### Step 5 Consider horizontal scaling instead of vertical ## Escalation If OOMs continue after limit",
              "score": null
            }
          ],
          "latency_ms": 3.417568001168547,
          "query_id": "cross_012",
          "dimension": "casual",
          "query": "monitoring stack"
        },
        {
          "key_facts": [
            "Prometheus",
            "Thanos",
            "Loki",
            "Grafana",
            "Jaeger",
            "OpenTelemetry"
          ],
          "found_facts": [
            "Prometheus",
            "Thanos"
          ],
          "missed_facts": [
            "Loki",
            "Grafana",
            "Jaeger",
            "OpenTelemetry"
          ],
          "coverage": 0.3333333333333333,
          "expected_docs": [
            "arch_monitoring_stack"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "howto_set_up_monitoring_alerts",
              "chunk_id": "howto_set_up_monitoring_alerts_fix_0",
              "content": "# How to Set Up Monitoring Alerts ## Prerequisites - CloudFlow Pro plan or higher ## Steps ### Step 1: Define alert rules Create alert conditions based on metrics thresholds. ```bash cloudflow alerts create --name 'High Error Rate' --condition 'error_rate > 0.05' ``` ### Step 2: Configure channels Set up notification channels: email, Slack, or PagerDuty. ```bash cloudflow alerts channel add --type slack --webhook https://hooks.slack.com/...",
              "score": null
            },
            {
              "doc_id": "arch_monitoring_stack",
              "chunk_id": "arch_monitoring_stack_fix_0",
              "content": "# Monitoring Stack Architecture ## Overview The Monitoring Stack provides observability into system health, performance, and errors. Includes metrics, logs, traces, and alerting. ## Design Principles - **Observability**: Every component emits metrics and traces - **Actionable Alerts**: Alerts include runbook links ## Components ### Metrics Collects time-series metrics. Retention is 15 days local, 1 year in object storage. **Technology:** Prometheus with Thanos **Scaling:** Federated with long-term storage ### Logging Centralized log aggregation.",
              "score": null
            },
            {
              "doc_id": "arch_monitoring_stack",
              "chunk_id": "arch_monitoring_stack_fix_2",
              "content": "**Technology:** Alertmanager with PagerDuty **Scaling:** HA pair ## Data Flow Application \u2192 Collector \u2192 Storage \u2192 Query \u2192 Dashboard/Alert ## Performance Characteristics - **Latency P50:** N/A - **Latency P99:** N/A - **Throughput:** 1M metrics/second, 100GB logs/day",
              "score": null
            },
            {
              "doc_id": "howto_set_up_monitoring_alerts",
              "chunk_id": "howto_set_up_monitoring_alerts_fix_1",
              "content": "``` ### Step 3: Set severity levels Assign severity to each alert: critical, warning, or info. > **Note:** Critical alerts bypass quiet hours ### Step 4: Test alerts Trigger a test alert to verify routing. ```bash cloudflow alerts test --name 'High Error Rate' ``` ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            },
            {
              "doc_id": "adr_005",
              "chunk_id": "adr_005_fix_1",
              "content": "**Pros:** - Simple mental model - Immediate feedback - Easy debugging **Cons:** - Tight coupling - Cascading failures - Timeout issues ### Option 3: Polling-based Workers poll for tasks. **Pros:** - Simple implementation - Easy rate limiting - No message broker **Cons:** - Polling overhead - Higher latency - Inefficient at scale ## Decision We will use an event-driven architecture with Kafka for workflow step execution.",
              "score": null
            }
          ],
          "latency_ms": 3.4647259999474045,
          "query_id": "cross_012",
          "dimension": "contextual",
          "query": "setting up alerts for our team's workflows, need to know what monitoring tools are available and how they connect"
        },
        {
          "key_facts": [
            "Prometheus",
            "Thanos",
            "Loki",
            "Grafana",
            "Jaeger",
            "OpenTelemetry"
          ],
          "found_facts": [
            "Prometheus",
            "Thanos",
            "Loki",
            "Grafana",
            "Jaeger",
            "OpenTelemetry"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "arch_monitoring_stack"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "arch_monitoring_stack",
              "chunk_id": "arch_monitoring_stack_fix_0",
              "content": "# Monitoring Stack Architecture ## Overview The Monitoring Stack provides observability into system health, performance, and errors. Includes metrics, logs, traces, and alerting. ## Design Principles - **Observability**: Every component emits metrics and traces - **Actionable Alerts**: Alerts include runbook links ## Components ### Metrics Collects time-series metrics. Retention is 15 days local, 1 year in object storage. **Technology:** Prometheus with Thanos **Scaling:** Federated with long-term storage ### Logging Centralized log aggregation.",
              "score": null
            },
            {
              "doc_id": "adr_005",
              "chunk_id": "adr_005_fix_1",
              "content": "**Pros:** - Simple mental model - Immediate feedback - Easy debugging **Cons:** - Tight coupling - Cascading failures - Timeout issues ### Option 3: Polling-based Workers poll for tasks. **Pros:** - Simple implementation - Easy rate limiting - No message broker **Cons:** - Polling overhead - Higher latency - Inefficient at scale ## Decision We will use an event-driven architecture with Kafka for workflow step execution.",
              "score": null
            },
            {
              "doc_id": "arch_monitoring_stack",
              "chunk_id": "arch_monitoring_stack_fix_2",
              "content": "**Technology:** Alertmanager with PagerDuty **Scaling:** HA pair ## Data Flow Application \u2192 Collector \u2192 Storage \u2192 Query \u2192 Dashboard/Alert ## Performance Characteristics - **Latency P50:** N/A - **Latency P99:** N/A - **Throughput:** 1M metrics/second, 100GB logs/day",
              "score": null
            },
            {
              "doc_id": "arch_monitoring_stack",
              "chunk_id": "arch_monitoring_stack_fix_1",
              "content": "Supports structured JSON logs with label indexing. **Technology:** Loki with Grafana **Scaling:** Distributed with sharding ### Tracing Distributed request tracing. Trace retention is 7 days. **Technology:** Jaeger with OpenTelemetry **Scaling:** Sampling at 10% ### Alerting Routes alerts to on-call engineers. Supports escalation policies and silencing.",
              "score": null
            },
            {
              "doc_id": "adr_006",
              "chunk_id": "adr_006_fix_1",
              "content": "**Pros:** - Native AWS support - No state file - Drift detection **Cons:** - AWS only - Verbose YAML/JSON - Slower development ### Option 3: Pulumi IaC with real programming languages. **Pros:** - Real languages - Type safety - Reusable components **Cons:** - Smaller community - Vendor lock-in - Commercial features ## Decision We will use Terraform with S3 backend for state and DynamoDB for locking.",
              "score": null
            }
          ],
          "latency_ms": 3.7736419999419013,
          "query_id": "cross_012",
          "dimension": "negation",
          "query": "why are there so many different monitoring tools instead of a unified platform"
        },
        {
          "key_facts": [
            "Redis Cluster",
            "Ristretto",
            "CloudFront",
            "cache-aside"
          ],
          "found_facts": [
            "Redis Cluster",
            "cache-aside"
          ],
          "missed_facts": [
            "Ristretto",
            "CloudFront"
          ],
          "coverage": 0.5,
          "expected_docs": [
            "arch_caching_layer",
            "adr_004",
            "config_cache"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "arch_caching_layer",
              "chunk_id": "arch_caching_layer_fix_0",
              "content": "# Caching Layer Architecture ## Overview The Caching Layer reduces latency and database load by caching frequently accessed data. Uses a multi-tier strategy with local and distributed caches. ## Design Principles - **Cache Invalidation**: Explicit invalidation on writes - **Graceful Degradation**: System works without cache ## Components ### Local Cache First-level cache with 100ms TTL. Reduces Redis round-trips for hot data.",
              "score": null
            },
            {
              "doc_id": "config_cache",
              "chunk_id": "config_cache_fix_0",
              "content": "# Cache Configuration Reference ## Overview This document describes all configuration options for Cache. ## Environment Variables | Variable | Type | Default | Description | |----------|------|---------|-------------| | `REDIS_URL` | string | required | Redis connection string | | `REDIS_POOL_SIZE` | integer | 10 | Connection pool size | | `CACHE_TTL_SECONDS` | integer | 300 | Default cache TTL | | `CACHE_PREFIX` | string | cf: | Key prefix for namespacing | | `CACHE_COMPRESSION` |",
              "score": null
            },
            {
              "doc_id": "adr_004",
              "chunk_id": "adr_004_fix_0",
              "content": "# ADR-004: Use Redis for Caching and Session Storage **Status:** Accepted **Date:** 2024-04-05 ## Context We need a fast data store for caching API responses, session data, and rate limiting counters. ## Options Considered ### Option 1: Redis Cluster In-memory data structure store. **Pros:** - Sub-millisecond latency - Rich data structures - Pub/sub support - Clustering **Cons:** - Memory cost - Persistence complexity - Cluster management ### Option 2: Memcached Simple key-value cache.",
              "score": null
            },
            {
              "doc_id": "adr_004",
              "chunk_id": "adr_004_fix_2",
              "content": "## Consequences - Implement cache-aside pattern - Set appropriate TTLs - Monitor memory usage - Plan for cache stampede prevention",
              "score": null
            },
            {
              "doc_id": "adr_004",
              "chunk_id": "adr_004_fix_1",
              "content": "**Pros:** - Simple operation - Multi-threaded - Lower memory overhead **Cons:** - No persistence - Limited data types - No clustering ### Option 3: Application-level cache In-process caching only. **Pros:** - No external dependency - Fastest possible - Simple **Cons:** - Not shared across pods - Memory pressure - Cold starts ## Decision We will use Redis Cluster for caching with a 6-node cluster for high availability.",
              "score": null
            }
          ],
          "latency_ms": 3.4310129994992167,
          "query_id": "cross_013",
          "dimension": "original",
          "query": "What caching strategy is used across the system?"
        },
        {
          "key_facts": [
            "Redis Cluster",
            "Ristretto",
            "CloudFront",
            "cache-aside"
          ],
          "found_facts": [
            "CloudFront"
          ],
          "missed_facts": [
            "Redis Cluster",
            "Ristretto",
            "cache-aside"
          ],
          "coverage": 0.25,
          "expected_docs": [
            "arch_caching_layer",
            "adr_004",
            "config_cache"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "arch_caching_layer",
              "chunk_id": "arch_caching_layer_fix_0",
              "content": "# Caching Layer Architecture ## Overview The Caching Layer reduces latency and database load by caching frequently accessed data. Uses a multi-tier strategy with local and distributed caches. ## Design Principles - **Cache Invalidation**: Explicit invalidation on writes - **Graceful Degradation**: System works without cache ## Components ### Local Cache First-level cache with 100ms TTL. Reduces Redis round-trips for hot data.",
              "score": null
            },
            {
              "doc_id": "arch_data_pipeline",
              "chunk_id": "arch_data_pipeline_fix_0",
              "content": "# Data Pipeline Architecture ## Overview The Data Pipeline processes and transforms data between workflow steps. It supports batch processing up to 10GB and streaming for real-time use cases. ## Design Principles - **Exactly-Once Processing**: Each record is processed exactly once - **Backpressure Handling**: System gracefully handles varying load ## Components ### Stream Processor Handles real-time data streams. Maximum throughput of 100000 messages per second.",
              "score": null
            },
            {
              "doc_id": "arch_caching_layer",
              "chunk_id": "arch_caching_layer_fix_2",
              "content": "**Technology:** CloudFront with Lambda@Edge **Scaling:** Global edge locations ## Data Flow Request \u2192 Local Cache \u2192 Redis \u2192 Database \u2192 Cache Population ## Performance Characteristics - **Latency P50:** 1ms (hit), 50ms (miss) - **Latency P99:** 5ms (hit), 200ms (miss) - **Throughput:** 100000 reads/second",
              "score": null
            },
            {
              "doc_id": "arch_api_gateway",
              "chunk_id": "arch_api_gateway_fix_0",
              "content": "# API Gateway Architecture ## Overview The API Gateway handles all incoming HTTP requests, providing authentication, rate limiting, and request routing. Built on Kong with custom plugins for CloudFlow-specific functionality. ## Design Principles - **Zero Trust**: Every request is authenticated and authorized - **Defense in Depth**: Multiple layers of security validation ## Components ### Kong Gateway Primary ingress controller. Handles TLS termination, request routing, and plugin execution.",
              "score": null
            },
            {
              "doc_id": "runbook_high_cpu_usage",
              "chunk_id": "runbook_high_cpu_usage_fix_1",
              "content": "Step 4 Profile the application: `cloudflow debug profile --pod api-xyz --duration 60s` ## Resolution ### Step 1 Scale up if traffic-related: `kubectl scale deployment/api --replicas=10 -n production` ### Step 2 Restart pods if memory leak suspected: `kubectl rollout restart deployment/api -n production` ### Step 3 Roll back recent deployment if regression: `kubectl rollout undo deployment/api -n production` ### Step 4 Add resource limits if unbounded: update deployment with CPU limits ## Escalation If CPU remains",
              "score": null
            }
          ],
          "latency_ms": 3.361644001415698,
          "query_id": "cross_013",
          "dimension": "synonym",
          "query": "how does cloudflow handle data caching and what layers are involved"
        },
        {
          "key_facts": [
            "Redis Cluster",
            "Ristretto",
            "CloudFront",
            "cache-aside"
          ],
          "found_facts": [
            "Redis Cluster",
            "Ristretto",
            "CloudFront",
            "cache-aside"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "arch_caching_layer",
            "adr_004",
            "config_cache"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "arch_caching_layer",
              "chunk_id": "arch_caching_layer_fix_0",
              "content": "# Caching Layer Architecture ## Overview The Caching Layer reduces latency and database load by caching frequently accessed data. Uses a multi-tier strategy with local and distributed caches. ## Design Principles - **Cache Invalidation**: Explicit invalidation on writes - **Graceful Degradation**: System works without cache ## Components ### Local Cache First-level cache with 100ms TTL. Reduces Redis round-trips for hot data.",
              "score": null
            },
            {
              "doc_id": "adr_004",
              "chunk_id": "adr_004_fix_0",
              "content": "# ADR-004: Use Redis for Caching and Session Storage **Status:** Accepted **Date:** 2024-04-05 ## Context We need a fast data store for caching API responses, session data, and rate limiting counters. ## Options Considered ### Option 1: Redis Cluster In-memory data structure store. **Pros:** - Sub-millisecond latency - Rich data structures - Pub/sub support - Clustering **Cons:** - Memory cost - Persistence complexity - Cluster management ### Option 2: Memcached Simple key-value cache.",
              "score": null
            },
            {
              "doc_id": "arch_caching_layer",
              "chunk_id": "arch_caching_layer_fix_2",
              "content": "**Technology:** CloudFront with Lambda@Edge **Scaling:** Global edge locations ## Data Flow Request \u2192 Local Cache \u2192 Redis \u2192 Database \u2192 Cache Population ## Performance Characteristics - **Latency P50:** 1ms (hit), 50ms (miss) - **Latency P99:** 5ms (hit), 200ms (miss) - **Throughput:** 100000 reads/second",
              "score": null
            },
            {
              "doc_id": "arch_caching_layer",
              "chunk_id": "arch_caching_layer_fix_1",
              "content": "**Technology:** Ristretto in-memory cache **Scaling:** Per-pod memory allocation ### Distributed Cache Second-level cache with configurable TTL. Maximum key size 512MB. **Technology:** Redis Cluster 7.x **Scaling:** 6-node cluster with sharding ### CDN Caches static assets and API responses. Cache hit ratio target 95%.",
              "score": null
            },
            {
              "doc_id": "adr_004",
              "chunk_id": "adr_004_fix_2",
              "content": "## Consequences - Implement cache-aside pattern - Set appropriate TTLs - Monitor memory usage - Plan for cache stampede prevention",
              "score": null
            }
          ],
          "latency_ms": 3.519036999932723,
          "query_id": "cross_013",
          "dimension": "problem",
          "query": "response times are slow even though we should be hitting cache, not sure where the bottleneck is"
        },
        {
          "key_facts": [
            "Redis Cluster",
            "Ristretto",
            "CloudFront",
            "cache-aside"
          ],
          "found_facts": [
            "Redis Cluster",
            "Ristretto",
            "cache-aside"
          ],
          "missed_facts": [
            "CloudFront"
          ],
          "coverage": 0.75,
          "expected_docs": [
            "arch_caching_layer",
            "adr_004",
            "config_cache"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "arch_caching_layer",
              "chunk_id": "arch_caching_layer_fix_0",
              "content": "# Caching Layer Architecture ## Overview The Caching Layer reduces latency and database load by caching frequently accessed data. Uses a multi-tier strategy with local and distributed caches. ## Design Principles - **Cache Invalidation**: Explicit invalidation on writes - **Graceful Degradation**: System works without cache ## Components ### Local Cache First-level cache with 100ms TTL. Reduces Redis round-trips for hot data.",
              "score": null
            },
            {
              "doc_id": "config_cache",
              "chunk_id": "config_cache_fix_0",
              "content": "# Cache Configuration Reference ## Overview This document describes all configuration options for Cache. ## Environment Variables | Variable | Type | Default | Description | |----------|------|---------|-------------| | `REDIS_URL` | string | required | Redis connection string | | `REDIS_POOL_SIZE` | integer | 10 | Connection pool size | | `CACHE_TTL_SECONDS` | integer | 300 | Default cache TTL | | `CACHE_PREFIX` | string | cf: | Key prefix for namespacing | | `CACHE_COMPRESSION` |",
              "score": null
            },
            {
              "doc_id": "adr_004",
              "chunk_id": "adr_004_fix_2",
              "content": "## Consequences - Implement cache-aside pattern - Set appropriate TTLs - Monitor memory usage - Plan for cache stampede prevention",
              "score": null
            },
            {
              "doc_id": "adr_004",
              "chunk_id": "adr_004_fix_0",
              "content": "# ADR-004: Use Redis for Caching and Session Storage **Status:** Accepted **Date:** 2024-04-05 ## Context We need a fast data store for caching API responses, session data, and rate limiting counters. ## Options Considered ### Option 1: Redis Cluster In-memory data structure store. **Pros:** - Sub-millisecond latency - Rich data structures - Pub/sub support - Clustering **Cons:** - Memory cost - Persistence complexity - Cluster management ### Option 2: Memcached Simple key-value cache.",
              "score": null
            },
            {
              "doc_id": "arch_caching_layer",
              "chunk_id": "arch_caching_layer_fix_1",
              "content": "**Technology:** Ristretto in-memory cache **Scaling:** Per-pod memory allocation ### Distributed Cache Second-level cache with configurable TTL. Maximum key size 512MB. **Technology:** Redis Cluster 7.x **Scaling:** 6-node cluster with sharding ### CDN Caches static assets and API responses. Cache hit ratio target 95%.",
              "score": null
            }
          ],
          "latency_ms": 4.044826999233919,
          "query_id": "cross_013",
          "dimension": "casual",
          "query": "caching layers explained"
        },
        {
          "key_facts": [
            "Redis Cluster",
            "Ristretto",
            "CloudFront",
            "cache-aside"
          ],
          "found_facts": [
            "Redis Cluster",
            "Ristretto",
            "cache-aside"
          ],
          "missed_facts": [
            "CloudFront"
          ],
          "coverage": 0.75,
          "expected_docs": [
            "arch_caching_layer",
            "adr_004",
            "config_cache"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "arch_caching_layer",
              "chunk_id": "arch_caching_layer_fix_0",
              "content": "# Caching Layer Architecture ## Overview The Caching Layer reduces latency and database load by caching frequently accessed data. Uses a multi-tier strategy with local and distributed caches. ## Design Principles - **Cache Invalidation**: Explicit invalidation on writes - **Graceful Degradation**: System works without cache ## Components ### Local Cache First-level cache with 100ms TTL. Reduces Redis round-trips for hot data.",
              "score": null
            },
            {
              "doc_id": "adr_004",
              "chunk_id": "adr_004_fix_0",
              "content": "# ADR-004: Use Redis for Caching and Session Storage **Status:** Accepted **Date:** 2024-04-05 ## Context We need a fast data store for caching API responses, session data, and rate limiting counters. ## Options Considered ### Option 1: Redis Cluster In-memory data structure store. **Pros:** - Sub-millisecond latency - Rich data structures - Pub/sub support - Clustering **Cons:** - Memory cost - Persistence complexity - Cluster management ### Option 2: Memcached Simple key-value cache.",
              "score": null
            },
            {
              "doc_id": "adr_004",
              "chunk_id": "adr_004_fix_2",
              "content": "## Consequences - Implement cache-aside pattern - Set appropriate TTLs - Monitor memory usage - Plan for cache stampede prevention",
              "score": null
            },
            {
              "doc_id": "arch_caching_layer",
              "chunk_id": "arch_caching_layer_fix_1",
              "content": "**Technology:** Ristretto in-memory cache **Scaling:** Per-pod memory allocation ### Distributed Cache Second-level cache with configurable TTL. Maximum key size 512MB. **Technology:** Redis Cluster 7.x **Scaling:** 6-node cluster with sharding ### CDN Caches static assets and API responses. Cache hit ratio target 95%.",
              "score": null
            },
            {
              "doc_id": "config_cache",
              "chunk_id": "config_cache_fix_0",
              "content": "# Cache Configuration Reference ## Overview This document describes all configuration options for Cache. ## Environment Variables | Variable | Type | Default | Description | |----------|------|---------|-------------| | `REDIS_URL` | string | required | Redis connection string | | `REDIS_POOL_SIZE` | integer | 10 | Connection pool size | | `CACHE_TTL_SECONDS` | integer | 300 | Default cache TTL | | `CACHE_PREFIX` | string | cf: | Key prefix for namespacing | | `CACHE_COMPRESSION` |",
              "score": null
            }
          ],
          "latency_ms": 3.965960000641644,
          "query_id": "cross_013",
          "dimension": "contextual",
          "query": "optimizing our API response times, need to understand all the caching layers from local to CDN and their TTLs"
        },
        {
          "key_facts": [
            "Redis Cluster",
            "Ristretto",
            "CloudFront",
            "cache-aside"
          ],
          "found_facts": [
            "Redis Cluster",
            "Ristretto"
          ],
          "missed_facts": [
            "CloudFront",
            "cache-aside"
          ],
          "coverage": 0.5,
          "expected_docs": [
            "arch_caching_layer",
            "adr_004",
            "config_cache"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "arch_caching_layer",
              "chunk_id": "arch_caching_layer_fix_0",
              "content": "# Caching Layer Architecture ## Overview The Caching Layer reduces latency and database load by caching frequently accessed data. Uses a multi-tier strategy with local and distributed caches. ## Design Principles - **Cache Invalidation**: Explicit invalidation on writes - **Graceful Degradation**: System works without cache ## Components ### Local Cache First-level cache with 100ms TTL. Reduces Redis round-trips for hot data.",
              "score": null
            },
            {
              "doc_id": "config_cache",
              "chunk_id": "config_cache_fix_0",
              "content": "# Cache Configuration Reference ## Overview This document describes all configuration options for Cache. ## Environment Variables | Variable | Type | Default | Description | |----------|------|---------|-------------| | `REDIS_URL` | string | required | Redis connection string | | `REDIS_POOL_SIZE` | integer | 10 | Connection pool size | | `CACHE_TTL_SECONDS` | integer | 300 | Default cache TTL | | `CACHE_PREFIX` | string | cf: | Key prefix for namespacing | | `CACHE_COMPRESSION` |",
              "score": null
            },
            {
              "doc_id": "arch_caching_layer",
              "chunk_id": "arch_caching_layer_fix_1",
              "content": "**Technology:** Ristretto in-memory cache **Scaling:** Per-pod memory allocation ### Distributed Cache Second-level cache with configurable TTL. Maximum key size 512MB. **Technology:** Redis Cluster 7.x **Scaling:** 6-node cluster with sharding ### CDN Caches static assets and API responses. Cache hit ratio target 95%.",
              "score": null
            },
            {
              "doc_id": "adr_004",
              "chunk_id": "adr_004_fix_1",
              "content": "**Pros:** - Simple operation - Multi-threaded - Lower memory overhead **Cons:** - No persistence - Limited data types - No clustering ### Option 3: Application-level cache In-process caching only. **Pros:** - No external dependency - Fastest possible - Simple **Cons:** - Not shared across pods - Memory pressure - Cold starts ## Decision We will use Redis Cluster for caching with a 6-node cluster for high availability.",
              "score": null
            },
            {
              "doc_id": "adr_004",
              "chunk_id": "adr_004_fix_0",
              "content": "# ADR-004: Use Redis for Caching and Session Storage **Status:** Accepted **Date:** 2024-04-05 ## Context We need a fast data store for caching API responses, session data, and rate limiting counters. ## Options Considered ### Option 1: Redis Cluster In-memory data structure store. **Pros:** - Sub-millisecond latency - Rich data structures - Pub/sub support - Clustering **Cons:** - Memory cost - Persistence complexity - Cluster management ### Option 2: Memcached Simple key-value cache.",
              "score": null
            }
          ],
          "latency_ms": 3.7285369999153772,
          "query_id": "cross_013",
          "dimension": "negation",
          "query": "why do we have both local and distributed cache instead of just using redis for everything"
        },
        {
          "key_facts": [
            "PostgreSQL",
            "S3",
            "Iceberg",
            "Parquet"
          ],
          "found_facts": [],
          "missed_facts": [
            "PostgreSQL",
            "S3",
            "Iceberg",
            "Parquet"
          ],
          "coverage": 0.0,
          "expected_docs": [
            "adr_001",
            "adr_008",
            "arch_data_pipeline"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "arch_data_pipeline",
              "chunk_id": "arch_data_pipeline_fix_0",
              "content": "# Data Pipeline Architecture ## Overview The Data Pipeline processes and transforms data between workflow steps. It supports batch processing up to 10GB and streaming for real-time use cases. ## Design Principles - **Exactly-Once Processing**: Each record is processed exactly once - **Backpressure Handling**: System gracefully handles varying load ## Components ### Stream Processor Handles real-time data streams. Maximum throughput of 100000 messages per second.",
              "score": null
            },
            {
              "doc_id": "api_integrations",
              "chunk_id": "api_integrations_fix_0",
              "content": "# Integrations API Reference ## Overview The Integrations API provides programmatic access to integrations functionality. Base URL: `https://api.cloudflow.io/v1/integrations` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /integrations List available integrations.",
              "score": null
            },
            {
              "doc_id": "howto_set_up_data_encryption",
              "chunk_id": "howto_set_up_data_encryption_fix_0",
              "content": "# How to Set Up Data Encryption ## Prerequisites - CloudFlow Enterprise plan ## Steps ### Step 1: Enable encryption at rest All data is encrypted at rest using AES-256 by default. > **Note:** Encryption is automatic for all plans ### Step 2: Configure KMS Bring your own encryption keys using AWS KMS or GCP KMS. ```bash cloudflow encryption configure --kms-key arn:aws:kms:...",
              "score": null
            },
            {
              "doc_id": "arch_api_gateway",
              "chunk_id": "arch_api_gateway_fix_0",
              "content": "# API Gateway Architecture ## Overview The API Gateway handles all incoming HTTP requests, providing authentication, rate limiting, and request routing. Built on Kong with custom plugins for CloudFlow-specific functionality. ## Design Principles - **Zero Trust**: Every request is authenticated and authorized - **Defense in Depth**: Multiple layers of security validation ## Components ### Kong Gateway Primary ingress controller. Handles TLS termination, request routing, and plugin execution.",
              "score": null
            },
            {
              "doc_id": "api_users",
              "chunk_id": "api_users_fix_0",
              "content": "# Users API Reference ## Overview The Users API provides programmatic access to users functionality. Base URL: `https://api.cloudflow.io/v1/users` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /users/me Get current user profile.",
              "score": null
            }
          ],
          "latency_ms": 3.3212680009455653,
          "query_id": "cross_014",
          "dimension": "original",
          "query": "How does CloudFlow handle data storage?"
        },
        {
          "key_facts": [
            "PostgreSQL",
            "S3",
            "Iceberg",
            "Parquet"
          ],
          "found_facts": [
            "PostgreSQL",
            "S3"
          ],
          "missed_facts": [
            "Iceberg",
            "Parquet"
          ],
          "coverage": 0.5,
          "expected_docs": [
            "adr_001",
            "adr_008",
            "arch_data_pipeline"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "api_integrations",
              "chunk_id": "api_integrations_fix_0",
              "content": "# Integrations API Reference ## Overview The Integrations API provides programmatic access to integrations functionality. Base URL: `https://api.cloudflow.io/v1/integrations` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /integrations List available integrations.",
              "score": null
            },
            {
              "doc_id": "howto_set_up_database_connection",
              "chunk_id": "howto_set_up_database_connection_fix_0",
              "content": "# How to Set Up Database Connection ## Prerequisites - Database with network access to CloudFlow - Connection credentials ## Steps ### Step 1: Add connection secret Store your database connection string as a secret. ```bash cloudflow secrets set DATABASE_URL postgresql://user:pass@host:5432/db ``` ### Step 2: Configure connection pool Set pool size based on your plan limits. Free tier allows 5 connections, Pro allows 20.",
              "score": null
            },
            {
              "doc_id": "adr_006",
              "chunk_id": "adr_006_fix_1",
              "content": "**Pros:** - Native AWS support - No state file - Drift detection **Cons:** - AWS only - Verbose YAML/JSON - Slower development ### Option 3: Pulumi IaC with real programming languages. **Pros:** - Real languages - Type safety - Reusable components **Cons:** - Smaller community - Vendor lock-in - Commercial features ## Decision We will use Terraform with S3 backend for state and DynamoDB for locking.",
              "score": null
            },
            {
              "doc_id": "arch_data_pipeline",
              "chunk_id": "arch_data_pipeline_fix_0",
              "content": "# Data Pipeline Architecture ## Overview The Data Pipeline processes and transforms data between workflow steps. It supports batch processing up to 10GB and streaming for real-time use cases. ## Design Principles - **Exactly-Once Processing**: Each record is processed exactly once - **Backpressure Handling**: System gracefully handles varying load ## Components ### Stream Processor Handles real-time data streams. Maximum throughput of 100000 messages per second.",
              "score": null
            },
            {
              "doc_id": "howto_set_up_data_encryption",
              "chunk_id": "howto_set_up_data_encryption_fix_0",
              "content": "# How to Set Up Data Encryption ## Prerequisites - CloudFlow Enterprise plan ## Steps ### Step 1: Enable encryption at rest All data is encrypted at rest using AES-256 by default. > **Note:** Encryption is automatic for all plans ### Step 2: Configure KMS Bring your own encryption keys using AWS KMS or GCP KMS. ```bash cloudflow encryption configure --kms-key arn:aws:kms:...",
              "score": null
            }
          ],
          "latency_ms": 3.434428999753436,
          "query_id": "cross_014",
          "dimension": "synonym",
          "query": "what databases and storage systems does cloudflow use"
        },
        {
          "key_facts": [
            "PostgreSQL",
            "S3",
            "Iceberg",
            "Parquet"
          ],
          "found_facts": [
            "S3",
            "Parquet"
          ],
          "missed_facts": [
            "PostgreSQL",
            "Iceberg"
          ],
          "coverage": 0.5,
          "expected_docs": [
            "adr_001",
            "adr_008",
            "arch_data_pipeline"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "arch_data_pipeline",
              "chunk_id": "arch_data_pipeline_fix_1",
              "content": "**Technology:** Apache Kafka with Flink **Scaling:** Partition-based scaling ### Batch Processor Processes large batch jobs. Supports data formats: JSON, CSV, Parquet, Avro. **Technology:** Apache Spark on Kubernetes **Scaling:** Dynamic resource allocation ### Data Lake Long-term data storage with time-travel queries. Retention configurable up to 7 years.",
              "score": null
            },
            {
              "doc_id": "adr_008",
              "chunk_id": "adr_008_fix_0",
              "content": "# ADR-008: Use S3 for Object Storage **Status:** Accepted **Date:** 2024-10-14 ## Context We need storage for workflow artifacts, execution outputs, and user uploads. ## Options Considered ### Option 1: Amazon S3 Object storage service. **Pros:** - 99.999999999% durability - Lifecycle policies - Event notifications - CDN integration **Cons:** - AWS lock-in - Eventual consistency for overwrites - Cost at scale ### Option 2: Self-hosted MinIO S3-compatible object storage.",
              "score": null
            },
            {
              "doc_id": "adr_004",
              "chunk_id": "adr_004_fix_0",
              "content": "# ADR-004: Use Redis for Caching and Session Storage **Status:** Accepted **Date:** 2024-04-05 ## Context We need a fast data store for caching API responses, session data, and rate limiting counters. ## Options Considered ### Option 1: Redis Cluster In-memory data structure store. **Pros:** - Sub-millisecond latency - Rich data structures - Pub/sub support - Clustering **Cons:** - Memory cost - Persistence complexity - Cluster management ### Option 2: Memcached Simple key-value cache.",
              "score": null
            },
            {
              "doc_id": "arch_monitoring_stack",
              "chunk_id": "arch_monitoring_stack_fix_0",
              "content": "# Monitoring Stack Architecture ## Overview The Monitoring Stack provides observability into system health, performance, and errors. Includes metrics, logs, traces, and alerting. ## Design Principles - **Observability**: Every component emits metrics and traces - **Actionable Alerts**: Alerts include runbook links ## Components ### Metrics Collects time-series metrics. Retention is 15 days local, 1 year in object storage. **Technology:** Prometheus with Thanos **Scaling:** Federated with long-term storage ### Logging Centralized log aggregation.",
              "score": null
            },
            {
              "doc_id": "arch_data_pipeline",
              "chunk_id": "arch_data_pipeline_fix_0",
              "content": "# Data Pipeline Architecture ## Overview The Data Pipeline processes and transforms data between workflow steps. It supports batch processing up to 10GB and streaming for real-time use cases. ## Design Principles - **Exactly-Once Processing**: Each record is processed exactly once - **Backpressure Handling**: System gracefully handles varying load ## Components ### Stream Processor Handles real-time data streams. Maximum throughput of 100000 messages per second.",
              "score": null
            }
          ],
          "latency_ms": 3.4440269992046524,
          "query_id": "cross_014",
          "dimension": "problem",
          "query": "running out of storage space and need to understand where all our data actually lives"
        },
        {
          "key_facts": [
            "PostgreSQL",
            "S3",
            "Iceberg",
            "Parquet"
          ],
          "found_facts": [
            "S3",
            "Parquet"
          ],
          "missed_facts": [
            "PostgreSQL",
            "Iceberg"
          ],
          "coverage": 0.5,
          "expected_docs": [
            "adr_001",
            "adr_008",
            "arch_data_pipeline"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "adr_008",
              "chunk_id": "adr_008_fix_0",
              "content": "# ADR-008: Use S3 for Object Storage **Status:** Accepted **Date:** 2024-10-14 ## Context We need storage for workflow artifacts, execution outputs, and user uploads. ## Options Considered ### Option 1: Amazon S3 Object storage service. **Pros:** - 99.999999999% durability - Lifecycle policies - Event notifications - CDN integration **Cons:** - AWS lock-in - Eventual consistency for overwrites - Cost at scale ### Option 2: Self-hosted MinIO S3-compatible object storage.",
              "score": null
            },
            {
              "doc_id": "adr_004",
              "chunk_id": "adr_004_fix_0",
              "content": "# ADR-004: Use Redis for Caching and Session Storage **Status:** Accepted **Date:** 2024-04-05 ## Context We need a fast data store for caching API responses, session data, and rate limiting counters. ## Options Considered ### Option 1: Redis Cluster In-memory data structure store. **Pros:** - Sub-millisecond latency - Rich data structures - Pub/sub support - Clustering **Cons:** - Memory cost - Persistence complexity - Cluster management ### Option 2: Memcached Simple key-value cache.",
              "score": null
            },
            {
              "doc_id": "arch_data_pipeline",
              "chunk_id": "arch_data_pipeline_fix_1",
              "content": "**Technology:** Apache Kafka with Flink **Scaling:** Partition-based scaling ### Batch Processor Processes large batch jobs. Supports data formats: JSON, CSV, Parquet, Avro. **Technology:** Apache Spark on Kubernetes **Scaling:** Dynamic resource allocation ### Data Lake Long-term data storage with time-travel queries. Retention configurable up to 7 years.",
              "score": null
            },
            {
              "doc_id": "arch_caching_layer",
              "chunk_id": "arch_caching_layer_fix_0",
              "content": "# Caching Layer Architecture ## Overview The Caching Layer reduces latency and database load by caching frequently accessed data. Uses a multi-tier strategy with local and distributed caches. ## Design Principles - **Cache Invalidation**: Explicit invalidation on writes - **Graceful Degradation**: System works without cache ## Components ### Local Cache First-level cache with 100ms TTL. Reduces Redis round-trips for hot data.",
              "score": null
            },
            {
              "doc_id": "adr_008",
              "chunk_id": "adr_008_fix_1",
              "content": "**Pros:** - No vendor lock-in - S3 compatible - On-premises option **Cons:** - Operational burden - Lower durability guarantees - Scaling complexity ### Option 3: Google Cloud Storage GCP object storage. **Pros:** - Strong consistency - Good performance - Unified billing **Cons:** - GCP lock-in - Fewer features than S3 - Smaller ecosystem ## Decision We will use Amazon S3 for object storage with intelligent tiering for cost optimization.",
              "score": null
            }
          ],
          "latency_ms": 3.149578000375186,
          "query_id": "cross_014",
          "dimension": "casual",
          "query": "storage architecture"
        },
        {
          "key_facts": [
            "PostgreSQL",
            "S3",
            "Iceberg",
            "Parquet"
          ],
          "found_facts": [
            "PostgreSQL",
            "S3",
            "Iceberg",
            "Parquet"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "adr_001",
            "adr_008",
            "arch_data_pipeline"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "adr_001",
              "chunk_id": "adr_001_fix_0",
              "content": "# ADR-001: Use PostgreSQL for Primary Database **Status:** Accepted **Date:** 2024-11-04 ## Context We need a database to store workflow definitions, execution state, and user data. The database must support ACID transactions, JSON queries, and scale to millions of records. ## Options Considered ### Option 1: PostgreSQL Mature relational database with JSONB support.",
              "score": null
            },
            {
              "doc_id": "arch_data_pipeline",
              "chunk_id": "arch_data_pipeline_fix_1",
              "content": "**Technology:** Apache Kafka with Flink **Scaling:** Partition-based scaling ### Batch Processor Processes large batch jobs. Supports data formats: JSON, CSV, Parquet, Avro. **Technology:** Apache Spark on Kubernetes **Scaling:** Dynamic resource allocation ### Data Lake Long-term data storage with time-travel queries. Retention configurable up to 7 years.",
              "score": null
            },
            {
              "doc_id": "adr_001",
              "chunk_id": "adr_001_fix_1",
              "content": "**Pros:** - ACID compliance - Rich JSONB queries - Team expertise - Proven at scale **Cons:** - Manual sharding for extreme scale - Requires connection pooling ### Option 2: MongoDB Document database with native JSON. **Pros:** - Flexible schema - Built-in sharding - Good for documents **Cons:** - Weaker consistency guarantees - Complex transactions - Higher operational cost ### Option 3: CockroachDB Distributed SQL database.",
              "score": null
            },
            {
              "doc_id": "adr_008",
              "chunk_id": "adr_008_fix_0",
              "content": "# ADR-008: Use S3 for Object Storage **Status:** Accepted **Date:** 2024-10-14 ## Context We need storage for workflow artifacts, execution outputs, and user uploads. ## Options Considered ### Option 1: Amazon S3 Object storage service. **Pros:** - 99.999999999% durability - Lifecycle policies - Event notifications - CDN integration **Cons:** - AWS lock-in - Eventual consistency for overwrites - Cost at scale ### Option 2: Self-hosted MinIO S3-compatible object storage.",
              "score": null
            },
            {
              "doc_id": "arch_data_pipeline",
              "chunk_id": "arch_data_pipeline_fix_2",
              "content": "**Technology:** S3 with Iceberg tables **Scaling:** Unlimited with tiered storage ## Data Flow Input \u2192 Validation \u2192 Transform \u2192 Enrich \u2192 Output ## Performance Characteristics - **Latency P50:** 100ms (streaming), 5min (batch) - **Latency P99:** 500ms (streaming), 30min (batch) - **Throughput:** 100000 messages/second (streaming)",
              "score": null
            }
          ],
          "latency_ms": 3.5500050016707974,
          "query_id": "cross_014",
          "dimension": "contextual",
          "query": "planning data retention policies for compliance, need to map out all the storage tiers from transactional DB to data lake"
        },
        {
          "key_facts": [
            "PostgreSQL",
            "S3",
            "Iceberg",
            "Parquet"
          ],
          "found_facts": [
            "PostgreSQL",
            "S3",
            "Parquet"
          ],
          "missed_facts": [
            "Iceberg"
          ],
          "coverage": 0.75,
          "expected_docs": [
            "adr_001",
            "adr_008",
            "arch_data_pipeline"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "adr_008",
              "chunk_id": "adr_008_fix_1",
              "content": "**Pros:** - No vendor lock-in - S3 compatible - On-premises option **Cons:** - Operational burden - Lower durability guarantees - Scaling complexity ### Option 3: Google Cloud Storage GCP object storage. **Pros:** - Strong consistency - Good performance - Unified billing **Cons:** - GCP lock-in - Fewer features than S3 - Smaller ecosystem ## Decision We will use Amazon S3 for object storage with intelligent tiering for cost optimization.",
              "score": null
            },
            {
              "doc_id": "adr_008",
              "chunk_id": "adr_008_fix_0",
              "content": "# ADR-008: Use S3 for Object Storage **Status:** Accepted **Date:** 2024-10-14 ## Context We need storage for workflow artifacts, execution outputs, and user uploads. ## Options Considered ### Option 1: Amazon S3 Object storage service. **Pros:** - 99.999999999% durability - Lifecycle policies - Event notifications - CDN integration **Cons:** - AWS lock-in - Eventual consistency for overwrites - Cost at scale ### Option 2: Self-hosted MinIO S3-compatible object storage.",
              "score": null
            },
            {
              "doc_id": "adr_004",
              "chunk_id": "adr_004_fix_1",
              "content": "**Pros:** - Simple operation - Multi-threaded - Lower memory overhead **Cons:** - No persistence - Limited data types - No clustering ### Option 3: Application-level cache In-process caching only. **Pros:** - No external dependency - Fastest possible - Simple **Cons:** - Not shared across pods - Memory pressure - Cold starts ## Decision We will use Redis Cluster for caching with a 6-node cluster for high availability.",
              "score": null
            },
            {
              "doc_id": "arch_data_pipeline",
              "chunk_id": "arch_data_pipeline_fix_1",
              "content": "**Technology:** Apache Kafka with Flink **Scaling:** Partition-based scaling ### Batch Processor Processes large batch jobs. Supports data formats: JSON, CSV, Parquet, Avro. **Technology:** Apache Spark on Kubernetes **Scaling:** Dynamic resource allocation ### Data Lake Long-term data storage with time-travel queries. Retention configurable up to 7 years.",
              "score": null
            },
            {
              "doc_id": "adr_001",
              "chunk_id": "adr_001_fix_2",
              "content": "**Pros:** - Automatic sharding - Strong consistency - PostgreSQL compatible **Cons:** - Higher latency - More expensive - Less mature ecosystem ## Decision We will use PostgreSQL as our primary database with JSONB for flexible workflow definitions. ## Consequences - Need PgBouncer for connection pooling - Must plan for sharding at 100M+ records - Requires regular VACUUM maintenance",
              "score": null
            }
          ],
          "latency_ms": 3.40534499991918,
          "query_id": "cross_014",
          "dimension": "negation",
          "query": "why do we need multiple storage solutions instead of consolidating into one"
        },
        {
          "key_facts": [
            "Bearer",
            "API key",
            "OAuth 2.0",
            "Authorization"
          ],
          "found_facts": [
            "Bearer",
            "API key",
            "OAuth 2.0",
            "Authorization"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "api_users",
            "api_integrations",
            "adr_003"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "arch_authentication_service",
              "chunk_id": "arch_authentication_service_fix_0",
              "content": "# Authentication Service Architecture ## Overview The Authentication Service handles user identity, session management, and token issuance. Supports OAuth 2.0, SAML, and API key authentication. ## Design Principles - **Zero Knowledge**: Passwords never stored in plaintext - **Token Rotation**: Refresh tokens rotate on each use ## Components ### Identity Provider Manages user accounts and credentials. Password hashing uses bcrypt with cost factor 12.",
              "score": null
            },
            {
              "doc_id": "adr_003",
              "chunk_id": "adr_003_fix_0",
              "content": "# ADR-003: Implement OAuth 2.0 with JWT for Authentication **Status:** Accepted **Date:** 2024-05-08 ## Context We need a secure authentication system that supports API keys, user sessions, and third-party integrations. ## Options Considered ### Option 1: OAuth 2.0 with JWT Industry standard token-based auth.",
              "score": null
            },
            {
              "doc_id": "adr_003",
              "chunk_id": "adr_003_fix_1",
              "content": "**Pros:** - Industry standard - Stateless validation - Supports refresh tokens - SSO compatible **Cons:** - Token size overhead - Cannot revoke individual tokens - Complexity ### Option 2: Session-based auth Traditional server-side sessions. **Pros:** - Simple implementation - Easy revocation - Smaller payloads **Cons:** - Requires session store - Not suitable for APIs - Scaling challenges ### Option 3: API keys only Simple key-based authentication.",
              "score": null
            },
            {
              "doc_id": "adr_003",
              "chunk_id": "adr_003_fix_2",
              "content": "**Pros:** - Very simple - Low overhead - Easy to implement **Cons:** - No user context - Hard to rotate - No SSO support ## Decision We will implement OAuth 2.0 with JWT tokens. Access tokens expire after 1 hour, refresh tokens after 7 days. ## Consequences - Implement token rotation on refresh - Support API keys for machine-to-machine auth - Add MFA for admin accounts",
              "score": null
            },
            {
              "doc_id": "api_workflows",
              "chunk_id": "api_workflows_fix_0",
              "content": "# Workflows API Reference ## Overview The Workflows API provides programmatic access to workflows functionality. Base URL: `https://api.cloudflow.io/v1/workflows` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /workflows List all workflows for the authenticated user.",
              "score": null
            }
          ],
          "latency_ms": 4.041180000058375,
          "query_id": "cross_015",
          "dimension": "original",
          "query": "What are all the API authentication methods?"
        },
        {
          "key_facts": [
            "Bearer",
            "API key",
            "OAuth 2.0",
            "Authorization"
          ],
          "found_facts": [
            "Bearer",
            "API key",
            "Authorization"
          ],
          "missed_facts": [
            "OAuth 2.0"
          ],
          "coverage": 0.75,
          "expected_docs": [
            "api_users",
            "api_integrations",
            "adr_003"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "api_integrations",
              "chunk_id": "api_integrations_fix_0",
              "content": "# Integrations API Reference ## Overview The Integrations API provides programmatic access to integrations functionality. Base URL: `https://api.cloudflow.io/v1/integrations` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /integrations List available integrations.",
              "score": null
            },
            {
              "doc_id": "api_users",
              "chunk_id": "api_users_fix_0",
              "content": "# Users API Reference ## Overview The Users API provides programmatic access to users functionality. Base URL: `https://api.cloudflow.io/v1/users` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /users/me Get current user profile.",
              "score": null
            },
            {
              "doc_id": "api_webhooks",
              "chunk_id": "api_webhooks_fix_0",
              "content": "# Webhooks API Reference ## Overview The Webhooks API provides programmatic access to webhooks functionality. Base URL: `https://api.cloudflow.io/v1/webhooks` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /webhooks List configured webhooks.",
              "score": null
            },
            {
              "doc_id": "api_workflows",
              "chunk_id": "api_workflows_fix_0",
              "content": "# Workflows API Reference ## Overview The Workflows API provides programmatic access to workflows functionality. Base URL: `https://api.cloudflow.io/v1/workflows` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /workflows List all workflows for the authenticated user.",
              "score": null
            },
            {
              "doc_id": "api_secrets",
              "chunk_id": "api_secrets_fix_0",
              "content": "# Secrets API Reference ## Overview The Secrets API provides programmatic access to secrets functionality. Base URL: `https://api.cloudflow.io/v1/secrets` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /secrets List secret names (values not returned).",
              "score": null
            }
          ],
          "latency_ms": 3.6495710010058247,
          "query_id": "cross_015",
          "dimension": "synonym",
          "query": "what credentials and tokens can I use to call cloudflow APIs"
        },
        {
          "key_facts": [
            "Bearer",
            "API key",
            "OAuth 2.0",
            "Authorization"
          ],
          "found_facts": [
            "API key"
          ],
          "missed_facts": [
            "Bearer",
            "OAuth 2.0",
            "Authorization"
          ],
          "coverage": 0.25,
          "expected_docs": [
            "api_users",
            "api_integrations",
            "adr_003"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "api_secrets",
              "chunk_id": "api_secrets_fix_2",
              "content": "**Parameters:** **Response:** - Success: `204` - Secret deleted - Error: `400` - Bad Request **Example:** ```json {} ```",
              "score": null
            },
            {
              "doc_id": "api_secrets",
              "chunk_id": "api_secrets_fix_1",
              "content": "**Parameters:** **Response:** - Success: `200` - Returns array of secret metadata - Error: `400` - Bad Request **Example:** ```json {\"secrets\": [{\"name\": \"DATABASE_URL\", \"created_at\": \"2024-01-01\", \"updated_at\": \"2024-01-10\"}]} ``` ### PUT /secrets/{name} Create or update a secret. **Parameters:** - `value` (string): Secret value (encrypted at rest) **Response:** - Success: `200` - Secret saved - Error: `400` - Bad Request **Example:** ```json {\"name\": \"DATABASE_URL\", \"updated_at\": \"2024-01-15T10:00:00Z\"} ``` ### DELETE /secrets/{name} Delete a secret.",
              "score": null
            },
            {
              "doc_id": "api_webhooks",
              "chunk_id": "api_webhooks_fix_2",
              "content": "**Parameters:** - `url` (string): Webhook URL (must be HTTPS) - `events` (array): Events to subscribe to - `secret` (string): Signing secret for verification **Response:** - Success: `201` - Webhook created - Error: `400` - Bad Request **Example:** ```json {\"id\": \"wh_02\", \"url\": \"https://example.com/new-hook\", \"secret\": \"whsec_...\"} ```",
              "score": null
            },
            {
              "doc_id": "howto_create_api_integration",
              "chunk_id": "howto_create_api_integration_fix_0",
              "content": "# How to Create API Integration ## Prerequisites - CloudFlow account ## Steps ### Step 1: Generate API key Create an API key for programmatic access. ```bash cloudflow apikeys create --name 'CI Integration' --scopes workflows:read,workflows:execute ``` ### Step 2: Store key securely Never commit API keys to source control. Use environment variables or secret management. > **Note:** Rotate keys every 90 days ### Step 3: Test authentication Verify the API key works correctly.",
              "score": null
            },
            {
              "doc_id": "api_teams",
              "chunk_id": "api_teams_fix_1",
              "content": "**Parameters:** **Response:** - Success: `200` - Returns array of team objects - Error: `400` - Bad Request **Example:** ```json {\"teams\": [{\"id\": \"team_01\", \"name\": \"Engineering\", \"member_count\": 15}]} ``` ### POST /teams/{id}/members Add a member to a team. **Parameters:** - `user_id` (string): User ID to add - `role` (string): Role: owner, admin, member, viewer **Response:** - Success: `201` - Member added - Error: `400` - Bad Request **Example:** ```json {\"team_id\": \"team_01\", \"user_id\": \"usr_002\", \"role\": \"member\"} ```",
              "score": null
            }
          ],
          "latency_ms": 3.499640999507392,
          "query_id": "cross_015",
          "dimension": "problem",
          "query": "getting 403 forbidden when calling the API with what I thought was a valid API key"
        },
        {
          "key_facts": [
            "Bearer",
            "API key",
            "OAuth 2.0",
            "Authorization"
          ],
          "found_facts": [
            "Bearer",
            "API key",
            "OAuth 2.0",
            "Authorization"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "api_users",
            "api_integrations",
            "adr_003"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "adr_003",
              "chunk_id": "adr_003_fix_0",
              "content": "# ADR-003: Implement OAuth 2.0 with JWT for Authentication **Status:** Accepted **Date:** 2024-05-08 ## Context We need a secure authentication system that supports API keys, user sessions, and third-party integrations. ## Options Considered ### Option 1: OAuth 2.0 with JWT Industry standard token-based auth.",
              "score": null
            },
            {
              "doc_id": "arch_authentication_service",
              "chunk_id": "arch_authentication_service_fix_0",
              "content": "# Authentication Service Architecture ## Overview The Authentication Service handles user identity, session management, and token issuance. Supports OAuth 2.0, SAML, and API key authentication. ## Design Principles - **Zero Knowledge**: Passwords never stored in plaintext - **Token Rotation**: Refresh tokens rotate on each use ## Components ### Identity Provider Manages user accounts and credentials. Password hashing uses bcrypt with cost factor 12.",
              "score": null
            },
            {
              "doc_id": "api_workflows",
              "chunk_id": "api_workflows_fix_0",
              "content": "# Workflows API Reference ## Overview The Workflows API provides programmatic access to workflows functionality. Base URL: `https://api.cloudflow.io/v1/workflows` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /workflows List all workflows for the authenticated user.",
              "score": null
            },
            {
              "doc_id": "adr_003",
              "chunk_id": "adr_003_fix_1",
              "content": "**Pros:** - Industry standard - Stateless validation - Supports refresh tokens - SSO compatible **Cons:** - Token size overhead - Cannot revoke individual tokens - Complexity ### Option 2: Session-based auth Traditional server-side sessions. **Pros:** - Simple implementation - Easy revocation - Smaller payloads **Cons:** - Requires session store - Not suitable for APIs - Scaling challenges ### Option 3: API keys only Simple key-based authentication.",
              "score": null
            },
            {
              "doc_id": "adr_003",
              "chunk_id": "adr_003_fix_2",
              "content": "**Pros:** - Very simple - Low overhead - Easy to implement **Cons:** - No user context - Hard to rotate - No SSO support ## Decision We will implement OAuth 2.0 with JWT tokens. Access tokens expire after 1 hour, refresh tokens after 7 days. ## Consequences - Implement token rotation on refresh - Support API keys for machine-to-machine auth - Add MFA for admin accounts",
              "score": null
            }
          ],
          "latency_ms": 3.7237889991956763,
          "query_id": "cross_015",
          "dimension": "casual",
          "query": "api auth options"
        },
        {
          "key_facts": [
            "Bearer",
            "API key",
            "OAuth 2.0",
            "Authorization"
          ],
          "found_facts": [
            "API key",
            "OAuth 2.0"
          ],
          "missed_facts": [
            "Bearer",
            "Authorization"
          ],
          "coverage": 0.5,
          "expected_docs": [
            "api_users",
            "api_integrations",
            "adr_003"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "arch_authentication_service",
              "chunk_id": "arch_authentication_service_fix_0",
              "content": "# Authentication Service Architecture ## Overview The Authentication Service handles user identity, session management, and token issuance. Supports OAuth 2.0, SAML, and API key authentication. ## Design Principles - **Zero Knowledge**: Passwords never stored in plaintext - **Token Rotation**: Refresh tokens rotate on each use ## Components ### Identity Provider Manages user accounts and credentials. Password hashing uses bcrypt with cost factor 12.",
              "score": null
            },
            {
              "doc_id": "adr_003",
              "chunk_id": "adr_003_fix_2",
              "content": "**Pros:** - Very simple - Low overhead - Easy to implement **Cons:** - No user context - Hard to rotate - No SSO support ## Decision We will implement OAuth 2.0 with JWT tokens. Access tokens expire after 1 hour, refresh tokens after 7 days. ## Consequences - Implement token rotation on refresh - Support API keys for machine-to-machine auth - Add MFA for admin accounts",
              "score": null
            },
            {
              "doc_id": "adr_003",
              "chunk_id": "adr_003_fix_0",
              "content": "# ADR-003: Implement OAuth 2.0 with JWT for Authentication **Status:** Accepted **Date:** 2024-05-08 ## Context We need a secure authentication system that supports API keys, user sessions, and third-party integrations. ## Options Considered ### Option 1: OAuth 2.0 with JWT Industry standard token-based auth.",
              "score": null
            },
            {
              "doc_id": "adr_003",
              "chunk_id": "adr_003_fix_1",
              "content": "**Pros:** - Industry standard - Stateless validation - Supports refresh tokens - SSO compatible **Cons:** - Token size overhead - Cannot revoke individual tokens - Complexity ### Option 2: Session-based auth Traditional server-side sessions. **Pros:** - Simple implementation - Easy revocation - Smaller payloads **Cons:** - Requires session store - Not suitable for APIs - Scaling challenges ### Option 3: API keys only Simple key-based authentication.",
              "score": null
            },
            {
              "doc_id": "howto_create_api_integration",
              "chunk_id": "howto_create_api_integration_fix_0",
              "content": "# How to Create API Integration ## Prerequisites - CloudFlow account ## Steps ### Step 1: Generate API key Create an API key for programmatic access. ```bash cloudflow apikeys create --name 'CI Integration' --scopes workflows:read,workflows:execute ``` ### Step 2: Store key securely Never commit API keys to source control. Use environment variables or secret management. > **Note:** Rotate keys every 90 days ### Step 3: Test authentication Verify the API key works correctly.",
              "score": null
            }
          ],
          "latency_ms": 3.945632000977639,
          "query_id": "cross_015",
          "dimension": "contextual",
          "query": "writing a CLI tool that needs to authenticate users and also support service accounts, what auth methods should I implement"
        },
        {
          "key_facts": [
            "Bearer",
            "API key",
            "OAuth 2.0",
            "Authorization"
          ],
          "found_facts": [
            "Bearer",
            "API key",
            "OAuth 2.0",
            "Authorization"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "api_users",
            "api_integrations",
            "adr_003"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "howto_create_api_integration",
              "chunk_id": "howto_create_api_integration_fix_0",
              "content": "# How to Create API Integration ## Prerequisites - CloudFlow account ## Steps ### Step 1: Generate API key Create an API key for programmatic access. ```bash cloudflow apikeys create --name 'CI Integration' --scopes workflows:read,workflows:execute ``` ### Step 2: Store key securely Never commit API keys to source control. Use environment variables or secret management. > **Note:** Rotate keys every 90 days ### Step 3: Test authentication Verify the API key works correctly.",
              "score": null
            },
            {
              "doc_id": "api_workflows",
              "chunk_id": "api_workflows_fix_0",
              "content": "# Workflows API Reference ## Overview The Workflows API provides programmatic access to workflows functionality. Base URL: `https://api.cloudflow.io/v1/workflows` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /workflows List all workflows for the authenticated user.",
              "score": null
            },
            {
              "doc_id": "api_secrets",
              "chunk_id": "api_secrets_fix_0",
              "content": "# Secrets API Reference ## Overview The Secrets API provides programmatic access to secrets functionality. Base URL: `https://api.cloudflow.io/v1/secrets` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /secrets List secret names (values not returned).",
              "score": null
            },
            {
              "doc_id": "howto_migrate_from_v1_to_v2_api",
              "chunk_id": "howto_migrate_from_v1_to_v2_api_fix_0",
              "content": "# How to Migrate from v1 to v2 API ## Prerequisites - Existing v1 integration - v2 API access enabled ## Steps ### Step 1: Review breaking changes The v2 API changes authentication from API keys to OAuth 2.0. Review the migration guide. ### Step 2: Update client library Upgrade to the latest SDK version. ```bash npm install @cloudflow/sdk@latest ``` ### Step 3: Update authentication Replace API key authentication with OAuth 2.0 client credentials flow.",
              "score": null
            },
            {
              "doc_id": "adr_003",
              "chunk_id": "adr_003_fix_1",
              "content": "**Pros:** - Industry standard - Stateless validation - Supports refresh tokens - SSO compatible **Cons:** - Token size overhead - Cannot revoke individual tokens - Complexity ### Option 2: Session-based auth Traditional server-side sessions. **Pros:** - Simple implementation - Easy revocation - Smaller payloads **Cons:** - Requires session store - Not suitable for APIs - Scaling challenges ### Option 3: API keys only Simple key-based authentication.",
              "score": null
            }
          ],
          "latency_ms": 3.4982889992534183,
          "query_id": "cross_015",
          "dimension": "negation",
          "query": "why can't I just use a simple API key for everything instead of dealing with OAuth flows"
        },
        {
          "key_facts": [
            "GitHub Actions",
            "cloudflow/deploy-action",
            "CLOUDFLOW_API_KEY"
          ],
          "found_facts": [
            "GitHub Actions"
          ],
          "missed_facts": [
            "cloudflow/deploy-action",
            "CLOUDFLOW_API_KEY"
          ],
          "coverage": 0.3333333333333333,
          "expected_docs": [
            "howto_set_up_ci/cd_pipeline"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "howto_set_up_ci_cd_pipeline",
              "chunk_id": "howto_set_up_ci_cd_pipeline_fix_0",
              "content": "# How to Set Up CI/CD Pipeline ## Prerequisites - GitHub account - CloudFlow API key - Repository admin access ## Steps ### Step 1: Create GitHub Actions workflow Create a new file at `.github/workflows/cloudflow.yml` with the deployment configuration. ```bash mkdir -p .github/workflows && touch .github/workflows/cloudflow.yml ``` ### Step 2: Configure secrets Add your CloudFlow API key to GitHub secrets.",
              "score": null
            },
            {
              "doc_id": "howto_configure_custom_domain",
              "chunk_id": "howto_configure_custom_domain_fix_0",
              "content": "# How to Configure Custom Domain ## Prerequisites - Domain with DNS access - CloudFlow Pro plan or higher ## Steps ### Step 1: Verify domain ownership Add a TXT record to verify domain ownership. ```bash cloudflow domains verify --domain app.example.com ``` ### Step 2: Configure DNS Add a CNAME record pointing to your CloudFlow endpoint.",
              "score": null
            },
            {
              "doc_id": "api_integrations",
              "chunk_id": "api_integrations_fix_0",
              "content": "# Integrations API Reference ## Overview The Integrations API provides programmatic access to integrations functionality. Base URL: `https://api.cloudflow.io/v1/integrations` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /integrations List available integrations.",
              "score": null
            },
            {
              "doc_id": "howto_create_api_integration",
              "chunk_id": "howto_create_api_integration_fix_0",
              "content": "# How to Create API Integration ## Prerequisites - CloudFlow account ## Steps ### Step 1: Generate API key Create an API key for programmatic access. ```bash cloudflow apikeys create --name 'CI Integration' --scopes workflows:read,workflows:execute ``` ### Step 2: Store key securely Never commit API keys to source control. Use environment variables or secret management. > **Note:** Rotate keys every 90 days ### Step 3: Test authentication Verify the API key works correctly.",
              "score": null
            },
            {
              "doc_id": "howto_set_up_database_connection",
              "chunk_id": "howto_set_up_database_connection_fix_0",
              "content": "# How to Set Up Database Connection ## Prerequisites - Database with network access to CloudFlow - Connection credentials ## Steps ### Step 1: Add connection secret Store your database connection string as a secret. ```bash cloudflow secrets set DATABASE_URL postgresql://user:pass@host:5432/db ``` ### Step 2: Configure connection pool Set pool size based on your plan limits. Free tier allows 5 connections, Pro allows 20.",
              "score": null
            }
          ],
          "latency_ms": 3.5787489996437216,
          "query_id": "howto_016",
          "dimension": "original",
          "query": "How do I set up CI/CD for CloudFlow?"
        },
        {
          "key_facts": [
            "GitHub Actions",
            "cloudflow/deploy-action",
            "CLOUDFLOW_API_KEY"
          ],
          "found_facts": [
            "GitHub Actions",
            "cloudflow/deploy-action",
            "CLOUDFLOW_API_KEY"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "howto_set_up_ci/cd_pipeline"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "howto_set_up_ci_cd_pipeline",
              "chunk_id": "howto_set_up_ci_cd_pipeline_fix_0",
              "content": "# How to Set Up CI/CD Pipeline ## Prerequisites - GitHub account - CloudFlow API key - Repository admin access ## Steps ### Step 1: Create GitHub Actions workflow Create a new file at `.github/workflows/cloudflow.yml` with the deployment configuration. ```bash mkdir -p .github/workflows && touch .github/workflows/cloudflow.yml ``` ### Step 2: Configure secrets Add your CloudFlow API key to GitHub secrets.",
              "score": null
            },
            {
              "doc_id": "howto_set_up_ci_cd_pipeline",
              "chunk_id": "howto_set_up_ci_cd_pipeline_fix_1",
              "content": "```bash gh secret set CLOUDFLOW_API_KEY ``` ### Step 3: Add deployment step Add the CloudFlow deployment action to your workflow. > **Note:** Use the official cloudflow/deploy-action@v2 ### Step 4: Test the pipeline Push a commit to trigger the workflow. ```bash git push origin main ``` ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            },
            {
              "doc_id": "howto_migrate_from_v1_to_v2_api",
              "chunk_id": "howto_migrate_from_v1_to_v2_api_fix_1",
              "content": "> **Note:** Generate new credentials in the dashboard ### Step 4: Test in staging Run your integration tests against the v2 staging endpoint. ```bash CLOUDFLOW_API_URL=https://api-v2-staging.cloudflow.io npm test ``` ### Step 5: Switch production Update production configuration to use v2 API. ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            },
            {
              "doc_id": "howto_implement_retry_logic",
              "chunk_id": "howto_implement_retry_logic_fix_1",
              "content": "### Step 4: Handle final failure Configure on_failure handler to notify or escalate when all retries are exhausted. ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            },
            {
              "doc_id": "runbook_high_cpu_usage",
              "chunk_id": "runbook_high_cpu_usage_fix_1",
              "content": "Step 4 Profile the application: `cloudflow debug profile --pod api-xyz --duration 60s` ## Resolution ### Step 1 Scale up if traffic-related: `kubectl scale deployment/api --replicas=10 -n production` ### Step 2 Restart pods if memory leak suspected: `kubectl rollout restart deployment/api -n production` ### Step 3 Roll back recent deployment if regression: `kubectl rollout undo deployment/api -n production` ### Step 4 Add resource limits if unbounded: update deployment with CPU limits ## Escalation If CPU remains",
              "score": null
            }
          ],
          "latency_ms": 3.937856999982614,
          "query_id": "howto_016",
          "dimension": "synonym",
          "query": "How do I configure continuous deployment pipelines for CloudFlow?"
        },
        {
          "key_facts": [
            "GitHub Actions",
            "cloudflow/deploy-action",
            "CLOUDFLOW_API_KEY"
          ],
          "found_facts": [
            "GitHub Actions",
            "cloudflow/deploy-action",
            "CLOUDFLOW_API_KEY"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "howto_set_up_ci/cd_pipeline"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "howto_set_up_ci_cd_pipeline",
              "chunk_id": "howto_set_up_ci_cd_pipeline_fix_0",
              "content": "# How to Set Up CI/CD Pipeline ## Prerequisites - GitHub account - CloudFlow API key - Repository admin access ## Steps ### Step 1: Create GitHub Actions workflow Create a new file at `.github/workflows/cloudflow.yml` with the deployment configuration. ```bash mkdir -p .github/workflows && touch .github/workflows/cloudflow.yml ``` ### Step 2: Configure secrets Add your CloudFlow API key to GitHub secrets.",
              "score": null
            },
            {
              "doc_id": "howto_set_up_ci_cd_pipeline",
              "chunk_id": "howto_set_up_ci_cd_pipeline_fix_1",
              "content": "```bash gh secret set CLOUDFLOW_API_KEY ``` ### Step 3: Add deployment step Add the CloudFlow deployment action to your workflow. > **Note:** Use the official cloudflow/deploy-action@v2 ### Step 4: Test the pipeline Push a commit to trigger the workflow. ```bash git push origin main ``` ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            },
            {
              "doc_id": "howto_migrate_from_v1_to_v2_api",
              "chunk_id": "howto_migrate_from_v1_to_v2_api_fix_1",
              "content": "> **Note:** Generate new credentials in the dashboard ### Step 4: Test in staging Run your integration tests against the v2 staging endpoint. ```bash CLOUDFLOW_API_URL=https://api-v2-staging.cloudflow.io npm test ``` ### Step 5: Switch production Update production configuration to use v2 API. ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            },
            {
              "doc_id": "howto_create_api_integration",
              "chunk_id": "howto_create_api_integration_fix_0",
              "content": "# How to Create API Integration ## Prerequisites - CloudFlow account ## Steps ### Step 1: Generate API key Create an API key for programmatic access. ```bash cloudflow apikeys create --name 'CI Integration' --scopes workflows:read,workflows:execute ``` ### Step 2: Store key securely Never commit API keys to source control. Use environment variables or secret management. > **Note:** Rotate keys every 90 days ### Step 3: Test authentication Verify the API key works correctly.",
              "score": null
            },
            {
              "doc_id": "howto_implement_workflow_versioning",
              "chunk_id": "howto_implement_workflow_versioning_fix_0",
              "content": "# How to Implement Workflow Versioning ## Prerequisites - Existing workflow ## Steps ### Step 1: Enable versioning Turn on versioning for your workflow. This is required for the feature. ```bash cloudflow workflows update wf_123 --enable-versioning ``` ### Step 2: Create new version Save changes as a new version instead of overwriting. ```bash cloudflow workflows publish wf_123 --version 2.0.0 ``` ### Step 3: Set active version Choose which version handles new executions.",
              "score": null
            }
          ],
          "latency_ms": 3.8014929996279534,
          "query_id": "howto_016",
          "dimension": "problem",
          "query": "I want to automatically deploy my CloudFlow workflows when I push to GitHub but don't know where to start"
        },
        {
          "key_facts": [
            "GitHub Actions",
            "cloudflow/deploy-action",
            "CLOUDFLOW_API_KEY"
          ],
          "found_facts": [
            "GitHub Actions"
          ],
          "missed_facts": [
            "cloudflow/deploy-action",
            "CLOUDFLOW_API_KEY"
          ],
          "coverage": 0.3333333333333333,
          "expected_docs": [
            "howto_set_up_ci/cd_pipeline"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "howto_set_up_ci_cd_pipeline",
              "chunk_id": "howto_set_up_ci_cd_pipeline_fix_0",
              "content": "# How to Set Up CI/CD Pipeline ## Prerequisites - GitHub account - CloudFlow API key - Repository admin access ## Steps ### Step 1: Create GitHub Actions workflow Create a new file at `.github/workflows/cloudflow.yml` with the deployment configuration. ```bash mkdir -p .github/workflows && touch .github/workflows/cloudflow.yml ``` ### Step 2: Configure secrets Add your CloudFlow API key to GitHub secrets.",
              "score": null
            },
            {
              "doc_id": "howto_migrate_from_v1_to_v2_api",
              "chunk_id": "howto_migrate_from_v1_to_v2_api_fix_1",
              "content": "> **Note:** Generate new credentials in the dashboard ### Step 4: Test in staging Run your integration tests against the v2 staging endpoint. ```bash CLOUDFLOW_API_URL=https://api-v2-staging.cloudflow.io npm test ``` ### Step 5: Switch production Update production configuration to use v2 API. ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            },
            {
              "doc_id": "howto_set_up_database_connection",
              "chunk_id": "howto_set_up_database_connection_fix_0",
              "content": "# How to Set Up Database Connection ## Prerequisites - Database with network access to CloudFlow - Connection credentials ## Steps ### Step 1: Add connection secret Store your database connection string as a secret. ```bash cloudflow secrets set DATABASE_URL postgresql://user:pass@host:5432/db ``` ### Step 2: Configure connection pool Set pool size based on your plan limits. Free tier allows 5 connections, Pro allows 20.",
              "score": null
            },
            {
              "doc_id": "howto_create_api_integration",
              "chunk_id": "howto_create_api_integration_fix_0",
              "content": "# How to Create API Integration ## Prerequisites - CloudFlow account ## Steps ### Step 1: Generate API key Create an API key for programmatic access. ```bash cloudflow apikeys create --name 'CI Integration' --scopes workflows:read,workflows:execute ``` ### Step 2: Store key securely Never commit API keys to source control. Use environment variables or secret management. > **Note:** Rotate keys every 90 days ### Step 3: Test authentication Verify the API key works correctly.",
              "score": null
            },
            {
              "doc_id": "howto_configure_custom_domain",
              "chunk_id": "howto_configure_custom_domain_fix_0",
              "content": "# How to Configure Custom Domain ## Prerequisites - Domain with DNS access - CloudFlow Pro plan or higher ## Steps ### Step 1: Verify domain ownership Add a TXT record to verify domain ownership. ```bash cloudflow domains verify --domain app.example.com ``` ### Step 2: Configure DNS Add a CNAME record pointing to your CloudFlow endpoint.",
              "score": null
            }
          ],
          "latency_ms": 4.993955000827555,
          "query_id": "howto_016",
          "dimension": "casual",
          "query": "cloudflow ci/cd setup"
        },
        {
          "key_facts": [
            "GitHub Actions",
            "cloudflow/deploy-action",
            "CLOUDFLOW_API_KEY"
          ],
          "found_facts": [
            "GitHub Actions",
            "cloudflow/deploy-action",
            "CLOUDFLOW_API_KEY"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "howto_set_up_ci/cd_pipeline"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "howto_set_up_ci_cd_pipeline",
              "chunk_id": "howto_set_up_ci_cd_pipeline_fix_0",
              "content": "# How to Set Up CI/CD Pipeline ## Prerequisites - GitHub account - CloudFlow API key - Repository admin access ## Steps ### Step 1: Create GitHub Actions workflow Create a new file at `.github/workflows/cloudflow.yml` with the deployment configuration. ```bash mkdir -p .github/workflows && touch .github/workflows/cloudflow.yml ``` ### Step 2: Configure secrets Add your CloudFlow API key to GitHub secrets.",
              "score": null
            },
            {
              "doc_id": "howto_set_up_ci_cd_pipeline",
              "chunk_id": "howto_set_up_ci_cd_pipeline_fix_1",
              "content": "```bash gh secret set CLOUDFLOW_API_KEY ``` ### Step 3: Add deployment step Add the CloudFlow deployment action to your workflow. > **Note:** Use the official cloudflow/deploy-action@v2 ### Step 4: Test the pipeline Push a commit to trigger the workflow. ```bash git push origin main ``` ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            },
            {
              "doc_id": "howto_migrate_from_v1_to_v2_api",
              "chunk_id": "howto_migrate_from_v1_to_v2_api_fix_1",
              "content": "> **Note:** Generate new credentials in the dashboard ### Step 4: Test in staging Run your integration tests against the v2 staging endpoint. ```bash CLOUDFLOW_API_URL=https://api-v2-staging.cloudflow.io npm test ``` ### Step 5: Switch production Update production configuration to use v2 API. ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            },
            {
              "doc_id": "howto_create_api_integration",
              "chunk_id": "howto_create_api_integration_fix_0",
              "content": "# How to Create API Integration ## Prerequisites - CloudFlow account ## Steps ### Step 1: Generate API key Create an API key for programmatic access. ```bash cloudflow apikeys create --name 'CI Integration' --scopes workflows:read,workflows:execute ``` ### Step 2: Store key securely Never commit API keys to source control. Use environment variables or secret management. > **Note:** Rotate keys every 90 days ### Step 3: Test authentication Verify the API key works correctly.",
              "score": null
            },
            {
              "doc_id": "api_integrations",
              "chunk_id": "api_integrations_fix_0",
              "content": "# Integrations API Reference ## Overview The Integrations API provides programmatic access to integrations functionality. Base URL: `https://api.cloudflow.io/v1/integrations` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /integrations List available integrations.",
              "score": null
            }
          ],
          "latency_ms": 3.867907000312698,
          "query_id": "howto_016",
          "dimension": "contextual",
          "query": "setting up automated deployments for my team's CloudFlow project, need to integrate with our GitHub repo"
        },
        {
          "key_facts": [
            "GitHub Actions",
            "cloudflow/deploy-action",
            "CLOUDFLOW_API_KEY"
          ],
          "found_facts": [
            "GitHub Actions",
            "cloudflow/deploy-action",
            "CLOUDFLOW_API_KEY"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "howto_set_up_ci/cd_pipeline"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "howto_set_up_ci_cd_pipeline",
              "chunk_id": "howto_set_up_ci_cd_pipeline_fix_1",
              "content": "```bash gh secret set CLOUDFLOW_API_KEY ``` ### Step 3: Add deployment step Add the CloudFlow deployment action to your workflow. > **Note:** Use the official cloudflow/deploy-action@v2 ### Step 4: Test the pipeline Push a commit to trigger the workflow. ```bash git push origin main ``` ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            },
            {
              "doc_id": "howto_implement_retry_logic",
              "chunk_id": "howto_implement_retry_logic_fix_1",
              "content": "### Step 4: Handle final failure Configure on_failure handler to notify or escalate when all retries are exhausted. ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            },
            {
              "doc_id": "howto_migrate_from_v1_to_v2_api",
              "chunk_id": "howto_migrate_from_v1_to_v2_api_fix_1",
              "content": "> **Note:** Generate new credentials in the dashboard ### Step 4: Test in staging Run your integration tests against the v2 staging endpoint. ```bash CLOUDFLOW_API_URL=https://api-v2-staging.cloudflow.io npm test ``` ### Step 5: Switch production Update production configuration to use v2 API. ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            },
            {
              "doc_id": "howto_set_up_ci_cd_pipeline",
              "chunk_id": "howto_set_up_ci_cd_pipeline_fix_0",
              "content": "# How to Set Up CI/CD Pipeline ## Prerequisites - GitHub account - CloudFlow API key - Repository admin access ## Steps ### Step 1: Create GitHub Actions workflow Create a new file at `.github/workflows/cloudflow.yml` with the deployment configuration. ```bash mkdir -p .github/workflows && touch .github/workflows/cloudflow.yml ``` ### Step 2: Configure secrets Add your CloudFlow API key to GitHub secrets.",
              "score": null
            },
            {
              "doc_id": "release_3_0_0",
              "chunk_id": "release_3_0_0_fix_1",
              "content": "```bash cloudflow upgrade --version 3.0.0 ``` ## Known Issues None at this time.",
              "score": null
            }
          ],
          "latency_ms": 3.6511329999484587,
          "query_id": "howto_016",
          "dimension": "negation",
          "query": "why aren't my CloudFlow deployments happening automatically when I push code?"
        },
        {
          "key_facts": [
            "CNAME",
            "ingress.cloudflow.io",
            "Let's Encrypt"
          ],
          "found_facts": [
            "CNAME",
            "ingress.cloudflow.io",
            "Let's Encrypt"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "howto_configure_custom_domain"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "howto_configure_custom_domain",
              "chunk_id": "howto_configure_custom_domain_fix_0",
              "content": "# How to Configure Custom Domain ## Prerequisites - Domain with DNS access - CloudFlow Pro plan or higher ## Steps ### Step 1: Verify domain ownership Add a TXT record to verify domain ownership. ```bash cloudflow domains verify --domain app.example.com ``` ### Step 2: Configure DNS Add a CNAME record pointing to your CloudFlow endpoint.",
              "score": null
            },
            {
              "doc_id": "howto_configure_custom_domain",
              "chunk_id": "howto_configure_custom_domain_fix_1",
              "content": "> **Note:** CNAME should point to ingress.cloudflow.io ### Step 3: Enable SSL CloudFlow automatically provisions SSL certificates via Let's Encrypt. ```bash cloudflow domains ssl --domain app.example.com ``` ### Step 4: Set as primary Make this the primary domain for your application. ```bash cloudflow domains set-primary --domain app.example.com ``` ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            },
            {
              "doc_id": "config_cache",
              "chunk_id": "config_cache_fix_0",
              "content": "# Cache Configuration Reference ## Overview This document describes all configuration options for Cache. ## Environment Variables | Variable | Type | Default | Description | |----------|------|---------|-------------| | `REDIS_URL` | string | required | Redis connection string | | `REDIS_POOL_SIZE` | integer | 10 | Connection pool size | | `CACHE_TTL_SECONDS` | integer | 300 | Default cache TTL | | `CACHE_PREFIX` | string | cf: | Key prefix for namespacing | | `CACHE_COMPRESSION` |",
              "score": null
            },
            {
              "doc_id": "config_api_gateway",
              "chunk_id": "config_api_gateway_fix_1",
              "content": "size | | `GATEWAY_CORS_ORIGINS` | string | * | Allowed CORS origins | | `GATEWAY_SSL_ENABLED` | boolean | true | Enable TLS termination | ## Configuration File Configuration can also be provided via `api gateway.yaml`: ```yaml gateway_port: 8000 gateway_rate_limit: 100 gateway_timeout_ms: 30000 gateway_max_body_size: 10MB gateway_cors_origins: * ``` ## Validation Configuration is validated at startup. Invalid configuration will prevent the service from starting.",
              "score": null
            },
            {
              "doc_id": "howto_create_scheduled_workflow",
              "chunk_id": "howto_create_scheduled_workflow_fix_0",
              "content": "# How to Create Scheduled Workflow ## Prerequisites - Workflow definition ## Steps ### Step 1: Define schedule Use cron expression to set the schedule. Example: '0 9 * * MON-FRI' runs at 9 AM on weekdays. ### Step 2: Set timezone Specify timezone for the schedule. Default is UTC.",
              "score": null
            }
          ],
          "latency_ms": 3.55705800029682,
          "query_id": "howto_017",
          "dimension": "original",
          "query": "How do I configure a custom domain?"
        },
        {
          "key_facts": [
            "CNAME",
            "ingress.cloudflow.io",
            "Let's Encrypt"
          ],
          "found_facts": [
            "CNAME",
            "ingress.cloudflow.io",
            "Let's Encrypt"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "howto_configure_custom_domain"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "howto_configure_custom_domain",
              "chunk_id": "howto_configure_custom_domain_fix_0",
              "content": "# How to Configure Custom Domain ## Prerequisites - Domain with DNS access - CloudFlow Pro plan or higher ## Steps ### Step 1: Verify domain ownership Add a TXT record to verify domain ownership. ```bash cloudflow domains verify --domain app.example.com ``` ### Step 2: Configure DNS Add a CNAME record pointing to your CloudFlow endpoint.",
              "score": null
            },
            {
              "doc_id": "howto_configure_custom_domain",
              "chunk_id": "howto_configure_custom_domain_fix_1",
              "content": "> **Note:** CNAME should point to ingress.cloudflow.io ### Step 3: Enable SSL CloudFlow automatically provisions SSL certificates via Let's Encrypt. ```bash cloudflow domains ssl --domain app.example.com ``` ### Step 4: Set as primary Make this the primary domain for your application. ```bash cloudflow domains set-primary --domain app.example.com ``` ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            },
            {
              "doc_id": "howto_set_up_ci_cd_pipeline",
              "chunk_id": "howto_set_up_ci_cd_pipeline_fix_0",
              "content": "# How to Set Up CI/CD Pipeline ## Prerequisites - GitHub account - CloudFlow API key - Repository admin access ## Steps ### Step 1: Create GitHub Actions workflow Create a new file at `.github/workflows/cloudflow.yml` with the deployment configuration. ```bash mkdir -p .github/workflows && touch .github/workflows/cloudflow.yml ``` ### Step 2: Configure secrets Add your CloudFlow API key to GitHub secrets.",
              "score": null
            },
            {
              "doc_id": "howto_configure_single_sign_on",
              "chunk_id": "howto_configure_single_sign_on_fix_0",
              "content": "# How to Configure Single Sign-On ## Prerequisites - Admin access - Enterprise plan - Identity provider access ## Steps ### Step 1: Choose provider CloudFlow supports SAML 2.0, OpenID Connect, and OAuth 2.0 providers. ### Step 2: Get metadata Download CloudFlow's SAML metadata for your IdP configuration. ```bash cloudflow sso metadata --format xml > cloudflow-metadata.xml ``` ### Step 3: Configure IdP Add CloudFlow as a service provider in your identity provider.",
              "score": null
            },
            {
              "doc_id": "howto_set_up_database_connection",
              "chunk_id": "howto_set_up_database_connection_fix_0",
              "content": "# How to Set Up Database Connection ## Prerequisites - Database with network access to CloudFlow - Connection credentials ## Steps ### Step 1: Add connection secret Store your database connection string as a secret. ```bash cloudflow secrets set DATABASE_URL postgresql://user:pass@host:5432/db ``` ### Step 2: Configure connection pool Set pool size based on your plan limits. Free tier allows 5 connections, Pro allows 20.",
              "score": null
            }
          ],
          "latency_ms": 4.381865001050755,
          "query_id": "howto_017",
          "dimension": "synonym",
          "query": "How do I set up my own domain name for CloudFlow?"
        },
        {
          "key_facts": [
            "CNAME",
            "ingress.cloudflow.io",
            "Let's Encrypt"
          ],
          "found_facts": [
            "CNAME",
            "ingress.cloudflow.io",
            "Let's Encrypt"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "howto_configure_custom_domain"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "howto_configure_custom_domain",
              "chunk_id": "howto_configure_custom_domain_fix_0",
              "content": "# How to Configure Custom Domain ## Prerequisites - Domain with DNS access - CloudFlow Pro plan or higher ## Steps ### Step 1: Verify domain ownership Add a TXT record to verify domain ownership. ```bash cloudflow domains verify --domain app.example.com ``` ### Step 2: Configure DNS Add a CNAME record pointing to your CloudFlow endpoint.",
              "score": null
            },
            {
              "doc_id": "howto_configure_custom_domain",
              "chunk_id": "howto_configure_custom_domain_fix_1",
              "content": "> **Note:** CNAME should point to ingress.cloudflow.io ### Step 3: Enable SSL CloudFlow automatically provisions SSL certificates via Let's Encrypt. ```bash cloudflow domains ssl --domain app.example.com ``` ### Step 4: Set as primary Make this the primary domain for your application. ```bash cloudflow domains set-primary --domain app.example.com ``` ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            },
            {
              "doc_id": "api_integrations",
              "chunk_id": "api_integrations_fix_0",
              "content": "# Integrations API Reference ## Overview The Integrations API provides programmatic access to integrations functionality. Base URL: `https://api.cloudflow.io/v1/integrations` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /integrations List available integrations.",
              "score": null
            },
            {
              "doc_id": "api_secrets",
              "chunk_id": "api_secrets_fix_0",
              "content": "# Secrets API Reference ## Overview The Secrets API provides programmatic access to secrets functionality. Base URL: `https://api.cloudflow.io/v1/secrets` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /secrets List secret names (values not returned).",
              "score": null
            },
            {
              "doc_id": "api_webhooks",
              "chunk_id": "api_webhooks_fix_0",
              "content": "# Webhooks API Reference ## Overview The Webhooks API provides programmatic access to webhooks functionality. Base URL: `https://api.cloudflow.io/v1/webhooks` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /webhooks List configured webhooks.",
              "score": null
            }
          ],
          "latency_ms": 4.18662100128131,
          "query_id": "howto_017",
          "dimension": "problem",
          "query": "I want to use app.mycompany.com instead of the default CloudFlow URL"
        },
        {
          "key_facts": [
            "CNAME",
            "ingress.cloudflow.io",
            "Let's Encrypt"
          ],
          "found_facts": [
            "CNAME",
            "ingress.cloudflow.io",
            "Let's Encrypt"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "howto_configure_custom_domain"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "howto_configure_custom_domain",
              "chunk_id": "howto_configure_custom_domain_fix_0",
              "content": "# How to Configure Custom Domain ## Prerequisites - Domain with DNS access - CloudFlow Pro plan or higher ## Steps ### Step 1: Verify domain ownership Add a TXT record to verify domain ownership. ```bash cloudflow domains verify --domain app.example.com ``` ### Step 2: Configure DNS Add a CNAME record pointing to your CloudFlow endpoint.",
              "score": null
            },
            {
              "doc_id": "howto_configure_custom_domain",
              "chunk_id": "howto_configure_custom_domain_fix_1",
              "content": "> **Note:** CNAME should point to ingress.cloudflow.io ### Step 3: Enable SSL CloudFlow automatically provisions SSL certificates via Let's Encrypt. ```bash cloudflow domains ssl --domain app.example.com ``` ### Step 4: Set as primary Make this the primary domain for your application. ```bash cloudflow domains set-primary --domain app.example.com ``` ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            },
            {
              "doc_id": "howto_set_up_ci_cd_pipeline",
              "chunk_id": "howto_set_up_ci_cd_pipeline_fix_0",
              "content": "# How to Set Up CI/CD Pipeline ## Prerequisites - GitHub account - CloudFlow API key - Repository admin access ## Steps ### Step 1: Create GitHub Actions workflow Create a new file at `.github/workflows/cloudflow.yml` with the deployment configuration. ```bash mkdir -p .github/workflows && touch .github/workflows/cloudflow.yml ``` ### Step 2: Configure secrets Add your CloudFlow API key to GitHub secrets.",
              "score": null
            },
            {
              "doc_id": "runbook_ssl_certificate_expiry",
              "chunk_id": "runbook_ssl_certificate_expiry_fix_1",
              "content": "### Step 4 Verify DNS is correct: `dig +short api.cloudflow.io` ## Resolution ### Step 1 Trigger manual renewal: `kubectl delete certificate api-cert -n production && kubectl apply -f certificate.yaml` ### Step 2 Check rate limits: Let's Encrypt has 50 certs/domain/week limit ### Step 3 Use staging issuer for testing: switch to letsencrypt-staging ClusterIssuer ### Step 4 If Let's Encrypt fails, manually upload certificate ## Escalation If certificate cannot be renewed, escalate to security team immediately.",
              "score": null
            },
            {
              "doc_id": "howto_migrate_from_v1_to_v2_api",
              "chunk_id": "howto_migrate_from_v1_to_v2_api_fix_1",
              "content": "> **Note:** Generate new credentials in the dashboard ### Step 4: Test in staging Run your integration tests against the v2 staging endpoint. ```bash CLOUDFLOW_API_URL=https://api-v2-staging.cloudflow.io npm test ``` ### Step 5: Switch production Update production configuration to use v2 API. ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            }
          ],
          "latency_ms": 3.4726599988061935,
          "query_id": "howto_017",
          "dimension": "casual",
          "query": "custom domain dns setup cloudflow"
        },
        {
          "key_facts": [
            "CNAME",
            "ingress.cloudflow.io",
            "Let's Encrypt"
          ],
          "found_facts": [
            "CNAME",
            "ingress.cloudflow.io",
            "Let's Encrypt"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "howto_configure_custom_domain"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "howto_configure_custom_domain",
              "chunk_id": "howto_configure_custom_domain_fix_1",
              "content": "> **Note:** CNAME should point to ingress.cloudflow.io ### Step 3: Enable SSL CloudFlow automatically provisions SSL certificates via Let's Encrypt. ```bash cloudflow domains ssl --domain app.example.com ``` ### Step 4: Set as primary Make this the primary domain for your application. ```bash cloudflow domains set-primary --domain app.example.com ``` ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            },
            {
              "doc_id": "howto_configure_custom_domain",
              "chunk_id": "howto_configure_custom_domain_fix_0",
              "content": "# How to Configure Custom Domain ## Prerequisites - Domain with DNS access - CloudFlow Pro plan or higher ## Steps ### Step 1: Verify domain ownership Add a TXT record to verify domain ownership. ```bash cloudflow domains verify --domain app.example.com ``` ### Step 2: Configure DNS Add a CNAME record pointing to your CloudFlow endpoint.",
              "score": null
            },
            {
              "doc_id": "howto_configure_single_sign_on",
              "chunk_id": "howto_configure_single_sign_on_fix_0",
              "content": "# How to Configure Single Sign-On ## Prerequisites - Admin access - Enterprise plan - Identity provider access ## Steps ### Step 1: Choose provider CloudFlow supports SAML 2.0, OpenID Connect, and OAuth 2.0 providers. ### Step 2: Get metadata Download CloudFlow's SAML metadata for your IdP configuration. ```bash cloudflow sso metadata --format xml > cloudflow-metadata.xml ``` ### Step 3: Configure IdP Add CloudFlow as a service provider in your identity provider.",
              "score": null
            },
            {
              "doc_id": "runbook_ssl_certificate_expiry",
              "chunk_id": "runbook_ssl_certificate_expiry_fix_0",
              "content": "# Runbook: SSL Certificate Expiry ## Severity **Level:** P3 - Medium **Response Time:** 15 minutes ## Symptoms - Certificate expiring within 7 days - SSL handshake failures - Browser security warnings ## Diagnosis ### Step 1 Check certificate expiry: `echo | openssl s_client -connect api.cloudflow.io:443 2>/dev/null | openssl x509 -noout -dates` ### Step 2 Verify cert-manager status: `kubectl get certificates -n production` ### Step 3 Check cert-manager logs: `kubectl logs -l app=cert-manager -n cert-manager --tail=50`",
              "score": null
            },
            {
              "doc_id": "runbook_ssl_certificate_expiry",
              "chunk_id": "runbook_ssl_certificate_expiry_fix_1",
              "content": "### Step 4 Verify DNS is correct: `dig +short api.cloudflow.io` ## Resolution ### Step 1 Trigger manual renewal: `kubectl delete certificate api-cert -n production && kubectl apply -f certificate.yaml` ### Step 2 Check rate limits: Let's Encrypt has 50 certs/domain/week limit ### Step 3 Use staging issuer for testing: switch to letsencrypt-staging ClusterIssuer ### Step 4 If Let's Encrypt fails, manually upload certificate ## Escalation If certificate cannot be renewed, escalate to security team immediately.",
              "score": null
            }
          ],
          "latency_ms": 3.374567000719253,
          "query_id": "howto_017",
          "dimension": "contextual",
          "query": "launching our app publicly and need to point our company domain to CloudFlow with SSL"
        },
        {
          "key_facts": [
            "CNAME",
            "ingress.cloudflow.io",
            "Let's Encrypt"
          ],
          "found_facts": [
            "CNAME",
            "ingress.cloudflow.io",
            "Let's Encrypt"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "howto_configure_custom_domain"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "howto_configure_custom_domain",
              "chunk_id": "howto_configure_custom_domain_fix_0",
              "content": "# How to Configure Custom Domain ## Prerequisites - Domain with DNS access - CloudFlow Pro plan or higher ## Steps ### Step 1: Verify domain ownership Add a TXT record to verify domain ownership. ```bash cloudflow domains verify --domain app.example.com ``` ### Step 2: Configure DNS Add a CNAME record pointing to your CloudFlow endpoint.",
              "score": null
            },
            {
              "doc_id": "howto_configure_custom_domain",
              "chunk_id": "howto_configure_custom_domain_fix_1",
              "content": "> **Note:** CNAME should point to ingress.cloudflow.io ### Step 3: Enable SSL CloudFlow automatically provisions SSL certificates via Let's Encrypt. ```bash cloudflow domains ssl --domain app.example.com ``` ### Step 4: Set as primary Make this the primary domain for your application. ```bash cloudflow domains set-primary --domain app.example.com ``` ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            },
            {
              "doc_id": "api_integrations",
              "chunk_id": "api_integrations_fix_0",
              "content": "# Integrations API Reference ## Overview The Integrations API provides programmatic access to integrations functionality. Base URL: `https://api.cloudflow.io/v1/integrations` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /integrations List available integrations.",
              "score": null
            },
            {
              "doc_id": "api_secrets",
              "chunk_id": "api_secrets_fix_0",
              "content": "# Secrets API Reference ## Overview The Secrets API provides programmatic access to secrets functionality. Base URL: `https://api.cloudflow.io/v1/secrets` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /secrets List secret names (values not returned).",
              "score": null
            },
            {
              "doc_id": "howto_create_api_integration",
              "chunk_id": "howto_create_api_integration_fix_1",
              "content": "```bash curl -H 'Authorization: Bearer cf_...' https://api.cloudflow.io/v1/me ``` ### Step 4: Implement rate limiting Handle rate limit responses (HTTP 429) with exponential backoff. > **Note:** Default limit is 100 requests per minute ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            }
          ],
          "latency_ms": 3.665169000669266,
          "query_id": "howto_017",
          "dimension": "negation",
          "query": "why can't I access my CloudFlow app through my own domain?"
        },
        {
          "key_facts": [
            "retry",
            "exponential backoff",
            "on_failure"
          ],
          "found_facts": [
            "retry",
            "exponential backoff",
            "on_failure"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "howto_implement_retry_logic"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "howto_implement_retry_logic",
              "chunk_id": "howto_implement_retry_logic_fix_0",
              "content": "# How to Implement Retry Logic ## Prerequisites - Existing workflow ## Steps ### Step 1: Define retry policy Add retry configuration to your workflow step. > **Note:** Maximum 5 retry attempts allowed ### Step 2: Configure backoff Set exponential backoff with base delay of 1 second. The formula is: delay = base * 2^attempt. ### Step 3: Add timeout Set maximum execution time to prevent infinite loops. Default timeout is 30 seconds.",
              "score": null
            },
            {
              "doc_id": "howto_implement_retry_logic",
              "chunk_id": "howto_implement_retry_logic_fix_1",
              "content": "### Step 4: Handle final failure Configure on_failure handler to notify or escalate when all retries are exhausted. ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            },
            {
              "doc_id": "config_workflow_engine",
              "chunk_id": "config_workflow_engine_fix_0",
              "content": "# Workflow Engine Configuration Reference ## Overview This document describes all configuration options for Workflow Engine. ## Environment Variables | Variable | Type | Default | Description | |----------|------|---------|-------------| | `WORKFLOW_MAX_STEPS` | integer | 100 | Maximum steps per workflow | | `WORKFLOW_TIMEOUT_SECONDS` | integer | 3600 | Maximum workflow execution time | | `WORKFLOW_RETRY_LIMIT` | integer | 3 | Default retry attempts for failed steps | | `WORKFLOW_CONCURRENT_LIMIT` | integer | 10 | Maximum",
              "score": null
            },
            {
              "doc_id": "adr_005",
              "chunk_id": "adr_005_fix_0",
              "content": "# ADR-005: Adopt Event-Driven Architecture for Workflow Execution **Status:** Accepted **Date:** 2024-12-04 ## Context We need an architecture that supports asynchronous workflow execution, step dependencies, and failure recovery. ## Options Considered ### Option 1: Event-driven with Kafka Publish-subscribe messaging. **Pros:** - Loose coupling - Scalability - Event replay - Audit trail **Cons:** - Eventual consistency - Operational complexity - Message ordering challenges ### Option 2: Request-response Synchronous API calls.",
              "score": null
            },
            {
              "doc_id": "arch_workflow_engine",
              "chunk_id": "arch_workflow_engine_fix_0",
              "content": "# Workflow Engine Architecture ## Overview The Workflow Engine is the core component responsible for executing user-defined workflows. It uses an event-driven architecture with support for parallel execution, conditional branching, and automatic retries. ## Design Principles - **Idempotency**: All operations can be safely retried without side effects - **Eventual Consistency**: State converges to consistent view within 5 seconds - **Fault Tolerance**: System continues operating despite component failures ## Components ### Scheduler Manages workflow scheduling and",
              "score": null
            }
          ],
          "latency_ms": 3.3996539987128926,
          "query_id": "howto_018",
          "dimension": "original",
          "query": "How do I implement retry logic?"
        },
        {
          "key_facts": [
            "retry",
            "exponential backoff",
            "on_failure"
          ],
          "found_facts": [
            "retry",
            "exponential backoff",
            "on_failure"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "howto_implement_retry_logic"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "howto_implement_retry_logic",
              "chunk_id": "howto_implement_retry_logic_fix_0",
              "content": "# How to Implement Retry Logic ## Prerequisites - Existing workflow ## Steps ### Step 1: Define retry policy Add retry configuration to your workflow step. > **Note:** Maximum 5 retry attempts allowed ### Step 2: Configure backoff Set exponential backoff with base delay of 1 second. The formula is: delay = base * 2^attempt. ### Step 3: Add timeout Set maximum execution time to prevent infinite loops. Default timeout is 30 seconds.",
              "score": null
            },
            {
              "doc_id": "config_workflow_engine",
              "chunk_id": "config_workflow_engine_fix_0",
              "content": "# Workflow Engine Configuration Reference ## Overview This document describes all configuration options for Workflow Engine. ## Environment Variables | Variable | Type | Default | Description | |----------|------|---------|-------------| | `WORKFLOW_MAX_STEPS` | integer | 100 | Maximum steps per workflow | | `WORKFLOW_TIMEOUT_SECONDS` | integer | 3600 | Maximum workflow execution time | | `WORKFLOW_RETRY_LIMIT` | integer | 3 | Default retry attempts for failed steps | | `WORKFLOW_CONCURRENT_LIMIT` | integer | 10 | Maximum",
              "score": null
            },
            {
              "doc_id": "howto_implement_workflow_versioning",
              "chunk_id": "howto_implement_workflow_versioning_fix_0",
              "content": "# How to Implement Workflow Versioning ## Prerequisites - Existing workflow ## Steps ### Step 1: Enable versioning Turn on versioning for your workflow. This is required for the feature. ```bash cloudflow workflows update wf_123 --enable-versioning ``` ### Step 2: Create new version Save changes as a new version instead of overwriting. ```bash cloudflow workflows publish wf_123 --version 2.0.0 ``` ### Step 3: Set active version Choose which version handles new executions.",
              "score": null
            },
            {
              "doc_id": "arch_workflow_engine",
              "chunk_id": "arch_workflow_engine_fix_0",
              "content": "# Workflow Engine Architecture ## Overview The Workflow Engine is the core component responsible for executing user-defined workflows. It uses an event-driven architecture with support for parallel execution, conditional branching, and automatic retries. ## Design Principles - **Idempotency**: All operations can be safely retried without side effects - **Eventual Consistency**: State converges to consistent view within 5 seconds - **Fault Tolerance**: System continues operating despite component failures ## Components ### Scheduler Manages workflow scheduling and",
              "score": null
            },
            {
              "doc_id": "howto_implement_retry_logic",
              "chunk_id": "howto_implement_retry_logic_fix_1",
              "content": "### Step 4: Handle final failure Configure on_failure handler to notify or escalate when all retries are exhausted. ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            }
          ],
          "latency_ms": 3.639381000539288,
          "query_id": "howto_018",
          "dimension": "synonym",
          "query": "How do I add automatic retries to my workflow steps?"
        },
        {
          "key_facts": [
            "retry",
            "exponential backoff",
            "on_failure"
          ],
          "found_facts": [
            "retry",
            "exponential backoff",
            "on_failure"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "howto_implement_retry_logic"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "howto_implement_retry_logic",
              "chunk_id": "howto_implement_retry_logic_fix_0",
              "content": "# How to Implement Retry Logic ## Prerequisites - Existing workflow ## Steps ### Step 1: Define retry policy Add retry configuration to your workflow step. > **Note:** Maximum 5 retry attempts allowed ### Step 2: Configure backoff Set exponential backoff with base delay of 1 second. The formula is: delay = base * 2^attempt. ### Step 3: Add timeout Set maximum execution time to prevent infinite loops. Default timeout is 30 seconds.",
              "score": null
            },
            {
              "doc_id": "howto_implement_retry_logic",
              "chunk_id": "howto_implement_retry_logic_fix_1",
              "content": "### Step 4: Handle final failure Configure on_failure handler to notify or escalate when all retries are exhausted. ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            },
            {
              "doc_id": "howto_set_up_monitoring_alerts",
              "chunk_id": "howto_set_up_monitoring_alerts_fix_1",
              "content": "``` ### Step 3: Set severity levels Assign severity to each alert: critical, warning, or info. > **Note:** Critical alerts bypass quiet hours ### Step 4: Test alerts Trigger a test alert to verify routing. ```bash cloudflow alerts test --name 'High Error Rate' ``` ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            },
            {
              "doc_id": "config_workflow_engine",
              "chunk_id": "config_workflow_engine_fix_0",
              "content": "# Workflow Engine Configuration Reference ## Overview This document describes all configuration options for Workflow Engine. ## Environment Variables | Variable | Type | Default | Description | |----------|------|---------|-------------| | `WORKFLOW_MAX_STEPS` | integer | 100 | Maximum steps per workflow | | `WORKFLOW_TIMEOUT_SECONDS` | integer | 3600 | Maximum workflow execution time | | `WORKFLOW_RETRY_LIMIT` | integer | 3 | Default retry attempts for failed steps | | `WORKFLOW_CONCURRENT_LIMIT` | integer | 10 | Maximum",
              "score": null
            },
            {
              "doc_id": "howto_debug_failed_execution",
              "chunk_id": "howto_debug_failed_execution_fix_0",
              "content": "# How to Debug Failed Execution ## Prerequisites - Failed execution ID ## Steps ### Step 1: Get execution details Retrieve the full execution record including step outputs. ```bash cloudflow executions get ex_123 --include-steps ``` ### Step 2: View step logs Check logs for the failed step. ```bash cloudflow executions logs ex_123 --step 3 ``` ### Step 3: Inspect input/output Review the data passed between steps.",
              "score": null
            }
          ],
          "latency_ms": 3.564962999007548,
          "query_id": "howto_018",
          "dimension": "problem",
          "query": "my workflow steps sometimes fail due to transient errors and I want them to automatically try again"
        },
        {
          "key_facts": [
            "retry",
            "exponential backoff",
            "on_failure"
          ],
          "found_facts": [
            "retry",
            "exponential backoff",
            "on_failure"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "howto_implement_retry_logic"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "howto_implement_retry_logic",
              "chunk_id": "howto_implement_retry_logic_fix_0",
              "content": "# How to Implement Retry Logic ## Prerequisites - Existing workflow ## Steps ### Step 1: Define retry policy Add retry configuration to your workflow step. > **Note:** Maximum 5 retry attempts allowed ### Step 2: Configure backoff Set exponential backoff with base delay of 1 second. The formula is: delay = base * 2^attempt. ### Step 3: Add timeout Set maximum execution time to prevent infinite loops. Default timeout is 30 seconds.",
              "score": null
            },
            {
              "doc_id": "config_workflow_engine",
              "chunk_id": "config_workflow_engine_fix_0",
              "content": "# Workflow Engine Configuration Reference ## Overview This document describes all configuration options for Workflow Engine. ## Environment Variables | Variable | Type | Default | Description | |----------|------|---------|-------------| | `WORKFLOW_MAX_STEPS` | integer | 100 | Maximum steps per workflow | | `WORKFLOW_TIMEOUT_SECONDS` | integer | 3600 | Maximum workflow execution time | | `WORKFLOW_RETRY_LIMIT` | integer | 3 | Default retry attempts for failed steps | | `WORKFLOW_CONCURRENT_LIMIT` | integer | 10 | Maximum",
              "score": null
            },
            {
              "doc_id": "howto_implement_retry_logic",
              "chunk_id": "howto_implement_retry_logic_fix_1",
              "content": "### Step 4: Handle final failure Configure on_failure handler to notify or escalate when all retries are exhausted. ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            },
            {
              "doc_id": "config_workflow_engine",
              "chunk_id": "config_workflow_engine_fix_1",
              "content": "concurrent executions per workflow | | `EXECUTOR_POOL_SIZE` | integer | 50 | Number of executor workers | | `EXECUTOR_MEMORY_LIMIT` | string | 512Mi | Memory limit per executor | ## Configuration File Configuration can also be provided via `workflow engine.yaml`: ```yaml workflow_max_steps: 100 workflow_timeout_seconds: 3600 workflow_retry_limit: 3 workflow_concurrent_limit: 10 executor_pool_size: 50 ``` ## Validation Configuration is validated at startup. Invalid configuration will prevent the service from starting.",
              "score": null
            },
            {
              "doc_id": "api_workflows",
              "chunk_id": "api_workflows_fix_2",
              "content": "**Parameters:** - `name` (string): Workflow name (required) - `definition` (object): Workflow definition in JSON format **Response:** - Success: `201` - Workflow created successfully - Error: `400` - Bad Request **Example:** ```json {\"id\": \"wf_456\", \"name\": \"New Workflow\", \"created_at\": \"2024-01-15T10:00:00Z\"} ``` ### DELETE /workflows/{id} Delete a workflow permanently. **Parameters:** - `id` (string): Workflow ID (required) **Response:** - Success: `204` - Workflow deleted - Error: `404` - Workflow not found **Example:** ```json {} ```",
              "score": null
            }
          ],
          "latency_ms": 3.352116000314709,
          "query_id": "howto_018",
          "dimension": "casual",
          "query": "workflow retry backoff config"
        },
        {
          "key_facts": [
            "retry",
            "exponential backoff",
            "on_failure"
          ],
          "found_facts": [
            "retry",
            "exponential backoff",
            "on_failure"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "howto_implement_retry_logic"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "howto_implement_retry_logic",
              "chunk_id": "howto_implement_retry_logic_fix_0",
              "content": "# How to Implement Retry Logic ## Prerequisites - Existing workflow ## Steps ### Step 1: Define retry policy Add retry configuration to your workflow step. > **Note:** Maximum 5 retry attempts allowed ### Step 2: Configure backoff Set exponential backoff with base delay of 1 second. The formula is: delay = base * 2^attempt. ### Step 3: Add timeout Set maximum execution time to prevent infinite loops. Default timeout is 30 seconds.",
              "score": null
            },
            {
              "doc_id": "adr_005",
              "chunk_id": "adr_005_fix_0",
              "content": "# ADR-005: Adopt Event-Driven Architecture for Workflow Execution **Status:** Accepted **Date:** 2024-12-04 ## Context We need an architecture that supports asynchronous workflow execution, step dependencies, and failure recovery. ## Options Considered ### Option 1: Event-driven with Kafka Publish-subscribe messaging. **Pros:** - Loose coupling - Scalability - Event replay - Audit trail **Cons:** - Eventual consistency - Operational complexity - Message ordering challenges ### Option 2: Request-response Synchronous API calls.",
              "score": null
            },
            {
              "doc_id": "howto_implement_retry_logic",
              "chunk_id": "howto_implement_retry_logic_fix_1",
              "content": "### Step 4: Handle final failure Configure on_failure handler to notify or escalate when all retries are exhausted. ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            },
            {
              "doc_id": "release_3_2_0",
              "chunk_id": "release_3_2_0_fix_0",
              "content": "# Release Notes - v3.2.0 **Release Date:** 2024-01-15 ## New Features - Workflow versioning with rollback support - New Python SDK with async support - Bulk operations API for workflows - Custom retry policies per step ## Bug Fixes - Fixed race condition in concurrent execution handler - Resolved memory leak in long-running workflows - Fixed timezone handling in scheduled triggers ## Breaking Changes - **BREAKING:** API v1 endpoints deprecated, will be removed in v4.0",
              "score": null
            },
            {
              "doc_id": "howto_create_scheduled_workflow",
              "chunk_id": "howto_create_scheduled_workflow_fix_1",
              "content": "> **Note:** Use IANA timezone names like America/New_York ### Step 3: Configure overlap handling Choose behavior when previous execution is still running: skip, queue, or cancel. ### Step 4: Enable notifications Get notified on schedule failures. ```bash cloudflow workflows notify --on-failure ``` ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            }
          ],
          "latency_ms": 3.421796000111499,
          "query_id": "howto_018",
          "dimension": "contextual",
          "query": "building a workflow that calls an external API which sometimes times out, need to handle temporary failures gracefully"
        },
        {
          "key_facts": [
            "retry",
            "exponential backoff",
            "on_failure"
          ],
          "found_facts": [
            "retry",
            "exponential backoff",
            "on_failure"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "howto_implement_retry_logic"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "howto_implement_retry_logic",
              "chunk_id": "howto_implement_retry_logic_fix_0",
              "content": "# How to Implement Retry Logic ## Prerequisites - Existing workflow ## Steps ### Step 1: Define retry policy Add retry configuration to your workflow step. > **Note:** Maximum 5 retry attempts allowed ### Step 2: Configure backoff Set exponential backoff with base delay of 1 second. The formula is: delay = base * 2^attempt. ### Step 3: Add timeout Set maximum execution time to prevent infinite loops. Default timeout is 30 seconds.",
              "score": null
            },
            {
              "doc_id": "howto_implement_retry_logic",
              "chunk_id": "howto_implement_retry_logic_fix_1",
              "content": "### Step 4: Handle final failure Configure on_failure handler to notify or escalate when all retries are exhausted. ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            },
            {
              "doc_id": "config_workflow_engine",
              "chunk_id": "config_workflow_engine_fix_0",
              "content": "# Workflow Engine Configuration Reference ## Overview This document describes all configuration options for Workflow Engine. ## Environment Variables | Variable | Type | Default | Description | |----------|------|---------|-------------| | `WORKFLOW_MAX_STEPS` | integer | 100 | Maximum steps per workflow | | `WORKFLOW_TIMEOUT_SECONDS` | integer | 3600 | Maximum workflow execution time | | `WORKFLOW_RETRY_LIMIT` | integer | 3 | Default retry attempts for failed steps | | `WORKFLOW_CONCURRENT_LIMIT` | integer | 10 | Maximum",
              "score": null
            },
            {
              "doc_id": "api_workflows",
              "chunk_id": "api_workflows_fix_2",
              "content": "**Parameters:** - `name` (string): Workflow name (required) - `definition` (object): Workflow definition in JSON format **Response:** - Success: `201` - Workflow created successfully - Error: `400` - Bad Request **Example:** ```json {\"id\": \"wf_456\", \"name\": \"New Workflow\", \"created_at\": \"2024-01-15T10:00:00Z\"} ``` ### DELETE /workflows/{id} Delete a workflow permanently. **Parameters:** - `id` (string): Workflow ID (required) **Response:** - Success: `204` - Workflow deleted - Error: `404` - Workflow not found **Example:** ```json {} ```",
              "score": null
            },
            {
              "doc_id": "howto_debug_failed_execution",
              "chunk_id": "howto_debug_failed_execution_fix_1",
              "content": "```bash cloudflow executions inspect ex_123 --step 3 ``` ### Step 4: Enable debug mode Re-run with verbose logging enabled. ```bash cloudflow workflows execute wf_123 --debug ``` ### Step 5: Check error patterns Look for similar failures in recent executions. ```bash cloudflow executions list --status failed --since 24h ``` ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            }
          ],
          "latency_ms": 3.337428999657277,
          "query_id": "howto_018",
          "dimension": "negation",
          "query": "why does my workflow fail permanently instead of retrying when a step errors?"
        },
        {
          "key_facts": [
            "DATABASE_URL",
            "cloudflow secrets set",
            "require-ssl"
          ],
          "found_facts": [
            "DATABASE_URL",
            "cloudflow secrets set"
          ],
          "missed_facts": [
            "require-ssl"
          ],
          "coverage": 0.6666666666666666,
          "expected_docs": [
            "howto_set_up_database_connecti"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "config_database",
              "chunk_id": "config_database_fix_0",
              "content": "# Database Configuration Reference ## Overview This document describes all configuration options for Database. ## Environment Variables | Variable | Type | Default | Description | |----------|------|---------|-------------| | `DATABASE_URL` | string | required | PostgreSQL connection string | | `DATABASE_POOL_MIN` | integer | 5 | Minimum connection pool size | | `DATABASE_POOL_MAX` | integer | 20 | Maximum connection pool size | | `DATABASE_STATEMENT_TIMEOUT` | integer | 30000 | Query timeout in milliseconds | |",
              "score": null
            },
            {
              "doc_id": "howto_set_up_database_connection",
              "chunk_id": "howto_set_up_database_connection_fix_0",
              "content": "# How to Set Up Database Connection ## Prerequisites - Database with network access to CloudFlow - Connection credentials ## Steps ### Step 1: Add connection secret Store your database connection string as a secret. ```bash cloudflow secrets set DATABASE_URL postgresql://user:pass@host:5432/db ``` ### Step 2: Configure connection pool Set pool size based on your plan limits. Free tier allows 5 connections, Pro allows 20.",
              "score": null
            },
            {
              "doc_id": "config_database",
              "chunk_id": "config_database_fix_1",
              "content": "`DATABASE_SSL_MODE` | string | require | SSL mode: disable, require, verify-full | | `DATABASE_MIGRATION_AUTO` | boolean | false | Run migrations on startup | ## Configuration File Configuration can also be provided via `database.yaml`: ```yaml database_url: required database_pool_min: 5 database_pool_max: 20 database_statement_timeout: 30000 database_ssl_mode: require ``` ## Validation Configuration is validated at startup. Invalid configuration will prevent the service from starting.",
              "score": null
            },
            {
              "doc_id": "adr_001",
              "chunk_id": "adr_001_fix_0",
              "content": "# ADR-001: Use PostgreSQL for Primary Database **Status:** Accepted **Date:** 2024-11-04 ## Context We need a database to store workflow definitions, execution state, and user data. The database must support ACID transactions, JSON queries, and scale to millions of records. ## Options Considered ### Option 1: PostgreSQL Mature relational database with JSONB support.",
              "score": null
            },
            {
              "doc_id": "runbook_database_connection_exhaustion",
              "chunk_id": "runbook_database_connection_exhaustion_fix_0",
              "content": "# Runbook: Database Connection Exhaustion ## Severity **Level:** P1 - Critical **Response Time:** 15 minutes ## Symptoms - Connection pool at 100% - Queries timing out - 'too many connections' errors in logs ## Diagnosis ### Step 1 Check active connections: `SELECT count(*) FROM pg_stat_activity` ### Step 2 Identify long-running queries: `SELECT pid, now() - pg_stat_activity.query_start AS duration, query FROM pg_stat_activity WHERE state != 'idle' ORDER BY duration DESC` ### Step 3 Check PgBouncer stats:",
              "score": null
            }
          ],
          "latency_ms": 3.3776039999793284,
          "query_id": "howto_019",
          "dimension": "original",
          "query": "How do I set up database connection?"
        },
        {
          "key_facts": [
            "DATABASE_URL",
            "cloudflow secrets set",
            "require-ssl"
          ],
          "found_facts": [
            "DATABASE_URL",
            "cloudflow secrets set",
            "require-ssl"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "howto_set_up_database_connecti"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "howto_set_up_database_connection",
              "chunk_id": "howto_set_up_database_connection_fix_0",
              "content": "# How to Set Up Database Connection ## Prerequisites - Database with network access to CloudFlow - Connection credentials ## Steps ### Step 1: Add connection secret Store your database connection string as a secret. ```bash cloudflow secrets set DATABASE_URL postgresql://user:pass@host:5432/db ``` ### Step 2: Configure connection pool Set pool size based on your plan limits. Free tier allows 5 connections, Pro allows 20.",
              "score": null
            },
            {
              "doc_id": "api_integrations",
              "chunk_id": "api_integrations_fix_0",
              "content": "# Integrations API Reference ## Overview The Integrations API provides programmatic access to integrations functionality. Base URL: `https://api.cloudflow.io/v1/integrations` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /integrations List available integrations.",
              "score": null
            },
            {
              "doc_id": "howto_set_up_database_connection",
              "chunk_id": "howto_set_up_database_connection_fix_1",
              "content": "> **Note:** Pool exhaustion will queue requests ### Step 3: Test connection Verify the database is accessible from CloudFlow. ```bash cloudflow db test --secret DATABASE_URL ``` ### Step 4: Enable SSL For production, always require SSL connections. ```bash cloudflow db configure --require-ssl ``` ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            },
            {
              "doc_id": "howto_migrate_from_v1_to_v2_api",
              "chunk_id": "howto_migrate_from_v1_to_v2_api_fix_1",
              "content": "> **Note:** Generate new credentials in the dashboard ### Step 4: Test in staging Run your integration tests against the v2 staging endpoint. ```bash CLOUDFLOW_API_URL=https://api-v2-staging.cloudflow.io npm test ``` ### Step 5: Switch production Update production configuration to use v2 API. ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            },
            {
              "doc_id": "howto_configure_custom_domain",
              "chunk_id": "howto_configure_custom_domain_fix_0",
              "content": "# How to Configure Custom Domain ## Prerequisites - Domain with DNS access - CloudFlow Pro plan or higher ## Steps ### Step 1: Verify domain ownership Add a TXT record to verify domain ownership. ```bash cloudflow domains verify --domain app.example.com ``` ### Step 2: Configure DNS Add a CNAME record pointing to your CloudFlow endpoint.",
              "score": null
            }
          ],
          "latency_ms": 3.907009999238653,
          "query_id": "howto_019",
          "dimension": "synonym",
          "query": "How do I connect CloudFlow to my database?"
        },
        {
          "key_facts": [
            "DATABASE_URL",
            "cloudflow secrets set",
            "require-ssl"
          ],
          "found_facts": [
            "DATABASE_URL",
            "cloudflow secrets set",
            "require-ssl"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "howto_set_up_database_connecti"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "howto_set_up_database_connection",
              "chunk_id": "howto_set_up_database_connection_fix_0",
              "content": "# How to Set Up Database Connection ## Prerequisites - Database with network access to CloudFlow - Connection credentials ## Steps ### Step 1: Add connection secret Store your database connection string as a secret. ```bash cloudflow secrets set DATABASE_URL postgresql://user:pass@host:5432/db ``` ### Step 2: Configure connection pool Set pool size based on your plan limits. Free tier allows 5 connections, Pro allows 20.",
              "score": null
            },
            {
              "doc_id": "api_integrations",
              "chunk_id": "api_integrations_fix_0",
              "content": "# Integrations API Reference ## Overview The Integrations API provides programmatic access to integrations functionality. Base URL: `https://api.cloudflow.io/v1/integrations` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /integrations List available integrations.",
              "score": null
            },
            {
              "doc_id": "howto_set_up_ci_cd_pipeline",
              "chunk_id": "howto_set_up_ci_cd_pipeline_fix_0",
              "content": "# How to Set Up CI/CD Pipeline ## Prerequisites - GitHub account - CloudFlow API key - Repository admin access ## Steps ### Step 1: Create GitHub Actions workflow Create a new file at `.github/workflows/cloudflow.yml` with the deployment configuration. ```bash mkdir -p .github/workflows && touch .github/workflows/cloudflow.yml ``` ### Step 2: Configure secrets Add your CloudFlow API key to GitHub secrets.",
              "score": null
            },
            {
              "doc_id": "howto_create_api_integration",
              "chunk_id": "howto_create_api_integration_fix_0",
              "content": "# How to Create API Integration ## Prerequisites - CloudFlow account ## Steps ### Step 1: Generate API key Create an API key for programmatic access. ```bash cloudflow apikeys create --name 'CI Integration' --scopes workflows:read,workflows:execute ``` ### Step 2: Store key securely Never commit API keys to source control. Use environment variables or secret management. > **Note:** Rotate keys every 90 days ### Step 3: Test authentication Verify the API key works correctly.",
              "score": null
            },
            {
              "doc_id": "howto_set_up_database_connection",
              "chunk_id": "howto_set_up_database_connection_fix_1",
              "content": "> **Note:** Pool exhaustion will queue requests ### Step 3: Test connection Verify the database is accessible from CloudFlow. ```bash cloudflow db test --secret DATABASE_URL ``` ### Step 4: Enable SSL For production, always require SSL connections. ```bash cloudflow db configure --require-ssl ``` ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            }
          ],
          "latency_ms": 3.7302609998732805,
          "query_id": "howto_019",
          "dimension": "problem",
          "query": "I need my CloudFlow workflows to read and write data from my existing database"
        },
        {
          "key_facts": [
            "DATABASE_URL",
            "cloudflow secrets set",
            "require-ssl"
          ],
          "found_facts": [
            "DATABASE_URL",
            "cloudflow secrets set",
            "require-ssl"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "howto_set_up_database_connecti"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "howto_set_up_database_connection",
              "chunk_id": "howto_set_up_database_connection_fix_0",
              "content": "# How to Set Up Database Connection ## Prerequisites - Database with network access to CloudFlow - Connection credentials ## Steps ### Step 1: Add connection secret Store your database connection string as a secret. ```bash cloudflow secrets set DATABASE_URL postgresql://user:pass@host:5432/db ``` ### Step 2: Configure connection pool Set pool size based on your plan limits. Free tier allows 5 connections, Pro allows 20.",
              "score": null
            },
            {
              "doc_id": "howto_set_up_database_connection",
              "chunk_id": "howto_set_up_database_connection_fix_1",
              "content": "> **Note:** Pool exhaustion will queue requests ### Step 3: Test connection Verify the database is accessible from CloudFlow. ```bash cloudflow db test --secret DATABASE_URL ``` ### Step 4: Enable SSL For production, always require SSL connections. ```bash cloudflow db configure --require-ssl ``` ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            },
            {
              "doc_id": "howto_configure_custom_domain",
              "chunk_id": "howto_configure_custom_domain_fix_0",
              "content": "# How to Configure Custom Domain ## Prerequisites - Domain with DNS access - CloudFlow Pro plan or higher ## Steps ### Step 1: Verify domain ownership Add a TXT record to verify domain ownership. ```bash cloudflow domains verify --domain app.example.com ``` ### Step 2: Configure DNS Add a CNAME record pointing to your CloudFlow endpoint.",
              "score": null
            },
            {
              "doc_id": "api_integrations",
              "chunk_id": "api_integrations_fix_0",
              "content": "# Integrations API Reference ## Overview The Integrations API provides programmatic access to integrations functionality. Base URL: `https://api.cloudflow.io/v1/integrations` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /integrations List available integrations.",
              "score": null
            },
            {
              "doc_id": "howto_configure_custom_domain",
              "chunk_id": "howto_configure_custom_domain_fix_1",
              "content": "> **Note:** CNAME should point to ingress.cloudflow.io ### Step 3: Enable SSL CloudFlow automatically provisions SSL certificates via Let's Encrypt. ```bash cloudflow domains ssl --domain app.example.com ``` ### Step 4: Set as primary Make this the primary domain for your application. ```bash cloudflow domains set-primary --domain app.example.com ``` ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            }
          ],
          "latency_ms": 4.429102998983581,
          "query_id": "howto_019",
          "dimension": "casual",
          "query": "cloudflow database connection string setup"
        },
        {
          "key_facts": [
            "DATABASE_URL",
            "cloudflow secrets set",
            "require-ssl"
          ],
          "found_facts": [
            "DATABASE_URL",
            "cloudflow secrets set",
            "require-ssl"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "howto_set_up_database_connecti"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "howto_set_up_database_connection",
              "chunk_id": "howto_set_up_database_connection_fix_0",
              "content": "# How to Set Up Database Connection ## Prerequisites - Database with network access to CloudFlow - Connection credentials ## Steps ### Step 1: Add connection secret Store your database connection string as a secret. ```bash cloudflow secrets set DATABASE_URL postgresql://user:pass@host:5432/db ``` ### Step 2: Configure connection pool Set pool size based on your plan limits. Free tier allows 5 connections, Pro allows 20.",
              "score": null
            },
            {
              "doc_id": "howto_set_up_database_connection",
              "chunk_id": "howto_set_up_database_connection_fix_1",
              "content": "> **Note:** Pool exhaustion will queue requests ### Step 3: Test connection Verify the database is accessible from CloudFlow. ```bash cloudflow db test --secret DATABASE_URL ``` ### Step 4: Enable SSL For production, always require SSL connections. ```bash cloudflow db configure --require-ssl ``` ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            },
            {
              "doc_id": "howto_migrate_from_v1_to_v2_api",
              "chunk_id": "howto_migrate_from_v1_to_v2_api_fix_1",
              "content": "> **Note:** Generate new credentials in the dashboard ### Step 4: Test in staging Run your integration tests against the v2 staging endpoint. ```bash CLOUDFLOW_API_URL=https://api-v2-staging.cloudflow.io npm test ``` ### Step 5: Switch production Update production configuration to use v2 API. ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            },
            {
              "doc_id": "howto_create_api_integration",
              "chunk_id": "howto_create_api_integration_fix_0",
              "content": "# How to Create API Integration ## Prerequisites - CloudFlow account ## Steps ### Step 1: Generate API key Create an API key for programmatic access. ```bash cloudflow apikeys create --name 'CI Integration' --scopes workflows:read,workflows:execute ``` ### Step 2: Store key securely Never commit API keys to source control. Use environment variables or secret management. > **Note:** Rotate keys every 90 days ### Step 3: Test authentication Verify the API key works correctly.",
              "score": null
            },
            {
              "doc_id": "howto_set_up_data_encryption",
              "chunk_id": "howto_set_up_data_encryption_fix_0",
              "content": "# How to Set Up Data Encryption ## Prerequisites - CloudFlow Enterprise plan ## Steps ### Step 1: Enable encryption at rest All data is encrypted at rest using AES-256 by default. > **Note:** Encryption is automatic for all plans ### Step 2: Configure KMS Bring your own encryption keys using AWS KMS or GCP KMS. ```bash cloudflow encryption configure --kms-key arn:aws:kms:...",
              "score": null
            }
          ],
          "latency_ms": 3.996957999333972,
          "query_id": "howto_019",
          "dimension": "contextual",
          "query": "migrating our app to CloudFlow and need to connect to our production database securely"
        },
        {
          "key_facts": [
            "DATABASE_URL",
            "cloudflow secrets set",
            "require-ssl"
          ],
          "found_facts": [
            "DATABASE_URL",
            "require-ssl"
          ],
          "missed_facts": [
            "cloudflow secrets set"
          ],
          "coverage": 0.6666666666666666,
          "expected_docs": [
            "howto_set_up_database_connecti"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "api_workflows",
              "chunk_id": "api_workflows_fix_0",
              "content": "# Workflows API Reference ## Overview The Workflows API provides programmatic access to workflows functionality. Base URL: `https://api.cloudflow.io/v1/workflows` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /workflows List all workflows for the authenticated user.",
              "score": null
            },
            {
              "doc_id": "howto_set_up_data_encryption",
              "chunk_id": "howto_set_up_data_encryption_fix_1",
              "content": "``` ### Step 3: Enable field-level encryption Encrypt sensitive fields within your workflow data. ```bash cloudflow encryption add-field --path $.user.ssn --key-alias sensitive-data ``` ### Step 4: Verify encryption Confirm encryption is active for your workspace. ```bash cloudflow encryption status ``` ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            },
            {
              "doc_id": "howto_set_up_database_connection",
              "chunk_id": "howto_set_up_database_connection_fix_1",
              "content": "> **Note:** Pool exhaustion will queue requests ### Step 3: Test connection Verify the database is accessible from CloudFlow. ```bash cloudflow db test --secret DATABASE_URL ``` ### Step 4: Enable SSL For production, always require SSL connections. ```bash cloudflow db configure --require-ssl ``` ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            },
            {
              "doc_id": "api_executions",
              "chunk_id": "api_executions_fix_0",
              "content": "# Executions API Reference ## Overview The Executions API provides programmatic access to executions functionality. Base URL: `https://api.cloudflow.io/v1/executions` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /executions List workflow executions.",
              "score": null
            },
            {
              "doc_id": "api_integrations",
              "chunk_id": "api_integrations_fix_0",
              "content": "# Integrations API Reference ## Overview The Integrations API provides programmatic access to integrations functionality. Base URL: `https://api.cloudflow.io/v1/integrations` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /integrations List available integrations.",
              "score": null
            }
          ],
          "latency_ms": 3.565172999515198,
          "query_id": "howto_019",
          "dimension": "negation",
          "query": "why can't my workflow access my database even though I have the credentials?"
        },
        {
          "key_facts": [
            "cron",
            "0 9 * * MON-FRI",
            "timezone"
          ],
          "found_facts": [
            "cron",
            "0 9 * * MON-FRI",
            "timezone"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "howto_create_scheduled_workflo"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "howto_create_scheduled_workflow",
              "chunk_id": "howto_create_scheduled_workflow_fix_0",
              "content": "# How to Create Scheduled Workflow ## Prerequisites - Workflow definition ## Steps ### Step 1: Define schedule Use cron expression to set the schedule. Example: '0 9 * * MON-FRI' runs at 9 AM on weekdays. ### Step 2: Set timezone Specify timezone for the schedule. Default is UTC.",
              "score": null
            },
            {
              "doc_id": "arch_workflow_engine",
              "chunk_id": "arch_workflow_engine_fix_0",
              "content": "# Workflow Engine Architecture ## Overview The Workflow Engine is the core component responsible for executing user-defined workflows. It uses an event-driven architecture with support for parallel execution, conditional branching, and automatic retries. ## Design Principles - **Idempotency**: All operations can be safely retried without side effects - **Eventual Consistency**: State converges to consistent view within 5 seconds - **Fault Tolerance**: System continues operating despite component failures ## Components ### Scheduler Manages workflow scheduling and",
              "score": null
            },
            {
              "doc_id": "arch_workflow_engine",
              "chunk_id": "arch_workflow_engine_fix_1",
              "content": "triggers. Supports cron expressions, webhooks, and event-based triggers. **Technology:** Custom Go service **Scaling:** Horizontal with leader election ### Executor Runs individual workflow steps in isolated containers. Maximum execution time is 30 minutes per step. **Technology:** Kubernetes Jobs **Scaling:** Auto-scaling based on queue depth ### State Store Persists workflow state, execution history, and step results. Uses optimistic locking for concurrent updates.",
              "score": null
            },
            {
              "doc_id": "howto_create_scheduled_workflow",
              "chunk_id": "howto_create_scheduled_workflow_fix_1",
              "content": "> **Note:** Use IANA timezone names like America/New_York ### Step 3: Configure overlap handling Choose behavior when previous execution is still running: skip, queue, or cancel. ### Step 4: Enable notifications Get notified on schedule failures. ```bash cloudflow workflows notify --on-failure ``` ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            },
            {
              "doc_id": "howto_create_api_integration",
              "chunk_id": "howto_create_api_integration_fix_0",
              "content": "# How to Create API Integration ## Prerequisites - CloudFlow account ## Steps ### Step 1: Generate API key Create an API key for programmatic access. ```bash cloudflow apikeys create --name 'CI Integration' --scopes workflows:read,workflows:execute ``` ### Step 2: Store key securely Never commit API keys to source control. Use environment variables or secret management. > **Note:** Rotate keys every 90 days ### Step 3: Test authentication Verify the API key works correctly.",
              "score": null
            }
          ],
          "latency_ms": 3.6536679999699118,
          "query_id": "howto_020",
          "dimension": "original",
          "query": "How do I create a scheduled workflow?"
        },
        {
          "key_facts": [
            "cron",
            "0 9 * * MON-FRI",
            "timezone"
          ],
          "found_facts": [
            "cron",
            "0 9 * * MON-FRI",
            "timezone"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "howto_create_scheduled_workflo"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "howto_create_scheduled_workflow",
              "chunk_id": "howto_create_scheduled_workflow_fix_0",
              "content": "# How to Create Scheduled Workflow ## Prerequisites - Workflow definition ## Steps ### Step 1: Define schedule Use cron expression to set the schedule. Example: '0 9 * * MON-FRI' runs at 9 AM on weekdays. ### Step 2: Set timezone Specify timezone for the schedule. Default is UTC.",
              "score": null
            },
            {
              "doc_id": "arch_workflow_engine",
              "chunk_id": "arch_workflow_engine_fix_0",
              "content": "# Workflow Engine Architecture ## Overview The Workflow Engine is the core component responsible for executing user-defined workflows. It uses an event-driven architecture with support for parallel execution, conditional branching, and automatic retries. ## Design Principles - **Idempotency**: All operations can be safely retried without side effects - **Eventual Consistency**: State converges to consistent view within 5 seconds - **Fault Tolerance**: System continues operating despite component failures ## Components ### Scheduler Manages workflow scheduling and",
              "score": null
            },
            {
              "doc_id": "arch_workflow_engine",
              "chunk_id": "arch_workflow_engine_fix_1",
              "content": "triggers. Supports cron expressions, webhooks, and event-based triggers. **Technology:** Custom Go service **Scaling:** Horizontal with leader election ### Executor Runs individual workflow steps in isolated containers. Maximum execution time is 30 minutes per step. **Technology:** Kubernetes Jobs **Scaling:** Auto-scaling based on queue depth ### State Store Persists workflow state, execution history, and step results. Uses optimistic locking for concurrent updates.",
              "score": null
            },
            {
              "doc_id": "howto_create_scheduled_workflow",
              "chunk_id": "howto_create_scheduled_workflow_fix_1",
              "content": "> **Note:** Use IANA timezone names like America/New_York ### Step 3: Configure overlap handling Choose behavior when previous execution is still running: skip, queue, or cancel. ### Step 4: Enable notifications Get notified on schedule failures. ```bash cloudflow workflows notify --on-failure ``` ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            },
            {
              "doc_id": "howto_implement_retry_logic",
              "chunk_id": "howto_implement_retry_logic_fix_0",
              "content": "# How to Implement Retry Logic ## Prerequisites - Existing workflow ## Steps ### Step 1: Define retry policy Add retry configuration to your workflow step. > **Note:** Maximum 5 retry attempts allowed ### Step 2: Configure backoff Set exponential backoff with base delay of 1 second. The formula is: delay = base * 2^attempt. ### Step 3: Add timeout Set maximum execution time to prevent infinite loops. Default timeout is 30 seconds.",
              "score": null
            }
          ],
          "latency_ms": 3.5802610000246204,
          "query_id": "howto_020",
          "dimension": "synonym",
          "query": "How do I run a workflow automatically on a recurring schedule?"
        },
        {
          "key_facts": [
            "cron",
            "0 9 * * MON-FRI",
            "timezone"
          ],
          "found_facts": [
            "cron",
            "0 9 * * MON-FRI",
            "timezone"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "howto_create_scheduled_workflo"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "howto_create_scheduled_workflow",
              "chunk_id": "howto_create_scheduled_workflow_fix_0",
              "content": "# How to Create Scheduled Workflow ## Prerequisites - Workflow definition ## Steps ### Step 1: Define schedule Use cron expression to set the schedule. Example: '0 9 * * MON-FRI' runs at 9 AM on weekdays. ### Step 2: Set timezone Specify timezone for the schedule. Default is UTC.",
              "score": null
            },
            {
              "doc_id": "howto_create_scheduled_workflow",
              "chunk_id": "howto_create_scheduled_workflow_fix_1",
              "content": "> **Note:** Use IANA timezone names like America/New_York ### Step 3: Configure overlap handling Choose behavior when previous execution is still running: skip, queue, or cancel. ### Step 4: Enable notifications Get notified on schedule failures. ```bash cloudflow workflows notify --on-failure ``` ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            },
            {
              "doc_id": "arch_workflow_engine",
              "chunk_id": "arch_workflow_engine_fix_0",
              "content": "# Workflow Engine Architecture ## Overview The Workflow Engine is the core component responsible for executing user-defined workflows. It uses an event-driven architecture with support for parallel execution, conditional branching, and automatic retries. ## Design Principles - **Idempotency**: All operations can be safely retried without side effects - **Eventual Consistency**: State converges to consistent view within 5 seconds - **Fault Tolerance**: System continues operating despite component failures ## Components ### Scheduler Manages workflow scheduling and",
              "score": null
            },
            {
              "doc_id": "arch_workflow_engine",
              "chunk_id": "arch_workflow_engine_fix_1",
              "content": "triggers. Supports cron expressions, webhooks, and event-based triggers. **Technology:** Custom Go service **Scaling:** Horizontal with leader election ### Executor Runs individual workflow steps in isolated containers. Maximum execution time is 30 minutes per step. **Technology:** Kubernetes Jobs **Scaling:** Auto-scaling based on queue depth ### State Store Persists workflow state, execution history, and step results. Uses optimistic locking for concurrent updates.",
              "score": null
            },
            {
              "doc_id": "config_workflow_engine",
              "chunk_id": "config_workflow_engine_fix_0",
              "content": "# Workflow Engine Configuration Reference ## Overview This document describes all configuration options for Workflow Engine. ## Environment Variables | Variable | Type | Default | Description | |----------|------|---------|-------------| | `WORKFLOW_MAX_STEPS` | integer | 100 | Maximum steps per workflow | | `WORKFLOW_TIMEOUT_SECONDS` | integer | 3600 | Maximum workflow execution time | | `WORKFLOW_RETRY_LIMIT` | integer | 3 | Default retry attempts for failed steps | | `WORKFLOW_CONCURRENT_LIMIT` | integer | 10 | Maximum",
              "score": null
            }
          ],
          "latency_ms": 3.46056799935468,
          "query_id": "howto_020",
          "dimension": "problem",
          "query": "I want my workflow to run every weekday morning at 9 AM automatically"
        },
        {
          "key_facts": [
            "cron",
            "0 9 * * MON-FRI",
            "timezone"
          ],
          "found_facts": [
            "cron",
            "0 9 * * MON-FRI",
            "timezone"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "howto_create_scheduled_workflo"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "howto_create_scheduled_workflow",
              "chunk_id": "howto_create_scheduled_workflow_fix_1",
              "content": "> **Note:** Use IANA timezone names like America/New_York ### Step 3: Configure overlap handling Choose behavior when previous execution is still running: skip, queue, or cancel. ### Step 4: Enable notifications Get notified on schedule failures. ```bash cloudflow workflows notify --on-failure ``` ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            },
            {
              "doc_id": "howto_create_scheduled_workflow",
              "chunk_id": "howto_create_scheduled_workflow_fix_0",
              "content": "# How to Create Scheduled Workflow ## Prerequisites - Workflow definition ## Steps ### Step 1: Define schedule Use cron expression to set the schedule. Example: '0 9 * * MON-FRI' runs at 9 AM on weekdays. ### Step 2: Set timezone Specify timezone for the schedule. Default is UTC.",
              "score": null
            },
            {
              "doc_id": "howto_implement_retry_logic",
              "chunk_id": "howto_implement_retry_logic_fix_1",
              "content": "### Step 4: Handle final failure Configure on_failure handler to notify or escalate when all retries are exhausted. ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            },
            {
              "doc_id": "api_executions",
              "chunk_id": "api_executions_fix_0",
              "content": "# Executions API Reference ## Overview The Executions API provides programmatic access to executions functionality. Base URL: `https://api.cloudflow.io/v1/executions` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /executions List workflow executions.",
              "score": null
            },
            {
              "doc_id": "howto_set_up_ci_cd_pipeline",
              "chunk_id": "howto_set_up_ci_cd_pipeline_fix_1",
              "content": "```bash gh secret set CLOUDFLOW_API_KEY ``` ### Step 3: Add deployment step Add the CloudFlow deployment action to your workflow. > **Note:** Use the official cloudflow/deploy-action@v2 ### Step 4: Test the pipeline Push a commit to trigger the workflow. ```bash git push origin main ``` ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            }
          ],
          "latency_ms": 3.7967750013194745,
          "query_id": "howto_020",
          "dimension": "casual",
          "query": "cron schedule workflow cloudflow"
        },
        {
          "key_facts": [
            "cron",
            "0 9 * * MON-FRI",
            "timezone"
          ],
          "found_facts": [
            "cron",
            "0 9 * * MON-FRI",
            "timezone"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "howto_create_scheduled_workflo"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "howto_create_scheduled_workflow",
              "chunk_id": "howto_create_scheduled_workflow_fix_0",
              "content": "# How to Create Scheduled Workflow ## Prerequisites - Workflow definition ## Steps ### Step 1: Define schedule Use cron expression to set the schedule. Example: '0 9 * * MON-FRI' runs at 9 AM on weekdays. ### Step 2: Set timezone Specify timezone for the schedule. Default is UTC.",
              "score": null
            },
            {
              "doc_id": "arch_workflow_engine",
              "chunk_id": "arch_workflow_engine_fix_0",
              "content": "# Workflow Engine Architecture ## Overview The Workflow Engine is the core component responsible for executing user-defined workflows. It uses an event-driven architecture with support for parallel execution, conditional branching, and automatic retries. ## Design Principles - **Idempotency**: All operations can be safely retried without side effects - **Eventual Consistency**: State converges to consistent view within 5 seconds - **Fault Tolerance**: System continues operating despite component failures ## Components ### Scheduler Manages workflow scheduling and",
              "score": null
            },
            {
              "doc_id": "api_workflows",
              "chunk_id": "api_workflows_fix_1",
              "content": "**Parameters:** - `limit` (integer): Maximum number of results (default: 50, max: 200) - `status` (string): Filter by status: active, paused, archived **Response:** - Success: `200` - Returns array of workflow objects - Error: `400` - Bad Request **Example:** ```json {\"workflows\": [{\"id\": \"wf_123\", \"name\": \"Daily Report\", \"status\": \"active\"}]} ``` ### POST /workflows Create a new workflow.",
              "score": null
            },
            {
              "doc_id": "config_workflow_engine",
              "chunk_id": "config_workflow_engine_fix_0",
              "content": "# Workflow Engine Configuration Reference ## Overview This document describes all configuration options for Workflow Engine. ## Environment Variables | Variable | Type | Default | Description | |----------|------|---------|-------------| | `WORKFLOW_MAX_STEPS` | integer | 100 | Maximum steps per workflow | | `WORKFLOW_TIMEOUT_SECONDS` | integer | 3600 | Maximum workflow execution time | | `WORKFLOW_RETRY_LIMIT` | integer | 3 | Default retry attempts for failed steps | | `WORKFLOW_CONCURRENT_LIMIT` | integer | 10 | Maximum",
              "score": null
            },
            {
              "doc_id": "howto_create_scheduled_workflow",
              "chunk_id": "howto_create_scheduled_workflow_fix_1",
              "content": "> **Note:** Use IANA timezone names like America/New_York ### Step 3: Configure overlap handling Choose behavior when previous execution is still running: skip, queue, or cancel. ### Step 4: Enable notifications Get notified on schedule failures. ```bash cloudflow workflows notify --on-failure ``` ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            }
          ],
          "latency_ms": 3.4960240009240806,
          "query_id": "howto_020",
          "dimension": "contextual",
          "query": "building a daily report generation workflow that needs to run at a specific time each day"
        },
        {
          "key_facts": [
            "cron",
            "0 9 * * MON-FRI",
            "timezone"
          ],
          "found_facts": [
            "cron",
            "0 9 * * MON-FRI",
            "timezone"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "howto_create_scheduled_workflo"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "howto_create_scheduled_workflow",
              "chunk_id": "howto_create_scheduled_workflow_fix_0",
              "content": "# How to Create Scheduled Workflow ## Prerequisites - Workflow definition ## Steps ### Step 1: Define schedule Use cron expression to set the schedule. Example: '0 9 * * MON-FRI' runs at 9 AM on weekdays. ### Step 2: Set timezone Specify timezone for the schedule. Default is UTC.",
              "score": null
            },
            {
              "doc_id": "howto_create_scheduled_workflow",
              "chunk_id": "howto_create_scheduled_workflow_fix_1",
              "content": "> **Note:** Use IANA timezone names like America/New_York ### Step 3: Configure overlap handling Choose behavior when previous execution is still running: skip, queue, or cancel. ### Step 4: Enable notifications Get notified on schedule failures. ```bash cloudflow workflows notify --on-failure ``` ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            },
            {
              "doc_id": "arch_workflow_engine",
              "chunk_id": "arch_workflow_engine_fix_0",
              "content": "# Workflow Engine Architecture ## Overview The Workflow Engine is the core component responsible for executing user-defined workflows. It uses an event-driven architecture with support for parallel execution, conditional branching, and automatic retries. ## Design Principles - **Idempotency**: All operations can be safely retried without side effects - **Eventual Consistency**: State converges to consistent view within 5 seconds - **Fault Tolerance**: System continues operating despite component failures ## Components ### Scheduler Manages workflow scheduling and",
              "score": null
            },
            {
              "doc_id": "howto_set_up_data_encryption",
              "chunk_id": "howto_set_up_data_encryption_fix_1",
              "content": "``` ### Step 3: Enable field-level encryption Encrypt sensitive fields within your workflow data. ```bash cloudflow encryption add-field --path $.user.ssn --key-alias sensitive-data ``` ### Step 4: Verify encryption Confirm encryption is active for your workspace. ```bash cloudflow encryption status ``` ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            },
            {
              "doc_id": "api_executions",
              "chunk_id": "api_executions_fix_0",
              "content": "# Executions API Reference ## Overview The Executions API provides programmatic access to executions functionality. Base URL: `https://api.cloudflow.io/v1/executions` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /executions List workflow executions.",
              "score": null
            }
          ],
          "latency_ms": 4.8133790005522314,
          "query_id": "howto_020",
          "dimension": "negation",
          "query": "why isn't my workflow running at the scheduled time I configured?"
        },
        {
          "key_facts": [
            "SAML 2.0",
            "OpenID Connect",
            "ACS URL"
          ],
          "found_facts": [
            "SAML 2.0",
            "OpenID Connect",
            "ACS URL"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "howto_configure_single_sign-on"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "howto_configure_single_sign_on",
              "chunk_id": "howto_configure_single_sign_on_fix_0",
              "content": "# How to Configure Single Sign-On ## Prerequisites - Admin access - Enterprise plan - Identity provider access ## Steps ### Step 1: Choose provider CloudFlow supports SAML 2.0, OpenID Connect, and OAuth 2.0 providers. ### Step 2: Get metadata Download CloudFlow's SAML metadata for your IdP configuration. ```bash cloudflow sso metadata --format xml > cloudflow-metadata.xml ``` ### Step 3: Configure IdP Add CloudFlow as a service provider in your identity provider.",
              "score": null
            },
            {
              "doc_id": "arch_authentication_service",
              "chunk_id": "arch_authentication_service_fix_1",
              "content": "**Technology:** Custom Go service **Scaling:** Horizontal stateless ### Token Service Issues and validates access tokens. Token lifetime is 1 hour with 7-day refresh tokens. **Technology:** JWT with RS256 **Scaling:** Horizontal stateless ### SSO Gateway Integrates with external identity providers. Supports Google, Microsoft, Okta, and custom SAML.",
              "score": null
            },
            {
              "doc_id": "howto_configure_single_sign_on",
              "chunk_id": "howto_configure_single_sign_on_fix_1",
              "content": "> **Note:** ACS URL: https://auth.cloudflow.io/saml/callback ### Step 4: Upload IdP metadata Provide your IdP's metadata to CloudFlow. ```bash cloudflow sso configure --idp-metadata ./idp-metadata.xml ``` ### Step 5: Test login Verify SSO login works correctly. ```bash cloudflow sso test ``` ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            },
            {
              "doc_id": "config_database",
              "chunk_id": "config_database_fix_1",
              "content": "`DATABASE_SSL_MODE` | string | require | SSL mode: disable, require, verify-full | | `DATABASE_MIGRATION_AUTO` | boolean | false | Run migrations on startup | ## Configuration File Configuration can also be provided via `database.yaml`: ```yaml database_url: required database_pool_min: 5 database_pool_max: 20 database_statement_timeout: 30000 database_ssl_mode: require ``` ## Validation Configuration is validated at startup. Invalid configuration will prevent the service from starting.",
              "score": null
            },
            {
              "doc_id": "adr_003",
              "chunk_id": "adr_003_fix_2",
              "content": "**Pros:** - Very simple - Low overhead - Easy to implement **Cons:** - No user context - Hard to rotate - No SSO support ## Decision We will implement OAuth 2.0 with JWT tokens. Access tokens expire after 1 hour, refresh tokens after 7 days. ## Consequences - Implement token rotation on refresh - Support API keys for machine-to-machine auth - Add MFA for admin accounts",
              "score": null
            }
          ],
          "latency_ms": 4.918324999380275,
          "query_id": "howto_021",
          "dimension": "original",
          "query": "How do I configure SSO?"
        },
        {
          "key_facts": [
            "SAML 2.0",
            "OpenID Connect",
            "ACS URL"
          ],
          "found_facts": [
            "SAML 2.0",
            "OpenID Connect",
            "ACS URL"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "howto_configure_single_sign-on"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "howto_configure_single_sign_on",
              "chunk_id": "howto_configure_single_sign_on_fix_0",
              "content": "# How to Configure Single Sign-On ## Prerequisites - Admin access - Enterprise plan - Identity provider access ## Steps ### Step 1: Choose provider CloudFlow supports SAML 2.0, OpenID Connect, and OAuth 2.0 providers. ### Step 2: Get metadata Download CloudFlow's SAML metadata for your IdP configuration. ```bash cloudflow sso metadata --format xml > cloudflow-metadata.xml ``` ### Step 3: Configure IdP Add CloudFlow as a service provider in your identity provider.",
              "score": null
            },
            {
              "doc_id": "api_integrations",
              "chunk_id": "api_integrations_fix_0",
              "content": "# Integrations API Reference ## Overview The Integrations API provides programmatic access to integrations functionality. Base URL: `https://api.cloudflow.io/v1/integrations` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /integrations List available integrations.",
              "score": null
            },
            {
              "doc_id": "api_users",
              "chunk_id": "api_users_fix_0",
              "content": "# Users API Reference ## Overview The Users API provides programmatic access to users functionality. Base URL: `https://api.cloudflow.io/v1/users` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /users/me Get current user profile.",
              "score": null
            },
            {
              "doc_id": "howto_configure_single_sign_on",
              "chunk_id": "howto_configure_single_sign_on_fix_1",
              "content": "> **Note:** ACS URL: https://auth.cloudflow.io/saml/callback ### Step 4: Upload IdP metadata Provide your IdP's metadata to CloudFlow. ```bash cloudflow sso configure --idp-metadata ./idp-metadata.xml ``` ### Step 5: Test login Verify SSO login works correctly. ```bash cloudflow sso test ``` ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            },
            {
              "doc_id": "api_secrets",
              "chunk_id": "api_secrets_fix_0",
              "content": "# Secrets API Reference ## Overview The Secrets API provides programmatic access to secrets functionality. Base URL: `https://api.cloudflow.io/v1/secrets` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /secrets List secret names (values not returned).",
              "score": null
            }
          ],
          "latency_ms": 4.330939998908434,
          "query_id": "howto_021",
          "dimension": "synonym",
          "query": "How do I set up single sign-on authentication for CloudFlow?"
        },
        {
          "key_facts": [
            "SAML 2.0",
            "OpenID Connect",
            "ACS URL"
          ],
          "found_facts": [
            "SAML 2.0",
            "OpenID Connect",
            "ACS URL"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "howto_configure_single_sign-on"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "howto_configure_single_sign_on",
              "chunk_id": "howto_configure_single_sign_on_fix_0",
              "content": "# How to Configure Single Sign-On ## Prerequisites - Admin access - Enterprise plan - Identity provider access ## Steps ### Step 1: Choose provider CloudFlow supports SAML 2.0, OpenID Connect, and OAuth 2.0 providers. ### Step 2: Get metadata Download CloudFlow's SAML metadata for your IdP configuration. ```bash cloudflow sso metadata --format xml > cloudflow-metadata.xml ``` ### Step 3: Configure IdP Add CloudFlow as a service provider in your identity provider.",
              "score": null
            },
            {
              "doc_id": "howto_configure_single_sign_on",
              "chunk_id": "howto_configure_single_sign_on_fix_1",
              "content": "> **Note:** ACS URL: https://auth.cloudflow.io/saml/callback ### Step 4: Upload IdP metadata Provide your IdP's metadata to CloudFlow. ```bash cloudflow sso configure --idp-metadata ./idp-metadata.xml ``` ### Step 5: Test login Verify SSO login works correctly. ```bash cloudflow sso test ``` ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            },
            {
              "doc_id": "api_teams",
              "chunk_id": "api_teams_fix_0",
              "content": "# Teams API Reference ## Overview The Teams API provides programmatic access to teams functionality. Base URL: `https://api.cloudflow.io/v1/teams` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /teams List teams the user belongs to.",
              "score": null
            },
            {
              "doc_id": "api_users",
              "chunk_id": "api_users_fix_0",
              "content": "# Users API Reference ## Overview The Users API provides programmatic access to users functionality. Base URL: `https://api.cloudflow.io/v1/users` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /users/me Get current user profile.",
              "score": null
            },
            {
              "doc_id": "api_integrations",
              "chunk_id": "api_integrations_fix_0",
              "content": "# Integrations API Reference ## Overview The Integrations API provides programmatic access to integrations functionality. Base URL: `https://api.cloudflow.io/v1/integrations` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /integrations List available integrations.",
              "score": null
            }
          ],
          "latency_ms": 4.115608999200049,
          "query_id": "howto_021",
          "dimension": "problem",
          "query": "I want my team to log into CloudFlow using our company's Okta/Azure AD credentials"
        },
        {
          "key_facts": [
            "SAML 2.0",
            "OpenID Connect",
            "ACS URL"
          ],
          "found_facts": [
            "SAML 2.0",
            "OpenID Connect",
            "ACS URL"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "howto_configure_single_sign-on"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "howto_configure_single_sign_on",
              "chunk_id": "howto_configure_single_sign_on_fix_1",
              "content": "> **Note:** ACS URL: https://auth.cloudflow.io/saml/callback ### Step 4: Upload IdP metadata Provide your IdP's metadata to CloudFlow. ```bash cloudflow sso configure --idp-metadata ./idp-metadata.xml ``` ### Step 5: Test login Verify SSO login works correctly. ```bash cloudflow sso test ``` ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            },
            {
              "doc_id": "howto_configure_single_sign_on",
              "chunk_id": "howto_configure_single_sign_on_fix_0",
              "content": "# How to Configure Single Sign-On ## Prerequisites - Admin access - Enterprise plan - Identity provider access ## Steps ### Step 1: Choose provider CloudFlow supports SAML 2.0, OpenID Connect, and OAuth 2.0 providers. ### Step 2: Get metadata Download CloudFlow's SAML metadata for your IdP configuration. ```bash cloudflow sso metadata --format xml > cloudflow-metadata.xml ``` ### Step 3: Configure IdP Add CloudFlow as a service provider in your identity provider.",
              "score": null
            },
            {
              "doc_id": "howto_configure_custom_domain",
              "chunk_id": "howto_configure_custom_domain_fix_1",
              "content": "> **Note:** CNAME should point to ingress.cloudflow.io ### Step 3: Enable SSL CloudFlow automatically provisions SSL certificates via Let's Encrypt. ```bash cloudflow domains ssl --domain app.example.com ``` ### Step 4: Set as primary Make this the primary domain for your application. ```bash cloudflow domains set-primary --domain app.example.com ``` ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            },
            {
              "doc_id": "howto_configure_custom_domain",
              "chunk_id": "howto_configure_custom_domain_fix_0",
              "content": "# How to Configure Custom Domain ## Prerequisites - Domain with DNS access - CloudFlow Pro plan or higher ## Steps ### Step 1: Verify domain ownership Add a TXT record to verify domain ownership. ```bash cloudflow domains verify --domain app.example.com ``` ### Step 2: Configure DNS Add a CNAME record pointing to your CloudFlow endpoint.",
              "score": null
            },
            {
              "doc_id": "howto_migrate_from_v1_to_v2_api",
              "chunk_id": "howto_migrate_from_v1_to_v2_api_fix_1",
              "content": "> **Note:** Generate new credentials in the dashboard ### Step 4: Test in staging Run your integration tests against the v2 staging endpoint. ```bash CLOUDFLOW_API_URL=https://api-v2-staging.cloudflow.io npm test ``` ### Step 5: Switch production Update production configuration to use v2 API. ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            }
          ],
          "latency_ms": 3.879829000652535,
          "query_id": "howto_021",
          "dimension": "casual",
          "query": "cloudflow sso setup"
        },
        {
          "key_facts": [
            "SAML 2.0",
            "OpenID Connect",
            "ACS URL"
          ],
          "found_facts": [
            "SAML 2.0",
            "OpenID Connect",
            "ACS URL"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "howto_configure_single_sign-on"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "howto_configure_single_sign_on",
              "chunk_id": "howto_configure_single_sign_on_fix_0",
              "content": "# How to Configure Single Sign-On ## Prerequisites - Admin access - Enterprise plan - Identity provider access ## Steps ### Step 1: Choose provider CloudFlow supports SAML 2.0, OpenID Connect, and OAuth 2.0 providers. ### Step 2: Get metadata Download CloudFlow's SAML metadata for your IdP configuration. ```bash cloudflow sso metadata --format xml > cloudflow-metadata.xml ``` ### Step 3: Configure IdP Add CloudFlow as a service provider in your identity provider.",
              "score": null
            },
            {
              "doc_id": "howto_configure_single_sign_on",
              "chunk_id": "howto_configure_single_sign_on_fix_1",
              "content": "> **Note:** ACS URL: https://auth.cloudflow.io/saml/callback ### Step 4: Upload IdP metadata Provide your IdP's metadata to CloudFlow. ```bash cloudflow sso configure --idp-metadata ./idp-metadata.xml ``` ### Step 5: Test login Verify SSO login works correctly. ```bash cloudflow sso test ``` ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            },
            {
              "doc_id": "api_integrations",
              "chunk_id": "api_integrations_fix_0",
              "content": "# Integrations API Reference ## Overview The Integrations API provides programmatic access to integrations functionality. Base URL: `https://api.cloudflow.io/v1/integrations` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /integrations List available integrations.",
              "score": null
            },
            {
              "doc_id": "arch_api_gateway",
              "chunk_id": "arch_api_gateway_fix_0",
              "content": "# API Gateway Architecture ## Overview The API Gateway handles all incoming HTTP requests, providing authentication, rate limiting, and request routing. Built on Kong with custom plugins for CloudFlow-specific functionality. ## Design Principles - **Zero Trust**: Every request is authenticated and authorized - **Defense in Depth**: Multiple layers of security validation ## Components ### Kong Gateway Primary ingress controller. Handles TLS termination, request routing, and plugin execution.",
              "score": null
            },
            {
              "doc_id": "arch_authentication_service",
              "chunk_id": "arch_authentication_service_fix_0",
              "content": "# Authentication Service Architecture ## Overview The Authentication Service handles user identity, session management, and token issuance. Supports OAuth 2.0, SAML, and API key authentication. ## Design Principles - **Zero Knowledge**: Passwords never stored in plaintext - **Token Rotation**: Refresh tokens rotate on each use ## Components ### Identity Provider Manages user accounts and credentials. Password hashing uses bcrypt with cost factor 12.",
              "score": null
            }
          ],
          "latency_ms": 4.876506000073277,
          "query_id": "howto_021",
          "dimension": "contextual",
          "query": "enterprise security requires us to integrate CloudFlow with our identity provider for centralized authentication"
        },
        {
          "key_facts": [
            "SAML 2.0",
            "OpenID Connect",
            "ACS URL"
          ],
          "found_facts": [
            "ACS URL"
          ],
          "missed_facts": [
            "SAML 2.0",
            "OpenID Connect"
          ],
          "coverage": 0.3333333333333333,
          "expected_docs": [
            "howto_configure_single_sign-on"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "arch_authentication_service",
              "chunk_id": "arch_authentication_service_fix_0",
              "content": "# Authentication Service Architecture ## Overview The Authentication Service handles user identity, session management, and token issuance. Supports OAuth 2.0, SAML, and API key authentication. ## Design Principles - **Zero Knowledge**: Passwords never stored in plaintext - **Token Rotation**: Refresh tokens rotate on each use ## Components ### Identity Provider Manages user accounts and credentials. Password hashing uses bcrypt with cost factor 12.",
              "score": null
            },
            {
              "doc_id": "adr_003",
              "chunk_id": "adr_003_fix_0",
              "content": "# ADR-003: Implement OAuth 2.0 with JWT for Authentication **Status:** Accepted **Date:** 2024-05-08 ## Context We need a secure authentication system that supports API keys, user sessions, and third-party integrations. ## Options Considered ### Option 1: OAuth 2.0 with JWT Industry standard token-based auth.",
              "score": null
            },
            {
              "doc_id": "api_users",
              "chunk_id": "api_users_fix_0",
              "content": "# Users API Reference ## Overview The Users API provides programmatic access to users functionality. Base URL: `https://api.cloudflow.io/v1/users` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /users/me Get current user profile.",
              "score": null
            },
            {
              "doc_id": "adr_003",
              "chunk_id": "adr_003_fix_2",
              "content": "**Pros:** - Very simple - Low overhead - Easy to implement **Cons:** - No user context - Hard to rotate - No SSO support ## Decision We will implement OAuth 2.0 with JWT tokens. Access tokens expire after 1 hour, refresh tokens after 7 days. ## Consequences - Implement token rotation on refresh - Support API keys for machine-to-machine auth - Add MFA for admin accounts",
              "score": null
            },
            {
              "doc_id": "howto_configure_single_sign_on",
              "chunk_id": "howto_configure_single_sign_on_fix_1",
              "content": "> **Note:** ACS URL: https://auth.cloudflow.io/saml/callback ### Step 4: Upload IdP metadata Provide your IdP's metadata to CloudFlow. ```bash cloudflow sso configure --idp-metadata ./idp-metadata.xml ``` ### Step 5: Test login Verify SSO login works correctly. ```bash cloudflow sso test ``` ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            }
          ],
          "latency_ms": 3.7711169989051996,
          "query_id": "howto_021",
          "dimension": "negation",
          "query": "why can't my users log in with their corporate credentials?"
        },
        {
          "key_facts": [
            "cloudflow executions get",
            "cloudflow executions logs",
            "--debug"
          ],
          "found_facts": [
            "cloudflow executions get",
            "cloudflow executions logs",
            "--debug"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "howto_debug_failed_execution"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "howto_debug_failed_execution",
              "chunk_id": "howto_debug_failed_execution_fix_0",
              "content": "# How to Debug Failed Execution ## Prerequisites - Failed execution ID ## Steps ### Step 1: Get execution details Retrieve the full execution record including step outputs. ```bash cloudflow executions get ex_123 --include-steps ``` ### Step 2: View step logs Check logs for the failed step. ```bash cloudflow executions logs ex_123 --step 3 ``` ### Step 3: Inspect input/output Review the data passed between steps.",
              "score": null
            },
            {
              "doc_id": "howto_debug_failed_execution",
              "chunk_id": "howto_debug_failed_execution_fix_1",
              "content": "```bash cloudflow executions inspect ex_123 --step 3 ``` ### Step 4: Enable debug mode Re-run with verbose logging enabled. ```bash cloudflow workflows execute wf_123 --debug ``` ### Step 5: Check error patterns Look for similar failures in recent executions. ```bash cloudflow executions list --status failed --since 24h ``` ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            },
            {
              "doc_id": "howto_implement_retry_logic",
              "chunk_id": "howto_implement_retry_logic_fix_1",
              "content": "### Step 4: Handle final failure Configure on_failure handler to notify or escalate when all retries are exhausted. ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            },
            {
              "doc_id": "api_executions",
              "chunk_id": "api_executions_fix_2",
              "content": "**Parameters:** - `id` (string): Execution ID - `reason` (string): Cancellation reason (optional) **Response:** - Success: `200` - Execution cancelled - Error: `400` - Bad Request **Example:** ```json {\"id\": \"ex_789\", \"status\": \"cancelled\", \"cancelled_at\": \"2024-01-15T10:05:00Z\"} ```",
              "score": null
            },
            {
              "doc_id": "runbook_database_connection_exhaustion",
              "chunk_id": "runbook_database_connection_exhaustion_fix_0",
              "content": "# Runbook: Database Connection Exhaustion ## Severity **Level:** P1 - Critical **Response Time:** 15 minutes ## Symptoms - Connection pool at 100% - Queries timing out - 'too many connections' errors in logs ## Diagnosis ### Step 1 Check active connections: `SELECT count(*) FROM pg_stat_activity` ### Step 2 Identify long-running queries: `SELECT pid, now() - pg_stat_activity.query_start AS duration, query FROM pg_stat_activity WHERE state != 'idle' ORDER BY duration DESC` ### Step 3 Check PgBouncer stats:",
              "score": null
            }
          ],
          "latency_ms": 3.2912529986788286,
          "query_id": "howto_022",
          "dimension": "original",
          "query": "How do I debug a failed execution?"
        },
        {
          "key_facts": [
            "cloudflow executions get",
            "cloudflow executions logs",
            "--debug"
          ],
          "found_facts": [
            "cloudflow executions get",
            "cloudflow executions logs",
            "--debug"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "howto_debug_failed_execution"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "howto_debug_failed_execution",
              "chunk_id": "howto_debug_failed_execution_fix_1",
              "content": "```bash cloudflow executions inspect ex_123 --step 3 ``` ### Step 4: Enable debug mode Re-run with verbose logging enabled. ```bash cloudflow workflows execute wf_123 --debug ``` ### Step 5: Check error patterns Look for similar failures in recent executions. ```bash cloudflow executions list --status failed --since 24h ``` ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            },
            {
              "doc_id": "howto_debug_failed_execution",
              "chunk_id": "howto_debug_failed_execution_fix_0",
              "content": "# How to Debug Failed Execution ## Prerequisites - Failed execution ID ## Steps ### Step 1: Get execution details Retrieve the full execution record including step outputs. ```bash cloudflow executions get ex_123 --include-steps ``` ### Step 2: View step logs Check logs for the failed step. ```bash cloudflow executions logs ex_123 --step 3 ``` ### Step 3: Inspect input/output Review the data passed between steps.",
              "score": null
            },
            {
              "doc_id": "howto_implement_retry_logic",
              "chunk_id": "howto_implement_retry_logic_fix_1",
              "content": "### Step 4: Handle final failure Configure on_failure handler to notify or escalate when all retries are exhausted. ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            },
            {
              "doc_id": "howto_implement_workflow_versioning",
              "chunk_id": "howto_implement_workflow_versioning_fix_1",
              "content": "```bash cloudflow workflows activate wf_123 --version 2.0.0 ``` ### Step 4: Roll back if needed Quickly revert to a previous version. ```bash cloudflow workflows rollback wf_123 --to-version 1.0.0 ``` ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            },
            {
              "doc_id": "howto_set_up_monitoring_alerts",
              "chunk_id": "howto_set_up_monitoring_alerts_fix_1",
              "content": "``` ### Step 3: Set severity levels Assign severity to each alert: critical, warning, or info. > **Note:** Critical alerts bypass quiet hours ### Step 4: Test alerts Trigger a test alert to verify routing. ```bash cloudflow alerts test --name 'High Error Rate' ``` ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            }
          ],
          "latency_ms": 3.801794000537484,
          "query_id": "howto_022",
          "dimension": "synonym",
          "query": "How do I troubleshoot a workflow that errored out?"
        },
        {
          "key_facts": [
            "cloudflow executions get",
            "cloudflow executions logs",
            "--debug"
          ],
          "found_facts": [
            "cloudflow executions get",
            "cloudflow executions logs",
            "--debug"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "howto_debug_failed_execution"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "howto_debug_failed_execution",
              "chunk_id": "howto_debug_failed_execution_fix_0",
              "content": "# How to Debug Failed Execution ## Prerequisites - Failed execution ID ## Steps ### Step 1: Get execution details Retrieve the full execution record including step outputs. ```bash cloudflow executions get ex_123 --include-steps ``` ### Step 2: View step logs Check logs for the failed step. ```bash cloudflow executions logs ex_123 --step 3 ``` ### Step 3: Inspect input/output Review the data passed between steps.",
              "score": null
            },
            {
              "doc_id": "howto_implement_retry_logic",
              "chunk_id": "howto_implement_retry_logic_fix_1",
              "content": "### Step 4: Handle final failure Configure on_failure handler to notify or escalate when all retries are exhausted. ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            },
            {
              "doc_id": "howto_debug_failed_execution",
              "chunk_id": "howto_debug_failed_execution_fix_1",
              "content": "```bash cloudflow executions inspect ex_123 --step 3 ``` ### Step 4: Enable debug mode Re-run with verbose logging enabled. ```bash cloudflow workflows execute wf_123 --debug ``` ### Step 5: Check error patterns Look for similar failures in recent executions. ```bash cloudflow executions list --status failed --since 24h ``` ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            },
            {
              "doc_id": "api_workflows",
              "chunk_id": "api_workflows_fix_2",
              "content": "**Parameters:** - `name` (string): Workflow name (required) - `definition` (object): Workflow definition in JSON format **Response:** - Success: `201` - Workflow created successfully - Error: `400` - Bad Request **Example:** ```json {\"id\": \"wf_456\", \"name\": \"New Workflow\", \"created_at\": \"2024-01-15T10:00:00Z\"} ``` ### DELETE /workflows/{id} Delete a workflow permanently. **Parameters:** - `id` (string): Workflow ID (required) **Response:** - Success: `204` - Workflow deleted - Error: `404` - Workflow not found **Example:** ```json {} ```",
              "score": null
            },
            {
              "doc_id": "howto_implement_retry_logic",
              "chunk_id": "howto_implement_retry_logic_fix_0",
              "content": "# How to Implement Retry Logic ## Prerequisites - Existing workflow ## Steps ### Step 1: Define retry policy Add retry configuration to your workflow step. > **Note:** Maximum 5 retry attempts allowed ### Step 2: Configure backoff Set exponential backoff with base delay of 1 second. The formula is: delay = base * 2^attempt. ### Step 3: Add timeout Set maximum execution time to prevent infinite loops. Default timeout is 30 seconds.",
              "score": null
            }
          ],
          "latency_ms": 4.312265000407933,
          "query_id": "howto_022",
          "dimension": "problem",
          "query": "my workflow failed and I need to figure out which step caused the error and why"
        },
        {
          "key_facts": [
            "cloudflow executions get",
            "cloudflow executions logs",
            "--debug"
          ],
          "found_facts": [
            "cloudflow executions get",
            "cloudflow executions logs",
            "--debug"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "howto_debug_failed_execution"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "howto_debug_failed_execution",
              "chunk_id": "howto_debug_failed_execution_fix_0",
              "content": "# How to Debug Failed Execution ## Prerequisites - Failed execution ID ## Steps ### Step 1: Get execution details Retrieve the full execution record including step outputs. ```bash cloudflow executions get ex_123 --include-steps ``` ### Step 2: View step logs Check logs for the failed step. ```bash cloudflow executions logs ex_123 --step 3 ``` ### Step 3: Inspect input/output Review the data passed between steps.",
              "score": null
            },
            {
              "doc_id": "howto_debug_failed_execution",
              "chunk_id": "howto_debug_failed_execution_fix_1",
              "content": "```bash cloudflow executions inspect ex_123 --step 3 ``` ### Step 4: Enable debug mode Re-run with verbose logging enabled. ```bash cloudflow workflows execute wf_123 --debug ``` ### Step 5: Check error patterns Look for similar failures in recent executions. ```bash cloudflow executions list --status failed --since 24h ``` ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            },
            {
              "doc_id": "release_3_0_0",
              "chunk_id": "release_3_0_0_fix_1",
              "content": "```bash cloudflow upgrade --version 3.0.0 ``` ## Known Issues None at this time.",
              "score": null
            },
            {
              "doc_id": "howto_implement_retry_logic",
              "chunk_id": "howto_implement_retry_logic_fix_1",
              "content": "### Step 4: Handle final failure Configure on_failure handler to notify or escalate when all retries are exhausted. ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            },
            {
              "doc_id": "howto_migrate_from_v1_to_v2_api",
              "chunk_id": "howto_migrate_from_v1_to_v2_api_fix_1",
              "content": "> **Note:** Generate new credentials in the dashboard ### Step 4: Test in staging Run your integration tests against the v2 staging endpoint. ```bash CLOUDFLOW_API_URL=https://api-v2-staging.cloudflow.io npm test ``` ### Step 5: Switch production Update production configuration to use v2 API. ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            }
          ],
          "latency_ms": 4.672836999816354,
          "query_id": "howto_022",
          "dimension": "casual",
          "query": "cloudflow execution logs debug failed"
        },
        {
          "key_facts": [
            "cloudflow executions get",
            "cloudflow executions logs",
            "--debug"
          ],
          "found_facts": [
            "cloudflow executions get",
            "cloudflow executions logs",
            "--debug"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "howto_debug_failed_execution"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "howto_debug_failed_execution",
              "chunk_id": "howto_debug_failed_execution_fix_1",
              "content": "```bash cloudflow executions inspect ex_123 --step 3 ``` ### Step 4: Enable debug mode Re-run with verbose logging enabled. ```bash cloudflow workflows execute wf_123 --debug ``` ### Step 5: Check error patterns Look for similar failures in recent executions. ```bash cloudflow executions list --status failed --since 24h ``` ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            },
            {
              "doc_id": "howto_debug_failed_execution",
              "chunk_id": "howto_debug_failed_execution_fix_0",
              "content": "# How to Debug Failed Execution ## Prerequisites - Failed execution ID ## Steps ### Step 1: Get execution details Retrieve the full execution record including step outputs. ```bash cloudflow executions get ex_123 --include-steps ``` ### Step 2: View step logs Check logs for the failed step. ```bash cloudflow executions logs ex_123 --step 3 ``` ### Step 3: Inspect input/output Review the data passed between steps.",
              "score": null
            },
            {
              "doc_id": "howto_implement_retry_logic",
              "chunk_id": "howto_implement_retry_logic_fix_1",
              "content": "### Step 4: Handle final failure Configure on_failure handler to notify or escalate when all retries are exhausted. ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            },
            {
              "doc_id": "howto_implement_workflow_versioning",
              "chunk_id": "howto_implement_workflow_versioning_fix_1",
              "content": "```bash cloudflow workflows activate wf_123 --version 2.0.0 ``` ### Step 4: Roll back if needed Quickly revert to a previous version. ```bash cloudflow workflows rollback wf_123 --to-version 1.0.0 ``` ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            },
            {
              "doc_id": "api_workflows",
              "chunk_id": "api_workflows_fix_2",
              "content": "**Parameters:** - `name` (string): Workflow name (required) - `definition` (object): Workflow definition in JSON format **Response:** - Success: `201` - Workflow created successfully - Error: `400` - Bad Request **Example:** ```json {\"id\": \"wf_456\", \"name\": \"New Workflow\", \"created_at\": \"2024-01-15T10:00:00Z\"} ``` ### DELETE /workflows/{id} Delete a workflow permanently. **Parameters:** - `id` (string): Workflow ID (required) **Response:** - Success: `204` - Workflow deleted - Error: `404` - Workflow not found **Example:** ```json {} ```",
              "score": null
            }
          ],
          "latency_ms": 4.001526000138256,
          "query_id": "howto_022",
          "dimension": "contextual",
          "query": "production workflow started failing last night and I need to investigate what went wrong"
        },
        {
          "key_facts": [
            "cloudflow executions get",
            "cloudflow executions logs",
            "--debug"
          ],
          "found_facts": [
            "cloudflow executions get",
            "cloudflow executions logs",
            "--debug"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "howto_debug_failed_execution"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "howto_debug_failed_execution",
              "chunk_id": "howto_debug_failed_execution_fix_0",
              "content": "# How to Debug Failed Execution ## Prerequisites - Failed execution ID ## Steps ### Step 1: Get execution details Retrieve the full execution record including step outputs. ```bash cloudflow executions get ex_123 --include-steps ``` ### Step 2: View step logs Check logs for the failed step. ```bash cloudflow executions logs ex_123 --step 3 ``` ### Step 3: Inspect input/output Review the data passed between steps.",
              "score": null
            },
            {
              "doc_id": "howto_debug_failed_execution",
              "chunk_id": "howto_debug_failed_execution_fix_1",
              "content": "```bash cloudflow executions inspect ex_123 --step 3 ``` ### Step 4: Enable debug mode Re-run with verbose logging enabled. ```bash cloudflow workflows execute wf_123 --debug ``` ### Step 5: Check error patterns Look for similar failures in recent executions. ```bash cloudflow executions list --status failed --since 24h ``` ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            },
            {
              "doc_id": "howto_implement_retry_logic",
              "chunk_id": "howto_implement_retry_logic_fix_1",
              "content": "### Step 4: Handle final failure Configure on_failure handler to notify or escalate when all retries are exhausted. ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            },
            {
              "doc_id": "howto_implement_workflow_versioning",
              "chunk_id": "howto_implement_workflow_versioning_fix_1",
              "content": "```bash cloudflow workflows activate wf_123 --version 2.0.0 ``` ### Step 4: Roll back if needed Quickly revert to a previous version. ```bash cloudflow workflows rollback wf_123 --to-version 1.0.0 ``` ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            },
            {
              "doc_id": "api_workflows",
              "chunk_id": "api_workflows_fix_1",
              "content": "**Parameters:** - `limit` (integer): Maximum number of results (default: 50, max: 200) - `status` (string): Filter by status: active, paused, archived **Response:** - Success: `200` - Returns array of workflow objects - Error: `400` - Bad Request **Example:** ```json {\"workflows\": [{\"id\": \"wf_123\", \"name\": \"Daily Report\", \"status\": \"active\"}]} ``` ### POST /workflows Create a new workflow.",
              "score": null
            }
          ],
          "latency_ms": 3.391800000827061,
          "query_id": "howto_022",
          "dimension": "negation",
          "query": "why did my workflow fail and where can I see what happened?"
        },
        {
          "key_facts": [
            "PostgreSQL",
            "MongoDB",
            "CockroachDB",
            "JSONB",
            "Team expertise"
          ],
          "found_facts": [
            "PostgreSQL",
            "MongoDB",
            "CockroachDB",
            "JSONB",
            "Team expertise"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "adr_001"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "adr_001",
              "chunk_id": "adr_001_fix_2",
              "content": "**Pros:** - Automatic sharding - Strong consistency - PostgreSQL compatible **Cons:** - Higher latency - More expensive - Less mature ecosystem ## Decision We will use PostgreSQL as our primary database with JSONB for flexible workflow definitions. ## Consequences - Need PgBouncer for connection pooling - Must plan for sharding at 100M+ records - Requires regular VACUUM maintenance",
              "score": null
            },
            {
              "doc_id": "adr_001",
              "chunk_id": "adr_001_fix_0",
              "content": "# ADR-001: Use PostgreSQL for Primary Database **Status:** Accepted **Date:** 2024-11-04 ## Context We need a database to store workflow definitions, execution state, and user data. The database must support ACID transactions, JSON queries, and scale to millions of records. ## Options Considered ### Option 1: PostgreSQL Mature relational database with JSONB support.",
              "score": null
            },
            {
              "doc_id": "config_database",
              "chunk_id": "config_database_fix_0",
              "content": "# Database Configuration Reference ## Overview This document describes all configuration options for Database. ## Environment Variables | Variable | Type | Default | Description | |----------|------|---------|-------------| | `DATABASE_URL` | string | required | PostgreSQL connection string | | `DATABASE_POOL_MIN` | integer | 5 | Minimum connection pool size | | `DATABASE_POOL_MAX` | integer | 20 | Maximum connection pool size | | `DATABASE_STATEMENT_TIMEOUT` | integer | 30000 | Query timeout in milliseconds | |",
              "score": null
            },
            {
              "doc_id": "adr_001",
              "chunk_id": "adr_001_fix_1",
              "content": "**Pros:** - ACID compliance - Rich JSONB queries - Team expertise - Proven at scale **Cons:** - Manual sharding for extreme scale - Requires connection pooling ### Option 2: MongoDB Document database with native JSON. **Pros:** - Flexible schema - Built-in sharding - Good for documents **Cons:** - Weaker consistency guarantees - Complex transactions - Higher operational cost ### Option 3: CockroachDB Distributed SQL database.",
              "score": null
            },
            {
              "doc_id": "howto_set_up_database_connection",
              "chunk_id": "howto_set_up_database_connection_fix_0",
              "content": "# How to Set Up Database Connection ## Prerequisites - Database with network access to CloudFlow - Connection credentials ## Steps ### Step 1: Add connection secret Store your database connection string as a secret. ```bash cloudflow secrets set DATABASE_URL postgresql://user:pass@host:5432/db ``` ### Step 2: Configure connection pool Set pool size based on your plan limits. Free tier allows 5 connections, Pro allows 20.",
              "score": null
            }
          ],
          "latency_ms": 3.4283079985470977,
          "query_id": "compare_023",
          "dimension": "original",
          "query": "What database options were considered and why was PostgreSQL chosen?"
        },
        {
          "key_facts": [
            "PostgreSQL",
            "MongoDB",
            "CockroachDB",
            "JSONB",
            "Team expertise"
          ],
          "found_facts": [
            "PostgreSQL",
            "MongoDB",
            "CockroachDB",
            "JSONB",
            "Team expertise"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "adr_001"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "adr_001",
              "chunk_id": "adr_001_fix_2",
              "content": "**Pros:** - Automatic sharding - Strong consistency - PostgreSQL compatible **Cons:** - Higher latency - More expensive - Less mature ecosystem ## Decision We will use PostgreSQL as our primary database with JSONB for flexible workflow definitions. ## Consequences - Need PgBouncer for connection pooling - Must plan for sharding at 100M+ records - Requires regular VACUUM maintenance",
              "score": null
            },
            {
              "doc_id": "adr_001",
              "chunk_id": "adr_001_fix_0",
              "content": "# ADR-001: Use PostgreSQL for Primary Database **Status:** Accepted **Date:** 2024-11-04 ## Context We need a database to store workflow definitions, execution state, and user data. The database must support ACID transactions, JSON queries, and scale to millions of records. ## Options Considered ### Option 1: PostgreSQL Mature relational database with JSONB support.",
              "score": null
            },
            {
              "doc_id": "arch_workflow_engine",
              "chunk_id": "arch_workflow_engine_fix_2",
              "content": "**Technology:** PostgreSQL with JSONB **Scaling:** Primary-replica with read replicas ## Data Flow Triggers \u2192 Scheduler \u2192 Job Queue \u2192 Executor \u2192 State Store \u2192 Webhooks/Notifications ## Performance Characteristics - **Latency P50:** 45ms - **Latency P99:** 250ms - **Throughput:** 5000 executions/minute",
              "score": null
            },
            {
              "doc_id": "adr_001",
              "chunk_id": "adr_001_fix_1",
              "content": "**Pros:** - ACID compliance - Rich JSONB queries - Team expertise - Proven at scale **Cons:** - Manual sharding for extreme scale - Requires connection pooling ### Option 2: MongoDB Document database with native JSON. **Pros:** - Flexible schema - Built-in sharding - Good for documents **Cons:** - Weaker consistency guarantees - Complex transactions - Higher operational cost ### Option 3: CockroachDB Distributed SQL database.",
              "score": null
            },
            {
              "doc_id": "arch_data_pipeline",
              "chunk_id": "arch_data_pipeline_fix_1",
              "content": "**Technology:** Apache Kafka with Flink **Scaling:** Partition-based scaling ### Batch Processor Processes large batch jobs. Supports data formats: JSON, CSV, Parquet, Avro. **Technology:** Apache Spark on Kubernetes **Scaling:** Dynamic resource allocation ### Data Lake Long-term data storage with time-travel queries. Retention configurable up to 7 years.",
              "score": null
            }
          ],
          "latency_ms": 3.5966720006399555,
          "query_id": "compare_023",
          "dimension": "synonym",
          "query": "What data stores did we evaluate before picking Postgres?"
        },
        {
          "key_facts": [
            "PostgreSQL",
            "MongoDB",
            "CockroachDB",
            "JSONB",
            "Team expertise"
          ],
          "found_facts": [
            "PostgreSQL",
            "MongoDB",
            "CockroachDB",
            "JSONB",
            "Team expertise"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "adr_001"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "adr_001",
              "chunk_id": "adr_001_fix_1",
              "content": "**Pros:** - ACID compliance - Rich JSONB queries - Team expertise - Proven at scale **Cons:** - Manual sharding for extreme scale - Requires connection pooling ### Option 2: MongoDB Document database with native JSON. **Pros:** - Flexible schema - Built-in sharding - Good for documents **Cons:** - Weaker consistency guarantees - Complex transactions - Higher operational cost ### Option 3: CockroachDB Distributed SQL database.",
              "score": null
            },
            {
              "doc_id": "adr_001",
              "chunk_id": "adr_001_fix_0",
              "content": "# ADR-001: Use PostgreSQL for Primary Database **Status:** Accepted **Date:** 2024-11-04 ## Context We need a database to store workflow definitions, execution state, and user data. The database must support ACID transactions, JSON queries, and scale to millions of records. ## Options Considered ### Option 1: PostgreSQL Mature relational database with JSONB support.",
              "score": null
            },
            {
              "doc_id": "adr_001",
              "chunk_id": "adr_001_fix_2",
              "content": "**Pros:** - Automatic sharding - Strong consistency - PostgreSQL compatible **Cons:** - Higher latency - More expensive - Less mature ecosystem ## Decision We will use PostgreSQL as our primary database with JSONB for flexible workflow definitions. ## Consequences - Need PgBouncer for connection pooling - Must plan for sharding at 100M+ records - Requires regular VACUUM maintenance",
              "score": null
            },
            {
              "doc_id": "config_database",
              "chunk_id": "config_database_fix_0",
              "content": "# Database Configuration Reference ## Overview This document describes all configuration options for Database. ## Environment Variables | Variable | Type | Default | Description | |----------|------|---------|-------------| | `DATABASE_URL` | string | required | PostgreSQL connection string | | `DATABASE_POOL_MIN` | integer | 5 | Minimum connection pool size | | `DATABASE_POOL_MAX` | integer | 20 | Maximum connection pool size | | `DATABASE_STATEMENT_TIMEOUT` | integer | 30000 | Query timeout in milliseconds | |",
              "score": null
            },
            {
              "doc_id": "arch_search_service",
              "chunk_id": "arch_search_service_fix_1",
              "content": "Supports filters, facets, and aggregations. **Technology:** Elasticsearch 8.x **Scaling:** 3-node cluster with replicas ### Suggestion Service Provides autocomplete suggestions. Returns suggestions within 20ms. **Technology:** Completion suggester **Scaling:** In-memory FST ## Data Flow Document \u2192 Analyzer \u2192 Index \u2192 Query \u2192 Rank \u2192 Results ## Performance Characteristics - **Latency P50:** 25ms - **Latency P99:** 100ms - **Throughput:** 2000 queries/second",
              "score": null
            }
          ],
          "latency_ms": 4.227477000313229,
          "query_id": "compare_023",
          "dimension": "problem",
          "query": "We're reviewing our db choice and need to document the alternatives we rejected"
        },
        {
          "key_facts": [
            "PostgreSQL",
            "MongoDB",
            "CockroachDB",
            "JSONB",
            "Team expertise"
          ],
          "found_facts": [
            "PostgreSQL",
            "MongoDB",
            "CockroachDB",
            "JSONB",
            "Team expertise"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "adr_001"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "adr_001",
              "chunk_id": "adr_001_fix_1",
              "content": "**Pros:** - ACID compliance - Rich JSONB queries - Team expertise - Proven at scale **Cons:** - Manual sharding for extreme scale - Requires connection pooling ### Option 2: MongoDB Document database with native JSON. **Pros:** - Flexible schema - Built-in sharding - Good for documents **Cons:** - Weaker consistency guarantees - Complex transactions - Higher operational cost ### Option 3: CockroachDB Distributed SQL database.",
              "score": null
            },
            {
              "doc_id": "adr_001",
              "chunk_id": "adr_001_fix_2",
              "content": "**Pros:** - Automatic sharding - Strong consistency - PostgreSQL compatible **Cons:** - Higher latency - More expensive - Less mature ecosystem ## Decision We will use PostgreSQL as our primary database with JSONB for flexible workflow definitions. ## Consequences - Need PgBouncer for connection pooling - Must plan for sharding at 100M+ records - Requires regular VACUUM maintenance",
              "score": null
            },
            {
              "doc_id": "adr_001",
              "chunk_id": "adr_001_fix_0",
              "content": "# ADR-001: Use PostgreSQL for Primary Database **Status:** Accepted **Date:** 2024-11-04 ## Context We need a database to store workflow definitions, execution state, and user data. The database must support ACID transactions, JSON queries, and scale to millions of records. ## Options Considered ### Option 1: PostgreSQL Mature relational database with JSONB support.",
              "score": null
            },
            {
              "doc_id": "arch_workflow_engine",
              "chunk_id": "arch_workflow_engine_fix_2",
              "content": "**Technology:** PostgreSQL with JSONB **Scaling:** Primary-replica with read replicas ## Data Flow Triggers \u2192 Scheduler \u2192 Job Queue \u2192 Executor \u2192 State Store \u2192 Webhooks/Notifications ## Performance Characteristics - **Latency P50:** 45ms - **Latency P99:** 250ms - **Throughput:** 5000 executions/minute",
              "score": null
            },
            {
              "doc_id": "adr_008",
              "chunk_id": "adr_008_fix_1",
              "content": "**Pros:** - No vendor lock-in - S3 compatible - On-premises option **Cons:** - Operational burden - Lower durability guarantees - Scaling complexity ### Option 3: Google Cloud Storage GCP object storage. **Pros:** - Strong consistency - Good performance - Unified billing **Cons:** - GCP lock-in - Fewer features than S3 - Smaller ecosystem ## Decision We will use Amazon S3 for object storage with intelligent tiering for cost optimization.",
              "score": null
            }
          ],
          "latency_ms": 3.3877320001920452,
          "query_id": "compare_023",
          "dimension": "casual",
          "query": "postgres vs mongodb vs cockroach decision"
        },
        {
          "key_facts": [
            "PostgreSQL",
            "MongoDB",
            "CockroachDB",
            "JSONB",
            "Team expertise"
          ],
          "found_facts": [
            "PostgreSQL",
            "MongoDB",
            "CockroachDB",
            "JSONB",
            "Team expertise"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "adr_001"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "adr_001",
              "chunk_id": "adr_001_fix_1",
              "content": "**Pros:** - ACID compliance - Rich JSONB queries - Team expertise - Proven at scale **Cons:** - Manual sharding for extreme scale - Requires connection pooling ### Option 2: MongoDB Document database with native JSON. **Pros:** - Flexible schema - Built-in sharding - Good for documents **Cons:** - Weaker consistency guarantees - Complex transactions - Higher operational cost ### Option 3: CockroachDB Distributed SQL database.",
              "score": null
            },
            {
              "doc_id": "adr_001",
              "chunk_id": "adr_001_fix_2",
              "content": "**Pros:** - Automatic sharding - Strong consistency - PostgreSQL compatible **Cons:** - Higher latency - More expensive - Less mature ecosystem ## Decision We will use PostgreSQL as our primary database with JSONB for flexible workflow definitions. ## Consequences - Need PgBouncer for connection pooling - Must plan for sharding at 100M+ records - Requires regular VACUUM maintenance",
              "score": null
            },
            {
              "doc_id": "adr_001",
              "chunk_id": "adr_001_fix_0",
              "content": "# ADR-001: Use PostgreSQL for Primary Database **Status:** Accepted **Date:** 2024-11-04 ## Context We need a database to store workflow definitions, execution state, and user data. The database must support ACID transactions, JSON queries, and scale to millions of records. ## Options Considered ### Option 1: PostgreSQL Mature relational database with JSONB support.",
              "score": null
            },
            {
              "doc_id": "arch_workflow_engine",
              "chunk_id": "arch_workflow_engine_fix_2",
              "content": "**Technology:** PostgreSQL with JSONB **Scaling:** Primary-replica with read replicas ## Data Flow Triggers \u2192 Scheduler \u2192 Job Queue \u2192 Executor \u2192 State Store \u2192 Webhooks/Notifications ## Performance Characteristics - **Latency P50:** 45ms - **Latency P99:** 250ms - **Throughput:** 5000 executions/minute",
              "score": null
            },
            {
              "doc_id": "adr_008",
              "chunk_id": "adr_008_fix_1",
              "content": "**Pros:** - No vendor lock-in - S3 compatible - On-premises option **Cons:** - Operational burden - Lower durability guarantees - Scaling complexity ### Option 3: Google Cloud Storage GCP object storage. **Pros:** - Strong consistency - Good performance - Unified billing **Cons:** - GCP lock-in - Fewer features than S3 - Smaller ecosystem ## Decision We will use Amazon S3 for object storage with intelligent tiering for cost optimization.",
              "score": null
            }
          ],
          "latency_ms": 3.5197689994674874,
          "query_id": "compare_023",
          "dimension": "contextual",
          "query": "New team member is asking why we use PostgreSQL instead of MongoDB - what's the rationale?"
        },
        {
          "key_facts": [
            "PostgreSQL",
            "MongoDB",
            "CockroachDB",
            "JSONB",
            "Team expertise"
          ],
          "found_facts": [
            "PostgreSQL",
            "MongoDB",
            "CockroachDB",
            "JSONB",
            "Team expertise"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "adr_001"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "adr_001",
              "chunk_id": "adr_001_fix_1",
              "content": "**Pros:** - ACID compliance - Rich JSONB queries - Team expertise - Proven at scale **Cons:** - Manual sharding for extreme scale - Requires connection pooling ### Option 2: MongoDB Document database with native JSON. **Pros:** - Flexible schema - Built-in sharding - Good for documents **Cons:** - Weaker consistency guarantees - Complex transactions - Higher operational cost ### Option 3: CockroachDB Distributed SQL database.",
              "score": null
            },
            {
              "doc_id": "adr_001",
              "chunk_id": "adr_001_fix_2",
              "content": "**Pros:** - Automatic sharding - Strong consistency - PostgreSQL compatible **Cons:** - Higher latency - More expensive - Less mature ecosystem ## Decision We will use PostgreSQL as our primary database with JSONB for flexible workflow definitions. ## Consequences - Need PgBouncer for connection pooling - Must plan for sharding at 100M+ records - Requires regular VACUUM maintenance",
              "score": null
            },
            {
              "doc_id": "adr_002",
              "chunk_id": "adr_002_fix_2",
              "content": "## Consequences - Need platform team expertise - Implement GitOps with ArgoCD - Use managed node groups for easier operations",
              "score": null
            },
            {
              "doc_id": "adr_001",
              "chunk_id": "adr_001_fix_0",
              "content": "# ADR-001: Use PostgreSQL for Primary Database **Status:** Accepted **Date:** 2024-11-04 ## Context We need a database to store workflow definitions, execution state, and user data. The database must support ACID transactions, JSON queries, and scale to millions of records. ## Options Considered ### Option 1: PostgreSQL Mature relational database with JSONB support.",
              "score": null
            },
            {
              "doc_id": "adr_008",
              "chunk_id": "adr_008_fix_1",
              "content": "**Pros:** - No vendor lock-in - S3 compatible - On-premises option **Cons:** - Operational burden - Lower durability guarantees - Scaling complexity ### Option 3: Google Cloud Storage GCP object storage. **Pros:** - Strong consistency - Good performance - Unified billing **Cons:** - GCP lock-in - Fewer features than S3 - Smaller ecosystem ## Decision We will use Amazon S3 for object storage with intelligent tiering for cost optimization.",
              "score": null
            }
          ],
          "latency_ms": 3.5082869999314426,
          "query_id": "compare_023",
          "dimension": "negation",
          "query": "Why didn't we go with MongoDB or CockroachDB?"
        },
        {
          "key_facts": [
            "Kubernetes",
            "ECS",
            "industry standard",
            "AWS lock-in",
            "ecosystem"
          ],
          "found_facts": [
            "Kubernetes",
            "ECS",
            "industry standard",
            "AWS lock-in",
            "ecosystem"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "adr_002"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "adr_002",
              "chunk_id": "adr_002_fix_0",
              "content": "# ADR-002: Adopt Kubernetes for Container Orchestration **Status:** Accepted **Date:** 2024-01-24 ## Context We need a platform to deploy and scale our microservices. The platform must support auto-scaling, rolling deployments, and multi-region deployment. ## Options Considered ### Option 1: Kubernetes (EKS) Industry-standard container orchestration. **Pros:** - Industry standard - Rich ecosystem - Cloud agnostic - Auto-scaling **Cons:** - Operational complexity - Learning curve - Resource overhead ### Option 2: AWS ECS Amazon's native container service.",
              "score": null
            },
            {
              "doc_id": "adr_002",
              "chunk_id": "adr_002_fix_1",
              "content": "**Pros:** - Simple operation - Native AWS integration - Lower overhead **Cons:** - AWS lock-in - Less flexible - Smaller community ### Option 3: Serverless (Lambda) Function-as-a-service approach. **Pros:** - Zero infrastructure - Pay per use - Auto-scaling **Cons:** - Cold starts - 15-minute limit - Vendor lock-in ## Decision We will use Kubernetes on EKS for container orchestration with Helm for deployments.",
              "score": null
            },
            {
              "doc_id": "runbook_high_cpu_usage",
              "chunk_id": "runbook_high_cpu_usage_fix_1",
              "content": "Step 4 Profile the application: `cloudflow debug profile --pod api-xyz --duration 60s` ## Resolution ### Step 1 Scale up if traffic-related: `kubectl scale deployment/api --replicas=10 -n production` ### Step 2 Restart pods if memory leak suspected: `kubectl rollout restart deployment/api -n production` ### Step 3 Roll back recent deployment if regression: `kubectl rollout undo deployment/api -n production` ### Step 4 Add resource limits if unbounded: update deployment with CPU limits ## Escalation If CPU remains",
              "score": null
            },
            {
              "doc_id": "adr_004",
              "chunk_id": "adr_004_fix_1",
              "content": "**Pros:** - Simple operation - Multi-threaded - Lower memory overhead **Cons:** - No persistence - Limited data types - No clustering ### Option 3: Application-level cache In-process caching only. **Pros:** - No external dependency - Fastest possible - Simple **Cons:** - Not shared across pods - Memory pressure - Cold starts ## Decision We will use Redis Cluster for caching with a 6-node cluster for high availability.",
              "score": null
            },
            {
              "doc_id": "adr_006",
              "chunk_id": "adr_006_fix_1",
              "content": "**Pros:** - Native AWS support - No state file - Drift detection **Cons:** - AWS only - Verbose YAML/JSON - Slower development ### Option 3: Pulumi IaC with real programming languages. **Pros:** - Real languages - Type safety - Reusable components **Cons:** - Smaller community - Vendor lock-in - Commercial features ## Decision We will use Terraform with S3 backend for state and DynamoDB for locking.",
              "score": null
            }
          ],
          "latency_ms": 3.413790998820332,
          "query_id": "compare_024",
          "dimension": "original",
          "query": "Compare Kubernetes vs ECS for container orchestration"
        },
        {
          "key_facts": [
            "Kubernetes",
            "ECS",
            "industry standard",
            "AWS lock-in",
            "ecosystem"
          ],
          "found_facts": [
            "Kubernetes",
            "ECS",
            "industry standard",
            "AWS lock-in",
            "ecosystem"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "adr_002"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "adr_002",
              "chunk_id": "adr_002_fix_1",
              "content": "**Pros:** - Simple operation - Native AWS integration - Lower overhead **Cons:** - AWS lock-in - Less flexible - Smaller community ### Option 3: Serverless (Lambda) Function-as-a-service approach. **Pros:** - Zero infrastructure - Pay per use - Auto-scaling **Cons:** - Cold starts - 15-minute limit - Vendor lock-in ## Decision We will use Kubernetes on EKS for container orchestration with Helm for deployments.",
              "score": null
            },
            {
              "doc_id": "adr_002",
              "chunk_id": "adr_002_fix_0",
              "content": "# ADR-002: Adopt Kubernetes for Container Orchestration **Status:** Accepted **Date:** 2024-01-24 ## Context We need a platform to deploy and scale our microservices. The platform must support auto-scaling, rolling deployments, and multi-region deployment. ## Options Considered ### Option 1: Kubernetes (EKS) Industry-standard container orchestration. **Pros:** - Industry standard - Rich ecosystem - Cloud agnostic - Auto-scaling **Cons:** - Operational complexity - Learning curve - Resource overhead ### Option 2: AWS ECS Amazon's native container service.",
              "score": null
            },
            {
              "doc_id": "adr_006",
              "chunk_id": "adr_006_fix_1",
              "content": "**Pros:** - Native AWS support - No state file - Drift detection **Cons:** - AWS only - Verbose YAML/JSON - Slower development ### Option 3: Pulumi IaC with real programming languages. **Pros:** - Real languages - Type safety - Reusable components **Cons:** - Smaller community - Vendor lock-in - Commercial features ## Decision We will use Terraform with S3 backend for state and DynamoDB for locking.",
              "score": null
            },
            {
              "doc_id": "adr_004",
              "chunk_id": "adr_004_fix_1",
              "content": "**Pros:** - Simple operation - Multi-threaded - Lower memory overhead **Cons:** - No persistence - Limited data types - No clustering ### Option 3: Application-level cache In-process caching only. **Pros:** - No external dependency - Fastest possible - Simple **Cons:** - Not shared across pods - Memory pressure - Cold starts ## Decision We will use Redis Cluster for caching with a 6-node cluster for high availability.",
              "score": null
            },
            {
              "doc_id": "runbook_high_cpu_usage",
              "chunk_id": "runbook_high_cpu_usage_fix_1",
              "content": "Step 4 Profile the application: `cloudflow debug profile --pod api-xyz --duration 60s` ## Resolution ### Step 1 Scale up if traffic-related: `kubectl scale deployment/api --replicas=10 -n production` ### Step 2 Restart pods if memory leak suspected: `kubectl rollout restart deployment/api -n production` ### Step 3 Roll back recent deployment if regression: `kubectl rollout undo deployment/api -n production` ### Step 4 Add resource limits if unbounded: update deployment with CPU limits ## Escalation If CPU remains",
              "score": null
            }
          ],
          "latency_ms": 3.4258030009368667,
          "query_id": "compare_024",
          "dimension": "synonym",
          "query": "K8s versus AWS container service - what's the difference?"
        },
        {
          "key_facts": [
            "Kubernetes",
            "ECS",
            "industry standard",
            "AWS lock-in",
            "ecosystem"
          ],
          "found_facts": [
            "Kubernetes",
            "ECS",
            "industry standard",
            "AWS lock-in",
            "ecosystem"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "adr_002"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "adr_002",
              "chunk_id": "adr_002_fix_1",
              "content": "**Pros:** - Simple operation - Native AWS integration - Lower overhead **Cons:** - AWS lock-in - Less flexible - Smaller community ### Option 3: Serverless (Lambda) Function-as-a-service approach. **Pros:** - Zero infrastructure - Pay per use - Auto-scaling **Cons:** - Cold starts - 15-minute limit - Vendor lock-in ## Decision We will use Kubernetes on EKS for container orchestration with Helm for deployments.",
              "score": null
            },
            {
              "doc_id": "adr_008",
              "chunk_id": "adr_008_fix_1",
              "content": "**Pros:** - No vendor lock-in - S3 compatible - On-premises option **Cons:** - Operational burden - Lower durability guarantees - Scaling complexity ### Option 3: Google Cloud Storage GCP object storage. **Pros:** - Strong consistency - Good performance - Unified billing **Cons:** - GCP lock-in - Fewer features than S3 - Smaller ecosystem ## Decision We will use Amazon S3 for object storage with intelligent tiering for cost optimization.",
              "score": null
            },
            {
              "doc_id": "adr_007",
              "chunk_id": "adr_007_fix_1",
              "content": "**Pros:** - Centralized enforcement - Low latency - Easy configuration **Cons:** - Gateway becomes bottleneck - Coarse granularity - Redis dependency ### Option 2: Per-service limiting Each service implements its own limits. **Pros:** - Fine-grained control - Service autonomy - No single point of failure **Cons:** - Inconsistent implementation - Higher complexity - Distributed state ### Option 3: Token bucket Token-based rate limiting.",
              "score": null
            },
            {
              "doc_id": "adr_004",
              "chunk_id": "adr_004_fix_1",
              "content": "**Pros:** - Simple operation - Multi-threaded - Lower memory overhead **Cons:** - No persistence - Limited data types - No clustering ### Option 3: Application-level cache In-process caching only. **Pros:** - No external dependency - Fastest possible - Simple **Cons:** - Not shared across pods - Memory pressure - Cold starts ## Decision We will use Redis Cluster for caching with a 6-node cluster for high availability.",
              "score": null
            },
            {
              "doc_id": "adr_002",
              "chunk_id": "adr_002_fix_0",
              "content": "# ADR-002: Adopt Kubernetes for Container Orchestration **Status:** Accepted **Date:** 2024-01-24 ## Context We need a platform to deploy and scale our microservices. The platform must support auto-scaling, rolling deployments, and multi-region deployment. ## Options Considered ### Option 1: Kubernetes (EKS) Industry-standard container orchestration. **Pros:** - Industry standard - Rich ecosystem - Cloud agnostic - Auto-scaling **Cons:** - Operational complexity - Learning curve - Resource overhead ### Option 2: AWS ECS Amazon's native container service.",
              "score": null
            }
          ],
          "latency_ms": 4.067359001055593,
          "query_id": "compare_024",
          "dimension": "problem",
          "query": "Trying to decide on our container platform and need to understand the vendor lock-in tradeoffs"
        },
        {
          "key_facts": [
            "Kubernetes",
            "ECS",
            "industry standard",
            "AWS lock-in",
            "ecosystem"
          ],
          "found_facts": [
            "Kubernetes",
            "ECS",
            "industry standard",
            "AWS lock-in",
            "ecosystem"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "adr_002"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "adr_002",
              "chunk_id": "adr_002_fix_1",
              "content": "**Pros:** - Simple operation - Native AWS integration - Lower overhead **Cons:** - AWS lock-in - Less flexible - Smaller community ### Option 3: Serverless (Lambda) Function-as-a-service approach. **Pros:** - Zero infrastructure - Pay per use - Auto-scaling **Cons:** - Cold starts - 15-minute limit - Vendor lock-in ## Decision We will use Kubernetes on EKS for container orchestration with Helm for deployments.",
              "score": null
            },
            {
              "doc_id": "adr_002",
              "chunk_id": "adr_002_fix_0",
              "content": "# ADR-002: Adopt Kubernetes for Container Orchestration **Status:** Accepted **Date:** 2024-01-24 ## Context We need a platform to deploy and scale our microservices. The platform must support auto-scaling, rolling deployments, and multi-region deployment. ## Options Considered ### Option 1: Kubernetes (EKS) Industry-standard container orchestration. **Pros:** - Industry standard - Rich ecosystem - Cloud agnostic - Auto-scaling **Cons:** - Operational complexity - Learning curve - Resource overhead ### Option 2: AWS ECS Amazon's native container service.",
              "score": null
            },
            {
              "doc_id": "adr_006",
              "chunk_id": "adr_006_fix_1",
              "content": "**Pros:** - Native AWS support - No state file - Drift detection **Cons:** - AWS only - Verbose YAML/JSON - Slower development ### Option 3: Pulumi IaC with real programming languages. **Pros:** - Real languages - Type safety - Reusable components **Cons:** - Smaller community - Vendor lock-in - Commercial features ## Decision We will use Terraform with S3 backend for state and DynamoDB for locking.",
              "score": null
            },
            {
              "doc_id": "adr_008",
              "chunk_id": "adr_008_fix_1",
              "content": "**Pros:** - No vendor lock-in - S3 compatible - On-premises option **Cons:** - Operational burden - Lower durability guarantees - Scaling complexity ### Option 3: Google Cloud Storage GCP object storage. **Pros:** - Strong consistency - Good performance - Unified billing **Cons:** - GCP lock-in - Fewer features than S3 - Smaller ecosystem ## Decision We will use Amazon S3 for object storage with intelligent tiering for cost optimization.",
              "score": null
            },
            {
              "doc_id": "adr_004",
              "chunk_id": "adr_004_fix_1",
              "content": "**Pros:** - Simple operation - Multi-threaded - Lower memory overhead **Cons:** - No persistence - Limited data types - No clustering ### Option 3: Application-level cache In-process caching only. **Pros:** - No external dependency - Fastest possible - Simple **Cons:** - Not shared across pods - Memory pressure - Cold starts ## Decision We will use Redis Cluster for caching with a 6-node cluster for high availability.",
              "score": null
            }
          ],
          "latency_ms": 3.794460000790423,
          "query_id": "compare_024",
          "dimension": "casual",
          "query": "k8s vs ecs pros cons"
        },
        {
          "key_facts": [
            "Kubernetes",
            "ECS",
            "industry standard",
            "AWS lock-in",
            "ecosystem"
          ],
          "found_facts": [
            "Kubernetes",
            "ECS",
            "industry standard",
            "AWS lock-in",
            "ecosystem"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "adr_002"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "adr_002",
              "chunk_id": "adr_002_fix_1",
              "content": "**Pros:** - Simple operation - Native AWS integration - Lower overhead **Cons:** - AWS lock-in - Less flexible - Smaller community ### Option 3: Serverless (Lambda) Function-as-a-service approach. **Pros:** - Zero infrastructure - Pay per use - Auto-scaling **Cons:** - Cold starts - 15-minute limit - Vendor lock-in ## Decision We will use Kubernetes on EKS for container orchestration with Helm for deployments.",
              "score": null
            },
            {
              "doc_id": "adr_002",
              "chunk_id": "adr_002_fix_0",
              "content": "# ADR-002: Adopt Kubernetes for Container Orchestration **Status:** Accepted **Date:** 2024-01-24 ## Context We need a platform to deploy and scale our microservices. The platform must support auto-scaling, rolling deployments, and multi-region deployment. ## Options Considered ### Option 1: Kubernetes (EKS) Industry-standard container orchestration. **Pros:** - Industry standard - Rich ecosystem - Cloud agnostic - Auto-scaling **Cons:** - Operational complexity - Learning curve - Resource overhead ### Option 2: AWS ECS Amazon's native container service.",
              "score": null
            },
            {
              "doc_id": "adr_006",
              "chunk_id": "adr_006_fix_1",
              "content": "**Pros:** - Native AWS support - No state file - Drift detection **Cons:** - AWS only - Verbose YAML/JSON - Slower development ### Option 3: Pulumi IaC with real programming languages. **Pros:** - Real languages - Type safety - Reusable components **Cons:** - Smaller community - Vendor lock-in - Commercial features ## Decision We will use Terraform with S3 backend for state and DynamoDB for locking.",
              "score": null
            },
            {
              "doc_id": "adr_004",
              "chunk_id": "adr_004_fix_1",
              "content": "**Pros:** - Simple operation - Multi-threaded - Lower memory overhead **Cons:** - No persistence - Limited data types - No clustering ### Option 3: Application-level cache In-process caching only. **Pros:** - No external dependency - Fastest possible - Simple **Cons:** - Not shared across pods - Memory pressure - Cold starts ## Decision We will use Redis Cluster for caching with a 6-node cluster for high availability.",
              "score": null
            },
            {
              "doc_id": "runbook_high_cpu_usage",
              "chunk_id": "runbook_high_cpu_usage_fix_1",
              "content": "Step 4 Profile the application: `cloudflow debug profile --pod api-xyz --duration 60s` ## Resolution ### Step 1 Scale up if traffic-related: `kubectl scale deployment/api --replicas=10 -n production` ### Step 2 Restart pods if memory leak suspected: `kubectl rollout restart deployment/api -n production` ### Step 3 Roll back recent deployment if regression: `kubectl rollout undo deployment/api -n production` ### Step 4 Add resource limits if unbounded: update deployment with CPU limits ## Escalation If CPU remains",
              "score": null
            }
          ],
          "latency_ms": 3.6093250000703847,
          "query_id": "compare_024",
          "dimension": "contextual",
          "query": "Infra team is proposing we move to ECS for simpler ops - what did we consider when we chose Kubernetes?"
        },
        {
          "key_facts": [
            "Kubernetes",
            "ECS",
            "industry standard",
            "AWS lock-in",
            "ecosystem"
          ],
          "found_facts": [
            "Kubernetes",
            "ECS",
            "industry standard",
            "AWS lock-in",
            "ecosystem"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "adr_002"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "adr_002",
              "chunk_id": "adr_002_fix_1",
              "content": "**Pros:** - Simple operation - Native AWS integration - Lower overhead **Cons:** - AWS lock-in - Less flexible - Smaller community ### Option 3: Serverless (Lambda) Function-as-a-service approach. **Pros:** - Zero infrastructure - Pay per use - Auto-scaling **Cons:** - Cold starts - 15-minute limit - Vendor lock-in ## Decision We will use Kubernetes on EKS for container orchestration with Helm for deployments.",
              "score": null
            },
            {
              "doc_id": "adr_002",
              "chunk_id": "adr_002_fix_0",
              "content": "# ADR-002: Adopt Kubernetes for Container Orchestration **Status:** Accepted **Date:** 2024-01-24 ## Context We need a platform to deploy and scale our microservices. The platform must support auto-scaling, rolling deployments, and multi-region deployment. ## Options Considered ### Option 1: Kubernetes (EKS) Industry-standard container orchestration. **Pros:** - Industry standard - Rich ecosystem - Cloud agnostic - Auto-scaling **Cons:** - Operational complexity - Learning curve - Resource overhead ### Option 2: AWS ECS Amazon's native container service.",
              "score": null
            },
            {
              "doc_id": "runbook_high_cpu_usage",
              "chunk_id": "runbook_high_cpu_usage_fix_1",
              "content": "Step 4 Profile the application: `cloudflow debug profile --pod api-xyz --duration 60s` ## Resolution ### Step 1 Scale up if traffic-related: `kubectl scale deployment/api --replicas=10 -n production` ### Step 2 Restart pods if memory leak suspected: `kubectl rollout restart deployment/api -n production` ### Step 3 Roll back recent deployment if regression: `kubectl rollout undo deployment/api -n production` ### Step 4 Add resource limits if unbounded: update deployment with CPU limits ## Escalation If CPU remains",
              "score": null
            },
            {
              "doc_id": "adr_006",
              "chunk_id": "adr_006_fix_1",
              "content": "**Pros:** - Native AWS support - No state file - Drift detection **Cons:** - AWS only - Verbose YAML/JSON - Slower development ### Option 3: Pulumi IaC with real programming languages. **Pros:** - Real languages - Type safety - Reusable components **Cons:** - Smaller community - Vendor lock-in - Commercial features ## Decision We will use Terraform with S3 backend for state and DynamoDB for locking.",
              "score": null
            },
            {
              "doc_id": "runbook_api_gateway_5xx_errors",
              "chunk_id": "runbook_api_gateway_5xx_errors_fix_1",
              "content": "-n production` ### Step 4 Test backend directly: `kubectl exec -it kong-xxx -n kong -- curl http://api.production.svc:8080/health` ## Resolution ### Step 1 Restart unhealthy backends: `kubectl rollout restart deployment/api -n production` ### Step 2 Enable circuit breaker if cascading: update Kong circuit-breaker plugin ### Step 3 Scale up backends: `kubectl scale deployment/api --replicas=5 -n production` ### Step 4 Check for resource exhaustion on backend pods ### Step 5 Verify network policies allow traffic ## Escalation",
              "score": null
            }
          ],
          "latency_ms": 3.3788860000640852,
          "query_id": "compare_024",
          "dimension": "negation",
          "query": "Why did we pick Kubernetes over ECS despite the complexity?"
        },
        {
          "key_facts": [
            "OAuth 2.0",
            "Session-based",
            "API keys",
            "stateless validation"
          ],
          "found_facts": [
            "OAuth 2.0",
            "Session-based",
            "API keys",
            "stateless validation"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "adr_003"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "arch_authentication_service",
              "chunk_id": "arch_authentication_service_fix_0",
              "content": "# Authentication Service Architecture ## Overview The Authentication Service handles user identity, session management, and token issuance. Supports OAuth 2.0, SAML, and API key authentication. ## Design Principles - **Zero Knowledge**: Passwords never stored in plaintext - **Token Rotation**: Refresh tokens rotate on each use ## Components ### Identity Provider Manages user accounts and credentials. Password hashing uses bcrypt with cost factor 12.",
              "score": null
            },
            {
              "doc_id": "adr_003",
              "chunk_id": "adr_003_fix_2",
              "content": "**Pros:** - Very simple - Low overhead - Easy to implement **Cons:** - No user context - Hard to rotate - No SSO support ## Decision We will implement OAuth 2.0 with JWT tokens. Access tokens expire after 1 hour, refresh tokens after 7 days. ## Consequences - Implement token rotation on refresh - Support API keys for machine-to-machine auth - Add MFA for admin accounts",
              "score": null
            },
            {
              "doc_id": "adr_003",
              "chunk_id": "adr_003_fix_1",
              "content": "**Pros:** - Industry standard - Stateless validation - Supports refresh tokens - SSO compatible **Cons:** - Token size overhead - Cannot revoke individual tokens - Complexity ### Option 2: Session-based auth Traditional server-side sessions. **Pros:** - Simple implementation - Easy revocation - Smaller payloads **Cons:** - Requires session store - Not suitable for APIs - Scaling challenges ### Option 3: API keys only Simple key-based authentication.",
              "score": null
            },
            {
              "doc_id": "adr_003",
              "chunk_id": "adr_003_fix_0",
              "content": "# ADR-003: Implement OAuth 2.0 with JWT for Authentication **Status:** Accepted **Date:** 2024-05-08 ## Context We need a secure authentication system that supports API keys, user sessions, and third-party integrations. ## Options Considered ### Option 1: OAuth 2.0 with JWT Industry standard token-based auth.",
              "score": null
            },
            {
              "doc_id": "arch_authentication_service",
              "chunk_id": "arch_authentication_service_fix_1",
              "content": "**Technology:** Custom Go service **Scaling:** Horizontal stateless ### Token Service Issues and validates access tokens. Token lifetime is 1 hour with 7-day refresh tokens. **Technology:** JWT with RS256 **Scaling:** Horizontal stateless ### SSO Gateway Integrates with external identity providers. Supports Google, Microsoft, Okta, and custom SAML.",
              "score": null
            }
          ],
          "latency_ms": 3.310568999950192,
          "query_id": "compare_025",
          "dimension": "original",
          "query": "What authentication options were evaluated?"
        },
        {
          "key_facts": [
            "OAuth 2.0",
            "Session-based",
            "API keys",
            "stateless validation"
          ],
          "found_facts": [
            "OAuth 2.0",
            "Session-based",
            "API keys",
            "stateless validation"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "adr_003"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "arch_authentication_service",
              "chunk_id": "arch_authentication_service_fix_0",
              "content": "# Authentication Service Architecture ## Overview The Authentication Service handles user identity, session management, and token issuance. Supports OAuth 2.0, SAML, and API key authentication. ## Design Principles - **Zero Knowledge**: Passwords never stored in plaintext - **Token Rotation**: Refresh tokens rotate on each use ## Components ### Identity Provider Manages user accounts and credentials. Password hashing uses bcrypt with cost factor 12.",
              "score": null
            },
            {
              "doc_id": "adr_003",
              "chunk_id": "adr_003_fix_2",
              "content": "**Pros:** - Very simple - Low overhead - Easy to implement **Cons:** - No user context - Hard to rotate - No SSO support ## Decision We will implement OAuth 2.0 with JWT tokens. Access tokens expire after 1 hour, refresh tokens after 7 days. ## Consequences - Implement token rotation on refresh - Support API keys for machine-to-machine auth - Add MFA for admin accounts",
              "score": null
            },
            {
              "doc_id": "adr_003",
              "chunk_id": "adr_003_fix_0",
              "content": "# ADR-003: Implement OAuth 2.0 with JWT for Authentication **Status:** Accepted **Date:** 2024-05-08 ## Context We need a secure authentication system that supports API keys, user sessions, and third-party integrations. ## Options Considered ### Option 1: OAuth 2.0 with JWT Industry standard token-based auth.",
              "score": null
            },
            {
              "doc_id": "adr_003",
              "chunk_id": "adr_003_fix_1",
              "content": "**Pros:** - Industry standard - Stateless validation - Supports refresh tokens - SSO compatible **Cons:** - Token size overhead - Cannot revoke individual tokens - Complexity ### Option 2: Session-based auth Traditional server-side sessions. **Pros:** - Simple implementation - Easy revocation - Smaller payloads **Cons:** - Requires session store - Not suitable for APIs - Scaling challenges ### Option 3: API keys only Simple key-based authentication.",
              "score": null
            },
            {
              "doc_id": "arch_authentication_service",
              "chunk_id": "arch_authentication_service_fix_1",
              "content": "**Technology:** Custom Go service **Scaling:** Horizontal stateless ### Token Service Issues and validates access tokens. Token lifetime is 1 hour with 7-day refresh tokens. **Technology:** JWT with RS256 **Scaling:** Horizontal stateless ### SSO Gateway Integrates with external identity providers. Supports Google, Microsoft, Okta, and custom SAML.",
              "score": null
            }
          ],
          "latency_ms": 4.28129800093302,
          "query_id": "compare_025",
          "dimension": "synonym",
          "query": "What auth approaches did we consider for the login system?"
        },
        {
          "key_facts": [
            "OAuth 2.0",
            "Session-based",
            "API keys",
            "stateless validation"
          ],
          "found_facts": [
            "OAuth 2.0",
            "Session-based",
            "API keys",
            "stateless validation"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "adr_003"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "adr_003",
              "chunk_id": "adr_003_fix_1",
              "content": "**Pros:** - Industry standard - Stateless validation - Supports refresh tokens - SSO compatible **Cons:** - Token size overhead - Cannot revoke individual tokens - Complexity ### Option 2: Session-based auth Traditional server-side sessions. **Pros:** - Simple implementation - Easy revocation - Smaller payloads **Cons:** - Requires session store - Not suitable for APIs - Scaling challenges ### Option 3: API keys only Simple key-based authentication.",
              "score": null
            },
            {
              "doc_id": "arch_authentication_service",
              "chunk_id": "arch_authentication_service_fix_0",
              "content": "# Authentication Service Architecture ## Overview The Authentication Service handles user identity, session management, and token issuance. Supports OAuth 2.0, SAML, and API key authentication. ## Design Principles - **Zero Knowledge**: Passwords never stored in plaintext - **Token Rotation**: Refresh tokens rotate on each use ## Components ### Identity Provider Manages user accounts and credentials. Password hashing uses bcrypt with cost factor 12.",
              "score": null
            },
            {
              "doc_id": "api_audit",
              "chunk_id": "api_audit_fix_0",
              "content": "# Audit API Reference ## Overview The Audit API provides programmatic access to audit functionality. Base URL: `https://api.cloudflow.io/v1/audit` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /audit/logs Retrieve audit logs for compliance.",
              "score": null
            },
            {
              "doc_id": "adr_003",
              "chunk_id": "adr_003_fix_0",
              "content": "# ADR-003: Implement OAuth 2.0 with JWT for Authentication **Status:** Accepted **Date:** 2024-05-08 ## Context We need a secure authentication system that supports API keys, user sessions, and third-party integrations. ## Options Considered ### Option 1: OAuth 2.0 with JWT Industry standard token-based auth.",
              "score": null
            },
            {
              "doc_id": "adr_003",
              "chunk_id": "adr_003_fix_2",
              "content": "**Pros:** - Very simple - Low overhead - Easy to implement **Cons:** - No user context - Hard to rotate - No SSO support ## Decision We will implement OAuth 2.0 with JWT tokens. Access tokens expire after 1 hour, refresh tokens after 7 days. ## Consequences - Implement token rotation on refresh - Support API keys for machine-to-machine auth - Add MFA for admin accounts",
              "score": null
            }
          ],
          "latency_ms": 3.708199001266621,
          "query_id": "compare_025",
          "dimension": "problem",
          "query": "Security audit wants to know what alternatives we rejected for our auth implementation"
        },
        {
          "key_facts": [
            "OAuth 2.0",
            "Session-based",
            "API keys",
            "stateless validation"
          ],
          "found_facts": [
            "OAuth 2.0",
            "Session-based",
            "API keys",
            "stateless validation"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "adr_003"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "adr_003",
              "chunk_id": "adr_003_fix_1",
              "content": "**Pros:** - Industry standard - Stateless validation - Supports refresh tokens - SSO compatible **Cons:** - Token size overhead - Cannot revoke individual tokens - Complexity ### Option 2: Session-based auth Traditional server-side sessions. **Pros:** - Simple implementation - Easy revocation - Smaller payloads **Cons:** - Requires session store - Not suitable for APIs - Scaling challenges ### Option 3: API keys only Simple key-based authentication.",
              "score": null
            },
            {
              "doc_id": "adr_003",
              "chunk_id": "adr_003_fix_2",
              "content": "**Pros:** - Very simple - Low overhead - Easy to implement **Cons:** - No user context - Hard to rotate - No SSO support ## Decision We will implement OAuth 2.0 with JWT tokens. Access tokens expire after 1 hour, refresh tokens after 7 days. ## Consequences - Implement token rotation on refresh - Support API keys for machine-to-machine auth - Add MFA for admin accounts",
              "score": null
            },
            {
              "doc_id": "arch_authentication_service",
              "chunk_id": "arch_authentication_service_fix_0",
              "content": "# Authentication Service Architecture ## Overview The Authentication Service handles user identity, session management, and token issuance. Supports OAuth 2.0, SAML, and API key authentication. ## Design Principles - **Zero Knowledge**: Passwords never stored in plaintext - **Token Rotation**: Refresh tokens rotate on each use ## Components ### Identity Provider Manages user accounts and credentials. Password hashing uses bcrypt with cost factor 12.",
              "score": null
            },
            {
              "doc_id": "adr_003",
              "chunk_id": "adr_003_fix_0",
              "content": "# ADR-003: Implement OAuth 2.0 with JWT for Authentication **Status:** Accepted **Date:** 2024-05-08 ## Context We need a secure authentication system that supports API keys, user sessions, and third-party integrations. ## Options Considered ### Option 1: OAuth 2.0 with JWT Industry standard token-based auth.",
              "score": null
            },
            {
              "doc_id": "howto_migrate_from_v1_to_v2_api",
              "chunk_id": "howto_migrate_from_v1_to_v2_api_fix_0",
              "content": "# How to Migrate from v1 to v2 API ## Prerequisites - Existing v1 integration - v2 API access enabled ## Steps ### Step 1: Review breaking changes The v2 API changes authentication from API keys to OAuth 2.0. Review the migration guide. ### Step 2: Update client library Upgrade to the latest SDK version. ```bash npm install @cloudflow/sdk@latest ``` ### Step 3: Update authentication Replace API key authentication with OAuth 2.0 client credentials flow.",
              "score": null
            }
          ],
          "latency_ms": 3.3801179997681174,
          "query_id": "compare_025",
          "dimension": "casual",
          "query": "oauth vs session vs api key auth comparison"
        },
        {
          "key_facts": [
            "OAuth 2.0",
            "Session-based",
            "API keys",
            "stateless validation"
          ],
          "found_facts": [
            "OAuth 2.0"
          ],
          "missed_facts": [
            "Session-based",
            "API keys",
            "stateless validation"
          ],
          "coverage": 0.25,
          "expected_docs": [
            "adr_003"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "howto_configure_single_sign_on",
              "chunk_id": "howto_configure_single_sign_on_fix_0",
              "content": "# How to Configure Single Sign-On ## Prerequisites - Admin access - Enterprise plan - Identity provider access ## Steps ### Step 1: Choose provider CloudFlow supports SAML 2.0, OpenID Connect, and OAuth 2.0 providers. ### Step 2: Get metadata Download CloudFlow's SAML metadata for your IdP configuration. ```bash cloudflow sso metadata --format xml > cloudflow-metadata.xml ``` ### Step 3: Configure IdP Add CloudFlow as a service provider in your identity provider.",
              "score": null
            },
            {
              "doc_id": "api_integrations",
              "chunk_id": "api_integrations_fix_0",
              "content": "# Integrations API Reference ## Overview The Integrations API provides programmatic access to integrations functionality. Base URL: `https://api.cloudflow.io/v1/integrations` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /integrations List available integrations.",
              "score": null
            },
            {
              "doc_id": "arch_api_gateway",
              "chunk_id": "arch_api_gateway_fix_0",
              "content": "# API Gateway Architecture ## Overview The API Gateway handles all incoming HTTP requests, providing authentication, rate limiting, and request routing. Built on Kong with custom plugins for CloudFlow-specific functionality. ## Design Principles - **Zero Trust**: Every request is authenticated and authorized - **Defense in Depth**: Multiple layers of security validation ## Components ### Kong Gateway Primary ingress controller. Handles TLS termination, request routing, and plugin execution.",
              "score": null
            },
            {
              "doc_id": "api_users",
              "chunk_id": "api_users_fix_0",
              "content": "# Users API Reference ## Overview The Users API provides programmatic access to users functionality. Base URL: `https://api.cloudflow.io/v1/users` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /users/me Get current user profile.",
              "score": null
            },
            {
              "doc_id": "api_secrets",
              "chunk_id": "api_secrets_fix_0",
              "content": "# Secrets API Reference ## Overview The Secrets API provides programmatic access to secrets functionality. Base URL: `https://api.cloudflow.io/v1/secrets` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /secrets List secret names (values not returned).",
              "score": null
            }
          ],
          "latency_ms": 3.418971000428428,
          "query_id": "compare_025",
          "dimension": "contextual",
          "query": "Building a new service that needs auth - what authentication strategy does CloudFlow use and why?"
        },
        {
          "key_facts": [
            "OAuth 2.0",
            "Session-based",
            "API keys",
            "stateless validation"
          ],
          "found_facts": [
            "OAuth 2.0",
            "Session-based",
            "API keys",
            "stateless validation"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "adr_003"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "adr_003",
              "chunk_id": "adr_003_fix_1",
              "content": "**Pros:** - Industry standard - Stateless validation - Supports refresh tokens - SSO compatible **Cons:** - Token size overhead - Cannot revoke individual tokens - Complexity ### Option 2: Session-based auth Traditional server-side sessions. **Pros:** - Simple implementation - Easy revocation - Smaller payloads **Cons:** - Requires session store - Not suitable for APIs - Scaling challenges ### Option 3: API keys only Simple key-based authentication.",
              "score": null
            },
            {
              "doc_id": "adr_003",
              "chunk_id": "adr_003_fix_2",
              "content": "**Pros:** - Very simple - Low overhead - Easy to implement **Cons:** - No user context - Hard to rotate - No SSO support ## Decision We will implement OAuth 2.0 with JWT tokens. Access tokens expire after 1 hour, refresh tokens after 7 days. ## Consequences - Implement token rotation on refresh - Support API keys for machine-to-machine auth - Add MFA for admin accounts",
              "score": null
            },
            {
              "doc_id": "arch_authentication_service",
              "chunk_id": "arch_authentication_service_fix_0",
              "content": "# Authentication Service Architecture ## Overview The Authentication Service handles user identity, session management, and token issuance. Supports OAuth 2.0, SAML, and API key authentication. ## Design Principles - **Zero Knowledge**: Passwords never stored in plaintext - **Token Rotation**: Refresh tokens rotate on each use ## Components ### Identity Provider Manages user accounts and credentials. Password hashing uses bcrypt with cost factor 12.",
              "score": null
            },
            {
              "doc_id": "adr_003",
              "chunk_id": "adr_003_fix_0",
              "content": "# ADR-003: Implement OAuth 2.0 with JWT for Authentication **Status:** Accepted **Date:** 2024-05-08 ## Context We need a secure authentication system that supports API keys, user sessions, and third-party integrations. ## Options Considered ### Option 1: OAuth 2.0 with JWT Industry standard token-based auth.",
              "score": null
            },
            {
              "doc_id": "arch_authentication_service",
              "chunk_id": "arch_authentication_service_fix_1",
              "content": "**Technology:** Custom Go service **Scaling:** Horizontal stateless ### Token Service Issues and validates access tokens. Token lifetime is 1 hour with 7-day refresh tokens. **Technology:** JWT with RS256 **Scaling:** Horizontal stateless ### SSO Gateway Integrates with external identity providers. Supports Google, Microsoft, Okta, and custom SAML.",
              "score": null
            }
          ],
          "latency_ms": 3.457151000475278,
          "query_id": "compare_025",
          "dimension": "negation",
          "query": "Why didn't we just use simple session-based authentication?"
        },
        {
          "key_facts": [
            "Redis Cluster",
            "Memcached",
            "data structures",
            "persistence"
          ],
          "found_facts": [
            "Redis Cluster",
            "Memcached",
            "data structures",
            "persistence"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "adr_004"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "arch_caching_layer",
              "chunk_id": "arch_caching_layer_fix_0",
              "content": "# Caching Layer Architecture ## Overview The Caching Layer reduces latency and database load by caching frequently accessed data. Uses a multi-tier strategy with local and distributed caches. ## Design Principles - **Cache Invalidation**: Explicit invalidation on writes - **Graceful Degradation**: System works without cache ## Components ### Local Cache First-level cache with 100ms TTL. Reduces Redis round-trips for hot data.",
              "score": null
            },
            {
              "doc_id": "adr_004",
              "chunk_id": "adr_004_fix_0",
              "content": "# ADR-004: Use Redis for Caching and Session Storage **Status:** Accepted **Date:** 2024-04-05 ## Context We need a fast data store for caching API responses, session data, and rate limiting counters. ## Options Considered ### Option 1: Redis Cluster In-memory data structure store. **Pros:** - Sub-millisecond latency - Rich data structures - Pub/sub support - Clustering **Cons:** - Memory cost - Persistence complexity - Cluster management ### Option 2: Memcached Simple key-value cache.",
              "score": null
            },
            {
              "doc_id": "config_cache",
              "chunk_id": "config_cache_fix_0",
              "content": "# Cache Configuration Reference ## Overview This document describes all configuration options for Cache. ## Environment Variables | Variable | Type | Default | Description | |----------|------|---------|-------------| | `REDIS_URL` | string | required | Redis connection string | | `REDIS_POOL_SIZE` | integer | 10 | Connection pool size | | `CACHE_TTL_SECONDS` | integer | 300 | Default cache TTL | | `CACHE_PREFIX` | string | cf: | Key prefix for namespacing | | `CACHE_COMPRESSION` |",
              "score": null
            },
            {
              "doc_id": "adr_004",
              "chunk_id": "adr_004_fix_1",
              "content": "**Pros:** - Simple operation - Multi-threaded - Lower memory overhead **Cons:** - No persistence - Limited data types - No clustering ### Option 3: Application-level cache In-process caching only. **Pros:** - No external dependency - Fastest possible - Simple **Cons:** - Not shared across pods - Memory pressure - Cold starts ## Decision We will use Redis Cluster for caching with a 6-node cluster for high availability.",
              "score": null
            },
            {
              "doc_id": "arch_caching_layer",
              "chunk_id": "arch_caching_layer_fix_1",
              "content": "**Technology:** Ristretto in-memory cache **Scaling:** Per-pod memory allocation ### Distributed Cache Second-level cache with configurable TTL. Maximum key size 512MB. **Technology:** Redis Cluster 7.x **Scaling:** 6-node cluster with sharding ### CDN Caches static assets and API responses. Cache hit ratio target 95%.",
              "score": null
            }
          ],
          "latency_ms": 3.3544800007803133,
          "query_id": "compare_026",
          "dimension": "original",
          "query": "Compare caching options: Redis vs Memcached"
        },
        {
          "key_facts": [
            "Redis Cluster",
            "Memcached",
            "data structures",
            "persistence"
          ],
          "found_facts": [
            "Redis Cluster",
            "Memcached",
            "data structures",
            "persistence"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "adr_004"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "arch_caching_layer",
              "chunk_id": "arch_caching_layer_fix_0",
              "content": "# Caching Layer Architecture ## Overview The Caching Layer reduces latency and database load by caching frequently accessed data. Uses a multi-tier strategy with local and distributed caches. ## Design Principles - **Cache Invalidation**: Explicit invalidation on writes - **Graceful Degradation**: System works without cache ## Components ### Local Cache First-level cache with 100ms TTL. Reduces Redis round-trips for hot data.",
              "score": null
            },
            {
              "doc_id": "config_cache",
              "chunk_id": "config_cache_fix_0",
              "content": "# Cache Configuration Reference ## Overview This document describes all configuration options for Cache. ## Environment Variables | Variable | Type | Default | Description | |----------|------|---------|-------------| | `REDIS_URL` | string | required | Redis connection string | | `REDIS_POOL_SIZE` | integer | 10 | Connection pool size | | `CACHE_TTL_SECONDS` | integer | 300 | Default cache TTL | | `CACHE_PREFIX` | string | cf: | Key prefix for namespacing | | `CACHE_COMPRESSION` |",
              "score": null
            },
            {
              "doc_id": "adr_004",
              "chunk_id": "adr_004_fix_0",
              "content": "# ADR-004: Use Redis for Caching and Session Storage **Status:** Accepted **Date:** 2024-04-05 ## Context We need a fast data store for caching API responses, session data, and rate limiting counters. ## Options Considered ### Option 1: Redis Cluster In-memory data structure store. **Pros:** - Sub-millisecond latency - Rich data structures - Pub/sub support - Clustering **Cons:** - Memory cost - Persistence complexity - Cluster management ### Option 2: Memcached Simple key-value cache.",
              "score": null
            },
            {
              "doc_id": "arch_caching_layer",
              "chunk_id": "arch_caching_layer_fix_1",
              "content": "**Technology:** Ristretto in-memory cache **Scaling:** Per-pod memory allocation ### Distributed Cache Second-level cache with configurable TTL. Maximum key size 512MB. **Technology:** Redis Cluster 7.x **Scaling:** 6-node cluster with sharding ### CDN Caches static assets and API responses. Cache hit ratio target 95%.",
              "score": null
            },
            {
              "doc_id": "adr_004",
              "chunk_id": "adr_004_fix_1",
              "content": "**Pros:** - Simple operation - Multi-threaded - Lower memory overhead **Cons:** - No persistence - Limited data types - No clustering ### Option 3: Application-level cache In-process caching only. **Pros:** - No external dependency - Fastest possible - Simple **Cons:** - Not shared across pods - Memory pressure - Cold starts ## Decision We will use Redis Cluster for caching with a 6-node cluster for high availability.",
              "score": null
            }
          ],
          "latency_ms": 4.1394130003027385,
          "query_id": "compare_026",
          "dimension": "synonym",
          "query": "What's the difference between Redis and Memcached for our cache layer?"
        },
        {
          "key_facts": [
            "Redis Cluster",
            "Memcached",
            "data structures",
            "persistence"
          ],
          "found_facts": [
            "Redis Cluster",
            "Memcached",
            "data structures",
            "persistence"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "adr_004"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "adr_004",
              "chunk_id": "adr_004_fix_0",
              "content": "# ADR-004: Use Redis for Caching and Session Storage **Status:** Accepted **Date:** 2024-04-05 ## Context We need a fast data store for caching API responses, session data, and rate limiting counters. ## Options Considered ### Option 1: Redis Cluster In-memory data structure store. **Pros:** - Sub-millisecond latency - Rich data structures - Pub/sub support - Clustering **Cons:** - Memory cost - Persistence complexity - Cluster management ### Option 2: Memcached Simple key-value cache.",
              "score": null
            },
            {
              "doc_id": "arch_caching_layer",
              "chunk_id": "arch_caching_layer_fix_0",
              "content": "# Caching Layer Architecture ## Overview The Caching Layer reduces latency and database load by caching frequently accessed data. Uses a multi-tier strategy with local and distributed caches. ## Design Principles - **Cache Invalidation**: Explicit invalidation on writes - **Graceful Degradation**: System works without cache ## Components ### Local Cache First-level cache with 100ms TTL. Reduces Redis round-trips for hot data.",
              "score": null
            },
            {
              "doc_id": "adr_004",
              "chunk_id": "adr_004_fix_1",
              "content": "**Pros:** - Simple operation - Multi-threaded - Lower memory overhead **Cons:** - No persistence - Limited data types - No clustering ### Option 3: Application-level cache In-process caching only. **Pros:** - No external dependency - Fastest possible - Simple **Cons:** - Not shared across pods - Memory pressure - Cold starts ## Decision We will use Redis Cluster for caching with a 6-node cluster for high availability.",
              "score": null
            },
            {
              "doc_id": "arch_caching_layer",
              "chunk_id": "arch_caching_layer_fix_1",
              "content": "**Technology:** Ristretto in-memory cache **Scaling:** Per-pod memory allocation ### Distributed Cache Second-level cache with configurable TTL. Maximum key size 512MB. **Technology:** Redis Cluster 7.x **Scaling:** 6-node cluster with sharding ### CDN Caches static assets and API responses. Cache hit ratio target 95%.",
              "score": null
            },
            {
              "doc_id": "config_cache",
              "chunk_id": "config_cache_fix_0",
              "content": "# Cache Configuration Reference ## Overview This document describes all configuration options for Cache. ## Environment Variables | Variable | Type | Default | Description | |----------|------|---------|-------------| | `REDIS_URL` | string | required | Redis connection string | | `REDIS_POOL_SIZE` | integer | 10 | Connection pool size | | `CACHE_TTL_SECONDS` | integer | 300 | Default cache TTL | | `CACHE_PREFIX` | string | cf: | Key prefix for namespacing | | `CACHE_COMPRESSION` |",
              "score": null
            }
          ],
          "latency_ms": 3.7710769993282156,
          "query_id": "compare_026",
          "dimension": "problem",
          "query": "Cache is getting expensive - should we switch from Redis to Memcached to save on memory?"
        },
        {
          "key_facts": [
            "Redis Cluster",
            "Memcached",
            "data structures",
            "persistence"
          ],
          "found_facts": [
            "Redis Cluster",
            "Memcached",
            "data structures",
            "persistence"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "adr_004"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "adr_004",
              "chunk_id": "adr_004_fix_0",
              "content": "# ADR-004: Use Redis for Caching and Session Storage **Status:** Accepted **Date:** 2024-04-05 ## Context We need a fast data store for caching API responses, session data, and rate limiting counters. ## Options Considered ### Option 1: Redis Cluster In-memory data structure store. **Pros:** - Sub-millisecond latency - Rich data structures - Pub/sub support - Clustering **Cons:** - Memory cost - Persistence complexity - Cluster management ### Option 2: Memcached Simple key-value cache.",
              "score": null
            },
            {
              "doc_id": "arch_caching_layer",
              "chunk_id": "arch_caching_layer_fix_0",
              "content": "# Caching Layer Architecture ## Overview The Caching Layer reduces latency and database load by caching frequently accessed data. Uses a multi-tier strategy with local and distributed caches. ## Design Principles - **Cache Invalidation**: Explicit invalidation on writes - **Graceful Degradation**: System works without cache ## Components ### Local Cache First-level cache with 100ms TTL. Reduces Redis round-trips for hot data.",
              "score": null
            },
            {
              "doc_id": "adr_004",
              "chunk_id": "adr_004_fix_1",
              "content": "**Pros:** - Simple operation - Multi-threaded - Lower memory overhead **Cons:** - No persistence - Limited data types - No clustering ### Option 3: Application-level cache In-process caching only. **Pros:** - No external dependency - Fastest possible - Simple **Cons:** - Not shared across pods - Memory pressure - Cold starts ## Decision We will use Redis Cluster for caching with a 6-node cluster for high availability.",
              "score": null
            },
            {
              "doc_id": "arch_caching_layer",
              "chunk_id": "arch_caching_layer_fix_1",
              "content": "**Technology:** Ristretto in-memory cache **Scaling:** Per-pod memory allocation ### Distributed Cache Second-level cache with configurable TTL. Maximum key size 512MB. **Technology:** Redis Cluster 7.x **Scaling:** 6-node cluster with sharding ### CDN Caches static assets and API responses. Cache hit ratio target 95%.",
              "score": null
            },
            {
              "doc_id": "adr_007",
              "chunk_id": "adr_007_fix_1",
              "content": "**Pros:** - Centralized enforcement - Low latency - Easy configuration **Cons:** - Gateway becomes bottleneck - Coarse granularity - Redis dependency ### Option 2: Per-service limiting Each service implements its own limits. **Pros:** - Fine-grained control - Service autonomy - No single point of failure **Cons:** - Inconsistent implementation - Higher complexity - Distributed state ### Option 3: Token bucket Token-based rate limiting.",
              "score": null
            }
          ],
          "latency_ms": 3.7730300009570783,
          "query_id": "compare_026",
          "dimension": "casual",
          "query": "redis vs memcached decision"
        },
        {
          "key_facts": [
            "Redis Cluster",
            "Memcached",
            "data structures",
            "persistence"
          ],
          "found_facts": [
            "Redis Cluster",
            "Memcached",
            "data structures",
            "persistence"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "adr_004"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "arch_caching_layer",
              "chunk_id": "arch_caching_layer_fix_0",
              "content": "# Caching Layer Architecture ## Overview The Caching Layer reduces latency and database load by caching frequently accessed data. Uses a multi-tier strategy with local and distributed caches. ## Design Principles - **Cache Invalidation**: Explicit invalidation on writes - **Graceful Degradation**: System works without cache ## Components ### Local Cache First-level cache with 100ms TTL. Reduces Redis round-trips for hot data.",
              "score": null
            },
            {
              "doc_id": "adr_004",
              "chunk_id": "adr_004_fix_0",
              "content": "# ADR-004: Use Redis for Caching and Session Storage **Status:** Accepted **Date:** 2024-04-05 ## Context We need a fast data store for caching API responses, session data, and rate limiting counters. ## Options Considered ### Option 1: Redis Cluster In-memory data structure store. **Pros:** - Sub-millisecond latency - Rich data structures - Pub/sub support - Clustering **Cons:** - Memory cost - Persistence complexity - Cluster management ### Option 2: Memcached Simple key-value cache.",
              "score": null
            },
            {
              "doc_id": "arch_caching_layer",
              "chunk_id": "arch_caching_layer_fix_1",
              "content": "**Technology:** Ristretto in-memory cache **Scaling:** Per-pod memory allocation ### Distributed Cache Second-level cache with configurable TTL. Maximum key size 512MB. **Technology:** Redis Cluster 7.x **Scaling:** 6-node cluster with sharding ### CDN Caches static assets and API responses. Cache hit ratio target 95%.",
              "score": null
            },
            {
              "doc_id": "config_cache",
              "chunk_id": "config_cache_fix_0",
              "content": "# Cache Configuration Reference ## Overview This document describes all configuration options for Cache. ## Environment Variables | Variable | Type | Default | Description | |----------|------|---------|-------------| | `REDIS_URL` | string | required | Redis connection string | | `REDIS_POOL_SIZE` | integer | 10 | Connection pool size | | `CACHE_TTL_SECONDS` | integer | 300 | Default cache TTL | | `CACHE_PREFIX` | string | cf: | Key prefix for namespacing | | `CACHE_COMPRESSION` |",
              "score": null
            },
            {
              "doc_id": "adr_004",
              "chunk_id": "adr_004_fix_1",
              "content": "**Pros:** - Simple operation - Multi-threaded - Lower memory overhead **Cons:** - No persistence - Limited data types - No clustering ### Option 3: Application-level cache In-process caching only. **Pros:** - No external dependency - Fastest possible - Simple **Cons:** - Not shared across pods - Memory pressure - Cold starts ## Decision We will use Redis Cluster for caching with a 6-node cluster for high availability.",
              "score": null
            }
          ],
          "latency_ms": 3.8341039999068016,
          "query_id": "compare_026",
          "dimension": "contextual",
          "query": "Evaluating cache solutions for a new microservice - why did we standardize on Redis over Memcached?"
        },
        {
          "key_facts": [
            "Redis Cluster",
            "Memcached",
            "data structures",
            "persistence"
          ],
          "found_facts": [
            "Redis Cluster",
            "Memcached",
            "data structures",
            "persistence"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "adr_004"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "arch_caching_layer",
              "chunk_id": "arch_caching_layer_fix_0",
              "content": "# Caching Layer Architecture ## Overview The Caching Layer reduces latency and database load by caching frequently accessed data. Uses a multi-tier strategy with local and distributed caches. ## Design Principles - **Cache Invalidation**: Explicit invalidation on writes - **Graceful Degradation**: System works without cache ## Components ### Local Cache First-level cache with 100ms TTL. Reduces Redis round-trips for hot data.",
              "score": null
            },
            {
              "doc_id": "adr_004",
              "chunk_id": "adr_004_fix_0",
              "content": "# ADR-004: Use Redis for Caching and Session Storage **Status:** Accepted **Date:** 2024-04-05 ## Context We need a fast data store for caching API responses, session data, and rate limiting counters. ## Options Considered ### Option 1: Redis Cluster In-memory data structure store. **Pros:** - Sub-millisecond latency - Rich data structures - Pub/sub support - Clustering **Cons:** - Memory cost - Persistence complexity - Cluster management ### Option 2: Memcached Simple key-value cache.",
              "score": null
            },
            {
              "doc_id": "adr_004",
              "chunk_id": "adr_004_fix_1",
              "content": "**Pros:** - Simple operation - Multi-threaded - Lower memory overhead **Cons:** - No persistence - Limited data types - No clustering ### Option 3: Application-level cache In-process caching only. **Pros:** - No external dependency - Fastest possible - Simple **Cons:** - Not shared across pods - Memory pressure - Cold starts ## Decision We will use Redis Cluster for caching with a 6-node cluster for high availability.",
              "score": null
            },
            {
              "doc_id": "adr_004",
              "chunk_id": "adr_004_fix_2",
              "content": "## Consequences - Implement cache-aside pattern - Set appropriate TTLs - Monitor memory usage - Plan for cache stampede prevention",
              "score": null
            },
            {
              "doc_id": "arch_caching_layer",
              "chunk_id": "arch_caching_layer_fix_1",
              "content": "**Technology:** Ristretto in-memory cache **Scaling:** Per-pod memory allocation ### Distributed Cache Second-level cache with configurable TTL. Maximum key size 512MB. **Technology:** Redis Cluster 7.x **Scaling:** 6-node cluster with sharding ### CDN Caches static assets and API responses. Cache hit ratio target 95%.",
              "score": null
            }
          ],
          "latency_ms": 3.725532000316889,
          "query_id": "compare_026",
          "dimension": "negation",
          "query": "Why isn't Memcached good enough if it's simpler to operate?"
        },
        {
          "key_facts": [
            "kubectl top pods",
            "cloudflow debug profile",
            "rollout restart"
          ],
          "found_facts": [
            "kubectl top pods"
          ],
          "missed_facts": [
            "cloudflow debug profile",
            "rollout restart"
          ],
          "coverage": 0.3333333333333333,
          "expected_docs": [
            "runbook_high_cpu_usage"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "runbook_high_cpu_usage",
              "chunk_id": "runbook_high_cpu_usage_fix_0",
              "content": "# Runbook: High CPU Usage ## Severity **Level:** P1 - Critical **Response Time:** 15 minutes ## Symptoms - CPU utilization > 80% sustained - Increased response latency - Autoscaler at maximum replicas ## Diagnosis ### Step 1 Check current CPU usage: `kubectl top pods -n production` ### Step 2 Identify hot pods: `kubectl top pods --sort-by=cpu -n production | head -10` ### Step 3 Check for recent deployments: `kubectl rollout history deployment/api -n production` ###",
              "score": null
            },
            {
              "doc_id": "runbook_database_connection_exhaustion",
              "chunk_id": "runbook_database_connection_exhaustion_fix_0",
              "content": "# Runbook: Database Connection Exhaustion ## Severity **Level:** P1 - Critical **Response Time:** 15 minutes ## Symptoms - Connection pool at 100% - Queries timing out - 'too many connections' errors in logs ## Diagnosis ### Step 1 Check active connections: `SELECT count(*) FROM pg_stat_activity` ### Step 2 Identify long-running queries: `SELECT pid, now() - pg_stat_activity.query_start AS duration, query FROM pg_stat_activity WHERE state != 'idle' ORDER BY duration DESC` ### Step 3 Check PgBouncer stats:",
              "score": null
            },
            {
              "doc_id": "runbook_high_cpu_usage",
              "chunk_id": "runbook_high_cpu_usage_fix_2",
              "content": "high after scaling, escalate to platform team. Page on-call if affecting users. ## Post-Incident - Document timeline in incident log - Schedule post-mortem within 48 hours - Update runbook if new patterns discovered",
              "score": null
            },
            {
              "doc_id": "runbook_out_of_memory_oom_kills",
              "chunk_id": "runbook_out_of_memory_oom_kills_fix_1",
              "content": "grep -A5 Limits` ### Step 4 Analyze heap dumps if available: `cloudflow debug heapdump --pod <pod-name>` ## Resolution ### Step 1 Increase memory limit: update deployment resources.limits.memory ### Step 2 Add memory request equal to limit to guarantee allocation ### Step 3 Enable JVM heap dump on OOM: add -XX:+HeapDumpOnOutOfMemoryError ### Step 4 Investigate memory leaks with profiling tools ### Step 5 Consider horizontal scaling instead of vertical ## Escalation If OOMs continue after limit",
              "score": null
            },
            {
              "doc_id": "runbook_out_of_memory_oom_kills",
              "chunk_id": "runbook_out_of_memory_oom_kills_fix_2",
              "content": "increase, escalate to development team for memory leak investigation. ## Post-Incident - Document timeline in incident log - Schedule post-mortem within 48 hours - Update runbook if new patterns discovered",
              "score": null
            }
          ],
          "latency_ms": 4.4462050009315135,
          "query_id": "troubleshoot_027",
          "dimension": "original",
          "query": "How do I diagnose high CPU usage?"
        },
        {
          "key_facts": [
            "kubectl top pods",
            "cloudflow debug profile",
            "rollout restart"
          ],
          "found_facts": [
            "kubectl top pods"
          ],
          "missed_facts": [
            "cloudflow debug profile",
            "rollout restart"
          ],
          "coverage": 0.3333333333333333,
          "expected_docs": [
            "runbook_high_cpu_usage"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "runbook_high_cpu_usage",
              "chunk_id": "runbook_high_cpu_usage_fix_0",
              "content": "# Runbook: High CPU Usage ## Severity **Level:** P1 - Critical **Response Time:** 15 minutes ## Symptoms - CPU utilization > 80% sustained - Increased response latency - Autoscaler at maximum replicas ## Diagnosis ### Step 1 Check current CPU usage: `kubectl top pods -n production` ### Step 2 Identify hot pods: `kubectl top pods --sort-by=cpu -n production | head -10` ### Step 3 Check for recent deployments: `kubectl rollout history deployment/api -n production` ###",
              "score": null
            },
            {
              "doc_id": "runbook_kafka_consumer_lag",
              "chunk_id": "runbook_kafka_consumer_lag_fix_1",
              "content": "grep ERROR` ### Step 4 Monitor throughput: `cloudflow metrics get kafka.consumer.records_per_second` ## Resolution ### Step 1 Scale up consumers: `kubectl scale deployment/workflow-executor --replicas=10 -n production` ### Step 2 Reset offset if messages are stale: `kafka-consumer-groups.sh --bootstrap-server kafka:9092 --group workflow-executor --reset-offsets --to-latest --execute --topic workflows` ### Step 3 Pause producer if backpressure needed: `cloudflow workflows pause-triggers` ### Step 4 Increase partition count for parallelism (requires coordination) ## Escalation If lag persists > 30 minutes, page platform",
              "score": null
            },
            {
              "doc_id": "runbook_database_connection_exhaustion",
              "chunk_id": "runbook_database_connection_exhaustion_fix_0",
              "content": "# Runbook: Database Connection Exhaustion ## Severity **Level:** P1 - Critical **Response Time:** 15 minutes ## Symptoms - Connection pool at 100% - Queries timing out - 'too many connections' errors in logs ## Diagnosis ### Step 1 Check active connections: `SELECT count(*) FROM pg_stat_activity` ### Step 2 Identify long-running queries: `SELECT pid, now() - pg_stat_activity.query_start AS duration, query FROM pg_stat_activity WHERE state != 'idle' ORDER BY duration DESC` ### Step 3 Check PgBouncer stats:",
              "score": null
            },
            {
              "doc_id": "runbook_high_cpu_usage",
              "chunk_id": "runbook_high_cpu_usage_fix_2",
              "content": "high after scaling, escalate to platform team. Page on-call if affecting users. ## Post-Incident - Document timeline in incident log - Schedule post-mortem within 48 hours - Update runbook if new patterns discovered",
              "score": null
            },
            {
              "doc_id": "arch_caching_layer",
              "chunk_id": "arch_caching_layer_fix_1",
              "content": "**Technology:** Ristretto in-memory cache **Scaling:** Per-pod memory allocation ### Distributed Cache Second-level cache with configurable TTL. Maximum key size 512MB. **Technology:** Redis Cluster 7.x **Scaling:** 6-node cluster with sharding ### CDN Caches static assets and API responses. Cache hit ratio target 95%.",
              "score": null
            }
          ],
          "latency_ms": 3.588245999708306,
          "query_id": "troubleshoot_027",
          "dimension": "synonym",
          "query": "How do I troubleshoot elevated processor utilization in the cluster?"
        },
        {
          "key_facts": [
            "kubectl top pods",
            "cloudflow debug profile",
            "rollout restart"
          ],
          "found_facts": [
            "kubectl top pods",
            "rollout restart"
          ],
          "missed_facts": [
            "cloudflow debug profile"
          ],
          "coverage": 0.6666666666666666,
          "expected_docs": [
            "runbook_high_cpu_usage"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "runbook_high_cpu_usage",
              "chunk_id": "runbook_high_cpu_usage_fix_0",
              "content": "# Runbook: High CPU Usage ## Severity **Level:** P1 - Critical **Response Time:** 15 minutes ## Symptoms - CPU utilization > 80% sustained - Increased response latency - Autoscaler at maximum replicas ## Diagnosis ### Step 1 Check current CPU usage: `kubectl top pods -n production` ### Step 2 Identify hot pods: `kubectl top pods --sort-by=cpu -n production | head -10` ### Step 3 Check for recent deployments: `kubectl rollout history deployment/api -n production` ###",
              "score": null
            },
            {
              "doc_id": "runbook_out_of_memory_oom_kills",
              "chunk_id": "runbook_out_of_memory_oom_kills_fix_0",
              "content": "# Runbook: Out of Memory (OOM) Kills ## Severity **Level:** P3 - Medium **Response Time:** 15 minutes ## Symptoms - Pods restarting with OOMKilled status - Memory usage at limit - Application slowdown before restart ## Diagnosis ### Step 1 Check OOM events: `kubectl get events -n production --field-selector reason=OOMKilled` ### Step 2 Review memory usage: `kubectl top pods -n production --sort-by=memory` ### Step 3 Check container limits: `kubectl describe pod <pod-name> -n production |",
              "score": null
            },
            {
              "doc_id": "runbook_api_gateway_5xx_errors",
              "chunk_id": "runbook_api_gateway_5xx_errors_fix_1",
              "content": "-n production` ### Step 4 Test backend directly: `kubectl exec -it kong-xxx -n kong -- curl http://api.production.svc:8080/health` ## Resolution ### Step 1 Restart unhealthy backends: `kubectl rollout restart deployment/api -n production` ### Step 2 Enable circuit breaker if cascading: update Kong circuit-breaker plugin ### Step 3 Scale up backends: `kubectl scale deployment/api --replicas=5 -n production` ### Step 4 Check for resource exhaustion on backend pods ### Step 5 Verify network policies allow traffic ## Escalation",
              "score": null
            },
            {
              "doc_id": "runbook_kafka_consumer_lag",
              "chunk_id": "runbook_kafka_consumer_lag_fix_0",
              "content": "# Runbook: Kafka Consumer Lag ## Severity **Level:** P1 - Critical **Response Time:** 1 hour ## Symptoms - Consumer lag > 10000 messages - Processing delays reported - Alerts from lag monitor ## Diagnosis ### Step 1 Check consumer group lag: `kafka-consumer-groups.sh --bootstrap-server kafka:9092 --describe --group workflow-executor` ### Step 2 Verify consumer health: `kubectl get pods -l app=workflow-executor -n production` ### Step 3 Check for processing errors: `kubectl logs -l app=workflow-executor --tail=100 -n production |",
              "score": null
            },
            {
              "doc_id": "runbook_api_gateway_5xx_errors",
              "chunk_id": "runbook_api_gateway_5xx_errors_fix_0",
              "content": "# Runbook: API Gateway 5xx Errors ## Severity **Level:** P3 - Medium **Response Time:** 1 hour ## Symptoms - Error rate > 1% - 5xx responses in access logs - Downstream service failures ## Diagnosis ### Step 1 Check Kong error logs: `kubectl logs -l app=kong -n kong --tail=100 | grep -E '5[0-9]{2}'` ### Step 2 Verify backend health: `kubectl get pods -n production -o wide` ### Step 3 Check service endpoints: `kubectl get endpoints",
              "score": null
            }
          ],
          "latency_ms": 3.6806579992116895,
          "query_id": "troubleshoot_027",
          "dimension": "problem",
          "query": "kubectl top shows pods at 95% CPU and response times are spiking to 5+ seconds"
        },
        {
          "key_facts": [
            "kubectl top pods",
            "cloudflow debug profile",
            "rollout restart"
          ],
          "found_facts": [
            "kubectl top pods",
            "cloudflow debug profile",
            "rollout restart"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "runbook_high_cpu_usage"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "runbook_high_cpu_usage",
              "chunk_id": "runbook_high_cpu_usage_fix_0",
              "content": "# Runbook: High CPU Usage ## Severity **Level:** P1 - Critical **Response Time:** 15 minutes ## Symptoms - CPU utilization > 80% sustained - Increased response latency - Autoscaler at maximum replicas ## Diagnosis ### Step 1 Check current CPU usage: `kubectl top pods -n production` ### Step 2 Identify hot pods: `kubectl top pods --sort-by=cpu -n production | head -10` ### Step 3 Check for recent deployments: `kubectl rollout history deployment/api -n production` ###",
              "score": null
            },
            {
              "doc_id": "runbook_out_of_memory_oom_kills",
              "chunk_id": "runbook_out_of_memory_oom_kills_fix_0",
              "content": "# Runbook: Out of Memory (OOM) Kills ## Severity **Level:** P3 - Medium **Response Time:** 15 minutes ## Symptoms - Pods restarting with OOMKilled status - Memory usage at limit - Application slowdown before restart ## Diagnosis ### Step 1 Check OOM events: `kubectl get events -n production --field-selector reason=OOMKilled` ### Step 2 Review memory usage: `kubectl top pods -n production --sort-by=memory` ### Step 3 Check container limits: `kubectl describe pod <pod-name> -n production |",
              "score": null
            },
            {
              "doc_id": "runbook_high_cpu_usage",
              "chunk_id": "runbook_high_cpu_usage_fix_1",
              "content": "Step 4 Profile the application: `cloudflow debug profile --pod api-xyz --duration 60s` ## Resolution ### Step 1 Scale up if traffic-related: `kubectl scale deployment/api --replicas=10 -n production` ### Step 2 Restart pods if memory leak suspected: `kubectl rollout restart deployment/api -n production` ### Step 3 Roll back recent deployment if regression: `kubectl rollout undo deployment/api -n production` ### Step 4 Add resource limits if unbounded: update deployment with CPU limits ## Escalation If CPU remains",
              "score": null
            },
            {
              "doc_id": "adr_004",
              "chunk_id": "adr_004_fix_1",
              "content": "**Pros:** - Simple operation - Multi-threaded - Lower memory overhead **Cons:** - No persistence - Limited data types - No clustering ### Option 3: Application-level cache In-process caching only. **Pros:** - No external dependency - Fastest possible - Simple **Cons:** - Not shared across pods - Memory pressure - Cold starts ## Decision We will use Redis Cluster for caching with a 6-node cluster for high availability.",
              "score": null
            },
            {
              "doc_id": "runbook_api_gateway_5xx_errors",
              "chunk_id": "runbook_api_gateway_5xx_errors_fix_1",
              "content": "-n production` ### Step 4 Test backend directly: `kubectl exec -it kong-xxx -n kong -- curl http://api.production.svc:8080/health` ## Resolution ### Step 1 Restart unhealthy backends: `kubectl rollout restart deployment/api -n production` ### Step 2 Enable circuit breaker if cascading: update Kong circuit-breaker plugin ### Step 3 Scale up backends: `kubectl scale deployment/api --replicas=5 -n production` ### Step 4 Check for resource exhaustion on backend pods ### Step 5 Verify network policies allow traffic ## Escalation",
              "score": null
            }
          ],
          "latency_ms": 3.3441809991927585,
          "query_id": "troubleshoot_027",
          "dimension": "casual",
          "query": "pods maxing out cpu, what now"
        },
        {
          "key_facts": [
            "kubectl top pods",
            "cloudflow debug profile",
            "rollout restart"
          ],
          "found_facts": [
            "kubectl top pods"
          ],
          "missed_facts": [
            "cloudflow debug profile",
            "rollout restart"
          ],
          "coverage": 0.3333333333333333,
          "expected_docs": [
            "runbook_high_cpu_usage"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "runbook_high_cpu_usage",
              "chunk_id": "runbook_high_cpu_usage_fix_0",
              "content": "# Runbook: High CPU Usage ## Severity **Level:** P1 - Critical **Response Time:** 15 minutes ## Symptoms - CPU utilization > 80% sustained - Increased response latency - Autoscaler at maximum replicas ## Diagnosis ### Step 1 Check current CPU usage: `kubectl top pods -n production` ### Step 2 Identify hot pods: `kubectl top pods --sort-by=cpu -n production | head -10` ### Step 3 Check for recent deployments: `kubectl rollout history deployment/api -n production` ###",
              "score": null
            },
            {
              "doc_id": "runbook_out_of_memory_oom_kills",
              "chunk_id": "runbook_out_of_memory_oom_kills_fix_1",
              "content": "grep -A5 Limits` ### Step 4 Analyze heap dumps if available: `cloudflow debug heapdump --pod <pod-name>` ## Resolution ### Step 1 Increase memory limit: update deployment resources.limits.memory ### Step 2 Add memory request equal to limit to guarantee allocation ### Step 3 Enable JVM heap dump on OOM: add -XX:+HeapDumpOnOutOfMemoryError ### Step 4 Investigate memory leaks with profiling tools ### Step 5 Consider horizontal scaling instead of vertical ## Escalation If OOMs continue after limit",
              "score": null
            },
            {
              "doc_id": "runbook_high_cpu_usage",
              "chunk_id": "runbook_high_cpu_usage_fix_2",
              "content": "high after scaling, escalate to platform team. Page on-call if affecting users. ## Post-Incident - Document timeline in incident log - Schedule post-mortem within 48 hours - Update runbook if new patterns discovered",
              "score": null
            },
            {
              "doc_id": "runbook_kafka_consumer_lag",
              "chunk_id": "runbook_kafka_consumer_lag_fix_1",
              "content": "grep ERROR` ### Step 4 Monitor throughput: `cloudflow metrics get kafka.consumer.records_per_second` ## Resolution ### Step 1 Scale up consumers: `kubectl scale deployment/workflow-executor --replicas=10 -n production` ### Step 2 Reset offset if messages are stale: `kafka-consumer-groups.sh --bootstrap-server kafka:9092 --group workflow-executor --reset-offsets --to-latest --execute --topic workflows` ### Step 3 Pause producer if backpressure needed: `cloudflow workflows pause-triggers` ### Step 4 Increase partition count for parallelism (requires coordination) ## Escalation If lag persists > 30 minutes, page platform",
              "score": null
            },
            {
              "doc_id": "runbook_out_of_memory_oom_kills",
              "chunk_id": "runbook_out_of_memory_oom_kills_fix_2",
              "content": "increase, escalate to development team for memory leak investigation. ## Post-Incident - Document timeline in incident log - Schedule post-mortem within 48 hours - Update runbook if new patterns discovered",
              "score": null
            }
          ],
          "latency_ms": 3.5316199991939357,
          "query_id": "troubleshoot_027",
          "dimension": "contextual",
          "query": "getting paged about high CPU in production, autoscaler is already at max replicas, need to find the root cause"
        },
        {
          "key_facts": [
            "kubectl top pods",
            "cloudflow debug profile",
            "rollout restart"
          ],
          "found_facts": [
            "kubectl top pods",
            "cloudflow debug profile",
            "rollout restart"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "runbook_high_cpu_usage"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "runbook_high_cpu_usage",
              "chunk_id": "runbook_high_cpu_usage_fix_0",
              "content": "# Runbook: High CPU Usage ## Severity **Level:** P1 - Critical **Response Time:** 15 minutes ## Symptoms - CPU utilization > 80% sustained - Increased response latency - Autoscaler at maximum replicas ## Diagnosis ### Step 1 Check current CPU usage: `kubectl top pods -n production` ### Step 2 Identify hot pods: `kubectl top pods --sort-by=cpu -n production | head -10` ### Step 3 Check for recent deployments: `kubectl rollout history deployment/api -n production` ###",
              "score": null
            },
            {
              "doc_id": "runbook_high_cpu_usage",
              "chunk_id": "runbook_high_cpu_usage_fix_1",
              "content": "Step 4 Profile the application: `cloudflow debug profile --pod api-xyz --duration 60s` ## Resolution ### Step 1 Scale up if traffic-related: `kubectl scale deployment/api --replicas=10 -n production` ### Step 2 Restart pods if memory leak suspected: `kubectl rollout restart deployment/api -n production` ### Step 3 Roll back recent deployment if regression: `kubectl rollout undo deployment/api -n production` ### Step 4 Add resource limits if unbounded: update deployment with CPU limits ## Escalation If CPU remains",
              "score": null
            },
            {
              "doc_id": "runbook_out_of_memory_oom_kills",
              "chunk_id": "runbook_out_of_memory_oom_kills_fix_1",
              "content": "grep -A5 Limits` ### Step 4 Analyze heap dumps if available: `cloudflow debug heapdump --pod <pod-name>` ## Resolution ### Step 1 Increase memory limit: update deployment resources.limits.memory ### Step 2 Add memory request equal to limit to guarantee allocation ### Step 3 Enable JVM heap dump on OOM: add -XX:+HeapDumpOnOutOfMemoryError ### Step 4 Investigate memory leaks with profiling tools ### Step 5 Consider horizontal scaling instead of vertical ## Escalation If OOMs continue after limit",
              "score": null
            },
            {
              "doc_id": "runbook_kafka_consumer_lag",
              "chunk_id": "runbook_kafka_consumer_lag_fix_1",
              "content": "grep ERROR` ### Step 4 Monitor throughput: `cloudflow metrics get kafka.consumer.records_per_second` ## Resolution ### Step 1 Scale up consumers: `kubectl scale deployment/workflow-executor --replicas=10 -n production` ### Step 2 Reset offset if messages are stale: `kafka-consumer-groups.sh --bootstrap-server kafka:9092 --group workflow-executor --reset-offsets --to-latest --execute --topic workflows` ### Step 3 Pause producer if backpressure needed: `cloudflow workflows pause-triggers` ### Step 4 Increase partition count for parallelism (requires coordination) ## Escalation If lag persists > 30 minutes, page platform",
              "score": null
            },
            {
              "doc_id": "runbook_api_gateway_5xx_errors",
              "chunk_id": "runbook_api_gateway_5xx_errors_fix_1",
              "content": "-n production` ### Step 4 Test backend directly: `kubectl exec -it kong-xxx -n kong -- curl http://api.production.svc:8080/health` ## Resolution ### Step 1 Restart unhealthy backends: `kubectl rollout restart deployment/api -n production` ### Step 2 Enable circuit breaker if cascading: update Kong circuit-breaker plugin ### Step 3 Scale up backends: `kubectl scale deployment/api --replicas=5 -n production` ### Step 4 Check for resource exhaustion on backend pods ### Step 5 Verify network policies allow traffic ## Escalation",
              "score": null
            }
          ],
          "latency_ms": 3.4531339988461696,
          "query_id": "troubleshoot_027",
          "dimension": "negation",
          "query": "why is CPU stuck at 80% even after I scaled up the deployment?"
        },
        {
          "key_facts": [
            "pg_stat_activity",
            "SHOW POOLS",
            "pg_terminate_backend"
          ],
          "found_facts": [
            "pg_stat_activity",
            "SHOW POOLS",
            "pg_terminate_backend"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "runbook_database_connection_ex"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "runbook_database_connection_exhaustion",
              "chunk_id": "runbook_database_connection_exhaustion_fix_0",
              "content": "# Runbook: Database Connection Exhaustion ## Severity **Level:** P1 - Critical **Response Time:** 15 minutes ## Symptoms - Connection pool at 100% - Queries timing out - 'too many connections' errors in logs ## Diagnosis ### Step 1 Check active connections: `SELECT count(*) FROM pg_stat_activity` ### Step 2 Identify long-running queries: `SELECT pid, now() - pg_stat_activity.query_start AS duration, query FROM pg_stat_activity WHERE state != 'idle' ORDER BY duration DESC` ### Step 3 Check PgBouncer stats:",
              "score": null
            },
            {
              "doc_id": "runbook_database_connection_exhaustion",
              "chunk_id": "runbook_database_connection_exhaustion_fix_1",
              "content": "`SHOW POOLS` ### Step 4 Verify connection limits: `SHOW max_connections` ## Resolution ### Step 1 Kill long-running queries: `SELECT pg_terminate_backend(pid) FROM pg_stat_activity WHERE duration > interval '5 minutes'` ### Step 2 Increase PgBouncer pool size temporarily: update pgbouncer.ini ### Step 3 Restart affected services to release connections: `kubectl rollout restart deployment/api` ### Step 4 Scale down non-critical services to free connections ## Escalation If connections remain exhausted, escalate to DBA. Consider enabling connection queueing.",
              "score": null
            },
            {
              "doc_id": "config_database",
              "chunk_id": "config_database_fix_0",
              "content": "# Database Configuration Reference ## Overview This document describes all configuration options for Database. ## Environment Variables | Variable | Type | Default | Description | |----------|------|---------|-------------| | `DATABASE_URL` | string | required | PostgreSQL connection string | | `DATABASE_POOL_MIN` | integer | 5 | Minimum connection pool size | | `DATABASE_POOL_MAX` | integer | 20 | Maximum connection pool size | | `DATABASE_STATEMENT_TIMEOUT` | integer | 30000 | Query timeout in milliseconds | |",
              "score": null
            },
            {
              "doc_id": "howto_set_up_database_connection",
              "chunk_id": "howto_set_up_database_connection_fix_1",
              "content": "> **Note:** Pool exhaustion will queue requests ### Step 3: Test connection Verify the database is accessible from CloudFlow. ```bash cloudflow db test --secret DATABASE_URL ``` ### Step 4: Enable SSL For production, always require SSL connections. ```bash cloudflow db configure --require-ssl ``` ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            },
            {
              "doc_id": "config_database",
              "chunk_id": "config_database_fix_1",
              "content": "`DATABASE_SSL_MODE` | string | require | SSL mode: disable, require, verify-full | | `DATABASE_MIGRATION_AUTO` | boolean | false | Run migrations on startup | ## Configuration File Configuration can also be provided via `database.yaml`: ```yaml database_url: required database_pool_min: 5 database_pool_max: 20 database_statement_timeout: 30000 database_ssl_mode: require ``` ## Validation Configuration is validated at startup. Invalid configuration will prevent the service from starting.",
              "score": null
            }
          ],
          "latency_ms": 3.2548150011280086,
          "query_id": "troubleshoot_028",
          "dimension": "original",
          "query": "What to do when database connections are exhausted?"
        },
        {
          "key_facts": [
            "pg_stat_activity",
            "SHOW POOLS",
            "pg_terminate_backend"
          ],
          "found_facts": [
            "pg_stat_activity",
            "SHOW POOLS",
            "pg_terminate_backend"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "runbook_database_connection_ex"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "runbook_database_connection_exhaustion",
              "chunk_id": "runbook_database_connection_exhaustion_fix_1",
              "content": "`SHOW POOLS` ### Step 4 Verify connection limits: `SHOW max_connections` ## Resolution ### Step 1 Kill long-running queries: `SELECT pg_terminate_backend(pid) FROM pg_stat_activity WHERE duration > interval '5 minutes'` ### Step 2 Increase PgBouncer pool size temporarily: update pgbouncer.ini ### Step 3 Restart affected services to release connections: `kubectl rollout restart deployment/api` ### Step 4 Scale down non-critical services to free connections ## Escalation If connections remain exhausted, escalate to DBA. Consider enabling connection queueing.",
              "score": null
            },
            {
              "doc_id": "config_database",
              "chunk_id": "config_database_fix_0",
              "content": "# Database Configuration Reference ## Overview This document describes all configuration options for Database. ## Environment Variables | Variable | Type | Default | Description | |----------|------|---------|-------------| | `DATABASE_URL` | string | required | PostgreSQL connection string | | `DATABASE_POOL_MIN` | integer | 5 | Minimum connection pool size | | `DATABASE_POOL_MAX` | integer | 20 | Maximum connection pool size | | `DATABASE_STATEMENT_TIMEOUT` | integer | 30000 | Query timeout in milliseconds | |",
              "score": null
            },
            {
              "doc_id": "runbook_database_connection_exhaustion",
              "chunk_id": "runbook_database_connection_exhaustion_fix_0",
              "content": "# Runbook: Database Connection Exhaustion ## Severity **Level:** P1 - Critical **Response Time:** 15 minutes ## Symptoms - Connection pool at 100% - Queries timing out - 'too many connections' errors in logs ## Diagnosis ### Step 1 Check active connections: `SELECT count(*) FROM pg_stat_activity` ### Step 2 Identify long-running queries: `SELECT pid, now() - pg_stat_activity.query_start AS duration, query FROM pg_stat_activity WHERE state != 'idle' ORDER BY duration DESC` ### Step 3 Check PgBouncer stats:",
              "score": null
            },
            {
              "doc_id": "howto_set_up_database_connection",
              "chunk_id": "howto_set_up_database_connection_fix_1",
              "content": "> **Note:** Pool exhaustion will queue requests ### Step 3: Test connection Verify the database is accessible from CloudFlow. ```bash cloudflow db test --secret DATABASE_URL ``` ### Step 4: Enable SSL For production, always require SSL connections. ```bash cloudflow db configure --require-ssl ``` ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            },
            {
              "doc_id": "config_workflow_engine",
              "chunk_id": "config_workflow_engine_fix_1",
              "content": "concurrent executions per workflow | | `EXECUTOR_POOL_SIZE` | integer | 50 | Number of executor workers | | `EXECUTOR_MEMORY_LIMIT` | string | 512Mi | Memory limit per executor | ## Configuration File Configuration can also be provided via `workflow engine.yaml`: ```yaml workflow_max_steps: 100 workflow_timeout_seconds: 3600 workflow_retry_limit: 3 workflow_concurrent_limit: 10 executor_pool_size: 50 ``` ## Validation Configuration is validated at startup. Invalid configuration will prevent the service from starting.",
              "score": null
            }
          ],
          "latency_ms": 3.7871870008530095,
          "query_id": "troubleshoot_028",
          "dimension": "synonym",
          "query": "How do I handle DB connection pool depletion?"
        },
        {
          "key_facts": [
            "pg_stat_activity",
            "SHOW POOLS",
            "pg_terminate_backend"
          ],
          "found_facts": [
            "pg_stat_activity",
            "SHOW POOLS",
            "pg_terminate_backend"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "runbook_database_connection_ex"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "runbook_database_connection_exhaustion",
              "chunk_id": "runbook_database_connection_exhaustion_fix_0",
              "content": "# Runbook: Database Connection Exhaustion ## Severity **Level:** P1 - Critical **Response Time:** 15 minutes ## Symptoms - Connection pool at 100% - Queries timing out - 'too many connections' errors in logs ## Diagnosis ### Step 1 Check active connections: `SELECT count(*) FROM pg_stat_activity` ### Step 2 Identify long-running queries: `SELECT pid, now() - pg_stat_activity.query_start AS duration, query FROM pg_stat_activity WHERE state != 'idle' ORDER BY duration DESC` ### Step 3 Check PgBouncer stats:",
              "score": null
            },
            {
              "doc_id": "runbook_database_connection_exhaustion",
              "chunk_id": "runbook_database_connection_exhaustion_fix_1",
              "content": "`SHOW POOLS` ### Step 4 Verify connection limits: `SHOW max_connections` ## Resolution ### Step 1 Kill long-running queries: `SELECT pg_terminate_backend(pid) FROM pg_stat_activity WHERE duration > interval '5 minutes'` ### Step 2 Increase PgBouncer pool size temporarily: update pgbouncer.ini ### Step 3 Restart affected services to release connections: `kubectl rollout restart deployment/api` ### Step 4 Scale down non-critical services to free connections ## Escalation If connections remain exhausted, escalate to DBA. Consider enabling connection queueing.",
              "score": null
            },
            {
              "doc_id": "runbook_api_gateway_5xx_errors",
              "chunk_id": "runbook_api_gateway_5xx_errors_fix_2",
              "content": "If 5xx persists > 15 minutes, page on-call. May indicate infrastructure issue. ## Post-Incident - Document timeline in incident log - Schedule post-mortem within 48 hours - Update runbook if new patterns discovered",
              "score": null
            },
            {
              "doc_id": "runbook_api_gateway_5xx_errors",
              "chunk_id": "runbook_api_gateway_5xx_errors_fix_0",
              "content": "# Runbook: API Gateway 5xx Errors ## Severity **Level:** P3 - Medium **Response Time:** 1 hour ## Symptoms - Error rate > 1% - 5xx responses in access logs - Downstream service failures ## Diagnosis ### Step 1 Check Kong error logs: `kubectl logs -l app=kong -n kong --tail=100 | grep -E '5[0-9]{2}'` ### Step 2 Verify backend health: `kubectl get pods -n production -o wide` ### Step 3 Check service endpoints: `kubectl get endpoints",
              "score": null
            },
            {
              "doc_id": "runbook_out_of_memory_oom_kills",
              "chunk_id": "runbook_out_of_memory_oom_kills_fix_2",
              "content": "increase, escalate to development team for memory leak investigation. ## Post-Incident - Document timeline in incident log - Schedule post-mortem within 48 hours - Update runbook if new patterns discovered",
              "score": null
            }
          ],
          "latency_ms": 3.9038340000843164,
          "query_id": "troubleshoot_028",
          "dimension": "problem",
          "query": "seeing 'too many connections' errors in logs and queries are timing out after 30 seconds"
        },
        {
          "key_facts": [
            "pg_stat_activity",
            "SHOW POOLS",
            "pg_terminate_backend"
          ],
          "found_facts": [
            "pg_stat_activity",
            "SHOW POOLS",
            "pg_terminate_backend"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "runbook_database_connection_ex"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "runbook_database_connection_exhaustion",
              "chunk_id": "runbook_database_connection_exhaustion_fix_1",
              "content": "`SHOW POOLS` ### Step 4 Verify connection limits: `SHOW max_connections` ## Resolution ### Step 1 Kill long-running queries: `SELECT pg_terminate_backend(pid) FROM pg_stat_activity WHERE duration > interval '5 minutes'` ### Step 2 Increase PgBouncer pool size temporarily: update pgbouncer.ini ### Step 3 Restart affected services to release connections: `kubectl rollout restart deployment/api` ### Step 4 Scale down non-critical services to free connections ## Escalation If connections remain exhausted, escalate to DBA. Consider enabling connection queueing.",
              "score": null
            },
            {
              "doc_id": "config_database",
              "chunk_id": "config_database_fix_0",
              "content": "# Database Configuration Reference ## Overview This document describes all configuration options for Database. ## Environment Variables | Variable | Type | Default | Description | |----------|------|---------|-------------| | `DATABASE_URL` | string | required | PostgreSQL connection string | | `DATABASE_POOL_MIN` | integer | 5 | Minimum connection pool size | | `DATABASE_POOL_MAX` | integer | 20 | Maximum connection pool size | | `DATABASE_STATEMENT_TIMEOUT` | integer | 30000 | Query timeout in milliseconds | |",
              "score": null
            },
            {
              "doc_id": "runbook_database_connection_exhaustion",
              "chunk_id": "runbook_database_connection_exhaustion_fix_0",
              "content": "# Runbook: Database Connection Exhaustion ## Severity **Level:** P1 - Critical **Response Time:** 15 minutes ## Symptoms - Connection pool at 100% - Queries timing out - 'too many connections' errors in logs ## Diagnosis ### Step 1 Check active connections: `SELECT count(*) FROM pg_stat_activity` ### Step 2 Identify long-running queries: `SELECT pid, now() - pg_stat_activity.query_start AS duration, query FROM pg_stat_activity WHERE state != 'idle' ORDER BY duration DESC` ### Step 3 Check PgBouncer stats:",
              "score": null
            },
            {
              "doc_id": "howto_set_up_database_connection",
              "chunk_id": "howto_set_up_database_connection_fix_1",
              "content": "> **Note:** Pool exhaustion will queue requests ### Step 3: Test connection Verify the database is accessible from CloudFlow. ```bash cloudflow db test --secret DATABASE_URL ``` ### Step 4: Enable SSL For production, always require SSL connections. ```bash cloudflow db configure --require-ssl ``` ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            },
            {
              "doc_id": "config_workflow_engine",
              "chunk_id": "config_workflow_engine_fix_1",
              "content": "concurrent executions per workflow | | `EXECUTOR_POOL_SIZE` | integer | 50 | Number of executor workers | | `EXECUTOR_MEMORY_LIMIT` | string | 512Mi | Memory limit per executor | ## Configuration File Configuration can also be provided via `workflow engine.yaml`: ```yaml workflow_max_steps: 100 workflow_timeout_seconds: 3600 workflow_retry_limit: 3 workflow_concurrent_limit: 10 executor_pool_size: 50 ``` ## Validation Configuration is validated at startup. Invalid configuration will prevent the service from starting.",
              "score": null
            }
          ],
          "latency_ms": 3.460607998931664,
          "query_id": "troubleshoot_028",
          "dimension": "casual",
          "query": "db connection pool maxed out"
        },
        {
          "key_facts": [
            "pg_stat_activity",
            "SHOW POOLS",
            "pg_terminate_backend"
          ],
          "found_facts": [
            "pg_stat_activity",
            "SHOW POOLS",
            "pg_terminate_backend"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "runbook_database_connection_ex"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "config_database",
              "chunk_id": "config_database_fix_1",
              "content": "`DATABASE_SSL_MODE` | string | require | SSL mode: disable, require, verify-full | | `DATABASE_MIGRATION_AUTO` | boolean | false | Run migrations on startup | ## Configuration File Configuration can also be provided via `database.yaml`: ```yaml database_url: required database_pool_min: 5 database_pool_max: 20 database_statement_timeout: 30000 database_ssl_mode: require ``` ## Validation Configuration is validated at startup. Invalid configuration will prevent the service from starting.",
              "score": null
            },
            {
              "doc_id": "runbook_database_connection_exhaustion",
              "chunk_id": "runbook_database_connection_exhaustion_fix_1",
              "content": "`SHOW POOLS` ### Step 4 Verify connection limits: `SHOW max_connections` ## Resolution ### Step 1 Kill long-running queries: `SELECT pg_terminate_backend(pid) FROM pg_stat_activity WHERE duration > interval '5 minutes'` ### Step 2 Increase PgBouncer pool size temporarily: update pgbouncer.ini ### Step 3 Restart affected services to release connections: `kubectl rollout restart deployment/api` ### Step 4 Scale down non-critical services to free connections ## Escalation If connections remain exhausted, escalate to DBA. Consider enabling connection queueing.",
              "score": null
            },
            {
              "doc_id": "config_database",
              "chunk_id": "config_database_fix_0",
              "content": "# Database Configuration Reference ## Overview This document describes all configuration options for Database. ## Environment Variables | Variable | Type | Default | Description | |----------|------|---------|-------------| | `DATABASE_URL` | string | required | PostgreSQL connection string | | `DATABASE_POOL_MIN` | integer | 5 | Minimum connection pool size | | `DATABASE_POOL_MAX` | integer | 20 | Maximum connection pool size | | `DATABASE_STATEMENT_TIMEOUT` | integer | 30000 | Query timeout in milliseconds | |",
              "score": null
            },
            {
              "doc_id": "runbook_database_connection_exhaustion",
              "chunk_id": "runbook_database_connection_exhaustion_fix_0",
              "content": "# Runbook: Database Connection Exhaustion ## Severity **Level:** P1 - Critical **Response Time:** 15 minutes ## Symptoms - Connection pool at 100% - Queries timing out - 'too many connections' errors in logs ## Diagnosis ### Step 1 Check active connections: `SELECT count(*) FROM pg_stat_activity` ### Step 2 Identify long-running queries: `SELECT pid, now() - pg_stat_activity.query_start AS duration, query FROM pg_stat_activity WHERE state != 'idle' ORDER BY duration DESC` ### Step 3 Check PgBouncer stats:",
              "score": null
            },
            {
              "doc_id": "howto_set_up_database_connection",
              "chunk_id": "howto_set_up_database_connection_fix_1",
              "content": "> **Note:** Pool exhaustion will queue requests ### Step 3: Test connection Verify the database is accessible from CloudFlow. ```bash cloudflow db test --secret DATABASE_URL ``` ### Step 4: Enable SSL For production, always require SSL connections. ```bash cloudflow db configure --require-ssl ``` ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            }
          ],
          "latency_ms": 3.4482259998185327,
          "query_id": "troubleshoot_028",
          "dimension": "contextual",
          "query": "just deployed a new feature and now the database is rejecting new connections, users are seeing errors"
        },
        {
          "key_facts": [
            "pg_stat_activity",
            "SHOW POOLS",
            "pg_terminate_backend"
          ],
          "found_facts": [
            "pg_stat_activity",
            "SHOW POOLS",
            "pg_terminate_backend"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "runbook_database_connection_ex"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "runbook_database_connection_exhaustion",
              "chunk_id": "runbook_database_connection_exhaustion_fix_0",
              "content": "# Runbook: Database Connection Exhaustion ## Severity **Level:** P1 - Critical **Response Time:** 15 minutes ## Symptoms - Connection pool at 100% - Queries timing out - 'too many connections' errors in logs ## Diagnosis ### Step 1 Check active connections: `SELECT count(*) FROM pg_stat_activity` ### Step 2 Identify long-running queries: `SELECT pid, now() - pg_stat_activity.query_start AS duration, query FROM pg_stat_activity WHERE state != 'idle' ORDER BY duration DESC` ### Step 3 Check PgBouncer stats:",
              "score": null
            },
            {
              "doc_id": "runbook_database_connection_exhaustion",
              "chunk_id": "runbook_database_connection_exhaustion_fix_1",
              "content": "`SHOW POOLS` ### Step 4 Verify connection limits: `SHOW max_connections` ## Resolution ### Step 1 Kill long-running queries: `SELECT pg_terminate_backend(pid) FROM pg_stat_activity WHERE duration > interval '5 minutes'` ### Step 2 Increase PgBouncer pool size temporarily: update pgbouncer.ini ### Step 3 Restart affected services to release connections: `kubectl rollout restart deployment/api` ### Step 4 Scale down non-critical services to free connections ## Escalation If connections remain exhausted, escalate to DBA. Consider enabling connection queueing.",
              "score": null
            },
            {
              "doc_id": "config_database",
              "chunk_id": "config_database_fix_0",
              "content": "# Database Configuration Reference ## Overview This document describes all configuration options for Database. ## Environment Variables | Variable | Type | Default | Description | |----------|------|---------|-------------| | `DATABASE_URL` | string | required | PostgreSQL connection string | | `DATABASE_POOL_MIN` | integer | 5 | Minimum connection pool size | | `DATABASE_POOL_MAX` | integer | 20 | Maximum connection pool size | | `DATABASE_STATEMENT_TIMEOUT` | integer | 30000 | Query timeout in milliseconds | |",
              "score": null
            },
            {
              "doc_id": "config_database",
              "chunk_id": "config_database_fix_1",
              "content": "`DATABASE_SSL_MODE` | string | require | SSL mode: disable, require, verify-full | | `DATABASE_MIGRATION_AUTO` | boolean | false | Run migrations on startup | ## Configuration File Configuration can also be provided via `database.yaml`: ```yaml database_url: required database_pool_min: 5 database_pool_max: 20 database_statement_timeout: 30000 database_ssl_mode: require ``` ## Validation Configuration is validated at startup. Invalid configuration will prevent the service from starting.",
              "score": null
            },
            {
              "doc_id": "howto_set_up_database_connection",
              "chunk_id": "howto_set_up_database_connection_fix_1",
              "content": "> **Note:** Pool exhaustion will queue requests ### Step 3: Test connection Verify the database is accessible from CloudFlow. ```bash cloudflow db test --secret DATABASE_URL ``` ### Step 4: Enable SSL For production, always require SSL connections. ```bash cloudflow db configure --require-ssl ``` ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            }
          ],
          "latency_ms": 4.386232998513151,
          "query_id": "troubleshoot_028",
          "dimension": "negation",
          "query": "why can't my app connect to the database anymore, it was working fine yesterday"
        },
        {
          "key_facts": [
            "kafka-consumer-groups.sh",
            "consumer lag",
            "reset-offsets"
          ],
          "found_facts": [
            "kafka-consumer-groups.sh",
            "consumer lag",
            "reset-offsets"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "runbook_kafka_consumer_lag"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "runbook_kafka_consumer_lag",
              "chunk_id": "runbook_kafka_consumer_lag_fix_0",
              "content": "# Runbook: Kafka Consumer Lag ## Severity **Level:** P1 - Critical **Response Time:** 1 hour ## Symptoms - Consumer lag > 10000 messages - Processing delays reported - Alerts from lag monitor ## Diagnosis ### Step 1 Check consumer group lag: `kafka-consumer-groups.sh --bootstrap-server kafka:9092 --describe --group workflow-executor` ### Step 2 Verify consumer health: `kubectl get pods -l app=workflow-executor -n production` ### Step 3 Check for processing errors: `kubectl logs -l app=workflow-executor --tail=100 -n production |",
              "score": null
            },
            {
              "doc_id": "runbook_kafka_consumer_lag",
              "chunk_id": "runbook_kafka_consumer_lag_fix_1",
              "content": "grep ERROR` ### Step 4 Monitor throughput: `cloudflow metrics get kafka.consumer.records_per_second` ## Resolution ### Step 1 Scale up consumers: `kubectl scale deployment/workflow-executor --replicas=10 -n production` ### Step 2 Reset offset if messages are stale: `kafka-consumer-groups.sh --bootstrap-server kafka:9092 --group workflow-executor --reset-offsets --to-latest --execute --topic workflows` ### Step 3 Pause producer if backpressure needed: `cloudflow workflows pause-triggers` ### Step 4 Increase partition count for parallelism (requires coordination) ## Escalation If lag persists > 30 minutes, page platform",
              "score": null
            },
            {
              "doc_id": "adr_005",
              "chunk_id": "adr_005_fix_2",
              "content": "## Consequences - Implement idempotent consumers - Use dead letter queues for failed messages - Monitor consumer lag",
              "score": null
            },
            {
              "doc_id": "adr_005",
              "chunk_id": "adr_005_fix_1",
              "content": "**Pros:** - Simple mental model - Immediate feedback - Easy debugging **Cons:** - Tight coupling - Cascading failures - Timeout issues ### Option 3: Polling-based Workers poll for tasks. **Pros:** - Simple implementation - Easy rate limiting - No message broker **Cons:** - Polling overhead - Higher latency - Inefficient at scale ## Decision We will use an event-driven architecture with Kafka for workflow step execution.",
              "score": null
            },
            {
              "doc_id": "runbook_kafka_consumer_lag",
              "chunk_id": "runbook_kafka_consumer_lag_fix_2",
              "content": "team. May need Kafka cluster scaling. ## Post-Incident - Document timeline in incident log - Schedule post-mortem within 48 hours - Update runbook if new patterns discovered",
              "score": null
            }
          ],
          "latency_ms": 3.798598001594655,
          "query_id": "troubleshoot_029",
          "dimension": "original",
          "query": "How to handle Kafka consumer lag?"
        },
        {
          "key_facts": [
            "kafka-consumer-groups.sh",
            "consumer lag",
            "reset-offsets"
          ],
          "found_facts": [
            "kafka-consumer-groups.sh",
            "consumer lag",
            "reset-offsets"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "runbook_kafka_consumer_lag"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "runbook_kafka_consumer_lag",
              "chunk_id": "runbook_kafka_consumer_lag_fix_0",
              "content": "# Runbook: Kafka Consumer Lag ## Severity **Level:** P1 - Critical **Response Time:** 1 hour ## Symptoms - Consumer lag > 10000 messages - Processing delays reported - Alerts from lag monitor ## Diagnosis ### Step 1 Check consumer group lag: `kafka-consumer-groups.sh --bootstrap-server kafka:9092 --describe --group workflow-executor` ### Step 2 Verify consumer health: `kubectl get pods -l app=workflow-executor -n production` ### Step 3 Check for processing errors: `kubectl logs -l app=workflow-executor --tail=100 -n production |",
              "score": null
            },
            {
              "doc_id": "runbook_kafka_consumer_lag",
              "chunk_id": "runbook_kafka_consumer_lag_fix_1",
              "content": "grep ERROR` ### Step 4 Monitor throughput: `cloudflow metrics get kafka.consumer.records_per_second` ## Resolution ### Step 1 Scale up consumers: `kubectl scale deployment/workflow-executor --replicas=10 -n production` ### Step 2 Reset offset if messages are stale: `kafka-consumer-groups.sh --bootstrap-server kafka:9092 --group workflow-executor --reset-offsets --to-latest --execute --topic workflows` ### Step 3 Pause producer if backpressure needed: `cloudflow workflows pause-triggers` ### Step 4 Increase partition count for parallelism (requires coordination) ## Escalation If lag persists > 30 minutes, page platform",
              "score": null
            },
            {
              "doc_id": "runbook_kafka_consumer_lag",
              "chunk_id": "runbook_kafka_consumer_lag_fix_2",
              "content": "team. May need Kafka cluster scaling. ## Post-Incident - Document timeline in incident log - Schedule post-mortem within 48 hours - Update runbook if new patterns discovered",
              "score": null
            },
            {
              "doc_id": "adr_005",
              "chunk_id": "adr_005_fix_1",
              "content": "**Pros:** - Simple mental model - Immediate feedback - Easy debugging **Cons:** - Tight coupling - Cascading failures - Timeout issues ### Option 3: Polling-based Workers poll for tasks. **Pros:** - Simple implementation - Easy rate limiting - No message broker **Cons:** - Polling overhead - Higher latency - Inefficient at scale ## Decision We will use an event-driven architecture with Kafka for workflow step execution.",
              "score": null
            },
            {
              "doc_id": "adr_005",
              "chunk_id": "adr_005_fix_0",
              "content": "# ADR-005: Adopt Event-Driven Architecture for Workflow Execution **Status:** Accepted **Date:** 2024-12-04 ## Context We need an architecture that supports asynchronous workflow execution, step dependencies, and failure recovery. ## Options Considered ### Option 1: Event-driven with Kafka Publish-subscribe messaging. **Pros:** - Loose coupling - Scalability - Event replay - Audit trail **Cons:** - Eventual consistency - Operational complexity - Message ordering challenges ### Option 2: Request-response Synchronous API calls.",
              "score": null
            }
          ],
          "latency_ms": 3.6722120003105374,
          "query_id": "troubleshoot_029",
          "dimension": "synonym",
          "query": "How do I fix message queue processing delays in Kafka?"
        },
        {
          "key_facts": [
            "kafka-consumer-groups.sh",
            "consumer lag",
            "reset-offsets"
          ],
          "found_facts": [
            "kafka-consumer-groups.sh",
            "consumer lag",
            "reset-offsets"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "runbook_kafka_consumer_lag"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "runbook_kafka_consumer_lag",
              "chunk_id": "runbook_kafka_consumer_lag_fix_0",
              "content": "# Runbook: Kafka Consumer Lag ## Severity **Level:** P1 - Critical **Response Time:** 1 hour ## Symptoms - Consumer lag > 10000 messages - Processing delays reported - Alerts from lag monitor ## Diagnosis ### Step 1 Check consumer group lag: `kafka-consumer-groups.sh --bootstrap-server kafka:9092 --describe --group workflow-executor` ### Step 2 Verify consumer health: `kubectl get pods -l app=workflow-executor -n production` ### Step 3 Check for processing errors: `kubectl logs -l app=workflow-executor --tail=100 -n production |",
              "score": null
            },
            {
              "doc_id": "runbook_kafka_consumer_lag",
              "chunk_id": "runbook_kafka_consumer_lag_fix_1",
              "content": "grep ERROR` ### Step 4 Monitor throughput: `cloudflow metrics get kafka.consumer.records_per_second` ## Resolution ### Step 1 Scale up consumers: `kubectl scale deployment/workflow-executor --replicas=10 -n production` ### Step 2 Reset offset if messages are stale: `kafka-consumer-groups.sh --bootstrap-server kafka:9092 --group workflow-executor --reset-offsets --to-latest --execute --topic workflows` ### Step 3 Pause producer if backpressure needed: `cloudflow workflows pause-triggers` ### Step 4 Increase partition count for parallelism (requires coordination) ## Escalation If lag persists > 30 minutes, page platform",
              "score": null
            },
            {
              "doc_id": "runbook_kafka_consumer_lag",
              "chunk_id": "runbook_kafka_consumer_lag_fix_2",
              "content": "team. May need Kafka cluster scaling. ## Post-Incident - Document timeline in incident log - Schedule post-mortem within 48 hours - Update runbook if new patterns discovered",
              "score": null
            },
            {
              "doc_id": "adr_005",
              "chunk_id": "adr_005_fix_1",
              "content": "**Pros:** - Simple mental model - Immediate feedback - Easy debugging **Cons:** - Tight coupling - Cascading failures - Timeout issues ### Option 3: Polling-based Workers poll for tasks. **Pros:** - Simple implementation - Easy rate limiting - No message broker **Cons:** - Polling overhead - Higher latency - Inefficient at scale ## Decision We will use an event-driven architecture with Kafka for workflow step execution.",
              "score": null
            },
            {
              "doc_id": "adr_005",
              "chunk_id": "adr_005_fix_0",
              "content": "# ADR-005: Adopt Event-Driven Architecture for Workflow Execution **Status:** Accepted **Date:** 2024-12-04 ## Context We need an architecture that supports asynchronous workflow execution, step dependencies, and failure recovery. ## Options Considered ### Option 1: Event-driven with Kafka Publish-subscribe messaging. **Pros:** - Loose coupling - Scalability - Event replay - Audit trail **Cons:** - Eventual consistency - Operational complexity - Message ordering challenges ### Option 2: Request-response Synchronous API calls.",
              "score": null
            }
          ],
          "latency_ms": 3.432675999647472,
          "query_id": "troubleshoot_029",
          "dimension": "problem",
          "query": "kafka-consumer-groups shows 50000 messages behind and workflows are delayed by 10+ minutes"
        },
        {
          "key_facts": [
            "kafka-consumer-groups.sh",
            "consumer lag",
            "reset-offsets"
          ],
          "found_facts": [
            "kafka-consumer-groups.sh",
            "consumer lag",
            "reset-offsets"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "runbook_kafka_consumer_lag"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "runbook_kafka_consumer_lag",
              "chunk_id": "runbook_kafka_consumer_lag_fix_0",
              "content": "# Runbook: Kafka Consumer Lag ## Severity **Level:** P1 - Critical **Response Time:** 1 hour ## Symptoms - Consumer lag > 10000 messages - Processing delays reported - Alerts from lag monitor ## Diagnosis ### Step 1 Check consumer group lag: `kafka-consumer-groups.sh --bootstrap-server kafka:9092 --describe --group workflow-executor` ### Step 2 Verify consumer health: `kubectl get pods -l app=workflow-executor -n production` ### Step 3 Check for processing errors: `kubectl logs -l app=workflow-executor --tail=100 -n production |",
              "score": null
            },
            {
              "doc_id": "runbook_kafka_consumer_lag",
              "chunk_id": "runbook_kafka_consumer_lag_fix_1",
              "content": "grep ERROR` ### Step 4 Monitor throughput: `cloudflow metrics get kafka.consumer.records_per_second` ## Resolution ### Step 1 Scale up consumers: `kubectl scale deployment/workflow-executor --replicas=10 -n production` ### Step 2 Reset offset if messages are stale: `kafka-consumer-groups.sh --bootstrap-server kafka:9092 --group workflow-executor --reset-offsets --to-latest --execute --topic workflows` ### Step 3 Pause producer if backpressure needed: `cloudflow workflows pause-triggers` ### Step 4 Increase partition count for parallelism (requires coordination) ## Escalation If lag persists > 30 minutes, page platform",
              "score": null
            },
            {
              "doc_id": "adr_005",
              "chunk_id": "adr_005_fix_2",
              "content": "## Consequences - Implement idempotent consumers - Use dead letter queues for failed messages - Monitor consumer lag",
              "score": null
            },
            {
              "doc_id": "runbook_kafka_consumer_lag",
              "chunk_id": "runbook_kafka_consumer_lag_fix_2",
              "content": "team. May need Kafka cluster scaling. ## Post-Incident - Document timeline in incident log - Schedule post-mortem within 48 hours - Update runbook if new patterns discovered",
              "score": null
            },
            {
              "doc_id": "runbook_database_connection_exhaustion",
              "chunk_id": "runbook_database_connection_exhaustion_fix_1",
              "content": "`SHOW POOLS` ### Step 4 Verify connection limits: `SHOW max_connections` ## Resolution ### Step 1 Kill long-running queries: `SELECT pg_terminate_backend(pid) FROM pg_stat_activity WHERE duration > interval '5 minutes'` ### Step 2 Increase PgBouncer pool size temporarily: update pgbouncer.ini ### Step 3 Restart affected services to release connections: `kubectl rollout restart deployment/api` ### Step 4 Scale down non-critical services to free connections ## Escalation If connections remain exhausted, escalate to DBA. Consider enabling connection queueing.",
              "score": null
            }
          ],
          "latency_ms": 3.5001120013475884,
          "query_id": "troubleshoot_029",
          "dimension": "casual",
          "query": "kafka lag through the roof, consumers cant keep up"
        },
        {
          "key_facts": [
            "kafka-consumer-groups.sh",
            "consumer lag",
            "reset-offsets"
          ],
          "found_facts": [
            "kafka-consumer-groups.sh",
            "consumer lag",
            "reset-offsets"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "runbook_kafka_consumer_lag"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "runbook_kafka_consumer_lag",
              "chunk_id": "runbook_kafka_consumer_lag_fix_1",
              "content": "grep ERROR` ### Step 4 Monitor throughput: `cloudflow metrics get kafka.consumer.records_per_second` ## Resolution ### Step 1 Scale up consumers: `kubectl scale deployment/workflow-executor --replicas=10 -n production` ### Step 2 Reset offset if messages are stale: `kafka-consumer-groups.sh --bootstrap-server kafka:9092 --group workflow-executor --reset-offsets --to-latest --execute --topic workflows` ### Step 3 Pause producer if backpressure needed: `cloudflow workflows pause-triggers` ### Step 4 Increase partition count for parallelism (requires coordination) ## Escalation If lag persists > 30 minutes, page platform",
              "score": null
            },
            {
              "doc_id": "runbook_kafka_consumer_lag",
              "chunk_id": "runbook_kafka_consumer_lag_fix_2",
              "content": "team. May need Kafka cluster scaling. ## Post-Incident - Document timeline in incident log - Schedule post-mortem within 48 hours - Update runbook if new patterns discovered",
              "score": null
            },
            {
              "doc_id": "runbook_kafka_consumer_lag",
              "chunk_id": "runbook_kafka_consumer_lag_fix_0",
              "content": "# Runbook: Kafka Consumer Lag ## Severity **Level:** P1 - Critical **Response Time:** 1 hour ## Symptoms - Consumer lag > 10000 messages - Processing delays reported - Alerts from lag monitor ## Diagnosis ### Step 1 Check consumer group lag: `kafka-consumer-groups.sh --bootstrap-server kafka:9092 --describe --group workflow-executor` ### Step 2 Verify consumer health: `kubectl get pods -l app=workflow-executor -n production` ### Step 3 Check for processing errors: `kubectl logs -l app=workflow-executor --tail=100 -n production |",
              "score": null
            },
            {
              "doc_id": "adr_005",
              "chunk_id": "adr_005_fix_2",
              "content": "## Consequences - Implement idempotent consumers - Use dead letter queues for failed messages - Monitor consumer lag",
              "score": null
            },
            {
              "doc_id": "arch_data_pipeline",
              "chunk_id": "arch_data_pipeline_fix_1",
              "content": "**Technology:** Apache Kafka with Flink **Scaling:** Partition-based scaling ### Batch Processor Processes large batch jobs. Supports data formats: JSON, CSV, Parquet, Avro. **Technology:** Apache Spark on Kubernetes **Scaling:** Dynamic resource allocation ### Data Lake Long-term data storage with time-travel queries. Retention configurable up to 7 years.",
              "score": null
            }
          ],
          "latency_ms": 4.275908000636264,
          "query_id": "troubleshoot_029",
          "dimension": "contextual",
          "query": "we had a traffic spike and now Kafka consumers are way behind, need to catch up before the backlog gets worse"
        },
        {
          "key_facts": [
            "kafka-consumer-groups.sh",
            "consumer lag",
            "reset-offsets"
          ],
          "found_facts": [
            "kafka-consumer-groups.sh",
            "consumer lag",
            "reset-offsets"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "runbook_kafka_consumer_lag"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "runbook_kafka_consumer_lag",
              "chunk_id": "runbook_kafka_consumer_lag_fix_0",
              "content": "# Runbook: Kafka Consumer Lag ## Severity **Level:** P1 - Critical **Response Time:** 1 hour ## Symptoms - Consumer lag > 10000 messages - Processing delays reported - Alerts from lag monitor ## Diagnosis ### Step 1 Check consumer group lag: `kafka-consumer-groups.sh --bootstrap-server kafka:9092 --describe --group workflow-executor` ### Step 2 Verify consumer health: `kubectl get pods -l app=workflow-executor -n production` ### Step 3 Check for processing errors: `kubectl logs -l app=workflow-executor --tail=100 -n production |",
              "score": null
            },
            {
              "doc_id": "runbook_kafka_consumer_lag",
              "chunk_id": "runbook_kafka_consumer_lag_fix_1",
              "content": "grep ERROR` ### Step 4 Monitor throughput: `cloudflow metrics get kafka.consumer.records_per_second` ## Resolution ### Step 1 Scale up consumers: `kubectl scale deployment/workflow-executor --replicas=10 -n production` ### Step 2 Reset offset if messages are stale: `kafka-consumer-groups.sh --bootstrap-server kafka:9092 --group workflow-executor --reset-offsets --to-latest --execute --topic workflows` ### Step 3 Pause producer if backpressure needed: `cloudflow workflows pause-triggers` ### Step 4 Increase partition count for parallelism (requires coordination) ## Escalation If lag persists > 30 minutes, page platform",
              "score": null
            },
            {
              "doc_id": "adr_005",
              "chunk_id": "adr_005_fix_1",
              "content": "**Pros:** - Simple mental model - Immediate feedback - Easy debugging **Cons:** - Tight coupling - Cascading failures - Timeout issues ### Option 3: Polling-based Workers poll for tasks. **Pros:** - Simple implementation - Easy rate limiting - No message broker **Cons:** - Polling overhead - Higher latency - Inefficient at scale ## Decision We will use an event-driven architecture with Kafka for workflow step execution.",
              "score": null
            },
            {
              "doc_id": "adr_005",
              "chunk_id": "adr_005_fix_0",
              "content": "# ADR-005: Adopt Event-Driven Architecture for Workflow Execution **Status:** Accepted **Date:** 2024-12-04 ## Context We need an architecture that supports asynchronous workflow execution, step dependencies, and failure recovery. ## Options Considered ### Option 1: Event-driven with Kafka Publish-subscribe messaging. **Pros:** - Loose coupling - Scalability - Event replay - Audit trail **Cons:** - Eventual consistency - Operational complexity - Message ordering challenges ### Option 2: Request-response Synchronous API calls.",
              "score": null
            },
            {
              "doc_id": "runbook_kafka_consumer_lag",
              "chunk_id": "runbook_kafka_consumer_lag_fix_2",
              "content": "team. May need Kafka cluster scaling. ## Post-Incident - Document timeline in incident log - Schedule post-mortem within 48 hours - Update runbook if new patterns discovered",
              "score": null
            }
          ],
          "latency_ms": 3.893224000421469,
          "query_id": "troubleshoot_029",
          "dimension": "negation",
          "query": "why aren't my Kafka consumers processing messages fast enough?"
        },
        {
          "key_facts": [
            "openssl s_client",
            "cert-manager",
            "Let's Encrypt"
          ],
          "found_facts": [
            "openssl s_client",
            "cert-manager",
            "Let's Encrypt"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "runbook_ssl_certificate_expir"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "runbook_ssl_certificate_expiry",
              "chunk_id": "runbook_ssl_certificate_expiry_fix_0",
              "content": "# Runbook: SSL Certificate Expiry ## Severity **Level:** P3 - Medium **Response Time:** 15 minutes ## Symptoms - Certificate expiring within 7 days - SSL handshake failures - Browser security warnings ## Diagnosis ### Step 1 Check certificate expiry: `echo | openssl s_client -connect api.cloudflow.io:443 2>/dev/null | openssl x509 -noout -dates` ### Step 2 Verify cert-manager status: `kubectl get certificates -n production` ### Step 3 Check cert-manager logs: `kubectl logs -l app=cert-manager -n cert-manager --tail=50`",
              "score": null
            },
            {
              "doc_id": "runbook_ssl_certificate_expiry",
              "chunk_id": "runbook_ssl_certificate_expiry_fix_1",
              "content": "### Step 4 Verify DNS is correct: `dig +short api.cloudflow.io` ## Resolution ### Step 1 Trigger manual renewal: `kubectl delete certificate api-cert -n production && kubectl apply -f certificate.yaml` ### Step 2 Check rate limits: Let's Encrypt has 50 certs/domain/week limit ### Step 3 Use staging issuer for testing: switch to letsencrypt-staging ClusterIssuer ### Step 4 If Let's Encrypt fails, manually upload certificate ## Escalation If certificate cannot be renewed, escalate to security team immediately.",
              "score": null
            },
            {
              "doc_id": "config_database",
              "chunk_id": "config_database_fix_1",
              "content": "`DATABASE_SSL_MODE` | string | require | SSL mode: disable, require, verify-full | | `DATABASE_MIGRATION_AUTO` | boolean | false | Run migrations on startup | ## Configuration File Configuration can also be provided via `database.yaml`: ```yaml database_url: required database_pool_min: 5 database_pool_max: 20 database_statement_timeout: 30000 database_ssl_mode: require ``` ## Validation Configuration is validated at startup. Invalid configuration will prevent the service from starting.",
              "score": null
            },
            {
              "doc_id": "arch_authentication_service",
              "chunk_id": "arch_authentication_service_fix_1",
              "content": "**Technology:** Custom Go service **Scaling:** Horizontal stateless ### Token Service Issues and validates access tokens. Token lifetime is 1 hour with 7-day refresh tokens. **Technology:** JWT with RS256 **Scaling:** Horizontal stateless ### SSO Gateway Integrates with external identity providers. Supports Google, Microsoft, Okta, and custom SAML.",
              "score": null
            },
            {
              "doc_id": "runbook_ssl_certificate_expiry",
              "chunk_id": "runbook_ssl_certificate_expiry_fix_2",
              "content": "Consider using backup certificate. ## Post-Incident - Document timeline in incident log - Schedule post-mortem within 48 hours - Update runbook if new patterns discovered",
              "score": null
            }
          ],
          "latency_ms": 3.714991999004269,
          "query_id": "troubleshoot_030",
          "dimension": "original",
          "query": "What to do when SSL certificate is expiring?"
        },
        {
          "key_facts": [
            "openssl s_client",
            "cert-manager",
            "Let's Encrypt"
          ],
          "found_facts": [
            "openssl s_client",
            "cert-manager",
            "Let's Encrypt"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "runbook_ssl_certificate_expir"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "runbook_ssl_certificate_expiry",
              "chunk_id": "runbook_ssl_certificate_expiry_fix_1",
              "content": "### Step 4 Verify DNS is correct: `dig +short api.cloudflow.io` ## Resolution ### Step 1 Trigger manual renewal: `kubectl delete certificate api-cert -n production && kubectl apply -f certificate.yaml` ### Step 2 Check rate limits: Let's Encrypt has 50 certs/domain/week limit ### Step 3 Use staging issuer for testing: switch to letsencrypt-staging ClusterIssuer ### Step 4 If Let's Encrypt fails, manually upload certificate ## Escalation If certificate cannot be renewed, escalate to security team immediately.",
              "score": null
            },
            {
              "doc_id": "runbook_ssl_certificate_expiry",
              "chunk_id": "runbook_ssl_certificate_expiry_fix_0",
              "content": "# Runbook: SSL Certificate Expiry ## Severity **Level:** P3 - Medium **Response Time:** 15 minutes ## Symptoms - Certificate expiring within 7 days - SSL handshake failures - Browser security warnings ## Diagnosis ### Step 1 Check certificate expiry: `echo | openssl s_client -connect api.cloudflow.io:443 2>/dev/null | openssl x509 -noout -dates` ### Step 2 Verify cert-manager status: `kubectl get certificates -n production` ### Step 3 Check cert-manager logs: `kubectl logs -l app=cert-manager -n cert-manager --tail=50`",
              "score": null
            },
            {
              "doc_id": "config_api_gateway",
              "chunk_id": "config_api_gateway_fix_1",
              "content": "size | | `GATEWAY_CORS_ORIGINS` | string | * | Allowed CORS origins | | `GATEWAY_SSL_ENABLED` | boolean | true | Enable TLS termination | ## Configuration File Configuration can also be provided via `api gateway.yaml`: ```yaml gateway_port: 8000 gateway_rate_limit: 100 gateway_timeout_ms: 30000 gateway_max_body_size: 10MB gateway_cors_origins: * ``` ## Validation Configuration is validated at startup. Invalid configuration will prevent the service from starting.",
              "score": null
            },
            {
              "doc_id": "arch_authentication_service",
              "chunk_id": "arch_authentication_service_fix_1",
              "content": "**Technology:** Custom Go service **Scaling:** Horizontal stateless ### Token Service Issues and validates access tokens. Token lifetime is 1 hour with 7-day refresh tokens. **Technology:** JWT with RS256 **Scaling:** Horizontal stateless ### SSO Gateway Integrates with external identity providers. Supports Google, Microsoft, Okta, and custom SAML.",
              "score": null
            },
            {
              "doc_id": "runbook_ssl_certificate_expiry",
              "chunk_id": "runbook_ssl_certificate_expiry_fix_2",
              "content": "Consider using backup certificate. ## Post-Incident - Document timeline in incident log - Schedule post-mortem within 48 hours - Update runbook if new patterns discovered",
              "score": null
            }
          ],
          "latency_ms": 3.6045060005562846,
          "query_id": "troubleshoot_030",
          "dimension": "synonym",
          "query": "How do I renew TLS certs before they expire?"
        },
        {
          "key_facts": [
            "openssl s_client",
            "cert-manager",
            "Let's Encrypt"
          ],
          "found_facts": [
            "openssl s_client",
            "cert-manager",
            "Let's Encrypt"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "runbook_ssl_certificate_expir"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "runbook_ssl_certificate_expiry",
              "chunk_id": "runbook_ssl_certificate_expiry_fix_0",
              "content": "# Runbook: SSL Certificate Expiry ## Severity **Level:** P3 - Medium **Response Time:** 15 minutes ## Symptoms - Certificate expiring within 7 days - SSL handshake failures - Browser security warnings ## Diagnosis ### Step 1 Check certificate expiry: `echo | openssl s_client -connect api.cloudflow.io:443 2>/dev/null | openssl x509 -noout -dates` ### Step 2 Verify cert-manager status: `kubectl get certificates -n production` ### Step 3 Check cert-manager logs: `kubectl logs -l app=cert-manager -n cert-manager --tail=50`",
              "score": null
            },
            {
              "doc_id": "runbook_ssl_certificate_expiry",
              "chunk_id": "runbook_ssl_certificate_expiry_fix_1",
              "content": "### Step 4 Verify DNS is correct: `dig +short api.cloudflow.io` ## Resolution ### Step 1 Trigger manual renewal: `kubectl delete certificate api-cert -n production && kubectl apply -f certificate.yaml` ### Step 2 Check rate limits: Let's Encrypt has 50 certs/domain/week limit ### Step 3 Use staging issuer for testing: switch to letsencrypt-staging ClusterIssuer ### Step 4 If Let's Encrypt fails, manually upload certificate ## Escalation If certificate cannot be renewed, escalate to security team immediately.",
              "score": null
            },
            {
              "doc_id": "arch_authentication_service",
              "chunk_id": "arch_authentication_service_fix_1",
              "content": "**Technology:** Custom Go service **Scaling:** Horizontal stateless ### Token Service Issues and validates access tokens. Token lifetime is 1 hour with 7-day refresh tokens. **Technology:** JWT with RS256 **Scaling:** Horizontal stateless ### SSO Gateway Integrates with external identity providers. Supports Google, Microsoft, Okta, and custom SAML.",
              "score": null
            },
            {
              "doc_id": "runbook_api_gateway_5xx_errors",
              "chunk_id": "runbook_api_gateway_5xx_errors_fix_2",
              "content": "If 5xx persists > 15 minutes, page on-call. May indicate infrastructure issue. ## Post-Incident - Document timeline in incident log - Schedule post-mortem within 48 hours - Update runbook if new patterns discovered",
              "score": null
            },
            {
              "doc_id": "api_users",
              "chunk_id": "api_users_fix_1",
              "content": "**Parameters:** **Response:** - Success: `200` - Returns user object - Error: `400` - Bad Request **Example:** ```json {\"id\": \"usr_001\", \"email\": \"user@example.com\", \"role\": \"admin\", \"created_at\": \"2023-06-01\"} ``` ### PATCH /users/me Update current user profile. **Parameters:** - `name` (string): Display name - `timezone` (string): Timezone (e.g., America/New_York) **Response:** - Success: `200` - User updated - Error: `400` - Bad Request **Example:** ```json {\"id\": \"usr_001\", \"name\": \"John Doe\", \"timezone\": \"America/New_York\"} ```",
              "score": null
            }
          ],
          "latency_ms": 3.5981550008727936,
          "query_id": "troubleshoot_030",
          "dimension": "problem",
          "query": "users are getting browser security warnings and openssl shows cert expires in 3 days"
        },
        {
          "key_facts": [
            "openssl s_client",
            "cert-manager",
            "Let's Encrypt"
          ],
          "found_facts": [
            "openssl s_client",
            "cert-manager",
            "Let's Encrypt"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "runbook_ssl_certificate_expir"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "runbook_ssl_certificate_expiry",
              "chunk_id": "runbook_ssl_certificate_expiry_fix_0",
              "content": "# Runbook: SSL Certificate Expiry ## Severity **Level:** P3 - Medium **Response Time:** 15 minutes ## Symptoms - Certificate expiring within 7 days - SSL handshake failures - Browser security warnings ## Diagnosis ### Step 1 Check certificate expiry: `echo | openssl s_client -connect api.cloudflow.io:443 2>/dev/null | openssl x509 -noout -dates` ### Step 2 Verify cert-manager status: `kubectl get certificates -n production` ### Step 3 Check cert-manager logs: `kubectl logs -l app=cert-manager -n cert-manager --tail=50`",
              "score": null
            },
            {
              "doc_id": "runbook_ssl_certificate_expiry",
              "chunk_id": "runbook_ssl_certificate_expiry_fix_1",
              "content": "### Step 4 Verify DNS is correct: `dig +short api.cloudflow.io` ## Resolution ### Step 1 Trigger manual renewal: `kubectl delete certificate api-cert -n production && kubectl apply -f certificate.yaml` ### Step 2 Check rate limits: Let's Encrypt has 50 certs/domain/week limit ### Step 3 Use staging issuer for testing: switch to letsencrypt-staging ClusterIssuer ### Step 4 If Let's Encrypt fails, manually upload certificate ## Escalation If certificate cannot be renewed, escalate to security team immediately.",
              "score": null
            },
            {
              "doc_id": "arch_authentication_service",
              "chunk_id": "arch_authentication_service_fix_1",
              "content": "**Technology:** Custom Go service **Scaling:** Horizontal stateless ### Token Service Issues and validates access tokens. Token lifetime is 1 hour with 7-day refresh tokens. **Technology:** JWT with RS256 **Scaling:** Horizontal stateless ### SSO Gateway Integrates with external identity providers. Supports Google, Microsoft, Okta, and custom SAML.",
              "score": null
            },
            {
              "doc_id": "config_database",
              "chunk_id": "config_database_fix_1",
              "content": "`DATABASE_SSL_MODE` | string | require | SSL mode: disable, require, verify-full | | `DATABASE_MIGRATION_AUTO` | boolean | false | Run migrations on startup | ## Configuration File Configuration can also be provided via `database.yaml`: ```yaml database_url: required database_pool_min: 5 database_pool_max: 20 database_statement_timeout: 30000 database_ssl_mode: require ``` ## Validation Configuration is validated at startup. Invalid configuration will prevent the service from starting.",
              "score": null
            },
            {
              "doc_id": "runbook_ssl_certificate_expiry",
              "chunk_id": "runbook_ssl_certificate_expiry_fix_2",
              "content": "Consider using backup certificate. ## Post-Incident - Document timeline in incident log - Schedule post-mortem within 48 hours - Update runbook if new patterns discovered",
              "score": null
            }
          ],
          "latency_ms": 3.764353999940795,
          "query_id": "troubleshoot_030",
          "dimension": "casual",
          "query": "ssl cert about to expire, how to renew"
        },
        {
          "key_facts": [
            "openssl s_client",
            "cert-manager",
            "Let's Encrypt"
          ],
          "found_facts": [
            "openssl s_client",
            "cert-manager",
            "Let's Encrypt"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "runbook_ssl_certificate_expir"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "runbook_ssl_certificate_expiry",
              "chunk_id": "runbook_ssl_certificate_expiry_fix_1",
              "content": "### Step 4 Verify DNS is correct: `dig +short api.cloudflow.io` ## Resolution ### Step 1 Trigger manual renewal: `kubectl delete certificate api-cert -n production && kubectl apply -f certificate.yaml` ### Step 2 Check rate limits: Let's Encrypt has 50 certs/domain/week limit ### Step 3 Use staging issuer for testing: switch to letsencrypt-staging ClusterIssuer ### Step 4 If Let's Encrypt fails, manually upload certificate ## Escalation If certificate cannot be renewed, escalate to security team immediately.",
              "score": null
            },
            {
              "doc_id": "runbook_ssl_certificate_expiry",
              "chunk_id": "runbook_ssl_certificate_expiry_fix_0",
              "content": "# Runbook: SSL Certificate Expiry ## Severity **Level:** P3 - Medium **Response Time:** 15 minutes ## Symptoms - Certificate expiring within 7 days - SSL handshake failures - Browser security warnings ## Diagnosis ### Step 1 Check certificate expiry: `echo | openssl s_client -connect api.cloudflow.io:443 2>/dev/null | openssl x509 -noout -dates` ### Step 2 Verify cert-manager status: `kubectl get certificates -n production` ### Step 3 Check cert-manager logs: `kubectl logs -l app=cert-manager -n cert-manager --tail=50`",
              "score": null
            },
            {
              "doc_id": "config_database",
              "chunk_id": "config_database_fix_1",
              "content": "`DATABASE_SSL_MODE` | string | require | SSL mode: disable, require, verify-full | | `DATABASE_MIGRATION_AUTO` | boolean | false | Run migrations on startup | ## Configuration File Configuration can also be provided via `database.yaml`: ```yaml database_url: required database_pool_min: 5 database_pool_max: 20 database_statement_timeout: 30000 database_ssl_mode: require ``` ## Validation Configuration is validated at startup. Invalid configuration will prevent the service from starting.",
              "score": null
            },
            {
              "doc_id": "howto_set_up_data_encryption",
              "chunk_id": "howto_set_up_data_encryption_fix_1",
              "content": "``` ### Step 3: Enable field-level encryption Encrypt sensitive fields within your workflow data. ```bash cloudflow encryption add-field --path $.user.ssn --key-alias sensitive-data ``` ### Step 4: Verify encryption Confirm encryption is active for your workspace. ```bash cloudflow encryption status ``` ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            },
            {
              "doc_id": "arch_authentication_service",
              "chunk_id": "arch_authentication_service_fix_1",
              "content": "**Technology:** Custom Go service **Scaling:** Horizontal stateless ### Token Service Issues and validates access tokens. Token lifetime is 1 hour with 7-day refresh tokens. **Technology:** JWT with RS256 **Scaling:** Horizontal stateless ### SSO Gateway Integrates with external identity providers. Supports Google, Microsoft, Okta, and custom SAML.",
              "score": null
            }
          ],
          "latency_ms": 3.965007999795489,
          "query_id": "troubleshoot_030",
          "dimension": "contextual",
          "query": "got an alert that our production SSL certificate expires next week, need to renew with Let's Encrypt"
        },
        {
          "key_facts": [
            "openssl s_client",
            "cert-manager",
            "Let's Encrypt"
          ],
          "found_facts": [
            "openssl s_client",
            "cert-manager",
            "Let's Encrypt"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "runbook_ssl_certificate_expir"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "runbook_ssl_certificate_expiry",
              "chunk_id": "runbook_ssl_certificate_expiry_fix_0",
              "content": "# Runbook: SSL Certificate Expiry ## Severity **Level:** P3 - Medium **Response Time:** 15 minutes ## Symptoms - Certificate expiring within 7 days - SSL handshake failures - Browser security warnings ## Diagnosis ### Step 1 Check certificate expiry: `echo | openssl s_client -connect api.cloudflow.io:443 2>/dev/null | openssl x509 -noout -dates` ### Step 2 Verify cert-manager status: `kubectl get certificates -n production` ### Step 3 Check cert-manager logs: `kubectl logs -l app=cert-manager -n cert-manager --tail=50`",
              "score": null
            },
            {
              "doc_id": "runbook_ssl_certificate_expiry",
              "chunk_id": "runbook_ssl_certificate_expiry_fix_1",
              "content": "### Step 4 Verify DNS is correct: `dig +short api.cloudflow.io` ## Resolution ### Step 1 Trigger manual renewal: `kubectl delete certificate api-cert -n production && kubectl apply -f certificate.yaml` ### Step 2 Check rate limits: Let's Encrypt has 50 certs/domain/week limit ### Step 3 Use staging issuer for testing: switch to letsencrypt-staging ClusterIssuer ### Step 4 If Let's Encrypt fails, manually upload certificate ## Escalation If certificate cannot be renewed, escalate to security team immediately.",
              "score": null
            },
            {
              "doc_id": "runbook_ssl_certificate_expiry",
              "chunk_id": "runbook_ssl_certificate_expiry_fix_2",
              "content": "Consider using backup certificate. ## Post-Incident - Document timeline in incident log - Schedule post-mortem within 48 hours - Update runbook if new patterns discovered",
              "score": null
            },
            {
              "doc_id": "config_database",
              "chunk_id": "config_database_fix_1",
              "content": "`DATABASE_SSL_MODE` | string | require | SSL mode: disable, require, verify-full | | `DATABASE_MIGRATION_AUTO` | boolean | false | Run migrations on startup | ## Configuration File Configuration can also be provided via `database.yaml`: ```yaml database_url: required database_pool_min: 5 database_pool_max: 20 database_statement_timeout: 30000 database_ssl_mode: require ``` ## Validation Configuration is validated at startup. Invalid configuration will prevent the service from starting.",
              "score": null
            },
            {
              "doc_id": "arch_authentication_service",
              "chunk_id": "arch_authentication_service_fix_1",
              "content": "**Technology:** Custom Go service **Scaling:** Horizontal stateless ### Token Service Issues and validates access tokens. Token lifetime is 1 hour with 7-day refresh tokens. **Technology:** JWT with RS256 **Scaling:** Horizontal stateless ### SSO Gateway Integrates with external identity providers. Supports Google, Microsoft, Okta, and custom SAML.",
              "score": null
            }
          ],
          "latency_ms": 3.495472999929916,
          "query_id": "troubleshoot_030",
          "dimension": "negation",
          "query": "why didn't cert-manager auto-renew our certificate?"
        },
        {
          "key_facts": [
            "OOMKilled",
            "resources.limits.memory",
            "HeapDumpOnOutOfMemoryError"
          ],
          "found_facts": [
            "OOMKilled",
            "resources.limits.memory",
            "HeapDumpOnOutOfMemoryError"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "runbook_out_of_memory_oom_kill"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "runbook_out_of_memory_oom_kills",
              "chunk_id": "runbook_out_of_memory_oom_kills_fix_0",
              "content": "# Runbook: Out of Memory (OOM) Kills ## Severity **Level:** P3 - Medium **Response Time:** 15 minutes ## Symptoms - Pods restarting with OOMKilled status - Memory usage at limit - Application slowdown before restart ## Diagnosis ### Step 1 Check OOM events: `kubectl get events -n production --field-selector reason=OOMKilled` ### Step 2 Review memory usage: `kubectl top pods -n production --sort-by=memory` ### Step 3 Check container limits: `kubectl describe pod <pod-name> -n production |",
              "score": null
            },
            {
              "doc_id": "runbook_out_of_memory_oom_kills",
              "chunk_id": "runbook_out_of_memory_oom_kills_fix_2",
              "content": "increase, escalate to development team for memory leak investigation. ## Post-Incident - Document timeline in incident log - Schedule post-mortem within 48 hours - Update runbook if new patterns discovered",
              "score": null
            },
            {
              "doc_id": "arch_authentication_service",
              "chunk_id": "arch_authentication_service_fix_2",
              "content": "**Technology:** Dex OIDC proxy **Scaling:** Active-passive ## Data Flow Credentials \u2192 Validation \u2192 Token Issue \u2192 Token Refresh \u2192 Logout ## Performance Characteristics - **Latency P50:** 50ms - **Latency P99:** 200ms - **Throughput:** 5000 auth/second",
              "score": null
            },
            {
              "doc_id": "adr_004",
              "chunk_id": "adr_004_fix_2",
              "content": "## Consequences - Implement cache-aside pattern - Set appropriate TTLs - Monitor memory usage - Plan for cache stampede prevention",
              "score": null
            },
            {
              "doc_id": "runbook_out_of_memory_oom_kills",
              "chunk_id": "runbook_out_of_memory_oom_kills_fix_1",
              "content": "grep -A5 Limits` ### Step 4 Analyze heap dumps if available: `cloudflow debug heapdump --pod <pod-name>` ## Resolution ### Step 1 Increase memory limit: update deployment resources.limits.memory ### Step 2 Add memory request equal to limit to guarantee allocation ### Step 3 Enable JVM heap dump on OOM: add -XX:+HeapDumpOnOutOfMemoryError ### Step 4 Investigate memory leaks with profiling tools ### Step 5 Consider horizontal scaling instead of vertical ## Escalation If OOMs continue after limit",
              "score": null
            }
          ],
          "latency_ms": 3.2441149996884633,
          "query_id": "troubleshoot_031",
          "dimension": "original",
          "query": "How to fix OOM kills?"
        },
        {
          "key_facts": [
            "OOMKilled",
            "resources.limits.memory",
            "HeapDumpOnOutOfMemoryError"
          ],
          "found_facts": [
            "OOMKilled",
            "resources.limits.memory",
            "HeapDumpOnOutOfMemoryError"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "runbook_out_of_memory_oom_kill"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "runbook_out_of_memory_oom_kills",
              "chunk_id": "runbook_out_of_memory_oom_kills_fix_1",
              "content": "grep -A5 Limits` ### Step 4 Analyze heap dumps if available: `cloudflow debug heapdump --pod <pod-name>` ## Resolution ### Step 1 Increase memory limit: update deployment resources.limits.memory ### Step 2 Add memory request equal to limit to guarantee allocation ### Step 3 Enable JVM heap dump on OOM: add -XX:+HeapDumpOnOutOfMemoryError ### Step 4 Investigate memory leaks with profiling tools ### Step 5 Consider horizontal scaling instead of vertical ## Escalation If OOMs continue after limit",
              "score": null
            },
            {
              "doc_id": "runbook_out_of_memory_oom_kills",
              "chunk_id": "runbook_out_of_memory_oom_kills_fix_0",
              "content": "# Runbook: Out of Memory (OOM) Kills ## Severity **Level:** P3 - Medium **Response Time:** 15 minutes ## Symptoms - Pods restarting with OOMKilled status - Memory usage at limit - Application slowdown before restart ## Diagnosis ### Step 1 Check OOM events: `kubectl get events -n production --field-selector reason=OOMKilled` ### Step 2 Review memory usage: `kubectl top pods -n production --sort-by=memory` ### Step 3 Check container limits: `kubectl describe pod <pod-name> -n production |",
              "score": null
            },
            {
              "doc_id": "adr_004",
              "chunk_id": "adr_004_fix_2",
              "content": "## Consequences - Implement cache-aside pattern - Set appropriate TTLs - Monitor memory usage - Plan for cache stampede prevention",
              "score": null
            },
            {
              "doc_id": "runbook_out_of_memory_oom_kills",
              "chunk_id": "runbook_out_of_memory_oom_kills_fix_2",
              "content": "increase, escalate to development team for memory leak investigation. ## Post-Incident - Document timeline in incident log - Schedule post-mortem within 48 hours - Update runbook if new patterns discovered",
              "score": null
            },
            {
              "doc_id": "runbook_high_cpu_usage",
              "chunk_id": "runbook_high_cpu_usage_fix_1",
              "content": "Step 4 Profile the application: `cloudflow debug profile --pod api-xyz --duration 60s` ## Resolution ### Step 1 Scale up if traffic-related: `kubectl scale deployment/api --replicas=10 -n production` ### Step 2 Restart pods if memory leak suspected: `kubectl rollout restart deployment/api -n production` ### Step 3 Roll back recent deployment if regression: `kubectl rollout undo deployment/api -n production` ### Step 4 Add resource limits if unbounded: update deployment with CPU limits ## Escalation If CPU remains",
              "score": null
            }
          ],
          "latency_ms": 3.6438589995668735,
          "query_id": "troubleshoot_031",
          "dimension": "synonym",
          "query": "How do I resolve out-of-memory container terminations?"
        },
        {
          "key_facts": [
            "OOMKilled",
            "resources.limits.memory",
            "HeapDumpOnOutOfMemoryError"
          ],
          "found_facts": [
            "OOMKilled"
          ],
          "missed_facts": [
            "resources.limits.memory",
            "HeapDumpOnOutOfMemoryError"
          ],
          "coverage": 0.3333333333333333,
          "expected_docs": [
            "runbook_out_of_memory_oom_kill"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "runbook_out_of_memory_oom_kills",
              "chunk_id": "runbook_out_of_memory_oom_kills_fix_0",
              "content": "# Runbook: Out of Memory (OOM) Kills ## Severity **Level:** P3 - Medium **Response Time:** 15 minutes ## Symptoms - Pods restarting with OOMKilled status - Memory usage at limit - Application slowdown before restart ## Diagnosis ### Step 1 Check OOM events: `kubectl get events -n production --field-selector reason=OOMKilled` ### Step 2 Review memory usage: `kubectl top pods -n production --sort-by=memory` ### Step 3 Check container limits: `kubectl describe pod <pod-name> -n production |",
              "score": null
            },
            {
              "doc_id": "runbook_high_cpu_usage",
              "chunk_id": "runbook_high_cpu_usage_fix_1",
              "content": "Step 4 Profile the application: `cloudflow debug profile --pod api-xyz --duration 60s` ## Resolution ### Step 1 Scale up if traffic-related: `kubectl scale deployment/api --replicas=10 -n production` ### Step 2 Restart pods if memory leak suspected: `kubectl rollout restart deployment/api -n production` ### Step 3 Roll back recent deployment if regression: `kubectl rollout undo deployment/api -n production` ### Step 4 Add resource limits if unbounded: update deployment with CPU limits ## Escalation If CPU remains",
              "score": null
            },
            {
              "doc_id": "runbook_high_cpu_usage",
              "chunk_id": "runbook_high_cpu_usage_fix_0",
              "content": "# Runbook: High CPU Usage ## Severity **Level:** P1 - Critical **Response Time:** 15 minutes ## Symptoms - CPU utilization > 80% sustained - Increased response latency - Autoscaler at maximum replicas ## Diagnosis ### Step 1 Check current CPU usage: `kubectl top pods -n production` ### Step 2 Identify hot pods: `kubectl top pods --sort-by=cpu -n production | head -10` ### Step 3 Check for recent deployments: `kubectl rollout history deployment/api -n production` ###",
              "score": null
            },
            {
              "doc_id": "runbook_api_gateway_5xx_errors",
              "chunk_id": "runbook_api_gateway_5xx_errors_fix_1",
              "content": "-n production` ### Step 4 Test backend directly: `kubectl exec -it kong-xxx -n kong -- curl http://api.production.svc:8080/health` ## Resolution ### Step 1 Restart unhealthy backends: `kubectl rollout restart deployment/api -n production` ### Step 2 Enable circuit breaker if cascading: update Kong circuit-breaker plugin ### Step 3 Scale up backends: `kubectl scale deployment/api --replicas=5 -n production` ### Step 4 Check for resource exhaustion on backend pods ### Step 5 Verify network policies allow traffic ## Escalation",
              "score": null
            },
            {
              "doc_id": "runbook_api_gateway_5xx_errors",
              "chunk_id": "runbook_api_gateway_5xx_errors_fix_0",
              "content": "# Runbook: API Gateway 5xx Errors ## Severity **Level:** P3 - Medium **Response Time:** 1 hour ## Symptoms - Error rate > 1% - 5xx responses in access logs - Downstream service failures ## Diagnosis ### Step 1 Check Kong error logs: `kubectl logs -l app=kong -n kong --tail=100 | grep -E '5[0-9]{2}'` ### Step 2 Verify backend health: `kubectl get pods -n production -o wide` ### Step 3 Check service endpoints: `kubectl get endpoints",
              "score": null
            }
          ],
          "latency_ms": 5.742470000768662,
          "query_id": "troubleshoot_031",
          "dimension": "problem",
          "query": "pods keep restarting with OOMKilled status and kubectl describe shows memory at limit"
        },
        {
          "key_facts": [
            "OOMKilled",
            "resources.limits.memory",
            "HeapDumpOnOutOfMemoryError"
          ],
          "found_facts": [
            "OOMKilled",
            "resources.limits.memory",
            "HeapDumpOnOutOfMemoryError"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "runbook_out_of_memory_oom_kill"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "runbook_out_of_memory_oom_kills",
              "chunk_id": "runbook_out_of_memory_oom_kills_fix_0",
              "content": "# Runbook: Out of Memory (OOM) Kills ## Severity **Level:** P3 - Medium **Response Time:** 15 minutes ## Symptoms - Pods restarting with OOMKilled status - Memory usage at limit - Application slowdown before restart ## Diagnosis ### Step 1 Check OOM events: `kubectl get events -n production --field-selector reason=OOMKilled` ### Step 2 Review memory usage: `kubectl top pods -n production --sort-by=memory` ### Step 3 Check container limits: `kubectl describe pod <pod-name> -n production |",
              "score": null
            },
            {
              "doc_id": "runbook_out_of_memory_oom_kills",
              "chunk_id": "runbook_out_of_memory_oom_kills_fix_1",
              "content": "grep -A5 Limits` ### Step 4 Analyze heap dumps if available: `cloudflow debug heapdump --pod <pod-name>` ## Resolution ### Step 1 Increase memory limit: update deployment resources.limits.memory ### Step 2 Add memory request equal to limit to guarantee allocation ### Step 3 Enable JVM heap dump on OOM: add -XX:+HeapDumpOnOutOfMemoryError ### Step 4 Investigate memory leaks with profiling tools ### Step 5 Consider horizontal scaling instead of vertical ## Escalation If OOMs continue after limit",
              "score": null
            },
            {
              "doc_id": "runbook_high_cpu_usage",
              "chunk_id": "runbook_high_cpu_usage_fix_1",
              "content": "Step 4 Profile the application: `cloudflow debug profile --pod api-xyz --duration 60s` ## Resolution ### Step 1 Scale up if traffic-related: `kubectl scale deployment/api --replicas=10 -n production` ### Step 2 Restart pods if memory leak suspected: `kubectl rollout restart deployment/api -n production` ### Step 3 Roll back recent deployment if regression: `kubectl rollout undo deployment/api -n production` ### Step 4 Add resource limits if unbounded: update deployment with CPU limits ## Escalation If CPU remains",
              "score": null
            },
            {
              "doc_id": "adr_004",
              "chunk_id": "adr_004_fix_1",
              "content": "**Pros:** - Simple operation - Multi-threaded - Lower memory overhead **Cons:** - No persistence - Limited data types - No clustering ### Option 3: Application-level cache In-process caching only. **Pros:** - No external dependency - Fastest possible - Simple **Cons:** - Not shared across pods - Memory pressure - Cold starts ## Decision We will use Redis Cluster for caching with a 6-node cluster for high availability.",
              "score": null
            },
            {
              "doc_id": "runbook_high_cpu_usage",
              "chunk_id": "runbook_high_cpu_usage_fix_0",
              "content": "# Runbook: High CPU Usage ## Severity **Level:** P1 - Critical **Response Time:** 15 minutes ## Symptoms - CPU utilization > 80% sustained - Increased response latency - Autoscaler at maximum replicas ## Diagnosis ### Step 1 Check current CPU usage: `kubectl top pods -n production` ### Step 2 Identify hot pods: `kubectl top pods --sort-by=cpu -n production | head -10` ### Step 3 Check for recent deployments: `kubectl rollout history deployment/api -n production` ###",
              "score": null
            }
          ],
          "latency_ms": 4.146235000007437,
          "query_id": "troubleshoot_031",
          "dimension": "casual",
          "query": "pods crashing oom, need more memory?"
        },
        {
          "key_facts": [
            "OOMKilled",
            "resources.limits.memory",
            "HeapDumpOnOutOfMemoryError"
          ],
          "found_facts": [
            "OOMKilled",
            "resources.limits.memory",
            "HeapDumpOnOutOfMemoryError"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "runbook_out_of_memory_oom_kill"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "runbook_out_of_memory_oom_kills",
              "chunk_id": "runbook_out_of_memory_oom_kills_fix_1",
              "content": "grep -A5 Limits` ### Step 4 Analyze heap dumps if available: `cloudflow debug heapdump --pod <pod-name>` ## Resolution ### Step 1 Increase memory limit: update deployment resources.limits.memory ### Step 2 Add memory request equal to limit to guarantee allocation ### Step 3 Enable JVM heap dump on OOM: add -XX:+HeapDumpOnOutOfMemoryError ### Step 4 Investigate memory leaks with profiling tools ### Step 5 Consider horizontal scaling instead of vertical ## Escalation If OOMs continue after limit",
              "score": null
            },
            {
              "doc_id": "runbook_out_of_memory_oom_kills",
              "chunk_id": "runbook_out_of_memory_oom_kills_fix_2",
              "content": "increase, escalate to development team for memory leak investigation. ## Post-Incident - Document timeline in incident log - Schedule post-mortem within 48 hours - Update runbook if new patterns discovered",
              "score": null
            },
            {
              "doc_id": "runbook_out_of_memory_oom_kills",
              "chunk_id": "runbook_out_of_memory_oom_kills_fix_0",
              "content": "# Runbook: Out of Memory (OOM) Kills ## Severity **Level:** P3 - Medium **Response Time:** 15 minutes ## Symptoms - Pods restarting with OOMKilled status - Memory usage at limit - Application slowdown before restart ## Diagnosis ### Step 1 Check OOM events: `kubectl get events -n production --field-selector reason=OOMKilled` ### Step 2 Review memory usage: `kubectl top pods -n production --sort-by=memory` ### Step 3 Check container limits: `kubectl describe pod <pod-name> -n production |",
              "score": null
            },
            {
              "doc_id": "arch_authentication_service",
              "chunk_id": "arch_authentication_service_fix_2",
              "content": "**Technology:** Dex OIDC proxy **Scaling:** Active-passive ## Data Flow Credentials \u2192 Validation \u2192 Token Issue \u2192 Token Refresh \u2192 Logout ## Performance Characteristics - **Latency P50:** 50ms - **Latency P99:** 200ms - **Throughput:** 5000 auth/second",
              "score": null
            },
            {
              "doc_id": "adr_004",
              "chunk_id": "adr_004_fix_2",
              "content": "## Consequences - Implement cache-aside pattern - Set appropriate TTLs - Monitor memory usage - Plan for cache stampede prevention",
              "score": null
            }
          ],
          "latency_ms": 3.8815729985799408,
          "query_id": "troubleshoot_031",
          "dimension": "contextual",
          "query": "our Java app keeps getting OOM killed after processing large files, need to figure out if it's a leak or just undersized"
        },
        {
          "key_facts": [
            "OOMKilled",
            "resources.limits.memory",
            "HeapDumpOnOutOfMemoryError"
          ],
          "found_facts": [
            "OOMKilled",
            "resources.limits.memory",
            "HeapDumpOnOutOfMemoryError"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "runbook_out_of_memory_oom_kill"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "runbook_out_of_memory_oom_kills",
              "chunk_id": "runbook_out_of_memory_oom_kills_fix_1",
              "content": "grep -A5 Limits` ### Step 4 Analyze heap dumps if available: `cloudflow debug heapdump --pod <pod-name>` ## Resolution ### Step 1 Increase memory limit: update deployment resources.limits.memory ### Step 2 Add memory request equal to limit to guarantee allocation ### Step 3 Enable JVM heap dump on OOM: add -XX:+HeapDumpOnOutOfMemoryError ### Step 4 Investigate memory leaks with profiling tools ### Step 5 Consider horizontal scaling instead of vertical ## Escalation If OOMs continue after limit",
              "score": null
            },
            {
              "doc_id": "runbook_out_of_memory_oom_kills",
              "chunk_id": "runbook_out_of_memory_oom_kills_fix_0",
              "content": "# Runbook: Out of Memory (OOM) Kills ## Severity **Level:** P3 - Medium **Response Time:** 15 minutes ## Symptoms - Pods restarting with OOMKilled status - Memory usage at limit - Application slowdown before restart ## Diagnosis ### Step 1 Check OOM events: `kubectl get events -n production --field-selector reason=OOMKilled` ### Step 2 Review memory usage: `kubectl top pods -n production --sort-by=memory` ### Step 3 Check container limits: `kubectl describe pod <pod-name> -n production |",
              "score": null
            },
            {
              "doc_id": "runbook_high_cpu_usage",
              "chunk_id": "runbook_high_cpu_usage_fix_1",
              "content": "Step 4 Profile the application: `cloudflow debug profile --pod api-xyz --duration 60s` ## Resolution ### Step 1 Scale up if traffic-related: `kubectl scale deployment/api --replicas=10 -n production` ### Step 2 Restart pods if memory leak suspected: `kubectl rollout restart deployment/api -n production` ### Step 3 Roll back recent deployment if regression: `kubectl rollout undo deployment/api -n production` ### Step 4 Add resource limits if unbounded: update deployment with CPU limits ## Escalation If CPU remains",
              "score": null
            },
            {
              "doc_id": "adr_004",
              "chunk_id": "adr_004_fix_0",
              "content": "# ADR-004: Use Redis for Caching and Session Storage **Status:** Accepted **Date:** 2024-04-05 ## Context We need a fast data store for caching API responses, session data, and rate limiting counters. ## Options Considered ### Option 1: Redis Cluster In-memory data structure store. **Pros:** - Sub-millisecond latency - Rich data structures - Pub/sub support - Clustering **Cons:** - Memory cost - Persistence complexity - Cluster management ### Option 2: Memcached Simple key-value cache.",
              "score": null
            },
            {
              "doc_id": "runbook_out_of_memory_oom_kills",
              "chunk_id": "runbook_out_of_memory_oom_kills_fix_2",
              "content": "increase, escalate to development team for memory leak investigation. ## Post-Incident - Document timeline in incident log - Schedule post-mortem within 48 hours - Update runbook if new patterns discovered",
              "score": null
            }
          ],
          "latency_ms": 3.8110410005174344,
          "query_id": "troubleshoot_031",
          "dimension": "negation",
          "query": "why do my containers keep dying from memory issues even with 4GB limit?"
        },
        {
          "key_facts": [
            "kubectl logs",
            "circuit breaker",
            "kubectl get endpoints"
          ],
          "found_facts": [
            "kubectl logs",
            "kubectl get endpoints"
          ],
          "missed_facts": [
            "circuit breaker"
          ],
          "coverage": 0.6666666666666666,
          "expected_docs": [
            "runbook_api_gateway_5xx_error"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "runbook_api_gateway_5xx_errors",
              "chunk_id": "runbook_api_gateway_5xx_errors_fix_0",
              "content": "# Runbook: API Gateway 5xx Errors ## Severity **Level:** P3 - Medium **Response Time:** 1 hour ## Symptoms - Error rate > 1% - 5xx responses in access logs - Downstream service failures ## Diagnosis ### Step 1 Check Kong error logs: `kubectl logs -l app=kong -n kong --tail=100 | grep -E '5[0-9]{2}'` ### Step 2 Verify backend health: `kubectl get pods -n production -o wide` ### Step 3 Check service endpoints: `kubectl get endpoints",
              "score": null
            },
            {
              "doc_id": "config_api_gateway",
              "chunk_id": "config_api_gateway_fix_0",
              "content": "# API Gateway Configuration Reference ## Overview This document describes all configuration options for API Gateway. ## Environment Variables | Variable | Type | Default | Description | |----------|------|---------|-------------| | `GATEWAY_PORT` | integer | 8000 | Gateway listen port | | `GATEWAY_RATE_LIMIT` | integer | 100 | Requests per minute per user | | `GATEWAY_TIMEOUT_MS` | integer | 30000 | Request timeout in milliseconds | | `GATEWAY_MAX_BODY_SIZE` | string | 10MB | Maximum request body",
              "score": null
            },
            {
              "doc_id": "config_api_gateway",
              "chunk_id": "config_api_gateway_fix_1",
              "content": "size | | `GATEWAY_CORS_ORIGINS` | string | * | Allowed CORS origins | | `GATEWAY_SSL_ENABLED` | boolean | true | Enable TLS termination | ## Configuration File Configuration can also be provided via `api gateway.yaml`: ```yaml gateway_port: 8000 gateway_rate_limit: 100 gateway_timeout_ms: 30000 gateway_max_body_size: 10MB gateway_cors_origins: * ``` ## Validation Configuration is validated at startup. Invalid configuration will prevent the service from starting.",
              "score": null
            },
            {
              "doc_id": "arch_api_gateway",
              "chunk_id": "arch_api_gateway_fix_0",
              "content": "# API Gateway Architecture ## Overview The API Gateway handles all incoming HTTP requests, providing authentication, rate limiting, and request routing. Built on Kong with custom plugins for CloudFlow-specific functionality. ## Design Principles - **Zero Trust**: Every request is authenticated and authorized - **Defense in Depth**: Multiple layers of security validation ## Components ### Kong Gateway Primary ingress controller. Handles TLS termination, request routing, and plugin execution.",
              "score": null
            },
            {
              "doc_id": "adr_007",
              "chunk_id": "adr_007_fix_0",
              "content": "# ADR-007: Implement Rate Limiting at API Gateway **Status:** Accepted **Date:** 2024-09-03 ## Context We need to protect our services from abuse and ensure fair resource usage across customers. ## Options Considered ### Option 1: Sliding window at gateway Rate limit at API gateway level.",
              "score": null
            }
          ],
          "latency_ms": 3.3451130002504215,
          "query_id": "troubleshoot_032",
          "dimension": "original",
          "query": "How to troubleshoot API Gateway 5xx errors?"
        },
        {
          "key_facts": [
            "kubectl logs",
            "circuit breaker",
            "kubectl get endpoints"
          ],
          "found_facts": [
            "kubectl logs",
            "circuit breaker",
            "kubectl get endpoints"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "runbook_api_gateway_5xx_error"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "runbook_api_gateway_5xx_errors",
              "chunk_id": "runbook_api_gateway_5xx_errors_fix_0",
              "content": "# Runbook: API Gateway 5xx Errors ## Severity **Level:** P3 - Medium **Response Time:** 1 hour ## Symptoms - Error rate > 1% - 5xx responses in access logs - Downstream service failures ## Diagnosis ### Step 1 Check Kong error logs: `kubectl logs -l app=kong -n kong --tail=100 | grep -E '5[0-9]{2}'` ### Step 2 Verify backend health: `kubectl get pods -n production -o wide` ### Step 3 Check service endpoints: `kubectl get endpoints",
              "score": null
            },
            {
              "doc_id": "howto_debug_failed_execution",
              "chunk_id": "howto_debug_failed_execution_fix_0",
              "content": "# How to Debug Failed Execution ## Prerequisites - Failed execution ID ## Steps ### Step 1: Get execution details Retrieve the full execution record including step outputs. ```bash cloudflow executions get ex_123 --include-steps ``` ### Step 2: View step logs Check logs for the failed step. ```bash cloudflow executions logs ex_123 --step 3 ``` ### Step 3: Inspect input/output Review the data passed between steps.",
              "score": null
            },
            {
              "doc_id": "runbook_api_gateway_5xx_errors",
              "chunk_id": "runbook_api_gateway_5xx_errors_fix_1",
              "content": "-n production` ### Step 4 Test backend directly: `kubectl exec -it kong-xxx -n kong -- curl http://api.production.svc:8080/health` ## Resolution ### Step 1 Restart unhealthy backends: `kubectl rollout restart deployment/api -n production` ### Step 2 Enable circuit breaker if cascading: update Kong circuit-breaker plugin ### Step 3 Scale up backends: `kubectl scale deployment/api --replicas=5 -n production` ### Step 4 Check for resource exhaustion on backend pods ### Step 5 Verify network policies allow traffic ## Escalation",
              "score": null
            },
            {
              "doc_id": "howto_debug_failed_execution",
              "chunk_id": "howto_debug_failed_execution_fix_1",
              "content": "```bash cloudflow executions inspect ex_123 --step 3 ``` ### Step 4: Enable debug mode Re-run with verbose logging enabled. ```bash cloudflow workflows execute wf_123 --debug ``` ### Step 5: Check error patterns Look for similar failures in recent executions. ```bash cloudflow executions list --status failed --since 24h ``` ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            },
            {
              "doc_id": "runbook_out_of_memory_oom_kills",
              "chunk_id": "runbook_out_of_memory_oom_kills_fix_1",
              "content": "grep -A5 Limits` ### Step 4 Analyze heap dumps if available: `cloudflow debug heapdump --pod <pod-name>` ## Resolution ### Step 1 Increase memory limit: update deployment resources.limits.memory ### Step 2 Add memory request equal to limit to guarantee allocation ### Step 3 Enable JVM heap dump on OOM: add -XX:+HeapDumpOnOutOfMemoryError ### Step 4 Investigate memory leaks with profiling tools ### Step 5 Consider horizontal scaling instead of vertical ## Escalation If OOMs continue after limit",
              "score": null
            }
          ],
          "latency_ms": 3.690916999403271,
          "query_id": "troubleshoot_032",
          "dimension": "synonym",
          "query": "How do I debug server errors from the ingress layer?"
        },
        {
          "key_facts": [
            "kubectl logs",
            "circuit breaker",
            "kubectl get endpoints"
          ],
          "found_facts": [
            "kubectl logs",
            "kubectl get endpoints"
          ],
          "missed_facts": [
            "circuit breaker"
          ],
          "coverage": 0.6666666666666666,
          "expected_docs": [
            "runbook_api_gateway_5xx_error"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "runbook_api_gateway_5xx_errors",
              "chunk_id": "runbook_api_gateway_5xx_errors_fix_0",
              "content": "# Runbook: API Gateway 5xx Errors ## Severity **Level:** P3 - Medium **Response Time:** 1 hour ## Symptoms - Error rate > 1% - 5xx responses in access logs - Downstream service failures ## Diagnosis ### Step 1 Check Kong error logs: `kubectl logs -l app=kong -n kong --tail=100 | grep -E '5[0-9]{2}'` ### Step 2 Verify backend health: `kubectl get pods -n production -o wide` ### Step 3 Check service endpoints: `kubectl get endpoints",
              "score": null
            },
            {
              "doc_id": "adr_007",
              "chunk_id": "adr_007_fix_0",
              "content": "# ADR-007: Implement Rate Limiting at API Gateway **Status:** Accepted **Date:** 2024-09-03 ## Context We need to protect our services from abuse and ensure fair resource usage across customers. ## Options Considered ### Option 1: Sliding window at gateway Rate limit at API gateway level.",
              "score": null
            },
            {
              "doc_id": "config_api_gateway",
              "chunk_id": "config_api_gateway_fix_0",
              "content": "# API Gateway Configuration Reference ## Overview This document describes all configuration options for API Gateway. ## Environment Variables | Variable | Type | Default | Description | |----------|------|---------|-------------| | `GATEWAY_PORT` | integer | 8000 | Gateway listen port | | `GATEWAY_RATE_LIMIT` | integer | 100 | Requests per minute per user | | `GATEWAY_TIMEOUT_MS` | integer | 30000 | Request timeout in milliseconds | | `GATEWAY_MAX_BODY_SIZE` | string | 10MB | Maximum request body",
              "score": null
            },
            {
              "doc_id": "config_api_gateway",
              "chunk_id": "config_api_gateway_fix_1",
              "content": "size | | `GATEWAY_CORS_ORIGINS` | string | * | Allowed CORS origins | | `GATEWAY_SSL_ENABLED` | boolean | true | Enable TLS termination | ## Configuration File Configuration can also be provided via `api gateway.yaml`: ```yaml gateway_port: 8000 gateway_rate_limit: 100 gateway_timeout_ms: 30000 gateway_max_body_size: 10MB gateway_cors_origins: * ``` ## Validation Configuration is validated at startup. Invalid configuration will prevent the service from starting.",
              "score": null
            },
            {
              "doc_id": "arch_api_gateway",
              "chunk_id": "arch_api_gateway_fix_0",
              "content": "# API Gateway Architecture ## Overview The API Gateway handles all incoming HTTP requests, providing authentication, rate limiting, and request routing. Built on Kong with custom plugins for CloudFlow-specific functionality. ## Design Principles - **Zero Trust**: Every request is authenticated and authorized - **Defense in Depth**: Multiple layers of security validation ## Components ### Kong Gateway Primary ingress controller. Handles TLS termination, request routing, and plugin execution.",
              "score": null
            }
          ],
          "latency_ms": 4.053513001053943,
          "query_id": "troubleshoot_032",
          "dimension": "problem",
          "query": "error rate jumped to 5% and gateway logs show 502 Bad Gateway errors from backend services"
        },
        {
          "key_facts": [
            "kubectl logs",
            "circuit breaker",
            "kubectl get endpoints"
          ],
          "found_facts": [
            "kubectl logs",
            "kubectl get endpoints"
          ],
          "missed_facts": [
            "circuit breaker"
          ],
          "coverage": 0.6666666666666666,
          "expected_docs": [
            "runbook_api_gateway_5xx_error"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "config_api_gateway",
              "chunk_id": "config_api_gateway_fix_0",
              "content": "# API Gateway Configuration Reference ## Overview This document describes all configuration options for API Gateway. ## Environment Variables | Variable | Type | Default | Description | |----------|------|---------|-------------| | `GATEWAY_PORT` | integer | 8000 | Gateway listen port | | `GATEWAY_RATE_LIMIT` | integer | 100 | Requests per minute per user | | `GATEWAY_TIMEOUT_MS` | integer | 30000 | Request timeout in milliseconds | | `GATEWAY_MAX_BODY_SIZE` | string | 10MB | Maximum request body",
              "score": null
            },
            {
              "doc_id": "runbook_api_gateway_5xx_errors",
              "chunk_id": "runbook_api_gateway_5xx_errors_fix_0",
              "content": "# Runbook: API Gateway 5xx Errors ## Severity **Level:** P3 - Medium **Response Time:** 1 hour ## Symptoms - Error rate > 1% - 5xx responses in access logs - Downstream service failures ## Diagnosis ### Step 1 Check Kong error logs: `kubectl logs -l app=kong -n kong --tail=100 | grep -E '5[0-9]{2}'` ### Step 2 Verify backend health: `kubectl get pods -n production -o wide` ### Step 3 Check service endpoints: `kubectl get endpoints",
              "score": null
            },
            {
              "doc_id": "arch_api_gateway",
              "chunk_id": "arch_api_gateway_fix_2",
              "content": "**Technology:** Redis-backed sliding window **Scaling:** Shared Redis cluster ## Data Flow Client \u2192 TLS \u2192 Kong \u2192 Auth \u2192 Rate Limit \u2192 Backend Services ## Performance Characteristics - **Latency P50:** 12ms - **Latency P99:** 45ms - **Throughput:** 50000 requests/second",
              "score": null
            },
            {
              "doc_id": "api_secrets",
              "chunk_id": "api_secrets_fix_2",
              "content": "**Parameters:** **Response:** - Success: `204` - Secret deleted - Error: `400` - Bad Request **Example:** ```json {} ```",
              "score": null
            },
            {
              "doc_id": "config_api_gateway",
              "chunk_id": "config_api_gateway_fix_1",
              "content": "size | | `GATEWAY_CORS_ORIGINS` | string | * | Allowed CORS origins | | `GATEWAY_SSL_ENABLED` | boolean | true | Enable TLS termination | ## Configuration File Configuration can also be provided via `api gateway.yaml`: ```yaml gateway_port: 8000 gateway_rate_limit: 100 gateway_timeout_ms: 30000 gateway_max_body_size: 10MB gateway_cors_origins: * ``` ## Validation Configuration is validated at startup. Invalid configuration will prevent the service from starting.",
              "score": null
            }
          ],
          "latency_ms": 4.086353999809944,
          "query_id": "troubleshoot_032",
          "dimension": "casual",
          "query": "gateway throwing 500s, backends look fine though"
        },
        {
          "key_facts": [
            "kubectl logs",
            "circuit breaker",
            "kubectl get endpoints"
          ],
          "found_facts": [
            "kubectl logs",
            "kubectl get endpoints"
          ],
          "missed_facts": [
            "circuit breaker"
          ],
          "coverage": 0.6666666666666666,
          "expected_docs": [
            "runbook_api_gateway_5xx_error"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "runbook_api_gateway_5xx_errors",
              "chunk_id": "runbook_api_gateway_5xx_errors_fix_0",
              "content": "# Runbook: API Gateway 5xx Errors ## Severity **Level:** P3 - Medium **Response Time:** 1 hour ## Symptoms - Error rate > 1% - 5xx responses in access logs - Downstream service failures ## Diagnosis ### Step 1 Check Kong error logs: `kubectl logs -l app=kong -n kong --tail=100 | grep -E '5[0-9]{2}'` ### Step 2 Verify backend health: `kubectl get pods -n production -o wide` ### Step 3 Check service endpoints: `kubectl get endpoints",
              "score": null
            },
            {
              "doc_id": "config_api_gateway",
              "chunk_id": "config_api_gateway_fix_0",
              "content": "# API Gateway Configuration Reference ## Overview This document describes all configuration options for API Gateway. ## Environment Variables | Variable | Type | Default | Description | |----------|------|---------|-------------| | `GATEWAY_PORT` | integer | 8000 | Gateway listen port | | `GATEWAY_RATE_LIMIT` | integer | 100 | Requests per minute per user | | `GATEWAY_TIMEOUT_MS` | integer | 30000 | Request timeout in milliseconds | | `GATEWAY_MAX_BODY_SIZE` | string | 10MB | Maximum request body",
              "score": null
            },
            {
              "doc_id": "adr_007",
              "chunk_id": "adr_007_fix_0",
              "content": "# ADR-007: Implement Rate Limiting at API Gateway **Status:** Accepted **Date:** 2024-09-03 ## Context We need to protect our services from abuse and ensure fair resource usage across customers. ## Options Considered ### Option 1: Sliding window at gateway Rate limit at API gateway level.",
              "score": null
            },
            {
              "doc_id": "arch_monitoring_stack",
              "chunk_id": "arch_monitoring_stack_fix_0",
              "content": "# Monitoring Stack Architecture ## Overview The Monitoring Stack provides observability into system health, performance, and errors. Includes metrics, logs, traces, and alerting. ## Design Principles - **Observability**: Every component emits metrics and traces - **Actionable Alerts**: Alerts include runbook links ## Components ### Metrics Collects time-series metrics. Retention is 15 days local, 1 year in object storage. **Technology:** Prometheus with Thanos **Scaling:** Federated with long-term storage ### Logging Centralized log aggregation.",
              "score": null
            },
            {
              "doc_id": "config_api_gateway",
              "chunk_id": "config_api_gateway_fix_1",
              "content": "size | | `GATEWAY_CORS_ORIGINS` | string | * | Allowed CORS origins | | `GATEWAY_SSL_ENABLED` | boolean | true | Enable TLS termination | ## Configuration File Configuration can also be provided via `api gateway.yaml`: ```yaml gateway_port: 8000 gateway_rate_limit: 100 gateway_timeout_ms: 30000 gateway_max_body_size: 10MB gateway_cors_origins: * ``` ## Validation Configuration is validated at startup. Invalid configuration will prevent the service from starting.",
              "score": null
            }
          ],
          "latency_ms": 3.9764190005371347,
          "query_id": "troubleshoot_032",
          "dimension": "contextual",
          "query": "customers are reporting intermittent errors, monitoring shows 5xx spike from API gateway after we scaled down backends"
        },
        {
          "key_facts": [
            "kubectl logs",
            "circuit breaker",
            "kubectl get endpoints"
          ],
          "found_facts": [
            "kubectl logs",
            "circuit breaker",
            "kubectl get endpoints"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "runbook_api_gateway_5xx_error"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "runbook_api_gateway_5xx_errors",
              "chunk_id": "runbook_api_gateway_5xx_errors_fix_0",
              "content": "# Runbook: API Gateway 5xx Errors ## Severity **Level:** P3 - Medium **Response Time:** 1 hour ## Symptoms - Error rate > 1% - 5xx responses in access logs - Downstream service failures ## Diagnosis ### Step 1 Check Kong error logs: `kubectl logs -l app=kong -n kong --tail=100 | grep -E '5[0-9]{2}'` ### Step 2 Verify backend health: `kubectl get pods -n production -o wide` ### Step 3 Check service endpoints: `kubectl get endpoints",
              "score": null
            },
            {
              "doc_id": "runbook_api_gateway_5xx_errors",
              "chunk_id": "runbook_api_gateway_5xx_errors_fix_1",
              "content": "-n production` ### Step 4 Test backend directly: `kubectl exec -it kong-xxx -n kong -- curl http://api.production.svc:8080/health` ## Resolution ### Step 1 Restart unhealthy backends: `kubectl rollout restart deployment/api -n production` ### Step 2 Enable circuit breaker if cascading: update Kong circuit-breaker plugin ### Step 3 Scale up backends: `kubectl scale deployment/api --replicas=5 -n production` ### Step 4 Check for resource exhaustion on backend pods ### Step 5 Verify network policies allow traffic ## Escalation",
              "score": null
            },
            {
              "doc_id": "runbook_high_cpu_usage",
              "chunk_id": "runbook_high_cpu_usage_fix_1",
              "content": "Step 4 Profile the application: `cloudflow debug profile --pod api-xyz --duration 60s` ## Resolution ### Step 1 Scale up if traffic-related: `kubectl scale deployment/api --replicas=10 -n production` ### Step 2 Restart pods if memory leak suspected: `kubectl rollout restart deployment/api -n production` ### Step 3 Roll back recent deployment if regression: `kubectl rollout undo deployment/api -n production` ### Step 4 Add resource limits if unbounded: update deployment with CPU limits ## Escalation If CPU remains",
              "score": null
            },
            {
              "doc_id": "config_api_gateway",
              "chunk_id": "config_api_gateway_fix_0",
              "content": "# API Gateway Configuration Reference ## Overview This document describes all configuration options for API Gateway. ## Environment Variables | Variable | Type | Default | Description | |----------|------|---------|-------------| | `GATEWAY_PORT` | integer | 8000 | Gateway listen port | | `GATEWAY_RATE_LIMIT` | integer | 100 | Requests per minute per user | | `GATEWAY_TIMEOUT_MS` | integer | 30000 | Request timeout in milliseconds | | `GATEWAY_MAX_BODY_SIZE` | string | 10MB | Maximum request body",
              "score": null
            },
            {
              "doc_id": "runbook_high_cpu_usage",
              "chunk_id": "runbook_high_cpu_usage_fix_0",
              "content": "# Runbook: High CPU Usage ## Severity **Level:** P1 - Critical **Response Time:** 15 minutes ## Symptoms - CPU utilization > 80% sustained - Increased response latency - Autoscaler at maximum replicas ## Diagnosis ### Step 1 Check current CPU usage: `kubectl top pods -n production` ### Step 2 Identify hot pods: `kubectl top pods --sort-by=cpu -n production | head -10` ### Step 3 Check for recent deployments: `kubectl rollout history deployment/api -n production` ###",
              "score": null
            }
          ],
          "latency_ms": 3.64219699986279,
          "query_id": "troubleshoot_032",
          "dimension": "negation",
          "query": "why is the API gateway returning 503 when all my backend pods are running?"
        },
        {
          "key_facts": [
            "Scheduler",
            "Executor",
            "State Store",
            "PostgreSQL with JSONB",
            "5000 executions/minute"
          ],
          "found_facts": [
            "Scheduler"
          ],
          "missed_facts": [
            "Executor",
            "State Store",
            "PostgreSQL with JSONB",
            "5000 executions/minute"
          ],
          "coverage": 0.2,
          "expected_docs": [
            "arch_workflow_engine"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "arch_workflow_engine",
              "chunk_id": "arch_workflow_engine_fix_0",
              "content": "# Workflow Engine Architecture ## Overview The Workflow Engine is the core component responsible for executing user-defined workflows. It uses an event-driven architecture with support for parallel execution, conditional branching, and automatic retries. ## Design Principles - **Idempotency**: All operations can be safely retried without side effects - **Eventual Consistency**: State converges to consistent view within 5 seconds - **Fault Tolerance**: System continues operating despite component failures ## Components ### Scheduler Manages workflow scheduling and",
              "score": null
            },
            {
              "doc_id": "arch_data_pipeline",
              "chunk_id": "arch_data_pipeline_fix_0",
              "content": "# Data Pipeline Architecture ## Overview The Data Pipeline processes and transforms data between workflow steps. It supports batch processing up to 10GB and streaming for real-time use cases. ## Design Principles - **Exactly-Once Processing**: Each record is processed exactly once - **Backpressure Handling**: System gracefully handles varying load ## Components ### Stream Processor Handles real-time data streams. Maximum throughput of 100000 messages per second.",
              "score": null
            },
            {
              "doc_id": "config_workflow_engine",
              "chunk_id": "config_workflow_engine_fix_0",
              "content": "# Workflow Engine Configuration Reference ## Overview This document describes all configuration options for Workflow Engine. ## Environment Variables | Variable | Type | Default | Description | |----------|------|---------|-------------| | `WORKFLOW_MAX_STEPS` | integer | 100 | Maximum steps per workflow | | `WORKFLOW_TIMEOUT_SECONDS` | integer | 3600 | Maximum workflow execution time | | `WORKFLOW_RETRY_LIMIT` | integer | 3 | Default retry attempts for failed steps | | `WORKFLOW_CONCURRENT_LIMIT` | integer | 10 | Maximum",
              "score": null
            },
            {
              "doc_id": "api_workflows",
              "chunk_id": "api_workflows_fix_0",
              "content": "# Workflows API Reference ## Overview The Workflows API provides programmatic access to workflows functionality. Base URL: `https://api.cloudflow.io/v1/workflows` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /workflows List all workflows for the authenticated user.",
              "score": null
            },
            {
              "doc_id": "adr_005",
              "chunk_id": "adr_005_fix_1",
              "content": "**Pros:** - Simple mental model - Immediate feedback - Easy debugging **Cons:** - Tight coupling - Cascading failures - Timeout issues ### Option 3: Polling-based Workers poll for tasks. **Pros:** - Simple implementation - Easy rate limiting - No message broker **Cons:** - Polling overhead - Higher latency - Inefficient at scale ## Decision We will use an event-driven architecture with Kafka for workflow step execution.",
              "score": null
            }
          ],
          "latency_ms": 3.5338350007805275,
          "query_id": "arch_033",
          "dimension": "original",
          "query": "Explain the Workflow Engine architecture"
        },
        {
          "key_facts": [
            "Scheduler",
            "Executor",
            "State Store",
            "PostgreSQL with JSONB",
            "5000 executions/minute"
          ],
          "found_facts": [
            "Scheduler"
          ],
          "missed_facts": [
            "Executor",
            "State Store",
            "PostgreSQL with JSONB",
            "5000 executions/minute"
          ],
          "coverage": 0.2,
          "expected_docs": [
            "arch_workflow_engine"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "arch_workflow_engine",
              "chunk_id": "arch_workflow_engine_fix_0",
              "content": "# Workflow Engine Architecture ## Overview The Workflow Engine is the core component responsible for executing user-defined workflows. It uses an event-driven architecture with support for parallel execution, conditional branching, and automatic retries. ## Design Principles - **Idempotency**: All operations can be safely retried without side effects - **Eventual Consistency**: State converges to consistent view within 5 seconds - **Fault Tolerance**: System continues operating despite component failures ## Components ### Scheduler Manages workflow scheduling and",
              "score": null
            },
            {
              "doc_id": "adr_005",
              "chunk_id": "adr_005_fix_1",
              "content": "**Pros:** - Simple mental model - Immediate feedback - Easy debugging **Cons:** - Tight coupling - Cascading failures - Timeout issues ### Option 3: Polling-based Workers poll for tasks. **Pros:** - Simple implementation - Easy rate limiting - No message broker **Cons:** - Polling overhead - Higher latency - Inefficient at scale ## Decision We will use an event-driven architecture with Kafka for workflow step execution.",
              "score": null
            },
            {
              "doc_id": "config_workflow_engine",
              "chunk_id": "config_workflow_engine_fix_0",
              "content": "# Workflow Engine Configuration Reference ## Overview This document describes all configuration options for Workflow Engine. ## Environment Variables | Variable | Type | Default | Description | |----------|------|---------|-------------| | `WORKFLOW_MAX_STEPS` | integer | 100 | Maximum steps per workflow | | `WORKFLOW_TIMEOUT_SECONDS` | integer | 3600 | Maximum workflow execution time | | `WORKFLOW_RETRY_LIMIT` | integer | 3 | Default retry attempts for failed steps | | `WORKFLOW_CONCURRENT_LIMIT` | integer | 10 | Maximum",
              "score": null
            },
            {
              "doc_id": "adr_005",
              "chunk_id": "adr_005_fix_0",
              "content": "# ADR-005: Adopt Event-Driven Architecture for Workflow Execution **Status:** Accepted **Date:** 2024-12-04 ## Context We need an architecture that supports asynchronous workflow execution, step dependencies, and failure recovery. ## Options Considered ### Option 1: Event-driven with Kafka Publish-subscribe messaging. **Pros:** - Loose coupling - Scalability - Event replay - Audit trail **Cons:** - Eventual consistency - Operational complexity - Message ordering challenges ### Option 2: Request-response Synchronous API calls.",
              "score": null
            },
            {
              "doc_id": "arch_data_pipeline",
              "chunk_id": "arch_data_pipeline_fix_0",
              "content": "# Data Pipeline Architecture ## Overview The Data Pipeline processes and transforms data between workflow steps. It supports batch processing up to 10GB and streaming for real-time use cases. ## Design Principles - **Exactly-Once Processing**: Each record is processed exactly once - **Backpressure Handling**: System gracefully handles varying load ## Components ### Stream Processor Handles real-time data streams. Maximum throughput of 100000 messages per second.",
              "score": null
            }
          ],
          "latency_ms": 4.724161999547505,
          "query_id": "arch_033",
          "dimension": "synonym",
          "query": "What is the design of the workflow orchestration system?"
        },
        {
          "key_facts": [
            "Scheduler",
            "Executor",
            "State Store",
            "PostgreSQL with JSONB",
            "5000 executions/minute"
          ],
          "found_facts": [
            "Scheduler",
            "Executor",
            "State Store",
            "PostgreSQL with JSONB",
            "5000 executions/minute"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "arch_workflow_engine"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "arch_workflow_engine",
              "chunk_id": "arch_workflow_engine_fix_0",
              "content": "# Workflow Engine Architecture ## Overview The Workflow Engine is the core component responsible for executing user-defined workflows. It uses an event-driven architecture with support for parallel execution, conditional branching, and automatic retries. ## Design Principles - **Idempotency**: All operations can be safely retried without side effects - **Eventual Consistency**: State converges to consistent view within 5 seconds - **Fault Tolerance**: System continues operating despite component failures ## Components ### Scheduler Manages workflow scheduling and",
              "score": null
            },
            {
              "doc_id": "arch_data_pipeline",
              "chunk_id": "arch_data_pipeline_fix_0",
              "content": "# Data Pipeline Architecture ## Overview The Data Pipeline processes and transforms data between workflow steps. It supports batch processing up to 10GB and streaming for real-time use cases. ## Design Principles - **Exactly-Once Processing**: Each record is processed exactly once - **Backpressure Handling**: System gracefully handles varying load ## Components ### Stream Processor Handles real-time data streams. Maximum throughput of 100000 messages per second.",
              "score": null
            },
            {
              "doc_id": "config_workflow_engine",
              "chunk_id": "config_workflow_engine_fix_1",
              "content": "concurrent executions per workflow | | `EXECUTOR_POOL_SIZE` | integer | 50 | Number of executor workers | | `EXECUTOR_MEMORY_LIMIT` | string | 512Mi | Memory limit per executor | ## Configuration File Configuration can also be provided via `workflow engine.yaml`: ```yaml workflow_max_steps: 100 workflow_timeout_seconds: 3600 workflow_retry_limit: 3 workflow_concurrent_limit: 10 executor_pool_size: 50 ``` ## Validation Configuration is validated at startup. Invalid configuration will prevent the service from starting.",
              "score": null
            },
            {
              "doc_id": "arch_workflow_engine",
              "chunk_id": "arch_workflow_engine_fix_2",
              "content": "**Technology:** PostgreSQL with JSONB **Scaling:** Primary-replica with read replicas ## Data Flow Triggers \u2192 Scheduler \u2192 Job Queue \u2192 Executor \u2192 State Store \u2192 Webhooks/Notifications ## Performance Characteristics - **Latency P50:** 45ms - **Latency P99:** 250ms - **Throughput:** 5000 executions/minute",
              "score": null
            },
            {
              "doc_id": "adr_005",
              "chunk_id": "adr_005_fix_1",
              "content": "**Pros:** - Simple mental model - Immediate feedback - Easy debugging **Cons:** - Tight coupling - Cascading failures - Timeout issues ### Option 3: Polling-based Workers poll for tasks. **Pros:** - Simple implementation - Easy rate limiting - No message broker **Cons:** - Polling overhead - Higher latency - Inefficient at scale ## Decision We will use an event-driven architecture with Kafka for workflow step execution.",
              "score": null
            }
          ],
          "latency_ms": 4.044346000227961,
          "query_id": "arch_033",
          "dimension": "problem",
          "query": "I'm seeing bottlenecks in workflow execution - need to understand how the scheduler and executor interact"
        },
        {
          "key_facts": [
            "Scheduler",
            "Executor",
            "State Store",
            "PostgreSQL with JSONB",
            "5000 executions/minute"
          ],
          "found_facts": [
            "Scheduler"
          ],
          "missed_facts": [
            "Executor",
            "State Store",
            "PostgreSQL with JSONB",
            "5000 executions/minute"
          ],
          "coverage": 0.2,
          "expected_docs": [
            "arch_workflow_engine"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "arch_workflow_engine",
              "chunk_id": "arch_workflow_engine_fix_0",
              "content": "# Workflow Engine Architecture ## Overview The Workflow Engine is the core component responsible for executing user-defined workflows. It uses an event-driven architecture with support for parallel execution, conditional branching, and automatic retries. ## Design Principles - **Idempotency**: All operations can be safely retried without side effects - **Eventual Consistency**: State converges to consistent view within 5 seconds - **Fault Tolerance**: System continues operating despite component failures ## Components ### Scheduler Manages workflow scheduling and",
              "score": null
            },
            {
              "doc_id": "config_workflow_engine",
              "chunk_id": "config_workflow_engine_fix_0",
              "content": "# Workflow Engine Configuration Reference ## Overview This document describes all configuration options for Workflow Engine. ## Environment Variables | Variable | Type | Default | Description | |----------|------|---------|-------------| | `WORKFLOW_MAX_STEPS` | integer | 100 | Maximum steps per workflow | | `WORKFLOW_TIMEOUT_SECONDS` | integer | 3600 | Maximum workflow execution time | | `WORKFLOW_RETRY_LIMIT` | integer | 3 | Default retry attempts for failed steps | | `WORKFLOW_CONCURRENT_LIMIT` | integer | 10 | Maximum",
              "score": null
            },
            {
              "doc_id": "api_workflows",
              "chunk_id": "api_workflows_fix_0",
              "content": "# Workflows API Reference ## Overview The Workflows API provides programmatic access to workflows functionality. Base URL: `https://api.cloudflow.io/v1/workflows` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /workflows List all workflows for the authenticated user.",
              "score": null
            },
            {
              "doc_id": "api_executions",
              "chunk_id": "api_executions_fix_0",
              "content": "# Executions API Reference ## Overview The Executions API provides programmatic access to executions functionality. Base URL: `https://api.cloudflow.io/v1/executions` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /executions List workflow executions.",
              "score": null
            },
            {
              "doc_id": "arch_api_gateway",
              "chunk_id": "arch_api_gateway_fix_0",
              "content": "# API Gateway Architecture ## Overview The API Gateway handles all incoming HTTP requests, providing authentication, rate limiting, and request routing. Built on Kong with custom plugins for CloudFlow-specific functionality. ## Design Principles - **Zero Trust**: Every request is authenticated and authorized - **Defense in Depth**: Multiple layers of security validation ## Components ### Kong Gateway Primary ingress controller. Handles TLS termination, request routing, and plugin execution.",
              "score": null
            }
          ],
          "latency_ms": 3.7393969996628584,
          "query_id": "arch_033",
          "dimension": "casual",
          "query": "workflow engine internals"
        },
        {
          "key_facts": [
            "Scheduler",
            "Executor",
            "State Store",
            "PostgreSQL with JSONB",
            "5000 executions/minute"
          ],
          "found_facts": [
            "Scheduler"
          ],
          "missed_facts": [
            "Executor",
            "State Store",
            "PostgreSQL with JSONB",
            "5000 executions/minute"
          ],
          "coverage": 0.2,
          "expected_docs": [
            "arch_workflow_engine"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "arch_workflow_engine",
              "chunk_id": "arch_workflow_engine_fix_0",
              "content": "# Workflow Engine Architecture ## Overview The Workflow Engine is the core component responsible for executing user-defined workflows. It uses an event-driven architecture with support for parallel execution, conditional branching, and automatic retries. ## Design Principles - **Idempotency**: All operations can be safely retried without side effects - **Eventual Consistency**: State converges to consistent view within 5 seconds - **Fault Tolerance**: System continues operating despite component failures ## Components ### Scheduler Manages workflow scheduling and",
              "score": null
            },
            {
              "doc_id": "api_executions",
              "chunk_id": "api_executions_fix_0",
              "content": "# Executions API Reference ## Overview The Executions API provides programmatic access to executions functionality. Base URL: `https://api.cloudflow.io/v1/executions` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /executions List workflow executions.",
              "score": null
            },
            {
              "doc_id": "api_workflows",
              "chunk_id": "api_workflows_fix_0",
              "content": "# Workflows API Reference ## Overview The Workflows API provides programmatic access to workflows functionality. Base URL: `https://api.cloudflow.io/v1/workflows` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /workflows List all workflows for the authenticated user.",
              "score": null
            },
            {
              "doc_id": "howto_create_scheduled_workflow",
              "chunk_id": "howto_create_scheduled_workflow_fix_1",
              "content": "> **Note:** Use IANA timezone names like America/New_York ### Step 3: Configure overlap handling Choose behavior when previous execution is still running: skip, queue, or cancel. ### Step 4: Enable notifications Get notified on schedule failures. ```bash cloudflow workflows notify --on-failure ``` ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            },
            {
              "doc_id": "howto_create_scheduled_workflow",
              "chunk_id": "howto_create_scheduled_workflow_fix_0",
              "content": "# How to Create Scheduled Workflow ## Prerequisites - Workflow definition ## Steps ### Step 1: Define schedule Use cron expression to set the schedule. Example: '0 9 * * MON-FRI' runs at 9 AM on weekdays. ### Step 2: Set timezone Specify timezone for the schedule. Default is UTC.",
              "score": null
            }
          ],
          "latency_ms": 3.7287279992597178,
          "query_id": "arch_033",
          "dimension": "contextual",
          "query": "Onboarding to the team and trying to understand how workflows get scheduled, executed, and where state is stored"
        },
        {
          "key_facts": [
            "Scheduler",
            "Executor",
            "State Store",
            "PostgreSQL with JSONB",
            "5000 executions/minute"
          ],
          "found_facts": [
            "Scheduler",
            "Executor"
          ],
          "missed_facts": [
            "State Store",
            "PostgreSQL with JSONB",
            "5000 executions/minute"
          ],
          "coverage": 0.4,
          "expected_docs": [
            "arch_workflow_engine"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "arch_workflow_engine",
              "chunk_id": "arch_workflow_engine_fix_0",
              "content": "# Workflow Engine Architecture ## Overview The Workflow Engine is the core component responsible for executing user-defined workflows. It uses an event-driven architecture with support for parallel execution, conditional branching, and automatic retries. ## Design Principles - **Idempotency**: All operations can be safely retried without side effects - **Eventual Consistency**: State converges to consistent view within 5 seconds - **Fault Tolerance**: System continues operating despite component failures ## Components ### Scheduler Manages workflow scheduling and",
              "score": null
            },
            {
              "doc_id": "config_workflow_engine",
              "chunk_id": "config_workflow_engine_fix_0",
              "content": "# Workflow Engine Configuration Reference ## Overview This document describes all configuration options for Workflow Engine. ## Environment Variables | Variable | Type | Default | Description | |----------|------|---------|-------------| | `WORKFLOW_MAX_STEPS` | integer | 100 | Maximum steps per workflow | | `WORKFLOW_TIMEOUT_SECONDS` | integer | 3600 | Maximum workflow execution time | | `WORKFLOW_RETRY_LIMIT` | integer | 3 | Default retry attempts for failed steps | | `WORKFLOW_CONCURRENT_LIMIT` | integer | 10 | Maximum",
              "score": null
            },
            {
              "doc_id": "howto_implement_retry_logic",
              "chunk_id": "howto_implement_retry_logic_fix_0",
              "content": "# How to Implement Retry Logic ## Prerequisites - Existing workflow ## Steps ### Step 1: Define retry policy Add retry configuration to your workflow step. > **Note:** Maximum 5 retry attempts allowed ### Step 2: Configure backoff Set exponential backoff with base delay of 1 second. The formula is: delay = base * 2^attempt. ### Step 3: Add timeout Set maximum execution time to prevent infinite loops. Default timeout is 30 seconds.",
              "score": null
            },
            {
              "doc_id": "config_workflow_engine",
              "chunk_id": "config_workflow_engine_fix_1",
              "content": "concurrent executions per workflow | | `EXECUTOR_POOL_SIZE` | integer | 50 | Number of executor workers | | `EXECUTOR_MEMORY_LIMIT` | string | 512Mi | Memory limit per executor | ## Configuration File Configuration can also be provided via `workflow engine.yaml`: ```yaml workflow_max_steps: 100 workflow_timeout_seconds: 3600 workflow_retry_limit: 3 workflow_concurrent_limit: 10 executor_pool_size: 50 ``` ## Validation Configuration is validated at startup. Invalid configuration will prevent the service from starting.",
              "score": null
            },
            {
              "doc_id": "arch_data_pipeline",
              "chunk_id": "arch_data_pipeline_fix_0",
              "content": "# Data Pipeline Architecture ## Overview The Data Pipeline processes and transforms data between workflow steps. It supports batch processing up to 10GB and streaming for real-time use cases. ## Design Principles - **Exactly-Once Processing**: Each record is processed exactly once - **Backpressure Handling**: System gracefully handles varying load ## Components ### Stream Processor Handles real-time data streams. Maximum throughput of 100000 messages per second.",
              "score": null
            }
          ],
          "latency_ms": 3.671020000183489,
          "query_id": "arch_033",
          "dimension": "negation",
          "query": "I don't understand how the workflow engine handles parallel execution and retries"
        },
        {
          "key_facts": [
            "Kong",
            "TLS termination",
            "50000 requests/second",
            "5ms"
          ],
          "found_facts": [
            "Kong",
            "TLS termination"
          ],
          "missed_facts": [
            "50000 requests/second",
            "5ms"
          ],
          "coverage": 0.5,
          "expected_docs": [
            "arch_api_gateway"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "arch_api_gateway",
              "chunk_id": "arch_api_gateway_fix_0",
              "content": "# API Gateway Architecture ## Overview The API Gateway handles all incoming HTTP requests, providing authentication, rate limiting, and request routing. Built on Kong with custom plugins for CloudFlow-specific functionality. ## Design Principles - **Zero Trust**: Every request is authenticated and authorized - **Defense in Depth**: Multiple layers of security validation ## Components ### Kong Gateway Primary ingress controller. Handles TLS termination, request routing, and plugin execution.",
              "score": null
            },
            {
              "doc_id": "config_api_gateway",
              "chunk_id": "config_api_gateway_fix_0",
              "content": "# API Gateway Configuration Reference ## Overview This document describes all configuration options for API Gateway. ## Environment Variables | Variable | Type | Default | Description | |----------|------|---------|-------------| | `GATEWAY_PORT` | integer | 8000 | Gateway listen port | | `GATEWAY_RATE_LIMIT` | integer | 100 | Requests per minute per user | | `GATEWAY_TIMEOUT_MS` | integer | 30000 | Request timeout in milliseconds | | `GATEWAY_MAX_BODY_SIZE` | string | 10MB | Maximum request body",
              "score": null
            },
            {
              "doc_id": "adr_007",
              "chunk_id": "adr_007_fix_0",
              "content": "# ADR-007: Implement Rate Limiting at API Gateway **Status:** Accepted **Date:** 2024-09-03 ## Context We need to protect our services from abuse and ensure fair resource usage across customers. ## Options Considered ### Option 1: Sliding window at gateway Rate limit at API gateway level.",
              "score": null
            },
            {
              "doc_id": "config_api_gateway",
              "chunk_id": "config_api_gateway_fix_1",
              "content": "size | | `GATEWAY_CORS_ORIGINS` | string | * | Allowed CORS origins | | `GATEWAY_SSL_ENABLED` | boolean | true | Enable TLS termination | ## Configuration File Configuration can also be provided via `api gateway.yaml`: ```yaml gateway_port: 8000 gateway_rate_limit: 100 gateway_timeout_ms: 30000 gateway_max_body_size: 10MB gateway_cors_origins: * ``` ## Validation Configuration is validated at startup. Invalid configuration will prevent the service from starting.",
              "score": null
            },
            {
              "doc_id": "api_integrations",
              "chunk_id": "api_integrations_fix_0",
              "content": "# Integrations API Reference ## Overview The Integrations API provides programmatic access to integrations functionality. Base URL: `https://api.cloudflow.io/v1/integrations` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /integrations List available integrations.",
              "score": null
            }
          ],
          "latency_ms": 4.296475999581162,
          "query_id": "arch_034",
          "dimension": "original",
          "query": "How does the API Gateway work?"
        },
        {
          "key_facts": [
            "Kong",
            "TLS termination",
            "50000 requests/second",
            "5ms"
          ],
          "found_facts": [
            "Kong",
            "TLS termination"
          ],
          "missed_facts": [
            "50000 requests/second",
            "5ms"
          ],
          "coverage": 0.5,
          "expected_docs": [
            "arch_api_gateway"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "arch_api_gateway",
              "chunk_id": "arch_api_gateway_fix_0",
              "content": "# API Gateway Architecture ## Overview The API Gateway handles all incoming HTTP requests, providing authentication, rate limiting, and request routing. Built on Kong with custom plugins for CloudFlow-specific functionality. ## Design Principles - **Zero Trust**: Every request is authenticated and authorized - **Defense in Depth**: Multiple layers of security validation ## Components ### Kong Gateway Primary ingress controller. Handles TLS termination, request routing, and plugin execution.",
              "score": null
            },
            {
              "doc_id": "config_api_gateway",
              "chunk_id": "config_api_gateway_fix_0",
              "content": "# API Gateway Configuration Reference ## Overview This document describes all configuration options for API Gateway. ## Environment Variables | Variable | Type | Default | Description | |----------|------|---------|-------------| | `GATEWAY_PORT` | integer | 8000 | Gateway listen port | | `GATEWAY_RATE_LIMIT` | integer | 100 | Requests per minute per user | | `GATEWAY_TIMEOUT_MS` | integer | 30000 | Request timeout in milliseconds | | `GATEWAY_MAX_BODY_SIZE` | string | 10MB | Maximum request body",
              "score": null
            },
            {
              "doc_id": "adr_007",
              "chunk_id": "adr_007_fix_1",
              "content": "**Pros:** - Centralized enforcement - Low latency - Easy configuration **Cons:** - Gateway becomes bottleneck - Coarse granularity - Redis dependency ### Option 2: Per-service limiting Each service implements its own limits. **Pros:** - Fine-grained control - Service autonomy - No single point of failure **Cons:** - Inconsistent implementation - Higher complexity - Distributed state ### Option 3: Token bucket Token-based rate limiting.",
              "score": null
            },
            {
              "doc_id": "adr_007",
              "chunk_id": "adr_007_fix_2",
              "content": "**Pros:** - Allows bursts - Smooth rate limiting - Well understood **Cons:** - More complex - Harder to explain to users - State management ## Decision We will implement sliding window rate limiting at the Kong API Gateway using Redis. ## Consequences - Default limit: 100 requests per minute - Implement per-plan limits - Return rate limit headers - Add retry-after on 429",
              "score": null
            },
            {
              "doc_id": "howto_configure_custom_domain",
              "chunk_id": "howto_configure_custom_domain_fix_1",
              "content": "> **Note:** CNAME should point to ingress.cloudflow.io ### Step 3: Enable SSL CloudFlow automatically provisions SSL certificates via Let's Encrypt. ```bash cloudflow domains ssl --domain app.example.com ``` ### Step 4: Set as primary Make this the primary domain for your application. ```bash cloudflow domains set-primary --domain app.example.com ``` ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            }
          ],
          "latency_ms": 4.0573599999333965,
          "query_id": "arch_034",
          "dimension": "synonym",
          "query": "What is the design of the ingress layer and request routing?"
        },
        {
          "key_facts": [
            "Kong",
            "TLS termination",
            "50000 requests/second",
            "5ms"
          ],
          "found_facts": [
            "Kong",
            "TLS termination"
          ],
          "missed_facts": [
            "50000 requests/second",
            "5ms"
          ],
          "coverage": 0.5,
          "expected_docs": [
            "arch_api_gateway"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "adr_007",
              "chunk_id": "adr_007_fix_0",
              "content": "# ADR-007: Implement Rate Limiting at API Gateway **Status:** Accepted **Date:** 2024-09-03 ## Context We need to protect our services from abuse and ensure fair resource usage across customers. ## Options Considered ### Option 1: Sliding window at gateway Rate limit at API gateway level.",
              "score": null
            },
            {
              "doc_id": "arch_api_gateway",
              "chunk_id": "arch_api_gateway_fix_0",
              "content": "# API Gateway Architecture ## Overview The API Gateway handles all incoming HTTP requests, providing authentication, rate limiting, and request routing. Built on Kong with custom plugins for CloudFlow-specific functionality. ## Design Principles - **Zero Trust**: Every request is authenticated and authorized - **Defense in Depth**: Multiple layers of security validation ## Components ### Kong Gateway Primary ingress controller. Handles TLS termination, request routing, and plugin execution.",
              "score": null
            },
            {
              "doc_id": "config_api_gateway",
              "chunk_id": "config_api_gateway_fix_0",
              "content": "# API Gateway Configuration Reference ## Overview This document describes all configuration options for API Gateway. ## Environment Variables | Variable | Type | Default | Description | |----------|------|---------|-------------| | `GATEWAY_PORT` | integer | 8000 | Gateway listen port | | `GATEWAY_RATE_LIMIT` | integer | 100 | Requests per minute per user | | `GATEWAY_TIMEOUT_MS` | integer | 30000 | Request timeout in milliseconds | | `GATEWAY_MAX_BODY_SIZE` | string | 10MB | Maximum request body",
              "score": null
            },
            {
              "doc_id": "howto_create_api_integration",
              "chunk_id": "howto_create_api_integration_fix_1",
              "content": "```bash curl -H 'Authorization: Bearer cf_...' https://api.cloudflow.io/v1/me ``` ### Step 4: Implement rate limiting Handle rate limit responses (HTTP 429) with exponential backoff. > **Note:** Default limit is 100 requests per minute ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            },
            {
              "doc_id": "arch_authentication_service",
              "chunk_id": "arch_authentication_service_fix_2",
              "content": "**Technology:** Dex OIDC proxy **Scaling:** Active-passive ## Data Flow Credentials \u2192 Validation \u2192 Token Issue \u2192 Token Refresh \u2192 Logout ## Performance Characteristics - **Latency P50:** 50ms - **Latency P99:** 200ms - **Throughput:** 5000 auth/second",
              "score": null
            }
          ],
          "latency_ms": 3.804849000516697,
          "query_id": "arch_034",
          "dimension": "problem",
          "query": "Getting 429 errors and want to understand how rate limiting and auth validation happen at the gateway"
        },
        {
          "key_facts": [
            "Kong",
            "TLS termination",
            "50000 requests/second",
            "5ms"
          ],
          "found_facts": [
            "Kong",
            "TLS termination"
          ],
          "missed_facts": [
            "50000 requests/second",
            "5ms"
          ],
          "coverage": 0.5,
          "expected_docs": [
            "arch_api_gateway"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "api_integrations",
              "chunk_id": "api_integrations_fix_0",
              "content": "# Integrations API Reference ## Overview The Integrations API provides programmatic access to integrations functionality. Base URL: `https://api.cloudflow.io/v1/integrations` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /integrations List available integrations.",
              "score": null
            },
            {
              "doc_id": "arch_api_gateway",
              "chunk_id": "arch_api_gateway_fix_0",
              "content": "# API Gateway Architecture ## Overview The API Gateway handles all incoming HTTP requests, providing authentication, rate limiting, and request routing. Built on Kong with custom plugins for CloudFlow-specific functionality. ## Design Principles - **Zero Trust**: Every request is authenticated and authorized - **Defense in Depth**: Multiple layers of security validation ## Components ### Kong Gateway Primary ingress controller. Handles TLS termination, request routing, and plugin execution.",
              "score": null
            },
            {
              "doc_id": "howto_migrate_from_v1_to_v2_api",
              "chunk_id": "howto_migrate_from_v1_to_v2_api_fix_1",
              "content": "> **Note:** Generate new credentials in the dashboard ### Step 4: Test in staging Run your integration tests against the v2 staging endpoint. ```bash CLOUDFLOW_API_URL=https://api-v2-staging.cloudflow.io npm test ``` ### Step 5: Switch production Update production configuration to use v2 API. ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            },
            {
              "doc_id": "howto_create_api_integration",
              "chunk_id": "howto_create_api_integration_fix_1",
              "content": "```bash curl -H 'Authorization: Bearer cf_...' https://api.cloudflow.io/v1/me ``` ### Step 4: Implement rate limiting Handle rate limit responses (HTTP 429) with exponential backoff. > **Note:** Default limit is 100 requests per minute ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            },
            {
              "doc_id": "api_webhooks",
              "chunk_id": "api_webhooks_fix_0",
              "content": "# Webhooks API Reference ## Overview The Webhooks API provides programmatic access to webhooks functionality. Base URL: `https://api.cloudflow.io/v1/webhooks` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /webhooks List configured webhooks.",
              "score": null
            }
          ],
          "latency_ms": 3.30804399891349,
          "query_id": "arch_034",
          "dimension": "casual",
          "query": "api gateway setup cloudflow"
        },
        {
          "key_facts": [
            "Kong",
            "TLS termination",
            "50000 requests/second",
            "5ms"
          ],
          "found_facts": [
            "Kong",
            "TLS termination"
          ],
          "missed_facts": [
            "50000 requests/second",
            "5ms"
          ],
          "coverage": 0.5,
          "expected_docs": [
            "arch_api_gateway"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "arch_api_gateway",
              "chunk_id": "arch_api_gateway_fix_0",
              "content": "# API Gateway Architecture ## Overview The API Gateway handles all incoming HTTP requests, providing authentication, rate limiting, and request routing. Built on Kong with custom plugins for CloudFlow-specific functionality. ## Design Principles - **Zero Trust**: Every request is authenticated and authorized - **Defense in Depth**: Multiple layers of security validation ## Components ### Kong Gateway Primary ingress controller. Handles TLS termination, request routing, and plugin execution.",
              "score": null
            },
            {
              "doc_id": "adr_007",
              "chunk_id": "adr_007_fix_0",
              "content": "# ADR-007: Implement Rate Limiting at API Gateway **Status:** Accepted **Date:** 2024-09-03 ## Context We need to protect our services from abuse and ensure fair resource usage across customers. ## Options Considered ### Option 1: Sliding window at gateway Rate limit at API gateway level.",
              "score": null
            },
            {
              "doc_id": "adr_007",
              "chunk_id": "adr_007_fix_2",
              "content": "**Pros:** - Allows bursts - Smooth rate limiting - Well understood **Cons:** - More complex - Harder to explain to users - State management ## Decision We will implement sliding window rate limiting at the Kong API Gateway using Redis. ## Consequences - Default limit: 100 requests per minute - Implement per-plan limits - Return rate limit headers - Add retry-after on 429",
              "score": null
            },
            {
              "doc_id": "howto_create_api_integration",
              "chunk_id": "howto_create_api_integration_fix_1",
              "content": "```bash curl -H 'Authorization: Bearer cf_...' https://api.cloudflow.io/v1/me ``` ### Step 4: Implement rate limiting Handle rate limit responses (HTTP 429) with exponential backoff. > **Note:** Default limit is 100 requests per minute ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            },
            {
              "doc_id": "adr_007",
              "chunk_id": "adr_007_fix_1",
              "content": "**Pros:** - Centralized enforcement - Low latency - Easy configuration **Cons:** - Gateway becomes bottleneck - Coarse granularity - Redis dependency ### Option 2: Per-service limiting Each service implements its own limits. **Pros:** - Fine-grained control - Service autonomy - No single point of failure **Cons:** - Inconsistent implementation - Higher complexity - Distributed state ### Option 3: Token bucket Token-based rate limiting.",
              "score": null
            }
          ],
          "latency_ms": 3.4460110000509303,
          "query_id": "arch_034",
          "dimension": "contextual",
          "query": "Doing a security review and need to understand how requests flow through authentication and rate limiting before hitting backend services"
        },
        {
          "key_facts": [
            "Kong",
            "TLS termination",
            "50000 requests/second",
            "5ms"
          ],
          "found_facts": [
            "Kong",
            "TLS termination"
          ],
          "missed_facts": [
            "50000 requests/second",
            "5ms"
          ],
          "coverage": 0.5,
          "expected_docs": [
            "arch_api_gateway"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "arch_api_gateway",
              "chunk_id": "arch_api_gateway_fix_0",
              "content": "# API Gateway Architecture ## Overview The API Gateway handles all incoming HTTP requests, providing authentication, rate limiting, and request routing. Built on Kong with custom plugins for CloudFlow-specific functionality. ## Design Principles - **Zero Trust**: Every request is authenticated and authorized - **Defense in Depth**: Multiple layers of security validation ## Components ### Kong Gateway Primary ingress controller. Handles TLS termination, request routing, and plugin execution.",
              "score": null
            },
            {
              "doc_id": "config_api_gateway",
              "chunk_id": "config_api_gateway_fix_0",
              "content": "# API Gateway Configuration Reference ## Overview This document describes all configuration options for API Gateway. ## Environment Variables | Variable | Type | Default | Description | |----------|------|---------|-------------| | `GATEWAY_PORT` | integer | 8000 | Gateway listen port | | `GATEWAY_RATE_LIMIT` | integer | 100 | Requests per minute per user | | `GATEWAY_TIMEOUT_MS` | integer | 30000 | Request timeout in milliseconds | | `GATEWAY_MAX_BODY_SIZE` | string | 10MB | Maximum request body",
              "score": null
            },
            {
              "doc_id": "config_api_gateway",
              "chunk_id": "config_api_gateway_fix_1",
              "content": "size | | `GATEWAY_CORS_ORIGINS` | string | * | Allowed CORS origins | | `GATEWAY_SSL_ENABLED` | boolean | true | Enable TLS termination | ## Configuration File Configuration can also be provided via `api gateway.yaml`: ```yaml gateway_port: 8000 gateway_rate_limit: 100 gateway_timeout_ms: 30000 gateway_max_body_size: 10MB gateway_cors_origins: * ``` ## Validation Configuration is validated at startup. Invalid configuration will prevent the service from starting.",
              "score": null
            },
            {
              "doc_id": "api_workflows",
              "chunk_id": "api_workflows_fix_0",
              "content": "# Workflows API Reference ## Overview The Workflows API provides programmatic access to workflows functionality. Base URL: `https://api.cloudflow.io/v1/workflows` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /workflows List all workflows for the authenticated user.",
              "score": null
            },
            {
              "doc_id": "runbook_ssl_certificate_expiry",
              "chunk_id": "runbook_ssl_certificate_expiry_fix_0",
              "content": "# Runbook: SSL Certificate Expiry ## Severity **Level:** P3 - Medium **Response Time:** 15 minutes ## Symptoms - Certificate expiring within 7 days - SSL handshake failures - Browser security warnings ## Diagnosis ### Step 1 Check certificate expiry: `echo | openssl s_client -connect api.cloudflow.io:443 2>/dev/null | openssl x509 -noout -dates` ### Step 2 Verify cert-manager status: `kubectl get certificates -n production` ### Step 3 Check cert-manager logs: `kubectl logs -l app=cert-manager -n cert-manager --tail=50`",
              "score": null
            }
          ],
          "latency_ms": 5.749283000113792,
          "query_id": "arch_034",
          "dimension": "negation",
          "query": "I don't get how TLS termination and token validation work in the API gateway"
        },
        {
          "key_facts": [
            "Kafka",
            "Flink",
            "Spark",
            "100000 messages per second",
            "Iceberg"
          ],
          "found_facts": [
            "Kafka",
            "Flink",
            "Spark",
            "100000 messages per second",
            "Iceberg"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "arch_data_pipeline"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "arch_data_pipeline",
              "chunk_id": "arch_data_pipeline_fix_0",
              "content": "# Data Pipeline Architecture ## Overview The Data Pipeline processes and transforms data between workflow steps. It supports batch processing up to 10GB and streaming for real-time use cases. ## Design Principles - **Exactly-Once Processing**: Each record is processed exactly once - **Backpressure Handling**: System gracefully handles varying load ## Components ### Stream Processor Handles real-time data streams. Maximum throughput of 100000 messages per second.",
              "score": null
            },
            {
              "doc_id": "arch_data_pipeline",
              "chunk_id": "arch_data_pipeline_fix_1",
              "content": "**Technology:** Apache Kafka with Flink **Scaling:** Partition-based scaling ### Batch Processor Processes large batch jobs. Supports data formats: JSON, CSV, Parquet, Avro. **Technology:** Apache Spark on Kubernetes **Scaling:** Dynamic resource allocation ### Data Lake Long-term data storage with time-travel queries. Retention configurable up to 7 years.",
              "score": null
            },
            {
              "doc_id": "arch_data_pipeline",
              "chunk_id": "arch_data_pipeline_fix_2",
              "content": "**Technology:** S3 with Iceberg tables **Scaling:** Unlimited with tiered storage ## Data Flow Input \u2192 Validation \u2192 Transform \u2192 Enrich \u2192 Output ## Performance Characteristics - **Latency P50:** 100ms (streaming), 5min (batch) - **Latency P99:** 500ms (streaming), 30min (batch) - **Throughput:** 100000 messages/second (streaming)",
              "score": null
            },
            {
              "doc_id": "arch_workflow_engine",
              "chunk_id": "arch_workflow_engine_fix_2",
              "content": "**Technology:** PostgreSQL with JSONB **Scaling:** Primary-replica with read replicas ## Data Flow Triggers \u2192 Scheduler \u2192 Job Queue \u2192 Executor \u2192 State Store \u2192 Webhooks/Notifications ## Performance Characteristics - **Latency P50:** 45ms - **Latency P99:** 250ms - **Throughput:** 5000 executions/minute",
              "score": null
            },
            {
              "doc_id": "arch_workflow_engine",
              "chunk_id": "arch_workflow_engine_fix_0",
              "content": "# Workflow Engine Architecture ## Overview The Workflow Engine is the core component responsible for executing user-defined workflows. It uses an event-driven architecture with support for parallel execution, conditional branching, and automatic retries. ## Design Principles - **Idempotency**: All operations can be safely retried without side effects - **Eventual Consistency**: State converges to consistent view within 5 seconds - **Fault Tolerance**: System continues operating despite component failures ## Components ### Scheduler Manages workflow scheduling and",
              "score": null
            }
          ],
          "latency_ms": 4.081484999915119,
          "query_id": "arch_035",
          "dimension": "original",
          "query": "Describe the Data Pipeline architecture"
        },
        {
          "key_facts": [
            "Kafka",
            "Flink",
            "Spark",
            "100000 messages per second",
            "Iceberg"
          ],
          "found_facts": [
            "Kafka",
            "Flink",
            "Spark",
            "100000 messages per second",
            "Iceberg"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "arch_data_pipeline"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "arch_data_pipeline",
              "chunk_id": "arch_data_pipeline_fix_0",
              "content": "# Data Pipeline Architecture ## Overview The Data Pipeline processes and transforms data between workflow steps. It supports batch processing up to 10GB and streaming for real-time use cases. ## Design Principles - **Exactly-Once Processing**: Each record is processed exactly once - **Backpressure Handling**: System gracefully handles varying load ## Components ### Stream Processor Handles real-time data streams. Maximum throughput of 100000 messages per second.",
              "score": null
            },
            {
              "doc_id": "arch_data_pipeline",
              "chunk_id": "arch_data_pipeline_fix_2",
              "content": "**Technology:** S3 with Iceberg tables **Scaling:** Unlimited with tiered storage ## Data Flow Input \u2192 Validation \u2192 Transform \u2192 Enrich \u2192 Output ## Performance Characteristics - **Latency P50:** 100ms (streaming), 5min (batch) - **Latency P99:** 500ms (streaming), 30min (batch) - **Throughput:** 100000 messages/second (streaming)",
              "score": null
            },
            {
              "doc_id": "arch_workflow_engine",
              "chunk_id": "arch_workflow_engine_fix_0",
              "content": "# Workflow Engine Architecture ## Overview The Workflow Engine is the core component responsible for executing user-defined workflows. It uses an event-driven architecture with support for parallel execution, conditional branching, and automatic retries. ## Design Principles - **Idempotency**: All operations can be safely retried without side effects - **Eventual Consistency**: State converges to consistent view within 5 seconds - **Fault Tolerance**: System continues operating despite component failures ## Components ### Scheduler Manages workflow scheduling and",
              "score": null
            },
            {
              "doc_id": "arch_data_pipeline",
              "chunk_id": "arch_data_pipeline_fix_1",
              "content": "**Technology:** Apache Kafka with Flink **Scaling:** Partition-based scaling ### Batch Processor Processes large batch jobs. Supports data formats: JSON, CSV, Parquet, Avro. **Technology:** Apache Spark on Kubernetes **Scaling:** Dynamic resource allocation ### Data Lake Long-term data storage with time-travel queries. Retention configurable up to 7 years.",
              "score": null
            },
            {
              "doc_id": "adr_005",
              "chunk_id": "adr_005_fix_1",
              "content": "**Pros:** - Simple mental model - Immediate feedback - Easy debugging **Cons:** - Tight coupling - Cascading failures - Timeout issues ### Option 3: Polling-based Workers poll for tasks. **Pros:** - Simple implementation - Easy rate limiting - No message broker **Cons:** - Polling overhead - Higher latency - Inefficient at scale ## Decision We will use an event-driven architecture with Kafka for workflow step execution.",
              "score": null
            }
          ],
          "latency_ms": 3.798326999458368,
          "query_id": "arch_035",
          "dimension": "synonym",
          "query": "How is the ETL and stream processing system designed?"
        },
        {
          "key_facts": [
            "Kafka",
            "Flink",
            "Spark",
            "100000 messages per second",
            "Iceberg"
          ],
          "found_facts": [
            "Kafka",
            "Flink",
            "Spark",
            "100000 messages per second",
            "Iceberg"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "arch_data_pipeline"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "arch_data_pipeline",
              "chunk_id": "arch_data_pipeline_fix_0",
              "content": "# Data Pipeline Architecture ## Overview The Data Pipeline processes and transforms data between workflow steps. It supports batch processing up to 10GB and streaming for real-time use cases. ## Design Principles - **Exactly-Once Processing**: Each record is processed exactly once - **Backpressure Handling**: System gracefully handles varying load ## Components ### Stream Processor Handles real-time data streams. Maximum throughput of 100000 messages per second.",
              "score": null
            },
            {
              "doc_id": "arch_data_pipeline",
              "chunk_id": "arch_data_pipeline_fix_2",
              "content": "**Technology:** S3 with Iceberg tables **Scaling:** Unlimited with tiered storage ## Data Flow Input \u2192 Validation \u2192 Transform \u2192 Enrich \u2192 Output ## Performance Characteristics - **Latency P50:** 100ms (streaming), 5min (batch) - **Latency P99:** 500ms (streaming), 30min (batch) - **Throughput:** 100000 messages/second (streaming)",
              "score": null
            },
            {
              "doc_id": "arch_data_pipeline",
              "chunk_id": "arch_data_pipeline_fix_1",
              "content": "**Technology:** Apache Kafka with Flink **Scaling:** Partition-based scaling ### Batch Processor Processes large batch jobs. Supports data formats: JSON, CSV, Parquet, Avro. **Technology:** Apache Spark on Kubernetes **Scaling:** Dynamic resource allocation ### Data Lake Long-term data storage with time-travel queries. Retention configurable up to 7 years.",
              "score": null
            },
            {
              "doc_id": "arch_workflow_engine",
              "chunk_id": "arch_workflow_engine_fix_2",
              "content": "**Technology:** PostgreSQL with JSONB **Scaling:** Primary-replica with read replicas ## Data Flow Triggers \u2192 Scheduler \u2192 Job Queue \u2192 Executor \u2192 State Store \u2192 Webhooks/Notifications ## Performance Characteristics - **Latency P50:** 45ms - **Latency P99:** 250ms - **Throughput:** 5000 executions/minute",
              "score": null
            },
            {
              "doc_id": "arch_caching_layer",
              "chunk_id": "arch_caching_layer_fix_2",
              "content": "**Technology:** CloudFront with Lambda@Edge **Scaling:** Global edge locations ## Data Flow Request \u2192 Local Cache \u2192 Redis \u2192 Database \u2192 Cache Population ## Performance Characteristics - **Latency P50:** 1ms (hit), 50ms (miss) - **Latency P99:** 5ms (hit), 200ms (miss) - **Throughput:** 100000 reads/second",
              "score": null
            }
          ],
          "latency_ms": 3.760286999749951,
          "query_id": "arch_035",
          "dimension": "problem",
          "query": "Seeing data processing delays and need to understand the batch vs streaming components and their throughput limits"
        },
        {
          "key_facts": [
            "Kafka",
            "Flink",
            "Spark",
            "100000 messages per second",
            "Iceberg"
          ],
          "found_facts": [
            "Kafka",
            "Flink",
            "Spark",
            "100000 messages per second",
            "Iceberg"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "arch_data_pipeline"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "arch_data_pipeline",
              "chunk_id": "arch_data_pipeline_fix_0",
              "content": "# Data Pipeline Architecture ## Overview The Data Pipeline processes and transforms data between workflow steps. It supports batch processing up to 10GB and streaming for real-time use cases. ## Design Principles - **Exactly-Once Processing**: Each record is processed exactly once - **Backpressure Handling**: System gracefully handles varying load ## Components ### Stream Processor Handles real-time data streams. Maximum throughput of 100000 messages per second.",
              "score": null
            },
            {
              "doc_id": "arch_data_pipeline",
              "chunk_id": "arch_data_pipeline_fix_1",
              "content": "**Technology:** Apache Kafka with Flink **Scaling:** Partition-based scaling ### Batch Processor Processes large batch jobs. Supports data formats: JSON, CSV, Parquet, Avro. **Technology:** Apache Spark on Kubernetes **Scaling:** Dynamic resource allocation ### Data Lake Long-term data storage with time-travel queries. Retention configurable up to 7 years.",
              "score": null
            },
            {
              "doc_id": "arch_data_pipeline",
              "chunk_id": "arch_data_pipeline_fix_2",
              "content": "**Technology:** S3 with Iceberg tables **Scaling:** Unlimited with tiered storage ## Data Flow Input \u2192 Validation \u2192 Transform \u2192 Enrich \u2192 Output ## Performance Characteristics - **Latency P50:** 100ms (streaming), 5min (batch) - **Latency P99:** 500ms (streaming), 30min (batch) - **Throughput:** 100000 messages/second (streaming)",
              "score": null
            },
            {
              "doc_id": "adr_001",
              "chunk_id": "adr_001_fix_0",
              "content": "# ADR-001: Use PostgreSQL for Primary Database **Status:** Accepted **Date:** 2024-11-04 ## Context We need a database to store workflow definitions, execution state, and user data. The database must support ACID transactions, JSON queries, and scale to millions of records. ## Options Considered ### Option 1: PostgreSQL Mature relational database with JSONB support.",
              "score": null
            },
            {
              "doc_id": "adr_001",
              "chunk_id": "adr_001_fix_2",
              "content": "**Pros:** - Automatic sharding - Strong consistency - PostgreSQL compatible **Cons:** - Higher latency - More expensive - Less mature ecosystem ## Decision We will use PostgreSQL as our primary database with JSONB for flexible workflow definitions. ## Consequences - Need PgBouncer for connection pooling - Must plan for sharding at 100M+ records - Requires regular VACUUM maintenance",
              "score": null
            }
          ],
          "latency_ms": 3.514748999805306,
          "query_id": "arch_035",
          "dimension": "casual",
          "query": "data pipeline architecture"
        },
        {
          "key_facts": [
            "Kafka",
            "Flink",
            "Spark",
            "100000 messages per second",
            "Iceberg"
          ],
          "found_facts": [
            "Kafka",
            "Flink",
            "Spark",
            "100000 messages per second",
            "Iceberg"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "arch_data_pipeline"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "arch_data_pipeline",
              "chunk_id": "arch_data_pipeline_fix_0",
              "content": "# Data Pipeline Architecture ## Overview The Data Pipeline processes and transforms data between workflow steps. It supports batch processing up to 10GB and streaming for real-time use cases. ## Design Principles - **Exactly-Once Processing**: Each record is processed exactly once - **Backpressure Handling**: System gracefully handles varying load ## Components ### Stream Processor Handles real-time data streams. Maximum throughput of 100000 messages per second.",
              "score": null
            },
            {
              "doc_id": "arch_data_pipeline",
              "chunk_id": "arch_data_pipeline_fix_2",
              "content": "**Technology:** S3 with Iceberg tables **Scaling:** Unlimited with tiered storage ## Data Flow Input \u2192 Validation \u2192 Transform \u2192 Enrich \u2192 Output ## Performance Characteristics - **Latency P50:** 100ms (streaming), 5min (batch) - **Latency P99:** 500ms (streaming), 30min (batch) - **Throughput:** 100000 messages/second (streaming)",
              "score": null
            },
            {
              "doc_id": "arch_data_pipeline",
              "chunk_id": "arch_data_pipeline_fix_1",
              "content": "**Technology:** Apache Kafka with Flink **Scaling:** Partition-based scaling ### Batch Processor Processes large batch jobs. Supports data formats: JSON, CSV, Parquet, Avro. **Technology:** Apache Spark on Kubernetes **Scaling:** Dynamic resource allocation ### Data Lake Long-term data storage with time-travel queries. Retention configurable up to 7 years.",
              "score": null
            },
            {
              "doc_id": "arch_workflow_engine",
              "chunk_id": "arch_workflow_engine_fix_2",
              "content": "**Technology:** PostgreSQL with JSONB **Scaling:** Primary-replica with read replicas ## Data Flow Triggers \u2192 Scheduler \u2192 Job Queue \u2192 Executor \u2192 State Store \u2192 Webhooks/Notifications ## Performance Characteristics - **Latency P50:** 45ms - **Latency P99:** 250ms - **Throughput:** 5000 executions/minute",
              "score": null
            },
            {
              "doc_id": "runbook_high_cpu_usage",
              "chunk_id": "runbook_high_cpu_usage_fix_1",
              "content": "Step 4 Profile the application: `cloudflow debug profile --pod api-xyz --duration 60s` ## Resolution ### Step 1 Scale up if traffic-related: `kubectl scale deployment/api --replicas=10 -n production` ### Step 2 Restart pods if memory leak suspected: `kubectl rollout restart deployment/api -n production` ### Step 3 Roll back recent deployment if regression: `kubectl rollout undo deployment/api -n production` ### Step 4 Add resource limits if unbounded: update deployment with CPU limits ## Escalation If CPU remains",
              "score": null
            }
          ],
          "latency_ms": 3.755527999601327,
          "query_id": "arch_035",
          "dimension": "contextual",
          "query": "Planning a high-volume data integration and need to understand how CloudFlow handles streaming and batch processing at scale"
        },
        {
          "key_facts": [
            "Kafka",
            "Flink",
            "Spark",
            "100000 messages per second",
            "Iceberg"
          ],
          "found_facts": [
            "Kafka",
            "Flink",
            "Spark",
            "100000 messages per second",
            "Iceberg"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "arch_data_pipeline"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "arch_data_pipeline",
              "chunk_id": "arch_data_pipeline_fix_0",
              "content": "# Data Pipeline Architecture ## Overview The Data Pipeline processes and transforms data between workflow steps. It supports batch processing up to 10GB and streaming for real-time use cases. ## Design Principles - **Exactly-Once Processing**: Each record is processed exactly once - **Backpressure Handling**: System gracefully handles varying load ## Components ### Stream Processor Handles real-time data streams. Maximum throughput of 100000 messages per second.",
              "score": null
            },
            {
              "doc_id": "arch_data_pipeline",
              "chunk_id": "arch_data_pipeline_fix_1",
              "content": "**Technology:** Apache Kafka with Flink **Scaling:** Partition-based scaling ### Batch Processor Processes large batch jobs. Supports data formats: JSON, CSV, Parquet, Avro. **Technology:** Apache Spark on Kubernetes **Scaling:** Dynamic resource allocation ### Data Lake Long-term data storage with time-travel queries. Retention configurable up to 7 years.",
              "score": null
            },
            {
              "doc_id": "arch_data_pipeline",
              "chunk_id": "arch_data_pipeline_fix_2",
              "content": "**Technology:** S3 with Iceberg tables **Scaling:** Unlimited with tiered storage ## Data Flow Input \u2192 Validation \u2192 Transform \u2192 Enrich \u2192 Output ## Performance Characteristics - **Latency P50:** 100ms (streaming), 5min (batch) - **Latency P99:** 500ms (streaming), 30min (batch) - **Throughput:** 100000 messages/second (streaming)",
              "score": null
            },
            {
              "doc_id": "arch_workflow_engine",
              "chunk_id": "arch_workflow_engine_fix_2",
              "content": "**Technology:** PostgreSQL with JSONB **Scaling:** Primary-replica with read replicas ## Data Flow Triggers \u2192 Scheduler \u2192 Job Queue \u2192 Executor \u2192 State Store \u2192 Webhooks/Notifications ## Performance Characteristics - **Latency P50:** 45ms - **Latency P99:** 250ms - **Throughput:** 5000 executions/minute",
              "score": null
            },
            {
              "doc_id": "arch_caching_layer",
              "chunk_id": "arch_caching_layer_fix_2",
              "content": "**Technology:** CloudFront with Lambda@Edge **Scaling:** Global edge locations ## Data Flow Request \u2192 Local Cache \u2192 Redis \u2192 Database \u2192 Cache Population ## Performance Characteristics - **Latency P50:** 1ms (hit), 50ms (miss) - **Latency P99:** 5ms (hit), 200ms (miss) - **Throughput:** 100000 reads/second",
              "score": null
            }
          ],
          "latency_ms": 3.51097199927608,
          "query_id": "arch_035",
          "dimension": "negation",
          "query": "I'm confused about how data moves between stream processor, batch processor, and data lake"
        },
        {
          "key_facts": [
            "Elasticsearch",
            "500ms",
            "autocomplete",
            "20ms"
          ],
          "found_facts": [
            "Elasticsearch",
            "500ms",
            "autocomplete",
            "20ms"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "arch_search_service"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "arch_search_service",
              "chunk_id": "arch_search_service_fix_0",
              "content": "# Search Service Architecture ## Overview The Search Service provides full-text and semantic search across workflows, executions, and documentation. Built on Elasticsearch with custom analyzers for technical content. ## Design Principles - **Relevance First**: Results ranked by relevance, not recency - **Typo Tolerance**: Handles misspellings and variations ## Components ### Indexer Indexes documents in near real-time. Typical indexing latency is 500ms. **Technology:** Custom Go service **Scaling:** Queue-based with backpressure ### Query Engine Executes search queries.",
              "score": null
            },
            {
              "doc_id": "arch_search_service",
              "chunk_id": "arch_search_service_fix_1",
              "content": "Supports filters, facets, and aggregations. **Technology:** Elasticsearch 8.x **Scaling:** 3-node cluster with replicas ### Suggestion Service Provides autocomplete suggestions. Returns suggestions within 20ms. **Technology:** Completion suggester **Scaling:** In-memory FST ## Data Flow Document \u2192 Analyzer \u2192 Index \u2192 Query \u2192 Rank \u2192 Results ## Performance Characteristics - **Latency P50:** 25ms - **Latency P99:** 100ms - **Throughput:** 2000 queries/second",
              "score": null
            },
            {
              "doc_id": "arch_monitoring_stack",
              "chunk_id": "arch_monitoring_stack_fix_1",
              "content": "Supports structured JSON logs with label indexing. **Technology:** Loki with Grafana **Scaling:** Distributed with sharding ### Tracing Distributed request tracing. Trace retention is 7 days. **Technology:** Jaeger with OpenTelemetry **Scaling:** Sampling at 10% ### Alerting Routes alerts to on-call engineers. Supports escalation policies and silencing.",
              "score": null
            },
            {
              "doc_id": "arch_authentication_service",
              "chunk_id": "arch_authentication_service_fix_0",
              "content": "# Authentication Service Architecture ## Overview The Authentication Service handles user identity, session management, and token issuance. Supports OAuth 2.0, SAML, and API key authentication. ## Design Principles - **Zero Knowledge**: Passwords never stored in plaintext - **Token Rotation**: Refresh tokens rotate on each use ## Components ### Identity Provider Manages user accounts and credentials. Password hashing uses bcrypt with cost factor 12.",
              "score": null
            },
            {
              "doc_id": "arch_data_pipeline",
              "chunk_id": "arch_data_pipeline_fix_1",
              "content": "**Technology:** Apache Kafka with Flink **Scaling:** Partition-based scaling ### Batch Processor Processes large batch jobs. Supports data formats: JSON, CSV, Parquet, Avro. **Technology:** Apache Spark on Kubernetes **Scaling:** Dynamic resource allocation ### Data Lake Long-term data storage with time-travel queries. Retention configurable up to 7 years.",
              "score": null
            }
          ],
          "latency_ms": 3.618452999944566,
          "query_id": "arch_036",
          "dimension": "original",
          "query": "How does the Search Service work?"
        },
        {
          "key_facts": [
            "Elasticsearch",
            "500ms",
            "autocomplete",
            "20ms"
          ],
          "found_facts": [
            "Elasticsearch",
            "500ms",
            "autocomplete",
            "20ms"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "arch_search_service"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "arch_search_service",
              "chunk_id": "arch_search_service_fix_0",
              "content": "# Search Service Architecture ## Overview The Search Service provides full-text and semantic search across workflows, executions, and documentation. Built on Elasticsearch with custom analyzers for technical content. ## Design Principles - **Relevance First**: Results ranked by relevance, not recency - **Typo Tolerance**: Handles misspellings and variations ## Components ### Indexer Indexes documents in near real-time. Typical indexing latency is 500ms. **Technology:** Custom Go service **Scaling:** Queue-based with backpressure ### Query Engine Executes search queries.",
              "score": null
            },
            {
              "doc_id": "arch_search_service",
              "chunk_id": "arch_search_service_fix_1",
              "content": "Supports filters, facets, and aggregations. **Technology:** Elasticsearch 8.x **Scaling:** 3-node cluster with replicas ### Suggestion Service Provides autocomplete suggestions. Returns suggestions within 20ms. **Technology:** Completion suggester **Scaling:** In-memory FST ## Data Flow Document \u2192 Analyzer \u2192 Index \u2192 Query \u2192 Rank \u2192 Results ## Performance Characteristics - **Latency P50:** 25ms - **Latency P99:** 100ms - **Throughput:** 2000 queries/second",
              "score": null
            },
            {
              "doc_id": "arch_monitoring_stack",
              "chunk_id": "arch_monitoring_stack_fix_1",
              "content": "Supports structured JSON logs with label indexing. **Technology:** Loki with Grafana **Scaling:** Distributed with sharding ### Tracing Distributed request tracing. Trace retention is 7 days. **Technology:** Jaeger with OpenTelemetry **Scaling:** Sampling at 10% ### Alerting Routes alerts to on-call engineers. Supports escalation policies and silencing.",
              "score": null
            },
            {
              "doc_id": "arch_data_pipeline",
              "chunk_id": "arch_data_pipeline_fix_1",
              "content": "**Technology:** Apache Kafka with Flink **Scaling:** Partition-based scaling ### Batch Processor Processes large batch jobs. Supports data formats: JSON, CSV, Parquet, Avro. **Technology:** Apache Spark on Kubernetes **Scaling:** Dynamic resource allocation ### Data Lake Long-term data storage with time-travel queries. Retention configurable up to 7 years.",
              "score": null
            },
            {
              "doc_id": "arch_caching_layer",
              "chunk_id": "arch_caching_layer_fix_0",
              "content": "# Caching Layer Architecture ## Overview The Caching Layer reduces latency and database load by caching frequently accessed data. Uses a multi-tier strategy with local and distributed caches. ## Design Principles - **Cache Invalidation**: Explicit invalidation on writes - **Graceful Degradation**: System works without cache ## Components ### Local Cache First-level cache with 100ms TTL. Reduces Redis round-trips for hot data.",
              "score": null
            }
          ],
          "latency_ms": 4.234330001054332,
          "query_id": "arch_036",
          "dimension": "synonym",
          "query": "What is the full-text search system design and how does indexing work?"
        },
        {
          "key_facts": [
            "Elasticsearch",
            "500ms",
            "autocomplete",
            "20ms"
          ],
          "found_facts": [
            "Elasticsearch",
            "500ms",
            "autocomplete",
            "20ms"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "arch_search_service"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "arch_search_service",
              "chunk_id": "arch_search_service_fix_0",
              "content": "# Search Service Architecture ## Overview The Search Service provides full-text and semantic search across workflows, executions, and documentation. Built on Elasticsearch with custom analyzers for technical content. ## Design Principles - **Relevance First**: Results ranked by relevance, not recency - **Typo Tolerance**: Handles misspellings and variations ## Components ### Indexer Indexes documents in near real-time. Typical indexing latency is 500ms. **Technology:** Custom Go service **Scaling:** Queue-based with backpressure ### Query Engine Executes search queries.",
              "score": null
            },
            {
              "doc_id": "arch_search_service",
              "chunk_id": "arch_search_service_fix_1",
              "content": "Supports filters, facets, and aggregations. **Technology:** Elasticsearch 8.x **Scaling:** 3-node cluster with replicas ### Suggestion Service Provides autocomplete suggestions. Returns suggestions within 20ms. **Technology:** Completion suggester **Scaling:** In-memory FST ## Data Flow Document \u2192 Analyzer \u2192 Index \u2192 Query \u2192 Rank \u2192 Results ## Performance Characteristics - **Latency P50:** 25ms - **Latency P99:** 100ms - **Throughput:** 2000 queries/second",
              "score": null
            },
            {
              "doc_id": "arch_caching_layer",
              "chunk_id": "arch_caching_layer_fix_2",
              "content": "**Technology:** CloudFront with Lambda@Edge **Scaling:** Global edge locations ## Data Flow Request \u2192 Local Cache \u2192 Redis \u2192 Database \u2192 Cache Population ## Performance Characteristics - **Latency P50:** 1ms (hit), 50ms (miss) - **Latency P99:** 5ms (hit), 200ms (miss) - **Throughput:** 100000 reads/second",
              "score": null
            },
            {
              "doc_id": "runbook_database_connection_exhaustion",
              "chunk_id": "runbook_database_connection_exhaustion_fix_0",
              "content": "# Runbook: Database Connection Exhaustion ## Severity **Level:** P1 - Critical **Response Time:** 15 minutes ## Symptoms - Connection pool at 100% - Queries timing out - 'too many connections' errors in logs ## Diagnosis ### Step 1 Check active connections: `SELECT count(*) FROM pg_stat_activity` ### Step 2 Identify long-running queries: `SELECT pid, now() - pg_stat_activity.query_start AS duration, query FROM pg_stat_activity WHERE state != 'idle' ORDER BY duration DESC` ### Step 3 Check PgBouncer stats:",
              "score": null
            },
            {
              "doc_id": "arch_data_pipeline",
              "chunk_id": "arch_data_pipeline_fix_2",
              "content": "**Technology:** S3 with Iceberg tables **Scaling:** Unlimited with tiered storage ## Data Flow Input \u2192 Validation \u2192 Transform \u2192 Enrich \u2192 Output ## Performance Characteristics - **Latency P50:** 100ms (streaming), 5min (batch) - **Latency P99:** 500ms (streaming), 30min (batch) - **Throughput:** 100000 messages/second (streaming)",
              "score": null
            }
          ],
          "latency_ms": 4.315872000006493,
          "query_id": "arch_036",
          "dimension": "problem",
          "query": "Search results are taking too long - need to understand the query engine and indexing latency"
        },
        {
          "key_facts": [
            "Elasticsearch",
            "500ms",
            "autocomplete",
            "20ms"
          ],
          "found_facts": [
            "Elasticsearch",
            "500ms",
            "autocomplete",
            "20ms"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "arch_search_service"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "arch_search_service",
              "chunk_id": "arch_search_service_fix_0",
              "content": "# Search Service Architecture ## Overview The Search Service provides full-text and semantic search across workflows, executions, and documentation. Built on Elasticsearch with custom analyzers for technical content. ## Design Principles - **Relevance First**: Results ranked by relevance, not recency - **Typo Tolerance**: Handles misspellings and variations ## Components ### Indexer Indexes documents in near real-time. Typical indexing latency is 500ms. **Technology:** Custom Go service **Scaling:** Queue-based with backpressure ### Query Engine Executes search queries.",
              "score": null
            },
            {
              "doc_id": "arch_search_service",
              "chunk_id": "arch_search_service_fix_1",
              "content": "Supports filters, facets, and aggregations. **Technology:** Elasticsearch 8.x **Scaling:** 3-node cluster with replicas ### Suggestion Service Provides autocomplete suggestions. Returns suggestions within 20ms. **Technology:** Completion suggester **Scaling:** In-memory FST ## Data Flow Document \u2192 Analyzer \u2192 Index \u2192 Query \u2192 Rank \u2192 Results ## Performance Characteristics - **Latency P50:** 25ms - **Latency P99:** 100ms - **Throughput:** 2000 queries/second",
              "score": null
            },
            {
              "doc_id": "arch_monitoring_stack",
              "chunk_id": "arch_monitoring_stack_fix_1",
              "content": "Supports structured JSON logs with label indexing. **Technology:** Loki with Grafana **Scaling:** Distributed with sharding ### Tracing Distributed request tracing. Trace retention is 7 days. **Technology:** Jaeger with OpenTelemetry **Scaling:** Sampling at 10% ### Alerting Routes alerts to on-call engineers. Supports escalation policies and silencing.",
              "score": null
            },
            {
              "doc_id": "arch_data_pipeline",
              "chunk_id": "arch_data_pipeline_fix_1",
              "content": "**Technology:** Apache Kafka with Flink **Scaling:** Partition-based scaling ### Batch Processor Processes large batch jobs. Supports data formats: JSON, CSV, Parquet, Avro. **Technology:** Apache Spark on Kubernetes **Scaling:** Dynamic resource allocation ### Data Lake Long-term data storage with time-travel queries. Retention configurable up to 7 years.",
              "score": null
            },
            {
              "doc_id": "adr_004",
              "chunk_id": "adr_004_fix_1",
              "content": "**Pros:** - Simple operation - Multi-threaded - Lower memory overhead **Cons:** - No persistence - Limited data types - No clustering ### Option 3: Application-level cache In-process caching only. **Pros:** - No external dependency - Fastest possible - Simple **Cons:** - Not shared across pods - Memory pressure - Cold starts ## Decision We will use Redis Cluster for caching with a 6-node cluster for high availability.",
              "score": null
            }
          ],
          "latency_ms": 4.146725999817136,
          "query_id": "arch_036",
          "dimension": "casual",
          "query": "search service architecture"
        },
        {
          "key_facts": [
            "Elasticsearch",
            "500ms",
            "autocomplete",
            "20ms"
          ],
          "found_facts": [
            "Elasticsearch",
            "500ms",
            "autocomplete",
            "20ms"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "arch_search_service"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "arch_search_service",
              "chunk_id": "arch_search_service_fix_1",
              "content": "Supports filters, facets, and aggregations. **Technology:** Elasticsearch 8.x **Scaling:** 3-node cluster with replicas ### Suggestion Service Provides autocomplete suggestions. Returns suggestions within 20ms. **Technology:** Completion suggester **Scaling:** In-memory FST ## Data Flow Document \u2192 Analyzer \u2192 Index \u2192 Query \u2192 Rank \u2192 Results ## Performance Characteristics - **Latency P50:** 25ms - **Latency P99:** 100ms - **Throughput:** 2000 queries/second",
              "score": null
            },
            {
              "doc_id": "api_integrations",
              "chunk_id": "api_integrations_fix_0",
              "content": "# Integrations API Reference ## Overview The Integrations API provides programmatic access to integrations functionality. Base URL: `https://api.cloudflow.io/v1/integrations` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /integrations List available integrations.",
              "score": null
            },
            {
              "doc_id": "howto_create_api_integration",
              "chunk_id": "howto_create_api_integration_fix_1",
              "content": "```bash curl -H 'Authorization: Bearer cf_...' https://api.cloudflow.io/v1/me ``` ### Step 4: Implement rate limiting Handle rate limit responses (HTTP 429) with exponential backoff. > **Note:** Default limit is 100 requests per minute ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            },
            {
              "doc_id": "arch_search_service",
              "chunk_id": "arch_search_service_fix_0",
              "content": "# Search Service Architecture ## Overview The Search Service provides full-text and semantic search across workflows, executions, and documentation. Built on Elasticsearch with custom analyzers for technical content. ## Design Principles - **Relevance First**: Results ranked by relevance, not recency - **Typo Tolerance**: Handles misspellings and variations ## Components ### Indexer Indexes documents in near real-time. Typical indexing latency is 500ms. **Technology:** Custom Go service **Scaling:** Queue-based with backpressure ### Query Engine Executes search queries.",
              "score": null
            },
            {
              "doc_id": "howto_implement_retry_logic",
              "chunk_id": "howto_implement_retry_logic_fix_1",
              "content": "### Step 4: Handle final failure Configure on_failure handler to notify or escalate when all retries are exhausted. ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            }
          ],
          "latency_ms": 3.896029000316048,
          "query_id": "arch_036",
          "dimension": "contextual",
          "query": "Building an autocomplete feature and need to understand how CloudFlow's suggestion service works and its response time"
        },
        {
          "key_facts": [
            "Elasticsearch",
            "500ms",
            "autocomplete",
            "20ms"
          ],
          "found_facts": [
            "Elasticsearch",
            "500ms",
            "autocomplete",
            "20ms"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "arch_search_service"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "arch_search_service",
              "chunk_id": "arch_search_service_fix_1",
              "content": "Supports filters, facets, and aggregations. **Technology:** Elasticsearch 8.x **Scaling:** 3-node cluster with replicas ### Suggestion Service Provides autocomplete suggestions. Returns suggestions within 20ms. **Technology:** Completion suggester **Scaling:** In-memory FST ## Data Flow Document \u2192 Analyzer \u2192 Index \u2192 Query \u2192 Rank \u2192 Results ## Performance Characteristics - **Latency P50:** 25ms - **Latency P99:** 100ms - **Throughput:** 2000 queries/second",
              "score": null
            },
            {
              "doc_id": "arch_search_service",
              "chunk_id": "arch_search_service_fix_0",
              "content": "# Search Service Architecture ## Overview The Search Service provides full-text and semantic search across workflows, executions, and documentation. Built on Elasticsearch with custom analyzers for technical content. ## Design Principles - **Relevance First**: Results ranked by relevance, not recency - **Typo Tolerance**: Handles misspellings and variations ## Components ### Indexer Indexes documents in near real-time. Typical indexing latency is 500ms. **Technology:** Custom Go service **Scaling:** Queue-based with backpressure ### Query Engine Executes search queries.",
              "score": null
            },
            {
              "doc_id": "arch_monitoring_stack",
              "chunk_id": "arch_monitoring_stack_fix_1",
              "content": "Supports structured JSON logs with label indexing. **Technology:** Loki with Grafana **Scaling:** Distributed with sharding ### Tracing Distributed request tracing. Trace retention is 7 days. **Technology:** Jaeger with OpenTelemetry **Scaling:** Sampling at 10% ### Alerting Routes alerts to on-call engineers. Supports escalation policies and silencing.",
              "score": null
            },
            {
              "doc_id": "adr_008",
              "chunk_id": "adr_008_fix_2",
              "content": "## Consequences - Enable versioning for critical buckets - Implement lifecycle policies - Use pre-signed URLs for uploads - Enable server-side encryption",
              "score": null
            },
            {
              "doc_id": "runbook_database_connection_exhaustion",
              "chunk_id": "runbook_database_connection_exhaustion_fix_2",
              "content": "## Post-Incident - Document timeline in incident log - Schedule post-mortem within 48 hours - Update runbook if new patterns discovered",
              "score": null
            }
          ],
          "latency_ms": 3.6521450001600897,
          "query_id": "arch_036",
          "dimension": "negation",
          "query": "I don't understand how documents get indexed and how autocomplete suggestions are generated"
        },
        {
          "key_facts": [
            "bcrypt",
            "cost factor 12",
            "RS256",
            "refresh tokens"
          ],
          "found_facts": [
            "bcrypt",
            "cost factor 12",
            "RS256",
            "refresh tokens"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "arch_authentication_service"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "arch_authentication_service",
              "chunk_id": "arch_authentication_service_fix_0",
              "content": "# Authentication Service Architecture ## Overview The Authentication Service handles user identity, session management, and token issuance. Supports OAuth 2.0, SAML, and API key authentication. ## Design Principles - **Zero Knowledge**: Passwords never stored in plaintext - **Token Rotation**: Refresh tokens rotate on each use ## Components ### Identity Provider Manages user accounts and credentials. Password hashing uses bcrypt with cost factor 12.",
              "score": null
            },
            {
              "doc_id": "arch_authentication_service",
              "chunk_id": "arch_authentication_service_fix_1",
              "content": "**Technology:** Custom Go service **Scaling:** Horizontal stateless ### Token Service Issues and validates access tokens. Token lifetime is 1 hour with 7-day refresh tokens. **Technology:** JWT with RS256 **Scaling:** Horizontal stateless ### SSO Gateway Integrates with external identity providers. Supports Google, Microsoft, Okta, and custom SAML.",
              "score": null
            },
            {
              "doc_id": "adr_003",
              "chunk_id": "adr_003_fix_0",
              "content": "# ADR-003: Implement OAuth 2.0 with JWT for Authentication **Status:** Accepted **Date:** 2024-05-08 ## Context We need a secure authentication system that supports API keys, user sessions, and third-party integrations. ## Options Considered ### Option 1: OAuth 2.0 with JWT Industry standard token-based auth.",
              "score": null
            },
            {
              "doc_id": "adr_003",
              "chunk_id": "adr_003_fix_2",
              "content": "**Pros:** - Very simple - Low overhead - Easy to implement **Cons:** - No user context - Hard to rotate - No SSO support ## Decision We will implement OAuth 2.0 with JWT tokens. Access tokens expire after 1 hour, refresh tokens after 7 days. ## Consequences - Implement token rotation on refresh - Support API keys for machine-to-machine auth - Add MFA for admin accounts",
              "score": null
            },
            {
              "doc_id": "adr_003",
              "chunk_id": "adr_003_fix_1",
              "content": "**Pros:** - Industry standard - Stateless validation - Supports refresh tokens - SSO compatible **Cons:** - Token size overhead - Cannot revoke individual tokens - Complexity ### Option 2: Session-based auth Traditional server-side sessions. **Pros:** - Simple implementation - Easy revocation - Smaller payloads **Cons:** - Requires session store - Not suitable for APIs - Scaling challenges ### Option 3: API keys only Simple key-based authentication.",
              "score": null
            }
          ],
          "latency_ms": 4.0736009996180655,
          "query_id": "arch_037",
          "dimension": "original",
          "query": "Explain the Authentication Service"
        },
        {
          "key_facts": [
            "bcrypt",
            "cost factor 12",
            "RS256",
            "refresh tokens"
          ],
          "found_facts": [
            "bcrypt",
            "cost factor 12",
            "RS256",
            "refresh tokens"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "arch_authentication_service"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "arch_authentication_service",
              "chunk_id": "arch_authentication_service_fix_0",
              "content": "# Authentication Service Architecture ## Overview The Authentication Service handles user identity, session management, and token issuance. Supports OAuth 2.0, SAML, and API key authentication. ## Design Principles - **Zero Knowledge**: Passwords never stored in plaintext - **Token Rotation**: Refresh tokens rotate on each use ## Components ### Identity Provider Manages user accounts and credentials. Password hashing uses bcrypt with cost factor 12.",
              "score": null
            },
            {
              "doc_id": "adr_003",
              "chunk_id": "adr_003_fix_2",
              "content": "**Pros:** - Very simple - Low overhead - Easy to implement **Cons:** - No user context - Hard to rotate - No SSO support ## Decision We will implement OAuth 2.0 with JWT tokens. Access tokens expire after 1 hour, refresh tokens after 7 days. ## Consequences - Implement token rotation on refresh - Support API keys for machine-to-machine auth - Add MFA for admin accounts",
              "score": null
            },
            {
              "doc_id": "arch_authentication_service",
              "chunk_id": "arch_authentication_service_fix_1",
              "content": "**Technology:** Custom Go service **Scaling:** Horizontal stateless ### Token Service Issues and validates access tokens. Token lifetime is 1 hour with 7-day refresh tokens. **Technology:** JWT with RS256 **Scaling:** Horizontal stateless ### SSO Gateway Integrates with external identity providers. Supports Google, Microsoft, Okta, and custom SAML.",
              "score": null
            },
            {
              "doc_id": "adr_003",
              "chunk_id": "adr_003_fix_1",
              "content": "**Pros:** - Industry standard - Stateless validation - Supports refresh tokens - SSO compatible **Cons:** - Token size overhead - Cannot revoke individual tokens - Complexity ### Option 2: Session-based auth Traditional server-side sessions. **Pros:** - Simple implementation - Easy revocation - Smaller payloads **Cons:** - Requires session store - Not suitable for APIs - Scaling challenges ### Option 3: API keys only Simple key-based authentication.",
              "score": null
            },
            {
              "doc_id": "adr_003",
              "chunk_id": "adr_003_fix_0",
              "content": "# ADR-003: Implement OAuth 2.0 with JWT for Authentication **Status:** Accepted **Date:** 2024-05-08 ## Context We need a secure authentication system that supports API keys, user sessions, and third-party integrations. ## Options Considered ### Option 1: OAuth 2.0 with JWT Industry standard token-based auth.",
              "score": null
            }
          ],
          "latency_ms": 3.7200820006546564,
          "query_id": "arch_037",
          "dimension": "synonym",
          "query": "How is the identity management and token issuance system designed?"
        },
        {
          "key_facts": [
            "bcrypt",
            "cost factor 12",
            "RS256",
            "refresh tokens"
          ],
          "found_facts": [
            "RS256",
            "refresh tokens"
          ],
          "missed_facts": [
            "bcrypt",
            "cost factor 12"
          ],
          "coverage": 0.5,
          "expected_docs": [
            "arch_authentication_service"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "arch_authentication_service",
              "chunk_id": "arch_authentication_service_fix_1",
              "content": "**Technology:** Custom Go service **Scaling:** Horizontal stateless ### Token Service Issues and validates access tokens. Token lifetime is 1 hour with 7-day refresh tokens. **Technology:** JWT with RS256 **Scaling:** Horizontal stateless ### SSO Gateway Integrates with external identity providers. Supports Google, Microsoft, Okta, and custom SAML.",
              "score": null
            },
            {
              "doc_id": "runbook_api_gateway_5xx_errors",
              "chunk_id": "runbook_api_gateway_5xx_errors_fix_2",
              "content": "If 5xx persists > 15 minutes, page on-call. May indicate infrastructure issue. ## Post-Incident - Document timeline in incident log - Schedule post-mortem within 48 hours - Update runbook if new patterns discovered",
              "score": null
            },
            {
              "doc_id": "adr_003",
              "chunk_id": "adr_003_fix_2",
              "content": "**Pros:** - Very simple - Low overhead - Easy to implement **Cons:** - No user context - Hard to rotate - No SSO support ## Decision We will implement OAuth 2.0 with JWT tokens. Access tokens expire after 1 hour, refresh tokens after 7 days. ## Consequences - Implement token rotation on refresh - Support API keys for machine-to-machine auth - Add MFA for admin accounts",
              "score": null
            },
            {
              "doc_id": "runbook_out_of_memory_oom_kills",
              "chunk_id": "runbook_out_of_memory_oom_kills_fix_2",
              "content": "increase, escalate to development team for memory leak investigation. ## Post-Incident - Document timeline in incident log - Schedule post-mortem within 48 hours - Update runbook if new patterns discovered",
              "score": null
            },
            {
              "doc_id": "adr_003",
              "chunk_id": "adr_003_fix_1",
              "content": "**Pros:** - Industry standard - Stateless validation - Supports refresh tokens - SSO compatible **Cons:** - Token size overhead - Cannot revoke individual tokens - Complexity ### Option 2: Session-based auth Traditional server-side sessions. **Pros:** - Simple implementation - Easy revocation - Smaller payloads **Cons:** - Requires session store - Not suitable for APIs - Scaling challenges ### Option 3: API keys only Simple key-based authentication.",
              "score": null
            }
          ],
          "latency_ms": 4.115318000913248,
          "query_id": "arch_037",
          "dimension": "problem",
          "query": "Users report getting logged out unexpectedly - need to understand token lifecycle and refresh mechanism"
        },
        {
          "key_facts": [
            "bcrypt",
            "cost factor 12",
            "RS256",
            "refresh tokens"
          ],
          "found_facts": [
            "bcrypt",
            "cost factor 12",
            "RS256",
            "refresh tokens"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "arch_authentication_service"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "arch_authentication_service",
              "chunk_id": "arch_authentication_service_fix_0",
              "content": "# Authentication Service Architecture ## Overview The Authentication Service handles user identity, session management, and token issuance. Supports OAuth 2.0, SAML, and API key authentication. ## Design Principles - **Zero Knowledge**: Passwords never stored in plaintext - **Token Rotation**: Refresh tokens rotate on each use ## Components ### Identity Provider Manages user accounts and credentials. Password hashing uses bcrypt with cost factor 12.",
              "score": null
            },
            {
              "doc_id": "adr_003",
              "chunk_id": "adr_003_fix_2",
              "content": "**Pros:** - Very simple - Low overhead - Easy to implement **Cons:** - No user context - Hard to rotate - No SSO support ## Decision We will implement OAuth 2.0 with JWT tokens. Access tokens expire after 1 hour, refresh tokens after 7 days. ## Consequences - Implement token rotation on refresh - Support API keys for machine-to-machine auth - Add MFA for admin accounts",
              "score": null
            },
            {
              "doc_id": "adr_003",
              "chunk_id": "adr_003_fix_0",
              "content": "# ADR-003: Implement OAuth 2.0 with JWT for Authentication **Status:** Accepted **Date:** 2024-05-08 ## Context We need a secure authentication system that supports API keys, user sessions, and third-party integrations. ## Options Considered ### Option 1: OAuth 2.0 with JWT Industry standard token-based auth.",
              "score": null
            },
            {
              "doc_id": "adr_003",
              "chunk_id": "adr_003_fix_1",
              "content": "**Pros:** - Industry standard - Stateless validation - Supports refresh tokens - SSO compatible **Cons:** - Token size overhead - Cannot revoke individual tokens - Complexity ### Option 2: Session-based auth Traditional server-side sessions. **Pros:** - Simple implementation - Easy revocation - Smaller payloads **Cons:** - Requires session store - Not suitable for APIs - Scaling challenges ### Option 3: API keys only Simple key-based authentication.",
              "score": null
            },
            {
              "doc_id": "arch_authentication_service",
              "chunk_id": "arch_authentication_service_fix_1",
              "content": "**Technology:** Custom Go service **Scaling:** Horizontal stateless ### Token Service Issues and validates access tokens. Token lifetime is 1 hour with 7-day refresh tokens. **Technology:** JWT with RS256 **Scaling:** Horizontal stateless ### SSO Gateway Integrates with external identity providers. Supports Google, Microsoft, Okta, and custom SAML.",
              "score": null
            }
          ],
          "latency_ms": 3.8523580005858094,
          "query_id": "arch_037",
          "dimension": "casual",
          "query": "auth service design"
        },
        {
          "key_facts": [
            "bcrypt",
            "cost factor 12",
            "RS256",
            "refresh tokens"
          ],
          "found_facts": [
            "bcrypt",
            "cost factor 12",
            "refresh tokens"
          ],
          "missed_facts": [
            "RS256"
          ],
          "coverage": 0.75,
          "expected_docs": [
            "arch_authentication_service"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "howto_configure_single_sign_on",
              "chunk_id": "howto_configure_single_sign_on_fix_0",
              "content": "# How to Configure Single Sign-On ## Prerequisites - Admin access - Enterprise plan - Identity provider access ## Steps ### Step 1: Choose provider CloudFlow supports SAML 2.0, OpenID Connect, and OAuth 2.0 providers. ### Step 2: Get metadata Download CloudFlow's SAML metadata for your IdP configuration. ```bash cloudflow sso metadata --format xml > cloudflow-metadata.xml ``` ### Step 3: Configure IdP Add CloudFlow as a service provider in your identity provider.",
              "score": null
            },
            {
              "doc_id": "howto_configure_single_sign_on",
              "chunk_id": "howto_configure_single_sign_on_fix_1",
              "content": "> **Note:** ACS URL: https://auth.cloudflow.io/saml/callback ### Step 4: Upload IdP metadata Provide your IdP's metadata to CloudFlow. ```bash cloudflow sso configure --idp-metadata ./idp-metadata.xml ``` ### Step 5: Test login Verify SSO login works correctly. ```bash cloudflow sso test ``` ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            },
            {
              "doc_id": "arch_authentication_service",
              "chunk_id": "arch_authentication_service_fix_0",
              "content": "# Authentication Service Architecture ## Overview The Authentication Service handles user identity, session management, and token issuance. Supports OAuth 2.0, SAML, and API key authentication. ## Design Principles - **Zero Knowledge**: Passwords never stored in plaintext - **Token Rotation**: Refresh tokens rotate on each use ## Components ### Identity Provider Manages user accounts and credentials. Password hashing uses bcrypt with cost factor 12.",
              "score": null
            },
            {
              "doc_id": "arch_api_gateway",
              "chunk_id": "arch_api_gateway_fix_0",
              "content": "# API Gateway Architecture ## Overview The API Gateway handles all incoming HTTP requests, providing authentication, rate limiting, and request routing. Built on Kong with custom plugins for CloudFlow-specific functionality. ## Design Principles - **Zero Trust**: Every request is authenticated and authorized - **Defense in Depth**: Multiple layers of security validation ## Components ### Kong Gateway Primary ingress controller. Handles TLS termination, request routing, and plugin execution.",
              "score": null
            },
            {
              "doc_id": "api_integrations",
              "chunk_id": "api_integrations_fix_0",
              "content": "# Integrations API Reference ## Overview The Integrations API provides programmatic access to integrations functionality. Base URL: `https://api.cloudflow.io/v1/integrations` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /integrations List available integrations.",
              "score": null
            }
          ],
          "latency_ms": 4.196949999823119,
          "query_id": "arch_037",
          "dimension": "contextual",
          "query": "Integrating with our corporate SSO and need to understand how CloudFlow handles OAuth, SAML, and password hashing"
        },
        {
          "key_facts": [
            "bcrypt",
            "cost factor 12",
            "RS256",
            "refresh tokens"
          ],
          "found_facts": [
            "bcrypt",
            "cost factor 12",
            "RS256",
            "refresh tokens"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "arch_authentication_service"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "arch_authentication_service",
              "chunk_id": "arch_authentication_service_fix_0",
              "content": "# Authentication Service Architecture ## Overview The Authentication Service handles user identity, session management, and token issuance. Supports OAuth 2.0, SAML, and API key authentication. ## Design Principles - **Zero Knowledge**: Passwords never stored in plaintext - **Token Rotation**: Refresh tokens rotate on each use ## Components ### Identity Provider Manages user accounts and credentials. Password hashing uses bcrypt with cost factor 12.",
              "score": null
            },
            {
              "doc_id": "adr_003",
              "chunk_id": "adr_003_fix_0",
              "content": "# ADR-003: Implement OAuth 2.0 with JWT for Authentication **Status:** Accepted **Date:** 2024-05-08 ## Context We need a secure authentication system that supports API keys, user sessions, and third-party integrations. ## Options Considered ### Option 1: OAuth 2.0 with JWT Industry standard token-based auth.",
              "score": null
            },
            {
              "doc_id": "adr_003",
              "chunk_id": "adr_003_fix_2",
              "content": "**Pros:** - Very simple - Low overhead - Easy to implement **Cons:** - No user context - Hard to rotate - No SSO support ## Decision We will implement OAuth 2.0 with JWT tokens. Access tokens expire after 1 hour, refresh tokens after 7 days. ## Consequences - Implement token rotation on refresh - Support API keys for machine-to-machine auth - Add MFA for admin accounts",
              "score": null
            },
            {
              "doc_id": "arch_authentication_service",
              "chunk_id": "arch_authentication_service_fix_1",
              "content": "**Technology:** Custom Go service **Scaling:** Horizontal stateless ### Token Service Issues and validates access tokens. Token lifetime is 1 hour with 7-day refresh tokens. **Technology:** JWT with RS256 **Scaling:** Horizontal stateless ### SSO Gateway Integrates with external identity providers. Supports Google, Microsoft, Okta, and custom SAML.",
              "score": null
            },
            {
              "doc_id": "arch_api_gateway",
              "chunk_id": "arch_api_gateway_fix_1",
              "content": "**Technology:** Kong 3.4 on Kubernetes **Scaling:** Horizontal with HPA ### Auth Plugin Validates JWT tokens, API keys, and OAuth2 flows. Token validation latency under 5ms. **Technology:** Custom Lua plugin **Scaling:** Stateless ### Rate Limiter Enforces per-user and per-IP rate limits. Default limit is 100 requests per minute.",
              "score": null
            }
          ],
          "latency_ms": 3.58154399873456,
          "query_id": "arch_037",
          "dimension": "negation",
          "query": "I don't get how password hashing and JWT token signing work in the auth service"
        },
        {
          "key_facts": [
            "GET /workflows",
            "limit",
            "status"
          ],
          "found_facts": [
            "GET /workflows",
            "limit",
            "status"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "api_workflows"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "api_workflows",
              "chunk_id": "api_workflows_fix_0",
              "content": "# Workflows API Reference ## Overview The Workflows API provides programmatic access to workflows functionality. Base URL: `https://api.cloudflow.io/v1/workflows` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /workflows List all workflows for the authenticated user.",
              "score": null
            },
            {
              "doc_id": "api_workflows",
              "chunk_id": "api_workflows_fix_1",
              "content": "**Parameters:** - `limit` (integer): Maximum number of results (default: 50, max: 200) - `status` (string): Filter by status: active, paused, archived **Response:** - Success: `200` - Returns array of workflow objects - Error: `400` - Bad Request **Example:** ```json {\"workflows\": [{\"id\": \"wf_123\", \"name\": \"Daily Report\", \"status\": \"active\"}]} ``` ### POST /workflows Create a new workflow.",
              "score": null
            },
            {
              "doc_id": "api_metrics",
              "chunk_id": "api_metrics_fix_0",
              "content": "# Metrics API Reference ## Overview The Metrics API provides programmatic access to metrics functionality. Base URL: `https://api.cloudflow.io/v1/metrics` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /metrics/workflows Get workflow performance metrics.",
              "score": null
            },
            {
              "doc_id": "api_executions",
              "chunk_id": "api_executions_fix_0",
              "content": "# Executions API Reference ## Overview The Executions API provides programmatic access to executions functionality. Base URL: `https://api.cloudflow.io/v1/executions` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /executions List workflow executions.",
              "score": null
            },
            {
              "doc_id": "api_executions",
              "chunk_id": "api_executions_fix_1",
              "content": "**Parameters:** - `workflow_id` (string): Filter by workflow ID - `since` (datetime): Filter executions after this timestamp **Response:** - Success: `200` - Returns array of execution objects - Error: `400` - Bad Request **Example:** ```json {\"executions\": [{\"id\": \"ex_789\", \"status\": \"completed\", \"duration_ms\": 1523}]} ``` ### POST /executions/{id}/cancel Cancel a running execution.",
              "score": null
            }
          ],
          "latency_ms": 3.300730999399093,
          "query_id": "api_038",
          "dimension": "original",
          "query": "How do I list workflows via API?"
        },
        {
          "key_facts": [
            "GET /workflows",
            "limit",
            "status"
          ],
          "found_facts": [
            "GET /workflows",
            "limit",
            "status"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "api_workflows"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "api_workflows",
              "chunk_id": "api_workflows_fix_0",
              "content": "# Workflows API Reference ## Overview The Workflows API provides programmatic access to workflows functionality. Base URL: `https://api.cloudflow.io/v1/workflows` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /workflows List all workflows for the authenticated user.",
              "score": null
            },
            {
              "doc_id": "api_workflows",
              "chunk_id": "api_workflows_fix_1",
              "content": "**Parameters:** - `limit` (integer): Maximum number of results (default: 50, max: 200) - `status` (string): Filter by status: active, paused, archived **Response:** - Success: `200` - Returns array of workflow objects - Error: `400` - Bad Request **Example:** ```json {\"workflows\": [{\"id\": \"wf_123\", \"name\": \"Daily Report\", \"status\": \"active\"}]} ``` ### POST /workflows Create a new workflow.",
              "score": null
            },
            {
              "doc_id": "api_executions",
              "chunk_id": "api_executions_fix_0",
              "content": "# Executions API Reference ## Overview The Executions API provides programmatic access to executions functionality. Base URL: `https://api.cloudflow.io/v1/executions` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /executions List workflow executions.",
              "score": null
            },
            {
              "doc_id": "api_integrations",
              "chunk_id": "api_integrations_fix_0",
              "content": "# Integrations API Reference ## Overview The Integrations API provides programmatic access to integrations functionality. Base URL: `https://api.cloudflow.io/v1/integrations` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /integrations List available integrations.",
              "score": null
            },
            {
              "doc_id": "api_metrics",
              "chunk_id": "api_metrics_fix_1",
              "content": "**Parameters:** - `period` (string): Time period: hour, day, week, month - `workflow_id` (string): Filter by workflow ID **Response:** - Success: `200` - Returns metrics data - Error: `400` - Bad Request **Example:** ```json {\"success_rate\": 0.985, \"avg_duration_ms\": 1250, \"p99_duration_ms\": 5000, \"total_executions\": 10000} ```",
              "score": null
            }
          ],
          "latency_ms": 3.9500800012319814,
          "query_id": "api_038",
          "dimension": "synonym",
          "query": "I need to fetch all my workflows through the REST endpoint"
        },
        {
          "key_facts": [
            "GET /workflows",
            "limit",
            "status"
          ],
          "found_facts": [
            "GET /workflows",
            "limit",
            "status"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "api_workflows"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "api_workflows",
              "chunk_id": "api_workflows_fix_1",
              "content": "**Parameters:** - `limit` (integer): Maximum number of results (default: 50, max: 200) - `status` (string): Filter by status: active, paused, archived **Response:** - Success: `200` - Returns array of workflow objects - Error: `400` - Bad Request **Example:** ```json {\"workflows\": [{\"id\": \"wf_123\", \"name\": \"Daily Report\", \"status\": \"active\"}]} ``` ### POST /workflows Create a new workflow.",
              "score": null
            },
            {
              "doc_id": "api_metrics",
              "chunk_id": "api_metrics_fix_1",
              "content": "**Parameters:** - `period` (string): Time period: hour, day, week, month - `workflow_id` (string): Filter by workflow ID **Response:** - Success: `200` - Returns metrics data - Error: `400` - Bad Request **Example:** ```json {\"success_rate\": 0.985, \"avg_duration_ms\": 1250, \"p99_duration_ms\": 5000, \"total_executions\": 10000} ```",
              "score": null
            },
            {
              "doc_id": "api_workflows",
              "chunk_id": "api_workflows_fix_2",
              "content": "**Parameters:** - `name` (string): Workflow name (required) - `definition` (object): Workflow definition in JSON format **Response:** - Success: `201` - Workflow created successfully - Error: `400` - Bad Request **Example:** ```json {\"id\": \"wf_456\", \"name\": \"New Workflow\", \"created_at\": \"2024-01-15T10:00:00Z\"} ``` ### DELETE /workflows/{id} Delete a workflow permanently. **Parameters:** - `id` (string): Workflow ID (required) **Response:** - Success: `204` - Workflow deleted - Error: `404` - Workflow not found **Example:** ```json {} ```",
              "score": null
            },
            {
              "doc_id": "api_workflows",
              "chunk_id": "api_workflows_fix_0",
              "content": "# Workflows API Reference ## Overview The Workflows API provides programmatic access to workflows functionality. Base URL: `https://api.cloudflow.io/v1/workflows` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /workflows List all workflows for the authenticated user.",
              "score": null
            },
            {
              "doc_id": "api_executions",
              "chunk_id": "api_executions_fix_1",
              "content": "**Parameters:** - `workflow_id` (string): Filter by workflow ID - `since` (datetime): Filter executions after this timestamp **Response:** - Success: `200` - Returns array of execution objects - Error: `400` - Bad Request **Example:** ```json {\"executions\": [{\"id\": \"ex_789\", \"status\": \"completed\", \"duration_ms\": 1523}]} ``` ### POST /executions/{id}/cancel Cancel a running execution.",
              "score": null
            }
          ],
          "latency_ms": 3.8944970001466572,
          "query_id": "api_038",
          "dimension": "problem",
          "query": "my GET request to /workflows returns empty array even though I have workflows"
        },
        {
          "key_facts": [
            "GET /workflows",
            "limit",
            "status"
          ],
          "found_facts": [
            "GET /workflows",
            "limit",
            "status"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "api_workflows"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "api_workflows",
              "chunk_id": "api_workflows_fix_0",
              "content": "# Workflows API Reference ## Overview The Workflows API provides programmatic access to workflows functionality. Base URL: `https://api.cloudflow.io/v1/workflows` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /workflows List all workflows for the authenticated user.",
              "score": null
            },
            {
              "doc_id": "api_executions",
              "chunk_id": "api_executions_fix_0",
              "content": "# Executions API Reference ## Overview The Executions API provides programmatic access to executions functionality. Base URL: `https://api.cloudflow.io/v1/executions` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /executions List workflow executions.",
              "score": null
            },
            {
              "doc_id": "api_metrics",
              "chunk_id": "api_metrics_fix_0",
              "content": "# Metrics API Reference ## Overview The Metrics API provides programmatic access to metrics functionality. Base URL: `https://api.cloudflow.io/v1/metrics` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /metrics/workflows Get workflow performance metrics.",
              "score": null
            },
            {
              "doc_id": "api_workflows",
              "chunk_id": "api_workflows_fix_1",
              "content": "**Parameters:** - `limit` (integer): Maximum number of results (default: 50, max: 200) - `status` (string): Filter by status: active, paused, archived **Response:** - Success: `200` - Returns array of workflow objects - Error: `400` - Bad Request **Example:** ```json {\"workflows\": [{\"id\": \"wf_123\", \"name\": \"Daily Report\", \"status\": \"active\"}]} ``` ### POST /workflows Create a new workflow.",
              "score": null
            },
            {
              "doc_id": "api_executions",
              "chunk_id": "api_executions_fix_1",
              "content": "**Parameters:** - `workflow_id` (string): Filter by workflow ID - `since` (datetime): Filter executions after this timestamp **Response:** - Success: `200` - Returns array of execution objects - Error: `400` - Bad Request **Example:** ```json {\"executions\": [{\"id\": \"ex_789\", \"status\": \"completed\", \"duration_ms\": 1523}]} ``` ### POST /executions/{id}/cancel Cancel a running execution.",
              "score": null
            }
          ],
          "latency_ms": 3.467741998974816,
          "query_id": "api_038",
          "dimension": "casual",
          "query": "list workflows endpoint"
        },
        {
          "key_facts": [
            "GET /workflows",
            "limit",
            "status"
          ],
          "found_facts": [
            "GET /workflows",
            "limit",
            "status"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "api_workflows"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "api_workflows",
              "chunk_id": "api_workflows_fix_0",
              "content": "# Workflows API Reference ## Overview The Workflows API provides programmatic access to workflows functionality. Base URL: `https://api.cloudflow.io/v1/workflows` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /workflows List all workflows for the authenticated user.",
              "score": null
            },
            {
              "doc_id": "api_metrics",
              "chunk_id": "api_metrics_fix_0",
              "content": "# Metrics API Reference ## Overview The Metrics API provides programmatic access to metrics functionality. Base URL: `https://api.cloudflow.io/v1/metrics` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /metrics/workflows Get workflow performance metrics.",
              "score": null
            },
            {
              "doc_id": "api_users",
              "chunk_id": "api_users_fix_0",
              "content": "# Users API Reference ## Overview The Users API provides programmatic access to users functionality. Base URL: `https://api.cloudflow.io/v1/users` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /users/me Get current user profile.",
              "score": null
            },
            {
              "doc_id": "api_workflows",
              "chunk_id": "api_workflows_fix_1",
              "content": "**Parameters:** - `limit` (integer): Maximum number of results (default: 50, max: 200) - `status` (string): Filter by status: active, paused, archived **Response:** - Success: `200` - Returns array of workflow objects - Error: `400` - Bad Request **Example:** ```json {\"workflows\": [{\"id\": \"wf_123\", \"name\": \"Daily Report\", \"status\": \"active\"}]} ``` ### POST /workflows Create a new workflow.",
              "score": null
            },
            {
              "doc_id": "api_executions",
              "chunk_id": "api_executions_fix_0",
              "content": "# Executions API Reference ## Overview The Executions API provides programmatic access to executions functionality. Base URL: `https://api.cloudflow.io/v1/executions` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /executions List workflow executions.",
              "score": null
            }
          ],
          "latency_ms": 3.697189000376966,
          "query_id": "api_038",
          "dimension": "contextual",
          "query": "building a dashboard that shows all user workflows, need the API call to retrieve them"
        },
        {
          "key_facts": [
            "GET /workflows",
            "limit",
            "status"
          ],
          "found_facts": [
            "GET /workflows",
            "limit",
            "status"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "api_workflows"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "api_workflows",
              "chunk_id": "api_workflows_fix_0",
              "content": "# Workflows API Reference ## Overview The Workflows API provides programmatic access to workflows functionality. Base URL: `https://api.cloudflow.io/v1/workflows` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /workflows List all workflows for the authenticated user.",
              "score": null
            },
            {
              "doc_id": "api_executions",
              "chunk_id": "api_executions_fix_0",
              "content": "# Executions API Reference ## Overview The Executions API provides programmatic access to executions functionality. Base URL: `https://api.cloudflow.io/v1/executions` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /executions List workflow executions.",
              "score": null
            },
            {
              "doc_id": "api_workflows",
              "chunk_id": "api_workflows_fix_1",
              "content": "**Parameters:** - `limit` (integer): Maximum number of results (default: 50, max: 200) - `status` (string): Filter by status: active, paused, archived **Response:** - Success: `200` - Returns array of workflow objects - Error: `400` - Bad Request **Example:** ```json {\"workflows\": [{\"id\": \"wf_123\", \"name\": \"Daily Report\", \"status\": \"active\"}]} ``` ### POST /workflows Create a new workflow.",
              "score": null
            },
            {
              "doc_id": "api_workflows",
              "chunk_id": "api_workflows_fix_2",
              "content": "**Parameters:** - `name` (string): Workflow name (required) - `definition` (object): Workflow definition in JSON format **Response:** - Success: `201` - Workflow created successfully - Error: `400` - Bad Request **Example:** ```json {\"id\": \"wf_456\", \"name\": \"New Workflow\", \"created_at\": \"2024-01-15T10:00:00Z\"} ``` ### DELETE /workflows/{id} Delete a workflow permanently. **Parameters:** - `id` (string): Workflow ID (required) **Response:** - Success: `204` - Workflow deleted - Error: `404` - Workflow not found **Example:** ```json {} ```",
              "score": null
            },
            {
              "doc_id": "api_metrics",
              "chunk_id": "api_metrics_fix_0",
              "content": "# Metrics API Reference ## Overview The Metrics API provides programmatic access to metrics functionality. Base URL: `https://api.cloudflow.io/v1/metrics` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /metrics/workflows Get workflow performance metrics.",
              "score": null
            }
          ],
          "latency_ms": 3.781295999942813,
          "query_id": "api_038",
          "dimension": "negation",
          "query": "why can't I see my workflows when calling the API"
        },
        {
          "key_facts": [
            "POST /workflows",
            "201",
            "name",
            "definition"
          ],
          "found_facts": [
            "201",
            "name",
            "definition"
          ],
          "missed_facts": [
            "POST /workflows"
          ],
          "coverage": 0.75,
          "expected_docs": [
            "api_workflows"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "api_workflows",
              "chunk_id": "api_workflows_fix_0",
              "content": "# Workflows API Reference ## Overview The Workflows API provides programmatic access to workflows functionality. Base URL: `https://api.cloudflow.io/v1/workflows` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /workflows List all workflows for the authenticated user.",
              "score": null
            },
            {
              "doc_id": "howto_create_api_integration",
              "chunk_id": "howto_create_api_integration_fix_0",
              "content": "# How to Create API Integration ## Prerequisites - CloudFlow account ## Steps ### Step 1: Generate API key Create an API key for programmatic access. ```bash cloudflow apikeys create --name 'CI Integration' --scopes workflows:read,workflows:execute ``` ### Step 2: Store key securely Never commit API keys to source control. Use environment variables or secret management. > **Note:** Rotate keys every 90 days ### Step 3: Test authentication Verify the API key works correctly.",
              "score": null
            },
            {
              "doc_id": "api_executions",
              "chunk_id": "api_executions_fix_0",
              "content": "# Executions API Reference ## Overview The Executions API provides programmatic access to executions functionality. Base URL: `https://api.cloudflow.io/v1/executions` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /executions List workflow executions.",
              "score": null
            },
            {
              "doc_id": "api_workflows",
              "chunk_id": "api_workflows_fix_2",
              "content": "**Parameters:** - `name` (string): Workflow name (required) - `definition` (object): Workflow definition in JSON format **Response:** - Success: `201` - Workflow created successfully - Error: `400` - Bad Request **Example:** ```json {\"id\": \"wf_456\", \"name\": \"New Workflow\", \"created_at\": \"2024-01-15T10:00:00Z\"} ``` ### DELETE /workflows/{id} Delete a workflow permanently. **Parameters:** - `id` (string): Workflow ID (required) **Response:** - Success: `204` - Workflow deleted - Error: `404` - Workflow not found **Example:** ```json {} ```",
              "score": null
            },
            {
              "doc_id": "api_integrations",
              "chunk_id": "api_integrations_fix_0",
              "content": "# Integrations API Reference ## Overview The Integrations API provides programmatic access to integrations functionality. Base URL: `https://api.cloudflow.io/v1/integrations` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /integrations List available integrations.",
              "score": null
            }
          ],
          "latency_ms": 3.736692000529729,
          "query_id": "api_039",
          "dimension": "original",
          "query": "How do I create a workflow via API?"
        },
        {
          "key_facts": [
            "POST /workflows",
            "201",
            "name",
            "definition"
          ],
          "found_facts": [
            "201",
            "name",
            "definition"
          ],
          "missed_facts": [
            "POST /workflows"
          ],
          "coverage": 0.75,
          "expected_docs": [
            "api_workflows"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "api_workflows",
              "chunk_id": "api_workflows_fix_0",
              "content": "# Workflows API Reference ## Overview The Workflows API provides programmatic access to workflows functionality. Base URL: `https://api.cloudflow.io/v1/workflows` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /workflows List all workflows for the authenticated user.",
              "score": null
            },
            {
              "doc_id": "api_executions",
              "chunk_id": "api_executions_fix_0",
              "content": "# Executions API Reference ## Overview The Executions API provides programmatic access to executions functionality. Base URL: `https://api.cloudflow.io/v1/executions` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /executions List workflow executions.",
              "score": null
            },
            {
              "doc_id": "api_integrations",
              "chunk_id": "api_integrations_fix_0",
              "content": "# Integrations API Reference ## Overview The Integrations API provides programmatic access to integrations functionality. Base URL: `https://api.cloudflow.io/v1/integrations` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /integrations List available integrations.",
              "score": null
            },
            {
              "doc_id": "api_workflows",
              "chunk_id": "api_workflows_fix_2",
              "content": "**Parameters:** - `name` (string): Workflow name (required) - `definition` (object): Workflow definition in JSON format **Response:** - Success: `201` - Workflow created successfully - Error: `400` - Bad Request **Example:** ```json {\"id\": \"wf_456\", \"name\": \"New Workflow\", \"created_at\": \"2024-01-15T10:00:00Z\"} ``` ### DELETE /workflows/{id} Delete a workflow permanently. **Parameters:** - `id` (string): Workflow ID (required) **Response:** - Success: `204` - Workflow deleted - Error: `404` - Workflow not found **Example:** ```json {} ```",
              "score": null
            },
            {
              "doc_id": "howto_create_api_integration",
              "chunk_id": "howto_create_api_integration_fix_0",
              "content": "# How to Create API Integration ## Prerequisites - CloudFlow account ## Steps ### Step 1: Generate API key Create an API key for programmatic access. ```bash cloudflow apikeys create --name 'CI Integration' --scopes workflows:read,workflows:execute ``` ### Step 2: Store key securely Never commit API keys to source control. Use environment variables or secret management. > **Note:** Rotate keys every 90 days ### Step 3: Test authentication Verify the API key works correctly.",
              "score": null
            }
          ],
          "latency_ms": 4.086585000550258,
          "query_id": "api_039",
          "dimension": "synonym",
          "query": "I need to programmatically add a new workflow using the REST interface"
        },
        {
          "key_facts": [
            "POST /workflows",
            "201",
            "name",
            "definition"
          ],
          "found_facts": [
            "POST /workflows",
            "201",
            "name",
            "definition"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "api_workflows"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "api_workflows",
              "chunk_id": "api_workflows_fix_2",
              "content": "**Parameters:** - `name` (string): Workflow name (required) - `definition` (object): Workflow definition in JSON format **Response:** - Success: `201` - Workflow created successfully - Error: `400` - Bad Request **Example:** ```json {\"id\": \"wf_456\", \"name\": \"New Workflow\", \"created_at\": \"2024-01-15T10:00:00Z\"} ``` ### DELETE /workflows/{id} Delete a workflow permanently. **Parameters:** - `id` (string): Workflow ID (required) **Response:** - Success: `204` - Workflow deleted - Error: `404` - Workflow not found **Example:** ```json {} ```",
              "score": null
            },
            {
              "doc_id": "api_metrics",
              "chunk_id": "api_metrics_fix_1",
              "content": "**Parameters:** - `period` (string): Time period: hour, day, week, month - `workflow_id` (string): Filter by workflow ID **Response:** - Success: `200` - Returns metrics data - Error: `400` - Bad Request **Example:** ```json {\"success_rate\": 0.985, \"avg_duration_ms\": 1250, \"p99_duration_ms\": 5000, \"total_executions\": 10000} ```",
              "score": null
            },
            {
              "doc_id": "api_workflows",
              "chunk_id": "api_workflows_fix_1",
              "content": "**Parameters:** - `limit` (integer): Maximum number of results (default: 50, max: 200) - `status` (string): Filter by status: active, paused, archived **Response:** - Success: `200` - Returns array of workflow objects - Error: `400` - Bad Request **Example:** ```json {\"workflows\": [{\"id\": \"wf_123\", \"name\": \"Daily Report\", \"status\": \"active\"}]} ``` ### POST /workflows Create a new workflow.",
              "score": null
            },
            {
              "doc_id": "api_secrets",
              "chunk_id": "api_secrets_fix_2",
              "content": "**Parameters:** **Response:** - Success: `204` - Secret deleted - Error: `400` - Bad Request **Example:** ```json {} ```",
              "score": null
            },
            {
              "doc_id": "config_workflow_engine",
              "chunk_id": "config_workflow_engine_fix_0",
              "content": "# Workflow Engine Configuration Reference ## Overview This document describes all configuration options for Workflow Engine. ## Environment Variables | Variable | Type | Default | Description | |----------|------|---------|-------------| | `WORKFLOW_MAX_STEPS` | integer | 100 | Maximum steps per workflow | | `WORKFLOW_TIMEOUT_SECONDS` | integer | 3600 | Maximum workflow execution time | | `WORKFLOW_RETRY_LIMIT` | integer | 3 | Default retry attempts for failed steps | | `WORKFLOW_CONCURRENT_LIMIT` | integer | 10 | Maximum",
              "score": null
            }
          ],
          "latency_ms": 3.6247940006433055,
          "query_id": "api_039",
          "dimension": "problem",
          "query": "getting 400 error when trying to POST a new workflow, what fields are required"
        },
        {
          "key_facts": [
            "POST /workflows",
            "201",
            "name",
            "definition"
          ],
          "found_facts": [
            "POST /workflows",
            "name"
          ],
          "missed_facts": [
            "201",
            "definition"
          ],
          "coverage": 0.5,
          "expected_docs": [
            "api_workflows"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "api_executions",
              "chunk_id": "api_executions_fix_1",
              "content": "**Parameters:** - `workflow_id` (string): Filter by workflow ID - `since` (datetime): Filter executions after this timestamp **Response:** - Success: `200` - Returns array of execution objects - Error: `400` - Bad Request **Example:** ```json {\"executions\": [{\"id\": \"ex_789\", \"status\": \"completed\", \"duration_ms\": 1523}]} ``` ### POST /executions/{id}/cancel Cancel a running execution.",
              "score": null
            },
            {
              "doc_id": "api_metrics",
              "chunk_id": "api_metrics_fix_1",
              "content": "**Parameters:** - `period` (string): Time period: hour, day, week, month - `workflow_id` (string): Filter by workflow ID **Response:** - Success: `200` - Returns metrics data - Error: `400` - Bad Request **Example:** ```json {\"success_rate\": 0.985, \"avg_duration_ms\": 1250, \"p99_duration_ms\": 5000, \"total_executions\": 10000} ```",
              "score": null
            },
            {
              "doc_id": "api_workflows",
              "chunk_id": "api_workflows_fix_1",
              "content": "**Parameters:** - `limit` (integer): Maximum number of results (default: 50, max: 200) - `status` (string): Filter by status: active, paused, archived **Response:** - Success: `200` - Returns array of workflow objects - Error: `400` - Bad Request **Example:** ```json {\"workflows\": [{\"id\": \"wf_123\", \"name\": \"Daily Report\", \"status\": \"active\"}]} ``` ### POST /workflows Create a new workflow.",
              "score": null
            },
            {
              "doc_id": "api_workflows",
              "chunk_id": "api_workflows_fix_0",
              "content": "# Workflows API Reference ## Overview The Workflows API provides programmatic access to workflows functionality. Base URL: `https://api.cloudflow.io/v1/workflows` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /workflows List all workflows for the authenticated user.",
              "score": null
            },
            {
              "doc_id": "api_executions",
              "chunk_id": "api_executions_fix_0",
              "content": "# Executions API Reference ## Overview The Executions API provides programmatic access to executions functionality. Base URL: `https://api.cloudflow.io/v1/executions` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /executions List workflow executions.",
              "score": null
            }
          ],
          "latency_ms": 3.4512409984017722,
          "query_id": "api_039",
          "dimension": "casual",
          "query": "POST workflows endpoint params"
        },
        {
          "key_facts": [
            "POST /workflows",
            "201",
            "name",
            "definition"
          ],
          "found_facts": [
            "name"
          ],
          "missed_facts": [
            "POST /workflows",
            "201",
            "definition"
          ],
          "coverage": 0.25,
          "expected_docs": [
            "api_workflows"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "api_workflows",
              "chunk_id": "api_workflows_fix_0",
              "content": "# Workflows API Reference ## Overview The Workflows API provides programmatic access to workflows functionality. Base URL: `https://api.cloudflow.io/v1/workflows` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /workflows List all workflows for the authenticated user.",
              "score": null
            },
            {
              "doc_id": "howto_create_api_integration",
              "chunk_id": "howto_create_api_integration_fix_0",
              "content": "# How to Create API Integration ## Prerequisites - CloudFlow account ## Steps ### Step 1: Generate API key Create an API key for programmatic access. ```bash cloudflow apikeys create --name 'CI Integration' --scopes workflows:read,workflows:execute ``` ### Step 2: Store key securely Never commit API keys to source control. Use environment variables or secret management. > **Note:** Rotate keys every 90 days ### Step 3: Test authentication Verify the API key works correctly.",
              "score": null
            },
            {
              "doc_id": "howto_set_up_ci_cd_pipeline",
              "chunk_id": "howto_set_up_ci_cd_pipeline_fix_0",
              "content": "# How to Set Up CI/CD Pipeline ## Prerequisites - GitHub account - CloudFlow API key - Repository admin access ## Steps ### Step 1: Create GitHub Actions workflow Create a new file at `.github/workflows/cloudflow.yml` with the deployment configuration. ```bash mkdir -p .github/workflows && touch .github/workflows/cloudflow.yml ``` ### Step 2: Configure secrets Add your CloudFlow API key to GitHub secrets.",
              "score": null
            },
            {
              "doc_id": "howto_set_up_ci_cd_pipeline",
              "chunk_id": "howto_set_up_ci_cd_pipeline_fix_1",
              "content": "```bash gh secret set CLOUDFLOW_API_KEY ``` ### Step 3: Add deployment step Add the CloudFlow deployment action to your workflow. > **Note:** Use the official cloudflow/deploy-action@v2 ### Step 4: Test the pipeline Push a commit to trigger the workflow. ```bash git push origin main ``` ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            },
            {
              "doc_id": "api_executions",
              "chunk_id": "api_executions_fix_0",
              "content": "# Executions API Reference ## Overview The Executions API provides programmatic access to executions functionality. Base URL: `https://api.cloudflow.io/v1/executions` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /executions List workflow executions.",
              "score": null
            }
          ],
          "latency_ms": 3.4430850009812275,
          "query_id": "api_039",
          "dimension": "contextual",
          "query": "automating workflow provisioning for new customers, need to know how to create workflows via API"
        },
        {
          "key_facts": [
            "POST /workflows",
            "201",
            "name",
            "definition"
          ],
          "found_facts": [
            "POST /workflows",
            "201",
            "name",
            "definition"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "api_workflows"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "api_workflows",
              "chunk_id": "api_workflows_fix_2",
              "content": "**Parameters:** - `name` (string): Workflow name (required) - `definition` (object): Workflow definition in JSON format **Response:** - Success: `201` - Workflow created successfully - Error: `400` - Bad Request **Example:** ```json {\"id\": \"wf_456\", \"name\": \"New Workflow\", \"created_at\": \"2024-01-15T10:00:00Z\"} ``` ### DELETE /workflows/{id} Delete a workflow permanently. **Parameters:** - `id` (string): Workflow ID (required) **Response:** - Success: `204` - Workflow deleted - Error: `404` - Workflow not found **Example:** ```json {} ```",
              "score": null
            },
            {
              "doc_id": "api_workflows",
              "chunk_id": "api_workflows_fix_1",
              "content": "**Parameters:** - `limit` (integer): Maximum number of results (default: 50, max: 200) - `status` (string): Filter by status: active, paused, archived **Response:** - Success: `200` - Returns array of workflow objects - Error: `400` - Bad Request **Example:** ```json {\"workflows\": [{\"id\": \"wf_123\", \"name\": \"Daily Report\", \"status\": \"active\"}]} ``` ### POST /workflows Create a new workflow.",
              "score": null
            },
            {
              "doc_id": "api_metrics",
              "chunk_id": "api_metrics_fix_1",
              "content": "**Parameters:** - `period` (string): Time period: hour, day, week, month - `workflow_id` (string): Filter by workflow ID **Response:** - Success: `200` - Returns metrics data - Error: `400` - Bad Request **Example:** ```json {\"success_rate\": 0.985, \"avg_duration_ms\": 1250, \"p99_duration_ms\": 5000, \"total_executions\": 10000} ```",
              "score": null
            },
            {
              "doc_id": "api_workflows",
              "chunk_id": "api_workflows_fix_0",
              "content": "# Workflows API Reference ## Overview The Workflows API provides programmatic access to workflows functionality. Base URL: `https://api.cloudflow.io/v1/workflows` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /workflows List all workflows for the authenticated user.",
              "score": null
            },
            {
              "doc_id": "api_audit",
              "chunk_id": "api_audit_fix_1",
              "content": "**Parameters:** - `start_date` (date): Start date for log range - `end_date` (date): End date for log range - `actor` (string): Filter by user ID **Response:** - Success: `200` - Returns array of audit log entries - Error: `400` - Bad Request **Example:** ```json {\"logs\": [{\"timestamp\": \"2024-01-15T10:00:00Z\", \"actor\": \"usr_001\", \"action\": \"workflow.created\", \"resource\": \"wf_123\"}]} ```",
              "score": null
            }
          ],
          "latency_ms": 3.264593000494642,
          "query_id": "api_039",
          "dimension": "negation",
          "query": "why isn't my workflow creation request working"
        },
        {
          "key_facts": [
            "POST /executions/{id}/cancel",
            "reason"
          ],
          "found_facts": [
            "POST /executions/{id}/cancel",
            "reason"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "api_executions"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "howto_implement_retry_logic",
              "chunk_id": "howto_implement_retry_logic_fix_0",
              "content": "# How to Implement Retry Logic ## Prerequisites - Existing workflow ## Steps ### Step 1: Define retry policy Add retry configuration to your workflow step. > **Note:** Maximum 5 retry attempts allowed ### Step 2: Configure backoff Set exponential backoff with base delay of 1 second. The formula is: delay = base * 2^attempt. ### Step 3: Add timeout Set maximum execution time to prevent infinite loops. Default timeout is 30 seconds.",
              "score": null
            },
            {
              "doc_id": "api_executions",
              "chunk_id": "api_executions_fix_1",
              "content": "**Parameters:** - `workflow_id` (string): Filter by workflow ID - `since` (datetime): Filter executions after this timestamp **Response:** - Success: `200` - Returns array of execution objects - Error: `400` - Bad Request **Example:** ```json {\"executions\": [{\"id\": \"ex_789\", \"status\": \"completed\", \"duration_ms\": 1523}]} ``` ### POST /executions/{id}/cancel Cancel a running execution.",
              "score": null
            },
            {
              "doc_id": "api_executions",
              "chunk_id": "api_executions_fix_2",
              "content": "**Parameters:** - `id` (string): Execution ID - `reason` (string): Cancellation reason (optional) **Response:** - Success: `200` - Execution cancelled - Error: `400` - Bad Request **Example:** ```json {\"id\": \"ex_789\", \"status\": \"cancelled\", \"cancelled_at\": \"2024-01-15T10:05:00Z\"} ```",
              "score": null
            },
            {
              "doc_id": "api_executions",
              "chunk_id": "api_executions_fix_0",
              "content": "# Executions API Reference ## Overview The Executions API provides programmatic access to executions functionality. Base URL: `https://api.cloudflow.io/v1/executions` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /executions List workflow executions.",
              "score": null
            },
            {
              "doc_id": "runbook_out_of_memory_oom_kills",
              "chunk_id": "runbook_out_of_memory_oom_kills_fix_2",
              "content": "increase, escalate to development team for memory leak investigation. ## Post-Incident - Document timeline in incident log - Schedule post-mortem within 48 hours - Update runbook if new patterns discovered",
              "score": null
            }
          ],
          "latency_ms": 3.323623001051601,
          "query_id": "api_040",
          "dimension": "original",
          "query": "How do I cancel a running execution?"
        },
        {
          "key_facts": [
            "POST /executions/{id}/cancel",
            "reason"
          ],
          "found_facts": [
            "POST /executions/{id}/cancel"
          ],
          "missed_facts": [
            "reason"
          ],
          "coverage": 0.5,
          "expected_docs": [
            "api_executions"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "api_executions",
              "chunk_id": "api_executions_fix_0",
              "content": "# Executions API Reference ## Overview The Executions API provides programmatic access to executions functionality. Base URL: `https://api.cloudflow.io/v1/executions` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /executions List workflow executions.",
              "score": null
            },
            {
              "doc_id": "api_executions",
              "chunk_id": "api_executions_fix_1",
              "content": "**Parameters:** - `workflow_id` (string): Filter by workflow ID - `since` (datetime): Filter executions after this timestamp **Response:** - Success: `200` - Returns array of execution objects - Error: `400` - Bad Request **Example:** ```json {\"executions\": [{\"id\": \"ex_789\", \"status\": \"completed\", \"duration_ms\": 1523}]} ``` ### POST /executions/{id}/cancel Cancel a running execution.",
              "score": null
            },
            {
              "doc_id": "api_workflows",
              "chunk_id": "api_workflows_fix_0",
              "content": "# Workflows API Reference ## Overview The Workflows API provides programmatic access to workflows functionality. Base URL: `https://api.cloudflow.io/v1/workflows` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /workflows List all workflows for the authenticated user.",
              "score": null
            },
            {
              "doc_id": "api_workflows",
              "chunk_id": "api_workflows_fix_1",
              "content": "**Parameters:** - `limit` (integer): Maximum number of results (default: 50, max: 200) - `status` (string): Filter by status: active, paused, archived **Response:** - Success: `200` - Returns array of workflow objects - Error: `400` - Bad Request **Example:** ```json {\"workflows\": [{\"id\": \"wf_123\", \"name\": \"Daily Report\", \"status\": \"active\"}]} ``` ### POST /workflows Create a new workflow.",
              "score": null
            },
            {
              "doc_id": "howto_create_api_integration",
              "chunk_id": "howto_create_api_integration_fix_0",
              "content": "# How to Create API Integration ## Prerequisites - CloudFlow account ## Steps ### Step 1: Generate API key Create an API key for programmatic access. ```bash cloudflow apikeys create --name 'CI Integration' --scopes workflows:read,workflows:execute ``` ### Step 2: Store key securely Never commit API keys to source control. Use environment variables or secret management. > **Note:** Rotate keys every 90 days ### Step 3: Test authentication Verify the API key works correctly.",
              "score": null
            }
          ],
          "latency_ms": 3.348690001075738,
          "query_id": "api_040",
          "dimension": "synonym",
          "query": "I need to stop an in-progress workflow execution through the API"
        },
        {
          "key_facts": [
            "POST /executions/{id}/cancel",
            "reason"
          ],
          "found_facts": [
            "POST /executions/{id}/cancel"
          ],
          "missed_facts": [
            "reason"
          ],
          "coverage": 0.5,
          "expected_docs": [
            "api_executions"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "api_executions",
              "chunk_id": "api_executions_fix_1",
              "content": "**Parameters:** - `workflow_id` (string): Filter by workflow ID - `since` (datetime): Filter executions after this timestamp **Response:** - Success: `200` - Returns array of execution objects - Error: `400` - Bad Request **Example:** ```json {\"executions\": [{\"id\": \"ex_789\", \"status\": \"completed\", \"duration_ms\": 1523}]} ``` ### POST /executions/{id}/cancel Cancel a running execution.",
              "score": null
            },
            {
              "doc_id": "api_workflows",
              "chunk_id": "api_workflows_fix_1",
              "content": "**Parameters:** - `limit` (integer): Maximum number of results (default: 50, max: 200) - `status` (string): Filter by status: active, paused, archived **Response:** - Success: `200` - Returns array of workflow objects - Error: `400` - Bad Request **Example:** ```json {\"workflows\": [{\"id\": \"wf_123\", \"name\": \"Daily Report\", \"status\": \"active\"}]} ``` ### POST /workflows Create a new workflow.",
              "score": null
            },
            {
              "doc_id": "api_workflows",
              "chunk_id": "api_workflows_fix_2",
              "content": "**Parameters:** - `name` (string): Workflow name (required) - `definition` (object): Workflow definition in JSON format **Response:** - Success: `201` - Workflow created successfully - Error: `400` - Bad Request **Example:** ```json {\"id\": \"wf_456\", \"name\": \"New Workflow\", \"created_at\": \"2024-01-15T10:00:00Z\"} ``` ### DELETE /workflows/{id} Delete a workflow permanently. **Parameters:** - `id` (string): Workflow ID (required) **Response:** - Success: `204` - Workflow deleted - Error: `404` - Workflow not found **Example:** ```json {} ```",
              "score": null
            },
            {
              "doc_id": "api_workflows",
              "chunk_id": "api_workflows_fix_0",
              "content": "# Workflows API Reference ## Overview The Workflows API provides programmatic access to workflows functionality. Base URL: `https://api.cloudflow.io/v1/workflows` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /workflows List all workflows for the authenticated user.",
              "score": null
            },
            {
              "doc_id": "api_executions",
              "chunk_id": "api_executions_fix_0",
              "content": "# Executions API Reference ## Overview The Executions API provides programmatic access to executions functionality. Base URL: `https://api.cloudflow.io/v1/executions` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /executions List workflow executions.",
              "score": null
            }
          ],
          "latency_ms": 3.418189000512939,
          "query_id": "api_040",
          "dimension": "problem",
          "query": "workflow stuck running for hours, how do I terminate it via API"
        },
        {
          "key_facts": [
            "POST /executions/{id}/cancel",
            "reason"
          ],
          "found_facts": [
            "POST /executions/{id}/cancel",
            "reason"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "api_executions"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "api_executions",
              "chunk_id": "api_executions_fix_2",
              "content": "**Parameters:** - `id` (string): Execution ID - `reason` (string): Cancellation reason (optional) **Response:** - Success: `200` - Execution cancelled - Error: `400` - Bad Request **Example:** ```json {\"id\": \"ex_789\", \"status\": \"cancelled\", \"cancelled_at\": \"2024-01-15T10:05:00Z\"} ```",
              "score": null
            },
            {
              "doc_id": "api_executions",
              "chunk_id": "api_executions_fix_1",
              "content": "**Parameters:** - `workflow_id` (string): Filter by workflow ID - `since` (datetime): Filter executions after this timestamp **Response:** - Success: `200` - Returns array of execution objects - Error: `400` - Bad Request **Example:** ```json {\"executions\": [{\"id\": \"ex_789\", \"status\": \"completed\", \"duration_ms\": 1523}]} ``` ### POST /executions/{id}/cancel Cancel a running execution.",
              "score": null
            },
            {
              "doc_id": "howto_implement_retry_logic",
              "chunk_id": "howto_implement_retry_logic_fix_0",
              "content": "# How to Implement Retry Logic ## Prerequisites - Existing workflow ## Steps ### Step 1: Define retry policy Add retry configuration to your workflow step. > **Note:** Maximum 5 retry attempts allowed ### Step 2: Configure backoff Set exponential backoff with base delay of 1 second. The formula is: delay = base * 2^attempt. ### Step 3: Add timeout Set maximum execution time to prevent infinite loops. Default timeout is 30 seconds.",
              "score": null
            },
            {
              "doc_id": "api_executions",
              "chunk_id": "api_executions_fix_0",
              "content": "# Executions API Reference ## Overview The Executions API provides programmatic access to executions functionality. Base URL: `https://api.cloudflow.io/v1/executions` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /executions List workflow executions.",
              "score": null
            },
            {
              "doc_id": "howto_create_scheduled_workflow",
              "chunk_id": "howto_create_scheduled_workflow_fix_1",
              "content": "> **Note:** Use IANA timezone names like America/New_York ### Step 3: Configure overlap handling Choose behavior when previous execution is still running: skip, queue, or cancel. ### Step 4: Enable notifications Get notified on schedule failures. ```bash cloudflow workflows notify --on-failure ``` ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            }
          ],
          "latency_ms": 3.267017998950905,
          "query_id": "api_040",
          "dimension": "casual",
          "query": "cancel execution endpoint"
        },
        {
          "key_facts": [
            "POST /executions/{id}/cancel",
            "reason"
          ],
          "found_facts": [
            "POST /executions/{id}/cancel"
          ],
          "missed_facts": [
            "reason"
          ],
          "coverage": 0.5,
          "expected_docs": [
            "api_executions"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "arch_workflow_engine",
              "chunk_id": "arch_workflow_engine_fix_0",
              "content": "# Workflow Engine Architecture ## Overview The Workflow Engine is the core component responsible for executing user-defined workflows. It uses an event-driven architecture with support for parallel execution, conditional branching, and automatic retries. ## Design Principles - **Idempotency**: All operations can be safely retried without side effects - **Eventual Consistency**: State converges to consistent view within 5 seconds - **Fault Tolerance**: System continues operating despite component failures ## Components ### Scheduler Manages workflow scheduling and",
              "score": null
            },
            {
              "doc_id": "api_executions",
              "chunk_id": "api_executions_fix_1",
              "content": "**Parameters:** - `workflow_id` (string): Filter by workflow ID - `since` (datetime): Filter executions after this timestamp **Response:** - Success: `200` - Returns array of execution objects - Error: `400` - Bad Request **Example:** ```json {\"executions\": [{\"id\": \"ex_789\", \"status\": \"completed\", \"duration_ms\": 1523}]} ``` ### POST /executions/{id}/cancel Cancel a running execution.",
              "score": null
            },
            {
              "doc_id": "api_executions",
              "chunk_id": "api_executions_fix_0",
              "content": "# Executions API Reference ## Overview The Executions API provides programmatic access to executions functionality. Base URL: `https://api.cloudflow.io/v1/executions` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /executions List workflow executions.",
              "score": null
            },
            {
              "doc_id": "howto_implement_retry_logic",
              "chunk_id": "howto_implement_retry_logic_fix_0",
              "content": "# How to Implement Retry Logic ## Prerequisites - Existing workflow ## Steps ### Step 1: Define retry policy Add retry configuration to your workflow step. > **Note:** Maximum 5 retry attempts allowed ### Step 2: Configure backoff Set exponential backoff with base delay of 1 second. The formula is: delay = base * 2^attempt. ### Step 3: Add timeout Set maximum execution time to prevent infinite loops. Default timeout is 30 seconds.",
              "score": null
            },
            {
              "doc_id": "adr_005",
              "chunk_id": "adr_005_fix_0",
              "content": "# ADR-005: Adopt Event-Driven Architecture for Workflow Execution **Status:** Accepted **Date:** 2024-12-04 ## Context We need an architecture that supports asynchronous workflow execution, step dependencies, and failure recovery. ## Options Considered ### Option 1: Event-driven with Kafka Publish-subscribe messaging. **Pros:** - Loose coupling - Scalability - Event replay - Audit trail **Cons:** - Eventual consistency - Operational complexity - Message ordering challenges ### Option 2: Request-response Synchronous API calls.",
              "score": null
            }
          ],
          "latency_ms": 3.8583390014537144,
          "query_id": "api_040",
          "dimension": "contextual",
          "query": "building a kill switch for runaway workflows, what's the API to abort executions"
        },
        {
          "key_facts": [
            "POST /executions/{id}/cancel",
            "reason"
          ],
          "found_facts": [
            "reason"
          ],
          "missed_facts": [
            "POST /executions/{id}/cancel"
          ],
          "coverage": 0.5,
          "expected_docs": [
            "api_executions"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "runbook_database_connection_exhaustion",
              "chunk_id": "runbook_database_connection_exhaustion_fix_0",
              "content": "# Runbook: Database Connection Exhaustion ## Severity **Level:** P1 - Critical **Response Time:** 15 minutes ## Symptoms - Connection pool at 100% - Queries timing out - 'too many connections' errors in logs ## Diagnosis ### Step 1 Check active connections: `SELECT count(*) FROM pg_stat_activity` ### Step 2 Identify long-running queries: `SELECT pid, now() - pg_stat_activity.query_start AS duration, query FROM pg_stat_activity WHERE state != 'idle' ORDER BY duration DESC` ### Step 3 Check PgBouncer stats:",
              "score": null
            },
            {
              "doc_id": "runbook_out_of_memory_oom_kills",
              "chunk_id": "runbook_out_of_memory_oom_kills_fix_1",
              "content": "grep -A5 Limits` ### Step 4 Analyze heap dumps if available: `cloudflow debug heapdump --pod <pod-name>` ## Resolution ### Step 1 Increase memory limit: update deployment resources.limits.memory ### Step 2 Add memory request equal to limit to guarantee allocation ### Step 3 Enable JVM heap dump on OOM: add -XX:+HeapDumpOnOutOfMemoryError ### Step 4 Investigate memory leaks with profiling tools ### Step 5 Consider horizontal scaling instead of vertical ## Escalation If OOMs continue after limit",
              "score": null
            },
            {
              "doc_id": "runbook_api_gateway_5xx_errors",
              "chunk_id": "runbook_api_gateway_5xx_errors_fix_2",
              "content": "If 5xx persists > 15 minutes, page on-call. May indicate infrastructure issue. ## Post-Incident - Document timeline in incident log - Schedule post-mortem within 48 hours - Update runbook if new patterns discovered",
              "score": null
            },
            {
              "doc_id": "runbook_out_of_memory_oom_kills",
              "chunk_id": "runbook_out_of_memory_oom_kills_fix_0",
              "content": "# Runbook: Out of Memory (OOM) Kills ## Severity **Level:** P3 - Medium **Response Time:** 15 minutes ## Symptoms - Pods restarting with OOMKilled status - Memory usage at limit - Application slowdown before restart ## Diagnosis ### Step 1 Check OOM events: `kubectl get events -n production --field-selector reason=OOMKilled` ### Step 2 Review memory usage: `kubectl top pods -n production --sort-by=memory` ### Step 3 Check container limits: `kubectl describe pod <pod-name> -n production |",
              "score": null
            },
            {
              "doc_id": "runbook_out_of_memory_oom_kills",
              "chunk_id": "runbook_out_of_memory_oom_kills_fix_2",
              "content": "increase, escalate to development team for memory leak investigation. ## Post-Incident - Document timeline in incident log - Schedule post-mortem within 48 hours - Update runbook if new patterns discovered",
              "score": null
            }
          ],
          "latency_ms": 3.7568099996860838,
          "query_id": "api_040",
          "dimension": "negation",
          "query": "why can't I stop this execution that's taking forever"
        },
        {
          "key_facts": [
            "GET /users/me",
            "email",
            "role"
          ],
          "found_facts": [
            "GET /users/me",
            "email",
            "role"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "api_users"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "api_users",
              "chunk_id": "api_users_fix_0",
              "content": "# Users API Reference ## Overview The Users API provides programmatic access to users functionality. Base URL: `https://api.cloudflow.io/v1/users` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /users/me Get current user profile.",
              "score": null
            },
            {
              "doc_id": "api_users",
              "chunk_id": "api_users_fix_1",
              "content": "**Parameters:** **Response:** - Success: `200` - Returns user object - Error: `400` - Bad Request **Example:** ```json {\"id\": \"usr_001\", \"email\": \"user@example.com\", \"role\": \"admin\", \"created_at\": \"2023-06-01\"} ``` ### PATCH /users/me Update current user profile. **Parameters:** - `name` (string): Display name - `timezone` (string): Timezone (e.g., America/New_York) **Response:** - Success: `200` - User updated - Error: `400` - Bad Request **Example:** ```json {\"id\": \"usr_001\", \"name\": \"John Doe\", \"timezone\": \"America/New_York\"} ```",
              "score": null
            },
            {
              "doc_id": "api_metrics",
              "chunk_id": "api_metrics_fix_0",
              "content": "# Metrics API Reference ## Overview The Metrics API provides programmatic access to metrics functionality. Base URL: `https://api.cloudflow.io/v1/metrics` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /metrics/workflows Get workflow performance metrics.",
              "score": null
            },
            {
              "doc_id": "api_workflows",
              "chunk_id": "api_workflows_fix_0",
              "content": "# Workflows API Reference ## Overview The Workflows API provides programmatic access to workflows functionality. Base URL: `https://api.cloudflow.io/v1/workflows` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /workflows List all workflows for the authenticated user.",
              "score": null
            },
            {
              "doc_id": "api_billing",
              "chunk_id": "api_billing_fix_0",
              "content": "# Billing API Reference ## Overview The Billing API provides programmatic access to billing functionality. Base URL: `https://api.cloudflow.io/v1/billing` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /billing/usage Get current billing period usage.",
              "score": null
            }
          ],
          "latency_ms": 3.8062719995650696,
          "query_id": "api_041",
          "dimension": "original",
          "query": "How do I get current user profile?"
        },
        {
          "key_facts": [
            "GET /users/me",
            "email",
            "role"
          ],
          "found_facts": [
            "GET /users/me"
          ],
          "missed_facts": [
            "email",
            "role"
          ],
          "coverage": 0.3333333333333333,
          "expected_docs": [
            "api_users"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "api_users",
              "chunk_id": "api_users_fix_0",
              "content": "# Users API Reference ## Overview The Users API provides programmatic access to users functionality. Base URL: `https://api.cloudflow.io/v1/users` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /users/me Get current user profile.",
              "score": null
            },
            {
              "doc_id": "api_workflows",
              "chunk_id": "api_workflows_fix_0",
              "content": "# Workflows API Reference ## Overview The Workflows API provides programmatic access to workflows functionality. Base URL: `https://api.cloudflow.io/v1/workflows` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /workflows List all workflows for the authenticated user.",
              "score": null
            },
            {
              "doc_id": "api_audit",
              "chunk_id": "api_audit_fix_0",
              "content": "# Audit API Reference ## Overview The Audit API provides programmatic access to audit functionality. Base URL: `https://api.cloudflow.io/v1/audit` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /audit/logs Retrieve audit logs for compliance.",
              "score": null
            },
            {
              "doc_id": "adr_003",
              "chunk_id": "adr_003_fix_0",
              "content": "# ADR-003: Implement OAuth 2.0 with JWT for Authentication **Status:** Accepted **Date:** 2024-05-08 ## Context We need a secure authentication system that supports API keys, user sessions, and third-party integrations. ## Options Considered ### Option 1: OAuth 2.0 with JWT Industry standard token-based auth.",
              "score": null
            },
            {
              "doc_id": "arch_authentication_service",
              "chunk_id": "arch_authentication_service_fix_0",
              "content": "# Authentication Service Architecture ## Overview The Authentication Service handles user identity, session management, and token issuance. Supports OAuth 2.0, SAML, and API key authentication. ## Design Principles - **Zero Knowledge**: Passwords never stored in plaintext - **Token Rotation**: Refresh tokens rotate on each use ## Components ### Identity Provider Manages user accounts and credentials. Password hashing uses bcrypt with cost factor 12.",
              "score": null
            }
          ],
          "latency_ms": 3.5613559994089883,
          "query_id": "api_041",
          "dimension": "synonym",
          "query": "I need to retrieve the authenticated user's info from the API"
        },
        {
          "key_facts": [
            "GET /users/me",
            "email",
            "role"
          ],
          "found_facts": [
            "GET /users/me",
            "email",
            "role"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "api_users"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "api_users",
              "chunk_id": "api_users_fix_0",
              "content": "# Users API Reference ## Overview The Users API provides programmatic access to users functionality. Base URL: `https://api.cloudflow.io/v1/users` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /users/me Get current user profile.",
              "score": null
            },
            {
              "doc_id": "api_users",
              "chunk_id": "api_users_fix_1",
              "content": "**Parameters:** **Response:** - Success: `200` - Returns user object - Error: `400` - Bad Request **Example:** ```json {\"id\": \"usr_001\", \"email\": \"user@example.com\", \"role\": \"admin\", \"created_at\": \"2023-06-01\"} ``` ### PATCH /users/me Update current user profile. **Parameters:** - `name` (string): Display name - `timezone` (string): Timezone (e.g., America/New_York) **Response:** - Success: `200` - User updated - Error: `400` - Bad Request **Example:** ```json {\"id\": \"usr_001\", \"name\": \"John Doe\", \"timezone\": \"America/New_York\"} ```",
              "score": null
            },
            {
              "doc_id": "api_workflows",
              "chunk_id": "api_workflows_fix_0",
              "content": "# Workflows API Reference ## Overview The Workflows API provides programmatic access to workflows functionality. Base URL: `https://api.cloudflow.io/v1/workflows` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /workflows List all workflows for the authenticated user.",
              "score": null
            },
            {
              "doc_id": "api_secrets",
              "chunk_id": "api_secrets_fix_0",
              "content": "# Secrets API Reference ## Overview The Secrets API provides programmatic access to secrets functionality. Base URL: `https://api.cloudflow.io/v1/secrets` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /secrets List secret names (values not returned).",
              "score": null
            },
            {
              "doc_id": "api_billing",
              "chunk_id": "api_billing_fix_0",
              "content": "# Billing API Reference ## Overview The Billing API provides programmatic access to billing functionality. Base URL: `https://api.cloudflow.io/v1/billing` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /billing/usage Get current billing period usage.",
              "score": null
            }
          ],
          "latency_ms": 3.6538680014928104,
          "query_id": "api_041",
          "dimension": "problem",
          "query": "getting 401 when calling users endpoint, how do I fetch my own profile"
        },
        {
          "key_facts": [
            "GET /users/me",
            "email",
            "role"
          ],
          "found_facts": [
            "GET /users/me",
            "email",
            "role"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "api_users"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "api_users",
              "chunk_id": "api_users_fix_0",
              "content": "# Users API Reference ## Overview The Users API provides programmatic access to users functionality. Base URL: `https://api.cloudflow.io/v1/users` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /users/me Get current user profile.",
              "score": null
            },
            {
              "doc_id": "api_users",
              "chunk_id": "api_users_fix_1",
              "content": "**Parameters:** **Response:** - Success: `200` - Returns user object - Error: `400` - Bad Request **Example:** ```json {\"id\": \"usr_001\", \"email\": \"user@example.com\", \"role\": \"admin\", \"created_at\": \"2023-06-01\"} ``` ### PATCH /users/me Update current user profile. **Parameters:** - `name` (string): Display name - `timezone` (string): Timezone (e.g., America/New_York) **Response:** - Success: `200` - User updated - Error: `400` - Bad Request **Example:** ```json {\"id\": \"usr_001\", \"name\": \"John Doe\", \"timezone\": \"America/New_York\"} ```",
              "score": null
            },
            {
              "doc_id": "arch_authentication_service",
              "chunk_id": "arch_authentication_service_fix_0",
              "content": "# Authentication Service Architecture ## Overview The Authentication Service handles user identity, session management, and token issuance. Supports OAuth 2.0, SAML, and API key authentication. ## Design Principles - **Zero Knowledge**: Passwords never stored in plaintext - **Token Rotation**: Refresh tokens rotate on each use ## Components ### Identity Provider Manages user accounts and credentials. Password hashing uses bcrypt with cost factor 12.",
              "score": null
            },
            {
              "doc_id": "adr_003",
              "chunk_id": "adr_003_fix_2",
              "content": "**Pros:** - Very simple - Low overhead - Easy to implement **Cons:** - No user context - Hard to rotate - No SSO support ## Decision We will implement OAuth 2.0 with JWT tokens. Access tokens expire after 1 hour, refresh tokens after 7 days. ## Consequences - Implement token rotation on refresh - Support API keys for machine-to-machine auth - Add MFA for admin accounts",
              "score": null
            },
            {
              "doc_id": "arch_api_gateway",
              "chunk_id": "arch_api_gateway_fix_0",
              "content": "# API Gateway Architecture ## Overview The API Gateway handles all incoming HTTP requests, providing authentication, rate limiting, and request routing. Built on Kong with custom plugins for CloudFlow-specific functionality. ## Design Principles - **Zero Trust**: Every request is authenticated and authorized - **Defense in Depth**: Multiple layers of security validation ## Components ### Kong Gateway Primary ingress controller. Handles TLS termination, request routing, and plugin execution.",
              "score": null
            }
          ],
          "latency_ms": 3.675619000205188,
          "query_id": "api_041",
          "dimension": "casual",
          "query": "users me endpoint"
        },
        {
          "key_facts": [
            "GET /users/me",
            "email",
            "role"
          ],
          "found_facts": [
            "GET /users/me",
            "email",
            "role"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "api_users"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "api_users",
              "chunk_id": "api_users_fix_0",
              "content": "# Users API Reference ## Overview The Users API provides programmatic access to users functionality. Base URL: `https://api.cloudflow.io/v1/users` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /users/me Get current user profile.",
              "score": null
            },
            {
              "doc_id": "api_users",
              "chunk_id": "api_users_fix_1",
              "content": "**Parameters:** **Response:** - Success: `200` - Returns user object - Error: `400` - Bad Request **Example:** ```json {\"id\": \"usr_001\", \"email\": \"user@example.com\", \"role\": \"admin\", \"created_at\": \"2023-06-01\"} ``` ### PATCH /users/me Update current user profile. **Parameters:** - `name` (string): Display name - `timezone` (string): Timezone (e.g., America/New_York) **Response:** - Success: `200` - User updated - Error: `400` - Bad Request **Example:** ```json {\"id\": \"usr_001\", \"name\": \"John Doe\", \"timezone\": \"America/New_York\"} ```",
              "score": null
            },
            {
              "doc_id": "api_audit",
              "chunk_id": "api_audit_fix_1",
              "content": "**Parameters:** - `start_date` (date): Start date for log range - `end_date` (date): End date for log range - `actor` (string): Filter by user ID **Response:** - Success: `200` - Returns array of audit log entries - Error: `400` - Bad Request **Example:** ```json {\"logs\": [{\"timestamp\": \"2024-01-15T10:00:00Z\", \"actor\": \"usr_001\", \"action\": \"workflow.created\", \"resource\": \"wf_123\"}]} ```",
              "score": null
            },
            {
              "doc_id": "api_teams",
              "chunk_id": "api_teams_fix_1",
              "content": "**Parameters:** **Response:** - Success: `200` - Returns array of team objects - Error: `400` - Bad Request **Example:** ```json {\"teams\": [{\"id\": \"team_01\", \"name\": \"Engineering\", \"member_count\": 15}]} ``` ### POST /teams/{id}/members Add a member to a team. **Parameters:** - `user_id` (string): User ID to add - `role` (string): Role: owner, admin, member, viewer **Response:** - Success: `201` - Member added - Error: `400` - Bad Request **Example:** ```json {\"team_id\": \"team_01\", \"user_id\": \"usr_002\", \"role\": \"member\"} ```",
              "score": null
            },
            {
              "doc_id": "api_webhooks",
              "chunk_id": "api_webhooks_fix_0",
              "content": "# Webhooks API Reference ## Overview The Webhooks API provides programmatic access to webhooks functionality. Base URL: `https://api.cloudflow.io/v1/webhooks` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /webhooks List configured webhooks.",
              "score": null
            }
          ],
          "latency_ms": 3.444547999606584,
          "query_id": "api_041",
          "dimension": "contextual",
          "query": "building a profile page, need to pull the logged-in user's data from the API"
        },
        {
          "key_facts": [
            "GET /users/me",
            "email",
            "role"
          ],
          "found_facts": [
            "GET /users/me",
            "email",
            "role"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "api_users"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "api_users",
              "chunk_id": "api_users_fix_0",
              "content": "# Users API Reference ## Overview The Users API provides programmatic access to users functionality. Base URL: `https://api.cloudflow.io/v1/users` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /users/me Get current user profile.",
              "score": null
            },
            {
              "doc_id": "api_users",
              "chunk_id": "api_users_fix_1",
              "content": "**Parameters:** **Response:** - Success: `200` - Returns user object - Error: `400` - Bad Request **Example:** ```json {\"id\": \"usr_001\", \"email\": \"user@example.com\", \"role\": \"admin\", \"created_at\": \"2023-06-01\"} ``` ### PATCH /users/me Update current user profile. **Parameters:** - `name` (string): Display name - `timezone` (string): Timezone (e.g., America/New_York) **Response:** - Success: `200` - User updated - Error: `400` - Bad Request **Example:** ```json {\"id\": \"usr_001\", \"name\": \"John Doe\", \"timezone\": \"America/New_York\"} ```",
              "score": null
            },
            {
              "doc_id": "api_teams",
              "chunk_id": "api_teams_fix_1",
              "content": "**Parameters:** **Response:** - Success: `200` - Returns array of team objects - Error: `400` - Bad Request **Example:** ```json {\"teams\": [{\"id\": \"team_01\", \"name\": \"Engineering\", \"member_count\": 15}]} ``` ### POST /teams/{id}/members Add a member to a team. **Parameters:** - `user_id` (string): User ID to add - `role` (string): Role: owner, admin, member, viewer **Response:** - Success: `201` - Member added - Error: `400` - Bad Request **Example:** ```json {\"team_id\": \"team_01\", \"user_id\": \"usr_002\", \"role\": \"member\"} ```",
              "score": null
            },
            {
              "doc_id": "api_secrets",
              "chunk_id": "api_secrets_fix_2",
              "content": "**Parameters:** **Response:** - Success: `204` - Secret deleted - Error: `400` - Bad Request **Example:** ```json {} ```",
              "score": null
            },
            {
              "doc_id": "api_audit",
              "chunk_id": "api_audit_fix_1",
              "content": "**Parameters:** - `start_date` (date): Start date for log range - `end_date` (date): End date for log range - `actor` (string): Filter by user ID **Response:** - Success: `200` - Returns array of audit log entries - Error: `400` - Bad Request **Example:** ```json {\"logs\": [{\"timestamp\": \"2024-01-15T10:00:00Z\", \"actor\": \"usr_001\", \"action\": \"workflow.created\", \"resource\": \"wf_123\"}]} ```",
              "score": null
            }
          ],
          "latency_ms": 3.2598840007267427,
          "query_id": "api_041",
          "dimension": "negation",
          "query": "why doesn't the API return my user details"
        },
        {
          "key_facts": [
            "POST /teams/{id}/members",
            "user_id",
            "role"
          ],
          "found_facts": [
            "POST /teams/{id}/members",
            "user_id",
            "role"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "api_teams"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "api_teams",
              "chunk_id": "api_teams_fix_1",
              "content": "**Parameters:** **Response:** - Success: `200` - Returns array of team objects - Error: `400` - Bad Request **Example:** ```json {\"teams\": [{\"id\": \"team_01\", \"name\": \"Engineering\", \"member_count\": 15}]} ``` ### POST /teams/{id}/members Add a member to a team. **Parameters:** - `user_id` (string): User ID to add - `role` (string): Role: owner, admin, member, viewer **Response:** - Success: `201` - Member added - Error: `400` - Bad Request **Example:** ```json {\"team_id\": \"team_01\", \"user_id\": \"usr_002\", \"role\": \"member\"} ```",
              "score": null
            },
            {
              "doc_id": "api_teams",
              "chunk_id": "api_teams_fix_0",
              "content": "# Teams API Reference ## Overview The Teams API provides programmatic access to teams functionality. Base URL: `https://api.cloudflow.io/v1/teams` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /teams List teams the user belongs to.",
              "score": null
            },
            {
              "doc_id": "howto_configure_single_sign_on",
              "chunk_id": "howto_configure_single_sign_on_fix_0",
              "content": "# How to Configure Single Sign-On ## Prerequisites - Admin access - Enterprise plan - Identity provider access ## Steps ### Step 1: Choose provider CloudFlow supports SAML 2.0, OpenID Connect, and OAuth 2.0 providers. ### Step 2: Get metadata Download CloudFlow's SAML metadata for your IdP configuration. ```bash cloudflow sso metadata --format xml > cloudflow-metadata.xml ``` ### Step 3: Configure IdP Add CloudFlow as a service provider in your identity provider.",
              "score": null
            },
            {
              "doc_id": "api_users",
              "chunk_id": "api_users_fix_0",
              "content": "# Users API Reference ## Overview The Users API provides programmatic access to users functionality. Base URL: `https://api.cloudflow.io/v1/users` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /users/me Get current user profile.",
              "score": null
            },
            {
              "doc_id": "runbook_kafka_consumer_lag",
              "chunk_id": "runbook_kafka_consumer_lag_fix_2",
              "content": "team. May need Kafka cluster scaling. ## Post-Incident - Document timeline in incident log - Schedule post-mortem within 48 hours - Update runbook if new patterns discovered",
              "score": null
            }
          ],
          "latency_ms": 3.1834220008022385,
          "query_id": "api_042",
          "dimension": "original",
          "query": "How do I add a team member?"
        },
        {
          "key_facts": [
            "POST /teams/{id}/members",
            "user_id",
            "role"
          ],
          "found_facts": [
            "POST /teams/{id}/members",
            "user_id",
            "role"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "api_teams"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "api_teams",
              "chunk_id": "api_teams_fix_0",
              "content": "# Teams API Reference ## Overview The Teams API provides programmatic access to teams functionality. Base URL: `https://api.cloudflow.io/v1/teams` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /teams List teams the user belongs to.",
              "score": null
            },
            {
              "doc_id": "api_teams",
              "chunk_id": "api_teams_fix_1",
              "content": "**Parameters:** **Response:** - Success: `200` - Returns array of team objects - Error: `400` - Bad Request **Example:** ```json {\"teams\": [{\"id\": \"team_01\", \"name\": \"Engineering\", \"member_count\": 15}]} ``` ### POST /teams/{id}/members Add a member to a team. **Parameters:** - `user_id` (string): User ID to add - `role` (string): Role: owner, admin, member, viewer **Response:** - Success: `201` - Member added - Error: `400` - Bad Request **Example:** ```json {\"team_id\": \"team_01\", \"user_id\": \"usr_002\", \"role\": \"member\"} ```",
              "score": null
            },
            {
              "doc_id": "api_users",
              "chunk_id": "api_users_fix_0",
              "content": "# Users API Reference ## Overview The Users API provides programmatic access to users functionality. Base URL: `https://api.cloudflow.io/v1/users` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /users/me Get current user profile.",
              "score": null
            },
            {
              "doc_id": "api_webhooks",
              "chunk_id": "api_webhooks_fix_0",
              "content": "# Webhooks API Reference ## Overview The Webhooks API provides programmatic access to webhooks functionality. Base URL: `https://api.cloudflow.io/v1/webhooks` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /webhooks List configured webhooks.",
              "score": null
            },
            {
              "doc_id": "api_workflows",
              "chunk_id": "api_workflows_fix_0",
              "content": "# Workflows API Reference ## Overview The Workflows API provides programmatic access to workflows functionality. Base URL: `https://api.cloudflow.io/v1/workflows` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /workflows List all workflows for the authenticated user.",
              "score": null
            }
          ],
          "latency_ms": 3.2675879992893897,
          "query_id": "api_042",
          "dimension": "synonym",
          "query": "I need to invite a user to a team via the API"
        },
        {
          "key_facts": [
            "POST /teams/{id}/members",
            "user_id",
            "role"
          ],
          "found_facts": [
            "POST /teams/{id}/members",
            "user_id",
            "role"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "api_teams"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "api_teams",
              "chunk_id": "api_teams_fix_1",
              "content": "**Parameters:** **Response:** - Success: `200` - Returns array of team objects - Error: `400` - Bad Request **Example:** ```json {\"teams\": [{\"id\": \"team_01\", \"name\": \"Engineering\", \"member_count\": 15}]} ``` ### POST /teams/{id}/members Add a member to a team. **Parameters:** - `user_id` (string): User ID to add - `role` (string): Role: owner, admin, member, viewer **Response:** - Success: `201` - Member added - Error: `400` - Bad Request **Example:** ```json {\"team_id\": \"team_01\", \"user_id\": \"usr_002\", \"role\": \"member\"} ```",
              "score": null
            },
            {
              "doc_id": "api_secrets",
              "chunk_id": "api_secrets_fix_2",
              "content": "**Parameters:** **Response:** - Success: `204` - Secret deleted - Error: `400` - Bad Request **Example:** ```json {} ```",
              "score": null
            },
            {
              "doc_id": "api_users",
              "chunk_id": "api_users_fix_1",
              "content": "**Parameters:** **Response:** - Success: `200` - Returns user object - Error: `400` - Bad Request **Example:** ```json {\"id\": \"usr_001\", \"email\": \"user@example.com\", \"role\": \"admin\", \"created_at\": \"2023-06-01\"} ``` ### PATCH /users/me Update current user profile. **Parameters:** - `name` (string): Display name - `timezone` (string): Timezone (e.g., America/New_York) **Response:** - Success: `200` - User updated - Error: `400` - Bad Request **Example:** ```json {\"id\": \"usr_001\", \"name\": \"John Doe\", \"timezone\": \"America/New_York\"} ```",
              "score": null
            },
            {
              "doc_id": "api_metrics",
              "chunk_id": "api_metrics_fix_1",
              "content": "**Parameters:** - `period` (string): Time period: hour, day, week, month - `workflow_id` (string): Filter by workflow ID **Response:** - Success: `200` - Returns metrics data - Error: `400` - Bad Request **Example:** ```json {\"success_rate\": 0.985, \"avg_duration_ms\": 1250, \"p99_duration_ms\": 5000, \"total_executions\": 10000} ```",
              "score": null
            },
            {
              "doc_id": "api_executions",
              "chunk_id": "api_executions_fix_2",
              "content": "**Parameters:** - `id` (string): Execution ID - `reason` (string): Cancellation reason (optional) **Response:** - Success: `200` - Execution cancelled - Error: `400` - Bad Request **Example:** ```json {\"id\": \"ex_789\", \"status\": \"cancelled\", \"cancelled_at\": \"2024-01-15T10:05:00Z\"} ```",
              "score": null
            }
          ],
          "latency_ms": 4.37297799908265,
          "query_id": "api_042",
          "dimension": "problem",
          "query": "getting 400 when trying to add someone to my team, what parameters does the endpoint need"
        },
        {
          "key_facts": [
            "POST /teams/{id}/members",
            "user_id",
            "role"
          ],
          "found_facts": [
            "POST /teams/{id}/members",
            "user_id",
            "role"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "api_teams"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "api_teams",
              "chunk_id": "api_teams_fix_1",
              "content": "**Parameters:** **Response:** - Success: `200` - Returns array of team objects - Error: `400` - Bad Request **Example:** ```json {\"teams\": [{\"id\": \"team_01\", \"name\": \"Engineering\", \"member_count\": 15}]} ``` ### POST /teams/{id}/members Add a member to a team. **Parameters:** - `user_id` (string): User ID to add - `role` (string): Role: owner, admin, member, viewer **Response:** - Success: `201` - Member added - Error: `400` - Bad Request **Example:** ```json {\"team_id\": \"team_01\", \"user_id\": \"usr_002\", \"role\": \"member\"} ```",
              "score": null
            },
            {
              "doc_id": "api_teams",
              "chunk_id": "api_teams_fix_0",
              "content": "# Teams API Reference ## Overview The Teams API provides programmatic access to teams functionality. Base URL: `https://api.cloudflow.io/v1/teams` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /teams List teams the user belongs to.",
              "score": null
            },
            {
              "doc_id": "adr_002",
              "chunk_id": "adr_002_fix_2",
              "content": "## Consequences - Need platform team expertise - Implement GitOps with ArgoCD - Use managed node groups for easier operations",
              "score": null
            },
            {
              "doc_id": "adr_005",
              "chunk_id": "adr_005_fix_1",
              "content": "**Pros:** - Simple mental model - Immediate feedback - Easy debugging **Cons:** - Tight coupling - Cascading failures - Timeout issues ### Option 3: Polling-based Workers poll for tasks. **Pros:** - Simple implementation - Easy rate limiting - No message broker **Cons:** - Polling overhead - Higher latency - Inefficient at scale ## Decision We will use an event-driven architecture with Kafka for workflow step execution.",
              "score": null
            },
            {
              "doc_id": "api_users",
              "chunk_id": "api_users_fix_1",
              "content": "**Parameters:** **Response:** - Success: `200` - Returns user object - Error: `400` - Bad Request **Example:** ```json {\"id\": \"usr_001\", \"email\": \"user@example.com\", \"role\": \"admin\", \"created_at\": \"2023-06-01\"} ``` ### PATCH /users/me Update current user profile. **Parameters:** - `name` (string): Display name - `timezone` (string): Timezone (e.g., America/New_York) **Response:** - Success: `200` - User updated - Error: `400` - Bad Request **Example:** ```json {\"id\": \"usr_001\", \"name\": \"John Doe\", \"timezone\": \"America/New_York\"} ```",
              "score": null
            }
          ],
          "latency_ms": 3.48687699988659,
          "query_id": "api_042",
          "dimension": "casual",
          "query": "team members POST endpoint"
        },
        {
          "key_facts": [
            "POST /teams/{id}/members",
            "user_id",
            "role"
          ],
          "found_facts": [
            "POST /teams/{id}/members",
            "user_id",
            "role"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "api_teams"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "api_teams",
              "chunk_id": "api_teams_fix_1",
              "content": "**Parameters:** **Response:** - Success: `200` - Returns array of team objects - Error: `400` - Bad Request **Example:** ```json {\"teams\": [{\"id\": \"team_01\", \"name\": \"Engineering\", \"member_count\": 15}]} ``` ### POST /teams/{id}/members Add a member to a team. **Parameters:** - `user_id` (string): User ID to add - `role` (string): Role: owner, admin, member, viewer **Response:** - Success: `201` - Member added - Error: `400` - Bad Request **Example:** ```json {\"team_id\": \"team_01\", \"user_id\": \"usr_002\", \"role\": \"member\"} ```",
              "score": null
            },
            {
              "doc_id": "api_teams",
              "chunk_id": "api_teams_fix_0",
              "content": "# Teams API Reference ## Overview The Teams API provides programmatic access to teams functionality. Base URL: `https://api.cloudflow.io/v1/teams` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /teams List teams the user belongs to.",
              "score": null
            },
            {
              "doc_id": "howto_create_api_integration",
              "chunk_id": "howto_create_api_integration_fix_0",
              "content": "# How to Create API Integration ## Prerequisites - CloudFlow account ## Steps ### Step 1: Generate API key Create an API key for programmatic access. ```bash cloudflow apikeys create --name 'CI Integration' --scopes workflows:read,workflows:execute ``` ### Step 2: Store key securely Never commit API keys to source control. Use environment variables or secret management. > **Note:** Rotate keys every 90 days ### Step 3: Test authentication Verify the API key works correctly.",
              "score": null
            },
            {
              "doc_id": "api_workflows",
              "chunk_id": "api_workflows_fix_0",
              "content": "# Workflows API Reference ## Overview The Workflows API provides programmatic access to workflows functionality. Base URL: `https://api.cloudflow.io/v1/workflows` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /workflows List all workflows for the authenticated user.",
              "score": null
            },
            {
              "doc_id": "api_users",
              "chunk_id": "api_users_fix_0",
              "content": "# Users API Reference ## Overview The Users API provides programmatic access to users functionality. Base URL: `https://api.cloudflow.io/v1/users` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /users/me Get current user profile.",
              "score": null
            }
          ],
          "latency_ms": 3.7167760001466377,
          "query_id": "api_042",
          "dimension": "contextual",
          "query": "automating team onboarding, need the API call to add new members to a team"
        },
        {
          "key_facts": [
            "POST /teams/{id}/members",
            "user_id",
            "role"
          ],
          "found_facts": [
            "POST /teams/{id}/members",
            "user_id",
            "role"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "api_teams"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "api_teams",
              "chunk_id": "api_teams_fix_1",
              "content": "**Parameters:** **Response:** - Success: `200` - Returns array of team objects - Error: `400` - Bad Request **Example:** ```json {\"teams\": [{\"id\": \"team_01\", \"name\": \"Engineering\", \"member_count\": 15}]} ``` ### POST /teams/{id}/members Add a member to a team. **Parameters:** - `user_id` (string): User ID to add - `role` (string): Role: owner, admin, member, viewer **Response:** - Success: `201` - Member added - Error: `400` - Bad Request **Example:** ```json {\"team_id\": \"team_01\", \"user_id\": \"usr_002\", \"role\": \"member\"} ```",
              "score": null
            },
            {
              "doc_id": "api_teams",
              "chunk_id": "api_teams_fix_0",
              "content": "# Teams API Reference ## Overview The Teams API provides programmatic access to teams functionality. Base URL: `https://api.cloudflow.io/v1/teams` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /teams List teams the user belongs to.",
              "score": null
            },
            {
              "doc_id": "api_users",
              "chunk_id": "api_users_fix_1",
              "content": "**Parameters:** **Response:** - Success: `200` - Returns user object - Error: `400` - Bad Request **Example:** ```json {\"id\": \"usr_001\", \"email\": \"user@example.com\", \"role\": \"admin\", \"created_at\": \"2023-06-01\"} ``` ### PATCH /users/me Update current user profile. **Parameters:** - `name` (string): Display name - `timezone` (string): Timezone (e.g., America/New_York) **Response:** - Success: `200` - User updated - Error: `400` - Bad Request **Example:** ```json {\"id\": \"usr_001\", \"name\": \"John Doe\", \"timezone\": \"America/New_York\"} ```",
              "score": null
            },
            {
              "doc_id": "api_users",
              "chunk_id": "api_users_fix_0",
              "content": "# Users API Reference ## Overview The Users API provides programmatic access to users functionality. Base URL: `https://api.cloudflow.io/v1/users` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /users/me Get current user profile.",
              "score": null
            },
            {
              "doc_id": "api_secrets",
              "chunk_id": "api_secrets_fix_2",
              "content": "**Parameters:** **Response:** - Success: `204` - Secret deleted - Error: `400` - Bad Request **Example:** ```json {} ```",
              "score": null
            }
          ],
          "latency_ms": 3.9029819999996107,
          "query_id": "api_042",
          "dimension": "negation",
          "query": "why can't I add users to my team through the API"
        },
        {
          "key_facts": [
            "POST /webhooks",
            "url",
            "events",
            "secret"
          ],
          "found_facts": [
            "POST /webhooks",
            "url",
            "events",
            "secret"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "api_webhooks"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "api_webhooks",
              "chunk_id": "api_webhooks_fix_0",
              "content": "# Webhooks API Reference ## Overview The Webhooks API provides programmatic access to webhooks functionality. Base URL: `https://api.cloudflow.io/v1/webhooks` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /webhooks List configured webhooks.",
              "score": null
            },
            {
              "doc_id": "api_webhooks",
              "chunk_id": "api_webhooks_fix_2",
              "content": "**Parameters:** - `url` (string): Webhook URL (must be HTTPS) - `events` (array): Events to subscribe to - `secret` (string): Signing secret for verification **Response:** - Success: `201` - Webhook created - Error: `400` - Bad Request **Example:** ```json {\"id\": \"wh_02\", \"url\": \"https://example.com/new-hook\", \"secret\": \"whsec_...\"} ```",
              "score": null
            },
            {
              "doc_id": "api_webhooks",
              "chunk_id": "api_webhooks_fix_1",
              "content": "**Parameters:** - `active` (boolean): Filter by active status **Response:** - Success: `200` - Returns array of webhook objects - Error: `400` - Bad Request **Example:** ```json {\"webhooks\": [{\"id\": \"wh_01\", \"url\": \"https://example.com/hook\", \"events\": [\"workflow.completed\"]}]} ``` ### POST /webhooks Create a new webhook endpoint.",
              "score": null
            },
            {
              "doc_id": "arch_workflow_engine",
              "chunk_id": "arch_workflow_engine_fix_2",
              "content": "**Technology:** PostgreSQL with JSONB **Scaling:** Primary-replica with read replicas ## Data Flow Triggers \u2192 Scheduler \u2192 Job Queue \u2192 Executor \u2192 State Store \u2192 Webhooks/Notifications ## Performance Characteristics - **Latency P50:** 45ms - **Latency P99:** 250ms - **Throughput:** 5000 executions/minute",
              "score": null
            },
            {
              "doc_id": "release_3_0_0",
              "chunk_id": "release_3_0_0_fix_0",
              "content": "# Release Notes - v3.0.0 **Release Date:** 2023-10-15 ## New Features - Complete UI redesign - New workflow editor with visual builder - Real-time execution monitoring - Team workspaces ## Bug Fixes - Fixed critical security issue in token validation - Resolved data loss bug in failed migrations ## Breaking Changes - **BREAKING:** Authentication API completely rewritten - **BREAKING:** Legacy webhook format no longer supported - **BREAKING:** Minimum Node.js version now 18 ## Upgrade Instructions",
              "score": null
            }
          ],
          "latency_ms": 3.3365369999955874,
          "query_id": "api_043",
          "dimension": "original",
          "query": "How do I create a webhook?"
        },
        {
          "key_facts": [
            "POST /webhooks",
            "url",
            "events",
            "secret"
          ],
          "found_facts": [
            "url"
          ],
          "missed_facts": [
            "POST /webhooks",
            "events",
            "secret"
          ],
          "coverage": 0.25,
          "expected_docs": [
            "api_webhooks"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "api_workflows",
              "chunk_id": "api_workflows_fix_0",
              "content": "# Workflows API Reference ## Overview The Workflows API provides programmatic access to workflows functionality. Base URL: `https://api.cloudflow.io/v1/workflows` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /workflows List all workflows for the authenticated user.",
              "score": null
            },
            {
              "doc_id": "adr_005",
              "chunk_id": "adr_005_fix_0",
              "content": "# ADR-005: Adopt Event-Driven Architecture for Workflow Execution **Status:** Accepted **Date:** 2024-12-04 ## Context We need an architecture that supports asynchronous workflow execution, step dependencies, and failure recovery. ## Options Considered ### Option 1: Event-driven with Kafka Publish-subscribe messaging. **Pros:** - Loose coupling - Scalability - Event replay - Audit trail **Cons:** - Eventual consistency - Operational complexity - Message ordering challenges ### Option 2: Request-response Synchronous API calls.",
              "score": null
            },
            {
              "doc_id": "api_executions",
              "chunk_id": "api_executions_fix_0",
              "content": "# Executions API Reference ## Overview The Executions API provides programmatic access to executions functionality. Base URL: `https://api.cloudflow.io/v1/executions` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /executions List workflow executions.",
              "score": null
            },
            {
              "doc_id": "api_integrations",
              "chunk_id": "api_integrations_fix_0",
              "content": "# Integrations API Reference ## Overview The Integrations API provides programmatic access to integrations functionality. Base URL: `https://api.cloudflow.io/v1/integrations` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /integrations List available integrations.",
              "score": null
            },
            {
              "doc_id": "api_webhooks",
              "chunk_id": "api_webhooks_fix_0",
              "content": "# Webhooks API Reference ## Overview The Webhooks API provides programmatic access to webhooks functionality. Base URL: `https://api.cloudflow.io/v1/webhooks` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /webhooks List configured webhooks.",
              "score": null
            }
          ],
          "latency_ms": 3.4056660006172024,
          "query_id": "api_043",
          "dimension": "synonym",
          "query": "I need to register a callback URL for workflow events via the API"
        },
        {
          "key_facts": [
            "POST /webhooks",
            "url",
            "events",
            "secret"
          ],
          "found_facts": [
            "POST /webhooks",
            "url",
            "events",
            "secret"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "api_webhooks"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "api_webhooks",
              "chunk_id": "api_webhooks_fix_2",
              "content": "**Parameters:** - `url` (string): Webhook URL (must be HTTPS) - `events` (array): Events to subscribe to - `secret` (string): Signing secret for verification **Response:** - Success: `201` - Webhook created - Error: `400` - Bad Request **Example:** ```json {\"id\": \"wh_02\", \"url\": \"https://example.com/new-hook\", \"secret\": \"whsec_...\"} ```",
              "score": null
            },
            {
              "doc_id": "api_webhooks",
              "chunk_id": "api_webhooks_fix_1",
              "content": "**Parameters:** - `active` (boolean): Filter by active status **Response:** - Success: `200` - Returns array of webhook objects - Error: `400` - Bad Request **Example:** ```json {\"webhooks\": [{\"id\": \"wh_01\", \"url\": \"https://example.com/hook\", \"events\": [\"workflow.completed\"]}]} ``` ### POST /webhooks Create a new webhook endpoint.",
              "score": null
            },
            {
              "doc_id": "api_webhooks",
              "chunk_id": "api_webhooks_fix_0",
              "content": "# Webhooks API Reference ## Overview The Webhooks API provides programmatic access to webhooks functionality. Base URL: `https://api.cloudflow.io/v1/webhooks` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /webhooks List configured webhooks.",
              "score": null
            },
            {
              "doc_id": "release_3_0_0",
              "chunk_id": "release_3_0_0_fix_0",
              "content": "# Release Notes - v3.0.0 **Release Date:** 2023-10-15 ## New Features - Complete UI redesign - New workflow editor with visual builder - Real-time execution monitoring - Team workspaces ## Bug Fixes - Fixed critical security issue in token validation - Resolved data loss bug in failed migrations ## Breaking Changes - **BREAKING:** Authentication API completely rewritten - **BREAKING:** Legacy webhook format no longer supported - **BREAKING:** Minimum Node.js version now 18 ## Upgrade Instructions",
              "score": null
            },
            {
              "doc_id": "api_secrets",
              "chunk_id": "api_secrets_fix_2",
              "content": "**Parameters:** **Response:** - Success: `204` - Secret deleted - Error: `400` - Bad Request **Example:** ```json {} ```",
              "score": null
            }
          ],
          "latency_ms": 3.7037309994047973,
          "query_id": "api_043",
          "dimension": "problem",
          "query": "webhook creation failing with invalid URL error, what format does it expect"
        },
        {
          "key_facts": [
            "POST /webhooks",
            "url",
            "events",
            "secret"
          ],
          "found_facts": [
            "POST /webhooks",
            "url",
            "events",
            "secret"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "api_webhooks"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "api_webhooks",
              "chunk_id": "api_webhooks_fix_1",
              "content": "**Parameters:** - `active` (boolean): Filter by active status **Response:** - Success: `200` - Returns array of webhook objects - Error: `400` - Bad Request **Example:** ```json {\"webhooks\": [{\"id\": \"wh_01\", \"url\": \"https://example.com/hook\", \"events\": [\"workflow.completed\"]}]} ``` ### POST /webhooks Create a new webhook endpoint.",
              "score": null
            },
            {
              "doc_id": "api_webhooks",
              "chunk_id": "api_webhooks_fix_2",
              "content": "**Parameters:** - `url` (string): Webhook URL (must be HTTPS) - `events` (array): Events to subscribe to - `secret` (string): Signing secret for verification **Response:** - Success: `201` - Webhook created - Error: `400` - Bad Request **Example:** ```json {\"id\": \"wh_02\", \"url\": \"https://example.com/new-hook\", \"secret\": \"whsec_...\"} ```",
              "score": null
            },
            {
              "doc_id": "api_webhooks",
              "chunk_id": "api_webhooks_fix_0",
              "content": "# Webhooks API Reference ## Overview The Webhooks API provides programmatic access to webhooks functionality. Base URL: `https://api.cloudflow.io/v1/webhooks` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /webhooks List configured webhooks.",
              "score": null
            },
            {
              "doc_id": "arch_workflow_engine",
              "chunk_id": "arch_workflow_engine_fix_2",
              "content": "**Technology:** PostgreSQL with JSONB **Scaling:** Primary-replica with read replicas ## Data Flow Triggers \u2192 Scheduler \u2192 Job Queue \u2192 Executor \u2192 State Store \u2192 Webhooks/Notifications ## Performance Characteristics - **Latency P50:** 45ms - **Latency P99:** 250ms - **Throughput:** 5000 executions/minute",
              "score": null
            },
            {
              "doc_id": "release_3_0_0",
              "chunk_id": "release_3_0_0_fix_0",
              "content": "# Release Notes - v3.0.0 **Release Date:** 2023-10-15 ## New Features - Complete UI redesign - New workflow editor with visual builder - Real-time execution monitoring - Team workspaces ## Bug Fixes - Fixed critical security issue in token validation - Resolved data loss bug in failed migrations ## Breaking Changes - **BREAKING:** Authentication API completely rewritten - **BREAKING:** Legacy webhook format no longer supported - **BREAKING:** Minimum Node.js version now 18 ## Upgrade Instructions",
              "score": null
            }
          ],
          "latency_ms": 3.3193149993167026,
          "query_id": "api_043",
          "dimension": "casual",
          "query": "webhooks POST endpoint"
        },
        {
          "key_facts": [
            "POST /webhooks",
            "url",
            "events",
            "secret"
          ],
          "found_facts": [
            "POST /webhooks",
            "url",
            "events",
            "secret"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "api_webhooks"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "api_webhooks",
              "chunk_id": "api_webhooks_fix_1",
              "content": "**Parameters:** - `active` (boolean): Filter by active status **Response:** - Success: `200` - Returns array of webhook objects - Error: `400` - Bad Request **Example:** ```json {\"webhooks\": [{\"id\": \"wh_01\", \"url\": \"https://example.com/hook\", \"events\": [\"workflow.completed\"]}]} ``` ### POST /webhooks Create a new webhook endpoint.",
              "score": null
            },
            {
              "doc_id": "howto_set_up_monitoring_alerts",
              "chunk_id": "howto_set_up_monitoring_alerts_fix_0",
              "content": "# How to Set Up Monitoring Alerts ## Prerequisites - CloudFlow Pro plan or higher ## Steps ### Step 1: Define alert rules Create alert conditions based on metrics thresholds. ```bash cloudflow alerts create --name 'High Error Rate' --condition 'error_rate > 0.05' ``` ### Step 2: Configure channels Set up notification channels: email, Slack, or PagerDuty. ```bash cloudflow alerts channel add --type slack --webhook https://hooks.slack.com/...",
              "score": null
            },
            {
              "doc_id": "api_webhooks",
              "chunk_id": "api_webhooks_fix_2",
              "content": "**Parameters:** - `url` (string): Webhook URL (must be HTTPS) - `events` (array): Events to subscribe to - `secret` (string): Signing secret for verification **Response:** - Success: `201` - Webhook created - Error: `400` - Bad Request **Example:** ```json {\"id\": \"wh_02\", \"url\": \"https://example.com/new-hook\", \"secret\": \"whsec_...\"} ```",
              "score": null
            },
            {
              "doc_id": "api_webhooks",
              "chunk_id": "api_webhooks_fix_0",
              "content": "# Webhooks API Reference ## Overview The Webhooks API provides programmatic access to webhooks functionality. Base URL: `https://api.cloudflow.io/v1/webhooks` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /webhooks List configured webhooks.",
              "score": null
            },
            {
              "doc_id": "arch_notification_service",
              "chunk_id": "arch_notification_service_fix_0",
              "content": "# Notification Service Architecture ## Overview The Notification Service delivers alerts and updates through multiple channels including email, Slack, webhooks, and mobile push notifications. ## Design Principles - **Delivery Guarantee**: At-least-once delivery with deduplication - **Channel Abstraction**: Unified API regardless of delivery channel ## Components ### Dispatcher Routes notifications to appropriate channel handlers. Supports priority queues for urgent notifications. **Technology:** Go microservice **Scaling:** Horizontal with queue partitioning ### Email Provider Sends transactional emails.",
              "score": null
            }
          ],
          "latency_ms": 4.169760000877432,
          "query_id": "api_043",
          "dimension": "contextual",
          "query": "integrating with Slack to notify on workflow completion, need to set up a webhook endpoint"
        },
        {
          "key_facts": [
            "POST /webhooks",
            "url",
            "events",
            "secret"
          ],
          "found_facts": [
            "POST /webhooks",
            "url",
            "events",
            "secret"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "api_webhooks"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "api_webhooks",
              "chunk_id": "api_webhooks_fix_2",
              "content": "**Parameters:** - `url` (string): Webhook URL (must be HTTPS) - `events` (array): Events to subscribe to - `secret` (string): Signing secret for verification **Response:** - Success: `201` - Webhook created - Error: `400` - Bad Request **Example:** ```json {\"id\": \"wh_02\", \"url\": \"https://example.com/new-hook\", \"secret\": \"whsec_...\"} ```",
              "score": null
            },
            {
              "doc_id": "api_webhooks",
              "chunk_id": "api_webhooks_fix_1",
              "content": "**Parameters:** - `active` (boolean): Filter by active status **Response:** - Success: `200` - Returns array of webhook objects - Error: `400` - Bad Request **Example:** ```json {\"webhooks\": [{\"id\": \"wh_01\", \"url\": \"https://example.com/hook\", \"events\": [\"workflow.completed\"]}]} ``` ### POST /webhooks Create a new webhook endpoint.",
              "score": null
            },
            {
              "doc_id": "api_webhooks",
              "chunk_id": "api_webhooks_fix_0",
              "content": "# Webhooks API Reference ## Overview The Webhooks API provides programmatic access to webhooks functionality. Base URL: `https://api.cloudflow.io/v1/webhooks` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /webhooks List configured webhooks.",
              "score": null
            },
            {
              "doc_id": "release_3_0_0",
              "chunk_id": "release_3_0_0_fix_0",
              "content": "# Release Notes - v3.0.0 **Release Date:** 2023-10-15 ## New Features - Complete UI redesign - New workflow editor with visual builder - Real-time execution monitoring - Team workspaces ## Bug Fixes - Fixed critical security issue in token validation - Resolved data loss bug in failed migrations ## Breaking Changes - **BREAKING:** Authentication API completely rewritten - **BREAKING:** Legacy webhook format no longer supported - **BREAKING:** Minimum Node.js version now 18 ## Upgrade Instructions",
              "score": null
            },
            {
              "doc_id": "howto_set_up_monitoring_alerts",
              "chunk_id": "howto_set_up_monitoring_alerts_fix_0",
              "content": "# How to Set Up Monitoring Alerts ## Prerequisites - CloudFlow Pro plan or higher ## Steps ### Step 1: Define alert rules Create alert conditions based on metrics thresholds. ```bash cloudflow alerts create --name 'High Error Rate' --condition 'error_rate > 0.05' ``` ### Step 2: Configure channels Set up notification channels: email, Slack, or PagerDuty. ```bash cloudflow alerts channel add --type slack --webhook https://hooks.slack.com/...",
              "score": null
            }
          ],
          "latency_ms": 3.8592410001001554,
          "query_id": "api_043",
          "dimension": "negation",
          "query": "why isn't my webhook getting created, what am I missing"
        },
        {
          "key_facts": [
            "GET /billing/usage",
            "executions",
            "compute_minutes"
          ],
          "found_facts": [
            "GET /billing/usage",
            "executions",
            "compute_minutes"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "api_billing"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "api_billing",
              "chunk_id": "api_billing_fix_0",
              "content": "# Billing API Reference ## Overview The Billing API provides programmatic access to billing functionality. Base URL: `https://api.cloudflow.io/v1/billing` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /billing/usage Get current billing period usage.",
              "score": null
            },
            {
              "doc_id": "api_metrics",
              "chunk_id": "api_metrics_fix_0",
              "content": "# Metrics API Reference ## Overview The Metrics API provides programmatic access to metrics functionality. Base URL: `https://api.cloudflow.io/v1/metrics` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /metrics/workflows Get workflow performance metrics.",
              "score": null
            },
            {
              "doc_id": "api_billing",
              "chunk_id": "api_billing_fix_1",
              "content": "**Parameters:** - `breakdown` (boolean): Include detailed breakdown by resource **Response:** - Success: `200` - Returns usage summary - Error: `400` - Bad Request **Example:** ```json {\"period\": \"2024-01\", \"executions\": 15000, \"compute_minutes\": 2500, \"storage_gb\": 50} ``` ### GET /billing/invoices List past invoices. **Parameters:** - `year` (integer): Filter by year **Response:** - Success: `200` - Returns array of invoice objects - Error: `400` - Bad Request **Example:** ```json {\"invoices\": [{\"id\": \"inv_202401\", \"amount\": 299.00, \"status\": \"paid\", \"date\": \"2024-01-01\"}]} ```",
              "score": null
            },
            {
              "doc_id": "api_users",
              "chunk_id": "api_users_fix_0",
              "content": "# Users API Reference ## Overview The Users API provides programmatic access to users functionality. Base URL: `https://api.cloudflow.io/v1/users` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /users/me Get current user profile.",
              "score": null
            },
            {
              "doc_id": "api_audit",
              "chunk_id": "api_audit_fix_0",
              "content": "# Audit API Reference ## Overview The Audit API provides programmatic access to audit functionality. Base URL: `https://api.cloudflow.io/v1/audit` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /audit/logs Retrieve audit logs for compliance.",
              "score": null
            }
          ],
          "latency_ms": 3.3783149992814288,
          "query_id": "api_044",
          "dimension": "original",
          "query": "How do I get billing usage?"
        },
        {
          "key_facts": [
            "GET /billing/usage",
            "executions",
            "compute_minutes"
          ],
          "found_facts": [
            "GET /billing/usage",
            "executions",
            "compute_minutes"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "api_billing"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "api_billing",
              "chunk_id": "api_billing_fix_0",
              "content": "# Billing API Reference ## Overview The Billing API provides programmatic access to billing functionality. Base URL: `https://api.cloudflow.io/v1/billing` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /billing/usage Get current billing period usage.",
              "score": null
            },
            {
              "doc_id": "api_metrics",
              "chunk_id": "api_metrics_fix_0",
              "content": "# Metrics API Reference ## Overview The Metrics API provides programmatic access to metrics functionality. Base URL: `https://api.cloudflow.io/v1/metrics` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /metrics/workflows Get workflow performance metrics.",
              "score": null
            },
            {
              "doc_id": "api_billing",
              "chunk_id": "api_billing_fix_1",
              "content": "**Parameters:** - `breakdown` (boolean): Include detailed breakdown by resource **Response:** - Success: `200` - Returns usage summary - Error: `400` - Bad Request **Example:** ```json {\"period\": \"2024-01\", \"executions\": 15000, \"compute_minutes\": 2500, \"storage_gb\": 50} ``` ### GET /billing/invoices List past invoices. **Parameters:** - `year` (integer): Filter by year **Response:** - Success: `200` - Returns array of invoice objects - Error: `400` - Bad Request **Example:** ```json {\"invoices\": [{\"id\": \"inv_202401\", \"amount\": 299.00, \"status\": \"paid\", \"date\": \"2024-01-01\"}]} ```",
              "score": null
            },
            {
              "doc_id": "api_users",
              "chunk_id": "api_users_fix_0",
              "content": "# Users API Reference ## Overview The Users API provides programmatic access to users functionality. Base URL: `https://api.cloudflow.io/v1/users` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /users/me Get current user profile.",
              "score": null
            },
            {
              "doc_id": "api_audit",
              "chunk_id": "api_audit_fix_0",
              "content": "# Audit API Reference ## Overview The Audit API provides programmatic access to audit functionality. Base URL: `https://api.cloudflow.io/v1/audit` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /audit/logs Retrieve audit logs for compliance.",
              "score": null
            }
          ],
          "latency_ms": 3.349099999468308,
          "query_id": "api_044",
          "dimension": "synonym",
          "query": "I need to pull our consumption metrics from the billing API"
        },
        {
          "key_facts": [
            "GET /billing/usage",
            "executions",
            "compute_minutes"
          ],
          "found_facts": [
            "GET /billing/usage",
            "executions",
            "compute_minutes"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "api_billing"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "api_metrics",
              "chunk_id": "api_metrics_fix_0",
              "content": "# Metrics API Reference ## Overview The Metrics API provides programmatic access to metrics functionality. Base URL: `https://api.cloudflow.io/v1/metrics` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /metrics/workflows Get workflow performance metrics.",
              "score": null
            },
            {
              "doc_id": "api_billing",
              "chunk_id": "api_billing_fix_0",
              "content": "# Billing API Reference ## Overview The Billing API provides programmatic access to billing functionality. Base URL: `https://api.cloudflow.io/v1/billing` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /billing/usage Get current billing period usage.",
              "score": null
            },
            {
              "doc_id": "arch_monitoring_stack",
              "chunk_id": "arch_monitoring_stack_fix_1",
              "content": "Supports structured JSON logs with label indexing. **Technology:** Loki with Grafana **Scaling:** Distributed with sharding ### Tracing Distributed request tracing. Trace retention is 7 days. **Technology:** Jaeger with OpenTelemetry **Scaling:** Sampling at 10% ### Alerting Routes alerts to on-call engineers. Supports escalation policies and silencing.",
              "score": null
            },
            {
              "doc_id": "api_billing",
              "chunk_id": "api_billing_fix_1",
              "content": "**Parameters:** - `breakdown` (boolean): Include detailed breakdown by resource **Response:** - Success: `200` - Returns usage summary - Error: `400` - Bad Request **Example:** ```json {\"period\": \"2024-01\", \"executions\": 15000, \"compute_minutes\": 2500, \"storage_gb\": 50} ``` ### GET /billing/invoices List past invoices. **Parameters:** - `year` (integer): Filter by year **Response:** - Success: `200` - Returns array of invoice objects - Error: `400` - Bad Request **Example:** ```json {\"invoices\": [{\"id\": \"inv_202401\", \"amount\": 299.00, \"status\": \"paid\", \"date\": \"2024-01-01\"}]} ```",
              "score": null
            },
            {
              "doc_id": "api_audit",
              "chunk_id": "api_audit_fix_0",
              "content": "# Audit API Reference ## Overview The Audit API provides programmatic access to audit functionality. Base URL: `https://api.cloudflow.io/v1/audit` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /audit/logs Retrieve audit logs for compliance.",
              "score": null
            }
          ],
          "latency_ms": 3.4632830011105398,
          "query_id": "api_044",
          "dimension": "problem",
          "query": "need to track our API costs, where can I find usage data via API"
        },
        {
          "key_facts": [
            "GET /billing/usage",
            "executions",
            "compute_minutes"
          ],
          "found_facts": [
            "GET /billing/usage"
          ],
          "missed_facts": [
            "executions",
            "compute_minutes"
          ],
          "coverage": 0.3333333333333333,
          "expected_docs": [
            "api_billing"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "api_billing",
              "chunk_id": "api_billing_fix_0",
              "content": "# Billing API Reference ## Overview The Billing API provides programmatic access to billing functionality. Base URL: `https://api.cloudflow.io/v1/billing` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /billing/usage Get current billing period usage.",
              "score": null
            },
            {
              "doc_id": "adr_007",
              "chunk_id": "adr_007_fix_0",
              "content": "# ADR-007: Implement Rate Limiting at API Gateway **Status:** Accepted **Date:** 2024-09-03 ## Context We need to protect our services from abuse and ensure fair resource usage across customers. ## Options Considered ### Option 1: Sliding window at gateway Rate limit at API gateway level.",
              "score": null
            },
            {
              "doc_id": "arch_api_gateway",
              "chunk_id": "arch_api_gateway_fix_0",
              "content": "# API Gateway Architecture ## Overview The API Gateway handles all incoming HTTP requests, providing authentication, rate limiting, and request routing. Built on Kong with custom plugins for CloudFlow-specific functionality. ## Design Principles - **Zero Trust**: Every request is authenticated and authorized - **Defense in Depth**: Multiple layers of security validation ## Components ### Kong Gateway Primary ingress controller. Handles TLS termination, request routing, and plugin execution.",
              "score": null
            },
            {
              "doc_id": "arch_authentication_service",
              "chunk_id": "arch_authentication_service_fix_1",
              "content": "**Technology:** Custom Go service **Scaling:** Horizontal stateless ### Token Service Issues and validates access tokens. Token lifetime is 1 hour with 7-day refresh tokens. **Technology:** JWT with RS256 **Scaling:** Horizontal stateless ### SSO Gateway Integrates with external identity providers. Supports Google, Microsoft, Okta, and custom SAML.",
              "score": null
            },
            {
              "doc_id": "api_metrics",
              "chunk_id": "api_metrics_fix_0",
              "content": "# Metrics API Reference ## Overview The Metrics API provides programmatic access to metrics functionality. Base URL: `https://api.cloudflow.io/v1/metrics` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /metrics/workflows Get workflow performance metrics.",
              "score": null
            }
          ],
          "latency_ms": 3.2694419987819856,
          "query_id": "api_044",
          "dimension": "casual",
          "query": "billing usage endpoint"
        },
        {
          "key_facts": [
            "GET /billing/usage",
            "executions",
            "compute_minutes"
          ],
          "found_facts": [
            "GET /billing/usage",
            "executions",
            "compute_minutes"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "api_billing"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "api_billing",
              "chunk_id": "api_billing_fix_1",
              "content": "**Parameters:** - `breakdown` (boolean): Include detailed breakdown by resource **Response:** - Success: `200` - Returns usage summary - Error: `400` - Bad Request **Example:** ```json {\"period\": \"2024-01\", \"executions\": 15000, \"compute_minutes\": 2500, \"storage_gb\": 50} ``` ### GET /billing/invoices List past invoices. **Parameters:** - `year` (integer): Filter by year **Response:** - Success: `200` - Returns array of invoice objects - Error: `400` - Bad Request **Example:** ```json {\"invoices\": [{\"id\": \"inv_202401\", \"amount\": 299.00, \"status\": \"paid\", \"date\": \"2024-01-01\"}]} ```",
              "score": null
            },
            {
              "doc_id": "api_billing",
              "chunk_id": "api_billing_fix_0",
              "content": "# Billing API Reference ## Overview The Billing API provides programmatic access to billing functionality. Base URL: `https://api.cloudflow.io/v1/billing` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /billing/usage Get current billing period usage.",
              "score": null
            },
            {
              "doc_id": "api_metrics",
              "chunk_id": "api_metrics_fix_1",
              "content": "**Parameters:** - `period` (string): Time period: hour, day, week, month - `workflow_id` (string): Filter by workflow ID **Response:** - Success: `200` - Returns metrics data - Error: `400` - Bad Request **Example:** ```json {\"success_rate\": 0.985, \"avg_duration_ms\": 1250, \"p99_duration_ms\": 5000, \"total_executions\": 10000} ```",
              "score": null
            },
            {
              "doc_id": "api_metrics",
              "chunk_id": "api_metrics_fix_0",
              "content": "# Metrics API Reference ## Overview The Metrics API provides programmatic access to metrics functionality. Base URL: `https://api.cloudflow.io/v1/metrics` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /metrics/workflows Get workflow performance metrics.",
              "score": null
            },
            {
              "doc_id": "adr_007",
              "chunk_id": "adr_007_fix_0",
              "content": "# ADR-007: Implement Rate Limiting at API Gateway **Status:** Accepted **Date:** 2024-09-03 ## Context We need to protect our services from abuse and ensure fair resource usage across customers. ## Options Considered ### Option 1: Sliding window at gateway Rate limit at API gateway level.",
              "score": null
            }
          ],
          "latency_ms": 3.6455120007303776,
          "query_id": "api_044",
          "dimension": "contextual",
          "query": "building an internal cost dashboard, need to fetch monthly usage via API"
        },
        {
          "key_facts": [
            "GET /billing/usage",
            "executions",
            "compute_minutes"
          ],
          "found_facts": [
            "executions",
            "compute_minutes"
          ],
          "missed_facts": [
            "GET /billing/usage"
          ],
          "coverage": 0.6666666666666666,
          "expected_docs": [
            "api_billing"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "api_metrics",
              "chunk_id": "api_metrics_fix_0",
              "content": "# Metrics API Reference ## Overview The Metrics API provides programmatic access to metrics functionality. Base URL: `https://api.cloudflow.io/v1/metrics` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /metrics/workflows Get workflow performance metrics.",
              "score": null
            },
            {
              "doc_id": "api_metrics",
              "chunk_id": "api_metrics_fix_1",
              "content": "**Parameters:** - `period` (string): Time period: hour, day, week, month - `workflow_id` (string): Filter by workflow ID **Response:** - Success: `200` - Returns metrics data - Error: `400` - Bad Request **Example:** ```json {\"success_rate\": 0.985, \"avg_duration_ms\": 1250, \"p99_duration_ms\": 5000, \"total_executions\": 10000} ```",
              "score": null
            },
            {
              "doc_id": "api_audit",
              "chunk_id": "api_audit_fix_1",
              "content": "**Parameters:** - `start_date` (date): Start date for log range - `end_date` (date): End date for log range - `actor` (string): Filter by user ID **Response:** - Success: `200` - Returns array of audit log entries - Error: `400` - Bad Request **Example:** ```json {\"logs\": [{\"timestamp\": \"2024-01-15T10:00:00Z\", \"actor\": \"usr_001\", \"action\": \"workflow.created\", \"resource\": \"wf_123\"}]} ```",
              "score": null
            },
            {
              "doc_id": "api_billing",
              "chunk_id": "api_billing_fix_1",
              "content": "**Parameters:** - `breakdown` (boolean): Include detailed breakdown by resource **Response:** - Success: `200` - Returns usage summary - Error: `400` - Bad Request **Example:** ```json {\"period\": \"2024-01\", \"executions\": 15000, \"compute_minutes\": 2500, \"storage_gb\": 50} ``` ### GET /billing/invoices List past invoices. **Parameters:** - `year` (integer): Filter by year **Response:** - Success: `200` - Returns array of invoice objects - Error: `400` - Bad Request **Example:** ```json {\"invoices\": [{\"id\": \"inv_202401\", \"amount\": 299.00, \"status\": \"paid\", \"date\": \"2024-01-01\"}]} ```",
              "score": null
            },
            {
              "doc_id": "runbook_api_gateway_5xx_errors",
              "chunk_id": "runbook_api_gateway_5xx_errors_fix_0",
              "content": "# Runbook: API Gateway 5xx Errors ## Severity **Level:** P3 - Medium **Response Time:** 1 hour ## Symptoms - Error rate > 1% - 5xx responses in access logs - Downstream service failures ## Diagnosis ### Step 1 Check Kong error logs: `kubectl logs -l app=kong -n kong --tail=100 | grep -E '5[0-9]{2}'` ### Step 2 Verify backend health: `kubectl get pods -n production -o wide` ### Step 3 Check service endpoints: `kubectl get endpoints",
              "score": null
            }
          ],
          "latency_ms": 3.3781239999370882,
          "query_id": "api_044",
          "dimension": "negation",
          "query": "why can't I find our usage metrics in the API response"
        },
        {
          "key_facts": [
            "PUT /secrets/{name}",
            "DELETE /secrets/{name}",
            "encrypted at rest"
          ],
          "found_facts": [
            "PUT /secrets/{name}",
            "DELETE /secrets/{name}",
            "encrypted at rest"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "api_secrets"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "api_secrets",
              "chunk_id": "api_secrets_fix_0",
              "content": "# Secrets API Reference ## Overview The Secrets API provides programmatic access to secrets functionality. Base URL: `https://api.cloudflow.io/v1/secrets` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /secrets List secret names (values not returned).",
              "score": null
            },
            {
              "doc_id": "api_secrets",
              "chunk_id": "api_secrets_fix_1",
              "content": "**Parameters:** **Response:** - Success: `200` - Returns array of secret metadata - Error: `400` - Bad Request **Example:** ```json {\"secrets\": [{\"name\": \"DATABASE_URL\", \"created_at\": \"2024-01-01\", \"updated_at\": \"2024-01-10\"}]} ``` ### PUT /secrets/{name} Create or update a secret. **Parameters:** - `value` (string): Secret value (encrypted at rest) **Response:** - Success: `200` - Secret saved - Error: `400` - Bad Request **Example:** ```json {\"name\": \"DATABASE_URL\", \"updated_at\": \"2024-01-15T10:00:00Z\"} ``` ### DELETE /secrets/{name} Delete a secret.",
              "score": null
            },
            {
              "doc_id": "howto_set_up_ci_cd_pipeline",
              "chunk_id": "howto_set_up_ci_cd_pipeline_fix_0",
              "content": "# How to Set Up CI/CD Pipeline ## Prerequisites - GitHub account - CloudFlow API key - Repository admin access ## Steps ### Step 1: Create GitHub Actions workflow Create a new file at `.github/workflows/cloudflow.yml` with the deployment configuration. ```bash mkdir -p .github/workflows && touch .github/workflows/cloudflow.yml ``` ### Step 2: Configure secrets Add your CloudFlow API key to GitHub secrets.",
              "score": null
            },
            {
              "doc_id": "howto_set_up_data_encryption",
              "chunk_id": "howto_set_up_data_encryption_fix_0",
              "content": "# How to Set Up Data Encryption ## Prerequisites - CloudFlow Enterprise plan ## Steps ### Step 1: Enable encryption at rest All data is encrypted at rest using AES-256 by default. > **Note:** Encryption is automatic for all plans ### Step 2: Configure KMS Bring your own encryption keys using AWS KMS or GCP KMS. ```bash cloudflow encryption configure --kms-key arn:aws:kms:...",
              "score": null
            },
            {
              "doc_id": "howto_create_api_integration",
              "chunk_id": "howto_create_api_integration_fix_0",
              "content": "# How to Create API Integration ## Prerequisites - CloudFlow account ## Steps ### Step 1: Generate API key Create an API key for programmatic access. ```bash cloudflow apikeys create --name 'CI Integration' --scopes workflows:read,workflows:execute ``` ### Step 2: Store key securely Never commit API keys to source control. Use environment variables or secret management. > **Note:** Rotate keys every 90 days ### Step 3: Test authentication Verify the API key works correctly.",
              "score": null
            }
          ],
          "latency_ms": 3.1818890001886757,
          "query_id": "api_045",
          "dimension": "original",
          "query": "How do I manage secrets?"
        },
        {
          "key_facts": [
            "PUT /secrets/{name}",
            "DELETE /secrets/{name}",
            "encrypted at rest"
          ],
          "found_facts": [
            "PUT /secrets/{name}",
            "DELETE /secrets/{name}",
            "encrypted at rest"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "api_secrets"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "api_secrets",
              "chunk_id": "api_secrets_fix_1",
              "content": "**Parameters:** **Response:** - Success: `200` - Returns array of secret metadata - Error: `400` - Bad Request **Example:** ```json {\"secrets\": [{\"name\": \"DATABASE_URL\", \"created_at\": \"2024-01-01\", \"updated_at\": \"2024-01-10\"}]} ``` ### PUT /secrets/{name} Create or update a secret. **Parameters:** - `value` (string): Secret value (encrypted at rest) **Response:** - Success: `200` - Secret saved - Error: `400` - Bad Request **Example:** ```json {\"name\": \"DATABASE_URL\", \"updated_at\": \"2024-01-15T10:00:00Z\"} ``` ### DELETE /secrets/{name} Delete a secret.",
              "score": null
            },
            {
              "doc_id": "api_secrets",
              "chunk_id": "api_secrets_fix_0",
              "content": "# Secrets API Reference ## Overview The Secrets API provides programmatic access to secrets functionality. Base URL: `https://api.cloudflow.io/v1/secrets` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /secrets List secret names (values not returned).",
              "score": null
            },
            {
              "doc_id": "howto_create_api_integration",
              "chunk_id": "howto_create_api_integration_fix_0",
              "content": "# How to Create API Integration ## Prerequisites - CloudFlow account ## Steps ### Step 1: Generate API key Create an API key for programmatic access. ```bash cloudflow apikeys create --name 'CI Integration' --scopes workflows:read,workflows:execute ``` ### Step 2: Store key securely Never commit API keys to source control. Use environment variables or secret management. > **Note:** Rotate keys every 90 days ### Step 3: Test authentication Verify the API key works correctly.",
              "score": null
            },
            {
              "doc_id": "howto_set_up_ci_cd_pipeline",
              "chunk_id": "howto_set_up_ci_cd_pipeline_fix_0",
              "content": "# How to Set Up CI/CD Pipeline ## Prerequisites - GitHub account - CloudFlow API key - Repository admin access ## Steps ### Step 1: Create GitHub Actions workflow Create a new file at `.github/workflows/cloudflow.yml` with the deployment configuration. ```bash mkdir -p .github/workflows && touch .github/workflows/cloudflow.yml ``` ### Step 2: Configure secrets Add your CloudFlow API key to GitHub secrets.",
              "score": null
            },
            {
              "doc_id": "howto_set_up_ci_cd_pipeline",
              "chunk_id": "howto_set_up_ci_cd_pipeline_fix_1",
              "content": "```bash gh secret set CLOUDFLOW_API_KEY ``` ### Step 3: Add deployment step Add the CloudFlow deployment action to your workflow. > **Note:** Use the official cloudflow/deploy-action@v2 ### Step 4: Test the pipeline Push a commit to trigger the workflow. ```bash git push origin main ``` ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            }
          ],
          "latency_ms": 3.41740700059745,
          "query_id": "api_045",
          "dimension": "synonym",
          "query": "I need to store and update environment variables via the secrets API"
        },
        {
          "key_facts": [
            "PUT /secrets/{name}",
            "DELETE /secrets/{name}",
            "encrypted at rest"
          ],
          "found_facts": [
            "PUT /secrets/{name}",
            "DELETE /secrets/{name}",
            "encrypted at rest"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "api_secrets"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "api_secrets",
              "chunk_id": "api_secrets_fix_1",
              "content": "**Parameters:** **Response:** - Success: `200` - Returns array of secret metadata - Error: `400` - Bad Request **Example:** ```json {\"secrets\": [{\"name\": \"DATABASE_URL\", \"created_at\": \"2024-01-01\", \"updated_at\": \"2024-01-10\"}]} ``` ### PUT /secrets/{name} Create or update a secret. **Parameters:** - `value` (string): Secret value (encrypted at rest) **Response:** - Success: `200` - Secret saved - Error: `400` - Bad Request **Example:** ```json {\"name\": \"DATABASE_URL\", \"updated_at\": \"2024-01-15T10:00:00Z\"} ``` ### DELETE /secrets/{name} Delete a secret.",
              "score": null
            },
            {
              "doc_id": "api_secrets",
              "chunk_id": "api_secrets_fix_0",
              "content": "# Secrets API Reference ## Overview The Secrets API provides programmatic access to secrets functionality. Base URL: `https://api.cloudflow.io/v1/secrets` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /secrets List secret names (values not returned).",
              "score": null
            },
            {
              "doc_id": "howto_create_api_integration",
              "chunk_id": "howto_create_api_integration_fix_0",
              "content": "# How to Create API Integration ## Prerequisites - CloudFlow account ## Steps ### Step 1: Generate API key Create an API key for programmatic access. ```bash cloudflow apikeys create --name 'CI Integration' --scopes workflows:read,workflows:execute ``` ### Step 2: Store key securely Never commit API keys to source control. Use environment variables or secret management. > **Note:** Rotate keys every 90 days ### Step 3: Test authentication Verify the API key works correctly.",
              "score": null
            },
            {
              "doc_id": "arch_authentication_service",
              "chunk_id": "arch_authentication_service_fix_0",
              "content": "# Authentication Service Architecture ## Overview The Authentication Service handles user identity, session management, and token issuance. Supports OAuth 2.0, SAML, and API key authentication. ## Design Principles - **Zero Knowledge**: Passwords never stored in plaintext - **Token Rotation**: Refresh tokens rotate on each use ## Components ### Identity Provider Manages user accounts and credentials. Password hashing uses bcrypt with cost factor 12.",
              "score": null
            },
            {
              "doc_id": "howto_set_up_ci_cd_pipeline",
              "chunk_id": "howto_set_up_ci_cd_pipeline_fix_0",
              "content": "# How to Set Up CI/CD Pipeline ## Prerequisites - GitHub account - CloudFlow API key - Repository admin access ## Steps ### Step 1: Create GitHub Actions workflow Create a new file at `.github/workflows/cloudflow.yml` with the deployment configuration. ```bash mkdir -p .github/workflows && touch .github/workflows/cloudflow.yml ``` ### Step 2: Configure secrets Add your CloudFlow API key to GitHub secrets.",
              "score": null
            }
          ],
          "latency_ms": 3.7119959997653496,
          "query_id": "api_045",
          "dimension": "problem",
          "query": "need to rotate database credentials, what's the API for updating secrets"
        },
        {
          "key_facts": [
            "PUT /secrets/{name}",
            "DELETE /secrets/{name}",
            "encrypted at rest"
          ],
          "found_facts": [
            "PUT /secrets/{name}",
            "DELETE /secrets/{name}",
            "encrypted at rest"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "api_secrets"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "api_secrets",
              "chunk_id": "api_secrets_fix_1",
              "content": "**Parameters:** **Response:** - Success: `200` - Returns array of secret metadata - Error: `400` - Bad Request **Example:** ```json {\"secrets\": [{\"name\": \"DATABASE_URL\", \"created_at\": \"2024-01-01\", \"updated_at\": \"2024-01-10\"}]} ``` ### PUT /secrets/{name} Create or update a secret. **Parameters:** - `value` (string): Secret value (encrypted at rest) **Response:** - Success: `200` - Secret saved - Error: `400` - Bad Request **Example:** ```json {\"name\": \"DATABASE_URL\", \"updated_at\": \"2024-01-15T10:00:00Z\"} ``` ### DELETE /secrets/{name} Delete a secret.",
              "score": null
            },
            {
              "doc_id": "api_secrets",
              "chunk_id": "api_secrets_fix_0",
              "content": "# Secrets API Reference ## Overview The Secrets API provides programmatic access to secrets functionality. Base URL: `https://api.cloudflow.io/v1/secrets` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /secrets List secret names (values not returned).",
              "score": null
            },
            {
              "doc_id": "adr_008",
              "chunk_id": "adr_008_fix_2",
              "content": "## Consequences - Enable versioning for critical buckets - Implement lifecycle policies - Use pre-signed URLs for uploads - Enable server-side encryption",
              "score": null
            },
            {
              "doc_id": "arch_authentication_service",
              "chunk_id": "arch_authentication_service_fix_0",
              "content": "# Authentication Service Architecture ## Overview The Authentication Service handles user identity, session management, and token issuance. Supports OAuth 2.0, SAML, and API key authentication. ## Design Principles - **Zero Knowledge**: Passwords never stored in plaintext - **Token Rotation**: Refresh tokens rotate on each use ## Components ### Identity Provider Manages user accounts and credentials. Password hashing uses bcrypt with cost factor 12.",
              "score": null
            },
            {
              "doc_id": "api_secrets",
              "chunk_id": "api_secrets_fix_2",
              "content": "**Parameters:** **Response:** - Success: `204` - Secret deleted - Error: `400` - Bad Request **Example:** ```json {} ```",
              "score": null
            }
          ],
          "latency_ms": 3.327541000544443,
          "query_id": "api_045",
          "dimension": "casual",
          "query": "secrets CRUD endpoints"
        },
        {
          "key_facts": [
            "PUT /secrets/{name}",
            "DELETE /secrets/{name}",
            "encrypted at rest"
          ],
          "found_facts": [
            "PUT /secrets/{name}",
            "DELETE /secrets/{name}",
            "encrypted at rest"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "api_secrets"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "howto_create_api_integration",
              "chunk_id": "howto_create_api_integration_fix_0",
              "content": "# How to Create API Integration ## Prerequisites - CloudFlow account ## Steps ### Step 1: Generate API key Create an API key for programmatic access. ```bash cloudflow apikeys create --name 'CI Integration' --scopes workflows:read,workflows:execute ``` ### Step 2: Store key securely Never commit API keys to source control. Use environment variables or secret management. > **Note:** Rotate keys every 90 days ### Step 3: Test authentication Verify the API key works correctly.",
              "score": null
            },
            {
              "doc_id": "api_secrets",
              "chunk_id": "api_secrets_fix_0",
              "content": "# Secrets API Reference ## Overview The Secrets API provides programmatic access to secrets functionality. Base URL: `https://api.cloudflow.io/v1/secrets` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /secrets List secret names (values not returned).",
              "score": null
            },
            {
              "doc_id": "howto_set_up_ci_cd_pipeline",
              "chunk_id": "howto_set_up_ci_cd_pipeline_fix_0",
              "content": "# How to Set Up CI/CD Pipeline ## Prerequisites - GitHub account - CloudFlow API key - Repository admin access ## Steps ### Step 1: Create GitHub Actions workflow Create a new file at `.github/workflows/cloudflow.yml` with the deployment configuration. ```bash mkdir -p .github/workflows && touch .github/workflows/cloudflow.yml ``` ### Step 2: Configure secrets Add your CloudFlow API key to GitHub secrets.",
              "score": null
            },
            {
              "doc_id": "api_secrets",
              "chunk_id": "api_secrets_fix_1",
              "content": "**Parameters:** **Response:** - Success: `200` - Returns array of secret metadata - Error: `400` - Bad Request **Example:** ```json {\"secrets\": [{\"name\": \"DATABASE_URL\", \"created_at\": \"2024-01-01\", \"updated_at\": \"2024-01-10\"}]} ``` ### PUT /secrets/{name} Create or update a secret. **Parameters:** - `value` (string): Secret value (encrypted at rest) **Response:** - Success: `200` - Secret saved - Error: `400` - Bad Request **Example:** ```json {\"name\": \"DATABASE_URL\", \"updated_at\": \"2024-01-15T10:00:00Z\"} ``` ### DELETE /secrets/{name} Delete a secret.",
              "score": null
            },
            {
              "doc_id": "api_workflows",
              "chunk_id": "api_workflows_fix_0",
              "content": "# Workflows API Reference ## Overview The Workflows API provides programmatic access to workflows functionality. Base URL: `https://api.cloudflow.io/v1/workflows` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /workflows List all workflows for the authenticated user.",
              "score": null
            }
          ],
          "latency_ms": 3.630012999565224,
          "query_id": "api_045",
          "dimension": "contextual",
          "query": "automating secret rotation in CI/CD, need to PUT and DELETE secrets via API"
        },
        {
          "key_facts": [
            "PUT /secrets/{name}",
            "DELETE /secrets/{name}",
            "encrypted at rest"
          ],
          "found_facts": [
            "PUT /secrets/{name}",
            "DELETE /secrets/{name}",
            "encrypted at rest"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "api_secrets"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "api_secrets",
              "chunk_id": "api_secrets_fix_1",
              "content": "**Parameters:** **Response:** - Success: `200` - Returns array of secret metadata - Error: `400` - Bad Request **Example:** ```json {\"secrets\": [{\"name\": \"DATABASE_URL\", \"created_at\": \"2024-01-01\", \"updated_at\": \"2024-01-10\"}]} ``` ### PUT /secrets/{name} Create or update a secret. **Parameters:** - `value` (string): Secret value (encrypted at rest) **Response:** - Success: `200` - Secret saved - Error: `400` - Bad Request **Example:** ```json {\"name\": \"DATABASE_URL\", \"updated_at\": \"2024-01-15T10:00:00Z\"} ``` ### DELETE /secrets/{name} Delete a secret.",
              "score": null
            },
            {
              "doc_id": "api_secrets",
              "chunk_id": "api_secrets_fix_2",
              "content": "**Parameters:** **Response:** - Success: `204` - Secret deleted - Error: `400` - Bad Request **Example:** ```json {} ```",
              "score": null
            },
            {
              "doc_id": "api_secrets",
              "chunk_id": "api_secrets_fix_0",
              "content": "# Secrets API Reference ## Overview The Secrets API provides programmatic access to secrets functionality. Base URL: `https://api.cloudflow.io/v1/secrets` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /secrets List secret names (values not returned).",
              "score": null
            },
            {
              "doc_id": "api_workflows",
              "chunk_id": "api_workflows_fix_2",
              "content": "**Parameters:** - `name` (string): Workflow name (required) - `definition` (object): Workflow definition in JSON format **Response:** - Success: `201` - Workflow created successfully - Error: `400` - Bad Request **Example:** ```json {\"id\": \"wf_456\", \"name\": \"New Workflow\", \"created_at\": \"2024-01-15T10:00:00Z\"} ``` ### DELETE /workflows/{id} Delete a workflow permanently. **Parameters:** - `id` (string): Workflow ID (required) **Response:** - Success: `204` - Workflow deleted - Error: `404` - Workflow not found **Example:** ```json {} ```",
              "score": null
            },
            {
              "doc_id": "api_webhooks",
              "chunk_id": "api_webhooks_fix_2",
              "content": "**Parameters:** - `url` (string): Webhook URL (must be HTTPS) - `events` (array): Events to subscribe to - `secret` (string): Signing secret for verification **Response:** - Success: `201` - Webhook created - Error: `400` - Bad Request **Example:** ```json {\"id\": \"wh_02\", \"url\": \"https://example.com/new-hook\", \"secret\": \"whsec_...\"} ```",
              "score": null
            }
          ],
          "latency_ms": 3.2553250002820278,
          "query_id": "api_045",
          "dimension": "negation",
          "query": "why can't I delete old secrets from the API"
        },
        {
          "key_facts": [
            "Workflow versioning",
            "Python SDK",
            "async support",
            "v4.0"
          ],
          "found_facts": [
            "Workflow versioning",
            "Python SDK",
            "async support",
            "v4.0"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "release_3_2_0"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "release_3_2_0",
              "chunk_id": "release_3_2_0_fix_0",
              "content": "# Release Notes - v3.2.0 **Release Date:** 2024-01-15 ## New Features - Workflow versioning with rollback support - New Python SDK with async support - Bulk operations API for workflows - Custom retry policies per step ## Bug Fixes - Fixed race condition in concurrent execution handler - Resolved memory leak in long-running workflows - Fixed timezone handling in scheduled triggers ## Breaking Changes - **BREAKING:** API v1 endpoints deprecated, will be removed in v4.0",
              "score": null
            },
            {
              "doc_id": "release_3_0_0",
              "chunk_id": "release_3_0_0_fix_0",
              "content": "# Release Notes - v3.0.0 **Release Date:** 2023-10-15 ## New Features - Complete UI redesign - New workflow editor with visual builder - Real-time execution monitoring - Team workspaces ## Bug Fixes - Fixed critical security issue in token validation - Resolved data loss bug in failed migrations ## Breaking Changes - **BREAKING:** Authentication API completely rewritten - **BREAKING:** Legacy webhook format no longer supported - **BREAKING:** Minimum Node.js version now 18 ## Upgrade Instructions",
              "score": null
            },
            {
              "doc_id": "release_3_1_0",
              "chunk_id": "release_3_1_0_fix_0",
              "content": "# Release Notes - v3.1.0 **Release Date:** 2023-12-01 ## New Features - GraphQL API beta release - Workflow templates marketplace - Enhanced audit logging - SSO support for Okta ## Bug Fixes - Fixed webhook delivery retry logic - Resolved issue with large payload handling - Fixed dashboard loading performance ## Upgrade Instructions ```bash cloudflow upgrade --version 3.1.0 ``` ## Known Issues None at this time.",
              "score": null
            },
            {
              "doc_id": "adr_008",
              "chunk_id": "adr_008_fix_2",
              "content": "## Consequences - Enable versioning for critical buckets - Implement lifecycle policies - Use pre-signed URLs for uploads - Enable server-side encryption",
              "score": null
            },
            {
              "doc_id": "release_2_9_0",
              "chunk_id": "release_2_9_0_fix_0",
              "content": "# Release Notes - v2.9.0 **Release Date:** 2023-08-01 ## New Features - Parallel step execution - Conditional branching in workflows - Slack integration improvements - Performance dashboard ## Bug Fixes - Fixed issue with special characters in secrets - Resolved timeout in large file uploads ## Upgrade Instructions ```bash cloudflow upgrade --version 2.9.0 ``` ## Known Issues None at this time.",
              "score": null
            }
          ],
          "latency_ms": 3.201665998858516,
          "query_id": "release_046",
          "dimension": "original",
          "query": "What's new in version 3.2.0?"
        },
        {
          "key_facts": [
            "Workflow versioning",
            "Python SDK",
            "async support",
            "v4.0"
          ],
          "found_facts": [
            "Workflow versioning",
            "Python SDK",
            "async support",
            "v4.0"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "release_3_2_0"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "release_3_2_0",
              "chunk_id": "release_3_2_0_fix_0",
              "content": "# Release Notes - v3.2.0 **Release Date:** 2024-01-15 ## New Features - Workflow versioning with rollback support - New Python SDK with async support - Bulk operations API for workflows - Custom retry policies per step ## Bug Fixes - Fixed race condition in concurrent execution handler - Resolved memory leak in long-running workflows - Fixed timezone handling in scheduled triggers ## Breaking Changes - **BREAKING:** API v1 endpoints deprecated, will be removed in v4.0",
              "score": null
            },
            {
              "doc_id": "release_3_0_0",
              "chunk_id": "release_3_0_0_fix_0",
              "content": "# Release Notes - v3.0.0 **Release Date:** 2023-10-15 ## New Features - Complete UI redesign - New workflow editor with visual builder - Real-time execution monitoring - Team workspaces ## Bug Fixes - Fixed critical security issue in token validation - Resolved data loss bug in failed migrations ## Breaking Changes - **BREAKING:** Authentication API completely rewritten - **BREAKING:** Legacy webhook format no longer supported - **BREAKING:** Minimum Node.js version now 18 ## Upgrade Instructions",
              "score": null
            },
            {
              "doc_id": "release_3_1_0",
              "chunk_id": "release_3_1_0_fix_0",
              "content": "# Release Notes - v3.1.0 **Release Date:** 2023-12-01 ## New Features - GraphQL API beta release - Workflow templates marketplace - Enhanced audit logging - SSO support for Okta ## Bug Fixes - Fixed webhook delivery retry logic - Resolved issue with large payload handling - Fixed dashboard loading performance ## Upgrade Instructions ```bash cloudflow upgrade --version 3.1.0 ``` ## Known Issues None at this time.",
              "score": null
            },
            {
              "doc_id": "runbook_high_cpu_usage",
              "chunk_id": "runbook_high_cpu_usage_fix_2",
              "content": "high after scaling, escalate to platform team. Page on-call if affecting users. ## Post-Incident - Document timeline in incident log - Schedule post-mortem within 48 hours - Update runbook if new patterns discovered",
              "score": null
            },
            {
              "doc_id": "runbook_out_of_memory_oom_kills",
              "chunk_id": "runbook_out_of_memory_oom_kills_fix_2",
              "content": "increase, escalate to development team for memory leak investigation. ## Post-Incident - Document timeline in incident log - Schedule post-mortem within 48 hours - Update runbook if new patterns discovered",
              "score": null
            }
          ],
          "latency_ms": 3.3227510011784034,
          "query_id": "release_046",
          "dimension": "synonym",
          "query": "What changes and updates are included in the v3.2.0 changelog?"
        },
        {
          "key_facts": [
            "Workflow versioning",
            "Python SDK",
            "async support",
            "v4.0"
          ],
          "found_facts": [
            "Workflow versioning",
            "Python SDK",
            "async support",
            "v4.0"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "release_3_2_0"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "release_3_2_0",
              "chunk_id": "release_3_2_0_fix_0",
              "content": "# Release Notes - v3.2.0 **Release Date:** 2024-01-15 ## New Features - Workflow versioning with rollback support - New Python SDK with async support - Bulk operations API for workflows - Custom retry policies per step ## Bug Fixes - Fixed race condition in concurrent execution handler - Resolved memory leak in long-running workflows - Fixed timezone handling in scheduled triggers ## Breaking Changes - **BREAKING:** API v1 endpoints deprecated, will be removed in v4.0",
              "score": null
            },
            {
              "doc_id": "release_3_0_0",
              "chunk_id": "release_3_0_0_fix_0",
              "content": "# Release Notes - v3.0.0 **Release Date:** 2023-10-15 ## New Features - Complete UI redesign - New workflow editor with visual builder - Real-time execution monitoring - Team workspaces ## Bug Fixes - Fixed critical security issue in token validation - Resolved data loss bug in failed migrations ## Breaking Changes - **BREAKING:** Authentication API completely rewritten - **BREAKING:** Legacy webhook format no longer supported - **BREAKING:** Minimum Node.js version now 18 ## Upgrade Instructions",
              "score": null
            },
            {
              "doc_id": "release_3_1_0",
              "chunk_id": "release_3_1_0_fix_0",
              "content": "# Release Notes - v3.1.0 **Release Date:** 2023-12-01 ## New Features - GraphQL API beta release - Workflow templates marketplace - Enhanced audit logging - SSO support for Okta ## Bug Fixes - Fixed webhook delivery retry logic - Resolved issue with large payload handling - Fixed dashboard loading performance ## Upgrade Instructions ```bash cloudflow upgrade --version 3.1.0 ``` ## Known Issues None at this time.",
              "score": null
            },
            {
              "doc_id": "release_2_9_0",
              "chunk_id": "release_2_9_0_fix_0",
              "content": "# Release Notes - v2.9.0 **Release Date:** 2023-08-01 ## New Features - Parallel step execution - Conditional branching in workflows - Slack integration improvements - Performance dashboard ## Bug Fixes - Fixed issue with special characters in secrets - Resolved timeout in large file uploads ## Upgrade Instructions ```bash cloudflow upgrade --version 2.9.0 ``` ## Known Issues None at this time.",
              "score": null
            },
            {
              "doc_id": "howto_migrate_from_v1_to_v2_api",
              "chunk_id": "howto_migrate_from_v1_to_v2_api_fix_0",
              "content": "# How to Migrate from v1 to v2 API ## Prerequisites - Existing v1 integration - v2 API access enabled ## Steps ### Step 1: Review breaking changes The v2 API changes authentication from API keys to OAuth 2.0. Review the migration guide. ### Step 2: Update client library Upgrade to the latest SDK version. ```bash npm install @cloudflow/sdk@latest ``` ### Step 3: Update authentication Replace API key authentication with OAuth 2.0 client credentials flow.",
              "score": null
            }
          ],
          "latency_ms": 3.33195800158137,
          "query_id": "release_046",
          "dimension": "problem",
          "query": "I upgraded to 3.2.0 and now the API v1 calls are showing deprecation warnings, what happened?"
        },
        {
          "key_facts": [
            "Workflow versioning",
            "Python SDK",
            "async support",
            "v4.0"
          ],
          "found_facts": [
            "Workflow versioning",
            "Python SDK",
            "async support",
            "v4.0"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "release_3_2_0"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "release_3_2_0",
              "chunk_id": "release_3_2_0_fix_0",
              "content": "# Release Notes - v3.2.0 **Release Date:** 2024-01-15 ## New Features - Workflow versioning with rollback support - New Python SDK with async support - Bulk operations API for workflows - Custom retry policies per step ## Bug Fixes - Fixed race condition in concurrent execution handler - Resolved memory leak in long-running workflows - Fixed timezone handling in scheduled triggers ## Breaking Changes - **BREAKING:** API v1 endpoints deprecated, will be removed in v4.0",
              "score": null
            },
            {
              "doc_id": "release_3_0_0",
              "chunk_id": "release_3_0_0_fix_0",
              "content": "# Release Notes - v3.0.0 **Release Date:** 2023-10-15 ## New Features - Complete UI redesign - New workflow editor with visual builder - Real-time execution monitoring - Team workspaces ## Bug Fixes - Fixed critical security issue in token validation - Resolved data loss bug in failed migrations ## Breaking Changes - **BREAKING:** Authentication API completely rewritten - **BREAKING:** Legacy webhook format no longer supported - **BREAKING:** Minimum Node.js version now 18 ## Upgrade Instructions",
              "score": null
            },
            {
              "doc_id": "release_3_1_0",
              "chunk_id": "release_3_1_0_fix_0",
              "content": "# Release Notes - v3.1.0 **Release Date:** 2023-12-01 ## New Features - GraphQL API beta release - Workflow templates marketplace - Enhanced audit logging - SSO support for Okta ## Bug Fixes - Fixed webhook delivery retry logic - Resolved issue with large payload handling - Fixed dashboard loading performance ## Upgrade Instructions ```bash cloudflow upgrade --version 3.1.0 ``` ## Known Issues None at this time.",
              "score": null
            },
            {
              "doc_id": "adr_008",
              "chunk_id": "adr_008_fix_2",
              "content": "## Consequences - Enable versioning for critical buckets - Implement lifecycle policies - Use pre-signed URLs for uploads - Enable server-side encryption",
              "score": null
            },
            {
              "doc_id": "release_2_9_0",
              "chunk_id": "release_2_9_0_fix_0",
              "content": "# Release Notes - v2.9.0 **Release Date:** 2023-08-01 ## New Features - Parallel step execution - Conditional branching in workflows - Slack integration improvements - Performance dashboard ## Bug Fixes - Fixed issue with special characters in secrets - Resolved timeout in large file uploads ## Upgrade Instructions ```bash cloudflow upgrade --version 2.9.0 ``` ## Known Issues None at this time.",
              "score": null
            }
          ],
          "latency_ms": 3.554713000994525,
          "query_id": "release_046",
          "dimension": "casual",
          "query": "3.2.0 new features"
        },
        {
          "key_facts": [
            "Workflow versioning",
            "Python SDK",
            "async support",
            "v4.0"
          ],
          "found_facts": [
            "Workflow versioning",
            "Python SDK",
            "async support",
            "v4.0"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "release_3_2_0"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "release_3_2_0",
              "chunk_id": "release_3_2_0_fix_0",
              "content": "# Release Notes - v3.2.0 **Release Date:** 2024-01-15 ## New Features - Workflow versioning with rollback support - New Python SDK with async support - Bulk operations API for workflows - Custom retry policies per step ## Bug Fixes - Fixed race condition in concurrent execution handler - Resolved memory leak in long-running workflows - Fixed timezone handling in scheduled triggers ## Breaking Changes - **BREAKING:** API v1 endpoints deprecated, will be removed in v4.0",
              "score": null
            },
            {
              "doc_id": "release_3_0_0",
              "chunk_id": "release_3_0_0_fix_0",
              "content": "# Release Notes - v3.0.0 **Release Date:** 2023-10-15 ## New Features - Complete UI redesign - New workflow editor with visual builder - Real-time execution monitoring - Team workspaces ## Bug Fixes - Fixed critical security issue in token validation - Resolved data loss bug in failed migrations ## Breaking Changes - **BREAKING:** Authentication API completely rewritten - **BREAKING:** Legacy webhook format no longer supported - **BREAKING:** Minimum Node.js version now 18 ## Upgrade Instructions",
              "score": null
            },
            {
              "doc_id": "release_3_1_0",
              "chunk_id": "release_3_1_0_fix_0",
              "content": "# Release Notes - v3.1.0 **Release Date:** 2023-12-01 ## New Features - GraphQL API beta release - Workflow templates marketplace - Enhanced audit logging - SSO support for Okta ## Bug Fixes - Fixed webhook delivery retry logic - Resolved issue with large payload handling - Fixed dashboard loading performance ## Upgrade Instructions ```bash cloudflow upgrade --version 3.1.0 ``` ## Known Issues None at this time.",
              "score": null
            },
            {
              "doc_id": "release_2_9_0",
              "chunk_id": "release_2_9_0_fix_0",
              "content": "# Release Notes - v2.9.0 **Release Date:** 2023-08-01 ## New Features - Parallel step execution - Conditional branching in workflows - Slack integration improvements - Performance dashboard ## Bug Fixes - Fixed issue with special characters in secrets - Resolved timeout in large file uploads ## Upgrade Instructions ```bash cloudflow upgrade --version 2.9.0 ``` ## Known Issues None at this time.",
              "score": null
            },
            {
              "doc_id": "howto_implement_workflow_versioning",
              "chunk_id": "howto_implement_workflow_versioning_fix_0",
              "content": "# How to Implement Workflow Versioning ## Prerequisites - Existing workflow ## Steps ### Step 1: Enable versioning Turn on versioning for your workflow. This is required for the feature. ```bash cloudflow workflows update wf_123 --enable-versioning ``` ### Step 2: Create new version Save changes as a new version instead of overwriting. ```bash cloudflow workflows publish wf_123 --version 2.0.0 ``` ### Step 3: Set active version Choose which version handles new executions.",
              "score": null
            }
          ],
          "latency_ms": 3.543804001310491,
          "query_id": "release_046",
          "dimension": "contextual",
          "query": "We're evaluating upgrading from 3.1 to 3.2.0, what new capabilities did they add like workflow versioning or SDK improvements?"
        },
        {
          "key_facts": [
            "Workflow versioning",
            "Python SDK",
            "async support",
            "v4.0"
          ],
          "found_facts": [
            "Workflow versioning",
            "Python SDK",
            "async support",
            "v4.0"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "release_3_2_0"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "release_3_2_0",
              "chunk_id": "release_3_2_0_fix_0",
              "content": "# Release Notes - v3.2.0 **Release Date:** 2024-01-15 ## New Features - Workflow versioning with rollback support - New Python SDK with async support - Bulk operations API for workflows - Custom retry policies per step ## Bug Fixes - Fixed race condition in concurrent execution handler - Resolved memory leak in long-running workflows - Fixed timezone handling in scheduled triggers ## Breaking Changes - **BREAKING:** API v1 endpoints deprecated, will be removed in v4.0",
              "score": null
            },
            {
              "doc_id": "release_3_1_0",
              "chunk_id": "release_3_1_0_fix_0",
              "content": "# Release Notes - v3.1.0 **Release Date:** 2023-12-01 ## New Features - GraphQL API beta release - Workflow templates marketplace - Enhanced audit logging - SSO support for Okta ## Bug Fixes - Fixed webhook delivery retry logic - Resolved issue with large payload handling - Fixed dashboard loading performance ## Upgrade Instructions ```bash cloudflow upgrade --version 3.1.0 ``` ## Known Issues None at this time.",
              "score": null
            },
            {
              "doc_id": "howto_implement_workflow_versioning",
              "chunk_id": "howto_implement_workflow_versioning_fix_1",
              "content": "```bash cloudflow workflows activate wf_123 --version 2.0.0 ``` ### Step 4: Roll back if needed Quickly revert to a previous version. ```bash cloudflow workflows rollback wf_123 --to-version 1.0.0 ``` ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            },
            {
              "doc_id": "howto_implement_workflow_versioning",
              "chunk_id": "howto_implement_workflow_versioning_fix_0",
              "content": "# How to Implement Workflow Versioning ## Prerequisites - Existing workflow ## Steps ### Step 1: Enable versioning Turn on versioning for your workflow. This is required for the feature. ```bash cloudflow workflows update wf_123 --enable-versioning ``` ### Step 2: Create new version Save changes as a new version instead of overwriting. ```bash cloudflow workflows publish wf_123 --version 2.0.0 ``` ### Step 3: Set active version Choose which version handles new executions.",
              "score": null
            },
            {
              "doc_id": "release_2_9_0",
              "chunk_id": "release_2_9_0_fix_0",
              "content": "# Release Notes - v2.9.0 **Release Date:** 2023-08-01 ## New Features - Parallel step execution - Conditional branching in workflows - Slack integration improvements - Performance dashboard ## Bug Fixes - Fixed issue with special characters in secrets - Resolved timeout in large file uploads ## Upgrade Instructions ```bash cloudflow upgrade --version 2.9.0 ``` ## Known Issues None at this time.",
              "score": null
            }
          ],
          "latency_ms": 3.746090000277036,
          "query_id": "release_046",
          "dimension": "negation",
          "query": "Why doesn't version 3.1 have workflow rollback support?"
        },
        {
          "key_facts": [
            "critical security issue",
            "token validation"
          ],
          "found_facts": [
            "critical security issue",
            "token validation"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "release_3_0_0"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "release_3_2_0",
              "chunk_id": "release_3_2_0_fix_0",
              "content": "# Release Notes - v3.2.0 **Release Date:** 2024-01-15 ## New Features - Workflow versioning with rollback support - New Python SDK with async support - Bulk operations API for workflows - Custom retry policies per step ## Bug Fixes - Fixed race condition in concurrent execution handler - Resolved memory leak in long-running workflows - Fixed timezone handling in scheduled triggers ## Breaking Changes - **BREAKING:** API v1 endpoints deprecated, will be removed in v4.0",
              "score": null
            },
            {
              "doc_id": "release_3_0_0",
              "chunk_id": "release_3_0_0_fix_0",
              "content": "# Release Notes - v3.0.0 **Release Date:** 2023-10-15 ## New Features - Complete UI redesign - New workflow editor with visual builder - Real-time execution monitoring - Team workspaces ## Bug Fixes - Fixed critical security issue in token validation - Resolved data loss bug in failed migrations ## Breaking Changes - **BREAKING:** Authentication API completely rewritten - **BREAKING:** Legacy webhook format no longer supported - **BREAKING:** Minimum Node.js version now 18 ## Upgrade Instructions",
              "score": null
            },
            {
              "doc_id": "release_3_1_0",
              "chunk_id": "release_3_1_0_fix_0",
              "content": "# Release Notes - v3.1.0 **Release Date:** 2023-12-01 ## New Features - GraphQL API beta release - Workflow templates marketplace - Enhanced audit logging - SSO support for Okta ## Bug Fixes - Fixed webhook delivery retry logic - Resolved issue with large payload handling - Fixed dashboard loading performance ## Upgrade Instructions ```bash cloudflow upgrade --version 3.1.0 ``` ## Known Issues None at this time.",
              "score": null
            },
            {
              "doc_id": "adr_008",
              "chunk_id": "adr_008_fix_2",
              "content": "## Consequences - Enable versioning for critical buckets - Implement lifecycle policies - Use pre-signed URLs for uploads - Enable server-side encryption",
              "score": null
            },
            {
              "doc_id": "runbook_out_of_memory_oom_kills",
              "chunk_id": "runbook_out_of_memory_oom_kills_fix_2",
              "content": "increase, escalate to development team for memory leak investigation. ## Post-Incident - Document timeline in incident log - Schedule post-mortem within 48 hours - Update runbook if new patterns discovered",
              "score": null
            }
          ],
          "latency_ms": 3.59531899994181,
          "query_id": "release_047",
          "dimension": "original",
          "query": "What security fixes were in v3.0.0?"
        },
        {
          "key_facts": [
            "critical security issue",
            "token validation"
          ],
          "found_facts": [
            "critical security issue",
            "token validation"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "release_3_0_0"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "release_3_2_0",
              "chunk_id": "release_3_2_0_fix_0",
              "content": "# Release Notes - v3.2.0 **Release Date:** 2024-01-15 ## New Features - Workflow versioning with rollback support - New Python SDK with async support - Bulk operations API for workflows - Custom retry policies per step ## Bug Fixes - Fixed race condition in concurrent execution handler - Resolved memory leak in long-running workflows - Fixed timezone handling in scheduled triggers ## Breaking Changes - **BREAKING:** API v1 endpoints deprecated, will be removed in v4.0",
              "score": null
            },
            {
              "doc_id": "release_3_0_0",
              "chunk_id": "release_3_0_0_fix_0",
              "content": "# Release Notes - v3.0.0 **Release Date:** 2023-10-15 ## New Features - Complete UI redesign - New workflow editor with visual builder - Real-time execution monitoring - Team workspaces ## Bug Fixes - Fixed critical security issue in token validation - Resolved data loss bug in failed migrations ## Breaking Changes - **BREAKING:** Authentication API completely rewritten - **BREAKING:** Legacy webhook format no longer supported - **BREAKING:** Minimum Node.js version now 18 ## Upgrade Instructions",
              "score": null
            },
            {
              "doc_id": "release_3_1_0",
              "chunk_id": "release_3_1_0_fix_0",
              "content": "# Release Notes - v3.1.0 **Release Date:** 2023-12-01 ## New Features - GraphQL API beta release - Workflow templates marketplace - Enhanced audit logging - SSO support for Okta ## Bug Fixes - Fixed webhook delivery retry logic - Resolved issue with large payload handling - Fixed dashboard loading performance ## Upgrade Instructions ```bash cloudflow upgrade --version 3.1.0 ``` ## Known Issues None at this time.",
              "score": null
            },
            {
              "doc_id": "release_2_9_0",
              "chunk_id": "release_2_9_0_fix_0",
              "content": "# Release Notes - v2.9.0 **Release Date:** 2023-08-01 ## New Features - Parallel step execution - Conditional branching in workflows - Slack integration improvements - Performance dashboard ## Bug Fixes - Fixed issue with special characters in secrets - Resolved timeout in large file uploads ## Upgrade Instructions ```bash cloudflow upgrade --version 2.9.0 ``` ## Known Issues None at this time.",
              "score": null
            },
            {
              "doc_id": "runbook_out_of_memory_oom_kills",
              "chunk_id": "runbook_out_of_memory_oom_kills_fix_2",
              "content": "increase, escalate to development team for memory leak investigation. ## Post-Incident - Document timeline in incident log - Schedule post-mortem within 48 hours - Update runbook if new patterns discovered",
              "score": null
            }
          ],
          "latency_ms": 3.412568001294858,
          "query_id": "release_047",
          "dimension": "synonym",
          "query": "What security patches and vulnerability fixes were shipped in release 3.0.0?"
        },
        {
          "key_facts": [
            "critical security issue",
            "token validation"
          ],
          "found_facts": [
            "token validation"
          ],
          "missed_facts": [
            "critical security issue"
          ],
          "coverage": 0.5,
          "expected_docs": [
            "release_3_0_0"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "adr_003",
              "chunk_id": "adr_003_fix_1",
              "content": "**Pros:** - Industry standard - Stateless validation - Supports refresh tokens - SSO compatible **Cons:** - Token size overhead - Cannot revoke individual tokens - Complexity ### Option 2: Session-based auth Traditional server-side sessions. **Pros:** - Simple implementation - Easy revocation - Smaller payloads **Cons:** - Requires session store - Not suitable for APIs - Scaling challenges ### Option 3: API keys only Simple key-based authentication.",
              "score": null
            },
            {
              "doc_id": "arch_authentication_service",
              "chunk_id": "arch_authentication_service_fix_2",
              "content": "**Technology:** Dex OIDC proxy **Scaling:** Active-passive ## Data Flow Credentials \u2192 Validation \u2192 Token Issue \u2192 Token Refresh \u2192 Logout ## Performance Characteristics - **Latency P50:** 50ms - **Latency P99:** 200ms - **Throughput:** 5000 auth/second",
              "score": null
            },
            {
              "doc_id": "arch_authentication_service",
              "chunk_id": "arch_authentication_service_fix_1",
              "content": "**Technology:** Custom Go service **Scaling:** Horizontal stateless ### Token Service Issues and validates access tokens. Token lifetime is 1 hour with 7-day refresh tokens. **Technology:** JWT with RS256 **Scaling:** Horizontal stateless ### SSO Gateway Integrates with external identity providers. Supports Google, Microsoft, Okta, and custom SAML.",
              "score": null
            },
            {
              "doc_id": "arch_authentication_service",
              "chunk_id": "arch_authentication_service_fix_0",
              "content": "# Authentication Service Architecture ## Overview The Authentication Service handles user identity, session management, and token issuance. Supports OAuth 2.0, SAML, and API key authentication. ## Design Principles - **Zero Knowledge**: Passwords never stored in plaintext - **Token Rotation**: Refresh tokens rotate on each use ## Components ### Identity Provider Manages user accounts and credentials. Password hashing uses bcrypt with cost factor 12.",
              "score": null
            },
            {
              "doc_id": "arch_api_gateway",
              "chunk_id": "arch_api_gateway_fix_1",
              "content": "**Technology:** Kong 3.4 on Kubernetes **Scaling:** Horizontal with HPA ### Auth Plugin Validates JWT tokens, API keys, and OAuth2 flows. Token validation latency under 5ms. **Technology:** Custom Lua plugin **Scaling:** Stateless ### Rate Limiter Enforces per-user and per-IP rate limits. Default limit is 100 requests per minute.",
              "score": null
            }
          ],
          "latency_ms": 3.534585999659612,
          "query_id": "release_047",
          "dimension": "problem",
          "query": "Our security team flagged a token validation issue, was this addressed in any recent updates?"
        },
        {
          "key_facts": [
            "critical security issue",
            "token validation"
          ],
          "found_facts": [
            "critical security issue",
            "token validation"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "release_3_0_0"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "release_3_0_0",
              "chunk_id": "release_3_0_0_fix_0",
              "content": "# Release Notes - v3.0.0 **Release Date:** 2023-10-15 ## New Features - Complete UI redesign - New workflow editor with visual builder - Real-time execution monitoring - Team workspaces ## Bug Fixes - Fixed critical security issue in token validation - Resolved data loss bug in failed migrations ## Breaking Changes - **BREAKING:** Authentication API completely rewritten - **BREAKING:** Legacy webhook format no longer supported - **BREAKING:** Minimum Node.js version now 18 ## Upgrade Instructions",
              "score": null
            },
            {
              "doc_id": "release_3_2_0",
              "chunk_id": "release_3_2_0_fix_0",
              "content": "# Release Notes - v3.2.0 **Release Date:** 2024-01-15 ## New Features - Workflow versioning with rollback support - New Python SDK with async support - Bulk operations API for workflows - Custom retry policies per step ## Bug Fixes - Fixed race condition in concurrent execution handler - Resolved memory leak in long-running workflows - Fixed timezone handling in scheduled triggers ## Breaking Changes - **BREAKING:** API v1 endpoints deprecated, will be removed in v4.0",
              "score": null
            },
            {
              "doc_id": "release_3_1_0",
              "chunk_id": "release_3_1_0_fix_0",
              "content": "# Release Notes - v3.1.0 **Release Date:** 2023-12-01 ## New Features - GraphQL API beta release - Workflow templates marketplace - Enhanced audit logging - SSO support for Okta ## Bug Fixes - Fixed webhook delivery retry logic - Resolved issue with large payload handling - Fixed dashboard loading performance ## Upgrade Instructions ```bash cloudflow upgrade --version 3.1.0 ``` ## Known Issues None at this time.",
              "score": null
            },
            {
              "doc_id": "adr_008",
              "chunk_id": "adr_008_fix_2",
              "content": "## Consequences - Enable versioning for critical buckets - Implement lifecycle policies - Use pre-signed URLs for uploads - Enable server-side encryption",
              "score": null
            },
            {
              "doc_id": "release_3_0_0",
              "chunk_id": "release_3_0_0_fix_1",
              "content": "```bash cloudflow upgrade --version 3.0.0 ``` ## Known Issues None at this time.",
              "score": null
            }
          ],
          "latency_ms": 3.583396999601973,
          "query_id": "release_047",
          "dimension": "casual",
          "query": "v3.0.0 security fixes"
        },
        {
          "key_facts": [
            "critical security issue",
            "token validation"
          ],
          "found_facts": [
            "critical security issue",
            "token validation"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "release_3_0_0"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "release_3_2_0",
              "chunk_id": "release_3_2_0_fix_0",
              "content": "# Release Notes - v3.2.0 **Release Date:** 2024-01-15 ## New Features - Workflow versioning with rollback support - New Python SDK with async support - Bulk operations API for workflows - Custom retry policies per step ## Bug Fixes - Fixed race condition in concurrent execution handler - Resolved memory leak in long-running workflows - Fixed timezone handling in scheduled triggers ## Breaking Changes - **BREAKING:** API v1 endpoints deprecated, will be removed in v4.0",
              "score": null
            },
            {
              "doc_id": "release_3_0_0",
              "chunk_id": "release_3_0_0_fix_0",
              "content": "# Release Notes - v3.0.0 **Release Date:** 2023-10-15 ## New Features - Complete UI redesign - New workflow editor with visual builder - Real-time execution monitoring - Team workspaces ## Bug Fixes - Fixed critical security issue in token validation - Resolved data loss bug in failed migrations ## Breaking Changes - **BREAKING:** Authentication API completely rewritten - **BREAKING:** Legacy webhook format no longer supported - **BREAKING:** Minimum Node.js version now 18 ## Upgrade Instructions",
              "score": null
            },
            {
              "doc_id": "release_3_1_0",
              "chunk_id": "release_3_1_0_fix_0",
              "content": "# Release Notes - v3.1.0 **Release Date:** 2023-12-01 ## New Features - GraphQL API beta release - Workflow templates marketplace - Enhanced audit logging - SSO support for Okta ## Bug Fixes - Fixed webhook delivery retry logic - Resolved issue with large payload handling - Fixed dashboard loading performance ## Upgrade Instructions ```bash cloudflow upgrade --version 3.1.0 ``` ## Known Issues None at this time.",
              "score": null
            },
            {
              "doc_id": "release_2_9_0",
              "chunk_id": "release_2_9_0_fix_0",
              "content": "# Release Notes - v2.9.0 **Release Date:** 2023-08-01 ## New Features - Parallel step execution - Conditional branching in workflows - Slack integration improvements - Performance dashboard ## Bug Fixes - Fixed issue with special characters in secrets - Resolved timeout in large file uploads ## Upgrade Instructions ```bash cloudflow upgrade --version 2.9.0 ``` ## Known Issues None at this time.",
              "score": null
            },
            {
              "doc_id": "arch_authentication_service",
              "chunk_id": "arch_authentication_service_fix_2",
              "content": "**Technology:** Dex OIDC proxy **Scaling:** Active-passive ## Data Flow Credentials \u2192 Validation \u2192 Token Issue \u2192 Token Refresh \u2192 Logout ## Performance Characteristics - **Latency P50:** 50ms - **Latency P99:** 200ms - **Throughput:** 5000 auth/second",
              "score": null
            }
          ],
          "latency_ms": 3.663166000478668,
          "query_id": "release_047",
          "dimension": "contextual",
          "query": "Running a security audit and need to verify if the token validation vulnerability was fixed, which version has that patch?"
        },
        {
          "key_facts": [
            "critical security issue",
            "token validation"
          ],
          "found_facts": [
            "critical security issue",
            "token validation"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "release_3_0_0"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "adr_003",
              "chunk_id": "adr_003_fix_1",
              "content": "**Pros:** - Industry standard - Stateless validation - Supports refresh tokens - SSO compatible **Cons:** - Token size overhead - Cannot revoke individual tokens - Complexity ### Option 2: Session-based auth Traditional server-side sessions. **Pros:** - Simple implementation - Easy revocation - Smaller payloads **Cons:** - Requires session store - Not suitable for APIs - Scaling challenges ### Option 3: API keys only Simple key-based authentication.",
              "score": null
            },
            {
              "doc_id": "arch_authentication_service",
              "chunk_id": "arch_authentication_service_fix_2",
              "content": "**Technology:** Dex OIDC proxy **Scaling:** Active-passive ## Data Flow Credentials \u2192 Validation \u2192 Token Issue \u2192 Token Refresh \u2192 Logout ## Performance Characteristics - **Latency P50:** 50ms - **Latency P99:** 200ms - **Throughput:** 5000 auth/second",
              "score": null
            },
            {
              "doc_id": "release_3_2_0",
              "chunk_id": "release_3_2_0_fix_0",
              "content": "# Release Notes - v3.2.0 **Release Date:** 2024-01-15 ## New Features - Workflow versioning with rollback support - New Python SDK with async support - Bulk operations API for workflows - Custom retry policies per step ## Bug Fixes - Fixed race condition in concurrent execution handler - Resolved memory leak in long-running workflows - Fixed timezone handling in scheduled triggers ## Breaking Changes - **BREAKING:** API v1 endpoints deprecated, will be removed in v4.0",
              "score": null
            },
            {
              "doc_id": "release_3_0_0",
              "chunk_id": "release_3_0_0_fix_0",
              "content": "# Release Notes - v3.0.0 **Release Date:** 2023-10-15 ## New Features - Complete UI redesign - New workflow editor with visual builder - Real-time execution monitoring - Team workspaces ## Bug Fixes - Fixed critical security issue in token validation - Resolved data loss bug in failed migrations ## Breaking Changes - **BREAKING:** Authentication API completely rewritten - **BREAKING:** Legacy webhook format no longer supported - **BREAKING:** Minimum Node.js version now 18 ## Upgrade Instructions",
              "score": null
            },
            {
              "doc_id": "arch_authentication_service",
              "chunk_id": "arch_authentication_service_fix_1",
              "content": "**Technology:** Custom Go service **Scaling:** Horizontal stateless ### Token Service Issues and validates access tokens. Token lifetime is 1 hour with 7-day refresh tokens. **Technology:** JWT with RS256 **Scaling:** Horizontal stateless ### SSO Gateway Integrates with external identity providers. Supports Google, Microsoft, Okta, and custom SAML.",
              "score": null
            }
          ],
          "latency_ms": 3.412908999962383,
          "query_id": "release_047",
          "dimension": "negation",
          "query": "Is there a known security vulnerability in token validation that hasn't been patched?"
        },
        {
          "key_facts": [
            "Authentication API",
            "Legacy webhook format",
            "Node.js version now 18"
          ],
          "found_facts": [
            "Authentication API",
            "Legacy webhook format",
            "Node.js version now 18"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "release_3_0_0"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "release_3_2_0",
              "chunk_id": "release_3_2_0_fix_0",
              "content": "# Release Notes - v3.2.0 **Release Date:** 2024-01-15 ## New Features - Workflow versioning with rollback support - New Python SDK with async support - Bulk operations API for workflows - Custom retry policies per step ## Bug Fixes - Fixed race condition in concurrent execution handler - Resolved memory leak in long-running workflows - Fixed timezone handling in scheduled triggers ## Breaking Changes - **BREAKING:** API v1 endpoints deprecated, will be removed in v4.0",
              "score": null
            },
            {
              "doc_id": "release_3_0_0",
              "chunk_id": "release_3_0_0_fix_0",
              "content": "# Release Notes - v3.0.0 **Release Date:** 2023-10-15 ## New Features - Complete UI redesign - New workflow editor with visual builder - Real-time execution monitoring - Team workspaces ## Bug Fixes - Fixed critical security issue in token validation - Resolved data loss bug in failed migrations ## Breaking Changes - **BREAKING:** Authentication API completely rewritten - **BREAKING:** Legacy webhook format no longer supported - **BREAKING:** Minimum Node.js version now 18 ## Upgrade Instructions",
              "score": null
            },
            {
              "doc_id": "release_3_1_0",
              "chunk_id": "release_3_1_0_fix_0",
              "content": "# Release Notes - v3.1.0 **Release Date:** 2023-12-01 ## New Features - GraphQL API beta release - Workflow templates marketplace - Enhanced audit logging - SSO support for Okta ## Bug Fixes - Fixed webhook delivery retry logic - Resolved issue with large payload handling - Fixed dashboard loading performance ## Upgrade Instructions ```bash cloudflow upgrade --version 3.1.0 ``` ## Known Issues None at this time.",
              "score": null
            },
            {
              "doc_id": "release_2_9_0",
              "chunk_id": "release_2_9_0_fix_0",
              "content": "# Release Notes - v2.9.0 **Release Date:** 2023-08-01 ## New Features - Parallel step execution - Conditional branching in workflows - Slack integration improvements - Performance dashboard ## Bug Fixes - Fixed issue with special characters in secrets - Resolved timeout in large file uploads ## Upgrade Instructions ```bash cloudflow upgrade --version 2.9.0 ``` ## Known Issues None at this time.",
              "score": null
            },
            {
              "doc_id": "runbook_high_cpu_usage",
              "chunk_id": "runbook_high_cpu_usage_fix_2",
              "content": "high after scaling, escalate to platform team. Page on-call if affecting users. ## Post-Incident - Document timeline in incident log - Schedule post-mortem within 48 hours - Update runbook if new patterns discovered",
              "score": null
            }
          ],
          "latency_ms": 3.2413099997938843,
          "query_id": "release_048",
          "dimension": "original",
          "query": "What breaking changes are in v3.0.0?"
        },
        {
          "key_facts": [
            "Authentication API",
            "Legacy webhook format",
            "Node.js version now 18"
          ],
          "found_facts": [
            "Authentication API",
            "Legacy webhook format",
            "Node.js version now 18"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "release_3_0_0"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "release_3_2_0",
              "chunk_id": "release_3_2_0_fix_0",
              "content": "# Release Notes - v3.2.0 **Release Date:** 2024-01-15 ## New Features - Workflow versioning with rollback support - New Python SDK with async support - Bulk operations API for workflows - Custom retry policies per step ## Bug Fixes - Fixed race condition in concurrent execution handler - Resolved memory leak in long-running workflows - Fixed timezone handling in scheduled triggers ## Breaking Changes - **BREAKING:** API v1 endpoints deprecated, will be removed in v4.0",
              "score": null
            },
            {
              "doc_id": "release_3_0_0",
              "chunk_id": "release_3_0_0_fix_0",
              "content": "# Release Notes - v3.0.0 **Release Date:** 2023-10-15 ## New Features - Complete UI redesign - New workflow editor with visual builder - Real-time execution monitoring - Team workspaces ## Bug Fixes - Fixed critical security issue in token validation - Resolved data loss bug in failed migrations ## Breaking Changes - **BREAKING:** Authentication API completely rewritten - **BREAKING:** Legacy webhook format no longer supported - **BREAKING:** Minimum Node.js version now 18 ## Upgrade Instructions",
              "score": null
            },
            {
              "doc_id": "release_3_1_0",
              "chunk_id": "release_3_1_0_fix_0",
              "content": "# Release Notes - v3.1.0 **Release Date:** 2023-12-01 ## New Features - GraphQL API beta release - Workflow templates marketplace - Enhanced audit logging - SSO support for Okta ## Bug Fixes - Fixed webhook delivery retry logic - Resolved issue with large payload handling - Fixed dashboard loading performance ## Upgrade Instructions ```bash cloudflow upgrade --version 3.1.0 ``` ## Known Issues None at this time.",
              "score": null
            },
            {
              "doc_id": "release_2_9_0",
              "chunk_id": "release_2_9_0_fix_0",
              "content": "# Release Notes - v2.9.0 **Release Date:** 2023-08-01 ## New Features - Parallel step execution - Conditional branching in workflows - Slack integration improvements - Performance dashboard ## Bug Fixes - Fixed issue with special characters in secrets - Resolved timeout in large file uploads ## Upgrade Instructions ```bash cloudflow upgrade --version 2.9.0 ``` ## Known Issues None at this time.",
              "score": null
            },
            {
              "doc_id": "release_3_2_0",
              "chunk_id": "release_3_2_0_fix_1",
              "content": "- **BREAKING:** Minimum Python version now 3.9 ## Upgrade Instructions ```bash cloudflow upgrade --version 3.2.0 ``` ## Known Issues None at this time.",
              "score": null
            }
          ],
          "latency_ms": 3.339312001116923,
          "query_id": "release_048",
          "dimension": "synonym",
          "query": "What backwards-incompatible changes and migration requirements are in version 3.0.0?"
        },
        {
          "key_facts": [
            "Authentication API",
            "Legacy webhook format",
            "Node.js version now 18"
          ],
          "found_facts": [
            "Authentication API",
            "Legacy webhook format",
            "Node.js version now 18"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "release_3_0_0"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "release_3_0_0",
              "chunk_id": "release_3_0_0_fix_0",
              "content": "# Release Notes - v3.0.0 **Release Date:** 2023-10-15 ## New Features - Complete UI redesign - New workflow editor with visual builder - Real-time execution monitoring - Team workspaces ## Bug Fixes - Fixed critical security issue in token validation - Resolved data loss bug in failed migrations ## Breaking Changes - **BREAKING:** Authentication API completely rewritten - **BREAKING:** Legacy webhook format no longer supported - **BREAKING:** Minimum Node.js version now 18 ## Upgrade Instructions",
              "score": null
            },
            {
              "doc_id": "api_webhooks",
              "chunk_id": "api_webhooks_fix_2",
              "content": "**Parameters:** - `url` (string): Webhook URL (must be HTTPS) - `events` (array): Events to subscribe to - `secret` (string): Signing secret for verification **Response:** - Success: `201` - Webhook created - Error: `400` - Bad Request **Example:** ```json {\"id\": \"wh_02\", \"url\": \"https://example.com/new-hook\", \"secret\": \"whsec_...\"} ```",
              "score": null
            },
            {
              "doc_id": "api_webhooks",
              "chunk_id": "api_webhooks_fix_1",
              "content": "**Parameters:** - `active` (boolean): Filter by active status **Response:** - Success: `200` - Returns array of webhook objects - Error: `400` - Bad Request **Example:** ```json {\"webhooks\": [{\"id\": \"wh_01\", \"url\": \"https://example.com/hook\", \"events\": [\"workflow.completed\"]}]} ``` ### POST /webhooks Create a new webhook endpoint.",
              "score": null
            },
            {
              "doc_id": "api_webhooks",
              "chunk_id": "api_webhooks_fix_0",
              "content": "# Webhooks API Reference ## Overview The Webhooks API provides programmatic access to webhooks functionality. Base URL: `https://api.cloudflow.io/v1/webhooks` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /webhooks List configured webhooks.",
              "score": null
            },
            {
              "doc_id": "release_3_1_0",
              "chunk_id": "release_3_1_0_fix_0",
              "content": "# Release Notes - v3.1.0 **Release Date:** 2023-12-01 ## New Features - GraphQL API beta release - Workflow templates marketplace - Enhanced audit logging - SSO support for Okta ## Bug Fixes - Fixed webhook delivery retry logic - Resolved issue with large payload handling - Fixed dashboard loading performance ## Upgrade Instructions ```bash cloudflow upgrade --version 3.1.0 ``` ## Known Issues None at this time.",
              "score": null
            }
          ],
          "latency_ms": 3.7678809985663975,
          "query_id": "release_048",
          "dimension": "problem",
          "query": "After upgrading to 3.0.0 our webhooks stopped working and auth is broken, what changed?"
        },
        {
          "key_facts": [
            "Authentication API",
            "Legacy webhook format",
            "Node.js version now 18"
          ],
          "found_facts": [
            "Authentication API",
            "Legacy webhook format",
            "Node.js version now 18"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "release_3_0_0"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "release_3_0_0",
              "chunk_id": "release_3_0_0_fix_0",
              "content": "# Release Notes - v3.0.0 **Release Date:** 2023-10-15 ## New Features - Complete UI redesign - New workflow editor with visual builder - Real-time execution monitoring - Team workspaces ## Bug Fixes - Fixed critical security issue in token validation - Resolved data loss bug in failed migrations ## Breaking Changes - **BREAKING:** Authentication API completely rewritten - **BREAKING:** Legacy webhook format no longer supported - **BREAKING:** Minimum Node.js version now 18 ## Upgrade Instructions",
              "score": null
            },
            {
              "doc_id": "release_3_1_0",
              "chunk_id": "release_3_1_0_fix_0",
              "content": "# Release Notes - v3.1.0 **Release Date:** 2023-12-01 ## New Features - GraphQL API beta release - Workflow templates marketplace - Enhanced audit logging - SSO support for Okta ## Bug Fixes - Fixed webhook delivery retry logic - Resolved issue with large payload handling - Fixed dashboard loading performance ## Upgrade Instructions ```bash cloudflow upgrade --version 3.1.0 ``` ## Known Issues None at this time.",
              "score": null
            },
            {
              "doc_id": "release_3_2_0",
              "chunk_id": "release_3_2_0_fix_0",
              "content": "# Release Notes - v3.2.0 **Release Date:** 2024-01-15 ## New Features - Workflow versioning with rollback support - New Python SDK with async support - Bulk operations API for workflows - Custom retry policies per step ## Bug Fixes - Fixed race condition in concurrent execution handler - Resolved memory leak in long-running workflows - Fixed timezone handling in scheduled triggers ## Breaking Changes - **BREAKING:** API v1 endpoints deprecated, will be removed in v4.0",
              "score": null
            },
            {
              "doc_id": "release_3_0_0",
              "chunk_id": "release_3_0_0_fix_1",
              "content": "```bash cloudflow upgrade --version 3.0.0 ``` ## Known Issues None at this time.",
              "score": null
            },
            {
              "doc_id": "howto_migrate_from_v1_to_v2_api",
              "chunk_id": "howto_migrate_from_v1_to_v2_api_fix_0",
              "content": "# How to Migrate from v1 to v2 API ## Prerequisites - Existing v1 integration - v2 API access enabled ## Steps ### Step 1: Review breaking changes The v2 API changes authentication from API keys to OAuth 2.0. Review the migration guide. ### Step 2: Update client library Upgrade to the latest SDK version. ```bash npm install @cloudflow/sdk@latest ``` ### Step 3: Update authentication Replace API key authentication with OAuth 2.0 client credentials flow.",
              "score": null
            }
          ],
          "latency_ms": 5.288263000693405,
          "query_id": "release_048",
          "dimension": "casual",
          "query": "v3.0 breaking changes nodejs"
        },
        {
          "key_facts": [
            "Authentication API",
            "Legacy webhook format",
            "Node.js version now 18"
          ],
          "found_facts": [
            "Authentication API",
            "Legacy webhook format",
            "Node.js version now 18"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "release_3_0_0"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "release_3_0_0",
              "chunk_id": "release_3_0_0_fix_0",
              "content": "# Release Notes - v3.0.0 **Release Date:** 2023-10-15 ## New Features - Complete UI redesign - New workflow editor with visual builder - Real-time execution monitoring - Team workspaces ## Bug Fixes - Fixed critical security issue in token validation - Resolved data loss bug in failed migrations ## Breaking Changes - **BREAKING:** Authentication API completely rewritten - **BREAKING:** Legacy webhook format no longer supported - **BREAKING:** Minimum Node.js version now 18 ## Upgrade Instructions",
              "score": null
            },
            {
              "doc_id": "release_3_1_0",
              "chunk_id": "release_3_1_0_fix_0",
              "content": "# Release Notes - v3.1.0 **Release Date:** 2023-12-01 ## New Features - GraphQL API beta release - Workflow templates marketplace - Enhanced audit logging - SSO support for Okta ## Bug Fixes - Fixed webhook delivery retry logic - Resolved issue with large payload handling - Fixed dashboard loading performance ## Upgrade Instructions ```bash cloudflow upgrade --version 3.1.0 ``` ## Known Issues None at this time.",
              "score": null
            },
            {
              "doc_id": "release_3_2_0",
              "chunk_id": "release_3_2_0_fix_0",
              "content": "# Release Notes - v3.2.0 **Release Date:** 2024-01-15 ## New Features - Workflow versioning with rollback support - New Python SDK with async support - Bulk operations API for workflows - Custom retry policies per step ## Bug Fixes - Fixed race condition in concurrent execution handler - Resolved memory leak in long-running workflows - Fixed timezone handling in scheduled triggers ## Breaking Changes - **BREAKING:** API v1 endpoints deprecated, will be removed in v4.0",
              "score": null
            },
            {
              "doc_id": "release_2_9_0",
              "chunk_id": "release_2_9_0_fix_0",
              "content": "# Release Notes - v2.9.0 **Release Date:** 2023-08-01 ## New Features - Parallel step execution - Conditional branching in workflows - Slack integration improvements - Performance dashboard ## Bug Fixes - Fixed issue with special characters in secrets - Resolved timeout in large file uploads ## Upgrade Instructions ```bash cloudflow upgrade --version 2.9.0 ``` ## Known Issues None at this time.",
              "score": null
            },
            {
              "doc_id": "adr_008",
              "chunk_id": "adr_008_fix_1",
              "content": "**Pros:** - No vendor lock-in - S3 compatible - On-premises option **Cons:** - Operational burden - Lower durability guarantees - Scaling complexity ### Option 3: Google Cloud Storage GCP object storage. **Pros:** - Strong consistency - Good performance - Unified billing **Cons:** - GCP lock-in - Fewer features than S3 - Smaller ecosystem ## Decision We will use Amazon S3 for object storage with intelligent tiering for cost optimization.",
              "score": null
            }
          ],
          "latency_ms": 3.7852330005989643,
          "query_id": "release_048",
          "dimension": "contextual",
          "query": "Planning to upgrade from 2.x to 3.0.0, what will break and do I need to update our Node.js version?"
        },
        {
          "key_facts": [
            "Authentication API",
            "Legacy webhook format",
            "Node.js version now 18"
          ],
          "found_facts": [
            "Authentication API",
            "Legacy webhook format",
            "Node.js version now 18"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "release_3_0_0"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "release_3_0_0",
              "chunk_id": "release_3_0_0_fix_0",
              "content": "# Release Notes - v3.0.0 **Release Date:** 2023-10-15 ## New Features - Complete UI redesign - New workflow editor with visual builder - Real-time execution monitoring - Team workspaces ## Bug Fixes - Fixed critical security issue in token validation - Resolved data loss bug in failed migrations ## Breaking Changes - **BREAKING:** Authentication API completely rewritten - **BREAKING:** Legacy webhook format no longer supported - **BREAKING:** Minimum Node.js version now 18 ## Upgrade Instructions",
              "score": null
            },
            {
              "doc_id": "api_webhooks",
              "chunk_id": "api_webhooks_fix_1",
              "content": "**Parameters:** - `active` (boolean): Filter by active status **Response:** - Success: `200` - Returns array of webhook objects - Error: `400` - Bad Request **Example:** ```json {\"webhooks\": [{\"id\": \"wh_01\", \"url\": \"https://example.com/hook\", \"events\": [\"workflow.completed\"]}]} ``` ### POST /webhooks Create a new webhook endpoint.",
              "score": null
            },
            {
              "doc_id": "api_webhooks",
              "chunk_id": "api_webhooks_fix_2",
              "content": "**Parameters:** - `url` (string): Webhook URL (must be HTTPS) - `events` (array): Events to subscribe to - `secret` (string): Signing secret for verification **Response:** - Success: `201` - Webhook created - Error: `400` - Bad Request **Example:** ```json {\"id\": \"wh_02\", \"url\": \"https://example.com/new-hook\", \"secret\": \"whsec_...\"} ```",
              "score": null
            },
            {
              "doc_id": "api_webhooks",
              "chunk_id": "api_webhooks_fix_0",
              "content": "# Webhooks API Reference ## Overview The Webhooks API provides programmatic access to webhooks functionality. Base URL: `https://api.cloudflow.io/v1/webhooks` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /webhooks List configured webhooks.",
              "score": null
            },
            {
              "doc_id": "release_3_1_0",
              "chunk_id": "release_3_1_0_fix_0",
              "content": "# Release Notes - v3.1.0 **Release Date:** 2023-12-01 ## New Features - GraphQL API beta release - Workflow templates marketplace - Enhanced audit logging - SSO support for Okta ## Bug Fixes - Fixed webhook delivery retry logic - Resolved issue with large payload handling - Fixed dashboard loading performance ## Upgrade Instructions ```bash cloudflow upgrade --version 3.1.0 ``` ## Known Issues None at this time.",
              "score": null
            }
          ],
          "latency_ms": 3.3792269987316104,
          "query_id": "release_048",
          "dimension": "negation",
          "query": "Why did the legacy webhook format stop working after updating?"
        },
        {
          "key_facts": [
            "GraphQL API beta",
            "2023-12-01"
          ],
          "found_facts": [
            "GraphQL API beta",
            "2023-12-01"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "release_3_1_0"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "release_3_1_0",
              "chunk_id": "release_3_1_0_fix_0",
              "content": "# Release Notes - v3.1.0 **Release Date:** 2023-12-01 ## New Features - GraphQL API beta release - Workflow templates marketplace - Enhanced audit logging - SSO support for Okta ## Bug Fixes - Fixed webhook delivery retry logic - Resolved issue with large payload handling - Fixed dashboard loading performance ## Upgrade Instructions ```bash cloudflow upgrade --version 3.1.0 ``` ## Known Issues None at this time.",
              "score": null
            },
            {
              "doc_id": "release_3_0_0",
              "chunk_id": "release_3_0_0_fix_0",
              "content": "# Release Notes - v3.0.0 **Release Date:** 2023-10-15 ## New Features - Complete UI redesign - New workflow editor with visual builder - Real-time execution monitoring - Team workspaces ## Bug Fixes - Fixed critical security issue in token validation - Resolved data loss bug in failed migrations ## Breaking Changes - **BREAKING:** Authentication API completely rewritten - **BREAKING:** Legacy webhook format no longer supported - **BREAKING:** Minimum Node.js version now 18 ## Upgrade Instructions",
              "score": null
            },
            {
              "doc_id": "release_3_2_0",
              "chunk_id": "release_3_2_0_fix_0",
              "content": "# Release Notes - v3.2.0 **Release Date:** 2024-01-15 ## New Features - Workflow versioning with rollback support - New Python SDK with async support - Bulk operations API for workflows - Custom retry policies per step ## Bug Fixes - Fixed race condition in concurrent execution handler - Resolved memory leak in long-running workflows - Fixed timezone handling in scheduled triggers ## Breaking Changes - **BREAKING:** API v1 endpoints deprecated, will be removed in v4.0",
              "score": null
            },
            {
              "doc_id": "arch_workflow_engine",
              "chunk_id": "arch_workflow_engine_fix_2",
              "content": "**Technology:** PostgreSQL with JSONB **Scaling:** Primary-replica with read replicas ## Data Flow Triggers \u2192 Scheduler \u2192 Job Queue \u2192 Executor \u2192 State Store \u2192 Webhooks/Notifications ## Performance Characteristics - **Latency P50:** 45ms - **Latency P99:** 250ms - **Throughput:** 5000 executions/minute",
              "score": null
            },
            {
              "doc_id": "arch_monitoring_stack",
              "chunk_id": "arch_monitoring_stack_fix_1",
              "content": "Supports structured JSON logs with label indexing. **Technology:** Loki with Grafana **Scaling:** Distributed with sharding ### Tracing Distributed request tracing. Trace retention is 7 days. **Technology:** Jaeger with OpenTelemetry **Scaling:** Sampling at 10% ### Alerting Routes alerts to on-call engineers. Supports escalation policies and silencing.",
              "score": null
            }
          ],
          "latency_ms": 3.227383000194095,
          "query_id": "release_049",
          "dimension": "original",
          "query": "When was GraphQL API released?"
        },
        {
          "key_facts": [
            "GraphQL API beta",
            "2023-12-01"
          ],
          "found_facts": [
            "GraphQL API beta",
            "2023-12-01"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "release_3_1_0"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "release_3_1_0",
              "chunk_id": "release_3_1_0_fix_0",
              "content": "# Release Notes - v3.1.0 **Release Date:** 2023-12-01 ## New Features - GraphQL API beta release - Workflow templates marketplace - Enhanced audit logging - SSO support for Okta ## Bug Fixes - Fixed webhook delivery retry logic - Resolved issue with large payload handling - Fixed dashboard loading performance ## Upgrade Instructions ```bash cloudflow upgrade --version 3.1.0 ``` ## Known Issues None at this time.",
              "score": null
            },
            {
              "doc_id": "release_3_0_0",
              "chunk_id": "release_3_0_0_fix_0",
              "content": "# Release Notes - v3.0.0 **Release Date:** 2023-10-15 ## New Features - Complete UI redesign - New workflow editor with visual builder - Real-time execution monitoring - Team workspaces ## Bug Fixes - Fixed critical security issue in token validation - Resolved data loss bug in failed migrations ## Breaking Changes - **BREAKING:** Authentication API completely rewritten - **BREAKING:** Legacy webhook format no longer supported - **BREAKING:** Minimum Node.js version now 18 ## Upgrade Instructions",
              "score": null
            },
            {
              "doc_id": "release_3_2_0",
              "chunk_id": "release_3_2_0_fix_0",
              "content": "# Release Notes - v3.2.0 **Release Date:** 2024-01-15 ## New Features - Workflow versioning with rollback support - New Python SDK with async support - Bulk operations API for workflows - Custom retry policies per step ## Bug Fixes - Fixed race condition in concurrent execution handler - Resolved memory leak in long-running workflows - Fixed timezone handling in scheduled triggers ## Breaking Changes - **BREAKING:** API v1 endpoints deprecated, will be removed in v4.0",
              "score": null
            },
            {
              "doc_id": "adr_001",
              "chunk_id": "adr_001_fix_1",
              "content": "**Pros:** - ACID compliance - Rich JSONB queries - Team expertise - Proven at scale **Cons:** - Manual sharding for extreme scale - Requires connection pooling ### Option 2: MongoDB Document database with native JSON. **Pros:** - Flexible schema - Built-in sharding - Good for documents **Cons:** - Weaker consistency guarantees - Complex transactions - Higher operational cost ### Option 3: CockroachDB Distributed SQL database.",
              "score": null
            },
            {
              "doc_id": "arch_workflow_engine",
              "chunk_id": "arch_workflow_engine_fix_2",
              "content": "**Technology:** PostgreSQL with JSONB **Scaling:** Primary-replica with read replicas ## Data Flow Triggers \u2192 Scheduler \u2192 Job Queue \u2192 Executor \u2192 State Store \u2192 Webhooks/Notifications ## Performance Characteristics - **Latency P50:** 45ms - **Latency P99:** 250ms - **Throughput:** 5000 executions/minute",
              "score": null
            }
          ],
          "latency_ms": 3.3761309987312416,
          "query_id": "release_049",
          "dimension": "synonym",
          "query": "What version introduced the GraphQL endpoint and when was it launched?"
        },
        {
          "key_facts": [
            "GraphQL API beta",
            "2023-12-01"
          ],
          "found_facts": [
            "GraphQL API beta",
            "2023-12-01"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "release_3_1_0"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "release_3_1_0",
              "chunk_id": "release_3_1_0_fix_0",
              "content": "# Release Notes - v3.1.0 **Release Date:** 2023-12-01 ## New Features - GraphQL API beta release - Workflow templates marketplace - Enhanced audit logging - SSO support for Okta ## Bug Fixes - Fixed webhook delivery retry logic - Resolved issue with large payload handling - Fixed dashboard loading performance ## Upgrade Instructions ```bash cloudflow upgrade --version 3.1.0 ``` ## Known Issues None at this time.",
              "score": null
            },
            {
              "doc_id": "release_3_0_0",
              "chunk_id": "release_3_0_0_fix_0",
              "content": "# Release Notes - v3.0.0 **Release Date:** 2023-10-15 ## New Features - Complete UI redesign - New workflow editor with visual builder - Real-time execution monitoring - Team workspaces ## Bug Fixes - Fixed critical security issue in token validation - Resolved data loss bug in failed migrations ## Breaking Changes - **BREAKING:** Authentication API completely rewritten - **BREAKING:** Legacy webhook format no longer supported - **BREAKING:** Minimum Node.js version now 18 ## Upgrade Instructions",
              "score": null
            },
            {
              "doc_id": "release_3_2_0",
              "chunk_id": "release_3_2_0_fix_0",
              "content": "# Release Notes - v3.2.0 **Release Date:** 2024-01-15 ## New Features - Workflow versioning with rollback support - New Python SDK with async support - Bulk operations API for workflows - Custom retry policies per step ## Bug Fixes - Fixed race condition in concurrent execution handler - Resolved memory leak in long-running workflows - Fixed timezone handling in scheduled triggers ## Breaking Changes - **BREAKING:** API v1 endpoints deprecated, will be removed in v4.0",
              "score": null
            },
            {
              "doc_id": "config_api_gateway",
              "chunk_id": "config_api_gateway_fix_0",
              "content": "# API Gateway Configuration Reference ## Overview This document describes all configuration options for API Gateway. ## Environment Variables | Variable | Type | Default | Description | |----------|------|---------|-------------| | `GATEWAY_PORT` | integer | 8000 | Gateway listen port | | `GATEWAY_RATE_LIMIT` | integer | 100 | Requests per minute per user | | `GATEWAY_TIMEOUT_MS` | integer | 30000 | Request timeout in milliseconds | | `GATEWAY_MAX_BODY_SIZE` | string | 10MB | Maximum request body",
              "score": null
            },
            {
              "doc_id": "adr_001",
              "chunk_id": "adr_001_fix_1",
              "content": "**Pros:** - ACID compliance - Rich JSONB queries - Team expertise - Proven at scale **Cons:** - Manual sharding for extreme scale - Requires connection pooling ### Option 2: MongoDB Document database with native JSON. **Pros:** - Flexible schema - Built-in sharding - Good for documents **Cons:** - Weaker consistency guarantees - Complex transactions - Higher operational cost ### Option 3: CockroachDB Distributed SQL database.",
              "score": null
            }
          ],
          "latency_ms": 3.7763470008940203,
          "query_id": "release_049",
          "dimension": "problem",
          "query": "I can't find the GraphQL API in my current version, when was it added?"
        },
        {
          "key_facts": [
            "GraphQL API beta",
            "2023-12-01"
          ],
          "found_facts": [
            "GraphQL API beta",
            "2023-12-01"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "release_3_1_0"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "release_3_1_0",
              "chunk_id": "release_3_1_0_fix_0",
              "content": "# Release Notes - v3.1.0 **Release Date:** 2023-12-01 ## New Features - GraphQL API beta release - Workflow templates marketplace - Enhanced audit logging - SSO support for Okta ## Bug Fixes - Fixed webhook delivery retry logic - Resolved issue with large payload handling - Fixed dashboard loading performance ## Upgrade Instructions ```bash cloudflow upgrade --version 3.1.0 ``` ## Known Issues None at this time.",
              "score": null
            },
            {
              "doc_id": "howto_migrate_from_v1_to_v2_api",
              "chunk_id": "howto_migrate_from_v1_to_v2_api_fix_1",
              "content": "> **Note:** Generate new credentials in the dashboard ### Step 4: Test in staging Run your integration tests against the v2 staging endpoint. ```bash CLOUDFLOW_API_URL=https://api-v2-staging.cloudflow.io npm test ``` ### Step 5: Switch production Update production configuration to use v2 API. ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            },
            {
              "doc_id": "release_3_0_0",
              "chunk_id": "release_3_0_0_fix_1",
              "content": "```bash cloudflow upgrade --version 3.0.0 ``` ## Known Issues None at this time.",
              "score": null
            },
            {
              "doc_id": "release_2_9_0",
              "chunk_id": "release_2_9_0_fix_0",
              "content": "# Release Notes - v2.9.0 **Release Date:** 2023-08-01 ## New Features - Parallel step execution - Conditional branching in workflows - Slack integration improvements - Performance dashboard ## Bug Fixes - Fixed issue with special characters in secrets - Resolved timeout in large file uploads ## Upgrade Instructions ```bash cloudflow upgrade --version 2.9.0 ``` ## Known Issues None at this time.",
              "score": null
            },
            {
              "doc_id": "api_integrations",
              "chunk_id": "api_integrations_fix_0",
              "content": "# Integrations API Reference ## Overview The Integrations API provides programmatic access to integrations functionality. Base URL: `https://api.cloudflow.io/v1/integrations` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /integrations List available integrations.",
              "score": null
            }
          ],
          "latency_ms": 3.208148000339861,
          "query_id": "release_049",
          "dimension": "casual",
          "query": "graphql api release date cloudflow"
        },
        {
          "key_facts": [
            "GraphQL API beta",
            "2023-12-01"
          ],
          "found_facts": [
            "GraphQL API beta",
            "2023-12-01"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "release_3_1_0"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "release_3_1_0",
              "chunk_id": "release_3_1_0_fix_0",
              "content": "# Release Notes - v3.1.0 **Release Date:** 2023-12-01 ## New Features - GraphQL API beta release - Workflow templates marketplace - Enhanced audit logging - SSO support for Okta ## Bug Fixes - Fixed webhook delivery retry logic - Resolved issue with large payload handling - Fixed dashboard loading performance ## Upgrade Instructions ```bash cloudflow upgrade --version 3.1.0 ``` ## Known Issues None at this time.",
              "score": null
            },
            {
              "doc_id": "howto_migrate_from_v1_to_v2_api",
              "chunk_id": "howto_migrate_from_v1_to_v2_api_fix_0",
              "content": "# How to Migrate from v1 to v2 API ## Prerequisites - Existing v1 integration - v2 API access enabled ## Steps ### Step 1: Review breaking changes The v2 API changes authentication from API keys to OAuth 2.0. Review the migration guide. ### Step 2: Update client library Upgrade to the latest SDK version. ```bash npm install @cloudflow/sdk@latest ``` ### Step 3: Update authentication Replace API key authentication with OAuth 2.0 client credentials flow.",
              "score": null
            },
            {
              "doc_id": "api_integrations",
              "chunk_id": "api_integrations_fix_0",
              "content": "# Integrations API Reference ## Overview The Integrations API provides programmatic access to integrations functionality. Base URL: `https://api.cloudflow.io/v1/integrations` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /integrations List available integrations.",
              "score": null
            },
            {
              "doc_id": "howto_migrate_from_v1_to_v2_api",
              "chunk_id": "howto_migrate_from_v1_to_v2_api_fix_1",
              "content": "> **Note:** Generate new credentials in the dashboard ### Step 4: Test in staging Run your integration tests against the v2 staging endpoint. ```bash CLOUDFLOW_API_URL=https://api-v2-staging.cloudflow.io npm test ``` ### Step 5: Switch production Update production configuration to use v2 API. ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            },
            {
              "doc_id": "api_webhooks",
              "chunk_id": "api_webhooks_fix_0",
              "content": "# Webhooks API Reference ## Overview The Webhooks API provides programmatic access to webhooks functionality. Base URL: `https://api.cloudflow.io/v1/webhooks` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /webhooks List configured webhooks.",
              "score": null
            }
          ],
          "latency_ms": 3.383073999430053,
          "query_id": "release_049",
          "dimension": "contextual",
          "query": "We want to migrate from REST to GraphQL, which CloudFlow version do we need to be on to use the GraphQL API?"
        },
        {
          "key_facts": [
            "GraphQL API beta",
            "2023-12-01"
          ],
          "found_facts": [
            "GraphQL API beta",
            "2023-12-01"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "release_3_1_0"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "release_3_1_0",
              "chunk_id": "release_3_1_0_fix_0",
              "content": "# Release Notes - v3.1.0 **Release Date:** 2023-12-01 ## New Features - GraphQL API beta release - Workflow templates marketplace - Enhanced audit logging - SSO support for Okta ## Bug Fixes - Fixed webhook delivery retry logic - Resolved issue with large payload handling - Fixed dashboard loading performance ## Upgrade Instructions ```bash cloudflow upgrade --version 3.1.0 ``` ## Known Issues None at this time.",
              "score": null
            },
            {
              "doc_id": "release_3_2_0",
              "chunk_id": "release_3_2_0_fix_0",
              "content": "# Release Notes - v3.2.0 **Release Date:** 2024-01-15 ## New Features - Workflow versioning with rollback support - New Python SDK with async support - Bulk operations API for workflows - Custom retry policies per step ## Bug Fixes - Fixed race condition in concurrent execution handler - Resolved memory leak in long-running workflows - Fixed timezone handling in scheduled triggers ## Breaking Changes - **BREAKING:** API v1 endpoints deprecated, will be removed in v4.0",
              "score": null
            },
            {
              "doc_id": "release_3_0_0",
              "chunk_id": "release_3_0_0_fix_0",
              "content": "# Release Notes - v3.0.0 **Release Date:** 2023-10-15 ## New Features - Complete UI redesign - New workflow editor with visual builder - Real-time execution monitoring - Team workspaces ## Bug Fixes - Fixed critical security issue in token validation - Resolved data loss bug in failed migrations ## Breaking Changes - **BREAKING:** Authentication API completely rewritten - **BREAKING:** Legacy webhook format no longer supported - **BREAKING:** Minimum Node.js version now 18 ## Upgrade Instructions",
              "score": null
            },
            {
              "doc_id": "adr_001",
              "chunk_id": "adr_001_fix_1",
              "content": "**Pros:** - ACID compliance - Rich JSONB queries - Team expertise - Proven at scale **Cons:** - Manual sharding for extreme scale - Requires connection pooling ### Option 2: MongoDB Document database with native JSON. **Pros:** - Flexible schema - Built-in sharding - Good for documents **Cons:** - Weaker consistency guarantees - Complex transactions - Higher operational cost ### Option 3: CockroachDB Distributed SQL database.",
              "score": null
            },
            {
              "doc_id": "arch_data_pipeline",
              "chunk_id": "arch_data_pipeline_fix_2",
              "content": "**Technology:** S3 with Iceberg tables **Scaling:** Unlimited with tiered storage ## Data Flow Input \u2192 Validation \u2192 Transform \u2192 Enrich \u2192 Output ## Performance Characteristics - **Latency P50:** 100ms (streaming), 5min (batch) - **Latency P99:** 500ms (streaming), 30min (batch) - **Throughput:** 100000 messages/second (streaming)",
              "score": null
            }
          ],
          "latency_ms": 3.2888890000322135,
          "query_id": "release_049",
          "dimension": "negation",
          "query": "Why doesn't v3.0.0 have GraphQL support?"
        },
        {
          "key_facts": [
            "PostgreSQL",
            "Redis Cluster",
            "S3",
            "Ristretto",
            "CloudFront",
            "Iceberg"
          ],
          "found_facts": [
            "Redis Cluster",
            "Ristretto"
          ],
          "missed_facts": [
            "PostgreSQL",
            "S3",
            "CloudFront",
            "Iceberg"
          ],
          "coverage": 0.3333333333333333,
          "expected_docs": [
            "adr_001",
            "adr_004",
            "adr_008",
            "arch_caching_layer",
            "arch_data_pipeline"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "arch_caching_layer",
              "chunk_id": "arch_caching_layer_fix_0",
              "content": "# Caching Layer Architecture ## Overview The Caching Layer reduces latency and database load by caching frequently accessed data. Uses a multi-tier strategy with local and distributed caches. ## Design Principles - **Cache Invalidation**: Explicit invalidation on writes - **Graceful Degradation**: System works without cache ## Components ### Local Cache First-level cache with 100ms TTL. Reduces Redis round-trips for hot data.",
              "score": null
            },
            {
              "doc_id": "arch_data_pipeline",
              "chunk_id": "arch_data_pipeline_fix_1",
              "content": "**Technology:** Apache Kafka with Flink **Scaling:** Partition-based scaling ### Batch Processor Processes large batch jobs. Supports data formats: JSON, CSV, Parquet, Avro. **Technology:** Apache Spark on Kubernetes **Scaling:** Dynamic resource allocation ### Data Lake Long-term data storage with time-travel queries. Retention configurable up to 7 years.",
              "score": null
            },
            {
              "doc_id": "adr_004",
              "chunk_id": "adr_004_fix_0",
              "content": "# ADR-004: Use Redis for Caching and Session Storage **Status:** Accepted **Date:** 2024-04-05 ## Context We need a fast data store for caching API responses, session data, and rate limiting counters. ## Options Considered ### Option 1: Redis Cluster In-memory data structure store. **Pros:** - Sub-millisecond latency - Rich data structures - Pub/sub support - Clustering **Cons:** - Memory cost - Persistence complexity - Cluster management ### Option 2: Memcached Simple key-value cache.",
              "score": null
            },
            {
              "doc_id": "arch_caching_layer",
              "chunk_id": "arch_caching_layer_fix_1",
              "content": "**Technology:** Ristretto in-memory cache **Scaling:** Per-pod memory allocation ### Distributed Cache Second-level cache with configurable TTL. Maximum key size 512MB. **Technology:** Redis Cluster 7.x **Scaling:** 6-node cluster with sharding ### CDN Caches static assets and API responses. Cache hit ratio target 95%.",
              "score": null
            },
            {
              "doc_id": "adr_004",
              "chunk_id": "adr_004_fix_1",
              "content": "**Pros:** - Simple operation - Multi-threaded - Lower memory overhead **Cons:** - No persistence - Limited data types - No clustering ### Option 3: Application-level cache In-process caching only. **Pros:** - No external dependency - Fastest possible - Simple **Cons:** - Not shared across pods - Memory pressure - Cold starts ## Decision We will use Redis Cluster for caching with a 6-node cluster for high availability.",
              "score": null
            }
          ],
          "latency_ms": 3.6757289999513887,
          "query_id": "complex_050",
          "dimension": "original",
          "query": "What are all the technologies used for data storage and caching?"
        },
        {
          "key_facts": [
            "PostgreSQL",
            "Redis Cluster",
            "S3",
            "Ristretto",
            "CloudFront",
            "Iceberg"
          ],
          "found_facts": [
            "Redis Cluster"
          ],
          "missed_facts": [
            "PostgreSQL",
            "S3",
            "Ristretto",
            "CloudFront",
            "Iceberg"
          ],
          "coverage": 0.16666666666666666,
          "expected_docs": [
            "adr_001",
            "adr_004",
            "adr_008",
            "arch_caching_layer",
            "arch_data_pipeline"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "adr_004",
              "chunk_id": "adr_004_fix_1",
              "content": "**Pros:** - Simple operation - Multi-threaded - Lower memory overhead **Cons:** - No persistence - Limited data types - No clustering ### Option 3: Application-level cache In-process caching only. **Pros:** - No external dependency - Fastest possible - Simple **Cons:** - Not shared across pods - Memory pressure - Cold starts ## Decision We will use Redis Cluster for caching with a 6-node cluster for high availability.",
              "score": null
            },
            {
              "doc_id": "arch_data_pipeline",
              "chunk_id": "arch_data_pipeline_fix_1",
              "content": "**Technology:** Apache Kafka with Flink **Scaling:** Partition-based scaling ### Batch Processor Processes large batch jobs. Supports data formats: JSON, CSV, Parquet, Avro. **Technology:** Apache Spark on Kubernetes **Scaling:** Dynamic resource allocation ### Data Lake Long-term data storage with time-travel queries. Retention configurable up to 7 years.",
              "score": null
            },
            {
              "doc_id": "adr_004",
              "chunk_id": "adr_004_fix_0",
              "content": "# ADR-004: Use Redis for Caching and Session Storage **Status:** Accepted **Date:** 2024-04-05 ## Context We need a fast data store for caching API responses, session data, and rate limiting counters. ## Options Considered ### Option 1: Redis Cluster In-memory data structure store. **Pros:** - Sub-millisecond latency - Rich data structures - Pub/sub support - Clustering **Cons:** - Memory cost - Persistence complexity - Cluster management ### Option 2: Memcached Simple key-value cache.",
              "score": null
            },
            {
              "doc_id": "arch_data_pipeline",
              "chunk_id": "arch_data_pipeline_fix_0",
              "content": "# Data Pipeline Architecture ## Overview The Data Pipeline processes and transforms data between workflow steps. It supports batch processing up to 10GB and streaming for real-time use cases. ## Design Principles - **Exactly-Once Processing**: Each record is processed exactly once - **Backpressure Handling**: System gracefully handles varying load ## Components ### Stream Processor Handles real-time data streams. Maximum throughput of 100000 messages per second.",
              "score": null
            },
            {
              "doc_id": "arch_caching_layer",
              "chunk_id": "arch_caching_layer_fix_0",
              "content": "# Caching Layer Architecture ## Overview The Caching Layer reduces latency and database load by caching frequently accessed data. Uses a multi-tier strategy with local and distributed caches. ## Design Principles - **Cache Invalidation**: Explicit invalidation on writes - **Graceful Degradation**: System works without cache ## Components ### Local Cache First-level cache with 100ms TTL. Reduces Redis round-trips for hot data.",
              "score": null
            }
          ],
          "latency_ms": 3.435612001339905,
          "query_id": "complex_050",
          "dimension": "synonym",
          "query": "what data stores, databases, and cache solutions does cloudflow use for persistence and memory caching?"
        },
        {
          "key_facts": [
            "PostgreSQL",
            "Redis Cluster",
            "S3",
            "Ristretto",
            "CloudFront",
            "Iceberg"
          ],
          "found_facts": [
            "Redis Cluster",
            "CloudFront"
          ],
          "missed_facts": [
            "PostgreSQL",
            "S3",
            "Ristretto",
            "Iceberg"
          ],
          "coverage": 0.3333333333333333,
          "expected_docs": [
            "adr_001",
            "adr_004",
            "adr_008",
            "arch_caching_layer",
            "arch_data_pipeline"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "arch_caching_layer",
              "chunk_id": "arch_caching_layer_fix_0",
              "content": "# Caching Layer Architecture ## Overview The Caching Layer reduces latency and database load by caching frequently accessed data. Uses a multi-tier strategy with local and distributed caches. ## Design Principles - **Cache Invalidation**: Explicit invalidation on writes - **Graceful Degradation**: System works without cache ## Components ### Local Cache First-level cache with 100ms TTL. Reduces Redis round-trips for hot data.",
              "score": null
            },
            {
              "doc_id": "adr_004",
              "chunk_id": "adr_004_fix_0",
              "content": "# ADR-004: Use Redis for Caching and Session Storage **Status:** Accepted **Date:** 2024-04-05 ## Context We need a fast data store for caching API responses, session data, and rate limiting counters. ## Options Considered ### Option 1: Redis Cluster In-memory data structure store. **Pros:** - Sub-millisecond latency - Rich data structures - Pub/sub support - Clustering **Cons:** - Memory cost - Persistence complexity - Cluster management ### Option 2: Memcached Simple key-value cache.",
              "score": null
            },
            {
              "doc_id": "arch_caching_layer",
              "chunk_id": "arch_caching_layer_fix_2",
              "content": "**Technology:** CloudFront with Lambda@Edge **Scaling:** Global edge locations ## Data Flow Request \u2192 Local Cache \u2192 Redis \u2192 Database \u2192 Cache Population ## Performance Characteristics - **Latency P50:** 1ms (hit), 50ms (miss) - **Latency P99:** 5ms (hit), 200ms (miss) - **Throughput:** 100000 reads/second",
              "score": null
            },
            {
              "doc_id": "config_cache",
              "chunk_id": "config_cache_fix_0",
              "content": "# Cache Configuration Reference ## Overview This document describes all configuration options for Cache. ## Environment Variables | Variable | Type | Default | Description | |----------|------|---------|-------------| | `REDIS_URL` | string | required | Redis connection string | | `REDIS_POOL_SIZE` | integer | 10 | Connection pool size | | `CACHE_TTL_SECONDS` | integer | 300 | Default cache TTL | | `CACHE_PREFIX` | string | cf: | Key prefix for namespacing | | `CACHE_COMPRESSION` |",
              "score": null
            },
            {
              "doc_id": "arch_data_pipeline",
              "chunk_id": "arch_data_pipeline_fix_0",
              "content": "# Data Pipeline Architecture ## Overview The Data Pipeline processes and transforms data between workflow steps. It supports batch processing up to 10GB and streaming for real-time use cases. ## Design Principles - **Exactly-Once Processing**: Each record is processed exactly once - **Backpressure Handling**: System gracefully handles varying load ## Components ### Stream Processor Handles real-time data streams. Maximum throughput of 100000 messages per second.",
              "score": null
            }
          ],
          "latency_ms": 3.8873129997227807,
          "query_id": "complex_050",
          "dimension": "problem",
          "query": "we're hitting slow queries and cache misses everywhere - what's our current data layer stack and where is data actually stored?"
        },
        {
          "key_facts": [
            "PostgreSQL",
            "Redis Cluster",
            "S3",
            "Ristretto",
            "CloudFront",
            "Iceberg"
          ],
          "found_facts": [
            "Redis Cluster",
            "S3"
          ],
          "missed_facts": [
            "PostgreSQL",
            "Ristretto",
            "CloudFront",
            "Iceberg"
          ],
          "coverage": 0.3333333333333333,
          "expected_docs": [
            "adr_001",
            "adr_004",
            "adr_008",
            "arch_caching_layer",
            "arch_data_pipeline"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "adr_004",
              "chunk_id": "adr_004_fix_0",
              "content": "# ADR-004: Use Redis for Caching and Session Storage **Status:** Accepted **Date:** 2024-04-05 ## Context We need a fast data store for caching API responses, session data, and rate limiting counters. ## Options Considered ### Option 1: Redis Cluster In-memory data structure store. **Pros:** - Sub-millisecond latency - Rich data structures - Pub/sub support - Clustering **Cons:** - Memory cost - Persistence complexity - Cluster management ### Option 2: Memcached Simple key-value cache.",
              "score": null
            },
            {
              "doc_id": "adr_004",
              "chunk_id": "adr_004_fix_1",
              "content": "**Pros:** - Simple operation - Multi-threaded - Lower memory overhead **Cons:** - No persistence - Limited data types - No clustering ### Option 3: Application-level cache In-process caching only. **Pros:** - No external dependency - Fastest possible - Simple **Cons:** - Not shared across pods - Memory pressure - Cold starts ## Decision We will use Redis Cluster for caching with a 6-node cluster for high availability.",
              "score": null
            },
            {
              "doc_id": "adr_008",
              "chunk_id": "adr_008_fix_1",
              "content": "**Pros:** - No vendor lock-in - S3 compatible - On-premises option **Cons:** - Operational burden - Lower durability guarantees - Scaling complexity ### Option 3: Google Cloud Storage GCP object storage. **Pros:** - Strong consistency - Good performance - Unified billing **Cons:** - GCP lock-in - Fewer features than S3 - Smaller ecosystem ## Decision We will use Amazon S3 for object storage with intelligent tiering for cost optimization.",
              "score": null
            },
            {
              "doc_id": "adr_008",
              "chunk_id": "adr_008_fix_0",
              "content": "# ADR-008: Use S3 for Object Storage **Status:** Accepted **Date:** 2024-10-14 ## Context We need storage for workflow artifacts, execution outputs, and user uploads. ## Options Considered ### Option 1: Amazon S3 Object storage service. **Pros:** - 99.999999999% durability - Lifecycle policies - Event notifications - CDN integration **Cons:** - AWS lock-in - Eventual consistency for overwrites - Cost at scale ### Option 2: Self-hosted MinIO S3-compatible object storage.",
              "score": null
            },
            {
              "doc_id": "arch_caching_layer",
              "chunk_id": "arch_caching_layer_fix_0",
              "content": "# Caching Layer Architecture ## Overview The Caching Layer reduces latency and database load by caching frequently accessed data. Uses a multi-tier strategy with local and distributed caches. ## Design Principles - **Cache Invalidation**: Explicit invalidation on writes - **Graceful Degradation**: System works without cache ## Components ### Local Cache First-level cache with 100ms TTL. Reduces Redis round-trips for hot data.",
              "score": null
            }
          ],
          "latency_ms": 3.5032579999096924,
          "query_id": "complex_050",
          "dimension": "casual",
          "query": "cloudflow storage and caching stack overview"
        },
        {
          "key_facts": [
            "PostgreSQL",
            "Redis Cluster",
            "S3",
            "Ristretto",
            "CloudFront",
            "Iceberg"
          ],
          "found_facts": [
            "Redis Cluster",
            "S3"
          ],
          "missed_facts": [
            "PostgreSQL",
            "Ristretto",
            "CloudFront",
            "Iceberg"
          ],
          "coverage": 0.3333333333333333,
          "expected_docs": [
            "adr_001",
            "adr_004",
            "adr_008",
            "arch_caching_layer",
            "arch_data_pipeline"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "adr_008",
              "chunk_id": "adr_008_fix_0",
              "content": "# ADR-008: Use S3 for Object Storage **Status:** Accepted **Date:** 2024-10-14 ## Context We need storage for workflow artifacts, execution outputs, and user uploads. ## Options Considered ### Option 1: Amazon S3 Object storage service. **Pros:** - 99.999999999% durability - Lifecycle policies - Event notifications - CDN integration **Cons:** - AWS lock-in - Eventual consistency for overwrites - Cost at scale ### Option 2: Self-hosted MinIO S3-compatible object storage.",
              "score": null
            },
            {
              "doc_id": "arch_data_pipeline",
              "chunk_id": "arch_data_pipeline_fix_1",
              "content": "**Technology:** Apache Kafka with Flink **Scaling:** Partition-based scaling ### Batch Processor Processes large batch jobs. Supports data formats: JSON, CSV, Parquet, Avro. **Technology:** Apache Spark on Kubernetes **Scaling:** Dynamic resource allocation ### Data Lake Long-term data storage with time-travel queries. Retention configurable up to 7 years.",
              "score": null
            },
            {
              "doc_id": "arch_monitoring_stack",
              "chunk_id": "arch_monitoring_stack_fix_0",
              "content": "# Monitoring Stack Architecture ## Overview The Monitoring Stack provides observability into system health, performance, and errors. Includes metrics, logs, traces, and alerting. ## Design Principles - **Observability**: Every component emits metrics and traces - **Actionable Alerts**: Alerts include runbook links ## Components ### Metrics Collects time-series metrics. Retention is 15 days local, 1 year in object storage. **Technology:** Prometheus with Thanos **Scaling:** Federated with long-term storage ### Logging Centralized log aggregation.",
              "score": null
            },
            {
              "doc_id": "adr_004",
              "chunk_id": "adr_004_fix_0",
              "content": "# ADR-004: Use Redis for Caching and Session Storage **Status:** Accepted **Date:** 2024-04-05 ## Context We need a fast data store for caching API responses, session data, and rate limiting counters. ## Options Considered ### Option 1: Redis Cluster In-memory data structure store. **Pros:** - Sub-millisecond latency - Rich data structures - Pub/sub support - Clustering **Cons:** - Memory cost - Persistence complexity - Cluster management ### Option 2: Memcached Simple key-value cache.",
              "score": null
            },
            {
              "doc_id": "runbook_ssl_certificate_expiry",
              "chunk_id": "runbook_ssl_certificate_expiry_fix_2",
              "content": "Consider using backup certificate. ## Post-Incident - Document timeline in incident log - Schedule post-mortem within 48 hours - Update runbook if new patterns discovered",
              "score": null
            }
          ],
          "latency_ms": 3.5647519998747157,
          "query_id": "complex_050",
          "dimension": "contextual",
          "query": "planning our disaster recovery strategy - need a complete inventory of all storage and caching technologies so we know what to back up and replicate"
        },
        {
          "key_facts": [
            "PostgreSQL",
            "Redis Cluster",
            "S3",
            "Ristretto",
            "CloudFront",
            "Iceberg"
          ],
          "found_facts": [
            "Redis Cluster"
          ],
          "missed_facts": [
            "PostgreSQL",
            "S3",
            "Ristretto",
            "CloudFront",
            "Iceberg"
          ],
          "coverage": 0.16666666666666666,
          "expected_docs": [
            "adr_001",
            "adr_004",
            "adr_008",
            "arch_caching_layer",
            "arch_data_pipeline"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "arch_caching_layer",
              "chunk_id": "arch_caching_layer_fix_0",
              "content": "# Caching Layer Architecture ## Overview The Caching Layer reduces latency and database load by caching frequently accessed data. Uses a multi-tier strategy with local and distributed caches. ## Design Principles - **Cache Invalidation**: Explicit invalidation on writes - **Graceful Degradation**: System works without cache ## Components ### Local Cache First-level cache with 100ms TTL. Reduces Redis round-trips for hot data.",
              "score": null
            },
            {
              "doc_id": "adr_004",
              "chunk_id": "adr_004_fix_0",
              "content": "# ADR-004: Use Redis for Caching and Session Storage **Status:** Accepted **Date:** 2024-04-05 ## Context We need a fast data store for caching API responses, session data, and rate limiting counters. ## Options Considered ### Option 1: Redis Cluster In-memory data structure store. **Pros:** - Sub-millisecond latency - Rich data structures - Pub/sub support - Clustering **Cons:** - Memory cost - Persistence complexity - Cluster management ### Option 2: Memcached Simple key-value cache.",
              "score": null
            },
            {
              "doc_id": "adr_004",
              "chunk_id": "adr_004_fix_2",
              "content": "## Consequences - Implement cache-aside pattern - Set appropriate TTLs - Monitor memory usage - Plan for cache stampede prevention",
              "score": null
            },
            {
              "doc_id": "adr_004",
              "chunk_id": "adr_004_fix_1",
              "content": "**Pros:** - Simple operation - Multi-threaded - Lower memory overhead **Cons:** - No persistence - Limited data types - No clustering ### Option 3: Application-level cache In-process caching only. **Pros:** - No external dependency - Fastest possible - Simple **Cons:** - Not shared across pods - Memory pressure - Cold starts ## Decision We will use Redis Cluster for caching with a 6-node cluster for high availability.",
              "score": null
            },
            {
              "doc_id": "config_cache",
              "chunk_id": "config_cache_fix_0",
              "content": "# Cache Configuration Reference ## Overview This document describes all configuration options for Cache. ## Environment Variables | Variable | Type | Default | Description | |----------|------|---------|-------------| | `REDIS_URL` | string | required | Redis connection string | | `REDIS_POOL_SIZE` | integer | 10 | Connection pool size | | `CACHE_TTL_SECONDS` | integer | 300 | Default cache TTL | | `CACHE_PREFIX` | string | cf: | Key prefix for namespacing | | `CACHE_COMPRESSION` |",
              "score": null
            }
          ],
          "latency_ms": 3.3832339995569782,
          "query_id": "complex_050",
          "dimension": "negation",
          "query": "do we even have a proper caching strategy? what's actually persisting data vs just caching it in memory?"
        },
        {
          "key_facts": [
            "OAuth 2.0",
            "JWT",
            "bcrypt",
            "AES-256",
            "TLS",
            "Zero Trust"
          ],
          "found_facts": [
            "TLS",
            "Zero Trust"
          ],
          "missed_facts": [
            "OAuth 2.0",
            "JWT",
            "bcrypt",
            "AES-256"
          ],
          "coverage": 0.3333333333333333,
          "expected_docs": [
            "adr_003",
            "arch_authentication_service",
            "arch_api_gateway",
            "howto_set_up_data_encryption"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "arch_api_gateway",
              "chunk_id": "arch_api_gateway_fix_0",
              "content": "# API Gateway Architecture ## Overview The API Gateway handles all incoming HTTP requests, providing authentication, rate limiting, and request routing. Built on Kong with custom plugins for CloudFlow-specific functionality. ## Design Principles - **Zero Trust**: Every request is authenticated and authorized - **Defense in Depth**: Multiple layers of security validation ## Components ### Kong Gateway Primary ingress controller. Handles TLS termination, request routing, and plugin execution.",
              "score": null
            },
            {
              "doc_id": "api_secrets",
              "chunk_id": "api_secrets_fix_0",
              "content": "# Secrets API Reference ## Overview The Secrets API provides programmatic access to secrets functionality. Base URL: `https://api.cloudflow.io/v1/secrets` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /secrets List secret names (values not returned).",
              "score": null
            },
            {
              "doc_id": "api_integrations",
              "chunk_id": "api_integrations_fix_0",
              "content": "# Integrations API Reference ## Overview The Integrations API provides programmatic access to integrations functionality. Base URL: `https://api.cloudflow.io/v1/integrations` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /integrations List available integrations.",
              "score": null
            },
            {
              "doc_id": "api_audit",
              "chunk_id": "api_audit_fix_0",
              "content": "# Audit API Reference ## Overview The Audit API provides programmatic access to audit functionality. Base URL: `https://api.cloudflow.io/v1/audit` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /audit/logs Retrieve audit logs for compliance.",
              "score": null
            },
            {
              "doc_id": "howto_set_up_data_encryption",
              "chunk_id": "howto_set_up_data_encryption_fix_1",
              "content": "``` ### Step 3: Enable field-level encryption Encrypt sensitive fields within your workflow data. ```bash cloudflow encryption add-field --path $.user.ssn --key-alias sensitive-data ``` ### Step 4: Verify encryption Confirm encryption is active for your workspace. ```bash cloudflow encryption status ``` ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            }
          ],
          "latency_ms": 3.2035389995144214,
          "query_id": "complex_051",
          "dimension": "original",
          "query": "How does CloudFlow ensure security across the stack?"
        },
        {
          "key_facts": [
            "OAuth 2.0",
            "JWT",
            "bcrypt",
            "AES-256",
            "TLS",
            "Zero Trust"
          ],
          "found_facts": [
            "AES-256",
            "TLS",
            "Zero Trust"
          ],
          "missed_facts": [
            "OAuth 2.0",
            "JWT",
            "bcrypt"
          ],
          "coverage": 0.5,
          "expected_docs": [
            "adr_003",
            "arch_authentication_service",
            "arch_api_gateway",
            "howto_set_up_data_encryption"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "arch_api_gateway",
              "chunk_id": "arch_api_gateway_fix_0",
              "content": "# API Gateway Architecture ## Overview The API Gateway handles all incoming HTTP requests, providing authentication, rate limiting, and request routing. Built on Kong with custom plugins for CloudFlow-specific functionality. ## Design Principles - **Zero Trust**: Every request is authenticated and authorized - **Defense in Depth**: Multiple layers of security validation ## Components ### Kong Gateway Primary ingress controller. Handles TLS termination, request routing, and plugin execution.",
              "score": null
            },
            {
              "doc_id": "api_integrations",
              "chunk_id": "api_integrations_fix_0",
              "content": "# Integrations API Reference ## Overview The Integrations API provides programmatic access to integrations functionality. Base URL: `https://api.cloudflow.io/v1/integrations` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /integrations List available integrations.",
              "score": null
            },
            {
              "doc_id": "howto_set_up_data_encryption",
              "chunk_id": "howto_set_up_data_encryption_fix_0",
              "content": "# How to Set Up Data Encryption ## Prerequisites - CloudFlow Enterprise plan ## Steps ### Step 1: Enable encryption at rest All data is encrypted at rest using AES-256 by default. > **Note:** Encryption is automatic for all plans ### Step 2: Configure KMS Bring your own encryption keys using AWS KMS or GCP KMS. ```bash cloudflow encryption configure --kms-key arn:aws:kms:...",
              "score": null
            },
            {
              "doc_id": "api_secrets",
              "chunk_id": "api_secrets_fix_0",
              "content": "# Secrets API Reference ## Overview The Secrets API provides programmatic access to secrets functionality. Base URL: `https://api.cloudflow.io/v1/secrets` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /secrets List secret names (values not returned).",
              "score": null
            },
            {
              "doc_id": "api_executions",
              "chunk_id": "api_executions_fix_0",
              "content": "# Executions API Reference ## Overview The Executions API provides programmatic access to executions functionality. Base URL: `https://api.cloudflow.io/v1/executions` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /executions List workflow executions.",
              "score": null
            }
          ],
          "latency_ms": 3.6802770009671804,
          "query_id": "complex_051",
          "dimension": "synonym",
          "query": "what security controls and protection mechanisms are implemented throughout the cloudflow architecture?"
        },
        {
          "key_facts": [
            "OAuth 2.0",
            "JWT",
            "bcrypt",
            "AES-256",
            "TLS",
            "Zero Trust"
          ],
          "found_facts": [
            "OAuth 2.0",
            "bcrypt",
            "AES-256"
          ],
          "missed_facts": [
            "JWT",
            "TLS",
            "Zero Trust"
          ],
          "coverage": 0.5,
          "expected_docs": [
            "adr_003",
            "arch_authentication_service",
            "arch_api_gateway",
            "howto_set_up_data_encryption"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "api_audit",
              "chunk_id": "api_audit_fix_0",
              "content": "# Audit API Reference ## Overview The Audit API provides programmatic access to audit functionality. Base URL: `https://api.cloudflow.io/v1/audit` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /audit/logs Retrieve audit logs for compliance.",
              "score": null
            },
            {
              "doc_id": "arch_monitoring_stack",
              "chunk_id": "arch_monitoring_stack_fix_0",
              "content": "# Monitoring Stack Architecture ## Overview The Monitoring Stack provides observability into system health, performance, and errors. Includes metrics, logs, traces, and alerting. ## Design Principles - **Observability**: Every component emits metrics and traces - **Actionable Alerts**: Alerts include runbook links ## Components ### Metrics Collects time-series metrics. Retention is 15 days local, 1 year in object storage. **Technology:** Prometheus with Thanos **Scaling:** Federated with long-term storage ### Logging Centralized log aggregation.",
              "score": null
            },
            {
              "doc_id": "arch_authentication_service",
              "chunk_id": "arch_authentication_service_fix_0",
              "content": "# Authentication Service Architecture ## Overview The Authentication Service handles user identity, session management, and token issuance. Supports OAuth 2.0, SAML, and API key authentication. ## Design Principles - **Zero Knowledge**: Passwords never stored in plaintext - **Token Rotation**: Refresh tokens rotate on each use ## Components ### Identity Provider Manages user accounts and credentials. Password hashing uses bcrypt with cost factor 12.",
              "score": null
            },
            {
              "doc_id": "howto_set_up_data_encryption",
              "chunk_id": "howto_set_up_data_encryption_fix_0",
              "content": "# How to Set Up Data Encryption ## Prerequisites - CloudFlow Enterprise plan ## Steps ### Step 1: Enable encryption at rest All data is encrypted at rest using AES-256 by default. > **Note:** Encryption is automatic for all plans ### Step 2: Configure KMS Bring your own encryption keys using AWS KMS or GCP KMS. ```bash cloudflow encryption configure --kms-key arn:aws:kms:...",
              "score": null
            },
            {
              "doc_id": "adr_003",
              "chunk_id": "adr_003_fix_1",
              "content": "**Pros:** - Industry standard - Stateless validation - Supports refresh tokens - SSO compatible **Cons:** - Token size overhead - Cannot revoke individual tokens - Complexity ### Option 2: Session-based auth Traditional server-side sessions. **Pros:** - Simple implementation - Easy revocation - Smaller payloads **Cons:** - Requires session store - Not suitable for APIs - Scaling challenges ### Option 3: API keys only Simple key-based authentication.",
              "score": null
            }
          ],
          "latency_ms": 3.7823980001121527,
          "query_id": "complex_051",
          "dimension": "problem",
          "query": "our security audit is asking about auth, encryption, and data protection - where do I find what security measures are in place at each layer?"
        },
        {
          "key_facts": [
            "OAuth 2.0",
            "JWT",
            "bcrypt",
            "AES-256",
            "TLS",
            "Zero Trust"
          ],
          "found_facts": [
            "AES-256",
            "TLS",
            "Zero Trust"
          ],
          "missed_facts": [
            "OAuth 2.0",
            "JWT",
            "bcrypt"
          ],
          "coverage": 0.5,
          "expected_docs": [
            "adr_003",
            "arch_authentication_service",
            "arch_api_gateway",
            "howto_set_up_data_encryption"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "howto_set_up_data_encryption",
              "chunk_id": "howto_set_up_data_encryption_fix_0",
              "content": "# How to Set Up Data Encryption ## Prerequisites - CloudFlow Enterprise plan ## Steps ### Step 1: Enable encryption at rest All data is encrypted at rest using AES-256 by default. > **Note:** Encryption is automatic for all plans ### Step 2: Configure KMS Bring your own encryption keys using AWS KMS or GCP KMS. ```bash cloudflow encryption configure --kms-key arn:aws:kms:...",
              "score": null
            },
            {
              "doc_id": "howto_set_up_data_encryption",
              "chunk_id": "howto_set_up_data_encryption_fix_1",
              "content": "``` ### Step 3: Enable field-level encryption Encrypt sensitive fields within your workflow data. ```bash cloudflow encryption add-field --path $.user.ssn --key-alias sensitive-data ``` ### Step 4: Verify encryption Confirm encryption is active for your workspace. ```bash cloudflow encryption status ``` ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            },
            {
              "doc_id": "arch_api_gateway",
              "chunk_id": "arch_api_gateway_fix_0",
              "content": "# API Gateway Architecture ## Overview The API Gateway handles all incoming HTTP requests, providing authentication, rate limiting, and request routing. Built on Kong with custom plugins for CloudFlow-specific functionality. ## Design Principles - **Zero Trust**: Every request is authenticated and authorized - **Defense in Depth**: Multiple layers of security validation ## Components ### Kong Gateway Primary ingress controller. Handles TLS termination, request routing, and plugin execution.",
              "score": null
            },
            {
              "doc_id": "api_secrets",
              "chunk_id": "api_secrets_fix_0",
              "content": "# Secrets API Reference ## Overview The Secrets API provides programmatic access to secrets functionality. Base URL: `https://api.cloudflow.io/v1/secrets` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /secrets List secret names (values not returned).",
              "score": null
            },
            {
              "doc_id": "api_integrations",
              "chunk_id": "api_integrations_fix_0",
              "content": "# Integrations API Reference ## Overview The Integrations API provides programmatic access to integrations functionality. Base URL: `https://api.cloudflow.io/v1/integrations` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /integrations List available integrations.",
              "score": null
            }
          ],
          "latency_ms": 3.8673559993185336,
          "query_id": "complex_051",
          "dimension": "casual",
          "query": "cloudflow security - auth encryption tls etc full stack overview"
        },
        {
          "key_facts": [
            "OAuth 2.0",
            "JWT",
            "bcrypt",
            "AES-256",
            "TLS",
            "Zero Trust"
          ],
          "found_facts": [
            "OAuth 2.0",
            "JWT",
            "bcrypt"
          ],
          "missed_facts": [
            "AES-256",
            "TLS",
            "Zero Trust"
          ],
          "coverage": 0.5,
          "expected_docs": [
            "adr_003",
            "arch_authentication_service",
            "arch_api_gateway",
            "howto_set_up_data_encryption"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "arch_authentication_service",
              "chunk_id": "arch_authentication_service_fix_0",
              "content": "# Authentication Service Architecture ## Overview The Authentication Service handles user identity, session management, and token issuance. Supports OAuth 2.0, SAML, and API key authentication. ## Design Principles - **Zero Knowledge**: Passwords never stored in plaintext - **Token Rotation**: Refresh tokens rotate on each use ## Components ### Identity Provider Manages user accounts and credentials. Password hashing uses bcrypt with cost factor 12.",
              "score": null
            },
            {
              "doc_id": "arch_authentication_service",
              "chunk_id": "arch_authentication_service_fix_1",
              "content": "**Technology:** Custom Go service **Scaling:** Horizontal stateless ### Token Service Issues and validates access tokens. Token lifetime is 1 hour with 7-day refresh tokens. **Technology:** JWT with RS256 **Scaling:** Horizontal stateless ### SSO Gateway Integrates with external identity providers. Supports Google, Microsoft, Okta, and custom SAML.",
              "score": null
            },
            {
              "doc_id": "arch_authentication_service",
              "chunk_id": "arch_authentication_service_fix_2",
              "content": "**Technology:** Dex OIDC proxy **Scaling:** Active-passive ## Data Flow Credentials \u2192 Validation \u2192 Token Issue \u2192 Token Refresh \u2192 Logout ## Performance Characteristics - **Latency P50:** 50ms - **Latency P99:** 200ms - **Throughput:** 5000 auth/second",
              "score": null
            },
            {
              "doc_id": "adr_003",
              "chunk_id": "adr_003_fix_2",
              "content": "**Pros:** - Very simple - Low overhead - Easy to implement **Cons:** - No user context - Hard to rotate - No SSO support ## Decision We will implement OAuth 2.0 with JWT tokens. Access tokens expire after 1 hour, refresh tokens after 7 days. ## Consequences - Implement token rotation on refresh - Support API keys for machine-to-machine auth - Add MFA for admin accounts",
              "score": null
            },
            {
              "doc_id": "adr_003",
              "chunk_id": "adr_003_fix_1",
              "content": "**Pros:** - Industry standard - Stateless validation - Supports refresh tokens - SSO compatible **Cons:** - Token size overhead - Cannot revoke individual tokens - Complexity ### Option 2: Session-based auth Traditional server-side sessions. **Pros:** - Simple implementation - Easy revocation - Smaller payloads **Cons:** - Requires session store - Not suitable for APIs - Scaling challenges ### Option 3: API keys only Simple key-based authentication.",
              "score": null
            }
          ],
          "latency_ms": 3.441302000283031,
          "query_id": "complex_051",
          "dimension": "contextual",
          "query": "preparing for SOC 2 compliance review and need to document our security posture - what authentication, encryption, and network security is implemented across gateway, services, and storage?"
        },
        {
          "key_facts": [
            "OAuth 2.0",
            "JWT",
            "bcrypt",
            "AES-256",
            "TLS",
            "Zero Trust"
          ],
          "found_facts": [
            "OAuth 2.0",
            "JWT",
            "bcrypt",
            "AES-256"
          ],
          "missed_facts": [
            "TLS",
            "Zero Trust"
          ],
          "coverage": 0.6666666666666666,
          "expected_docs": [
            "adr_003",
            "arch_authentication_service",
            "arch_api_gateway",
            "howto_set_up_data_encryption"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "howto_set_up_data_encryption",
              "chunk_id": "howto_set_up_data_encryption_fix_0",
              "content": "# How to Set Up Data Encryption ## Prerequisites - CloudFlow Enterprise plan ## Steps ### Step 1: Enable encryption at rest All data is encrypted at rest using AES-256 by default. > **Note:** Encryption is automatic for all plans ### Step 2: Configure KMS Bring your own encryption keys using AWS KMS or GCP KMS. ```bash cloudflow encryption configure --kms-key arn:aws:kms:...",
              "score": null
            },
            {
              "doc_id": "arch_authentication_service",
              "chunk_id": "arch_authentication_service_fix_0",
              "content": "# Authentication Service Architecture ## Overview The Authentication Service handles user identity, session management, and token issuance. Supports OAuth 2.0, SAML, and API key authentication. ## Design Principles - **Zero Knowledge**: Passwords never stored in plaintext - **Token Rotation**: Refresh tokens rotate on each use ## Components ### Identity Provider Manages user accounts and credentials. Password hashing uses bcrypt with cost factor 12.",
              "score": null
            },
            {
              "doc_id": "howto_set_up_data_encryption",
              "chunk_id": "howto_set_up_data_encryption_fix_1",
              "content": "``` ### Step 3: Enable field-level encryption Encrypt sensitive fields within your workflow data. ```bash cloudflow encryption add-field --path $.user.ssn --key-alias sensitive-data ``` ### Step 4: Verify encryption Confirm encryption is active for your workspace. ```bash cloudflow encryption status ``` ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            },
            {
              "doc_id": "adr_003",
              "chunk_id": "adr_003_fix_0",
              "content": "# ADR-003: Implement OAuth 2.0 with JWT for Authentication **Status:** Accepted **Date:** 2024-05-08 ## Context We need a secure authentication system that supports API keys, user sessions, and third-party integrations. ## Options Considered ### Option 1: OAuth 2.0 with JWT Industry standard token-based auth.",
              "score": null
            },
            {
              "doc_id": "adr_003",
              "chunk_id": "adr_003_fix_1",
              "content": "**Pros:** - Industry standard - Stateless validation - Supports refresh tokens - SSO compatible **Cons:** - Token size overhead - Cannot revoke individual tokens - Complexity ### Option 2: Session-based auth Traditional server-side sessions. **Pros:** - Simple implementation - Easy revocation - Smaller payloads **Cons:** - Requires session store - Not suitable for APIs - Scaling challenges ### Option 3: API keys only Simple key-based authentication.",
              "score": null
            }
          ],
          "latency_ms": 3.4352809998381417,
          "query_id": "complex_051",
          "dimension": "negation",
          "query": "is our data even encrypted? and what about authentication - is it properly secured end-to-end or are there gaps?"
        },
        {
          "key_facts": [
            "DATABASE_URL",
            "REDIS_URL",
            "GATEWAY_SSL_ENABLED",
            "DATABASE_SSL_MODE"
          ],
          "found_facts": [
            "DATABASE_URL",
            "REDIS_URL",
            "DATABASE_SSL_MODE"
          ],
          "missed_facts": [
            "GATEWAY_SSL_ENABLED"
          ],
          "coverage": 0.75,
          "expected_docs": [
            "config_workflow_engine",
            "config_api_gateway",
            "config_database",
            "config_cache"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "config_database",
              "chunk_id": "config_database_fix_1",
              "content": "`DATABASE_SSL_MODE` | string | require | SSL mode: disable, require, verify-full | | `DATABASE_MIGRATION_AUTO` | boolean | false | Run migrations on startup | ## Configuration File Configuration can also be provided via `database.yaml`: ```yaml database_url: required database_pool_min: 5 database_pool_max: 20 database_statement_timeout: 30000 database_ssl_mode: require ``` ## Validation Configuration is validated at startup. Invalid configuration will prevent the service from starting.",
              "score": null
            },
            {
              "doc_id": "adr_002",
              "chunk_id": "adr_002_fix_0",
              "content": "# ADR-002: Adopt Kubernetes for Container Orchestration **Status:** Accepted **Date:** 2024-01-24 ## Context We need a platform to deploy and scale our microservices. The platform must support auto-scaling, rolling deployments, and multi-region deployment. ## Options Considered ### Option 1: Kubernetes (EKS) Industry-standard container orchestration. **Pros:** - Industry standard - Rich ecosystem - Cloud agnostic - Auto-scaling **Cons:** - Operational complexity - Learning curve - Resource overhead ### Option 2: AWS ECS Amazon's native container service.",
              "score": null
            },
            {
              "doc_id": "adr_002",
              "chunk_id": "adr_002_fix_1",
              "content": "**Pros:** - Simple operation - Native AWS integration - Lower overhead **Cons:** - AWS lock-in - Less flexible - Smaller community ### Option 3: Serverless (Lambda) Function-as-a-service approach. **Pros:** - Zero infrastructure - Pay per use - Auto-scaling **Cons:** - Cold starts - 15-minute limit - Vendor lock-in ## Decision We will use Kubernetes on EKS for container orchestration with Helm for deployments.",
              "score": null
            },
            {
              "doc_id": "config_cache",
              "chunk_id": "config_cache_fix_0",
              "content": "# Cache Configuration Reference ## Overview This document describes all configuration options for Cache. ## Environment Variables | Variable | Type | Default | Description | |----------|------|---------|-------------| | `REDIS_URL` | string | required | Redis connection string | | `REDIS_POOL_SIZE` | integer | 10 | Connection pool size | | `CACHE_TTL_SECONDS` | integer | 300 | Default cache TTL | | `CACHE_PREFIX` | string | cf: | Key prefix for namespacing | | `CACHE_COMPRESSION` |",
              "score": null
            },
            {
              "doc_id": "config_database",
              "chunk_id": "config_database_fix_0",
              "content": "# Database Configuration Reference ## Overview This document describes all configuration options for Database. ## Environment Variables | Variable | Type | Default | Description | |----------|------|---------|-------------| | `DATABASE_URL` | string | required | PostgreSQL connection string | | `DATABASE_POOL_MIN` | integer | 5 | Minimum connection pool size | | `DATABASE_POOL_MAX` | integer | 20 | Maximum connection pool size | | `DATABASE_STATEMENT_TIMEOUT` | integer | 30000 | Query timeout in milliseconds | |",
              "score": null
            }
          ],
          "latency_ms": 3.3349639998050407,
          "query_id": "complex_052",
          "dimension": "original",
          "query": "What configuration is needed for production deployment?"
        },
        {
          "key_facts": [
            "DATABASE_URL",
            "REDIS_URL",
            "GATEWAY_SSL_ENABLED",
            "DATABASE_SSL_MODE"
          ],
          "found_facts": [],
          "missed_facts": [
            "DATABASE_URL",
            "REDIS_URL",
            "GATEWAY_SSL_ENABLED",
            "DATABASE_SSL_MODE"
          ],
          "coverage": 0.0,
          "expected_docs": [
            "config_workflow_engine",
            "config_api_gateway",
            "config_database",
            "config_cache"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "howto_migrate_from_v1_to_v2_api",
              "chunk_id": "howto_migrate_from_v1_to_v2_api_fix_1",
              "content": "> **Note:** Generate new credentials in the dashboard ### Step 4: Test in staging Run your integration tests against the v2 staging endpoint. ```bash CLOUDFLOW_API_URL=https://api-v2-staging.cloudflow.io npm test ``` ### Step 5: Switch production Update production configuration to use v2 API. ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            },
            {
              "doc_id": "howto_set_up_ci_cd_pipeline",
              "chunk_id": "howto_set_up_ci_cd_pipeline_fix_0",
              "content": "# How to Set Up CI/CD Pipeline ## Prerequisites - GitHub account - CloudFlow API key - Repository admin access ## Steps ### Step 1: Create GitHub Actions workflow Create a new file at `.github/workflows/cloudflow.yml` with the deployment configuration. ```bash mkdir -p .github/workflows && touch .github/workflows/cloudflow.yml ``` ### Step 2: Configure secrets Add your CloudFlow API key to GitHub secrets.",
              "score": null
            },
            {
              "doc_id": "howto_configure_custom_domain",
              "chunk_id": "howto_configure_custom_domain_fix_0",
              "content": "# How to Configure Custom Domain ## Prerequisites - Domain with DNS access - CloudFlow Pro plan or higher ## Steps ### Step 1: Verify domain ownership Add a TXT record to verify domain ownership. ```bash cloudflow domains verify --domain app.example.com ``` ### Step 2: Configure DNS Add a CNAME record pointing to your CloudFlow endpoint.",
              "score": null
            },
            {
              "doc_id": "howto_set_up_ci_cd_pipeline",
              "chunk_id": "howto_set_up_ci_cd_pipeline_fix_1",
              "content": "```bash gh secret set CLOUDFLOW_API_KEY ``` ### Step 3: Add deployment step Add the CloudFlow deployment action to your workflow. > **Note:** Use the official cloudflow/deploy-action@v2 ### Step 4: Test the pipeline Push a commit to trigger the workflow. ```bash git push origin main ``` ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            },
            {
              "doc_id": "api_integrations",
              "chunk_id": "api_integrations_fix_0",
              "content": "# Integrations API Reference ## Overview The Integrations API provides programmatic access to integrations functionality. Base URL: `https://api.cloudflow.io/v1/integrations` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /integrations List available integrations.",
              "score": null
            }
          ],
          "latency_ms": 3.6839339991274755,
          "query_id": "complex_052",
          "dimension": "synonym",
          "query": "what env vars and settings are required to deploy cloudflow to a prod environment?"
        },
        {
          "key_facts": [
            "DATABASE_URL",
            "REDIS_URL",
            "GATEWAY_SSL_ENABLED",
            "DATABASE_SSL_MODE"
          ],
          "found_facts": [
            "DATABASE_URL",
            "GATEWAY_SSL_ENABLED",
            "DATABASE_SSL_MODE"
          ],
          "missed_facts": [
            "REDIS_URL"
          ],
          "coverage": 0.75,
          "expected_docs": [
            "config_workflow_engine",
            "config_api_gateway",
            "config_database",
            "config_cache"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "config_database",
              "chunk_id": "config_database_fix_1",
              "content": "`DATABASE_SSL_MODE` | string | require | SSL mode: disable, require, verify-full | | `DATABASE_MIGRATION_AUTO` | boolean | false | Run migrations on startup | ## Configuration File Configuration can also be provided via `database.yaml`: ```yaml database_url: required database_pool_min: 5 database_pool_max: 20 database_statement_timeout: 30000 database_ssl_mode: require ``` ## Validation Configuration is validated at startup. Invalid configuration will prevent the service from starting.",
              "score": null
            },
            {
              "doc_id": "config_api_gateway",
              "chunk_id": "config_api_gateway_fix_1",
              "content": "size | | `GATEWAY_CORS_ORIGINS` | string | * | Allowed CORS origins | | `GATEWAY_SSL_ENABLED` | boolean | true | Enable TLS termination | ## Configuration File Configuration can also be provided via `api gateway.yaml`: ```yaml gateway_port: 8000 gateway_rate_limit: 100 gateway_timeout_ms: 30000 gateway_max_body_size: 10MB gateway_cors_origins: * ``` ## Validation Configuration is validated at startup. Invalid configuration will prevent the service from starting.",
              "score": null
            },
            {
              "doc_id": "runbook_ssl_certificate_expiry",
              "chunk_id": "runbook_ssl_certificate_expiry_fix_1",
              "content": "### Step 4 Verify DNS is correct: `dig +short api.cloudflow.io` ## Resolution ### Step 1 Trigger manual renewal: `kubectl delete certificate api-cert -n production && kubectl apply -f certificate.yaml` ### Step 2 Check rate limits: Let's Encrypt has 50 certs/domain/week limit ### Step 3 Use staging issuer for testing: switch to letsencrypt-staging ClusterIssuer ### Step 4 If Let's Encrypt fails, manually upload certificate ## Escalation If certificate cannot be renewed, escalate to security team immediately.",
              "score": null
            },
            {
              "doc_id": "howto_set_up_database_connection",
              "chunk_id": "howto_set_up_database_connection_fix_1",
              "content": "> **Note:** Pool exhaustion will queue requests ### Step 3: Test connection Verify the database is accessible from CloudFlow. ```bash cloudflow db test --secret DATABASE_URL ``` ### Step 4: Enable SSL For production, always require SSL connections. ```bash cloudflow db configure --require-ssl ``` ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            },
            {
              "doc_id": "runbook_ssl_certificate_expiry",
              "chunk_id": "runbook_ssl_certificate_expiry_fix_0",
              "content": "# Runbook: SSL Certificate Expiry ## Severity **Level:** P3 - Medium **Response Time:** 15 minutes ## Symptoms - Certificate expiring within 7 days - SSL handshake failures - Browser security warnings ## Diagnosis ### Step 1 Check certificate expiry: `echo | openssl s_client -connect api.cloudflow.io:443 2>/dev/null | openssl x509 -noout -dates` ### Step 2 Verify cert-manager status: `kubectl get certificates -n production` ### Step 3 Check cert-manager logs: `kubectl logs -l app=cert-manager -n cert-manager --tail=50`",
              "score": null
            }
          ],
          "latency_ms": 3.6835440005233977,
          "query_id": "complex_052",
          "dimension": "problem",
          "query": "deploying to prod but getting connection failures and ssl errors - what are all the required config settings I might be missing?"
        },
        {
          "key_facts": [
            "DATABASE_URL",
            "REDIS_URL",
            "GATEWAY_SSL_ENABLED",
            "DATABASE_SSL_MODE"
          ],
          "found_facts": [
            "DATABASE_URL",
            "REDIS_URL",
            "GATEWAY_SSL_ENABLED",
            "DATABASE_SSL_MODE"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "config_workflow_engine",
            "config_api_gateway",
            "config_database",
            "config_cache"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "config_database",
              "chunk_id": "config_database_fix_1",
              "content": "`DATABASE_SSL_MODE` | string | require | SSL mode: disable, require, verify-full | | `DATABASE_MIGRATION_AUTO` | boolean | false | Run migrations on startup | ## Configuration File Configuration can also be provided via `database.yaml`: ```yaml database_url: required database_pool_min: 5 database_pool_max: 20 database_statement_timeout: 30000 database_ssl_mode: require ``` ## Validation Configuration is validated at startup. Invalid configuration will prevent the service from starting.",
              "score": null
            },
            {
              "doc_id": "arch_api_gateway",
              "chunk_id": "arch_api_gateway_fix_2",
              "content": "**Technology:** Redis-backed sliding window **Scaling:** Shared Redis cluster ## Data Flow Client \u2192 TLS \u2192 Kong \u2192 Auth \u2192 Rate Limit \u2192 Backend Services ## Performance Characteristics - **Latency P50:** 12ms - **Latency P99:** 45ms - **Throughput:** 50000 requests/second",
              "score": null
            },
            {
              "doc_id": "config_api_gateway",
              "chunk_id": "config_api_gateway_fix_1",
              "content": "size | | `GATEWAY_CORS_ORIGINS` | string | * | Allowed CORS origins | | `GATEWAY_SSL_ENABLED` | boolean | true | Enable TLS termination | ## Configuration File Configuration can also be provided via `api gateway.yaml`: ```yaml gateway_port: 8000 gateway_rate_limit: 100 gateway_timeout_ms: 30000 gateway_max_body_size: 10MB gateway_cors_origins: * ``` ## Validation Configuration is validated at startup. Invalid configuration will prevent the service from starting.",
              "score": null
            },
            {
              "doc_id": "howto_set_up_database_connection",
              "chunk_id": "howto_set_up_database_connection_fix_1",
              "content": "> **Note:** Pool exhaustion will queue requests ### Step 3: Test connection Verify the database is accessible from CloudFlow. ```bash cloudflow db test --secret DATABASE_URL ``` ### Step 4: Enable SSL For production, always require SSL connections. ```bash cloudflow db configure --require-ssl ``` ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            },
            {
              "doc_id": "config_cache",
              "chunk_id": "config_cache_fix_0",
              "content": "# Cache Configuration Reference ## Overview This document describes all configuration options for Cache. ## Environment Variables | Variable | Type | Default | Description | |----------|------|---------|-------------| | `REDIS_URL` | string | required | Redis connection string | | `REDIS_POOL_SIZE` | integer | 10 | Connection pool size | | `CACHE_TTL_SECONDS` | integer | 300 | Default cache TTL | | `CACHE_PREFIX` | string | cf: | Key prefix for namespacing | | `CACHE_COMPRESSION` |",
              "score": null
            }
          ],
          "latency_ms": 3.5172329990018625,
          "query_id": "complex_052",
          "dimension": "casual",
          "query": "prod config checklist - db redis gateway ssl required settings"
        },
        {
          "key_facts": [
            "DATABASE_URL",
            "REDIS_URL",
            "GATEWAY_SSL_ENABLED",
            "DATABASE_SSL_MODE"
          ],
          "found_facts": [
            "DATABASE_URL",
            "REDIS_URL",
            "DATABASE_SSL_MODE"
          ],
          "missed_facts": [
            "GATEWAY_SSL_ENABLED"
          ],
          "coverage": 0.75,
          "expected_docs": [
            "config_workflow_engine",
            "config_api_gateway",
            "config_database",
            "config_cache"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "config_database",
              "chunk_id": "config_database_fix_0",
              "content": "# Database Configuration Reference ## Overview This document describes all configuration options for Database. ## Environment Variables | Variable | Type | Default | Description | |----------|------|---------|-------------| | `DATABASE_URL` | string | required | PostgreSQL connection string | | `DATABASE_POOL_MIN` | integer | 5 | Minimum connection pool size | | `DATABASE_POOL_MAX` | integer | 20 | Maximum connection pool size | | `DATABASE_STATEMENT_TIMEOUT` | integer | 30000 | Query timeout in milliseconds | |",
              "score": null
            },
            {
              "doc_id": "config_cache",
              "chunk_id": "config_cache_fix_0",
              "content": "# Cache Configuration Reference ## Overview This document describes all configuration options for Cache. ## Environment Variables | Variable | Type | Default | Description | |----------|------|---------|-------------| | `REDIS_URL` | string | required | Redis connection string | | `REDIS_POOL_SIZE` | integer | 10 | Connection pool size | | `CACHE_TTL_SECONDS` | integer | 300 | Default cache TTL | | `CACHE_PREFIX` | string | cf: | Key prefix for namespacing | | `CACHE_COMPRESSION` |",
              "score": null
            },
            {
              "doc_id": "arch_caching_layer",
              "chunk_id": "arch_caching_layer_fix_1",
              "content": "**Technology:** Ristretto in-memory cache **Scaling:** Per-pod memory allocation ### Distributed Cache Second-level cache with configurable TTL. Maximum key size 512MB. **Technology:** Redis Cluster 7.x **Scaling:** 6-node cluster with sharding ### CDN Caches static assets and API responses. Cache hit ratio target 95%.",
              "score": null
            },
            {
              "doc_id": "config_database",
              "chunk_id": "config_database_fix_1",
              "content": "`DATABASE_SSL_MODE` | string | require | SSL mode: disable, require, verify-full | | `DATABASE_MIGRATION_AUTO` | boolean | false | Run migrations on startup | ## Configuration File Configuration can also be provided via `database.yaml`: ```yaml database_url: required database_pool_min: 5 database_pool_max: 20 database_statement_timeout: 30000 database_ssl_mode: require ``` ## Validation Configuration is validated at startup. Invalid configuration will prevent the service from starting.",
              "score": null
            },
            {
              "doc_id": "adr_004",
              "chunk_id": "adr_004_fix_1",
              "content": "**Pros:** - Simple operation - Multi-threaded - Lower memory overhead **Cons:** - No persistence - Limited data types - No clustering ### Option 3: Application-level cache In-process caching only. **Pros:** - No external dependency - Fastest possible - Simple **Cons:** - Not shared across pods - Memory pressure - Cold starts ## Decision We will use Redis Cluster for caching with a 6-node cluster for high availability.",
              "score": null
            }
          ],
          "latency_ms": 3.3811699995567324,
          "query_id": "complex_052",
          "dimension": "contextual",
          "query": "setting up a new production cluster and need to configure database, cache, and gateway - what are the essential environment variables and security settings?"
        },
        {
          "key_facts": [
            "DATABASE_URL",
            "REDIS_URL",
            "GATEWAY_SSL_ENABLED",
            "DATABASE_SSL_MODE"
          ],
          "found_facts": [
            "DATABASE_URL"
          ],
          "missed_facts": [
            "REDIS_URL",
            "GATEWAY_SSL_ENABLED",
            "DATABASE_SSL_MODE"
          ],
          "coverage": 0.25,
          "expected_docs": [
            "config_workflow_engine",
            "config_api_gateway",
            "config_database",
            "config_cache"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "runbook_database_connection_exhaustion",
              "chunk_id": "runbook_database_connection_exhaustion_fix_1",
              "content": "`SHOW POOLS` ### Step 4 Verify connection limits: `SHOW max_connections` ## Resolution ### Step 1 Kill long-running queries: `SELECT pg_terminate_backend(pid) FROM pg_stat_activity WHERE duration > interval '5 minutes'` ### Step 2 Increase PgBouncer pool size temporarily: update pgbouncer.ini ### Step 3 Restart affected services to release connections: `kubectl rollout restart deployment/api` ### Step 4 Scale down non-critical services to free connections ## Escalation If connections remain exhausted, escalate to DBA. Consider enabling connection queueing.",
              "score": null
            },
            {
              "doc_id": "release_2_9_0",
              "chunk_id": "release_2_9_0_fix_0",
              "content": "# Release Notes - v2.9.0 **Release Date:** 2023-08-01 ## New Features - Parallel step execution - Conditional branching in workflows - Slack integration improvements - Performance dashboard ## Bug Fixes - Fixed issue with special characters in secrets - Resolved timeout in large file uploads ## Upgrade Instructions ```bash cloudflow upgrade --version 2.9.0 ``` ## Known Issues None at this time.",
              "score": null
            },
            {
              "doc_id": "arch_authentication_service",
              "chunk_id": "arch_authentication_service_fix_2",
              "content": "**Technology:** Dex OIDC proxy **Scaling:** Active-passive ## Data Flow Credentials \u2192 Validation \u2192 Token Issue \u2192 Token Refresh \u2192 Logout ## Performance Characteristics - **Latency P50:** 50ms - **Latency P99:** 200ms - **Throughput:** 5000 auth/second",
              "score": null
            },
            {
              "doc_id": "howto_set_up_database_connection",
              "chunk_id": "howto_set_up_database_connection_fix_1",
              "content": "> **Note:** Pool exhaustion will queue requests ### Step 3: Test connection Verify the database is accessible from CloudFlow. ```bash cloudflow db test --secret DATABASE_URL ``` ### Step 4: Enable SSL For production, always require SSL connections. ```bash cloudflow db configure --require-ssl ``` ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            },
            {
              "doc_id": "howto_implement_retry_logic",
              "chunk_id": "howto_implement_retry_logic_fix_1",
              "content": "### Step 4: Handle final failure Configure on_failure handler to notify or escalate when all retries are exhausted. ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            }
          ],
          "latency_ms": 3.363115998581634,
          "query_id": "complex_052",
          "dimension": "negation",
          "query": "why does prod keep failing to connect? what production-only configs am I forgetting compared to dev?"
        },
        {
          "key_facts": [
            "Idempotency",
            "retry",
            "exponential backoff",
            "dead letter queues"
          ],
          "found_facts": [
            "exponential backoff"
          ],
          "missed_facts": [
            "Idempotency",
            "retry",
            "dead letter queues"
          ],
          "coverage": 0.25,
          "expected_docs": [
            "arch_workflow_engine",
            "adr_005",
            "howto_implement_retry_logic"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "howto_implement_retry_logic",
              "chunk_id": "howto_implement_retry_logic_fix_1",
              "content": "### Step 4: Handle final failure Configure on_failure handler to notify or escalate when all retries are exhausted. ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            },
            {
              "doc_id": "howto_debug_failed_execution",
              "chunk_id": "howto_debug_failed_execution_fix_0",
              "content": "# How to Debug Failed Execution ## Prerequisites - Failed execution ID ## Steps ### Step 1: Get execution details Retrieve the full execution record including step outputs. ```bash cloudflow executions get ex_123 --include-steps ``` ### Step 2: View step logs Check logs for the failed step. ```bash cloudflow executions logs ex_123 --step 3 ``` ### Step 3: Inspect input/output Review the data passed between steps.",
              "score": null
            },
            {
              "doc_id": "howto_set_up_database_connection",
              "chunk_id": "howto_set_up_database_connection_fix_1",
              "content": "> **Note:** Pool exhaustion will queue requests ### Step 3: Test connection Verify the database is accessible from CloudFlow. ```bash cloudflow db test --secret DATABASE_URL ``` ### Step 4: Enable SSL For production, always require SSL connections. ```bash cloudflow db configure --require-ssl ``` ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            },
            {
              "doc_id": "howto_debug_failed_execution",
              "chunk_id": "howto_debug_failed_execution_fix_1",
              "content": "```bash cloudflow executions inspect ex_123 --step 3 ``` ### Step 4: Enable debug mode Re-run with verbose logging enabled. ```bash cloudflow workflows execute wf_123 --debug ``` ### Step 5: Check error patterns Look for similar failures in recent executions. ```bash cloudflow executions list --status failed --since 24h ``` ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            },
            {
              "doc_id": "howto_create_api_integration",
              "chunk_id": "howto_create_api_integration_fix_1",
              "content": "```bash curl -H 'Authorization: Bearer cf_...' https://api.cloudflow.io/v1/me ``` ### Step 4: Implement rate limiting Handle rate limit responses (HTTP 429) with exponential backoff. > **Note:** Default limit is 100 requests per minute ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            }
          ],
          "latency_ms": 4.433951999089913,
          "query_id": "complex_053",
          "dimension": "original",
          "query": "How does CloudFlow handle failures and recovery?"
        },
        {
          "key_facts": [
            "Idempotency",
            "retry",
            "exponential backoff",
            "dead letter queues"
          ],
          "found_facts": [
            "Idempotency",
            "retry"
          ],
          "missed_facts": [
            "exponential backoff",
            "dead letter queues"
          ],
          "coverage": 0.5,
          "expected_docs": [
            "arch_workflow_engine",
            "adr_005",
            "howto_implement_retry_logic"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "arch_workflow_engine",
              "chunk_id": "arch_workflow_engine_fix_0",
              "content": "# Workflow Engine Architecture ## Overview The Workflow Engine is the core component responsible for executing user-defined workflows. It uses an event-driven architecture with support for parallel execution, conditional branching, and automatic retries. ## Design Principles - **Idempotency**: All operations can be safely retried without side effects - **Eventual Consistency**: State converges to consistent view within 5 seconds - **Fault Tolerance**: System continues operating despite component failures ## Components ### Scheduler Manages workflow scheduling and",
              "score": null
            },
            {
              "doc_id": "howto_implement_retry_logic",
              "chunk_id": "howto_implement_retry_logic_fix_1",
              "content": "### Step 4: Handle final failure Configure on_failure handler to notify or escalate when all retries are exhausted. ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            },
            {
              "doc_id": "api_executions",
              "chunk_id": "api_executions_fix_0",
              "content": "# Executions API Reference ## Overview The Executions API provides programmatic access to executions functionality. Base URL: `https://api.cloudflow.io/v1/executions` ## Authentication All requests require a valid API key in the `Authorization` header: ``` Authorization: Bearer <your-api-key> ``` ## Endpoints ### GET /executions List workflow executions.",
              "score": null
            },
            {
              "doc_id": "howto_debug_failed_execution",
              "chunk_id": "howto_debug_failed_execution_fix_1",
              "content": "```bash cloudflow executions inspect ex_123 --step 3 ``` ### Step 4: Enable debug mode Re-run with verbose logging enabled. ```bash cloudflow workflows execute wf_123 --debug ``` ### Step 5: Check error patterns Look for similar failures in recent executions. ```bash cloudflow executions list --status failed --since 24h ``` ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            },
            {
              "doc_id": "config_workflow_engine",
              "chunk_id": "config_workflow_engine_fix_0",
              "content": "# Workflow Engine Configuration Reference ## Overview This document describes all configuration options for Workflow Engine. ## Environment Variables | Variable | Type | Default | Description | |----------|------|---------|-------------| | `WORKFLOW_MAX_STEPS` | integer | 100 | Maximum steps per workflow | | `WORKFLOW_TIMEOUT_SECONDS` | integer | 3600 | Maximum workflow execution time | | `WORKFLOW_RETRY_LIMIT` | integer | 3 | Default retry attempts for failed steps | | `WORKFLOW_CONCURRENT_LIMIT` | integer | 10 | Maximum",
              "score": null
            }
          ],
          "latency_ms": 5.64837399906537,
          "query_id": "complex_053",
          "dimension": "synonym",
          "query": "what fault tolerance and error recovery mechanisms does cloudflow implement for workflow execution?"
        },
        {
          "key_facts": [
            "Idempotency",
            "retry",
            "exponential backoff",
            "dead letter queues"
          ],
          "found_facts": [
            "Idempotency",
            "retry",
            "exponential backoff"
          ],
          "missed_facts": [
            "dead letter queues"
          ],
          "coverage": 0.75,
          "expected_docs": [
            "arch_workflow_engine",
            "adr_005",
            "howto_implement_retry_logic"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "howto_implement_retry_logic",
              "chunk_id": "howto_implement_retry_logic_fix_0",
              "content": "# How to Implement Retry Logic ## Prerequisites - Existing workflow ## Steps ### Step 1: Define retry policy Add retry configuration to your workflow step. > **Note:** Maximum 5 retry attempts allowed ### Step 2: Configure backoff Set exponential backoff with base delay of 1 second. The formula is: delay = base * 2^attempt. ### Step 3: Add timeout Set maximum execution time to prevent infinite loops. Default timeout is 30 seconds.",
              "score": null
            },
            {
              "doc_id": "arch_workflow_engine",
              "chunk_id": "arch_workflow_engine_fix_0",
              "content": "# Workflow Engine Architecture ## Overview The Workflow Engine is the core component responsible for executing user-defined workflows. It uses an event-driven architecture with support for parallel execution, conditional branching, and automatic retries. ## Design Principles - **Idempotency**: All operations can be safely retried without side effects - **Eventual Consistency**: State converges to consistent view within 5 seconds - **Fault Tolerance**: System continues operating despite component failures ## Components ### Scheduler Manages workflow scheduling and",
              "score": null
            },
            {
              "doc_id": "adr_005",
              "chunk_id": "adr_005_fix_0",
              "content": "# ADR-005: Adopt Event-Driven Architecture for Workflow Execution **Status:** Accepted **Date:** 2024-12-04 ## Context We need an architecture that supports asynchronous workflow execution, step dependencies, and failure recovery. ## Options Considered ### Option 1: Event-driven with Kafka Publish-subscribe messaging. **Pros:** - Loose coupling - Scalability - Event replay - Audit trail **Cons:** - Eventual consistency - Operational complexity - Message ordering challenges ### Option 2: Request-response Synchronous API calls.",
              "score": null
            },
            {
              "doc_id": "howto_implement_retry_logic",
              "chunk_id": "howto_implement_retry_logic_fix_1",
              "content": "### Step 4: Handle final failure Configure on_failure handler to notify or escalate when all retries are exhausted. ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            },
            {
              "doc_id": "release_3_2_0",
              "chunk_id": "release_3_2_0_fix_0",
              "content": "# Release Notes - v3.2.0 **Release Date:** 2024-01-15 ## New Features - Workflow versioning with rollback support - New Python SDK with async support - Bulk operations API for workflows - Custom retry policies per step ## Bug Fixes - Fixed race condition in concurrent execution handler - Resolved memory leak in long-running workflows - Fixed timezone handling in scheduled triggers ## Breaking Changes - **BREAKING:** API v1 endpoints deprecated, will be removed in v4.0",
              "score": null
            }
          ],
          "latency_ms": 3.9652890009165276,
          "query_id": "complex_053",
          "dimension": "problem",
          "query": "our workflows keep failing and we're seeing duplicate executions - how does the retry and recovery system work and are there dead letter queues?"
        },
        {
          "key_facts": [
            "Idempotency",
            "retry",
            "exponential backoff",
            "dead letter queues"
          ],
          "found_facts": [],
          "missed_facts": [
            "Idempotency",
            "retry",
            "exponential backoff",
            "dead letter queues"
          ],
          "coverage": 0.0,
          "expected_docs": [
            "arch_workflow_engine",
            "adr_005",
            "howto_implement_retry_logic"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "howto_implement_retry_logic",
              "chunk_id": "howto_implement_retry_logic_fix_1",
              "content": "### Step 4: Handle final failure Configure on_failure handler to notify or escalate when all retries are exhausted. ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            },
            {
              "doc_id": "howto_configure_single_sign_on",
              "chunk_id": "howto_configure_single_sign_on_fix_1",
              "content": "> **Note:** ACS URL: https://auth.cloudflow.io/saml/callback ### Step 4: Upload IdP metadata Provide your IdP's metadata to CloudFlow. ```bash cloudflow sso configure --idp-metadata ./idp-metadata.xml ``` ### Step 5: Test login Verify SSO login works correctly. ```bash cloudflow sso test ``` ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            },
            {
              "doc_id": "howto_debug_failed_execution",
              "chunk_id": "howto_debug_failed_execution_fix_0",
              "content": "# How to Debug Failed Execution ## Prerequisites - Failed execution ID ## Steps ### Step 1: Get execution details Retrieve the full execution record including step outputs. ```bash cloudflow executions get ex_123 --include-steps ``` ### Step 2: View step logs Check logs for the failed step. ```bash cloudflow executions logs ex_123 --step 3 ``` ### Step 3: Inspect input/output Review the data passed between steps.",
              "score": null
            },
            {
              "doc_id": "howto_set_up_database_connection",
              "chunk_id": "howto_set_up_database_connection_fix_1",
              "content": "> **Note:** Pool exhaustion will queue requests ### Step 3: Test connection Verify the database is accessible from CloudFlow. ```bash cloudflow db test --secret DATABASE_URL ``` ### Step 4: Enable SSL For production, always require SSL connections. ```bash cloudflow db configure --require-ssl ``` ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            },
            {
              "doc_id": "howto_debug_failed_execution",
              "chunk_id": "howto_debug_failed_execution_fix_1",
              "content": "```bash cloudflow executions inspect ex_123 --step 3 ``` ### Step 4: Enable debug mode Re-run with verbose logging enabled. ```bash cloudflow workflows execute wf_123 --debug ``` ### Step 5: Check error patterns Look for similar failures in recent executions. ```bash cloudflow executions list --status failed --since 24h ``` ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            }
          ],
          "latency_ms": 3.4702360007941024,
          "query_id": "complex_053",
          "dimension": "casual",
          "query": "cloudflow failure handling - retries backoff dlq idempotency"
        },
        {
          "key_facts": [
            "Idempotency",
            "retry",
            "exponential backoff",
            "dead letter queues"
          ],
          "found_facts": [
            "Idempotency",
            "retry",
            "exponential backoff",
            "dead letter queues"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "arch_workflow_engine",
            "adr_005",
            "howto_implement_retry_logic"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "howto_implement_retry_logic",
              "chunk_id": "howto_implement_retry_logic_fix_1",
              "content": "### Step 4: Handle final failure Configure on_failure handler to notify or escalate when all retries are exhausted. ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            },
            {
              "doc_id": "arch_workflow_engine",
              "chunk_id": "arch_workflow_engine_fix_0",
              "content": "# Workflow Engine Architecture ## Overview The Workflow Engine is the core component responsible for executing user-defined workflows. It uses an event-driven architecture with support for parallel execution, conditional branching, and automatic retries. ## Design Principles - **Idempotency**: All operations can be safely retried without side effects - **Eventual Consistency**: State converges to consistent view within 5 seconds - **Fault Tolerance**: System continues operating despite component failures ## Components ### Scheduler Manages workflow scheduling and",
              "score": null
            },
            {
              "doc_id": "adr_005",
              "chunk_id": "adr_005_fix_0",
              "content": "# ADR-005: Adopt Event-Driven Architecture for Workflow Execution **Status:** Accepted **Date:** 2024-12-04 ## Context We need an architecture that supports asynchronous workflow execution, step dependencies, and failure recovery. ## Options Considered ### Option 1: Event-driven with Kafka Publish-subscribe messaging. **Pros:** - Loose coupling - Scalability - Event replay - Audit trail **Cons:** - Eventual consistency - Operational complexity - Message ordering challenges ### Option 2: Request-response Synchronous API calls.",
              "score": null
            },
            {
              "doc_id": "howto_implement_retry_logic",
              "chunk_id": "howto_implement_retry_logic_fix_0",
              "content": "# How to Implement Retry Logic ## Prerequisites - Existing workflow ## Steps ### Step 1: Define retry policy Add retry configuration to your workflow step. > **Note:** Maximum 5 retry attempts allowed ### Step 2: Configure backoff Set exponential backoff with base delay of 1 second. The formula is: delay = base * 2^attempt. ### Step 3: Add timeout Set maximum execution time to prevent infinite loops. Default timeout is 30 seconds.",
              "score": null
            },
            {
              "doc_id": "adr_005",
              "chunk_id": "adr_005_fix_2",
              "content": "## Consequences - Implement idempotent consumers - Use dead letter queues for failed messages - Monitor consumer lag",
              "score": null
            }
          ],
          "latency_ms": 3.4613700008776505,
          "query_id": "complex_053",
          "dimension": "contextual",
          "query": "designing a critical financial workflow that must handle failures gracefully - how do retries, idempotency, and dead letter queues work together in cloudflow?"
        },
        {
          "key_facts": [
            "Idempotency",
            "retry",
            "exponential backoff",
            "dead letter queues"
          ],
          "found_facts": [
            "Idempotency",
            "retry",
            "exponential backoff"
          ],
          "missed_facts": [
            "dead letter queues"
          ],
          "coverage": 0.75,
          "expected_docs": [
            "arch_workflow_engine",
            "adr_005",
            "howto_implement_retry_logic"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "arch_workflow_engine",
              "chunk_id": "arch_workflow_engine_fix_0",
              "content": "# Workflow Engine Architecture ## Overview The Workflow Engine is the core component responsible for executing user-defined workflows. It uses an event-driven architecture with support for parallel execution, conditional branching, and automatic retries. ## Design Principles - **Idempotency**: All operations can be safely retried without side effects - **Eventual Consistency**: State converges to consistent view within 5 seconds - **Fault Tolerance**: System continues operating despite component failures ## Components ### Scheduler Manages workflow scheduling and",
              "score": null
            },
            {
              "doc_id": "howto_implement_retry_logic",
              "chunk_id": "howto_implement_retry_logic_fix_1",
              "content": "### Step 4: Handle final failure Configure on_failure handler to notify or escalate when all retries are exhausted. ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            },
            {
              "doc_id": "howto_implement_retry_logic",
              "chunk_id": "howto_implement_retry_logic_fix_0",
              "content": "# How to Implement Retry Logic ## Prerequisites - Existing workflow ## Steps ### Step 1: Define retry policy Add retry configuration to your workflow step. > **Note:** Maximum 5 retry attempts allowed ### Step 2: Configure backoff Set exponential backoff with base delay of 1 second. The formula is: delay = base * 2^attempt. ### Step 3: Add timeout Set maximum execution time to prevent infinite loops. Default timeout is 30 seconds.",
              "score": null
            },
            {
              "doc_id": "howto_debug_failed_execution",
              "chunk_id": "howto_debug_failed_execution_fix_1",
              "content": "```bash cloudflow executions inspect ex_123 --step 3 ``` ### Step 4: Enable debug mode Re-run with verbose logging enabled. ```bash cloudflow workflows execute wf_123 --debug ``` ### Step 5: Check error patterns Look for similar failures in recent executions. ```bash cloudflow executions list --status failed --since 24h ``` ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            },
            {
              "doc_id": "howto_create_scheduled_workflow",
              "chunk_id": "howto_create_scheduled_workflow_fix_1",
              "content": "> **Note:** Use IANA timezone names like America/New_York ### Step 3: Configure overlap handling Choose behavior when previous execution is still running: skip, queue, or cancel. ### Step 4: Enable notifications Get notified on schedule failures. ```bash cloudflow workflows notify --on-failure ``` ## Verification To verify the setup completed successfully: ```bash cloudflow status ``` Expected output: `Status: OK`",
              "score": null
            }
          ],
          "latency_ms": 3.5264109992567683,
          "query_id": "complex_053",
          "dimension": "negation",
          "query": "why do failed workflows sometimes process twice? is there proper idempotency and what happens when all retries are exhausted?"
        }
      ]
    }
  ]
}