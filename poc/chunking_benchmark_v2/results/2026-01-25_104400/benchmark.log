================================================================================
Benchmark Log - 2026-01-25 10:44:00
================================================================================

[10:44:00] INFO  | Loading config from /home/fujin/Code/personal-library-manager/poc/chunking_benchmark_v2/config_fast_enrichment.yaml

================================================================================
[10:44:00] INFO  | COMPREHENSIVE RETRIEVAL BENCHMARK
================================================================================
[10:44:00] DEBUG | Timer 'total_benchmark' started

================================================================================
[10:44:00] INFO  | BENCHMARK CONFIGURATION
================================================================================
[10:44:00] METRIC | device=cuda
[10:44:00] METRIC | ollama_available=False
[10:44:00] INFO  | Device: cuda
[10:44:00] INFO  | Ollama available: False
[10:44:00] INFO  | Loading corpus...
[10:44:00] METRIC | documents=5
[10:44:00] METRIC | queries=20
[10:44:00] METRIC | total_facts=53
[10:44:00] METRIC | has_human_queries=True
[10:44:00] INFO  | Loaded 5 documents
[10:44:00] INFO  | Loaded 20 queries
[10:44:00] INFO  | Total key facts: 53
[10:44:00] INFO  | Human query variations: True
[10:44:00] INFO  | Enabled embedders: 1
[10:44:00] INFO  | Enabled rerankers: 0
[10:44:00] INFO  | Enabled LLMs: 1
[10:44:00] INFO  | Enabled chunking strategies: 1
[10:44:00] INFO  | Enabled retrieval strategies: 3
[10:44:00] METRIC | total_combinations=3
[10:44:00] METRIC | total_evaluations=3
[10:44:00] INFO  | Total strategy combinations: 3
[10:44:00] INFO  | Total evaluations (with k and metrics): 3

================================================================================
[10:44:00] INFO  | RETRIEVAL STRATEGY: semantic
================================================================================
[10:44:00] INFO  | Loading embedder: BAAI/bge-base-en-v1.5
[10:44:03] PROG  | [  1/  3] ██████░░░░░░░░░░░░░░  33.3% | semantic_bge-base-en-v1.5_fixed_512_0pct
[10:44:03] DEBUG | Chunking document 'api_reference': 1792 words, 16663 chars
[10:44:03] DEBUG |   Chunk 0: chars [0-3160] words=376 tokens≈500 preview='# CloudFlow API Reference Version 2.1.0 | Last Updated: January 2026 ## Overview The CloudFlow API i...'
[10:44:03] DEBUG |   Chunk 1: chars [3161-6111] words=379 tokens≈504 preview='## Rate Limiting To ensure fair usage and system stability, CloudFlow enforces rate limits on all AP...'
[10:44:03] DEBUG |   Chunk 2: chars [6112-8827] words=296 tokens≈393 preview='**Endpoint:** `POST /workflows` **Request Body:** ```json { "name": "Email Campaign Automation", "de...'
[10:44:03] DEBUG |   Chunk 3: chars [8828-11402] words=269 tokens≈357 preview='**Endpoint:** `GET /pipelines/{pipeline_id}/executions` **Query Parameters:** - `status` (optional):...'
[10:44:03] DEBUG |   Chunk 4: chars [11403-14151] words=365 tokens≈485 preview='### Error Response Format ```json { "error": { "code": "invalid_parameter", "message": "The 'limit' ...'
[10:44:03] DEBUG |   Chunk 5: chars [14152-15213] words=107 tokens≈142 preview='**Supported Events:** - `workflow.started` - `workflow.completed` - `workflow.failed` - `pipeline.co...'
[10:44:03] DEBUG | Created 6 chunks for document 'api_reference'
[10:44:03] DEBUG | Chunking document 'architecture_overview': 4607 words, 36722 chars
[10:44:03] DEBUG |   Chunk 0: chars [0-3230] words=370 tokens≈492 preview='# CloudFlow Platform - System Architecture Overview **Document Version:** 2.3.1 **Last Updated:** Ja...'
[10:44:03] DEBUG |   Chunk 1: chars [3231-5982] words=380 tokens≈505 preview='**Technology**: Node.js with Express.js framework **Replicas**: 12 pods (production), auto-scaling 8...'
[10:44:03] DEBUG |   Chunk 2: chars [5983-7875] words=248 tokens≈329 preview='**Technology**: Node.js with TypeScript, Bull queue library **Replicas**: 16 pods (production), auto...'
[10:44:03] DEBUG |   Chunk 3: chars [7876-9478] words=219 tokens≈291 preview='**Technology**: Go with distributed locking via Redis **Replicas**: 4 pods (production), active-pass...'
[10:44:03] DEBUG |   Chunk 4: chars [9479-11878] words=354 tokens≈470 preview='**Technology**: Node.js with worker pool pattern **Replicas**: 8 pods (production), auto-scaling 6-1...'
[10:44:03] DEBUG |   Chunk 5: chars [11879-14634] words=368 tokens≈489 preview='Success/failure events published back to Kafka ``` **Event Schema**: ```json { "event_id": "uuid-v4"...'
[10:44:03] DEBUG |   Chunk 6: chars [14635-17394] words=384 tokens≈510 preview='**Session Storage**: - Key pattern: `session:{user_id}` - TTL: 15 minutes (aligned with JWT expiry) ...'
[10:44:03] DEBUG |   Chunk 7: chars [17395-20141] words=381 tokens≈506 preview='records: 500 - Session timeout: 30 seconds --- ## Message Queue Patterns ### Pub/Sub Pattern Used fo...'
[10:44:03] DEBUG |   Chunk 8: chars [20142-22843] words=384 tokens≈510 preview='### Cache Invalidation Strategies **Time-Based Expiration (TTL)**: - Short-lived data: 5-15 minutes ...'
[10:44:03] DEBUG |   Chunk 9: chars [22844-25362] words=384 tokens≈510 preview='per API key (default: 1000 RPM) ### Secrets Management **HashiCorp Vault Integration**: - Dynamic da...'
[10:44:03] DEBUG |   Chunk 10: chars [25363-27842] words=384 tokens≈510 preview='- Execution start rate: 500 per second - Completion rate: 450 per second (average 2-second execution...'
[10:44:03] DEBUG |   Chunk 11: chars [27843-30286] words=375 tokens≈498 preview='Service (2) - PostgreSQL Primary - PostgreSQL Replica - PostgreSQL Replica - Redis Primary (2) - Red...'
[10:44:03] DEBUG |   Chunk 12: chars [30287-32834] words=376 tokens≈500 preview='Resume normal operations - Recovery time: 1-3 hours depending on data volume - Data loss: None if co...'
[10:44:03] DEBUG | Created 13 chunks for document 'architecture_overview'
[10:44:03] DEBUG | Chunking document 'deployment_guide': 2777 words, 25837 chars
[10:44:03] DEBUG |   Chunk 0: chars [0-3165] words=384 tokens≈510 preview='# CloudFlow Platform - Deployment and Operations Guide **Version:** 2.4.0 **Last Updated:** January ...'
[10:44:03] DEBUG |   Chunk 1: chars [3166-6560] words=384 tokens≈510 preview='create cluster -f cluster-config.yaml ``` ### Storage Configuration Install the EBS CSI driver for p...'
[10:44:03] DEBUG |   Chunk 2: chars [6561-8755] words=251 tokens≈333 preview='- secretName: cloudflow-tls hosts: - api.cloudflow.io healthCheck: enabled: true livenessProbe: path...'
[10:44:03] DEBUG |   Chunk 3: chars [8756-12138] words=384 tokens≈510 preview='Deploy PostgreSQL using the Bitnami Helm chart: ```yaml # postgres-values.yaml global: postgresql: a...'
[10:44:03] DEBUG |   Chunk 4: chars [12139-14139] words=204 tokens≈271 preview='--set prometheus.prometheusSpec.retention=30d \ --set prometheus.prometheusSpec.storageSpec.volumeCl...'
[10:44:03] DEBUG |   Chunk 5: chars [14140-17194] words=372 tokens≈494 preview='Configure log aggregation: ```bash # Install Fluent Bit for log forwarding helm install fluent-bit f...'
[10:44:03] DEBUG |   Chunk 6: chars [17195-20069] words=384 tokens≈510 preview='**Verify Service Health**: ```bash kubectl get pods -n cloudflow-prod curl https://api.cloudflow.io/...'
[10:44:03] DEBUG |   Chunk 7: chars [20070-23221] words=384 tokens≈510 preview='podSelector: matchLabels: app: cloudflow policyTypes: - Ingress - Egress ingress: - from: - namespac...'
[10:44:03] DEBUG |   Chunk 8: chars [23222-23448] words=30 tokens≈39 preview='UTC - **Secondary**: Wednesday 10:00-12:00 UTC All deployments should target these windows to minimi...'
[10:44:03] DEBUG | Created 9 chunks for document 'deployment_guide'
[10:44:03] DEBUG | Chunking document 'troubleshooting_guide': 4032 words, 32183 chars
[10:44:03] DEBUG |   Chunk 0: chars [0-2666] words=320 tokens≈425 preview='# CloudFlow Platform Troubleshooting Guide **Version:** 3.2.1 **Last Updated:** January 2026 **Audie...'
[10:44:03] DEBUG |   Chunk 1: chars [2667-5007] words=321 tokens≈426 preview='**Diagnosis:** ```bash # Check system time timedatectl status # Compare with NTP server ntpdate -q p...'
[10:44:03] DEBUG |   Chunk 2: chars [5008-8037] words=376 tokens≈500 preview='Review Query Execution Plans** ```sql -- Connect to CloudFlow database cloudflow db connect --readon...'
[10:44:03] DEBUG |   Chunk 3: chars [8038-10862] words=356 tokens≈473 preview='Workflow Context Accumulation** Large workflow executions may accumulate state in memory. **Solution...'
[10:44:03] DEBUG |   Chunk 4: chars [10863-13827] words=384 tokens≈510 preview='**Implement connection pooling optimization:** ```bash # Use PgBouncer for connection pooling kubect...'
[10:44:03] DEBUG |   Chunk 5: chars [13828-16401] words=334 tokens≈444 preview='exec_7h3j6k9m2n --show-bottlenecks ``` #### Solutions **1. Increase workflow timeout (if justified):...'
[10:44:03] DEBUG |   Chunk 6: chars [16402-19022] words=329 tokens≈437 preview='Data Validation Errors** ``` ValidationError: Field 'customer_id' is required but missing in 234 rec...'
[10:44:03] DEBUG |   Chunk 7: chars [19023-20595] words=194 tokens≈258 preview='Waiting {retry_after} seconds...") time.sleep(retry_after) continue remaining = int(response.headers...'
[10:44:03] DEBUG |   Chunk 8: chars [20596-23454] words=384 tokens≈510 preview='Leverage caching:** ```bash # Enable client-side caching export CLOUDFLOW_CACHE_ENABLED=true export ...'
[10:44:03] DEBUG |   Chunk 9: chars [23455-26239] words=384 tokens≈510 preview='in" # Find workflow failures by ID kubectl logs -n cloudflow deployment/cloudflow-workflow-engine | ...'
[10:44:03] DEBUG |   Chunk 10: chars [26240-29039] words=384 tokens≈510 preview='database nc -zv cloudflow-db.internal.company.com 5432 # Trace route traceroute api.cloudflow.io # C...'
[10:44:03] DEBUG |   Chunk 11: chars [29040-31206] words=266 tokens≈353 preview='outage" # Create war room cloudflow incident war-room create incident-2024012401 ``` **For SEV-2 (Hi...'
[10:44:03] DEBUG | Created 12 chunks for document 'troubleshooting_guide'
[10:44:03] DEBUG | Chunking document 'user_guide': 3735 words, 31582 chars
[10:44:03] DEBUG |   Chunk 0: chars [0-2617] words=375 tokens≈498 preview='# CloudFlow User Guide Welcome to CloudFlow, the modern workflow automation platform that helps you ...'
[10:44:03] DEBUG |   Chunk 1: chars [2618-5224] words=348 tokens≈462 preview='Open the Visual Editor by clicking **"Create Workflow"** or editing an existing workflow 2. Add a tr...'
[10:44:03] DEBUG |   Chunk 2: chars [5225-8372] words=381 tokens≈506 preview='### HTTP Requests Make HTTP requests to any API endpoint: **Configuration:** - **Method**: GET, POST...'
[10:44:03] DEBUG |   Chunk 3: chars [8373-11268] words=336 tokens≈446 preview='``` **Slack App Integration:** - Install the CloudFlow Slack app in your workspace - Authenticate on...'
[10:44:03] DEBUG |   Chunk 4: chars [11269-13371] words=375 tokens≈498 preview='### Cron Syntax Use standard cron expressions to define schedules: ``` * * * * * ┬ ┬ ┬ ┬ ┬ │ │ │ │ │...'
[10:44:03] DEBUG |   Chunk 5: chars [13372-16105] words=383 tokens≈509 preview='Save and activate **Testing Schedules:** Use the built-in schedule calculator to preview upcoming ex...'
[10:44:03] DEBUG |   Chunk 6: chars [16106-18686] words=363 tokens≈482 preview='Filter by error type, date range, or execution ID 3. Click an execution to view full details 4. Clic...'
[10:44:03] DEBUG |   Chunk 7: chars [18687-21448] words=364 tokens≈484 preview='Handle Errors Gracefully Always implement error handling for external API calls and database operati...'
[10:44:03] DEBUG |   Chunk 8: chars [21449-24677] words=384 tokens≈510 preview='Version Control For critical workflows: - Export YAML definitions regularly - Store in version contr...'
[10:44:03] DEBUG |   Chunk 9: chars [24678-27975] words=371 tokens≈493 preview='Requester: {{trigger.body.requester}} Description: {{trigger.body.description}} Approve: https://app...'
[10:44:03] DEBUG |   Chunk 10: chars [27976-28481] words=55 tokens≈73 preview='- **Documentation**: https://docs.cloudflow.io - **Community Forum**: https://community.cloudflow.io...'
[10:44:03] DEBUG | Created 11 chunks for document 'user_guide'
[10:44:03] INFO  |   Created 51 chunks
[10:44:04] INFO  |   Indexed in 1.12s
[10:44:04] TRACE | === FACT COVERAGE ANALYSIS ===
[10:44:04] TRACE | Query: What is the API rate limit per minute?
[10:44:04] TRACE | Total key facts: 2
[10:44:04] TRACE | Found facts: 2/2
[10:44:04] TRACE |   FOUND[1] 100 requests per minute per authenticated user
[10:44:04] TRACE |   FOUND[2] 20 requests per minute for unauthenticated requests
[10:44:04] TRACE | === END FACT COVERAGE ===
[10:44:04] TRACE | === FACT COVERAGE ANALYSIS ===
[10:44:04] TRACE | Query: How many requests can I make per minute to the API?
[10:44:04] TRACE | Total key facts: 2
[10:44:04] TRACE | Found facts: 2/2
[10:44:04] TRACE |   FOUND[1] 100 requests per minute per authenticated user
[10:44:04] TRACE |   FOUND[2] 20 requests per minute for unauthenticated requests
[10:44:04] TRACE | === END FACT COVERAGE ===
[10:44:04] TRACE | === FACT COVERAGE ANALYSIS ===
[10:44:04] TRACE | Query: My API calls are getting blocked, what's the limit?
[10:44:04] TRACE | Total key facts: 2
[10:44:04] TRACE | Found facts: 2/2
[10:44:04] TRACE |   FOUND[1] 100 requests per minute per authenticated user
[10:44:04] TRACE |   FOUND[2] 20 requests per minute for unauthenticated requests
[10:44:04] TRACE | === END FACT COVERAGE ===
[10:44:04] TRACE | === FACT COVERAGE ANALYSIS ===
[10:44:04] TRACE | Query: api rate limit
[10:44:04] TRACE | Total key facts: 2
[10:44:04] TRACE | Found facts: 2/2
[10:44:04] TRACE |   FOUND[1] 100 requests per minute per authenticated user
[10:44:04] TRACE |   FOUND[2] 20 requests per minute for unauthenticated requests
[10:44:04] TRACE | === END FACT COVERAGE ===
[10:44:04] TRACE | === FACT COVERAGE ANALYSIS ===
[10:44:04] TRACE | Query: I'm building a batch job that calls CloudFlow API repeatedly, what rate limit should I expect?
[10:44:04] TRACE | Total key facts: 2
[10:44:04] TRACE | Found facts: 2/2
[10:44:04] TRACE |   FOUND[1] 100 requests per minute per authenticated user
[10:44:04] TRACE |   FOUND[2] 20 requests per minute for unauthenticated requests
[10:44:04] TRACE | === END FACT COVERAGE ===
[10:44:04] TRACE | === FACT COVERAGE ANALYSIS ===
[10:44:04] TRACE | Query: Why is the API rejecting my requests after 100 calls?
[10:44:04] TRACE | Total key facts: 2
[10:44:04] TRACE | Found facts: 2/2
[10:44:04] TRACE |   FOUND[1] 100 requests per minute per authenticated user
[10:44:04] TRACE |   FOUND[2] 20 requests per minute for unauthenticated requests
[10:44:04] TRACE | === END FACT COVERAGE ===
[10:44:04] TRACE | === FACT COVERAGE ANALYSIS ===
[10:44:04] TRACE | Query: How do I fix 429 Too Many Requests errors?
[10:44:04] TRACE | Total key facts: 5
[10:44:04] TRACE | Found facts: 3/5
[10:44:04] TRACE |   FOUND[1] 429 Too Many Requests
[10:44:04] TRACE |   FOUND[2] X-RateLimit-Remaining
[10:44:04] TRACE |   FOUND[3] Retry-After
[10:44:04] TRACE | Missed facts: 2/5
[10:44:04] TRACE |   MISSED[1] Monitor X-RateLimit-Remaining header values
[10:44:04] TRACE |   MISSED[2] Implement exponential backoff when receiving 429 responses
[10:44:04] TRACE |     -> Found in chunk_id=api_reference_fix_1 (rank not in top-5)
[10:44:04] TRACE | === END FACT COVERAGE ===
[10:44:04] TRACE | === FACT COVERAGE ANALYSIS ===
[10:44:04] TRACE | Query: What should I do when I get rate limited?
[10:44:04] TRACE | Total key facts: 5
[10:44:04] TRACE | Found facts: 3/5
[10:44:04] TRACE |   FOUND[1] 429 Too Many Requests
[10:44:04] TRACE |   FOUND[2] X-RateLimit-Remaining
[10:44:04] TRACE |   FOUND[3] Implement exponential backoff when receiving 429 responses
[10:44:04] TRACE | Missed facts: 2/5
[10:44:04] TRACE |   MISSED[1] Retry-After
[10:44:04] TRACE |     -> Found in chunk_id=troubleshooting_guide_fix_6 (rank not in top-5)
[10:44:04] TRACE |   MISSED[2] Monitor X-RateLimit-Remaining header values
[10:44:04] TRACE | === END FACT COVERAGE ===
[10:44:04] TRACE | === FACT COVERAGE ANALYSIS ===
[10:44:04] TRACE | Query: My requests keep failing with 429 status code
[10:44:04] TRACE | Total key facts: 5
[10:44:04] TRACE | Found facts: 1/5
[10:44:04] TRACE |   FOUND[1] X-RateLimit-Remaining
[10:44:04] TRACE | Missed facts: 4/5
[10:44:04] TRACE |   MISSED[1] 429 Too Many Requests
[10:44:04] TRACE |     -> Found in chunk_id=api_reference_fix_1 (rank not in top-5)
[10:44:04] TRACE |   MISSED[2] Retry-After
[10:44:04] TRACE |     -> Found in chunk_id=troubleshooting_guide_fix_6 (rank not in top-5)
[10:44:04] TRACE |   MISSED[3] Monitor X-RateLimit-Remaining header values
[10:44:04] TRACE |   MISSED[4] Implement exponential backoff when receiving 429 responses
[10:44:04] TRACE |     -> Found in chunk_id=api_reference_fix_1 (rank not in top-5)
[10:44:04] TRACE | === END FACT COVERAGE ===
[10:44:04] TRACE | === FACT COVERAGE ANALYSIS ===
[10:44:04] TRACE | Query: 429 error fix
[10:44:04] TRACE | Total key facts: 5
[10:44:04] TRACE | Found facts: 3/5
[10:44:04] TRACE |   FOUND[1] 429 Too Many Requests
[10:44:04] TRACE |   FOUND[2] X-RateLimit-Remaining
[10:44:04] TRACE |   FOUND[3] Retry-After
[10:44:04] TRACE | Missed facts: 2/5
[10:44:04] TRACE |   MISSED[1] Monitor X-RateLimit-Remaining header values
[10:44:04] TRACE |   MISSED[2] Implement exponential backoff when receiving 429 responses
[10:44:04] TRACE |     -> Found in chunk_id=api_reference_fix_1 (rank not in top-5)
[10:44:04] TRACE | === END FACT COVERAGE ===
[10:44:04] TRACE | === FACT COVERAGE ANALYSIS ===
[10:44:04] TRACE | Query: I'm getting throttled by CloudFlow API, how can I handle this in my application?
[10:44:04] TRACE | Total key facts: 5
[10:44:04] TRACE | Found facts: 3/5
[10:44:04] TRACE |   FOUND[1] 429 Too Many Requests
[10:44:04] TRACE |   FOUND[2] X-RateLimit-Remaining
[10:44:04] TRACE |   FOUND[3] Implement exponential backoff when receiving 429 responses
[10:44:04] TRACE | Missed facts: 2/5
[10:44:04] TRACE |   MISSED[1] Retry-After
[10:44:04] TRACE |     -> Found in chunk_id=troubleshooting_guide_fix_6 (rank not in top-5)
[10:44:04] TRACE |   MISSED[2] Monitor X-RateLimit-Remaining header values
[10:44:04] TRACE | === END FACT COVERAGE ===
[10:44:04] TRACE | === FACT COVERAGE ANALYSIS ===
[10:44:04] TRACE | Query: Why am I being blocked with rate limit errors?
[10:44:04] TRACE | Total key facts: 5
[10:44:04] TRACE | Found facts: 4/5
[10:44:04] TRACE |   FOUND[1] 429 Too Many Requests
[10:44:04] TRACE |   FOUND[2] X-RateLimit-Remaining
[10:44:04] TRACE |   FOUND[3] Retry-After
[10:44:04] TRACE |   FOUND[4] Implement exponential backoff when receiving 429 responses
[10:44:04] TRACE | Missed facts: 1/5
[10:44:04] TRACE |   MISSED[1] Monitor X-RateLimit-Remaining header values
[10:44:04] TRACE | === END FACT COVERAGE ===
[10:44:04] TRACE | === FACT COVERAGE ANALYSIS ===
[10:44:04] TRACE | Query: What is the JWT token expiration time?
[10:44:04] TRACE | Total key facts: 3
[10:44:04] TRACE | Found facts: 1/3
[10:44:04] TRACE |   FOUND[1] 3600 seconds
[10:44:04] TRACE | Missed facts: 2/3
[10:44:04] TRACE |   MISSED[1] max 3600 seconds from iat
[10:44:04] TRACE |   MISSED[2] All tokens expire after 3600 seconds
[10:44:04] TRACE |     -> Found in chunk_id=api_reference_fix_0 (rank not in top-5)
[10:44:04] TRACE | === END FACT COVERAGE ===
[10:44:04] TRACE | === FACT COVERAGE ANALYSIS ===
[10:44:04] TRACE | Query: How long do access tokens last?
[10:44:04] TRACE | Total key facts: 3
[10:44:04] TRACE | Found facts: 1/3
[10:44:04] TRACE |   FOUND[1] 3600 seconds
[10:44:04] TRACE | Missed facts: 2/3
[10:44:04] TRACE |   MISSED[1] max 3600 seconds from iat
[10:44:04] TRACE |   MISSED[2] All tokens expire after 3600 seconds
[10:44:04] TRACE |     -> Found in chunk_id=api_reference_fix_0 (rank not in top-5)
[10:44:04] TRACE | === END FACT COVERAGE ===
[10:44:04] TRACE | === FACT COVERAGE ANALYSIS ===
[10:44:04] TRACE | Query: My authentication keeps expiring after an hour
[10:44:04] TRACE | Total key facts: 3
[10:44:04] TRACE | Found facts: 1/3
[10:44:04] TRACE |   FOUND[1] 3600 seconds
[10:44:04] TRACE | Missed facts: 2/3
[10:44:04] TRACE |   MISSED[1] max 3600 seconds from iat
[10:44:04] TRACE |   MISSED[2] All tokens expire after 3600 seconds
[10:44:04] TRACE |     -> Found in chunk_id=api_reference_fix_0 (rank not in top-5)
[10:44:04] TRACE | === END FACT COVERAGE ===
[10:44:04] TRACE | === FACT COVERAGE ANALYSIS ===
[10:44:04] TRACE | Query: token expiry time
[10:44:04] TRACE | Total key facts: 3
[10:44:04] TRACE | Found facts: 1/3
[10:44:04] TRACE |   FOUND[1] 3600 seconds
[10:44:04] TRACE | Missed facts: 2/3
[10:44:04] TRACE |   MISSED[1] max 3600 seconds from iat
[10:44:04] TRACE |   MISSED[2] All tokens expire after 3600 seconds
[10:44:04] TRACE |     -> Found in chunk_id=api_reference_fix_0 (rank not in top-5)
[10:44:04] TRACE | === END FACT COVERAGE ===
[10:44:04] TRACE | === FACT COVERAGE ANALYSIS ===
[10:44:04] TRACE | Query: I need to implement token refresh logic, when do JWT tokens expire?
[10:44:04] TRACE | Total key facts: 3
[10:44:04] TRACE | Found facts: 1/3
[10:44:04] TRACE |   FOUND[1] 3600 seconds
[10:44:04] TRACE | Missed facts: 2/3
[10:44:04] TRACE |   MISSED[1] max 3600 seconds from iat
[10:44:04] TRACE |   MISSED[2] All tokens expire after 3600 seconds
[10:44:04] TRACE |     -> Found in chunk_id=api_reference_fix_0 (rank not in top-5)
[10:44:04] TRACE | === END FACT COVERAGE ===
[10:44:04] TRACE | === FACT COVERAGE ANALYSIS ===
[10:44:04] TRACE | Query: Why does my token stop working after 3600 seconds?
[10:44:04] TRACE | Total key facts: 3
[10:44:04] TRACE | Found facts: 1/3
[10:44:04] TRACE |   FOUND[1] 3600 seconds
[10:44:04] TRACE | Missed facts: 2/3
[10:44:04] TRACE |   MISSED[1] max 3600 seconds from iat
[10:44:04] TRACE |   MISSED[2] All tokens expire after 3600 seconds
[10:44:04] TRACE |     -> Found in chunk_id=api_reference_fix_0 (rank not in top-5)
[10:44:04] TRACE | === END FACT COVERAGE ===
[10:44:04] TRACE | === FACT COVERAGE ANALYSIS ===
[10:44:04] TRACE | Query: What database technology does CloudFlow use?
[10:44:04] TRACE | Total key facts: 3
[10:44:04] TRACE | Found facts: 3/3
[10:44:04] TRACE |   FOUND[1] PostgreSQL 15.4
[10:44:04] TRACE |   FOUND[2] Redis 7.2
[10:44:04] TRACE |   FOUND[3] Apache Kafka 3.6
[10:44:04] TRACE | === END FACT COVERAGE ===
[10:44:04] TRACE | === FACT COVERAGE ANALYSIS ===
[10:44:04] TRACE | Query: Which database system powers CloudFlow?
[10:44:04] TRACE | Total key facts: 3
[10:44:04] TRACE | Found facts: 3/3
[10:44:04] TRACE |   FOUND[1] PostgreSQL 15.4
[10:44:04] TRACE |   FOUND[2] Redis 7.2
[10:44:04] TRACE |   FOUND[3] Apache Kafka 3.6
[10:44:04] TRACE | === END FACT COVERAGE ===
[10:44:04] TRACE | === FACT COVERAGE ANALYSIS ===
[10:44:04] TRACE | Query: I need to understand the data storage architecture
[10:44:04] TRACE | Total key facts: 3
[10:44:04] TRACE | Found facts: 3/3
[10:44:04] TRACE |   FOUND[1] PostgreSQL 15.4
[10:44:04] TRACE |   FOUND[2] Redis 7.2
[10:44:04] TRACE |   FOUND[3] Apache Kafka 3.6
[10:44:04] TRACE | === END FACT COVERAGE ===
[10:44:04] TRACE | === FACT COVERAGE ANALYSIS ===
[10:44:04] TRACE | Query: database stack
[10:44:04] TRACE | Total key facts: 3
[10:44:04] TRACE | Found facts: 0/3
[10:44:04] TRACE | Missed facts: 3/3
[10:44:04] TRACE |   MISSED[1] PostgreSQL 15.4
[10:44:04] TRACE |     -> Found in chunk_id=architecture_overview_fix_0 (rank not in top-5)
[10:44:04] TRACE |   MISSED[2] Redis 7.2
[10:44:04] TRACE |     -> Found in chunk_id=architecture_overview_fix_0 (rank not in top-5)
[10:44:04] TRACE |   MISSED[3] Apache Kafka 3.6
[10:44:04] TRACE |     -> Found in chunk_id=architecture_overview_fix_0 (rank not in top-5)
[10:44:04] TRACE | === END FACT COVERAGE ===
[10:44:04] TRACE | === FACT COVERAGE ANALYSIS ===
[10:44:04] TRACE | Query: For capacity planning, what databases does CloudFlow rely on?
[10:44:04] TRACE | Total key facts: 3
[10:44:04] TRACE | Found facts: 3/3
[10:44:04] TRACE |   FOUND[1] PostgreSQL 15.4
[10:44:04] TRACE |   FOUND[2] Redis 7.2
[10:44:04] TRACE |   FOUND[3] Apache Kafka 3.6
[10:44:04] TRACE | === END FACT COVERAGE ===
[10:44:04] TRACE | === FACT COVERAGE ANALYSIS ===
[10:44:04] TRACE | Query: Is CloudFlow using MySQL or something else?
[10:44:04] TRACE | Total key facts: 3
[10:44:04] TRACE | Found facts: 3/3
[10:44:04] TRACE |   FOUND[1] PostgreSQL 15.4
[10:44:04] TRACE |   FOUND[2] Redis 7.2
[10:44:04] TRACE |   FOUND[3] Apache Kafka 3.6
[10:44:04] TRACE | === END FACT COVERAGE ===
[10:44:04] TRACE | === FACT COVERAGE ANALYSIS ===
[10:44:04] TRACE | Query: What is the Kubernetes namespace for production deployment?
[10:44:04] TRACE | Total key facts: 1
[10:44:04] TRACE | Found facts: 1/1
[10:44:04] TRACE |   FOUND[1] cloudflow-prod
[10:44:04] TRACE | === END FACT COVERAGE ===
[10:44:04] TRACE | === FACT COVERAGE ANALYSIS ===
[10:44:04] TRACE | Query: Which namespace is used for CloudFlow production?
[10:44:04] TRACE | Total key facts: 1
[10:44:04] TRACE | Found facts: 1/1
[10:44:04] TRACE |   FOUND[1] cloudflow-prod
[10:44:04] TRACE | === END FACT COVERAGE ===
[10:44:04] TRACE | === FACT COVERAGE ANALYSIS ===
[10:44:04] TRACE | Query: I can't find the production pods
[10:44:04] TRACE | Total key facts: 1
[10:44:04] TRACE | Found facts: 1/1
[10:44:04] TRACE |   FOUND[1] cloudflow-prod
[10:44:04] TRACE | === END FACT COVERAGE ===
[10:44:04] TRACE | === FACT COVERAGE ANALYSIS ===
[10:44:04] TRACE | Query: prod namespace
[10:44:04] TRACE | Total key facts: 1
[10:44:04] TRACE | Found facts: 1/1
[10:44:04] TRACE |   FOUND[1] cloudflow-prod
[10:44:04] TRACE | === END FACT COVERAGE ===
[10:44:04] TRACE | === FACT COVERAGE ANALYSIS ===
[10:44:04] TRACE | Query: I need to deploy to production, what namespace should I use?
[10:44:04] TRACE | Total key facts: 1
[10:44:04] TRACE | Found facts: 1/1
[10:44:04] TRACE |   FOUND[1] cloudflow-prod
[10:44:04] TRACE | === END FACT COVERAGE ===
[10:44:04] TRACE | === FACT COVERAGE ANALYSIS ===
[10:44:04] TRACE | Query: Why can't I see resources in the default namespace?
[10:44:04] TRACE | Total key facts: 1
[10:44:04] TRACE | Found facts: 1/1
[10:44:04] TRACE |   FOUND[1] cloudflow-prod
[10:44:04] TRACE | === END FACT COVERAGE ===
[10:44:04] TRACE | === FACT COVERAGE ANALYSIS ===
[10:44:04] TRACE | Query: What are the resource requirements for the API Gateway?
[10:44:04] TRACE | Total key facts: 1
[10:44:04] TRACE | Found facts: 1/1
[10:44:04] TRACE |   FOUND[1] 2 vCPU, 4GB RAM per pod
[10:44:04] TRACE | === END FACT COVERAGE ===
[10:44:04] TRACE | === FACT COVERAGE ANALYSIS ===
[10:44:04] TRACE | Query: How much CPU and memory does the API Gateway need?
[10:44:04] TRACE | Total key facts: 1
[10:44:04] TRACE | Found facts: 1/1
[10:44:04] TRACE |   FOUND[1] 2 vCPU, 4GB RAM per pod
[10:44:04] TRACE | === END FACT COVERAGE ===
[10:44:04] TRACE | === FACT COVERAGE ANALYSIS ===
[10:44:04] TRACE | Query: The API Gateway pods keep getting OOMKilled
[10:44:04] TRACE | Total key facts: 1
[10:44:04] TRACE | Found facts: 1/1
[10:44:04] TRACE |   FOUND[1] 2 vCPU, 4GB RAM per pod
[10:44:04] TRACE | === END FACT COVERAGE ===
[10:44:04] TRACE | === FACT COVERAGE ANALYSIS ===
[10:44:04] TRACE | Query: api gateway resources
[10:44:04] TRACE | Total key facts: 1
[10:44:04] TRACE | Found facts: 0/1
[10:44:04] TRACE | Missed facts: 1/1
[10:44:04] TRACE |   MISSED[1] 2 vCPU, 4GB RAM per pod
[10:44:04] TRACE |     -> Found in chunk_id=architecture_overview_fix_1 (rank not in top-5)
[10:44:04] TRACE | === END FACT COVERAGE ===
[10:44:04] TRACE | === FACT COVERAGE ANALYSIS ===
[10:44:04] TRACE | Query: I'm provisioning infrastructure, what are the compute specs for API Gateway?
[10:44:04] TRACE | Total key facts: 1
[10:44:04] TRACE | Found facts: 1/1
[10:44:04] TRACE |   FOUND[1] 2 vCPU, 4GB RAM per pod
[10:44:04] TRACE | === END FACT COVERAGE ===
[10:44:04] TRACE | === FACT COVERAGE ANALYSIS ===
[10:44:04] TRACE | Query: Why is 1GB RAM not enough for API Gateway?
[10:44:04] TRACE | Total key facts: 1
[10:44:04] TRACE | Found facts: 1/1
[10:44:04] TRACE |   FOUND[1] 2 vCPU, 4GB RAM per pod
[10:44:04] TRACE | === END FACT COVERAGE ===
[10:44:04] TRACE | === FACT COVERAGE ANALYSIS ===
[10:44:04] TRACE | Query: What are the health check endpoints?
[10:44:04] TRACE | Total key facts: 2
[10:44:04] TRACE | Found facts: 1/2
[10:44:04] TRACE |   FOUND[1] /health
[10:44:04] TRACE | Missed facts: 1/2
[10:44:04] TRACE |   MISSED[1] /ready
[10:44:04] TRACE |     -> Found in chunk_id=deployment_guide_fix_2 (rank not in top-5)
[10:44:04] TRACE | === END FACT COVERAGE ===
[10:44:04] TRACE | === FACT COVERAGE ANALYSIS ===
[10:44:04] TRACE | Query: Which URLs should I use for health monitoring?
[10:44:04] TRACE | Total key facts: 2
[10:44:04] TRACE | Found facts: 1/2
[10:44:04] TRACE |   FOUND[1] /health
[10:44:04] TRACE | Missed facts: 1/2
[10:44:04] TRACE |   MISSED[1] /ready
[10:44:04] TRACE |     -> Found in chunk_id=deployment_guide_fix_2 (rank not in top-5)
[10:44:04] TRACE | === END FACT COVERAGE ===
[10:44:04] TRACE | === FACT COVERAGE ANALYSIS ===
[10:44:04] TRACE | Query: My load balancer health checks are failing
[10:44:04] TRACE | Total key facts: 2
[10:44:04] TRACE | Found facts: 1/2
[10:44:04] TRACE |   FOUND[1] /health
[10:44:04] TRACE | Missed facts: 1/2
[10:44:04] TRACE |   MISSED[1] /ready
[10:44:04] TRACE |     -> Found in chunk_id=deployment_guide_fix_2 (rank not in top-5)
[10:44:04] TRACE | === END FACT COVERAGE ===
[10:44:04] TRACE | === FACT COVERAGE ANALYSIS ===
[10:44:04] TRACE | Query: health endpoints
[10:44:04] TRACE | Total key facts: 2
[10:44:04] TRACE | Found facts: 1/2
[10:44:04] TRACE |   FOUND[1] /health
[10:44:04] TRACE | Missed facts: 1/2
[10:44:04] TRACE |   MISSED[1] /ready
[10:44:04] TRACE |     -> Found in chunk_id=deployment_guide_fix_2 (rank not in top-5)
[10:44:04] TRACE | === END FACT COVERAGE ===
[10:44:04] TRACE | === FACT COVERAGE ANALYSIS ===
[10:44:04] TRACE | Query: Setting up monitoring, what endpoints indicate service health?
[10:44:04] TRACE | Total key facts: 2
[10:44:04] TRACE | Found facts: 1/2
[10:44:04] TRACE |   FOUND[1] /health
[10:44:04] TRACE | Missed facts: 1/2
[10:44:04] TRACE |   MISSED[1] /ready
[10:44:04] TRACE |     -> Found in chunk_id=deployment_guide_fix_2 (rank not in top-5)
[10:44:04] TRACE | === END FACT COVERAGE ===
[10:44:04] TRACE | === FACT COVERAGE ANALYSIS ===
[10:44:04] TRACE | Query: Why doesn't /status return health information?
[10:44:04] TRACE | Total key facts: 2
[10:44:04] TRACE | Found facts: 1/2
[10:44:04] TRACE |   FOUND[1] /health
[10:44:04] TRACE | Missed facts: 1/2
[10:44:04] TRACE |   MISSED[1] /ready
[10:44:04] TRACE |     -> Found in chunk_id=deployment_guide_fix_2 (rank not in top-5)
[10:44:04] TRACE | === END FACT COVERAGE ===
[10:44:04] TRACE | === FACT COVERAGE ANALYSIS ===
[10:44:04] TRACE | Query: What are the HPA scaling parameters?
[10:44:04] TRACE | Total key facts: 3
[10:44:04] TRACE | Found facts: 2/3
[10:44:04] TRACE |   FOUND[1] minReplicas: 3
[10:44:04] TRACE |   FOUND[2] maxReplicas: 10
[10:44:04] TRACE | Missed facts: 1/3
[10:44:04] TRACE |   MISSED[1] targetCPUUtilizationPercentage: 70
[10:44:04] TRACE |     -> Found in chunk_id=deployment_guide_fix_1 (rank not in top-5)
[10:44:04] TRACE | === END FACT COVERAGE ===
[10:44:04] TRACE | === FACT COVERAGE ANALYSIS ===
[10:44:04] TRACE | Query: How does horizontal pod autoscaling work?
[10:44:04] TRACE | Total key facts: 3
[10:44:04] TRACE | Found facts: 2/3
[10:44:04] TRACE |   FOUND[1] minReplicas: 3
[10:44:04] TRACE |   FOUND[2] maxReplicas: 10
[10:44:04] TRACE | Missed facts: 1/3
[10:44:04] TRACE |   MISSED[1] targetCPUUtilizationPercentage: 70
[10:44:04] TRACE |     -> Found in chunk_id=deployment_guide_fix_1 (rank not in top-5)
[10:44:04] TRACE | === END FACT COVERAGE ===
[10:44:04] TRACE | === FACT COVERAGE ANALYSIS ===
[10:44:04] TRACE | Query: Pods aren't scaling up during traffic spikes
[10:44:04] TRACE | Total key facts: 3
[10:44:04] TRACE | Found facts: 2/3
[10:44:04] TRACE |   FOUND[1] minReplicas: 3
[10:44:04] TRACE |   FOUND[2] maxReplicas: 10
[10:44:04] TRACE | Missed facts: 1/3
[10:44:04] TRACE |   MISSED[1] targetCPUUtilizationPercentage: 70
[10:44:04] TRACE |     -> Found in chunk_id=deployment_guide_fix_1 (rank not in top-5)
[10:44:04] TRACE | === END FACT COVERAGE ===
[10:44:04] TRACE | === FACT COVERAGE ANALYSIS ===
[10:44:04] TRACE | Query: autoscaling config
[10:44:04] TRACE | Total key facts: 3
[10:44:04] TRACE | Found facts: 2/3
[10:44:04] TRACE |   FOUND[1] minReplicas: 3
[10:44:04] TRACE |   FOUND[2] maxReplicas: 10
[10:44:04] TRACE | Missed facts: 1/3
[10:44:04] TRACE |   MISSED[1] targetCPUUtilizationPercentage: 70
[10:44:04] TRACE |     -> Found in chunk_id=deployment_guide_fix_1 (rank not in top-5)
[10:44:04] TRACE | === END FACT COVERAGE ===
[10:44:04] TRACE | === FACT COVERAGE ANALYSIS ===
[10:44:04] TRACE | Query: I need to configure autoscaling, what are the min/max replicas and thresholds?
[10:44:04] TRACE | Total key facts: 3
[10:44:04] TRACE | Found facts: 2/3
[10:44:04] TRACE |   FOUND[1] minReplicas: 3
[10:44:04] TRACE |   FOUND[2] maxReplicas: 10
[10:44:04] TRACE | Missed facts: 1/3
[10:44:04] TRACE |   MISSED[1] targetCPUUtilizationPercentage: 70
[10:44:04] TRACE |     -> Found in chunk_id=deployment_guide_fix_1 (rank not in top-5)
[10:44:04] TRACE | === END FACT COVERAGE ===
[10:44:04] TRACE | === FACT COVERAGE ANALYSIS ===
[10:44:04] TRACE | Query: Why do we have 3 replicas even with low traffic?
[10:44:04] TRACE | Total key facts: 3
[10:44:04] TRACE | Found facts: 0/3
[10:44:04] TRACE | Missed facts: 3/3
[10:44:04] TRACE |   MISSED[1] minReplicas: 3
[10:44:04] TRACE |     -> Found in chunk_id=deployment_guide_fix_1 (rank not in top-5)
[10:44:04] TRACE |   MISSED[2] maxReplicas: 10
[10:44:04] TRACE |     -> Found in chunk_id=deployment_guide_fix_1 (rank not in top-5)
[10:44:04] TRACE |   MISSED[3] targetCPUUtilizationPercentage: 70
[10:44:04] TRACE |     -> Found in chunk_id=deployment_guide_fix_1 (rank not in top-5)
[10:44:04] TRACE | === END FACT COVERAGE ===
[10:44:04] TRACE | === FACT COVERAGE ANALYSIS ===
[10:44:04] TRACE | Query: What is the P99 latency target for API operations?
[10:44:04] TRACE | Total key facts: 2
[10:44:04] TRACE | Found facts: 1/2
[10:44:04] TRACE |   FOUND[1] average P99 latency of 180ms for API operations
[10:44:04] TRACE | Missed facts: 1/2
[10:44:04] TRACE |   MISSED[1] P99 latency: < 200ms
[10:44:04] TRACE |     -> Found in chunk_id=architecture_overview_fix_1 (rank not in top-5)
[10:44:04] TRACE | === END FACT COVERAGE ===
[10:44:04] TRACE | === FACT COVERAGE ANALYSIS ===
[10:44:04] TRACE | Query: What's the 99th percentile response time target?
[10:44:04] TRACE | Total key facts: 2
[10:44:04] TRACE | Found facts: 0/2
[10:44:04] TRACE | Missed facts: 2/2
[10:44:04] TRACE |   MISSED[1] P99 latency: < 200ms
[10:44:04] TRACE |     -> Found in chunk_id=architecture_overview_fix_1 (rank not in top-5)
[10:44:04] TRACE |   MISSED[2] average P99 latency of 180ms for API operations
[10:44:04] TRACE |     -> Found in chunk_id=architecture_overview_fix_0 (rank not in top-5)
[10:44:04] TRACE | === END FACT COVERAGE ===
[10:44:04] TRACE | === FACT COVERAGE ANALYSIS ===
[10:44:04] TRACE | Query: Our API latency is 500ms, is that acceptable?
[10:44:04] TRACE | Total key facts: 2
[10:44:04] TRACE | Found facts: 0/2
[10:44:04] TRACE | Missed facts: 2/2
[10:44:04] TRACE |   MISSED[1] P99 latency: < 200ms
[10:44:04] TRACE |     -> Found in chunk_id=architecture_overview_fix_1 (rank not in top-5)
[10:44:04] TRACE |   MISSED[2] average P99 latency of 180ms for API operations
[10:44:04] TRACE |     -> Found in chunk_id=architecture_overview_fix_0 (rank not in top-5)
[10:44:04] TRACE | === END FACT COVERAGE ===
[10:44:04] TRACE | === FACT COVERAGE ANALYSIS ===
[10:44:04] TRACE | Query: api latency target
[10:44:04] TRACE | Total key facts: 2
[10:44:04] TRACE | Found facts: 0/2
[10:44:04] TRACE | Missed facts: 2/2
[10:44:04] TRACE |   MISSED[1] P99 latency: < 200ms
[10:44:04] TRACE |     -> Found in chunk_id=architecture_overview_fix_1 (rank not in top-5)
[10:44:04] TRACE |   MISSED[2] average P99 latency of 180ms for API operations
[10:44:04] TRACE |     -> Found in chunk_id=architecture_overview_fix_0 (rank not in top-5)
[10:44:04] TRACE | === END FACT COVERAGE ===
[10:44:04] TRACE | === FACT COVERAGE ANALYSIS ===
[10:44:04] TRACE | Query: Setting SLOs for our service, what P99 latency does CloudFlow guarantee?
[10:44:04] TRACE | Total key facts: 2
[10:44:04] TRACE | Found facts: 1/2
[10:44:04] TRACE |   FOUND[1] average P99 latency of 180ms for API operations
[10:44:04] TRACE | Missed facts: 1/2
[10:44:04] TRACE |   MISSED[1] P99 latency: < 200ms
[10:44:04] TRACE |     -> Found in chunk_id=architecture_overview_fix_1 (rank not in top-5)
[10:44:04] TRACE | === END FACT COVERAGE ===
[10:44:04] TRACE | === FACT COVERAGE ANALYSIS ===
[10:44:04] TRACE | Query: Why are we getting alerts at 200ms latency?
[10:44:04] TRACE | Total key facts: 2
[10:44:04] TRACE | Found facts: 0/2
[10:44:04] TRACE | Missed facts: 2/2
[10:44:04] TRACE |   MISSED[1] P99 latency: < 200ms
[10:44:04] TRACE |     -> Found in chunk_id=architecture_overview_fix_1 (rank not in top-5)
[10:44:04] TRACE |   MISSED[2] average P99 latency of 180ms for API operations
[10:44:04] TRACE |     -> Found in chunk_id=architecture_overview_fix_0 (rank not in top-5)
[10:44:04] TRACE | === END FACT COVERAGE ===
[10:44:04] TRACE | === FACT COVERAGE ANALYSIS ===
[10:44:04] TRACE | Query: What are the disaster recovery RPO and RTO values?
[10:44:04] TRACE | Total key facts: 2
[10:44:04] TRACE | Found facts: 0/2
[10:44:04] TRACE | Missed facts: 2/2
[10:44:04] TRACE |   MISSED[1] RPO (Recovery Point Objective): 1 hour
[10:44:04] TRACE |     -> Found in chunk_id=architecture_overview_fix_10 (rank not in top-5)
[10:44:04] TRACE |   MISSED[2] RTO (Recovery Time Objective): 4 hours
[10:44:04] TRACE |     -> Found in chunk_id=architecture_overview_fix_10 (rank not in top-5)
[10:44:04] TRACE | === END FACT COVERAGE ===
[10:44:04] TRACE | === FACT COVERAGE ANALYSIS ===
[10:44:04] TRACE | Query: What's the maximum data loss and recovery time?
[10:44:04] TRACE | Total key facts: 2
[10:44:04] TRACE | Found facts: 0/2
[10:44:04] TRACE | Missed facts: 2/2
[10:44:04] TRACE |   MISSED[1] RPO (Recovery Point Objective): 1 hour
[10:44:04] TRACE |     -> Found in chunk_id=architecture_overview_fix_10 (rank not in top-5)
[10:44:04] TRACE |   MISSED[2] RTO (Recovery Time Objective): 4 hours
[10:44:04] TRACE |     -> Found in chunk_id=architecture_overview_fix_10 (rank not in top-5)
[10:44:04] TRACE | === END FACT COVERAGE ===
[10:44:04] TRACE | === FACT COVERAGE ANALYSIS ===
[10:44:04] TRACE | Query: How long will it take to recover from a disaster?
[10:44:04] TRACE | Total key facts: 2
[10:44:04] TRACE | Found facts: 0/2
[10:44:04] TRACE | Missed facts: 2/2
[10:44:04] TRACE |   MISSED[1] RPO (Recovery Point Objective): 1 hour
[10:44:04] TRACE |     -> Found in chunk_id=architecture_overview_fix_10 (rank not in top-5)
[10:44:04] TRACE |   MISSED[2] RTO (Recovery Time Objective): 4 hours
[10:44:04] TRACE |     -> Found in chunk_id=architecture_overview_fix_10 (rank not in top-5)
[10:44:04] TRACE | === END FACT COVERAGE ===
[10:44:04] TRACE | === FACT COVERAGE ANALYSIS ===
[10:44:04] TRACE | Query: RPO RTO
[10:44:04] TRACE | Total key facts: 2
[10:44:04] TRACE | Found facts: 0/2
[10:44:04] TRACE | Missed facts: 2/2
[10:44:04] TRACE |   MISSED[1] RPO (Recovery Point Objective): 1 hour
[10:44:04] TRACE |     -> Found in chunk_id=architecture_overview_fix_10 (rank not in top-5)
[10:44:04] TRACE |   MISSED[2] RTO (Recovery Time Objective): 4 hours
[10:44:04] TRACE |     -> Found in chunk_id=architecture_overview_fix_10 (rank not in top-5)
[10:44:04] TRACE | === END FACT COVERAGE ===
[10:44:04] TRACE | === FACT COVERAGE ANALYSIS ===
[10:44:04] TRACE | Query: For our business continuity plan, what are CloudFlow's recovery objectives?
[10:44:04] TRACE | Total key facts: 2
[10:44:04] TRACE | Found facts: 0/2
[10:44:04] TRACE | Missed facts: 2/2
[10:44:04] TRACE |   MISSED[1] RPO (Recovery Point Objective): 1 hour
[10:44:04] TRACE |     -> Found in chunk_id=architecture_overview_fix_10 (rank not in top-5)
[10:44:04] TRACE |   MISSED[2] RTO (Recovery Time Objective): 4 hours
[10:44:04] TRACE |     -> Found in chunk_id=architecture_overview_fix_10 (rank not in top-5)
[10:44:04] TRACE | === END FACT COVERAGE ===
[10:44:04] TRACE | === FACT COVERAGE ANALYSIS ===
[10:44:04] TRACE | Query: Why can't we guarantee zero data loss?
[10:44:04] TRACE | Total key facts: 2
[10:44:04] TRACE | Found facts: 0/2
[10:44:04] TRACE | Missed facts: 2/2
[10:44:04] TRACE |   MISSED[1] RPO (Recovery Point Objective): 1 hour
[10:44:04] TRACE |     -> Found in chunk_id=architecture_overview_fix_10 (rank not in top-5)
[10:44:04] TRACE |   MISSED[2] RTO (Recovery Time Objective): 4 hours
[10:44:04] TRACE |     -> Found in chunk_id=architecture_overview_fix_10 (rank not in top-5)
[10:44:04] TRACE | === END FACT COVERAGE ===
[10:44:04] TRACE | === FACT COVERAGE ANALYSIS ===
[10:44:04] TRACE | Query: What is the maximum workflow execution timeout?
[10:44:04] TRACE | Total key facts: 2
[10:44:04] TRACE | Found facts: 1/2
[10:44:04] TRACE |   FOUND[1] 3600 seconds
[10:44:04] TRACE | Missed facts: 1/2
[10:44:04] TRACE |   MISSED[1] exceeded maximum execution time of 3600 seconds
[10:44:04] TRACE |     -> Found in chunk_id=troubleshooting_guide_fix_4 (rank not in top-5)
[10:44:04] TRACE | === END FACT COVERAGE ===
[10:44:04] TRACE | === FACT COVERAGE ANALYSIS ===
[10:44:04] TRACE | Query: How long can a workflow run before timing out?
[10:44:04] TRACE | Total key facts: 2
[10:44:04] TRACE | Found facts: 1/2
[10:44:04] TRACE |   FOUND[1] 3600 seconds
[10:44:04] TRACE | Missed facts: 1/2
[10:44:04] TRACE |   MISSED[1] exceeded maximum execution time of 3600 seconds
[10:44:04] TRACE |     -> Found in chunk_id=troubleshooting_guide_fix_4 (rank not in top-5)
[10:44:04] TRACE | === END FACT COVERAGE ===
[10:44:04] TRACE | === FACT COVERAGE ANALYSIS ===
[10:44:04] TRACE | Query: My workflow is being killed after an hour
[10:44:04] TRACE | Total key facts: 2
[10:44:04] TRACE | Found facts: 1/2
[10:44:04] TRACE |   FOUND[1] 3600 seconds
[10:44:04] TRACE | Missed facts: 1/2
[10:44:04] TRACE |   MISSED[1] exceeded maximum execution time of 3600 seconds
[10:44:04] TRACE |     -> Found in chunk_id=troubleshooting_guide_fix_4 (rank not in top-5)
[10:44:04] TRACE | === END FACT COVERAGE ===
[10:44:04] TRACE | === FACT COVERAGE ANALYSIS ===
[10:44:04] TRACE | Query: workflow timeout
[10:44:04] TRACE | Total key facts: 2
[10:44:04] TRACE | Found facts: 1/2
[10:44:04] TRACE |   FOUND[1] 3600 seconds
[10:44:04] TRACE | Missed facts: 1/2
[10:44:04] TRACE |   MISSED[1] exceeded maximum execution time of 3600 seconds
[10:44:04] TRACE |     -> Found in chunk_id=troubleshooting_guide_fix_4 (rank not in top-5)
[10:44:04] TRACE | === END FACT COVERAGE ===
[10:44:04] TRACE | === FACT COVERAGE ANALYSIS ===
[10:44:04] TRACE | Query: I have a long-running data processing workflow, what's the time limit?
[10:44:04] TRACE | Total key facts: 2
[10:44:04] TRACE | Found facts: 1/2
[10:44:04] TRACE |   FOUND[1] 3600 seconds
[10:44:04] TRACE | Missed facts: 1/2
[10:44:04] TRACE |   MISSED[1] exceeded maximum execution time of 3600 seconds
[10:44:04] TRACE |     -> Found in chunk_id=troubleshooting_guide_fix_4 (rank not in top-5)
[10:44:04] TRACE | === END FACT COVERAGE ===
[10:44:04] TRACE | === FACT COVERAGE ANALYSIS ===
[10:44:04] TRACE | Query: Why did my workflow fail after 3600 seconds?
[10:44:04] TRACE | Total key facts: 2
[10:44:04] TRACE | Found facts: 1/2
[10:44:04] TRACE |   FOUND[1] 3600 seconds
[10:44:04] TRACE | Missed facts: 1/2
[10:44:04] TRACE |   MISSED[1] exceeded maximum execution time of 3600 seconds
[10:44:04] TRACE |     -> Found in chunk_id=troubleshooting_guide_fix_4 (rank not in top-5)
[10:44:04] TRACE | === END FACT COVERAGE ===
[10:44:04] TRACE | === FACT COVERAGE ANALYSIS ===
[10:44:04] TRACE | Query: What JWT algorithm is used for token signing?
[10:44:04] TRACE | Total key facts: 3
[10:44:04] TRACE | Found facts: 2/3
[10:44:04] TRACE |   FOUND[1] RS256
[10:44:04] TRACE |   FOUND[2] RS256 algorithm
[10:44:04] TRACE | Missed facts: 1/3
[10:44:04] TRACE |   MISSED[1] RS256 signing algorithm
[10:44:04] TRACE |     -> Found in chunk_id=api_reference_fix_0 (rank not in top-5)
[10:44:04] TRACE | === END FACT COVERAGE ===
[10:44:04] TRACE | === FACT COVERAGE ANALYSIS ===
[10:44:04] TRACE | Query: Which signing algorithm does CloudFlow use for JWTs?
[10:44:04] TRACE | Total key facts: 3
[10:44:04] TRACE | Found facts: 2/3
[10:44:04] TRACE |   FOUND[1] RS256
[10:44:04] TRACE |   FOUND[2] RS256 signing algorithm
[10:44:04] TRACE | Missed facts: 1/3
[10:44:04] TRACE |   MISSED[1] RS256 algorithm
[10:44:04] TRACE |     -> Found in chunk_id=architecture_overview_fix_1 (rank not in top-5)
[10:44:04] TRACE | === END FACT COVERAGE ===
[10:44:04] TRACE | === FACT COVERAGE ANALYSIS ===
[10:44:04] TRACE | Query: My JWT validation is failing with algorithm mismatch
[10:44:04] TRACE | Total key facts: 3
[10:44:04] TRACE | Found facts: 2/3
[10:44:04] TRACE |   FOUND[1] RS256
[10:44:04] TRACE |   FOUND[2] RS256 algorithm
[10:44:04] TRACE | Missed facts: 1/3
[10:44:04] TRACE |   MISSED[1] RS256 signing algorithm
[10:44:04] TRACE |     -> Found in chunk_id=api_reference_fix_0 (rank not in top-5)
[10:44:04] TRACE | === END FACT COVERAGE ===
[10:44:04] TRACE | === FACT COVERAGE ANALYSIS ===
[10:44:04] TRACE | Query: jwt algorithm
[10:44:04] TRACE | Total key facts: 3
[10:44:04] TRACE | Found facts: 2/3
[10:44:04] TRACE |   FOUND[1] RS256
[10:44:04] TRACE |   FOUND[2] RS256 algorithm
[10:44:04] TRACE | Missed facts: 1/3
[10:44:04] TRACE |   MISSED[1] RS256 signing algorithm
[10:44:04] TRACE |     -> Found in chunk_id=api_reference_fix_0 (rank not in top-5)
[10:44:04] TRACE | === END FACT COVERAGE ===
[10:44:04] TRACE | === FACT COVERAGE ANALYSIS ===
[10:44:04] TRACE | Query: I'm implementing JWT verification, what algorithm should I expect?
[10:44:04] TRACE | Total key facts: 3
[10:44:04] TRACE | Found facts: 2/3
[10:44:04] TRACE |   FOUND[1] RS256
[10:44:04] TRACE |   FOUND[2] RS256 algorithm
[10:44:04] TRACE | Missed facts: 1/3
[10:44:04] TRACE |   MISSED[1] RS256 signing algorithm
[10:44:04] TRACE |     -> Found in chunk_id=api_reference_fix_0 (rank not in top-5)
[10:44:04] TRACE | === END FACT COVERAGE ===
[10:44:04] TRACE | === FACT COVERAGE ANALYSIS ===
[10:44:04] TRACE | Query: Why doesn't HS256 work for token validation?
[10:44:04] TRACE | Total key facts: 3
[10:44:04] TRACE | Found facts: 2/3
[10:44:04] TRACE |   FOUND[1] RS256
[10:44:04] TRACE |   FOUND[2] RS256 algorithm
[10:44:04] TRACE | Missed facts: 1/3
[10:44:04] TRACE |   MISSED[1] RS256 signing algorithm
[10:44:04] TRACE |     -> Found in chunk_id=api_reference_fix_0 (rank not in top-5)
[10:44:04] TRACE | === END FACT COVERAGE ===
[10:44:04] TRACE | === FACT COVERAGE ANALYSIS ===
[10:44:04] TRACE | Query: What is the Redis cache TTL for workflow definitions?
[10:44:04] TRACE | Total key facts: 2
[10:44:04] TRACE | Found facts: 1/2
[10:44:04] TRACE |   FOUND[1] TTL: 1 hour
[10:44:04] TRACE | Missed facts: 1/2
[10:44:04] TRACE |   MISSED[1] Workflow Definitions: TTL: 1 hour
[10:44:04] TRACE | === END FACT COVERAGE ===
[10:44:04] TRACE | === FACT COVERAGE ANALYSIS ===
[10:44:04] TRACE | Query: How long are workflow definitions cached?
[10:44:04] TRACE | Total key facts: 2
[10:44:04] TRACE | Found facts: 1/2
[10:44:04] TRACE |   FOUND[1] TTL: 1 hour
[10:44:04] TRACE | Missed facts: 1/2
[10:44:04] TRACE |   MISSED[1] Workflow Definitions: TTL: 1 hour
[10:44:04] TRACE | === END FACT COVERAGE ===
[10:44:04] TRACE | === FACT COVERAGE ANALYSIS ===
[10:44:04] TRACE | Query: My workflow updates aren't reflecting immediately
[10:44:04] TRACE | Total key facts: 2
[10:44:04] TRACE | Found facts: 0/2
[10:44:04] TRACE | Missed facts: 2/2
[10:44:04] TRACE |   MISSED[1] TTL: 1 hour
[10:44:04] TRACE |     -> Found in chunk_id=architecture_overview_fix_6 (rank not in top-5)
[10:44:04] TRACE |   MISSED[2] Workflow Definitions: TTL: 1 hour
[10:44:04] TRACE | === END FACT COVERAGE ===
[10:44:04] TRACE | === FACT COVERAGE ANALYSIS ===
[10:44:04] TRACE | Query: cache ttl workflows
[10:44:04] TRACE | Total key facts: 2
[10:44:04] TRACE | Found facts: 1/2
[10:44:04] TRACE |   FOUND[1] TTL: 1 hour
[10:44:04] TRACE | Missed facts: 1/2
[10:44:04] TRACE |   MISSED[1] Workflow Definitions: TTL: 1 hour
[10:44:04] TRACE | === END FACT COVERAGE ===
[10:44:04] TRACE | === FACT COVERAGE ANALYSIS ===
[10:44:04] TRACE | Query: After updating a workflow, how long until the cache expires?
[10:44:04] TRACE | Total key facts: 2
[10:44:04] TRACE | Found facts: 0/2
[10:44:04] TRACE | Missed facts: 2/2
[10:44:04] TRACE |   MISSED[1] TTL: 1 hour
[10:44:04] TRACE |     -> Found in chunk_id=architecture_overview_fix_6 (rank not in top-5)
[10:44:04] TRACE |   MISSED[2] Workflow Definitions: TTL: 1 hour
[10:44:04] TRACE | === END FACT COVERAGE ===
[10:44:04] TRACE | === FACT COVERAGE ANALYSIS ===
[10:44:04] TRACE | Query: Why are changes taking an hour to appear?
[10:44:04] TRACE | Total key facts: 2
[10:44:04] TRACE | Found facts: 0/2
[10:44:04] TRACE | Missed facts: 2/2
[10:44:04] TRACE |   MISSED[1] TTL: 1 hour
[10:44:04] TRACE |     -> Found in chunk_id=architecture_overview_fix_6 (rank not in top-5)
[10:44:04] TRACE |   MISSED[2] Workflow Definitions: TTL: 1 hour
[10:44:04] TRACE | === END FACT COVERAGE ===
[10:44:04] TRACE | === FACT COVERAGE ANALYSIS ===
[10:44:04] TRACE | Query: What monitoring tools does CloudFlow use?
[10:44:04] TRACE | Total key facts: 3
[10:44:04] TRACE | Found facts: 3/3
[10:44:04] TRACE |   FOUND[1] Prometheus
[10:44:04] TRACE |   FOUND[2] Grafana
[10:44:04] TRACE |   FOUND[3] Jaeger for distributed tracing
[10:44:04] TRACE | === END FACT COVERAGE ===
[10:44:04] TRACE | === FACT COVERAGE ANALYSIS ===
[10:44:04] TRACE | Query: Which observability platform is integrated?
[10:44:04] TRACE | Total key facts: 3
[10:44:04] TRACE | Found facts: 3/3
[10:44:04] TRACE |   FOUND[1] Prometheus
[10:44:04] TRACE |   FOUND[2] Grafana
[10:44:04] TRACE |   FOUND[3] Jaeger for distributed tracing
[10:44:04] TRACE | === END FACT COVERAGE ===
[10:44:04] TRACE | === FACT COVERAGE ANALYSIS ===
[10:44:04] TRACE | Query: Where can I view CloudFlow metrics and logs?
[10:44:04] TRACE | Total key facts: 3
[10:44:04] TRACE | Found facts: 2/3
[10:44:04] TRACE |   FOUND[1] Prometheus
[10:44:04] TRACE |   FOUND[2] Grafana
[10:44:04] TRACE | Missed facts: 1/3
[10:44:04] TRACE |   MISSED[1] Jaeger for distributed tracing
[10:44:04] TRACE |     -> Found in chunk_id=architecture_overview_fix_0 (rank not in top-5)
[10:44:04] TRACE | === END FACT COVERAGE ===
[10:44:04] TRACE | === FACT COVERAGE ANALYSIS ===
[10:44:04] TRACE | Query: monitoring stack
[10:44:04] TRACE | Total key facts: 3
[10:44:04] TRACE | Found facts: 0/3
[10:44:04] TRACE | Missed facts: 3/3
[10:44:04] TRACE |   MISSED[1] Prometheus
[10:44:04] TRACE |     -> Found in chunk_id=architecture_overview_fix_0 (rank not in top-5)
[10:44:04] TRACE |   MISSED[2] Grafana
[10:44:04] TRACE |     -> Found in chunk_id=architecture_overview_fix_0 (rank not in top-5)
[10:44:04] TRACE |   MISSED[3] Jaeger for distributed tracing
[10:44:04] TRACE |     -> Found in chunk_id=architecture_overview_fix_0 (rank not in top-5)
[10:44:04] TRACE | === END FACT COVERAGE ===
[10:44:04] TRACE | === FACT COVERAGE ANALYSIS ===
[10:44:04] TRACE | Query: I need to set up dashboards, what monitoring systems are available?
[10:44:04] TRACE | Total key facts: 3
[10:44:04] TRACE | Found facts: 2/3
[10:44:04] TRACE |   FOUND[1] Prometheus
[10:44:04] TRACE |   FOUND[2] Grafana
[10:44:04] TRACE | Missed facts: 1/3
[10:44:04] TRACE |   MISSED[1] Jaeger for distributed tracing
[10:44:04] TRACE |     -> Found in chunk_id=architecture_overview_fix_0 (rank not in top-5)
[10:44:04] TRACE | === END FACT COVERAGE ===
[10:44:04] TRACE | === FACT COVERAGE ANALYSIS ===
[10:44:04] TRACE | Query: Why can't I see metrics in Datadog?
[10:44:04] TRACE | Total key facts: 3
[10:44:04] TRACE | Found facts: 2/3
[10:44:04] TRACE |   FOUND[1] Prometheus
[10:44:04] TRACE |   FOUND[2] Grafana
[10:44:04] TRACE | Missed facts: 1/3
[10:44:04] TRACE |   MISSED[1] Jaeger for distributed tracing
[10:44:04] TRACE |     -> Found in chunk_id=architecture_overview_fix_0 (rank not in top-5)
[10:44:04] TRACE | === END FACT COVERAGE ===
[10:44:04] TRACE | === FACT COVERAGE ANALYSIS ===
[10:44:04] TRACE | Query: How do I diagnose database connection pool exhaustion?
[10:44:04] TRACE | Total key facts: 4
[10:44:04] TRACE | Found facts: 3/4
[10:44:04] TRACE |   FOUND[1] could not obtain connection from pool within 5000ms
[10:44:04] TRACE |   FOUND[2] connection pool exhausted (100/100 connections in use)
[10:44:04] TRACE |   FOUND[3] PgBouncer
[10:44:04] TRACE | Missed facts: 1/4
[10:44:04] TRACE |   MISSED[1] max_db_connections = 100
[10:44:04] TRACE |     -> Found in chunk_id=deployment_guide_fix_3 (rank not in top-5)
[10:44:04] TRACE | === END FACT COVERAGE ===
[10:44:04] TRACE | === FACT COVERAGE ANALYSIS ===
[10:44:04] TRACE | Query: What should I check when I run out of database connections?
[10:44:04] TRACE | Total key facts: 4
[10:44:04] TRACE | Found facts: 3/4
[10:44:04] TRACE |   FOUND[1] could not obtain connection from pool within 5000ms
[10:44:04] TRACE |   FOUND[2] connection pool exhausted (100/100 connections in use)
[10:44:04] TRACE |   FOUND[3] PgBouncer
[10:44:04] TRACE | Missed facts: 1/4
[10:44:04] TRACE |   MISSED[1] max_db_connections = 100
[10:44:04] TRACE |     -> Found in chunk_id=deployment_guide_fix_3 (rank not in top-5)
[10:44:04] TRACE | === END FACT COVERAGE ===
[10:44:04] TRACE | === FACT COVERAGE ANALYSIS ===
[10:44:04] TRACE | Query: Getting 'could not obtain connection from pool' errors
[10:44:04] TRACE | Total key facts: 4
[10:44:04] TRACE | Found facts: 1/4
[10:44:04] TRACE |   FOUND[1] PgBouncer
[10:44:04] TRACE | Missed facts: 3/4
[10:44:04] TRACE |   MISSED[1] could not obtain connection from pool within 5000ms
[10:44:04] TRACE |     -> Found in chunk_id=troubleshooting_guide_fix_3 (rank not in top-5)
[10:44:04] TRACE |   MISSED[2] connection pool exhausted (100/100 connections in use)
[10:44:04] TRACE |     -> Found in chunk_id=troubleshooting_guide_fix_3 (rank not in top-5)
[10:44:04] TRACE |   MISSED[3] max_db_connections = 100
[10:44:04] TRACE |     -> Found in chunk_id=deployment_guide_fix_3 (rank not in top-5)
[10:44:04] TRACE | === END FACT COVERAGE ===
[10:44:04] TRACE | === FACT COVERAGE ANALYSIS ===
[10:44:04] TRACE | Query: connection pool full
[10:44:04] TRACE | Total key facts: 4
[10:44:04] TRACE | Found facts: 3/4
[10:44:04] TRACE |   FOUND[1] could not obtain connection from pool within 5000ms
[10:44:04] TRACE |   FOUND[2] connection pool exhausted (100/100 connections in use)
[10:44:04] TRACE |   FOUND[3] PgBouncer
[10:44:04] TRACE | Missed facts: 1/4
[10:44:04] TRACE |   MISSED[1] max_db_connections = 100
[10:44:04] TRACE |     -> Found in chunk_id=deployment_guide_fix_3 (rank not in top-5)
[10:44:04] TRACE | === END FACT COVERAGE ===
[10:44:04] TRACE | === FACT COVERAGE ANALYSIS ===
[10:44:04] TRACE | Query: My app is failing with connection pool errors, how do I troubleshoot?
[10:44:04] TRACE | Total key facts: 4
[10:44:04] TRACE | Found facts: 1/4
[10:44:04] TRACE |   FOUND[1] PgBouncer
[10:44:04] TRACE | Missed facts: 3/4
[10:44:04] TRACE |   MISSED[1] could not obtain connection from pool within 5000ms
[10:44:04] TRACE |     -> Found in chunk_id=troubleshooting_guide_fix_3 (rank not in top-5)
[10:44:04] TRACE |   MISSED[2] connection pool exhausted (100/100 connections in use)
[10:44:04] TRACE |     -> Found in chunk_id=troubleshooting_guide_fix_3 (rank not in top-5)
[10:44:04] TRACE |   MISSED[3] max_db_connections = 100
[10:44:04] TRACE |     -> Found in chunk_id=deployment_guide_fix_3 (rank not in top-5)
[10:44:04] TRACE | === END FACT COVERAGE ===
[10:44:04] TRACE | === FACT COVERAGE ANALYSIS ===
[10:44:04] TRACE | Query: Why can't I get a database connection even though CPU is low?
[10:44:04] TRACE | Total key facts: 4
[10:44:04] TRACE | Found facts: 3/4
[10:44:04] TRACE |   FOUND[1] could not obtain connection from pool within 5000ms
[10:44:04] TRACE |   FOUND[2] connection pool exhausted (100/100 connections in use)
[10:44:04] TRACE |   FOUND[3] PgBouncer
[10:44:04] TRACE | Missed facts: 1/4
[10:44:04] TRACE |   MISSED[1] max_db_connections = 100
[10:44:04] TRACE |     -> Found in chunk_id=deployment_guide_fix_3 (rank not in top-5)
[10:44:04] TRACE | === END FACT COVERAGE ===
[10:44:04] TRACE | === FACT COVERAGE ANALYSIS ===
[10:44:04] TRACE | Query: How do I handle API authentication?
[10:44:04] TRACE | Total key facts: 3
[10:44:04] TRACE | Found facts: 3/3
[10:44:04] TRACE |   FOUND[1] OAuth 2.0
[10:44:04] TRACE |   FOUND[2] API keys
[10:44:04] TRACE |   FOUND[3] JWT tokens
[10:44:04] TRACE | === END FACT COVERAGE ===
[10:44:04] TRACE | === FACT COVERAGE ANALYSIS ===
[10:44:04] TRACE | Query: What authentication methods are supported?
[10:44:04] TRACE | Total key facts: 3
[10:44:04] TRACE | Found facts: 3/3
[10:44:04] TRACE |   FOUND[1] OAuth 2.0
[10:44:04] TRACE |   FOUND[2] API keys
[10:44:04] TRACE |   FOUND[3] JWT tokens
[10:44:04] TRACE | === END FACT COVERAGE ===
[10:44:04] TRACE | === FACT COVERAGE ANALYSIS ===
[10:44:04] TRACE | Query: My API requests are getting 401 errors
[10:44:04] TRACE | Total key facts: 3
[10:44:04] TRACE | Found facts: 2/3
[10:44:04] TRACE |   FOUND[1] OAuth 2.0
[10:44:04] TRACE |   FOUND[2] API keys
[10:44:04] TRACE | Missed facts: 1/3
[10:44:04] TRACE |   MISSED[1] JWT tokens
[10:44:04] TRACE |     -> Found in chunk_id=api_reference_fix_0 (rank not in top-5)
[10:44:04] TRACE | === END FACT COVERAGE ===
[10:44:04] TRACE | === FACT COVERAGE ANALYSIS ===
[10:44:04] TRACE | Query: auth methods
[10:44:04] TRACE | Total key facts: 3
[10:44:04] TRACE | Found facts: 3/3
[10:44:04] TRACE |   FOUND[1] OAuth 2.0
[10:44:04] TRACE |   FOUND[2] API keys
[10:44:04] TRACE |   FOUND[3] JWT tokens
[10:44:04] TRACE | === END FACT COVERAGE ===
[10:44:04] TRACE | === FACT COVERAGE ANALYSIS ===
[10:44:04] TRACE | Query: I'm integrating with CloudFlow API, what authentication options do I have?
[10:44:04] TRACE | Total key facts: 3
[10:44:04] TRACE | Found facts: 3/3
[10:44:04] TRACE |   FOUND[1] OAuth 2.0
[10:44:04] TRACE |   FOUND[2] API keys
[10:44:04] TRACE |   FOUND[3] JWT tokens
[10:44:04] TRACE | === END FACT COVERAGE ===
[10:44:04] TRACE | === FACT COVERAGE ANALYSIS ===
[10:44:04] TRACE | Query: Why isn't basic auth working?
[10:44:04] TRACE | Total key facts: 3
[10:44:04] TRACE | Found facts: 1/3
[10:44:04] TRACE |   FOUND[1] API keys
[10:44:04] TRACE | Missed facts: 2/3
[10:44:04] TRACE |   MISSED[1] OAuth 2.0
[10:44:04] TRACE |     -> Found in chunk_id=api_reference_fix_0 (rank not in top-5)
[10:44:04] TRACE |   MISSED[2] JWT tokens
[10:44:04] TRACE |     -> Found in chunk_id=api_reference_fix_0 (rank not in top-5)
[10:44:04] TRACE | === END FACT COVERAGE ===
[10:44:04] TRACE | === FACT COVERAGE ANALYSIS ===
[10:44:04] TRACE | Query: What is PgBouncer and why is it used in CloudFlow?
[10:44:04] TRACE | Total key facts: 5
[10:44:04] TRACE | Found facts: 5/5
[10:44:04] TRACE |   FOUND[1] PgBouncer
[10:44:04] TRACE |   FOUND[2] connection pooling
[10:44:04] TRACE |   FOUND[3] max_db_connections = 100
[10:44:04] TRACE |   FOUND[4] default_pool_size = 25
[10:44:04] TRACE |   FOUND[5] pool_mode = transaction
[10:44:04] TRACE | === END FACT COVERAGE ===
[10:44:04] TRACE | === FACT COVERAGE ANALYSIS ===
[10:44:04] TRACE | Query: What's the purpose of the connection pooler?
[10:44:04] TRACE | Total key facts: 5
[10:44:04] TRACE | Found facts: 2/5
[10:44:04] TRACE |   FOUND[1] PgBouncer
[10:44:04] TRACE |   FOUND[2] connection pooling
[10:44:04] TRACE | Missed facts: 3/5
[10:44:04] TRACE |   MISSED[1] max_db_connections = 100
[10:44:04] TRACE |     -> Found in chunk_id=deployment_guide_fix_3 (rank not in top-5)
[10:44:04] TRACE |   MISSED[2] default_pool_size = 25
[10:44:04] TRACE |     -> Found in chunk_id=deployment_guide_fix_3 (rank not in top-5)
[10:44:04] TRACE |   MISSED[3] pool_mode = transaction
[10:44:04] TRACE |     -> Found in chunk_id=deployment_guide_fix_3 (rank not in top-5)
[10:44:04] TRACE | === END FACT COVERAGE ===
[10:44:04] TRACE | === FACT COVERAGE ANALYSIS ===
[10:44:04] TRACE | Query: Should I connect directly to PostgreSQL or through PgBouncer?
[10:44:04] TRACE | Total key facts: 5
[10:44:04] TRACE | Found facts: 5/5
[10:44:04] TRACE |   FOUND[1] PgBouncer
[10:44:04] TRACE |   FOUND[2] connection pooling
[10:44:04] TRACE |   FOUND[3] max_db_connections = 100
[10:44:04] TRACE |   FOUND[4] default_pool_size = 25
[10:44:04] TRACE |   FOUND[5] pool_mode = transaction
[10:44:04] TRACE | === END FACT COVERAGE ===
[10:44:04] TRACE | === FACT COVERAGE ANALYSIS ===
[10:44:04] TRACE | Query: pgbouncer purpose
[10:44:04] TRACE | Total key facts: 5
[10:44:04] TRACE | Found facts: 5/5
[10:44:04] TRACE |   FOUND[1] PgBouncer
[10:44:04] TRACE |   FOUND[2] connection pooling
[10:44:04] TRACE |   FOUND[3] max_db_connections = 100
[10:44:04] TRACE |   FOUND[4] default_pool_size = 25
[10:44:04] TRACE |   FOUND[5] pool_mode = transaction
[10:44:04] TRACE | === END FACT COVERAGE ===
[10:44:04] TRACE | === FACT COVERAGE ANALYSIS ===
[10:44:04] TRACE | Query: Optimizing database connections, what role does PgBouncer play?
[10:44:04] TRACE | Total key facts: 5
[10:44:04] TRACE | Found facts: 5/5
[10:44:04] TRACE |   FOUND[1] PgBouncer
[10:44:04] TRACE |   FOUND[2] connection pooling
[10:44:04] TRACE |   FOUND[3] max_db_connections = 100
[10:44:04] TRACE |   FOUND[4] default_pool_size = 25
[10:44:04] TRACE |   FOUND[5] pool_mode = transaction
[10:44:04] TRACE | === END FACT COVERAGE ===
[10:44:04] TRACE | === FACT COVERAGE ANALYSIS ===
[10:44:04] TRACE | Query: Why can't I connect directly to the database?
[10:44:04] TRACE | Total key facts: 5
[10:44:04] TRACE | Found facts: 2/5
[10:44:04] TRACE |   FOUND[1] PgBouncer
[10:44:04] TRACE |   FOUND[2] connection pooling
[10:44:04] TRACE | Missed facts: 3/5
[10:44:04] TRACE |   MISSED[1] max_db_connections = 100
[10:44:04] TRACE |     -> Found in chunk_id=deployment_guide_fix_3 (rank not in top-5)
[10:44:04] TRACE |   MISSED[2] default_pool_size = 25
[10:44:04] TRACE |     -> Found in chunk_id=deployment_guide_fix_3 (rank not in top-5)
[10:44:04] TRACE |   MISSED[3] pool_mode = transaction
[10:44:04] TRACE |     -> Found in chunk_id=deployment_guide_fix_3 (rank not in top-5)
[10:44:04] TRACE | === END FACT COVERAGE ===
[10:44:04] TRACE | === FACT COVERAGE ANALYSIS ===
[10:44:04] TRACE | Query: How do I implement retry logic for failed workflow steps?
[10:44:04] TRACE | Total key facts: 4
[10:44:04] TRACE | Found facts: 3/4
[10:44:04] TRACE |   FOUND[1] max_attempts: 3
[10:44:04] TRACE |   FOUND[2] backoff_type: exponential
[10:44:04] TRACE |   FOUND[3] initial_interval: 1000
[10:44:04] TRACE | Missed facts: 1/4
[10:44:04] TRACE |   MISSED[1] max retries: 3
[10:44:04] TRACE |     -> Found in chunk_id=troubleshooting_guide_fix_5 (rank not in top-5)
[10:44:04] TRACE | === END FACT COVERAGE ===
[10:44:04] TRACE | === FACT COVERAGE ANALYSIS ===
[10:44:04] TRACE | Query: What's the retry strategy for transient failures?
[10:44:04] TRACE | Total key facts: 4
[10:44:04] TRACE | Found facts: 2/4
[10:44:04] TRACE |   FOUND[1] max_attempts: 3
[10:44:04] TRACE |   FOUND[2] initial_interval: 1000
[10:44:04] TRACE | Missed facts: 2/4
[10:44:04] TRACE |   MISSED[1] backoff_type: exponential
[10:44:04] TRACE |     -> Found in chunk_id=user_guide_fix_7 (rank not in top-5)
[10:44:04] TRACE |   MISSED[2] max retries: 3
[10:44:04] TRACE |     -> Found in chunk_id=troubleshooting_guide_fix_5 (rank not in top-5)
[10:44:04] TRACE | === END FACT COVERAGE ===
[10:44:04] TRACE | === FACT COVERAGE ANALYSIS ===
[10:44:04] TRACE | Query: My workflow fails on temporary network errors
[10:44:04] TRACE | Total key facts: 4
[10:44:04] TRACE | Found facts: 3/4
[10:44:04] TRACE |   FOUND[1] max_attempts: 3
[10:44:04] TRACE |   FOUND[2] backoff_type: exponential
[10:44:04] TRACE |   FOUND[3] max retries: 3
[10:44:04] TRACE | Missed facts: 1/4
[10:44:04] TRACE |   MISSED[1] initial_interval: 1000
[10:44:04] TRACE |     -> Found in chunk_id=user_guide_fix_5 (rank not in top-5)
[10:44:04] TRACE | === END FACT COVERAGE ===
[10:44:04] TRACE | === FACT COVERAGE ANALYSIS ===
[10:44:04] TRACE | Query: retry config
[10:44:04] TRACE | Total key facts: 4
[10:44:04] TRACE | Found facts: 3/4
[10:44:04] TRACE |   FOUND[1] max_attempts: 3
[10:44:04] TRACE |   FOUND[2] backoff_type: exponential
[10:44:04] TRACE |   FOUND[3] initial_interval: 1000
[10:44:04] TRACE | Missed facts: 1/4
[10:44:04] TRACE |   MISSED[1] max retries: 3
[10:44:04] TRACE |     -> Found in chunk_id=troubleshooting_guide_fix_5 (rank not in top-5)
[10:44:04] TRACE | === END FACT COVERAGE ===
[10:44:04] TRACE | === FACT COVERAGE ANALYSIS ===
[10:44:04] TRACE | Query: I want workflows to automatically retry on errors, what are the options?
[10:44:04] TRACE | Total key facts: 4
[10:44:04] TRACE | Found facts: 3/4
[10:44:04] TRACE |   FOUND[1] max_attempts: 3
[10:44:04] TRACE |   FOUND[2] backoff_type: exponential
[10:44:04] TRACE |   FOUND[3] initial_interval: 1000
[10:44:04] TRACE | Missed facts: 1/4
[10:44:04] TRACE |   MISSED[1] max retries: 3
[10:44:04] TRACE |     -> Found in chunk_id=troubleshooting_guide_fix_5 (rank not in top-5)
[10:44:04] TRACE | === END FACT COVERAGE ===
[10:44:04] TRACE | === FACT COVERAGE ANALYSIS ===
[10:44:04] TRACE | Query: Why doesn't my workflow retry after failing?
[10:44:04] TRACE | Total key facts: 4
[10:44:04] TRACE | Found facts: 3/4
[10:44:04] TRACE |   FOUND[1] max_attempts: 3
[10:44:04] TRACE |   FOUND[2] backoff_type: exponential
[10:44:04] TRACE |   FOUND[3] initial_interval: 1000
[10:44:04] TRACE | Missed facts: 1/4
[10:44:04] TRACE |   MISSED[1] max retries: 3
[10:44:04] TRACE |     -> Found in chunk_id=troubleshooting_guide_fix_5 (rank not in top-5)
[10:44:04] TRACE | === END FACT COVERAGE ===
[10:44:04] TRACE | === FACT COVERAGE ANALYSIS ===
[10:44:04] TRACE | Query: What Helm chart repository should I use for deployment?
[10:44:04] TRACE | Total key facts: 1
[10:44:04] TRACE | Found facts: 0/1
[10:44:04] TRACE | Missed facts: 1/1
[10:44:04] TRACE |   MISSED[1] helm repo add cloudflow https://charts.cloudflow.io
[10:44:04] TRACE |     -> Found in chunk_id=deployment_guide_fix_1 (rank not in top-5)
[10:44:04] TRACE | === END FACT COVERAGE ===
[10:44:04] TRACE | === FACT COVERAGE ANALYSIS ===
[10:44:04] TRACE | Query: Where is the CloudFlow Helm chart hosted?
[10:44:04] TRACE | Total key facts: 1
[10:44:04] TRACE | Found facts: 0/1
[10:44:04] TRACE | Missed facts: 1/1
[10:44:04] TRACE |   MISSED[1] helm repo add cloudflow https://charts.cloudflow.io
[10:44:04] TRACE |     -> Found in chunk_id=deployment_guide_fix_1 (rank not in top-5)
[10:44:04] TRACE | === END FACT COVERAGE ===
[10:44:04] TRACE | === FACT COVERAGE ANALYSIS ===
[10:44:04] TRACE | Query: helm repo add is failing, what's the correct URL?
[10:44:04] TRACE | Total key facts: 1
[10:44:04] TRACE | Found facts: 0/1
[10:44:04] TRACE | Missed facts: 1/1
[10:44:04] TRACE |   MISSED[1] helm repo add cloudflow https://charts.cloudflow.io
[10:44:04] TRACE |     -> Found in chunk_id=deployment_guide_fix_1 (rank not in top-5)
[10:44:04] TRACE | === END FACT COVERAGE ===
[10:44:04] TRACE | === FACT COVERAGE ANALYSIS ===
[10:44:04] TRACE | Query: helm repo
[10:44:04] TRACE | Total key facts: 1
[10:44:04] TRACE | Found facts: 0/1
[10:44:04] TRACE | Missed facts: 1/1
[10:44:04] TRACE |   MISSED[1] helm repo add cloudflow https://charts.cloudflow.io
[10:44:04] TRACE |     -> Found in chunk_id=deployment_guide_fix_1 (rank not in top-5)
[10:44:04] TRACE | === END FACT COVERAGE ===
[10:44:04] TRACE | === FACT COVERAGE ANALYSIS ===
[10:44:04] TRACE | Query: Setting up deployment pipeline, which Helm repository has CloudFlow charts?
[10:44:04] TRACE | Total key facts: 1
[10:44:04] TRACE | Found facts: 0/1
[10:44:04] TRACE | Missed facts: 1/1
[10:44:04] TRACE |   MISSED[1] helm repo add cloudflow https://charts.cloudflow.io
[10:44:04] TRACE |     -> Found in chunk_id=deployment_guide_fix_1 (rank not in top-5)
[10:44:04] TRACE | === END FACT COVERAGE ===
[10:44:04] TRACE | === FACT COVERAGE ANALYSIS ===
[10:44:04] TRACE | Query: Why can't I find CloudFlow in the official Helm hub?
[10:44:04] TRACE | Total key facts: 1
[10:44:04] TRACE | Found facts: 1/1
[10:44:04] TRACE |   FOUND[1] helm repo add cloudflow https://charts.cloudflow.io
[10:44:04] TRACE | === END FACT COVERAGE ===
[10:44:04] TRACE | === FACT COVERAGE ANALYSIS ===
[10:44:04] TRACE | Query: What is the minimum scheduling interval for workflows?
[10:44:04] TRACE | Total key facts: 2
[10:44:04] TRACE | Found facts: 0/2
[10:44:04] TRACE | Missed facts: 2/2
[10:44:04] TRACE |   MISSED[1] minimum scheduling interval is 1 minute
[10:44:04] TRACE |   MISSED[2] The minimum scheduling interval is **1 minute**
[10:44:04] TRACE |     -> Found in chunk_id=user_guide_fix_4 (rank not in top-5)
[10:44:04] TRACE | === END FACT COVERAGE ===
[10:44:04] TRACE | === FACT COVERAGE ANALYSIS ===
[10:44:04] TRACE | Query: How frequently can I schedule a workflow?
[10:44:04] TRACE | Total key facts: 2
[10:44:04] TRACE | Found facts: 1/2
[10:44:04] TRACE |   FOUND[1] The minimum scheduling interval is **1 minute**
[10:44:04] TRACE | Missed facts: 1/2
[10:44:04] TRACE |   MISSED[1] minimum scheduling interval is 1 minute
[10:44:04] TRACE | === END FACT COVERAGE ===
[10:44:04] TRACE | === FACT COVERAGE ANALYSIS ===
[10:44:04] TRACE | Query: My every-30-seconds schedule is being rejected
[10:44:04] TRACE | Total key facts: 2
[10:44:04] TRACE | Found facts: 1/2
[10:44:04] TRACE |   FOUND[1] The minimum scheduling interval is **1 minute**
[10:44:04] TRACE | Missed facts: 1/2
[10:44:04] TRACE |   MISSED[1] minimum scheduling interval is 1 minute
[10:44:04] TRACE | === END FACT COVERAGE ===
[10:44:04] TRACE | === FACT COVERAGE ANALYSIS ===
[10:44:04] TRACE | Query: min schedule interval
[10:44:04] TRACE | Total key facts: 2
[10:44:04] TRACE | Found facts: 1/2
[10:44:04] TRACE |   FOUND[1] The minimum scheduling interval is **1 minute**
[10:44:04] TRACE | Missed facts: 1/2
[10:44:04] TRACE |   MISSED[1] minimum scheduling interval is 1 minute
[10:44:04] TRACE | === END FACT COVERAGE ===
[10:44:04] TRACE | === FACT COVERAGE ANALYSIS ===
[10:44:04] TRACE | Query: I need near real-time execution, what's the fastest schedule I can set?
[10:44:04] TRACE | Total key facts: 2
[10:44:04] TRACE | Found facts: 1/2
[10:44:04] TRACE |   FOUND[1] The minimum scheduling interval is **1 minute**
[10:44:04] TRACE | Missed facts: 1/2
[10:44:04] TRACE |   MISSED[1] minimum scheduling interval is 1 minute
[10:44:04] TRACE | === END FACT COVERAGE ===
[10:44:04] TRACE | === FACT COVERAGE ANALYSIS ===
[10:44:04] TRACE | Query: Why can't I schedule workflows every 30 seconds?
[10:44:04] TRACE | Total key facts: 2
[10:44:04] TRACE | Found facts: 1/2
[10:44:04] TRACE |   FOUND[1] The minimum scheduling interval is **1 minute**
[10:44:04] TRACE | Missed facts: 1/2
[10:44:04] TRACE |   MISSED[1] minimum scheduling interval is 1 minute
[10:44:04] TRACE | === END FACT COVERAGE ===
[10:44:04] INFO  |   k=5 exact_match: 67.9% (36/53)
[10:44:04] INFO  |     synonym: 60.4%
[10:44:04] INFO  |     problem: 54.7%
[10:44:04] INFO  |     casual: 54.7%
[10:44:04] INFO  |     contextual: 62.3%
[10:44:04] INFO  |     negation: 54.7%

================================================================================
[10:44:05] INFO  | RETRIEVAL STRATEGY: hybrid
================================================================================
[10:44:05] INFO  | Loading embedder: BAAI/bge-base-en-v1.5
[10:44:07] PROG  | [  2/  3] █████████████░░░░░░░  66.7% | hybrid_bge-base-en-v1.5_fixed_512_0pct
[10:44:07] DEBUG | Chunking document 'api_reference': 1792 words, 16663 chars
[10:44:07] DEBUG |   Chunk 0: chars [0-3160] words=376 tokens≈500 preview='# CloudFlow API Reference Version 2.1.0 | Last Updated: January 2026 ## Overview The CloudFlow API i...'
[10:44:07] DEBUG |   Chunk 1: chars [3161-6111] words=379 tokens≈504 preview='## Rate Limiting To ensure fair usage and system stability, CloudFlow enforces rate limits on all AP...'
[10:44:07] DEBUG |   Chunk 2: chars [6112-8827] words=296 tokens≈393 preview='**Endpoint:** `POST /workflows` **Request Body:** ```json { "name": "Email Campaign Automation", "de...'
[10:44:07] DEBUG |   Chunk 3: chars [8828-11402] words=269 tokens≈357 preview='**Endpoint:** `GET /pipelines/{pipeline_id}/executions` **Query Parameters:** - `status` (optional):...'
[10:44:07] DEBUG |   Chunk 4: chars [11403-14151] words=365 tokens≈485 preview='### Error Response Format ```json { "error": { "code": "invalid_parameter", "message": "The 'limit' ...'
[10:44:07] DEBUG |   Chunk 5: chars [14152-15213] words=107 tokens≈142 preview='**Supported Events:** - `workflow.started` - `workflow.completed` - `workflow.failed` - `pipeline.co...'
[10:44:07] DEBUG | Created 6 chunks for document 'api_reference'
[10:44:07] DEBUG | Chunking document 'architecture_overview': 4607 words, 36722 chars
[10:44:07] DEBUG |   Chunk 0: chars [0-3230] words=370 tokens≈492 preview='# CloudFlow Platform - System Architecture Overview **Document Version:** 2.3.1 **Last Updated:** Ja...'
[10:44:07] DEBUG |   Chunk 1: chars [3231-5982] words=380 tokens≈505 preview='**Technology**: Node.js with Express.js framework **Replicas**: 12 pods (production), auto-scaling 8...'
[10:44:07] DEBUG |   Chunk 2: chars [5983-7875] words=248 tokens≈329 preview='**Technology**: Node.js with TypeScript, Bull queue library **Replicas**: 16 pods (production), auto...'
[10:44:07] DEBUG |   Chunk 3: chars [7876-9478] words=219 tokens≈291 preview='**Technology**: Go with distributed locking via Redis **Replicas**: 4 pods (production), active-pass...'
[10:44:07] DEBUG |   Chunk 4: chars [9479-11878] words=354 tokens≈470 preview='**Technology**: Node.js with worker pool pattern **Replicas**: 8 pods (production), auto-scaling 6-1...'
[10:44:07] DEBUG |   Chunk 5: chars [11879-14634] words=368 tokens≈489 preview='Success/failure events published back to Kafka ``` **Event Schema**: ```json { "event_id": "uuid-v4"...'
[10:44:07] DEBUG |   Chunk 6: chars [14635-17394] words=384 tokens≈510 preview='**Session Storage**: - Key pattern: `session:{user_id}` - TTL: 15 minutes (aligned with JWT expiry) ...'
[10:44:07] DEBUG |   Chunk 7: chars [17395-20141] words=381 tokens≈506 preview='records: 500 - Session timeout: 30 seconds --- ## Message Queue Patterns ### Pub/Sub Pattern Used fo...'
[10:44:07] DEBUG |   Chunk 8: chars [20142-22843] words=384 tokens≈510 preview='### Cache Invalidation Strategies **Time-Based Expiration (TTL)**: - Short-lived data: 5-15 minutes ...'
[10:44:07] DEBUG |   Chunk 9: chars [22844-25362] words=384 tokens≈510 preview='per API key (default: 1000 RPM) ### Secrets Management **HashiCorp Vault Integration**: - Dynamic da...'
[10:44:07] DEBUG |   Chunk 10: chars [25363-27842] words=384 tokens≈510 preview='- Execution start rate: 500 per second - Completion rate: 450 per second (average 2-second execution...'
[10:44:07] DEBUG |   Chunk 11: chars [27843-30286] words=375 tokens≈498 preview='Service (2) - PostgreSQL Primary - PostgreSQL Replica - PostgreSQL Replica - Redis Primary (2) - Red...'
[10:44:07] DEBUG |   Chunk 12: chars [30287-32834] words=376 tokens≈500 preview='Resume normal operations - Recovery time: 1-3 hours depending on data volume - Data loss: None if co...'
[10:44:07] DEBUG | Created 13 chunks for document 'architecture_overview'
[10:44:07] DEBUG | Chunking document 'deployment_guide': 2777 words, 25837 chars
[10:44:07] DEBUG |   Chunk 0: chars [0-3165] words=384 tokens≈510 preview='# CloudFlow Platform - Deployment and Operations Guide **Version:** 2.4.0 **Last Updated:** January ...'
[10:44:07] DEBUG |   Chunk 1: chars [3166-6560] words=384 tokens≈510 preview='create cluster -f cluster-config.yaml ``` ### Storage Configuration Install the EBS CSI driver for p...'
[10:44:07] DEBUG |   Chunk 2: chars [6561-8755] words=251 tokens≈333 preview='- secretName: cloudflow-tls hosts: - api.cloudflow.io healthCheck: enabled: true livenessProbe: path...'
[10:44:07] DEBUG |   Chunk 3: chars [8756-12138] words=384 tokens≈510 preview='Deploy PostgreSQL using the Bitnami Helm chart: ```yaml # postgres-values.yaml global: postgresql: a...'
[10:44:07] DEBUG |   Chunk 4: chars [12139-14139] words=204 tokens≈271 preview='--set prometheus.prometheusSpec.retention=30d \ --set prometheus.prometheusSpec.storageSpec.volumeCl...'
[10:44:07] DEBUG |   Chunk 5: chars [14140-17194] words=372 tokens≈494 preview='Configure log aggregation: ```bash # Install Fluent Bit for log forwarding helm install fluent-bit f...'
[10:44:07] DEBUG |   Chunk 6: chars [17195-20069] words=384 tokens≈510 preview='**Verify Service Health**: ```bash kubectl get pods -n cloudflow-prod curl https://api.cloudflow.io/...'
[10:44:07] DEBUG |   Chunk 7: chars [20070-23221] words=384 tokens≈510 preview='podSelector: matchLabels: app: cloudflow policyTypes: - Ingress - Egress ingress: - from: - namespac...'
[10:44:07] DEBUG |   Chunk 8: chars [23222-23448] words=30 tokens≈39 preview='UTC - **Secondary**: Wednesday 10:00-12:00 UTC All deployments should target these windows to minimi...'
[10:44:07] DEBUG | Created 9 chunks for document 'deployment_guide'
[10:44:07] DEBUG | Chunking document 'troubleshooting_guide': 4032 words, 32183 chars
[10:44:07] DEBUG |   Chunk 0: chars [0-2666] words=320 tokens≈425 preview='# CloudFlow Platform Troubleshooting Guide **Version:** 3.2.1 **Last Updated:** January 2026 **Audie...'
[10:44:07] DEBUG |   Chunk 1: chars [2667-5007] words=321 tokens≈426 preview='**Diagnosis:** ```bash # Check system time timedatectl status # Compare with NTP server ntpdate -q p...'
[10:44:07] DEBUG |   Chunk 2: chars [5008-8037] words=376 tokens≈500 preview='Review Query Execution Plans** ```sql -- Connect to CloudFlow database cloudflow db connect --readon...'
[10:44:07] DEBUG |   Chunk 3: chars [8038-10862] words=356 tokens≈473 preview='Workflow Context Accumulation** Large workflow executions may accumulate state in memory. **Solution...'
[10:44:07] DEBUG |   Chunk 4: chars [10863-13827] words=384 tokens≈510 preview='**Implement connection pooling optimization:** ```bash # Use PgBouncer for connection pooling kubect...'
[10:44:07] DEBUG |   Chunk 5: chars [13828-16401] words=334 tokens≈444 preview='exec_7h3j6k9m2n --show-bottlenecks ``` #### Solutions **1. Increase workflow timeout (if justified):...'
[10:44:07] DEBUG |   Chunk 6: chars [16402-19022] words=329 tokens≈437 preview='Data Validation Errors** ``` ValidationError: Field 'customer_id' is required but missing in 234 rec...'
[10:44:07] DEBUG |   Chunk 7: chars [19023-20595] words=194 tokens≈258 preview='Waiting {retry_after} seconds...") time.sleep(retry_after) continue remaining = int(response.headers...'
[10:44:07] DEBUG |   Chunk 8: chars [20596-23454] words=384 tokens≈510 preview='Leverage caching:** ```bash # Enable client-side caching export CLOUDFLOW_CACHE_ENABLED=true export ...'
[10:44:07] DEBUG |   Chunk 9: chars [23455-26239] words=384 tokens≈510 preview='in" # Find workflow failures by ID kubectl logs -n cloudflow deployment/cloudflow-workflow-engine | ...'
[10:44:07] DEBUG |   Chunk 10: chars [26240-29039] words=384 tokens≈510 preview='database nc -zv cloudflow-db.internal.company.com 5432 # Trace route traceroute api.cloudflow.io # C...'
[10:44:07] DEBUG |   Chunk 11: chars [29040-31206] words=266 tokens≈353 preview='outage" # Create war room cloudflow incident war-room create incident-2024012401 ``` **For SEV-2 (Hi...'
[10:44:07] DEBUG | Created 12 chunks for document 'troubleshooting_guide'
[10:44:07] DEBUG | Chunking document 'user_guide': 3735 words, 31582 chars
[10:44:07] DEBUG |   Chunk 0: chars [0-2617] words=375 tokens≈498 preview='# CloudFlow User Guide Welcome to CloudFlow, the modern workflow automation platform that helps you ...'
[10:44:07] DEBUG |   Chunk 1: chars [2618-5224] words=348 tokens≈462 preview='Open the Visual Editor by clicking **"Create Workflow"** or editing an existing workflow 2. Add a tr...'
[10:44:07] DEBUG |   Chunk 2: chars [5225-8372] words=381 tokens≈506 preview='### HTTP Requests Make HTTP requests to any API endpoint: **Configuration:** - **Method**: GET, POST...'
[10:44:07] DEBUG |   Chunk 3: chars [8373-11268] words=336 tokens≈446 preview='``` **Slack App Integration:** - Install the CloudFlow Slack app in your workspace - Authenticate on...'
[10:44:07] DEBUG |   Chunk 4: chars [11269-13371] words=375 tokens≈498 preview='### Cron Syntax Use standard cron expressions to define schedules: ``` * * * * * ┬ ┬ ┬ ┬ ┬ │ │ │ │ │...'
[10:44:07] DEBUG |   Chunk 5: chars [13372-16105] words=383 tokens≈509 preview='Save and activate **Testing Schedules:** Use the built-in schedule calculator to preview upcoming ex...'
[10:44:07] DEBUG |   Chunk 6: chars [16106-18686] words=363 tokens≈482 preview='Filter by error type, date range, or execution ID 3. Click an execution to view full details 4. Clic...'
[10:44:07] DEBUG |   Chunk 7: chars [18687-21448] words=364 tokens≈484 preview='Handle Errors Gracefully Always implement error handling for external API calls and database operati...'
[10:44:07] DEBUG |   Chunk 8: chars [21449-24677] words=384 tokens≈510 preview='Version Control For critical workflows: - Export YAML definitions regularly - Store in version contr...'
[10:44:07] DEBUG |   Chunk 9: chars [24678-27975] words=371 tokens≈493 preview='Requester: {{trigger.body.requester}} Description: {{trigger.body.description}} Approve: https://app...'
[10:44:07] DEBUG |   Chunk 10: chars [27976-28481] words=55 tokens≈73 preview='- **Documentation**: https://docs.cloudflow.io - **Community Forum**: https://community.cloudflow.io...'
[10:44:07] DEBUG | Created 11 chunks for document 'user_guide'
[10:44:07] INFO  |   Created 51 chunks
[10:44:08] INFO  |   Indexed in 0.96s
[10:44:08] TRACE | === FACT COVERAGE ANALYSIS ===
[10:44:08] TRACE | Query: What is the API rate limit per minute?
[10:44:08] TRACE | Total key facts: 2
[10:44:08] TRACE | Found facts: 2/2
[10:44:08] TRACE |   FOUND[1] 100 requests per minute per authenticated user
[10:44:08] TRACE |   FOUND[2] 20 requests per minute for unauthenticated requests
[10:44:08] TRACE | === END FACT COVERAGE ===
[10:44:08] TRACE | === FACT COVERAGE ANALYSIS ===
[10:44:08] TRACE | Query: How many requests can I make per minute to the API?
[10:44:08] TRACE | Total key facts: 2
[10:44:08] TRACE | Found facts: 2/2
[10:44:08] TRACE |   FOUND[1] 100 requests per minute per authenticated user
[10:44:08] TRACE |   FOUND[2] 20 requests per minute for unauthenticated requests
[10:44:08] TRACE | === END FACT COVERAGE ===
[10:44:08] TRACE | === FACT COVERAGE ANALYSIS ===
[10:44:08] TRACE | Query: My API calls are getting blocked, what's the limit?
[10:44:08] TRACE | Total key facts: 2
[10:44:08] TRACE | Found facts: 2/2
[10:44:08] TRACE |   FOUND[1] 100 requests per minute per authenticated user
[10:44:08] TRACE |   FOUND[2] 20 requests per minute for unauthenticated requests
[10:44:08] TRACE | === END FACT COVERAGE ===
[10:44:08] TRACE | === FACT COVERAGE ANALYSIS ===
[10:44:08] TRACE | Query: api rate limit
[10:44:08] TRACE | Total key facts: 2
[10:44:08] TRACE | Found facts: 2/2
[10:44:08] TRACE |   FOUND[1] 100 requests per minute per authenticated user
[10:44:08] TRACE |   FOUND[2] 20 requests per minute for unauthenticated requests
[10:44:08] TRACE | === END FACT COVERAGE ===
[10:44:08] TRACE | === FACT COVERAGE ANALYSIS ===
[10:44:08] TRACE | Query: I'm building a batch job that calls CloudFlow API repeatedly, what rate limit should I expect?
[10:44:08] TRACE | Total key facts: 2
[10:44:08] TRACE | Found facts: 2/2
[10:44:08] TRACE |   FOUND[1] 100 requests per minute per authenticated user
[10:44:08] TRACE |   FOUND[2] 20 requests per minute for unauthenticated requests
[10:44:08] TRACE | === END FACT COVERAGE ===
[10:44:08] TRACE | === FACT COVERAGE ANALYSIS ===
[10:44:08] TRACE | Query: Why is the API rejecting my requests after 100 calls?
[10:44:08] TRACE | Total key facts: 2
[10:44:08] TRACE | Found facts: 2/2
[10:44:08] TRACE |   FOUND[1] 100 requests per minute per authenticated user
[10:44:08] TRACE |   FOUND[2] 20 requests per minute for unauthenticated requests
[10:44:08] TRACE | === END FACT COVERAGE ===
[10:44:08] TRACE | === FACT COVERAGE ANALYSIS ===
[10:44:08] TRACE | Query: How do I fix 429 Too Many Requests errors?
[10:44:08] TRACE | Total key facts: 5
[10:44:08] TRACE | Found facts: 4/5
[10:44:08] TRACE |   FOUND[1] 429 Too Many Requests
[10:44:08] TRACE |   FOUND[2] X-RateLimit-Remaining
[10:44:08] TRACE |   FOUND[3] Retry-After
[10:44:08] TRACE |   FOUND[4] Implement exponential backoff when receiving 429 responses
[10:44:08] TRACE | Missed facts: 1/5
[10:44:08] TRACE |   MISSED[1] Monitor X-RateLimit-Remaining header values
[10:44:08] TRACE | === END FACT COVERAGE ===
[10:44:08] TRACE | === FACT COVERAGE ANALYSIS ===
[10:44:08] TRACE | Query: What should I do when I get rate limited?
[10:44:08] TRACE | Total key facts: 5
[10:44:08] TRACE | Found facts: 3/5
[10:44:08] TRACE |   FOUND[1] 429 Too Many Requests
[10:44:08] TRACE |   FOUND[2] X-RateLimit-Remaining
[10:44:08] TRACE |   FOUND[3] Implement exponential backoff when receiving 429 responses
[10:44:08] TRACE | Missed facts: 2/5
[10:44:08] TRACE |   MISSED[1] Retry-After
[10:44:08] TRACE |     -> Found in chunk_id=troubleshooting_guide_fix_6 (rank not in top-5)
[10:44:08] TRACE |   MISSED[2] Monitor X-RateLimit-Remaining header values
[10:44:08] TRACE | === END FACT COVERAGE ===
[10:44:08] TRACE | === FACT COVERAGE ANALYSIS ===
[10:44:08] TRACE | Query: My requests keep failing with 429 status code
[10:44:08] TRACE | Total key facts: 5
[10:44:08] TRACE | Found facts: 3/5
[10:44:08] TRACE |   FOUND[1] 429 Too Many Requests
[10:44:08] TRACE |   FOUND[2] X-RateLimit-Remaining
[10:44:08] TRACE |   FOUND[3] Retry-After
[10:44:08] TRACE | Missed facts: 2/5
[10:44:08] TRACE |   MISSED[1] Monitor X-RateLimit-Remaining header values
[10:44:08] TRACE |   MISSED[2] Implement exponential backoff when receiving 429 responses
[10:44:08] TRACE |     -> Found in chunk_id=api_reference_fix_1 (rank not in top-5)
[10:44:08] TRACE | === END FACT COVERAGE ===
[10:44:08] TRACE | === FACT COVERAGE ANALYSIS ===
[10:44:08] TRACE | Query: 429 error fix
[10:44:08] TRACE | Total key facts: 5
[10:44:08] TRACE | Found facts: 3/5
[10:44:08] TRACE |   FOUND[1] 429 Too Many Requests
[10:44:08] TRACE |   FOUND[2] X-RateLimit-Remaining
[10:44:08] TRACE |   FOUND[3] Retry-After
[10:44:08] TRACE | Missed facts: 2/5
[10:44:08] TRACE |   MISSED[1] Monitor X-RateLimit-Remaining header values
[10:44:08] TRACE |   MISSED[2] Implement exponential backoff when receiving 429 responses
[10:44:08] TRACE |     -> Found in chunk_id=api_reference_fix_1 (rank not in top-5)
[10:44:08] TRACE | === END FACT COVERAGE ===
[10:44:08] TRACE | === FACT COVERAGE ANALYSIS ===
[10:44:08] TRACE | Query: I'm getting throttled by CloudFlow API, how can I handle this in my application?
[10:44:08] TRACE | Total key facts: 5
[10:44:08] TRACE | Found facts: 0/5
[10:44:08] TRACE | Missed facts: 5/5
[10:44:08] TRACE |   MISSED[1] 429 Too Many Requests
[10:44:08] TRACE |     -> Found in chunk_id=api_reference_fix_1 (rank not in top-5)
[10:44:08] TRACE |   MISSED[2] X-RateLimit-Remaining
[10:44:08] TRACE |     -> Found in chunk_id=api_reference_fix_1 (rank not in top-5)
[10:44:08] TRACE |   MISSED[3] Retry-After
[10:44:08] TRACE |     -> Found in chunk_id=troubleshooting_guide_fix_6 (rank not in top-5)
[10:44:08] TRACE |   MISSED[4] Monitor X-RateLimit-Remaining header values
[10:44:08] TRACE |   MISSED[5] Implement exponential backoff when receiving 429 responses
[10:44:08] TRACE |     -> Found in chunk_id=api_reference_fix_1 (rank not in top-5)
[10:44:08] TRACE | === END FACT COVERAGE ===
[10:44:08] TRACE | === FACT COVERAGE ANALYSIS ===
[10:44:08] TRACE | Query: Why am I being blocked with rate limit errors?
[10:44:08] TRACE | Total key facts: 5
[10:44:08] TRACE | Found facts: 4/5
[10:44:08] TRACE |   FOUND[1] 429 Too Many Requests
[10:44:08] TRACE |   FOUND[2] X-RateLimit-Remaining
[10:44:08] TRACE |   FOUND[3] Retry-After
[10:44:08] TRACE |   FOUND[4] Implement exponential backoff when receiving 429 responses
[10:44:08] TRACE | Missed facts: 1/5
[10:44:08] TRACE |   MISSED[1] Monitor X-RateLimit-Remaining header values
[10:44:08] TRACE | === END FACT COVERAGE ===
[10:44:08] TRACE | === FACT COVERAGE ANALYSIS ===
[10:44:08] TRACE | Query: What is the JWT token expiration time?
[10:44:08] TRACE | Total key facts: 3
[10:44:08] TRACE | Found facts: 1/3
[10:44:08] TRACE |   FOUND[1] 3600 seconds
[10:44:08] TRACE | Missed facts: 2/3
[10:44:08] TRACE |   MISSED[1] max 3600 seconds from iat
[10:44:08] TRACE |   MISSED[2] All tokens expire after 3600 seconds
[10:44:08] TRACE |     -> Found in chunk_id=api_reference_fix_0 (rank not in top-5)
[10:44:08] TRACE | === END FACT COVERAGE ===
[10:44:08] TRACE | === FACT COVERAGE ANALYSIS ===
[10:44:08] TRACE | Query: How long do access tokens last?
[10:44:08] TRACE | Total key facts: 3
[10:44:08] TRACE | Found facts: 1/3
[10:44:08] TRACE |   FOUND[1] 3600 seconds
[10:44:08] TRACE | Missed facts: 2/3
[10:44:08] TRACE |   MISSED[1] max 3600 seconds from iat
[10:44:08] TRACE |   MISSED[2] All tokens expire after 3600 seconds
[10:44:08] TRACE |     -> Found in chunk_id=api_reference_fix_0 (rank not in top-5)
[10:44:08] TRACE | === END FACT COVERAGE ===
[10:44:08] TRACE | === FACT COVERAGE ANALYSIS ===
[10:44:08] TRACE | Query: My authentication keeps expiring after an hour
[10:44:08] TRACE | Total key facts: 3
[10:44:08] TRACE | Found facts: 1/3
[10:44:08] TRACE |   FOUND[1] 3600 seconds
[10:44:08] TRACE | Missed facts: 2/3
[10:44:08] TRACE |   MISSED[1] max 3600 seconds from iat
[10:44:08] TRACE |   MISSED[2] All tokens expire after 3600 seconds
[10:44:08] TRACE |     -> Found in chunk_id=api_reference_fix_0 (rank not in top-5)
[10:44:08] TRACE | === END FACT COVERAGE ===
[10:44:08] TRACE | === FACT COVERAGE ANALYSIS ===
[10:44:08] TRACE | Query: token expiry time
[10:44:08] TRACE | Total key facts: 3
[10:44:08] TRACE | Found facts: 1/3
[10:44:08] TRACE |   FOUND[1] 3600 seconds
[10:44:08] TRACE | Missed facts: 2/3
[10:44:08] TRACE |   MISSED[1] max 3600 seconds from iat
[10:44:08] TRACE |   MISSED[2] All tokens expire after 3600 seconds
[10:44:08] TRACE |     -> Found in chunk_id=api_reference_fix_0 (rank not in top-5)
[10:44:08] TRACE | === END FACT COVERAGE ===
[10:44:08] TRACE | === FACT COVERAGE ANALYSIS ===
[10:44:08] TRACE | Query: I need to implement token refresh logic, when do JWT tokens expire?
[10:44:08] TRACE | Total key facts: 3
[10:44:08] TRACE | Found facts: 1/3
[10:44:08] TRACE |   FOUND[1] 3600 seconds
[10:44:08] TRACE | Missed facts: 2/3
[10:44:08] TRACE |   MISSED[1] max 3600 seconds from iat
[10:44:08] TRACE |   MISSED[2] All tokens expire after 3600 seconds
[10:44:08] TRACE |     -> Found in chunk_id=api_reference_fix_0 (rank not in top-5)
[10:44:08] TRACE | === END FACT COVERAGE ===
[10:44:08] TRACE | === FACT COVERAGE ANALYSIS ===
[10:44:08] TRACE | Query: Why does my token stop working after 3600 seconds?
[10:44:08] TRACE | Total key facts: 3
[10:44:08] TRACE | Found facts: 1/3
[10:44:08] TRACE |   FOUND[1] 3600 seconds
[10:44:08] TRACE | Missed facts: 2/3
[10:44:08] TRACE |   MISSED[1] max 3600 seconds from iat
[10:44:08] TRACE |   MISSED[2] All tokens expire after 3600 seconds
[10:44:08] TRACE |     -> Found in chunk_id=api_reference_fix_0 (rank not in top-5)
[10:44:08] TRACE | === END FACT COVERAGE ===
[10:44:08] TRACE | === FACT COVERAGE ANALYSIS ===
[10:44:08] TRACE | Query: What database technology does CloudFlow use?
[10:44:08] TRACE | Total key facts: 3
[10:44:08] TRACE | Found facts: 3/3
[10:44:08] TRACE |   FOUND[1] PostgreSQL 15.4
[10:44:08] TRACE |   FOUND[2] Redis 7.2
[10:44:08] TRACE |   FOUND[3] Apache Kafka 3.6
[10:44:08] TRACE | === END FACT COVERAGE ===
[10:44:08] TRACE | === FACT COVERAGE ANALYSIS ===
[10:44:08] TRACE | Query: Which database system powers CloudFlow?
[10:44:08] TRACE | Total key facts: 3
[10:44:08] TRACE | Found facts: 3/3
[10:44:08] TRACE |   FOUND[1] PostgreSQL 15.4
[10:44:08] TRACE |   FOUND[2] Redis 7.2
[10:44:08] TRACE |   FOUND[3] Apache Kafka 3.6
[10:44:08] TRACE | === END FACT COVERAGE ===
[10:44:08] TRACE | === FACT COVERAGE ANALYSIS ===
[10:44:08] TRACE | Query: I need to understand the data storage architecture
[10:44:08] TRACE | Total key facts: 3
[10:44:08] TRACE | Found facts: 3/3
[10:44:08] TRACE |   FOUND[1] PostgreSQL 15.4
[10:44:08] TRACE |   FOUND[2] Redis 7.2
[10:44:08] TRACE |   FOUND[3] Apache Kafka 3.6
[10:44:08] TRACE | === END FACT COVERAGE ===
[10:44:08] TRACE | === FACT COVERAGE ANALYSIS ===
[10:44:08] TRACE | Query: database stack
[10:44:08] TRACE | Total key facts: 3
[10:44:08] TRACE | Found facts: 0/3
[10:44:08] TRACE | Missed facts: 3/3
[10:44:08] TRACE |   MISSED[1] PostgreSQL 15.4
[10:44:08] TRACE |     -> Found in chunk_id=architecture_overview_fix_0 (rank not in top-5)
[10:44:08] TRACE |   MISSED[2] Redis 7.2
[10:44:08] TRACE |     -> Found in chunk_id=architecture_overview_fix_0 (rank not in top-5)
[10:44:08] TRACE |   MISSED[3] Apache Kafka 3.6
[10:44:08] TRACE |     -> Found in chunk_id=architecture_overview_fix_0 (rank not in top-5)
[10:44:08] TRACE | === END FACT COVERAGE ===
[10:44:08] TRACE | === FACT COVERAGE ANALYSIS ===
[10:44:08] TRACE | Query: For capacity planning, what databases does CloudFlow rely on?
[10:44:08] TRACE | Total key facts: 3
[10:44:08] TRACE | Found facts: 3/3
[10:44:08] TRACE |   FOUND[1] PostgreSQL 15.4
[10:44:08] TRACE |   FOUND[2] Redis 7.2
[10:44:08] TRACE |   FOUND[3] Apache Kafka 3.6
[10:44:08] TRACE | === END FACT COVERAGE ===
[10:44:08] TRACE | === FACT COVERAGE ANALYSIS ===
[10:44:08] TRACE | Query: Is CloudFlow using MySQL or something else?
[10:44:08] TRACE | Total key facts: 3
[10:44:08] TRACE | Found facts: 0/3
[10:44:08] TRACE | Missed facts: 3/3
[10:44:08] TRACE |   MISSED[1] PostgreSQL 15.4
[10:44:08] TRACE |     -> Found in chunk_id=architecture_overview_fix_0 (rank not in top-5)
[10:44:08] TRACE |   MISSED[2] Redis 7.2
[10:44:08] TRACE |     -> Found in chunk_id=architecture_overview_fix_0 (rank not in top-5)
[10:44:08] TRACE |   MISSED[3] Apache Kafka 3.6
[10:44:08] TRACE |     -> Found in chunk_id=architecture_overview_fix_0 (rank not in top-5)
[10:44:08] TRACE | === END FACT COVERAGE ===
[10:44:08] TRACE | === FACT COVERAGE ANALYSIS ===
[10:44:08] TRACE | Query: What is the Kubernetes namespace for production deployment?
[10:44:08] TRACE | Total key facts: 1
[10:44:08] TRACE | Found facts: 1/1
[10:44:08] TRACE |   FOUND[1] cloudflow-prod
[10:44:08] TRACE | === END FACT COVERAGE ===
[10:44:08] TRACE | === FACT COVERAGE ANALYSIS ===
[10:44:08] TRACE | Query: Which namespace is used for CloudFlow production?
[10:44:08] TRACE | Total key facts: 1
[10:44:08] TRACE | Found facts: 1/1
[10:44:08] TRACE |   FOUND[1] cloudflow-prod
[10:44:08] TRACE | === END FACT COVERAGE ===
[10:44:08] TRACE | === FACT COVERAGE ANALYSIS ===
[10:44:08] TRACE | Query: I can't find the production pods
[10:44:08] TRACE | Total key facts: 1
[10:44:08] TRACE | Found facts: 1/1
[10:44:08] TRACE |   FOUND[1] cloudflow-prod
[10:44:08] TRACE | === END FACT COVERAGE ===
[10:44:08] TRACE | === FACT COVERAGE ANALYSIS ===
[10:44:08] TRACE | Query: prod namespace
[10:44:08] TRACE | Total key facts: 1
[10:44:08] TRACE | Found facts: 1/1
[10:44:08] TRACE |   FOUND[1] cloudflow-prod
[10:44:08] TRACE | === END FACT COVERAGE ===
[10:44:08] TRACE | === FACT COVERAGE ANALYSIS ===
[10:44:08] TRACE | Query: I need to deploy to production, what namespace should I use?
[10:44:08] TRACE | Total key facts: 1
[10:44:08] TRACE | Found facts: 1/1
[10:44:08] TRACE |   FOUND[1] cloudflow-prod
[10:44:08] TRACE | === END FACT COVERAGE ===
[10:44:08] TRACE | === FACT COVERAGE ANALYSIS ===
[10:44:08] TRACE | Query: Why can't I see resources in the default namespace?
[10:44:08] TRACE | Total key facts: 1
[10:44:08] TRACE | Found facts: 1/1
[10:44:08] TRACE |   FOUND[1] cloudflow-prod
[10:44:08] TRACE | === END FACT COVERAGE ===
[10:44:08] TRACE | === FACT COVERAGE ANALYSIS ===
[10:44:08] TRACE | Query: What are the resource requirements for the API Gateway?
[10:44:08] TRACE | Total key facts: 1
[10:44:08] TRACE | Found facts: 0/1
[10:44:08] TRACE | Missed facts: 1/1
[10:44:08] TRACE |   MISSED[1] 2 vCPU, 4GB RAM per pod
[10:44:08] TRACE |     -> Found in chunk_id=architecture_overview_fix_1 (rank not in top-5)
[10:44:08] TRACE | === END FACT COVERAGE ===
[10:44:08] TRACE | === FACT COVERAGE ANALYSIS ===
[10:44:08] TRACE | Query: How much CPU and memory does the API Gateway need?
[10:44:08] TRACE | Total key facts: 1
[10:44:08] TRACE | Found facts: 1/1
[10:44:08] TRACE |   FOUND[1] 2 vCPU, 4GB RAM per pod
[10:44:08] TRACE | === END FACT COVERAGE ===
[10:44:08] TRACE | === FACT COVERAGE ANALYSIS ===
[10:44:08] TRACE | Query: The API Gateway pods keep getting OOMKilled
[10:44:08] TRACE | Total key facts: 1
[10:44:08] TRACE | Found facts: 1/1
[10:44:08] TRACE |   FOUND[1] 2 vCPU, 4GB RAM per pod
[10:44:08] TRACE | === END FACT COVERAGE ===
[10:44:08] TRACE | === FACT COVERAGE ANALYSIS ===
[10:44:08] TRACE | Query: api gateway resources
[10:44:08] TRACE | Total key facts: 1
[10:44:08] TRACE | Found facts: 1/1
[10:44:08] TRACE |   FOUND[1] 2 vCPU, 4GB RAM per pod
[10:44:08] TRACE | === END FACT COVERAGE ===
[10:44:08] TRACE | === FACT COVERAGE ANALYSIS ===
[10:44:08] TRACE | Query: I'm provisioning infrastructure, what are the compute specs for API Gateway?
[10:44:08] TRACE | Total key facts: 1
[10:44:08] TRACE | Found facts: 1/1
[10:44:08] TRACE |   FOUND[1] 2 vCPU, 4GB RAM per pod
[10:44:08] TRACE | === END FACT COVERAGE ===
[10:44:08] TRACE | === FACT COVERAGE ANALYSIS ===
[10:44:08] TRACE | Query: Why is 1GB RAM not enough for API Gateway?
[10:44:08] TRACE | Total key facts: 1
[10:44:08] TRACE | Found facts: 1/1
[10:44:08] TRACE |   FOUND[1] 2 vCPU, 4GB RAM per pod
[10:44:08] TRACE | === END FACT COVERAGE ===
[10:44:08] TRACE | === FACT COVERAGE ANALYSIS ===
[10:44:08] TRACE | Query: What are the health check endpoints?
[10:44:08] TRACE | Total key facts: 2
[10:44:08] TRACE | Found facts: 2/2
[10:44:08] TRACE |   FOUND[1] /health
[10:44:08] TRACE |   FOUND[2] /ready
[10:44:08] TRACE | === END FACT COVERAGE ===
[10:44:08] TRACE | === FACT COVERAGE ANALYSIS ===
[10:44:08] TRACE | Query: Which URLs should I use for health monitoring?
[10:44:08] TRACE | Total key facts: 2
[10:44:08] TRACE | Found facts: 2/2
[10:44:08] TRACE |   FOUND[1] /health
[10:44:08] TRACE |   FOUND[2] /ready
[10:44:08] TRACE | === END FACT COVERAGE ===
[10:44:08] TRACE | === FACT COVERAGE ANALYSIS ===
[10:44:08] TRACE | Query: My load balancer health checks are failing
[10:44:08] TRACE | Total key facts: 2
[10:44:08] TRACE | Found facts: 1/2
[10:44:08] TRACE |   FOUND[1] /health
[10:44:08] TRACE | Missed facts: 1/2
[10:44:08] TRACE |   MISSED[1] /ready
[10:44:08] TRACE |     -> Found in chunk_id=deployment_guide_fix_2 (rank not in top-5)
[10:44:08] TRACE | === END FACT COVERAGE ===
[10:44:08] TRACE | === FACT COVERAGE ANALYSIS ===
[10:44:08] TRACE | Query: health endpoints
[10:44:08] TRACE | Total key facts: 2
[10:44:08] TRACE | Found facts: 2/2
[10:44:08] TRACE |   FOUND[1] /health
[10:44:08] TRACE |   FOUND[2] /ready
[10:44:08] TRACE | === END FACT COVERAGE ===
[10:44:08] TRACE | === FACT COVERAGE ANALYSIS ===
[10:44:08] TRACE | Query: Setting up monitoring, what endpoints indicate service health?
[10:44:08] TRACE | Total key facts: 2
[10:44:08] TRACE | Found facts: 2/2
[10:44:08] TRACE |   FOUND[1] /health
[10:44:08] TRACE |   FOUND[2] /ready
[10:44:08] TRACE | === END FACT COVERAGE ===
[10:44:08] TRACE | === FACT COVERAGE ANALYSIS ===
[10:44:08] TRACE | Query: Why doesn't /status return health information?
[10:44:08] TRACE | Total key facts: 2
[10:44:08] TRACE | Found facts: 2/2
[10:44:08] TRACE |   FOUND[1] /health
[10:44:08] TRACE |   FOUND[2] /ready
[10:44:08] TRACE | === END FACT COVERAGE ===
[10:44:08] TRACE | === FACT COVERAGE ANALYSIS ===
[10:44:08] TRACE | Query: What are the HPA scaling parameters?
[10:44:08] TRACE | Total key facts: 3
[10:44:08] TRACE | Found facts: 2/3
[10:44:08] TRACE |   FOUND[1] minReplicas: 3
[10:44:08] TRACE |   FOUND[2] maxReplicas: 10
[10:44:08] TRACE | Missed facts: 1/3
[10:44:08] TRACE |   MISSED[1] targetCPUUtilizationPercentage: 70
[10:44:08] TRACE |     -> Found in chunk_id=deployment_guide_fix_1 (rank not in top-5)
[10:44:08] TRACE | === END FACT COVERAGE ===
[10:44:08] TRACE | === FACT COVERAGE ANALYSIS ===
[10:44:08] TRACE | Query: How does horizontal pod autoscaling work?
[10:44:08] TRACE | Total key facts: 3
[10:44:08] TRACE | Found facts: 2/3
[10:44:08] TRACE |   FOUND[1] minReplicas: 3
[10:44:08] TRACE |   FOUND[2] maxReplicas: 10
[10:44:08] TRACE | Missed facts: 1/3
[10:44:08] TRACE |   MISSED[1] targetCPUUtilizationPercentage: 70
[10:44:08] TRACE |     -> Found in chunk_id=deployment_guide_fix_1 (rank not in top-5)
[10:44:08] TRACE | === END FACT COVERAGE ===
[10:44:08] TRACE | === FACT COVERAGE ANALYSIS ===
[10:44:08] TRACE | Query: Pods aren't scaling up during traffic spikes
[10:44:08] TRACE | Total key facts: 3
[10:44:08] TRACE | Found facts: 2/3
[10:44:08] TRACE |   FOUND[1] minReplicas: 3
[10:44:08] TRACE |   FOUND[2] maxReplicas: 10
[10:44:08] TRACE | Missed facts: 1/3
[10:44:08] TRACE |   MISSED[1] targetCPUUtilizationPercentage: 70
[10:44:08] TRACE |     -> Found in chunk_id=deployment_guide_fix_1 (rank not in top-5)
[10:44:08] TRACE | === END FACT COVERAGE ===
[10:44:08] TRACE | === FACT COVERAGE ANALYSIS ===
[10:44:08] TRACE | Query: autoscaling config
[10:44:08] TRACE | Total key facts: 3
[10:44:08] TRACE | Found facts: 2/3
[10:44:08] TRACE |   FOUND[1] minReplicas: 3
[10:44:08] TRACE |   FOUND[2] maxReplicas: 10
[10:44:08] TRACE | Missed facts: 1/3
[10:44:08] TRACE |   MISSED[1] targetCPUUtilizationPercentage: 70
[10:44:08] TRACE |     -> Found in chunk_id=deployment_guide_fix_1 (rank not in top-5)
[10:44:08] TRACE | === END FACT COVERAGE ===
[10:44:08] TRACE | === FACT COVERAGE ANALYSIS ===
[10:44:08] TRACE | Query: I need to configure autoscaling, what are the min/max replicas and thresholds?
[10:44:08] TRACE | Total key facts: 3
[10:44:08] TRACE | Found facts: 0/3
[10:44:08] TRACE | Missed facts: 3/3
[10:44:08] TRACE |   MISSED[1] minReplicas: 3
[10:44:08] TRACE |     -> Found in chunk_id=deployment_guide_fix_1 (rank not in top-5)
[10:44:08] TRACE |   MISSED[2] maxReplicas: 10
[10:44:08] TRACE |     -> Found in chunk_id=deployment_guide_fix_1 (rank not in top-5)
[10:44:08] TRACE |   MISSED[3] targetCPUUtilizationPercentage: 70
[10:44:08] TRACE |     -> Found in chunk_id=deployment_guide_fix_1 (rank not in top-5)
[10:44:08] TRACE | === END FACT COVERAGE ===
[10:44:08] TRACE | === FACT COVERAGE ANALYSIS ===
[10:44:08] TRACE | Query: Why do we have 3 replicas even with low traffic?
[10:44:08] TRACE | Total key facts: 3
[10:44:08] TRACE | Found facts: 0/3
[10:44:08] TRACE | Missed facts: 3/3
[10:44:08] TRACE |   MISSED[1] minReplicas: 3
[10:44:08] TRACE |     -> Found in chunk_id=deployment_guide_fix_1 (rank not in top-5)
[10:44:08] TRACE |   MISSED[2] maxReplicas: 10
[10:44:08] TRACE |     -> Found in chunk_id=deployment_guide_fix_1 (rank not in top-5)
[10:44:08] TRACE |   MISSED[3] targetCPUUtilizationPercentage: 70
[10:44:08] TRACE |     -> Found in chunk_id=deployment_guide_fix_1 (rank not in top-5)
[10:44:08] TRACE | === END FACT COVERAGE ===
[10:44:08] TRACE | === FACT COVERAGE ANALYSIS ===
[10:44:08] TRACE | Query: What is the P99 latency target for API operations?
[10:44:08] TRACE | Total key facts: 2
[10:44:08] TRACE | Found facts: 1/2
[10:44:08] TRACE |   FOUND[1] average P99 latency of 180ms for API operations
[10:44:08] TRACE | Missed facts: 1/2
[10:44:08] TRACE |   MISSED[1] P99 latency: < 200ms
[10:44:08] TRACE |     -> Found in chunk_id=architecture_overview_fix_1 (rank not in top-5)
[10:44:08] TRACE | === END FACT COVERAGE ===
[10:44:08] TRACE | === FACT COVERAGE ANALYSIS ===
[10:44:08] TRACE | Query: What's the 99th percentile response time target?
[10:44:08] TRACE | Total key facts: 2
[10:44:08] TRACE | Found facts: 0/2
[10:44:08] TRACE | Missed facts: 2/2
[10:44:08] TRACE |   MISSED[1] P99 latency: < 200ms
[10:44:08] TRACE |     -> Found in chunk_id=architecture_overview_fix_1 (rank not in top-5)
[10:44:08] TRACE |   MISSED[2] average P99 latency of 180ms for API operations
[10:44:08] TRACE |     -> Found in chunk_id=architecture_overview_fix_0 (rank not in top-5)
[10:44:08] TRACE | === END FACT COVERAGE ===
[10:44:08] TRACE | === FACT COVERAGE ANALYSIS ===
[10:44:08] TRACE | Query: Our API latency is 500ms, is that acceptable?
[10:44:08] TRACE | Total key facts: 2
[10:44:08] TRACE | Found facts: 1/2
[10:44:08] TRACE |   FOUND[1] average P99 latency of 180ms for API operations
[10:44:08] TRACE | Missed facts: 1/2
[10:44:08] TRACE |   MISSED[1] P99 latency: < 200ms
[10:44:08] TRACE |     -> Found in chunk_id=architecture_overview_fix_1 (rank not in top-5)
[10:44:08] TRACE | === END FACT COVERAGE ===
[10:44:08] TRACE | === FACT COVERAGE ANALYSIS ===
[10:44:08] TRACE | Query: api latency target
[10:44:08] TRACE | Total key facts: 2
[10:44:08] TRACE | Found facts: 0/2
[10:44:08] TRACE | Missed facts: 2/2
[10:44:08] TRACE |   MISSED[1] P99 latency: < 200ms
[10:44:08] TRACE |     -> Found in chunk_id=architecture_overview_fix_1 (rank not in top-5)
[10:44:08] TRACE |   MISSED[2] average P99 latency of 180ms for API operations
[10:44:08] TRACE |     -> Found in chunk_id=architecture_overview_fix_0 (rank not in top-5)
[10:44:08] TRACE | === END FACT COVERAGE ===
[10:44:08] TRACE | === FACT COVERAGE ANALYSIS ===
[10:44:08] TRACE | Query: Setting SLOs for our service, what P99 latency does CloudFlow guarantee?
[10:44:08] TRACE | Total key facts: 2
[10:44:08] TRACE | Found facts: 1/2
[10:44:08] TRACE |   FOUND[1] average P99 latency of 180ms for API operations
[10:44:08] TRACE | Missed facts: 1/2
[10:44:08] TRACE |   MISSED[1] P99 latency: < 200ms
[10:44:08] TRACE |     -> Found in chunk_id=architecture_overview_fix_1 (rank not in top-5)
[10:44:08] TRACE | === END FACT COVERAGE ===
[10:44:08] TRACE | === FACT COVERAGE ANALYSIS ===
[10:44:08] TRACE | Query: Why are we getting alerts at 200ms latency?
[10:44:08] TRACE | Total key facts: 2
[10:44:08] TRACE | Found facts: 0/2
[10:44:08] TRACE | Missed facts: 2/2
[10:44:08] TRACE |   MISSED[1] P99 latency: < 200ms
[10:44:08] TRACE |     -> Found in chunk_id=architecture_overview_fix_1 (rank not in top-5)
[10:44:08] TRACE |   MISSED[2] average P99 latency of 180ms for API operations
[10:44:08] TRACE |     -> Found in chunk_id=architecture_overview_fix_0 (rank not in top-5)
[10:44:08] TRACE | === END FACT COVERAGE ===
[10:44:08] TRACE | === FACT COVERAGE ANALYSIS ===
[10:44:08] TRACE | Query: What are the disaster recovery RPO and RTO values?
[10:44:08] TRACE | Total key facts: 2
[10:44:08] TRACE | Found facts: 0/2
[10:44:08] TRACE | Missed facts: 2/2
[10:44:08] TRACE |   MISSED[1] RPO (Recovery Point Objective): 1 hour
[10:44:08] TRACE |     -> Found in chunk_id=architecture_overview_fix_10 (rank not in top-5)
[10:44:08] TRACE |   MISSED[2] RTO (Recovery Time Objective): 4 hours
[10:44:08] TRACE |     -> Found in chunk_id=architecture_overview_fix_10 (rank not in top-5)
[10:44:08] TRACE | === END FACT COVERAGE ===
[10:44:08] TRACE | === FACT COVERAGE ANALYSIS ===
[10:44:08] TRACE | Query: What's the maximum data loss and recovery time?
[10:44:08] TRACE | Total key facts: 2
[10:44:08] TRACE | Found facts: 2/2
[10:44:08] TRACE |   FOUND[1] RPO (Recovery Point Objective): 1 hour
[10:44:08] TRACE |   FOUND[2] RTO (Recovery Time Objective): 4 hours
[10:44:08] TRACE | === END FACT COVERAGE ===
[10:44:08] TRACE | === FACT COVERAGE ANALYSIS ===
[10:44:08] TRACE | Query: How long will it take to recover from a disaster?
[10:44:08] TRACE | Total key facts: 2
[10:44:08] TRACE | Found facts: 0/2
[10:44:08] TRACE | Missed facts: 2/2
[10:44:08] TRACE |   MISSED[1] RPO (Recovery Point Objective): 1 hour
[10:44:08] TRACE |     -> Found in chunk_id=architecture_overview_fix_10 (rank not in top-5)
[10:44:08] TRACE |   MISSED[2] RTO (Recovery Time Objective): 4 hours
[10:44:08] TRACE |     -> Found in chunk_id=architecture_overview_fix_10 (rank not in top-5)
[10:44:08] TRACE | === END FACT COVERAGE ===
[10:44:08] TRACE | === FACT COVERAGE ANALYSIS ===
[10:44:08] TRACE | Query: RPO RTO
[10:44:08] TRACE | Total key facts: 2
[10:44:08] TRACE | Found facts: 0/2
[10:44:08] TRACE | Missed facts: 2/2
[10:44:08] TRACE |   MISSED[1] RPO (Recovery Point Objective): 1 hour
[10:44:08] TRACE |     -> Found in chunk_id=architecture_overview_fix_10 (rank not in top-5)
[10:44:08] TRACE |   MISSED[2] RTO (Recovery Time Objective): 4 hours
[10:44:08] TRACE |     -> Found in chunk_id=architecture_overview_fix_10 (rank not in top-5)
[10:44:08] TRACE | === END FACT COVERAGE ===
[10:44:08] TRACE | === FACT COVERAGE ANALYSIS ===
[10:44:08] TRACE | Query: For our business continuity plan, what are CloudFlow's recovery objectives?
[10:44:08] TRACE | Total key facts: 2
[10:44:08] TRACE | Found facts: 0/2
[10:44:08] TRACE | Missed facts: 2/2
[10:44:08] TRACE |   MISSED[1] RPO (Recovery Point Objective): 1 hour
[10:44:08] TRACE |     -> Found in chunk_id=architecture_overview_fix_10 (rank not in top-5)
[10:44:08] TRACE |   MISSED[2] RTO (Recovery Time Objective): 4 hours
[10:44:08] TRACE |     -> Found in chunk_id=architecture_overview_fix_10 (rank not in top-5)
[10:44:08] TRACE | === END FACT COVERAGE ===
[10:44:08] TRACE | === FACT COVERAGE ANALYSIS ===
[10:44:08] TRACE | Query: Why can't we guarantee zero data loss?
[10:44:08] TRACE | Total key facts: 2
[10:44:08] TRACE | Found facts: 0/2
[10:44:08] TRACE | Missed facts: 2/2
[10:44:08] TRACE |   MISSED[1] RPO (Recovery Point Objective): 1 hour
[10:44:08] TRACE |     -> Found in chunk_id=architecture_overview_fix_10 (rank not in top-5)
[10:44:08] TRACE |   MISSED[2] RTO (Recovery Time Objective): 4 hours
[10:44:08] TRACE |     -> Found in chunk_id=architecture_overview_fix_10 (rank not in top-5)
[10:44:08] TRACE | === END FACT COVERAGE ===
[10:44:08] TRACE | === FACT COVERAGE ANALYSIS ===
[10:44:08] TRACE | Query: What is the maximum workflow execution timeout?
[10:44:08] TRACE | Total key facts: 2
[10:44:08] TRACE | Found facts: 1/2
[10:44:08] TRACE |   FOUND[1] 3600 seconds
[10:44:08] TRACE | Missed facts: 1/2
[10:44:08] TRACE |   MISSED[1] exceeded maximum execution time of 3600 seconds
[10:44:08] TRACE |     -> Found in chunk_id=troubleshooting_guide_fix_4 (rank not in top-5)
[10:44:08] TRACE | === END FACT COVERAGE ===
[10:44:08] TRACE | === FACT COVERAGE ANALYSIS ===
[10:44:08] TRACE | Query: How long can a workflow run before timing out?
[10:44:08] TRACE | Total key facts: 2
[10:44:08] TRACE | Found facts: 1/2
[10:44:08] TRACE |   FOUND[1] 3600 seconds
[10:44:08] TRACE | Missed facts: 1/2
[10:44:08] TRACE |   MISSED[1] exceeded maximum execution time of 3600 seconds
[10:44:08] TRACE |     -> Found in chunk_id=troubleshooting_guide_fix_4 (rank not in top-5)
[10:44:08] TRACE | === END FACT COVERAGE ===
[10:44:08] TRACE | === FACT COVERAGE ANALYSIS ===
[10:44:08] TRACE | Query: My workflow is being killed after an hour
[10:44:08] TRACE | Total key facts: 2
[10:44:08] TRACE | Found facts: 1/2
[10:44:08] TRACE |   FOUND[1] 3600 seconds
[10:44:08] TRACE | Missed facts: 1/2
[10:44:08] TRACE |   MISSED[1] exceeded maximum execution time of 3600 seconds
[10:44:08] TRACE |     -> Found in chunk_id=troubleshooting_guide_fix_4 (rank not in top-5)
[10:44:08] TRACE | === END FACT COVERAGE ===
[10:44:08] TRACE | === FACT COVERAGE ANALYSIS ===
[10:44:08] TRACE | Query: workflow timeout
[10:44:08] TRACE | Total key facts: 2
[10:44:08] TRACE | Found facts: 1/2
[10:44:08] TRACE |   FOUND[1] 3600 seconds
[10:44:08] TRACE | Missed facts: 1/2
[10:44:08] TRACE |   MISSED[1] exceeded maximum execution time of 3600 seconds
[10:44:08] TRACE |     -> Found in chunk_id=troubleshooting_guide_fix_4 (rank not in top-5)
[10:44:08] TRACE | === END FACT COVERAGE ===
[10:44:08] TRACE | === FACT COVERAGE ANALYSIS ===
[10:44:08] TRACE | Query: I have a long-running data processing workflow, what's the time limit?
[10:44:08] TRACE | Total key facts: 2
[10:44:08] TRACE | Found facts: 0/2
[10:44:08] TRACE | Missed facts: 2/2
[10:44:08] TRACE |   MISSED[1] 3600 seconds
[10:44:08] TRACE |     -> Found in chunk_id=api_reference_fix_0 (rank not in top-5)
[10:44:08] TRACE |   MISSED[2] exceeded maximum execution time of 3600 seconds
[10:44:08] TRACE |     -> Found in chunk_id=troubleshooting_guide_fix_4 (rank not in top-5)
[10:44:08] TRACE | === END FACT COVERAGE ===
[10:44:08] TRACE | === FACT COVERAGE ANALYSIS ===
[10:44:08] TRACE | Query: Why did my workflow fail after 3600 seconds?
[10:44:08] TRACE | Total key facts: 2
[10:44:08] TRACE | Found facts: 1/2
[10:44:08] TRACE |   FOUND[1] 3600 seconds
[10:44:08] TRACE | Missed facts: 1/2
[10:44:08] TRACE |   MISSED[1] exceeded maximum execution time of 3600 seconds
[10:44:08] TRACE |     -> Found in chunk_id=troubleshooting_guide_fix_4 (rank not in top-5)
[10:44:08] TRACE | === END FACT COVERAGE ===
[10:44:08] TRACE | === FACT COVERAGE ANALYSIS ===
[10:44:08] TRACE | Query: What JWT algorithm is used for token signing?
[10:44:08] TRACE | Total key facts: 3
[10:44:08] TRACE | Found facts: 3/3
[10:44:08] TRACE |   FOUND[1] RS256
[10:44:08] TRACE |   FOUND[2] RS256 algorithm
[10:44:08] TRACE |   FOUND[3] RS256 signing algorithm
[10:44:08] TRACE | === END FACT COVERAGE ===
[10:44:08] TRACE | === FACT COVERAGE ANALYSIS ===
[10:44:08] TRACE | Query: Which signing algorithm does CloudFlow use for JWTs?
[10:44:08] TRACE | Total key facts: 3
[10:44:08] TRACE | Found facts: 2/3
[10:44:08] TRACE |   FOUND[1] RS256
[10:44:08] TRACE |   FOUND[2] RS256 signing algorithm
[10:44:08] TRACE | Missed facts: 1/3
[10:44:08] TRACE |   MISSED[1] RS256 algorithm
[10:44:08] TRACE |     -> Found in chunk_id=architecture_overview_fix_1 (rank not in top-5)
[10:44:08] TRACE | === END FACT COVERAGE ===
[10:44:08] TRACE | === FACT COVERAGE ANALYSIS ===
[10:44:08] TRACE | Query: My JWT validation is failing with algorithm mismatch
[10:44:08] TRACE | Total key facts: 3
[10:44:08] TRACE | Found facts: 2/3
[10:44:08] TRACE |   FOUND[1] RS256
[10:44:08] TRACE |   FOUND[2] RS256 algorithm
[10:44:08] TRACE | Missed facts: 1/3
[10:44:08] TRACE |   MISSED[1] RS256 signing algorithm
[10:44:08] TRACE |     -> Found in chunk_id=api_reference_fix_0 (rank not in top-5)
[10:44:08] TRACE | === END FACT COVERAGE ===
[10:44:08] TRACE | === FACT COVERAGE ANALYSIS ===
[10:44:08] TRACE | Query: jwt algorithm
[10:44:08] TRACE | Total key facts: 3
[10:44:08] TRACE | Found facts: 3/3
[10:44:08] TRACE |   FOUND[1] RS256
[10:44:08] TRACE |   FOUND[2] RS256 algorithm
[10:44:08] TRACE |   FOUND[3] RS256 signing algorithm
[10:44:08] TRACE | === END FACT COVERAGE ===
[10:44:08] TRACE | === FACT COVERAGE ANALYSIS ===
[10:44:08] TRACE | Query: I'm implementing JWT verification, what algorithm should I expect?
[10:44:08] TRACE | Total key facts: 3
[10:44:08] TRACE | Found facts: 3/3
[10:44:08] TRACE |   FOUND[1] RS256
[10:44:08] TRACE |   FOUND[2] RS256 algorithm
[10:44:08] TRACE |   FOUND[3] RS256 signing algorithm
[10:44:08] TRACE | === END FACT COVERAGE ===
[10:44:08] TRACE | === FACT COVERAGE ANALYSIS ===
[10:44:08] TRACE | Query: Why doesn't HS256 work for token validation?
[10:44:08] TRACE | Total key facts: 3
[10:44:08] TRACE | Found facts: 2/3
[10:44:08] TRACE |   FOUND[1] RS256
[10:44:08] TRACE |   FOUND[2] RS256 algorithm
[10:44:08] TRACE | Missed facts: 1/3
[10:44:08] TRACE |   MISSED[1] RS256 signing algorithm
[10:44:08] TRACE |     -> Found in chunk_id=api_reference_fix_0 (rank not in top-5)
[10:44:08] TRACE | === END FACT COVERAGE ===
[10:44:08] TRACE | === FACT COVERAGE ANALYSIS ===
[10:44:08] TRACE | Query: What is the Redis cache TTL for workflow definitions?
[10:44:08] TRACE | Total key facts: 2
[10:44:08] TRACE | Found facts: 1/2
[10:44:08] TRACE |   FOUND[1] TTL: 1 hour
[10:44:08] TRACE | Missed facts: 1/2
[10:44:08] TRACE |   MISSED[1] Workflow Definitions: TTL: 1 hour
[10:44:08] TRACE | === END FACT COVERAGE ===
[10:44:08] TRACE | === FACT COVERAGE ANALYSIS ===
[10:44:08] TRACE | Query: How long are workflow definitions cached?
[10:44:08] TRACE | Total key facts: 2
[10:44:08] TRACE | Found facts: 0/2
[10:44:08] TRACE | Missed facts: 2/2
[10:44:08] TRACE |   MISSED[1] TTL: 1 hour
[10:44:08] TRACE |     -> Found in chunk_id=architecture_overview_fix_6 (rank not in top-5)
[10:44:08] TRACE |   MISSED[2] Workflow Definitions: TTL: 1 hour
[10:44:08] TRACE | === END FACT COVERAGE ===
[10:44:08] TRACE | === FACT COVERAGE ANALYSIS ===
[10:44:08] TRACE | Query: My workflow updates aren't reflecting immediately
[10:44:08] TRACE | Total key facts: 2
[10:44:08] TRACE | Found facts: 0/2
[10:44:08] TRACE | Missed facts: 2/2
[10:44:08] TRACE |   MISSED[1] TTL: 1 hour
[10:44:08] TRACE |     -> Found in chunk_id=architecture_overview_fix_6 (rank not in top-5)
[10:44:08] TRACE |   MISSED[2] Workflow Definitions: TTL: 1 hour
[10:44:08] TRACE | === END FACT COVERAGE ===
[10:44:08] TRACE | === FACT COVERAGE ANALYSIS ===
[10:44:08] TRACE | Query: cache ttl workflows
[10:44:08] TRACE | Total key facts: 2
[10:44:08] TRACE | Found facts: 0/2
[10:44:08] TRACE | Missed facts: 2/2
[10:44:08] TRACE |   MISSED[1] TTL: 1 hour
[10:44:08] TRACE |     -> Found in chunk_id=architecture_overview_fix_6 (rank not in top-5)
[10:44:08] TRACE |   MISSED[2] Workflow Definitions: TTL: 1 hour
[10:44:08] TRACE | === END FACT COVERAGE ===
[10:44:08] TRACE | === FACT COVERAGE ANALYSIS ===
[10:44:08] TRACE | Query: After updating a workflow, how long until the cache expires?
[10:44:08] TRACE | Total key facts: 2
[10:44:08] TRACE | Found facts: 0/2
[10:44:08] TRACE | Missed facts: 2/2
[10:44:08] TRACE |   MISSED[1] TTL: 1 hour
[10:44:08] TRACE |     -> Found in chunk_id=architecture_overview_fix_6 (rank not in top-5)
[10:44:08] TRACE |   MISSED[2] Workflow Definitions: TTL: 1 hour
[10:44:08] TRACE | === END FACT COVERAGE ===
[10:44:08] TRACE | === FACT COVERAGE ANALYSIS ===
[10:44:08] TRACE | Query: Why are changes taking an hour to appear?
[10:44:08] TRACE | Total key facts: 2
[10:44:08] TRACE | Found facts: 0/2
[10:44:08] TRACE | Missed facts: 2/2
[10:44:08] TRACE |   MISSED[1] TTL: 1 hour
[10:44:08] TRACE |     -> Found in chunk_id=architecture_overview_fix_6 (rank not in top-5)
[10:44:08] TRACE |   MISSED[2] Workflow Definitions: TTL: 1 hour
[10:44:08] TRACE | === END FACT COVERAGE ===
[10:44:08] TRACE | === FACT COVERAGE ANALYSIS ===
[10:44:08] TRACE | Query: What monitoring tools does CloudFlow use?
[10:44:08] TRACE | Total key facts: 3
[10:44:08] TRACE | Found facts: 2/3
[10:44:08] TRACE |   FOUND[1] Prometheus
[10:44:08] TRACE |   FOUND[2] Grafana
[10:44:08] TRACE | Missed facts: 1/3
[10:44:08] TRACE |   MISSED[1] Jaeger for distributed tracing
[10:44:08] TRACE |     -> Found in chunk_id=architecture_overview_fix_0 (rank not in top-5)
[10:44:08] TRACE | === END FACT COVERAGE ===
[10:44:08] TRACE | === FACT COVERAGE ANALYSIS ===
[10:44:08] TRACE | Query: Which observability platform is integrated?
[10:44:08] TRACE | Total key facts: 3
[10:44:08] TRACE | Found facts: 3/3
[10:44:08] TRACE |   FOUND[1] Prometheus
[10:44:08] TRACE |   FOUND[2] Grafana
[10:44:08] TRACE |   FOUND[3] Jaeger for distributed tracing
[10:44:08] TRACE | === END FACT COVERAGE ===
[10:44:08] TRACE | === FACT COVERAGE ANALYSIS ===
[10:44:08] TRACE | Query: Where can I view CloudFlow metrics and logs?
[10:44:08] TRACE | Total key facts: 3
[10:44:08] TRACE | Found facts: 2/3
[10:44:08] TRACE |   FOUND[1] Prometheus
[10:44:08] TRACE |   FOUND[2] Grafana
[10:44:08] TRACE | Missed facts: 1/3
[10:44:08] TRACE |   MISSED[1] Jaeger for distributed tracing
[10:44:08] TRACE |     -> Found in chunk_id=architecture_overview_fix_0 (rank not in top-5)
[10:44:08] TRACE | === END FACT COVERAGE ===
[10:44:08] TRACE | === FACT COVERAGE ANALYSIS ===
[10:44:08] TRACE | Query: monitoring stack
[10:44:08] TRACE | Total key facts: 3
[10:44:08] TRACE | Found facts: 2/3
[10:44:08] TRACE |   FOUND[1] Prometheus
[10:44:08] TRACE |   FOUND[2] Grafana
[10:44:08] TRACE | Missed facts: 1/3
[10:44:08] TRACE |   MISSED[1] Jaeger for distributed tracing
[10:44:08] TRACE |     -> Found in chunk_id=architecture_overview_fix_0 (rank not in top-5)
[10:44:08] TRACE | === END FACT COVERAGE ===
[10:44:08] TRACE | === FACT COVERAGE ANALYSIS ===
[10:44:08] TRACE | Query: I need to set up dashboards, what monitoring systems are available?
[10:44:08] TRACE | Total key facts: 3
[10:44:08] TRACE | Found facts: 2/3
[10:44:08] TRACE |   FOUND[1] Prometheus
[10:44:08] TRACE |   FOUND[2] Grafana
[10:44:08] TRACE | Missed facts: 1/3
[10:44:08] TRACE |   MISSED[1] Jaeger for distributed tracing
[10:44:08] TRACE |     -> Found in chunk_id=architecture_overview_fix_0 (rank not in top-5)
[10:44:08] TRACE | === END FACT COVERAGE ===
[10:44:08] TRACE | === FACT COVERAGE ANALYSIS ===
[10:44:08] TRACE | Query: Why can't I see metrics in Datadog?
[10:44:08] TRACE | Total key facts: 3
[10:44:08] TRACE | Found facts: 2/3
[10:44:08] TRACE |   FOUND[1] Prometheus
[10:44:08] TRACE |   FOUND[2] Grafana
[10:44:08] TRACE | Missed facts: 1/3
[10:44:08] TRACE |   MISSED[1] Jaeger for distributed tracing
[10:44:08] TRACE |     -> Found in chunk_id=architecture_overview_fix_0 (rank not in top-5)
[10:44:08] TRACE | === END FACT COVERAGE ===
[10:44:08] TRACE | === FACT COVERAGE ANALYSIS ===
[10:44:08] TRACE | Query: How do I diagnose database connection pool exhaustion?
[10:44:08] TRACE | Total key facts: 4
[10:44:08] TRACE | Found facts: 3/4
[10:44:08] TRACE |   FOUND[1] could not obtain connection from pool within 5000ms
[10:44:08] TRACE |   FOUND[2] connection pool exhausted (100/100 connections in use)
[10:44:08] TRACE |   FOUND[3] PgBouncer
[10:44:08] TRACE | Missed facts: 1/4
[10:44:08] TRACE |   MISSED[1] max_db_connections = 100
[10:44:08] TRACE |     -> Found in chunk_id=deployment_guide_fix_3 (rank not in top-5)
[10:44:08] TRACE | === END FACT COVERAGE ===
[10:44:08] TRACE | === FACT COVERAGE ANALYSIS ===
[10:44:08] TRACE | Query: What should I check when I run out of database connections?
[10:44:08] TRACE | Total key facts: 4
[10:44:08] TRACE | Found facts: 3/4
[10:44:08] TRACE |   FOUND[1] could not obtain connection from pool within 5000ms
[10:44:08] TRACE |   FOUND[2] connection pool exhausted (100/100 connections in use)
[10:44:08] TRACE |   FOUND[3] PgBouncer
[10:44:08] TRACE | Missed facts: 1/4
[10:44:08] TRACE |   MISSED[1] max_db_connections = 100
[10:44:08] TRACE |     -> Found in chunk_id=deployment_guide_fix_3 (rank not in top-5)
[10:44:08] TRACE | === END FACT COVERAGE ===
[10:44:08] TRACE | === FACT COVERAGE ANALYSIS ===
[10:44:08] TRACE | Query: Getting 'could not obtain connection from pool' errors
[10:44:08] TRACE | Total key facts: 4
[10:44:08] TRACE | Found facts: 3/4
[10:44:08] TRACE |   FOUND[1] could not obtain connection from pool within 5000ms
[10:44:08] TRACE |   FOUND[2] connection pool exhausted (100/100 connections in use)
[10:44:08] TRACE |   FOUND[3] PgBouncer
[10:44:08] TRACE | Missed facts: 1/4
[10:44:08] TRACE |   MISSED[1] max_db_connections = 100
[10:44:08] TRACE |     -> Found in chunk_id=deployment_guide_fix_3 (rank not in top-5)
[10:44:08] TRACE | === END FACT COVERAGE ===
[10:44:08] TRACE | === FACT COVERAGE ANALYSIS ===
[10:44:08] TRACE | Query: connection pool full
[10:44:08] TRACE | Total key facts: 4
[10:44:08] TRACE | Found facts: 3/4
[10:44:08] TRACE |   FOUND[1] could not obtain connection from pool within 5000ms
[10:44:08] TRACE |   FOUND[2] connection pool exhausted (100/100 connections in use)
[10:44:08] TRACE |   FOUND[3] PgBouncer
[10:44:08] TRACE | Missed facts: 1/4
[10:44:08] TRACE |   MISSED[1] max_db_connections = 100
[10:44:08] TRACE |     -> Found in chunk_id=deployment_guide_fix_3 (rank not in top-5)
[10:44:08] TRACE | === END FACT COVERAGE ===
[10:44:08] TRACE | === FACT COVERAGE ANALYSIS ===
[10:44:08] TRACE | Query: My app is failing with connection pool errors, how do I troubleshoot?
[10:44:08] TRACE | Total key facts: 4
[10:44:08] TRACE | Found facts: 3/4
[10:44:08] TRACE |   FOUND[1] could not obtain connection from pool within 5000ms
[10:44:08] TRACE |   FOUND[2] connection pool exhausted (100/100 connections in use)
[10:44:08] TRACE |   FOUND[3] PgBouncer
[10:44:08] TRACE | Missed facts: 1/4
[10:44:08] TRACE |   MISSED[1] max_db_connections = 100
[10:44:08] TRACE |     -> Found in chunk_id=deployment_guide_fix_3 (rank not in top-5)
[10:44:08] TRACE | === END FACT COVERAGE ===
[10:44:08] TRACE | === FACT COVERAGE ANALYSIS ===
[10:44:08] TRACE | Query: Why can't I get a database connection even though CPU is low?
[10:44:08] TRACE | Total key facts: 4
[10:44:08] TRACE | Found facts: 3/4
[10:44:08] TRACE |   FOUND[1] could not obtain connection from pool within 5000ms
[10:44:08] TRACE |   FOUND[2] connection pool exhausted (100/100 connections in use)
[10:44:08] TRACE |   FOUND[3] PgBouncer
[10:44:08] TRACE | Missed facts: 1/4
[10:44:08] TRACE |   MISSED[1] max_db_connections = 100
[10:44:08] TRACE |     -> Found in chunk_id=deployment_guide_fix_3 (rank not in top-5)
[10:44:08] TRACE | === END FACT COVERAGE ===
[10:44:08] TRACE | === FACT COVERAGE ANALYSIS ===
[10:44:08] TRACE | Query: How do I handle API authentication?
[10:44:08] TRACE | Total key facts: 3
[10:44:08] TRACE | Found facts: 3/3
[10:44:08] TRACE |   FOUND[1] OAuth 2.0
[10:44:08] TRACE |   FOUND[2] API keys
[10:44:08] TRACE |   FOUND[3] JWT tokens
[10:44:08] TRACE | === END FACT COVERAGE ===
[10:44:08] TRACE | === FACT COVERAGE ANALYSIS ===
[10:44:08] TRACE | Query: What authentication methods are supported?
[10:44:08] TRACE | Total key facts: 3
[10:44:08] TRACE | Found facts: 3/3
[10:44:08] TRACE |   FOUND[1] OAuth 2.0
[10:44:08] TRACE |   FOUND[2] API keys
[10:44:08] TRACE |   FOUND[3] JWT tokens
[10:44:08] TRACE | === END FACT COVERAGE ===
[10:44:08] TRACE | === FACT COVERAGE ANALYSIS ===
[10:44:08] TRACE | Query: My API requests are getting 401 errors
[10:44:08] TRACE | Total key facts: 3
[10:44:08] TRACE | Found facts: 2/3
[10:44:08] TRACE |   FOUND[1] OAuth 2.0
[10:44:08] TRACE |   FOUND[2] API keys
[10:44:08] TRACE | Missed facts: 1/3
[10:44:08] TRACE |   MISSED[1] JWT tokens
[10:44:08] TRACE |     -> Found in chunk_id=api_reference_fix_0 (rank not in top-5)
[10:44:08] TRACE | === END FACT COVERAGE ===
[10:44:08] TRACE | === FACT COVERAGE ANALYSIS ===
[10:44:08] TRACE | Query: auth methods
[10:44:08] TRACE | Total key facts: 3
[10:44:08] TRACE | Found facts: 3/3
[10:44:08] TRACE |   FOUND[1] OAuth 2.0
[10:44:08] TRACE |   FOUND[2] API keys
[10:44:08] TRACE |   FOUND[3] JWT tokens
[10:44:08] TRACE | === END FACT COVERAGE ===
[10:44:08] TRACE | === FACT COVERAGE ANALYSIS ===
[10:44:08] TRACE | Query: I'm integrating with CloudFlow API, what authentication options do I have?
[10:44:08] TRACE | Total key facts: 3
[10:44:08] TRACE | Found facts: 3/3
[10:44:08] TRACE |   FOUND[1] OAuth 2.0
[10:44:08] TRACE |   FOUND[2] API keys
[10:44:08] TRACE |   FOUND[3] JWT tokens
[10:44:08] TRACE | === END FACT COVERAGE ===
[10:44:08] TRACE | === FACT COVERAGE ANALYSIS ===
[10:44:08] TRACE | Query: Why isn't basic auth working?
[10:44:08] TRACE | Total key facts: 3
[10:44:08] TRACE | Found facts: 2/3
[10:44:08] TRACE |   FOUND[1] OAuth 2.0
[10:44:08] TRACE |   FOUND[2] API keys
[10:44:08] TRACE | Missed facts: 1/3
[10:44:08] TRACE |   MISSED[1] JWT tokens
[10:44:08] TRACE |     -> Found in chunk_id=api_reference_fix_0 (rank not in top-5)
[10:44:08] TRACE | === END FACT COVERAGE ===
[10:44:08] TRACE | === FACT COVERAGE ANALYSIS ===
[10:44:08] TRACE | Query: What is PgBouncer and why is it used in CloudFlow?
[10:44:08] TRACE | Total key facts: 5
[10:44:08] TRACE | Found facts: 5/5
[10:44:08] TRACE |   FOUND[1] PgBouncer
[10:44:08] TRACE |   FOUND[2] connection pooling
[10:44:08] TRACE |   FOUND[3] max_db_connections = 100
[10:44:08] TRACE |   FOUND[4] default_pool_size = 25
[10:44:08] TRACE |   FOUND[5] pool_mode = transaction
[10:44:08] TRACE | === END FACT COVERAGE ===
[10:44:08] TRACE | === FACT COVERAGE ANALYSIS ===
[10:44:08] TRACE | Query: What's the purpose of the connection pooler?
[10:44:08] TRACE | Total key facts: 5
[10:44:08] TRACE | Found facts: 2/5
[10:44:08] TRACE |   FOUND[1] PgBouncer
[10:44:08] TRACE |   FOUND[2] connection pooling
[10:44:08] TRACE | Missed facts: 3/5
[10:44:08] TRACE |   MISSED[1] max_db_connections = 100
[10:44:08] TRACE |     -> Found in chunk_id=deployment_guide_fix_3 (rank not in top-5)
[10:44:08] TRACE |   MISSED[2] default_pool_size = 25
[10:44:08] TRACE |     -> Found in chunk_id=deployment_guide_fix_3 (rank not in top-5)
[10:44:08] TRACE |   MISSED[3] pool_mode = transaction
[10:44:08] TRACE |     -> Found in chunk_id=deployment_guide_fix_3 (rank not in top-5)
[10:44:08] TRACE | === END FACT COVERAGE ===
[10:44:08] TRACE | === FACT COVERAGE ANALYSIS ===
[10:44:08] TRACE | Query: Should I connect directly to PostgreSQL or through PgBouncer?
[10:44:08] TRACE | Total key facts: 5
[10:44:08] TRACE | Found facts: 2/5
[10:44:08] TRACE |   FOUND[1] PgBouncer
[10:44:08] TRACE |   FOUND[2] connection pooling
[10:44:08] TRACE | Missed facts: 3/5
[10:44:08] TRACE |   MISSED[1] max_db_connections = 100
[10:44:08] TRACE |     -> Found in chunk_id=deployment_guide_fix_3 (rank not in top-5)
[10:44:08] TRACE |   MISSED[2] default_pool_size = 25
[10:44:08] TRACE |     -> Found in chunk_id=deployment_guide_fix_3 (rank not in top-5)
[10:44:08] TRACE |   MISSED[3] pool_mode = transaction
[10:44:08] TRACE |     -> Found in chunk_id=deployment_guide_fix_3 (rank not in top-5)
[10:44:08] TRACE | === END FACT COVERAGE ===
[10:44:08] TRACE | === FACT COVERAGE ANALYSIS ===
[10:44:08] TRACE | Query: pgbouncer purpose
[10:44:08] TRACE | Total key facts: 5
[10:44:08] TRACE | Found facts: 5/5
[10:44:08] TRACE |   FOUND[1] PgBouncer
[10:44:08] TRACE |   FOUND[2] connection pooling
[10:44:08] TRACE |   FOUND[3] max_db_connections = 100
[10:44:08] TRACE |   FOUND[4] default_pool_size = 25
[10:44:08] TRACE |   FOUND[5] pool_mode = transaction
[10:44:08] TRACE | === END FACT COVERAGE ===
[10:44:08] TRACE | === FACT COVERAGE ANALYSIS ===
[10:44:08] TRACE | Query: Optimizing database connections, what role does PgBouncer play?
[10:44:08] TRACE | Total key facts: 5
[10:44:08] TRACE | Found facts: 5/5
[10:44:08] TRACE |   FOUND[1] PgBouncer
[10:44:08] TRACE |   FOUND[2] connection pooling
[10:44:08] TRACE |   FOUND[3] max_db_connections = 100
[10:44:08] TRACE |   FOUND[4] default_pool_size = 25
[10:44:08] TRACE |   FOUND[5] pool_mode = transaction
[10:44:08] TRACE | === END FACT COVERAGE ===
[10:44:08] TRACE | === FACT COVERAGE ANALYSIS ===
[10:44:08] TRACE | Query: Why can't I connect directly to the database?
[10:44:08] TRACE | Total key facts: 5
[10:44:08] TRACE | Found facts: 2/5
[10:44:08] TRACE |   FOUND[1] PgBouncer
[10:44:08] TRACE |   FOUND[2] connection pooling
[10:44:08] TRACE | Missed facts: 3/5
[10:44:08] TRACE |   MISSED[1] max_db_connections = 100
[10:44:08] TRACE |     -> Found in chunk_id=deployment_guide_fix_3 (rank not in top-5)
[10:44:08] TRACE |   MISSED[2] default_pool_size = 25
[10:44:08] TRACE |     -> Found in chunk_id=deployment_guide_fix_3 (rank not in top-5)
[10:44:08] TRACE |   MISSED[3] pool_mode = transaction
[10:44:08] TRACE |     -> Found in chunk_id=deployment_guide_fix_3 (rank not in top-5)
[10:44:08] TRACE | === END FACT COVERAGE ===
[10:44:08] TRACE | === FACT COVERAGE ANALYSIS ===
[10:44:08] TRACE | Query: How do I implement retry logic for failed workflow steps?
[10:44:08] TRACE | Total key facts: 4
[10:44:08] TRACE | Found facts: 4/4
[10:44:08] TRACE |   FOUND[1] max_attempts: 3
[10:44:08] TRACE |   FOUND[2] backoff_type: exponential
[10:44:08] TRACE |   FOUND[3] initial_interval: 1000
[10:44:08] TRACE |   FOUND[4] max retries: 3
[10:44:08] TRACE | === END FACT COVERAGE ===
[10:44:08] TRACE | === FACT COVERAGE ANALYSIS ===
[10:44:08] TRACE | Query: What's the retry strategy for transient failures?
[10:44:08] TRACE | Total key facts: 4
[10:44:08] TRACE | Found facts: 4/4
[10:44:08] TRACE |   FOUND[1] max_attempts: 3
[10:44:08] TRACE |   FOUND[2] backoff_type: exponential
[10:44:08] TRACE |   FOUND[3] initial_interval: 1000
[10:44:08] TRACE |   FOUND[4] max retries: 3
[10:44:08] TRACE | === END FACT COVERAGE ===
[10:44:08] TRACE | === FACT COVERAGE ANALYSIS ===
[10:44:08] TRACE | Query: My workflow fails on temporary network errors
[10:44:08] TRACE | Total key facts: 4
[10:44:08] TRACE | Found facts: 0/4
[10:44:08] TRACE | Missed facts: 4/4
[10:44:08] TRACE |   MISSED[1] max_attempts: 3
[10:44:08] TRACE |     -> Found in chunk_id=user_guide_fix_5 (rank not in top-5)
[10:44:08] TRACE |   MISSED[2] backoff_type: exponential
[10:44:08] TRACE |     -> Found in chunk_id=user_guide_fix_7 (rank not in top-5)
[10:44:08] TRACE |   MISSED[3] initial_interval: 1000
[10:44:08] TRACE |     -> Found in chunk_id=user_guide_fix_5 (rank not in top-5)
[10:44:08] TRACE |   MISSED[4] max retries: 3
[10:44:08] TRACE |     -> Found in chunk_id=troubleshooting_guide_fix_5 (rank not in top-5)
[10:44:08] TRACE | === END FACT COVERAGE ===
[10:44:08] TRACE | === FACT COVERAGE ANALYSIS ===
[10:44:08] TRACE | Query: retry config
[10:44:08] TRACE | Total key facts: 4
[10:44:08] TRACE | Found facts: 3/4
[10:44:08] TRACE |   FOUND[1] max_attempts: 3
[10:44:08] TRACE |   FOUND[2] initial_interval: 1000
[10:44:08] TRACE |   FOUND[3] max retries: 3
[10:44:08] TRACE | Missed facts: 1/4
[10:44:08] TRACE |   MISSED[1] backoff_type: exponential
[10:44:08] TRACE |     -> Found in chunk_id=user_guide_fix_7 (rank not in top-5)
[10:44:08] TRACE | === END FACT COVERAGE ===
[10:44:08] TRACE | === FACT COVERAGE ANALYSIS ===
[10:44:08] TRACE | Query: I want workflows to automatically retry on errors, what are the options?
[10:44:08] TRACE | Total key facts: 4
[10:44:08] TRACE | Found facts: 3/4
[10:44:08] TRACE |   FOUND[1] max_attempts: 3
[10:44:08] TRACE |   FOUND[2] initial_interval: 1000
[10:44:08] TRACE |   FOUND[3] max retries: 3
[10:44:08] TRACE | Missed facts: 1/4
[10:44:08] TRACE |   MISSED[1] backoff_type: exponential
[10:44:08] TRACE |     -> Found in chunk_id=user_guide_fix_7 (rank not in top-5)
[10:44:08] TRACE | === END FACT COVERAGE ===
[10:44:08] TRACE | === FACT COVERAGE ANALYSIS ===
[10:44:08] TRACE | Query: Why doesn't my workflow retry after failing?
[10:44:08] TRACE | Total key facts: 4
[10:44:08] TRACE | Found facts: 3/4
[10:44:08] TRACE |   FOUND[1] max_attempts: 3
[10:44:08] TRACE |   FOUND[2] initial_interval: 1000
[10:44:08] TRACE |   FOUND[3] max retries: 3
[10:44:08] TRACE | Missed facts: 1/4
[10:44:08] TRACE |   MISSED[1] backoff_type: exponential
[10:44:08] TRACE |     -> Found in chunk_id=user_guide_fix_7 (rank not in top-5)
[10:44:08] TRACE | === END FACT COVERAGE ===
[10:44:08] TRACE | === FACT COVERAGE ANALYSIS ===
[10:44:08] TRACE | Query: What Helm chart repository should I use for deployment?
[10:44:08] TRACE | Total key facts: 1
[10:44:08] TRACE | Found facts: 1/1
[10:44:08] TRACE |   FOUND[1] helm repo add cloudflow https://charts.cloudflow.io
[10:44:08] TRACE | === END FACT COVERAGE ===
[10:44:08] TRACE | === FACT COVERAGE ANALYSIS ===
[10:44:08] TRACE | Query: Where is the CloudFlow Helm chart hosted?
[10:44:08] TRACE | Total key facts: 1
[10:44:08] TRACE | Found facts: 1/1
[10:44:08] TRACE |   FOUND[1] helm repo add cloudflow https://charts.cloudflow.io
[10:44:08] TRACE | === END FACT COVERAGE ===
[10:44:08] TRACE | === FACT COVERAGE ANALYSIS ===
[10:44:08] TRACE | Query: helm repo add is failing, what's the correct URL?
[10:44:08] TRACE | Total key facts: 1
[10:44:08] TRACE | Found facts: 1/1
[10:44:08] TRACE |   FOUND[1] helm repo add cloudflow https://charts.cloudflow.io
[10:44:08] TRACE | === END FACT COVERAGE ===
[10:44:08] TRACE | === FACT COVERAGE ANALYSIS ===
[10:44:08] TRACE | Query: helm repo
[10:44:08] TRACE | Total key facts: 1
[10:44:08] TRACE | Found facts: 1/1
[10:44:08] TRACE |   FOUND[1] helm repo add cloudflow https://charts.cloudflow.io
[10:44:08] TRACE | === END FACT COVERAGE ===
[10:44:08] TRACE | === FACT COVERAGE ANALYSIS ===
[10:44:08] TRACE | Query: Setting up deployment pipeline, which Helm repository has CloudFlow charts?
[10:44:08] TRACE | Total key facts: 1
[10:44:08] TRACE | Found facts: 1/1
[10:44:08] TRACE |   FOUND[1] helm repo add cloudflow https://charts.cloudflow.io
[10:44:08] TRACE | === END FACT COVERAGE ===
[10:44:08] TRACE | === FACT COVERAGE ANALYSIS ===
[10:44:08] TRACE | Query: Why can't I find CloudFlow in the official Helm hub?
[10:44:08] TRACE | Total key facts: 1
[10:44:08] TRACE | Found facts: 1/1
[10:44:08] TRACE |   FOUND[1] helm repo add cloudflow https://charts.cloudflow.io
[10:44:08] TRACE | === END FACT COVERAGE ===
[10:44:08] TRACE | === FACT COVERAGE ANALYSIS ===
[10:44:08] TRACE | Query: What is the minimum scheduling interval for workflows?
[10:44:08] TRACE | Total key facts: 2
[10:44:08] TRACE | Found facts: 1/2
[10:44:08] TRACE |   FOUND[1] The minimum scheduling interval is **1 minute**
[10:44:08] TRACE | Missed facts: 1/2
[10:44:08] TRACE |   MISSED[1] minimum scheduling interval is 1 minute
[10:44:08] TRACE | === END FACT COVERAGE ===
[10:44:08] TRACE | === FACT COVERAGE ANALYSIS ===
[10:44:08] TRACE | Query: How frequently can I schedule a workflow?
[10:44:08] TRACE | Total key facts: 2
[10:44:08] TRACE | Found facts: 1/2
[10:44:08] TRACE |   FOUND[1] The minimum scheduling interval is **1 minute**
[10:44:08] TRACE | Missed facts: 1/2
[10:44:08] TRACE |   MISSED[1] minimum scheduling interval is 1 minute
[10:44:08] TRACE | === END FACT COVERAGE ===
[10:44:08] TRACE | === FACT COVERAGE ANALYSIS ===
[10:44:08] TRACE | Query: My every-30-seconds schedule is being rejected
[10:44:08] TRACE | Total key facts: 2
[10:44:08] TRACE | Found facts: 1/2
[10:44:08] TRACE |   FOUND[1] The minimum scheduling interval is **1 minute**
[10:44:08] TRACE | Missed facts: 1/2
[10:44:08] TRACE |   MISSED[1] minimum scheduling interval is 1 minute
[10:44:08] TRACE | === END FACT COVERAGE ===
[10:44:08] TRACE | === FACT COVERAGE ANALYSIS ===
[10:44:08] TRACE | Query: min schedule interval
[10:44:08] TRACE | Total key facts: 2
[10:44:08] TRACE | Found facts: 1/2
[10:44:08] TRACE |   FOUND[1] The minimum scheduling interval is **1 minute**
[10:44:08] TRACE | Missed facts: 1/2
[10:44:08] TRACE |   MISSED[1] minimum scheduling interval is 1 minute
[10:44:08] TRACE | === END FACT COVERAGE ===
[10:44:08] TRACE | === FACT COVERAGE ANALYSIS ===
[10:44:08] TRACE | Query: I need near real-time execution, what's the fastest schedule I can set?
[10:44:08] TRACE | Total key facts: 2
[10:44:08] TRACE | Found facts: 1/2
[10:44:08] TRACE |   FOUND[1] The minimum scheduling interval is **1 minute**
[10:44:08] TRACE | Missed facts: 1/2
[10:44:08] TRACE |   MISSED[1] minimum scheduling interval is 1 minute
[10:44:08] TRACE | === END FACT COVERAGE ===
[10:44:08] TRACE | === FACT COVERAGE ANALYSIS ===
[10:44:08] TRACE | Query: Why can't I schedule workflows every 30 seconds?
[10:44:08] TRACE | Total key facts: 2
[10:44:08] TRACE | Found facts: 1/2
[10:44:08] TRACE |   FOUND[1] The minimum scheduling interval is **1 minute**
[10:44:08] TRACE | Missed facts: 1/2
[10:44:08] TRACE |   MISSED[1] minimum scheduling interval is 1 minute
[10:44:08] TRACE | === END FACT COVERAGE ===
[10:44:08] INFO  |   k=5 exact_match: 75.5% (40/53)
[10:44:08] INFO  |     synonym: 69.8%
[10:44:08] INFO  |     problem: 54.7%
[10:44:08] INFO  |     casual: 64.2%
[10:44:08] INFO  |     contextual: 60.4%
[10:44:08] INFO  |     negation: 52.8%

================================================================================
[10:44:08] INFO  | RETRIEVAL STRATEGY: enriched_hybrid_fast
================================================================================
[10:44:08] INFO  | Loading embedder: BAAI/bge-base-en-v1.5
[10:44:11] PROG  | [  3/  3] ████████████████████ 100.0% | enriched_hybrid_fast_bge-base-en-v1.5_fixed_512_0pct
[10:44:11] DEBUG | Chunking document 'api_reference': 1792 words, 16663 chars
[10:44:11] DEBUG |   Chunk 0: chars [0-3160] words=376 tokens≈500 preview='# CloudFlow API Reference Version 2.1.0 | Last Updated: January 2026 ## Overview The CloudFlow API i...'
[10:44:11] DEBUG |   Chunk 1: chars [3161-6111] words=379 tokens≈504 preview='## Rate Limiting To ensure fair usage and system stability, CloudFlow enforces rate limits on all AP...'
[10:44:11] DEBUG |   Chunk 2: chars [6112-8827] words=296 tokens≈393 preview='**Endpoint:** `POST /workflows` **Request Body:** ```json { "name": "Email Campaign Automation", "de...'
[10:44:11] DEBUG |   Chunk 3: chars [8828-11402] words=269 tokens≈357 preview='**Endpoint:** `GET /pipelines/{pipeline_id}/executions` **Query Parameters:** - `status` (optional):...'
[10:44:11] DEBUG |   Chunk 4: chars [11403-14151] words=365 tokens≈485 preview='### Error Response Format ```json { "error": { "code": "invalid_parameter", "message": "The 'limit' ...'
[10:44:11] DEBUG |   Chunk 5: chars [14152-15213] words=107 tokens≈142 preview='**Supported Events:** - `workflow.started` - `workflow.completed` - `workflow.failed` - `pipeline.co...'
[10:44:11] DEBUG | Created 6 chunks for document 'api_reference'
[10:44:11] DEBUG | Chunking document 'architecture_overview': 4607 words, 36722 chars
[10:44:11] DEBUG |   Chunk 0: chars [0-3230] words=370 tokens≈492 preview='# CloudFlow Platform - System Architecture Overview **Document Version:** 2.3.1 **Last Updated:** Ja...'
[10:44:11] DEBUG |   Chunk 1: chars [3231-5982] words=380 tokens≈505 preview='**Technology**: Node.js with Express.js framework **Replicas**: 12 pods (production), auto-scaling 8...'
[10:44:11] DEBUG |   Chunk 2: chars [5983-7875] words=248 tokens≈329 preview='**Technology**: Node.js with TypeScript, Bull queue library **Replicas**: 16 pods (production), auto...'
[10:44:11] DEBUG |   Chunk 3: chars [7876-9478] words=219 tokens≈291 preview='**Technology**: Go with distributed locking via Redis **Replicas**: 4 pods (production), active-pass...'
[10:44:11] DEBUG |   Chunk 4: chars [9479-11878] words=354 tokens≈470 preview='**Technology**: Node.js with worker pool pattern **Replicas**: 8 pods (production), auto-scaling 6-1...'
[10:44:11] DEBUG |   Chunk 5: chars [11879-14634] words=368 tokens≈489 preview='Success/failure events published back to Kafka ``` **Event Schema**: ```json { "event_id": "uuid-v4"...'
[10:44:11] DEBUG |   Chunk 6: chars [14635-17394] words=384 tokens≈510 preview='**Session Storage**: - Key pattern: `session:{user_id}` - TTL: 15 minutes (aligned with JWT expiry) ...'
[10:44:11] DEBUG |   Chunk 7: chars [17395-20141] words=381 tokens≈506 preview='records: 500 - Session timeout: 30 seconds --- ## Message Queue Patterns ### Pub/Sub Pattern Used fo...'
[10:44:11] DEBUG |   Chunk 8: chars [20142-22843] words=384 tokens≈510 preview='### Cache Invalidation Strategies **Time-Based Expiration (TTL)**: - Short-lived data: 5-15 minutes ...'
[10:44:11] DEBUG |   Chunk 9: chars [22844-25362] words=384 tokens≈510 preview='per API key (default: 1000 RPM) ### Secrets Management **HashiCorp Vault Integration**: - Dynamic da...'
[10:44:11] DEBUG |   Chunk 10: chars [25363-27842] words=384 tokens≈510 preview='- Execution start rate: 500 per second - Completion rate: 450 per second (average 2-second execution...'
[10:44:11] DEBUG |   Chunk 11: chars [27843-30286] words=375 tokens≈498 preview='Service (2) - PostgreSQL Primary - PostgreSQL Replica - PostgreSQL Replica - Redis Primary (2) - Red...'
[10:44:11] DEBUG |   Chunk 12: chars [30287-32834] words=376 tokens≈500 preview='Resume normal operations - Recovery time: 1-3 hours depending on data volume - Data loss: None if co...'
[10:44:11] DEBUG | Created 13 chunks for document 'architecture_overview'
[10:44:11] DEBUG | Chunking document 'deployment_guide': 2777 words, 25837 chars
[10:44:11] DEBUG |   Chunk 0: chars [0-3165] words=384 tokens≈510 preview='# CloudFlow Platform - Deployment and Operations Guide **Version:** 2.4.0 **Last Updated:** January ...'
[10:44:11] DEBUG |   Chunk 1: chars [3166-6560] words=384 tokens≈510 preview='create cluster -f cluster-config.yaml ``` ### Storage Configuration Install the EBS CSI driver for p...'
[10:44:11] DEBUG |   Chunk 2: chars [6561-8755] words=251 tokens≈333 preview='- secretName: cloudflow-tls hosts: - api.cloudflow.io healthCheck: enabled: true livenessProbe: path...'
[10:44:11] DEBUG |   Chunk 3: chars [8756-12138] words=384 tokens≈510 preview='Deploy PostgreSQL using the Bitnami Helm chart: ```yaml # postgres-values.yaml global: postgresql: a...'
[10:44:11] DEBUG |   Chunk 4: chars [12139-14139] words=204 tokens≈271 preview='--set prometheus.prometheusSpec.retention=30d \ --set prometheus.prometheusSpec.storageSpec.volumeCl...'
[10:44:11] DEBUG |   Chunk 5: chars [14140-17194] words=372 tokens≈494 preview='Configure log aggregation: ```bash # Install Fluent Bit for log forwarding helm install fluent-bit f...'
[10:44:11] DEBUG |   Chunk 6: chars [17195-20069] words=384 tokens≈510 preview='**Verify Service Health**: ```bash kubectl get pods -n cloudflow-prod curl https://api.cloudflow.io/...'
[10:44:11] DEBUG |   Chunk 7: chars [20070-23221] words=384 tokens≈510 preview='podSelector: matchLabels: app: cloudflow policyTypes: - Ingress - Egress ingress: - from: - namespac...'
[10:44:11] DEBUG |   Chunk 8: chars [23222-23448] words=30 tokens≈39 preview='UTC - **Secondary**: Wednesday 10:00-12:00 UTC All deployments should target these windows to minimi...'
[10:44:11] DEBUG | Created 9 chunks for document 'deployment_guide'
[10:44:11] DEBUG | Chunking document 'troubleshooting_guide': 4032 words, 32183 chars
[10:44:11] DEBUG |   Chunk 0: chars [0-2666] words=320 tokens≈425 preview='# CloudFlow Platform Troubleshooting Guide **Version:** 3.2.1 **Last Updated:** January 2026 **Audie...'
[10:44:11] DEBUG |   Chunk 1: chars [2667-5007] words=321 tokens≈426 preview='**Diagnosis:** ```bash # Check system time timedatectl status # Compare with NTP server ntpdate -q p...'
[10:44:11] DEBUG |   Chunk 2: chars [5008-8037] words=376 tokens≈500 preview='Review Query Execution Plans** ```sql -- Connect to CloudFlow database cloudflow db connect --readon...'
[10:44:11] DEBUG |   Chunk 3: chars [8038-10862] words=356 tokens≈473 preview='Workflow Context Accumulation** Large workflow executions may accumulate state in memory. **Solution...'
[10:44:11] DEBUG |   Chunk 4: chars [10863-13827] words=384 tokens≈510 preview='**Implement connection pooling optimization:** ```bash # Use PgBouncer for connection pooling kubect...'
[10:44:11] DEBUG |   Chunk 5: chars [13828-16401] words=334 tokens≈444 preview='exec_7h3j6k9m2n --show-bottlenecks ``` #### Solutions **1. Increase workflow timeout (if justified):...'
[10:44:11] DEBUG |   Chunk 6: chars [16402-19022] words=329 tokens≈437 preview='Data Validation Errors** ``` ValidationError: Field 'customer_id' is required but missing in 234 rec...'
[10:44:11] DEBUG |   Chunk 7: chars [19023-20595] words=194 tokens≈258 preview='Waiting {retry_after} seconds...") time.sleep(retry_after) continue remaining = int(response.headers...'
[10:44:11] DEBUG |   Chunk 8: chars [20596-23454] words=384 tokens≈510 preview='Leverage caching:** ```bash # Enable client-side caching export CLOUDFLOW_CACHE_ENABLED=true export ...'
[10:44:11] DEBUG |   Chunk 9: chars [23455-26239] words=384 tokens≈510 preview='in" # Find workflow failures by ID kubectl logs -n cloudflow deployment/cloudflow-workflow-engine | ...'
[10:44:11] DEBUG |   Chunk 10: chars [26240-29039] words=384 tokens≈510 preview='database nc -zv cloudflow-db.internal.company.com 5432 # Trace route traceroute api.cloudflow.io # C...'
[10:44:11] DEBUG |   Chunk 11: chars [29040-31206] words=266 tokens≈353 preview='outage" # Create war room cloudflow incident war-room create incident-2024012401 ``` **For SEV-2 (Hi...'
[10:44:11] DEBUG | Created 12 chunks for document 'troubleshooting_guide'
[10:44:11] DEBUG | Chunking document 'user_guide': 3735 words, 31582 chars
[10:44:11] DEBUG |   Chunk 0: chars [0-2617] words=375 tokens≈498 preview='# CloudFlow User Guide Welcome to CloudFlow, the modern workflow automation platform that helps you ...'
[10:44:11] DEBUG |   Chunk 1: chars [2618-5224] words=348 tokens≈462 preview='Open the Visual Editor by clicking **"Create Workflow"** or editing an existing workflow 2. Add a tr...'
[10:44:11] DEBUG |   Chunk 2: chars [5225-8372] words=381 tokens≈506 preview='### HTTP Requests Make HTTP requests to any API endpoint: **Configuration:** - **Method**: GET, POST...'
[10:44:11] DEBUG |   Chunk 3: chars [8373-11268] words=336 tokens≈446 preview='``` **Slack App Integration:** - Install the CloudFlow Slack app in your workspace - Authenticate on...'
[10:44:11] DEBUG |   Chunk 4: chars [11269-13371] words=375 tokens≈498 preview='### Cron Syntax Use standard cron expressions to define schedules: ``` * * * * * ┬ ┬ ┬ ┬ ┬ │ │ │ │ │...'
[10:44:11] DEBUG |   Chunk 5: chars [13372-16105] words=383 tokens≈509 preview='Save and activate **Testing Schedules:** Use the built-in schedule calculator to preview upcoming ex...'
[10:44:11] DEBUG |   Chunk 6: chars [16106-18686] words=363 tokens≈482 preview='Filter by error type, date range, or execution ID 3. Click an execution to view full details 4. Clic...'
[10:44:11] DEBUG |   Chunk 7: chars [18687-21448] words=364 tokens≈484 preview='Handle Errors Gracefully Always implement error handling for external API calls and database operati...'
[10:44:11] DEBUG |   Chunk 8: chars [21449-24677] words=384 tokens≈510 preview='Version Control For critical workflows: - Export YAML definitions regularly - Store in version contr...'
[10:44:11] DEBUG |   Chunk 9: chars [24678-27975] words=371 tokens≈493 preview='Requester: {{trigger.body.requester}} Description: {{trigger.body.description}} Approve: https://app...'
[10:44:11] DEBUG |   Chunk 10: chars [27976-28481] words=55 tokens≈73 preview='- **Documentation**: https://docs.cloudflow.io - **Community Forum**: https://community.cloudflow.io...'
[10:44:11] DEBUG | Created 11 chunks for document 'user_guide'
[10:44:11] INFO  |   Created 51 chunks
[10:44:11] TRACE | [fast-enricher] INPUT [3160 chars]: # CloudFlow API Reference Version 2.1.0 | Last Updated: January 2026 ## Overview The CloudFlow API is a RESTful service that enables developers to programmatically manage cloud workflows, data pipelines, and automation tasks. This documentation provides comprehensive details on authentication, endpoints, request/response formats, error handling, and best practices. **Base URL:** `https://api.cloudflow.io/v2` **API Status:** https://status.cloudflow.io ## Authentication CloudFlow supports three a...
[10:44:11] DEBUG | [fast-enricher] YAKE extracted 10 keywords: ['Reference Version', 'API Reference', 'API', 'API Keys', 'automation tasks']...
[10:44:11] TRACE | [fast-enricher] YAKE keywords with scores: ['Reference Version(0.006)', 'API Reference(0.021)', 'API(0.037)', 'API Keys(0.046)', 'automation tasks(0.052)', 'January(0.060)', 'enables developers(0.063)', 'Version(0.070)', 'Updated(0.070)', 'Overview(0.070)']
[10:44:11] TRACE | [fast-enricher] YAKE extraction took 78.0ms
[10:44:11] DEBUG | [fast-enricher] spaCy extracted 11 entities: {'ORG': ['API', 'CloudFlow', 'PKCE', 'JWT'], 'PERSON': ['API Keys API', 'JWT Tokens', 'JSON', 'JWTs', 'Example JWT'], 'LAW': ['the Authorization Code'], 'PRODUCT': ['RS256']}
[10:44:11] TRACE | [fast-enricher] spaCy ORG: ['API', 'CloudFlow', 'PKCE', 'JWT']
[10:44:11] TRACE | [fast-enricher] spaCy PERSON: ['API Keys API', 'JWT Tokens', 'JSON', 'JWTs', 'Example JWT']
[10:44:11] TRACE | [fast-enricher] spaCy LAW: ['the Authorization Code']
[10:44:11] TRACE | [fast-enricher] spaCy PRODUCT: ['RS256']
[10:44:11] TRACE | [fast-enricher] spaCy NER took 523.1ms
[10:44:11] TRACE | [fast-enricher] Generated prefix: Reference Version, API Reference, API, API Keys, automation tasks, January, enables developers | API, CloudFlow, API Keys API, JWT Tokens, the Authorization Code
[10:44:11] DEBUG | [fast-enricher] Enriched in 601.4ms | keywords=10 entities=11 | prefix_len=161
[10:44:11] TRACE | [enriched-hybrid] CACHE MISS: enriched chunk (total enrichments: 1)
[10:44:11] TRACE | [fast-enricher] INPUT [2950 chars]: ## Rate Limiting To ensure fair usage and system stability, CloudFlow enforces rate limits on all API endpoints. **Default Limits:** - 100 requests per minute per authenticated user - 20 requests per minute for unauthenticated requests - Burst allowance: 150 requests in a 10-second window ### Rate Limit Headers Every API response includes rate limit information: ``` X-RateLimit-Limit: 100 X-RateLimit-Remaining: 87 X-RateLimit-Reset: 1640995200 ``` When you exceed the rate limit, you'll receive a...
[10:44:11] DEBUG | [fast-enricher] YAKE extracted 10 keywords: ['Rate Limiting', 'Rate Limit', 'Rate', 'system stability', 'CloudFlow enforces']...
[10:44:11] TRACE | [fast-enricher] YAKE keywords with scores: ['Rate Limiting(0.018)', 'Rate Limit(0.019)', 'Rate(0.044)', 'system stability(0.047)', 'CloudFlow enforces(0.047)', 'enforces rate(0.054)', 'API(0.058)', 'ensure fair(0.059)', 'fair usage(0.059)', 'API endpoints(0.072)']
[10:44:11] TRACE | [fast-enricher] YAKE extraction took 8.9ms
[10:44:11] DEBUG | [fast-enricher] spaCy extracted 4 entities: {'ORG': ['CloudFlow', 'API'], 'PERSON': ['max'], 'LAW': ['ISO 8601']}
[10:44:11] TRACE | [fast-enricher] spaCy ORG: ['CloudFlow', 'API']
[10:44:11] TRACE | [fast-enricher] spaCy PERSON: ['max']
[10:44:11] TRACE | [fast-enricher] spaCy LAW: ['ISO 8601']
[10:44:11] TRACE | [fast-enricher] spaCy NER took 42.8ms
[10:44:11] TRACE | [fast-enricher] Generated prefix: Rate Limiting, Rate Limit, Rate, system stability, CloudFlow enforces, enforces rate, API | CloudFlow, API, max, ISO 8601
[10:44:11] DEBUG | [fast-enricher] Enriched in 52.0ms | keywords=10 entities=4 | prefix_len=121
[10:44:11] TRACE | [enriched-hybrid] CACHE MISS: enriched chunk (total enrichments: 2)
[10:44:11] TRACE | [fast-enricher] INPUT [2715 chars]: **Endpoint:** `POST /workflows` **Request Body:** ```json { "name": "Email Campaign Automation", "description": "Sends personalized emails based on user behavior", "trigger": { "type": "webhook", "url": "https://your-app.com/webhook" }, "steps": [ { "type": "fetch_data", "source": "users_table", "filters": {"active": true} }, { "type": "transform", "script": "data.map(user => ({ ...user, segment: calculateSegment(user) }))" }, { "type": "send_email", "template_id": "tpl_welcome_v2" } ] } ``` **E...
[10:44:11] DEBUG | [fast-enricher] YAKE extracted 10 keywords: ['Request Body', 'Details Retrieve', 'Workflow Details', 'Endpoint', 'Response']...
[10:44:11] TRACE | [fast-enricher] YAKE keywords with scores: ['Request Body(0.012)', 'Details Retrieve(0.018)', 'Workflow Details(0.021)', 'Endpoint(0.024)', 'Response(0.028)', 'Request(0.036)', 'detailed information(0.045)', 'Workflow(0.046)', 'Retrieve detailed(0.052)', 'Created(0.057)']
[10:44:11] TRACE | [fast-enricher] YAKE extraction took 5.4ms
[10:44:11] DEBUG | [fast-enricher] spaCy extracted 2 entities: {'PERSON': ['Workflow Modify'], 'ORG': ['Delete Workflow']}
[10:44:11] TRACE | [fast-enricher] spaCy PERSON: ['Workflow Modify']
[10:44:11] TRACE | [fast-enricher] spaCy ORG: ['Delete Workflow']
[10:44:11] TRACE | [fast-enricher] spaCy NER took 20.6ms
[10:44:11] TRACE | [fast-enricher] Generated prefix: Request Body, Details Retrieve, Workflow Details, Endpoint, Response, Request, detailed information | Workflow Modify, Delete Workflow
[10:44:11] DEBUG | [fast-enricher] Enriched in 26.2ms | keywords=10 entities=2 | prefix_len=134
[10:44:11] TRACE | [enriched-hybrid] CACHE MISS: enriched chunk (total enrichments: 3)
[10:44:11] TRACE | [fast-enricher] INPUT [2574 chars]: **Endpoint:** `GET /pipelines/{pipeline_id}/executions` **Query Parameters:** - `status` (optional): Filter by status (`running`, `completed`, `failed`) - `start_date` (optional): ISO 8601 timestamp - `end_date` (optional): ISO 8601 timestamp - `limit` (optional): Items per page (max 100) - `offset` (optional): Pagination offset **Example Request:** ```python import requests api_key = "YOUR_API_KEY" pipeline_id = "pipe_4x9k2m" url = f"https://api.cloudflow.io/v2/pipelines/{pipeline_id}/execution...
[10:44:11] DEBUG | [fast-enricher] YAKE extracted 10 keywords: ['Query Parameters', 'Pagination offset', 'optional', 'Retrieve performance', 'Metrics Retrieve']...
[10:44:11] TRACE | [fast-enricher] YAKE keywords with scores: ['Query Parameters(0.004)', 'Pagination offset(0.007)', 'optional(0.010)', 'Retrieve performance(0.011)', 'Metrics Retrieve(0.015)', 'ISO(0.018)', 'Workflow Metrics(0.022)', 'Total number(0.023)', 'timestamp(0.033)', 'performance metrics(0.034)']
[10:44:11] TRACE | [fast-enricher] YAKE extraction took 7.5ms
[10:44:11] DEBUG | [fast-enricher] spaCy extracted 2 entities: {'LAW': ['ISO 8601'], 'GPE': ['JSON']}
[10:44:11] TRACE | [fast-enricher] spaCy LAW: ['ISO 8601']
[10:44:11] TRACE | [fast-enricher] spaCy GPE: ['JSON']
[10:44:11] TRACE | [fast-enricher] spaCy NER took 26.3ms
[10:44:11] TRACE | [fast-enricher] Generated prefix: Query Parameters, Pagination offset, optional, Retrieve performance, Metrics Retrieve, ISO, Workflow Metrics | ISO 8601, JSON
[10:44:11] DEBUG | [fast-enricher] Enriched in 34.1ms | keywords=10 entities=2 | prefix_len=125
[10:44:11] TRACE | [enriched-hybrid] CACHE MISS: enriched chunk (total enrichments: 4)
[10:44:11] TRACE | [fast-enricher] INPUT [2748 chars]: ### Error Response Format ```json { "error": { "code": "invalid_parameter", "message": "The 'limit' parameter must be between 1 and 100", "field": "limit", "request_id": "req_8k3m9x2p" } } ``` ### HTTP Status Codes | Status Code | Description | Common Causes | |------------|-------------|---------------| | 400 | Bad Request | Invalid parameters, malformed JSON, validation errors | | 401 | Unauthorized | Missing or invalid authentication credentials | | 403 | Forbidden | Insufficient permissions ...
[10:44:11] DEBUG | [fast-enricher] YAKE extracted 10 keywords: ['malformed JSON', 'requested resource', 'Bad Gateway', 'Error Codes', 'HTTP Status']...
[10:44:11] TRACE | [fast-enricher] YAKE keywords with scores: ['malformed JSON(0.002)', 'requested resource(0.002)', 'Bad Gateway(0.002)', 'Error Codes(0.002)', 'HTTP Status(0.002)', 'Response Format(0.002)', 'Server Error(0.002)', 'Status Codes(0.003)', 'Internal Server(0.003)', 'Unexpected server(0.003)']
[10:44:11] TRACE | [fast-enricher] YAKE extraction took 10.7ms
[10:44:11] DEBUG | [fast-enricher] spaCy extracted 11 entities: {'ORG': ['Status Codes', 'Invalid', 'Insufficient', 'Resource', 'Temporary'], 'PERSON': ['Description', 'Common Causes', 'JSON', 'Rate', 'Invalid API'], 'GPE': ['Forbidden']}
[10:44:11] TRACE | [fast-enricher] spaCy ORG: ['Status Codes', 'Invalid', 'Insufficient', 'Resource', 'Temporary']
[10:44:11] TRACE | [fast-enricher] spaCy PERSON: ['Description', 'Common Causes', 'JSON', 'Rate', 'Invalid API']
[10:44:11] TRACE | [fast-enricher] spaCy GPE: ['Forbidden']
[10:44:11] TRACE | [fast-enricher] spaCy NER took 34.8ms
[10:44:11] TRACE | [fast-enricher] Generated prefix: malformed JSON, requested resource, Bad Gateway, Error Codes, HTTP Status, Response Format, Server Error | Status Codes, Invalid, Description, Common Causes, Forbidden
[10:44:11] DEBUG | [fast-enricher] Enriched in 45.7ms | keywords=10 entities=11 | prefix_len=167
[10:44:11] TRACE | [enriched-hybrid] CACHE MISS: enriched chunk (total enrichments: 5)
[10:44:11] TRACE | [fast-enricher] INPUT [1061 chars]: **Supported Events:** - `workflow.started` - `workflow.completed` - `workflow.failed` - `pipeline.completed` - `pipeline.failed` **Webhook Payload Example:** ```json { "event": "workflow.completed", "timestamp": "2026-01-24T10:45:00Z", "data": { "workflow_id": "wf_8x7k2m9p", "execution_id": "exec_9k3m7n2q", "status": "completed", "duration_ms": 3420, "records_processed": 1247 } } ``` Configure webhooks in your account settings or via the API: ```bash curl -X POST https://api.cloudflow.io/v2/webh...
[10:44:11] DEBUG | [fast-enricher] YAKE extracted 10 keywords: ['Supported Events', 'Community Forum', 'Slack channels', 'Configure webhooks', 'Enterprise customers']...
[10:44:11] TRACE | [fast-enricher] YAKE keywords with scores: ['Supported Events(0.004)', 'Community Forum(0.004)', 'Slack channels(0.010)', 'Configure webhooks(0.010)', 'Enterprise customers(0.011)', 'Webhook Payload(0.015)', 'dedicated Slack(0.017)', 'priority support(0.018)', 'account settings(0.033)', 'Documentation(0.033)']
[10:44:11] TRACE | [fast-enricher] YAKE extraction took 7.3ms
[10:44:11] DEBUG | [fast-enricher] spaCy extracted 2 entities: {'ORG': ['API', 'Slack']}
[10:44:11] TRACE | [fast-enricher] spaCy ORG: ['API', 'Slack']
[10:44:11] TRACE | [fast-enricher] spaCy NER took 14.5ms
[10:44:11] TRACE | [fast-enricher] Generated prefix: Supported Events, Community Forum, Slack channels, Configure webhooks, Enterprise customers, Webhook Payload, dedicated Slack | API, Slack
[10:44:11] DEBUG | [fast-enricher] Enriched in 21.9ms | keywords=10 entities=2 | prefix_len=138
[10:44:11] TRACE | [enriched-hybrid] CACHE MISS: enriched chunk (total enrichments: 6)
[10:44:11] TRACE | [fast-enricher] INPUT [3230 chars]: # CloudFlow Platform - System Architecture Overview **Document Version:** 2.3.1 **Last Updated:** January 15, 2026 **Owner:** Platform Architecture Team **Status:** Production ## Executive Summary CloudFlow is a distributed, cloud-native workflow automation platform designed to orchestrate complex business processes at scale. The platform processes over 2.5 million workflow executions daily with an average P99 latency of 180ms for API operations and 4.2 seconds for workflow execution. This docum...
[10:44:11] DEBUG | [fast-enricher] YAKE extracted 10 keywords: ['Executive Summary', 'Architecture Team', 'Document Version', 'Architecture', 'System Architecture']...
[10:44:11] TRACE | [fast-enricher] YAKE keywords with scores: ['Executive Summary(0.004)', 'Architecture Team(0.010)', 'Document Version(0.012)', 'Architecture(0.014)', 'System Architecture(0.015)', 'Platform Architecture(0.019)', 'cloud-native workflow(0.021)', 'workflow automation(0.022)', 'automation platform(0.022)', 'API Gateway(0.023)']
[10:44:11] TRACE | [fast-enricher] YAKE extraction took 18.0ms
[10:44:12] DEBUG | [fast-enricher] spaCy extracted 14 entities: {'ORG': ['Platform - System Architecture Overview **Document Version:**', 'P99', 'API', 'Data Flow Architecture](#data-flow', 'AWS'], 'LAW': ['Contents 1'], 'GPE': ['mTLS'], 'PERSON': ['Load Balancer', 'API Gateway Layer', '│', '└─┬───────', 'Workflow'], 'PRODUCT': ['Termination - 443', '┘ ```']}
[10:44:12] TRACE | [fast-enricher] spaCy ORG: ['Platform - System Architecture Overview **Document Version:**', 'P99', 'API', 'Data Flow Architecture](#data-flow', 'AWS']
[10:44:12] TRACE | [fast-enricher] spaCy LAW: ['Contents 1']
[10:44:12] TRACE | [fast-enricher] spaCy GPE: ['mTLS']
[10:44:12] TRACE | [fast-enricher] spaCy PERSON: ['Load Balancer', 'API Gateway Layer', '│', '└─┬───────', 'Workflow']
[10:44:12] TRACE | [fast-enricher] spaCy PRODUCT: ['Termination - 443', '┘ ```']
[10:44:12] TRACE | [fast-enricher] spaCy NER took 125.5ms
[10:44:12] TRACE | [fast-enricher] Generated prefix: Executive Summary, Architecture Team, Document Version, Architecture, System Architecture, Platform Architecture, cloud-native workflow | Platform - System Architecture Overview **Document Version:**, P99, Contents 1, mTLS, Load Balancer
[10:44:12] DEBUG | [fast-enricher] Enriched in 143.8ms | keywords=10 entities=14 | prefix_len=237
[10:44:12] TRACE | [enriched-hybrid] CACHE MISS: enriched chunk (total enrichments: 7)
[10:44:12] TRACE | [fast-enricher] INPUT [2751 chars]: **Technology**: Node.js with Express.js framework **Replicas**: 12 pods (production), auto-scaling 8-20 based on CPU **Resource Allocation**: 2 vCPU, 4GB RAM per pod **Key Responsibilities**: - JWT token validation (delegated to Auth Service for initial validation) - Rate limiting: 1000 requests per minute per API key (sliding window) - Request/response transformation and validation using JSON Schema - Routing to downstream services based on URL path patterns - CORS handling for web clients - Re...
[10:44:12] DEBUG | [fast-enricher] YAKE extracted 10 keywords: ['Critical Endpoints', 'Auth Service', 'JSON Schema', 'Resource Allocation', 'Performance Targets']...
[10:44:12] TRACE | [fast-enricher] YAKE keywords with scores: ['Critical Endpoints(0.003)', 'Auth Service(0.003)', 'JSON Schema(0.003)', 'Resource Allocation(0.003)', 'Performance Targets(0.003)', 'Rate limiting(0.004)', 'Key Responsibilities(0.005)', 'JWT token(0.006)', 'token generation(0.006)', 'CORS handling(0.007)']
[10:44:12] TRACE | [fast-enricher] YAKE extraction took 15.6ms
[10:44:12] DEBUG | [fast-enricher] spaCy extracted 10 entities: {'ORG': ['Express.js', 'CPU', 'Auth Service', 'API', 'POST'], 'PERSON': ['JSON Schema - Routing', 'Generate'], 'PRODUCT': ['RS256'], 'GPE': ['Redis', 'PostgreSQL']}
[10:44:12] TRACE | [fast-enricher] spaCy ORG: ['Express.js', 'CPU', 'Auth Service', 'API', 'POST']
[10:44:12] TRACE | [fast-enricher] spaCy PERSON: ['JSON Schema - Routing', 'Generate']
[10:44:12] TRACE | [fast-enricher] spaCy PRODUCT: ['RS256']
[10:44:12] TRACE | [fast-enricher] spaCy GPE: ['Redis', 'PostgreSQL']
[10:44:12] TRACE | [fast-enricher] spaCy NER took 72.0ms
[10:44:12] TRACE | [fast-enricher] Generated prefix: Critical Endpoints, Auth Service, JSON Schema, Resource Allocation, Performance Targets, Rate limiting, Key Responsibilities | Express.js, CPU, JSON Schema - Routing, Generate, RS256
[10:44:12] DEBUG | [fast-enricher] Enriched in 87.9ms | keywords=10 entities=10 | prefix_len=182
[10:44:12] TRACE | [enriched-hybrid] CACHE MISS: enriched chunk (total enrichments: 8)
[10:44:12] TRACE | [fast-enricher] INPUT [1892 chars]: **Technology**: Node.js with TypeScript, Bull queue library **Replicas**: 16 pods (production), auto-scaling 12-24 **Resource Allocation**: 4 vCPU, 8GB RAM per pod **Key Responsibilities**: - Parse and validate workflow definitions (JSON-based DSL) - Execute workflow steps with state machine pattern - Handle retries with exponential backoff (max 3 retries, backoff: 2^n seconds) - Coordinate parallel and sequential task execution - Manage workflow state persistence and recovery - Support for cond...
[10:44:12] DEBUG | [fast-enricher] YAKE extracted 10 keywords: ['Resource Allocation', 'Key Responsibilities', 'Performance Targets', 'Scheduler Service', 'State Management']...
[10:44:12] TRACE | [fast-enricher] YAKE keywords with scores: ['Resource Allocation(0.002)', 'Key Responsibilities(0.002)', 'Performance Targets(0.002)', 'Scheduler Service(0.002)', 'State Management(0.003)', 'Execution Model(0.003)', 'JSON-based DSL(0.004)', 'Execute workflow(0.004)', 'Manage workflow(0.004)', 'Simple workflow(0.004)']
[10:44:12] TRACE | [fast-enricher] YAKE extraction took 10.6ms
[10:44:12] DEBUG | [fast-enricher] spaCy extracted 6 entities: {'ORG': ['TypeScript', 'Bull', 'DSL', 'RUNNING', 'COMPLETED'], 'GPE': ['PostgreSQL']}
[10:44:12] TRACE | [fast-enricher] spaCy ORG: ['TypeScript', 'Bull', 'DSL', 'RUNNING', 'COMPLETED']
[10:44:12] TRACE | [fast-enricher] spaCy GPE: ['PostgreSQL']
[10:44:12] TRACE | [fast-enricher] spaCy NER took 34.5ms
[10:44:12] TRACE | [fast-enricher] Generated prefix: Resource Allocation, Key Responsibilities, Performance Targets, Scheduler Service, State Management, Execution Model, JSON-based DSL | TypeScript, Bull, PostgreSQL
[10:44:12] DEBUG | [fast-enricher] Enriched in 45.3ms | keywords=10 entities=6 | prefix_len=163
[10:44:12] TRACE | [enriched-hybrid] CACHE MISS: enriched chunk (total enrichments: 9)
[10:44:12] TRACE | [fast-enricher] INPUT [1602 chars]: **Technology**: Go with distributed locking via Redis **Replicas**: 4 pods (production), active-passive with leader election **Resource Allocation**: 2 vCPU, 4GB RAM per pod **Key Responsibilities**: - Parse and validate cron expressions (extended format supporting seconds) - Maintain schedule registry in PostgreSQL - Distributed scheduling with leader election (one active scheduler) - Missed execution handling with configurable catch-up policy - Schedule conflict detection and resolution - Time...
[10:44:12] DEBUG | [fast-enricher] YAKE extracted 10 keywords: ['leader election', 'Missed execution', 'Resource Allocation', 'Key Responsibilities', 'Reliability Features']...
[10:44:12] TRACE | [fast-enricher] YAKE keywords with scores: ['leader election(0.001)', 'Missed execution(0.002)', 'Resource Allocation(0.002)', 'Key Responsibilities(0.002)', 'Reliability Features(0.002)', 'Performance Targets(0.002)', 'Scheduling Architecture(0.003)', 'Leader Scheduler(0.003)', 'Notification Service(0.003)', 'Multi-channel notification(0.003)']
[10:44:12] TRACE | [fast-enricher] YAKE extraction took 14.3ms
[10:44:12] DEBUG | [fast-enricher] spaCy extracted 6 entities: {'GPE': ['PostgreSQL'], 'PERSON': ['Leader Scheduler', 'Kafka'], 'ORG': ['Redis', 'SMS'], 'NORP': ['Scan']}
[10:44:12] TRACE | [fast-enricher] spaCy GPE: ['PostgreSQL']
[10:44:12] TRACE | [fast-enricher] spaCy PERSON: ['Leader Scheduler', 'Kafka']
[10:44:12] TRACE | [fast-enricher] spaCy ORG: ['Redis', 'SMS']
[10:44:12] TRACE | [fast-enricher] spaCy NORP: ['Scan']
[10:44:12] TRACE | [fast-enricher] spaCy NER took 43.6ms
[10:44:12] TRACE | [fast-enricher] Generated prefix: leader election, Missed execution, Resource Allocation, Key Responsibilities, Reliability Features, Performance Targets, Scheduling Architecture | PostgreSQL, Leader Scheduler, Kafka, Redis, SMS
[10:44:12] DEBUG | [fast-enricher] Enriched in 58.2ms | keywords=10 entities=6 | prefix_len=194
[10:44:12] TRACE | [enriched-hybrid] CACHE MISS: enriched chunk (total enrichments: 10)
[10:44:12] TRACE | [fast-enricher] INPUT [2399 chars]: **Technology**: Node.js with worker pool pattern **Replicas**: 8 pods (production), auto-scaling 6-16 **Resource Allocation**: 2 vCPU, 4GB RAM per pod **Key Responsibilities**: - Consume notification events from Kafka - Template rendering with Handlebars (cached in Redis) - Multi-channel delivery (Email via SendGrid, SMS via Twilio, Webhooks) - Retry logic with dead letter queue for failed deliveries - Delivery status tracking and analytics - User preference management (opt-in/opt-out) **Notific...
[10:44:12] DEBUG | [fast-enricher] YAKE extracted 10 keywords: ['SMS Queue', 'Webhook Queue', 'Email Queue', 'Resource Allocation', 'Key Responsibilities']...
[10:44:12] TRACE | [fast-enricher] YAKE keywords with scores: ['SMS Queue(0.003)', 'Webhook Queue(0.003)', 'Email Queue(0.004)', 'Resource Allocation(0.005)', 'Key Responsibilities(0.005)', 'Compiled templates(0.005)', 'AWS SNS(0.005)', 'Template Management(0.006)', 'Load Balancer(0.006)', 'Consume notification(0.006)']
[10:44:12] TRACE | [fast-enricher] YAKE extraction took 15.6ms
[10:44:12] DEBUG | [fast-enricher] spaCy extracted 15 entities: {'PERSON': ['Consume', 'Handlebars', 'Email', 'Kafka', 'Email Queue'], 'ORG': ['Kafka - Template', 'SMS', 'Twilio,', 'Webhooks) -', '┌───────────────┼───────────────'], 'GPE': ['Redis', 'PostgreSQL', 'SendGrid'], 'FAC': ['Socket.io *', 'the API Gateway']}
[10:44:12] TRACE | [fast-enricher] spaCy PERSON: ['Consume', 'Handlebars', 'Email', 'Kafka', 'Email Queue']
[10:44:12] TRACE | [fast-enricher] spaCy ORG: ['Kafka - Template', 'SMS', 'Twilio,', 'Webhooks) -', '┌───────────────┼───────────────']
[10:44:12] TRACE | [fast-enricher] spaCy GPE: ['Redis', 'PostgreSQL', 'SendGrid']
[10:44:12] TRACE | [fast-enricher] spaCy FAC: ['Socket.io *', 'the API Gateway']
[10:44:12] TRACE | [fast-enricher] spaCy NER took 69.8ms
[10:44:12] TRACE | [fast-enricher] Generated prefix: SMS Queue, Webhook Queue, Email Queue, Resource Allocation, Key Responsibilities, Compiled templates, AWS SNS | Consume, Handlebars, Kafka - Template, SMS, Redis
[10:44:12] DEBUG | [fast-enricher] Enriched in 85.6ms | keywords=10 entities=15 | prefix_len=161
[10:44:12] TRACE | [enriched-hybrid] CACHE MISS: enriched chunk (total enrichments: 11)
[10:44:12] TRACE | [fast-enricher] INPUT [2755 chars]: Success/failure events published back to Kafka ``` **Event Schema**: ```json { "event_id": "uuid-v4", "event_type": "workflow.execution.completed", "timestamp": "2026-01-15T10:30:00.000Z", "correlation_id": "request-trace-id", "payload": { "workflow_id": "wf-12345", "execution_id": "exec-67890", "status": "COMPLETED", "duration_ms": 4230 }, "metadata": { "source_service": "workflow-engine", "schema_version": "1.0" } } ``` ### Inter-Service Communication Services communicate using two primary pat...
[10:44:12] DEBUG | [fast-enricher] YAKE extracted 10 keywords: ['Monthly partitions', 'Indexing Strategy', 'Core Tables', 'Performance Characteristics', 'Partitioning Strategy']...
[10:44:12] TRACE | [fast-enricher] YAKE keywords with scores: ['Monthly partitions(0.002)', 'Indexing Strategy(0.002)', 'Core Tables(0.002)', 'Performance Characteristics(0.002)', 'Partitioning Strategy(0.002)', 'Redis Caching(0.002)', 'Redis Cluster(0.002)', 'Caching Layer(0.002)', 'Usage Patterns(0.002)', 'Cluster Configuration(0.002)']
[10:44:12] TRACE | [fast-enricher] YAKE extraction took 13.6ms
[10:44:12] DEBUG | [fast-enricher] spaCy extracted 11 entities: {'ORG': ['Success', 'created_at - Automatic', 'P99', 'TPS - Replication', 'RAM'], 'PERSON': ['Kafka', 'Drop', 'Redis Caching Layer'], 'GPE': ['correlation_id', 'GB'], 'WORK_OF_ART': ['workflow_id']}
[10:44:12] TRACE | [fast-enricher] spaCy ORG: ['Success', 'created_at - Automatic', 'P99', 'TPS - Replication', 'RAM']
[10:44:12] TRACE | [fast-enricher] spaCy PERSON: ['Kafka', 'Drop', 'Redis Caching Layer']
[10:44:12] TRACE | [fast-enricher] spaCy GPE: ['correlation_id', 'GB']
[10:44:12] TRACE | [fast-enricher] spaCy WORK_OF_ART: ['workflow_id']
[10:44:12] TRACE | [fast-enricher] spaCy NER took 53.1ms
[10:44:12] TRACE | [fast-enricher] Generated prefix: Monthly partitions, Indexing Strategy, Core Tables, Performance Characteristics, Partitioning Strategy, Redis Caching, Redis Cluster | Success, created_at - Automatic, Kafka, Drop, correlation_id
[10:44:12] DEBUG | [fast-enricher] Enriched in 67.0ms | keywords=10 entities=11 | prefix_len=195
[10:44:12] TRACE | [enriched-hybrid] CACHE MISS: enriched chunk (total enrichments: 12)
[10:44:12] TRACE | [fast-enricher] INPUT [2759 chars]: **Session Storage**: - Key pattern: `session:{user_id}` - TTL: 15 minutes (aligned with JWT expiry) - Data: User session state, preferences - Invalidation: On logout or password change 2. **Rate Limiting Counters**: - Key pattern: `ratelimit:{api_key}:{window}` - TTL: 60 seconds (sliding window) - Data: Request count per window - Algorithm: Token bucket with Redis INCR 3. **Compiled Templates**: - Key pattern: `template:{template_id}:{version}` - TTL: 1 hour - Data: Compiled Handlebars template ...
[10:44:12] DEBUG | [fast-enricher] YAKE extracted 10 keywords: ['Key pattern', 'JWT expiry', 'TTL', 'Data', 'Key']...
[10:44:12] TRACE | [fast-enricher] YAKE keywords with scores: ['Key pattern(0.004)', 'JWT expiry(0.015)', 'TTL(0.017)', 'Data(0.017)', 'Key(0.023)', 'Invalidation(0.029)', 'pattern(0.036)', 'password change(0.043)', 'User(0.051)', 'Limiting Counters(0.052)']
[10:44:12] TRACE | [fast-enricher] YAKE extraction took 9.2ms
[10:44:12] DEBUG | [fast-enricher] spaCy extracted 6 entities: {'ORG': ['JWT', 'Redis', 'Production Metrics', 'P99', 'AZ'], 'PERSON': ['Kafka Event Streaming']}
[10:44:12] TRACE | [fast-enricher] spaCy ORG: ['JWT', 'Redis', 'Production Metrics', 'P99', 'AZ']
[10:44:12] TRACE | [fast-enricher] spaCy PERSON: ['Kafka Event Streaming']
[10:44:12] TRACE | [fast-enricher] spaCy NER took 50.8ms
[10:44:12] TRACE | [fast-enricher] Generated prefix: Key pattern, JWT expiry, TTL, Data, Key, Invalidation, pattern | JWT, Redis, Kafka Event Streaming
[10:44:12] DEBUG | [fast-enricher] Enriched in 60.2ms | keywords=10 entities=6 | prefix_len=98
[10:44:12] TRACE | [enriched-hybrid] CACHE MISS: enriched chunk (total enrichments: 13)
[10:44:12] TRACE | [fast-enricher] INPUT [2746 chars]: records: 500 - Session timeout: 30 seconds --- ## Message Queue Patterns ### Pub/Sub Pattern Used for broadcasting events to multiple interested consumers: ``` Workflow Engine publishes workflow.execution.completed │ ┌───────────────┼───────────────┬──────────────┐ ▼ ▼ ▼ ▼ Analytics Notification Audit Logger Billing Service Service Service Service ``` Each service maintains its own consumer group and processes events independently. Failures in one consumer don't affect others. ### Request-Reply ...
[10:44:12] DEBUG | [fast-enricher] YAKE extracted 10 keywords: ['multiple interested', 'Pattern', 'Pub', 'interested consumers', 'consumer group']...
[10:44:12] TRACE | [fast-enricher] YAKE keywords with scores: ['multiple interested(0.042)', 'Pattern(0.047)', 'Pub(0.055)', 'interested consumers(0.067)', 'consumer group(0.067)', 'Queue Patterns(0.084)', 'Session timeout(0.087)', 'events independently(0.088)', 'Pattern Synchronous(0.090)', 'Saga Pattern(0.093)']
[10:44:12] TRACE | [fast-enricher] YAKE extraction took 9.8ms
[10:44:12] DEBUG | [fast-enricher] spaCy extracted 4 entities: {'ORG': ['Pub/Sub Pattern', 'Request-Reply Pattern Synchronous', 'DLQ', 'Cache-Aside Pattern (Lazy Loading']}
[10:44:12] TRACE | [fast-enricher] spaCy ORG: ['Pub/Sub Pattern', 'Request-Reply Pattern Synchronous', 'DLQ', 'Cache-Aside Pattern (Lazy Loading']
[10:44:12] TRACE | [fast-enricher] spaCy NER took 28.6ms
[10:44:12] TRACE | [fast-enricher] Generated prefix: multiple interested, Pattern, Pub, interested consumers, consumer group, Queue Patterns, Session timeout | Pub/Sub Pattern, Request-Reply Pattern Synchronous
[10:44:12] DEBUG | [fast-enricher] Enriched in 38.6ms | keywords=10 entities=4 | prefix_len=157
[10:44:12] TRACE | [enriched-hybrid] CACHE MISS: enriched chunk (total enrichments: 14)
[10:44:12] TRACE | [fast-enricher] INPUT [2701 chars]: ### Cache Invalidation Strategies **Time-Based Expiration (TTL)**: - Short-lived data: 5-15 minutes (session tokens, rate limit counters) - Medium-lived data: 1 hour (workflow definitions, templates) - Long-lived data: 24 hours (static configuration) **Event-Based Invalidation**: ``` Database Update Event → Kafka (cache.invalidation topic) │ ▼ All service instances consume event │ ▼ Redis DEL for affected keys ``` **Pattern-Based Invalidation**: - Use Redis SCAN + DEL for wildcard patterns - Exa...
[10:44:12] DEBUG | [fast-enricher] YAKE extracted 10 keywords: ['Node.js Map', 'Invalidation Strategies', 'Cache Warming', 'Cache Invalidation', 'Short-lived data']...
[10:44:12] TRACE | [fast-enricher] YAKE keywords with scores: ['Node.js Map(0.003)', 'Invalidation Strategies(0.003)', 'Cache Warming(0.004)', 'Cache Invalidation(0.004)', 'Short-lived data(0.004)', 'Medium-lived data(0.004)', 'Long-lived data(0.004)', 'cache miss(0.005)', 'Redis DEL(0.006)', 'Redis SCAN(0.006)']
[10:44:12] TRACE | [fast-enricher] YAKE extraction took 17.2ms
[10:44:12] DEBUG | [fast-enricher] spaCy extracted 15 entities: {'ORG': ['TTL', '`user:*:{user_id}` **', 'Multi-Level Caching Application', 'LRU', 'PKI'], 'PERSON': ['Kafka', 'Redis DEL', 'L2 Cache', 'Database', 'Redis'], 'GPE': ['mTLS', 'Redis', 'PostgreSQL'], 'PRODUCT': ['API Gateway', 'RS256']}
[10:44:12] TRACE | [fast-enricher] spaCy ORG: ['TTL', '`user:*:{user_id}` **', 'Multi-Level Caching Application', 'LRU', 'PKI']
[10:44:12] TRACE | [fast-enricher] spaCy PERSON: ['Kafka', 'Redis DEL', 'L2 Cache', 'Database', 'Redis']
[10:44:12] TRACE | [fast-enricher] spaCy GPE: ['mTLS', 'Redis', 'PostgreSQL']
[10:44:12] TRACE | [fast-enricher] spaCy PRODUCT: ['API Gateway', 'RS256']
[10:44:12] TRACE | [fast-enricher] spaCy NER took 73.1ms
[10:44:12] TRACE | [fast-enricher] Generated prefix: Node.js Map, Invalidation Strategies, Cache Warming, Cache Invalidation, Short-lived data, Medium-lived data, Long-lived data | TTL, `user:*:{user_id}` **, Kafka, Redis DEL, mTLS
[10:44:12] DEBUG | [fast-enricher] Enriched in 90.6ms | keywords=10 entities=15 | prefix_len=178
[10:44:12] TRACE | [enriched-hybrid] CACHE MISS: enriched chunk (total enrichments: 15)
[10:44:12] TRACE | [fast-enricher] INPUT [2518 chars]: per API key (default: 1000 RPM) ### Secrets Management **HashiCorp Vault Integration**: - Dynamic database credentials: Generated on-demand, 1-hour TTL - Encryption keys: Transit secrets engine for encryption-as-a-service - API keys for external services: Stored with versioning - Rotation policy: Automated rotation every 90 days with notification **Secret Access Pattern**: ``` Service starts → Vault authentication (Kubernetes service account) │ ▼ Request secret lease (1-hour TTL) │ ▼ Vault retur...
[10:44:12] DEBUG | [fast-enricher] YAKE extracted 10 keywords: ['API Operations', 'API key', 'Redis SET', 'Redis DEL', 'API Gateway']...
[10:44:12] TRACE | [fast-enricher] YAKE keywords with scores: ['API Operations(0.001)', 'API key(0.002)', 'Redis SET(0.002)', 'Redis DEL(0.002)', 'API Gateway(0.002)', 'Simple SELECT(0.002)', 'Performance Characteristics(0.002)', 'Throughput Capacity(0.002)', 'AWS KMS(0.002)', 'Batch operations(0.002)']
[10:44:12] TRACE | [fast-enricher] YAKE extraction took 13.8ms
[10:44:12] DEBUG | [fast-enricher] spaCy extracted 14 entities: {'ORG': ['API', 'TTL - Encryption', 'Vault', 'Kubernetes', 'KMS'], 'PERSON': ['Request', 'TTL', 'Renew', '150ms', '200ms'], 'GPE': ['PostgreSQL'], 'WORK_OF_ART': ['GET /workflows/{id', 'POST', 'GET']}
[10:44:12] TRACE | [fast-enricher] spaCy ORG: ['API', 'TTL - Encryption', 'Vault', 'Kubernetes', 'KMS']
[10:44:12] TRACE | [fast-enricher] spaCy PERSON: ['Request', 'TTL', 'Renew', '150ms', '200ms']
[10:44:12] TRACE | [fast-enricher] spaCy GPE: ['PostgreSQL']
[10:44:12] TRACE | [fast-enricher] spaCy WORK_OF_ART: ['GET /workflows/{id', 'POST', 'GET']
[10:44:12] TRACE | [fast-enricher] spaCy NER took 67.8ms
[10:44:12] TRACE | [fast-enricher] Generated prefix: API Operations, API key, Redis SET, Redis DEL, API Gateway, Simple SELECT, Performance Characteristics | API, TTL - Encryption, Request, TTL, PostgreSQL
[10:44:12] DEBUG | [fast-enricher] Enriched in 81.9ms | keywords=10 entities=14 | prefix_len=152
[10:44:12] TRACE | [enriched-hybrid] CACHE MISS: enriched chunk (total enrichments: 16)
[10:44:12] TRACE | [fast-enricher] INPUT [2479 chars]: - Execution start rate: 500 per second - Completion rate: 450 per second (average 2-second execution time) **Database**: - Read throughput: 50,000 queries per second (across replicas) - Write throughput: 15,000 transactions per second - Connection capacity: 2,000 concurrent connections **Kafka**: - Message ingestion: 100,000 messages per second - Consumer throughput: 80,000 messages per second (aggregated) - End-to-end latency: < 100ms (P99) ### Resource Utilization **CPU Utilization** (Target: ...
[10:44:12] DEBUG | [fast-enricher] YAKE extracted 10 keywords: ['Workflow Engine', 'API Gateway', 'Resource Utilization', 'Gbps average', 'Auth Service']...
[10:44:12] TRACE | [fast-enricher] YAKE keywords with scores: ['Workflow Engine(0.000)', 'API Gateway(0.000)', 'Resource Utilization(0.000)', 'Gbps average(0.000)', 'Auth Service(0.001)', 'Gbps peak(0.001)', 'API error(0.001)', 'CPU Utilization(0.001)', 'Maximum acceptable(0.002)', 'Error rate(0.002)']
[10:44:12] TRACE | [fast-enricher] YAKE extraction took 12.8ms
[10:44:12] DEBUG | [fast-enricher] spaCy extracted 10 entities: {'PERSON': ['Kafka', 'GB', 'Continuous'], 'ORG': ['Target', '- Auth Service', 'Monitoring & Alerting', 'P95', 'P99'], 'PRODUCT': ['P50'], 'GPE': ['Failover']}
[10:44:12] TRACE | [fast-enricher] spaCy PERSON: ['Kafka', 'GB', 'Continuous']
[10:44:12] TRACE | [fast-enricher] spaCy ORG: ['Target', '- Auth Service', 'Monitoring & Alerting', 'P95', 'P99']
[10:44:12] TRACE | [fast-enricher] spaCy PRODUCT: ['P50']
[10:44:12] TRACE | [fast-enricher] spaCy GPE: ['Failover']
[10:44:12] TRACE | [fast-enricher] spaCy NER took 68.6ms
[10:44:12] TRACE | [fast-enricher] Generated prefix: Workflow Engine, API Gateway, Resource Utilization, Gbps average, Auth Service, Gbps peak, API error | Kafka, GB, Target, - Auth Service, P50
[10:44:12] DEBUG | [fast-enricher] Enriched in 81.6ms | keywords=10 entities=10 | prefix_len=141
[10:44:12] TRACE | [enriched-hybrid] CACHE MISS: enriched chunk (total enrichments: 17)
[10:44:12] TRACE | [fast-enricher] INPUT [2443 chars]: Service (2) - PostgreSQL Primary - PostgreSQL Replica - PostgreSQL Replica - Redis Primary (2) - Redis Replica (2) - Redis Replica (2) - Kafka Broker (2) - Kafka Broker (2) - Kafka Broker (1) ``` **Automatic Failover**: - Database: 30-60 seconds (automatic promotion of replica) - Redis: < 10 seconds (Sentinel-based failover) - Kafka: < 30 seconds (controller election) - Services: Kubernetes health checks with 10-second liveness probes ### Backup Strategy **Database Backups**: - Automated snapsho...
[10:44:12] DEBUG | [fast-enricher] YAKE extracted 10 keywords: ['Kafka Broker', 'Redis Replica', 'Backup Strategy', 'Backup Schedule', 'Kubernetes health']...
[10:44:12] TRACE | [fast-enricher] YAKE keywords with scores: ['Kafka Broker(0.000)', 'Redis Replica(0.003)', 'Backup Strategy(0.004)', 'Backup Schedule(0.004)', 'Kubernetes health(0.004)', 'Weekly automated(0.004)', 'automatic promotion(0.005)', 'Multiple health(0.005)', 'Configuration Backups(0.006)', 'Exported daily(0.006)']
[10:44:12] TRACE | [fast-enricher] YAKE extraction took 15.3ms
[10:44:12] DEBUG | [fast-enricher] spaCy extracted 9 entities: {'PERSON': ['Redis Replica', 'Kafka Broker'], 'ORG': ['Sentinel', 'S3', 'WAL', 'AZ', 'ALB - Recovery'], 'NORP': ['Glacier'], 'GPE': ['DR']}
[10:44:12] TRACE | [fast-enricher] spaCy PERSON: ['Redis Replica', 'Kafka Broker']
[10:44:12] TRACE | [fast-enricher] spaCy ORG: ['Sentinel', 'S3', 'WAL', 'AZ', 'ALB - Recovery']
[10:44:12] TRACE | [fast-enricher] spaCy NORP: ['Glacier']
[10:44:12] TRACE | [fast-enricher] spaCy GPE: ['DR']
[10:44:12] TRACE | [fast-enricher] spaCy NER took 60.6ms
[10:44:12] TRACE | [fast-enricher] Generated prefix: Kafka Broker, Redis Replica, Backup Strategy, Backup Schedule, Kubernetes health, Weekly automated, automatic promotion | Redis Replica, Kafka Broker, Sentinel, S3, Glacier
[10:44:12] DEBUG | [fast-enricher] Enriched in 76.1ms | keywords=10 entities=9 | prefix_len=172
[10:44:12] TRACE | [enriched-hybrid] CACHE MISS: enriched chunk (total enrichments: 18)
[10:44:12] TRACE | [fast-enricher] INPUT [2547 chars]: Resume normal operations - Recovery time: 1-3 hours depending on data volume - Data loss: None if corruption detected quickly ### Testing & Validation **DR Drill Schedule**: - Monthly: Automated failover test (single AZ failure simulation) - Quarterly: Full DR region failover (non-production hours) - Annually: Complete disaster simulation with all stakeholders **Last DR Test Results** (Dec 15, 2025): - Scenario: Full region failover - Actual RTO: 2 hours 23 minutes (target: 4 hours) - Actual RPO...
[10:44:12] DEBUG | [fast-enricher] YAKE extracted 10 keywords: ['Database size', 'Active users', 'Auth Service', 'Daily workflow', 'API requests']...
[10:44:12] TRACE | [fast-enricher] YAKE keywords with scores: ['Database size(0.001)', 'Active users(0.001)', 'Auth Service(0.002)', 'Daily workflow(0.002)', 'API requests(0.002)', 'API Gateway(0.002)', 'Test Results(0.002)', 'Business Continuity(0.002)', 'Communication Plan(0.002)', 'Engineering Manager(0.002)']
[10:44:12] TRACE | [fast-enricher] YAKE extraction took 14.0ms
[10:44:12] DEBUG | [fast-enricher] spaCy extracted 16 entities: {'ORG': ['AZ', 'DNS', 'Auth Service', 'Vault', 'Twilio'], 'PRODUCT': ['P1'], 'PERSON': ['Critical Path', 'API Gateway', 'Redis', 'Kafka', 'Scheduler'], 'GPE': ['PostgreSQL', 'Redis', 'Kafka', 'SendGrid'], 'LAW': ['Sep 1, 2025']}
[10:44:12] TRACE | [fast-enricher] spaCy ORG: ['AZ', 'DNS', 'Auth Service', 'Vault', 'Twilio']
[10:44:12] TRACE | [fast-enricher] spaCy PRODUCT: ['P1']
[10:44:12] TRACE | [fast-enricher] spaCy PERSON: ['Critical Path', 'API Gateway', 'Redis', 'Kafka', 'Scheduler']
[10:44:12] TRACE | [fast-enricher] spaCy GPE: ['PostgreSQL', 'Redis', 'Kafka', 'SendGrid']
[10:44:12] TRACE | [fast-enricher] spaCy LAW: ['Sep 1, 2025']
[10:44:12] TRACE | [fast-enricher] spaCy NER took 63.2ms
[10:44:12] TRACE | [fast-enricher] Generated prefix: Database size, Active users, Auth Service, Daily workflow, API requests, API Gateway, Test Results | AZ, DNS, P1, Critical Path, API Gateway
[10:44:12] DEBUG | [fast-enricher] Enriched in 77.4ms | keywords=10 entities=16 | prefix_len=140
[10:44:12] TRACE | [enriched-hybrid] CACHE MISS: enriched chunk (total enrichments: 19)
[10:44:12] TRACE | [fast-enricher] INPUT [3165 chars]: # CloudFlow Platform - Deployment and Operations Guide **Version:** 2.4.0 **Last Updated:** January 2026 **Target Environment:** Production (AWS EKS) ## Table of Contents 1. [Overview](#overview) 2. [Prerequisites](#prerequisites) 3. [Infrastructure Setup](#infrastructure-setup) 4. [Kubernetes Deployment](#kubernetes-deployment) 5. [Database Configuration](#database-configuration) 6. [Monitoring and Observability](#monitoring-and-observability) 7. [Backup and Disaster Recovery](#backup-and-disas...
[10:44:12] DEBUG | [fast-enricher] YAKE extracted 10 keywords: ['Target Environment', 'Operations Guide', 'AWS EKS', 'EKS', 'EKS cluster']...
[10:44:12] TRACE | [fast-enricher] YAKE keywords with scores: ['Target Environment(0.026)', 'Operations Guide(0.036)', 'AWS EKS(0.042)', 'EKS(0.051)', 'EKS cluster(0.058)', 'January(0.059)', 'AWS(0.064)', 'Version(0.064)', 'Updated(0.067)', 'Target(0.067)']
[10:44:12] TRACE | [fast-enricher] YAKE extraction took 13.5ms
[10:44:12] DEBUG | [fast-enricher] spaCy extracted 11 entities: {'ORG': ['Platform - Deployment and Operations Guide **', 'CloudFlow', 'Amazon EKS (Elastic Kubernetes Service', 'us-west-2', 'IAM'], 'LAW': ['Contents 1'], 'PERSON': ['Backup', 'Disaster Recovery](#backup-and-disaster', 'Required Tools', 'Redis', 'Grafana']}
[10:44:12] TRACE | [fast-enricher] spaCy ORG: ['Platform - Deployment and Operations Guide **', 'CloudFlow', 'Amazon EKS (Elastic Kubernetes Service', 'us-west-2', 'IAM']
[10:44:12] TRACE | [fast-enricher] spaCy LAW: ['Contents 1']
[10:44:12] TRACE | [fast-enricher] spaCy PERSON: ['Backup', 'Disaster Recovery](#backup-and-disaster', 'Required Tools', 'Redis', 'Grafana']
[10:44:12] TRACE | [fast-enricher] spaCy NER took 78.1ms
[10:44:12] TRACE | [fast-enricher] Generated prefix: Target Environment, Operations Guide, AWS EKS, EKS, EKS cluster, January, AWS | Platform - Deployment and Operations Guide **, CloudFlow, Contents 1, Backup, Disaster Recovery](#backup-and-disaster
[10:44:12] DEBUG | [fast-enricher] Enriched in 91.8ms | keywords=10 entities=11 | prefix_len=197
[10:44:12] TRACE | [enriched-hybrid] CACHE MISS: enriched chunk (total enrichments: 20)
[10:44:12] TRACE | [fast-enricher] INPUT [3394 chars]: create cluster -f cluster-config.yaml ``` ### Storage Configuration Install the EBS CSI driver for persistent volumes: ```bash eksctl create addon --name aws-ebs-csi-driver \ --cluster cloudflow-production \ --service-account-role-arn arn:aws:iam::ACCOUNT_ID:role/AmazonEKS_EBS_CSI_DriverRole \ --force ``` --- ## Kubernetes Deployment ### Namespace Setup Create the CloudFlow namespace and configure resource quotas: ```yaml # namespace.yaml apiVersion: v1 kind: Namespace metadata: name: cloudflow-...
[10:44:12] DEBUG | [fast-enricher] YAKE extracted 10 keywords: ['openssl rand', 'Prefix tls', 'from-literal', 'IfNotPresent resources', 'ClusterIP port']...
[10:44:12] TRACE | [fast-enricher] YAKE keywords with scores: ['openssl rand(0.002)', 'Prefix tls(0.005)', 'from-literal(0.008)', 'IfNotPresent resources(0.008)', 'ClusterIP port(0.008)', 'helm repo(0.008)', 'force yaml(0.009)', 'ResourceQuota metadata(0.009)', 'nginx annotations(0.009)', 'true minReplicas(0.010)']
[10:44:12] TRACE | [fast-enricher] YAKE extraction took 10.1ms
[10:44:13] DEBUG | [fast-enricher] spaCy extracted 10 entities: {'GPE': ['addon'], 'PERSON': ['Namespace', 'metadata', 'helm repo'], 'ORG': ['ResourceQuota', 'bash helm repo', 'minReplicas', 'api.cloudflow.io'], 'NORP': ['ClusterIP'], 'WORK_OF_ART': ['letsencrypt-prod']}
[10:44:13] TRACE | [fast-enricher] spaCy GPE: ['addon']
[10:44:13] TRACE | [fast-enricher] spaCy PERSON: ['Namespace', 'metadata', 'helm repo']
[10:44:13] TRACE | [fast-enricher] spaCy ORG: ['ResourceQuota', 'bash helm repo', 'minReplicas', 'api.cloudflow.io']
[10:44:13] TRACE | [fast-enricher] spaCy NORP: ['ClusterIP']
[10:44:13] TRACE | [fast-enricher] spaCy WORK_OF_ART: ['letsencrypt-prod']
[10:44:13] TRACE | [fast-enricher] spaCy NER took 36.8ms
[10:44:13] TRACE | [fast-enricher] Generated prefix: openssl rand, Prefix tls, from-literal, IfNotPresent resources, ClusterIP port, helm repo, force yaml | addon, Namespace, metadata, ResourceQuota, bash helm repo
[10:44:13] DEBUG | [fast-enricher] Enriched in 47.2ms | keywords=10 entities=10 | prefix_len=161
[10:44:13] TRACE | [enriched-hybrid] CACHE MISS: enriched chunk (total enrichments: 21)
[10:44:13] TRACE | [fast-enricher] INPUT [2194 chars]: - secretName: cloudflow-tls hosts: - api.cloudflow.io healthCheck: enabled: true livenessProbe: path: /health initialDelaySeconds: 30 periodSeconds: 10 timeoutSeconds: 5 failureThreshold: 3 readinessProbe: path: /ready initialDelaySeconds: 10 periodSeconds: 5 timeoutSeconds: 3 failureThreshold: 3 env: - name: NODE_ENV value: "production" - name: LOG_LEVEL value: "info" - name: API_PORT value: "3000" - name: WORKER_CONCURRENCY value: "10" - name: AWS_REGION value: "us-east-1" - name: METRICS_ENAB...
[10:44:13] DEBUG | [fast-enricher] YAKE extracted 10 keywords: ['Test health', 'RESTARTS AGE', 'curl http', 'Database Configuration', 'Verification Verify']...
[10:44:13] TRACE | [fast-enricher] YAKE keywords with scores: ['Test health(0.002)', 'RESTARTS AGE(0.002)', 'curl http(0.003)', 'Database Configuration(0.003)', 'Verification Verify(0.003)', 'Expected output(0.003)', 'Test readiness(0.004)', 'Deployment Verification(0.004)', 'endpoint curl(0.004)', 'Check service(0.004)']
[10:44:13] TRACE | [fast-enricher] YAKE extraction took 11.7ms
[10:44:13] DEBUG | [fast-enricher] spaCy extracted 5 entities: {'NORP': ['Helm'], 'PERSON': ['NAME', '1/1 Running', 'Port'], 'ORG': ['CloudFlow']}
[10:44:13] TRACE | [fast-enricher] spaCy NORP: ['Helm']
[10:44:13] TRACE | [fast-enricher] spaCy PERSON: ['NAME', '1/1 Running', 'Port']
[10:44:13] TRACE | [fast-enricher] spaCy ORG: ['CloudFlow']
[10:44:13] TRACE | [fast-enricher] spaCy NER took 48.7ms
[10:44:13] TRACE | [fast-enricher] Generated prefix: Test health, RESTARTS AGE, curl http, Database Configuration, Verification Verify, Expected output, Test readiness | Helm, NAME, 1/1 Running, CloudFlow
[10:44:13] DEBUG | [fast-enricher] Enriched in 60.6ms | keywords=10 entities=5 | prefix_len=151
[10:44:13] TRACE | [enriched-hybrid] CACHE MISS: enriched chunk (total enrichments: 22)
[10:44:13] TRACE | [fast-enricher] INPUT [3382 chars]: Deploy PostgreSQL using the Bitnami Helm chart: ```yaml # postgres-values.yaml global: postgresql: auth: username: cloudflow database: cloudflow existingSecret: postgres-credentials image: tag: "14.10.0" primary: resources: limits: cpu: 4000m memory: 8Gi requests: cpu: 2000m memory: 4Gi persistence: enabled: true size: 100Gi storageClass: gp3 extendedConfiguration: | max_connections = 100 shared_buffers = 2GB effective_cache_size = 6GB maintenance_work_mem = 512MB checkpoint_completion_target = ...
[10:44:13] DEBUG | [fast-enricher] YAKE extracted 10 keywords: ['Database Migrations', 'Install Prometheus', 'Migrations Run', 'Run database', 'Prometheus Setup']...
[10:44:13] TRACE | [fast-enricher] YAKE keywords with scores: ['Database Migrations(0.006)', 'Install Prometheus(0.007)', 'Migrations Run(0.008)', 'Run database(0.008)', 'Prometheus Setup(0.009)', 'Setup Install(0.009)', 'Bitnami Helm(0.010)', 'prometheus prometheus-community(0.011)', 'PgBouncer Configuration(0.012)', 'Helm chart(0.014)']
[10:44:13] TRACE | [fast-enricher] YAKE extraction took 8.2ms
[10:44:13] DEBUG | [fast-enricher] spaCy extracted 3 entities: {'ORG': ['Deploy PostgreSQL'], 'FAC': ['the Bitnami Helm'], 'PERSON': ['PgBouncer']}
[10:44:13] TRACE | [fast-enricher] spaCy ORG: ['Deploy PostgreSQL']
[10:44:13] TRACE | [fast-enricher] spaCy FAC: ['the Bitnami Helm']
[10:44:13] TRACE | [fast-enricher] spaCy PERSON: ['PgBouncer']
[10:44:13] TRACE | [fast-enricher] spaCy NER took 15.9ms
[10:44:13] TRACE | [fast-enricher] Generated prefix: Database Migrations, Install Prometheus, Migrations Run, Run database, Prometheus Setup, Setup Install, Bitnami Helm | Deploy PostgreSQL, the Bitnami Helm, PgBouncer
[10:44:13] DEBUG | [fast-enricher] Enriched in 24.3ms | keywords=10 entities=3 | prefix_len=165
[10:44:13] TRACE | [enriched-hybrid] CACHE MISS: enriched chunk (total enrichments: 23)
[10:44:13] TRACE | [fast-enricher] INPUT [2000 chars]: --set prometheus.prometheusSpec.retention=30d \ --set prometheus.prometheusSpec.storageSpec.volumeClaimTemplate.spec.resources.requests.storage=100Gi ``` CloudFlow exposes Prometheus metrics on port 9090 at the `/metrics` endpoint. The metrics include: - **HTTP Metrics**: Request rate, latency, error rate - **Database Metrics**: Connection pool status, query duration - **Worker Metrics**: Job queue size, processing time, failure rate - **System Metrics**: CPU usage, memory usage, heap statistics...
[10:44:13] DEBUG | [fast-enricher] YAKE extracted 10 keywords: ['TYPE cloudflow', 'Prometheus metrics', 'CloudFlow', 'exposes Prometheus', 'metrics']...
[10:44:13] TRACE | [fast-enricher] YAKE keywords with scores: ['TYPE cloudflow(0.009)', 'Prometheus metrics(0.012)', 'CloudFlow(0.019)', 'exposes Prometheus(0.020)', 'metrics(0.020)', 'total Total(0.022)', 'total(0.026)', 'HTTP(0.026)', 'TYPE(0.029)', 'HTTP Metrics(0.040)']
[10:44:13] TRACE | [fast-enricher] YAKE extraction took 9.9ms
[10:44:13] DEBUG | [fast-enricher] spaCy extracted 11 entities: {'ORG': ['CPU', 'HELP cloudflow_worker_jobs_processed_total Total', 'CloudFlow', 'P95/P99', 'CloudWatch CloudFlow'], 'LAW': ['HELP cloudflow_http_request_duration_seconds'], 'PERSON': ['histogram cloudflow_http_request_duration_seconds_bucket{method="POST",route="/api', 'Grafana Dashboards', 'Access Grafana', 'JSON', 'Queue']}
[10:44:13] TRACE | [fast-enricher] spaCy ORG: ['CPU', 'HELP cloudflow_worker_jobs_processed_total Total', 'CloudFlow', 'P95/P99', 'CloudWatch CloudFlow']
[10:44:13] TRACE | [fast-enricher] spaCy LAW: ['HELP cloudflow_http_request_duration_seconds']
[10:44:13] TRACE | [fast-enricher] spaCy PERSON: ['histogram cloudflow_http_request_duration_seconds_bucket{method="POST",route="/api', 'Grafana Dashboards', 'Access Grafana', 'JSON', 'Queue']
[10:44:13] TRACE | [fast-enricher] spaCy NER took 39.8ms
[10:44:13] TRACE | [fast-enricher] Generated prefix: TYPE cloudflow, Prometheus metrics, CloudFlow, exposes Prometheus, metrics, total Total, total | CPU, HELP cloudflow_worker_jobs_processed_total Total, HELP cloudflow_http_request_duration_seconds, histogram cloudflow_http_request_duration_seconds_bucket{method="POST",route="/api, Grafana Dashboards
[10:44:13] DEBUG | [fast-enricher] Enriched in 50.0ms | keywords=10 entities=11 | prefix_len=300
[10:44:13] TRACE | [enriched-hybrid] CACHE MISS: enriched chunk (total enrichments: 24)
[10:44:13] TRACE | [fast-enricher] INPUT [3054 chars]: Configure log aggregation: ```bash # Install Fluent Bit for log forwarding helm install fluent-bit fluent/fluent-bit \ --namespace logging \ --create-namespace \ --set cloudWatch.enabled=true \ --set cloudWatch.region=us-east-1 \ --set cloudWatch.logGroupName=/aws/eks/cloudflow-production/application ``` Query logs using CloudWatch Insights: ```sql fields @timestamp, @message, level, requestId, userId | filter namespace = "cloudflow-prod" | filter level = "error" | sort @timestamp desc | limit 1...
[10:44:13] DEBUG | [fast-enricher] YAKE extracted 10 keywords: ['Disaster Recovery', 'Recovery Procedure', 'Backup Strategy', 'Query logs', 'CloudWatch Insights']...
[10:44:13] TRACE | [fast-enricher] YAKE keywords with scores: ['Disaster Recovery(0.002)', 'Recovery Procedure(0.004)', 'Backup Strategy(0.010)', 'Query logs(0.011)', 'CloudWatch Insights(0.012)', 'Velero Install(0.013)', 'Install Velero(0.013)', 'Retained(0.013)', 'backup schedule(0.014)', 'Strategy CloudFlow(0.015)']
[10:44:13] TRACE | [fast-enricher] YAKE extraction took 8.2ms
[10:44:13] DEBUG | [fast-enricher] spaCy extracted 5 entities: {'ORG': ['CloudWatch Insights', 'WAL'], 'PERSON': ['Backup', 'Automated Backup', 'Velero Install Velero']}
[10:44:13] TRACE | [fast-enricher] spaCy ORG: ['CloudWatch Insights', 'WAL']
[10:44:13] TRACE | [fast-enricher] spaCy PERSON: ['Backup', 'Automated Backup', 'Velero Install Velero']
[10:44:13] TRACE | [fast-enricher] spaCy NER took 21.3ms
[10:44:13] TRACE | [fast-enricher] Generated prefix: Disaster Recovery, Recovery Procedure, Backup Strategy, Query logs, CloudWatch Insights, Velero Install, Install Velero | CloudWatch Insights, WAL, Backup, Automated Backup
[10:44:13] DEBUG | [fast-enricher] Enriched in 29.7ms | keywords=10 entities=5 | prefix_len=172
[10:44:13] TRACE | [enriched-hybrid] CACHE MISS: enriched chunk (total enrichments: 25)
[10:44:13] TRACE | [fast-enricher] INPUT [2874 chars]: **Verify Service Health**: ```bash kubectl get pods -n cloudflow-prod curl https://api.cloudflow.io/health ``` **Recovery Time Objective (RTO)**: 4 hours **Recovery Point Objective (RPO)**: 24 hours --- ## Scaling and Performance ### Horizontal Pod Autoscaling CloudFlow is configured with Horizontal Pod Autoscaler (HPA) to automatically scale based on resource utilization: ```yaml # hpa.yaml apiVersion: autoscaling/v2 kind: HorizontalPodAutoscaler metadata: name: cloudflow-api-hpa namespace: clo...
[10:44:13] DEBUG | [fast-enricher] YAKE extracted 10 keywords: ['API latency', 'Horizontal Pod', 'Redis Caching', 'Pod Autoscaler', 'Verify Service']...
[10:44:13] TRACE | [fast-enricher] YAKE keywords with scores: ['API latency(0.001)', 'Horizontal Pod(0.002)', 'Redis Caching(0.003)', 'Pod Autoscaler(0.003)', 'Verify Service(0.003)', 'Service Health(0.003)', 'Recovery Time(0.003)', 'Time Objective(0.003)', 'Recovery Point(0.003)', 'Point Objective(0.003)']
[10:44:13] TRACE | [fast-enricher] YAKE extraction took 8.6ms
[10:44:13] DEBUG | [fast-enricher] spaCy extracted 7 entities: {'ORG': ['RTO', 'RPO', 'Horizontal Pod Autoscaling', 'Horizontal Pod Autoscaler', 'HPA'], 'GPE': ['PgBouncer'], 'PERSON': ['Redis Caching']}
[10:44:13] TRACE | [fast-enricher] spaCy ORG: ['RTO', 'RPO', 'Horizontal Pod Autoscaling', 'Horizontal Pod Autoscaler', 'HPA']
[10:44:13] TRACE | [fast-enricher] spaCy GPE: ['PgBouncer']
[10:44:13] TRACE | [fast-enricher] spaCy PERSON: ['Redis Caching']
[10:44:13] TRACE | [fast-enricher] spaCy NER took 30.8ms
[10:44:13] TRACE | [fast-enricher] Generated prefix: API latency, Horizontal Pod, Redis Caching, Pod Autoscaler, Verify Service, Service Health, Recovery Time | RTO, RPO, PgBouncer, Redis Caching
[10:44:13] DEBUG | [fast-enricher] Enriched in 39.6ms | keywords=10 entities=7 | prefix_len=142
[10:44:13] TRACE | [enriched-hybrid] CACHE MISS: enriched chunk (total enrichments: 26)
[10:44:13] TRACE | [fast-enricher] INPUT [3151 chars]: podSelector: matchLabels: app: cloudflow policyTypes: - Ingress - Egress ingress: - from: - namespaceSelector: matchLabels: name: cloudflow-prod - podSelector: matchLabels: app: nginx-ingress ports: - protocol: TCP port: 3000 egress: - to: - podSelector: matchLabels: app: postgresql ports: - protocol: TCP port: 5432 - to: - podSelector: matchLabels: app: redis ports: - protocol: TCP port: 6379 ``` ### Pod Security Standards Enforce Pod Security Standards at the namespace level: ```bash kubectl l...
[10:44:13] DEBUG | [fast-enricher] YAKE extracted 10 keywords: ['TCP port', 'SHOW POOLS', 'External Secrets', 'Secrets Operator', 'kubectl top']...
[10:44:13] TRACE | [fast-enricher] YAKE keywords with scores: ['TCP port(0.001)', 'SHOW POOLS(0.002)', 'External Secrets(0.002)', 'Secrets Operator(0.002)', 'kubectl top(0.003)', 'Egress ingress(0.003)', 'bash kubectl(0.003)', 'Install External(0.003)', 'Windows Scheduled(0.003)', 'Scheduled maintenance(0.003)']
[10:44:13] TRACE | [fast-enricher] YAKE extraction took 10.1ms
[10:44:13] DEBUG | [fast-enricher] spaCy extracted 7 entities: {'ORG': ['Install External Secrets Operator', 'EOF'], 'PERSON': ['Create SecretStore', 'external-secrets.io/v1beta1', 'jwt', 'Check PgBouncer', 'View']}
[10:44:13] TRACE | [fast-enricher] spaCy ORG: ['Install External Secrets Operator', 'EOF']
[10:44:13] TRACE | [fast-enricher] spaCy PERSON: ['Create SecretStore', 'external-secrets.io/v1beta1', 'jwt', 'Check PgBouncer', 'View']
[10:44:13] TRACE | [fast-enricher] spaCy NER took 46.7ms
[10:44:13] TRACE | [fast-enricher] Generated prefix: TCP port, SHOW POOLS, External Secrets, Secrets Operator, kubectl top, Egress ingress, bash kubectl | Install External Secrets Operator, EOF, Create SecretStore, external-secrets.io/v1beta1
[10:44:13] DEBUG | [fast-enricher] Enriched in 56.9ms | keywords=10 entities=7 | prefix_len=189
[10:44:13] TRACE | [enriched-hybrid] CACHE MISS: enriched chunk (total enrichments: 27)
[10:44:13] TRACE | [fast-enricher] INPUT [226 chars]: UTC - **Secondary**: Wednesday 10:00-12:00 UTC All deployments should target these windows to minimize user impact. --- **Document Version**: 2.4.0 **Maintained by**: Platform Engineering Team **Last Review**: January 24, 2026...
[10:44:13] DEBUG | [fast-enricher] YAKE extracted 10 keywords: ['user impact', 'UTC', 'Secondary', 'Wednesday', 'Document Version']...
[10:44:13] TRACE | [fast-enricher] YAKE keywords with scores: ['user impact(0.033)', 'UTC(0.039)', 'Secondary(0.039)', 'Wednesday(0.039)', 'Document Version(0.045)', 'minimize user(0.054)', 'Platform Engineering(0.063)', 'Engineering Team(0.063)', 'January(0.125)', 'impact(0.141)']
[10:44:13] TRACE | [fast-enricher] YAKE extraction took 3.9ms
[10:44:13] DEBUG | [fast-enricher] spaCy extracted 2 entities: {'ORG': ['UTC', 'Platform Engineering Team *']}
[10:44:13] TRACE | [fast-enricher] spaCy ORG: ['UTC', 'Platform Engineering Team *']
[10:44:13] TRACE | [fast-enricher] spaCy NER took 9.3ms
[10:44:13] TRACE | [fast-enricher] Generated prefix: user impact, UTC, Secondary, Wednesday, Document Version, minimize user, Platform Engineering | UTC, Platform Engineering Team *
[10:44:13] DEBUG | [fast-enricher] Enriched in 13.4ms | keywords=10 entities=2 | prefix_len=128
[10:44:13] TRACE | [enriched-hybrid] CACHE MISS: enriched chunk (total enrichments: 28)
[10:44:13] TRACE | [fast-enricher] INPUT [2666 chars]: # CloudFlow Platform Troubleshooting Guide **Version:** 3.2.1 **Last Updated:** January 2026 **Audience:** Platform Engineers, DevOps, Support Teams ## Table of Contents 1. [Overview](#overview) 2. [Authentication & Authorization Issues](#authentication--authorization-issues) 3. [Performance Problems](#performance-problems) 4. [Database Connection Issues](#database-connection-issues) 5. [Workflow Execution Failures](#workflow-execution-failures) 6. [Rate Limiting & Throttling](#rate-limiting--th...
[10:44:13] DEBUG | [fast-enricher] YAKE extracted 10 keywords: ['Support Teams', 'Platform Engineers', 'Version', 'January', 'Audience']...
[10:44:13] TRACE | [fast-enricher] YAKE keywords with scores: ['Support Teams(0.005)', 'Platform Engineers(0.037)', 'Version(0.058)', 'January(0.058)', 'Audience(0.058)', 'Updated(0.070)', 'Engineers(0.070)', 'Support(0.070)', 'Teams(0.070)', 'Table(0.070)']
[10:44:13] TRACE | [fast-enricher] YAKE extraction took 9.8ms
[10:44:13] DEBUG | [fast-enricher] spaCy extracted 8 entities: {'ORG': ['DevOps', 'Support Teams', 'Authentication & Authorization Issues](#authentication', 'Rate Limiting & Throttling](#rate', 'Log Analysis &'], 'LAW': ['Contents 1'], 'PERSON': ['Quick Diagnostic', 'Invalid JWT Signature']}
[10:44:13] TRACE | [fast-enricher] spaCy ORG: ['DevOps', 'Support Teams', 'Authentication & Authorization Issues](#authentication', 'Rate Limiting & Throttling](#rate', 'Log Analysis &']
[10:44:13] TRACE | [fast-enricher] spaCy LAW: ['Contents 1']
[10:44:13] TRACE | [fast-enricher] spaCy PERSON: ['Quick Diagnostic', 'Invalid JWT Signature']
[10:44:13] TRACE | [fast-enricher] spaCy NER took 43.2ms
[10:44:13] TRACE | [fast-enricher] Generated prefix: Support Teams, Platform Engineers, Version, January, Audience, Updated, Engineers | DevOps, Support Teams, Contents 1, Quick Diagnostic, Invalid JWT Signature
[10:44:13] DEBUG | [fast-enricher] Enriched in 53.2ms | keywords=10 entities=8 | prefix_len=158
[10:44:13] TRACE | [enriched-hybrid] CACHE MISS: enriched chunk (total enrichments: 29)
[10:44:13] TRACE | [fast-enricher] INPUT [2340 chars]: **Diagnosis:** ```bash # Check system time timedatectl status # Compare with NTP server ntpdate -q pool.ntp.org # Check JWT issued time vs current time jwt_iat=$(echo $CF_ACCESS_TOKEN | cut -d'.' -f2 | base64 -d | jq '.iat') current_time=$(date +%s) skew=$((current_time - jwt_iat)) echo "Clock skew: $skew seconds" ``` **Resolution:** ```bash # Sync with NTP server sudo ntpdate -s pool.ntp.org # Enable automatic time sync sudo timedatectl set-ntp true # Restart CloudFlow client cloudflow restart ...
[10:44:13] DEBUG | [fast-enricher] YAKE extracted 10 keywords: ['Diagnosis Steps', 'Resolution', 'Forbidden Errors', 'Timeout errors', 'RBAC Policy']...
[10:44:13] TRACE | [fast-enricher] YAKE keywords with scores: ['Diagnosis Steps(0.031)', 'Resolution(0.048)', 'Forbidden Errors(0.055)', 'Timeout errors(0.055)', 'RBAC Policy(0.059)', 'Performance Problems(0.066)', 'Diagnosis(0.072)', 'Policy Violations(0.074)', 'Database query(0.088)', 'Execute workflows(0.088)']
[10:44:13] TRACE | [fast-enricher] YAKE extraction took 7.5ms
[10:44:13] DEBUG | [fast-enricher] spaCy extracted 3 entities: {'ORG': ['RBAC', 'UI'], 'PERSON': ['Identify Slow Queries']}
[10:44:13] TRACE | [fast-enricher] spaCy ORG: ['RBAC', 'UI']
[10:44:13] TRACE | [fast-enricher] spaCy PERSON: ['Identify Slow Queries']
[10:44:13] TRACE | [fast-enricher] spaCy NER took 23.2ms
[10:44:13] TRACE | [fast-enricher] Generated prefix: Diagnosis Steps, Resolution, Forbidden Errors, Timeout errors, RBAC Policy, Performance Problems, Diagnosis | RBAC, UI, Identify Slow Queries
[10:44:13] DEBUG | [fast-enricher] Enriched in 30.8ms | keywords=10 entities=3 | prefix_len=141
[10:44:13] TRACE | [enriched-hybrid] CACHE MISS: enriched chunk (total enrichments: 30)
[10:44:13] TRACE | [fast-enricher] INPUT [3029 chars]: Review Query Execution Plans** ```sql -- Connect to CloudFlow database cloudflow db connect --readonly -- Explain slow query EXPLAIN ANALYZE SELECT w.*, e.status, e.error_message FROM workflows w LEFT JOIN executions e ON w.id = e.workflow_id WHERE w.workspace_id = 'ws_abc123' AND e.created_at > NOW() - INTERVAL '7 days' ORDER BY e.created_at DESC; -- Check for missing indexes SELECT schemaname, tablename, indexname, indexdef FROM pg_indexes WHERE tablename IN ('workflows', 'executions', 'workfl...
[10:44:13] DEBUG | [fast-enricher] YAKE extracted 10 keywords: ['Execution Plans', 'Query Execution', 'Review Query', 'Memory Leaks', 'High API']...
[10:44:13] TRACE | [fast-enricher] YAKE keywords with scores: ['Execution Plans(0.006)', 'Query Execution(0.022)', 'Review Query(0.034)', 'Memory Leaks(0.038)', 'High API(0.050)', 'Breakdown Analysis(0.050)', 'Plans(0.064)', 'Optimize Queries(0.071)', 'Network latency(0.075)', 'Execution(0.086)']
[10:44:13] TRACE | [fast-enricher] YAKE extraction took 4.8ms
[10:44:13] DEBUG | [fast-enricher] spaCy extracted 1 entities: {'ORG': ['API']}
[10:44:13] TRACE | [fast-enricher] spaCy ORG: ['API']
[10:44:13] TRACE | [fast-enricher] spaCy NER took 12.8ms
[10:44:13] TRACE | [fast-enricher] Generated prefix: Execution Plans, Query Execution, Review Query, Memory Leaks, High API, Breakdown Analysis, Plans | API
[10:44:13] DEBUG | [fast-enricher] Enriched in 17.8ms | keywords=10 entities=1 | prefix_len=103
[10:44:13] TRACE | [enriched-hybrid] CACHE MISS: enriched chunk (total enrichments: 31)
[10:44:13] TRACE | [fast-enricher] INPUT [2824 chars]: Workflow Context Accumulation** Large workflow executions may accumulate state in memory. **Solution:** ```bash # Configure context cleanup cloudflow config set workflow.context.max_size_mb 100 cloudflow config set workflow.context.cleanup_threshold 0.8 # Enable context persistence to disk cloudflow config set workflow.context.persistence.enabled true cloudflow config set workflow.context.persistence.backend s3 ``` **2. Connection Pool Leaks** **Diagnosis:** ```bash # Check active connections cl...
[10:44:13] DEBUG | [fast-enricher] YAKE extracted 10 keywords: ['Context Accumulation', 'Large workflow', 'Workflow Context', 'accumulate state', 'workflow executions']...
[10:44:13] TRACE | [fast-enricher] YAKE keywords with scores: ['Context Accumulation(0.006)', 'Large workflow(0.017)', 'Workflow Context(0.022)', 'accumulate state(0.058)', 'workflow executions(0.060)', 'Accumulation(0.070)', 'Large(0.070)', 'Context(0.088)', 'Resolution(0.093)', 'Connection Pool(0.099)']
[10:44:13] TRACE | [fast-enricher] YAKE extraction took 6.2ms
[10:44:13] DEBUG | [fast-enricher] spaCy extracted 0 entities: {}
[10:44:13] TRACE | [fast-enricher] spaCy NER took 17.6ms
[10:44:13] TRACE | [fast-enricher] Generated prefix: Context Accumulation, Large workflow, Workflow Context, accumulate state, workflow executions, Accumulation, Large
[10:44:13] DEBUG | [fast-enricher] Enriched in 24.0ms | keywords=10 entities=0 | prefix_len=114
[10:44:13] TRACE | [enriched-hybrid] CACHE MISS: enriched chunk (total enrichments: 32)
[10:44:13] TRACE | [fast-enricher] INPUT [2964 chars]: **Implement connection pooling optimization:** ```bash # Use PgBouncer for connection pooling kubectl apply -f cloudflow-pgbouncer.yaml # Configure CloudFlow to use PgBouncer cloudflow config set db.host pgbouncer.cloudflow.svc.cluster.local cloudflow config set db.port 6432 cloudflow config set db.pool.mode transaction ``` 2. **Add read replicas:** ```bash # Route read-only queries to replicas cloudflow db replicas add --count 2 cloudflow config set db.read_replicas "replica-1:5432,replica-2:54...
[10:44:13] DEBUG | [fast-enricher] YAKE extracted 10 keywords: ['Timeout Errors', 'Error Messages', 'pooling optimization', 'Duration', 'Status']...
[10:44:13] TRACE | [fast-enricher] YAKE keywords with scores: ['Timeout Errors(0.024)', 'Error Messages(0.031)', 'pooling optimization(0.031)', 'Duration(0.031)', 'Status(0.031)', 'Implement connection(0.045)', 'cloudflow workflows(0.054)', 'connection pooling(0.054)', 'Maximum Connection(0.054)', 'data(0.057)']
[10:44:13] TRACE | [fast-enricher] YAKE extraction took 8.1ms
[10:44:13] DEBUG | [fast-enricher] spaCy extracted 2 entities: {'ORG': ['CloudFlow'], 'PERSON': ['Sample']}
[10:44:13] TRACE | [fast-enricher] spaCy ORG: ['CloudFlow']
[10:44:13] TRACE | [fast-enricher] spaCy PERSON: ['Sample']
[10:44:13] TRACE | [fast-enricher] spaCy NER took 24.9ms
[10:44:13] TRACE | [fast-enricher] Generated prefix: Timeout Errors, Error Messages, pooling optimization, Duration, Status, Implement connection, cloudflow workflows | CloudFlow, Sample
[10:44:13] DEBUG | [fast-enricher] Enriched in 33.2ms | keywords=10 entities=2 | prefix_len=133
[10:44:13] TRACE | [enriched-hybrid] CACHE MISS: enriched chunk (total enrichments: 33)
[10:44:13] TRACE | [fast-enricher] INPUT [2573 chars]: exec_7h3j6k9m2n --show-bottlenecks ``` #### Solutions **1. Increase workflow timeout (if justified):** ```bash # Update workflow configuration cloudflow workflows update wf_9k2n4m8p1q \ --timeout 7200 \ --reason "Large dataset processing requires extended time" # Verify update cloudflow workflows get wf_9k2n4m8p1q | grep timeout ``` **2. Optimize slow steps:** ```bash # Enable parallel processing cloudflow workflows update wf_9k2n4m8p1q \ --step data_transformation \ --parallel-workers 8 \ --bat...
[10:44:13] DEBUG | [fast-enricher] YAKE extracted 10 keywords: ['cloudflow workflows', 'workflows update', 'Connection timeout', 'workflows', 'Attempt']...
[10:44:13] TRACE | [fast-enricher] YAKE keywords with scores: ['cloudflow workflows(0.008)', 'workflows update(0.013)', 'Connection timeout(0.018)', 'workflows(0.022)', 'Attempt(0.022)', 'cloudflow(0.027)', 'timeout(0.032)', 'Update(0.033)', 'failed(0.037)', 'step data(0.039)']
[10:44:13] TRACE | [fast-enricher] YAKE extraction took 10.7ms
[10:44:13] DEBUG | [fast-enricher] spaCy extracted 11 entities: {'PERSON': ['Add', 'Max', 'View', 'NETWORK_ERROR', 'List'], 'ORG': ['Exponential Backoff'], 'LAW': ['Attempt 1:', 'Attempt 1: FAILED - NetworkError:', 'Attempt 2: FAILED - NetworkError:', 'Attempt 3: FAILED - NetworkError:', 'Attempt 4:']}
[10:44:13] TRACE | [fast-enricher] spaCy PERSON: ['Add', 'Max', 'View', 'NETWORK_ERROR', 'List']
[10:44:13] TRACE | [fast-enricher] spaCy ORG: ['Exponential Backoff']
[10:44:13] TRACE | [fast-enricher] spaCy LAW: ['Attempt 1:', 'Attempt 1: FAILED - NetworkError:', 'Attempt 2: FAILED - NetworkError:', 'Attempt 3: FAILED - NetworkError:', 'Attempt 4:']
[10:44:13] TRACE | [fast-enricher] spaCy NER took 60.8ms
[10:44:13] TRACE | [fast-enricher] Generated prefix: cloudflow workflows, workflows update, Connection timeout, workflows, Attempt, cloudflow, timeout | Add, Max, Exponential Backoff, Attempt 1:, Attempt 1: FAILED - NetworkError:
[10:44:13] DEBUG | [fast-enricher] Enriched in 71.7ms | keywords=10 entities=11 | prefix_len=176
[10:44:13] TRACE | [enriched-hybrid] CACHE MISS: enriched chunk (total enrichments: 34)
[10:44:13] TRACE | [fast-enricher] INPUT [2620 chars]: Data Validation Errors** ``` ValidationError: Field 'customer_id' is required but missing in 234 records ``` **Resolution:** ```bash # Add data quality checks cloudflow workflows update wf_9k2n4m8p1q \ --step data_ingestion \ --add-validator required_fields \ --validator-config '{"fields": ["customer_id", "timestamp", "amount"]}' # Configure error handling cloudflow workflows update wf_9k2n4m8p1q \ --step data_validation \ --on-error continue \ --error-threshold 5% # Fail if > 5% of records inva...
[10:44:13] DEBUG | [fast-enricher] YAKE extracted 10 keywords: ['Data Validation', 'Validation Errors', 'Rate Limit', 'Error Response', 'Concurrent Workflows']...
[10:44:13] TRACE | [fast-enricher] YAKE keywords with scores: ['Data Validation(0.009)', 'Validation Errors(0.010)', 'Rate Limit(0.022)', 'Error Response(0.032)', 'Concurrent Workflows(0.032)', 'Resolution(0.035)', 'Rate(0.036)', 'Limit Status(0.036)', 'Rate Limiting(0.040)', 'Checking Rate(0.040)']
[10:44:13] TRACE | [fast-enricher] YAKE extraction took 7.3ms
[10:44:13] DEBUG | [fast-enricher] spaCy extracted 9 entities: {'ORG': ['Data Validation Errors', 'Rate Limiting & Throttling', 'Premium', 'Unlimited'], 'PERSON': ['API Failures', 'Tier', 'Concurrent Workflows', 'Handling Rate Limits'], 'GPE': ['Custom']}
[10:44:13] TRACE | [fast-enricher] spaCy ORG: ['Data Validation Errors', 'Rate Limiting & Throttling', 'Premium', 'Unlimited']
[10:44:13] TRACE | [fast-enricher] spaCy PERSON: ['API Failures', 'Tier', 'Concurrent Workflows', 'Handling Rate Limits']
[10:44:13] TRACE | [fast-enricher] spaCy GPE: ['Custom']
[10:44:13] TRACE | [fast-enricher] spaCy NER took 25.9ms
[10:44:13] TRACE | [fast-enricher] Generated prefix: Data Validation, Validation Errors, Rate Limit, Error Response, Concurrent Workflows, Resolution, Rate | Data Validation Errors, Rate Limiting & Throttling, API Failures, Tier, Custom
[10:44:13] DEBUG | [fast-enricher] Enriched in 33.4ms | keywords=10 entities=9 | prefix_len=183
[10:44:13] TRACE | [enriched-hybrid] CACHE MISS: enriched chunk (total enrichments: 35)
[10:44:13] TRACE | [fast-enricher] INPUT [1572 chars]: Waiting {retry_after} seconds...") time.sleep(retry_after) continue remaining = int(response.headers.get('X-RateLimit-Remaining', 0)) if remaining < 10: print(f"Warning: Only {remaining} requests remaining") return response raise Exception("Max retries exceeded due to rate limiting") ``` **Bash script with rate limit checking:** ```bash #!/bin/bash check_rate_limit() { local remaining=$(curl -s -I https://api.cloudflow.io/api/v1/workflows \ -H "Authorization: Bearer $CF_ACCESS_TOKEN" | \ grep -i...
[10:44:13] DEBUG | [fast-enricher] YAKE extracted 10 keywords: ['Optimization Strategies', 'awk print', 'rate limit', 'raise Exception', 'Max retries']...
[10:44:13] TRACE | [fast-enricher] YAKE keywords with scores: ['Optimization Strategies(0.004)', 'awk print(0.005)', 'rate limit(0.010)', 'raise Exception(0.012)', 'Max retries(0.012)', 'rate limiting(0.020)', 'Warning(0.021)', 'Authorization(0.021)', 'Bearer(0.021)', 'ACCESS(0.021)']
[10:44:13] TRACE | [fast-enricher] YAKE extraction took 9.0ms
[10:44:13] DEBUG | [fast-enricher] spaCy extracted 3 entities: {'PERSON': ['time.sleep(retry_after', 'Batch'], 'ORG': ['API']}
[10:44:13] TRACE | [fast-enricher] spaCy PERSON: ['time.sleep(retry_after', 'Batch']
[10:44:13] TRACE | [fast-enricher] spaCy ORG: ['API']
[10:44:13] TRACE | [fast-enricher] spaCy NER took 38.2ms
[10:44:13] TRACE | [fast-enricher] Generated prefix: Optimization Strategies, awk print, rate limit, raise Exception, Max retries, rate limiting, Warning | time.sleep(retry_after, Batch, API
[10:44:13] DEBUG | [fast-enricher] Enriched in 47.4ms | keywords=10 entities=3 | prefix_len=137
[10:44:13] TRACE | [enriched-hybrid] CACHE MISS: enriched chunk (total enrichments: 36)
[10:44:13] TRACE | [fast-enricher] INPUT [2858 chars]: Leverage caching:** ```bash # Enable client-side caching export CLOUDFLOW_CACHE_ENABLED=true export CLOUDFLOW_CACHE_TTL=300 # Cache workflow metadata cloudflow workflows list --use-cache --cache-ttl 600 ``` --- ## Log Analysis & Debugging ### Accessing CloudFlow Logs #### Kubernetes Deployments ```bash # List all CloudFlow pods kubectl get pods -n cloudflow # Tail logs from API server kubectl logs -f -n cloudflow deployment/cloudflow-api --tail=100 # Get logs from specific pod kubectl logs -n cl...
[10:44:13] DEBUG | [fast-enricher] YAKE extracted 10 keywords: ['Find workflow', 'Log Levels', 'Authentication Failures', 'debugging information', 'detailed debugging']...
[10:44:13] TRACE | [fast-enricher] YAKE keywords with scores: ['Find workflow(0.002)', 'Log Levels(0.002)', 'Authentication Failures(0.002)', 'debugging information(0.002)', 'detailed debugging(0.002)', 'Common Issues(0.003)', 'Kubernetes Deployments(0.003)', 'cloudflow deployment(0.004)', 'Grep Patterns(0.004)', 'Log Analysis(0.004)']
[10:44:13] TRACE | [fast-enricher] YAKE extraction took 8.4ms
[10:44:13] DEBUG | [fast-enricher] spaCy extracted 3 entities: {'ORG': ['CloudFlow Logs', 'Log Levels'], 'PERSON': ['Find']}
[10:44:13] TRACE | [fast-enricher] spaCy ORG: ['CloudFlow Logs', 'Log Levels']
[10:44:13] TRACE | [fast-enricher] spaCy PERSON: ['Find']
[10:44:13] TRACE | [fast-enricher] spaCy NER took 21.9ms
[10:44:13] TRACE | [fast-enricher] Generated prefix: Find workflow, Log Levels, Authentication Failures, debugging information, detailed debugging, Common Issues, Kubernetes Deployments | CloudFlow Logs, Log Levels, Find
[10:44:13] DEBUG | [fast-enricher] Enriched in 30.5ms | keywords=10 entities=3 | prefix_len=167
[10:44:13] TRACE | [enriched-hybrid] CACHE MISS: enriched chunk (total enrichments: 37)
[10:44:13] TRACE | [fast-enricher] INPUT [2784 chars]: in" # Find workflow failures by ID kubectl logs -n cloudflow deployment/cloudflow-workflow-engine | \ grep "exec_7h3j6k9m2n" ``` #### Rate Limiting ```bash # Find rate limit events kubectl logs -n cloudflow deployment/cloudflow-api | \ grep -E "429|rate.*limit|throttle" # Count rate limit errors by hour kubectl logs -n cloudflow deployment/cloudflow-api --since=24h | \ grep "rate_limit_exceeded" | \ awk '{print $1}' | \ cut -d'T' -f1-2 | \ sort | uniq -c ``` ### Advanced Log Analysis #### Using ...
[10:44:13] DEBUG | [fast-enricher] YAKE extracted 10 keywords: ['cloudflow deployment', 'kubectl logs', 'Parse JSON', 'JSON Logs', 'Find rate']...
[10:44:13] TRACE | [fast-enricher] YAKE keywords with scores: ['cloudflow deployment(0.001)', 'kubectl logs(0.003)', 'Parse JSON(0.003)', 'JSON Logs(0.003)', 'Find rate(0.004)', 'Count rate(0.004)', 'rate limit(0.004)', 'Rate Limiting(0.005)', 'cloudflow(0.006)', 'Count errors(0.006)']
[10:44:13] TRACE | [fast-enricher] YAKE extraction took 12.9ms
[10:44:13] DEBUG | [fast-enricher] spaCy extracted 8 entities: {'PERSON': ['Find', 'Parse JSON', 'Filter', 'Trace', 'CORRELATION_ID'], 'ORG': ['CloudFlow', 'TRACE', 'DNS']}
[10:44:13] TRACE | [fast-enricher] spaCy PERSON: ['Find', 'Parse JSON', 'Filter', 'Trace', 'CORRELATION_ID']
[10:44:13] TRACE | [fast-enricher] spaCy ORG: ['CloudFlow', 'TRACE', 'DNS']
[10:44:13] TRACE | [fast-enricher] spaCy NER took 64.0ms
[10:44:13] TRACE | [fast-enricher] Generated prefix: cloudflow deployment, kubectl logs, Parse JSON, JSON Logs, Find rate, Count rate, rate limit | Find, Parse JSON, CloudFlow, TRACE
[10:44:13] DEBUG | [fast-enricher] Enriched in 77.1ms | keywords=10 entities=8 | prefix_len=129
[10:44:13] TRACE | [enriched-hybrid] CACHE MISS: enriched chunk (total enrichments: 38)
[10:44:13] TRACE | [fast-enricher] INPUT [2799 chars]: database nc -zv cloudflow-db.internal.company.com 5432 # Trace route traceroute api.cloudflow.io # Capture packets tcpdump -i any -w /tmp/capture.pcap port 5432 ``` --- ## Escalation Procedures ### Severity Levels CloudFlow incidents are classified into four severity levels: #### SEV-1: Critical (P1) - **Definition:** Complete service outage or severe degradation affecting all users - **Examples:** - API returns 5xx errors for > 5 minutes - Database completely unavailable - Authentication system...
[10:44:13] DEBUG | [fast-enricher] YAKE extracted 10 keywords: ['health check', 'Run health', 'Capture packets', 'Capture system', 'metrics dashboard']...
[10:44:13] TRACE | [fast-enricher] YAKE keywords with scores: ['health check(0.003)', 'Run health(0.004)', 'Capture packets(0.004)', 'Capture system(0.004)', 'metrics dashboard(0.005)', 'Trace route(0.005)', 'Review recent(0.005)', 'Complete API(0.005)', 'status page(0.005)', 'API outage(0.005)']
[10:44:13] TRACE | [fast-enricher] YAKE extraction took 7.1ms
[10:44:13] DEBUG | [fast-enricher] spaCy extracted 7 entities: {'PERSON': ['database nc -zv cloudflow-db.internal.company.com 5432 #', 'Run', 'Generate', 'Page'], 'ORG': ['Trace', 'API', 'Notify']}
[10:44:13] TRACE | [fast-enricher] spaCy PERSON: ['database nc -zv cloudflow-db.internal.company.com 5432 #', 'Run', 'Generate', 'Page']
[10:44:13] TRACE | [fast-enricher] spaCy ORG: ['Trace', 'API', 'Notify']
[10:44:13] TRACE | [fast-enricher] spaCy NER took 21.1ms
[10:44:13] TRACE | [fast-enricher] Generated prefix: health check, Run health, Capture packets, Capture system, metrics dashboard, Trace route, Review recent | database nc -zv cloudflow-db.internal.company.com 5432 #, Run, Trace, API
[10:44:13] DEBUG | [fast-enricher] Enriched in 28.5ms | keywords=10 entities=7 | prefix_len=180
[10:44:13] TRACE | [enriched-hybrid] CACHE MISS: enriched chunk (total enrichments: 39)
[10:44:13] TRACE | [fast-enricher] INPUT [2166 chars]: outage" # Create war room cloudflow incident war-room create incident-2024012401 ``` **For SEV-2 (High):** ```bash # Create incident and notify cloudflow incident create \ --severity SEV-2 \ --title "High rate of workflow failures" \ --description "Workflow success rate dropped to 75%" \ --notify-oncall # Update status page cloudflow status update \ --status "partial_outage" \ --message "We are experiencing elevated error rates" ``` **For SEV-3/SEV-4:** ```bash # Create support ticket cloudflow ...
[10:44:13] DEBUG | [fast-enricher] YAKE extracted 10 keywords: ['priority MEDIUM', 'Required sections', 'High rate', 'Generate postmortem', 'Intermittent timeout']...
[10:44:13] TRACE | [fast-enricher] YAKE keywords with scores: ['priority MEDIUM(0.015)', 'Required sections(0.015)', 'High rate(0.017)', 'Generate postmortem(0.017)', 'Intermittent timeout(0.019)', 'Create(0.022)', 'bash(0.023)', 'ticket create(0.026)', 'workflow failures(0.027)', 'Update status(0.029)']
[10:44:13] TRACE | [fast-enricher] YAKE extraction took 9.7ms
[10:44:13] DEBUG | [fast-enricher] spaCy extracted 2 entities: {'PERSON': ['Generate', 'incident-2024012401 \\']}
[10:44:13] TRACE | [fast-enricher] spaCy PERSON: ['Generate', 'incident-2024012401 \\']
[10:44:13] TRACE | [fast-enricher] spaCy NER took 29.4ms
[10:44:13] TRACE | [fast-enricher] Generated prefix: priority MEDIUM, Required sections, High rate, Generate postmortem, Intermittent timeout, Create, bash | Generate, incident-2024012401 \
[10:44:13] DEBUG | [fast-enricher] Enriched in 39.3ms | keywords=10 entities=2 | prefix_len=136
[10:44:13] TRACE | [enriched-hybrid] CACHE MISS: enriched chunk (total enrichments: 40)
[10:44:13] TRACE | [fast-enricher] INPUT [2617 chars]: # CloudFlow User Guide Welcome to CloudFlow, the modern workflow automation platform that helps you connect your apps, automate repetitive tasks, and build powerful integrations without writing code. ## Table of Contents 1. [Getting Started](#getting-started) 2. [Workflow Creation](#workflow-creation) 3. [Available Actions](#available-actions) 4. [Variables and Expressions](#variables-and-expressions) 5. [Scheduling](#scheduling) 6. [Error Handling](#error-handling) 7. [Workflow Limits](#workflo...
[10:44:13] DEBUG | [fast-enricher] YAKE extracted 10 keywords: ['Google Drive', 'workflow', 'User Guide', 'Visual Editor', 'Google']...
[10:44:13] TRACE | [fast-enricher] YAKE keywords with scores: ['Google Drive(0.019)', 'workflow(0.028)', 'User Guide(0.039)', 'Visual Editor(0.053)', 'Google(0.058)', 'automate repetitive(0.058)', 'repetitive tasks(0.058)', 'Drive(0.061)', 'automation platform(0.063)', 'build powerful(0.063)']
[10:44:13] TRACE | [fast-enricher] YAKE extraction took 15.4ms
[10:44:13] DEBUG | [fast-enricher] spaCy extracted 10 entities: {'ORG': ['CloudFlow', 'Slack'], 'LAW': ['Contents 1'], 'PERSON': ['Workflow Creation](#workflow-creation', 'Common Workflow Patterns](#common-workflow'], 'FAC': ['Google Drive', 'Google Drive - New File'], 'WORK_OF_ART': ['Google Drive to', 'Slack - Send Message'], 'PRODUCT': ['Google Drive']}
[10:44:13] TRACE | [fast-enricher] spaCy ORG: ['CloudFlow', 'Slack']
[10:44:13] TRACE | [fast-enricher] spaCy LAW: ['Contents 1']
[10:44:13] TRACE | [fast-enricher] spaCy PERSON: ['Workflow Creation](#workflow-creation', 'Common Workflow Patterns](#common-workflow']
[10:44:13] TRACE | [fast-enricher] spaCy FAC: ['Google Drive', 'Google Drive - New File']
[10:44:13] TRACE | [fast-enricher] spaCy WORK_OF_ART: ['Google Drive to', 'Slack - Send Message']
[10:44:13] TRACE | [fast-enricher] spaCy PRODUCT: ['Google Drive']
[10:44:13] TRACE | [fast-enricher] spaCy NER took 62.9ms
[10:44:13] TRACE | [fast-enricher] Generated prefix: Google Drive, workflow, User Guide, Visual Editor, Google, automate repetitive, repetitive tasks | CloudFlow, Slack, Contents 1, Workflow Creation](#workflow-creation, Common Workflow Patterns](#common-workflow
[10:44:13] DEBUG | [fast-enricher] Enriched in 78.5ms | keywords=10 entities=10 | prefix_len=210
[10:44:13] TRACE | [enriched-hybrid] CACHE MISS: enriched chunk (total enrichments: 41)
[10:44:13] TRACE | [fast-enricher] INPUT [2606 chars]: Open the Visual Editor by clicking **"Create Workflow"** or editing an existing workflow 2. Add a trigger by clicking the **"Add Trigger"** button 3. Configure your trigger settings in the right panel 4. Add actions by clicking the **"+"** button below any step 5. Connect conditional branches by adding **"Condition"** blocks 6. Use the **"Test"** button to validate your workflow with sample data ### YAML Definition For advanced users and version control integration, CloudFlow supports YAML-based...
[10:44:13] DEBUG | [fast-enricher] YAKE extracted 10 keywords: ['Visual Editor', 'Create Workflow', 'Triggers', 'Workflow', 'Create']...
[10:44:13] TRACE | [fast-enricher] YAKE keywords with scores: ['Visual Editor(0.005)', 'Create Workflow(0.036)', 'Triggers(0.058)', 'Workflow(0.065)', 'Create(0.067)', 'Visual(0.073)', 'Editor(0.073)', 'YAML(0.077)', 'clicking(0.085)', 'trigger(0.086)']
[10:44:13] TRACE | [fast-enricher] YAKE extraction took 9.7ms
[10:44:13] DEBUG | [fast-enricher] spaCy extracted 10 entities: {'ORG': ['CloudFlow', 'YAML', 'POST', 'PUT', 'Gmail, Outlook) -'], 'PERSON': ['Git', 'JSON', '[Scheduling](#scheduling'], 'FAC': ['Google Drive'], 'GPE': ['Dropbox']}
[10:44:13] TRACE | [fast-enricher] spaCy ORG: ['CloudFlow', 'YAML', 'POST', 'PUT', 'Gmail, Outlook) -']
[10:44:13] TRACE | [fast-enricher] spaCy PERSON: ['Git', 'JSON', '[Scheduling](#scheduling']
[10:44:13] TRACE | [fast-enricher] spaCy FAC: ['Google Drive']
[10:44:13] TRACE | [fast-enricher] spaCy GPE: ['Dropbox']
[10:44:13] TRACE | [fast-enricher] spaCy NER took 43.4ms
[10:44:13] TRACE | [fast-enricher] Generated prefix: Visual Editor, Create Workflow, Triggers, Workflow, Create, Visual, Editor | CloudFlow, YAML, Git, JSON, Google Drive
[10:44:13] DEBUG | [fast-enricher] Enriched in 53.3ms | keywords=10 entities=10 | prefix_len=117
[10:44:13] TRACE | [enriched-hybrid] CACHE MISS: enriched chunk (total enrichments: 42)
[10:44:13] TRACE | [fast-enricher] INPUT [3147 chars]: ### HTTP Requests Make HTTP requests to any API endpoint: **Configuration:** - **Method**: GET, POST, PUT, PATCH, DELETE, HEAD - **URL**: Full endpoint URL (supports variable interpolation) - **Headers**: Custom headers as key-value pairs - **Query Parameters**: URL parameters - **Body**: JSON, form data, or raw text - **Authentication**: Basic Auth, Bearer Token, API Key, OAuth 2.0 **Example:** ```yaml - id: fetch_user action: http_request config: method: GET url: "https://api.example.com/users...
[10:44:13] DEBUG | [fast-enricher] YAKE extracted 10 keywords: ['Basic Auth', 'Bearer Token', 'Important Notes', 'HTTP Requests', 'Query Parameters']...
[10:44:13] TRACE | [fast-enricher] YAKE keywords with scores: ['Basic Auth(0.002)', 'Bearer Token(0.002)', 'Important Notes(0.002)', 'HTTP Requests(0.003)', 'Query Parameters(0.003)', 'API Key(0.003)', 'Make HTTP(0.004)', 'Notifications Send(0.004)', 'Response Handling(0.005)', 'Requests Make(0.005)']
[10:44:13] TRACE | [fast-enricher] YAKE extraction took 9.4ms
[10:44:13] DEBUG | [fast-enricher] spaCy extracted 9 entities: {'ORG': ['API', 'POST', 'PUT', 'SQL', 'SMTP'], 'PERSON': ['Bearer Token', 'API Key', 'Redis', 'Email Notifications Send']}
[10:44:13] TRACE | [fast-enricher] spaCy ORG: ['API', 'POST', 'PUT', 'SQL', 'SMTP']
[10:44:13] TRACE | [fast-enricher] spaCy PERSON: ['Bearer Token', 'API Key', 'Redis', 'Email Notifications Send']
[10:44:13] TRACE | [fast-enricher] spaCy NER took 39.3ms
[10:44:13] TRACE | [fast-enricher] Generated prefix: Basic Auth, Bearer Token, Important Notes, HTTP Requests, Query Parameters, API Key, Make HTTP | API, POST, Bearer Token, API Key
[10:44:13] DEBUG | [fast-enricher] Enriched in 48.9ms | keywords=10 entities=9 | prefix_len=129
[10:44:13] TRACE | [enriched-hybrid] CACHE MISS: enriched chunk (total enrichments: 43)
[10:44:13] TRACE | [fast-enricher] INPUT [2895 chars]: ``` **Slack App Integration:** - Install the CloudFlow Slack app in your workspace - Authenticate once per workspace - Use @mentions, emojis, and rich formatting - Send messages as a bot or as yourself ## Variables and Expressions CloudFlow uses a powerful templating system to work with dynamic data in your workflows. ### Variable Syntax Variables are enclosed in double curly braces: `{{variable_name}}` **Accessing Trigger Data:** ``` {{trigger.body.order_id}} {{trigger.headers.user_agent}} {{tr...
[10:44:13] DEBUG | [fast-enricher] YAKE extracted 10 keywords: ['Current execution', 'Current timestamp', 'Due Date', 'Current workflow', 'Order Status']...
[10:44:13] TRACE | [fast-enricher] YAKE keywords with scores: ['Current execution(0.002)', 'Current timestamp(0.002)', 'Due Date(0.002)', 'Current workflow(0.002)', 'Order Status(0.003)', 'Extract substring(0.003)', 'Format date(0.003)', 'Current time(0.003)', 'Remove whitespace(0.003)', 'Remove duplicates(0.003)']
[10:44:13] TRACE | [fast-enricher] YAKE extraction took 11.2ms
[10:44:14] DEBUG | [fast-enricher] spaCy extracted 11 entities: {'PERSON': ['Workflow', 'Convert', 'Remove', 'Split', 'Format'], 'ORG': ['substring(text', 'num2'], 'LAW': ['ISO 8601'], 'GPE': ['MM', 'YYYY'], 'PRODUCT': ['Round']}
[10:44:14] TRACE | [fast-enricher] spaCy PERSON: ['Workflow', 'Convert', 'Remove', 'Split', 'Format']
[10:44:14] TRACE | [fast-enricher] spaCy ORG: ['substring(text', 'num2']
[10:44:14] TRACE | [fast-enricher] spaCy LAW: ['ISO 8601']
[10:44:14] TRACE | [fast-enricher] spaCy GPE: ['MM', 'YYYY']
[10:44:14] TRACE | [fast-enricher] spaCy PRODUCT: ['Round']
[10:44:14] TRACE | [fast-enricher] spaCy NER took 57.5ms
[10:44:14] TRACE | [fast-enricher] Generated prefix: Current execution, Current timestamp, Due Date, Current workflow, Order Status, Extract substring, Format date | Workflow, Convert, substring(text, num2, ISO 8601
[10:44:14] DEBUG | [fast-enricher] Enriched in 68.9ms | keywords=10 entities=11 | prefix_len=162
[10:44:14] TRACE | [enriched-hybrid] CACHE MISS: enriched chunk (total enrichments: 44)
[10:44:14] TRACE | [fast-enricher] INPUT [2102 chars]: ### Cron Syntax Use standard cron expressions to define schedules: ``` * * * * * ┬ ┬ ┬ ┬ ┬ │ │ │ │ │ │ │ │ │ └─── Day of Week (0-6, Sunday=0) │ │ │ └──────── Month (1-12) │ │ └───────────── Day of Month (1-31) │ └────────────────── Hour (0-23) └─────────────────────── Minute (0-59) ``` **Common Cron Patterns:** | Pattern | Description | |---------|-------------| | `*/5 * * * *` | Every 5 minutes | | `0 * * * *` | Every hour at minute 0 | | `0 9 * * *` | Daily at 9:00 AM | | `0 9 * * 1` | Every M...
[10:44:14] DEBUG | [fast-enricher] YAKE extracted 10 keywords: ['Cron Patterns', 'Day', 'business hours', 'Common Cron', 'Month']...
[10:44:14] TRACE | [fast-enricher] YAKE keywords with scores: ['Cron Patterns(0.019)', 'Day(0.021)', 'business hours(0.022)', 'Common Cron(0.023)', 'Month(0.024)', 'Cron Syntax(0.025)', 'Hour(0.031)', 'Sunday(0.032)', 'Minute(0.035)', 'Timezone(0.035)']
[10:44:14] TRACE | [fast-enricher] YAKE extraction took 11.0ms
[10:44:14] DEBUG | [fast-enricher] spaCy extracted 12 entities: {'PERSON': ['Cron Syntax', '┬ ┬ ┬ ┬ ┬', 'Convert', 'IANA'], 'ORG': ['UTC', 'GMT/BST', 'CloudFlow', 'DST'], 'EVENT': ['America/New_York'], 'GPE': ['Australia'], 'NORP': ['Australian'], 'WORK_OF_ART': ['Trigger']}
[10:44:14] TRACE | [fast-enricher] spaCy PERSON: ['Cron Syntax', '┬ ┬ ┬ ┬ ┬', 'Convert', 'IANA']
[10:44:14] TRACE | [fast-enricher] spaCy ORG: ['UTC', 'GMT/BST', 'CloudFlow', 'DST']
[10:44:14] TRACE | [fast-enricher] spaCy EVENT: ['America/New_York']
[10:44:14] TRACE | [fast-enricher] spaCy GPE: ['Australia']
[10:44:14] TRACE | [fast-enricher] spaCy NORP: ['Australian']
[10:44:14] TRACE | [fast-enricher] spaCy WORK_OF_ART: ['Trigger']
[10:44:14] TRACE | [fast-enricher] spaCy NER took 68.5ms
[10:44:14] TRACE | [fast-enricher] Generated prefix: Cron Patterns, Day, business hours, Common Cron, Month, Cron Syntax, Hour | Cron Syntax, ┬ ┬ ┬ ┬ ┬, UTC, GMT/BST, America/New_York
[10:44:14] DEBUG | [fast-enricher] Enriched in 79.7ms | keywords=10 entities=12 | prefix_len=130
[10:44:14] TRACE | [enriched-hybrid] CACHE MISS: enriched chunk (total enrichments: 45)
[10:44:14] TRACE | [fast-enricher] INPUT [2733 chars]: Save and activate **Testing Schedules:** Use the built-in schedule calculator to preview upcoming executions: ``` Next 5 executions: 1. 2026-01-24 14:00:00 UTC 2. 2026-01-25 14:00:00 UTC 3. 2026-01-26 14:00:00 UTC 4. 2026-01-27 14:00:00 UTC 5. 2026-01-28 14:00:00 UTC ``` ## Error Handling Robust error handling ensures your workflows are resilient and reliable. ### Retry Policies Configure automatic retries for failed actions: ```yaml - id: api_call action: http_request config: url: "https://api....
[10:44:14] DEBUG | [fast-enricher] YAKE extracted 10 keywords: ['Testing Schedules', 'Handling Robust', 'Attempt', 'Error Handling', 'Dead Letter']...
[10:44:14] TRACE | [fast-enricher] YAKE keywords with scores: ['Testing Schedules(0.006)', 'Handling Robust(0.006)', 'Attempt(0.006)', 'Error Handling(0.008)', 'Dead Letter(0.009)', 'Letter Queue(0.009)', 'Robust error(0.013)', 'Wait(0.014)', 'handling ensures(0.015)', 'built-in schedule(0.016)']
[10:44:14] TRACE | [fast-enricher] YAKE extraction took 10.2ms
[10:44:14] DEBUG | [fast-enricher] spaCy extracted 5 entities: {'ORG': ['Fallback Actions Execute', 'CloudFlow', 'Dead Letter Queue', 'DLQ'], 'PERSON': ['Queue']}
[10:44:14] TRACE | [fast-enricher] spaCy ORG: ['Fallback Actions Execute', 'CloudFlow', 'Dead Letter Queue', 'DLQ']
[10:44:14] TRACE | [fast-enricher] spaCy PERSON: ['Queue']
[10:44:14] TRACE | [fast-enricher] spaCy NER took 39.3ms
[10:44:14] TRACE | [fast-enricher] Generated prefix: Testing Schedules, Handling Robust, Attempt, Error Handling, Dead Letter, Letter Queue, Robust error | Fallback Actions Execute, CloudFlow, Queue
[10:44:14] DEBUG | [fast-enricher] Enriched in 49.8ms | keywords=10 entities=5 | prefix_len=145
[10:44:14] TRACE | [enriched-hybrid] CACHE MISS: enriched chunk (total enrichments: 46)
[10:44:14] TRACE | [fast-enricher] INPUT [2580 chars]: Filter by error type, date range, or execution ID 3. Click an execution to view full details 4. Click **"Retry"** to reprocess with the same input data ### Error Notifications Get notified when workflows fail: ```yaml workflow: notifications: on_failure: - type: email to: "ops-team@company.com" - type: slack channel: "#alerts" message: "Workflow {{workflow.name}} failed: {{error.message}}" on_success_after_retry: - type: slack channel: "#monitoring" message: "Workflow recovered after {{error.att...
[10:44:14] DEBUG | [fast-enricher] YAKE extracted 10 keywords: ['workflow', 'date range', 'type', 'workflows', 'error type']...
[10:44:14] TRACE | [fast-enricher] YAKE keywords with scores: ['workflow(0.036)', 'date range(0.037)', 'type(0.064)', 'workflows(0.064)', 'error type(0.066)', 'execution(0.081)', 'Execution Timeout(0.087)', 'Steps(0.088)', 'Error Notifications(0.089)', 'Limits(0.098)']
[10:44:14] TRACE | [fast-enricher] YAKE extraction took 9.8ms
[10:44:14] DEBUG | [fast-enricher] spaCy extracted 2 entities: {'ORG': ['MB'], 'WORK_OF_ART': ['Sync Customer Data']}
[10:44:14] TRACE | [fast-enricher] spaCy ORG: ['MB']
[10:44:14] TRACE | [fast-enricher] spaCy WORK_OF_ART: ['Sync Customer Data']
[10:44:14] TRACE | [fast-enricher] spaCy NER took 65.7ms
[10:44:14] TRACE | [fast-enricher] Generated prefix: workflow, date range, type, workflows, error type, execution, Execution Timeout | MB, Sync Customer Data
[10:44:14] DEBUG | [fast-enricher] Enriched in 75.6ms | keywords=10 entities=2 | prefix_len=104
[10:44:14] TRACE | [enriched-hybrid] CACHE MISS: enriched chunk (total enrichments: 47)
[10:44:14] TRACE | [fast-enricher] INPUT [2761 chars]: Handle Errors Gracefully Always implement error handling for external API calls and database operations: ```yaml - id: fetch_data action: http_request config: url: "https://api.example.com/data" retry: max_attempts: 3 backoff_type: exponential on_error: - id: log_error action: database_query config: query: "INSERT INTO error_log (workflow_id, error) VALUES ($1, $2)" parameters: - "{{workflow.id}}" - "{{error.message}}" ``` ### 3. Use Secrets for Sensitive Data Never hardcode API keys, passwords,...
[10:44:14] DEBUG | [fast-enricher] YAKE extracted 10 keywords: ['external API', 'API calls', 'Errors Gracefully', 'Store secrets', 'API keys']...
[10:44:14] TRACE | [fast-enricher] YAKE keywords with scores: ['external API(0.038)', 'API calls(0.038)', 'Errors Gracefully(0.038)', 'Store secrets(0.054)', 'API keys(0.066)', 'API(0.078)', 'Gracefully(0.078)', 'Secrets(0.084)', 'hardcode API(0.087)', 'Idempotency Keys(0.091)']
[10:44:14] TRACE | [fast-enricher] YAKE extraction took 8.3ms
[10:44:14] DEBUG | [fast-enricher] spaCy extracted 5 entities: {'ORG': ['API', 'Validate Input Data Always', 'Keep Workflows Modular Break'], 'PERSON': ['Log Add', 'Trigger']}
[10:44:14] TRACE | [fast-enricher] spaCy ORG: ['API', 'Validate Input Data Always', 'Keep Workflows Modular Break']
[10:44:14] TRACE | [fast-enricher] spaCy PERSON: ['Log Add', 'Trigger']
[10:44:14] TRACE | [fast-enricher] spaCy NER took 30.5ms
[10:44:14] TRACE | [fast-enricher] Generated prefix: external API, API calls, Errors Gracefully, Store secrets, API keys, API, Gracefully | API, Validate Input Data Always, Log Add, Trigger
[10:44:14] DEBUG | [fast-enricher] Enriched in 39.0ms | keywords=10 entities=5 | prefix_len=136
[10:44:14] TRACE | [enriched-hybrid] CACHE MISS: enriched chunk (total enrichments: 48)
[10:44:14] TRACE | [fast-enricher] INPUT [3228 chars]: Version Control For critical workflows: - Export YAML definitions regularly - Store in version control (Git) - Use pull requests for changes - Tag releases with semantic versioning ## Common Workflow Patterns Here are proven patterns for common automation scenarios: ### Pattern 1: Form to Database Capture form submissions and store in database: ```yaml name: "Contact Form to Database" trigger: type: webhook method: POST steps: - id: validate_submission action: javascript code: | if (!input.email...
[10:44:14] DEBUG | [fast-enricher] YAKE extracted 10 keywords: ['Polling Periodically', 'API Polling', 'Version Control', 'Expense Approval', 'Export YAML']...
[10:44:14] TRACE | [fast-enricher] YAKE keywords with scores: ['Polling Periodically(0.003)', 'API Polling(0.003)', 'Version Control(0.003)', 'Expense Approval(0.004)', 'Export YAML(0.004)', 'POST steps(0.004)', 'Approval Required(0.004)', 'Capture form(0.004)', 'Tag releases(0.005)', 'Approval Implement(0.005)']
[10:44:14] TRACE | [fast-enricher] YAKE extraction took 8.7ms
[10:44:14] DEBUG | [fast-enricher] spaCy extracted 6 entities: {'PERSON': ['Git'], 'ORG': ['API', 'Multi-Step Approval Implement', 'expense_id'], 'WORK_OF_ART': ['Expense Approval Workflow', 'Expense Approval Required:']}
[10:44:14] TRACE | [fast-enricher] spaCy PERSON: ['Git']
[10:44:14] TRACE | [fast-enricher] spaCy ORG: ['API', 'Multi-Step Approval Implement', 'expense_id']
[10:44:14] TRACE | [fast-enricher] spaCy WORK_OF_ART: ['Expense Approval Workflow', 'Expense Approval Required:']
[10:44:14] TRACE | [fast-enricher] spaCy NER took 31.3ms
[10:44:14] TRACE | [fast-enricher] Generated prefix: Polling Periodically, API Polling, Version Control, Expense Approval, Export YAML, POST steps, Approval Required | Git, API, Multi-Step Approval Implement, Expense Approval Workflow, Expense Approval Required:
[10:44:14] DEBUG | [fast-enricher] Enriched in 40.3ms | keywords=10 entities=6 | prefix_len=209
[10:44:14] TRACE | [enriched-hybrid] CACHE MISS: enriched chunk (total enrichments: 49)
[10:44:14] TRACE | [fast-enricher] INPUT [3297 chars]: Requester: {{trigger.body.requester}} Description: {{trigger.body.description}} Approve: https://app.company.com/approve/{{steps.create_approval_request.rows[0].id}} Reject: https://app.company.com/reject/{{steps.create_approval_request.rows[0].id}} - id: wait_for_approval action: wait_for_webhook config: timeout: 86400 # 24 hours webhook_path: "/approval/{{steps.create_approval_request.rows[0].id}}" - id: process_approval action: javascript code: | if (input.status === 'approved') { return { ap...
[10:44:14] DEBUG | [fast-enricher] YAKE extracted 10 keywords: ['Expense rejected', 'query config', 'approval action', 'request config', 'config']...
[10:44:14] TRACE | [fast-enricher] YAKE keywords with scores: ['Expense rejected(0.002)', 'query config(0.002)', 'approval action(0.004)', 'request config(0.007)', 'config(0.008)', 'action(0.009)', 'expense request(0.009)', 'webhook config(0.010)', 'alert action(0.011)', 'query(0.011)']
[10:44:14] TRACE | [fast-enricher] YAKE extraction took 13.2ms
[10:44:14] DEBUG | [fast-enricher] spaCy extracted 9 entities: {'PERSON': ['Requester', 'Bearer', 'Getting Help'], 'WORK_OF_ART': ['Sync Customers: CRM'], 'ORG': ['UTC', 'INTERVAL', 'GROUP BY error_type', 'SET', 'CloudFlow']}
[10:44:14] TRACE | [fast-enricher] spaCy PERSON: ['Requester', 'Bearer', 'Getting Help']
[10:44:14] TRACE | [fast-enricher] spaCy WORK_OF_ART: ['Sync Customers: CRM']
[10:44:14] TRACE | [fast-enricher] spaCy ORG: ['UTC', 'INTERVAL', 'GROUP BY error_type', 'SET', 'CloudFlow']
[10:44:14] TRACE | [fast-enricher] spaCy NER took 82.3ms
[10:44:14] TRACE | [fast-enricher] Generated prefix: Expense rejected, query config, approval action, request config, config, action, expense request | Requester, Bearer, Sync Customers: CRM, UTC, INTERVAL
[10:44:14] DEBUG | [fast-enricher] Enriched in 95.7ms | keywords=10 entities=9 | prefix_len=152
[10:44:14] TRACE | [enriched-hybrid] CACHE MISS: enriched chunk (total enrichments: 50)
[10:44:14] TRACE | [fast-enricher] INPUT [505 chars]: - **Documentation**: https://docs.cloudflow.io - **Community Forum**: https://community.cloudflow.io - **Support Email**: support@cloudflow.io - **Status Page**: https://status.cloudflow.io - **API Reference**: https://api.cloudflow.io/docs **Enterprise Support:** - 24/7 phone support - Dedicated Slack channel - Custom onboarding and training - SLA guarantees Contact sales@cloudflow.io to learn more. --- **Version**: 2.1.0 **Last Updated**: January 2026 **License**: © 2026 CloudFlow Technologies...
[10:44:14] DEBUG | [fast-enricher] YAKE extracted 10 keywords: ['Status Page', 'Documentation', 'Status', 'Page', 'CloudFlow Technologies']...
[10:44:14] TRACE | [fast-enricher] YAKE keywords with scores: ['Status Page(0.005)', 'Documentation(0.040)', 'Status(0.071)', 'Page(0.071)', 'CloudFlow Technologies(0.119)', 'Version(0.127)', 'January(0.127)', 'License(0.127)', 'Updated(0.211)', 'learn(0.231)']
[10:44:14] TRACE | [fast-enricher] YAKE extraction took 3.1ms
[10:44:14] DEBUG | [fast-enricher] spaCy extracted 1 entities: {'ORG': ['CloudFlow Technologies Inc.']}
[10:44:14] TRACE | [fast-enricher] spaCy ORG: ['CloudFlow Technologies Inc.']
[10:44:14] TRACE | [fast-enricher] spaCy NER took 13.2ms
[10:44:14] TRACE | [fast-enricher] Generated prefix: Status Page, Documentation, Status, Page, CloudFlow Technologies, Version, January | CloudFlow Technologies Inc.
[10:44:14] DEBUG | [fast-enricher] Enriched in 16.5ms | keywords=10 entities=1 | prefix_len=112
[10:44:14] TRACE | [enriched-hybrid] CACHE MISS: enriched chunk (total enrichments: 51)
[10:44:15] INFO  |   Indexed in 4.31s
[10:44:15] TRACE | [enriched-hybrid] === RETRIEVE START ===
[10:44:15] TRACE | [enriched-hybrid] QUERY: What is the API rate limit per minute?
[10:44:15] TRACE | [enriched-hybrid] n_candidates=50 (k=5 * multiplier=10)
[10:44:15] TRACE | [enriched-hybrid] TOP-10 SEMANTIC SCORES:
[10:44:15] TRACE | [enriched-hybrid]   SEM[0] idx=1 chunk_id=api_reference_fix_1 score=0.7560 | ## Rate Limiting To ensure fair usage and system stability, CloudFlow enforces rate limits on all AP...
[10:44:15] TRACE | [enriched-hybrid]   SEM[1] idx=4 chunk_id=api_reference_fix_4 score=0.6459 | ### Error Response Format ```json { "error": { "code": "invalid_parameter", "message": "The 'limit' ...
[10:44:15] TRACE | [enriched-hybrid]   SEM[2] idx=35 chunk_id=troubleshooting_guide_fix_7 score=0.6456 | Waiting {retry_after} seconds...") time.sleep(retry_after) continue remaining = int(response.headers...
[10:44:15] TRACE | [enriched-hybrid]   SEM[3] idx=16 chunk_id=architecture_overview_fix_10 score=0.6323 | - Execution start rate: 500 per second - Completion rate: 450 per second (average 2-second execution...
[10:44:15] TRACE | [enriched-hybrid]   SEM[4] idx=3 chunk_id=api_reference_fix_3 score=0.6213 | **Endpoint:** `GET /pipelines/{pipeline_id}/executions` **Query Parameters:** - `status` (optional):...
[10:44:15] TRACE | [enriched-hybrid]   SEM[5] idx=7 chunk_id=architecture_overview_fix_1 score=0.6091 | **Technology**: Node.js with Express.js framework **Replicas**: 12 pods (production), auto-scaling 8...
[10:44:15] TRACE | [enriched-hybrid]   SEM[6] idx=34 chunk_id=troubleshooting_guide_fix_6 score=0.6057 | Data Validation Errors** ``` ValidationError: Field 'customer_id' is required but missing in 234 rec...
[10:44:15] TRACE | [enriched-hybrid]   SEM[7] idx=46 chunk_id=user_guide_fix_6 score=0.5849 | Filter by error type, date range, or execution ID 3. Click an execution to view full details 4. Clic...
[10:44:15] TRACE | [enriched-hybrid]   SEM[8] idx=29 chunk_id=troubleshooting_guide_fix_1 score=0.5770 | **Diagnosis:** ```bash # Check system time timedatectl status # Compare with NTP server ntpdate -q p...
[10:44:15] TRACE | [enriched-hybrid]   SEM[9] idx=15 chunk_id=architecture_overview_fix_9 score=0.5764 | per API key (default: 1000 RPM) ### Secrets Management **HashiCorp Vault Integration**: - Dynamic da...
[10:44:15] TRACE | [enriched-hybrid] TOP-10 BM25 SCORES:
[10:44:15] TRACE | [enriched-hybrid]   BM25[0] idx=34 chunk_id=troubleshooting_guide_fix_6 score=8.3699 | Data Validation Errors** ``` ValidationError: Field 'customer_id' is required but missing in 234 rec...
[10:44:15] TRACE | [enriched-hybrid]   BM25[1] idx=1 chunk_id=api_reference_fix_1 score=7.6121 | ## Rate Limiting To ensure fair usage and system stability, CloudFlow enforces rate limits on all AP...
[10:44:15] TRACE | [enriched-hybrid]   BM25[2] idx=35 chunk_id=troubleshooting_guide_fix_7 score=5.8202 | Waiting {retry_after} seconds...") time.sleep(retry_after) continue remaining = int(response.headers...
[10:44:15] TRACE | [enriched-hybrid]   BM25[3] idx=4 chunk_id=api_reference_fix_4 score=5.8118 | ### Error Response Format ```json { "error": { "code": "invalid_parameter", "message": "The 'limit' ...
[10:44:15] TRACE | [enriched-hybrid]   BM25[4] idx=16 chunk_id=architecture_overview_fix_10 score=5.3231 | - Execution start rate: 500 per second - Completion rate: 450 per second (average 2-second execution...
[10:44:15] TRACE | [enriched-hybrid]   BM25[5] idx=42 chunk_id=user_guide_fix_2 score=4.7836 | ### HTTP Requests Make HTTP requests to any API endpoint: **Configuration:** - **Method**: GET, POST...
[10:44:15] TRACE | [enriched-hybrid]   BM25[6] idx=7 chunk_id=architecture_overview_fix_1 score=4.3337 | **Technology**: Node.js with Express.js framework **Replicas**: 12 pods (production), auto-scaling 8...
[10:44:15] TRACE | [enriched-hybrid]   BM25[7] idx=14 chunk_id=architecture_overview_fix_8 score=4.0642 | ### Cache Invalidation Strategies **Time-Based Expiration (TTL)**: - Short-lived data: 5-15 minutes ...
[10:44:15] TRACE | [enriched-hybrid]   BM25[8] idx=37 chunk_id=troubleshooting_guide_fix_9 score=4.0589 | in" # Find workflow failures by ID kubectl logs -n cloudflow deployment/cloudflow-workflow-engine | ...
[10:44:15] TRACE | [enriched-hybrid]   BM25[9] idx=15 chunk_id=architecture_overview_fix_9 score=4.0523 | per API key (default: 1000 RPM) ### Secrets Management **HashiCorp Vault Integration**: - Dynamic da...
[10:44:15] TRACE | [enriched-hybrid] RRF SCORE CALCULATION (top 10):
[10:44:15] TRACE | [enriched-hybrid]   RRF[0] idx=1 total=0.0331 (sem_rank=0 comp=0.0167 + bm25_rank=1 comp=0.0164)
[10:44:15] TRACE | [enriched-hybrid]   RRF[1] idx=4 total=0.0323 (sem_rank=1 comp=0.0164 + bm25_rank=3 comp=0.0159)
[10:44:15] TRACE | [enriched-hybrid]   RRF[2] idx=35 total=0.0323 (sem_rank=2 comp=0.0161 + bm25_rank=2 comp=0.0161)
[10:44:15] TRACE | [enriched-hybrid]   RRF[3] idx=34 total=0.0318 (sem_rank=6 comp=0.0152 + bm25_rank=0 comp=0.0167)
[10:44:15] TRACE | [enriched-hybrid]   RRF[4] idx=16 total=0.0315 (sem_rank=3 comp=0.0159 + bm25_rank=4 comp=0.0156)
[10:44:15] TRACE | [enriched-hybrid]   RRF[5] idx=7 total=0.0305 (sem_rank=5 comp=0.0154 + bm25_rank=6 comp=0.0152)
[10:44:15] TRACE | [enriched-hybrid]   RRF[6] idx=15 total=0.0290 (sem_rank=9 comp=0.0145 + bm25_rank=9 comp=0.0145)
[10:44:15] TRACE | [enriched-hybrid]   RRF[7] idx=42 total=0.0289 (sem_rank=14 comp=0.0135 + bm25_rank=5 comp=0.0154)
[10:44:15] TRACE | [enriched-hybrid]   RRF[8] idx=25 total=0.0282 (sem_rank=11 comp=0.0141 + bm25_rank=11 comp=0.0141)
[10:44:15] TRACE | [enriched-hybrid]   RRF[9] idx=0 total=0.0276 (sem_rank=12 comp=0.0139 + bm25_rank=13 comp=0.0137)
[10:44:15] TRACE | [enriched-hybrid] FINAL TOP-5 RESULTS:
[10:44:15] TRACE | [enriched-hybrid]   RESULT[1] chunk_id=api_reference_fix_1 doc_id=api_reference rrf_score=0.0331 sem_rank=0 bm25_rank=1 dominant=semantic | enriched: Rate Limiting, Rate Limit, Rate, system stability, CloudFlow enforces, enforces rate, API | CloudFlow, API, max, ISO 8601  ## Rate Limiting To ensure fair usage and system stability, CloudFlow enforce...
[10:44:15] TRACE | [enriched-hybrid]   RESULT[2] chunk_id=api_reference_fix_4 doc_id=api_reference rrf_score=0.0323 sem_rank=1 bm25_rank=3 dominant=semantic | enriched: malformed JSON, requested resource, Bad Gateway, Error Codes, HTTP Status, Response Format, Server Error | Status Codes, Invalid, Description, Common Causes, Forbidden  ### Error Response Format ```js...
[10:44:15] TRACE | [enriched-hybrid]   RESULT[3] chunk_id=troubleshooting_guide_fix_7 doc_id=troubleshooting_guide rrf_score=0.0323 sem_rank=2 bm25_rank=2 dominant=balanced | enriched: Optimization Strategies, awk print, rate limit, raise Exception, Max retries, rate limiting, Warning | time.sleep(retry_after, Batch, API  Waiting {retry_after} seconds...") time.sleep(retry_after) co...
[10:44:15] TRACE | [enriched-hybrid]   RESULT[4] chunk_id=troubleshooting_guide_fix_6 doc_id=troubleshooting_guide rrf_score=0.0318 sem_rank=6 bm25_rank=0 dominant=bm25 | enriched: Data Validation, Validation Errors, Rate Limit, Error Response, Concurrent Workflows, Resolution, Rate | Data Validation Errors, Rate Limiting & Throttling, API Failures, Tier, Custom  Data Validation...
[10:44:15] TRACE | [enriched-hybrid]   RESULT[5] chunk_id=architecture_overview_fix_10 doc_id=architecture_overview rrf_score=0.0315 sem_rank=3 bm25_rank=4 dominant=semantic | enriched: Workflow Engine, API Gateway, Resource Utilization, Gbps average, Auth Service, Gbps peak, API error | Kafka, GB, Target, - Auth Service, P50  - Execution start rate: 500 per second - Completion rate:...
[10:44:15] TRACE | [enriched-hybrid] === RETRIEVE END ===
[10:44:15] TRACE | === FACT COVERAGE ANALYSIS ===
[10:44:15] TRACE | Query: What is the API rate limit per minute?
[10:44:15] TRACE | Total key facts: 2
[10:44:15] TRACE | Found facts: 2/2
[10:44:15] TRACE |   FOUND[1] 100 requests per minute per authenticated user
[10:44:15] TRACE |   FOUND[2] 20 requests per minute for unauthenticated requests
[10:44:15] TRACE | === END FACT COVERAGE ===
[10:44:15] TRACE | [enriched-hybrid] === RETRIEVE START ===
[10:44:15] TRACE | [enriched-hybrid] QUERY: How many requests can I make per minute to the API?
[10:44:15] TRACE | [enriched-hybrid] n_candidates=50 (k=5 * multiplier=10)
[10:44:15] TRACE | [enriched-hybrid] TOP-10 SEMANTIC SCORES:
[10:44:15] TRACE | [enriched-hybrid]   SEM[0] idx=1 chunk_id=api_reference_fix_1 score=0.7024 | ## Rate Limiting To ensure fair usage and system stability, CloudFlow enforces rate limits on all AP...
[10:44:15] TRACE | [enriched-hybrid]   SEM[1] idx=3 chunk_id=api_reference_fix_3 score=0.6366 | **Endpoint:** `GET /pipelines/{pipeline_id}/executions` **Query Parameters:** - `status` (optional):...
[10:44:15] TRACE | [enriched-hybrid]   SEM[2] idx=4 chunk_id=api_reference_fix_4 score=0.6336 | ### Error Response Format ```json { "error": { "code": "invalid_parameter", "message": "The 'limit' ...
[10:44:15] TRACE | [enriched-hybrid]   SEM[3] idx=35 chunk_id=troubleshooting_guide_fix_7 score=0.6208 | Waiting {retry_after} seconds...") time.sleep(retry_after) continue remaining = int(response.headers...
[10:44:15] TRACE | [enriched-hybrid]   SEM[4] idx=16 chunk_id=architecture_overview_fix_10 score=0.6148 | - Execution start rate: 500 per second - Completion rate: 450 per second (average 2-second execution...
[10:44:15] TRACE | [enriched-hybrid]   SEM[5] idx=42 chunk_id=user_guide_fix_2 score=0.6088 | ### HTTP Requests Make HTTP requests to any API endpoint: **Configuration:** - **Method**: GET, POST...
[10:44:15] TRACE | [enriched-hybrid]   SEM[6] idx=7 chunk_id=architecture_overview_fix_1 score=0.5963 | **Technology**: Node.js with Express.js framework **Replicas**: 12 pods (production), auto-scaling 8...
[10:44:15] TRACE | [enriched-hybrid]   SEM[7] idx=34 chunk_id=troubleshooting_guide_fix_6 score=0.5947 | Data Validation Errors** ``` ValidationError: Field 'customer_id' is required but missing in 234 rec...
[10:44:15] TRACE | [enriched-hybrid]   SEM[8] idx=48 chunk_id=user_guide_fix_8 score=0.5927 | Version Control For critical workflows: - Export YAML definitions regularly - Store in version contr...
[10:44:15] TRACE | [enriched-hybrid]   SEM[9] idx=0 chunk_id=api_reference_fix_0 score=0.5919 | # CloudFlow API Reference Version 2.1.0 | Last Updated: January 2026 ## Overview The CloudFlow API i...
[10:44:15] TRACE | [enriched-hybrid] TOP-10 BM25 SCORES:
[10:44:15] TRACE | [enriched-hybrid]   BM25[0] idx=1 chunk_id=api_reference_fix_1 score=8.8645 | ## Rate Limiting To ensure fair usage and system stability, CloudFlow enforces rate limits on all AP...
[10:44:15] TRACE | [enriched-hybrid]   BM25[1] idx=46 chunk_id=user_guide_fix_6 score=7.9456 | Filter by error type, date range, or execution ID 3. Click an execution to view full details 4. Clic...
[10:44:15] TRACE | [enriched-hybrid]   BM25[2] idx=42 chunk_id=user_guide_fix_2 score=7.0748 | ### HTTP Requests Make HTTP requests to any API endpoint: **Configuration:** - **Method**: GET, POST...
[10:44:15] TRACE | [enriched-hybrid]   BM25[3] idx=4 chunk_id=api_reference_fix_4 score=6.2530 | ### Error Response Format ```json { "error": { "code": "invalid_parameter", "message": "The 'limit' ...
[10:44:15] TRACE | [enriched-hybrid]   BM25[4] idx=15 chunk_id=architecture_overview_fix_9 score=5.3679 | per API key (default: 1000 RPM) ### Secrets Management **HashiCorp Vault Integration**: - Dynamic da...
[10:44:15] TRACE | [enriched-hybrid]   BM25[5] idx=10 chunk_id=architecture_overview_fix_4 score=5.1196 | **Technology**: Node.js with worker pool pattern **Replicas**: 8 pods (production), auto-scaling 6-1...
[10:44:15] TRACE | [enriched-hybrid]   BM25[6] idx=7 chunk_id=architecture_overview_fix_1 score=5.0326 | **Technology**: Node.js with Express.js framework **Replicas**: 12 pods (production), auto-scaling 8...
[10:44:15] TRACE | [enriched-hybrid]   BM25[7] idx=34 chunk_id=troubleshooting_guide_fix_6 score=4.5878 | Data Validation Errors** ``` ValidationError: Field 'customer_id' is required but missing in 234 rec...
[10:44:15] TRACE | [enriched-hybrid]   BM25[8] idx=44 chunk_id=user_guide_fix_4 score=4.3226 | ### Cron Syntax Use standard cron expressions to define schedules: ``` * * * * * ┬ ┬ ┬ ┬ ┬ │ │ │ │ │...
[10:44:15] TRACE | [enriched-hybrid]   BM25[9] idx=14 chunk_id=architecture_overview_fix_8 score=4.1505 | ### Cache Invalidation Strategies **Time-Based Expiration (TTL)**: - Short-lived data: 5-15 minutes ...
[10:44:15] TRACE | [enriched-hybrid] RRF SCORE CALCULATION (top 10):
[10:44:15] TRACE | [enriched-hybrid]   RRF[0] idx=1 total=0.0333 (sem_rank=0 comp=0.0167 + bm25_rank=0 comp=0.0167)
[10:44:15] TRACE | [enriched-hybrid]   RRF[1] idx=4 total=0.0320 (sem_rank=2 comp=0.0161 + bm25_rank=3 comp=0.0159)
[10:44:15] TRACE | [enriched-hybrid]   RRF[2] idx=42 total=0.0315 (sem_rank=5 comp=0.0154 + bm25_rank=2 comp=0.0161)
[10:44:15] TRACE | [enriched-hybrid]   RRF[3] idx=46 total=0.0307 (sem_rank=10 comp=0.0143 + bm25_rank=1 comp=0.0164)
[10:44:15] TRACE | [enriched-hybrid]   RRF[4] idx=7 total=0.0303 (sem_rank=6 comp=0.0152 + bm25_rank=6 comp=0.0152)
[10:44:15] TRACE | [enriched-hybrid]   RRF[5] idx=34 total=0.0299 (sem_rank=7 comp=0.0149 + bm25_rank=7 comp=0.0149)
[10:44:15] TRACE | [enriched-hybrid]   RRF[6] idx=15 total=0.0295 (sem_rank=12 comp=0.0139 + bm25_rank=4 comp=0.0156)
[10:44:15] TRACE | [enriched-hybrid]   RRF[7] idx=16 total=0.0286 (sem_rank=4 comp=0.0156 + bm25_rank=17 comp=0.0130)
[10:44:15] TRACE | [enriched-hybrid]   RRF[8] idx=35 total=0.0282 (sem_rank=3 comp=0.0159 + bm25_rank=21 comp=0.0123)
[10:44:15] TRACE | [enriched-hybrid]   RRF[9] idx=3 total=0.0282 (sem_rank=1 comp=0.0164 + bm25_rank=25 comp=0.0118)
[10:44:15] TRACE | [enriched-hybrid] FINAL TOP-5 RESULTS:
[10:44:15] TRACE | [enriched-hybrid]   RESULT[1] chunk_id=api_reference_fix_1 doc_id=api_reference rrf_score=0.0333 sem_rank=0 bm25_rank=0 dominant=balanced | enriched: Rate Limiting, Rate Limit, Rate, system stability, CloudFlow enforces, enforces rate, API | CloudFlow, API, max, ISO 8601  ## Rate Limiting To ensure fair usage and system stability, CloudFlow enforce...
[10:44:15] TRACE | [enriched-hybrid]   RESULT[2] chunk_id=api_reference_fix_4 doc_id=api_reference rrf_score=0.0320 sem_rank=2 bm25_rank=3 dominant=semantic | enriched: malformed JSON, requested resource, Bad Gateway, Error Codes, HTTP Status, Response Format, Server Error | Status Codes, Invalid, Description, Common Causes, Forbidden  ### Error Response Format ```js...
[10:44:15] TRACE | [enriched-hybrid]   RESULT[3] chunk_id=user_guide_fix_2 doc_id=user_guide rrf_score=0.0315 sem_rank=5 bm25_rank=2 dominant=bm25 | enriched: Basic Auth, Bearer Token, Important Notes, HTTP Requests, Query Parameters, API Key, Make HTTP | API, POST, Bearer Token, API Key  ### HTTP Requests Make HTTP requests to any API endpoint: **Configura...
[10:44:15] TRACE | [enriched-hybrid]   RESULT[4] chunk_id=user_guide_fix_6 doc_id=user_guide rrf_score=0.0307 sem_rank=10 bm25_rank=1 dominant=bm25 | enriched: workflow, date range, type, workflows, error type, execution, Execution Timeout | MB, Sync Customer Data  Filter by error type, date range, or execution ID 3. Click an execution to view full details 4...
[10:44:15] TRACE | [enriched-hybrid]   RESULT[5] chunk_id=architecture_overview_fix_1 doc_id=architecture_overview rrf_score=0.0303 sem_rank=6 bm25_rank=6 dominant=balanced | enriched: Critical Endpoints, Auth Service, JSON Schema, Resource Allocation, Performance Targets, Rate limiting, Key Responsibilities | Express.js, CPU, JSON Schema - Routing, Generate, RS256  **Technology**: ...
[10:44:15] TRACE | [enriched-hybrid] === RETRIEVE END ===
[10:44:15] TRACE | === FACT COVERAGE ANALYSIS ===
[10:44:15] TRACE | Query: How many requests can I make per minute to the API?
[10:44:15] TRACE | Total key facts: 2
[10:44:15] TRACE | Found facts: 2/2
[10:44:15] TRACE |   FOUND[1] 100 requests per minute per authenticated user
[10:44:15] TRACE |   FOUND[2] 20 requests per minute for unauthenticated requests
[10:44:15] TRACE | === END FACT COVERAGE ===
[10:44:15] TRACE | [enriched-hybrid] === RETRIEVE START ===
[10:44:15] TRACE | [enriched-hybrid] QUERY: My API calls are getting blocked, what's the limit?
[10:44:15] TRACE | [enriched-hybrid] n_candidates=50 (k=5 * multiplier=10)
[10:44:15] TRACE | [enriched-hybrid] TOP-10 SEMANTIC SCORES:
[10:44:15] TRACE | [enriched-hybrid]   SEM[0] idx=1 chunk_id=api_reference_fix_1 score=0.6575 | ## Rate Limiting To ensure fair usage and system stability, CloudFlow enforces rate limits on all AP...
[10:44:15] TRACE | [enriched-hybrid]   SEM[1] idx=4 chunk_id=api_reference_fix_4 score=0.6425 | ### Error Response Format ```json { "error": { "code": "invalid_parameter", "message": "The 'limit' ...
[10:44:15] TRACE | [enriched-hybrid]   SEM[2] idx=35 chunk_id=troubleshooting_guide_fix_7 score=0.6273 | Waiting {retry_after} seconds...") time.sleep(retry_after) continue remaining = int(response.headers...
[10:44:15] TRACE | [enriched-hybrid]   SEM[3] idx=34 chunk_id=troubleshooting_guide_fix_6 score=0.5867 | Data Validation Errors** ``` ValidationError: Field 'customer_id' is required but missing in 234 rec...
[10:44:15] TRACE | [enriched-hybrid]   SEM[4] idx=47 chunk_id=user_guide_fix_7 score=0.5609 | Handle Errors Gracefully Always implement error handling for external API calls and database operati...
[10:44:15] TRACE | [enriched-hybrid]   SEM[5] idx=49 chunk_id=user_guide_fix_9 score=0.5599 | Requester: {{trigger.body.requester}} Description: {{trigger.body.description}} Approve: https://app...
[10:44:15] TRACE | [enriched-hybrid]   SEM[6] idx=42 chunk_id=user_guide_fix_2 score=0.5565 | ### HTTP Requests Make HTTP requests to any API endpoint: **Configuration:** - **Method**: GET, POST...
[10:44:15] TRACE | [enriched-hybrid]   SEM[7] idx=29 chunk_id=troubleshooting_guide_fix_1 score=0.5562 | **Diagnosis:** ```bash # Check system time timedatectl status # Compare with NTP server ntpdate -q p...
[10:44:15] TRACE | [enriched-hybrid]   SEM[8] idx=3 chunk_id=api_reference_fix_3 score=0.5548 | **Endpoint:** `GET /pipelines/{pipeline_id}/executions` **Query Parameters:** - `status` (optional):...
[10:44:15] TRACE | [enriched-hybrid]   SEM[9] idx=28 chunk_id=troubleshooting_guide_fix_0 score=0.5496 | # CloudFlow Platform Troubleshooting Guide **Version:** 3.2.1 **Last Updated:** January 2026 **Audie...
[10:44:15] TRACE | [enriched-hybrid] TOP-10 BM25 SCORES:
[10:44:15] TRACE | [enriched-hybrid]   BM25[0] idx=39 chunk_id=troubleshooting_guide_fix_11 score=4.4396 | outage" # Create war room cloudflow incident war-room create incident-2024012401 ``` **For SEV-2 (Hi...
[10:44:15] TRACE | [enriched-hybrid]   BM25[1] idx=35 chunk_id=troubleshooting_guide_fix_7 score=4.0954 | Waiting {retry_after} seconds...") time.sleep(retry_after) continue remaining = int(response.headers...
[10:44:15] TRACE | [enriched-hybrid]   BM25[2] idx=47 chunk_id=user_guide_fix_7 score=3.5218 | Handle Errors Gracefully Always implement error handling for external API calls and database operati...
[10:44:15] TRACE | [enriched-hybrid]   BM25[3] idx=28 chunk_id=troubleshooting_guide_fix_0 score=3.5123 | # CloudFlow Platform Troubleshooting Guide **Version:** 3.2.1 **Last Updated:** January 2026 **Audie...
[10:44:15] TRACE | [enriched-hybrid]   BM25[4] idx=1 chunk_id=api_reference_fix_1 score=3.4818 | ## Rate Limiting To ensure fair usage and system stability, CloudFlow enforces rate limits on all AP...
[10:44:15] TRACE | [enriched-hybrid]   BM25[5] idx=38 chunk_id=troubleshooting_guide_fix_10 score=3.2856 | database nc -zv cloudflow-db.internal.company.com 5432 # Trace route traceroute api.cloudflow.io # C...
[10:44:15] TRACE | [enriched-hybrid]   BM25[6] idx=14 chunk_id=architecture_overview_fix_8 score=3.0933 | ### Cache Invalidation Strategies **Time-Based Expiration (TTL)**: - Short-lived data: 5-15 minutes ...
[10:44:15] TRACE | [enriched-hybrid]   BM25[7] idx=11 chunk_id=architecture_overview_fix_5 score=2.8040 | Success/failure events published back to Kafka ``` **Event Schema**: ```json { "event_id": "uuid-v4"...
[10:44:15] TRACE | [enriched-hybrid]   BM25[8] idx=40 chunk_id=user_guide_fix_0 score=2.5709 | # CloudFlow User Guide Welcome to CloudFlow, the modern workflow automation platform that helps you ...
[10:44:15] TRACE | [enriched-hybrid]   BM25[9] idx=48 chunk_id=user_guide_fix_8 score=2.5684 | Version Control For critical workflows: - Export YAML definitions regularly - Store in version contr...
[10:44:15] TRACE | [enriched-hybrid] RRF SCORE CALCULATION (top 10):
[10:44:15] TRACE | [enriched-hybrid]   RRF[0] idx=35 total=0.0325 (sem_rank=2 comp=0.0161 + bm25_rank=1 comp=0.0164)
[10:44:15] TRACE | [enriched-hybrid]   RRF[1] idx=1 total=0.0323 (sem_rank=0 comp=0.0167 + bm25_rank=4 comp=0.0156)
[10:44:15] TRACE | [enriched-hybrid]   RRF[2] idx=47 total=0.0318 (sem_rank=4 comp=0.0156 + bm25_rank=2 comp=0.0161)
[10:44:15] TRACE | [enriched-hybrid]   RRF[3] idx=28 total=0.0304 (sem_rank=9 comp=0.0145 + bm25_rank=3 comp=0.0159)
[10:44:15] TRACE | [enriched-hybrid]   RRF[4] idx=4 total=0.0301 (sem_rank=1 comp=0.0164 + bm25_rank=13 comp=0.0137)
