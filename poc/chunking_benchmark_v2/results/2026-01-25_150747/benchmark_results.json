{
  "metadata": {
    "timestamp": "2026-01-25_150747",
    "num_documents": 5,
    "num_queries": 20,
    "total_facts": 53,
    "has_human_queries": true
  },
  "evaluations": [
    {
      "strategy": "hybrid",
      "chunking": "fixed_512_0pct",
      "embedder": "BAAI/bge-base-en-v1.5",
      "reranker": null,
      "llm": null,
      "k": 5,
      "metric": "exact_match",
      "num_chunks": 51,
      "index_time_s": 1.1369604620012979,
      "aggregate": {
        "original": {
          "coverage": 0.7547169811320755,
          "found": 40,
          "total": 53,
          "avg_latency_ms": 12.241939449813799,
          "p95_latency_ms": 13.080488699597481
        },
        "synonym": {
          "coverage": 0.6981132075471698,
          "found": 37,
          "total": 53,
          "avg_latency_ms": 12.187971849925816,
          "p95_latency_ms": 13.072916348755825
        },
        "problem": {
          "coverage": 0.5471698113207547,
          "found": 29,
          "total": 53,
          "avg_latency_ms": 12.261649649553874,
          "p95_latency_ms": 13.084725849694223
        },
        "casual": {
          "coverage": 0.660377358490566,
          "found": 35,
          "total": 53,
          "avg_latency_ms": 12.094135000188544,
          "p95_latency_ms": 12.821501549296954
        },
        "contextual": {
          "coverage": 0.6037735849056604,
          "found": 32,
          "total": 53,
          "avg_latency_ms": 12.492944749646995,
          "p95_latency_ms": 13.250982649697107
        },
        "negation": {
          "coverage": 0.5094339622641509,
          "found": 27,
          "total": 53,
          "avg_latency_ms": 12.37932490039384,
          "p95_latency_ms": 13.338882848438516
        }
      },
      "per_query": [
        {
          "key_facts": [
            "100 requests per minute per authenticated user",
            "20 requests per minute for unauthenticated requests"
          ],
          "found_facts": [
            "100 requests per minute per authenticated user",
            "20 requests per minute for unauthenticated requests"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "api_reference"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_1",
              "content": "## Rate Limiting To ensure fair usage and system stability, CloudFlow enforces rate limits on all API endpoints. **Default Limits:** - 100 requests per minute per authenticated user - 20 requests per minute for unauthenticated requests - Burst allowance: 150 requests in a 10-second window ### Rate Limit Headers Every API response includes rate limit information: ``` X-RateLimit-Limit: 100 X-RateLimit-Remaining: 87 X-RateLimit-Reset: 1640995200 ``` When you exceed the rate limit, you'll receive a `429 Too Many Requests` response: ```json { \"error\": { \"code\": \"rate_limit_exceeded\", \"message\": \"Rate limit exceeded. Please retry after 42 seconds.\", \"retry_after\": 42 } } ``` **Best Practices:** - Monitor `X-RateLimit-Remaining` header values - Implement exponential backoff when receiving 429 responses - Cache responses when appropriate to reduce API calls - Consider upgrading to Enterprise tier for higher limits (1000 req/min) ## Pagination List endpoints return paginated results to optimize performance. Use `limit` and `offset` parameters to navigate through result sets. **Parameters:** - `limit`: Number of items per page (default: 20, max: 100) - `offset`: Number of items to skip (default: 0) **Example Request:** ```bash curl -H \"X-API-Key: YOUR_API_KEY\" \\ \"https://api.cloudflow.io/v2/workflows?limit=50&offset=100\" ``` **Response Structure:** ```json { \"data\": [...], \"pagination\": { \"total\": 347, \"limit\": 50, \"offset\": 100, \"has_more\": true } } ``` **Python Example:** ```python import requests def fetch_all_workflows(api_key): base_url = \"https://api.cloudflow.io/v2/workflows\" headers = {\"X-API-Key\": api_key} all_workflows = [] offset = 0 limit = 100 while True: response = requests.get( base_url, headers=headers, params={\"limit\": limit, \"offset\": offset} ) data = response.json() all_workflows.extend(data['data']) if not data['pagination']['has_more']: break offset += limit return all_workflows ``` ## Endpoints ### Workflows #### List Workflows Retrieve a paginated list of all workflows in your account. **Endpoint:** `GET /workflows` **Query Parameters:** - `status` (optional): Filter by status (`active`, `paused`, `archived`) - `created_after` (optional): ISO 8601 timestamp - `limit` (optional): Items per page (max 100) - `offset` (optional): Pagination offset **Example Request:** ```bash curl -H \"X-API-Key: YOUR_API_KEY\" \\ \"https://api.cloudflow.io/v2/workflows?status=active&limit=25\" ``` **Example Response (200 OK):** ```json { \"data\": [ { \"id\": \"wf_8x7k2m9p\", \"name\": \"Data Processing Pipeline\", \"description\": \"Processes customer data every hour\", \"status\": \"active\", \"trigger\": { \"type\": \"schedule\", \"cron\": \"0 * * * *\" }, \"steps\": 12, \"created_at\": \"2026-01-15T10:30:00Z\", \"updated_at\": \"2026-01-23T14:22:00Z\", \"last_run\": \"2026-01-24T09:00:00Z\" } ], \"pagination\": { \"total\": 1, \"limit\": 25, \"offset\": 0, \"has_more\": false } } ``` #### Create Workflow Create a new workflow with specified configuration.",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_7",
              "content": "Waiting {retry_after} seconds...\") time.sleep(retry_after) continue remaining = int(response.headers.get('X-RateLimit-Remaining', 0)) if remaining < 10: print(f\"Warning: Only {remaining} requests remaining\") return response raise Exception(\"Max retries exceeded due to rate limiting\") ``` **Bash script with rate limit checking:** ```bash #!/bin/bash check_rate_limit() { local remaining=$(curl -s -I https://api.cloudflow.io/api/v1/workflows \\ -H \"Authorization: Bearer $CF_ACCESS_TOKEN\" | \\ grep -i \"x-ratelimit-remaining\" | \\ awk '{print $2}' | tr -d '\\r') if [ \"$remaining\" -lt 10 ]; then echo \"Warning: Only $remaining requests remaining\" local reset=$(curl -s -I https://api.cloudflow.io/api/v1/workflows \\ -H \"Authorization: Bearer $CF_ACCESS_TOKEN\" | \\ grep -i \"x-ratelimit-reset\" | \\ awk '{print $2}' | tr -d '\\r') local wait_time=$((reset - $(date +%s))) echo \"Rate limit resets in $wait_time seconds\" sleep $wait_time fi } # Use before API calls check_rate_limit cloudflow workflows execute wf_9k2n4m8p1q ``` #### Optimization Strategies **1. Implement request batching:** ```bash # Batch multiple workflow executions cloudflow workflows execute-batch \\ --workflow-ids \"wf_id1,wf_id2,wf_id3,wf_id4,wf_id5\" \\ --batch-size 5 # This counts as 1 API request instead of 5 ``` **2. Use webhooks instead of polling:** ```bash # Configure webhook for workflow completion cloudflow webhooks create \\ --event workflow.completed \\ --url https://your-service.com/webhooks/cloudflow \\ --secret $WEBHOOK_SECRET # Verify webhook cloudflow webhooks test webhook_abc123 ``` **3.",
              "score": null
            },
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_4",
              "content": "### Error Response Format ```json { \"error\": { \"code\": \"invalid_parameter\", \"message\": \"The 'limit' parameter must be between 1 and 100\", \"field\": \"limit\", \"request_id\": \"req_8k3m9x2p\" } } ``` ### HTTP Status Codes | Status Code | Description | Common Causes | |------------|-------------|---------------| | 400 | Bad Request | Invalid parameters, malformed JSON, validation errors | | 401 | Unauthorized | Missing or invalid authentication credentials | | 403 | Forbidden | Insufficient permissions for requested resource | | 404 | Not Found | Resource does not exist or has been deleted | | 429 | Too Many Requests | Rate limit exceeded, retry after specified period | | 500 | Internal Server Error | Unexpected server error, contact support if persists | | 502 | Bad Gateway | Temporary service issue, retry with exponential backoff | | 503 | Service Unavailable | Scheduled maintenance or temporary outage | ### Error Codes CloudFlow returns specific error codes to help you identify and resolve issues: - `invalid_parameter`: One or more request parameters are invalid - `missing_required_field`: Required field is missing from request body - `authentication_failed`: Invalid API key or token - `insufficient_permissions`: User lacks required scope or permission - `resource_not_found`: Requested resource does not exist - `rate_limit_exceeded`: Too many requests, see rate limiting section - `workflow_execution_failed`: Workflow execution encountered an error - `invalid_json`: Request body contains malformed JSON - `duplicate_resource`: Resource with same identifier already exists - `quota_exceeded`: Account quota limit reached ### Error Handling Best Practices **Python Example:** ```python import requests import time def make_api_request(url, headers, max_retries=3): for attempt in range(max_retries): try: response = requests.get(url, headers=headers) if response.status_code == 200: return response.json() elif response.status_code == 429: # Rate limit exceeded retry_after = int(response.headers.get('X-RateLimit-Reset', 60)) print(f\"Rate limited. Waiting {retry_after} seconds...\") time.sleep(retry_after) continue elif response.status_code >= 500: # Server error - retry with exponential backoff wait_time = 2 ** attempt print(f\"Server error. Retrying in {wait_time} seconds...\") time.sleep(wait_time) continue else: # Client error - don't retry error_data = response.json() raise Exception(f\"API Error: {error_data['error']['message']}\") except requests.exceptions.RequestException as e: print(f\"Request failed: {e}\") if attempt < max_retries - 1: time.sleep(2 ** attempt) else: raise raise Exception(\"Max retries exceeded\") ``` ## Webhooks CloudFlow can send webhook notifications when specific events occur in your workflows.",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_10",
              "content": "- Execution start rate: 500 per second - Completion rate: 450 per second (average 2-second execution time) **Database**: - Read throughput: 50,000 queries per second (across replicas) - Write throughput: 15,000 transactions per second - Connection capacity: 2,000 concurrent connections **Kafka**: - Message ingestion: 100,000 messages per second - Consumer throughput: 80,000 messages per second (aggregated) - End-to-end latency: < 100ms (P99) ### Resource Utilization **CPU Utilization** (Target: 60-70% average): - API Gateway: 55% average, 80% peak - Workflow Engine: 65% average, 85% peak - Auth Service: 40% average, 60% peak **Memory Utilization** (Target: < 80%): - API Gateway: 2.5GB average per pod (4GB allocated) - Workflow Engine: 6GB average per pod (8GB allocated) - Notification Service: 2.8GB average per pod (4GB allocated) **Network Throughput**: - Ingress: 2 Gbps average, 5 Gbps peak - Egress: 1.5 Gbps average, 4 Gbps peak - Internal (service mesh): 8 Gbps average ### Monitoring & Alerting **Key Metrics** (Monitored via Prometheus): - Request rate (per service, per endpoint) - Error rate (4xx, 5xx responses) - Latency percentiles (P50, P95, P99) - Resource utilization (CPU, memory, disk) - Cache hit/miss ratios - Database connection pool saturation - Kafka consumer lag **Alerts** (via PagerDuty): - P1 (Immediate): API error rate > 1%, database connection failure - P2 (< 15 min): P99 latency > 500ms, cache hit rate < 80% - P3 (< 1 hour): Resource utilization > 85%, replica count < minimum **SLI/SLO/SLA**: - SLI: API success rate (non-5xx responses) - SLO: 99.9% uptime per month (43 minutes downtime allowance) - SLA: 99.5% uptime guarantee (customer-facing SLA with credits) --- ## Disaster Recovery ### Recovery Objectives **RPO (Recovery Point Objective): 1 hour** - Maximum acceptable data loss: 1 hour of transactions - Achieved through: Continuous database replication + hourly snapshots - Kafka retention: 7 days allows event replay **RTO (Recovery Time Objective): 4 hours** - Maximum acceptable downtime: 4 hours for full system recovery - Includes: Failover, data verification, and service restoration - Automated runbooks reduce RTO to < 2 hours for common scenarios ### High Availability Architecture **Multi-AZ Deployment**: ``` Region: us-east-1 AZ-1a: AZ-1b: AZ-1c: - API Gateway (4) - API Gateway (4) - API Gateway (4) - Workflow Engine (6) - Workflow Engine (5) - Workflow Engine (5) - Auth Service (3) - Auth Service (3) - Auth",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_6",
              "content": "Data Validation Errors** ``` ValidationError: Field 'customer_id' is required but missing in 234 records ``` **Resolution:** ```bash # Add data quality checks cloudflow workflows update wf_9k2n4m8p1q \\ --step data_ingestion \\ --add-validator required_fields \\ --validator-config '{\"fields\": [\"customer_id\", \"timestamp\", \"amount\"]}' # Configure error handling cloudflow workflows update wf_9k2n4m8p1q \\ --step data_validation \\ --on-error continue \\ --error-threshold 5% # Fail if > 5% of records invalid ``` **2. External API Failures** ``` ExternalAPIError: API request to https://partner-api.example.com failed with status 502 ``` **Resolution:** ```bash # Add circuit breaker cloudflow workflows update wf_9k2n4m8p1q \\ --step external_api_call \\ --circuit-breaker-enabled true \\ --circuit-breaker-threshold 5 \\ --circuit-breaker-timeout 30000 # Configure fallback behavior cloudflow workflows update wf_9k2n4m8p1q \\ --step external_api_call \\ --fallback-action use_cached_data \\ --cache-ttl 3600 ``` --- ## Rate Limiting & Throttling ### 429 Too Many Requests #### Error Response ```json { \"error\": \"rate_limit_exceeded\", \"message\": \"API rate limit exceeded. Retry after 45 seconds.\", \"status\": 429, \"headers\": { \"X-RateLimit-Limit\": \"1000\", \"X-RateLimit-Remaining\": \"0\", \"X-RateLimit-Reset\": \"1706112345\", \"Retry-After\": \"45\" } } ``` #### Rate Limit Tiers CloudFlow enforces the following rate limits per workspace: | Tier | Requests/Minute | Requests/Hour | Concurrent Workflows | |------|-----------------|---------------|----------------------| | Free | 60 | 1,000 | 5 | | Standard | 1,000 | 50,000 | 50 | | Premium | 5,000 | 250,000 | 200 | | Enterprise | Custom | Custom | Unlimited | #### Checking Rate Limit Status ```bash # Check current rate limit status curl -I https://api.cloudflow.io/api/v1/workflows \\ -H \"Authorization: Bearer $CF_ACCESS_TOKEN\" # Extract rate limit headers curl -s -I https://api.cloudflow.io/api/v1/workflows \\ -H \"Authorization: Bearer $CF_ACCESS_TOKEN\" | \\ grep -i \"x-ratelimit\" # Output: # X-RateLimit-Limit: 1000 # X-RateLimit-Remaining: 247 # X-RateLimit-Reset: 1706112400 # Monitor rate limit usage cloudflow metrics query \\ --metric rate_limit_remaining \\ --workspace ws_abc123 \\ --last 1h \\ --interval 1m ``` #### Handling Rate Limits in Code **Python example with retry logic:** ```python import time import requests def cloudflow_api_call_with_retry(url, headers, max_retries=3): for attempt in range(max_retries): response = requests.get(url, headers=headers) if response.status_code == 429: retry_after = int(response.headers.get('Retry-After', 60)) print(f\"Rate limited.",
              "score": null
            }
          ],
          "latency_ms": 12.032573999022134,
          "query_id": "realistic_001",
          "dimension": "original",
          "query": "What is the API rate limit per minute?"
        },
        {
          "key_facts": [
            "100 requests per minute per authenticated user",
            "20 requests per minute for unauthenticated requests"
          ],
          "found_facts": [
            "100 requests per minute per authenticated user",
            "20 requests per minute for unauthenticated requests"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "api_reference"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_1",
              "content": "## Rate Limiting To ensure fair usage and system stability, CloudFlow enforces rate limits on all API endpoints. **Default Limits:** - 100 requests per minute per authenticated user - 20 requests per minute for unauthenticated requests - Burst allowance: 150 requests in a 10-second window ### Rate Limit Headers Every API response includes rate limit information: ``` X-RateLimit-Limit: 100 X-RateLimit-Remaining: 87 X-RateLimit-Reset: 1640995200 ``` When you exceed the rate limit, you'll receive a `429 Too Many Requests` response: ```json { \"error\": { \"code\": \"rate_limit_exceeded\", \"message\": \"Rate limit exceeded. Please retry after 42 seconds.\", \"retry_after\": 42 } } ``` **Best Practices:** - Monitor `X-RateLimit-Remaining` header values - Implement exponential backoff when receiving 429 responses - Cache responses when appropriate to reduce API calls - Consider upgrading to Enterprise tier for higher limits (1000 req/min) ## Pagination List endpoints return paginated results to optimize performance. Use `limit` and `offset` parameters to navigate through result sets. **Parameters:** - `limit`: Number of items per page (default: 20, max: 100) - `offset`: Number of items to skip (default: 0) **Example Request:** ```bash curl -H \"X-API-Key: YOUR_API_KEY\" \\ \"https://api.cloudflow.io/v2/workflows?limit=50&offset=100\" ``` **Response Structure:** ```json { \"data\": [...], \"pagination\": { \"total\": 347, \"limit\": 50, \"offset\": 100, \"has_more\": true } } ``` **Python Example:** ```python import requests def fetch_all_workflows(api_key): base_url = \"https://api.cloudflow.io/v2/workflows\" headers = {\"X-API-Key\": api_key} all_workflows = [] offset = 0 limit = 100 while True: response = requests.get( base_url, headers=headers, params={\"limit\": limit, \"offset\": offset} ) data = response.json() all_workflows.extend(data['data']) if not data['pagination']['has_more']: break offset += limit return all_workflows ``` ## Endpoints ### Workflows #### List Workflows Retrieve a paginated list of all workflows in your account. **Endpoint:** `GET /workflows` **Query Parameters:** - `status` (optional): Filter by status (`active`, `paused`, `archived`) - `created_after` (optional): ISO 8601 timestamp - `limit` (optional): Items per page (max 100) - `offset` (optional): Pagination offset **Example Request:** ```bash curl -H \"X-API-Key: YOUR_API_KEY\" \\ \"https://api.cloudflow.io/v2/workflows?status=active&limit=25\" ``` **Example Response (200 OK):** ```json { \"data\": [ { \"id\": \"wf_8x7k2m9p\", \"name\": \"Data Processing Pipeline\", \"description\": \"Processes customer data every hour\", \"status\": \"active\", \"trigger\": { \"type\": \"schedule\", \"cron\": \"0 * * * *\" }, \"steps\": 12, \"created_at\": \"2026-01-15T10:30:00Z\", \"updated_at\": \"2026-01-23T14:22:00Z\", \"last_run\": \"2026-01-24T09:00:00Z\" } ], \"pagination\": { \"total\": 1, \"limit\": 25, \"offset\": 0, \"has_more\": false } } ``` #### Create Workflow Create a new workflow with specified configuration.",
              "score": null
            },
            {
              "doc_id": "user_guide",
              "chunk_id": "user_guide_fix_2",
              "content": "### HTTP Requests Make HTTP requests to any API endpoint: **Configuration:** - **Method**: GET, POST, PUT, PATCH, DELETE, HEAD - **URL**: Full endpoint URL (supports variable interpolation) - **Headers**: Custom headers as key-value pairs - **Query Parameters**: URL parameters - **Body**: JSON, form data, or raw text - **Authentication**: Basic Auth, Bearer Token, API Key, OAuth 2.0 **Example:** ```yaml - id: fetch_user action: http_request config: method: GET url: \"https://api.example.com/users/{{user_id}}\" headers: Authorization: \"Bearer {{secrets.API_TOKEN}}\" Content-Type: \"application/json\" timeout: 30 ``` **Response Handling:** Access the response in subsequent steps: - `{{steps.fetch_user.status}}` - HTTP status code - `{{steps.fetch_user.body}}` - Response body - `{{steps.fetch_user.headers}}` - Response headers ### Database Queries Execute queries against supported databases (PostgreSQL, MySQL, MongoDB, Redis): **SQL Databases (PostgreSQL, MySQL):** ```yaml - id: get_orders action: database_query config: connection: \"{{secrets.DB_CONNECTION_STRING}}\" query: | SELECT * FROM orders WHERE customer_id = $1 AND status = $2 ORDER BY created_at DESC LIMIT 10 parameters: - \"{{trigger.customer_id}}\" - \"pending\" ``` **MongoDB:** ```yaml - id: find_documents action: mongodb_query config: connection: \"{{secrets.MONGO_URI}}\" database: \"production\" collection: \"users\" operation: find filter: email: \"{{trigger.email}}\" options: limit: 1 ``` **Important Notes:** - Always use parameterized queries to prevent SQL injection - Connection strings should be stored in secrets - Query timeout is 30 seconds by default - Maximum result set size is 10MB ### Email Notifications Send emails via SMTP or integrated email providers: **Configuration:** ```yaml - id: send_notification action: email config: provider: \"smtp\" # or \"sendgrid\", \"mailgun\", \"ses\" from: \"notifications@cloudflow.io\" to: \"{{trigger.recipient}}\" cc: \"manager@company.com\" subject: \"Alert: {{alert_type}}\" body: | Hello {{user.name}}, This is an automated notification about {{event_description}}. Details: - Time: {{timestamp}} - Type: {{alert_type}} - Priority: {{priority}} Best regards, CloudFlow Automation html: true attachments: - name: \"report.pdf\" content: \"{{steps.generate_report.output}}\" encoding: \"base64\" ``` **Email Templates:** CloudFlow supports dynamic templates with conditional content: ``` {{#if priority == 'high'}} \u26a0\ufe0f URGENT: Immediate attention required {{else}} \u2139\ufe0f Informational notification {{/if}} ``` ### Slack Messages Send messages to Slack channels or users: **Channel Messages:** ```yaml - id: notify_team action: slack_message config: channel: \"#deployments\" text: \"Deployment completed successfully!\" blocks: - type: \"section\" text: type: \"mrkdwn\" text: \"*Deployment Summary*\\nEnvironment: {{environment}}\\nVersion: {{version}}\\nDuration: {{duration}}s\" - type: \"actions\" elements: - type: \"button\" text: \"View Logs\" url: \"{{logs_url}}\" thread_ts: \"{{trigger.thread_ts}}\" # Reply to thread ``` **Direct Messages:** ```yaml - id: dm_user action: slack_message config: user: \"{{user_slack_id}}\" text: \"Your report is ready for review.\"",
              "score": null
            },
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_4",
              "content": "### Error Response Format ```json { \"error\": { \"code\": \"invalid_parameter\", \"message\": \"The 'limit' parameter must be between 1 and 100\", \"field\": \"limit\", \"request_id\": \"req_8k3m9x2p\" } } ``` ### HTTP Status Codes | Status Code | Description | Common Causes | |------------|-------------|---------------| | 400 | Bad Request | Invalid parameters, malformed JSON, validation errors | | 401 | Unauthorized | Missing or invalid authentication credentials | | 403 | Forbidden | Insufficient permissions for requested resource | | 404 | Not Found | Resource does not exist or has been deleted | | 429 | Too Many Requests | Rate limit exceeded, retry after specified period | | 500 | Internal Server Error | Unexpected server error, contact support if persists | | 502 | Bad Gateway | Temporary service issue, retry with exponential backoff | | 503 | Service Unavailable | Scheduled maintenance or temporary outage | ### Error Codes CloudFlow returns specific error codes to help you identify and resolve issues: - `invalid_parameter`: One or more request parameters are invalid - `missing_required_field`: Required field is missing from request body - `authentication_failed`: Invalid API key or token - `insufficient_permissions`: User lacks required scope or permission - `resource_not_found`: Requested resource does not exist - `rate_limit_exceeded`: Too many requests, see rate limiting section - `workflow_execution_failed`: Workflow execution encountered an error - `invalid_json`: Request body contains malformed JSON - `duplicate_resource`: Resource with same identifier already exists - `quota_exceeded`: Account quota limit reached ### Error Handling Best Practices **Python Example:** ```python import requests import time def make_api_request(url, headers, max_retries=3): for attempt in range(max_retries): try: response = requests.get(url, headers=headers) if response.status_code == 200: return response.json() elif response.status_code == 429: # Rate limit exceeded retry_after = int(response.headers.get('X-RateLimit-Reset', 60)) print(f\"Rate limited. Waiting {retry_after} seconds...\") time.sleep(retry_after) continue elif response.status_code >= 500: # Server error - retry with exponential backoff wait_time = 2 ** attempt print(f\"Server error. Retrying in {wait_time} seconds...\") time.sleep(wait_time) continue else: # Client error - don't retry error_data = response.json() raise Exception(f\"API Error: {error_data['error']['message']}\") except requests.exceptions.RequestException as e: print(f\"Request failed: {e}\") if attempt < max_retries - 1: time.sleep(2 ** attempt) else: raise raise Exception(\"Max retries exceeded\") ``` ## Webhooks CloudFlow can send webhook notifications when specific events occur in your workflows.",
              "score": null
            },
            {
              "doc_id": "user_guide",
              "chunk_id": "user_guide_fix_6",
              "content": "Filter by error type, date range, or execution ID 3. Click an execution to view full details 4. Click **\"Retry\"** to reprocess with the same input data ### Error Notifications Get notified when workflows fail: ```yaml workflow: notifications: on_failure: - type: email to: \"ops-team@company.com\" - type: slack channel: \"#alerts\" message: \"Workflow {{workflow.name}} failed: {{error.message}}\" on_success_after_retry: - type: slack channel: \"#monitoring\" message: \"Workflow recovered after {{error.attempts}} attempts\" ``` ## Workflow Limits CloudFlow enforces the following limits to ensure platform stability and performance: ### Steps Per Workflow - **Maximum**: 50 steps per workflow - **Recommendation**: Keep workflows focused and modular. If you need more steps, consider splitting into multiple workflows connected via webhooks. ### Execution Timeout - **Default**: 3600 seconds (60 minutes) - **Behavior**: Workflows exceeding this timeout are automatically terminated - **Custom Timeouts**: Enterprise plans can request custom timeout limits **Setting Step-Level Timeouts:** ```yaml - id: long_running_task action: http_request config: url: \"https://api.example.com/process\" timeout: 300 # 5 minutes for this specific step ``` ### Execution Limits - **Maximum**: 1000 executions per day (per workflow) - **Rate Limiting**: 100 concurrent executions per workflow - **Burst Limit**: 10 executions per second **What happens when limits are reached:** - New executions are queued automatically - Webhook triggers return HTTP 429 (Too Many Requests) - Scheduled executions are skipped (logged in audit trail) - Email notifications sent to workflow owner **Monitoring Usage:** View real-time metrics in your workflow dashboard: - Executions today: 847 / 1000 - Average duration: 12.3s - Success rate: 98.2% - Current queue depth: 3 ### Data Limits - **Maximum request/response size**: 10MB per action - **Maximum execution payload**: 50MB total - **Variable value size**: 1MB per variable ### Enterprise Plan Limits Enterprise customers can request increased limits: - Up to 100 steps per workflow - Up to 10,000 executions per day - Up to 7200 second timeout (2 hours) - Priority execution queue - Dedicated capacity allocation Contact sales@cloudflow.io for Enterprise pricing and custom limits. ## Best Practices Follow these best practices to build reliable, maintainable workflows: ### 1. Use Descriptive Names **Good:** - Workflow: \"Sync Customer Data from Salesforce to Database\" - Step: \"validate_customer_email\" **Bad:** - Workflow: \"Workflow 1\" - Step: \"step3\" ### 2.",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_1",
              "content": "**Technology**: Node.js with Express.js framework **Replicas**: 12 pods (production), auto-scaling 8-20 based on CPU **Resource Allocation**: 2 vCPU, 4GB RAM per pod **Key Responsibilities**: - JWT token validation (delegated to Auth Service for initial validation) - Rate limiting: 1000 requests per minute per API key (sliding window) - Request/response transformation and validation using JSON Schema - Routing to downstream services based on URL path patterns - CORS handling for web clients - Request/response logging and correlation ID injection **Critical Endpoints**: - `POST /api/v1/workflows` - Create new workflow - `GET /api/v1/workflows/:id` - Retrieve workflow status - `POST /api/v1/workflows/:id/execute` - Trigger workflow execution - `GET /api/v1/workflows/:id/history` - Get execution history **Dependencies**: - Auth Service (for token validation) - Redis (for rate limiting counters) - All downstream microservices **Performance Targets**: - P50 latency: < 50ms - P99 latency: < 200ms - Throughput: 10,000 RPS sustained --- ### Auth Service **Purpose**: Centralized authentication and authorization service handling user identity, token generation, and permission validation. **Technology**: Go with gRPC for internal communication, REST for external **Replicas**: 8 pods (production), auto-scaling 6-12 **Resource Allocation**: 1 vCPU, 2GB RAM per pod **Key Responsibilities**: - User authentication via multiple providers (OAuth2, SAML, local credentials) - JWT token generation and validation (RS256 algorithm) - Role-based access control (RBAC) with fine-grained permissions - Session management with Redis-backed storage - API key generation and validation for service accounts - MFA enforcement for administrative operations **Authentication Flow**: ``` Client Request \u2192 API Gateway \u2192 Auth Service \u2502 \u251c\u2500 Validate credentials \u251c\u2500 Check MFA if required \u251c\u2500 Generate JWT (15min expiry) \u251c\u2500 Generate refresh token (7 days) \u2514\u2500 Store session in Redis ``` **Token Structure**: - Access Token: JWT with 15-minute expiry - Refresh Token: Opaque token with 7-day expiry, stored in PostgreSQL - Claims: user_id, email, roles[], permissions[], tenant_id **Security Features**: - Password hashing: Argon2id with 64MB memory, 4 iterations - Token rotation on refresh to prevent replay attacks - Brute force protection: 5 failed attempts \u2192 15-minute lockout - Secrets stored in HashiCorp Vault, rotated every 90 days **Performance Targets**: - Token validation: < 10ms (P99) - Token generation: < 50ms (P99) - Throughput: 5,000 RPS for validation operations --- ### Workflow Engine **Purpose**: Core orchestration service that executes workflow definitions, manages state transitions, and coordinates task execution across distributed systems.",
              "score": null
            }
          ],
          "latency_ms": 11.438857000030112,
          "query_id": "realistic_001",
          "dimension": "synonym",
          "query": "How many requests can I make per minute to the API?"
        },
        {
          "key_facts": [
            "100 requests per minute per authenticated user",
            "20 requests per minute for unauthenticated requests"
          ],
          "found_facts": [
            "100 requests per minute per authenticated user",
            "20 requests per minute for unauthenticated requests"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "api_reference"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_1",
              "content": "## Rate Limiting To ensure fair usage and system stability, CloudFlow enforces rate limits on all API endpoints. **Default Limits:** - 100 requests per minute per authenticated user - 20 requests per minute for unauthenticated requests - Burst allowance: 150 requests in a 10-second window ### Rate Limit Headers Every API response includes rate limit information: ``` X-RateLimit-Limit: 100 X-RateLimit-Remaining: 87 X-RateLimit-Reset: 1640995200 ``` When you exceed the rate limit, you'll receive a `429 Too Many Requests` response: ```json { \"error\": { \"code\": \"rate_limit_exceeded\", \"message\": \"Rate limit exceeded. Please retry after 42 seconds.\", \"retry_after\": 42 } } ``` **Best Practices:** - Monitor `X-RateLimit-Remaining` header values - Implement exponential backoff when receiving 429 responses - Cache responses when appropriate to reduce API calls - Consider upgrading to Enterprise tier for higher limits (1000 req/min) ## Pagination List endpoints return paginated results to optimize performance. Use `limit` and `offset` parameters to navigate through result sets. **Parameters:** - `limit`: Number of items per page (default: 20, max: 100) - `offset`: Number of items to skip (default: 0) **Example Request:** ```bash curl -H \"X-API-Key: YOUR_API_KEY\" \\ \"https://api.cloudflow.io/v2/workflows?limit=50&offset=100\" ``` **Response Structure:** ```json { \"data\": [...], \"pagination\": { \"total\": 347, \"limit\": 50, \"offset\": 100, \"has_more\": true } } ``` **Python Example:** ```python import requests def fetch_all_workflows(api_key): base_url = \"https://api.cloudflow.io/v2/workflows\" headers = {\"X-API-Key\": api_key} all_workflows = [] offset = 0 limit = 100 while True: response = requests.get( base_url, headers=headers, params={\"limit\": limit, \"offset\": offset} ) data = response.json() all_workflows.extend(data['data']) if not data['pagination']['has_more']: break offset += limit return all_workflows ``` ## Endpoints ### Workflows #### List Workflows Retrieve a paginated list of all workflows in your account. **Endpoint:** `GET /workflows` **Query Parameters:** - `status` (optional): Filter by status (`active`, `paused`, `archived`) - `created_after` (optional): ISO 8601 timestamp - `limit` (optional): Items per page (max 100) - `offset` (optional): Pagination offset **Example Request:** ```bash curl -H \"X-API-Key: YOUR_API_KEY\" \\ \"https://api.cloudflow.io/v2/workflows?status=active&limit=25\" ``` **Example Response (200 OK):** ```json { \"data\": [ { \"id\": \"wf_8x7k2m9p\", \"name\": \"Data Processing Pipeline\", \"description\": \"Processes customer data every hour\", \"status\": \"active\", \"trigger\": { \"type\": \"schedule\", \"cron\": \"0 * * * *\" }, \"steps\": 12, \"created_at\": \"2026-01-15T10:30:00Z\", \"updated_at\": \"2026-01-23T14:22:00Z\", \"last_run\": \"2026-01-24T09:00:00Z\" } ], \"pagination\": { \"total\": 1, \"limit\": 25, \"offset\": 0, \"has_more\": false } } ``` #### Create Workflow Create a new workflow with specified configuration.",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_7",
              "content": "Waiting {retry_after} seconds...\") time.sleep(retry_after) continue remaining = int(response.headers.get('X-RateLimit-Remaining', 0)) if remaining < 10: print(f\"Warning: Only {remaining} requests remaining\") return response raise Exception(\"Max retries exceeded due to rate limiting\") ``` **Bash script with rate limit checking:** ```bash #!/bin/bash check_rate_limit() { local remaining=$(curl -s -I https://api.cloudflow.io/api/v1/workflows \\ -H \"Authorization: Bearer $CF_ACCESS_TOKEN\" | \\ grep -i \"x-ratelimit-remaining\" | \\ awk '{print $2}' | tr -d '\\r') if [ \"$remaining\" -lt 10 ]; then echo \"Warning: Only $remaining requests remaining\" local reset=$(curl -s -I https://api.cloudflow.io/api/v1/workflows \\ -H \"Authorization: Bearer $CF_ACCESS_TOKEN\" | \\ grep -i \"x-ratelimit-reset\" | \\ awk '{print $2}' | tr -d '\\r') local wait_time=$((reset - $(date +%s))) echo \"Rate limit resets in $wait_time seconds\" sleep $wait_time fi } # Use before API calls check_rate_limit cloudflow workflows execute wf_9k2n4m8p1q ``` #### Optimization Strategies **1. Implement request batching:** ```bash # Batch multiple workflow executions cloudflow workflows execute-batch \\ --workflow-ids \"wf_id1,wf_id2,wf_id3,wf_id4,wf_id5\" \\ --batch-size 5 # This counts as 1 API request instead of 5 ``` **2. Use webhooks instead of polling:** ```bash # Configure webhook for workflow completion cloudflow webhooks create \\ --event workflow.completed \\ --url https://your-service.com/webhooks/cloudflow \\ --secret $WEBHOOK_SECRET # Verify webhook cloudflow webhooks test webhook_abc123 ``` **3.",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_0",
              "content": "# CloudFlow Platform Troubleshooting Guide **Version:** 3.2.1 **Last Updated:** January 2026 **Audience:** Platform Engineers, DevOps, Support Teams ## Table of Contents 1. [Overview](#overview) 2. [Authentication & Authorization Issues](#authentication--authorization-issues) 3. [Performance Problems](#performance-problems) 4. [Database Connection Issues](#database-connection-issues) 5. [Workflow Execution Failures](#workflow-execution-failures) 6. [Rate Limiting & Throttling](#rate-limiting--throttling) 7. [Log Analysis & Debugging](#log-analysis--debugging) 8. [Escalation Procedures](#escalation-procedures) --- ## Overview This guide provides comprehensive troubleshooting steps for common CloudFlow platform issues encountered in production environments. Each section includes error symptoms, root cause analysis, resolution steps, and preventive measures. ### Quick Diagnostic Checklist Before diving into specific issues, perform these initial checks: - Verify service health: `cloudflow status --all` - Check API connectivity: `curl -I https://api.cloudflow.io/health` - Review recent deployments: `kubectl get deployments -n cloudflow --sort-by=.metadata.creationTimestamp` - Inspect platform metrics: `cloudflow metrics --last 1h` --- ## Authentication & Authorization Issues ### 401 Unauthorized Errors #### Symptoms - API requests return `401 Unauthorized` - Error message: `Authentication credentials were not provided or are invalid` - Frontend displays \"Session expired\" message #### Common Causes **1. Token Expiration** CloudFlow access tokens expire after 3600 seconds (1 hour) by default. Refresh tokens are valid for 30 days. **Verification:** ```bash # Decode JWT to check expiration echo $CF_ACCESS_TOKEN | cut -d'.' -f2 | base64 -d | jq '.exp' # Compare with current time date +%s ``` **Resolution:** ```bash # Refresh the access token curl -X POST https://api.cloudflow.io/auth/refresh \\ -H \"Content-Type: application/json\" \\ -d '{\"refresh_token\": \"'$CF_REFRESH_TOKEN'\"}' # Update environment variable export CF_ACCESS_TOKEN=\"<new_token>\" ``` **2. Invalid JWT Signature** Error: `JWT signature verification failed` **Causes:** - Token was modified or corrupted - Using wrong signing key - Token generated with different secret **Resolution:** ```bash # Validate token structure cloudflow auth validate --token $CF_ACCESS_TOKEN # Generate new token cloudflow auth login --username <user> --password <pass> # For service accounts cloudflow auth service-account --create --name <sa-name> --scopes \"workflow:read,workflow:write\" ``` **3. Clock Skew Issues** JWT validation fails when system clocks are out of sync (tolerance: \u00b1300 seconds).",
              "score": null
            },
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_4",
              "content": "### Error Response Format ```json { \"error\": { \"code\": \"invalid_parameter\", \"message\": \"The 'limit' parameter must be between 1 and 100\", \"field\": \"limit\", \"request_id\": \"req_8k3m9x2p\" } } ``` ### HTTP Status Codes | Status Code | Description | Common Causes | |------------|-------------|---------------| | 400 | Bad Request | Invalid parameters, malformed JSON, validation errors | | 401 | Unauthorized | Missing or invalid authentication credentials | | 403 | Forbidden | Insufficient permissions for requested resource | | 404 | Not Found | Resource does not exist or has been deleted | | 429 | Too Many Requests | Rate limit exceeded, retry after specified period | | 500 | Internal Server Error | Unexpected server error, contact support if persists | | 502 | Bad Gateway | Temporary service issue, retry with exponential backoff | | 503 | Service Unavailable | Scheduled maintenance or temporary outage | ### Error Codes CloudFlow returns specific error codes to help you identify and resolve issues: - `invalid_parameter`: One or more request parameters are invalid - `missing_required_field`: Required field is missing from request body - `authentication_failed`: Invalid API key or token - `insufficient_permissions`: User lacks required scope or permission - `resource_not_found`: Requested resource does not exist - `rate_limit_exceeded`: Too many requests, see rate limiting section - `workflow_execution_failed`: Workflow execution encountered an error - `invalid_json`: Request body contains malformed JSON - `duplicate_resource`: Resource with same identifier already exists - `quota_exceeded`: Account quota limit reached ### Error Handling Best Practices **Python Example:** ```python import requests import time def make_api_request(url, headers, max_retries=3): for attempt in range(max_retries): try: response = requests.get(url, headers=headers) if response.status_code == 200: return response.json() elif response.status_code == 429: # Rate limit exceeded retry_after = int(response.headers.get('X-RateLimit-Reset', 60)) print(f\"Rate limited. Waiting {retry_after} seconds...\") time.sleep(retry_after) continue elif response.status_code >= 500: # Server error - retry with exponential backoff wait_time = 2 ** attempt print(f\"Server error. Retrying in {wait_time} seconds...\") time.sleep(wait_time) continue else: # Client error - don't retry error_data = response.json() raise Exception(f\"API Error: {error_data['error']['message']}\") except requests.exceptions.RequestException as e: print(f\"Request failed: {e}\") if attempt < max_retries - 1: time.sleep(2 ** attempt) else: raise raise Exception(\"Max retries exceeded\") ``` ## Webhooks CloudFlow can send webhook notifications when specific events occur in your workflows.",
              "score": null
            },
            {
              "doc_id": "user_guide",
              "chunk_id": "user_guide_fix_7",
              "content": "Handle Errors Gracefully Always implement error handling for external API calls and database operations: ```yaml - id: fetch_data action: http_request config: url: \"https://api.example.com/data\" retry: max_attempts: 3 backoff_type: exponential on_error: - id: log_error action: database_query config: query: \"INSERT INTO error_log (workflow_id, error) VALUES ($1, $2)\" parameters: - \"{{workflow.id}}\" - \"{{error.message}}\" ``` ### 3. Use Secrets for Sensitive Data Never hardcode API keys, passwords, or tokens in workflows: **Bad:** ```yaml headers: Authorization: \"Bearer sk_live_abc123xyz789\" ``` **Good:** ```yaml headers: Authorization: \"Bearer {{secrets.API_TOKEN}}\" ``` Store secrets in **Settings** > **Secrets** with encryption at rest. ### 4. Validate Input Data Always validate trigger data before processing: ```yaml - id: validate_input action: javascript code: | const required_fields = ['email', 'name', 'order_id']; for (const field of required_fields) { if (!input[field]) { throw new Error(`Missing required field: ${field}`); } } // Validate email format const email_regex = /^[^\\s@]+@[^\\s@]+\\.[^\\s@]+$/; if (!email_regex.test(input.email)) { throw new Error('Invalid email format'); } return { validated: true }; ``` ### 5. Use Idempotency Keys For operations that shouldn't be repeated (payments, record creation), use idempotency keys: ```yaml - id: create_charge action: http_request config: url: \"https://api.stripe.com/v1/charges\" method: POST headers: Idempotency-Key: \"{{workflow.id}}-{{execution.id}}\" body: amount: \"{{amount}}\" ``` ### 6. Monitor and Log Add logging steps for important workflow milestones: ```yaml - id: log_start action: database_query config: query: \"INSERT INTO workflow_audit (execution_id, step, timestamp) VALUES ($1, $2, $3)\" parameters: - \"{{execution.id}}\" - \"workflow_started\" - \"{{now()}}\" ``` ### 7. Keep Workflows Modular Break complex workflows into smaller, reusable components: - Use sub-workflows for repeated logic - Trigger child workflows via webhooks - Share common configurations via templates ### 8. Test Thoroughly Before activating a workflow: 1. Use test mode with sample data 2. Verify all actions execute correctly 3. Test error handling paths 4. Review execution logs 5. Start with a limited scope (e.g., test channel, small dataset) ### 9. Document Your Workflows Add descriptions to workflows and steps: ```yaml workflow: name: \"Daily Sales Report\" description: | Generates a daily sales report and distributes it to the sales team. Runs at 8:00 AM EST Monday-Friday. Data source: PostgreSQL sales database Recipients: sales-team@company.com steps: - id: fetch_sales name: \"Fetch Yesterday's Sales\" description: \"Query database for all completed orders from previous day\" ``` ### 10.",
              "score": null
            }
          ],
          "latency_ms": 11.694584998622304,
          "query_id": "realistic_001",
          "dimension": "problem",
          "query": "My API calls are getting blocked, what's the limit?"
        },
        {
          "key_facts": [
            "100 requests per minute per authenticated user",
            "20 requests per minute for unauthenticated requests"
          ],
          "found_facts": [
            "100 requests per minute per authenticated user",
            "20 requests per minute for unauthenticated requests"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "api_reference"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_1",
              "content": "## Rate Limiting To ensure fair usage and system stability, CloudFlow enforces rate limits on all API endpoints. **Default Limits:** - 100 requests per minute per authenticated user - 20 requests per minute for unauthenticated requests - Burst allowance: 150 requests in a 10-second window ### Rate Limit Headers Every API response includes rate limit information: ``` X-RateLimit-Limit: 100 X-RateLimit-Remaining: 87 X-RateLimit-Reset: 1640995200 ``` When you exceed the rate limit, you'll receive a `429 Too Many Requests` response: ```json { \"error\": { \"code\": \"rate_limit_exceeded\", \"message\": \"Rate limit exceeded. Please retry after 42 seconds.\", \"retry_after\": 42 } } ``` **Best Practices:** - Monitor `X-RateLimit-Remaining` header values - Implement exponential backoff when receiving 429 responses - Cache responses when appropriate to reduce API calls - Consider upgrading to Enterprise tier for higher limits (1000 req/min) ## Pagination List endpoints return paginated results to optimize performance. Use `limit` and `offset` parameters to navigate through result sets. **Parameters:** - `limit`: Number of items per page (default: 20, max: 100) - `offset`: Number of items to skip (default: 0) **Example Request:** ```bash curl -H \"X-API-Key: YOUR_API_KEY\" \\ \"https://api.cloudflow.io/v2/workflows?limit=50&offset=100\" ``` **Response Structure:** ```json { \"data\": [...], \"pagination\": { \"total\": 347, \"limit\": 50, \"offset\": 100, \"has_more\": true } } ``` **Python Example:** ```python import requests def fetch_all_workflows(api_key): base_url = \"https://api.cloudflow.io/v2/workflows\" headers = {\"X-API-Key\": api_key} all_workflows = [] offset = 0 limit = 100 while True: response = requests.get( base_url, headers=headers, params={\"limit\": limit, \"offset\": offset} ) data = response.json() all_workflows.extend(data['data']) if not data['pagination']['has_more']: break offset += limit return all_workflows ``` ## Endpoints ### Workflows #### List Workflows Retrieve a paginated list of all workflows in your account. **Endpoint:** `GET /workflows` **Query Parameters:** - `status` (optional): Filter by status (`active`, `paused`, `archived`) - `created_after` (optional): ISO 8601 timestamp - `limit` (optional): Items per page (max 100) - `offset` (optional): Pagination offset **Example Request:** ```bash curl -H \"X-API-Key: YOUR_API_KEY\" \\ \"https://api.cloudflow.io/v2/workflows?status=active&limit=25\" ``` **Example Response (200 OK):** ```json { \"data\": [ { \"id\": \"wf_8x7k2m9p\", \"name\": \"Data Processing Pipeline\", \"description\": \"Processes customer data every hour\", \"status\": \"active\", \"trigger\": { \"type\": \"schedule\", \"cron\": \"0 * * * *\" }, \"steps\": 12, \"created_at\": \"2026-01-15T10:30:00Z\", \"updated_at\": \"2026-01-23T14:22:00Z\", \"last_run\": \"2026-01-24T09:00:00Z\" } ], \"pagination\": { \"total\": 1, \"limit\": 25, \"offset\": 0, \"has_more\": false } } ``` #### Create Workflow Create a new workflow with specified configuration.",
              "score": null
            },
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_4",
              "content": "### Error Response Format ```json { \"error\": { \"code\": \"invalid_parameter\", \"message\": \"The 'limit' parameter must be between 1 and 100\", \"field\": \"limit\", \"request_id\": \"req_8k3m9x2p\" } } ``` ### HTTP Status Codes | Status Code | Description | Common Causes | |------------|-------------|---------------| | 400 | Bad Request | Invalid parameters, malformed JSON, validation errors | | 401 | Unauthorized | Missing or invalid authentication credentials | | 403 | Forbidden | Insufficient permissions for requested resource | | 404 | Not Found | Resource does not exist or has been deleted | | 429 | Too Many Requests | Rate limit exceeded, retry after specified period | | 500 | Internal Server Error | Unexpected server error, contact support if persists | | 502 | Bad Gateway | Temporary service issue, retry with exponential backoff | | 503 | Service Unavailable | Scheduled maintenance or temporary outage | ### Error Codes CloudFlow returns specific error codes to help you identify and resolve issues: - `invalid_parameter`: One or more request parameters are invalid - `missing_required_field`: Required field is missing from request body - `authentication_failed`: Invalid API key or token - `insufficient_permissions`: User lacks required scope or permission - `resource_not_found`: Requested resource does not exist - `rate_limit_exceeded`: Too many requests, see rate limiting section - `workflow_execution_failed`: Workflow execution encountered an error - `invalid_json`: Request body contains malformed JSON - `duplicate_resource`: Resource with same identifier already exists - `quota_exceeded`: Account quota limit reached ### Error Handling Best Practices **Python Example:** ```python import requests import time def make_api_request(url, headers, max_retries=3): for attempt in range(max_retries): try: response = requests.get(url, headers=headers) if response.status_code == 200: return response.json() elif response.status_code == 429: # Rate limit exceeded retry_after = int(response.headers.get('X-RateLimit-Reset', 60)) print(f\"Rate limited. Waiting {retry_after} seconds...\") time.sleep(retry_after) continue elif response.status_code >= 500: # Server error - retry with exponential backoff wait_time = 2 ** attempt print(f\"Server error. Retrying in {wait_time} seconds...\") time.sleep(wait_time) continue else: # Client error - don't retry error_data = response.json() raise Exception(f\"API Error: {error_data['error']['message']}\") except requests.exceptions.RequestException as e: print(f\"Request failed: {e}\") if attempt < max_retries - 1: time.sleep(2 ** attempt) else: raise raise Exception(\"Max retries exceeded\") ``` ## Webhooks CloudFlow can send webhook notifications when specific events occur in your workflows.",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_7",
              "content": "Waiting {retry_after} seconds...\") time.sleep(retry_after) continue remaining = int(response.headers.get('X-RateLimit-Remaining', 0)) if remaining < 10: print(f\"Warning: Only {remaining} requests remaining\") return response raise Exception(\"Max retries exceeded due to rate limiting\") ``` **Bash script with rate limit checking:** ```bash #!/bin/bash check_rate_limit() { local remaining=$(curl -s -I https://api.cloudflow.io/api/v1/workflows \\ -H \"Authorization: Bearer $CF_ACCESS_TOKEN\" | \\ grep -i \"x-ratelimit-remaining\" | \\ awk '{print $2}' | tr -d '\\r') if [ \"$remaining\" -lt 10 ]; then echo \"Warning: Only $remaining requests remaining\" local reset=$(curl -s -I https://api.cloudflow.io/api/v1/workflows \\ -H \"Authorization: Bearer $CF_ACCESS_TOKEN\" | \\ grep -i \"x-ratelimit-reset\" | \\ awk '{print $2}' | tr -d '\\r') local wait_time=$((reset - $(date +%s))) echo \"Rate limit resets in $wait_time seconds\" sleep $wait_time fi } # Use before API calls check_rate_limit cloudflow workflows execute wf_9k2n4m8p1q ``` #### Optimization Strategies **1. Implement request batching:** ```bash # Batch multiple workflow executions cloudflow workflows execute-batch \\ --workflow-ids \"wf_id1,wf_id2,wf_id3,wf_id4,wf_id5\" \\ --batch-size 5 # This counts as 1 API request instead of 5 ``` **2. Use webhooks instead of polling:** ```bash # Configure webhook for workflow completion cloudflow webhooks create \\ --event workflow.completed \\ --url https://your-service.com/webhooks/cloudflow \\ --secret $WEBHOOK_SECRET # Verify webhook cloudflow webhooks test webhook_abc123 ``` **3.",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_6",
              "content": "Data Validation Errors** ``` ValidationError: Field 'customer_id' is required but missing in 234 records ``` **Resolution:** ```bash # Add data quality checks cloudflow workflows update wf_9k2n4m8p1q \\ --step data_ingestion \\ --add-validator required_fields \\ --validator-config '{\"fields\": [\"customer_id\", \"timestamp\", \"amount\"]}' # Configure error handling cloudflow workflows update wf_9k2n4m8p1q \\ --step data_validation \\ --on-error continue \\ --error-threshold 5% # Fail if > 5% of records invalid ``` **2. External API Failures** ``` ExternalAPIError: API request to https://partner-api.example.com failed with status 502 ``` **Resolution:** ```bash # Add circuit breaker cloudflow workflows update wf_9k2n4m8p1q \\ --step external_api_call \\ --circuit-breaker-enabled true \\ --circuit-breaker-threshold 5 \\ --circuit-breaker-timeout 30000 # Configure fallback behavior cloudflow workflows update wf_9k2n4m8p1q \\ --step external_api_call \\ --fallback-action use_cached_data \\ --cache-ttl 3600 ``` --- ## Rate Limiting & Throttling ### 429 Too Many Requests #### Error Response ```json { \"error\": \"rate_limit_exceeded\", \"message\": \"API rate limit exceeded. Retry after 45 seconds.\", \"status\": 429, \"headers\": { \"X-RateLimit-Limit\": \"1000\", \"X-RateLimit-Remaining\": \"0\", \"X-RateLimit-Reset\": \"1706112345\", \"Retry-After\": \"45\" } } ``` #### Rate Limit Tiers CloudFlow enforces the following rate limits per workspace: | Tier | Requests/Minute | Requests/Hour | Concurrent Workflows | |------|-----------------|---------------|----------------------| | Free | 60 | 1,000 | 5 | | Standard | 1,000 | 50,000 | 50 | | Premium | 5,000 | 250,000 | 200 | | Enterprise | Custom | Custom | Unlimited | #### Checking Rate Limit Status ```bash # Check current rate limit status curl -I https://api.cloudflow.io/api/v1/workflows \\ -H \"Authorization: Bearer $CF_ACCESS_TOKEN\" # Extract rate limit headers curl -s -I https://api.cloudflow.io/api/v1/workflows \\ -H \"Authorization: Bearer $CF_ACCESS_TOKEN\" | \\ grep -i \"x-ratelimit\" # Output: # X-RateLimit-Limit: 1000 # X-RateLimit-Remaining: 247 # X-RateLimit-Reset: 1706112400 # Monitor rate limit usage cloudflow metrics query \\ --metric rate_limit_remaining \\ --workspace ws_abc123 \\ --last 1h \\ --interval 1m ``` #### Handling Rate Limits in Code **Python example with retry logic:** ```python import time import requests def cloudflow_api_call_with_retry(url, headers, max_retries=3): for attempt in range(max_retries): response = requests.get(url, headers=headers) if response.status_code == 429: retry_after = int(response.headers.get('Retry-After', 60)) print(f\"Rate limited.",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_10",
              "content": "- Execution start rate: 500 per second - Completion rate: 450 per second (average 2-second execution time) **Database**: - Read throughput: 50,000 queries per second (across replicas) - Write throughput: 15,000 transactions per second - Connection capacity: 2,000 concurrent connections **Kafka**: - Message ingestion: 100,000 messages per second - Consumer throughput: 80,000 messages per second (aggregated) - End-to-end latency: < 100ms (P99) ### Resource Utilization **CPU Utilization** (Target: 60-70% average): - API Gateway: 55% average, 80% peak - Workflow Engine: 65% average, 85% peak - Auth Service: 40% average, 60% peak **Memory Utilization** (Target: < 80%): - API Gateway: 2.5GB average per pod (4GB allocated) - Workflow Engine: 6GB average per pod (8GB allocated) - Notification Service: 2.8GB average per pod (4GB allocated) **Network Throughput**: - Ingress: 2 Gbps average, 5 Gbps peak - Egress: 1.5 Gbps average, 4 Gbps peak - Internal (service mesh): 8 Gbps average ### Monitoring & Alerting **Key Metrics** (Monitored via Prometheus): - Request rate (per service, per endpoint) - Error rate (4xx, 5xx responses) - Latency percentiles (P50, P95, P99) - Resource utilization (CPU, memory, disk) - Cache hit/miss ratios - Database connection pool saturation - Kafka consumer lag **Alerts** (via PagerDuty): - P1 (Immediate): API error rate > 1%, database connection failure - P2 (< 15 min): P99 latency > 500ms, cache hit rate < 80% - P3 (< 1 hour): Resource utilization > 85%, replica count < minimum **SLI/SLO/SLA**: - SLI: API success rate (non-5xx responses) - SLO: 99.9% uptime per month (43 minutes downtime allowance) - SLA: 99.5% uptime guarantee (customer-facing SLA with credits) --- ## Disaster Recovery ### Recovery Objectives **RPO (Recovery Point Objective): 1 hour** - Maximum acceptable data loss: 1 hour of transactions - Achieved through: Continuous database replication + hourly snapshots - Kafka retention: 7 days allows event replay **RTO (Recovery Time Objective): 4 hours** - Maximum acceptable downtime: 4 hours for full system recovery - Includes: Failover, data verification, and service restoration - Automated runbooks reduce RTO to < 2 hours for common scenarios ### High Availability Architecture **Multi-AZ Deployment**: ``` Region: us-east-1 AZ-1a: AZ-1b: AZ-1c: - API Gateway (4) - API Gateway (4) - API Gateway (4) - Workflow Engine (6) - Workflow Engine (5) - Workflow Engine (5) - Auth Service (3) - Auth Service (3) - Auth",
              "score": null
            }
          ],
          "latency_ms": 11.461970996606397,
          "query_id": "realistic_001",
          "dimension": "casual",
          "query": "api rate limit"
        },
        {
          "key_facts": [
            "100 requests per minute per authenticated user",
            "20 requests per minute for unauthenticated requests"
          ],
          "found_facts": [
            "100 requests per minute per authenticated user",
            "20 requests per minute for unauthenticated requests"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "api_reference"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_7",
              "content": "Waiting {retry_after} seconds...\") time.sleep(retry_after) continue remaining = int(response.headers.get('X-RateLimit-Remaining', 0)) if remaining < 10: print(f\"Warning: Only {remaining} requests remaining\") return response raise Exception(\"Max retries exceeded due to rate limiting\") ``` **Bash script with rate limit checking:** ```bash #!/bin/bash check_rate_limit() { local remaining=$(curl -s -I https://api.cloudflow.io/api/v1/workflows \\ -H \"Authorization: Bearer $CF_ACCESS_TOKEN\" | \\ grep -i \"x-ratelimit-remaining\" | \\ awk '{print $2}' | tr -d '\\r') if [ \"$remaining\" -lt 10 ]; then echo \"Warning: Only $remaining requests remaining\" local reset=$(curl -s -I https://api.cloudflow.io/api/v1/workflows \\ -H \"Authorization: Bearer $CF_ACCESS_TOKEN\" | \\ grep -i \"x-ratelimit-reset\" | \\ awk '{print $2}' | tr -d '\\r') local wait_time=$((reset - $(date +%s))) echo \"Rate limit resets in $wait_time seconds\" sleep $wait_time fi } # Use before API calls check_rate_limit cloudflow workflows execute wf_9k2n4m8p1q ``` #### Optimization Strategies **1. Implement request batching:** ```bash # Batch multiple workflow executions cloudflow workflows execute-batch \\ --workflow-ids \"wf_id1,wf_id2,wf_id3,wf_id4,wf_id5\" \\ --batch-size 5 # This counts as 1 API request instead of 5 ``` **2. Use webhooks instead of polling:** ```bash # Configure webhook for workflow completion cloudflow webhooks create \\ --event workflow.completed \\ --url https://your-service.com/webhooks/cloudflow \\ --secret $WEBHOOK_SECRET # Verify webhook cloudflow webhooks test webhook_abc123 ``` **3.",
              "score": null
            },
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_1",
              "content": "## Rate Limiting To ensure fair usage and system stability, CloudFlow enforces rate limits on all API endpoints. **Default Limits:** - 100 requests per minute per authenticated user - 20 requests per minute for unauthenticated requests - Burst allowance: 150 requests in a 10-second window ### Rate Limit Headers Every API response includes rate limit information: ``` X-RateLimit-Limit: 100 X-RateLimit-Remaining: 87 X-RateLimit-Reset: 1640995200 ``` When you exceed the rate limit, you'll receive a `429 Too Many Requests` response: ```json { \"error\": { \"code\": \"rate_limit_exceeded\", \"message\": \"Rate limit exceeded. Please retry after 42 seconds.\", \"retry_after\": 42 } } ``` **Best Practices:** - Monitor `X-RateLimit-Remaining` header values - Implement exponential backoff when receiving 429 responses - Cache responses when appropriate to reduce API calls - Consider upgrading to Enterprise tier for higher limits (1000 req/min) ## Pagination List endpoints return paginated results to optimize performance. Use `limit` and `offset` parameters to navigate through result sets. **Parameters:** - `limit`: Number of items per page (default: 20, max: 100) - `offset`: Number of items to skip (default: 0) **Example Request:** ```bash curl -H \"X-API-Key: YOUR_API_KEY\" \\ \"https://api.cloudflow.io/v2/workflows?limit=50&offset=100\" ``` **Response Structure:** ```json { \"data\": [...], \"pagination\": { \"total\": 347, \"limit\": 50, \"offset\": 100, \"has_more\": true } } ``` **Python Example:** ```python import requests def fetch_all_workflows(api_key): base_url = \"https://api.cloudflow.io/v2/workflows\" headers = {\"X-API-Key\": api_key} all_workflows = [] offset = 0 limit = 100 while True: response = requests.get( base_url, headers=headers, params={\"limit\": limit, \"offset\": offset} ) data = response.json() all_workflows.extend(data['data']) if not data['pagination']['has_more']: break offset += limit return all_workflows ``` ## Endpoints ### Workflows #### List Workflows Retrieve a paginated list of all workflows in your account. **Endpoint:** `GET /workflows` **Query Parameters:** - `status` (optional): Filter by status (`active`, `paused`, `archived`) - `created_after` (optional): ISO 8601 timestamp - `limit` (optional): Items per page (max 100) - `offset` (optional): Pagination offset **Example Request:** ```bash curl -H \"X-API-Key: YOUR_API_KEY\" \\ \"https://api.cloudflow.io/v2/workflows?status=active&limit=25\" ``` **Example Response (200 OK):** ```json { \"data\": [ { \"id\": \"wf_8x7k2m9p\", \"name\": \"Data Processing Pipeline\", \"description\": \"Processes customer data every hour\", \"status\": \"active\", \"trigger\": { \"type\": \"schedule\", \"cron\": \"0 * * * *\" }, \"steps\": 12, \"created_at\": \"2026-01-15T10:30:00Z\", \"updated_at\": \"2026-01-23T14:22:00Z\", \"last_run\": \"2026-01-24T09:00:00Z\" } ], \"pagination\": { \"total\": 1, \"limit\": 25, \"offset\": 0, \"has_more\": false } } ``` #### Create Workflow Create a new workflow with specified configuration.",
              "score": null
            },
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_0",
              "content": "# CloudFlow API Reference Version 2.1.0 | Last Updated: January 2026 ## Overview The CloudFlow API is a RESTful service that enables developers to programmatically manage cloud workflows, data pipelines, and automation tasks. This documentation provides comprehensive details on authentication, endpoints, request/response formats, error handling, and best practices. **Base URL:** `https://api.cloudflow.io/v2` **API Status:** https://status.cloudflow.io ## Authentication CloudFlow supports three authentication methods to suit different use cases and security requirements. ### API Keys API keys provide simple authentication for server-to-server communication. Include your API key in the request header: ```bash curl -H \"X-API-Key: cf_live_a1b2c3d4e5f6g7h8i9j0\" \\ https://api.cloudflow.io/v2/workflows ``` **Security Notes:** - Never expose API keys in client-side code - Rotate keys every 90 days - Use separate keys for development and production environments ### OAuth 2.0 OAuth 2.0 is recommended for applications that access CloudFlow on behalf of users. We support the Authorization Code flow with PKCE. **Authorization Endpoint:** `https://auth.cloudflow.io/oauth/authorize` **Token Endpoint:** `https://auth.cloudflow.io/oauth/token` **Supported Scopes:** - `workflows:read` - Read workflow configurations - `workflows:write` - Create and modify workflows - `pipelines:read` - Read pipeline data - `pipelines:write` - Create and manage pipelines - `analytics:read` - Access analytics and metrics - `admin:full` - Full administrative access Example authorization request: ```bash curl -X POST https://auth.cloudflow.io/oauth/token \\ -H \"Content-Type: application/x-www-form-urlencoded\" \\ -d \"grant_type=authorization_code\" \\ -d \"code=AUTH_CODE_HERE\" \\ -d \"client_id=YOUR_CLIENT_ID\" \\ -d \"client_secret=YOUR_CLIENT_SECRET\" \\ -d \"redirect_uri=https://yourapp.com/callback\" ``` ### JWT Tokens For advanced use cases, CloudFlow supports JSON Web Tokens (JWT) with RS256 signing algorithm. JWTs must include the following claims: - `iss` (issuer): Your application identifier - `sub` (subject): User or service account ID - `aud` (audience): `https://api.cloudflow.io` - `exp` (expiration): Unix timestamp (max 3600 seconds from `iat`) - `iat` (issued at): Unix timestamp - `scope`: Space-separated list of requested scopes Example JWT header: ```python import jwt import time from cryptography.hazmat.primitives import serialization from cryptography.hazmat.backends import default_backend # Load your private key with open('private_key.pem', 'rb') as key_file: private_key = serialization.load_pem_private_key( key_file.read(), password=None, backend=default_backend() ) # Create JWT payload payload = { 'iss': 'your-app-id', 'sub': 'user-12345', 'aud': 'https://api.cloudflow.io', 'exp': int(time.time()) + 3600, 'iat': int(time.time()), 'scope': 'workflows:read workflows:write' } # Generate token token = jwt.encode(payload, private_key, algorithm='RS256') # Use in API request headers = {'Authorization': f'Bearer {token}'} ``` **Token Expiration:** All tokens expire after 3600 seconds (1 hour). Implement token refresh logic in your application.",
              "score": null
            },
            {
              "doc_id": "user_guide",
              "chunk_id": "user_guide_fix_0",
              "content": "# CloudFlow User Guide Welcome to CloudFlow, the modern workflow automation platform that helps you connect your apps, automate repetitive tasks, and build powerful integrations without writing code. ## Table of Contents 1. [Getting Started](#getting-started) 2. [Workflow Creation](#workflow-creation) 3. [Available Actions](#available-actions) 4. [Variables and Expressions](#variables-and-expressions) 5. [Scheduling](#scheduling) 6. [Error Handling](#error-handling) 7. [Workflow Limits](#workflow-limits) 8. [Best Practices](#best-practices) 9. [Common Workflow Patterns](#common-workflow-patterns) ## Getting Started ### Account Setup Creating your CloudFlow account takes just a few minutes: 1. Navigate to https://app.cloudflow.io/signup 2. Enter your email address and create a strong password 3. Verify your email address by clicking the link sent to your inbox 4. Complete your profile by providing your organization name and use case 5. Choose your plan (Free, Professional, or Enterprise) Once your account is created, you'll be redirected to your CloudFlow dashboard where you can start building workflows immediately. ### Your First Workflow Let's create a simple workflow that sends you a Slack notification when a new file is uploaded to your Google Drive: 1. Click the **\"Create Workflow\"** button in the top-right corner 2. Give your workflow a name: \"Google Drive to Slack Notifier\" 3. Select **\"Google Drive - New File\"** as your trigger 4. Authenticate your Google Drive account when prompted 5. Choose the folder you want to monitor 6. Click **\"Add Action\"** and select **\"Slack - Send Message\"** 7. Authenticate your Slack account 8. Select the channel where you want to receive notifications 9. Customize your message using variables: `New file uploaded: {{trigger.file.name}}` 10. Click **\"Save & Activate\"** to enable your workflow Congratulations! Your first workflow is now live. Every time a file is added to the specified Google Drive folder, you'll receive a Slack notification. ## Workflow Creation CloudFlow offers multiple ways to create and manage workflows, giving you the flexibility to work in the way that suits you best. ### Visual Editor The Visual Editor is CloudFlow's drag-and-drop interface for building workflows without code. It's perfect for users who prefer a graphical approach: **Key Features:** - Drag-and-drop action blocks from the sidebar - Visual connections between steps show your workflow logic - Inline configuration for each action - Real-time validation and error checking - Test mode to run your workflow with sample data **Using the Visual Editor:** 1.",
              "score": null
            },
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_4",
              "content": "### Error Response Format ```json { \"error\": { \"code\": \"invalid_parameter\", \"message\": \"The 'limit' parameter must be between 1 and 100\", \"field\": \"limit\", \"request_id\": \"req_8k3m9x2p\" } } ``` ### HTTP Status Codes | Status Code | Description | Common Causes | |------------|-------------|---------------| | 400 | Bad Request | Invalid parameters, malformed JSON, validation errors | | 401 | Unauthorized | Missing or invalid authentication credentials | | 403 | Forbidden | Insufficient permissions for requested resource | | 404 | Not Found | Resource does not exist or has been deleted | | 429 | Too Many Requests | Rate limit exceeded, retry after specified period | | 500 | Internal Server Error | Unexpected server error, contact support if persists | | 502 | Bad Gateway | Temporary service issue, retry with exponential backoff | | 503 | Service Unavailable | Scheduled maintenance or temporary outage | ### Error Codes CloudFlow returns specific error codes to help you identify and resolve issues: - `invalid_parameter`: One or more request parameters are invalid - `missing_required_field`: Required field is missing from request body - `authentication_failed`: Invalid API key or token - `insufficient_permissions`: User lacks required scope or permission - `resource_not_found`: Requested resource does not exist - `rate_limit_exceeded`: Too many requests, see rate limiting section - `workflow_execution_failed`: Workflow execution encountered an error - `invalid_json`: Request body contains malformed JSON - `duplicate_resource`: Resource with same identifier already exists - `quota_exceeded`: Account quota limit reached ### Error Handling Best Practices **Python Example:** ```python import requests import time def make_api_request(url, headers, max_retries=3): for attempt in range(max_retries): try: response = requests.get(url, headers=headers) if response.status_code == 200: return response.json() elif response.status_code == 429: # Rate limit exceeded retry_after = int(response.headers.get('X-RateLimit-Reset', 60)) print(f\"Rate limited. Waiting {retry_after} seconds...\") time.sleep(retry_after) continue elif response.status_code >= 500: # Server error - retry with exponential backoff wait_time = 2 ** attempt print(f\"Server error. Retrying in {wait_time} seconds...\") time.sleep(wait_time) continue else: # Client error - don't retry error_data = response.json() raise Exception(f\"API Error: {error_data['error']['message']}\") except requests.exceptions.RequestException as e: print(f\"Request failed: {e}\") if attempt < max_retries - 1: time.sleep(2 ** attempt) else: raise raise Exception(\"Max retries exceeded\") ``` ## Webhooks CloudFlow can send webhook notifications when specific events occur in your workflows.",
              "score": null
            }
          ],
          "latency_ms": 11.786023998865858,
          "query_id": "realistic_001",
          "dimension": "contextual",
          "query": "I'm building a batch job that calls CloudFlow API repeatedly, what rate limit should I expect?"
        },
        {
          "key_facts": [
            "100 requests per minute per authenticated user",
            "20 requests per minute for unauthenticated requests"
          ],
          "found_facts": [
            "100 requests per minute per authenticated user",
            "20 requests per minute for unauthenticated requests"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "api_reference"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_1",
              "content": "## Rate Limiting To ensure fair usage and system stability, CloudFlow enforces rate limits on all API endpoints. **Default Limits:** - 100 requests per minute per authenticated user - 20 requests per minute for unauthenticated requests - Burst allowance: 150 requests in a 10-second window ### Rate Limit Headers Every API response includes rate limit information: ``` X-RateLimit-Limit: 100 X-RateLimit-Remaining: 87 X-RateLimit-Reset: 1640995200 ``` When you exceed the rate limit, you'll receive a `429 Too Many Requests` response: ```json { \"error\": { \"code\": \"rate_limit_exceeded\", \"message\": \"Rate limit exceeded. Please retry after 42 seconds.\", \"retry_after\": 42 } } ``` **Best Practices:** - Monitor `X-RateLimit-Remaining` header values - Implement exponential backoff when receiving 429 responses - Cache responses when appropriate to reduce API calls - Consider upgrading to Enterprise tier for higher limits (1000 req/min) ## Pagination List endpoints return paginated results to optimize performance. Use `limit` and `offset` parameters to navigate through result sets. **Parameters:** - `limit`: Number of items per page (default: 20, max: 100) - `offset`: Number of items to skip (default: 0) **Example Request:** ```bash curl -H \"X-API-Key: YOUR_API_KEY\" \\ \"https://api.cloudflow.io/v2/workflows?limit=50&offset=100\" ``` **Response Structure:** ```json { \"data\": [...], \"pagination\": { \"total\": 347, \"limit\": 50, \"offset\": 100, \"has_more\": true } } ``` **Python Example:** ```python import requests def fetch_all_workflows(api_key): base_url = \"https://api.cloudflow.io/v2/workflows\" headers = {\"X-API-Key\": api_key} all_workflows = [] offset = 0 limit = 100 while True: response = requests.get( base_url, headers=headers, params={\"limit\": limit, \"offset\": offset} ) data = response.json() all_workflows.extend(data['data']) if not data['pagination']['has_more']: break offset += limit return all_workflows ``` ## Endpoints ### Workflows #### List Workflows Retrieve a paginated list of all workflows in your account. **Endpoint:** `GET /workflows` **Query Parameters:** - `status` (optional): Filter by status (`active`, `paused`, `archived`) - `created_after` (optional): ISO 8601 timestamp - `limit` (optional): Items per page (max 100) - `offset` (optional): Pagination offset **Example Request:** ```bash curl -H \"X-API-Key: YOUR_API_KEY\" \\ \"https://api.cloudflow.io/v2/workflows?status=active&limit=25\" ``` **Example Response (200 OK):** ```json { \"data\": [ { \"id\": \"wf_8x7k2m9p\", \"name\": \"Data Processing Pipeline\", \"description\": \"Processes customer data every hour\", \"status\": \"active\", \"trigger\": { \"type\": \"schedule\", \"cron\": \"0 * * * *\" }, \"steps\": 12, \"created_at\": \"2026-01-15T10:30:00Z\", \"updated_at\": \"2026-01-23T14:22:00Z\", \"last_run\": \"2026-01-24T09:00:00Z\" } ], \"pagination\": { \"total\": 1, \"limit\": 25, \"offset\": 0, \"has_more\": false } } ``` #### Create Workflow Create a new workflow with specified configuration.",
              "score": null
            },
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_4",
              "content": "### Error Response Format ```json { \"error\": { \"code\": \"invalid_parameter\", \"message\": \"The 'limit' parameter must be between 1 and 100\", \"field\": \"limit\", \"request_id\": \"req_8k3m9x2p\" } } ``` ### HTTP Status Codes | Status Code | Description | Common Causes | |------------|-------------|---------------| | 400 | Bad Request | Invalid parameters, malformed JSON, validation errors | | 401 | Unauthorized | Missing or invalid authentication credentials | | 403 | Forbidden | Insufficient permissions for requested resource | | 404 | Not Found | Resource does not exist or has been deleted | | 429 | Too Many Requests | Rate limit exceeded, retry after specified period | | 500 | Internal Server Error | Unexpected server error, contact support if persists | | 502 | Bad Gateway | Temporary service issue, retry with exponential backoff | | 503 | Service Unavailable | Scheduled maintenance or temporary outage | ### Error Codes CloudFlow returns specific error codes to help you identify and resolve issues: - `invalid_parameter`: One or more request parameters are invalid - `missing_required_field`: Required field is missing from request body - `authentication_failed`: Invalid API key or token - `insufficient_permissions`: User lacks required scope or permission - `resource_not_found`: Requested resource does not exist - `rate_limit_exceeded`: Too many requests, see rate limiting section - `workflow_execution_failed`: Workflow execution encountered an error - `invalid_json`: Request body contains malformed JSON - `duplicate_resource`: Resource with same identifier already exists - `quota_exceeded`: Account quota limit reached ### Error Handling Best Practices **Python Example:** ```python import requests import time def make_api_request(url, headers, max_retries=3): for attempt in range(max_retries): try: response = requests.get(url, headers=headers) if response.status_code == 200: return response.json() elif response.status_code == 429: # Rate limit exceeded retry_after = int(response.headers.get('X-RateLimit-Reset', 60)) print(f\"Rate limited. Waiting {retry_after} seconds...\") time.sleep(retry_after) continue elif response.status_code >= 500: # Server error - retry with exponential backoff wait_time = 2 ** attempt print(f\"Server error. Retrying in {wait_time} seconds...\") time.sleep(wait_time) continue else: # Client error - don't retry error_data = response.json() raise Exception(f\"API Error: {error_data['error']['message']}\") except requests.exceptions.RequestException as e: print(f\"Request failed: {e}\") if attempt < max_retries - 1: time.sleep(2 ** attempt) else: raise raise Exception(\"Max retries exceeded\") ``` ## Webhooks CloudFlow can send webhook notifications when specific events occur in your workflows.",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_6",
              "content": "Data Validation Errors** ``` ValidationError: Field 'customer_id' is required but missing in 234 records ``` **Resolution:** ```bash # Add data quality checks cloudflow workflows update wf_9k2n4m8p1q \\ --step data_ingestion \\ --add-validator required_fields \\ --validator-config '{\"fields\": [\"customer_id\", \"timestamp\", \"amount\"]}' # Configure error handling cloudflow workflows update wf_9k2n4m8p1q \\ --step data_validation \\ --on-error continue \\ --error-threshold 5% # Fail if > 5% of records invalid ``` **2. External API Failures** ``` ExternalAPIError: API request to https://partner-api.example.com failed with status 502 ``` **Resolution:** ```bash # Add circuit breaker cloudflow workflows update wf_9k2n4m8p1q \\ --step external_api_call \\ --circuit-breaker-enabled true \\ --circuit-breaker-threshold 5 \\ --circuit-breaker-timeout 30000 # Configure fallback behavior cloudflow workflows update wf_9k2n4m8p1q \\ --step external_api_call \\ --fallback-action use_cached_data \\ --cache-ttl 3600 ``` --- ## Rate Limiting & Throttling ### 429 Too Many Requests #### Error Response ```json { \"error\": \"rate_limit_exceeded\", \"message\": \"API rate limit exceeded. Retry after 45 seconds.\", \"status\": 429, \"headers\": { \"X-RateLimit-Limit\": \"1000\", \"X-RateLimit-Remaining\": \"0\", \"X-RateLimit-Reset\": \"1706112345\", \"Retry-After\": \"45\" } } ``` #### Rate Limit Tiers CloudFlow enforces the following rate limits per workspace: | Tier | Requests/Minute | Requests/Hour | Concurrent Workflows | |------|-----------------|---------------|----------------------| | Free | 60 | 1,000 | 5 | | Standard | 1,000 | 50,000 | 50 | | Premium | 5,000 | 250,000 | 200 | | Enterprise | Custom | Custom | Unlimited | #### Checking Rate Limit Status ```bash # Check current rate limit status curl -I https://api.cloudflow.io/api/v1/workflows \\ -H \"Authorization: Bearer $CF_ACCESS_TOKEN\" # Extract rate limit headers curl -s -I https://api.cloudflow.io/api/v1/workflows \\ -H \"Authorization: Bearer $CF_ACCESS_TOKEN\" | \\ grep -i \"x-ratelimit\" # Output: # X-RateLimit-Limit: 1000 # X-RateLimit-Remaining: 247 # X-RateLimit-Reset: 1706112400 # Monitor rate limit usage cloudflow metrics query \\ --metric rate_limit_remaining \\ --workspace ws_abc123 \\ --last 1h \\ --interval 1m ``` #### Handling Rate Limits in Code **Python example with retry logic:** ```python import time import requests def cloudflow_api_call_with_retry(url, headers, max_retries=3): for attempt in range(max_retries): response = requests.get(url, headers=headers) if response.status_code == 429: retry_after = int(response.headers.get('Retry-After', 60)) print(f\"Rate limited.",
              "score": null
            },
            {
              "doc_id": "user_guide",
              "chunk_id": "user_guide_fix_2",
              "content": "### HTTP Requests Make HTTP requests to any API endpoint: **Configuration:** - **Method**: GET, POST, PUT, PATCH, DELETE, HEAD - **URL**: Full endpoint URL (supports variable interpolation) - **Headers**: Custom headers as key-value pairs - **Query Parameters**: URL parameters - **Body**: JSON, form data, or raw text - **Authentication**: Basic Auth, Bearer Token, API Key, OAuth 2.0 **Example:** ```yaml - id: fetch_user action: http_request config: method: GET url: \"https://api.example.com/users/{{user_id}}\" headers: Authorization: \"Bearer {{secrets.API_TOKEN}}\" Content-Type: \"application/json\" timeout: 30 ``` **Response Handling:** Access the response in subsequent steps: - `{{steps.fetch_user.status}}` - HTTP status code - `{{steps.fetch_user.body}}` - Response body - `{{steps.fetch_user.headers}}` - Response headers ### Database Queries Execute queries against supported databases (PostgreSQL, MySQL, MongoDB, Redis): **SQL Databases (PostgreSQL, MySQL):** ```yaml - id: get_orders action: database_query config: connection: \"{{secrets.DB_CONNECTION_STRING}}\" query: | SELECT * FROM orders WHERE customer_id = $1 AND status = $2 ORDER BY created_at DESC LIMIT 10 parameters: - \"{{trigger.customer_id}}\" - \"pending\" ``` **MongoDB:** ```yaml - id: find_documents action: mongodb_query config: connection: \"{{secrets.MONGO_URI}}\" database: \"production\" collection: \"users\" operation: find filter: email: \"{{trigger.email}}\" options: limit: 1 ``` **Important Notes:** - Always use parameterized queries to prevent SQL injection - Connection strings should be stored in secrets - Query timeout is 30 seconds by default - Maximum result set size is 10MB ### Email Notifications Send emails via SMTP or integrated email providers: **Configuration:** ```yaml - id: send_notification action: email config: provider: \"smtp\" # or \"sendgrid\", \"mailgun\", \"ses\" from: \"notifications@cloudflow.io\" to: \"{{trigger.recipient}}\" cc: \"manager@company.com\" subject: \"Alert: {{alert_type}}\" body: | Hello {{user.name}}, This is an automated notification about {{event_description}}. Details: - Time: {{timestamp}} - Type: {{alert_type}} - Priority: {{priority}} Best regards, CloudFlow Automation html: true attachments: - name: \"report.pdf\" content: \"{{steps.generate_report.output}}\" encoding: \"base64\" ``` **Email Templates:** CloudFlow supports dynamic templates with conditional content: ``` {{#if priority == 'high'}} \u26a0\ufe0f URGENT: Immediate attention required {{else}} \u2139\ufe0f Informational notification {{/if}} ``` ### Slack Messages Send messages to Slack channels or users: **Channel Messages:** ```yaml - id: notify_team action: slack_message config: channel: \"#deployments\" text: \"Deployment completed successfully!\" blocks: - type: \"section\" text: type: \"mrkdwn\" text: \"*Deployment Summary*\\nEnvironment: {{environment}}\\nVersion: {{version}}\\nDuration: {{duration}}s\" - type: \"actions\" elements: - type: \"button\" text: \"View Logs\" url: \"{{logs_url}}\" thread_ts: \"{{trigger.thread_ts}}\" # Reply to thread ``` **Direct Messages:** ```yaml - id: dm_user action: slack_message config: user: \"{{user_slack_id}}\" text: \"Your report is ready for review.\"",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_7",
              "content": "Waiting {retry_after} seconds...\") time.sleep(retry_after) continue remaining = int(response.headers.get('X-RateLimit-Remaining', 0)) if remaining < 10: print(f\"Warning: Only {remaining} requests remaining\") return response raise Exception(\"Max retries exceeded due to rate limiting\") ``` **Bash script with rate limit checking:** ```bash #!/bin/bash check_rate_limit() { local remaining=$(curl -s -I https://api.cloudflow.io/api/v1/workflows \\ -H \"Authorization: Bearer $CF_ACCESS_TOKEN\" | \\ grep -i \"x-ratelimit-remaining\" | \\ awk '{print $2}' | tr -d '\\r') if [ \"$remaining\" -lt 10 ]; then echo \"Warning: Only $remaining requests remaining\" local reset=$(curl -s -I https://api.cloudflow.io/api/v1/workflows \\ -H \"Authorization: Bearer $CF_ACCESS_TOKEN\" | \\ grep -i \"x-ratelimit-reset\" | \\ awk '{print $2}' | tr -d '\\r') local wait_time=$((reset - $(date +%s))) echo \"Rate limit resets in $wait_time seconds\" sleep $wait_time fi } # Use before API calls check_rate_limit cloudflow workflows execute wf_9k2n4m8p1q ``` #### Optimization Strategies **1. Implement request batching:** ```bash # Batch multiple workflow executions cloudflow workflows execute-batch \\ --workflow-ids \"wf_id1,wf_id2,wf_id3,wf_id4,wf_id5\" \\ --batch-size 5 # This counts as 1 API request instead of 5 ``` **2. Use webhooks instead of polling:** ```bash # Configure webhook for workflow completion cloudflow webhooks create \\ --event workflow.completed \\ --url https://your-service.com/webhooks/cloudflow \\ --secret $WEBHOOK_SECRET # Verify webhook cloudflow webhooks test webhook_abc123 ``` **3.",
              "score": null
            }
          ],
          "latency_ms": 11.503328001708724,
          "query_id": "realistic_001",
          "dimension": "negation",
          "query": "Why is the API rejecting my requests after 100 calls?"
        },
        {
          "key_facts": [
            "429 Too Many Requests",
            "X-RateLimit-Remaining",
            "Retry-After",
            "Monitor X-RateLimit-Remaining header values",
            "Implement exponential backoff when receiving 429 responses"
          ],
          "found_facts": [
            "429 Too Many Requests",
            "X-RateLimit-Remaining",
            "Retry-After",
            "Implement exponential backoff when receiving 429 responses"
          ],
          "missed_facts": [
            "Monitor X-RateLimit-Remaining header values"
          ],
          "coverage": 0.8,
          "expected_docs": [
            "api_reference",
            "troubleshooting_guide"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_4",
              "content": "### Error Response Format ```json { \"error\": { \"code\": \"invalid_parameter\", \"message\": \"The 'limit' parameter must be between 1 and 100\", \"field\": \"limit\", \"request_id\": \"req_8k3m9x2p\" } } ``` ### HTTP Status Codes | Status Code | Description | Common Causes | |------------|-------------|---------------| | 400 | Bad Request | Invalid parameters, malformed JSON, validation errors | | 401 | Unauthorized | Missing or invalid authentication credentials | | 403 | Forbidden | Insufficient permissions for requested resource | | 404 | Not Found | Resource does not exist or has been deleted | | 429 | Too Many Requests | Rate limit exceeded, retry after specified period | | 500 | Internal Server Error | Unexpected server error, contact support if persists | | 502 | Bad Gateway | Temporary service issue, retry with exponential backoff | | 503 | Service Unavailable | Scheduled maintenance or temporary outage | ### Error Codes CloudFlow returns specific error codes to help you identify and resolve issues: - `invalid_parameter`: One or more request parameters are invalid - `missing_required_field`: Required field is missing from request body - `authentication_failed`: Invalid API key or token - `insufficient_permissions`: User lacks required scope or permission - `resource_not_found`: Requested resource does not exist - `rate_limit_exceeded`: Too many requests, see rate limiting section - `workflow_execution_failed`: Workflow execution encountered an error - `invalid_json`: Request body contains malformed JSON - `duplicate_resource`: Resource with same identifier already exists - `quota_exceeded`: Account quota limit reached ### Error Handling Best Practices **Python Example:** ```python import requests import time def make_api_request(url, headers, max_retries=3): for attempt in range(max_retries): try: response = requests.get(url, headers=headers) if response.status_code == 200: return response.json() elif response.status_code == 429: # Rate limit exceeded retry_after = int(response.headers.get('X-RateLimit-Reset', 60)) print(f\"Rate limited. Waiting {retry_after} seconds...\") time.sleep(retry_after) continue elif response.status_code >= 500: # Server error - retry with exponential backoff wait_time = 2 ** attempt print(f\"Server error. Retrying in {wait_time} seconds...\") time.sleep(wait_time) continue else: # Client error - don't retry error_data = response.json() raise Exception(f\"API Error: {error_data['error']['message']}\") except requests.exceptions.RequestException as e: print(f\"Request failed: {e}\") if attempt < max_retries - 1: time.sleep(2 ** attempt) else: raise raise Exception(\"Max retries exceeded\") ``` ## Webhooks CloudFlow can send webhook notifications when specific events occur in your workflows.",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_6",
              "content": "Data Validation Errors** ``` ValidationError: Field 'customer_id' is required but missing in 234 records ``` **Resolution:** ```bash # Add data quality checks cloudflow workflows update wf_9k2n4m8p1q \\ --step data_ingestion \\ --add-validator required_fields \\ --validator-config '{\"fields\": [\"customer_id\", \"timestamp\", \"amount\"]}' # Configure error handling cloudflow workflows update wf_9k2n4m8p1q \\ --step data_validation \\ --on-error continue \\ --error-threshold 5% # Fail if > 5% of records invalid ``` **2. External API Failures** ``` ExternalAPIError: API request to https://partner-api.example.com failed with status 502 ``` **Resolution:** ```bash # Add circuit breaker cloudflow workflows update wf_9k2n4m8p1q \\ --step external_api_call \\ --circuit-breaker-enabled true \\ --circuit-breaker-threshold 5 \\ --circuit-breaker-timeout 30000 # Configure fallback behavior cloudflow workflows update wf_9k2n4m8p1q \\ --step external_api_call \\ --fallback-action use_cached_data \\ --cache-ttl 3600 ``` --- ## Rate Limiting & Throttling ### 429 Too Many Requests #### Error Response ```json { \"error\": \"rate_limit_exceeded\", \"message\": \"API rate limit exceeded. Retry after 45 seconds.\", \"status\": 429, \"headers\": { \"X-RateLimit-Limit\": \"1000\", \"X-RateLimit-Remaining\": \"0\", \"X-RateLimit-Reset\": \"1706112345\", \"Retry-After\": \"45\" } } ``` #### Rate Limit Tiers CloudFlow enforces the following rate limits per workspace: | Tier | Requests/Minute | Requests/Hour | Concurrent Workflows | |------|-----------------|---------------|----------------------| | Free | 60 | 1,000 | 5 | | Standard | 1,000 | 50,000 | 50 | | Premium | 5,000 | 250,000 | 200 | | Enterprise | Custom | Custom | Unlimited | #### Checking Rate Limit Status ```bash # Check current rate limit status curl -I https://api.cloudflow.io/api/v1/workflows \\ -H \"Authorization: Bearer $CF_ACCESS_TOKEN\" # Extract rate limit headers curl -s -I https://api.cloudflow.io/api/v1/workflows \\ -H \"Authorization: Bearer $CF_ACCESS_TOKEN\" | \\ grep -i \"x-ratelimit\" # Output: # X-RateLimit-Limit: 1000 # X-RateLimit-Remaining: 247 # X-RateLimit-Reset: 1706112400 # Monitor rate limit usage cloudflow metrics query \\ --metric rate_limit_remaining \\ --workspace ws_abc123 \\ --last 1h \\ --interval 1m ``` #### Handling Rate Limits in Code **Python example with retry logic:** ```python import time import requests def cloudflow_api_call_with_retry(url, headers, max_retries=3): for attempt in range(max_retries): response = requests.get(url, headers=headers) if response.status_code == 429: retry_after = int(response.headers.get('Retry-After', 60)) print(f\"Rate limited.",
              "score": null
            },
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_1",
              "content": "## Rate Limiting To ensure fair usage and system stability, CloudFlow enforces rate limits on all API endpoints. **Default Limits:** - 100 requests per minute per authenticated user - 20 requests per minute for unauthenticated requests - Burst allowance: 150 requests in a 10-second window ### Rate Limit Headers Every API response includes rate limit information: ``` X-RateLimit-Limit: 100 X-RateLimit-Remaining: 87 X-RateLimit-Reset: 1640995200 ``` When you exceed the rate limit, you'll receive a `429 Too Many Requests` response: ```json { \"error\": { \"code\": \"rate_limit_exceeded\", \"message\": \"Rate limit exceeded. Please retry after 42 seconds.\", \"retry_after\": 42 } } ``` **Best Practices:** - Monitor `X-RateLimit-Remaining` header values - Implement exponential backoff when receiving 429 responses - Cache responses when appropriate to reduce API calls - Consider upgrading to Enterprise tier for higher limits (1000 req/min) ## Pagination List endpoints return paginated results to optimize performance. Use `limit` and `offset` parameters to navigate through result sets. **Parameters:** - `limit`: Number of items per page (default: 20, max: 100) - `offset`: Number of items to skip (default: 0) **Example Request:** ```bash curl -H \"X-API-Key: YOUR_API_KEY\" \\ \"https://api.cloudflow.io/v2/workflows?limit=50&offset=100\" ``` **Response Structure:** ```json { \"data\": [...], \"pagination\": { \"total\": 347, \"limit\": 50, \"offset\": 100, \"has_more\": true } } ``` **Python Example:** ```python import requests def fetch_all_workflows(api_key): base_url = \"https://api.cloudflow.io/v2/workflows\" headers = {\"X-API-Key\": api_key} all_workflows = [] offset = 0 limit = 100 while True: response = requests.get( base_url, headers=headers, params={\"limit\": limit, \"offset\": offset} ) data = response.json() all_workflows.extend(data['data']) if not data['pagination']['has_more']: break offset += limit return all_workflows ``` ## Endpoints ### Workflows #### List Workflows Retrieve a paginated list of all workflows in your account. **Endpoint:** `GET /workflows` **Query Parameters:** - `status` (optional): Filter by status (`active`, `paused`, `archived`) - `created_after` (optional): ISO 8601 timestamp - `limit` (optional): Items per page (max 100) - `offset` (optional): Pagination offset **Example Request:** ```bash curl -H \"X-API-Key: YOUR_API_KEY\" \\ \"https://api.cloudflow.io/v2/workflows?status=active&limit=25\" ``` **Example Response (200 OK):** ```json { \"data\": [ { \"id\": \"wf_8x7k2m9p\", \"name\": \"Data Processing Pipeline\", \"description\": \"Processes customer data every hour\", \"status\": \"active\", \"trigger\": { \"type\": \"schedule\", \"cron\": \"0 * * * *\" }, \"steps\": 12, \"created_at\": \"2026-01-15T10:30:00Z\", \"updated_at\": \"2026-01-23T14:22:00Z\", \"last_run\": \"2026-01-24T09:00:00Z\" } ], \"pagination\": { \"total\": 1, \"limit\": 25, \"offset\": 0, \"has_more\": false } } ``` #### Create Workflow Create a new workflow with specified configuration.",
              "score": null
            },
            {
              "doc_id": "user_guide",
              "chunk_id": "user_guide_fix_6",
              "content": "Filter by error type, date range, or execution ID 3. Click an execution to view full details 4. Click **\"Retry\"** to reprocess with the same input data ### Error Notifications Get notified when workflows fail: ```yaml workflow: notifications: on_failure: - type: email to: \"ops-team@company.com\" - type: slack channel: \"#alerts\" message: \"Workflow {{workflow.name}} failed: {{error.message}}\" on_success_after_retry: - type: slack channel: \"#monitoring\" message: \"Workflow recovered after {{error.attempts}} attempts\" ``` ## Workflow Limits CloudFlow enforces the following limits to ensure platform stability and performance: ### Steps Per Workflow - **Maximum**: 50 steps per workflow - **Recommendation**: Keep workflows focused and modular. If you need more steps, consider splitting into multiple workflows connected via webhooks. ### Execution Timeout - **Default**: 3600 seconds (60 minutes) - **Behavior**: Workflows exceeding this timeout are automatically terminated - **Custom Timeouts**: Enterprise plans can request custom timeout limits **Setting Step-Level Timeouts:** ```yaml - id: long_running_task action: http_request config: url: \"https://api.example.com/process\" timeout: 300 # 5 minutes for this specific step ``` ### Execution Limits - **Maximum**: 1000 executions per day (per workflow) - **Rate Limiting**: 100 concurrent executions per workflow - **Burst Limit**: 10 executions per second **What happens when limits are reached:** - New executions are queued automatically - Webhook triggers return HTTP 429 (Too Many Requests) - Scheduled executions are skipped (logged in audit trail) - Email notifications sent to workflow owner **Monitoring Usage:** View real-time metrics in your workflow dashboard: - Executions today: 847 / 1000 - Average duration: 12.3s - Success rate: 98.2% - Current queue depth: 3 ### Data Limits - **Maximum request/response size**: 10MB per action - **Maximum execution payload**: 50MB total - **Variable value size**: 1MB per variable ### Enterprise Plan Limits Enterprise customers can request increased limits: - Up to 100 steps per workflow - Up to 10,000 executions per day - Up to 7200 second timeout (2 hours) - Priority execution queue - Dedicated capacity allocation Contact sales@cloudflow.io for Enterprise pricing and custom limits. ## Best Practices Follow these best practices to build reliable, maintainable workflows: ### 1. Use Descriptive Names **Good:** - Workflow: \"Sync Customer Data from Salesforce to Database\" - Step: \"validate_customer_email\" **Bad:** - Workflow: \"Workflow 1\" - Step: \"step3\" ### 2.",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_7",
              "content": "Waiting {retry_after} seconds...\") time.sleep(retry_after) continue remaining = int(response.headers.get('X-RateLimit-Remaining', 0)) if remaining < 10: print(f\"Warning: Only {remaining} requests remaining\") return response raise Exception(\"Max retries exceeded due to rate limiting\") ``` **Bash script with rate limit checking:** ```bash #!/bin/bash check_rate_limit() { local remaining=$(curl -s -I https://api.cloudflow.io/api/v1/workflows \\ -H \"Authorization: Bearer $CF_ACCESS_TOKEN\" | \\ grep -i \"x-ratelimit-remaining\" | \\ awk '{print $2}' | tr -d '\\r') if [ \"$remaining\" -lt 10 ]; then echo \"Warning: Only $remaining requests remaining\" local reset=$(curl -s -I https://api.cloudflow.io/api/v1/workflows \\ -H \"Authorization: Bearer $CF_ACCESS_TOKEN\" | \\ grep -i \"x-ratelimit-reset\" | \\ awk '{print $2}' | tr -d '\\r') local wait_time=$((reset - $(date +%s))) echo \"Rate limit resets in $wait_time seconds\" sleep $wait_time fi } # Use before API calls check_rate_limit cloudflow workflows execute wf_9k2n4m8p1q ``` #### Optimization Strategies **1. Implement request batching:** ```bash # Batch multiple workflow executions cloudflow workflows execute-batch \\ --workflow-ids \"wf_id1,wf_id2,wf_id3,wf_id4,wf_id5\" \\ --batch-size 5 # This counts as 1 API request instead of 5 ``` **2. Use webhooks instead of polling:** ```bash # Configure webhook for workflow completion cloudflow webhooks create \\ --event workflow.completed \\ --url https://your-service.com/webhooks/cloudflow \\ --secret $WEBHOOK_SECRET # Verify webhook cloudflow webhooks test webhook_abc123 ``` **3.",
              "score": null
            }
          ],
          "latency_ms": 11.401187999581452,
          "query_id": "realistic_002",
          "dimension": "original",
          "query": "How do I fix 429 Too Many Requests errors?"
        },
        {
          "key_facts": [
            "429 Too Many Requests",
            "X-RateLimit-Remaining",
            "Retry-After",
            "Monitor X-RateLimit-Remaining header values",
            "Implement exponential backoff when receiving 429 responses"
          ],
          "found_facts": [
            "429 Too Many Requests",
            "X-RateLimit-Remaining",
            "Implement exponential backoff when receiving 429 responses"
          ],
          "missed_facts": [
            "Retry-After",
            "Monitor X-RateLimit-Remaining header values"
          ],
          "coverage": 0.6,
          "expected_docs": [
            "api_reference",
            "troubleshooting_guide"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_1",
              "content": "## Rate Limiting To ensure fair usage and system stability, CloudFlow enforces rate limits on all API endpoints. **Default Limits:** - 100 requests per minute per authenticated user - 20 requests per minute for unauthenticated requests - Burst allowance: 150 requests in a 10-second window ### Rate Limit Headers Every API response includes rate limit information: ``` X-RateLimit-Limit: 100 X-RateLimit-Remaining: 87 X-RateLimit-Reset: 1640995200 ``` When you exceed the rate limit, you'll receive a `429 Too Many Requests` response: ```json { \"error\": { \"code\": \"rate_limit_exceeded\", \"message\": \"Rate limit exceeded. Please retry after 42 seconds.\", \"retry_after\": 42 } } ``` **Best Practices:** - Monitor `X-RateLimit-Remaining` header values - Implement exponential backoff when receiving 429 responses - Cache responses when appropriate to reduce API calls - Consider upgrading to Enterprise tier for higher limits (1000 req/min) ## Pagination List endpoints return paginated results to optimize performance. Use `limit` and `offset` parameters to navigate through result sets. **Parameters:** - `limit`: Number of items per page (default: 20, max: 100) - `offset`: Number of items to skip (default: 0) **Example Request:** ```bash curl -H \"X-API-Key: YOUR_API_KEY\" \\ \"https://api.cloudflow.io/v2/workflows?limit=50&offset=100\" ``` **Response Structure:** ```json { \"data\": [...], \"pagination\": { \"total\": 347, \"limit\": 50, \"offset\": 100, \"has_more\": true } } ``` **Python Example:** ```python import requests def fetch_all_workflows(api_key): base_url = \"https://api.cloudflow.io/v2/workflows\" headers = {\"X-API-Key\": api_key} all_workflows = [] offset = 0 limit = 100 while True: response = requests.get( base_url, headers=headers, params={\"limit\": limit, \"offset\": offset} ) data = response.json() all_workflows.extend(data['data']) if not data['pagination']['has_more']: break offset += limit return all_workflows ``` ## Endpoints ### Workflows #### List Workflows Retrieve a paginated list of all workflows in your account. **Endpoint:** `GET /workflows` **Query Parameters:** - `status` (optional): Filter by status (`active`, `paused`, `archived`) - `created_after` (optional): ISO 8601 timestamp - `limit` (optional): Items per page (max 100) - `offset` (optional): Pagination offset **Example Request:** ```bash curl -H \"X-API-Key: YOUR_API_KEY\" \\ \"https://api.cloudflow.io/v2/workflows?status=active&limit=25\" ``` **Example Response (200 OK):** ```json { \"data\": [ { \"id\": \"wf_8x7k2m9p\", \"name\": \"Data Processing Pipeline\", \"description\": \"Processes customer data every hour\", \"status\": \"active\", \"trigger\": { \"type\": \"schedule\", \"cron\": \"0 * * * *\" }, \"steps\": 12, \"created_at\": \"2026-01-15T10:30:00Z\", \"updated_at\": \"2026-01-23T14:22:00Z\", \"last_run\": \"2026-01-24T09:00:00Z\" } ], \"pagination\": { \"total\": 1, \"limit\": 25, \"offset\": 0, \"has_more\": false } } ``` #### Create Workflow Create a new workflow with specified configuration.",
              "score": null
            },
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_4",
              "content": "### Error Response Format ```json { \"error\": { \"code\": \"invalid_parameter\", \"message\": \"The 'limit' parameter must be between 1 and 100\", \"field\": \"limit\", \"request_id\": \"req_8k3m9x2p\" } } ``` ### HTTP Status Codes | Status Code | Description | Common Causes | |------------|-------------|---------------| | 400 | Bad Request | Invalid parameters, malformed JSON, validation errors | | 401 | Unauthorized | Missing or invalid authentication credentials | | 403 | Forbidden | Insufficient permissions for requested resource | | 404 | Not Found | Resource does not exist or has been deleted | | 429 | Too Many Requests | Rate limit exceeded, retry after specified period | | 500 | Internal Server Error | Unexpected server error, contact support if persists | | 502 | Bad Gateway | Temporary service issue, retry with exponential backoff | | 503 | Service Unavailable | Scheduled maintenance or temporary outage | ### Error Codes CloudFlow returns specific error codes to help you identify and resolve issues: - `invalid_parameter`: One or more request parameters are invalid - `missing_required_field`: Required field is missing from request body - `authentication_failed`: Invalid API key or token - `insufficient_permissions`: User lacks required scope or permission - `resource_not_found`: Requested resource does not exist - `rate_limit_exceeded`: Too many requests, see rate limiting section - `workflow_execution_failed`: Workflow execution encountered an error - `invalid_json`: Request body contains malformed JSON - `duplicate_resource`: Resource with same identifier already exists - `quota_exceeded`: Account quota limit reached ### Error Handling Best Practices **Python Example:** ```python import requests import time def make_api_request(url, headers, max_retries=3): for attempt in range(max_retries): try: response = requests.get(url, headers=headers) if response.status_code == 200: return response.json() elif response.status_code == 429: # Rate limit exceeded retry_after = int(response.headers.get('X-RateLimit-Reset', 60)) print(f\"Rate limited. Waiting {retry_after} seconds...\") time.sleep(retry_after) continue elif response.status_code >= 500: # Server error - retry with exponential backoff wait_time = 2 ** attempt print(f\"Server error. Retrying in {wait_time} seconds...\") time.sleep(wait_time) continue else: # Client error - don't retry error_data = response.json() raise Exception(f\"API Error: {error_data['error']['message']}\") except requests.exceptions.RequestException as e: print(f\"Request failed: {e}\") if attempt < max_retries - 1: time.sleep(2 ** attempt) else: raise raise Exception(\"Max retries exceeded\") ``` ## Webhooks CloudFlow can send webhook notifications when specific events occur in your workflows.",
              "score": null
            },
            {
              "doc_id": "user_guide",
              "chunk_id": "user_guide_fix_5",
              "content": "Save and activate **Testing Schedules:** Use the built-in schedule calculator to preview upcoming executions: ``` Next 5 executions: 1. 2026-01-24 14:00:00 UTC 2. 2026-01-25 14:00:00 UTC 3. 2026-01-26 14:00:00 UTC 4. 2026-01-27 14:00:00 UTC 5. 2026-01-28 14:00:00 UTC ``` ## Error Handling Robust error handling ensures your workflows are resilient and reliable. ### Retry Policies Configure automatic retries for failed actions: ```yaml - id: api_call action: http_request config: url: \"https://api.example.com/data\" retry: max_attempts: 3 backoff_type: \"exponential\" # or \"fixed\", \"linear\" initial_interval: 1000 # milliseconds max_interval: 30000 multiplier: 2.0 retry_on: - timeout - network_error - status: [500, 502, 503, 504] ``` **Backoff Strategies:** - **Fixed**: Wait the same amount of time between retries - Attempt 1: Wait 1s - Attempt 2: Wait 1s - Attempt 3: Wait 1s - **Linear**: Increase wait time by a fixed amount - Attempt 1: Wait 1s - Attempt 2: Wait 2s - Attempt 3: Wait 3s - **Exponential**: Double the wait time with each retry (recommended) - Attempt 1: Wait 1s - Attempt 2: Wait 2s - Attempt 3: Wait 4s **Retry Conditions:** Control which errors trigger retries: - `timeout`: Request timeout - `network_error`: Connection failures - `status`: Specific HTTP status codes - `error_code`: Application-specific error codes ### Fallback Actions Execute alternative actions when the primary action fails: ```yaml - id: primary_payment action: http_request config: url: \"https://primary-payment-gateway.com/charge\" method: POST body: amount: \"{{amount}}\" on_error: - id: fallback_payment action: http_request config: url: \"https://backup-payment-gateway.com/charge\" method: POST body: amount: \"{{amount}}\" - id: notify_admin action: email config: to: \"admin@company.com\" subject: \"Payment Gateway Failure\" body: \"Primary gateway failed, switched to backup\" ``` **Error Object:** Access error details in fallback actions: ``` {{error.message}} # Error message {{error.code}} # Error code {{error.step_id}} # Failed step ID {{error.timestamp}} # When the error occurred {{error.attempts}} # Number of retry attempts ``` ### Dead Letter Queue When all retries and fallbacks fail, CloudFlow can route failed executions to a Dead Letter Queue (DLQ) for manual review: **Enable DLQ:** ```yaml workflow: error_handling: dead_letter_queue: enabled: true retain_days: 30 ``` **DLQ Features:** - View failed executions in the dashboard - Inspect complete execution context and error details - Retry individual executions after fixing issues - Export failed executions for analysis - Set up alerts for DLQ threshold breaches **Accessing the DLQ:** 1. Navigate to **Workflows** > **[Your Workflow]** > **Dead Letter Queue** 2.",
              "score": null
            },
            {
              "doc_id": "user_guide",
              "chunk_id": "user_guide_fix_6",
              "content": "Filter by error type, date range, or execution ID 3. Click an execution to view full details 4. Click **\"Retry\"** to reprocess with the same input data ### Error Notifications Get notified when workflows fail: ```yaml workflow: notifications: on_failure: - type: email to: \"ops-team@company.com\" - type: slack channel: \"#alerts\" message: \"Workflow {{workflow.name}} failed: {{error.message}}\" on_success_after_retry: - type: slack channel: \"#monitoring\" message: \"Workflow recovered after {{error.attempts}} attempts\" ``` ## Workflow Limits CloudFlow enforces the following limits to ensure platform stability and performance: ### Steps Per Workflow - **Maximum**: 50 steps per workflow - **Recommendation**: Keep workflows focused and modular. If you need more steps, consider splitting into multiple workflows connected via webhooks. ### Execution Timeout - **Default**: 3600 seconds (60 minutes) - **Behavior**: Workflows exceeding this timeout are automatically terminated - **Custom Timeouts**: Enterprise plans can request custom timeout limits **Setting Step-Level Timeouts:** ```yaml - id: long_running_task action: http_request config: url: \"https://api.example.com/process\" timeout: 300 # 5 minutes for this specific step ``` ### Execution Limits - **Maximum**: 1000 executions per day (per workflow) - **Rate Limiting**: 100 concurrent executions per workflow - **Burst Limit**: 10 executions per second **What happens when limits are reached:** - New executions are queued automatically - Webhook triggers return HTTP 429 (Too Many Requests) - Scheduled executions are skipped (logged in audit trail) - Email notifications sent to workflow owner **Monitoring Usage:** View real-time metrics in your workflow dashboard: - Executions today: 847 / 1000 - Average duration: 12.3s - Success rate: 98.2% - Current queue depth: 3 ### Data Limits - **Maximum request/response size**: 10MB per action - **Maximum execution payload**: 50MB total - **Variable value size**: 1MB per variable ### Enterprise Plan Limits Enterprise customers can request increased limits: - Up to 100 steps per workflow - Up to 10,000 executions per day - Up to 7200 second timeout (2 hours) - Priority execution queue - Dedicated capacity allocation Contact sales@cloudflow.io for Enterprise pricing and custom limits. ## Best Practices Follow these best practices to build reliable, maintainable workflows: ### 1. Use Descriptive Names **Good:** - Workflow: \"Sync Customer Data from Salesforce to Database\" - Step: \"validate_customer_email\" **Bad:** - Workflow: \"Workflow 1\" - Step: \"step3\" ### 2.",
              "score": null
            },
            {
              "doc_id": "user_guide",
              "chunk_id": "user_guide_fix_9",
              "content": "Requester: {{trigger.body.requester}} Description: {{trigger.body.description}} Approve: https://app.company.com/approve/{{steps.create_approval_request.rows[0].id}} Reject: https://app.company.com/reject/{{steps.create_approval_request.rows[0].id}} - id: wait_for_approval action: wait_for_webhook config: timeout: 86400 # 24 hours webhook_path: \"/approval/{{steps.create_approval_request.rows[0].id}}\" - id: process_approval action: javascript code: | if (input.status === 'approved') { return { approved: true }; } else { throw new Error('Expense rejected'); } on_error: - id: notify_rejection action: email config: to: \"{{trigger.body.requester}}\" subject: \"Expense Rejected\" body: \"Your expense request has been rejected.\" - id: process_payment action: http_request config: method: POST url: \"https://api.accounting.com/payments\" body: amount: \"{{trigger.body.amount}}\" recipient: \"{{trigger.body.requester}}\" ``` ### Pattern 4: Data Synchronization Keep two systems in sync bidirectionally: ```yaml name: \"Sync Customers: CRM to Database\" trigger: type: event source: \"salesforce\" event: \"customer.updated\" steps: - id: fetch_customer action: http_request config: url: \"https://api.salesforce.com/customers/{{trigger.customer_id}}\" headers: Authorization: \"Bearer {{secrets.SALESFORCE_TOKEN}}\" - id: check_existing action: database_query config: query: \"SELECT id, last_updated FROM customers WHERE salesforce_id = $1\" parameters: - \"{{trigger.customer_id}}\" - id: update_or_insert action: database_query config: query: | INSERT INTO customers (salesforce_id, name, email, phone, last_updated) VALUES ($1, $2, $3, $4, NOW()) ON CONFLICT (salesforce_id) DO UPDATE SET name = $2, email = $3, phone = $4, last_updated = NOW() parameters: - \"{{trigger.customer_id}}\" - \"{{steps.fetch_customer.body.name}}\" - \"{{steps.fetch_customer.body.email}}\" - \"{{steps.fetch_customer.body.phone}}\" - id: log_sync action: database_query config: query: | INSERT INTO sync_log (source, target, record_id, synced_at) VALUES ('salesforce', 'postgresql', $1, NOW()) parameters: - \"{{trigger.customer_id}}\" ``` ### Pattern 5: Error Aggregation and Alerting Aggregate errors and send smart alerts: ```yaml name: \"Application Error Monitor\" schedule: cron: \"*/5 * * * *\" timezone: \"UTC\" steps: - id: fetch_recent_errors action: database_query config: query: | SELECT error_type, COUNT(*) as count, MAX(created_at) as last_seen FROM error_logs WHERE created_at > NOW() - INTERVAL '5 minutes' AND alerted = false GROUP BY error_type HAVING COUNT(*) > 5 - id: format_alert action: javascript condition: \"{{length(steps.fetch_recent_errors.rows) > 0}}\" code: | const errors = input.rows; let message = \"\u26a0\ufe0f Error Alert\\n\\n\"; for (const error of errors) { message += `\u2022 ${error.error_type}: ${error.count} occurrences (last: ${error.last_seen})\\n`; } return { message, total_errors: errors.reduce((sum, e) => sum + e.count, 0) }; - id: send_alert action: slack_message condition: \"{{steps.format_alert.output.total_errors > 10}}\" config: channel: \"#incidents\" text: \"{{steps.format_alert.output.message}}\" priority: \"high\" - id: mark_alerted action: database_query config: query: | UPDATE error_logs SET alerted = true WHERE created_at > NOW() - INTERVAL '5 minutes' ``` --- ## Getting Help Need assistance with CloudFlow?",
              "score": null
            }
          ],
          "latency_ms": 11.562137999135302,
          "query_id": "realistic_002",
          "dimension": "synonym",
          "query": "What should I do when I get rate limited?"
        },
        {
          "key_facts": [
            "429 Too Many Requests",
            "X-RateLimit-Remaining",
            "Retry-After",
            "Monitor X-RateLimit-Remaining header values",
            "Implement exponential backoff when receiving 429 responses"
          ],
          "found_facts": [
            "429 Too Many Requests",
            "X-RateLimit-Remaining",
            "Retry-After"
          ],
          "missed_facts": [
            "Monitor X-RateLimit-Remaining header values",
            "Implement exponential backoff when receiving 429 responses"
          ],
          "coverage": 0.6,
          "expected_docs": [
            "api_reference",
            "troubleshooting_guide"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_4",
              "content": "### Error Response Format ```json { \"error\": { \"code\": \"invalid_parameter\", \"message\": \"The 'limit' parameter must be between 1 and 100\", \"field\": \"limit\", \"request_id\": \"req_8k3m9x2p\" } } ``` ### HTTP Status Codes | Status Code | Description | Common Causes | |------------|-------------|---------------| | 400 | Bad Request | Invalid parameters, malformed JSON, validation errors | | 401 | Unauthorized | Missing or invalid authentication credentials | | 403 | Forbidden | Insufficient permissions for requested resource | | 404 | Not Found | Resource does not exist or has been deleted | | 429 | Too Many Requests | Rate limit exceeded, retry after specified period | | 500 | Internal Server Error | Unexpected server error, contact support if persists | | 502 | Bad Gateway | Temporary service issue, retry with exponential backoff | | 503 | Service Unavailable | Scheduled maintenance or temporary outage | ### Error Codes CloudFlow returns specific error codes to help you identify and resolve issues: - `invalid_parameter`: One or more request parameters are invalid - `missing_required_field`: Required field is missing from request body - `authentication_failed`: Invalid API key or token - `insufficient_permissions`: User lacks required scope or permission - `resource_not_found`: Requested resource does not exist - `rate_limit_exceeded`: Too many requests, see rate limiting section - `workflow_execution_failed`: Workflow execution encountered an error - `invalid_json`: Request body contains malformed JSON - `duplicate_resource`: Resource with same identifier already exists - `quota_exceeded`: Account quota limit reached ### Error Handling Best Practices **Python Example:** ```python import requests import time def make_api_request(url, headers, max_retries=3): for attempt in range(max_retries): try: response = requests.get(url, headers=headers) if response.status_code == 200: return response.json() elif response.status_code == 429: # Rate limit exceeded retry_after = int(response.headers.get('X-RateLimit-Reset', 60)) print(f\"Rate limited. Waiting {retry_after} seconds...\") time.sleep(retry_after) continue elif response.status_code >= 500: # Server error - retry with exponential backoff wait_time = 2 ** attempt print(f\"Server error. Retrying in {wait_time} seconds...\") time.sleep(wait_time) continue else: # Client error - don't retry error_data = response.json() raise Exception(f\"API Error: {error_data['error']['message']}\") except requests.exceptions.RequestException as e: print(f\"Request failed: {e}\") if attempt < max_retries - 1: time.sleep(2 ** attempt) else: raise raise Exception(\"Max retries exceeded\") ``` ## Webhooks CloudFlow can send webhook notifications when specific events occur in your workflows.",
              "score": null
            },
            {
              "doc_id": "user_guide",
              "chunk_id": "user_guide_fix_2",
              "content": "### HTTP Requests Make HTTP requests to any API endpoint: **Configuration:** - **Method**: GET, POST, PUT, PATCH, DELETE, HEAD - **URL**: Full endpoint URL (supports variable interpolation) - **Headers**: Custom headers as key-value pairs - **Query Parameters**: URL parameters - **Body**: JSON, form data, or raw text - **Authentication**: Basic Auth, Bearer Token, API Key, OAuth 2.0 **Example:** ```yaml - id: fetch_user action: http_request config: method: GET url: \"https://api.example.com/users/{{user_id}}\" headers: Authorization: \"Bearer {{secrets.API_TOKEN}}\" Content-Type: \"application/json\" timeout: 30 ``` **Response Handling:** Access the response in subsequent steps: - `{{steps.fetch_user.status}}` - HTTP status code - `{{steps.fetch_user.body}}` - Response body - `{{steps.fetch_user.headers}}` - Response headers ### Database Queries Execute queries against supported databases (PostgreSQL, MySQL, MongoDB, Redis): **SQL Databases (PostgreSQL, MySQL):** ```yaml - id: get_orders action: database_query config: connection: \"{{secrets.DB_CONNECTION_STRING}}\" query: | SELECT * FROM orders WHERE customer_id = $1 AND status = $2 ORDER BY created_at DESC LIMIT 10 parameters: - \"{{trigger.customer_id}}\" - \"pending\" ``` **MongoDB:** ```yaml - id: find_documents action: mongodb_query config: connection: \"{{secrets.MONGO_URI}}\" database: \"production\" collection: \"users\" operation: find filter: email: \"{{trigger.email}}\" options: limit: 1 ``` **Important Notes:** - Always use parameterized queries to prevent SQL injection - Connection strings should be stored in secrets - Query timeout is 30 seconds by default - Maximum result set size is 10MB ### Email Notifications Send emails via SMTP or integrated email providers: **Configuration:** ```yaml - id: send_notification action: email config: provider: \"smtp\" # or \"sendgrid\", \"mailgun\", \"ses\" from: \"notifications@cloudflow.io\" to: \"{{trigger.recipient}}\" cc: \"manager@company.com\" subject: \"Alert: {{alert_type}}\" body: | Hello {{user.name}}, This is an automated notification about {{event_description}}. Details: - Time: {{timestamp}} - Type: {{alert_type}} - Priority: {{priority}} Best regards, CloudFlow Automation html: true attachments: - name: \"report.pdf\" content: \"{{steps.generate_report.output}}\" encoding: \"base64\" ``` **Email Templates:** CloudFlow supports dynamic templates with conditional content: ``` {{#if priority == 'high'}} \u26a0\ufe0f URGENT: Immediate attention required {{else}} \u2139\ufe0f Informational notification {{/if}} ``` ### Slack Messages Send messages to Slack channels or users: **Channel Messages:** ```yaml - id: notify_team action: slack_message config: channel: \"#deployments\" text: \"Deployment completed successfully!\" blocks: - type: \"section\" text: type: \"mrkdwn\" text: \"*Deployment Summary*\\nEnvironment: {{environment}}\\nVersion: {{version}}\\nDuration: {{duration}}s\" - type: \"actions\" elements: - type: \"button\" text: \"View Logs\" url: \"{{logs_url}}\" thread_ts: \"{{trigger.thread_ts}}\" # Reply to thread ``` **Direct Messages:** ```yaml - id: dm_user action: slack_message config: user: \"{{user_slack_id}}\" text: \"Your report is ready for review.\"",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_6",
              "content": "Data Validation Errors** ``` ValidationError: Field 'customer_id' is required but missing in 234 records ``` **Resolution:** ```bash # Add data quality checks cloudflow workflows update wf_9k2n4m8p1q \\ --step data_ingestion \\ --add-validator required_fields \\ --validator-config '{\"fields\": [\"customer_id\", \"timestamp\", \"amount\"]}' # Configure error handling cloudflow workflows update wf_9k2n4m8p1q \\ --step data_validation \\ --on-error continue \\ --error-threshold 5% # Fail if > 5% of records invalid ``` **2. External API Failures** ``` ExternalAPIError: API request to https://partner-api.example.com failed with status 502 ``` **Resolution:** ```bash # Add circuit breaker cloudflow workflows update wf_9k2n4m8p1q \\ --step external_api_call \\ --circuit-breaker-enabled true \\ --circuit-breaker-threshold 5 \\ --circuit-breaker-timeout 30000 # Configure fallback behavior cloudflow workflows update wf_9k2n4m8p1q \\ --step external_api_call \\ --fallback-action use_cached_data \\ --cache-ttl 3600 ``` --- ## Rate Limiting & Throttling ### 429 Too Many Requests #### Error Response ```json { \"error\": \"rate_limit_exceeded\", \"message\": \"API rate limit exceeded. Retry after 45 seconds.\", \"status\": 429, \"headers\": { \"X-RateLimit-Limit\": \"1000\", \"X-RateLimit-Remaining\": \"0\", \"X-RateLimit-Reset\": \"1706112345\", \"Retry-After\": \"45\" } } ``` #### Rate Limit Tiers CloudFlow enforces the following rate limits per workspace: | Tier | Requests/Minute | Requests/Hour | Concurrent Workflows | |------|-----------------|---------------|----------------------| | Free | 60 | 1,000 | 5 | | Standard | 1,000 | 50,000 | 50 | | Premium | 5,000 | 250,000 | 200 | | Enterprise | Custom | Custom | Unlimited | #### Checking Rate Limit Status ```bash # Check current rate limit status curl -I https://api.cloudflow.io/api/v1/workflows \\ -H \"Authorization: Bearer $CF_ACCESS_TOKEN\" # Extract rate limit headers curl -s -I https://api.cloudflow.io/api/v1/workflows \\ -H \"Authorization: Bearer $CF_ACCESS_TOKEN\" | \\ grep -i \"x-ratelimit\" # Output: # X-RateLimit-Limit: 1000 # X-RateLimit-Remaining: 247 # X-RateLimit-Reset: 1706112400 # Monitor rate limit usage cloudflow metrics query \\ --metric rate_limit_remaining \\ --workspace ws_abc123 \\ --last 1h \\ --interval 1m ``` #### Handling Rate Limits in Code **Python example with retry logic:** ```python import time import requests def cloudflow_api_call_with_retry(url, headers, max_retries=3): for attempt in range(max_retries): response = requests.get(url, headers=headers) if response.status_code == 429: retry_after = int(response.headers.get('Retry-After', 60)) print(f\"Rate limited.",
              "score": null
            },
            {
              "doc_id": "user_guide",
              "chunk_id": "user_guide_fix_6",
              "content": "Filter by error type, date range, or execution ID 3. Click an execution to view full details 4. Click **\"Retry\"** to reprocess with the same input data ### Error Notifications Get notified when workflows fail: ```yaml workflow: notifications: on_failure: - type: email to: \"ops-team@company.com\" - type: slack channel: \"#alerts\" message: \"Workflow {{workflow.name}} failed: {{error.message}}\" on_success_after_retry: - type: slack channel: \"#monitoring\" message: \"Workflow recovered after {{error.attempts}} attempts\" ``` ## Workflow Limits CloudFlow enforces the following limits to ensure platform stability and performance: ### Steps Per Workflow - **Maximum**: 50 steps per workflow - **Recommendation**: Keep workflows focused and modular. If you need more steps, consider splitting into multiple workflows connected via webhooks. ### Execution Timeout - **Default**: 3600 seconds (60 minutes) - **Behavior**: Workflows exceeding this timeout are automatically terminated - **Custom Timeouts**: Enterprise plans can request custom timeout limits **Setting Step-Level Timeouts:** ```yaml - id: long_running_task action: http_request config: url: \"https://api.example.com/process\" timeout: 300 # 5 minutes for this specific step ``` ### Execution Limits - **Maximum**: 1000 executions per day (per workflow) - **Rate Limiting**: 100 concurrent executions per workflow - **Burst Limit**: 10 executions per second **What happens when limits are reached:** - New executions are queued automatically - Webhook triggers return HTTP 429 (Too Many Requests) - Scheduled executions are skipped (logged in audit trail) - Email notifications sent to workflow owner **Monitoring Usage:** View real-time metrics in your workflow dashboard: - Executions today: 847 / 1000 - Average duration: 12.3s - Success rate: 98.2% - Current queue depth: 3 ### Data Limits - **Maximum request/response size**: 10MB per action - **Maximum execution payload**: 50MB total - **Variable value size**: 1MB per variable ### Enterprise Plan Limits Enterprise customers can request increased limits: - Up to 100 steps per workflow - Up to 10,000 executions per day - Up to 7200 second timeout (2 hours) - Priority execution queue - Dedicated capacity allocation Contact sales@cloudflow.io for Enterprise pricing and custom limits. ## Best Practices Follow these best practices to build reliable, maintainable workflows: ### 1. Use Descriptive Names **Good:** - Workflow: \"Sync Customer Data from Salesforce to Database\" - Step: \"validate_customer_email\" **Bad:** - Workflow: \"Workflow 1\" - Step: \"step3\" ### 2.",
              "score": null
            },
            {
              "doc_id": "user_guide",
              "chunk_id": "user_guide_fix_7",
              "content": "Handle Errors Gracefully Always implement error handling for external API calls and database operations: ```yaml - id: fetch_data action: http_request config: url: \"https://api.example.com/data\" retry: max_attempts: 3 backoff_type: exponential on_error: - id: log_error action: database_query config: query: \"INSERT INTO error_log (workflow_id, error) VALUES ($1, $2)\" parameters: - \"{{workflow.id}}\" - \"{{error.message}}\" ``` ### 3. Use Secrets for Sensitive Data Never hardcode API keys, passwords, or tokens in workflows: **Bad:** ```yaml headers: Authorization: \"Bearer sk_live_abc123xyz789\" ``` **Good:** ```yaml headers: Authorization: \"Bearer {{secrets.API_TOKEN}}\" ``` Store secrets in **Settings** > **Secrets** with encryption at rest. ### 4. Validate Input Data Always validate trigger data before processing: ```yaml - id: validate_input action: javascript code: | const required_fields = ['email', 'name', 'order_id']; for (const field of required_fields) { if (!input[field]) { throw new Error(`Missing required field: ${field}`); } } // Validate email format const email_regex = /^[^\\s@]+@[^\\s@]+\\.[^\\s@]+$/; if (!email_regex.test(input.email)) { throw new Error('Invalid email format'); } return { validated: true }; ``` ### 5. Use Idempotency Keys For operations that shouldn't be repeated (payments, record creation), use idempotency keys: ```yaml - id: create_charge action: http_request config: url: \"https://api.stripe.com/v1/charges\" method: POST headers: Idempotency-Key: \"{{workflow.id}}-{{execution.id}}\" body: amount: \"{{amount}}\" ``` ### 6. Monitor and Log Add logging steps for important workflow milestones: ```yaml - id: log_start action: database_query config: query: \"INSERT INTO workflow_audit (execution_id, step, timestamp) VALUES ($1, $2, $3)\" parameters: - \"{{execution.id}}\" - \"workflow_started\" - \"{{now()}}\" ``` ### 7. Keep Workflows Modular Break complex workflows into smaller, reusable components: - Use sub-workflows for repeated logic - Trigger child workflows via webhooks - Share common configurations via templates ### 8. Test Thoroughly Before activating a workflow: 1. Use test mode with sample data 2. Verify all actions execute correctly 3. Test error handling paths 4. Review execution logs 5. Start with a limited scope (e.g., test channel, small dataset) ### 9. Document Your Workflows Add descriptions to workflows and steps: ```yaml workflow: name: \"Daily Sales Report\" description: | Generates a daily sales report and distributes it to the sales team. Runs at 8:00 AM EST Monday-Friday. Data source: PostgreSQL sales database Recipients: sales-team@company.com steps: - id: fetch_sales name: \"Fetch Yesterday's Sales\" description: \"Query database for all completed orders from previous day\" ``` ### 10.",
              "score": null
            }
          ],
          "latency_ms": 11.317400996631477,
          "query_id": "realistic_002",
          "dimension": "problem",
          "query": "My requests keep failing with 429 status code"
        },
        {
          "key_facts": [
            "429 Too Many Requests",
            "X-RateLimit-Remaining",
            "Retry-After",
            "Monitor X-RateLimit-Remaining header values",
            "Implement exponential backoff when receiving 429 responses"
          ],
          "found_facts": [
            "429 Too Many Requests",
            "X-RateLimit-Remaining",
            "Retry-After"
          ],
          "missed_facts": [
            "Monitor X-RateLimit-Remaining header values",
            "Implement exponential backoff when receiving 429 responses"
          ],
          "coverage": 0.6,
          "expected_docs": [
            "api_reference",
            "troubleshooting_guide"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_4",
              "content": "### Error Response Format ```json { \"error\": { \"code\": \"invalid_parameter\", \"message\": \"The 'limit' parameter must be between 1 and 100\", \"field\": \"limit\", \"request_id\": \"req_8k3m9x2p\" } } ``` ### HTTP Status Codes | Status Code | Description | Common Causes | |------------|-------------|---------------| | 400 | Bad Request | Invalid parameters, malformed JSON, validation errors | | 401 | Unauthorized | Missing or invalid authentication credentials | | 403 | Forbidden | Insufficient permissions for requested resource | | 404 | Not Found | Resource does not exist or has been deleted | | 429 | Too Many Requests | Rate limit exceeded, retry after specified period | | 500 | Internal Server Error | Unexpected server error, contact support if persists | | 502 | Bad Gateway | Temporary service issue, retry with exponential backoff | | 503 | Service Unavailable | Scheduled maintenance or temporary outage | ### Error Codes CloudFlow returns specific error codes to help you identify and resolve issues: - `invalid_parameter`: One or more request parameters are invalid - `missing_required_field`: Required field is missing from request body - `authentication_failed`: Invalid API key or token - `insufficient_permissions`: User lacks required scope or permission - `resource_not_found`: Requested resource does not exist - `rate_limit_exceeded`: Too many requests, see rate limiting section - `workflow_execution_failed`: Workflow execution encountered an error - `invalid_json`: Request body contains malformed JSON - `duplicate_resource`: Resource with same identifier already exists - `quota_exceeded`: Account quota limit reached ### Error Handling Best Practices **Python Example:** ```python import requests import time def make_api_request(url, headers, max_retries=3): for attempt in range(max_retries): try: response = requests.get(url, headers=headers) if response.status_code == 200: return response.json() elif response.status_code == 429: # Rate limit exceeded retry_after = int(response.headers.get('X-RateLimit-Reset', 60)) print(f\"Rate limited. Waiting {retry_after} seconds...\") time.sleep(retry_after) continue elif response.status_code >= 500: # Server error - retry with exponential backoff wait_time = 2 ** attempt print(f\"Server error. Retrying in {wait_time} seconds...\") time.sleep(wait_time) continue else: # Client error - don't retry error_data = response.json() raise Exception(f\"API Error: {error_data['error']['message']}\") except requests.exceptions.RequestException as e: print(f\"Request failed: {e}\") if attempt < max_retries - 1: time.sleep(2 ** attempt) else: raise raise Exception(\"Max retries exceeded\") ``` ## Webhooks CloudFlow can send webhook notifications when specific events occur in your workflows.",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_6",
              "content": "Data Validation Errors** ``` ValidationError: Field 'customer_id' is required but missing in 234 records ``` **Resolution:** ```bash # Add data quality checks cloudflow workflows update wf_9k2n4m8p1q \\ --step data_ingestion \\ --add-validator required_fields \\ --validator-config '{\"fields\": [\"customer_id\", \"timestamp\", \"amount\"]}' # Configure error handling cloudflow workflows update wf_9k2n4m8p1q \\ --step data_validation \\ --on-error continue \\ --error-threshold 5% # Fail if > 5% of records invalid ``` **2. External API Failures** ``` ExternalAPIError: API request to https://partner-api.example.com failed with status 502 ``` **Resolution:** ```bash # Add circuit breaker cloudflow workflows update wf_9k2n4m8p1q \\ --step external_api_call \\ --circuit-breaker-enabled true \\ --circuit-breaker-threshold 5 \\ --circuit-breaker-timeout 30000 # Configure fallback behavior cloudflow workflows update wf_9k2n4m8p1q \\ --step external_api_call \\ --fallback-action use_cached_data \\ --cache-ttl 3600 ``` --- ## Rate Limiting & Throttling ### 429 Too Many Requests #### Error Response ```json { \"error\": \"rate_limit_exceeded\", \"message\": \"API rate limit exceeded. Retry after 45 seconds.\", \"status\": 429, \"headers\": { \"X-RateLimit-Limit\": \"1000\", \"X-RateLimit-Remaining\": \"0\", \"X-RateLimit-Reset\": \"1706112345\", \"Retry-After\": \"45\" } } ``` #### Rate Limit Tiers CloudFlow enforces the following rate limits per workspace: | Tier | Requests/Minute | Requests/Hour | Concurrent Workflows | |------|-----------------|---------------|----------------------| | Free | 60 | 1,000 | 5 | | Standard | 1,000 | 50,000 | 50 | | Premium | 5,000 | 250,000 | 200 | | Enterprise | Custom | Custom | Unlimited | #### Checking Rate Limit Status ```bash # Check current rate limit status curl -I https://api.cloudflow.io/api/v1/workflows \\ -H \"Authorization: Bearer $CF_ACCESS_TOKEN\" # Extract rate limit headers curl -s -I https://api.cloudflow.io/api/v1/workflows \\ -H \"Authorization: Bearer $CF_ACCESS_TOKEN\" | \\ grep -i \"x-ratelimit\" # Output: # X-RateLimit-Limit: 1000 # X-RateLimit-Remaining: 247 # X-RateLimit-Reset: 1706112400 # Monitor rate limit usage cloudflow metrics query \\ --metric rate_limit_remaining \\ --workspace ws_abc123 \\ --last 1h \\ --interval 1m ``` #### Handling Rate Limits in Code **Python example with retry logic:** ```python import time import requests def cloudflow_api_call_with_retry(url, headers, max_retries=3): for attempt in range(max_retries): response = requests.get(url, headers=headers) if response.status_code == 429: retry_after = int(response.headers.get('Retry-After', 60)) print(f\"Rate limited.",
              "score": null
            },
            {
              "doc_id": "user_guide",
              "chunk_id": "user_guide_fix_6",
              "content": "Filter by error type, date range, or execution ID 3. Click an execution to view full details 4. Click **\"Retry\"** to reprocess with the same input data ### Error Notifications Get notified when workflows fail: ```yaml workflow: notifications: on_failure: - type: email to: \"ops-team@company.com\" - type: slack channel: \"#alerts\" message: \"Workflow {{workflow.name}} failed: {{error.message}}\" on_success_after_retry: - type: slack channel: \"#monitoring\" message: \"Workflow recovered after {{error.attempts}} attempts\" ``` ## Workflow Limits CloudFlow enforces the following limits to ensure platform stability and performance: ### Steps Per Workflow - **Maximum**: 50 steps per workflow - **Recommendation**: Keep workflows focused and modular. If you need more steps, consider splitting into multiple workflows connected via webhooks. ### Execution Timeout - **Default**: 3600 seconds (60 minutes) - **Behavior**: Workflows exceeding this timeout are automatically terminated - **Custom Timeouts**: Enterprise plans can request custom timeout limits **Setting Step-Level Timeouts:** ```yaml - id: long_running_task action: http_request config: url: \"https://api.example.com/process\" timeout: 300 # 5 minutes for this specific step ``` ### Execution Limits - **Maximum**: 1000 executions per day (per workflow) - **Rate Limiting**: 100 concurrent executions per workflow - **Burst Limit**: 10 executions per second **What happens when limits are reached:** - New executions are queued automatically - Webhook triggers return HTTP 429 (Too Many Requests) - Scheduled executions are skipped (logged in audit trail) - Email notifications sent to workflow owner **Monitoring Usage:** View real-time metrics in your workflow dashboard: - Executions today: 847 / 1000 - Average duration: 12.3s - Success rate: 98.2% - Current queue depth: 3 ### Data Limits - **Maximum request/response size**: 10MB per action - **Maximum execution payload**: 50MB total - **Variable value size**: 1MB per variable ### Enterprise Plan Limits Enterprise customers can request increased limits: - Up to 100 steps per workflow - Up to 10,000 executions per day - Up to 7200 second timeout (2 hours) - Priority execution queue - Dedicated capacity allocation Contact sales@cloudflow.io for Enterprise pricing and custom limits. ## Best Practices Follow these best practices to build reliable, maintainable workflows: ### 1. Use Descriptive Names **Good:** - Workflow: \"Sync Customer Data from Salesforce to Database\" - Step: \"validate_customer_email\" **Bad:** - Workflow: \"Workflow 1\" - Step: \"step3\" ### 2.",
              "score": null
            },
            {
              "doc_id": "user_guide",
              "chunk_id": "user_guide_fix_7",
              "content": "Handle Errors Gracefully Always implement error handling for external API calls and database operations: ```yaml - id: fetch_data action: http_request config: url: \"https://api.example.com/data\" retry: max_attempts: 3 backoff_type: exponential on_error: - id: log_error action: database_query config: query: \"INSERT INTO error_log (workflow_id, error) VALUES ($1, $2)\" parameters: - \"{{workflow.id}}\" - \"{{error.message}}\" ``` ### 3. Use Secrets for Sensitive Data Never hardcode API keys, passwords, or tokens in workflows: **Bad:** ```yaml headers: Authorization: \"Bearer sk_live_abc123xyz789\" ``` **Good:** ```yaml headers: Authorization: \"Bearer {{secrets.API_TOKEN}}\" ``` Store secrets in **Settings** > **Secrets** with encryption at rest. ### 4. Validate Input Data Always validate trigger data before processing: ```yaml - id: validate_input action: javascript code: | const required_fields = ['email', 'name', 'order_id']; for (const field of required_fields) { if (!input[field]) { throw new Error(`Missing required field: ${field}`); } } // Validate email format const email_regex = /^[^\\s@]+@[^\\s@]+\\.[^\\s@]+$/; if (!email_regex.test(input.email)) { throw new Error('Invalid email format'); } return { validated: true }; ``` ### 5. Use Idempotency Keys For operations that shouldn't be repeated (payments, record creation), use idempotency keys: ```yaml - id: create_charge action: http_request config: url: \"https://api.stripe.com/v1/charges\" method: POST headers: Idempotency-Key: \"{{workflow.id}}-{{execution.id}}\" body: amount: \"{{amount}}\" ``` ### 6. Monitor and Log Add logging steps for important workflow milestones: ```yaml - id: log_start action: database_query config: query: \"INSERT INTO workflow_audit (execution_id, step, timestamp) VALUES ($1, $2, $3)\" parameters: - \"{{execution.id}}\" - \"workflow_started\" - \"{{now()}}\" ``` ### 7. Keep Workflows Modular Break complex workflows into smaller, reusable components: - Use sub-workflows for repeated logic - Trigger child workflows via webhooks - Share common configurations via templates ### 8. Test Thoroughly Before activating a workflow: 1. Use test mode with sample data 2. Verify all actions execute correctly 3. Test error handling paths 4. Review execution logs 5. Start with a limited scope (e.g., test channel, small dataset) ### 9. Document Your Workflows Add descriptions to workflows and steps: ```yaml workflow: name: \"Daily Sales Report\" description: | Generates a daily sales report and distributes it to the sales team. Runs at 8:00 AM EST Monday-Friday. Data source: PostgreSQL sales database Recipients: sales-team@company.com steps: - id: fetch_sales name: \"Fetch Yesterday's Sales\" description: \"Query database for all completed orders from previous day\" ``` ### 10.",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_0",
              "content": "# CloudFlow Platform Troubleshooting Guide **Version:** 3.2.1 **Last Updated:** January 2026 **Audience:** Platform Engineers, DevOps, Support Teams ## Table of Contents 1. [Overview](#overview) 2. [Authentication & Authorization Issues](#authentication--authorization-issues) 3. [Performance Problems](#performance-problems) 4. [Database Connection Issues](#database-connection-issues) 5. [Workflow Execution Failures](#workflow-execution-failures) 6. [Rate Limiting & Throttling](#rate-limiting--throttling) 7. [Log Analysis & Debugging](#log-analysis--debugging) 8. [Escalation Procedures](#escalation-procedures) --- ## Overview This guide provides comprehensive troubleshooting steps for common CloudFlow platform issues encountered in production environments. Each section includes error symptoms, root cause analysis, resolution steps, and preventive measures. ### Quick Diagnostic Checklist Before diving into specific issues, perform these initial checks: - Verify service health: `cloudflow status --all` - Check API connectivity: `curl -I https://api.cloudflow.io/health` - Review recent deployments: `kubectl get deployments -n cloudflow --sort-by=.metadata.creationTimestamp` - Inspect platform metrics: `cloudflow metrics --last 1h` --- ## Authentication & Authorization Issues ### 401 Unauthorized Errors #### Symptoms - API requests return `401 Unauthorized` - Error message: `Authentication credentials were not provided or are invalid` - Frontend displays \"Session expired\" message #### Common Causes **1. Token Expiration** CloudFlow access tokens expire after 3600 seconds (1 hour) by default. Refresh tokens are valid for 30 days. **Verification:** ```bash # Decode JWT to check expiration echo $CF_ACCESS_TOKEN | cut -d'.' -f2 | base64 -d | jq '.exp' # Compare with current time date +%s ``` **Resolution:** ```bash # Refresh the access token curl -X POST https://api.cloudflow.io/auth/refresh \\ -H \"Content-Type: application/json\" \\ -d '{\"refresh_token\": \"'$CF_REFRESH_TOKEN'\"}' # Update environment variable export CF_ACCESS_TOKEN=\"<new_token>\" ``` **2. Invalid JWT Signature** Error: `JWT signature verification failed` **Causes:** - Token was modified or corrupted - Using wrong signing key - Token generated with different secret **Resolution:** ```bash # Validate token structure cloudflow auth validate --token $CF_ACCESS_TOKEN # Generate new token cloudflow auth login --username <user> --password <pass> # For service accounts cloudflow auth service-account --create --name <sa-name> --scopes \"workflow:read,workflow:write\" ``` **3. Clock Skew Issues** JWT validation fails when system clocks are out of sync (tolerance: \u00b1300 seconds).",
              "score": null
            }
          ],
          "latency_ms": 11.637348001386272,
          "query_id": "realistic_002",
          "dimension": "casual",
          "query": "429 error fix"
        },
        {
          "key_facts": [
            "429 Too Many Requests",
            "X-RateLimit-Remaining",
            "Retry-After",
            "Monitor X-RateLimit-Remaining header values",
            "Implement exponential backoff when receiving 429 responses"
          ],
          "found_facts": [],
          "missed_facts": [
            "429 Too Many Requests",
            "X-RateLimit-Remaining",
            "Retry-After",
            "Monitor X-RateLimit-Remaining header values",
            "Implement exponential backoff when receiving 429 responses"
          ],
          "coverage": 0.0,
          "expected_docs": [
            "api_reference",
            "troubleshooting_guide"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_0",
              "content": "# CloudFlow Platform Troubleshooting Guide **Version:** 3.2.1 **Last Updated:** January 2026 **Audience:** Platform Engineers, DevOps, Support Teams ## Table of Contents 1. [Overview](#overview) 2. [Authentication & Authorization Issues](#authentication--authorization-issues) 3. [Performance Problems](#performance-problems) 4. [Database Connection Issues](#database-connection-issues) 5. [Workflow Execution Failures](#workflow-execution-failures) 6. [Rate Limiting & Throttling](#rate-limiting--throttling) 7. [Log Analysis & Debugging](#log-analysis--debugging) 8. [Escalation Procedures](#escalation-procedures) --- ## Overview This guide provides comprehensive troubleshooting steps for common CloudFlow platform issues encountered in production environments. Each section includes error symptoms, root cause analysis, resolution steps, and preventive measures. ### Quick Diagnostic Checklist Before diving into specific issues, perform these initial checks: - Verify service health: `cloudflow status --all` - Check API connectivity: `curl -I https://api.cloudflow.io/health` - Review recent deployments: `kubectl get deployments -n cloudflow --sort-by=.metadata.creationTimestamp` - Inspect platform metrics: `cloudflow metrics --last 1h` --- ## Authentication & Authorization Issues ### 401 Unauthorized Errors #### Symptoms - API requests return `401 Unauthorized` - Error message: `Authentication credentials were not provided or are invalid` - Frontend displays \"Session expired\" message #### Common Causes **1. Token Expiration** CloudFlow access tokens expire after 3600 seconds (1 hour) by default. Refresh tokens are valid for 30 days. **Verification:** ```bash # Decode JWT to check expiration echo $CF_ACCESS_TOKEN | cut -d'.' -f2 | base64 -d | jq '.exp' # Compare with current time date +%s ``` **Resolution:** ```bash # Refresh the access token curl -X POST https://api.cloudflow.io/auth/refresh \\ -H \"Content-Type: application/json\" \\ -d '{\"refresh_token\": \"'$CF_REFRESH_TOKEN'\"}' # Update environment variable export CF_ACCESS_TOKEN=\"<new_token>\" ``` **2. Invalid JWT Signature** Error: `JWT signature verification failed` **Causes:** - Token was modified or corrupted - Using wrong signing key - Token generated with different secret **Resolution:** ```bash # Validate token structure cloudflow auth validate --token $CF_ACCESS_TOKEN # Generate new token cloudflow auth login --username <user> --password <pass> # For service accounts cloudflow auth service-account --create --name <sa-name> --scopes \"workflow:read,workflow:write\" ``` **3. Clock Skew Issues** JWT validation fails when system clocks are out of sync (tolerance: \u00b1300 seconds).",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_3",
              "content": "Workflow Context Accumulation** Large workflow executions may accumulate state in memory. **Solution:** ```bash # Configure context cleanup cloudflow config set workflow.context.max_size_mb 100 cloudflow config set workflow.context.cleanup_threshold 0.8 # Enable context persistence to disk cloudflow config set workflow.context.persistence.enabled true cloudflow config set workflow.context.persistence.backend s3 ``` **2. Connection Pool Leaks** **Diagnosis:** ```bash # Check active connections cloudflow db connections --verbose # Expected output: # Active: 45/100 # Idle: 23 # Waiting: 2 # Average age: 245s ``` **Resolution:** ```bash # Adjust connection pool settings cloudflow config set db.pool.max_connections 100 cloudflow config set db.pool.min_connections 10 cloudflow config set db.pool.idle_timeout 300 cloudflow config set db.pool.max_lifetime 1800 # Force connection pool reset cloudflow db pool reset --confirm ``` **3. Event Stream Buffers** Unbounded event buffers can cause memory exhaustion. ```bash # Configure event buffer limits cloudflow config set events.buffer.max_size 10000 cloudflow config set events.buffer.overflow_strategy drop_oldest # Enable event streaming to external sink cloudflow config set events.sink.type kafka cloudflow config set events.sink.kafka.brokers \"kafka-1:9092,kafka-2:9092\" cloudflow config set events.sink.kafka.topic cloudflow-events ``` --- ## Database Connection Issues ### Connection Pool Exhaustion #### Symptoms - Error: `could not obtain connection from pool within 5000ms` - Error: `connection pool exhausted (100/100 connections in use)` - API requests fail with `503 Service Unavailable` - Database CPU usage normal, but connection count at maximum #### Investigation ```bash # Check current connection pool status cloudflow db pool status --detailed # Output example: # Pool Statistics: # Total Connections: 100/100 (100%) # Active: 87 # Idle: 13 # Waiting Requests: 45 # Average Wait Time: 3420ms # Max Wait Time: 8234ms # Identify long-running queries cloudflow db queries --status running --min-duration 30000 # Check connection distribution by client SELECT application_name, state, COUNT(*) as conn_count, AVG(EXTRACT(EPOCH FROM (NOW() - state_change))) as avg_duration_sec FROM pg_stat_activity WHERE datname = 'cloudflow_production' GROUP BY application_name, state ORDER BY conn_count DESC; ``` #### Resolution **Immediate mitigation:** ```bash # Temporarily increase connection limit (requires database restart) cloudflow db config set max_connections 150 # Kill idle connections older than 5 minutes cloudflow db connections kill --idle-timeout 300 # Restart connection pool without downtime kubectl rollout restart deployment/cloudflow-api -n cloudflow kubectl rollout status deployment/cloudflow-api -n cloudflow ``` **Long-term solutions:** 1.",
              "score": null
            },
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_0",
              "content": "# CloudFlow API Reference Version 2.1.0 | Last Updated: January 2026 ## Overview The CloudFlow API is a RESTful service that enables developers to programmatically manage cloud workflows, data pipelines, and automation tasks. This documentation provides comprehensive details on authentication, endpoints, request/response formats, error handling, and best practices. **Base URL:** `https://api.cloudflow.io/v2` **API Status:** https://status.cloudflow.io ## Authentication CloudFlow supports three authentication methods to suit different use cases and security requirements. ### API Keys API keys provide simple authentication for server-to-server communication. Include your API key in the request header: ```bash curl -H \"X-API-Key: cf_live_a1b2c3d4e5f6g7h8i9j0\" \\ https://api.cloudflow.io/v2/workflows ``` **Security Notes:** - Never expose API keys in client-side code - Rotate keys every 90 days - Use separate keys for development and production environments ### OAuth 2.0 OAuth 2.0 is recommended for applications that access CloudFlow on behalf of users. We support the Authorization Code flow with PKCE. **Authorization Endpoint:** `https://auth.cloudflow.io/oauth/authorize` **Token Endpoint:** `https://auth.cloudflow.io/oauth/token` **Supported Scopes:** - `workflows:read` - Read workflow configurations - `workflows:write` - Create and modify workflows - `pipelines:read` - Read pipeline data - `pipelines:write` - Create and manage pipelines - `analytics:read` - Access analytics and metrics - `admin:full` - Full administrative access Example authorization request: ```bash curl -X POST https://auth.cloudflow.io/oauth/token \\ -H \"Content-Type: application/x-www-form-urlencoded\" \\ -d \"grant_type=authorization_code\" \\ -d \"code=AUTH_CODE_HERE\" \\ -d \"client_id=YOUR_CLIENT_ID\" \\ -d \"client_secret=YOUR_CLIENT_SECRET\" \\ -d \"redirect_uri=https://yourapp.com/callback\" ``` ### JWT Tokens For advanced use cases, CloudFlow supports JSON Web Tokens (JWT) with RS256 signing algorithm. JWTs must include the following claims: - `iss` (issuer): Your application identifier - `sub` (subject): User or service account ID - `aud` (audience): `https://api.cloudflow.io` - `exp` (expiration): Unix timestamp (max 3600 seconds from `iat`) - `iat` (issued at): Unix timestamp - `scope`: Space-separated list of requested scopes Example JWT header: ```python import jwt import time from cryptography.hazmat.primitives import serialization from cryptography.hazmat.backends import default_backend # Load your private key with open('private_key.pem', 'rb') as key_file: private_key = serialization.load_pem_private_key( key_file.read(), password=None, backend=default_backend() ) # Create JWT payload payload = { 'iss': 'your-app-id', 'sub': 'user-12345', 'aud': 'https://api.cloudflow.io', 'exp': int(time.time()) + 3600, 'iat': int(time.time()), 'scope': 'workflows:read workflows:write' } # Generate token token = jwt.encode(payload, private_key, algorithm='RS256') # Use in API request headers = {'Authorization': f'Bearer {token}'} ``` **Token Expiration:** All tokens expire after 3600 seconds (1 hour). Implement token refresh logic in your application.",
              "score": null
            },
            {
              "doc_id": "user_guide",
              "chunk_id": "user_guide_fix_0",
              "content": "# CloudFlow User Guide Welcome to CloudFlow, the modern workflow automation platform that helps you connect your apps, automate repetitive tasks, and build powerful integrations without writing code. ## Table of Contents 1. [Getting Started](#getting-started) 2. [Workflow Creation](#workflow-creation) 3. [Available Actions](#available-actions) 4. [Variables and Expressions](#variables-and-expressions) 5. [Scheduling](#scheduling) 6. [Error Handling](#error-handling) 7. [Workflow Limits](#workflow-limits) 8. [Best Practices](#best-practices) 9. [Common Workflow Patterns](#common-workflow-patterns) ## Getting Started ### Account Setup Creating your CloudFlow account takes just a few minutes: 1. Navigate to https://app.cloudflow.io/signup 2. Enter your email address and create a strong password 3. Verify your email address by clicking the link sent to your inbox 4. Complete your profile by providing your organization name and use case 5. Choose your plan (Free, Professional, or Enterprise) Once your account is created, you'll be redirected to your CloudFlow dashboard where you can start building workflows immediately. ### Your First Workflow Let's create a simple workflow that sends you a Slack notification when a new file is uploaded to your Google Drive: 1. Click the **\"Create Workflow\"** button in the top-right corner 2. Give your workflow a name: \"Google Drive to Slack Notifier\" 3. Select **\"Google Drive - New File\"** as your trigger 4. Authenticate your Google Drive account when prompted 5. Choose the folder you want to monitor 6. Click **\"Add Action\"** and select **\"Slack - Send Message\"** 7. Authenticate your Slack account 8. Select the channel where you want to receive notifications 9. Customize your message using variables: `New file uploaded: {{trigger.file.name}}` 10. Click **\"Save & Activate\"** to enable your workflow Congratulations! Your first workflow is now live. Every time a file is added to the specified Google Drive folder, you'll receive a Slack notification. ## Workflow Creation CloudFlow offers multiple ways to create and manage workflows, giving you the flexibility to work in the way that suits you best. ### Visual Editor The Visual Editor is CloudFlow's drag-and-drop interface for building workflows without code. It's perfect for users who prefer a graphical approach: **Key Features:** - Drag-and-drop action blocks from the sidebar - Visual connections between steps show your workflow logic - Inline configuration for each action - Real-time validation and error checking - Test mode to run your workflow with sample data **Using the Visual Editor:** 1.",
              "score": null
            },
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_4",
              "content": "### Error Response Format ```json { \"error\": { \"code\": \"invalid_parameter\", \"message\": \"The 'limit' parameter must be between 1 and 100\", \"field\": \"limit\", \"request_id\": \"req_8k3m9x2p\" } } ``` ### HTTP Status Codes | Status Code | Description | Common Causes | |------------|-------------|---------------| | 400 | Bad Request | Invalid parameters, malformed JSON, validation errors | | 401 | Unauthorized | Missing or invalid authentication credentials | | 403 | Forbidden | Insufficient permissions for requested resource | | 404 | Not Found | Resource does not exist or has been deleted | | 429 | Too Many Requests | Rate limit exceeded, retry after specified period | | 500 | Internal Server Error | Unexpected server error, contact support if persists | | 502 | Bad Gateway | Temporary service issue, retry with exponential backoff | | 503 | Service Unavailable | Scheduled maintenance or temporary outage | ### Error Codes CloudFlow returns specific error codes to help you identify and resolve issues: - `invalid_parameter`: One or more request parameters are invalid - `missing_required_field`: Required field is missing from request body - `authentication_failed`: Invalid API key or token - `insufficient_permissions`: User lacks required scope or permission - `resource_not_found`: Requested resource does not exist - `rate_limit_exceeded`: Too many requests, see rate limiting section - `workflow_execution_failed`: Workflow execution encountered an error - `invalid_json`: Request body contains malformed JSON - `duplicate_resource`: Resource with same identifier already exists - `quota_exceeded`: Account quota limit reached ### Error Handling Best Practices **Python Example:** ```python import requests import time def make_api_request(url, headers, max_retries=3): for attempt in range(max_retries): try: response = requests.get(url, headers=headers) if response.status_code == 200: return response.json() elif response.status_code == 429: # Rate limit exceeded retry_after = int(response.headers.get('X-RateLimit-Reset', 60)) print(f\"Rate limited. Waiting {retry_after} seconds...\") time.sleep(retry_after) continue elif response.status_code >= 500: # Server error - retry with exponential backoff wait_time = 2 ** attempt print(f\"Server error. Retrying in {wait_time} seconds...\") time.sleep(wait_time) continue else: # Client error - don't retry error_data = response.json() raise Exception(f\"API Error: {error_data['error']['message']}\") except requests.exceptions.RequestException as e: print(f\"Request failed: {e}\") if attempt < max_retries - 1: time.sleep(2 ** attempt) else: raise raise Exception(\"Max retries exceeded\") ``` ## Webhooks CloudFlow can send webhook notifications when specific events occur in your workflows.",
              "score": null
            }
          ],
          "latency_ms": 11.983954002062092,
          "query_id": "realistic_002",
          "dimension": "contextual",
          "query": "I'm getting throttled by CloudFlow API, how can I handle this in my application?"
        },
        {
          "key_facts": [
            "429 Too Many Requests",
            "X-RateLimit-Remaining",
            "Retry-After",
            "Monitor X-RateLimit-Remaining header values",
            "Implement exponential backoff when receiving 429 responses"
          ],
          "found_facts": [
            "429 Too Many Requests",
            "X-RateLimit-Remaining",
            "Retry-After",
            "Implement exponential backoff when receiving 429 responses"
          ],
          "missed_facts": [
            "Monitor X-RateLimit-Remaining header values"
          ],
          "coverage": 0.8,
          "expected_docs": [
            "api_reference",
            "troubleshooting_guide"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_4",
              "content": "### Error Response Format ```json { \"error\": { \"code\": \"invalid_parameter\", \"message\": \"The 'limit' parameter must be between 1 and 100\", \"field\": \"limit\", \"request_id\": \"req_8k3m9x2p\" } } ``` ### HTTP Status Codes | Status Code | Description | Common Causes | |------------|-------------|---------------| | 400 | Bad Request | Invalid parameters, malformed JSON, validation errors | | 401 | Unauthorized | Missing or invalid authentication credentials | | 403 | Forbidden | Insufficient permissions for requested resource | | 404 | Not Found | Resource does not exist or has been deleted | | 429 | Too Many Requests | Rate limit exceeded, retry after specified period | | 500 | Internal Server Error | Unexpected server error, contact support if persists | | 502 | Bad Gateway | Temporary service issue, retry with exponential backoff | | 503 | Service Unavailable | Scheduled maintenance or temporary outage | ### Error Codes CloudFlow returns specific error codes to help you identify and resolve issues: - `invalid_parameter`: One or more request parameters are invalid - `missing_required_field`: Required field is missing from request body - `authentication_failed`: Invalid API key or token - `insufficient_permissions`: User lacks required scope or permission - `resource_not_found`: Requested resource does not exist - `rate_limit_exceeded`: Too many requests, see rate limiting section - `workflow_execution_failed`: Workflow execution encountered an error - `invalid_json`: Request body contains malformed JSON - `duplicate_resource`: Resource with same identifier already exists - `quota_exceeded`: Account quota limit reached ### Error Handling Best Practices **Python Example:** ```python import requests import time def make_api_request(url, headers, max_retries=3): for attempt in range(max_retries): try: response = requests.get(url, headers=headers) if response.status_code == 200: return response.json() elif response.status_code == 429: # Rate limit exceeded retry_after = int(response.headers.get('X-RateLimit-Reset', 60)) print(f\"Rate limited. Waiting {retry_after} seconds...\") time.sleep(retry_after) continue elif response.status_code >= 500: # Server error - retry with exponential backoff wait_time = 2 ** attempt print(f\"Server error. Retrying in {wait_time} seconds...\") time.sleep(wait_time) continue else: # Client error - don't retry error_data = response.json() raise Exception(f\"API Error: {error_data['error']['message']}\") except requests.exceptions.RequestException as e: print(f\"Request failed: {e}\") if attempt < max_retries - 1: time.sleep(2 ** attempt) else: raise raise Exception(\"Max retries exceeded\") ``` ## Webhooks CloudFlow can send webhook notifications when specific events occur in your workflows.",
              "score": null
            },
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_1",
              "content": "## Rate Limiting To ensure fair usage and system stability, CloudFlow enforces rate limits on all API endpoints. **Default Limits:** - 100 requests per minute per authenticated user - 20 requests per minute for unauthenticated requests - Burst allowance: 150 requests in a 10-second window ### Rate Limit Headers Every API response includes rate limit information: ``` X-RateLimit-Limit: 100 X-RateLimit-Remaining: 87 X-RateLimit-Reset: 1640995200 ``` When you exceed the rate limit, you'll receive a `429 Too Many Requests` response: ```json { \"error\": { \"code\": \"rate_limit_exceeded\", \"message\": \"Rate limit exceeded. Please retry after 42 seconds.\", \"retry_after\": 42 } } ``` **Best Practices:** - Monitor `X-RateLimit-Remaining` header values - Implement exponential backoff when receiving 429 responses - Cache responses when appropriate to reduce API calls - Consider upgrading to Enterprise tier for higher limits (1000 req/min) ## Pagination List endpoints return paginated results to optimize performance. Use `limit` and `offset` parameters to navigate through result sets. **Parameters:** - `limit`: Number of items per page (default: 20, max: 100) - `offset`: Number of items to skip (default: 0) **Example Request:** ```bash curl -H \"X-API-Key: YOUR_API_KEY\" \\ \"https://api.cloudflow.io/v2/workflows?limit=50&offset=100\" ``` **Response Structure:** ```json { \"data\": [...], \"pagination\": { \"total\": 347, \"limit\": 50, \"offset\": 100, \"has_more\": true } } ``` **Python Example:** ```python import requests def fetch_all_workflows(api_key): base_url = \"https://api.cloudflow.io/v2/workflows\" headers = {\"X-API-Key\": api_key} all_workflows = [] offset = 0 limit = 100 while True: response = requests.get( base_url, headers=headers, params={\"limit\": limit, \"offset\": offset} ) data = response.json() all_workflows.extend(data['data']) if not data['pagination']['has_more']: break offset += limit return all_workflows ``` ## Endpoints ### Workflows #### List Workflows Retrieve a paginated list of all workflows in your account. **Endpoint:** `GET /workflows` **Query Parameters:** - `status` (optional): Filter by status (`active`, `paused`, `archived`) - `created_after` (optional): ISO 8601 timestamp - `limit` (optional): Items per page (max 100) - `offset` (optional): Pagination offset **Example Request:** ```bash curl -H \"X-API-Key: YOUR_API_KEY\" \\ \"https://api.cloudflow.io/v2/workflows?status=active&limit=25\" ``` **Example Response (200 OK):** ```json { \"data\": [ { \"id\": \"wf_8x7k2m9p\", \"name\": \"Data Processing Pipeline\", \"description\": \"Processes customer data every hour\", \"status\": \"active\", \"trigger\": { \"type\": \"schedule\", \"cron\": \"0 * * * *\" }, \"steps\": 12, \"created_at\": \"2026-01-15T10:30:00Z\", \"updated_at\": \"2026-01-23T14:22:00Z\", \"last_run\": \"2026-01-24T09:00:00Z\" } ], \"pagination\": { \"total\": 1, \"limit\": 25, \"offset\": 0, \"has_more\": false } } ``` #### Create Workflow Create a new workflow with specified configuration.",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_6",
              "content": "Data Validation Errors** ``` ValidationError: Field 'customer_id' is required but missing in 234 records ``` **Resolution:** ```bash # Add data quality checks cloudflow workflows update wf_9k2n4m8p1q \\ --step data_ingestion \\ --add-validator required_fields \\ --validator-config '{\"fields\": [\"customer_id\", \"timestamp\", \"amount\"]}' # Configure error handling cloudflow workflows update wf_9k2n4m8p1q \\ --step data_validation \\ --on-error continue \\ --error-threshold 5% # Fail if > 5% of records invalid ``` **2. External API Failures** ``` ExternalAPIError: API request to https://partner-api.example.com failed with status 502 ``` **Resolution:** ```bash # Add circuit breaker cloudflow workflows update wf_9k2n4m8p1q \\ --step external_api_call \\ --circuit-breaker-enabled true \\ --circuit-breaker-threshold 5 \\ --circuit-breaker-timeout 30000 # Configure fallback behavior cloudflow workflows update wf_9k2n4m8p1q \\ --step external_api_call \\ --fallback-action use_cached_data \\ --cache-ttl 3600 ``` --- ## Rate Limiting & Throttling ### 429 Too Many Requests #### Error Response ```json { \"error\": \"rate_limit_exceeded\", \"message\": \"API rate limit exceeded. Retry after 45 seconds.\", \"status\": 429, \"headers\": { \"X-RateLimit-Limit\": \"1000\", \"X-RateLimit-Remaining\": \"0\", \"X-RateLimit-Reset\": \"1706112345\", \"Retry-After\": \"45\" } } ``` #### Rate Limit Tiers CloudFlow enforces the following rate limits per workspace: | Tier | Requests/Minute | Requests/Hour | Concurrent Workflows | |------|-----------------|---------------|----------------------| | Free | 60 | 1,000 | 5 | | Standard | 1,000 | 50,000 | 50 | | Premium | 5,000 | 250,000 | 200 | | Enterprise | Custom | Custom | Unlimited | #### Checking Rate Limit Status ```bash # Check current rate limit status curl -I https://api.cloudflow.io/api/v1/workflows \\ -H \"Authorization: Bearer $CF_ACCESS_TOKEN\" # Extract rate limit headers curl -s -I https://api.cloudflow.io/api/v1/workflows \\ -H \"Authorization: Bearer $CF_ACCESS_TOKEN\" | \\ grep -i \"x-ratelimit\" # Output: # X-RateLimit-Limit: 1000 # X-RateLimit-Remaining: 247 # X-RateLimit-Reset: 1706112400 # Monitor rate limit usage cloudflow metrics query \\ --metric rate_limit_remaining \\ --workspace ws_abc123 \\ --last 1h \\ --interval 1m ``` #### Handling Rate Limits in Code **Python example with retry logic:** ```python import time import requests def cloudflow_api_call_with_retry(url, headers, max_retries=3): for attempt in range(max_retries): response = requests.get(url, headers=headers) if response.status_code == 429: retry_after = int(response.headers.get('Retry-After', 60)) print(f\"Rate limited.",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_7",
              "content": "Waiting {retry_after} seconds...\") time.sleep(retry_after) continue remaining = int(response.headers.get('X-RateLimit-Remaining', 0)) if remaining < 10: print(f\"Warning: Only {remaining} requests remaining\") return response raise Exception(\"Max retries exceeded due to rate limiting\") ``` **Bash script with rate limit checking:** ```bash #!/bin/bash check_rate_limit() { local remaining=$(curl -s -I https://api.cloudflow.io/api/v1/workflows \\ -H \"Authorization: Bearer $CF_ACCESS_TOKEN\" | \\ grep -i \"x-ratelimit-remaining\" | \\ awk '{print $2}' | tr -d '\\r') if [ \"$remaining\" -lt 10 ]; then echo \"Warning: Only $remaining requests remaining\" local reset=$(curl -s -I https://api.cloudflow.io/api/v1/workflows \\ -H \"Authorization: Bearer $CF_ACCESS_TOKEN\" | \\ grep -i \"x-ratelimit-reset\" | \\ awk '{print $2}' | tr -d '\\r') local wait_time=$((reset - $(date +%s))) echo \"Rate limit resets in $wait_time seconds\" sleep $wait_time fi } # Use before API calls check_rate_limit cloudflow workflows execute wf_9k2n4m8p1q ``` #### Optimization Strategies **1. Implement request batching:** ```bash # Batch multiple workflow executions cloudflow workflows execute-batch \\ --workflow-ids \"wf_id1,wf_id2,wf_id3,wf_id4,wf_id5\" \\ --batch-size 5 # This counts as 1 API request instead of 5 ``` **2. Use webhooks instead of polling:** ```bash # Configure webhook for workflow completion cloudflow webhooks create \\ --event workflow.completed \\ --url https://your-service.com/webhooks/cloudflow \\ --secret $WEBHOOK_SECRET # Verify webhook cloudflow webhooks test webhook_abc123 ``` **3.",
              "score": null
            },
            {
              "doc_id": "user_guide",
              "chunk_id": "user_guide_fix_7",
              "content": "Handle Errors Gracefully Always implement error handling for external API calls and database operations: ```yaml - id: fetch_data action: http_request config: url: \"https://api.example.com/data\" retry: max_attempts: 3 backoff_type: exponential on_error: - id: log_error action: database_query config: query: \"INSERT INTO error_log (workflow_id, error) VALUES ($1, $2)\" parameters: - \"{{workflow.id}}\" - \"{{error.message}}\" ``` ### 3. Use Secrets for Sensitive Data Never hardcode API keys, passwords, or tokens in workflows: **Bad:** ```yaml headers: Authorization: \"Bearer sk_live_abc123xyz789\" ``` **Good:** ```yaml headers: Authorization: \"Bearer {{secrets.API_TOKEN}}\" ``` Store secrets in **Settings** > **Secrets** with encryption at rest. ### 4. Validate Input Data Always validate trigger data before processing: ```yaml - id: validate_input action: javascript code: | const required_fields = ['email', 'name', 'order_id']; for (const field of required_fields) { if (!input[field]) { throw new Error(`Missing required field: ${field}`); } } // Validate email format const email_regex = /^[^\\s@]+@[^\\s@]+\\.[^\\s@]+$/; if (!email_regex.test(input.email)) { throw new Error('Invalid email format'); } return { validated: true }; ``` ### 5. Use Idempotency Keys For operations that shouldn't be repeated (payments, record creation), use idempotency keys: ```yaml - id: create_charge action: http_request config: url: \"https://api.stripe.com/v1/charges\" method: POST headers: Idempotency-Key: \"{{workflow.id}}-{{execution.id}}\" body: amount: \"{{amount}}\" ``` ### 6. Monitor and Log Add logging steps for important workflow milestones: ```yaml - id: log_start action: database_query config: query: \"INSERT INTO workflow_audit (execution_id, step, timestamp) VALUES ($1, $2, $3)\" parameters: - \"{{execution.id}}\" - \"workflow_started\" - \"{{now()}}\" ``` ### 7. Keep Workflows Modular Break complex workflows into smaller, reusable components: - Use sub-workflows for repeated logic - Trigger child workflows via webhooks - Share common configurations via templates ### 8. Test Thoroughly Before activating a workflow: 1. Use test mode with sample data 2. Verify all actions execute correctly 3. Test error handling paths 4. Review execution logs 5. Start with a limited scope (e.g., test channel, small dataset) ### 9. Document Your Workflows Add descriptions to workflows and steps: ```yaml workflow: name: \"Daily Sales Report\" description: | Generates a daily sales report and distributes it to the sales team. Runs at 8:00 AM EST Monday-Friday. Data source: PostgreSQL sales database Recipients: sales-team@company.com steps: - id: fetch_sales name: \"Fetch Yesterday's Sales\" description: \"Query database for all completed orders from previous day\" ``` ### 10.",
              "score": null
            }
          ],
          "latency_ms": 11.618412001553224,
          "query_id": "realistic_002",
          "dimension": "negation",
          "query": "Why am I being blocked with rate limit errors?"
        },
        {
          "key_facts": [
            "3600 seconds",
            "max 3600 seconds from iat",
            "All tokens expire after 3600 seconds"
          ],
          "found_facts": [
            "3600 seconds"
          ],
          "missed_facts": [
            "max 3600 seconds from iat",
            "All tokens expire after 3600 seconds"
          ],
          "coverage": 0.3333333333333333,
          "expected_docs": [
            "api_reference"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_0",
              "content": "# CloudFlow Platform Troubleshooting Guide **Version:** 3.2.1 **Last Updated:** January 2026 **Audience:** Platform Engineers, DevOps, Support Teams ## Table of Contents 1. [Overview](#overview) 2. [Authentication & Authorization Issues](#authentication--authorization-issues) 3. [Performance Problems](#performance-problems) 4. [Database Connection Issues](#database-connection-issues) 5. [Workflow Execution Failures](#workflow-execution-failures) 6. [Rate Limiting & Throttling](#rate-limiting--throttling) 7. [Log Analysis & Debugging](#log-analysis--debugging) 8. [Escalation Procedures](#escalation-procedures) --- ## Overview This guide provides comprehensive troubleshooting steps for common CloudFlow platform issues encountered in production environments. Each section includes error symptoms, root cause analysis, resolution steps, and preventive measures. ### Quick Diagnostic Checklist Before diving into specific issues, perform these initial checks: - Verify service health: `cloudflow status --all` - Check API connectivity: `curl -I https://api.cloudflow.io/health` - Review recent deployments: `kubectl get deployments -n cloudflow --sort-by=.metadata.creationTimestamp` - Inspect platform metrics: `cloudflow metrics --last 1h` --- ## Authentication & Authorization Issues ### 401 Unauthorized Errors #### Symptoms - API requests return `401 Unauthorized` - Error message: `Authentication credentials were not provided or are invalid` - Frontend displays \"Session expired\" message #### Common Causes **1. Token Expiration** CloudFlow access tokens expire after 3600 seconds (1 hour) by default. Refresh tokens are valid for 30 days. **Verification:** ```bash # Decode JWT to check expiration echo $CF_ACCESS_TOKEN | cut -d'.' -f2 | base64 -d | jq '.exp' # Compare with current time date +%s ``` **Resolution:** ```bash # Refresh the access token curl -X POST https://api.cloudflow.io/auth/refresh \\ -H \"Content-Type: application/json\" \\ -d '{\"refresh_token\": \"'$CF_REFRESH_TOKEN'\"}' # Update environment variable export CF_ACCESS_TOKEN=\"<new_token>\" ``` **2. Invalid JWT Signature** Error: `JWT signature verification failed` **Causes:** - Token was modified or corrupted - Using wrong signing key - Token generated with different secret **Resolution:** ```bash # Validate token structure cloudflow auth validate --token $CF_ACCESS_TOKEN # Generate new token cloudflow auth login --username <user> --password <pass> # For service accounts cloudflow auth service-account --create --name <sa-name> --scopes \"workflow:read,workflow:write\" ``` **3. Clock Skew Issues** JWT validation fails when system clocks are out of sync (tolerance: \u00b1300 seconds).",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_1",
              "content": "**Technology**: Node.js with Express.js framework **Replicas**: 12 pods (production), auto-scaling 8-20 based on CPU **Resource Allocation**: 2 vCPU, 4GB RAM per pod **Key Responsibilities**: - JWT token validation (delegated to Auth Service for initial validation) - Rate limiting: 1000 requests per minute per API key (sliding window) - Request/response transformation and validation using JSON Schema - Routing to downstream services based on URL path patterns - CORS handling for web clients - Request/response logging and correlation ID injection **Critical Endpoints**: - `POST /api/v1/workflows` - Create new workflow - `GET /api/v1/workflows/:id` - Retrieve workflow status - `POST /api/v1/workflows/:id/execute` - Trigger workflow execution - `GET /api/v1/workflows/:id/history` - Get execution history **Dependencies**: - Auth Service (for token validation) - Redis (for rate limiting counters) - All downstream microservices **Performance Targets**: - P50 latency: < 50ms - P99 latency: < 200ms - Throughput: 10,000 RPS sustained --- ### Auth Service **Purpose**: Centralized authentication and authorization service handling user identity, token generation, and permission validation. **Technology**: Go with gRPC for internal communication, REST for external **Replicas**: 8 pods (production), auto-scaling 6-12 **Resource Allocation**: 1 vCPU, 2GB RAM per pod **Key Responsibilities**: - User authentication via multiple providers (OAuth2, SAML, local credentials) - JWT token generation and validation (RS256 algorithm) - Role-based access control (RBAC) with fine-grained permissions - Session management with Redis-backed storage - API key generation and validation for service accounts - MFA enforcement for administrative operations **Authentication Flow**: ``` Client Request \u2192 API Gateway \u2192 Auth Service \u2502 \u251c\u2500 Validate credentials \u251c\u2500 Check MFA if required \u251c\u2500 Generate JWT (15min expiry) \u251c\u2500 Generate refresh token (7 days) \u2514\u2500 Store session in Redis ``` **Token Structure**: - Access Token: JWT with 15-minute expiry - Refresh Token: Opaque token with 7-day expiry, stored in PostgreSQL - Claims: user_id, email, roles[], permissions[], tenant_id **Security Features**: - Password hashing: Argon2id with 64MB memory, 4 iterations - Token rotation on refresh to prevent replay attacks - Brute force protection: 5 failed attempts \u2192 15-minute lockout - Secrets stored in HashiCorp Vault, rotated every 90 days **Performance Targets**: - Token validation: < 10ms (P99) - Token generation: < 50ms (P99) - Throughput: 5,000 RPS for validation operations --- ### Workflow Engine **Purpose**: Core orchestration service that executes workflow definitions, manages state transitions, and coordinates task execution across distributed systems.",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_6",
              "content": "**Session Storage**: - Key pattern: `session:{user_id}` - TTL: 15 minutes (aligned with JWT expiry) - Data: User session state, preferences - Invalidation: On logout or password change 2. **Rate Limiting Counters**: - Key pattern: `ratelimit:{api_key}:{window}` - TTL: 60 seconds (sliding window) - Data: Request count per window - Algorithm: Token bucket with Redis INCR 3. **Compiled Templates**: - Key pattern: `template:{template_id}:{version}` - TTL: 1 hour - Data: Compiled Handlebars template - Invalidation: On template update 4. **Workflow Definitions**: - Key pattern: `workflow:def:{workflow_id}` - TTL: 1 hour - Data: Parsed workflow JSON - Invalidation: On workflow update or manual flush 5. **User Profiles**: - Key pattern: `user:profile:{user_id}` - TTL: 30 minutes - Data: User metadata (name, email, roles) - Invalidation: On profile update **Cache Hit Rates** (Production Metrics): - Session lookups: 98.5% - Workflow definitions: 94.2% - Template cache: 99.1% - User profiles: 91.8% **Performance Characteristics**: - GET operation P99: < 2ms - SET operation P99: < 3ms - Throughput: 100,000 operations per second - Network latency: < 1ms (same AZ) --- ### Kafka Event Streaming **Cluster Configuration**: - 5 broker nodes (distributed across 3 AZs) - Instance type: kafka.m5.2xlarge (8 vCPU, 32GB RAM) - Storage: 10TB per broker (gp3 SSD) - ZooKeeper ensemble: 3 nodes for cluster coordination - Replication factor: 3 (min in-sync replicas: 2) **Topic Architecture**: ``` workflow.events (32 partitions): - Workflow lifecycle events (created, started, completed, failed) - Retention: 7 days - Message rate: 5,000/sec peak - Consumer groups: workflow-engine, analytics-pipeline notifications.email (16 partitions): - Email notification triggers - Retention: 3 days - Message rate: 2,000/sec peak - Consumer groups: notification-service notifications.sms (8 partitions): - SMS notification triggers - Retention: 3 days - Message rate: 500/sec peak - Consumer groups: notification-service audit.logs (24 partitions): - Security and compliance audit events - Retention: 90 days (compliance requirement) - Message rate: 3,000/sec peak - Consumer groups: audit-processor, security-monitor dead-letter-queue (8 partitions): - Failed message processing - Retention: 30 days - Manual intervention required - Consumer groups: ops-team-alerts ``` **Producer Configuration**: - Acknowledgment: `acks=all` (wait for all in-sync replicas) - Compression: LZ4 (reduces network bandwidth by 60%) - Batching: 100ms linger time, 100KB batch size - Idempotence: Enabled to prevent duplicates **Consumer Configuration**: - Auto-commit: Disabled (manual commit after processing) - Offset reset: Earliest (replay from beginning on new consumer group) - Max poll",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_8",
              "content": "### Cache Invalidation Strategies **Time-Based Expiration (TTL)**: - Short-lived data: 5-15 minutes (session tokens, rate limit counters) - Medium-lived data: 1 hour (workflow definitions, templates) - Long-lived data: 24 hours (static configuration) **Event-Based Invalidation**: ``` Database Update Event \u2192 Kafka (cache.invalidation topic) \u2502 \u25bc All service instances consume event \u2502 \u25bc Redis DEL for affected keys ``` **Pattern-Based Invalidation**: - Use Redis SCAN + DEL for wildcard patterns - Example: Invalidate all user caches: `user:*:{user_id}` **Cache Warming**: - Scheduled job runs every hour to pre-populate frequently accessed data - Targets: Top 1000 workflows, system configuration, active user profiles - Reduces cache miss rate during traffic spikes ### Multi-Level Caching Application implements L1 (in-memory) and L2 (Redis) caching: ``` Request \u2192 L1 Cache (Node.js Map, 1000 entry LRU) \u2502 MISS \u2502 \u25bc L2 Cache (Redis, distributed) \u2502 MISS \u2502 \u25bc Database (PostgreSQL) ``` L1 cache reduces Redis network calls by 40% for hot data. --- ## Security Architecture ### Network Security **Zero-Trust Network Model**: - All service-to-service communication encrypted with mTLS - Certificate rotation: Every 90 days (automated via cert-manager) - Certificate authority: Internal PKI with HashiCorp Vault - Network segmentation: Private subnets for services, public subnet for ALB only **Service Mesh (Istio)**: ``` Service A \u2192 Envoy Sidecar (mTLS client cert) \u2502 Encrypted channel \u2502 Envoy Sidecar (mTLS server cert) \u2192 Service B ``` **Firewall Rules**: - Security groups: Deny all by default, explicit allow rules - Ingress: Only ALB can reach API Gateway (port 8080) - Egress: Services can only reach specific dependencies - Database access: Limited to application subnets only ### Authentication & Authorization **JWT Token Validation**: - Algorithm: RS256 (asymmetric signing) - Key rotation: Every 30 days with 7-day overlap period - Public key distribution: JWKS endpoint cached in Redis - Validation: Signature, expiry, issuer, audience claims - Token revocation: Blacklist in Redis for compromised tokens **Permission Model**: ``` User \u2192 Roles \u2192 Permissions \u2198 \u2197 Tenants (Multi-tenancy isolation) ``` Example permissions: - `workflow:read` - View workflows - `workflow:write` - Create/update workflows - `workflow:execute` - Trigger workflow execution - `workflow:delete` - Delete workflows - `admin:*` - All administrative operations **API Key Management**: - Format: `cfk_live_<32-char-random>` (production), `cfk_test_<32-char-random>` (sandbox) - Hashing: SHA-256 before storage in PostgreSQL - Scoping: API keys can be scoped to specific workflows or operations - Rate limits: Configurable",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_1",
              "content": "**Diagnosis:** ```bash # Check system time timedatectl status # Compare with NTP server ntpdate -q pool.ntp.org # Check JWT issued time vs current time jwt_iat=$(echo $CF_ACCESS_TOKEN | cut -d'.' -f2 | base64 -d | jq '.iat') current_time=$(date +%s) skew=$((current_time - jwt_iat)) echo \"Clock skew: $skew seconds\" ``` **Resolution:** ```bash # Sync with NTP server sudo ntpdate -s pool.ntp.org # Enable automatic time sync sudo timedatectl set-ntp true # Restart CloudFlow client cloudflow restart ``` **4. Insufficient Permissions** Error: `User does not have required permissions for this operation` **Check user roles:** ```bash cloudflow auth whoami --verbose # Expected output: # User: john.doe@company.com # Roles: developer, workflow-admin # Scopes: workflow:*, data:read, metrics:read ``` **Request permission elevation:** ```bash # Submit access request cloudflow auth request-access \\ --resource \"workflow:production:deploy\" \\ --justification \"Deploy critical hotfix for TICKET-1234\" # Check pending approvals cloudflow auth list-requests --status pending ``` ### 403 Forbidden Errors #### RBAC Policy Violations CloudFlow uses role-based access control (RBAC) with the following hierarchy: - `viewer` - Read-only access - `developer` - Create and modify workflows (non-production) - `operator` - Execute workflows, view logs - `admin` - Full access to workspace - `platform-admin` - Cross-workspace administration **Verify resource permissions:** ```bash # Check effective permissions cloudflow rbac check \\ --user john.doe@company.com \\ --resource workflow:prod-pipeline \\ --action execute # List all policies affecting user cloudflow rbac policies --user john.doe@company.com --verbose ``` --- ## Performance Problems ### Slow Query Performance #### Symptoms - API response times > 5000ms - Database query latency warnings in logs - CloudFlow UI becomes unresponsive - Timeout errors: `Request exceeded maximum duration of 30000ms` #### Diagnosis Steps **1. Identify Slow Queries** ```bash # Query CloudFlow metrics cloudflow metrics query --metric api_request_duration_ms \\ --filter \"p95 > 5000\" \\ --last 1h # Check database slow query log kubectl logs -n cloudflow deploy/cloudflow-db-primary | \\ grep \"slow query\" | \\ tail -n 50 # Analyze query patterns cloudflow db analyze-queries --min-duration 5000 --limit 20 ``` **2.",
              "score": null
            }
          ],
          "latency_ms": 11.43721500193351,
          "query_id": "realistic_003",
          "dimension": "original",
          "query": "What is the JWT token expiration time?"
        },
        {
          "key_facts": [
            "3600 seconds",
            "max 3600 seconds from iat",
            "All tokens expire after 3600 seconds"
          ],
          "found_facts": [
            "3600 seconds"
          ],
          "missed_facts": [
            "max 3600 seconds from iat",
            "All tokens expire after 3600 seconds"
          ],
          "coverage": 0.3333333333333333,
          "expected_docs": [
            "api_reference"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_0",
              "content": "# CloudFlow Platform Troubleshooting Guide **Version:** 3.2.1 **Last Updated:** January 2026 **Audience:** Platform Engineers, DevOps, Support Teams ## Table of Contents 1. [Overview](#overview) 2. [Authentication & Authorization Issues](#authentication--authorization-issues) 3. [Performance Problems](#performance-problems) 4. [Database Connection Issues](#database-connection-issues) 5. [Workflow Execution Failures](#workflow-execution-failures) 6. [Rate Limiting & Throttling](#rate-limiting--throttling) 7. [Log Analysis & Debugging](#log-analysis--debugging) 8. [Escalation Procedures](#escalation-procedures) --- ## Overview This guide provides comprehensive troubleshooting steps for common CloudFlow platform issues encountered in production environments. Each section includes error symptoms, root cause analysis, resolution steps, and preventive measures. ### Quick Diagnostic Checklist Before diving into specific issues, perform these initial checks: - Verify service health: `cloudflow status --all` - Check API connectivity: `curl -I https://api.cloudflow.io/health` - Review recent deployments: `kubectl get deployments -n cloudflow --sort-by=.metadata.creationTimestamp` - Inspect platform metrics: `cloudflow metrics --last 1h` --- ## Authentication & Authorization Issues ### 401 Unauthorized Errors #### Symptoms - API requests return `401 Unauthorized` - Error message: `Authentication credentials were not provided or are invalid` - Frontend displays \"Session expired\" message #### Common Causes **1. Token Expiration** CloudFlow access tokens expire after 3600 seconds (1 hour) by default. Refresh tokens are valid for 30 days. **Verification:** ```bash # Decode JWT to check expiration echo $CF_ACCESS_TOKEN | cut -d'.' -f2 | base64 -d | jq '.exp' # Compare with current time date +%s ``` **Resolution:** ```bash # Refresh the access token curl -X POST https://api.cloudflow.io/auth/refresh \\ -H \"Content-Type: application/json\" \\ -d '{\"refresh_token\": \"'$CF_REFRESH_TOKEN'\"}' # Update environment variable export CF_ACCESS_TOKEN=\"<new_token>\" ``` **2. Invalid JWT Signature** Error: `JWT signature verification failed` **Causes:** - Token was modified or corrupted - Using wrong signing key - Token generated with different secret **Resolution:** ```bash # Validate token structure cloudflow auth validate --token $CF_ACCESS_TOKEN # Generate new token cloudflow auth login --username <user> --password <pass> # For service accounts cloudflow auth service-account --create --name <sa-name> --scopes \"workflow:read,workflow:write\" ``` **3. Clock Skew Issues** JWT validation fails when system clocks are out of sync (tolerance: \u00b1300 seconds).",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_8",
              "content": "### Cache Invalidation Strategies **Time-Based Expiration (TTL)**: - Short-lived data: 5-15 minutes (session tokens, rate limit counters) - Medium-lived data: 1 hour (workflow definitions, templates) - Long-lived data: 24 hours (static configuration) **Event-Based Invalidation**: ``` Database Update Event \u2192 Kafka (cache.invalidation topic) \u2502 \u25bc All service instances consume event \u2502 \u25bc Redis DEL for affected keys ``` **Pattern-Based Invalidation**: - Use Redis SCAN + DEL for wildcard patterns - Example: Invalidate all user caches: `user:*:{user_id}` **Cache Warming**: - Scheduled job runs every hour to pre-populate frequently accessed data - Targets: Top 1000 workflows, system configuration, active user profiles - Reduces cache miss rate during traffic spikes ### Multi-Level Caching Application implements L1 (in-memory) and L2 (Redis) caching: ``` Request \u2192 L1 Cache (Node.js Map, 1000 entry LRU) \u2502 MISS \u2502 \u25bc L2 Cache (Redis, distributed) \u2502 MISS \u2502 \u25bc Database (PostgreSQL) ``` L1 cache reduces Redis network calls by 40% for hot data. --- ## Security Architecture ### Network Security **Zero-Trust Network Model**: - All service-to-service communication encrypted with mTLS - Certificate rotation: Every 90 days (automated via cert-manager) - Certificate authority: Internal PKI with HashiCorp Vault - Network segmentation: Private subnets for services, public subnet for ALB only **Service Mesh (Istio)**: ``` Service A \u2192 Envoy Sidecar (mTLS client cert) \u2502 Encrypted channel \u2502 Envoy Sidecar (mTLS server cert) \u2192 Service B ``` **Firewall Rules**: - Security groups: Deny all by default, explicit allow rules - Ingress: Only ALB can reach API Gateway (port 8080) - Egress: Services can only reach specific dependencies - Database access: Limited to application subnets only ### Authentication & Authorization **JWT Token Validation**: - Algorithm: RS256 (asymmetric signing) - Key rotation: Every 30 days with 7-day overlap period - Public key distribution: JWKS endpoint cached in Redis - Validation: Signature, expiry, issuer, audience claims - Token revocation: Blacklist in Redis for compromised tokens **Permission Model**: ``` User \u2192 Roles \u2192 Permissions \u2198 \u2197 Tenants (Multi-tenancy isolation) ``` Example permissions: - `workflow:read` - View workflows - `workflow:write` - Create/update workflows - `workflow:execute` - Trigger workflow execution - `workflow:delete` - Delete workflows - `admin:*` - All administrative operations **API Key Management**: - Format: `cfk_live_<32-char-random>` (production), `cfk_test_<32-char-random>` (sandbox) - Hashing: SHA-256 before storage in PostgreSQL - Scoping: API keys can be scoped to specific workflows or operations - Rate limits: Configurable",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_1",
              "content": "**Diagnosis:** ```bash # Check system time timedatectl status # Compare with NTP server ntpdate -q pool.ntp.org # Check JWT issued time vs current time jwt_iat=$(echo $CF_ACCESS_TOKEN | cut -d'.' -f2 | base64 -d | jq '.iat') current_time=$(date +%s) skew=$((current_time - jwt_iat)) echo \"Clock skew: $skew seconds\" ``` **Resolution:** ```bash # Sync with NTP server sudo ntpdate -s pool.ntp.org # Enable automatic time sync sudo timedatectl set-ntp true # Restart CloudFlow client cloudflow restart ``` **4. Insufficient Permissions** Error: `User does not have required permissions for this operation` **Check user roles:** ```bash cloudflow auth whoami --verbose # Expected output: # User: john.doe@company.com # Roles: developer, workflow-admin # Scopes: workflow:*, data:read, metrics:read ``` **Request permission elevation:** ```bash # Submit access request cloudflow auth request-access \\ --resource \"workflow:production:deploy\" \\ --justification \"Deploy critical hotfix for TICKET-1234\" # Check pending approvals cloudflow auth list-requests --status pending ``` ### 403 Forbidden Errors #### RBAC Policy Violations CloudFlow uses role-based access control (RBAC) with the following hierarchy: - `viewer` - Read-only access - `developer` - Create and modify workflows (non-production) - `operator` - Execute workflows, view logs - `admin` - Full access to workspace - `platform-admin` - Cross-workspace administration **Verify resource permissions:** ```bash # Check effective permissions cloudflow rbac check \\ --user john.doe@company.com \\ --resource workflow:prod-pipeline \\ --action execute # List all policies affecting user cloudflow rbac policies --user john.doe@company.com --verbose ``` --- ## Performance Problems ### Slow Query Performance #### Symptoms - API response times > 5000ms - Database query latency warnings in logs - CloudFlow UI becomes unresponsive - Timeout errors: `Request exceeded maximum duration of 30000ms` #### Diagnosis Steps **1. Identify Slow Queries** ```bash # Query CloudFlow metrics cloudflow metrics query --metric api_request_duration_ms \\ --filter \"p95 > 5000\" \\ --last 1h # Check database slow query log kubectl logs -n cloudflow deploy/cloudflow-db-primary | \\ grep \"slow query\" | \\ tail -n 50 # Analyze query patterns cloudflow db analyze-queries --min-duration 5000 --limit 20 ``` **2.",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_9",
              "content": "per API key (default: 1000 RPM) ### Secrets Management **HashiCorp Vault Integration**: - Dynamic database credentials: Generated on-demand, 1-hour TTL - Encryption keys: Transit secrets engine for encryption-as-a-service - API keys for external services: Stored with versioning - Rotation policy: Automated rotation every 90 days with notification **Secret Access Pattern**: ``` Service starts \u2192 Vault authentication (Kubernetes service account) \u2502 \u25bc Request secret lease (1-hour TTL) \u2502 \u25bc Vault returns dynamic credentials \u2502 \u25bc Service connects to PostgreSQL \u2502 Renew lease every 30 minutes ``` ### Data Protection **Encryption at Rest**: - PostgreSQL: AES-256 encryption enabled at volume level - Redis: Encryption enabled using AWS KMS - Kafka: Encryption at rest for all data on brokers - S3 backups: Server-side encryption with KMS (SSE-KMS) **Encryption in Transit**: - External traffic: TLS 1.3 only (TLS 1.2 deprecated) - Internal traffic: mTLS via Istio service mesh - Database connections: SSL/TLS enforced (sslmode=require) **Data Masking**: - Sensitive fields (email, phone) masked in logs - PII redacted from error messages and stack traces - Audit logs: Full data retention with access controls **Compliance**: - GDPR: User data export and deletion workflows - SOC 2 Type II: Audit logging for all data access - HIPAA: PHI isolation in dedicated tenants with enhanced encryption --- ## Performance Characteristics ### Latency Targets **API Operations** (P99 latency): - `GET /workflows/{id}`: < 50ms (cache hit), < 150ms (cache miss) - `POST /workflows`: < 200ms (includes validation and database write) - `POST /workflows/{id}/execute`: < 100ms (async, returns execution ID) - `GET /workflows/{id}/history`: < 300ms (paginated, 50 records per page) **Workflow Execution** (P99 latency): - Simple workflow (< 5 steps): < 2 seconds - Medium workflow (5-15 steps): < 5 seconds - Complex workflow (> 15 steps): < 15 seconds - Parallel execution: Linear scaling up to 10 concurrent branches **Database Operations** (P99 latency): - Simple SELECT: < 5ms - JOIN query: < 20ms - INSERT/UPDATE: < 10ms - Batch operations: < 100ms (batch size: 100 records) **Cache Operations** (P99 latency): - Redis GET: < 2ms - Redis SET: < 3ms - Redis DEL: < 2ms ### Throughput Capacity **API Gateway**: - Sustained: 10,000 requests per second - Peak: 25,000 requests per second (5-minute burst) - Rate limiting: 1,000 requests per minute per API key **Workflow Engine**: - Concurrent executions: 8,000 workflows (across 16 pods)",
              "score": null
            },
            {
              "doc_id": "user_guide",
              "chunk_id": "user_guide_fix_9",
              "content": "Requester: {{trigger.body.requester}} Description: {{trigger.body.description}} Approve: https://app.company.com/approve/{{steps.create_approval_request.rows[0].id}} Reject: https://app.company.com/reject/{{steps.create_approval_request.rows[0].id}} - id: wait_for_approval action: wait_for_webhook config: timeout: 86400 # 24 hours webhook_path: \"/approval/{{steps.create_approval_request.rows[0].id}}\" - id: process_approval action: javascript code: | if (input.status === 'approved') { return { approved: true }; } else { throw new Error('Expense rejected'); } on_error: - id: notify_rejection action: email config: to: \"{{trigger.body.requester}}\" subject: \"Expense Rejected\" body: \"Your expense request has been rejected.\" - id: process_payment action: http_request config: method: POST url: \"https://api.accounting.com/payments\" body: amount: \"{{trigger.body.amount}}\" recipient: \"{{trigger.body.requester}}\" ``` ### Pattern 4: Data Synchronization Keep two systems in sync bidirectionally: ```yaml name: \"Sync Customers: CRM to Database\" trigger: type: event source: \"salesforce\" event: \"customer.updated\" steps: - id: fetch_customer action: http_request config: url: \"https://api.salesforce.com/customers/{{trigger.customer_id}}\" headers: Authorization: \"Bearer {{secrets.SALESFORCE_TOKEN}}\" - id: check_existing action: database_query config: query: \"SELECT id, last_updated FROM customers WHERE salesforce_id = $1\" parameters: - \"{{trigger.customer_id}}\" - id: update_or_insert action: database_query config: query: | INSERT INTO customers (salesforce_id, name, email, phone, last_updated) VALUES ($1, $2, $3, $4, NOW()) ON CONFLICT (salesforce_id) DO UPDATE SET name = $2, email = $3, phone = $4, last_updated = NOW() parameters: - \"{{trigger.customer_id}}\" - \"{{steps.fetch_customer.body.name}}\" - \"{{steps.fetch_customer.body.email}}\" - \"{{steps.fetch_customer.body.phone}}\" - id: log_sync action: database_query config: query: | INSERT INTO sync_log (source, target, record_id, synced_at) VALUES ('salesforce', 'postgresql', $1, NOW()) parameters: - \"{{trigger.customer_id}}\" ``` ### Pattern 5: Error Aggregation and Alerting Aggregate errors and send smart alerts: ```yaml name: \"Application Error Monitor\" schedule: cron: \"*/5 * * * *\" timezone: \"UTC\" steps: - id: fetch_recent_errors action: database_query config: query: | SELECT error_type, COUNT(*) as count, MAX(created_at) as last_seen FROM error_logs WHERE created_at > NOW() - INTERVAL '5 minutes' AND alerted = false GROUP BY error_type HAVING COUNT(*) > 5 - id: format_alert action: javascript condition: \"{{length(steps.fetch_recent_errors.rows) > 0}}\" code: | const errors = input.rows; let message = \"\u26a0\ufe0f Error Alert\\n\\n\"; for (const error of errors) { message += `\u2022 ${error.error_type}: ${error.count} occurrences (last: ${error.last_seen})\\n`; } return { message, total_errors: errors.reduce((sum, e) => sum + e.count, 0) }; - id: send_alert action: slack_message condition: \"{{steps.format_alert.output.total_errors > 10}}\" config: channel: \"#incidents\" text: \"{{steps.format_alert.output.message}}\" priority: \"high\" - id: mark_alerted action: database_query config: query: | UPDATE error_logs SET alerted = true WHERE created_at > NOW() - INTERVAL '5 minutes' ``` --- ## Getting Help Need assistance with CloudFlow?",
              "score": null
            }
          ],
          "latency_ms": 11.330174998875009,
          "query_id": "realistic_003",
          "dimension": "synonym",
          "query": "How long do access tokens last?"
        },
        {
          "key_facts": [
            "3600 seconds",
            "max 3600 seconds from iat",
            "All tokens expire after 3600 seconds"
          ],
          "found_facts": [
            "3600 seconds"
          ],
          "missed_facts": [
            "max 3600 seconds from iat",
            "All tokens expire after 3600 seconds"
          ],
          "coverage": 0.3333333333333333,
          "expected_docs": [
            "api_reference"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_4",
              "content": "### Error Response Format ```json { \"error\": { \"code\": \"invalid_parameter\", \"message\": \"The 'limit' parameter must be between 1 and 100\", \"field\": \"limit\", \"request_id\": \"req_8k3m9x2p\" } } ``` ### HTTP Status Codes | Status Code | Description | Common Causes | |------------|-------------|---------------| | 400 | Bad Request | Invalid parameters, malformed JSON, validation errors | | 401 | Unauthorized | Missing or invalid authentication credentials | | 403 | Forbidden | Insufficient permissions for requested resource | | 404 | Not Found | Resource does not exist or has been deleted | | 429 | Too Many Requests | Rate limit exceeded, retry after specified period | | 500 | Internal Server Error | Unexpected server error, contact support if persists | | 502 | Bad Gateway | Temporary service issue, retry with exponential backoff | | 503 | Service Unavailable | Scheduled maintenance or temporary outage | ### Error Codes CloudFlow returns specific error codes to help you identify and resolve issues: - `invalid_parameter`: One or more request parameters are invalid - `missing_required_field`: Required field is missing from request body - `authentication_failed`: Invalid API key or token - `insufficient_permissions`: User lacks required scope or permission - `resource_not_found`: Requested resource does not exist - `rate_limit_exceeded`: Too many requests, see rate limiting section - `workflow_execution_failed`: Workflow execution encountered an error - `invalid_json`: Request body contains malformed JSON - `duplicate_resource`: Resource with same identifier already exists - `quota_exceeded`: Account quota limit reached ### Error Handling Best Practices **Python Example:** ```python import requests import time def make_api_request(url, headers, max_retries=3): for attempt in range(max_retries): try: response = requests.get(url, headers=headers) if response.status_code == 200: return response.json() elif response.status_code == 429: # Rate limit exceeded retry_after = int(response.headers.get('X-RateLimit-Reset', 60)) print(f\"Rate limited. Waiting {retry_after} seconds...\") time.sleep(retry_after) continue elif response.status_code >= 500: # Server error - retry with exponential backoff wait_time = 2 ** attempt print(f\"Server error. Retrying in {wait_time} seconds...\") time.sleep(wait_time) continue else: # Client error - don't retry error_data = response.json() raise Exception(f\"API Error: {error_data['error']['message']}\") except requests.exceptions.RequestException as e: print(f\"Request failed: {e}\") if attempt < max_retries - 1: time.sleep(2 ** attempt) else: raise raise Exception(\"Max retries exceeded\") ``` ## Webhooks CloudFlow can send webhook notifications when specific events occur in your workflows.",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_0",
              "content": "# CloudFlow Platform Troubleshooting Guide **Version:** 3.2.1 **Last Updated:** January 2026 **Audience:** Platform Engineers, DevOps, Support Teams ## Table of Contents 1. [Overview](#overview) 2. [Authentication & Authorization Issues](#authentication--authorization-issues) 3. [Performance Problems](#performance-problems) 4. [Database Connection Issues](#database-connection-issues) 5. [Workflow Execution Failures](#workflow-execution-failures) 6. [Rate Limiting & Throttling](#rate-limiting--throttling) 7. [Log Analysis & Debugging](#log-analysis--debugging) 8. [Escalation Procedures](#escalation-procedures) --- ## Overview This guide provides comprehensive troubleshooting steps for common CloudFlow platform issues encountered in production environments. Each section includes error symptoms, root cause analysis, resolution steps, and preventive measures. ### Quick Diagnostic Checklist Before diving into specific issues, perform these initial checks: - Verify service health: `cloudflow status --all` - Check API connectivity: `curl -I https://api.cloudflow.io/health` - Review recent deployments: `kubectl get deployments -n cloudflow --sort-by=.metadata.creationTimestamp` - Inspect platform metrics: `cloudflow metrics --last 1h` --- ## Authentication & Authorization Issues ### 401 Unauthorized Errors #### Symptoms - API requests return `401 Unauthorized` - Error message: `Authentication credentials were not provided or are invalid` - Frontend displays \"Session expired\" message #### Common Causes **1. Token Expiration** CloudFlow access tokens expire after 3600 seconds (1 hour) by default. Refresh tokens are valid for 30 days. **Verification:** ```bash # Decode JWT to check expiration echo $CF_ACCESS_TOKEN | cut -d'.' -f2 | base64 -d | jq '.exp' # Compare with current time date +%s ``` **Resolution:** ```bash # Refresh the access token curl -X POST https://api.cloudflow.io/auth/refresh \\ -H \"Content-Type: application/json\" \\ -d '{\"refresh_token\": \"'$CF_REFRESH_TOKEN'\"}' # Update environment variable export CF_ACCESS_TOKEN=\"<new_token>\" ``` **2. Invalid JWT Signature** Error: `JWT signature verification failed` **Causes:** - Token was modified or corrupted - Using wrong signing key - Token generated with different secret **Resolution:** ```bash # Validate token structure cloudflow auth validate --token $CF_ACCESS_TOKEN # Generate new token cloudflow auth login --username <user> --password <pass> # For service accounts cloudflow auth service-account --create --name <sa-name> --scopes \"workflow:read,workflow:write\" ``` **3. Clock Skew Issues** JWT validation fails when system clocks are out of sync (tolerance: \u00b1300 seconds).",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_8",
              "content": "### Cache Invalidation Strategies **Time-Based Expiration (TTL)**: - Short-lived data: 5-15 minutes (session tokens, rate limit counters) - Medium-lived data: 1 hour (workflow definitions, templates) - Long-lived data: 24 hours (static configuration) **Event-Based Invalidation**: ``` Database Update Event \u2192 Kafka (cache.invalidation topic) \u2502 \u25bc All service instances consume event \u2502 \u25bc Redis DEL for affected keys ``` **Pattern-Based Invalidation**: - Use Redis SCAN + DEL for wildcard patterns - Example: Invalidate all user caches: `user:*:{user_id}` **Cache Warming**: - Scheduled job runs every hour to pre-populate frequently accessed data - Targets: Top 1000 workflows, system configuration, active user profiles - Reduces cache miss rate during traffic spikes ### Multi-Level Caching Application implements L1 (in-memory) and L2 (Redis) caching: ``` Request \u2192 L1 Cache (Node.js Map, 1000 entry LRU) \u2502 MISS \u2502 \u25bc L2 Cache (Redis, distributed) \u2502 MISS \u2502 \u25bc Database (PostgreSQL) ``` L1 cache reduces Redis network calls by 40% for hot data. --- ## Security Architecture ### Network Security **Zero-Trust Network Model**: - All service-to-service communication encrypted with mTLS - Certificate rotation: Every 90 days (automated via cert-manager) - Certificate authority: Internal PKI with HashiCorp Vault - Network segmentation: Private subnets for services, public subnet for ALB only **Service Mesh (Istio)**: ``` Service A \u2192 Envoy Sidecar (mTLS client cert) \u2502 Encrypted channel \u2502 Envoy Sidecar (mTLS server cert) \u2192 Service B ``` **Firewall Rules**: - Security groups: Deny all by default, explicit allow rules - Ingress: Only ALB can reach API Gateway (port 8080) - Egress: Services can only reach specific dependencies - Database access: Limited to application subnets only ### Authentication & Authorization **JWT Token Validation**: - Algorithm: RS256 (asymmetric signing) - Key rotation: Every 30 days with 7-day overlap period - Public key distribution: JWKS endpoint cached in Redis - Validation: Signature, expiry, issuer, audience claims - Token revocation: Blacklist in Redis for compromised tokens **Permission Model**: ``` User \u2192 Roles \u2192 Permissions \u2198 \u2197 Tenants (Multi-tenancy isolation) ``` Example permissions: - `workflow:read` - View workflows - `workflow:write` - Create/update workflows - `workflow:execute` - Trigger workflow execution - `workflow:delete` - Delete workflows - `admin:*` - All administrative operations **API Key Management**: - Format: `cfk_live_<32-char-random>` (production), `cfk_test_<32-char-random>` (sandbox) - Hashing: SHA-256 before storage in PostgreSQL - Scoping: API keys can be scoped to specific workflows or operations - Rate limits: Configurable",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_6",
              "content": "**Session Storage**: - Key pattern: `session:{user_id}` - TTL: 15 minutes (aligned with JWT expiry) - Data: User session state, preferences - Invalidation: On logout or password change 2. **Rate Limiting Counters**: - Key pattern: `ratelimit:{api_key}:{window}` - TTL: 60 seconds (sliding window) - Data: Request count per window - Algorithm: Token bucket with Redis INCR 3. **Compiled Templates**: - Key pattern: `template:{template_id}:{version}` - TTL: 1 hour - Data: Compiled Handlebars template - Invalidation: On template update 4. **Workflow Definitions**: - Key pattern: `workflow:def:{workflow_id}` - TTL: 1 hour - Data: Parsed workflow JSON - Invalidation: On workflow update or manual flush 5. **User Profiles**: - Key pattern: `user:profile:{user_id}` - TTL: 30 minutes - Data: User metadata (name, email, roles) - Invalidation: On profile update **Cache Hit Rates** (Production Metrics): - Session lookups: 98.5% - Workflow definitions: 94.2% - Template cache: 99.1% - User profiles: 91.8% **Performance Characteristics**: - GET operation P99: < 2ms - SET operation P99: < 3ms - Throughput: 100,000 operations per second - Network latency: < 1ms (same AZ) --- ### Kafka Event Streaming **Cluster Configuration**: - 5 broker nodes (distributed across 3 AZs) - Instance type: kafka.m5.2xlarge (8 vCPU, 32GB RAM) - Storage: 10TB per broker (gp3 SSD) - ZooKeeper ensemble: 3 nodes for cluster coordination - Replication factor: 3 (min in-sync replicas: 2) **Topic Architecture**: ``` workflow.events (32 partitions): - Workflow lifecycle events (created, started, completed, failed) - Retention: 7 days - Message rate: 5,000/sec peak - Consumer groups: workflow-engine, analytics-pipeline notifications.email (16 partitions): - Email notification triggers - Retention: 3 days - Message rate: 2,000/sec peak - Consumer groups: notification-service notifications.sms (8 partitions): - SMS notification triggers - Retention: 3 days - Message rate: 500/sec peak - Consumer groups: notification-service audit.logs (24 partitions): - Security and compliance audit events - Retention: 90 days (compliance requirement) - Message rate: 3,000/sec peak - Consumer groups: audit-processor, security-monitor dead-letter-queue (8 partitions): - Failed message processing - Retention: 30 days - Manual intervention required - Consumer groups: ops-team-alerts ``` **Producer Configuration**: - Acknowledgment: `acks=all` (wait for all in-sync replicas) - Compression: LZ4 (reduces network bandwidth by 60%) - Batching: 100ms linger time, 100KB batch size - Idempotence: Enabled to prevent duplicates **Consumer Configuration**: - Auto-commit: Disabled (manual commit after processing) - Offset reset: Earliest (replay from beginning on new consumer group) - Max poll",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_7",
              "content": "records: 500 - Session timeout: 30 seconds --- ## Message Queue Patterns ### Pub/Sub Pattern Used for broadcasting events to multiple interested consumers: ``` Workflow Engine publishes workflow.execution.completed \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u25bc \u25bc \u25bc \u25bc Analytics Notification Audit Logger Billing Service Service Service Service ``` Each service maintains its own consumer group and processes events independently. Failures in one consumer don't affect others. ### Request-Reply Pattern Synchronous communication over async messaging (used sparingly): ``` API Gateway publishes request \u2192 Kafka (reply-to: temp-queue-123) \u2502 \u25bc Service processes request \u2502 \u25bc Service publishes response \u2192 temp-queue-123 \u2502 \u25bc API Gateway receives response ``` Timeout: 5 seconds, fallback to direct HTTP call if no response. ### Saga Pattern Distributed transaction management for multi-service workflows: ``` Step 1: Create Order \u2192 SUCCESS \u2192 Step 2: Reserve Inventory \u2502 FAILURE \u2502 \u25bc Compensating Transaction \u2502 \u25bc Cancel Order (Rollback) ``` Orchestrated by Workflow Engine with compensation logic defined in workflow DSL. ### Dead Letter Queue (DLQ) Failed messages after all retry attempts are routed to DLQ: ``` Message processing fails (3 retries with exponential backoff) \u2502 \u25bc Publish to dead-letter-queue topic \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u25bc \u25bc Alert to PagerDuty Store in PostgreSQL for analysis \u2502 \u2502 \u25bc \u25bc Manual investigation Automated pattern detection ``` DLQ Processing SLA: < 4 hours for critical events, < 24 hours for non-critical. --- ## Caching Strategy ### Cache-Aside Pattern (Lazy Loading) Primary caching pattern used across all services: ``` 1. Application checks cache (Redis GET) 2. If HIT \u2192 Return cached data 3. If MISS: a. Query database (PostgreSQL) b. Store result in cache with TTL c. Return data to application ``` **Implementation Example** (Workflow Definition): ```javascript async function getWorkflowDefinition(workflowId) { const cacheKey = `workflow:def:${workflowId}`; // Step 1: Check cache let workflow = await redis.get(cacheKey); if (workflow) { metrics.increment('cache.hit.workflow_def'); return JSON.parse(workflow); } // Step 2: Cache miss - query database metrics.increment('cache.miss.workflow_def'); workflow = await db.query( 'SELECT * FROM workflows WHERE id = $1', [workflowId] ); // Step 3: Store in cache (1 hour TTL) await redis.setex(cacheKey, 3600, JSON.stringify(workflow)); return workflow; } ``` ### Write-Through Pattern Used for critical data where cache consistency is paramount: ``` 1. Application writes to database 2. Database transaction commits 3. Application updates cache 4. Return success to client ``` Applied to: User profiles, authentication sessions, system configuration.",
              "score": null
            }
          ],
          "latency_ms": 11.455339001258835,
          "query_id": "realistic_003",
          "dimension": "problem",
          "query": "My authentication keeps expiring after an hour"
        },
        {
          "key_facts": [
            "3600 seconds",
            "max 3600 seconds from iat",
            "All tokens expire after 3600 seconds"
          ],
          "found_facts": [
            "3600 seconds"
          ],
          "missed_facts": [
            "max 3600 seconds from iat",
            "All tokens expire after 3600 seconds"
          ],
          "coverage": 0.3333333333333333,
          "expected_docs": [
            "api_reference"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_0",
              "content": "# CloudFlow Platform Troubleshooting Guide **Version:** 3.2.1 **Last Updated:** January 2026 **Audience:** Platform Engineers, DevOps, Support Teams ## Table of Contents 1. [Overview](#overview) 2. [Authentication & Authorization Issues](#authentication--authorization-issues) 3. [Performance Problems](#performance-problems) 4. [Database Connection Issues](#database-connection-issues) 5. [Workflow Execution Failures](#workflow-execution-failures) 6. [Rate Limiting & Throttling](#rate-limiting--throttling) 7. [Log Analysis & Debugging](#log-analysis--debugging) 8. [Escalation Procedures](#escalation-procedures) --- ## Overview This guide provides comprehensive troubleshooting steps for common CloudFlow platform issues encountered in production environments. Each section includes error symptoms, root cause analysis, resolution steps, and preventive measures. ### Quick Diagnostic Checklist Before diving into specific issues, perform these initial checks: - Verify service health: `cloudflow status --all` - Check API connectivity: `curl -I https://api.cloudflow.io/health` - Review recent deployments: `kubectl get deployments -n cloudflow --sort-by=.metadata.creationTimestamp` - Inspect platform metrics: `cloudflow metrics --last 1h` --- ## Authentication & Authorization Issues ### 401 Unauthorized Errors #### Symptoms - API requests return `401 Unauthorized` - Error message: `Authentication credentials were not provided or are invalid` - Frontend displays \"Session expired\" message #### Common Causes **1. Token Expiration** CloudFlow access tokens expire after 3600 seconds (1 hour) by default. Refresh tokens are valid for 30 days. **Verification:** ```bash # Decode JWT to check expiration echo $CF_ACCESS_TOKEN | cut -d'.' -f2 | base64 -d | jq '.exp' # Compare with current time date +%s ``` **Resolution:** ```bash # Refresh the access token curl -X POST https://api.cloudflow.io/auth/refresh \\ -H \"Content-Type: application/json\" \\ -d '{\"refresh_token\": \"'$CF_REFRESH_TOKEN'\"}' # Update environment variable export CF_ACCESS_TOKEN=\"<new_token>\" ``` **2. Invalid JWT Signature** Error: `JWT signature verification failed` **Causes:** - Token was modified or corrupted - Using wrong signing key - Token generated with different secret **Resolution:** ```bash # Validate token structure cloudflow auth validate --token $CF_ACCESS_TOKEN # Generate new token cloudflow auth login --username <user> --password <pass> # For service accounts cloudflow auth service-account --create --name <sa-name> --scopes \"workflow:read,workflow:write\" ``` **3. Clock Skew Issues** JWT validation fails when system clocks are out of sync (tolerance: \u00b1300 seconds).",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_8",
              "content": "### Cache Invalidation Strategies **Time-Based Expiration (TTL)**: - Short-lived data: 5-15 minutes (session tokens, rate limit counters) - Medium-lived data: 1 hour (workflow definitions, templates) - Long-lived data: 24 hours (static configuration) **Event-Based Invalidation**: ``` Database Update Event \u2192 Kafka (cache.invalidation topic) \u2502 \u25bc All service instances consume event \u2502 \u25bc Redis DEL for affected keys ``` **Pattern-Based Invalidation**: - Use Redis SCAN + DEL for wildcard patterns - Example: Invalidate all user caches: `user:*:{user_id}` **Cache Warming**: - Scheduled job runs every hour to pre-populate frequently accessed data - Targets: Top 1000 workflows, system configuration, active user profiles - Reduces cache miss rate during traffic spikes ### Multi-Level Caching Application implements L1 (in-memory) and L2 (Redis) caching: ``` Request \u2192 L1 Cache (Node.js Map, 1000 entry LRU) \u2502 MISS \u2502 \u25bc L2 Cache (Redis, distributed) \u2502 MISS \u2502 \u25bc Database (PostgreSQL) ``` L1 cache reduces Redis network calls by 40% for hot data. --- ## Security Architecture ### Network Security **Zero-Trust Network Model**: - All service-to-service communication encrypted with mTLS - Certificate rotation: Every 90 days (automated via cert-manager) - Certificate authority: Internal PKI with HashiCorp Vault - Network segmentation: Private subnets for services, public subnet for ALB only **Service Mesh (Istio)**: ``` Service A \u2192 Envoy Sidecar (mTLS client cert) \u2502 Encrypted channel \u2502 Envoy Sidecar (mTLS server cert) \u2192 Service B ``` **Firewall Rules**: - Security groups: Deny all by default, explicit allow rules - Ingress: Only ALB can reach API Gateway (port 8080) - Egress: Services can only reach specific dependencies - Database access: Limited to application subnets only ### Authentication & Authorization **JWT Token Validation**: - Algorithm: RS256 (asymmetric signing) - Key rotation: Every 30 days with 7-day overlap period - Public key distribution: JWKS endpoint cached in Redis - Validation: Signature, expiry, issuer, audience claims - Token revocation: Blacklist in Redis for compromised tokens **Permission Model**: ``` User \u2192 Roles \u2192 Permissions \u2198 \u2197 Tenants (Multi-tenancy isolation) ``` Example permissions: - `workflow:read` - View workflows - `workflow:write` - Create/update workflows - `workflow:execute` - Trigger workflow execution - `workflow:delete` - Delete workflows - `admin:*` - All administrative operations **API Key Management**: - Format: `cfk_live_<32-char-random>` (production), `cfk_test_<32-char-random>` (sandbox) - Hashing: SHA-256 before storage in PostgreSQL - Scoping: API keys can be scoped to specific workflows or operations - Rate limits: Configurable",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_6",
              "content": "**Session Storage**: - Key pattern: `session:{user_id}` - TTL: 15 minutes (aligned with JWT expiry) - Data: User session state, preferences - Invalidation: On logout or password change 2. **Rate Limiting Counters**: - Key pattern: `ratelimit:{api_key}:{window}` - TTL: 60 seconds (sliding window) - Data: Request count per window - Algorithm: Token bucket with Redis INCR 3. **Compiled Templates**: - Key pattern: `template:{template_id}:{version}` - TTL: 1 hour - Data: Compiled Handlebars template - Invalidation: On template update 4. **Workflow Definitions**: - Key pattern: `workflow:def:{workflow_id}` - TTL: 1 hour - Data: Parsed workflow JSON - Invalidation: On workflow update or manual flush 5. **User Profiles**: - Key pattern: `user:profile:{user_id}` - TTL: 30 minutes - Data: User metadata (name, email, roles) - Invalidation: On profile update **Cache Hit Rates** (Production Metrics): - Session lookups: 98.5% - Workflow definitions: 94.2% - Template cache: 99.1% - User profiles: 91.8% **Performance Characteristics**: - GET operation P99: < 2ms - SET operation P99: < 3ms - Throughput: 100,000 operations per second - Network latency: < 1ms (same AZ) --- ### Kafka Event Streaming **Cluster Configuration**: - 5 broker nodes (distributed across 3 AZs) - Instance type: kafka.m5.2xlarge (8 vCPU, 32GB RAM) - Storage: 10TB per broker (gp3 SSD) - ZooKeeper ensemble: 3 nodes for cluster coordination - Replication factor: 3 (min in-sync replicas: 2) **Topic Architecture**: ``` workflow.events (32 partitions): - Workflow lifecycle events (created, started, completed, failed) - Retention: 7 days - Message rate: 5,000/sec peak - Consumer groups: workflow-engine, analytics-pipeline notifications.email (16 partitions): - Email notification triggers - Retention: 3 days - Message rate: 2,000/sec peak - Consumer groups: notification-service notifications.sms (8 partitions): - SMS notification triggers - Retention: 3 days - Message rate: 500/sec peak - Consumer groups: notification-service audit.logs (24 partitions): - Security and compliance audit events - Retention: 90 days (compliance requirement) - Message rate: 3,000/sec peak - Consumer groups: audit-processor, security-monitor dead-letter-queue (8 partitions): - Failed message processing - Retention: 30 days - Manual intervention required - Consumer groups: ops-team-alerts ``` **Producer Configuration**: - Acknowledgment: `acks=all` (wait for all in-sync replicas) - Compression: LZ4 (reduces network bandwidth by 60%) - Batching: 100ms linger time, 100KB batch size - Idempotence: Enabled to prevent duplicates **Consumer Configuration**: - Auto-commit: Disabled (manual commit after processing) - Offset reset: Earliest (replay from beginning on new consumer group) - Max poll",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_1",
              "content": "**Technology**: Node.js with Express.js framework **Replicas**: 12 pods (production), auto-scaling 8-20 based on CPU **Resource Allocation**: 2 vCPU, 4GB RAM per pod **Key Responsibilities**: - JWT token validation (delegated to Auth Service for initial validation) - Rate limiting: 1000 requests per minute per API key (sliding window) - Request/response transformation and validation using JSON Schema - Routing to downstream services based on URL path patterns - CORS handling for web clients - Request/response logging and correlation ID injection **Critical Endpoints**: - `POST /api/v1/workflows` - Create new workflow - `GET /api/v1/workflows/:id` - Retrieve workflow status - `POST /api/v1/workflows/:id/execute` - Trigger workflow execution - `GET /api/v1/workflows/:id/history` - Get execution history **Dependencies**: - Auth Service (for token validation) - Redis (for rate limiting counters) - All downstream microservices **Performance Targets**: - P50 latency: < 50ms - P99 latency: < 200ms - Throughput: 10,000 RPS sustained --- ### Auth Service **Purpose**: Centralized authentication and authorization service handling user identity, token generation, and permission validation. **Technology**: Go with gRPC for internal communication, REST for external **Replicas**: 8 pods (production), auto-scaling 6-12 **Resource Allocation**: 1 vCPU, 2GB RAM per pod **Key Responsibilities**: - User authentication via multiple providers (OAuth2, SAML, local credentials) - JWT token generation and validation (RS256 algorithm) - Role-based access control (RBAC) with fine-grained permissions - Session management with Redis-backed storage - API key generation and validation for service accounts - MFA enforcement for administrative operations **Authentication Flow**: ``` Client Request \u2192 API Gateway \u2192 Auth Service \u2502 \u251c\u2500 Validate credentials \u251c\u2500 Check MFA if required \u251c\u2500 Generate JWT (15min expiry) \u251c\u2500 Generate refresh token (7 days) \u2514\u2500 Store session in Redis ``` **Token Structure**: - Access Token: JWT with 15-minute expiry - Refresh Token: Opaque token with 7-day expiry, stored in PostgreSQL - Claims: user_id, email, roles[], permissions[], tenant_id **Security Features**: - Password hashing: Argon2id with 64MB memory, 4 iterations - Token rotation on refresh to prevent replay attacks - Brute force protection: 5 failed attempts \u2192 15-minute lockout - Secrets stored in HashiCorp Vault, rotated every 90 days **Performance Targets**: - Token validation: < 10ms (P99) - Token generation: < 50ms (P99) - Throughput: 5,000 RPS for validation operations --- ### Workflow Engine **Purpose**: Core orchestration service that executes workflow definitions, manages state transitions, and coordinates task execution across distributed systems.",
              "score": null
            },
            {
              "doc_id": "user_guide",
              "chunk_id": "user_guide_fix_5",
              "content": "Save and activate **Testing Schedules:** Use the built-in schedule calculator to preview upcoming executions: ``` Next 5 executions: 1. 2026-01-24 14:00:00 UTC 2. 2026-01-25 14:00:00 UTC 3. 2026-01-26 14:00:00 UTC 4. 2026-01-27 14:00:00 UTC 5. 2026-01-28 14:00:00 UTC ``` ## Error Handling Robust error handling ensures your workflows are resilient and reliable. ### Retry Policies Configure automatic retries for failed actions: ```yaml - id: api_call action: http_request config: url: \"https://api.example.com/data\" retry: max_attempts: 3 backoff_type: \"exponential\" # or \"fixed\", \"linear\" initial_interval: 1000 # milliseconds max_interval: 30000 multiplier: 2.0 retry_on: - timeout - network_error - status: [500, 502, 503, 504] ``` **Backoff Strategies:** - **Fixed**: Wait the same amount of time between retries - Attempt 1: Wait 1s - Attempt 2: Wait 1s - Attempt 3: Wait 1s - **Linear**: Increase wait time by a fixed amount - Attempt 1: Wait 1s - Attempt 2: Wait 2s - Attempt 3: Wait 3s - **Exponential**: Double the wait time with each retry (recommended) - Attempt 1: Wait 1s - Attempt 2: Wait 2s - Attempt 3: Wait 4s **Retry Conditions:** Control which errors trigger retries: - `timeout`: Request timeout - `network_error`: Connection failures - `status`: Specific HTTP status codes - `error_code`: Application-specific error codes ### Fallback Actions Execute alternative actions when the primary action fails: ```yaml - id: primary_payment action: http_request config: url: \"https://primary-payment-gateway.com/charge\" method: POST body: amount: \"{{amount}}\" on_error: - id: fallback_payment action: http_request config: url: \"https://backup-payment-gateway.com/charge\" method: POST body: amount: \"{{amount}}\" - id: notify_admin action: email config: to: \"admin@company.com\" subject: \"Payment Gateway Failure\" body: \"Primary gateway failed, switched to backup\" ``` **Error Object:** Access error details in fallback actions: ``` {{error.message}} # Error message {{error.code}} # Error code {{error.step_id}} # Failed step ID {{error.timestamp}} # When the error occurred {{error.attempts}} # Number of retry attempts ``` ### Dead Letter Queue When all retries and fallbacks fail, CloudFlow can route failed executions to a Dead Letter Queue (DLQ) for manual review: **Enable DLQ:** ```yaml workflow: error_handling: dead_letter_queue: enabled: true retain_days: 30 ``` **DLQ Features:** - View failed executions in the dashboard - Inspect complete execution context and error details - Retry individual executions after fixing issues - Export failed executions for analysis - Set up alerts for DLQ threshold breaches **Accessing the DLQ:** 1. Navigate to **Workflows** > **[Your Workflow]** > **Dead Letter Queue** 2.",
              "score": null
            }
          ],
          "latency_ms": 11.325486997520784,
          "query_id": "realistic_003",
          "dimension": "casual",
          "query": "token expiry time"
        },
        {
          "key_facts": [
            "3600 seconds",
            "max 3600 seconds from iat",
            "All tokens expire after 3600 seconds"
          ],
          "found_facts": [
            "3600 seconds"
          ],
          "missed_facts": [
            "max 3600 seconds from iat",
            "All tokens expire after 3600 seconds"
          ],
          "coverage": 0.3333333333333333,
          "expected_docs": [
            "api_reference"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_0",
              "content": "# CloudFlow Platform Troubleshooting Guide **Version:** 3.2.1 **Last Updated:** January 2026 **Audience:** Platform Engineers, DevOps, Support Teams ## Table of Contents 1. [Overview](#overview) 2. [Authentication & Authorization Issues](#authentication--authorization-issues) 3. [Performance Problems](#performance-problems) 4. [Database Connection Issues](#database-connection-issues) 5. [Workflow Execution Failures](#workflow-execution-failures) 6. [Rate Limiting & Throttling](#rate-limiting--throttling) 7. [Log Analysis & Debugging](#log-analysis--debugging) 8. [Escalation Procedures](#escalation-procedures) --- ## Overview This guide provides comprehensive troubleshooting steps for common CloudFlow platform issues encountered in production environments. Each section includes error symptoms, root cause analysis, resolution steps, and preventive measures. ### Quick Diagnostic Checklist Before diving into specific issues, perform these initial checks: - Verify service health: `cloudflow status --all` - Check API connectivity: `curl -I https://api.cloudflow.io/health` - Review recent deployments: `kubectl get deployments -n cloudflow --sort-by=.metadata.creationTimestamp` - Inspect platform metrics: `cloudflow metrics --last 1h` --- ## Authentication & Authorization Issues ### 401 Unauthorized Errors #### Symptoms - API requests return `401 Unauthorized` - Error message: `Authentication credentials were not provided or are invalid` - Frontend displays \"Session expired\" message #### Common Causes **1. Token Expiration** CloudFlow access tokens expire after 3600 seconds (1 hour) by default. Refresh tokens are valid for 30 days. **Verification:** ```bash # Decode JWT to check expiration echo $CF_ACCESS_TOKEN | cut -d'.' -f2 | base64 -d | jq '.exp' # Compare with current time date +%s ``` **Resolution:** ```bash # Refresh the access token curl -X POST https://api.cloudflow.io/auth/refresh \\ -H \"Content-Type: application/json\" \\ -d '{\"refresh_token\": \"'$CF_REFRESH_TOKEN'\"}' # Update environment variable export CF_ACCESS_TOKEN=\"<new_token>\" ``` **2. Invalid JWT Signature** Error: `JWT signature verification failed` **Causes:** - Token was modified or corrupted - Using wrong signing key - Token generated with different secret **Resolution:** ```bash # Validate token structure cloudflow auth validate --token $CF_ACCESS_TOKEN # Generate new token cloudflow auth login --username <user> --password <pass> # For service accounts cloudflow auth service-account --create --name <sa-name> --scopes \"workflow:read,workflow:write\" ``` **3. Clock Skew Issues** JWT validation fails when system clocks are out of sync (tolerance: \u00b1300 seconds).",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_8",
              "content": "### Cache Invalidation Strategies **Time-Based Expiration (TTL)**: - Short-lived data: 5-15 minutes (session tokens, rate limit counters) - Medium-lived data: 1 hour (workflow definitions, templates) - Long-lived data: 24 hours (static configuration) **Event-Based Invalidation**: ``` Database Update Event \u2192 Kafka (cache.invalidation topic) \u2502 \u25bc All service instances consume event \u2502 \u25bc Redis DEL for affected keys ``` **Pattern-Based Invalidation**: - Use Redis SCAN + DEL for wildcard patterns - Example: Invalidate all user caches: `user:*:{user_id}` **Cache Warming**: - Scheduled job runs every hour to pre-populate frequently accessed data - Targets: Top 1000 workflows, system configuration, active user profiles - Reduces cache miss rate during traffic spikes ### Multi-Level Caching Application implements L1 (in-memory) and L2 (Redis) caching: ``` Request \u2192 L1 Cache (Node.js Map, 1000 entry LRU) \u2502 MISS \u2502 \u25bc L2 Cache (Redis, distributed) \u2502 MISS \u2502 \u25bc Database (PostgreSQL) ``` L1 cache reduces Redis network calls by 40% for hot data. --- ## Security Architecture ### Network Security **Zero-Trust Network Model**: - All service-to-service communication encrypted with mTLS - Certificate rotation: Every 90 days (automated via cert-manager) - Certificate authority: Internal PKI with HashiCorp Vault - Network segmentation: Private subnets for services, public subnet for ALB only **Service Mesh (Istio)**: ``` Service A \u2192 Envoy Sidecar (mTLS client cert) \u2502 Encrypted channel \u2502 Envoy Sidecar (mTLS server cert) \u2192 Service B ``` **Firewall Rules**: - Security groups: Deny all by default, explicit allow rules - Ingress: Only ALB can reach API Gateway (port 8080) - Egress: Services can only reach specific dependencies - Database access: Limited to application subnets only ### Authentication & Authorization **JWT Token Validation**: - Algorithm: RS256 (asymmetric signing) - Key rotation: Every 30 days with 7-day overlap period - Public key distribution: JWKS endpoint cached in Redis - Validation: Signature, expiry, issuer, audience claims - Token revocation: Blacklist in Redis for compromised tokens **Permission Model**: ``` User \u2192 Roles \u2192 Permissions \u2198 \u2197 Tenants (Multi-tenancy isolation) ``` Example permissions: - `workflow:read` - View workflows - `workflow:write` - Create/update workflows - `workflow:execute` - Trigger workflow execution - `workflow:delete` - Delete workflows - `admin:*` - All administrative operations **API Key Management**: - Format: `cfk_live_<32-char-random>` (production), `cfk_test_<32-char-random>` (sandbox) - Hashing: SHA-256 before storage in PostgreSQL - Scoping: API keys can be scoped to specific workflows or operations - Rate limits: Configurable",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_1",
              "content": "**Technology**: Node.js with Express.js framework **Replicas**: 12 pods (production), auto-scaling 8-20 based on CPU **Resource Allocation**: 2 vCPU, 4GB RAM per pod **Key Responsibilities**: - JWT token validation (delegated to Auth Service for initial validation) - Rate limiting: 1000 requests per minute per API key (sliding window) - Request/response transformation and validation using JSON Schema - Routing to downstream services based on URL path patterns - CORS handling for web clients - Request/response logging and correlation ID injection **Critical Endpoints**: - `POST /api/v1/workflows` - Create new workflow - `GET /api/v1/workflows/:id` - Retrieve workflow status - `POST /api/v1/workflows/:id/execute` - Trigger workflow execution - `GET /api/v1/workflows/:id/history` - Get execution history **Dependencies**: - Auth Service (for token validation) - Redis (for rate limiting counters) - All downstream microservices **Performance Targets**: - P50 latency: < 50ms - P99 latency: < 200ms - Throughput: 10,000 RPS sustained --- ### Auth Service **Purpose**: Centralized authentication and authorization service handling user identity, token generation, and permission validation. **Technology**: Go with gRPC for internal communication, REST for external **Replicas**: 8 pods (production), auto-scaling 6-12 **Resource Allocation**: 1 vCPU, 2GB RAM per pod **Key Responsibilities**: - User authentication via multiple providers (OAuth2, SAML, local credentials) - JWT token generation and validation (RS256 algorithm) - Role-based access control (RBAC) with fine-grained permissions - Session management with Redis-backed storage - API key generation and validation for service accounts - MFA enforcement for administrative operations **Authentication Flow**: ``` Client Request \u2192 API Gateway \u2192 Auth Service \u2502 \u251c\u2500 Validate credentials \u251c\u2500 Check MFA if required \u251c\u2500 Generate JWT (15min expiry) \u251c\u2500 Generate refresh token (7 days) \u2514\u2500 Store session in Redis ``` **Token Structure**: - Access Token: JWT with 15-minute expiry - Refresh Token: Opaque token with 7-day expiry, stored in PostgreSQL - Claims: user_id, email, roles[], permissions[], tenant_id **Security Features**: - Password hashing: Argon2id with 64MB memory, 4 iterations - Token rotation on refresh to prevent replay attacks - Brute force protection: 5 failed attempts \u2192 15-minute lockout - Secrets stored in HashiCorp Vault, rotated every 90 days **Performance Targets**: - Token validation: < 10ms (P99) - Token generation: < 50ms (P99) - Throughput: 5,000 RPS for validation operations --- ### Workflow Engine **Purpose**: Core orchestration service that executes workflow definitions, manages state transitions, and coordinates task execution across distributed systems.",
              "score": null
            },
            {
              "doc_id": "user_guide",
              "chunk_id": "user_guide_fix_9",
              "content": "Requester: {{trigger.body.requester}} Description: {{trigger.body.description}} Approve: https://app.company.com/approve/{{steps.create_approval_request.rows[0].id}} Reject: https://app.company.com/reject/{{steps.create_approval_request.rows[0].id}} - id: wait_for_approval action: wait_for_webhook config: timeout: 86400 # 24 hours webhook_path: \"/approval/{{steps.create_approval_request.rows[0].id}}\" - id: process_approval action: javascript code: | if (input.status === 'approved') { return { approved: true }; } else { throw new Error('Expense rejected'); } on_error: - id: notify_rejection action: email config: to: \"{{trigger.body.requester}}\" subject: \"Expense Rejected\" body: \"Your expense request has been rejected.\" - id: process_payment action: http_request config: method: POST url: \"https://api.accounting.com/payments\" body: amount: \"{{trigger.body.amount}}\" recipient: \"{{trigger.body.requester}}\" ``` ### Pattern 4: Data Synchronization Keep two systems in sync bidirectionally: ```yaml name: \"Sync Customers: CRM to Database\" trigger: type: event source: \"salesforce\" event: \"customer.updated\" steps: - id: fetch_customer action: http_request config: url: \"https://api.salesforce.com/customers/{{trigger.customer_id}}\" headers: Authorization: \"Bearer {{secrets.SALESFORCE_TOKEN}}\" - id: check_existing action: database_query config: query: \"SELECT id, last_updated FROM customers WHERE salesforce_id = $1\" parameters: - \"{{trigger.customer_id}}\" - id: update_or_insert action: database_query config: query: | INSERT INTO customers (salesforce_id, name, email, phone, last_updated) VALUES ($1, $2, $3, $4, NOW()) ON CONFLICT (salesforce_id) DO UPDATE SET name = $2, email = $3, phone = $4, last_updated = NOW() parameters: - \"{{trigger.customer_id}}\" - \"{{steps.fetch_customer.body.name}}\" - \"{{steps.fetch_customer.body.email}}\" - \"{{steps.fetch_customer.body.phone}}\" - id: log_sync action: database_query config: query: | INSERT INTO sync_log (source, target, record_id, synced_at) VALUES ('salesforce', 'postgresql', $1, NOW()) parameters: - \"{{trigger.customer_id}}\" ``` ### Pattern 5: Error Aggregation and Alerting Aggregate errors and send smart alerts: ```yaml name: \"Application Error Monitor\" schedule: cron: \"*/5 * * * *\" timezone: \"UTC\" steps: - id: fetch_recent_errors action: database_query config: query: | SELECT error_type, COUNT(*) as count, MAX(created_at) as last_seen FROM error_logs WHERE created_at > NOW() - INTERVAL '5 minutes' AND alerted = false GROUP BY error_type HAVING COUNT(*) > 5 - id: format_alert action: javascript condition: \"{{length(steps.fetch_recent_errors.rows) > 0}}\" code: | const errors = input.rows; let message = \"\u26a0\ufe0f Error Alert\\n\\n\"; for (const error of errors) { message += `\u2022 ${error.error_type}: ${error.count} occurrences (last: ${error.last_seen})\\n`; } return { message, total_errors: errors.reduce((sum, e) => sum + e.count, 0) }; - id: send_alert action: slack_message condition: \"{{steps.format_alert.output.total_errors > 10}}\" config: channel: \"#incidents\" text: \"{{steps.format_alert.output.message}}\" priority: \"high\" - id: mark_alerted action: database_query config: query: | UPDATE error_logs SET alerted = true WHERE created_at > NOW() - INTERVAL '5 minutes' ``` --- ## Getting Help Need assistance with CloudFlow?",
              "score": null
            },
            {
              "doc_id": "user_guide",
              "chunk_id": "user_guide_fix_5",
              "content": "Save and activate **Testing Schedules:** Use the built-in schedule calculator to preview upcoming executions: ``` Next 5 executions: 1. 2026-01-24 14:00:00 UTC 2. 2026-01-25 14:00:00 UTC 3. 2026-01-26 14:00:00 UTC 4. 2026-01-27 14:00:00 UTC 5. 2026-01-28 14:00:00 UTC ``` ## Error Handling Robust error handling ensures your workflows are resilient and reliable. ### Retry Policies Configure automatic retries for failed actions: ```yaml - id: api_call action: http_request config: url: \"https://api.example.com/data\" retry: max_attempts: 3 backoff_type: \"exponential\" # or \"fixed\", \"linear\" initial_interval: 1000 # milliseconds max_interval: 30000 multiplier: 2.0 retry_on: - timeout - network_error - status: [500, 502, 503, 504] ``` **Backoff Strategies:** - **Fixed**: Wait the same amount of time between retries - Attempt 1: Wait 1s - Attempt 2: Wait 1s - Attempt 3: Wait 1s - **Linear**: Increase wait time by a fixed amount - Attempt 1: Wait 1s - Attempt 2: Wait 2s - Attempt 3: Wait 3s - **Exponential**: Double the wait time with each retry (recommended) - Attempt 1: Wait 1s - Attempt 2: Wait 2s - Attempt 3: Wait 4s **Retry Conditions:** Control which errors trigger retries: - `timeout`: Request timeout - `network_error`: Connection failures - `status`: Specific HTTP status codes - `error_code`: Application-specific error codes ### Fallback Actions Execute alternative actions when the primary action fails: ```yaml - id: primary_payment action: http_request config: url: \"https://primary-payment-gateway.com/charge\" method: POST body: amount: \"{{amount}}\" on_error: - id: fallback_payment action: http_request config: url: \"https://backup-payment-gateway.com/charge\" method: POST body: amount: \"{{amount}}\" - id: notify_admin action: email config: to: \"admin@company.com\" subject: \"Payment Gateway Failure\" body: \"Primary gateway failed, switched to backup\" ``` **Error Object:** Access error details in fallback actions: ``` {{error.message}} # Error message {{error.code}} # Error code {{error.step_id}} # Failed step ID {{error.timestamp}} # When the error occurred {{error.attempts}} # Number of retry attempts ``` ### Dead Letter Queue When all retries and fallbacks fail, CloudFlow can route failed executions to a Dead Letter Queue (DLQ) for manual review: **Enable DLQ:** ```yaml workflow: error_handling: dead_letter_queue: enabled: true retain_days: 30 ``` **DLQ Features:** - View failed executions in the dashboard - Inspect complete execution context and error details - Retry individual executions after fixing issues - Export failed executions for analysis - Set up alerts for DLQ threshold breaches **Accessing the DLQ:** 1. Navigate to **Workflows** > **[Your Workflow]** > **Dead Letter Queue** 2.",
              "score": null
            }
          ],
          "latency_ms": 11.613533999479841,
          "query_id": "realistic_003",
          "dimension": "contextual",
          "query": "I need to implement token refresh logic, when do JWT tokens expire?"
        },
        {
          "key_facts": [
            "3600 seconds",
            "max 3600 seconds from iat",
            "All tokens expire after 3600 seconds"
          ],
          "found_facts": [
            "3600 seconds"
          ],
          "missed_facts": [
            "max 3600 seconds from iat",
            "All tokens expire after 3600 seconds"
          ],
          "coverage": 0.3333333333333333,
          "expected_docs": [
            "api_reference"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_0",
              "content": "# CloudFlow Platform Troubleshooting Guide **Version:** 3.2.1 **Last Updated:** January 2026 **Audience:** Platform Engineers, DevOps, Support Teams ## Table of Contents 1. [Overview](#overview) 2. [Authentication & Authorization Issues](#authentication--authorization-issues) 3. [Performance Problems](#performance-problems) 4. [Database Connection Issues](#database-connection-issues) 5. [Workflow Execution Failures](#workflow-execution-failures) 6. [Rate Limiting & Throttling](#rate-limiting--throttling) 7. [Log Analysis & Debugging](#log-analysis--debugging) 8. [Escalation Procedures](#escalation-procedures) --- ## Overview This guide provides comprehensive troubleshooting steps for common CloudFlow platform issues encountered in production environments. Each section includes error symptoms, root cause analysis, resolution steps, and preventive measures. ### Quick Diagnostic Checklist Before diving into specific issues, perform these initial checks: - Verify service health: `cloudflow status --all` - Check API connectivity: `curl -I https://api.cloudflow.io/health` - Review recent deployments: `kubectl get deployments -n cloudflow --sort-by=.metadata.creationTimestamp` - Inspect platform metrics: `cloudflow metrics --last 1h` --- ## Authentication & Authorization Issues ### 401 Unauthorized Errors #### Symptoms - API requests return `401 Unauthorized` - Error message: `Authentication credentials were not provided or are invalid` - Frontend displays \"Session expired\" message #### Common Causes **1. Token Expiration** CloudFlow access tokens expire after 3600 seconds (1 hour) by default. Refresh tokens are valid for 30 days. **Verification:** ```bash # Decode JWT to check expiration echo $CF_ACCESS_TOKEN | cut -d'.' -f2 | base64 -d | jq '.exp' # Compare with current time date +%s ``` **Resolution:** ```bash # Refresh the access token curl -X POST https://api.cloudflow.io/auth/refresh \\ -H \"Content-Type: application/json\" \\ -d '{\"refresh_token\": \"'$CF_REFRESH_TOKEN'\"}' # Update environment variable export CF_ACCESS_TOKEN=\"<new_token>\" ``` **2. Invalid JWT Signature** Error: `JWT signature verification failed` **Causes:** - Token was modified or corrupted - Using wrong signing key - Token generated with different secret **Resolution:** ```bash # Validate token structure cloudflow auth validate --token $CF_ACCESS_TOKEN # Generate new token cloudflow auth login --username <user> --password <pass> # For service accounts cloudflow auth service-account --create --name <sa-name> --scopes \"workflow:read,workflow:write\" ``` **3. Clock Skew Issues** JWT validation fails when system clocks are out of sync (tolerance: \u00b1300 seconds).",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_1",
              "content": "**Diagnosis:** ```bash # Check system time timedatectl status # Compare with NTP server ntpdate -q pool.ntp.org # Check JWT issued time vs current time jwt_iat=$(echo $CF_ACCESS_TOKEN | cut -d'.' -f2 | base64 -d | jq '.iat') current_time=$(date +%s) skew=$((current_time - jwt_iat)) echo \"Clock skew: $skew seconds\" ``` **Resolution:** ```bash # Sync with NTP server sudo ntpdate -s pool.ntp.org # Enable automatic time sync sudo timedatectl set-ntp true # Restart CloudFlow client cloudflow restart ``` **4. Insufficient Permissions** Error: `User does not have required permissions for this operation` **Check user roles:** ```bash cloudflow auth whoami --verbose # Expected output: # User: john.doe@company.com # Roles: developer, workflow-admin # Scopes: workflow:*, data:read, metrics:read ``` **Request permission elevation:** ```bash # Submit access request cloudflow auth request-access \\ --resource \"workflow:production:deploy\" \\ --justification \"Deploy critical hotfix for TICKET-1234\" # Check pending approvals cloudflow auth list-requests --status pending ``` ### 403 Forbidden Errors #### RBAC Policy Violations CloudFlow uses role-based access control (RBAC) with the following hierarchy: - `viewer` - Read-only access - `developer` - Create and modify workflows (non-production) - `operator` - Execute workflows, view logs - `admin` - Full access to workspace - `platform-admin` - Cross-workspace administration **Verify resource permissions:** ```bash # Check effective permissions cloudflow rbac check \\ --user john.doe@company.com \\ --resource workflow:prod-pipeline \\ --action execute # List all policies affecting user cloudflow rbac policies --user john.doe@company.com --verbose ``` --- ## Performance Problems ### Slow Query Performance #### Symptoms - API response times > 5000ms - Database query latency warnings in logs - CloudFlow UI becomes unresponsive - Timeout errors: `Request exceeded maximum duration of 30000ms` #### Diagnosis Steps **1. Identify Slow Queries** ```bash # Query CloudFlow metrics cloudflow metrics query --metric api_request_duration_ms \\ --filter \"p95 > 5000\" \\ --last 1h # Check database slow query log kubectl logs -n cloudflow deploy/cloudflow-db-primary | \\ grep \"slow query\" | \\ tail -n 50 # Analyze query patterns cloudflow db analyze-queries --min-duration 5000 --limit 20 ``` **2.",
              "score": null
            },
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_4",
              "content": "### Error Response Format ```json { \"error\": { \"code\": \"invalid_parameter\", \"message\": \"The 'limit' parameter must be between 1 and 100\", \"field\": \"limit\", \"request_id\": \"req_8k3m9x2p\" } } ``` ### HTTP Status Codes | Status Code | Description | Common Causes | |------------|-------------|---------------| | 400 | Bad Request | Invalid parameters, malformed JSON, validation errors | | 401 | Unauthorized | Missing or invalid authentication credentials | | 403 | Forbidden | Insufficient permissions for requested resource | | 404 | Not Found | Resource does not exist or has been deleted | | 429 | Too Many Requests | Rate limit exceeded, retry after specified period | | 500 | Internal Server Error | Unexpected server error, contact support if persists | | 502 | Bad Gateway | Temporary service issue, retry with exponential backoff | | 503 | Service Unavailable | Scheduled maintenance or temporary outage | ### Error Codes CloudFlow returns specific error codes to help you identify and resolve issues: - `invalid_parameter`: One or more request parameters are invalid - `missing_required_field`: Required field is missing from request body - `authentication_failed`: Invalid API key or token - `insufficient_permissions`: User lacks required scope or permission - `resource_not_found`: Requested resource does not exist - `rate_limit_exceeded`: Too many requests, see rate limiting section - `workflow_execution_failed`: Workflow execution encountered an error - `invalid_json`: Request body contains malformed JSON - `duplicate_resource`: Resource with same identifier already exists - `quota_exceeded`: Account quota limit reached ### Error Handling Best Practices **Python Example:** ```python import requests import time def make_api_request(url, headers, max_retries=3): for attempt in range(max_retries): try: response = requests.get(url, headers=headers) if response.status_code == 200: return response.json() elif response.status_code == 429: # Rate limit exceeded retry_after = int(response.headers.get('X-RateLimit-Reset', 60)) print(f\"Rate limited. Waiting {retry_after} seconds...\") time.sleep(retry_after) continue elif response.status_code >= 500: # Server error - retry with exponential backoff wait_time = 2 ** attempt print(f\"Server error. Retrying in {wait_time} seconds...\") time.sleep(wait_time) continue else: # Client error - don't retry error_data = response.json() raise Exception(f\"API Error: {error_data['error']['message']}\") except requests.exceptions.RequestException as e: print(f\"Request failed: {e}\") if attempt < max_retries - 1: time.sleep(2 ** attempt) else: raise raise Exception(\"Max retries exceeded\") ``` ## Webhooks CloudFlow can send webhook notifications when specific events occur in your workflows.",
              "score": null
            },
            {
              "doc_id": "user_guide",
              "chunk_id": "user_guide_fix_6",
              "content": "Filter by error type, date range, or execution ID 3. Click an execution to view full details 4. Click **\"Retry\"** to reprocess with the same input data ### Error Notifications Get notified when workflows fail: ```yaml workflow: notifications: on_failure: - type: email to: \"ops-team@company.com\" - type: slack channel: \"#alerts\" message: \"Workflow {{workflow.name}} failed: {{error.message}}\" on_success_after_retry: - type: slack channel: \"#monitoring\" message: \"Workflow recovered after {{error.attempts}} attempts\" ``` ## Workflow Limits CloudFlow enforces the following limits to ensure platform stability and performance: ### Steps Per Workflow - **Maximum**: 50 steps per workflow - **Recommendation**: Keep workflows focused and modular. If you need more steps, consider splitting into multiple workflows connected via webhooks. ### Execution Timeout - **Default**: 3600 seconds (60 minutes) - **Behavior**: Workflows exceeding this timeout are automatically terminated - **Custom Timeouts**: Enterprise plans can request custom timeout limits **Setting Step-Level Timeouts:** ```yaml - id: long_running_task action: http_request config: url: \"https://api.example.com/process\" timeout: 300 # 5 minutes for this specific step ``` ### Execution Limits - **Maximum**: 1000 executions per day (per workflow) - **Rate Limiting**: 100 concurrent executions per workflow - **Burst Limit**: 10 executions per second **What happens when limits are reached:** - New executions are queued automatically - Webhook triggers return HTTP 429 (Too Many Requests) - Scheduled executions are skipped (logged in audit trail) - Email notifications sent to workflow owner **Monitoring Usage:** View real-time metrics in your workflow dashboard: - Executions today: 847 / 1000 - Average duration: 12.3s - Success rate: 98.2% - Current queue depth: 3 ### Data Limits - **Maximum request/response size**: 10MB per action - **Maximum execution payload**: 50MB total - **Variable value size**: 1MB per variable ### Enterprise Plan Limits Enterprise customers can request increased limits: - Up to 100 steps per workflow - Up to 10,000 executions per day - Up to 7200 second timeout (2 hours) - Priority execution queue - Dedicated capacity allocation Contact sales@cloudflow.io for Enterprise pricing and custom limits. ## Best Practices Follow these best practices to build reliable, maintainable workflows: ### 1. Use Descriptive Names **Good:** - Workflow: \"Sync Customer Data from Salesforce to Database\" - Step: \"validate_customer_email\" **Bad:** - Workflow: \"Workflow 1\" - Step: \"step3\" ### 2.",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_6",
              "content": "**Session Storage**: - Key pattern: `session:{user_id}` - TTL: 15 minutes (aligned with JWT expiry) - Data: User session state, preferences - Invalidation: On logout or password change 2. **Rate Limiting Counters**: - Key pattern: `ratelimit:{api_key}:{window}` - TTL: 60 seconds (sliding window) - Data: Request count per window - Algorithm: Token bucket with Redis INCR 3. **Compiled Templates**: - Key pattern: `template:{template_id}:{version}` - TTL: 1 hour - Data: Compiled Handlebars template - Invalidation: On template update 4. **Workflow Definitions**: - Key pattern: `workflow:def:{workflow_id}` - TTL: 1 hour - Data: Parsed workflow JSON - Invalidation: On workflow update or manual flush 5. **User Profiles**: - Key pattern: `user:profile:{user_id}` - TTL: 30 minutes - Data: User metadata (name, email, roles) - Invalidation: On profile update **Cache Hit Rates** (Production Metrics): - Session lookups: 98.5% - Workflow definitions: 94.2% - Template cache: 99.1% - User profiles: 91.8% **Performance Characteristics**: - GET operation P99: < 2ms - SET operation P99: < 3ms - Throughput: 100,000 operations per second - Network latency: < 1ms (same AZ) --- ### Kafka Event Streaming **Cluster Configuration**: - 5 broker nodes (distributed across 3 AZs) - Instance type: kafka.m5.2xlarge (8 vCPU, 32GB RAM) - Storage: 10TB per broker (gp3 SSD) - ZooKeeper ensemble: 3 nodes for cluster coordination - Replication factor: 3 (min in-sync replicas: 2) **Topic Architecture**: ``` workflow.events (32 partitions): - Workflow lifecycle events (created, started, completed, failed) - Retention: 7 days - Message rate: 5,000/sec peak - Consumer groups: workflow-engine, analytics-pipeline notifications.email (16 partitions): - Email notification triggers - Retention: 3 days - Message rate: 2,000/sec peak - Consumer groups: notification-service notifications.sms (8 partitions): - SMS notification triggers - Retention: 3 days - Message rate: 500/sec peak - Consumer groups: notification-service audit.logs (24 partitions): - Security and compliance audit events - Retention: 90 days (compliance requirement) - Message rate: 3,000/sec peak - Consumer groups: audit-processor, security-monitor dead-letter-queue (8 partitions): - Failed message processing - Retention: 30 days - Manual intervention required - Consumer groups: ops-team-alerts ``` **Producer Configuration**: - Acknowledgment: `acks=all` (wait for all in-sync replicas) - Compression: LZ4 (reduces network bandwidth by 60%) - Batching: 100ms linger time, 100KB batch size - Idempotence: Enabled to prevent duplicates **Consumer Configuration**: - Auto-commit: Disabled (manual commit after processing) - Offset reset: Earliest (replay from beginning on new consumer group) - Max poll",
              "score": null
            }
          ],
          "latency_ms": 11.488871001347434,
          "query_id": "realistic_003",
          "dimension": "negation",
          "query": "Why does my token stop working after 3600 seconds?"
        },
        {
          "key_facts": [
            "PostgreSQL 15.4",
            "Redis 7.2",
            "Apache Kafka 3.6"
          ],
          "found_facts": [
            "PostgreSQL 15.4",
            "Redis 7.2",
            "Apache Kafka 3.6"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "architecture_overview"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_0",
              "content": "# CloudFlow Platform - System Architecture Overview **Document Version:** 2.3.1 **Last Updated:** January 15, 2026 **Owner:** Platform Architecture Team **Status:** Production ## Executive Summary CloudFlow is a distributed, cloud-native workflow automation platform designed to orchestrate complex business processes at scale. The platform processes over 2.5 million workflow executions daily with an average P99 latency of 180ms for API operations and 4.2 seconds for workflow execution. This document provides a comprehensive overview of the system architecture, including microservices design, data flow patterns, infrastructure decisions, and operational considerations. ## Table of Contents 1. [High-Level Architecture](#high-level-architecture) 2. [Microservices Breakdown](#microservices-breakdown) 3. [Data Flow Architecture](#data-flow-architecture) 4. [Database Architecture](#database-architecture) 5. [Message Queue Patterns](#message-queue-patterns) 6. [Caching Strategy](#caching-strategy) 7. [Security Architecture](#security-architecture) 8. [Performance Characteristics](#performance-characteristics) 9. [Disaster Recovery](#disaster-recovery) --- ## High-Level Architecture CloudFlow follows a microservices architecture pattern deployed across multiple availability zones in AWS. The platform is designed with the following core principles: - **Scalability**: Horizontal scaling for all services with auto-scaling groups - **Resilience**: Circuit breakers, bulkheads, and graceful degradation - **Observability**: Distributed tracing, centralized logging, and comprehensive metrics - **Security**: Zero-trust network architecture with mTLS encryption ### Architecture Diagram (Conceptual) ``` \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 Load Balancer (ALB) \u2502 \u2502 (TLS Termination - 443) \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u25bc \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 API Gateway Layer \u2502 \u2502 (Rate Limiting, Auth, Request Routing) \u2502 \u2514\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u2502 \u2502 \u2502 \u2502 \u25bc \u25bc \u25bc \u25bc \u25bc \u250c\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502Auth\u2502 \u2502Workflow\u2502 \u2502Scheduler\u2502 \u2502Notification\u2502 \u2502User Service\u2502 \u2502Svc \u2502 \u2502 Engine \u2502 \u2502 Service \u2502 \u2502 Service \u2502 \u2502 \u2502 \u2514\u2500\u252c\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u2502 \u2502 \u2502 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 \u2502 \u2502 \u25bc \u25bc \u25bc \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502PostgreSQL\u2502 \u2502 Redis \u2502 \u2502 Kafka \u2502 \u2502 Cluster \u2502 \u2502 Cluster \u2502 \u2502 Cluster \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 ``` ### Technology Stack - **Runtime**: Node.js 20.x (API Gateway, Workflow Engine), Go 1.21 (Auth Service, Scheduler) - **Container Orchestration**: Kubernetes 1.28 (EKS) - **Service Mesh**: Istio 1.20 for service-to-service communication - **Databases**: PostgreSQL 15.4, Redis 7.2 - **Message Broker**: Apache Kafka 3.6 - **Monitoring**: Prometheus, Grafana, Jaeger for distributed tracing - **Secrets Management**: HashiCorp Vault 1.15 --- ## Microservices Breakdown ### API Gateway **Purpose**: Single entry point for all client requests, providing authentication, rate limiting, request routing, and protocol translation.",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_4",
              "content": "**Implement connection pooling optimization:** ```bash # Use PgBouncer for connection pooling kubectl apply -f cloudflow-pgbouncer.yaml # Configure CloudFlow to use PgBouncer cloudflow config set db.host pgbouncer.cloudflow.svc.cluster.local cloudflow config set db.port 6432 cloudflow config set db.pool.mode transaction ``` 2. **Add read replicas:** ```bash # Route read-only queries to replicas cloudflow db replicas add --count 2 cloudflow config set db.read_replicas \"replica-1:5432,replica-2:5432\" cloudflow config set db.read_write_split true ``` ### Connection Timeout Errors #### Error Messages - `connection timeout after 30000ms` - `could not connect to database server at 10.0.2.45:5432` - `database server unreachable` #### Troubleshooting Steps ```bash # Test network connectivity telnet cloudflow-db.internal.company.com 5432 # Check DNS resolution nslookup cloudflow-db.internal.company.com # Verify database is accepting connections pg_isready -h cloudflow-db.internal.company.com -p 5432 -U cloudflow # Check firewall rules sudo iptables -L -n | grep 5432 # Test from CloudFlow pod network kubectl run -n cloudflow debug-pod --rm -i --tty \\ --image=postgres:14 -- \\ psql -h cloudflow-db.internal.company.com -U cloudflow -d cloudflow_production # Review database logs for connection rejections kubectl logs -n cloudflow statefulset/cloudflow-db --tail=100 | \\ grep -i \"connection\\|reject\\|authentication\" ``` ### Maximum Connection Limit (100) Reached This is a hard limit in CloudFlow's database tier. #### Permanent Solutions **Option 1: Upgrade database tier** ```bash # Check available tiers cloudflow db tiers list # Upgrade to higher tier (supports 200 connections) cloudflow db upgrade --tier standard-plus --confirm # Monitor migration progress cloudflow db migration status ``` **Option 2: Implement aggressive connection reuse** ```bash # Reduce connection lifetime cloudflow config set db.pool.max_lifetime 600 # 10 minutes # Enable prepared statement caching cloudflow config set db.prepared_statements.cache true cloudflow config set db.prepared_statements.max_size 250 # Reduce idle connection timeout cloudflow config set db.pool.idle_timeout 120 # 2 minutes ``` --- ## Workflow Execution Failures ### Timeout Errors (3600 second limit) #### Error Message ``` WorkflowExecutionError: Workflow exceeded maximum execution time of 3600 seconds Status: TIMEOUT Workflow ID: wf_9k2n4m8p1q Execution ID: exec_7h3j6k9m2n ``` #### Analysis ```bash # Get workflow execution details cloudflow workflows executions get exec_7h3j6k9m2n --verbose # Check step-by-step breakdown cloudflow workflows executions steps exec_7h3j6k9m2n # Sample output: # Step 1: data_ingestion - Duration: 245s - Status: SUCCESS # Step 2: data_validation - Duration: 123s - Status: SUCCESS # Step 3: data_transformation - Duration: 3189s - Status: TIMEOUT # Step 4: data_export - Duration: 0s - Status: SKIPPED # Identify bottleneck step cloudflow workflows analyze",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_2",
              "content": "Review Query Execution Plans** ```sql -- Connect to CloudFlow database cloudflow db connect --readonly -- Explain slow query EXPLAIN ANALYZE SELECT w.*, e.status, e.error_message FROM workflows w LEFT JOIN executions e ON w.id = e.workflow_id WHERE w.workspace_id = 'ws_abc123' AND e.created_at > NOW() - INTERVAL '7 days' ORDER BY e.created_at DESC; -- Check for missing indexes SELECT schemaname, tablename, indexname, indexdef FROM pg_indexes WHERE tablename IN ('workflows', 'executions', 'workflow_steps') ORDER BY tablename, indexname; ``` **3. Optimize Queries** Common optimization techniques: **Add missing indexes:** ```sql -- Index for workflow lookup by workspace CREATE INDEX CONCURRENTLY idx_workflows_workspace_created ON workflows(workspace_id, created_at DESC); -- Index for execution status queries CREATE INDEX CONCURRENTLY idx_executions_status_created ON executions(workflow_id, status, created_at DESC) WHERE status IN ('running', 'pending'); -- Composite index for common filter combinations CREATE INDEX CONCURRENTLY idx_executions_workspace_status ON executions(workspace_id, status, created_at) INCLUDE (error_message, retry_count); ``` **Use query result caching:** ```bash # Enable query cache for workspace metadata cloudflow config set cache.workspace.ttl 3600 # Configure Redis cache backend cloudflow config set cache.backend redis cloudflow config set cache.redis.host redis.cloudflow.svc.cluster.local cloudflow config set cache.redis.port 6379 ``` ### High API Latency #### Latency Breakdown Analysis ```bash # Generate latency report cloudflow metrics latency-report --endpoint \"/api/v1/workflows\" --last 24h # Sample output: # Endpoint: POST /api/v1/workflows/execute # P50: 245ms | P95: 1823ms | P99: 4521ms # Breakdown: # - Auth: 45ms (18%) # - DB Query: 156ms (64%) # - Business Logic: 32ms (13%) # - Response Serialization: 12ms (5%) ``` **Network latency issues:** ```bash # Test connectivity to CloudFlow API time curl -w \"@curl-format.txt\" -o /dev/null -s https://api.cloudflow.io/health # Create curl-format.txt: cat > curl-format.txt << EOF time_namelookup: %{time_namelookup}s\\n time_connect: %{time_connect}s\\n time_appconnect: %{time_appconnect}s\\n time_pretransfer: %{time_pretransfer}s\\n time_redirect: %{time_redirect}s\\n time_starttransfer: %{time_starttransfer}s\\n ----------\\n time_total: %{time_total}s\\n EOF # Trace route to API endpoint traceroute api.cloudflow.io # Check DNS resolution time dig api.cloudflow.io | grep \"Query time\" ``` ### Memory Leaks #### Detection ```bash # Monitor CloudFlow service memory usage kubectl top pods -n cloudflow --sort-by=memory # Get detailed memory metrics for specific pod kubectl exec -n cloudflow deploy/cloudflow-api -- \\ curl localhost:9090/metrics | grep memory # Check for OOMKilled pods kubectl get pods -n cloudflow --field-selector=status.phase=Failed | \\ grep OOMKilled # Review memory limits and requests kubectl describe deployment cloudflow-api -n cloudflow | \\ grep -A 5 \"Limits\\|Requests\" ``` #### Common Causes **1.",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_10",
              "content": "database nc -zv cloudflow-db.internal.company.com 5432 # Trace route traceroute api.cloudflow.io # Capture packets tcpdump -i any -w /tmp/capture.pcap port 5432 ``` --- ## Escalation Procedures ### Severity Levels CloudFlow incidents are classified into four severity levels: #### SEV-1: Critical (P1) - **Definition:** Complete service outage or severe degradation affecting all users - **Examples:** - API returns 5xx errors for > 5 minutes - Database completely unavailable - Authentication system down - Data loss or corruption - **Response Time:** Immediate (< 15 minutes) - **Escalation:** Page on-call engineer immediately #### SEV-2: High (P2) - **Definition:** Major functionality impaired affecting multiple users - **Examples:** - Workflow execution success rate < 90% - Significant performance degradation (p95 latency > 10s) - Rate limiting affecting large customer segment - **Response Time:** < 1 hour - **Escalation:** Create incident ticket and notify on-call #### SEV-3: Medium (P3) - **Definition:** Partial functionality degraded affecting some users - **Examples:** - Intermittent failures for specific workflow types - Minor performance issues - Non-critical feature unavailable - **Response Time:** < 4 hours - **Escalation:** Create ticket, normal business hours support #### SEV-4: Low (P4) - **Definition:** Minor issues with minimal user impact - **Examples:** - Cosmetic issues - Documentation errors - Feature requests - **Response Time:** < 2 business days - **Escalation:** Standard support ticket ### Escalation Steps #### Step 1: Initial Assessment (0-5 minutes) ```bash # Run health check cloudflow health check --comprehensive # Check status page curl https://status.cloudflow.io/api/v1/status.json # Review recent changes cloudflow audit log --last 2h --event-type \"deployment,configuration\" # Check metrics dashboard cloudflow metrics dashboard --incident-mode ``` #### Step 2: Gather Information (5-15 minutes) Create incident document with: - Incident timestamp and duration - Affected services and endpoints - Error rates and user impact - Recent changes or deployments - Relevant log excerpts - Correlation IDs for failed requests ```bash # Generate incident report cloudflow debug incident-report \\ --start \"2026-01-24T10:30:00Z\" \\ --end \"2026-01-24T11:00:00Z\" \\ --output incident-report.md # Capture system snapshot cloudflow debug snapshot --output snapshot-$(date +%Y%m%d-%H%M%S).tar.gz ``` #### Step 3: Escalate Based on Severity **For SEV-1 (Critical):** ```bash # Page on-call engineer cloudflow incident create \\ --severity SEV-1 \\ --title \"Complete API outage\" \\ --description \"All API requests returning 503\" \\ --page-oncall # Notify status page cloudflow status update \\ --status \"major_outage\" \\ --message \"We are investigating a complete service",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_8",
              "content": "Leverage caching:** ```bash # Enable client-side caching export CLOUDFLOW_CACHE_ENABLED=true export CLOUDFLOW_CACHE_TTL=300 # Cache workflow metadata cloudflow workflows list --use-cache --cache-ttl 600 ``` --- ## Log Analysis & Debugging ### Accessing CloudFlow Logs #### Kubernetes Deployments ```bash # List all CloudFlow pods kubectl get pods -n cloudflow # Tail logs from API server kubectl logs -f -n cloudflow deployment/cloudflow-api --tail=100 # Get logs from specific pod kubectl logs -n cloudflow cloudflow-api-7d4f6b8c9d-x7k2m # Get logs from previous crashed pod kubectl logs -n cloudflow cloudflow-api-7d4f6b8c9d-x7k2m --previous # Get logs from all pods in deployment kubectl logs -n cloudflow deployment/cloudflow-api --all-containers=true # Stream logs from multiple pods kubectl logs -n cloudflow -l app=cloudflow-api -f --max-log-requests=10 ``` #### Log Levels CloudFlow supports the following log levels: - `TRACE` - Very detailed debugging information - `DEBUG` - Detailed debugging information - `INFO` - Informational messages (default) - `WARN` - Warning messages - `ERROR` - Error messages - `FATAL` - Fatal errors causing shutdown **Changing log levels:** ```bash # Set global log level cloudflow config set logging.level DEBUG # Set log level for specific component cloudflow config set logging.components.database DEBUG cloudflow config set logging.components.auth INFO cloudflow config set logging.components.workflows TRACE # Temporary log level increase (resets after 1 hour) cloudflow debug set-log-level DEBUG --duration 3600 # View current log configuration cloudflow config get logging --format json ``` ### Grep Patterns for Common Issues #### Authentication Failures ```bash # Find all authentication errors kubectl logs -n cloudflow deployment/cloudflow-api | \\ grep -i \"authentication\\|401\\|unauthorized\" | \\ tail -n 50 # Find JWT validation failures kubectl logs -n cloudflow deployment/cloudflow-api | \\ grep -E \"JWT|token.*invalid|signature.*failed\" # Find clock skew issues kubectl logs -n cloudflow deployment/cloudflow-api | \\ grep -i \"clock skew\\|time.*sync\\|nbf\\|exp\" ``` #### Database Errors ```bash # Find database connection errors kubectl logs -n cloudflow deployment/cloudflow-api | \\ grep -E \"connection.*pool|could not connect|database.*timeout\" # Find slow queries kubectl logs -n cloudflow deployment/cloudflow-api | \\ grep \"slow query\" | \\ awk '{print $NF}' | \\ sort -n | \\ tail -n 20 # Find deadlock errors kubectl logs -n cloudflow deployment/cloudflow-api | \\ grep -i \"deadlock detected\" ``` #### Workflow Execution Errors ```bash # Find workflow timeout errors kubectl logs -n cloudflow deployment/cloudflow-workflow-engine | \\ grep -E \"timeout|exceeded.*3600\" # Find workflow retry attempts kubectl logs -n cloudflow deployment/cloudflow-workflow-engine | \\ grep -E \"retry attempt [0-9]|retrying",
              "score": null
            }
          ],
          "latency_ms": 11.301601996819954,
          "query_id": "realistic_004",
          "dimension": "original",
          "query": "What database technology does CloudFlow use?"
        },
        {
          "key_facts": [
            "PostgreSQL 15.4",
            "Redis 7.2",
            "Apache Kafka 3.6"
          ],
          "found_facts": [
            "PostgreSQL 15.4",
            "Redis 7.2",
            "Apache Kafka 3.6"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "architecture_overview"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_0",
              "content": "# CloudFlow Platform - System Architecture Overview **Document Version:** 2.3.1 **Last Updated:** January 15, 2026 **Owner:** Platform Architecture Team **Status:** Production ## Executive Summary CloudFlow is a distributed, cloud-native workflow automation platform designed to orchestrate complex business processes at scale. The platform processes over 2.5 million workflow executions daily with an average P99 latency of 180ms for API operations and 4.2 seconds for workflow execution. This document provides a comprehensive overview of the system architecture, including microservices design, data flow patterns, infrastructure decisions, and operational considerations. ## Table of Contents 1. [High-Level Architecture](#high-level-architecture) 2. [Microservices Breakdown](#microservices-breakdown) 3. [Data Flow Architecture](#data-flow-architecture) 4. [Database Architecture](#database-architecture) 5. [Message Queue Patterns](#message-queue-patterns) 6. [Caching Strategy](#caching-strategy) 7. [Security Architecture](#security-architecture) 8. [Performance Characteristics](#performance-characteristics) 9. [Disaster Recovery](#disaster-recovery) --- ## High-Level Architecture CloudFlow follows a microservices architecture pattern deployed across multiple availability zones in AWS. The platform is designed with the following core principles: - **Scalability**: Horizontal scaling for all services with auto-scaling groups - **Resilience**: Circuit breakers, bulkheads, and graceful degradation - **Observability**: Distributed tracing, centralized logging, and comprehensive metrics - **Security**: Zero-trust network architecture with mTLS encryption ### Architecture Diagram (Conceptual) ``` \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 Load Balancer (ALB) \u2502 \u2502 (TLS Termination - 443) \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u25bc \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 API Gateway Layer \u2502 \u2502 (Rate Limiting, Auth, Request Routing) \u2502 \u2514\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u2502 \u2502 \u2502 \u2502 \u25bc \u25bc \u25bc \u25bc \u25bc \u250c\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502Auth\u2502 \u2502Workflow\u2502 \u2502Scheduler\u2502 \u2502Notification\u2502 \u2502User Service\u2502 \u2502Svc \u2502 \u2502 Engine \u2502 \u2502 Service \u2502 \u2502 Service \u2502 \u2502 \u2502 \u2514\u2500\u252c\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u2502 \u2502 \u2502 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 \u2502 \u2502 \u25bc \u25bc \u25bc \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502PostgreSQL\u2502 \u2502 Redis \u2502 \u2502 Kafka \u2502 \u2502 Cluster \u2502 \u2502 Cluster \u2502 \u2502 Cluster \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 ``` ### Technology Stack - **Runtime**: Node.js 20.x (API Gateway, Workflow Engine), Go 1.21 (Auth Service, Scheduler) - **Container Orchestration**: Kubernetes 1.28 (EKS) - **Service Mesh**: Istio 1.20 for service-to-service communication - **Databases**: PostgreSQL 15.4, Redis 7.2 - **Message Broker**: Apache Kafka 3.6 - **Monitoring**: Prometheus, Grafana, Jaeger for distributed tracing - **Secrets Management**: HashiCorp Vault 1.15 --- ## Microservices Breakdown ### API Gateway **Purpose**: Single entry point for all client requests, providing authentication, rate limiting, request routing, and protocol translation.",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_10",
              "content": "database nc -zv cloudflow-db.internal.company.com 5432 # Trace route traceroute api.cloudflow.io # Capture packets tcpdump -i any -w /tmp/capture.pcap port 5432 ``` --- ## Escalation Procedures ### Severity Levels CloudFlow incidents are classified into four severity levels: #### SEV-1: Critical (P1) - **Definition:** Complete service outage or severe degradation affecting all users - **Examples:** - API returns 5xx errors for > 5 minutes - Database completely unavailable - Authentication system down - Data loss or corruption - **Response Time:** Immediate (< 15 minutes) - **Escalation:** Page on-call engineer immediately #### SEV-2: High (P2) - **Definition:** Major functionality impaired affecting multiple users - **Examples:** - Workflow execution success rate < 90% - Significant performance degradation (p95 latency > 10s) - Rate limiting affecting large customer segment - **Response Time:** < 1 hour - **Escalation:** Create incident ticket and notify on-call #### SEV-3: Medium (P3) - **Definition:** Partial functionality degraded affecting some users - **Examples:** - Intermittent failures for specific workflow types - Minor performance issues - Non-critical feature unavailable - **Response Time:** < 4 hours - **Escalation:** Create ticket, normal business hours support #### SEV-4: Low (P4) - **Definition:** Minor issues with minimal user impact - **Examples:** - Cosmetic issues - Documentation errors - Feature requests - **Response Time:** < 2 business days - **Escalation:** Standard support ticket ### Escalation Steps #### Step 1: Initial Assessment (0-5 minutes) ```bash # Run health check cloudflow health check --comprehensive # Check status page curl https://status.cloudflow.io/api/v1/status.json # Review recent changes cloudflow audit log --last 2h --event-type \"deployment,configuration\" # Check metrics dashboard cloudflow metrics dashboard --incident-mode ``` #### Step 2: Gather Information (5-15 minutes) Create incident document with: - Incident timestamp and duration - Affected services and endpoints - Error rates and user impact - Recent changes or deployments - Relevant log excerpts - Correlation IDs for failed requests ```bash # Generate incident report cloudflow debug incident-report \\ --start \"2026-01-24T10:30:00Z\" \\ --end \"2026-01-24T11:00:00Z\" \\ --output incident-report.md # Capture system snapshot cloudflow debug snapshot --output snapshot-$(date +%Y%m%d-%H%M%S).tar.gz ``` #### Step 3: Escalate Based on Severity **For SEV-1 (Critical):** ```bash # Page on-call engineer cloudflow incident create \\ --severity SEV-1 \\ --title \"Complete API outage\" \\ --description \"All API requests returning 503\" \\ --page-oncall # Notify status page cloudflow status update \\ --status \"major_outage\" \\ --message \"We are investigating a complete service",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_0",
              "content": "# CloudFlow Platform Troubleshooting Guide **Version:** 3.2.1 **Last Updated:** January 2026 **Audience:** Platform Engineers, DevOps, Support Teams ## Table of Contents 1. [Overview](#overview) 2. [Authentication & Authorization Issues](#authentication--authorization-issues) 3. [Performance Problems](#performance-problems) 4. [Database Connection Issues](#database-connection-issues) 5. [Workflow Execution Failures](#workflow-execution-failures) 6. [Rate Limiting & Throttling](#rate-limiting--throttling) 7. [Log Analysis & Debugging](#log-analysis--debugging) 8. [Escalation Procedures](#escalation-procedures) --- ## Overview This guide provides comprehensive troubleshooting steps for common CloudFlow platform issues encountered in production environments. Each section includes error symptoms, root cause analysis, resolution steps, and preventive measures. ### Quick Diagnostic Checklist Before diving into specific issues, perform these initial checks: - Verify service health: `cloudflow status --all` - Check API connectivity: `curl -I https://api.cloudflow.io/health` - Review recent deployments: `kubectl get deployments -n cloudflow --sort-by=.metadata.creationTimestamp` - Inspect platform metrics: `cloudflow metrics --last 1h` --- ## Authentication & Authorization Issues ### 401 Unauthorized Errors #### Symptoms - API requests return `401 Unauthorized` - Error message: `Authentication credentials were not provided or are invalid` - Frontend displays \"Session expired\" message #### Common Causes **1. Token Expiration** CloudFlow access tokens expire after 3600 seconds (1 hour) by default. Refresh tokens are valid for 30 days. **Verification:** ```bash # Decode JWT to check expiration echo $CF_ACCESS_TOKEN | cut -d'.' -f2 | base64 -d | jq '.exp' # Compare with current time date +%s ``` **Resolution:** ```bash # Refresh the access token curl -X POST https://api.cloudflow.io/auth/refresh \\ -H \"Content-Type: application/json\" \\ -d '{\"refresh_token\": \"'$CF_REFRESH_TOKEN'\"}' # Update environment variable export CF_ACCESS_TOKEN=\"<new_token>\" ``` **2. Invalid JWT Signature** Error: `JWT signature verification failed` **Causes:** - Token was modified or corrupted - Using wrong signing key - Token generated with different secret **Resolution:** ```bash # Validate token structure cloudflow auth validate --token $CF_ACCESS_TOKEN # Generate new token cloudflow auth login --username <user> --password <pass> # For service accounts cloudflow auth service-account --create --name <sa-name> --scopes \"workflow:read,workflow:write\" ``` **3. Clock Skew Issues** JWT validation fails when system clocks are out of sync (tolerance: \u00b1300 seconds).",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_2",
              "content": "**Technology**: Node.js with TypeScript, Bull queue library **Replicas**: 16 pods (production), auto-scaling 12-24 **Resource Allocation**: 4 vCPU, 8GB RAM per pod **Key Responsibilities**: - Parse and validate workflow definitions (JSON-based DSL) - Execute workflow steps with state machine pattern - Handle retries with exponential backoff (max 3 retries, backoff: 2^n seconds) - Coordinate parallel and sequential task execution - Manage workflow state persistence and recovery - Support for conditional branching and loops - Sub-workflow invocation and composition **Workflow Execution Model**: ``` Workflow Submitted \u2192 Validation \u2192 Queue in Kafka \u2502 \u25bc Workflow Engine picks up \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u25bc \u25bc \u25bc [Task 1 Exec] [Task 2 Exec] [Task 3 Exec] \u2502 \u2502 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u25bc State Update \u2192 PostgreSQL \u2502 \u25bc Publish Event \u2192 Kafka ``` **Workflow DSL Example**: ```json { \"workflow_id\": \"customer-onboarding-v2\", \"version\": \"2.1.0\", \"steps\": [ { \"id\": \"validate-customer\", \"type\": \"http_request\", \"config\": { \"url\": \"https://validation.internal/verify\", \"method\": \"POST\", \"timeout\": 5000, \"retry\": {\"max_attempts\": 3, \"backoff\": \"exponential\"} } }, { \"id\": \"create-account\", \"type\": \"database_operation\", \"depends_on\": [\"validate-customer\"] } ] } ``` **State Management**: - Workflow states: PENDING, RUNNING, COMPLETED, FAILED, CANCELLED - Step states: QUEUED, EXECUTING, SUCCESS, FAILED, SKIPPED - State transitions stored in PostgreSQL with event sourcing pattern - Checkpointing every 10 steps for long-running workflows **Performance Targets**: - Simple workflow (3-5 steps): < 2 seconds (P99) - Complex workflow (20+ steps): < 5 seconds (P99) - Throughput: 500 concurrent workflow executions per pod --- ### Scheduler Service **Purpose**: Time-based workflow triggering system supporting cron-like schedules and one-time delayed executions.",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_4",
              "content": "**Implement connection pooling optimization:** ```bash # Use PgBouncer for connection pooling kubectl apply -f cloudflow-pgbouncer.yaml # Configure CloudFlow to use PgBouncer cloudflow config set db.host pgbouncer.cloudflow.svc.cluster.local cloudflow config set db.port 6432 cloudflow config set db.pool.mode transaction ``` 2. **Add read replicas:** ```bash # Route read-only queries to replicas cloudflow db replicas add --count 2 cloudflow config set db.read_replicas \"replica-1:5432,replica-2:5432\" cloudflow config set db.read_write_split true ``` ### Connection Timeout Errors #### Error Messages - `connection timeout after 30000ms` - `could not connect to database server at 10.0.2.45:5432` - `database server unreachable` #### Troubleshooting Steps ```bash # Test network connectivity telnet cloudflow-db.internal.company.com 5432 # Check DNS resolution nslookup cloudflow-db.internal.company.com # Verify database is accepting connections pg_isready -h cloudflow-db.internal.company.com -p 5432 -U cloudflow # Check firewall rules sudo iptables -L -n | grep 5432 # Test from CloudFlow pod network kubectl run -n cloudflow debug-pod --rm -i --tty \\ --image=postgres:14 -- \\ psql -h cloudflow-db.internal.company.com -U cloudflow -d cloudflow_production # Review database logs for connection rejections kubectl logs -n cloudflow statefulset/cloudflow-db --tail=100 | \\ grep -i \"connection\\|reject\\|authentication\" ``` ### Maximum Connection Limit (100) Reached This is a hard limit in CloudFlow's database tier. #### Permanent Solutions **Option 1: Upgrade database tier** ```bash # Check available tiers cloudflow db tiers list # Upgrade to higher tier (supports 200 connections) cloudflow db upgrade --tier standard-plus --confirm # Monitor migration progress cloudflow db migration status ``` **Option 2: Implement aggressive connection reuse** ```bash # Reduce connection lifetime cloudflow config set db.pool.max_lifetime 600 # 10 minutes # Enable prepared statement caching cloudflow config set db.prepared_statements.cache true cloudflow config set db.prepared_statements.max_size 250 # Reduce idle connection timeout cloudflow config set db.pool.idle_timeout 120 # 2 minutes ``` --- ## Workflow Execution Failures ### Timeout Errors (3600 second limit) #### Error Message ``` WorkflowExecutionError: Workflow exceeded maximum execution time of 3600 seconds Status: TIMEOUT Workflow ID: wf_9k2n4m8p1q Execution ID: exec_7h3j6k9m2n ``` #### Analysis ```bash # Get workflow execution details cloudflow workflows executions get exec_7h3j6k9m2n --verbose # Check step-by-step breakdown cloudflow workflows executions steps exec_7h3j6k9m2n # Sample output: # Step 1: data_ingestion - Duration: 245s - Status: SUCCESS # Step 2: data_validation - Duration: 123s - Status: SUCCESS # Step 3: data_transformation - Duration: 3189s - Status: TIMEOUT # Step 4: data_export - Duration: 0s - Status: SKIPPED # Identify bottleneck step cloudflow workflows analyze",
              "score": null
            }
          ],
          "latency_ms": 11.31661999897915,
          "query_id": "realistic_004",
          "dimension": "synonym",
          "query": "Which database system powers CloudFlow?"
        },
        {
          "key_facts": [
            "PostgreSQL 15.4",
            "Redis 7.2",
            "Apache Kafka 3.6"
          ],
          "found_facts": [
            "PostgreSQL 15.4",
            "Redis 7.2",
            "Apache Kafka 3.6"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "architecture_overview"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_0",
              "content": "# CloudFlow Platform - System Architecture Overview **Document Version:** 2.3.1 **Last Updated:** January 15, 2026 **Owner:** Platform Architecture Team **Status:** Production ## Executive Summary CloudFlow is a distributed, cloud-native workflow automation platform designed to orchestrate complex business processes at scale. The platform processes over 2.5 million workflow executions daily with an average P99 latency of 180ms for API operations and 4.2 seconds for workflow execution. This document provides a comprehensive overview of the system architecture, including microservices design, data flow patterns, infrastructure decisions, and operational considerations. ## Table of Contents 1. [High-Level Architecture](#high-level-architecture) 2. [Microservices Breakdown](#microservices-breakdown) 3. [Data Flow Architecture](#data-flow-architecture) 4. [Database Architecture](#database-architecture) 5. [Message Queue Patterns](#message-queue-patterns) 6. [Caching Strategy](#caching-strategy) 7. [Security Architecture](#security-architecture) 8. [Performance Characteristics](#performance-characteristics) 9. [Disaster Recovery](#disaster-recovery) --- ## High-Level Architecture CloudFlow follows a microservices architecture pattern deployed across multiple availability zones in AWS. The platform is designed with the following core principles: - **Scalability**: Horizontal scaling for all services with auto-scaling groups - **Resilience**: Circuit breakers, bulkheads, and graceful degradation - **Observability**: Distributed tracing, centralized logging, and comprehensive metrics - **Security**: Zero-trust network architecture with mTLS encryption ### Architecture Diagram (Conceptual) ``` \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 Load Balancer (ALB) \u2502 \u2502 (TLS Termination - 443) \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u25bc \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 API Gateway Layer \u2502 \u2502 (Rate Limiting, Auth, Request Routing) \u2502 \u2514\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u2502 \u2502 \u2502 \u2502 \u25bc \u25bc \u25bc \u25bc \u25bc \u250c\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502Auth\u2502 \u2502Workflow\u2502 \u2502Scheduler\u2502 \u2502Notification\u2502 \u2502User Service\u2502 \u2502Svc \u2502 \u2502 Engine \u2502 \u2502 Service \u2502 \u2502 Service \u2502 \u2502 \u2502 \u2514\u2500\u252c\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u2502 \u2502 \u2502 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 \u2502 \u2502 \u25bc \u25bc \u25bc \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502PostgreSQL\u2502 \u2502 Redis \u2502 \u2502 Kafka \u2502 \u2502 Cluster \u2502 \u2502 Cluster \u2502 \u2502 Cluster \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 ``` ### Technology Stack - **Runtime**: Node.js 20.x (API Gateway, Workflow Engine), Go 1.21 (Auth Service, Scheduler) - **Container Orchestration**: Kubernetes 1.28 (EKS) - **Service Mesh**: Istio 1.20 for service-to-service communication - **Databases**: PostgreSQL 15.4, Redis 7.2 - **Message Broker**: Apache Kafka 3.6 - **Monitoring**: Prometheus, Grafana, Jaeger for distributed tracing - **Secrets Management**: HashiCorp Vault 1.15 --- ## Microservices Breakdown ### API Gateway **Purpose**: Single entry point for all client requests, providing authentication, rate limiting, request routing, and protocol translation.",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_8",
              "content": "### Cache Invalidation Strategies **Time-Based Expiration (TTL)**: - Short-lived data: 5-15 minutes (session tokens, rate limit counters) - Medium-lived data: 1 hour (workflow definitions, templates) - Long-lived data: 24 hours (static configuration) **Event-Based Invalidation**: ``` Database Update Event \u2192 Kafka (cache.invalidation topic) \u2502 \u25bc All service instances consume event \u2502 \u25bc Redis DEL for affected keys ``` **Pattern-Based Invalidation**: - Use Redis SCAN + DEL for wildcard patterns - Example: Invalidate all user caches: `user:*:{user_id}` **Cache Warming**: - Scheduled job runs every hour to pre-populate frequently accessed data - Targets: Top 1000 workflows, system configuration, active user profiles - Reduces cache miss rate during traffic spikes ### Multi-Level Caching Application implements L1 (in-memory) and L2 (Redis) caching: ``` Request \u2192 L1 Cache (Node.js Map, 1000 entry LRU) \u2502 MISS \u2502 \u25bc L2 Cache (Redis, distributed) \u2502 MISS \u2502 \u25bc Database (PostgreSQL) ``` L1 cache reduces Redis network calls by 40% for hot data. --- ## Security Architecture ### Network Security **Zero-Trust Network Model**: - All service-to-service communication encrypted with mTLS - Certificate rotation: Every 90 days (automated via cert-manager) - Certificate authority: Internal PKI with HashiCorp Vault - Network segmentation: Private subnets for services, public subnet for ALB only **Service Mesh (Istio)**: ``` Service A \u2192 Envoy Sidecar (mTLS client cert) \u2502 Encrypted channel \u2502 Envoy Sidecar (mTLS server cert) \u2192 Service B ``` **Firewall Rules**: - Security groups: Deny all by default, explicit allow rules - Ingress: Only ALB can reach API Gateway (port 8080) - Egress: Services can only reach specific dependencies - Database access: Limited to application subnets only ### Authentication & Authorization **JWT Token Validation**: - Algorithm: RS256 (asymmetric signing) - Key rotation: Every 30 days with 7-day overlap period - Public key distribution: JWKS endpoint cached in Redis - Validation: Signature, expiry, issuer, audience claims - Token revocation: Blacklist in Redis for compromised tokens **Permission Model**: ``` User \u2192 Roles \u2192 Permissions \u2198 \u2197 Tenants (Multi-tenancy isolation) ``` Example permissions: - `workflow:read` - View workflows - `workflow:write` - Create/update workflows - `workflow:execute` - Trigger workflow execution - `workflow:delete` - Delete workflows - `admin:*` - All administrative operations **API Key Management**: - Format: `cfk_live_<32-char-random>` (production), `cfk_test_<32-char-random>` (sandbox) - Hashing: SHA-256 before storage in PostgreSQL - Scoping: API keys can be scoped to specific workflows or operations - Rate limits: Configurable",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_1",
              "content": "**Technology**: Node.js with Express.js framework **Replicas**: 12 pods (production), auto-scaling 8-20 based on CPU **Resource Allocation**: 2 vCPU, 4GB RAM per pod **Key Responsibilities**: - JWT token validation (delegated to Auth Service for initial validation) - Rate limiting: 1000 requests per minute per API key (sliding window) - Request/response transformation and validation using JSON Schema - Routing to downstream services based on URL path patterns - CORS handling for web clients - Request/response logging and correlation ID injection **Critical Endpoints**: - `POST /api/v1/workflows` - Create new workflow - `GET /api/v1/workflows/:id` - Retrieve workflow status - `POST /api/v1/workflows/:id/execute` - Trigger workflow execution - `GET /api/v1/workflows/:id/history` - Get execution history **Dependencies**: - Auth Service (for token validation) - Redis (for rate limiting counters) - All downstream microservices **Performance Targets**: - P50 latency: < 50ms - P99 latency: < 200ms - Throughput: 10,000 RPS sustained --- ### Auth Service **Purpose**: Centralized authentication and authorization service handling user identity, token generation, and permission validation. **Technology**: Go with gRPC for internal communication, REST for external **Replicas**: 8 pods (production), auto-scaling 6-12 **Resource Allocation**: 1 vCPU, 2GB RAM per pod **Key Responsibilities**: - User authentication via multiple providers (OAuth2, SAML, local credentials) - JWT token generation and validation (RS256 algorithm) - Role-based access control (RBAC) with fine-grained permissions - Session management with Redis-backed storage - API key generation and validation for service accounts - MFA enforcement for administrative operations **Authentication Flow**: ``` Client Request \u2192 API Gateway \u2192 Auth Service \u2502 \u251c\u2500 Validate credentials \u251c\u2500 Check MFA if required \u251c\u2500 Generate JWT (15min expiry) \u251c\u2500 Generate refresh token (7 days) \u2514\u2500 Store session in Redis ``` **Token Structure**: - Access Token: JWT with 15-minute expiry - Refresh Token: Opaque token with 7-day expiry, stored in PostgreSQL - Claims: user_id, email, roles[], permissions[], tenant_id **Security Features**: - Password hashing: Argon2id with 64MB memory, 4 iterations - Token rotation on refresh to prevent replay attacks - Brute force protection: 5 failed attempts \u2192 15-minute lockout - Secrets stored in HashiCorp Vault, rotated every 90 days **Performance Targets**: - Token validation: < 10ms (P99) - Token generation: < 50ms (P99) - Throughput: 5,000 RPS for validation operations --- ### Workflow Engine **Purpose**: Core orchestration service that executes workflow definitions, manages state transitions, and coordinates task execution across distributed systems.",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_5",
              "content": "Success/failure events published back to Kafka ``` **Event Schema**: ```json { \"event_id\": \"uuid-v4\", \"event_type\": \"workflow.execution.completed\", \"timestamp\": \"2026-01-15T10:30:00.000Z\", \"correlation_id\": \"request-trace-id\", \"payload\": { \"workflow_id\": \"wf-12345\", \"execution_id\": \"exec-67890\", \"status\": \"COMPLETED\", \"duration_ms\": 4230 }, \"metadata\": { \"source_service\": \"workflow-engine\", \"schema_version\": \"1.0\" } } ``` ### Inter-Service Communication Services communicate using two primary patterns: **Synchronous (gRPC)**: - Auth Service validation calls from API Gateway - User Service profile lookups - Low latency requirement (< 50ms) - Request-response pattern with circuit breaker **Asynchronous (Kafka)**: - Workflow execution events - Notification triggers - Audit log events - Eventual consistency acceptable --- ## Database Architecture ### PostgreSQL Primary Database **Cluster Configuration**: - Primary-replica setup with 1 primary + 2 read replicas - Instance type: db.r6g.2xlarge (8 vCPU, 64GB RAM) - Storage: 2TB gp3 SSD with 12,000 IOPS - Multi-AZ deployment for high availability - Automated backups: Daily snapshots, 30-day retention - Point-in-time recovery: 5-minute granularity **Database Schema Design**: ``` Core Tables: - users (5M rows): User accounts and profiles - workflows (2M rows): Workflow definitions - workflow_executions (500M rows, partitioned): Execution history - workflow_steps (2B rows, partitioned): Individual step records - schedules (100K rows): Scheduled workflow triggers - notifications (1B rows, partitioned): Notification delivery log Partitioning Strategy: - workflow_executions: Monthly partitions by created_at - workflow_steps: Monthly partitions by created_at - notifications: Weekly partitions by created_at - Automatic partition creation via cron job - Retention policy: Drop partitions older than 12 months ``` **Indexing Strategy**: - Primary keys: UUIDs with B-tree indexes - Frequently queried columns: Compound indexes (e.g., user_id + status + created_at) - JSONB columns: GIN indexes for workflow definitions - Full-text search: GiST indexes on description fields **Connection Pooling**: - PgBouncer in transaction mode - Pool size: 100 connections per service - Max client connections: 2000 - Connection timeout: 30 seconds **Performance Characteristics**: - Read query P99: < 20ms - Write query P99: < 50ms - Transaction throughput: 15,000 TPS - Replication lag: < 500ms --- ### Redis Caching Layer **Cluster Configuration**: - Redis Cluster with 6 nodes (3 primary + 3 replica) - Instance type: cache.r6g.xlarge (4 vCPU, 26GB RAM) - Total memory: 78GB usable cache space - Persistence: RDB snapshots every 5 minutes + AOF - Eviction policy: allkeys-lru **Cache Usage Patterns**: 1.",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_12",
              "content": "Resume normal operations - Recovery time: 1-3 hours depending on data volume - Data loss: None if corruption detected quickly ### Testing & Validation **DR Drill Schedule**: - Monthly: Automated failover test (single AZ failure simulation) - Quarterly: Full DR region failover (non-production hours) - Annually: Complete disaster simulation with all stakeholders **Last DR Test Results** (Dec 15, 2025): - Scenario: Full region failover - Actual RTO: 2 hours 23 minutes (target: 4 hours) - Actual RPO: 42 minutes (target: 1 hour) - Issues identified: DNS propagation slower than expected (resolved) - Success criteria: Met all recovery objectives ### Business Continuity **Communication Plan**: - Status page: status.cloudflow.com (updated every 15 minutes during incident) - Customer notifications: Email + SMS for all P1 incidents - Internal escalation: PagerDuty \u2192 Incident Commander \u2192 Engineering Manager \u2192 CTO **Data Retention Policy**: - Active data: 12 months in hot storage (PostgreSQL) - Archived data: 7 years in cold storage (S3 Glacier) - Audit logs: 7 years (compliance requirement) - Backup retention: 30 days standard, 90 days monthly snapshots --- ## Appendix ### Service Dependency Matrix | Service | Depends On | Critical Path | |---------|-----------|---------------| | API Gateway | Auth Service, Redis | Yes | | Auth Service | PostgreSQL, Redis, Vault | Yes | | Workflow Engine | PostgreSQL, Kafka, Redis | Yes | | Scheduler | PostgreSQL, Redis, Kafka | No | | Notification Service | Kafka, PostgreSQL, SendGrid, Twilio | No | ### Capacity Planning **Current Capacity** (Jan 2026): - Daily workflow executions: 2.5M - Active users: 150,000 - API requests per day: 50M - Database size: 1.2TB - Kafka throughput: 100K msg/sec peak **6-Month Projection** (Jul 2026): - Daily workflow executions: 4M (+60%) - Active users: 225,000 (+50%) - API requests per day: 80M (+60%) - Database size: 2TB (+67%) - Required scaling: +30% compute, +50% storage ### Contact Information - **Architecture Team**: architecture@cloudflow.internal - **On-Call Engineer**: PagerDuty escalation - **Security Team**: security@cloudflow.internal - **Documentation**: https://wiki.cloudflow.internal/architecture --- **Document Revision History**: - v2.3.1 (Jan 15, 2026): Updated performance metrics, added DR test results - v2.3.0 (Dec 1, 2025): Added multi-AZ deployment details - v2.2.0 (Oct 15, 2025): Security architecture overhaul - v2.1.0 (Sep 1, 2025): Initial Kafka migration documentation - v2.0.0 (Jul 1, 2025): Complete microservices rewrite",
              "score": null
            }
          ],
          "latency_ms": 11.384236000594683,
          "query_id": "realistic_004",
          "dimension": "problem",
          "query": "I need to understand the data storage architecture"
        },
        {
          "key_facts": [
            "PostgreSQL 15.4",
            "Redis 7.2",
            "Apache Kafka 3.6"
          ],
          "found_facts": [],
          "missed_facts": [
            "PostgreSQL 15.4",
            "Redis 7.2",
            "Apache Kafka 3.6"
          ],
          "coverage": 0.0,
          "expected_docs": [
            "architecture_overview"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_4",
              "content": "**Implement connection pooling optimization:** ```bash # Use PgBouncer for connection pooling kubectl apply -f cloudflow-pgbouncer.yaml # Configure CloudFlow to use PgBouncer cloudflow config set db.host pgbouncer.cloudflow.svc.cluster.local cloudflow config set db.port 6432 cloudflow config set db.pool.mode transaction ``` 2. **Add read replicas:** ```bash # Route read-only queries to replicas cloudflow db replicas add --count 2 cloudflow config set db.read_replicas \"replica-1:5432,replica-2:5432\" cloudflow config set db.read_write_split true ``` ### Connection Timeout Errors #### Error Messages - `connection timeout after 30000ms` - `could not connect to database server at 10.0.2.45:5432` - `database server unreachable` #### Troubleshooting Steps ```bash # Test network connectivity telnet cloudflow-db.internal.company.com 5432 # Check DNS resolution nslookup cloudflow-db.internal.company.com # Verify database is accepting connections pg_isready -h cloudflow-db.internal.company.com -p 5432 -U cloudflow # Check firewall rules sudo iptables -L -n | grep 5432 # Test from CloudFlow pod network kubectl run -n cloudflow debug-pod --rm -i --tty \\ --image=postgres:14 -- \\ psql -h cloudflow-db.internal.company.com -U cloudflow -d cloudflow_production # Review database logs for connection rejections kubectl logs -n cloudflow statefulset/cloudflow-db --tail=100 | \\ grep -i \"connection\\|reject\\|authentication\" ``` ### Maximum Connection Limit (100) Reached This is a hard limit in CloudFlow's database tier. #### Permanent Solutions **Option 1: Upgrade database tier** ```bash # Check available tiers cloudflow db tiers list # Upgrade to higher tier (supports 200 connections) cloudflow db upgrade --tier standard-plus --confirm # Monitor migration progress cloudflow db migration status ``` **Option 2: Implement aggressive connection reuse** ```bash # Reduce connection lifetime cloudflow config set db.pool.max_lifetime 600 # 10 minutes # Enable prepared statement caching cloudflow config set db.prepared_statements.cache true cloudflow config set db.prepared_statements.max_size 250 # Reduce idle connection timeout cloudflow config set db.pool.idle_timeout 120 # 2 minutes ``` --- ## Workflow Execution Failures ### Timeout Errors (3600 second limit) #### Error Message ``` WorkflowExecutionError: Workflow exceeded maximum execution time of 3600 seconds Status: TIMEOUT Workflow ID: wf_9k2n4m8p1q Execution ID: exec_7h3j6k9m2n ``` #### Analysis ```bash # Get workflow execution details cloudflow workflows executions get exec_7h3j6k9m2n --verbose # Check step-by-step breakdown cloudflow workflows executions steps exec_7h3j6k9m2n # Sample output: # Step 1: data_ingestion - Duration: 245s - Status: SUCCESS # Step 2: data_validation - Duration: 123s - Status: SUCCESS # Step 3: data_transformation - Duration: 3189s - Status: TIMEOUT # Step 4: data_export - Duration: 0s - Status: SKIPPED # Identify bottleneck step cloudflow workflows analyze",
              "score": null
            },
            {
              "doc_id": "user_guide",
              "chunk_id": "user_guide_fix_7",
              "content": "Handle Errors Gracefully Always implement error handling for external API calls and database operations: ```yaml - id: fetch_data action: http_request config: url: \"https://api.example.com/data\" retry: max_attempts: 3 backoff_type: exponential on_error: - id: log_error action: database_query config: query: \"INSERT INTO error_log (workflow_id, error) VALUES ($1, $2)\" parameters: - \"{{workflow.id}}\" - \"{{error.message}}\" ``` ### 3. Use Secrets for Sensitive Data Never hardcode API keys, passwords, or tokens in workflows: **Bad:** ```yaml headers: Authorization: \"Bearer sk_live_abc123xyz789\" ``` **Good:** ```yaml headers: Authorization: \"Bearer {{secrets.API_TOKEN}}\" ``` Store secrets in **Settings** > **Secrets** with encryption at rest. ### 4. Validate Input Data Always validate trigger data before processing: ```yaml - id: validate_input action: javascript code: | const required_fields = ['email', 'name', 'order_id']; for (const field of required_fields) { if (!input[field]) { throw new Error(`Missing required field: ${field}`); } } // Validate email format const email_regex = /^[^\\s@]+@[^\\s@]+\\.[^\\s@]+$/; if (!email_regex.test(input.email)) { throw new Error('Invalid email format'); } return { validated: true }; ``` ### 5. Use Idempotency Keys For operations that shouldn't be repeated (payments, record creation), use idempotency keys: ```yaml - id: create_charge action: http_request config: url: \"https://api.stripe.com/v1/charges\" method: POST headers: Idempotency-Key: \"{{workflow.id}}-{{execution.id}}\" body: amount: \"{{amount}}\" ``` ### 6. Monitor and Log Add logging steps for important workflow milestones: ```yaml - id: log_start action: database_query config: query: \"INSERT INTO workflow_audit (execution_id, step, timestamp) VALUES ($1, $2, $3)\" parameters: - \"{{execution.id}}\" - \"workflow_started\" - \"{{now()}}\" ``` ### 7. Keep Workflows Modular Break complex workflows into smaller, reusable components: - Use sub-workflows for repeated logic - Trigger child workflows via webhooks - Share common configurations via templates ### 8. Test Thoroughly Before activating a workflow: 1. Use test mode with sample data 2. Verify all actions execute correctly 3. Test error handling paths 4. Review execution logs 5. Start with a limited scope (e.g., test channel, small dataset) ### 9. Document Your Workflows Add descriptions to workflows and steps: ```yaml workflow: name: \"Daily Sales Report\" description: | Generates a daily sales report and distributes it to the sales team. Runs at 8:00 AM EST Monday-Friday. Data source: PostgreSQL sales database Recipients: sales-team@company.com steps: - id: fetch_sales name: \"Fetch Yesterday's Sales\" description: \"Query database for all completed orders from previous day\" ``` ### 10.",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_10",
              "content": "database nc -zv cloudflow-db.internal.company.com 5432 # Trace route traceroute api.cloudflow.io # Capture packets tcpdump -i any -w /tmp/capture.pcap port 5432 ``` --- ## Escalation Procedures ### Severity Levels CloudFlow incidents are classified into four severity levels: #### SEV-1: Critical (P1) - **Definition:** Complete service outage or severe degradation affecting all users - **Examples:** - API returns 5xx errors for > 5 minutes - Database completely unavailable - Authentication system down - Data loss or corruption - **Response Time:** Immediate (< 15 minutes) - **Escalation:** Page on-call engineer immediately #### SEV-2: High (P2) - **Definition:** Major functionality impaired affecting multiple users - **Examples:** - Workflow execution success rate < 90% - Significant performance degradation (p95 latency > 10s) - Rate limiting affecting large customer segment - **Response Time:** < 1 hour - **Escalation:** Create incident ticket and notify on-call #### SEV-3: Medium (P3) - **Definition:** Partial functionality degraded affecting some users - **Examples:** - Intermittent failures for specific workflow types - Minor performance issues - Non-critical feature unavailable - **Response Time:** < 4 hours - **Escalation:** Create ticket, normal business hours support #### SEV-4: Low (P4) - **Definition:** Minor issues with minimal user impact - **Examples:** - Cosmetic issues - Documentation errors - Feature requests - **Response Time:** < 2 business days - **Escalation:** Standard support ticket ### Escalation Steps #### Step 1: Initial Assessment (0-5 minutes) ```bash # Run health check cloudflow health check --comprehensive # Check status page curl https://status.cloudflow.io/api/v1/status.json # Review recent changes cloudflow audit log --last 2h --event-type \"deployment,configuration\" # Check metrics dashboard cloudflow metrics dashboard --incident-mode ``` #### Step 2: Gather Information (5-15 minutes) Create incident document with: - Incident timestamp and duration - Affected services and endpoints - Error rates and user impact - Recent changes or deployments - Relevant log excerpts - Correlation IDs for failed requests ```bash # Generate incident report cloudflow debug incident-report \\ --start \"2026-01-24T10:30:00Z\" \\ --end \"2026-01-24T11:00:00Z\" \\ --output incident-report.md # Capture system snapshot cloudflow debug snapshot --output snapshot-$(date +%Y%m%d-%H%M%S).tar.gz ``` #### Step 3: Escalate Based on Severity **For SEV-1 (Critical):** ```bash # Page on-call engineer cloudflow incident create \\ --severity SEV-1 \\ --title \"Complete API outage\" \\ --description \"All API requests returning 503\" \\ --page-oncall # Notify status page cloudflow status update \\ --status \"major_outage\" \\ --message \"We are investigating a complete service",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_5",
              "content": "Success/failure events published back to Kafka ``` **Event Schema**: ```json { \"event_id\": \"uuid-v4\", \"event_type\": \"workflow.execution.completed\", \"timestamp\": \"2026-01-15T10:30:00.000Z\", \"correlation_id\": \"request-trace-id\", \"payload\": { \"workflow_id\": \"wf-12345\", \"execution_id\": \"exec-67890\", \"status\": \"COMPLETED\", \"duration_ms\": 4230 }, \"metadata\": { \"source_service\": \"workflow-engine\", \"schema_version\": \"1.0\" } } ``` ### Inter-Service Communication Services communicate using two primary patterns: **Synchronous (gRPC)**: - Auth Service validation calls from API Gateway - User Service profile lookups - Low latency requirement (< 50ms) - Request-response pattern with circuit breaker **Asynchronous (Kafka)**: - Workflow execution events - Notification triggers - Audit log events - Eventual consistency acceptable --- ## Database Architecture ### PostgreSQL Primary Database **Cluster Configuration**: - Primary-replica setup with 1 primary + 2 read replicas - Instance type: db.r6g.2xlarge (8 vCPU, 64GB RAM) - Storage: 2TB gp3 SSD with 12,000 IOPS - Multi-AZ deployment for high availability - Automated backups: Daily snapshots, 30-day retention - Point-in-time recovery: 5-minute granularity **Database Schema Design**: ``` Core Tables: - users (5M rows): User accounts and profiles - workflows (2M rows): Workflow definitions - workflow_executions (500M rows, partitioned): Execution history - workflow_steps (2B rows, partitioned): Individual step records - schedules (100K rows): Scheduled workflow triggers - notifications (1B rows, partitioned): Notification delivery log Partitioning Strategy: - workflow_executions: Monthly partitions by created_at - workflow_steps: Monthly partitions by created_at - notifications: Weekly partitions by created_at - Automatic partition creation via cron job - Retention policy: Drop partitions older than 12 months ``` **Indexing Strategy**: - Primary keys: UUIDs with B-tree indexes - Frequently queried columns: Compound indexes (e.g., user_id + status + created_at) - JSONB columns: GIN indexes for workflow definitions - Full-text search: GiST indexes on description fields **Connection Pooling**: - PgBouncer in transaction mode - Pool size: 100 connections per service - Max client connections: 2000 - Connection timeout: 30 seconds **Performance Characteristics**: - Read query P99: < 20ms - Write query P99: < 50ms - Transaction throughput: 15,000 TPS - Replication lag: < 500ms --- ### Redis Caching Layer **Cluster Configuration**: - Redis Cluster with 6 nodes (3 primary + 3 replica) - Instance type: cache.r6g.xlarge (4 vCPU, 26GB RAM) - Total memory: 78GB usable cache space - Persistence: RDB snapshots every 5 minutes + AOF - Eviction policy: allkeys-lru **Cache Usage Patterns**: 1.",
              "score": null
            },
            {
              "doc_id": "user_guide",
              "chunk_id": "user_guide_fix_2",
              "content": "### HTTP Requests Make HTTP requests to any API endpoint: **Configuration:** - **Method**: GET, POST, PUT, PATCH, DELETE, HEAD - **URL**: Full endpoint URL (supports variable interpolation) - **Headers**: Custom headers as key-value pairs - **Query Parameters**: URL parameters - **Body**: JSON, form data, or raw text - **Authentication**: Basic Auth, Bearer Token, API Key, OAuth 2.0 **Example:** ```yaml - id: fetch_user action: http_request config: method: GET url: \"https://api.example.com/users/{{user_id}}\" headers: Authorization: \"Bearer {{secrets.API_TOKEN}}\" Content-Type: \"application/json\" timeout: 30 ``` **Response Handling:** Access the response in subsequent steps: - `{{steps.fetch_user.status}}` - HTTP status code - `{{steps.fetch_user.body}}` - Response body - `{{steps.fetch_user.headers}}` - Response headers ### Database Queries Execute queries against supported databases (PostgreSQL, MySQL, MongoDB, Redis): **SQL Databases (PostgreSQL, MySQL):** ```yaml - id: get_orders action: database_query config: connection: \"{{secrets.DB_CONNECTION_STRING}}\" query: | SELECT * FROM orders WHERE customer_id = $1 AND status = $2 ORDER BY created_at DESC LIMIT 10 parameters: - \"{{trigger.customer_id}}\" - \"pending\" ``` **MongoDB:** ```yaml - id: find_documents action: mongodb_query config: connection: \"{{secrets.MONGO_URI}}\" database: \"production\" collection: \"users\" operation: find filter: email: \"{{trigger.email}}\" options: limit: 1 ``` **Important Notes:** - Always use parameterized queries to prevent SQL injection - Connection strings should be stored in secrets - Query timeout is 30 seconds by default - Maximum result set size is 10MB ### Email Notifications Send emails via SMTP or integrated email providers: **Configuration:** ```yaml - id: send_notification action: email config: provider: \"smtp\" # or \"sendgrid\", \"mailgun\", \"ses\" from: \"notifications@cloudflow.io\" to: \"{{trigger.recipient}}\" cc: \"manager@company.com\" subject: \"Alert: {{alert_type}}\" body: | Hello {{user.name}}, This is an automated notification about {{event_description}}. Details: - Time: {{timestamp}} - Type: {{alert_type}} - Priority: {{priority}} Best regards, CloudFlow Automation html: true attachments: - name: \"report.pdf\" content: \"{{steps.generate_report.output}}\" encoding: \"base64\" ``` **Email Templates:** CloudFlow supports dynamic templates with conditional content: ``` {{#if priority == 'high'}} \u26a0\ufe0f URGENT: Immediate attention required {{else}} \u2139\ufe0f Informational notification {{/if}} ``` ### Slack Messages Send messages to Slack channels or users: **Channel Messages:** ```yaml - id: notify_team action: slack_message config: channel: \"#deployments\" text: \"Deployment completed successfully!\" blocks: - type: \"section\" text: type: \"mrkdwn\" text: \"*Deployment Summary*\\nEnvironment: {{environment}}\\nVersion: {{version}}\\nDuration: {{duration}}s\" - type: \"actions\" elements: - type: \"button\" text: \"View Logs\" url: \"{{logs_url}}\" thread_ts: \"{{trigger.thread_ts}}\" # Reply to thread ``` **Direct Messages:** ```yaml - id: dm_user action: slack_message config: user: \"{{user_slack_id}}\" text: \"Your report is ready for review.\"",
              "score": null
            }
          ],
          "latency_ms": 11.247850998188369,
          "query_id": "realistic_004",
          "dimension": "casual",
          "query": "database stack"
        },
        {
          "key_facts": [
            "PostgreSQL 15.4",
            "Redis 7.2",
            "Apache Kafka 3.6"
          ],
          "found_facts": [
            "PostgreSQL 15.4",
            "Redis 7.2",
            "Apache Kafka 3.6"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "architecture_overview"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_2",
              "content": "Review Query Execution Plans** ```sql -- Connect to CloudFlow database cloudflow db connect --readonly -- Explain slow query EXPLAIN ANALYZE SELECT w.*, e.status, e.error_message FROM workflows w LEFT JOIN executions e ON w.id = e.workflow_id WHERE w.workspace_id = 'ws_abc123' AND e.created_at > NOW() - INTERVAL '7 days' ORDER BY e.created_at DESC; -- Check for missing indexes SELECT schemaname, tablename, indexname, indexdef FROM pg_indexes WHERE tablename IN ('workflows', 'executions', 'workflow_steps') ORDER BY tablename, indexname; ``` **3. Optimize Queries** Common optimization techniques: **Add missing indexes:** ```sql -- Index for workflow lookup by workspace CREATE INDEX CONCURRENTLY idx_workflows_workspace_created ON workflows(workspace_id, created_at DESC); -- Index for execution status queries CREATE INDEX CONCURRENTLY idx_executions_status_created ON executions(workflow_id, status, created_at DESC) WHERE status IN ('running', 'pending'); -- Composite index for common filter combinations CREATE INDEX CONCURRENTLY idx_executions_workspace_status ON executions(workspace_id, status, created_at) INCLUDE (error_message, retry_count); ``` **Use query result caching:** ```bash # Enable query cache for workspace metadata cloudflow config set cache.workspace.ttl 3600 # Configure Redis cache backend cloudflow config set cache.backend redis cloudflow config set cache.redis.host redis.cloudflow.svc.cluster.local cloudflow config set cache.redis.port 6379 ``` ### High API Latency #### Latency Breakdown Analysis ```bash # Generate latency report cloudflow metrics latency-report --endpoint \"/api/v1/workflows\" --last 24h # Sample output: # Endpoint: POST /api/v1/workflows/execute # P50: 245ms | P95: 1823ms | P99: 4521ms # Breakdown: # - Auth: 45ms (18%) # - DB Query: 156ms (64%) # - Business Logic: 32ms (13%) # - Response Serialization: 12ms (5%) ``` **Network latency issues:** ```bash # Test connectivity to CloudFlow API time curl -w \"@curl-format.txt\" -o /dev/null -s https://api.cloudflow.io/health # Create curl-format.txt: cat > curl-format.txt << EOF time_namelookup: %{time_namelookup}s\\n time_connect: %{time_connect}s\\n time_appconnect: %{time_appconnect}s\\n time_pretransfer: %{time_pretransfer}s\\n time_redirect: %{time_redirect}s\\n time_starttransfer: %{time_starttransfer}s\\n ----------\\n time_total: %{time_total}s\\n EOF # Trace route to API endpoint traceroute api.cloudflow.io # Check DNS resolution time dig api.cloudflow.io | grep \"Query time\" ``` ### Memory Leaks #### Detection ```bash # Monitor CloudFlow service memory usage kubectl top pods -n cloudflow --sort-by=memory # Get detailed memory metrics for specific pod kubectl exec -n cloudflow deploy/cloudflow-api -- \\ curl localhost:9090/metrics | grep memory # Check for OOMKilled pods kubectl get pods -n cloudflow --field-selector=status.phase=Failed | \\ grep OOMKilled # Review memory limits and requests kubectl describe deployment cloudflow-api -n cloudflow | \\ grep -A 5 \"Limits\\|Requests\" ``` #### Common Causes **1.",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_8",
              "content": "Leverage caching:** ```bash # Enable client-side caching export CLOUDFLOW_CACHE_ENABLED=true export CLOUDFLOW_CACHE_TTL=300 # Cache workflow metadata cloudflow workflows list --use-cache --cache-ttl 600 ``` --- ## Log Analysis & Debugging ### Accessing CloudFlow Logs #### Kubernetes Deployments ```bash # List all CloudFlow pods kubectl get pods -n cloudflow # Tail logs from API server kubectl logs -f -n cloudflow deployment/cloudflow-api --tail=100 # Get logs from specific pod kubectl logs -n cloudflow cloudflow-api-7d4f6b8c9d-x7k2m # Get logs from previous crashed pod kubectl logs -n cloudflow cloudflow-api-7d4f6b8c9d-x7k2m --previous # Get logs from all pods in deployment kubectl logs -n cloudflow deployment/cloudflow-api --all-containers=true # Stream logs from multiple pods kubectl logs -n cloudflow -l app=cloudflow-api -f --max-log-requests=10 ``` #### Log Levels CloudFlow supports the following log levels: - `TRACE` - Very detailed debugging information - `DEBUG` - Detailed debugging information - `INFO` - Informational messages (default) - `WARN` - Warning messages - `ERROR` - Error messages - `FATAL` - Fatal errors causing shutdown **Changing log levels:** ```bash # Set global log level cloudflow config set logging.level DEBUG # Set log level for specific component cloudflow config set logging.components.database DEBUG cloudflow config set logging.components.auth INFO cloudflow config set logging.components.workflows TRACE # Temporary log level increase (resets after 1 hour) cloudflow debug set-log-level DEBUG --duration 3600 # View current log configuration cloudflow config get logging --format json ``` ### Grep Patterns for Common Issues #### Authentication Failures ```bash # Find all authentication errors kubectl logs -n cloudflow deployment/cloudflow-api | \\ grep -i \"authentication\\|401\\|unauthorized\" | \\ tail -n 50 # Find JWT validation failures kubectl logs -n cloudflow deployment/cloudflow-api | \\ grep -E \"JWT|token.*invalid|signature.*failed\" # Find clock skew issues kubectl logs -n cloudflow deployment/cloudflow-api | \\ grep -i \"clock skew\\|time.*sync\\|nbf\\|exp\" ``` #### Database Errors ```bash # Find database connection errors kubectl logs -n cloudflow deployment/cloudflow-api | \\ grep -E \"connection.*pool|could not connect|database.*timeout\" # Find slow queries kubectl logs -n cloudflow deployment/cloudflow-api | \\ grep \"slow query\" | \\ awk '{print $NF}' | \\ sort -n | \\ tail -n 20 # Find deadlock errors kubectl logs -n cloudflow deployment/cloudflow-api | \\ grep -i \"deadlock detected\" ``` #### Workflow Execution Errors ```bash # Find workflow timeout errors kubectl logs -n cloudflow deployment/cloudflow-workflow-engine | \\ grep -E \"timeout|exceeded.*3600\" # Find workflow retry attempts kubectl logs -n cloudflow deployment/cloudflow-workflow-engine | \\ grep -E \"retry attempt [0-9]|retrying",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_0",
              "content": "# CloudFlow Platform - System Architecture Overview **Document Version:** 2.3.1 **Last Updated:** January 15, 2026 **Owner:** Platform Architecture Team **Status:** Production ## Executive Summary CloudFlow is a distributed, cloud-native workflow automation platform designed to orchestrate complex business processes at scale. The platform processes over 2.5 million workflow executions daily with an average P99 latency of 180ms for API operations and 4.2 seconds for workflow execution. This document provides a comprehensive overview of the system architecture, including microservices design, data flow patterns, infrastructure decisions, and operational considerations. ## Table of Contents 1. [High-Level Architecture](#high-level-architecture) 2. [Microservices Breakdown](#microservices-breakdown) 3. [Data Flow Architecture](#data-flow-architecture) 4. [Database Architecture](#database-architecture) 5. [Message Queue Patterns](#message-queue-patterns) 6. [Caching Strategy](#caching-strategy) 7. [Security Architecture](#security-architecture) 8. [Performance Characteristics](#performance-characteristics) 9. [Disaster Recovery](#disaster-recovery) --- ## High-Level Architecture CloudFlow follows a microservices architecture pattern deployed across multiple availability zones in AWS. The platform is designed with the following core principles: - **Scalability**: Horizontal scaling for all services with auto-scaling groups - **Resilience**: Circuit breakers, bulkheads, and graceful degradation - **Observability**: Distributed tracing, centralized logging, and comprehensive metrics - **Security**: Zero-trust network architecture with mTLS encryption ### Architecture Diagram (Conceptual) ``` \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 Load Balancer (ALB) \u2502 \u2502 (TLS Termination - 443) \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u25bc \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 API Gateway Layer \u2502 \u2502 (Rate Limiting, Auth, Request Routing) \u2502 \u2514\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u2502 \u2502 \u2502 \u2502 \u25bc \u25bc \u25bc \u25bc \u25bc \u250c\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502Auth\u2502 \u2502Workflow\u2502 \u2502Scheduler\u2502 \u2502Notification\u2502 \u2502User Service\u2502 \u2502Svc \u2502 \u2502 Engine \u2502 \u2502 Service \u2502 \u2502 Service \u2502 \u2502 \u2502 \u2514\u2500\u252c\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u2502 \u2502 \u2502 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 \u2502 \u2502 \u25bc \u25bc \u25bc \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502PostgreSQL\u2502 \u2502 Redis \u2502 \u2502 Kafka \u2502 \u2502 Cluster \u2502 \u2502 Cluster \u2502 \u2502 Cluster \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 ``` ### Technology Stack - **Runtime**: Node.js 20.x (API Gateway, Workflow Engine), Go 1.21 (Auth Service, Scheduler) - **Container Orchestration**: Kubernetes 1.28 (EKS) - **Service Mesh**: Istio 1.20 for service-to-service communication - **Databases**: PostgreSQL 15.4, Redis 7.2 - **Message Broker**: Apache Kafka 3.6 - **Monitoring**: Prometheus, Grafana, Jaeger for distributed tracing - **Secrets Management**: HashiCorp Vault 1.15 --- ## Microservices Breakdown ### API Gateway **Purpose**: Single entry point for all client requests, providing authentication, rate limiting, request routing, and protocol translation.",
              "score": null
            },
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_0",
              "content": "# CloudFlow Platform - Deployment and Operations Guide **Version:** 2.4.0 **Last Updated:** January 2026 **Target Environment:** Production (AWS EKS) ## Table of Contents 1. [Overview](#overview) 2. [Prerequisites](#prerequisites) 3. [Infrastructure Setup](#infrastructure-setup) 4. [Kubernetes Deployment](#kubernetes-deployment) 5. [Database Configuration](#database-configuration) 6. [Monitoring and Observability](#monitoring-and-observability) 7. [Backup and Disaster Recovery](#backup-and-disaster-recovery) 8. [Scaling and Performance](#scaling-and-performance) 9. [Security and Compliance](#security-and-compliance) 10. [Troubleshooting](#troubleshooting) --- ## Overview CloudFlow is a cloud-native workflow orchestration platform designed for high-availability production environments. This guide provides comprehensive instructions for deploying and operating CloudFlow on Amazon EKS (Elastic Kubernetes Service). ### Architecture Summary CloudFlow consists of the following components: - **API Server**: REST API for workflow management (Node.js/Express) - **Worker Service**: Background job processor (Node.js) - **Scheduler**: Cron-based task scheduler (Node.js) - **PostgreSQL**: Primary data store (version 14) - **Redis**: Cache and message queue (version 7.0) - **PgBouncer**: Database connection pooler ### Deployment Model - **Namespace**: `cloudflow-prod` - **Cluster**: AWS EKS 1.28 - **Region**: us-east-1 (primary), us-west-2 (disaster recovery) - **High Availability**: Multi-AZ deployment across 3 availability zones --- ## Prerequisites Before beginning the deployment process, ensure you have the following: ### Required Tools - `kubectl` (v1.28 or later) - `helm` (v3.12 or later) - `aws-cli` (v2.13 or later) - `eksctl` (v0.165 or later) - `terraform` (v1.6 or later) - for infrastructure provisioning ### Access Requirements - AWS account with appropriate IAM permissions - EKS cluster admin access - Container registry access (ECR) - Domain name and SSL certificates - Secrets management access (AWS Secrets Manager or Vault) ### Network Requirements - VPC with at least 3 public and 3 private subnets - NAT Gateway configured - Security groups allowing: - Ingress: HTTPS (443), HTTP (80) - Internal: PostgreSQL (5432), Redis (6379) - Monitoring: Prometheus (9090), Grafana (3000) --- ## Infrastructure Setup ### EKS Cluster Creation Use the following `eksctl` configuration to create the EKS cluster: ```yaml # cluster-config.yaml apiVersion: eksctl.io/v1alpha5 kind: ClusterConfig metadata: name: cloudflow-production region: us-east-1 version: \"1.28\" availabilityZones: - us-east-1a - us-east-1b - us-east-1c vpc: cidr: 10.0.0.0/16 nat: gateway: HighlyAvailable managedNodeGroups: - name: cloudflow-workers instanceType: m5.xlarge minSize: 3 maxSize: 10 desiredCapacity: 5 volumeSize: 100 privateNetworking: true labels: role: worker environment: production tags: Environment: production Application: cloudflow iam: withAddonPolicies: ebs: true efs: true albIngress: true cloudWatch: true cloudWatch: clusterLogging: enableTypes: - api - audit - authenticator - controllerManager - scheduler ``` Create the cluster: ```bash eksctl",
              "score": null
            },
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_0",
              "content": "# CloudFlow API Reference Version 2.1.0 | Last Updated: January 2026 ## Overview The CloudFlow API is a RESTful service that enables developers to programmatically manage cloud workflows, data pipelines, and automation tasks. This documentation provides comprehensive details on authentication, endpoints, request/response formats, error handling, and best practices. **Base URL:** `https://api.cloudflow.io/v2` **API Status:** https://status.cloudflow.io ## Authentication CloudFlow supports three authentication methods to suit different use cases and security requirements. ### API Keys API keys provide simple authentication for server-to-server communication. Include your API key in the request header: ```bash curl -H \"X-API-Key: cf_live_a1b2c3d4e5f6g7h8i9j0\" \\ https://api.cloudflow.io/v2/workflows ``` **Security Notes:** - Never expose API keys in client-side code - Rotate keys every 90 days - Use separate keys for development and production environments ### OAuth 2.0 OAuth 2.0 is recommended for applications that access CloudFlow on behalf of users. We support the Authorization Code flow with PKCE. **Authorization Endpoint:** `https://auth.cloudflow.io/oauth/authorize` **Token Endpoint:** `https://auth.cloudflow.io/oauth/token` **Supported Scopes:** - `workflows:read` - Read workflow configurations - `workflows:write` - Create and modify workflows - `pipelines:read` - Read pipeline data - `pipelines:write` - Create and manage pipelines - `analytics:read` - Access analytics and metrics - `admin:full` - Full administrative access Example authorization request: ```bash curl -X POST https://auth.cloudflow.io/oauth/token \\ -H \"Content-Type: application/x-www-form-urlencoded\" \\ -d \"grant_type=authorization_code\" \\ -d \"code=AUTH_CODE_HERE\" \\ -d \"client_id=YOUR_CLIENT_ID\" \\ -d \"client_secret=YOUR_CLIENT_SECRET\" \\ -d \"redirect_uri=https://yourapp.com/callback\" ``` ### JWT Tokens For advanced use cases, CloudFlow supports JSON Web Tokens (JWT) with RS256 signing algorithm. JWTs must include the following claims: - `iss` (issuer): Your application identifier - `sub` (subject): User or service account ID - `aud` (audience): `https://api.cloudflow.io` - `exp` (expiration): Unix timestamp (max 3600 seconds from `iat`) - `iat` (issued at): Unix timestamp - `scope`: Space-separated list of requested scopes Example JWT header: ```python import jwt import time from cryptography.hazmat.primitives import serialization from cryptography.hazmat.backends import default_backend # Load your private key with open('private_key.pem', 'rb') as key_file: private_key = serialization.load_pem_private_key( key_file.read(), password=None, backend=default_backend() ) # Create JWT payload payload = { 'iss': 'your-app-id', 'sub': 'user-12345', 'aud': 'https://api.cloudflow.io', 'exp': int(time.time()) + 3600, 'iat': int(time.time()), 'scope': 'workflows:read workflows:write' } # Generate token token = jwt.encode(payload, private_key, algorithm='RS256') # Use in API request headers = {'Authorization': f'Bearer {token}'} ``` **Token Expiration:** All tokens expire after 3600 seconds (1 hour). Implement token refresh logic in your application.",
              "score": null
            }
          ],
          "latency_ms": 11.61012699958519,
          "query_id": "realistic_004",
          "dimension": "contextual",
          "query": "For capacity planning, what databases does CloudFlow rely on?"
        },
        {
          "key_facts": [
            "PostgreSQL 15.4",
            "Redis 7.2",
            "Apache Kafka 3.6"
          ],
          "found_facts": [],
          "missed_facts": [
            "PostgreSQL 15.4",
            "Redis 7.2",
            "Apache Kafka 3.6"
          ],
          "coverage": 0.0,
          "expected_docs": [
            "architecture_overview"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_0",
              "content": "# CloudFlow API Reference Version 2.1.0 | Last Updated: January 2026 ## Overview The CloudFlow API is a RESTful service that enables developers to programmatically manage cloud workflows, data pipelines, and automation tasks. This documentation provides comprehensive details on authentication, endpoints, request/response formats, error handling, and best practices. **Base URL:** `https://api.cloudflow.io/v2` **API Status:** https://status.cloudflow.io ## Authentication CloudFlow supports three authentication methods to suit different use cases and security requirements. ### API Keys API keys provide simple authentication for server-to-server communication. Include your API key in the request header: ```bash curl -H \"X-API-Key: cf_live_a1b2c3d4e5f6g7h8i9j0\" \\ https://api.cloudflow.io/v2/workflows ``` **Security Notes:** - Never expose API keys in client-side code - Rotate keys every 90 days - Use separate keys for development and production environments ### OAuth 2.0 OAuth 2.0 is recommended for applications that access CloudFlow on behalf of users. We support the Authorization Code flow with PKCE. **Authorization Endpoint:** `https://auth.cloudflow.io/oauth/authorize` **Token Endpoint:** `https://auth.cloudflow.io/oauth/token` **Supported Scopes:** - `workflows:read` - Read workflow configurations - `workflows:write` - Create and modify workflows - `pipelines:read` - Read pipeline data - `pipelines:write` - Create and manage pipelines - `analytics:read` - Access analytics and metrics - `admin:full` - Full administrative access Example authorization request: ```bash curl -X POST https://auth.cloudflow.io/oauth/token \\ -H \"Content-Type: application/x-www-form-urlencoded\" \\ -d \"grant_type=authorization_code\" \\ -d \"code=AUTH_CODE_HERE\" \\ -d \"client_id=YOUR_CLIENT_ID\" \\ -d \"client_secret=YOUR_CLIENT_SECRET\" \\ -d \"redirect_uri=https://yourapp.com/callback\" ``` ### JWT Tokens For advanced use cases, CloudFlow supports JSON Web Tokens (JWT) with RS256 signing algorithm. JWTs must include the following claims: - `iss` (issuer): Your application identifier - `sub` (subject): User or service account ID - `aud` (audience): `https://api.cloudflow.io` - `exp` (expiration): Unix timestamp (max 3600 seconds from `iat`) - `iat` (issued at): Unix timestamp - `scope`: Space-separated list of requested scopes Example JWT header: ```python import jwt import time from cryptography.hazmat.primitives import serialization from cryptography.hazmat.backends import default_backend # Load your private key with open('private_key.pem', 'rb') as key_file: private_key = serialization.load_pem_private_key( key_file.read(), password=None, backend=default_backend() ) # Create JWT payload payload = { 'iss': 'your-app-id', 'sub': 'user-12345', 'aud': 'https://api.cloudflow.io', 'exp': int(time.time()) + 3600, 'iat': int(time.time()), 'scope': 'workflows:read workflows:write' } # Generate token token = jwt.encode(payload, private_key, algorithm='RS256') # Use in API request headers = {'Authorization': f'Bearer {token}'} ``` **Token Expiration:** All tokens expire after 3600 seconds (1 hour). Implement token refresh logic in your application.",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_4",
              "content": "**Implement connection pooling optimization:** ```bash # Use PgBouncer for connection pooling kubectl apply -f cloudflow-pgbouncer.yaml # Configure CloudFlow to use PgBouncer cloudflow config set db.host pgbouncer.cloudflow.svc.cluster.local cloudflow config set db.port 6432 cloudflow config set db.pool.mode transaction ``` 2. **Add read replicas:** ```bash # Route read-only queries to replicas cloudflow db replicas add --count 2 cloudflow config set db.read_replicas \"replica-1:5432,replica-2:5432\" cloudflow config set db.read_write_split true ``` ### Connection Timeout Errors #### Error Messages - `connection timeout after 30000ms` - `could not connect to database server at 10.0.2.45:5432` - `database server unreachable` #### Troubleshooting Steps ```bash # Test network connectivity telnet cloudflow-db.internal.company.com 5432 # Check DNS resolution nslookup cloudflow-db.internal.company.com # Verify database is accepting connections pg_isready -h cloudflow-db.internal.company.com -p 5432 -U cloudflow # Check firewall rules sudo iptables -L -n | grep 5432 # Test from CloudFlow pod network kubectl run -n cloudflow debug-pod --rm -i --tty \\ --image=postgres:14 -- \\ psql -h cloudflow-db.internal.company.com -U cloudflow -d cloudflow_production # Review database logs for connection rejections kubectl logs -n cloudflow statefulset/cloudflow-db --tail=100 | \\ grep -i \"connection\\|reject\\|authentication\" ``` ### Maximum Connection Limit (100) Reached This is a hard limit in CloudFlow's database tier. #### Permanent Solutions **Option 1: Upgrade database tier** ```bash # Check available tiers cloudflow db tiers list # Upgrade to higher tier (supports 200 connections) cloudflow db upgrade --tier standard-plus --confirm # Monitor migration progress cloudflow db migration status ``` **Option 2: Implement aggressive connection reuse** ```bash # Reduce connection lifetime cloudflow config set db.pool.max_lifetime 600 # 10 minutes # Enable prepared statement caching cloudflow config set db.prepared_statements.cache true cloudflow config set db.prepared_statements.max_size 250 # Reduce idle connection timeout cloudflow config set db.pool.idle_timeout 120 # 2 minutes ``` --- ## Workflow Execution Failures ### Timeout Errors (3600 second limit) #### Error Message ``` WorkflowExecutionError: Workflow exceeded maximum execution time of 3600 seconds Status: TIMEOUT Workflow ID: wf_9k2n4m8p1q Execution ID: exec_7h3j6k9m2n ``` #### Analysis ```bash # Get workflow execution details cloudflow workflows executions get exec_7h3j6k9m2n --verbose # Check step-by-step breakdown cloudflow workflows executions steps exec_7h3j6k9m2n # Sample output: # Step 1: data_ingestion - Duration: 245s - Status: SUCCESS # Step 2: data_validation - Duration: 123s - Status: SUCCESS # Step 3: data_transformation - Duration: 3189s - Status: TIMEOUT # Step 4: data_export - Duration: 0s - Status: SKIPPED # Identify bottleneck step cloudflow workflows analyze",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_0",
              "content": "# CloudFlow Platform Troubleshooting Guide **Version:** 3.2.1 **Last Updated:** January 2026 **Audience:** Platform Engineers, DevOps, Support Teams ## Table of Contents 1. [Overview](#overview) 2. [Authentication & Authorization Issues](#authentication--authorization-issues) 3. [Performance Problems](#performance-problems) 4. [Database Connection Issues](#database-connection-issues) 5. [Workflow Execution Failures](#workflow-execution-failures) 6. [Rate Limiting & Throttling](#rate-limiting--throttling) 7. [Log Analysis & Debugging](#log-analysis--debugging) 8. [Escalation Procedures](#escalation-procedures) --- ## Overview This guide provides comprehensive troubleshooting steps for common CloudFlow platform issues encountered in production environments. Each section includes error symptoms, root cause analysis, resolution steps, and preventive measures. ### Quick Diagnostic Checklist Before diving into specific issues, perform these initial checks: - Verify service health: `cloudflow status --all` - Check API connectivity: `curl -I https://api.cloudflow.io/health` - Review recent deployments: `kubectl get deployments -n cloudflow --sort-by=.metadata.creationTimestamp` - Inspect platform metrics: `cloudflow metrics --last 1h` --- ## Authentication & Authorization Issues ### 401 Unauthorized Errors #### Symptoms - API requests return `401 Unauthorized` - Error message: `Authentication credentials were not provided or are invalid` - Frontend displays \"Session expired\" message #### Common Causes **1. Token Expiration** CloudFlow access tokens expire after 3600 seconds (1 hour) by default. Refresh tokens are valid for 30 days. **Verification:** ```bash # Decode JWT to check expiration echo $CF_ACCESS_TOKEN | cut -d'.' -f2 | base64 -d | jq '.exp' # Compare with current time date +%s ``` **Resolution:** ```bash # Refresh the access token curl -X POST https://api.cloudflow.io/auth/refresh \\ -H \"Content-Type: application/json\" \\ -d '{\"refresh_token\": \"'$CF_REFRESH_TOKEN'\"}' # Update environment variable export CF_ACCESS_TOKEN=\"<new_token>\" ``` **2. Invalid JWT Signature** Error: `JWT signature verification failed` **Causes:** - Token was modified or corrupted - Using wrong signing key - Token generated with different secret **Resolution:** ```bash # Validate token structure cloudflow auth validate --token $CF_ACCESS_TOKEN # Generate new token cloudflow auth login --username <user> --password <pass> # For service accounts cloudflow auth service-account --create --name <sa-name> --scopes \"workflow:read,workflow:write\" ``` **3. Clock Skew Issues** JWT validation fails when system clocks are out of sync (tolerance: \u00b1300 seconds).",
              "score": null
            },
            {
              "doc_id": "user_guide",
              "chunk_id": "user_guide_fix_0",
              "content": "# CloudFlow User Guide Welcome to CloudFlow, the modern workflow automation platform that helps you connect your apps, automate repetitive tasks, and build powerful integrations without writing code. ## Table of Contents 1. [Getting Started](#getting-started) 2. [Workflow Creation](#workflow-creation) 3. [Available Actions](#available-actions) 4. [Variables and Expressions](#variables-and-expressions) 5. [Scheduling](#scheduling) 6. [Error Handling](#error-handling) 7. [Workflow Limits](#workflow-limits) 8. [Best Practices](#best-practices) 9. [Common Workflow Patterns](#common-workflow-patterns) ## Getting Started ### Account Setup Creating your CloudFlow account takes just a few minutes: 1. Navigate to https://app.cloudflow.io/signup 2. Enter your email address and create a strong password 3. Verify your email address by clicking the link sent to your inbox 4. Complete your profile by providing your organization name and use case 5. Choose your plan (Free, Professional, or Enterprise) Once your account is created, you'll be redirected to your CloudFlow dashboard where you can start building workflows immediately. ### Your First Workflow Let's create a simple workflow that sends you a Slack notification when a new file is uploaded to your Google Drive: 1. Click the **\"Create Workflow\"** button in the top-right corner 2. Give your workflow a name: \"Google Drive to Slack Notifier\" 3. Select **\"Google Drive - New File\"** as your trigger 4. Authenticate your Google Drive account when prompted 5. Choose the folder you want to monitor 6. Click **\"Add Action\"** and select **\"Slack - Send Message\"** 7. Authenticate your Slack account 8. Select the channel where you want to receive notifications 9. Customize your message using variables: `New file uploaded: {{trigger.file.name}}` 10. Click **\"Save & Activate\"** to enable your workflow Congratulations! Your first workflow is now live. Every time a file is added to the specified Google Drive folder, you'll receive a Slack notification. ## Workflow Creation CloudFlow offers multiple ways to create and manage workflows, giving you the flexibility to work in the way that suits you best. ### Visual Editor The Visual Editor is CloudFlow's drag-and-drop interface for building workflows without code. It's perfect for users who prefer a graphical approach: **Key Features:** - Drag-and-drop action blocks from the sidebar - Visual connections between steps show your workflow logic - Inline configuration for each action - Real-time validation and error checking - Test mode to run your workflow with sample data **Using the Visual Editor:** 1.",
              "score": null
            },
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_0",
              "content": "# CloudFlow Platform - Deployment and Operations Guide **Version:** 2.4.0 **Last Updated:** January 2026 **Target Environment:** Production (AWS EKS) ## Table of Contents 1. [Overview](#overview) 2. [Prerequisites](#prerequisites) 3. [Infrastructure Setup](#infrastructure-setup) 4. [Kubernetes Deployment](#kubernetes-deployment) 5. [Database Configuration](#database-configuration) 6. [Monitoring and Observability](#monitoring-and-observability) 7. [Backup and Disaster Recovery](#backup-and-disaster-recovery) 8. [Scaling and Performance](#scaling-and-performance) 9. [Security and Compliance](#security-and-compliance) 10. [Troubleshooting](#troubleshooting) --- ## Overview CloudFlow is a cloud-native workflow orchestration platform designed for high-availability production environments. This guide provides comprehensive instructions for deploying and operating CloudFlow on Amazon EKS (Elastic Kubernetes Service). ### Architecture Summary CloudFlow consists of the following components: - **API Server**: REST API for workflow management (Node.js/Express) - **Worker Service**: Background job processor (Node.js) - **Scheduler**: Cron-based task scheduler (Node.js) - **PostgreSQL**: Primary data store (version 14) - **Redis**: Cache and message queue (version 7.0) - **PgBouncer**: Database connection pooler ### Deployment Model - **Namespace**: `cloudflow-prod` - **Cluster**: AWS EKS 1.28 - **Region**: us-east-1 (primary), us-west-2 (disaster recovery) - **High Availability**: Multi-AZ deployment across 3 availability zones --- ## Prerequisites Before beginning the deployment process, ensure you have the following: ### Required Tools - `kubectl` (v1.28 or later) - `helm` (v3.12 or later) - `aws-cli` (v2.13 or later) - `eksctl` (v0.165 or later) - `terraform` (v1.6 or later) - for infrastructure provisioning ### Access Requirements - AWS account with appropriate IAM permissions - EKS cluster admin access - Container registry access (ECR) - Domain name and SSL certificates - Secrets management access (AWS Secrets Manager or Vault) ### Network Requirements - VPC with at least 3 public and 3 private subnets - NAT Gateway configured - Security groups allowing: - Ingress: HTTPS (443), HTTP (80) - Internal: PostgreSQL (5432), Redis (6379) - Monitoring: Prometheus (9090), Grafana (3000) --- ## Infrastructure Setup ### EKS Cluster Creation Use the following `eksctl` configuration to create the EKS cluster: ```yaml # cluster-config.yaml apiVersion: eksctl.io/v1alpha5 kind: ClusterConfig metadata: name: cloudflow-production region: us-east-1 version: \"1.28\" availabilityZones: - us-east-1a - us-east-1b - us-east-1c vpc: cidr: 10.0.0.0/16 nat: gateway: HighlyAvailable managedNodeGroups: - name: cloudflow-workers instanceType: m5.xlarge minSize: 3 maxSize: 10 desiredCapacity: 5 volumeSize: 100 privateNetworking: true labels: role: worker environment: production tags: Environment: production Application: cloudflow iam: withAddonPolicies: ebs: true efs: true albIngress: true cloudWatch: true cloudWatch: clusterLogging: enableTypes: - api - audit - authenticator - controllerManager - scheduler ``` Create the cluster: ```bash eksctl",
              "score": null
            }
          ],
          "latency_ms": 11.725372001819778,
          "query_id": "realistic_004",
          "dimension": "negation",
          "query": "Is CloudFlow using MySQL or something else?"
        },
        {
          "key_facts": [
            "cloudflow-prod"
          ],
          "found_facts": [
            "cloudflow-prod"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "deployment_guide"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_0",
              "content": "# CloudFlow Platform - Deployment and Operations Guide **Version:** 2.4.0 **Last Updated:** January 2026 **Target Environment:** Production (AWS EKS) ## Table of Contents 1. [Overview](#overview) 2. [Prerequisites](#prerequisites) 3. [Infrastructure Setup](#infrastructure-setup) 4. [Kubernetes Deployment](#kubernetes-deployment) 5. [Database Configuration](#database-configuration) 6. [Monitoring and Observability](#monitoring-and-observability) 7. [Backup and Disaster Recovery](#backup-and-disaster-recovery) 8. [Scaling and Performance](#scaling-and-performance) 9. [Security and Compliance](#security-and-compliance) 10. [Troubleshooting](#troubleshooting) --- ## Overview CloudFlow is a cloud-native workflow orchestration platform designed for high-availability production environments. This guide provides comprehensive instructions for deploying and operating CloudFlow on Amazon EKS (Elastic Kubernetes Service). ### Architecture Summary CloudFlow consists of the following components: - **API Server**: REST API for workflow management (Node.js/Express) - **Worker Service**: Background job processor (Node.js) - **Scheduler**: Cron-based task scheduler (Node.js) - **PostgreSQL**: Primary data store (version 14) - **Redis**: Cache and message queue (version 7.0) - **PgBouncer**: Database connection pooler ### Deployment Model - **Namespace**: `cloudflow-prod` - **Cluster**: AWS EKS 1.28 - **Region**: us-east-1 (primary), us-west-2 (disaster recovery) - **High Availability**: Multi-AZ deployment across 3 availability zones --- ## Prerequisites Before beginning the deployment process, ensure you have the following: ### Required Tools - `kubectl` (v1.28 or later) - `helm` (v3.12 or later) - `aws-cli` (v2.13 or later) - `eksctl` (v0.165 or later) - `terraform` (v1.6 or later) - for infrastructure provisioning ### Access Requirements - AWS account with appropriate IAM permissions - EKS cluster admin access - Container registry access (ECR) - Domain name and SSL certificates - Secrets management access (AWS Secrets Manager or Vault) ### Network Requirements - VPC with at least 3 public and 3 private subnets - NAT Gateway configured - Security groups allowing: - Ingress: HTTPS (443), HTTP (80) - Internal: PostgreSQL (5432), Redis (6379) - Monitoring: Prometheus (9090), Grafana (3000) --- ## Infrastructure Setup ### EKS Cluster Creation Use the following `eksctl` configuration to create the EKS cluster: ```yaml # cluster-config.yaml apiVersion: eksctl.io/v1alpha5 kind: ClusterConfig metadata: name: cloudflow-production region: us-east-1 version: \"1.28\" availabilityZones: - us-east-1a - us-east-1b - us-east-1c vpc: cidr: 10.0.0.0/16 nat: gateway: HighlyAvailable managedNodeGroups: - name: cloudflow-workers instanceType: m5.xlarge minSize: 3 maxSize: 10 desiredCapacity: 5 volumeSize: 100 privateNetworking: true labels: role: worker environment: production tags: Environment: production Application: cloudflow iam: withAddonPolicies: ebs: true efs: true albIngress: true cloudWatch: true cloudWatch: clusterLogging: enableTypes: - api - audit - authenticator - controllerManager - scheduler ``` Create the cluster: ```bash eksctl",
              "score": null
            },
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_1",
              "content": "create cluster -f cluster-config.yaml ``` ### Storage Configuration Install the EBS CSI driver for persistent volumes: ```bash eksctl create addon --name aws-ebs-csi-driver \\ --cluster cloudflow-production \\ --service-account-role-arn arn:aws:iam::ACCOUNT_ID:role/AmazonEKS_EBS_CSI_DriverRole \\ --force ``` --- ## Kubernetes Deployment ### Namespace Setup Create the CloudFlow namespace and configure resource quotas: ```yaml # namespace.yaml apiVersion: v1 kind: Namespace metadata: name: cloudflow-prod labels: name: cloudflow-prod environment: production --- apiVersion: v1 kind: ResourceQuota metadata: name: cloudflow-quota namespace: cloudflow-prod spec: hard: requests.cpu: \"50\" requests.memory: 100Gi persistentvolumeclaims: \"10\" services.loadbalancers: \"2\" ``` Apply the namespace configuration: ```bash kubectl apply -f namespace.yaml ``` ### Environment Configuration CloudFlow requires the following environment variables: | Variable | Description | Example | Required | |----------|-------------|---------|----------| | `DATABASE_URL` | PostgreSQL connection string | `postgresql://user:pass@host:5432/cloudflow` | Yes | | `REDIS_URL` | Redis connection string | `redis://redis-master.cloudflow-prod.svc.cluster.local:6379` | Yes | | `JWT_SECRET` | Secret key for JWT token signing | `<generated-secret-256-bit>` | Yes | | `LOG_LEVEL` | Application log verbosity | `info`, `debug`, `warn`, `error` | No (default: `info`) | | `API_PORT` | API server port | `3000` | No (default: `3000`) | | `WORKER_CONCURRENCY` | Number of concurrent workers | `10` | No (default: `5`) | | `SESSION_SECRET` | Session encryption key | `<generated-secret-256-bit>` | Yes | | `AWS_REGION` | AWS region for services | `us-east-1` | Yes | | `S3_BUCKET` | S3 bucket for file storage | `cloudflow-prod-storage` | Yes | | `SMTP_HOST` | Email server hostname | `smtp.sendgrid.net` | No | | `SMTP_PORT` | Email server port | `587` | No | | `METRICS_ENABLED` | Enable Prometheus metrics | `true` | No (default: `false`) | Create a Kubernetes secret for sensitive variables: ```bash kubectl create secret generic cloudflow-secrets \\ --namespace cloudflow-prod \\ --from-literal=DATABASE_URL=\"postgresql://cloudflow:$(cat db-password.txt)@pgbouncer.cloudflow-prod.svc.cluster.local:5432/cloudflow\" \\ --from-literal=REDIS_URL=\"redis://redis-master.cloudflow-prod.svc.cluster.local:6379\" \\ --from-literal=JWT_SECRET=\"$(openssl rand -base64 32)\" \\ --from-literal=SESSION_SECRET=\"$(openssl rand -base64 32)\" ``` ### Helm Chart Deployment Add the CloudFlow Helm repository: ```bash helm repo add cloudflow https://charts.cloudflow.io helm repo update ``` Create a values file for production configuration: ```yaml # values-production.yaml replicaCount: 3 image: repository: 123456789012.dkr.ecr.us-east-1.amazonaws.com/cloudflow tag: \"2.4.0\" pullPolicy: IfNotPresent resources: limits: cpu: 2000m memory: 4Gi requests: cpu: 2000m memory: 4Gi autoscaling: enabled: true minReplicas: 3 maxReplicas: 10 targetCPUUtilizationPercentage: 70 targetMemoryUtilizationPercentage: 80 service: type: ClusterIP port: 80 targetPort: 3000 annotations: service.beta.kubernetes.io/aws-load-balancer-type: \"nlb\" ingress: enabled: true className: nginx annotations: cert-manager.io/cluster-issuer: \"letsencrypt-prod\" nginx.ingress.kubernetes.io/ssl-redirect: \"true\" hosts: - host: api.cloudflow.io paths: - path: / pathType: Prefix tls:",
              "score": null
            },
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_7",
              "content": "podSelector: matchLabels: app: cloudflow policyTypes: - Ingress - Egress ingress: - from: - namespaceSelector: matchLabels: name: cloudflow-prod - podSelector: matchLabels: app: nginx-ingress ports: - protocol: TCP port: 3000 egress: - to: - podSelector: matchLabels: app: postgresql ports: - protocol: TCP port: 5432 - to: - podSelector: matchLabels: app: redis ports: - protocol: TCP port: 6379 ``` ### Pod Security Standards Enforce Pod Security Standards at the namespace level: ```bash kubectl label namespace cloudflow-prod \\ pod-security.kubernetes.io/enforce=restricted \\ pod-security.kubernetes.io/audit=restricted \\ pod-security.kubernetes.io/warn=restricted ``` ### Secrets Management Use AWS Secrets Manager for sensitive credentials: ```bash # Install External Secrets Operator helm install external-secrets external-secrets/external-secrets \\ --namespace external-secrets-system \\ --create-namespace # Create SecretStore kubectl apply -f - <<EOF apiVersion: external-secrets.io/v1beta1 kind: SecretStore metadata: name: aws-secrets-manager namespace: cloudflow-prod spec: provider: aws: service: SecretsManager region: us-east-1 auth: jwt: serviceAccountRef: name: external-secrets-sa EOF ``` --- ## Troubleshooting ### Common Issues #### Pods in CrashLoopBackOff **Symptoms**: Pods continuously restart **Diagnosis**: ```bash kubectl logs -n cloudflow-prod <pod-name> --previous kubectl describe pod -n cloudflow-prod <pod-name> ``` **Common Causes**: - Database connection failure - Invalid environment variables - Insufficient resources #### High Memory Usage **Symptoms**: Pods being OOMKilled **Diagnosis**: ```bash kubectl top pods -n cloudflow-prod ``` **Resolution**: - Increase memory limits in deployment - Check for memory leaks in application logs - Review heap dump for Node.js processes #### Database Connection Pool Exhausted **Symptoms**: \"Too many connections\" errors **Diagnosis**: ```bash # Check PgBouncer stats kubectl exec -it pgbouncer-xxx -n cloudflow-prod -- \\ psql -p 5432 -U pgbouncer pgbouncer -c \"SHOW POOLS;\" ``` **Resolution**: - Increase `max_db_connections` in PgBouncer - Optimize application connection usage - Add more PostgreSQL replicas ### Support Contacts - **On-call Engineer**: pagerduty.com/cloudflow-oncall - **Slack Channel**: #cloudflow-operations - **Documentation**: https://docs.cloudflow.io - **Status Page**: https://status.cloudflow.io --- ## Appendix ### Useful Commands ```bash # View all resources in namespace kubectl get all -n cloudflow-prod # Check resource usage kubectl top pods -n cloudflow-prod kubectl top nodes # View logs kubectl logs -f deployment/cloudflow-api -n cloudflow-prod # Execute command in pod kubectl exec -it <pod-name> -n cloudflow-prod -- /bin/sh # Port forward for local testing kubectl port-forward svc/cloudflow-api 8080:80 -n cloudflow-prod # Rollback deployment helm rollback cloudflow -n cloudflow-prod # Update deployment helm upgrade cloudflow cloudflow/cloudflow \\ --namespace cloudflow-prod \\ --values values-production.yaml ``` ### Maintenance Windows Scheduled maintenance occurs during the following windows: - **Primary**: Sunday 02:00-06:00",
              "score": null
            },
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_5",
              "content": "Configure log aggregation: ```bash # Install Fluent Bit for log forwarding helm install fluent-bit fluent/fluent-bit \\ --namespace logging \\ --create-namespace \\ --set cloudWatch.enabled=true \\ --set cloudWatch.region=us-east-1 \\ --set cloudWatch.logGroupName=/aws/eks/cloudflow-production/application ``` Query logs using CloudWatch Insights: ```sql fields @timestamp, @message, level, requestId, userId | filter namespace = \"cloudflow-prod\" | filter level = \"error\" | sort @timestamp desc | limit 100 ``` --- ## Backup and Disaster Recovery ### Database Backup Strategy CloudFlow implements a comprehensive backup strategy with the following retention policy: - **Daily snapshots**: Retained for 30 days - **Weekly backups**: Retained for 90 days - **Monthly backups**: Retained for 1 year #### Automated Backup with Velero Install Velero for cluster-wide backups: ```bash velero install \\ --provider aws \\ --plugins velero/velero-plugin-for-aws:v1.8.0 \\ --bucket cloudflow-velero-backups \\ --backup-location-config region=us-east-1 \\ --snapshot-location-config region=us-east-1 \\ --secret-file ./credentials-velero ``` Create a daily backup schedule: ```yaml # backup-schedule.yaml apiVersion: velero.io/v1 kind: Schedule metadata: name: cloudflow-daily-backup namespace: velero spec: schedule: \"0 2 * * *\" # 2 AM UTC daily template: includedNamespaces: - cloudflow-prod ttl: 720h0m0s # 30 days storageLocation: default snapshotVolumes: true ``` Apply the backup schedule: ```bash kubectl apply -f backup-schedule.yaml ``` #### PostgreSQL Backup Configure PostgreSQL continuous archiving with WAL-G: ```bash # Create backup cronjob kubectl apply -f - <<EOF apiVersion: batch/v1 kind: CronJob metadata: name: postgres-backup namespace: cloudflow-prod spec: schedule: \"0 1 * * *\" # 1 AM UTC daily jobTemplate: spec: template: spec: containers: - name: backup image: postgres:14 env: - name: PGPASSWORD valueFrom: secretKeyRef: name: postgres-credentials key: password command: - /bin/bash - -c - | TIMESTAMP=\\$(date +%Y%m%d_%H%M%S) pg_dump -h postgresql.cloudflow-prod.svc.cluster.local \\ -U cloudflow -d cloudflow \\ | gzip > /backups/cloudflow_\\${TIMESTAMP}.sql.gz aws s3 cp /backups/cloudflow_\\${TIMESTAMP}.sql.gz \\ s3://cloudflow-db-backups/daily/ volumeMounts: - name: backup-storage mountPath: /backups volumes: - name: backup-storage emptyDir: {} restartPolicy: OnFailure EOF ``` ### Disaster Recovery Procedure In case of catastrophic failure, follow these steps to restore CloudFlow: 1. **Restore EKS Cluster** (if necessary): ```bash eksctl create cluster -f cluster-config.yaml ``` 2. **Restore Velero Backup**: ```bash # List available backups velero backup get # Restore from backup velero restore create --from-backup cloudflow-daily-backup-20260124 ``` 3. **Restore Database**: ```bash # Download backup from S3 aws s3 cp s3://cloudflow-db-backups/daily/cloudflow_20260124_010000.sql.gz . # Restore database gunzip -c cloudflow_20260124_010000.sql.gz | \\ psql -h postgresql.cloudflow-prod.svc.cluster.local -U cloudflow -d cloudflow ``` 4.",
              "score": null
            },
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_6",
              "content": "**Verify Service Health**: ```bash kubectl get pods -n cloudflow-prod curl https://api.cloudflow.io/health ``` **Recovery Time Objective (RTO)**: 4 hours **Recovery Point Objective (RPO)**: 24 hours --- ## Scaling and Performance ### Horizontal Pod Autoscaling CloudFlow is configured with Horizontal Pod Autoscaler (HPA) to automatically scale based on resource utilization: ```yaml # hpa.yaml apiVersion: autoscaling/v2 kind: HorizontalPodAutoscaler metadata: name: cloudflow-api-hpa namespace: cloudflow-prod spec: scaleTargetRef: apiVersion: apps/v1 kind: Deployment name: cloudflow-api minReplicas: 3 maxReplicas: 10 metrics: - type: Resource resource: name: cpu target: type: Utilization averageUtilization: 70 - type: Resource resource: name: memory target: type: Utilization averageUtilization: 80 behavior: scaleDown: stabilizationWindowSeconds: 300 policies: - type: Percent value: 50 periodSeconds: 60 scaleUp: stabilizationWindowSeconds: 60 policies: - type: Percent value: 100 periodSeconds: 30 - type: Pods value: 2 periodSeconds: 30 selectPolicy: Max ``` Monitor autoscaling events: ```bash kubectl get hpa -n cloudflow-prod -w kubectl describe hpa cloudflow-api-hpa -n cloudflow-prod ``` ### Performance Optimization #### Connection Pooling Ensure optimal database connection pooling settings in PgBouncer: - **Pool Mode**: Transaction (optimal for microservices) - **Default Pool Size**: 25 connections per user - **Max DB Connections**: 100 (matches PostgreSQL `max_connections`) #### Redis Caching Implement Redis caching for frequently accessed data: ```javascript // Example caching strategy const CACHE_TTL = { workflows: 300, // 5 minutes userSessions: 3600, // 1 hour apiResults: 60 // 1 minute }; ``` #### Load Testing Perform regular load testing to validate scaling configuration: ```bash # Install k6 for load testing kubectl apply -f - <<EOF apiVersion: v1 kind: ConfigMap metadata: name: loadtest-script namespace: cloudflow-prod data: script.js: | import http from 'k6/http'; import { check, sleep } from 'k6'; export let options = { stages: [ { duration: '2m', target: 100 }, { duration: '5m', target: 100 }, { duration: '2m', target: 200 }, { duration: '5m', target: 200 }, { duration: '2m', target: 0 }, ], thresholds: { http_req_duration: ['p(95)<500'], }, }; export default function () { let res = http.get('https://api.cloudflow.io/api/workflows'); check(res, { 'status is 200': (r) => r.status === 200 }); sleep(1); } EOF ``` **Performance Targets**: - P95 API latency: < 500ms - P99 API latency: < 1000ms - Error rate: < 0.1% - Throughput: > 1000 requests/second --- ## Security and Compliance ### Network Policies Implement network policies to restrict pod-to-pod communication: ```yaml # network-policy.yaml apiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: cloudflow-network-policy namespace: cloudflow-prod spec:",
              "score": null
            }
          ],
          "latency_ms": 11.715793996700086,
          "query_id": "realistic_005",
          "dimension": "original",
          "query": "What is the Kubernetes namespace for production deployment?"
        },
        {
          "key_facts": [
            "cloudflow-prod"
          ],
          "found_facts": [
            "cloudflow-prod"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "deployment_guide"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_1",
              "content": "create cluster -f cluster-config.yaml ``` ### Storage Configuration Install the EBS CSI driver for persistent volumes: ```bash eksctl create addon --name aws-ebs-csi-driver \\ --cluster cloudflow-production \\ --service-account-role-arn arn:aws:iam::ACCOUNT_ID:role/AmazonEKS_EBS_CSI_DriverRole \\ --force ``` --- ## Kubernetes Deployment ### Namespace Setup Create the CloudFlow namespace and configure resource quotas: ```yaml # namespace.yaml apiVersion: v1 kind: Namespace metadata: name: cloudflow-prod labels: name: cloudflow-prod environment: production --- apiVersion: v1 kind: ResourceQuota metadata: name: cloudflow-quota namespace: cloudflow-prod spec: hard: requests.cpu: \"50\" requests.memory: 100Gi persistentvolumeclaims: \"10\" services.loadbalancers: \"2\" ``` Apply the namespace configuration: ```bash kubectl apply -f namespace.yaml ``` ### Environment Configuration CloudFlow requires the following environment variables: | Variable | Description | Example | Required | |----------|-------------|---------|----------| | `DATABASE_URL` | PostgreSQL connection string | `postgresql://user:pass@host:5432/cloudflow` | Yes | | `REDIS_URL` | Redis connection string | `redis://redis-master.cloudflow-prod.svc.cluster.local:6379` | Yes | | `JWT_SECRET` | Secret key for JWT token signing | `<generated-secret-256-bit>` | Yes | | `LOG_LEVEL` | Application log verbosity | `info`, `debug`, `warn`, `error` | No (default: `info`) | | `API_PORT` | API server port | `3000` | No (default: `3000`) | | `WORKER_CONCURRENCY` | Number of concurrent workers | `10` | No (default: `5`) | | `SESSION_SECRET` | Session encryption key | `<generated-secret-256-bit>` | Yes | | `AWS_REGION` | AWS region for services | `us-east-1` | Yes | | `S3_BUCKET` | S3 bucket for file storage | `cloudflow-prod-storage` | Yes | | `SMTP_HOST` | Email server hostname | `smtp.sendgrid.net` | No | | `SMTP_PORT` | Email server port | `587` | No | | `METRICS_ENABLED` | Enable Prometheus metrics | `true` | No (default: `false`) | Create a Kubernetes secret for sensitive variables: ```bash kubectl create secret generic cloudflow-secrets \\ --namespace cloudflow-prod \\ --from-literal=DATABASE_URL=\"postgresql://cloudflow:$(cat db-password.txt)@pgbouncer.cloudflow-prod.svc.cluster.local:5432/cloudflow\" \\ --from-literal=REDIS_URL=\"redis://redis-master.cloudflow-prod.svc.cluster.local:6379\" \\ --from-literal=JWT_SECRET=\"$(openssl rand -base64 32)\" \\ --from-literal=SESSION_SECRET=\"$(openssl rand -base64 32)\" ``` ### Helm Chart Deployment Add the CloudFlow Helm repository: ```bash helm repo add cloudflow https://charts.cloudflow.io helm repo update ``` Create a values file for production configuration: ```yaml # values-production.yaml replicaCount: 3 image: repository: 123456789012.dkr.ecr.us-east-1.amazonaws.com/cloudflow tag: \"2.4.0\" pullPolicy: IfNotPresent resources: limits: cpu: 2000m memory: 4Gi requests: cpu: 2000m memory: 4Gi autoscaling: enabled: true minReplicas: 3 maxReplicas: 10 targetCPUUtilizationPercentage: 70 targetMemoryUtilizationPercentage: 80 service: type: ClusterIP port: 80 targetPort: 3000 annotations: service.beta.kubernetes.io/aws-load-balancer-type: \"nlb\" ingress: enabled: true className: nginx annotations: cert-manager.io/cluster-issuer: \"letsencrypt-prod\" nginx.ingress.kubernetes.io/ssl-redirect: \"true\" hosts: - host: api.cloudflow.io paths: - path: / pathType: Prefix tls:",
              "score": null
            },
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_7",
              "content": "podSelector: matchLabels: app: cloudflow policyTypes: - Ingress - Egress ingress: - from: - namespaceSelector: matchLabels: name: cloudflow-prod - podSelector: matchLabels: app: nginx-ingress ports: - protocol: TCP port: 3000 egress: - to: - podSelector: matchLabels: app: postgresql ports: - protocol: TCP port: 5432 - to: - podSelector: matchLabels: app: redis ports: - protocol: TCP port: 6379 ``` ### Pod Security Standards Enforce Pod Security Standards at the namespace level: ```bash kubectl label namespace cloudflow-prod \\ pod-security.kubernetes.io/enforce=restricted \\ pod-security.kubernetes.io/audit=restricted \\ pod-security.kubernetes.io/warn=restricted ``` ### Secrets Management Use AWS Secrets Manager for sensitive credentials: ```bash # Install External Secrets Operator helm install external-secrets external-secrets/external-secrets \\ --namespace external-secrets-system \\ --create-namespace # Create SecretStore kubectl apply -f - <<EOF apiVersion: external-secrets.io/v1beta1 kind: SecretStore metadata: name: aws-secrets-manager namespace: cloudflow-prod spec: provider: aws: service: SecretsManager region: us-east-1 auth: jwt: serviceAccountRef: name: external-secrets-sa EOF ``` --- ## Troubleshooting ### Common Issues #### Pods in CrashLoopBackOff **Symptoms**: Pods continuously restart **Diagnosis**: ```bash kubectl logs -n cloudflow-prod <pod-name> --previous kubectl describe pod -n cloudflow-prod <pod-name> ``` **Common Causes**: - Database connection failure - Invalid environment variables - Insufficient resources #### High Memory Usage **Symptoms**: Pods being OOMKilled **Diagnosis**: ```bash kubectl top pods -n cloudflow-prod ``` **Resolution**: - Increase memory limits in deployment - Check for memory leaks in application logs - Review heap dump for Node.js processes #### Database Connection Pool Exhausted **Symptoms**: \"Too many connections\" errors **Diagnosis**: ```bash # Check PgBouncer stats kubectl exec -it pgbouncer-xxx -n cloudflow-prod -- \\ psql -p 5432 -U pgbouncer pgbouncer -c \"SHOW POOLS;\" ``` **Resolution**: - Increase `max_db_connections` in PgBouncer - Optimize application connection usage - Add more PostgreSQL replicas ### Support Contacts - **On-call Engineer**: pagerduty.com/cloudflow-oncall - **Slack Channel**: #cloudflow-operations - **Documentation**: https://docs.cloudflow.io - **Status Page**: https://status.cloudflow.io --- ## Appendix ### Useful Commands ```bash # View all resources in namespace kubectl get all -n cloudflow-prod # Check resource usage kubectl top pods -n cloudflow-prod kubectl top nodes # View logs kubectl logs -f deployment/cloudflow-api -n cloudflow-prod # Execute command in pod kubectl exec -it <pod-name> -n cloudflow-prod -- /bin/sh # Port forward for local testing kubectl port-forward svc/cloudflow-api 8080:80 -n cloudflow-prod # Rollback deployment helm rollback cloudflow -n cloudflow-prod # Update deployment helm upgrade cloudflow cloudflow/cloudflow \\ --namespace cloudflow-prod \\ --values values-production.yaml ``` ### Maintenance Windows Scheduled maintenance occurs during the following windows: - **Primary**: Sunday 02:00-06:00",
              "score": null
            },
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_0",
              "content": "# CloudFlow API Reference Version 2.1.0 | Last Updated: January 2026 ## Overview The CloudFlow API is a RESTful service that enables developers to programmatically manage cloud workflows, data pipelines, and automation tasks. This documentation provides comprehensive details on authentication, endpoints, request/response formats, error handling, and best practices. **Base URL:** `https://api.cloudflow.io/v2` **API Status:** https://status.cloudflow.io ## Authentication CloudFlow supports three authentication methods to suit different use cases and security requirements. ### API Keys API keys provide simple authentication for server-to-server communication. Include your API key in the request header: ```bash curl -H \"X-API-Key: cf_live_a1b2c3d4e5f6g7h8i9j0\" \\ https://api.cloudflow.io/v2/workflows ``` **Security Notes:** - Never expose API keys in client-side code - Rotate keys every 90 days - Use separate keys for development and production environments ### OAuth 2.0 OAuth 2.0 is recommended for applications that access CloudFlow on behalf of users. We support the Authorization Code flow with PKCE. **Authorization Endpoint:** `https://auth.cloudflow.io/oauth/authorize` **Token Endpoint:** `https://auth.cloudflow.io/oauth/token` **Supported Scopes:** - `workflows:read` - Read workflow configurations - `workflows:write` - Create and modify workflows - `pipelines:read` - Read pipeline data - `pipelines:write` - Create and manage pipelines - `analytics:read` - Access analytics and metrics - `admin:full` - Full administrative access Example authorization request: ```bash curl -X POST https://auth.cloudflow.io/oauth/token \\ -H \"Content-Type: application/x-www-form-urlencoded\" \\ -d \"grant_type=authorization_code\" \\ -d \"code=AUTH_CODE_HERE\" \\ -d \"client_id=YOUR_CLIENT_ID\" \\ -d \"client_secret=YOUR_CLIENT_SECRET\" \\ -d \"redirect_uri=https://yourapp.com/callback\" ``` ### JWT Tokens For advanced use cases, CloudFlow supports JSON Web Tokens (JWT) with RS256 signing algorithm. JWTs must include the following claims: - `iss` (issuer): Your application identifier - `sub` (subject): User or service account ID - `aud` (audience): `https://api.cloudflow.io` - `exp` (expiration): Unix timestamp (max 3600 seconds from `iat`) - `iat` (issued at): Unix timestamp - `scope`: Space-separated list of requested scopes Example JWT header: ```python import jwt import time from cryptography.hazmat.primitives import serialization from cryptography.hazmat.backends import default_backend # Load your private key with open('private_key.pem', 'rb') as key_file: private_key = serialization.load_pem_private_key( key_file.read(), password=None, backend=default_backend() ) # Create JWT payload payload = { 'iss': 'your-app-id', 'sub': 'user-12345', 'aud': 'https://api.cloudflow.io', 'exp': int(time.time()) + 3600, 'iat': int(time.time()), 'scope': 'workflows:read workflows:write' } # Generate token token = jwt.encode(payload, private_key, algorithm='RS256') # Use in API request headers = {'Authorization': f'Bearer {token}'} ``` **Token Expiration:** All tokens expire after 3600 seconds (1 hour). Implement token refresh logic in your application.",
              "score": null
            },
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_0",
              "content": "# CloudFlow Platform - Deployment and Operations Guide **Version:** 2.4.0 **Last Updated:** January 2026 **Target Environment:** Production (AWS EKS) ## Table of Contents 1. [Overview](#overview) 2. [Prerequisites](#prerequisites) 3. [Infrastructure Setup](#infrastructure-setup) 4. [Kubernetes Deployment](#kubernetes-deployment) 5. [Database Configuration](#database-configuration) 6. [Monitoring and Observability](#monitoring-and-observability) 7. [Backup and Disaster Recovery](#backup-and-disaster-recovery) 8. [Scaling and Performance](#scaling-and-performance) 9. [Security and Compliance](#security-and-compliance) 10. [Troubleshooting](#troubleshooting) --- ## Overview CloudFlow is a cloud-native workflow orchestration platform designed for high-availability production environments. This guide provides comprehensive instructions for deploying and operating CloudFlow on Amazon EKS (Elastic Kubernetes Service). ### Architecture Summary CloudFlow consists of the following components: - **API Server**: REST API for workflow management (Node.js/Express) - **Worker Service**: Background job processor (Node.js) - **Scheduler**: Cron-based task scheduler (Node.js) - **PostgreSQL**: Primary data store (version 14) - **Redis**: Cache and message queue (version 7.0) - **PgBouncer**: Database connection pooler ### Deployment Model - **Namespace**: `cloudflow-prod` - **Cluster**: AWS EKS 1.28 - **Region**: us-east-1 (primary), us-west-2 (disaster recovery) - **High Availability**: Multi-AZ deployment across 3 availability zones --- ## Prerequisites Before beginning the deployment process, ensure you have the following: ### Required Tools - `kubectl` (v1.28 or later) - `helm` (v3.12 or later) - `aws-cli` (v2.13 or later) - `eksctl` (v0.165 or later) - `terraform` (v1.6 or later) - for infrastructure provisioning ### Access Requirements - AWS account with appropriate IAM permissions - EKS cluster admin access - Container registry access (ECR) - Domain name and SSL certificates - Secrets management access (AWS Secrets Manager or Vault) ### Network Requirements - VPC with at least 3 public and 3 private subnets - NAT Gateway configured - Security groups allowing: - Ingress: HTTPS (443), HTTP (80) - Internal: PostgreSQL (5432), Redis (6379) - Monitoring: Prometheus (9090), Grafana (3000) --- ## Infrastructure Setup ### EKS Cluster Creation Use the following `eksctl` configuration to create the EKS cluster: ```yaml # cluster-config.yaml apiVersion: eksctl.io/v1alpha5 kind: ClusterConfig metadata: name: cloudflow-production region: us-east-1 version: \"1.28\" availabilityZones: - us-east-1a - us-east-1b - us-east-1c vpc: cidr: 10.0.0.0/16 nat: gateway: HighlyAvailable managedNodeGroups: - name: cloudflow-workers instanceType: m5.xlarge minSize: 3 maxSize: 10 desiredCapacity: 5 volumeSize: 100 privateNetworking: true labels: role: worker environment: production tags: Environment: production Application: cloudflow iam: withAddonPolicies: ebs: true efs: true albIngress: true cloudWatch: true cloudWatch: clusterLogging: enableTypes: - api - audit - authenticator - controllerManager - scheduler ``` Create the cluster: ```bash eksctl",
              "score": null
            },
            {
              "doc_id": "user_guide",
              "chunk_id": "user_guide_fix_0",
              "content": "# CloudFlow User Guide Welcome to CloudFlow, the modern workflow automation platform that helps you connect your apps, automate repetitive tasks, and build powerful integrations without writing code. ## Table of Contents 1. [Getting Started](#getting-started) 2. [Workflow Creation](#workflow-creation) 3. [Available Actions](#available-actions) 4. [Variables and Expressions](#variables-and-expressions) 5. [Scheduling](#scheduling) 6. [Error Handling](#error-handling) 7. [Workflow Limits](#workflow-limits) 8. [Best Practices](#best-practices) 9. [Common Workflow Patterns](#common-workflow-patterns) ## Getting Started ### Account Setup Creating your CloudFlow account takes just a few minutes: 1. Navigate to https://app.cloudflow.io/signup 2. Enter your email address and create a strong password 3. Verify your email address by clicking the link sent to your inbox 4. Complete your profile by providing your organization name and use case 5. Choose your plan (Free, Professional, or Enterprise) Once your account is created, you'll be redirected to your CloudFlow dashboard where you can start building workflows immediately. ### Your First Workflow Let's create a simple workflow that sends you a Slack notification when a new file is uploaded to your Google Drive: 1. Click the **\"Create Workflow\"** button in the top-right corner 2. Give your workflow a name: \"Google Drive to Slack Notifier\" 3. Select **\"Google Drive - New File\"** as your trigger 4. Authenticate your Google Drive account when prompted 5. Choose the folder you want to monitor 6. Click **\"Add Action\"** and select **\"Slack - Send Message\"** 7. Authenticate your Slack account 8. Select the channel where you want to receive notifications 9. Customize your message using variables: `New file uploaded: {{trigger.file.name}}` 10. Click **\"Save & Activate\"** to enable your workflow Congratulations! Your first workflow is now live. Every time a file is added to the specified Google Drive folder, you'll receive a Slack notification. ## Workflow Creation CloudFlow offers multiple ways to create and manage workflows, giving you the flexibility to work in the way that suits you best. ### Visual Editor The Visual Editor is CloudFlow's drag-and-drop interface for building workflows without code. It's perfect for users who prefer a graphical approach: **Key Features:** - Drag-and-drop action blocks from the sidebar - Visual connections between steps show your workflow logic - Inline configuration for each action - Real-time validation and error checking - Test mode to run your workflow with sample data **Using the Visual Editor:** 1.",
              "score": null
            }
          ],
          "latency_ms": 11.404043001675745,
          "query_id": "realistic_005",
          "dimension": "synonym",
          "query": "Which namespace is used for CloudFlow production?"
        },
        {
          "key_facts": [
            "cloudflow-prod"
          ],
          "found_facts": [
            "cloudflow-prod"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "deployment_guide"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_8",
              "content": "Leverage caching:** ```bash # Enable client-side caching export CLOUDFLOW_CACHE_ENABLED=true export CLOUDFLOW_CACHE_TTL=300 # Cache workflow metadata cloudflow workflows list --use-cache --cache-ttl 600 ``` --- ## Log Analysis & Debugging ### Accessing CloudFlow Logs #### Kubernetes Deployments ```bash # List all CloudFlow pods kubectl get pods -n cloudflow # Tail logs from API server kubectl logs -f -n cloudflow deployment/cloudflow-api --tail=100 # Get logs from specific pod kubectl logs -n cloudflow cloudflow-api-7d4f6b8c9d-x7k2m # Get logs from previous crashed pod kubectl logs -n cloudflow cloudflow-api-7d4f6b8c9d-x7k2m --previous # Get logs from all pods in deployment kubectl logs -n cloudflow deployment/cloudflow-api --all-containers=true # Stream logs from multiple pods kubectl logs -n cloudflow -l app=cloudflow-api -f --max-log-requests=10 ``` #### Log Levels CloudFlow supports the following log levels: - `TRACE` - Very detailed debugging information - `DEBUG` - Detailed debugging information - `INFO` - Informational messages (default) - `WARN` - Warning messages - `ERROR` - Error messages - `FATAL` - Fatal errors causing shutdown **Changing log levels:** ```bash # Set global log level cloudflow config set logging.level DEBUG # Set log level for specific component cloudflow config set logging.components.database DEBUG cloudflow config set logging.components.auth INFO cloudflow config set logging.components.workflows TRACE # Temporary log level increase (resets after 1 hour) cloudflow debug set-log-level DEBUG --duration 3600 # View current log configuration cloudflow config get logging --format json ``` ### Grep Patterns for Common Issues #### Authentication Failures ```bash # Find all authentication errors kubectl logs -n cloudflow deployment/cloudflow-api | \\ grep -i \"authentication\\|401\\|unauthorized\" | \\ tail -n 50 # Find JWT validation failures kubectl logs -n cloudflow deployment/cloudflow-api | \\ grep -E \"JWT|token.*invalid|signature.*failed\" # Find clock skew issues kubectl logs -n cloudflow deployment/cloudflow-api | \\ grep -i \"clock skew\\|time.*sync\\|nbf\\|exp\" ``` #### Database Errors ```bash # Find database connection errors kubectl logs -n cloudflow deployment/cloudflow-api | \\ grep -E \"connection.*pool|could not connect|database.*timeout\" # Find slow queries kubectl logs -n cloudflow deployment/cloudflow-api | \\ grep \"slow query\" | \\ awk '{print $NF}' | \\ sort -n | \\ tail -n 20 # Find deadlock errors kubectl logs -n cloudflow deployment/cloudflow-api | \\ grep -i \"deadlock detected\" ``` #### Workflow Execution Errors ```bash # Find workflow timeout errors kubectl logs -n cloudflow deployment/cloudflow-workflow-engine | \\ grep -E \"timeout|exceeded.*3600\" # Find workflow retry attempts kubectl logs -n cloudflow deployment/cloudflow-workflow-engine | \\ grep -E \"retry attempt [0-9]|retrying",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_9",
              "content": "in\" # Find workflow failures by ID kubectl logs -n cloudflow deployment/cloudflow-workflow-engine | \\ grep \"exec_7h3j6k9m2n\" ``` #### Rate Limiting ```bash # Find rate limit events kubectl logs -n cloudflow deployment/cloudflow-api | \\ grep -E \"429|rate.*limit|throttle\" # Count rate limit errors by hour kubectl logs -n cloudflow deployment/cloudflow-api --since=24h | \\ grep \"rate_limit_exceeded\" | \\ awk '{print $1}' | \\ cut -d'T' -f1-2 | \\ sort | uniq -c ``` ### Advanced Log Analysis #### Using jq for JSON Logs ```bash # Parse JSON logs and filter by level kubectl logs -n cloudflow deployment/cloudflow-api | \\ jq 'select(.level == \"ERROR\")' # Extract specific fields kubectl logs -n cloudflow deployment/cloudflow-api | \\ jq '{timestamp: .timestamp, level: .level, message: .message, execution_id: .context.execution_id}' # Filter by workflow ID kubectl logs -n cloudflow deployment/cloudflow-workflow-engine | \\ jq 'select(.workflow_id == \"wf_9k2n4m8p1q\")' # Count errors by type kubectl logs -n cloudflow deployment/cloudflow-api --since=1h | \\ jq -r 'select(.level == \"ERROR\") | .error_type' | \\ sort | uniq -c | sort -rn ``` #### Correlation IDs CloudFlow uses correlation IDs to trace requests across services. ```bash # Extract correlation ID from error CORRELATION_ID=\"corr_8h4j9k2m5n\" # Trace request across all services for pod in $(kubectl get pods -n cloudflow -l tier=backend -o name); do echo \"=== $pod ===\" kubectl logs -n cloudflow $pod | grep $CORRELATION_ID done # Export full trace to file cloudflow debug trace $CORRELATION_ID --output trace-$CORRELATION_ID.json ``` ### Debugging Commands #### Enable Debug Mode for Workflow Execution ```bash # Execute workflow with debug logging cloudflow workflows execute wf_9k2n4m8p1q \\ --debug \\ --log-level TRACE \\ --output-logs /tmp/workflow-debug.log # Enable step-by-step execution cloudflow workflows execute wf_9k2n4m8p1q \\ --step-mode interactive \\ --breakpoint-on-error # Capture full execution context cloudflow workflows execute wf_9k2n4m8p1q \\ --capture-context \\ --context-output /tmp/execution-context.json ``` #### Database Query Debugging ```bash # Enable query logging cloudflow db config set log_statement all cloudflow db config set log_duration on cloudflow db config set log_min_duration_statement 1000 # Log queries > 1s # Capture query plan for slow endpoint cloudflow debug capture-queries \\ --endpoint \"/api/v1/workflows/list\" \\ --duration 60 \\ --output query-analysis.txt # Analyze query performance cloudflow db analyze-performance --last 1h ``` #### Network Debugging ```bash # Test connectivity from CloudFlow pod kubectl run -n cloudflow netdebug --rm -i --tty \\ --image=nicolaka/netshoot -- /bin/bash # Inside pod: # Check DNS resolution nslookup api.cloudflow.io # Check connectivity to",
              "score": null
            },
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_7",
              "content": "podSelector: matchLabels: app: cloudflow policyTypes: - Ingress - Egress ingress: - from: - namespaceSelector: matchLabels: name: cloudflow-prod - podSelector: matchLabels: app: nginx-ingress ports: - protocol: TCP port: 3000 egress: - to: - podSelector: matchLabels: app: postgresql ports: - protocol: TCP port: 5432 - to: - podSelector: matchLabels: app: redis ports: - protocol: TCP port: 6379 ``` ### Pod Security Standards Enforce Pod Security Standards at the namespace level: ```bash kubectl label namespace cloudflow-prod \\ pod-security.kubernetes.io/enforce=restricted \\ pod-security.kubernetes.io/audit=restricted \\ pod-security.kubernetes.io/warn=restricted ``` ### Secrets Management Use AWS Secrets Manager for sensitive credentials: ```bash # Install External Secrets Operator helm install external-secrets external-secrets/external-secrets \\ --namespace external-secrets-system \\ --create-namespace # Create SecretStore kubectl apply -f - <<EOF apiVersion: external-secrets.io/v1beta1 kind: SecretStore metadata: name: aws-secrets-manager namespace: cloudflow-prod spec: provider: aws: service: SecretsManager region: us-east-1 auth: jwt: serviceAccountRef: name: external-secrets-sa EOF ``` --- ## Troubleshooting ### Common Issues #### Pods in CrashLoopBackOff **Symptoms**: Pods continuously restart **Diagnosis**: ```bash kubectl logs -n cloudflow-prod <pod-name> --previous kubectl describe pod -n cloudflow-prod <pod-name> ``` **Common Causes**: - Database connection failure - Invalid environment variables - Insufficient resources #### High Memory Usage **Symptoms**: Pods being OOMKilled **Diagnosis**: ```bash kubectl top pods -n cloudflow-prod ``` **Resolution**: - Increase memory limits in deployment - Check for memory leaks in application logs - Review heap dump for Node.js processes #### Database Connection Pool Exhausted **Symptoms**: \"Too many connections\" errors **Diagnosis**: ```bash # Check PgBouncer stats kubectl exec -it pgbouncer-xxx -n cloudflow-prod -- \\ psql -p 5432 -U pgbouncer pgbouncer -c \"SHOW POOLS;\" ``` **Resolution**: - Increase `max_db_connections` in PgBouncer - Optimize application connection usage - Add more PostgreSQL replicas ### Support Contacts - **On-call Engineer**: pagerduty.com/cloudflow-oncall - **Slack Channel**: #cloudflow-operations - **Documentation**: https://docs.cloudflow.io - **Status Page**: https://status.cloudflow.io --- ## Appendix ### Useful Commands ```bash # View all resources in namespace kubectl get all -n cloudflow-prod # Check resource usage kubectl top pods -n cloudflow-prod kubectl top nodes # View logs kubectl logs -f deployment/cloudflow-api -n cloudflow-prod # Execute command in pod kubectl exec -it <pod-name> -n cloudflow-prod -- /bin/sh # Port forward for local testing kubectl port-forward svc/cloudflow-api 8080:80 -n cloudflow-prod # Rollback deployment helm rollback cloudflow -n cloudflow-prod # Update deployment helm upgrade cloudflow cloudflow/cloudflow \\ --namespace cloudflow-prod \\ --values values-production.yaml ``` ### Maintenance Windows Scheduled maintenance occurs during the following windows: - **Primary**: Sunday 02:00-06:00",
              "score": null
            },
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_1",
              "content": "create cluster -f cluster-config.yaml ``` ### Storage Configuration Install the EBS CSI driver for persistent volumes: ```bash eksctl create addon --name aws-ebs-csi-driver \\ --cluster cloudflow-production \\ --service-account-role-arn arn:aws:iam::ACCOUNT_ID:role/AmazonEKS_EBS_CSI_DriverRole \\ --force ``` --- ## Kubernetes Deployment ### Namespace Setup Create the CloudFlow namespace and configure resource quotas: ```yaml # namespace.yaml apiVersion: v1 kind: Namespace metadata: name: cloudflow-prod labels: name: cloudflow-prod environment: production --- apiVersion: v1 kind: ResourceQuota metadata: name: cloudflow-quota namespace: cloudflow-prod spec: hard: requests.cpu: \"50\" requests.memory: 100Gi persistentvolumeclaims: \"10\" services.loadbalancers: \"2\" ``` Apply the namespace configuration: ```bash kubectl apply -f namespace.yaml ``` ### Environment Configuration CloudFlow requires the following environment variables: | Variable | Description | Example | Required | |----------|-------------|---------|----------| | `DATABASE_URL` | PostgreSQL connection string | `postgresql://user:pass@host:5432/cloudflow` | Yes | | `REDIS_URL` | Redis connection string | `redis://redis-master.cloudflow-prod.svc.cluster.local:6379` | Yes | | `JWT_SECRET` | Secret key for JWT token signing | `<generated-secret-256-bit>` | Yes | | `LOG_LEVEL` | Application log verbosity | `info`, `debug`, `warn`, `error` | No (default: `info`) | | `API_PORT` | API server port | `3000` | No (default: `3000`) | | `WORKER_CONCURRENCY` | Number of concurrent workers | `10` | No (default: `5`) | | `SESSION_SECRET` | Session encryption key | `<generated-secret-256-bit>` | Yes | | `AWS_REGION` | AWS region for services | `us-east-1` | Yes | | `S3_BUCKET` | S3 bucket for file storage | `cloudflow-prod-storage` | Yes | | `SMTP_HOST` | Email server hostname | `smtp.sendgrid.net` | No | | `SMTP_PORT` | Email server port | `587` | No | | `METRICS_ENABLED` | Enable Prometheus metrics | `true` | No (default: `false`) | Create a Kubernetes secret for sensitive variables: ```bash kubectl create secret generic cloudflow-secrets \\ --namespace cloudflow-prod \\ --from-literal=DATABASE_URL=\"postgresql://cloudflow:$(cat db-password.txt)@pgbouncer.cloudflow-prod.svc.cluster.local:5432/cloudflow\" \\ --from-literal=REDIS_URL=\"redis://redis-master.cloudflow-prod.svc.cluster.local:6379\" \\ --from-literal=JWT_SECRET=\"$(openssl rand -base64 32)\" \\ --from-literal=SESSION_SECRET=\"$(openssl rand -base64 32)\" ``` ### Helm Chart Deployment Add the CloudFlow Helm repository: ```bash helm repo add cloudflow https://charts.cloudflow.io helm repo update ``` Create a values file for production configuration: ```yaml # values-production.yaml replicaCount: 3 image: repository: 123456789012.dkr.ecr.us-east-1.amazonaws.com/cloudflow tag: \"2.4.0\" pullPolicy: IfNotPresent resources: limits: cpu: 2000m memory: 4Gi requests: cpu: 2000m memory: 4Gi autoscaling: enabled: true minReplicas: 3 maxReplicas: 10 targetCPUUtilizationPercentage: 70 targetMemoryUtilizationPercentage: 80 service: type: ClusterIP port: 80 targetPort: 3000 annotations: service.beta.kubernetes.io/aws-load-balancer-type: \"nlb\" ingress: enabled: true className: nginx annotations: cert-manager.io/cluster-issuer: \"letsencrypt-prod\" nginx.ingress.kubernetes.io/ssl-redirect: \"true\" hosts: - host: api.cloudflow.io paths: - path: / pathType: Prefix tls:",
              "score": null
            },
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_0",
              "content": "# CloudFlow Platform - Deployment and Operations Guide **Version:** 2.4.0 **Last Updated:** January 2026 **Target Environment:** Production (AWS EKS) ## Table of Contents 1. [Overview](#overview) 2. [Prerequisites](#prerequisites) 3. [Infrastructure Setup](#infrastructure-setup) 4. [Kubernetes Deployment](#kubernetes-deployment) 5. [Database Configuration](#database-configuration) 6. [Monitoring and Observability](#monitoring-and-observability) 7. [Backup and Disaster Recovery](#backup-and-disaster-recovery) 8. [Scaling and Performance](#scaling-and-performance) 9. [Security and Compliance](#security-and-compliance) 10. [Troubleshooting](#troubleshooting) --- ## Overview CloudFlow is a cloud-native workflow orchestration platform designed for high-availability production environments. This guide provides comprehensive instructions for deploying and operating CloudFlow on Amazon EKS (Elastic Kubernetes Service). ### Architecture Summary CloudFlow consists of the following components: - **API Server**: REST API for workflow management (Node.js/Express) - **Worker Service**: Background job processor (Node.js) - **Scheduler**: Cron-based task scheduler (Node.js) - **PostgreSQL**: Primary data store (version 14) - **Redis**: Cache and message queue (version 7.0) - **PgBouncer**: Database connection pooler ### Deployment Model - **Namespace**: `cloudflow-prod` - **Cluster**: AWS EKS 1.28 - **Region**: us-east-1 (primary), us-west-2 (disaster recovery) - **High Availability**: Multi-AZ deployment across 3 availability zones --- ## Prerequisites Before beginning the deployment process, ensure you have the following: ### Required Tools - `kubectl` (v1.28 or later) - `helm` (v3.12 or later) - `aws-cli` (v2.13 or later) - `eksctl` (v0.165 or later) - `terraform` (v1.6 or later) - for infrastructure provisioning ### Access Requirements - AWS account with appropriate IAM permissions - EKS cluster admin access - Container registry access (ECR) - Domain name and SSL certificates - Secrets management access (AWS Secrets Manager or Vault) ### Network Requirements - VPC with at least 3 public and 3 private subnets - NAT Gateway configured - Security groups allowing: - Ingress: HTTPS (443), HTTP (80) - Internal: PostgreSQL (5432), Redis (6379) - Monitoring: Prometheus (9090), Grafana (3000) --- ## Infrastructure Setup ### EKS Cluster Creation Use the following `eksctl` configuration to create the EKS cluster: ```yaml # cluster-config.yaml apiVersion: eksctl.io/v1alpha5 kind: ClusterConfig metadata: name: cloudflow-production region: us-east-1 version: \"1.28\" availabilityZones: - us-east-1a - us-east-1b - us-east-1c vpc: cidr: 10.0.0.0/16 nat: gateway: HighlyAvailable managedNodeGroups: - name: cloudflow-workers instanceType: m5.xlarge minSize: 3 maxSize: 10 desiredCapacity: 5 volumeSize: 100 privateNetworking: true labels: role: worker environment: production tags: Environment: production Application: cloudflow iam: withAddonPolicies: ebs: true efs: true albIngress: true cloudWatch: true cloudWatch: clusterLogging: enableTypes: - api - audit - authenticator - controllerManager - scheduler ``` Create the cluster: ```bash eksctl",
              "score": null
            }
          ],
          "latency_ms": 11.444068000855623,
          "query_id": "realistic_005",
          "dimension": "problem",
          "query": "I can't find the production pods"
        },
        {
          "key_facts": [
            "cloudflow-prod"
          ],
          "found_facts": [
            "cloudflow-prod"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "deployment_guide"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_7",
              "content": "podSelector: matchLabels: app: cloudflow policyTypes: - Ingress - Egress ingress: - from: - namespaceSelector: matchLabels: name: cloudflow-prod - podSelector: matchLabels: app: nginx-ingress ports: - protocol: TCP port: 3000 egress: - to: - podSelector: matchLabels: app: postgresql ports: - protocol: TCP port: 5432 - to: - podSelector: matchLabels: app: redis ports: - protocol: TCP port: 6379 ``` ### Pod Security Standards Enforce Pod Security Standards at the namespace level: ```bash kubectl label namespace cloudflow-prod \\ pod-security.kubernetes.io/enforce=restricted \\ pod-security.kubernetes.io/audit=restricted \\ pod-security.kubernetes.io/warn=restricted ``` ### Secrets Management Use AWS Secrets Manager for sensitive credentials: ```bash # Install External Secrets Operator helm install external-secrets external-secrets/external-secrets \\ --namespace external-secrets-system \\ --create-namespace # Create SecretStore kubectl apply -f - <<EOF apiVersion: external-secrets.io/v1beta1 kind: SecretStore metadata: name: aws-secrets-manager namespace: cloudflow-prod spec: provider: aws: service: SecretsManager region: us-east-1 auth: jwt: serviceAccountRef: name: external-secrets-sa EOF ``` --- ## Troubleshooting ### Common Issues #### Pods in CrashLoopBackOff **Symptoms**: Pods continuously restart **Diagnosis**: ```bash kubectl logs -n cloudflow-prod <pod-name> --previous kubectl describe pod -n cloudflow-prod <pod-name> ``` **Common Causes**: - Database connection failure - Invalid environment variables - Insufficient resources #### High Memory Usage **Symptoms**: Pods being OOMKilled **Diagnosis**: ```bash kubectl top pods -n cloudflow-prod ``` **Resolution**: - Increase memory limits in deployment - Check for memory leaks in application logs - Review heap dump for Node.js processes #### Database Connection Pool Exhausted **Symptoms**: \"Too many connections\" errors **Diagnosis**: ```bash # Check PgBouncer stats kubectl exec -it pgbouncer-xxx -n cloudflow-prod -- \\ psql -p 5432 -U pgbouncer pgbouncer -c \"SHOW POOLS;\" ``` **Resolution**: - Increase `max_db_connections` in PgBouncer - Optimize application connection usage - Add more PostgreSQL replicas ### Support Contacts - **On-call Engineer**: pagerduty.com/cloudflow-oncall - **Slack Channel**: #cloudflow-operations - **Documentation**: https://docs.cloudflow.io - **Status Page**: https://status.cloudflow.io --- ## Appendix ### Useful Commands ```bash # View all resources in namespace kubectl get all -n cloudflow-prod # Check resource usage kubectl top pods -n cloudflow-prod kubectl top nodes # View logs kubectl logs -f deployment/cloudflow-api -n cloudflow-prod # Execute command in pod kubectl exec -it <pod-name> -n cloudflow-prod -- /bin/sh # Port forward for local testing kubectl port-forward svc/cloudflow-api 8080:80 -n cloudflow-prod # Rollback deployment helm rollback cloudflow -n cloudflow-prod # Update deployment helm upgrade cloudflow cloudflow/cloudflow \\ --namespace cloudflow-prod \\ --values values-production.yaml ``` ### Maintenance Windows Scheduled maintenance occurs during the following windows: - **Primary**: Sunday 02:00-06:00",
              "score": null
            },
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_1",
              "content": "create cluster -f cluster-config.yaml ``` ### Storage Configuration Install the EBS CSI driver for persistent volumes: ```bash eksctl create addon --name aws-ebs-csi-driver \\ --cluster cloudflow-production \\ --service-account-role-arn arn:aws:iam::ACCOUNT_ID:role/AmazonEKS_EBS_CSI_DriverRole \\ --force ``` --- ## Kubernetes Deployment ### Namespace Setup Create the CloudFlow namespace and configure resource quotas: ```yaml # namespace.yaml apiVersion: v1 kind: Namespace metadata: name: cloudflow-prod labels: name: cloudflow-prod environment: production --- apiVersion: v1 kind: ResourceQuota metadata: name: cloudflow-quota namespace: cloudflow-prod spec: hard: requests.cpu: \"50\" requests.memory: 100Gi persistentvolumeclaims: \"10\" services.loadbalancers: \"2\" ``` Apply the namespace configuration: ```bash kubectl apply -f namespace.yaml ``` ### Environment Configuration CloudFlow requires the following environment variables: | Variable | Description | Example | Required | |----------|-------------|---------|----------| | `DATABASE_URL` | PostgreSQL connection string | `postgresql://user:pass@host:5432/cloudflow` | Yes | | `REDIS_URL` | Redis connection string | `redis://redis-master.cloudflow-prod.svc.cluster.local:6379` | Yes | | `JWT_SECRET` | Secret key for JWT token signing | `<generated-secret-256-bit>` | Yes | | `LOG_LEVEL` | Application log verbosity | `info`, `debug`, `warn`, `error` | No (default: `info`) | | `API_PORT` | API server port | `3000` | No (default: `3000`) | | `WORKER_CONCURRENCY` | Number of concurrent workers | `10` | No (default: `5`) | | `SESSION_SECRET` | Session encryption key | `<generated-secret-256-bit>` | Yes | | `AWS_REGION` | AWS region for services | `us-east-1` | Yes | | `S3_BUCKET` | S3 bucket for file storage | `cloudflow-prod-storage` | Yes | | `SMTP_HOST` | Email server hostname | `smtp.sendgrid.net` | No | | `SMTP_PORT` | Email server port | `587` | No | | `METRICS_ENABLED` | Enable Prometheus metrics | `true` | No (default: `false`) | Create a Kubernetes secret for sensitive variables: ```bash kubectl create secret generic cloudflow-secrets \\ --namespace cloudflow-prod \\ --from-literal=DATABASE_URL=\"postgresql://cloudflow:$(cat db-password.txt)@pgbouncer.cloudflow-prod.svc.cluster.local:5432/cloudflow\" \\ --from-literal=REDIS_URL=\"redis://redis-master.cloudflow-prod.svc.cluster.local:6379\" \\ --from-literal=JWT_SECRET=\"$(openssl rand -base64 32)\" \\ --from-literal=SESSION_SECRET=\"$(openssl rand -base64 32)\" ``` ### Helm Chart Deployment Add the CloudFlow Helm repository: ```bash helm repo add cloudflow https://charts.cloudflow.io helm repo update ``` Create a values file for production configuration: ```yaml # values-production.yaml replicaCount: 3 image: repository: 123456789012.dkr.ecr.us-east-1.amazonaws.com/cloudflow tag: \"2.4.0\" pullPolicy: IfNotPresent resources: limits: cpu: 2000m memory: 4Gi requests: cpu: 2000m memory: 4Gi autoscaling: enabled: true minReplicas: 3 maxReplicas: 10 targetCPUUtilizationPercentage: 70 targetMemoryUtilizationPercentage: 80 service: type: ClusterIP port: 80 targetPort: 3000 annotations: service.beta.kubernetes.io/aws-load-balancer-type: \"nlb\" ingress: enabled: true className: nginx annotations: cert-manager.io/cluster-issuer: \"letsencrypt-prod\" nginx.ingress.kubernetes.io/ssl-redirect: \"true\" hosts: - host: api.cloudflow.io paths: - path: / pathType: Prefix tls:",
              "score": null
            },
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_5",
              "content": "Configure log aggregation: ```bash # Install Fluent Bit for log forwarding helm install fluent-bit fluent/fluent-bit \\ --namespace logging \\ --create-namespace \\ --set cloudWatch.enabled=true \\ --set cloudWatch.region=us-east-1 \\ --set cloudWatch.logGroupName=/aws/eks/cloudflow-production/application ``` Query logs using CloudWatch Insights: ```sql fields @timestamp, @message, level, requestId, userId | filter namespace = \"cloudflow-prod\" | filter level = \"error\" | sort @timestamp desc | limit 100 ``` --- ## Backup and Disaster Recovery ### Database Backup Strategy CloudFlow implements a comprehensive backup strategy with the following retention policy: - **Daily snapshots**: Retained for 30 days - **Weekly backups**: Retained for 90 days - **Monthly backups**: Retained for 1 year #### Automated Backup with Velero Install Velero for cluster-wide backups: ```bash velero install \\ --provider aws \\ --plugins velero/velero-plugin-for-aws:v1.8.0 \\ --bucket cloudflow-velero-backups \\ --backup-location-config region=us-east-1 \\ --snapshot-location-config region=us-east-1 \\ --secret-file ./credentials-velero ``` Create a daily backup schedule: ```yaml # backup-schedule.yaml apiVersion: velero.io/v1 kind: Schedule metadata: name: cloudflow-daily-backup namespace: velero spec: schedule: \"0 2 * * *\" # 2 AM UTC daily template: includedNamespaces: - cloudflow-prod ttl: 720h0m0s # 30 days storageLocation: default snapshotVolumes: true ``` Apply the backup schedule: ```bash kubectl apply -f backup-schedule.yaml ``` #### PostgreSQL Backup Configure PostgreSQL continuous archiving with WAL-G: ```bash # Create backup cronjob kubectl apply -f - <<EOF apiVersion: batch/v1 kind: CronJob metadata: name: postgres-backup namespace: cloudflow-prod spec: schedule: \"0 1 * * *\" # 1 AM UTC daily jobTemplate: spec: template: spec: containers: - name: backup image: postgres:14 env: - name: PGPASSWORD valueFrom: secretKeyRef: name: postgres-credentials key: password command: - /bin/bash - -c - | TIMESTAMP=\\$(date +%Y%m%d_%H%M%S) pg_dump -h postgresql.cloudflow-prod.svc.cluster.local \\ -U cloudflow -d cloudflow \\ | gzip > /backups/cloudflow_\\${TIMESTAMP}.sql.gz aws s3 cp /backups/cloudflow_\\${TIMESTAMP}.sql.gz \\ s3://cloudflow-db-backups/daily/ volumeMounts: - name: backup-storage mountPath: /backups volumes: - name: backup-storage emptyDir: {} restartPolicy: OnFailure EOF ``` ### Disaster Recovery Procedure In case of catastrophic failure, follow these steps to restore CloudFlow: 1. **Restore EKS Cluster** (if necessary): ```bash eksctl create cluster -f cluster-config.yaml ``` 2. **Restore Velero Backup**: ```bash # List available backups velero backup get # Restore from backup velero restore create --from-backup cloudflow-daily-backup-20260124 ``` 3. **Restore Database**: ```bash # Download backup from S3 aws s3 cp s3://cloudflow-db-backups/daily/cloudflow_20260124_010000.sql.gz . # Restore database gunzip -c cloudflow_20260124_010000.sql.gz | \\ psql -h postgresql.cloudflow-prod.svc.cluster.local -U cloudflow -d cloudflow ``` 4.",
              "score": null
            },
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_2",
              "content": "- secretName: cloudflow-tls hosts: - api.cloudflow.io healthCheck: enabled: true livenessProbe: path: /health initialDelaySeconds: 30 periodSeconds: 10 timeoutSeconds: 5 failureThreshold: 3 readinessProbe: path: /ready initialDelaySeconds: 10 periodSeconds: 5 timeoutSeconds: 3 failureThreshold: 3 env: - name: NODE_ENV value: \"production\" - name: LOG_LEVEL value: \"info\" - name: API_PORT value: \"3000\" - name: WORKER_CONCURRENCY value: \"10\" - name: AWS_REGION value: \"us-east-1\" - name: METRICS_ENABLED value: \"true\" envFrom: - secretRef: name: cloudflow-secrets podAnnotations: prometheus.io/scrape: \"true\" prometheus.io/port: \"9090\" prometheus.io/path: \"/metrics\" podSecurityContext: runAsNonRoot: true runAsUser: 1000 fsGroup: 1000 securityContext: allowPrivilegeEscalation: false capabilities: drop: - ALL readOnlyRootFilesystem: true persistence: enabled: true storageClass: gp3 accessMode: ReadWriteOnce size: 50Gi redis: enabled: true architecture: replication auth: enabled: true master: persistence: size: 20Gi replica: replicaCount: 2 persistence: size: 20Gi ``` Deploy CloudFlow using Helm: ```bash helm install cloudflow cloudflow/cloudflow \\ --namespace cloudflow-prod \\ --values values-production.yaml \\ --wait \\ --timeout 10m ``` ### Deployment Verification Verify the deployment status: ```bash # Check pod status kubectl get pods -n cloudflow-prod # Expected output: # NAME READY STATUS RESTARTS AGE # cloudflow-api-7d8f9c5b6d-4xk2p 1/1 Running 0 2m # cloudflow-api-7d8f9c5b6d-9hj5m 1/1 Running 0 2m # cloudflow-api-7d8f9c5b6d-tn8wq 1/1 Running 0 2m # Check service endpoints kubectl get svc -n cloudflow-prod # Check ingress kubectl get ingress -n cloudflow-prod ``` Test health endpoints: ```bash # Port forward for testing kubectl port-forward -n cloudflow-prod svc/cloudflow-api 8080:80 # Test health endpoint curl http://localhost:8080/health # Expected: {\"status\":\"healthy\",\"timestamp\":\"2026-01-24T10:30:00Z\"} # Test readiness endpoint curl http://localhost:8080/ready # Expected: {\"status\":\"ready\",\"dependencies\":{\"database\":\"connected\",\"redis\":\"connected\"}} ``` --- ## Database Configuration ### PostgreSQL Setup CloudFlow uses PostgreSQL 14 as its primary data store.",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_5",
              "content": "Success/failure events published back to Kafka ``` **Event Schema**: ```json { \"event_id\": \"uuid-v4\", \"event_type\": \"workflow.execution.completed\", \"timestamp\": \"2026-01-15T10:30:00.000Z\", \"correlation_id\": \"request-trace-id\", \"payload\": { \"workflow_id\": \"wf-12345\", \"execution_id\": \"exec-67890\", \"status\": \"COMPLETED\", \"duration_ms\": 4230 }, \"metadata\": { \"source_service\": \"workflow-engine\", \"schema_version\": \"1.0\" } } ``` ### Inter-Service Communication Services communicate using two primary patterns: **Synchronous (gRPC)**: - Auth Service validation calls from API Gateway - User Service profile lookups - Low latency requirement (< 50ms) - Request-response pattern with circuit breaker **Asynchronous (Kafka)**: - Workflow execution events - Notification triggers - Audit log events - Eventual consistency acceptable --- ## Database Architecture ### PostgreSQL Primary Database **Cluster Configuration**: - Primary-replica setup with 1 primary + 2 read replicas - Instance type: db.r6g.2xlarge (8 vCPU, 64GB RAM) - Storage: 2TB gp3 SSD with 12,000 IOPS - Multi-AZ deployment for high availability - Automated backups: Daily snapshots, 30-day retention - Point-in-time recovery: 5-minute granularity **Database Schema Design**: ``` Core Tables: - users (5M rows): User accounts and profiles - workflows (2M rows): Workflow definitions - workflow_executions (500M rows, partitioned): Execution history - workflow_steps (2B rows, partitioned): Individual step records - schedules (100K rows): Scheduled workflow triggers - notifications (1B rows, partitioned): Notification delivery log Partitioning Strategy: - workflow_executions: Monthly partitions by created_at - workflow_steps: Monthly partitions by created_at - notifications: Weekly partitions by created_at - Automatic partition creation via cron job - Retention policy: Drop partitions older than 12 months ``` **Indexing Strategy**: - Primary keys: UUIDs with B-tree indexes - Frequently queried columns: Compound indexes (e.g., user_id + status + created_at) - JSONB columns: GIN indexes for workflow definitions - Full-text search: GiST indexes on description fields **Connection Pooling**: - PgBouncer in transaction mode - Pool size: 100 connections per service - Max client connections: 2000 - Connection timeout: 30 seconds **Performance Characteristics**: - Read query P99: < 20ms - Write query P99: < 50ms - Transaction throughput: 15,000 TPS - Replication lag: < 500ms --- ### Redis Caching Layer **Cluster Configuration**: - Redis Cluster with 6 nodes (3 primary + 3 replica) - Instance type: cache.r6g.xlarge (4 vCPU, 26GB RAM) - Total memory: 78GB usable cache space - Persistence: RDB snapshots every 5 minutes + AOF - Eviction policy: allkeys-lru **Cache Usage Patterns**: 1.",
              "score": null
            }
          ],
          "latency_ms": 11.640403001365485,
          "query_id": "realistic_005",
          "dimension": "casual",
          "query": "prod namespace"
        },
        {
          "key_facts": [
            "cloudflow-prod"
          ],
          "found_facts": [
            "cloudflow-prod"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "deployment_guide"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_1",
              "content": "create cluster -f cluster-config.yaml ``` ### Storage Configuration Install the EBS CSI driver for persistent volumes: ```bash eksctl create addon --name aws-ebs-csi-driver \\ --cluster cloudflow-production \\ --service-account-role-arn arn:aws:iam::ACCOUNT_ID:role/AmazonEKS_EBS_CSI_DriverRole \\ --force ``` --- ## Kubernetes Deployment ### Namespace Setup Create the CloudFlow namespace and configure resource quotas: ```yaml # namespace.yaml apiVersion: v1 kind: Namespace metadata: name: cloudflow-prod labels: name: cloudflow-prod environment: production --- apiVersion: v1 kind: ResourceQuota metadata: name: cloudflow-quota namespace: cloudflow-prod spec: hard: requests.cpu: \"50\" requests.memory: 100Gi persistentvolumeclaims: \"10\" services.loadbalancers: \"2\" ``` Apply the namespace configuration: ```bash kubectl apply -f namespace.yaml ``` ### Environment Configuration CloudFlow requires the following environment variables: | Variable | Description | Example | Required | |----------|-------------|---------|----------| | `DATABASE_URL` | PostgreSQL connection string | `postgresql://user:pass@host:5432/cloudflow` | Yes | | `REDIS_URL` | Redis connection string | `redis://redis-master.cloudflow-prod.svc.cluster.local:6379` | Yes | | `JWT_SECRET` | Secret key for JWT token signing | `<generated-secret-256-bit>` | Yes | | `LOG_LEVEL` | Application log verbosity | `info`, `debug`, `warn`, `error` | No (default: `info`) | | `API_PORT` | API server port | `3000` | No (default: `3000`) | | `WORKER_CONCURRENCY` | Number of concurrent workers | `10` | No (default: `5`) | | `SESSION_SECRET` | Session encryption key | `<generated-secret-256-bit>` | Yes | | `AWS_REGION` | AWS region for services | `us-east-1` | Yes | | `S3_BUCKET` | S3 bucket for file storage | `cloudflow-prod-storage` | Yes | | `SMTP_HOST` | Email server hostname | `smtp.sendgrid.net` | No | | `SMTP_PORT` | Email server port | `587` | No | | `METRICS_ENABLED` | Enable Prometheus metrics | `true` | No (default: `false`) | Create a Kubernetes secret for sensitive variables: ```bash kubectl create secret generic cloudflow-secrets \\ --namespace cloudflow-prod \\ --from-literal=DATABASE_URL=\"postgresql://cloudflow:$(cat db-password.txt)@pgbouncer.cloudflow-prod.svc.cluster.local:5432/cloudflow\" \\ --from-literal=REDIS_URL=\"redis://redis-master.cloudflow-prod.svc.cluster.local:6379\" \\ --from-literal=JWT_SECRET=\"$(openssl rand -base64 32)\" \\ --from-literal=SESSION_SECRET=\"$(openssl rand -base64 32)\" ``` ### Helm Chart Deployment Add the CloudFlow Helm repository: ```bash helm repo add cloudflow https://charts.cloudflow.io helm repo update ``` Create a values file for production configuration: ```yaml # values-production.yaml replicaCount: 3 image: repository: 123456789012.dkr.ecr.us-east-1.amazonaws.com/cloudflow tag: \"2.4.0\" pullPolicy: IfNotPresent resources: limits: cpu: 2000m memory: 4Gi requests: cpu: 2000m memory: 4Gi autoscaling: enabled: true minReplicas: 3 maxReplicas: 10 targetCPUUtilizationPercentage: 70 targetMemoryUtilizationPercentage: 80 service: type: ClusterIP port: 80 targetPort: 3000 annotations: service.beta.kubernetes.io/aws-load-balancer-type: \"nlb\" ingress: enabled: true className: nginx annotations: cert-manager.io/cluster-issuer: \"letsencrypt-prod\" nginx.ingress.kubernetes.io/ssl-redirect: \"true\" hosts: - host: api.cloudflow.io paths: - path: / pathType: Prefix tls:",
              "score": null
            },
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_3",
              "content": "Deploy PostgreSQL using the Bitnami Helm chart: ```yaml # postgres-values.yaml global: postgresql: auth: username: cloudflow database: cloudflow existingSecret: postgres-credentials image: tag: \"14.10.0\" primary: resources: limits: cpu: 4000m memory: 8Gi requests: cpu: 2000m memory: 4Gi persistence: enabled: true size: 100Gi storageClass: gp3 extendedConfiguration: | max_connections = 100 shared_buffers = 2GB effective_cache_size = 6GB maintenance_work_mem = 512MB checkpoint_completion_target = 0.9 wal_buffers = 16MB default_statistics_target = 100 random_page_cost = 1.1 effective_io_concurrency = 200 work_mem = 10MB min_wal_size = 1GB max_wal_size = 4GB readReplicas: replicaCount: 2 resources: limits: cpu: 2000m memory: 4Gi requests: cpu: 1000m memory: 2Gi metrics: enabled: true serviceMonitor: enabled: true ``` Install PostgreSQL: ```bash # Create password secret kubectl create secret generic postgres-credentials \\ --namespace cloudflow-prod \\ --from-literal=postgres-password=\"$(openssl rand -base64 32)\" \\ --from-literal=password=\"$(openssl rand -base64 32)\" # Install PostgreSQL helm install postgresql bitnami/postgresql \\ --namespace cloudflow-prod \\ --values postgres-values.yaml \\ --wait ``` ### PgBouncer Configuration Deploy PgBouncer for connection pooling to handle up to 100 connections efficiently: ```yaml # pgbouncer.yaml apiVersion: v1 kind: ConfigMap metadata: name: pgbouncer-config namespace: cloudflow-prod data: pgbouncer.ini: | [databases] cloudflow = host=postgresql.cloudflow-prod.svc.cluster.local port=5432 dbname=cloudflow [pgbouncer] listen_addr = 0.0.0.0 listen_port = 5432 auth_type = md5 auth_file = /etc/pgbouncer/userlist.txt pool_mode = transaction max_client_conn = 1000 default_pool_size = 25 reserve_pool_size = 5 reserve_pool_timeout = 3 max_db_connections = 100 max_user_connections = 100 server_lifetime = 3600 server_idle_timeout = 600 log_connections = 1 log_disconnections = 1 log_pooler_errors = 1 --- apiVersion: apps/v1 kind: Deployment metadata: name: pgbouncer namespace: cloudflow-prod spec: replicas: 2 selector: matchLabels: app: pgbouncer template: metadata: labels: app: pgbouncer spec: containers: - name: pgbouncer image: pgbouncer/pgbouncer:1.21.0 ports: - containerPort: 5432 resources: requests: cpu: 500m memory: 512Mi limits: cpu: 1000m memory: 1Gi volumeMounts: - name: config mountPath: /etc/pgbouncer volumes: - name: config configMap: name: pgbouncer-config --- apiVersion: v1 kind: Service metadata: name: pgbouncer namespace: cloudflow-prod spec: selector: app: pgbouncer ports: - port: 5432 targetPort: 5432 type: ClusterIP ``` Apply the PgBouncer configuration: ```bash kubectl apply -f pgbouncer.yaml ``` ### Database Migrations Run database migrations before deploying new versions: ```bash # Create migration job kubectl create job cloudflow-migrate-$(date +%s) \\ --namespace cloudflow-prod \\ --image=123456789012.dkr.ecr.us-east-1.amazonaws.com/cloudflow:2.4.0 \\ -- npm run migrate # Monitor migration progress kubectl logs -f job/cloudflow-migrate-<timestamp> -n cloudflow-prod ``` --- ## Monitoring and Observability ### Prometheus Setup Install Prometheus using the kube-prometheus-stack: ```bash helm install prometheus prometheus-community/kube-prometheus-stack \\ --namespace monitoring \\ --create-namespace \\ --set prometheus.prometheusSpec.serviceMonitorSelectorNilUsesHelmValues=false \\",
              "score": null
            },
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_7",
              "content": "podSelector: matchLabels: app: cloudflow policyTypes: - Ingress - Egress ingress: - from: - namespaceSelector: matchLabels: name: cloudflow-prod - podSelector: matchLabels: app: nginx-ingress ports: - protocol: TCP port: 3000 egress: - to: - podSelector: matchLabels: app: postgresql ports: - protocol: TCP port: 5432 - to: - podSelector: matchLabels: app: redis ports: - protocol: TCP port: 6379 ``` ### Pod Security Standards Enforce Pod Security Standards at the namespace level: ```bash kubectl label namespace cloudflow-prod \\ pod-security.kubernetes.io/enforce=restricted \\ pod-security.kubernetes.io/audit=restricted \\ pod-security.kubernetes.io/warn=restricted ``` ### Secrets Management Use AWS Secrets Manager for sensitive credentials: ```bash # Install External Secrets Operator helm install external-secrets external-secrets/external-secrets \\ --namespace external-secrets-system \\ --create-namespace # Create SecretStore kubectl apply -f - <<EOF apiVersion: external-secrets.io/v1beta1 kind: SecretStore metadata: name: aws-secrets-manager namespace: cloudflow-prod spec: provider: aws: service: SecretsManager region: us-east-1 auth: jwt: serviceAccountRef: name: external-secrets-sa EOF ``` --- ## Troubleshooting ### Common Issues #### Pods in CrashLoopBackOff **Symptoms**: Pods continuously restart **Diagnosis**: ```bash kubectl logs -n cloudflow-prod <pod-name> --previous kubectl describe pod -n cloudflow-prod <pod-name> ``` **Common Causes**: - Database connection failure - Invalid environment variables - Insufficient resources #### High Memory Usage **Symptoms**: Pods being OOMKilled **Diagnosis**: ```bash kubectl top pods -n cloudflow-prod ``` **Resolution**: - Increase memory limits in deployment - Check for memory leaks in application logs - Review heap dump for Node.js processes #### Database Connection Pool Exhausted **Symptoms**: \"Too many connections\" errors **Diagnosis**: ```bash # Check PgBouncer stats kubectl exec -it pgbouncer-xxx -n cloudflow-prod -- \\ psql -p 5432 -U pgbouncer pgbouncer -c \"SHOW POOLS;\" ``` **Resolution**: - Increase `max_db_connections` in PgBouncer - Optimize application connection usage - Add more PostgreSQL replicas ### Support Contacts - **On-call Engineer**: pagerduty.com/cloudflow-oncall - **Slack Channel**: #cloudflow-operations - **Documentation**: https://docs.cloudflow.io - **Status Page**: https://status.cloudflow.io --- ## Appendix ### Useful Commands ```bash # View all resources in namespace kubectl get all -n cloudflow-prod # Check resource usage kubectl top pods -n cloudflow-prod kubectl top nodes # View logs kubectl logs -f deployment/cloudflow-api -n cloudflow-prod # Execute command in pod kubectl exec -it <pod-name> -n cloudflow-prod -- /bin/sh # Port forward for local testing kubectl port-forward svc/cloudflow-api 8080:80 -n cloudflow-prod # Rollback deployment helm rollback cloudflow -n cloudflow-prod # Update deployment helm upgrade cloudflow cloudflow/cloudflow \\ --namespace cloudflow-prod \\ --values values-production.yaml ``` ### Maintenance Windows Scheduled maintenance occurs during the following windows: - **Primary**: Sunday 02:00-06:00",
              "score": null
            },
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_5",
              "content": "Configure log aggregation: ```bash # Install Fluent Bit for log forwarding helm install fluent-bit fluent/fluent-bit \\ --namespace logging \\ --create-namespace \\ --set cloudWatch.enabled=true \\ --set cloudWatch.region=us-east-1 \\ --set cloudWatch.logGroupName=/aws/eks/cloudflow-production/application ``` Query logs using CloudWatch Insights: ```sql fields @timestamp, @message, level, requestId, userId | filter namespace = \"cloudflow-prod\" | filter level = \"error\" | sort @timestamp desc | limit 100 ``` --- ## Backup and Disaster Recovery ### Database Backup Strategy CloudFlow implements a comprehensive backup strategy with the following retention policy: - **Daily snapshots**: Retained for 30 days - **Weekly backups**: Retained for 90 days - **Monthly backups**: Retained for 1 year #### Automated Backup with Velero Install Velero for cluster-wide backups: ```bash velero install \\ --provider aws \\ --plugins velero/velero-plugin-for-aws:v1.8.0 \\ --bucket cloudflow-velero-backups \\ --backup-location-config region=us-east-1 \\ --snapshot-location-config region=us-east-1 \\ --secret-file ./credentials-velero ``` Create a daily backup schedule: ```yaml # backup-schedule.yaml apiVersion: velero.io/v1 kind: Schedule metadata: name: cloudflow-daily-backup namespace: velero spec: schedule: \"0 2 * * *\" # 2 AM UTC daily template: includedNamespaces: - cloudflow-prod ttl: 720h0m0s # 30 days storageLocation: default snapshotVolumes: true ``` Apply the backup schedule: ```bash kubectl apply -f backup-schedule.yaml ``` #### PostgreSQL Backup Configure PostgreSQL continuous archiving with WAL-G: ```bash # Create backup cronjob kubectl apply -f - <<EOF apiVersion: batch/v1 kind: CronJob metadata: name: postgres-backup namespace: cloudflow-prod spec: schedule: \"0 1 * * *\" # 1 AM UTC daily jobTemplate: spec: template: spec: containers: - name: backup image: postgres:14 env: - name: PGPASSWORD valueFrom: secretKeyRef: name: postgres-credentials key: password command: - /bin/bash - -c - | TIMESTAMP=\\$(date +%Y%m%d_%H%M%S) pg_dump -h postgresql.cloudflow-prod.svc.cluster.local \\ -U cloudflow -d cloudflow \\ | gzip > /backups/cloudflow_\\${TIMESTAMP}.sql.gz aws s3 cp /backups/cloudflow_\\${TIMESTAMP}.sql.gz \\ s3://cloudflow-db-backups/daily/ volumeMounts: - name: backup-storage mountPath: /backups volumes: - name: backup-storage emptyDir: {} restartPolicy: OnFailure EOF ``` ### Disaster Recovery Procedure In case of catastrophic failure, follow these steps to restore CloudFlow: 1. **Restore EKS Cluster** (if necessary): ```bash eksctl create cluster -f cluster-config.yaml ``` 2. **Restore Velero Backup**: ```bash # List available backups velero backup get # Restore from backup velero restore create --from-backup cloudflow-daily-backup-20260124 ``` 3. **Restore Database**: ```bash # Download backup from S3 aws s3 cp s3://cloudflow-db-backups/daily/cloudflow_20260124_010000.sql.gz . # Restore database gunzip -c cloudflow_20260124_010000.sql.gz | \\ psql -h postgresql.cloudflow-prod.svc.cluster.local -U cloudflow -d cloudflow ``` 4.",
              "score": null
            },
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_8",
              "content": "UTC - **Secondary**: Wednesday 10:00-12:00 UTC All deployments should target these windows to minimize user impact. --- **Document Version**: 2.4.0 **Maintained by**: Platform Engineering Team **Last Review**: January 24, 2026",
              "score": null
            }
          ],
          "latency_ms": 11.971330000960734,
          "query_id": "realistic_005",
          "dimension": "contextual",
          "query": "I need to deploy to production, what namespace should I use?"
        },
        {
          "key_facts": [
            "cloudflow-prod"
          ],
          "found_facts": [
            "cloudflow-prod"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "deployment_guide"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_7",
              "content": "podSelector: matchLabels: app: cloudflow policyTypes: - Ingress - Egress ingress: - from: - namespaceSelector: matchLabels: name: cloudflow-prod - podSelector: matchLabels: app: nginx-ingress ports: - protocol: TCP port: 3000 egress: - to: - podSelector: matchLabels: app: postgresql ports: - protocol: TCP port: 5432 - to: - podSelector: matchLabels: app: redis ports: - protocol: TCP port: 6379 ``` ### Pod Security Standards Enforce Pod Security Standards at the namespace level: ```bash kubectl label namespace cloudflow-prod \\ pod-security.kubernetes.io/enforce=restricted \\ pod-security.kubernetes.io/audit=restricted \\ pod-security.kubernetes.io/warn=restricted ``` ### Secrets Management Use AWS Secrets Manager for sensitive credentials: ```bash # Install External Secrets Operator helm install external-secrets external-secrets/external-secrets \\ --namespace external-secrets-system \\ --create-namespace # Create SecretStore kubectl apply -f - <<EOF apiVersion: external-secrets.io/v1beta1 kind: SecretStore metadata: name: aws-secrets-manager namespace: cloudflow-prod spec: provider: aws: service: SecretsManager region: us-east-1 auth: jwt: serviceAccountRef: name: external-secrets-sa EOF ``` --- ## Troubleshooting ### Common Issues #### Pods in CrashLoopBackOff **Symptoms**: Pods continuously restart **Diagnosis**: ```bash kubectl logs -n cloudflow-prod <pod-name> --previous kubectl describe pod -n cloudflow-prod <pod-name> ``` **Common Causes**: - Database connection failure - Invalid environment variables - Insufficient resources #### High Memory Usage **Symptoms**: Pods being OOMKilled **Diagnosis**: ```bash kubectl top pods -n cloudflow-prod ``` **Resolution**: - Increase memory limits in deployment - Check for memory leaks in application logs - Review heap dump for Node.js processes #### Database Connection Pool Exhausted **Symptoms**: \"Too many connections\" errors **Diagnosis**: ```bash # Check PgBouncer stats kubectl exec -it pgbouncer-xxx -n cloudflow-prod -- \\ psql -p 5432 -U pgbouncer pgbouncer -c \"SHOW POOLS;\" ``` **Resolution**: - Increase `max_db_connections` in PgBouncer - Optimize application connection usage - Add more PostgreSQL replicas ### Support Contacts - **On-call Engineer**: pagerduty.com/cloudflow-oncall - **Slack Channel**: #cloudflow-operations - **Documentation**: https://docs.cloudflow.io - **Status Page**: https://status.cloudflow.io --- ## Appendix ### Useful Commands ```bash # View all resources in namespace kubectl get all -n cloudflow-prod # Check resource usage kubectl top pods -n cloudflow-prod kubectl top nodes # View logs kubectl logs -f deployment/cloudflow-api -n cloudflow-prod # Execute command in pod kubectl exec -it <pod-name> -n cloudflow-prod -- /bin/sh # Port forward for local testing kubectl port-forward svc/cloudflow-api 8080:80 -n cloudflow-prod # Rollback deployment helm rollback cloudflow -n cloudflow-prod # Update deployment helm upgrade cloudflow cloudflow/cloudflow \\ --namespace cloudflow-prod \\ --values values-production.yaml ``` ### Maintenance Windows Scheduled maintenance occurs during the following windows: - **Primary**: Sunday 02:00-06:00",
              "score": null
            },
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_5",
              "content": "Configure log aggregation: ```bash # Install Fluent Bit for log forwarding helm install fluent-bit fluent/fluent-bit \\ --namespace logging \\ --create-namespace \\ --set cloudWatch.enabled=true \\ --set cloudWatch.region=us-east-1 \\ --set cloudWatch.logGroupName=/aws/eks/cloudflow-production/application ``` Query logs using CloudWatch Insights: ```sql fields @timestamp, @message, level, requestId, userId | filter namespace = \"cloudflow-prod\" | filter level = \"error\" | sort @timestamp desc | limit 100 ``` --- ## Backup and Disaster Recovery ### Database Backup Strategy CloudFlow implements a comprehensive backup strategy with the following retention policy: - **Daily snapshots**: Retained for 30 days - **Weekly backups**: Retained for 90 days - **Monthly backups**: Retained for 1 year #### Automated Backup with Velero Install Velero for cluster-wide backups: ```bash velero install \\ --provider aws \\ --plugins velero/velero-plugin-for-aws:v1.8.0 \\ --bucket cloudflow-velero-backups \\ --backup-location-config region=us-east-1 \\ --snapshot-location-config region=us-east-1 \\ --secret-file ./credentials-velero ``` Create a daily backup schedule: ```yaml # backup-schedule.yaml apiVersion: velero.io/v1 kind: Schedule metadata: name: cloudflow-daily-backup namespace: velero spec: schedule: \"0 2 * * *\" # 2 AM UTC daily template: includedNamespaces: - cloudflow-prod ttl: 720h0m0s # 30 days storageLocation: default snapshotVolumes: true ``` Apply the backup schedule: ```bash kubectl apply -f backup-schedule.yaml ``` #### PostgreSQL Backup Configure PostgreSQL continuous archiving with WAL-G: ```bash # Create backup cronjob kubectl apply -f - <<EOF apiVersion: batch/v1 kind: CronJob metadata: name: postgres-backup namespace: cloudflow-prod spec: schedule: \"0 1 * * *\" # 1 AM UTC daily jobTemplate: spec: template: spec: containers: - name: backup image: postgres:14 env: - name: PGPASSWORD valueFrom: secretKeyRef: name: postgres-credentials key: password command: - /bin/bash - -c - | TIMESTAMP=\\$(date +%Y%m%d_%H%M%S) pg_dump -h postgresql.cloudflow-prod.svc.cluster.local \\ -U cloudflow -d cloudflow \\ | gzip > /backups/cloudflow_\\${TIMESTAMP}.sql.gz aws s3 cp /backups/cloudflow_\\${TIMESTAMP}.sql.gz \\ s3://cloudflow-db-backups/daily/ volumeMounts: - name: backup-storage mountPath: /backups volumes: - name: backup-storage emptyDir: {} restartPolicy: OnFailure EOF ``` ### Disaster Recovery Procedure In case of catastrophic failure, follow these steps to restore CloudFlow: 1. **Restore EKS Cluster** (if necessary): ```bash eksctl create cluster -f cluster-config.yaml ``` 2. **Restore Velero Backup**: ```bash # List available backups velero backup get # Restore from backup velero restore create --from-backup cloudflow-daily-backup-20260124 ``` 3. **Restore Database**: ```bash # Download backup from S3 aws s3 cp s3://cloudflow-db-backups/daily/cloudflow_20260124_010000.sql.gz . # Restore database gunzip -c cloudflow_20260124_010000.sql.gz | \\ psql -h postgresql.cloudflow-prod.svc.cluster.local -U cloudflow -d cloudflow ``` 4.",
              "score": null
            },
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_4",
              "content": "### Error Response Format ```json { \"error\": { \"code\": \"invalid_parameter\", \"message\": \"The 'limit' parameter must be between 1 and 100\", \"field\": \"limit\", \"request_id\": \"req_8k3m9x2p\" } } ``` ### HTTP Status Codes | Status Code | Description | Common Causes | |------------|-------------|---------------| | 400 | Bad Request | Invalid parameters, malformed JSON, validation errors | | 401 | Unauthorized | Missing or invalid authentication credentials | | 403 | Forbidden | Insufficient permissions for requested resource | | 404 | Not Found | Resource does not exist or has been deleted | | 429 | Too Many Requests | Rate limit exceeded, retry after specified period | | 500 | Internal Server Error | Unexpected server error, contact support if persists | | 502 | Bad Gateway | Temporary service issue, retry with exponential backoff | | 503 | Service Unavailable | Scheduled maintenance or temporary outage | ### Error Codes CloudFlow returns specific error codes to help you identify and resolve issues: - `invalid_parameter`: One or more request parameters are invalid - `missing_required_field`: Required field is missing from request body - `authentication_failed`: Invalid API key or token - `insufficient_permissions`: User lacks required scope or permission - `resource_not_found`: Requested resource does not exist - `rate_limit_exceeded`: Too many requests, see rate limiting section - `workflow_execution_failed`: Workflow execution encountered an error - `invalid_json`: Request body contains malformed JSON - `duplicate_resource`: Resource with same identifier already exists - `quota_exceeded`: Account quota limit reached ### Error Handling Best Practices **Python Example:** ```python import requests import time def make_api_request(url, headers, max_retries=3): for attempt in range(max_retries): try: response = requests.get(url, headers=headers) if response.status_code == 200: return response.json() elif response.status_code == 429: # Rate limit exceeded retry_after = int(response.headers.get('X-RateLimit-Reset', 60)) print(f\"Rate limited. Waiting {retry_after} seconds...\") time.sleep(retry_after) continue elif response.status_code >= 500: # Server error - retry with exponential backoff wait_time = 2 ** attempt print(f\"Server error. Retrying in {wait_time} seconds...\") time.sleep(wait_time) continue else: # Client error - don't retry error_data = response.json() raise Exception(f\"API Error: {error_data['error']['message']}\") except requests.exceptions.RequestException as e: print(f\"Request failed: {e}\") if attempt < max_retries - 1: time.sleep(2 ** attempt) else: raise raise Exception(\"Max retries exceeded\") ``` ## Webhooks CloudFlow can send webhook notifications when specific events occur in your workflows.",
              "score": null
            },
            {
              "doc_id": "user_guide",
              "chunk_id": "user_guide_fix_2",
              "content": "### HTTP Requests Make HTTP requests to any API endpoint: **Configuration:** - **Method**: GET, POST, PUT, PATCH, DELETE, HEAD - **URL**: Full endpoint URL (supports variable interpolation) - **Headers**: Custom headers as key-value pairs - **Query Parameters**: URL parameters - **Body**: JSON, form data, or raw text - **Authentication**: Basic Auth, Bearer Token, API Key, OAuth 2.0 **Example:** ```yaml - id: fetch_user action: http_request config: method: GET url: \"https://api.example.com/users/{{user_id}}\" headers: Authorization: \"Bearer {{secrets.API_TOKEN}}\" Content-Type: \"application/json\" timeout: 30 ``` **Response Handling:** Access the response in subsequent steps: - `{{steps.fetch_user.status}}` - HTTP status code - `{{steps.fetch_user.body}}` - Response body - `{{steps.fetch_user.headers}}` - Response headers ### Database Queries Execute queries against supported databases (PostgreSQL, MySQL, MongoDB, Redis): **SQL Databases (PostgreSQL, MySQL):** ```yaml - id: get_orders action: database_query config: connection: \"{{secrets.DB_CONNECTION_STRING}}\" query: | SELECT * FROM orders WHERE customer_id = $1 AND status = $2 ORDER BY created_at DESC LIMIT 10 parameters: - \"{{trigger.customer_id}}\" - \"pending\" ``` **MongoDB:** ```yaml - id: find_documents action: mongodb_query config: connection: \"{{secrets.MONGO_URI}}\" database: \"production\" collection: \"users\" operation: find filter: email: \"{{trigger.email}}\" options: limit: 1 ``` **Important Notes:** - Always use parameterized queries to prevent SQL injection - Connection strings should be stored in secrets - Query timeout is 30 seconds by default - Maximum result set size is 10MB ### Email Notifications Send emails via SMTP or integrated email providers: **Configuration:** ```yaml - id: send_notification action: email config: provider: \"smtp\" # or \"sendgrid\", \"mailgun\", \"ses\" from: \"notifications@cloudflow.io\" to: \"{{trigger.recipient}}\" cc: \"manager@company.com\" subject: \"Alert: {{alert_type}}\" body: | Hello {{user.name}}, This is an automated notification about {{event_description}}. Details: - Time: {{timestamp}} - Type: {{alert_type}} - Priority: {{priority}} Best regards, CloudFlow Automation html: true attachments: - name: \"report.pdf\" content: \"{{steps.generate_report.output}}\" encoding: \"base64\" ``` **Email Templates:** CloudFlow supports dynamic templates with conditional content: ``` {{#if priority == 'high'}} \u26a0\ufe0f URGENT: Immediate attention required {{else}} \u2139\ufe0f Informational notification {{/if}} ``` ### Slack Messages Send messages to Slack channels or users: **Channel Messages:** ```yaml - id: notify_team action: slack_message config: channel: \"#deployments\" text: \"Deployment completed successfully!\" blocks: - type: \"section\" text: type: \"mrkdwn\" text: \"*Deployment Summary*\\nEnvironment: {{environment}}\\nVersion: {{version}}\\nDuration: {{duration}}s\" - type: \"actions\" elements: - type: \"button\" text: \"View Logs\" url: \"{{logs_url}}\" thread_ts: \"{{trigger.thread_ts}}\" # Reply to thread ``` **Direct Messages:** ```yaml - id: dm_user action: slack_message config: user: \"{{user_slack_id}}\" text: \"Your report is ready for review.\"",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_11",
              "content": "outage\" # Create war room cloudflow incident war-room create incident-2024012401 ``` **For SEV-2 (High):** ```bash # Create incident and notify cloudflow incident create \\ --severity SEV-2 \\ --title \"High rate of workflow failures\" \\ --description \"Workflow success rate dropped to 75%\" \\ --notify-oncall # Update status page cloudflow status update \\ --status \"partial_outage\" \\ --message \"We are experiencing elevated error rates\" ``` **For SEV-3/SEV-4:** ```bash # Create support ticket cloudflow support ticket create \\ --priority MEDIUM \\ --subject \"Intermittent timeout errors\" \\ --description \"See attached logs and reproduction steps\" ``` ### Contact Information - **On-call hotline:** +1-888-CLOUDFLOW (24/7) - **Support email:** support@cloudflow.io - **Incident Slack channel:** #cloudflow-incidents - **Status page:** https://status.cloudflow.io - **Documentation:** https://docs.cloudflow.io ### Post-Incident Review After resolving incidents, complete a postmortem: ```bash # Generate postmortem template cloudflow incident postmortem incident-2024012401 \\ --template standard \\ --output postmortem-2024012401.md # Required sections: # 1. Incident summary # 2. Impact assessment # 3. Timeline of events # 4. Root cause analysis # 5. Resolution steps # 6. Action items # 7. Lessons learned ``` --- ## Additional Resources - **CloudFlow Documentation:** https://docs.cloudflow.io - **API Reference:** https://api-docs.cloudflow.io - **Community Forum:** https://community.cloudflow.io - **GitHub Issues:** https://github.com/cloudflow/platform/issues - **Training Portal:** https://training.cloudflow.io - **Monitoring Dashboard:** https://monitoring.cloudflow.io ### Getting Help If this troubleshooting guide doesn't resolve your issue: 1. Search the knowledge base: `cloudflow kb search \"your issue\"` 2. Check community forum for similar issues 3. Contact support with detailed logs and reproduction steps 4. For urgent issues, use emergency escalation procedures **Remember:** Always capture logs, metrics, and reproduction steps before escalating! --- *Last updated: January 24, 2026* *Document version: 3.2.1* *Feedback: docs-feedback@cloudflow.io*",
              "score": null
            }
          ],
          "latency_ms": 11.642837998806499,
          "query_id": "realistic_005",
          "dimension": "negation",
          "query": "Why can't I see resources in the default namespace?"
        },
        {
          "key_facts": [
            "2 vCPU, 4GB RAM per pod"
          ],
          "found_facts": [],
          "missed_facts": [
            "2 vCPU, 4GB RAM per pod"
          ],
          "coverage": 0.0,
          "expected_docs": [
            "architecture_overview"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_4",
              "content": "### Error Response Format ```json { \"error\": { \"code\": \"invalid_parameter\", \"message\": \"The 'limit' parameter must be between 1 and 100\", \"field\": \"limit\", \"request_id\": \"req_8k3m9x2p\" } } ``` ### HTTP Status Codes | Status Code | Description | Common Causes | |------------|-------------|---------------| | 400 | Bad Request | Invalid parameters, malformed JSON, validation errors | | 401 | Unauthorized | Missing or invalid authentication credentials | | 403 | Forbidden | Insufficient permissions for requested resource | | 404 | Not Found | Resource does not exist or has been deleted | | 429 | Too Many Requests | Rate limit exceeded, retry after specified period | | 500 | Internal Server Error | Unexpected server error, contact support if persists | | 502 | Bad Gateway | Temporary service issue, retry with exponential backoff | | 503 | Service Unavailable | Scheduled maintenance or temporary outage | ### Error Codes CloudFlow returns specific error codes to help you identify and resolve issues: - `invalid_parameter`: One or more request parameters are invalid - `missing_required_field`: Required field is missing from request body - `authentication_failed`: Invalid API key or token - `insufficient_permissions`: User lacks required scope or permission - `resource_not_found`: Requested resource does not exist - `rate_limit_exceeded`: Too many requests, see rate limiting section - `workflow_execution_failed`: Workflow execution encountered an error - `invalid_json`: Request body contains malformed JSON - `duplicate_resource`: Resource with same identifier already exists - `quota_exceeded`: Account quota limit reached ### Error Handling Best Practices **Python Example:** ```python import requests import time def make_api_request(url, headers, max_retries=3): for attempt in range(max_retries): try: response = requests.get(url, headers=headers) if response.status_code == 200: return response.json() elif response.status_code == 429: # Rate limit exceeded retry_after = int(response.headers.get('X-RateLimit-Reset', 60)) print(f\"Rate limited. Waiting {retry_after} seconds...\") time.sleep(retry_after) continue elif response.status_code >= 500: # Server error - retry with exponential backoff wait_time = 2 ** attempt print(f\"Server error. Retrying in {wait_time} seconds...\") time.sleep(wait_time) continue else: # Client error - don't retry error_data = response.json() raise Exception(f\"API Error: {error_data['error']['message']}\") except requests.exceptions.RequestException as e: print(f\"Request failed: {e}\") if attempt < max_retries - 1: time.sleep(2 ** attempt) else: raise raise Exception(\"Max retries exceeded\") ``` ## Webhooks CloudFlow can send webhook notifications when specific events occur in your workflows.",
              "score": null
            },
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_0",
              "content": "# CloudFlow API Reference Version 2.1.0 | Last Updated: January 2026 ## Overview The CloudFlow API is a RESTful service that enables developers to programmatically manage cloud workflows, data pipelines, and automation tasks. This documentation provides comprehensive details on authentication, endpoints, request/response formats, error handling, and best practices. **Base URL:** `https://api.cloudflow.io/v2` **API Status:** https://status.cloudflow.io ## Authentication CloudFlow supports three authentication methods to suit different use cases and security requirements. ### API Keys API keys provide simple authentication for server-to-server communication. Include your API key in the request header: ```bash curl -H \"X-API-Key: cf_live_a1b2c3d4e5f6g7h8i9j0\" \\ https://api.cloudflow.io/v2/workflows ``` **Security Notes:** - Never expose API keys in client-side code - Rotate keys every 90 days - Use separate keys for development and production environments ### OAuth 2.0 OAuth 2.0 is recommended for applications that access CloudFlow on behalf of users. We support the Authorization Code flow with PKCE. **Authorization Endpoint:** `https://auth.cloudflow.io/oauth/authorize` **Token Endpoint:** `https://auth.cloudflow.io/oauth/token` **Supported Scopes:** - `workflows:read` - Read workflow configurations - `workflows:write` - Create and modify workflows - `pipelines:read` - Read pipeline data - `pipelines:write` - Create and manage pipelines - `analytics:read` - Access analytics and metrics - `admin:full` - Full administrative access Example authorization request: ```bash curl -X POST https://auth.cloudflow.io/oauth/token \\ -H \"Content-Type: application/x-www-form-urlencoded\" \\ -d \"grant_type=authorization_code\" \\ -d \"code=AUTH_CODE_HERE\" \\ -d \"client_id=YOUR_CLIENT_ID\" \\ -d \"client_secret=YOUR_CLIENT_SECRET\" \\ -d \"redirect_uri=https://yourapp.com/callback\" ``` ### JWT Tokens For advanced use cases, CloudFlow supports JSON Web Tokens (JWT) with RS256 signing algorithm. JWTs must include the following claims: - `iss` (issuer): Your application identifier - `sub` (subject): User or service account ID - `aud` (audience): `https://api.cloudflow.io` - `exp` (expiration): Unix timestamp (max 3600 seconds from `iat`) - `iat` (issued at): Unix timestamp - `scope`: Space-separated list of requested scopes Example JWT header: ```python import jwt import time from cryptography.hazmat.primitives import serialization from cryptography.hazmat.backends import default_backend # Load your private key with open('private_key.pem', 'rb') as key_file: private_key = serialization.load_pem_private_key( key_file.read(), password=None, backend=default_backend() ) # Create JWT payload payload = { 'iss': 'your-app-id', 'sub': 'user-12345', 'aud': 'https://api.cloudflow.io', 'exp': int(time.time()) + 3600, 'iat': int(time.time()), 'scope': 'workflows:read workflows:write' } # Generate token token = jwt.encode(payload, private_key, algorithm='RS256') # Use in API request headers = {'Authorization': f'Bearer {token}'} ``` **Token Expiration:** All tokens expire after 3600 seconds (1 hour). Implement token refresh logic in your application.",
              "score": null
            },
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_0",
              "content": "# CloudFlow Platform - Deployment and Operations Guide **Version:** 2.4.0 **Last Updated:** January 2026 **Target Environment:** Production (AWS EKS) ## Table of Contents 1. [Overview](#overview) 2. [Prerequisites](#prerequisites) 3. [Infrastructure Setup](#infrastructure-setup) 4. [Kubernetes Deployment](#kubernetes-deployment) 5. [Database Configuration](#database-configuration) 6. [Monitoring and Observability](#monitoring-and-observability) 7. [Backup and Disaster Recovery](#backup-and-disaster-recovery) 8. [Scaling and Performance](#scaling-and-performance) 9. [Security and Compliance](#security-and-compliance) 10. [Troubleshooting](#troubleshooting) --- ## Overview CloudFlow is a cloud-native workflow orchestration platform designed for high-availability production environments. This guide provides comprehensive instructions for deploying and operating CloudFlow on Amazon EKS (Elastic Kubernetes Service). ### Architecture Summary CloudFlow consists of the following components: - **API Server**: REST API for workflow management (Node.js/Express) - **Worker Service**: Background job processor (Node.js) - **Scheduler**: Cron-based task scheduler (Node.js) - **PostgreSQL**: Primary data store (version 14) - **Redis**: Cache and message queue (version 7.0) - **PgBouncer**: Database connection pooler ### Deployment Model - **Namespace**: `cloudflow-prod` - **Cluster**: AWS EKS 1.28 - **Region**: us-east-1 (primary), us-west-2 (disaster recovery) - **High Availability**: Multi-AZ deployment across 3 availability zones --- ## Prerequisites Before beginning the deployment process, ensure you have the following: ### Required Tools - `kubectl` (v1.28 or later) - `helm` (v3.12 or later) - `aws-cli` (v2.13 or later) - `eksctl` (v0.165 or later) - `terraform` (v1.6 or later) - for infrastructure provisioning ### Access Requirements - AWS account with appropriate IAM permissions - EKS cluster admin access - Container registry access (ECR) - Domain name and SSL certificates - Secrets management access (AWS Secrets Manager or Vault) ### Network Requirements - VPC with at least 3 public and 3 private subnets - NAT Gateway configured - Security groups allowing: - Ingress: HTTPS (443), HTTP (80) - Internal: PostgreSQL (5432), Redis (6379) - Monitoring: Prometheus (9090), Grafana (3000) --- ## Infrastructure Setup ### EKS Cluster Creation Use the following `eksctl` configuration to create the EKS cluster: ```yaml # cluster-config.yaml apiVersion: eksctl.io/v1alpha5 kind: ClusterConfig metadata: name: cloudflow-production region: us-east-1 version: \"1.28\" availabilityZones: - us-east-1a - us-east-1b - us-east-1c vpc: cidr: 10.0.0.0/16 nat: gateway: HighlyAvailable managedNodeGroups: - name: cloudflow-workers instanceType: m5.xlarge minSize: 3 maxSize: 10 desiredCapacity: 5 volumeSize: 100 privateNetworking: true labels: role: worker environment: production tags: Environment: production Application: cloudflow iam: withAddonPolicies: ebs: true efs: true albIngress: true cloudWatch: true cloudWatch: clusterLogging: enableTypes: - api - audit - authenticator - controllerManager - scheduler ``` Create the cluster: ```bash eksctl",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_0",
              "content": "# CloudFlow Platform Troubleshooting Guide **Version:** 3.2.1 **Last Updated:** January 2026 **Audience:** Platform Engineers, DevOps, Support Teams ## Table of Contents 1. [Overview](#overview) 2. [Authentication & Authorization Issues](#authentication--authorization-issues) 3. [Performance Problems](#performance-problems) 4. [Database Connection Issues](#database-connection-issues) 5. [Workflow Execution Failures](#workflow-execution-failures) 6. [Rate Limiting & Throttling](#rate-limiting--throttling) 7. [Log Analysis & Debugging](#log-analysis--debugging) 8. [Escalation Procedures](#escalation-procedures) --- ## Overview This guide provides comprehensive troubleshooting steps for common CloudFlow platform issues encountered in production environments. Each section includes error symptoms, root cause analysis, resolution steps, and preventive measures. ### Quick Diagnostic Checklist Before diving into specific issues, perform these initial checks: - Verify service health: `cloudflow status --all` - Check API connectivity: `curl -I https://api.cloudflow.io/health` - Review recent deployments: `kubectl get deployments -n cloudflow --sort-by=.metadata.creationTimestamp` - Inspect platform metrics: `cloudflow metrics --last 1h` --- ## Authentication & Authorization Issues ### 401 Unauthorized Errors #### Symptoms - API requests return `401 Unauthorized` - Error message: `Authentication credentials were not provided or are invalid` - Frontend displays \"Session expired\" message #### Common Causes **1. Token Expiration** CloudFlow access tokens expire after 3600 seconds (1 hour) by default. Refresh tokens are valid for 30 days. **Verification:** ```bash # Decode JWT to check expiration echo $CF_ACCESS_TOKEN | cut -d'.' -f2 | base64 -d | jq '.exp' # Compare with current time date +%s ``` **Resolution:** ```bash # Refresh the access token curl -X POST https://api.cloudflow.io/auth/refresh \\ -H \"Content-Type: application/json\" \\ -d '{\"refresh_token\": \"'$CF_REFRESH_TOKEN'\"}' # Update environment variable export CF_ACCESS_TOKEN=\"<new_token>\" ``` **2. Invalid JWT Signature** Error: `JWT signature verification failed` **Causes:** - Token was modified or corrupted - Using wrong signing key - Token generated with different secret **Resolution:** ```bash # Validate token structure cloudflow auth validate --token $CF_ACCESS_TOKEN # Generate new token cloudflow auth login --username <user> --password <pass> # For service accounts cloudflow auth service-account --create --name <sa-name> --scopes \"workflow:read,workflow:write\" ``` **3. Clock Skew Issues** JWT validation fails when system clocks are out of sync (tolerance: \u00b1300 seconds).",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_7",
              "content": "records: 500 - Session timeout: 30 seconds --- ## Message Queue Patterns ### Pub/Sub Pattern Used for broadcasting events to multiple interested consumers: ``` Workflow Engine publishes workflow.execution.completed \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u25bc \u25bc \u25bc \u25bc Analytics Notification Audit Logger Billing Service Service Service Service ``` Each service maintains its own consumer group and processes events independently. Failures in one consumer don't affect others. ### Request-Reply Pattern Synchronous communication over async messaging (used sparingly): ``` API Gateway publishes request \u2192 Kafka (reply-to: temp-queue-123) \u2502 \u25bc Service processes request \u2502 \u25bc Service publishes response \u2192 temp-queue-123 \u2502 \u25bc API Gateway receives response ``` Timeout: 5 seconds, fallback to direct HTTP call if no response. ### Saga Pattern Distributed transaction management for multi-service workflows: ``` Step 1: Create Order \u2192 SUCCESS \u2192 Step 2: Reserve Inventory \u2502 FAILURE \u2502 \u25bc Compensating Transaction \u2502 \u25bc Cancel Order (Rollback) ``` Orchestrated by Workflow Engine with compensation logic defined in workflow DSL. ### Dead Letter Queue (DLQ) Failed messages after all retry attempts are routed to DLQ: ``` Message processing fails (3 retries with exponential backoff) \u2502 \u25bc Publish to dead-letter-queue topic \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u25bc \u25bc Alert to PagerDuty Store in PostgreSQL for analysis \u2502 \u2502 \u25bc \u25bc Manual investigation Automated pattern detection ``` DLQ Processing SLA: < 4 hours for critical events, < 24 hours for non-critical. --- ## Caching Strategy ### Cache-Aside Pattern (Lazy Loading) Primary caching pattern used across all services: ``` 1. Application checks cache (Redis GET) 2. If HIT \u2192 Return cached data 3. If MISS: a. Query database (PostgreSQL) b. Store result in cache with TTL c. Return data to application ``` **Implementation Example** (Workflow Definition): ```javascript async function getWorkflowDefinition(workflowId) { const cacheKey = `workflow:def:${workflowId}`; // Step 1: Check cache let workflow = await redis.get(cacheKey); if (workflow) { metrics.increment('cache.hit.workflow_def'); return JSON.parse(workflow); } // Step 2: Cache miss - query database metrics.increment('cache.miss.workflow_def'); workflow = await db.query( 'SELECT * FROM workflows WHERE id = $1', [workflowId] ); // Step 3: Store in cache (1 hour TTL) await redis.setex(cacheKey, 3600, JSON.stringify(workflow)); return workflow; } ``` ### Write-Through Pattern Used for critical data where cache consistency is paramount: ``` 1. Application writes to database 2. Database transaction commits 3. Application updates cache 4. Return success to client ``` Applied to: User profiles, authentication sessions, system configuration.",
              "score": null
            }
          ],
          "latency_ms": 11.482098001579288,
          "query_id": "realistic_006",
          "dimension": "original",
          "query": "What are the resource requirements for the API Gateway?"
        },
        {
          "key_facts": [
            "2 vCPU, 4GB RAM per pod"
          ],
          "found_facts": [
            "2 vCPU, 4GB RAM per pod"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "architecture_overview"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_1",
              "content": "**Technology**: Node.js with Express.js framework **Replicas**: 12 pods (production), auto-scaling 8-20 based on CPU **Resource Allocation**: 2 vCPU, 4GB RAM per pod **Key Responsibilities**: - JWT token validation (delegated to Auth Service for initial validation) - Rate limiting: 1000 requests per minute per API key (sliding window) - Request/response transformation and validation using JSON Schema - Routing to downstream services based on URL path patterns - CORS handling for web clients - Request/response logging and correlation ID injection **Critical Endpoints**: - `POST /api/v1/workflows` - Create new workflow - `GET /api/v1/workflows/:id` - Retrieve workflow status - `POST /api/v1/workflows/:id/execute` - Trigger workflow execution - `GET /api/v1/workflows/:id/history` - Get execution history **Dependencies**: - Auth Service (for token validation) - Redis (for rate limiting counters) - All downstream microservices **Performance Targets**: - P50 latency: < 50ms - P99 latency: < 200ms - Throughput: 10,000 RPS sustained --- ### Auth Service **Purpose**: Centralized authentication and authorization service handling user identity, token generation, and permission validation. **Technology**: Go with gRPC for internal communication, REST for external **Replicas**: 8 pods (production), auto-scaling 6-12 **Resource Allocation**: 1 vCPU, 2GB RAM per pod **Key Responsibilities**: - User authentication via multiple providers (OAuth2, SAML, local credentials) - JWT token generation and validation (RS256 algorithm) - Role-based access control (RBAC) with fine-grained permissions - Session management with Redis-backed storage - API key generation and validation for service accounts - MFA enforcement for administrative operations **Authentication Flow**: ``` Client Request \u2192 API Gateway \u2192 Auth Service \u2502 \u251c\u2500 Validate credentials \u251c\u2500 Check MFA if required \u251c\u2500 Generate JWT (15min expiry) \u251c\u2500 Generate refresh token (7 days) \u2514\u2500 Store session in Redis ``` **Token Structure**: - Access Token: JWT with 15-minute expiry - Refresh Token: Opaque token with 7-day expiry, stored in PostgreSQL - Claims: user_id, email, roles[], permissions[], tenant_id **Security Features**: - Password hashing: Argon2id with 64MB memory, 4 iterations - Token rotation on refresh to prevent replay attacks - Brute force protection: 5 failed attempts \u2192 15-minute lockout - Secrets stored in HashiCorp Vault, rotated every 90 days **Performance Targets**: - Token validation: < 10ms (P99) - Token generation: < 50ms (P99) - Throughput: 5,000 RPS for validation operations --- ### Workflow Engine **Purpose**: Core orchestration service that executes workflow definitions, manages state transitions, and coordinates task execution across distributed systems.",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_4",
              "content": "**Technology**: Node.js with worker pool pattern **Replicas**: 8 pods (production), auto-scaling 6-16 **Resource Allocation**: 2 vCPU, 4GB RAM per pod **Key Responsibilities**: - Consume notification events from Kafka - Template rendering with Handlebars (cached in Redis) - Multi-channel delivery (Email via SendGrid, SMS via Twilio, Webhooks) - Retry logic with dead letter queue for failed deliveries - Delivery status tracking and analytics - User preference management (opt-in/opt-out) **Notification Flow**: ``` Event Source \u2192 Kafka (notifications.email topic) \u2502 \u25bc Notification Service Consumer \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u25bc \u25bc \u25bc [Email Queue] [SMS Queue] [Webhook Queue] \u2502 \u2502 \u2502 \u25bc \u25bc \u25bc [SendGrid] [Twilio] [HTTP POST] \u2502 \u2502 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u25bc Delivery Status \u2192 PostgreSQL ``` **Channel Configuration**: - Email: SendGrid with IP warming, DKIM/SPF configured - SMS: Twilio with fallback to AWS SNS - Webhook: Signed payloads (HMAC-SHA256), retry 3 times with exponential backoff - In-app: WebSocket connections via Socket.io **Template Management**: - Templates stored in PostgreSQL with versioning - Compiled templates cached in Redis (1-hour TTL) - Support for localization (i18n) with 12 languages - A/B testing capability for email campaigns **Performance Targets**: - Email delivery: < 5 seconds (P99) from event to SendGrid - SMS delivery: < 3 seconds (P99) - Throughput: 10,000 notifications per minute --- ## Data Flow Architecture ### Synchronous Request Flow Client requests follow a synchronous path through the API Gateway to backend services: ``` 1. Client \u2192 Load Balancer (TLS termination) 2. Load Balancer \u2192 API Gateway (HTTP/2) 3. API Gateway \u2192 Auth Service (JWT validation via gRPC) 4. API Gateway \u2192 Target Service (HTTP/REST or gRPC) 5. Target Service \u2192 Database/Cache (data retrieval) 6. Response propagates back through the chain ``` **Timeout Configuration**: - Client \u2192 Load Balancer: 60 seconds - Load Balancer \u2192 API Gateway: 55 seconds - API Gateway \u2192 Services: 45 seconds - Service \u2192 Database: 10 seconds ### Asynchronous Event Flow Long-running operations and inter-service communication use event-driven patterns: ``` 1. Service publishes event \u2192 Kafka topic 2. Kafka persists event (replication factor: 3) 3. Consumer groups subscribe to topics 4. Consumers process events with at-least-once delivery 5. State updates written to PostgreSQL 6.",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_10",
              "content": "- Execution start rate: 500 per second - Completion rate: 450 per second (average 2-second execution time) **Database**: - Read throughput: 50,000 queries per second (across replicas) - Write throughput: 15,000 transactions per second - Connection capacity: 2,000 concurrent connections **Kafka**: - Message ingestion: 100,000 messages per second - Consumer throughput: 80,000 messages per second (aggregated) - End-to-end latency: < 100ms (P99) ### Resource Utilization **CPU Utilization** (Target: 60-70% average): - API Gateway: 55% average, 80% peak - Workflow Engine: 65% average, 85% peak - Auth Service: 40% average, 60% peak **Memory Utilization** (Target: < 80%): - API Gateway: 2.5GB average per pod (4GB allocated) - Workflow Engine: 6GB average per pod (8GB allocated) - Notification Service: 2.8GB average per pod (4GB allocated) **Network Throughput**: - Ingress: 2 Gbps average, 5 Gbps peak - Egress: 1.5 Gbps average, 4 Gbps peak - Internal (service mesh): 8 Gbps average ### Monitoring & Alerting **Key Metrics** (Monitored via Prometheus): - Request rate (per service, per endpoint) - Error rate (4xx, 5xx responses) - Latency percentiles (P50, P95, P99) - Resource utilization (CPU, memory, disk) - Cache hit/miss ratios - Database connection pool saturation - Kafka consumer lag **Alerts** (via PagerDuty): - P1 (Immediate): API error rate > 1%, database connection failure - P2 (< 15 min): P99 latency > 500ms, cache hit rate < 80% - P3 (< 1 hour): Resource utilization > 85%, replica count < minimum **SLI/SLO/SLA**: - SLI: API success rate (non-5xx responses) - SLO: 99.9% uptime per month (43 minutes downtime allowance) - SLA: 99.5% uptime guarantee (customer-facing SLA with credits) --- ## Disaster Recovery ### Recovery Objectives **RPO (Recovery Point Objective): 1 hour** - Maximum acceptable data loss: 1 hour of transactions - Achieved through: Continuous database replication + hourly snapshots - Kafka retention: 7 days allows event replay **RTO (Recovery Time Objective): 4 hours** - Maximum acceptable downtime: 4 hours for full system recovery - Includes: Failover, data verification, and service restoration - Automated runbooks reduce RTO to < 2 hours for common scenarios ### High Availability Architecture **Multi-AZ Deployment**: ``` Region: us-east-1 AZ-1a: AZ-1b: AZ-1c: - API Gateway (4) - API Gateway (4) - API Gateway (4) - Workflow Engine (6) - Workflow Engine (5) - Workflow Engine (5) - Auth Service (3) - Auth Service (3) - Auth",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_0",
              "content": "# CloudFlow Platform - System Architecture Overview **Document Version:** 2.3.1 **Last Updated:** January 15, 2026 **Owner:** Platform Architecture Team **Status:** Production ## Executive Summary CloudFlow is a distributed, cloud-native workflow automation platform designed to orchestrate complex business processes at scale. The platform processes over 2.5 million workflow executions daily with an average P99 latency of 180ms for API operations and 4.2 seconds for workflow execution. This document provides a comprehensive overview of the system architecture, including microservices design, data flow patterns, infrastructure decisions, and operational considerations. ## Table of Contents 1. [High-Level Architecture](#high-level-architecture) 2. [Microservices Breakdown](#microservices-breakdown) 3. [Data Flow Architecture](#data-flow-architecture) 4. [Database Architecture](#database-architecture) 5. [Message Queue Patterns](#message-queue-patterns) 6. [Caching Strategy](#caching-strategy) 7. [Security Architecture](#security-architecture) 8. [Performance Characteristics](#performance-characteristics) 9. [Disaster Recovery](#disaster-recovery) --- ## High-Level Architecture CloudFlow follows a microservices architecture pattern deployed across multiple availability zones in AWS. The platform is designed with the following core principles: - **Scalability**: Horizontal scaling for all services with auto-scaling groups - **Resilience**: Circuit breakers, bulkheads, and graceful degradation - **Observability**: Distributed tracing, centralized logging, and comprehensive metrics - **Security**: Zero-trust network architecture with mTLS encryption ### Architecture Diagram (Conceptual) ``` \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 Load Balancer (ALB) \u2502 \u2502 (TLS Termination - 443) \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u25bc \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 API Gateway Layer \u2502 \u2502 (Rate Limiting, Auth, Request Routing) \u2502 \u2514\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u2502 \u2502 \u2502 \u2502 \u25bc \u25bc \u25bc \u25bc \u25bc \u250c\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502Auth\u2502 \u2502Workflow\u2502 \u2502Scheduler\u2502 \u2502Notification\u2502 \u2502User Service\u2502 \u2502Svc \u2502 \u2502 Engine \u2502 \u2502 Service \u2502 \u2502 Service \u2502 \u2502 \u2502 \u2514\u2500\u252c\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u2502 \u2502 \u2502 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 \u2502 \u2502 \u25bc \u25bc \u25bc \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502PostgreSQL\u2502 \u2502 Redis \u2502 \u2502 Kafka \u2502 \u2502 Cluster \u2502 \u2502 Cluster \u2502 \u2502 Cluster \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 ``` ### Technology Stack - **Runtime**: Node.js 20.x (API Gateway, Workflow Engine), Go 1.21 (Auth Service, Scheduler) - **Container Orchestration**: Kubernetes 1.28 (EKS) - **Service Mesh**: Istio 1.20 for service-to-service communication - **Databases**: PostgreSQL 15.4, Redis 7.2 - **Message Broker**: Apache Kafka 3.6 - **Monitoring**: Prometheus, Grafana, Jaeger for distributed tracing - **Secrets Management**: HashiCorp Vault 1.15 --- ## Microservices Breakdown ### API Gateway **Purpose**: Single entry point for all client requests, providing authentication, rate limiting, request routing, and protocol translation.",
              "score": null
            },
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_4",
              "content": "### Error Response Format ```json { \"error\": { \"code\": \"invalid_parameter\", \"message\": \"The 'limit' parameter must be between 1 and 100\", \"field\": \"limit\", \"request_id\": \"req_8k3m9x2p\" } } ``` ### HTTP Status Codes | Status Code | Description | Common Causes | |------------|-------------|---------------| | 400 | Bad Request | Invalid parameters, malformed JSON, validation errors | | 401 | Unauthorized | Missing or invalid authentication credentials | | 403 | Forbidden | Insufficient permissions for requested resource | | 404 | Not Found | Resource does not exist or has been deleted | | 429 | Too Many Requests | Rate limit exceeded, retry after specified period | | 500 | Internal Server Error | Unexpected server error, contact support if persists | | 502 | Bad Gateway | Temporary service issue, retry with exponential backoff | | 503 | Service Unavailable | Scheduled maintenance or temporary outage | ### Error Codes CloudFlow returns specific error codes to help you identify and resolve issues: - `invalid_parameter`: One or more request parameters are invalid - `missing_required_field`: Required field is missing from request body - `authentication_failed`: Invalid API key or token - `insufficient_permissions`: User lacks required scope or permission - `resource_not_found`: Requested resource does not exist - `rate_limit_exceeded`: Too many requests, see rate limiting section - `workflow_execution_failed`: Workflow execution encountered an error - `invalid_json`: Request body contains malformed JSON - `duplicate_resource`: Resource with same identifier already exists - `quota_exceeded`: Account quota limit reached ### Error Handling Best Practices **Python Example:** ```python import requests import time def make_api_request(url, headers, max_retries=3): for attempt in range(max_retries): try: response = requests.get(url, headers=headers) if response.status_code == 200: return response.json() elif response.status_code == 429: # Rate limit exceeded retry_after = int(response.headers.get('X-RateLimit-Reset', 60)) print(f\"Rate limited. Waiting {retry_after} seconds...\") time.sleep(retry_after) continue elif response.status_code >= 500: # Server error - retry with exponential backoff wait_time = 2 ** attempt print(f\"Server error. Retrying in {wait_time} seconds...\") time.sleep(wait_time) continue else: # Client error - don't retry error_data = response.json() raise Exception(f\"API Error: {error_data['error']['message']}\") except requests.exceptions.RequestException as e: print(f\"Request failed: {e}\") if attempt < max_retries - 1: time.sleep(2 ** attempt) else: raise raise Exception(\"Max retries exceeded\") ``` ## Webhooks CloudFlow can send webhook notifications when specific events occur in your workflows.",
              "score": null
            }
          ],
          "latency_ms": 11.659498999506468,
          "query_id": "realistic_006",
          "dimension": "synonym",
          "query": "How much CPU and memory does the API Gateway need?"
        },
        {
          "key_facts": [
            "2 vCPU, 4GB RAM per pod"
          ],
          "found_facts": [
            "2 vCPU, 4GB RAM per pod"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "architecture_overview"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_7",
              "content": "podSelector: matchLabels: app: cloudflow policyTypes: - Ingress - Egress ingress: - from: - namespaceSelector: matchLabels: name: cloudflow-prod - podSelector: matchLabels: app: nginx-ingress ports: - protocol: TCP port: 3000 egress: - to: - podSelector: matchLabels: app: postgresql ports: - protocol: TCP port: 5432 - to: - podSelector: matchLabels: app: redis ports: - protocol: TCP port: 6379 ``` ### Pod Security Standards Enforce Pod Security Standards at the namespace level: ```bash kubectl label namespace cloudflow-prod \\ pod-security.kubernetes.io/enforce=restricted \\ pod-security.kubernetes.io/audit=restricted \\ pod-security.kubernetes.io/warn=restricted ``` ### Secrets Management Use AWS Secrets Manager for sensitive credentials: ```bash # Install External Secrets Operator helm install external-secrets external-secrets/external-secrets \\ --namespace external-secrets-system \\ --create-namespace # Create SecretStore kubectl apply -f - <<EOF apiVersion: external-secrets.io/v1beta1 kind: SecretStore metadata: name: aws-secrets-manager namespace: cloudflow-prod spec: provider: aws: service: SecretsManager region: us-east-1 auth: jwt: serviceAccountRef: name: external-secrets-sa EOF ``` --- ## Troubleshooting ### Common Issues #### Pods in CrashLoopBackOff **Symptoms**: Pods continuously restart **Diagnosis**: ```bash kubectl logs -n cloudflow-prod <pod-name> --previous kubectl describe pod -n cloudflow-prod <pod-name> ``` **Common Causes**: - Database connection failure - Invalid environment variables - Insufficient resources #### High Memory Usage **Symptoms**: Pods being OOMKilled **Diagnosis**: ```bash kubectl top pods -n cloudflow-prod ``` **Resolution**: - Increase memory limits in deployment - Check for memory leaks in application logs - Review heap dump for Node.js processes #### Database Connection Pool Exhausted **Symptoms**: \"Too many connections\" errors **Diagnosis**: ```bash # Check PgBouncer stats kubectl exec -it pgbouncer-xxx -n cloudflow-prod -- \\ psql -p 5432 -U pgbouncer pgbouncer -c \"SHOW POOLS;\" ``` **Resolution**: - Increase `max_db_connections` in PgBouncer - Optimize application connection usage - Add more PostgreSQL replicas ### Support Contacts - **On-call Engineer**: pagerduty.com/cloudflow-oncall - **Slack Channel**: #cloudflow-operations - **Documentation**: https://docs.cloudflow.io - **Status Page**: https://status.cloudflow.io --- ## Appendix ### Useful Commands ```bash # View all resources in namespace kubectl get all -n cloudflow-prod # Check resource usage kubectl top pods -n cloudflow-prod kubectl top nodes # View logs kubectl logs -f deployment/cloudflow-api -n cloudflow-prod # Execute command in pod kubectl exec -it <pod-name> -n cloudflow-prod -- /bin/sh # Port forward for local testing kubectl port-forward svc/cloudflow-api 8080:80 -n cloudflow-prod # Rollback deployment helm rollback cloudflow -n cloudflow-prod # Update deployment helm upgrade cloudflow cloudflow/cloudflow \\ --namespace cloudflow-prod \\ --values values-production.yaml ``` ### Maintenance Windows Scheduled maintenance occurs during the following windows: - **Primary**: Sunday 02:00-06:00",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_4",
              "content": "**Technology**: Node.js with worker pool pattern **Replicas**: 8 pods (production), auto-scaling 6-16 **Resource Allocation**: 2 vCPU, 4GB RAM per pod **Key Responsibilities**: - Consume notification events from Kafka - Template rendering with Handlebars (cached in Redis) - Multi-channel delivery (Email via SendGrid, SMS via Twilio, Webhooks) - Retry logic with dead letter queue for failed deliveries - Delivery status tracking and analytics - User preference management (opt-in/opt-out) **Notification Flow**: ``` Event Source \u2192 Kafka (notifications.email topic) \u2502 \u25bc Notification Service Consumer \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u25bc \u25bc \u25bc [Email Queue] [SMS Queue] [Webhook Queue] \u2502 \u2502 \u2502 \u25bc \u25bc \u25bc [SendGrid] [Twilio] [HTTP POST] \u2502 \u2502 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u25bc Delivery Status \u2192 PostgreSQL ``` **Channel Configuration**: - Email: SendGrid with IP warming, DKIM/SPF configured - SMS: Twilio with fallback to AWS SNS - Webhook: Signed payloads (HMAC-SHA256), retry 3 times with exponential backoff - In-app: WebSocket connections via Socket.io **Template Management**: - Templates stored in PostgreSQL with versioning - Compiled templates cached in Redis (1-hour TTL) - Support for localization (i18n) with 12 languages - A/B testing capability for email campaigns **Performance Targets**: - Email delivery: < 5 seconds (P99) from event to SendGrid - SMS delivery: < 3 seconds (P99) - Throughput: 10,000 notifications per minute --- ## Data Flow Architecture ### Synchronous Request Flow Client requests follow a synchronous path through the API Gateway to backend services: ``` 1. Client \u2192 Load Balancer (TLS termination) 2. Load Balancer \u2192 API Gateway (HTTP/2) 3. API Gateway \u2192 Auth Service (JWT validation via gRPC) 4. API Gateway \u2192 Target Service (HTTP/REST or gRPC) 5. Target Service \u2192 Database/Cache (data retrieval) 6. Response propagates back through the chain ``` **Timeout Configuration**: - Client \u2192 Load Balancer: 60 seconds - Load Balancer \u2192 API Gateway: 55 seconds - API Gateway \u2192 Services: 45 seconds - Service \u2192 Database: 10 seconds ### Asynchronous Event Flow Long-running operations and inter-service communication use event-driven patterns: ``` 1. Service publishes event \u2192 Kafka topic 2. Kafka persists event (replication factor: 3) 3. Consumer groups subscribe to topics 4. Consumers process events with at-least-once delivery 5. State updates written to PostgreSQL 6.",
              "score": null
            },
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_6",
              "content": "**Verify Service Health**: ```bash kubectl get pods -n cloudflow-prod curl https://api.cloudflow.io/health ``` **Recovery Time Objective (RTO)**: 4 hours **Recovery Point Objective (RPO)**: 24 hours --- ## Scaling and Performance ### Horizontal Pod Autoscaling CloudFlow is configured with Horizontal Pod Autoscaler (HPA) to automatically scale based on resource utilization: ```yaml # hpa.yaml apiVersion: autoscaling/v2 kind: HorizontalPodAutoscaler metadata: name: cloudflow-api-hpa namespace: cloudflow-prod spec: scaleTargetRef: apiVersion: apps/v1 kind: Deployment name: cloudflow-api minReplicas: 3 maxReplicas: 10 metrics: - type: Resource resource: name: cpu target: type: Utilization averageUtilization: 70 - type: Resource resource: name: memory target: type: Utilization averageUtilization: 80 behavior: scaleDown: stabilizationWindowSeconds: 300 policies: - type: Percent value: 50 periodSeconds: 60 scaleUp: stabilizationWindowSeconds: 60 policies: - type: Percent value: 100 periodSeconds: 30 - type: Pods value: 2 periodSeconds: 30 selectPolicy: Max ``` Monitor autoscaling events: ```bash kubectl get hpa -n cloudflow-prod -w kubectl describe hpa cloudflow-api-hpa -n cloudflow-prod ``` ### Performance Optimization #### Connection Pooling Ensure optimal database connection pooling settings in PgBouncer: - **Pool Mode**: Transaction (optimal for microservices) - **Default Pool Size**: 25 connections per user - **Max DB Connections**: 100 (matches PostgreSQL `max_connections`) #### Redis Caching Implement Redis caching for frequently accessed data: ```javascript // Example caching strategy const CACHE_TTL = { workflows: 300, // 5 minutes userSessions: 3600, // 1 hour apiResults: 60 // 1 minute }; ``` #### Load Testing Perform regular load testing to validate scaling configuration: ```bash # Install k6 for load testing kubectl apply -f - <<EOF apiVersion: v1 kind: ConfigMap metadata: name: loadtest-script namespace: cloudflow-prod data: script.js: | import http from 'k6/http'; import { check, sleep } from 'k6'; export let options = { stages: [ { duration: '2m', target: 100 }, { duration: '5m', target: 100 }, { duration: '2m', target: 200 }, { duration: '5m', target: 200 }, { duration: '2m', target: 0 }, ], thresholds: { http_req_duration: ['p(95)<500'], }, }; export default function () { let res = http.get('https://api.cloudflow.io/api/workflows'); check(res, { 'status is 200': (r) => r.status === 200 }); sleep(1); } EOF ``` **Performance Targets**: - P95 API latency: < 500ms - P99 API latency: < 1000ms - Error rate: < 0.1% - Throughput: > 1000 requests/second --- ## Security and Compliance ### Network Policies Implement network policies to restrict pod-to-pod communication: ```yaml # network-policy.yaml apiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: cloudflow-network-policy namespace: cloudflow-prod spec:",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_1",
              "content": "**Technology**: Node.js with Express.js framework **Replicas**: 12 pods (production), auto-scaling 8-20 based on CPU **Resource Allocation**: 2 vCPU, 4GB RAM per pod **Key Responsibilities**: - JWT token validation (delegated to Auth Service for initial validation) - Rate limiting: 1000 requests per minute per API key (sliding window) - Request/response transformation and validation using JSON Schema - Routing to downstream services based on URL path patterns - CORS handling for web clients - Request/response logging and correlation ID injection **Critical Endpoints**: - `POST /api/v1/workflows` - Create new workflow - `GET /api/v1/workflows/:id` - Retrieve workflow status - `POST /api/v1/workflows/:id/execute` - Trigger workflow execution - `GET /api/v1/workflows/:id/history` - Get execution history **Dependencies**: - Auth Service (for token validation) - Redis (for rate limiting counters) - All downstream microservices **Performance Targets**: - P50 latency: < 50ms - P99 latency: < 200ms - Throughput: 10,000 RPS sustained --- ### Auth Service **Purpose**: Centralized authentication and authorization service handling user identity, token generation, and permission validation. **Technology**: Go with gRPC for internal communication, REST for external **Replicas**: 8 pods (production), auto-scaling 6-12 **Resource Allocation**: 1 vCPU, 2GB RAM per pod **Key Responsibilities**: - User authentication via multiple providers (OAuth2, SAML, local credentials) - JWT token generation and validation (RS256 algorithm) - Role-based access control (RBAC) with fine-grained permissions - Session management with Redis-backed storage - API key generation and validation for service accounts - MFA enforcement for administrative operations **Authentication Flow**: ``` Client Request \u2192 API Gateway \u2192 Auth Service \u2502 \u251c\u2500 Validate credentials \u251c\u2500 Check MFA if required \u251c\u2500 Generate JWT (15min expiry) \u251c\u2500 Generate refresh token (7 days) \u2514\u2500 Store session in Redis ``` **Token Structure**: - Access Token: JWT with 15-minute expiry - Refresh Token: Opaque token with 7-day expiry, stored in PostgreSQL - Claims: user_id, email, roles[], permissions[], tenant_id **Security Features**: - Password hashing: Argon2id with 64MB memory, 4 iterations - Token rotation on refresh to prevent replay attacks - Brute force protection: 5 failed attempts \u2192 15-minute lockout - Secrets stored in HashiCorp Vault, rotated every 90 days **Performance Targets**: - Token validation: < 10ms (P99) - Token generation: < 50ms (P99) - Throughput: 5,000 RPS for validation operations --- ### Workflow Engine **Purpose**: Core orchestration service that executes workflow definitions, manages state transitions, and coordinates task execution across distributed systems.",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_10",
              "content": "- Execution start rate: 500 per second - Completion rate: 450 per second (average 2-second execution time) **Database**: - Read throughput: 50,000 queries per second (across replicas) - Write throughput: 15,000 transactions per second - Connection capacity: 2,000 concurrent connections **Kafka**: - Message ingestion: 100,000 messages per second - Consumer throughput: 80,000 messages per second (aggregated) - End-to-end latency: < 100ms (P99) ### Resource Utilization **CPU Utilization** (Target: 60-70% average): - API Gateway: 55% average, 80% peak - Workflow Engine: 65% average, 85% peak - Auth Service: 40% average, 60% peak **Memory Utilization** (Target: < 80%): - API Gateway: 2.5GB average per pod (4GB allocated) - Workflow Engine: 6GB average per pod (8GB allocated) - Notification Service: 2.8GB average per pod (4GB allocated) **Network Throughput**: - Ingress: 2 Gbps average, 5 Gbps peak - Egress: 1.5 Gbps average, 4 Gbps peak - Internal (service mesh): 8 Gbps average ### Monitoring & Alerting **Key Metrics** (Monitored via Prometheus): - Request rate (per service, per endpoint) - Error rate (4xx, 5xx responses) - Latency percentiles (P50, P95, P99) - Resource utilization (CPU, memory, disk) - Cache hit/miss ratios - Database connection pool saturation - Kafka consumer lag **Alerts** (via PagerDuty): - P1 (Immediate): API error rate > 1%, database connection failure - P2 (< 15 min): P99 latency > 500ms, cache hit rate < 80% - P3 (< 1 hour): Resource utilization > 85%, replica count < minimum **SLI/SLO/SLA**: - SLI: API success rate (non-5xx responses) - SLO: 99.9% uptime per month (43 minutes downtime allowance) - SLA: 99.5% uptime guarantee (customer-facing SLA with credits) --- ## Disaster Recovery ### Recovery Objectives **RPO (Recovery Point Objective): 1 hour** - Maximum acceptable data loss: 1 hour of transactions - Achieved through: Continuous database replication + hourly snapshots - Kafka retention: 7 days allows event replay **RTO (Recovery Time Objective): 4 hours** - Maximum acceptable downtime: 4 hours for full system recovery - Includes: Failover, data verification, and service restoration - Automated runbooks reduce RTO to < 2 hours for common scenarios ### High Availability Architecture **Multi-AZ Deployment**: ``` Region: us-east-1 AZ-1a: AZ-1b: AZ-1c: - API Gateway (4) - API Gateway (4) - API Gateway (4) - Workflow Engine (6) - Workflow Engine (5) - Workflow Engine (5) - Auth Service (3) - Auth Service (3) - Auth",
              "score": null
            }
          ],
          "latency_ms": 11.364619997038972,
          "query_id": "realistic_006",
          "dimension": "problem",
          "query": "The API Gateway pods keep getting OOMKilled"
        },
        {
          "key_facts": [
            "2 vCPU, 4GB RAM per pod"
          ],
          "found_facts": [
            "2 vCPU, 4GB RAM per pod"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "architecture_overview"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_7",
              "content": "records: 500 - Session timeout: 30 seconds --- ## Message Queue Patterns ### Pub/Sub Pattern Used for broadcasting events to multiple interested consumers: ``` Workflow Engine publishes workflow.execution.completed \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u25bc \u25bc \u25bc \u25bc Analytics Notification Audit Logger Billing Service Service Service Service ``` Each service maintains its own consumer group and processes events independently. Failures in one consumer don't affect others. ### Request-Reply Pattern Synchronous communication over async messaging (used sparingly): ``` API Gateway publishes request \u2192 Kafka (reply-to: temp-queue-123) \u2502 \u25bc Service processes request \u2502 \u25bc Service publishes response \u2192 temp-queue-123 \u2502 \u25bc API Gateway receives response ``` Timeout: 5 seconds, fallback to direct HTTP call if no response. ### Saga Pattern Distributed transaction management for multi-service workflows: ``` Step 1: Create Order \u2192 SUCCESS \u2192 Step 2: Reserve Inventory \u2502 FAILURE \u2502 \u25bc Compensating Transaction \u2502 \u25bc Cancel Order (Rollback) ``` Orchestrated by Workflow Engine with compensation logic defined in workflow DSL. ### Dead Letter Queue (DLQ) Failed messages after all retry attempts are routed to DLQ: ``` Message processing fails (3 retries with exponential backoff) \u2502 \u25bc Publish to dead-letter-queue topic \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u25bc \u25bc Alert to PagerDuty Store in PostgreSQL for analysis \u2502 \u2502 \u25bc \u25bc Manual investigation Automated pattern detection ``` DLQ Processing SLA: < 4 hours for critical events, < 24 hours for non-critical. --- ## Caching Strategy ### Cache-Aside Pattern (Lazy Loading) Primary caching pattern used across all services: ``` 1. Application checks cache (Redis GET) 2. If HIT \u2192 Return cached data 3. If MISS: a. Query database (PostgreSQL) b. Store result in cache with TTL c. Return data to application ``` **Implementation Example** (Workflow Definition): ```javascript async function getWorkflowDefinition(workflowId) { const cacheKey = `workflow:def:${workflowId}`; // Step 1: Check cache let workflow = await redis.get(cacheKey); if (workflow) { metrics.increment('cache.hit.workflow_def'); return JSON.parse(workflow); } // Step 2: Cache miss - query database metrics.increment('cache.miss.workflow_def'); workflow = await db.query( 'SELECT * FROM workflows WHERE id = $1', [workflowId] ); // Step 3: Store in cache (1 hour TTL) await redis.setex(cacheKey, 3600, JSON.stringify(workflow)); return workflow; } ``` ### Write-Through Pattern Used for critical data where cache consistency is paramount: ``` 1. Application writes to database 2. Database transaction commits 3. Application updates cache 4. Return success to client ``` Applied to: User profiles, authentication sessions, system configuration.",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_0",
              "content": "# CloudFlow Platform - System Architecture Overview **Document Version:** 2.3.1 **Last Updated:** January 15, 2026 **Owner:** Platform Architecture Team **Status:** Production ## Executive Summary CloudFlow is a distributed, cloud-native workflow automation platform designed to orchestrate complex business processes at scale. The platform processes over 2.5 million workflow executions daily with an average P99 latency of 180ms for API operations and 4.2 seconds for workflow execution. This document provides a comprehensive overview of the system architecture, including microservices design, data flow patterns, infrastructure decisions, and operational considerations. ## Table of Contents 1. [High-Level Architecture](#high-level-architecture) 2. [Microservices Breakdown](#microservices-breakdown) 3. [Data Flow Architecture](#data-flow-architecture) 4. [Database Architecture](#database-architecture) 5. [Message Queue Patterns](#message-queue-patterns) 6. [Caching Strategy](#caching-strategy) 7. [Security Architecture](#security-architecture) 8. [Performance Characteristics](#performance-characteristics) 9. [Disaster Recovery](#disaster-recovery) --- ## High-Level Architecture CloudFlow follows a microservices architecture pattern deployed across multiple availability zones in AWS. The platform is designed with the following core principles: - **Scalability**: Horizontal scaling for all services with auto-scaling groups - **Resilience**: Circuit breakers, bulkheads, and graceful degradation - **Observability**: Distributed tracing, centralized logging, and comprehensive metrics - **Security**: Zero-trust network architecture with mTLS encryption ### Architecture Diagram (Conceptual) ``` \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 Load Balancer (ALB) \u2502 \u2502 (TLS Termination - 443) \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u25bc \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 API Gateway Layer \u2502 \u2502 (Rate Limiting, Auth, Request Routing) \u2502 \u2514\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u2502 \u2502 \u2502 \u2502 \u25bc \u25bc \u25bc \u25bc \u25bc \u250c\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502Auth\u2502 \u2502Workflow\u2502 \u2502Scheduler\u2502 \u2502Notification\u2502 \u2502User Service\u2502 \u2502Svc \u2502 \u2502 Engine \u2502 \u2502 Service \u2502 \u2502 Service \u2502 \u2502 \u2502 \u2514\u2500\u252c\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u2502 \u2502 \u2502 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 \u2502 \u2502 \u25bc \u25bc \u25bc \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502PostgreSQL\u2502 \u2502 Redis \u2502 \u2502 Kafka \u2502 \u2502 Cluster \u2502 \u2502 Cluster \u2502 \u2502 Cluster \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 ``` ### Technology Stack - **Runtime**: Node.js 20.x (API Gateway, Workflow Engine), Go 1.21 (Auth Service, Scheduler) - **Container Orchestration**: Kubernetes 1.28 (EKS) - **Service Mesh**: Istio 1.20 for service-to-service communication - **Databases**: PostgreSQL 15.4, Redis 7.2 - **Message Broker**: Apache Kafka 3.6 - **Monitoring**: Prometheus, Grafana, Jaeger for distributed tracing - **Secrets Management**: HashiCorp Vault 1.15 --- ## Microservices Breakdown ### API Gateway **Purpose**: Single entry point for all client requests, providing authentication, rate limiting, request routing, and protocol translation.",
              "score": null
            },
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_0",
              "content": "# CloudFlow API Reference Version 2.1.0 | Last Updated: January 2026 ## Overview The CloudFlow API is a RESTful service that enables developers to programmatically manage cloud workflows, data pipelines, and automation tasks. This documentation provides comprehensive details on authentication, endpoints, request/response formats, error handling, and best practices. **Base URL:** `https://api.cloudflow.io/v2` **API Status:** https://status.cloudflow.io ## Authentication CloudFlow supports three authentication methods to suit different use cases and security requirements. ### API Keys API keys provide simple authentication for server-to-server communication. Include your API key in the request header: ```bash curl -H \"X-API-Key: cf_live_a1b2c3d4e5f6g7h8i9j0\" \\ https://api.cloudflow.io/v2/workflows ``` **Security Notes:** - Never expose API keys in client-side code - Rotate keys every 90 days - Use separate keys for development and production environments ### OAuth 2.0 OAuth 2.0 is recommended for applications that access CloudFlow on behalf of users. We support the Authorization Code flow with PKCE. **Authorization Endpoint:** `https://auth.cloudflow.io/oauth/authorize` **Token Endpoint:** `https://auth.cloudflow.io/oauth/token` **Supported Scopes:** - `workflows:read` - Read workflow configurations - `workflows:write` - Create and modify workflows - `pipelines:read` - Read pipeline data - `pipelines:write` - Create and manage pipelines - `analytics:read` - Access analytics and metrics - `admin:full` - Full administrative access Example authorization request: ```bash curl -X POST https://auth.cloudflow.io/oauth/token \\ -H \"Content-Type: application/x-www-form-urlencoded\" \\ -d \"grant_type=authorization_code\" \\ -d \"code=AUTH_CODE_HERE\" \\ -d \"client_id=YOUR_CLIENT_ID\" \\ -d \"client_secret=YOUR_CLIENT_SECRET\" \\ -d \"redirect_uri=https://yourapp.com/callback\" ``` ### JWT Tokens For advanced use cases, CloudFlow supports JSON Web Tokens (JWT) with RS256 signing algorithm. JWTs must include the following claims: - `iss` (issuer): Your application identifier - `sub` (subject): User or service account ID - `aud` (audience): `https://api.cloudflow.io` - `exp` (expiration): Unix timestamp (max 3600 seconds from `iat`) - `iat` (issued at): Unix timestamp - `scope`: Space-separated list of requested scopes Example JWT header: ```python import jwt import time from cryptography.hazmat.primitives import serialization from cryptography.hazmat.backends import default_backend # Load your private key with open('private_key.pem', 'rb') as key_file: private_key = serialization.load_pem_private_key( key_file.read(), password=None, backend=default_backend() ) # Create JWT payload payload = { 'iss': 'your-app-id', 'sub': 'user-12345', 'aud': 'https://api.cloudflow.io', 'exp': int(time.time()) + 3600, 'iat': int(time.time()), 'scope': 'workflows:read workflows:write' } # Generate token token = jwt.encode(payload, private_key, algorithm='RS256') # Use in API request headers = {'Authorization': f'Bearer {token}'} ``` **Token Expiration:** All tokens expire after 3600 seconds (1 hour). Implement token refresh logic in your application.",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_4",
              "content": "**Technology**: Node.js with worker pool pattern **Replicas**: 8 pods (production), auto-scaling 6-16 **Resource Allocation**: 2 vCPU, 4GB RAM per pod **Key Responsibilities**: - Consume notification events from Kafka - Template rendering with Handlebars (cached in Redis) - Multi-channel delivery (Email via SendGrid, SMS via Twilio, Webhooks) - Retry logic with dead letter queue for failed deliveries - Delivery status tracking and analytics - User preference management (opt-in/opt-out) **Notification Flow**: ``` Event Source \u2192 Kafka (notifications.email topic) \u2502 \u25bc Notification Service Consumer \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u25bc \u25bc \u25bc [Email Queue] [SMS Queue] [Webhook Queue] \u2502 \u2502 \u2502 \u25bc \u25bc \u25bc [SendGrid] [Twilio] [HTTP POST] \u2502 \u2502 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u25bc Delivery Status \u2192 PostgreSQL ``` **Channel Configuration**: - Email: SendGrid with IP warming, DKIM/SPF configured - SMS: Twilio with fallback to AWS SNS - Webhook: Signed payloads (HMAC-SHA256), retry 3 times with exponential backoff - In-app: WebSocket connections via Socket.io **Template Management**: - Templates stored in PostgreSQL with versioning - Compiled templates cached in Redis (1-hour TTL) - Support for localization (i18n) with 12 languages - A/B testing capability for email campaigns **Performance Targets**: - Email delivery: < 5 seconds (P99) from event to SendGrid - SMS delivery: < 3 seconds (P99) - Throughput: 10,000 notifications per minute --- ## Data Flow Architecture ### Synchronous Request Flow Client requests follow a synchronous path through the API Gateway to backend services: ``` 1. Client \u2192 Load Balancer (TLS termination) 2. Load Balancer \u2192 API Gateway (HTTP/2) 3. API Gateway \u2192 Auth Service (JWT validation via gRPC) 4. API Gateway \u2192 Target Service (HTTP/REST or gRPC) 5. Target Service \u2192 Database/Cache (data retrieval) 6. Response propagates back through the chain ``` **Timeout Configuration**: - Client \u2192 Load Balancer: 60 seconds - Load Balancer \u2192 API Gateway: 55 seconds - API Gateway \u2192 Services: 45 seconds - Service \u2192 Database: 10 seconds ### Asynchronous Event Flow Long-running operations and inter-service communication use event-driven patterns: ``` 1. Service publishes event \u2192 Kafka topic 2. Kafka persists event (replication factor: 3) 3. Consumer groups subscribe to topics 4. Consumers process events with at-least-once delivery 5. State updates written to PostgreSQL 6.",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_1",
              "content": "**Technology**: Node.js with Express.js framework **Replicas**: 12 pods (production), auto-scaling 8-20 based on CPU **Resource Allocation**: 2 vCPU, 4GB RAM per pod **Key Responsibilities**: - JWT token validation (delegated to Auth Service for initial validation) - Rate limiting: 1000 requests per minute per API key (sliding window) - Request/response transformation and validation using JSON Schema - Routing to downstream services based on URL path patterns - CORS handling for web clients - Request/response logging and correlation ID injection **Critical Endpoints**: - `POST /api/v1/workflows` - Create new workflow - `GET /api/v1/workflows/:id` - Retrieve workflow status - `POST /api/v1/workflows/:id/execute` - Trigger workflow execution - `GET /api/v1/workflows/:id/history` - Get execution history **Dependencies**: - Auth Service (for token validation) - Redis (for rate limiting counters) - All downstream microservices **Performance Targets**: - P50 latency: < 50ms - P99 latency: < 200ms - Throughput: 10,000 RPS sustained --- ### Auth Service **Purpose**: Centralized authentication and authorization service handling user identity, token generation, and permission validation. **Technology**: Go with gRPC for internal communication, REST for external **Replicas**: 8 pods (production), auto-scaling 6-12 **Resource Allocation**: 1 vCPU, 2GB RAM per pod **Key Responsibilities**: - User authentication via multiple providers (OAuth2, SAML, local credentials) - JWT token generation and validation (RS256 algorithm) - Role-based access control (RBAC) with fine-grained permissions - Session management with Redis-backed storage - API key generation and validation for service accounts - MFA enforcement for administrative operations **Authentication Flow**: ``` Client Request \u2192 API Gateway \u2192 Auth Service \u2502 \u251c\u2500 Validate credentials \u251c\u2500 Check MFA if required \u251c\u2500 Generate JWT (15min expiry) \u251c\u2500 Generate refresh token (7 days) \u2514\u2500 Store session in Redis ``` **Token Structure**: - Access Token: JWT with 15-minute expiry - Refresh Token: Opaque token with 7-day expiry, stored in PostgreSQL - Claims: user_id, email, roles[], permissions[], tenant_id **Security Features**: - Password hashing: Argon2id with 64MB memory, 4 iterations - Token rotation on refresh to prevent replay attacks - Brute force protection: 5 failed attempts \u2192 15-minute lockout - Secrets stored in HashiCorp Vault, rotated every 90 days **Performance Targets**: - Token validation: < 10ms (P99) - Token generation: < 50ms (P99) - Throughput: 5,000 RPS for validation operations --- ### Workflow Engine **Purpose**: Core orchestration service that executes workflow definitions, manages state transitions, and coordinates task execution across distributed systems.",
              "score": null
            }
          ],
          "latency_ms": 11.229948999243788,
          "query_id": "realistic_006",
          "dimension": "casual",
          "query": "api gateway resources"
        },
        {
          "key_facts": [
            "2 vCPU, 4GB RAM per pod"
          ],
          "found_facts": [
            "2 vCPU, 4GB RAM per pod"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "architecture_overview"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_0",
              "content": "# CloudFlow Platform - Deployment and Operations Guide **Version:** 2.4.0 **Last Updated:** January 2026 **Target Environment:** Production (AWS EKS) ## Table of Contents 1. [Overview](#overview) 2. [Prerequisites](#prerequisites) 3. [Infrastructure Setup](#infrastructure-setup) 4. [Kubernetes Deployment](#kubernetes-deployment) 5. [Database Configuration](#database-configuration) 6. [Monitoring and Observability](#monitoring-and-observability) 7. [Backup and Disaster Recovery](#backup-and-disaster-recovery) 8. [Scaling and Performance](#scaling-and-performance) 9. [Security and Compliance](#security-and-compliance) 10. [Troubleshooting](#troubleshooting) --- ## Overview CloudFlow is a cloud-native workflow orchestration platform designed for high-availability production environments. This guide provides comprehensive instructions for deploying and operating CloudFlow on Amazon EKS (Elastic Kubernetes Service). ### Architecture Summary CloudFlow consists of the following components: - **API Server**: REST API for workflow management (Node.js/Express) - **Worker Service**: Background job processor (Node.js) - **Scheduler**: Cron-based task scheduler (Node.js) - **PostgreSQL**: Primary data store (version 14) - **Redis**: Cache and message queue (version 7.0) - **PgBouncer**: Database connection pooler ### Deployment Model - **Namespace**: `cloudflow-prod` - **Cluster**: AWS EKS 1.28 - **Region**: us-east-1 (primary), us-west-2 (disaster recovery) - **High Availability**: Multi-AZ deployment across 3 availability zones --- ## Prerequisites Before beginning the deployment process, ensure you have the following: ### Required Tools - `kubectl` (v1.28 or later) - `helm` (v3.12 or later) - `aws-cli` (v2.13 or later) - `eksctl` (v0.165 or later) - `terraform` (v1.6 or later) - for infrastructure provisioning ### Access Requirements - AWS account with appropriate IAM permissions - EKS cluster admin access - Container registry access (ECR) - Domain name and SSL certificates - Secrets management access (AWS Secrets Manager or Vault) ### Network Requirements - VPC with at least 3 public and 3 private subnets - NAT Gateway configured - Security groups allowing: - Ingress: HTTPS (443), HTTP (80) - Internal: PostgreSQL (5432), Redis (6379) - Monitoring: Prometheus (9090), Grafana (3000) --- ## Infrastructure Setup ### EKS Cluster Creation Use the following `eksctl` configuration to create the EKS cluster: ```yaml # cluster-config.yaml apiVersion: eksctl.io/v1alpha5 kind: ClusterConfig metadata: name: cloudflow-production region: us-east-1 version: \"1.28\" availabilityZones: - us-east-1a - us-east-1b - us-east-1c vpc: cidr: 10.0.0.0/16 nat: gateway: HighlyAvailable managedNodeGroups: - name: cloudflow-workers instanceType: m5.xlarge minSize: 3 maxSize: 10 desiredCapacity: 5 volumeSize: 100 privateNetworking: true labels: role: worker environment: production tags: Environment: production Application: cloudflow iam: withAddonPolicies: ebs: true efs: true albIngress: true cloudWatch: true cloudWatch: clusterLogging: enableTypes: - api - audit - authenticator - controllerManager - scheduler ``` Create the cluster: ```bash eksctl",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_0",
              "content": "# CloudFlow Platform - System Architecture Overview **Document Version:** 2.3.1 **Last Updated:** January 15, 2026 **Owner:** Platform Architecture Team **Status:** Production ## Executive Summary CloudFlow is a distributed, cloud-native workflow automation platform designed to orchestrate complex business processes at scale. The platform processes over 2.5 million workflow executions daily with an average P99 latency of 180ms for API operations and 4.2 seconds for workflow execution. This document provides a comprehensive overview of the system architecture, including microservices design, data flow patterns, infrastructure decisions, and operational considerations. ## Table of Contents 1. [High-Level Architecture](#high-level-architecture) 2. [Microservices Breakdown](#microservices-breakdown) 3. [Data Flow Architecture](#data-flow-architecture) 4. [Database Architecture](#database-architecture) 5. [Message Queue Patterns](#message-queue-patterns) 6. [Caching Strategy](#caching-strategy) 7. [Security Architecture](#security-architecture) 8. [Performance Characteristics](#performance-characteristics) 9. [Disaster Recovery](#disaster-recovery) --- ## High-Level Architecture CloudFlow follows a microservices architecture pattern deployed across multiple availability zones in AWS. The platform is designed with the following core principles: - **Scalability**: Horizontal scaling for all services with auto-scaling groups - **Resilience**: Circuit breakers, bulkheads, and graceful degradation - **Observability**: Distributed tracing, centralized logging, and comprehensive metrics - **Security**: Zero-trust network architecture with mTLS encryption ### Architecture Diagram (Conceptual) ``` \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 Load Balancer (ALB) \u2502 \u2502 (TLS Termination - 443) \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u25bc \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 API Gateway Layer \u2502 \u2502 (Rate Limiting, Auth, Request Routing) \u2502 \u2514\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u2502 \u2502 \u2502 \u2502 \u25bc \u25bc \u25bc \u25bc \u25bc \u250c\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502Auth\u2502 \u2502Workflow\u2502 \u2502Scheduler\u2502 \u2502Notification\u2502 \u2502User Service\u2502 \u2502Svc \u2502 \u2502 Engine \u2502 \u2502 Service \u2502 \u2502 Service \u2502 \u2502 \u2502 \u2514\u2500\u252c\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u2502 \u2502 \u2502 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 \u2502 \u2502 \u25bc \u25bc \u25bc \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502PostgreSQL\u2502 \u2502 Redis \u2502 \u2502 Kafka \u2502 \u2502 Cluster \u2502 \u2502 Cluster \u2502 \u2502 Cluster \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 ``` ### Technology Stack - **Runtime**: Node.js 20.x (API Gateway, Workflow Engine), Go 1.21 (Auth Service, Scheduler) - **Container Orchestration**: Kubernetes 1.28 (EKS) - **Service Mesh**: Istio 1.20 for service-to-service communication - **Databases**: PostgreSQL 15.4, Redis 7.2 - **Message Broker**: Apache Kafka 3.6 - **Monitoring**: Prometheus, Grafana, Jaeger for distributed tracing - **Secrets Management**: HashiCorp Vault 1.15 --- ## Microservices Breakdown ### API Gateway **Purpose**: Single entry point for all client requests, providing authentication, rate limiting, request routing, and protocol translation.",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_10",
              "content": "database nc -zv cloudflow-db.internal.company.com 5432 # Trace route traceroute api.cloudflow.io # Capture packets tcpdump -i any -w /tmp/capture.pcap port 5432 ``` --- ## Escalation Procedures ### Severity Levels CloudFlow incidents are classified into four severity levels: #### SEV-1: Critical (P1) - **Definition:** Complete service outage or severe degradation affecting all users - **Examples:** - API returns 5xx errors for > 5 minutes - Database completely unavailable - Authentication system down - Data loss or corruption - **Response Time:** Immediate (< 15 minutes) - **Escalation:** Page on-call engineer immediately #### SEV-2: High (P2) - **Definition:** Major functionality impaired affecting multiple users - **Examples:** - Workflow execution success rate < 90% - Significant performance degradation (p95 latency > 10s) - Rate limiting affecting large customer segment - **Response Time:** < 1 hour - **Escalation:** Create incident ticket and notify on-call #### SEV-3: Medium (P3) - **Definition:** Partial functionality degraded affecting some users - **Examples:** - Intermittent failures for specific workflow types - Minor performance issues - Non-critical feature unavailable - **Response Time:** < 4 hours - **Escalation:** Create ticket, normal business hours support #### SEV-4: Low (P4) - **Definition:** Minor issues with minimal user impact - **Examples:** - Cosmetic issues - Documentation errors - Feature requests - **Response Time:** < 2 business days - **Escalation:** Standard support ticket ### Escalation Steps #### Step 1: Initial Assessment (0-5 minutes) ```bash # Run health check cloudflow health check --comprehensive # Check status page curl https://status.cloudflow.io/api/v1/status.json # Review recent changes cloudflow audit log --last 2h --event-type \"deployment,configuration\" # Check metrics dashboard cloudflow metrics dashboard --incident-mode ``` #### Step 2: Gather Information (5-15 minutes) Create incident document with: - Incident timestamp and duration - Affected services and endpoints - Error rates and user impact - Recent changes or deployments - Relevant log excerpts - Correlation IDs for failed requests ```bash # Generate incident report cloudflow debug incident-report \\ --start \"2026-01-24T10:30:00Z\" \\ --end \"2026-01-24T11:00:00Z\" \\ --output incident-report.md # Capture system snapshot cloudflow debug snapshot --output snapshot-$(date +%Y%m%d-%H%M%S).tar.gz ``` #### Step 3: Escalate Based on Severity **For SEV-1 (Critical):** ```bash # Page on-call engineer cloudflow incident create \\ --severity SEV-1 \\ --title \"Complete API outage\" \\ --description \"All API requests returning 503\" \\ --page-oncall # Notify status page cloudflow status update \\ --status \"major_outage\" \\ --message \"We are investigating a complete service",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_0",
              "content": "# CloudFlow Platform Troubleshooting Guide **Version:** 3.2.1 **Last Updated:** January 2026 **Audience:** Platform Engineers, DevOps, Support Teams ## Table of Contents 1. [Overview](#overview) 2. [Authentication & Authorization Issues](#authentication--authorization-issues) 3. [Performance Problems](#performance-problems) 4. [Database Connection Issues](#database-connection-issues) 5. [Workflow Execution Failures](#workflow-execution-failures) 6. [Rate Limiting & Throttling](#rate-limiting--throttling) 7. [Log Analysis & Debugging](#log-analysis--debugging) 8. [Escalation Procedures](#escalation-procedures) --- ## Overview This guide provides comprehensive troubleshooting steps for common CloudFlow platform issues encountered in production environments. Each section includes error symptoms, root cause analysis, resolution steps, and preventive measures. ### Quick Diagnostic Checklist Before diving into specific issues, perform these initial checks: - Verify service health: `cloudflow status --all` - Check API connectivity: `curl -I https://api.cloudflow.io/health` - Review recent deployments: `kubectl get deployments -n cloudflow --sort-by=.metadata.creationTimestamp` - Inspect platform metrics: `cloudflow metrics --last 1h` --- ## Authentication & Authorization Issues ### 401 Unauthorized Errors #### Symptoms - API requests return `401 Unauthorized` - Error message: `Authentication credentials were not provided or are invalid` - Frontend displays \"Session expired\" message #### Common Causes **1. Token Expiration** CloudFlow access tokens expire after 3600 seconds (1 hour) by default. Refresh tokens are valid for 30 days. **Verification:** ```bash # Decode JWT to check expiration echo $CF_ACCESS_TOKEN | cut -d'.' -f2 | base64 -d | jq '.exp' # Compare with current time date +%s ``` **Resolution:** ```bash # Refresh the access token curl -X POST https://api.cloudflow.io/auth/refresh \\ -H \"Content-Type: application/json\" \\ -d '{\"refresh_token\": \"'$CF_REFRESH_TOKEN'\"}' # Update environment variable export CF_ACCESS_TOKEN=\"<new_token>\" ``` **2. Invalid JWT Signature** Error: `JWT signature verification failed` **Causes:** - Token was modified or corrupted - Using wrong signing key - Token generated with different secret **Resolution:** ```bash # Validate token structure cloudflow auth validate --token $CF_ACCESS_TOKEN # Generate new token cloudflow auth login --username <user> --password <pass> # For service accounts cloudflow auth service-account --create --name <sa-name> --scopes \"workflow:read,workflow:write\" ``` **3. Clock Skew Issues** JWT validation fails when system clocks are out of sync (tolerance: \u00b1300 seconds).",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_4",
              "content": "**Technology**: Node.js with worker pool pattern **Replicas**: 8 pods (production), auto-scaling 6-16 **Resource Allocation**: 2 vCPU, 4GB RAM per pod **Key Responsibilities**: - Consume notification events from Kafka - Template rendering with Handlebars (cached in Redis) - Multi-channel delivery (Email via SendGrid, SMS via Twilio, Webhooks) - Retry logic with dead letter queue for failed deliveries - Delivery status tracking and analytics - User preference management (opt-in/opt-out) **Notification Flow**: ``` Event Source \u2192 Kafka (notifications.email topic) \u2502 \u25bc Notification Service Consumer \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u25bc \u25bc \u25bc [Email Queue] [SMS Queue] [Webhook Queue] \u2502 \u2502 \u2502 \u25bc \u25bc \u25bc [SendGrid] [Twilio] [HTTP POST] \u2502 \u2502 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u25bc Delivery Status \u2192 PostgreSQL ``` **Channel Configuration**: - Email: SendGrid with IP warming, DKIM/SPF configured - SMS: Twilio with fallback to AWS SNS - Webhook: Signed payloads (HMAC-SHA256), retry 3 times with exponential backoff - In-app: WebSocket connections via Socket.io **Template Management**: - Templates stored in PostgreSQL with versioning - Compiled templates cached in Redis (1-hour TTL) - Support for localization (i18n) with 12 languages - A/B testing capability for email campaigns **Performance Targets**: - Email delivery: < 5 seconds (P99) from event to SendGrid - SMS delivery: < 3 seconds (P99) - Throughput: 10,000 notifications per minute --- ## Data Flow Architecture ### Synchronous Request Flow Client requests follow a synchronous path through the API Gateway to backend services: ``` 1. Client \u2192 Load Balancer (TLS termination) 2. Load Balancer \u2192 API Gateway (HTTP/2) 3. API Gateway \u2192 Auth Service (JWT validation via gRPC) 4. API Gateway \u2192 Target Service (HTTP/REST or gRPC) 5. Target Service \u2192 Database/Cache (data retrieval) 6. Response propagates back through the chain ``` **Timeout Configuration**: - Client \u2192 Load Balancer: 60 seconds - Load Balancer \u2192 API Gateway: 55 seconds - API Gateway \u2192 Services: 45 seconds - Service \u2192 Database: 10 seconds ### Asynchronous Event Flow Long-running operations and inter-service communication use event-driven patterns: ``` 1. Service publishes event \u2192 Kafka topic 2. Kafka persists event (replication factor: 3) 3. Consumer groups subscribe to topics 4. Consumers process events with at-least-once delivery 5. State updates written to PostgreSQL 6.",
              "score": null
            }
          ],
          "latency_ms": 11.655230999167543,
          "query_id": "realistic_006",
          "dimension": "contextual",
          "query": "I'm provisioning infrastructure, what are the compute specs for API Gateway?"
        },
        {
          "key_facts": [
            "2 vCPU, 4GB RAM per pod"
          ],
          "found_facts": [
            "2 vCPU, 4GB RAM per pod"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "architecture_overview"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_4",
              "content": "### Error Response Format ```json { \"error\": { \"code\": \"invalid_parameter\", \"message\": \"The 'limit' parameter must be between 1 and 100\", \"field\": \"limit\", \"request_id\": \"req_8k3m9x2p\" } } ``` ### HTTP Status Codes | Status Code | Description | Common Causes | |------------|-------------|---------------| | 400 | Bad Request | Invalid parameters, malformed JSON, validation errors | | 401 | Unauthorized | Missing or invalid authentication credentials | | 403 | Forbidden | Insufficient permissions for requested resource | | 404 | Not Found | Resource does not exist or has been deleted | | 429 | Too Many Requests | Rate limit exceeded, retry after specified period | | 500 | Internal Server Error | Unexpected server error, contact support if persists | | 502 | Bad Gateway | Temporary service issue, retry with exponential backoff | | 503 | Service Unavailable | Scheduled maintenance or temporary outage | ### Error Codes CloudFlow returns specific error codes to help you identify and resolve issues: - `invalid_parameter`: One or more request parameters are invalid - `missing_required_field`: Required field is missing from request body - `authentication_failed`: Invalid API key or token - `insufficient_permissions`: User lacks required scope or permission - `resource_not_found`: Requested resource does not exist - `rate_limit_exceeded`: Too many requests, see rate limiting section - `workflow_execution_failed`: Workflow execution encountered an error - `invalid_json`: Request body contains malformed JSON - `duplicate_resource`: Resource with same identifier already exists - `quota_exceeded`: Account quota limit reached ### Error Handling Best Practices **Python Example:** ```python import requests import time def make_api_request(url, headers, max_retries=3): for attempt in range(max_retries): try: response = requests.get(url, headers=headers) if response.status_code == 200: return response.json() elif response.status_code == 429: # Rate limit exceeded retry_after = int(response.headers.get('X-RateLimit-Reset', 60)) print(f\"Rate limited. Waiting {retry_after} seconds...\") time.sleep(retry_after) continue elif response.status_code >= 500: # Server error - retry with exponential backoff wait_time = 2 ** attempt print(f\"Server error. Retrying in {wait_time} seconds...\") time.sleep(wait_time) continue else: # Client error - don't retry error_data = response.json() raise Exception(f\"API Error: {error_data['error']['message']}\") except requests.exceptions.RequestException as e: print(f\"Request failed: {e}\") if attempt < max_retries - 1: time.sleep(2 ** attempt) else: raise raise Exception(\"Max retries exceeded\") ``` ## Webhooks CloudFlow can send webhook notifications when specific events occur in your workflows.",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_4",
              "content": "**Technology**: Node.js with worker pool pattern **Replicas**: 8 pods (production), auto-scaling 6-16 **Resource Allocation**: 2 vCPU, 4GB RAM per pod **Key Responsibilities**: - Consume notification events from Kafka - Template rendering with Handlebars (cached in Redis) - Multi-channel delivery (Email via SendGrid, SMS via Twilio, Webhooks) - Retry logic with dead letter queue for failed deliveries - Delivery status tracking and analytics - User preference management (opt-in/opt-out) **Notification Flow**: ``` Event Source \u2192 Kafka (notifications.email topic) \u2502 \u25bc Notification Service Consumer \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u25bc \u25bc \u25bc [Email Queue] [SMS Queue] [Webhook Queue] \u2502 \u2502 \u2502 \u25bc \u25bc \u25bc [SendGrid] [Twilio] [HTTP POST] \u2502 \u2502 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u25bc Delivery Status \u2192 PostgreSQL ``` **Channel Configuration**: - Email: SendGrid with IP warming, DKIM/SPF configured - SMS: Twilio with fallback to AWS SNS - Webhook: Signed payloads (HMAC-SHA256), retry 3 times with exponential backoff - In-app: WebSocket connections via Socket.io **Template Management**: - Templates stored in PostgreSQL with versioning - Compiled templates cached in Redis (1-hour TTL) - Support for localization (i18n) with 12 languages - A/B testing capability for email campaigns **Performance Targets**: - Email delivery: < 5 seconds (P99) from event to SendGrid - SMS delivery: < 3 seconds (P99) - Throughput: 10,000 notifications per minute --- ## Data Flow Architecture ### Synchronous Request Flow Client requests follow a synchronous path through the API Gateway to backend services: ``` 1. Client \u2192 Load Balancer (TLS termination) 2. Load Balancer \u2192 API Gateway (HTTP/2) 3. API Gateway \u2192 Auth Service (JWT validation via gRPC) 4. API Gateway \u2192 Target Service (HTTP/REST or gRPC) 5. Target Service \u2192 Database/Cache (data retrieval) 6. Response propagates back through the chain ``` **Timeout Configuration**: - Client \u2192 Load Balancer: 60 seconds - Load Balancer \u2192 API Gateway: 55 seconds - API Gateway \u2192 Services: 45 seconds - Service \u2192 Database: 10 seconds ### Asynchronous Event Flow Long-running operations and inter-service communication use event-driven patterns: ``` 1. Service publishes event \u2192 Kafka topic 2. Kafka persists event (replication factor: 3) 3. Consumer groups subscribe to topics 4. Consumers process events with at-least-once delivery 5. State updates written to PostgreSQL 6.",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_1",
              "content": "**Technology**: Node.js with Express.js framework **Replicas**: 12 pods (production), auto-scaling 8-20 based on CPU **Resource Allocation**: 2 vCPU, 4GB RAM per pod **Key Responsibilities**: - JWT token validation (delegated to Auth Service for initial validation) - Rate limiting: 1000 requests per minute per API key (sliding window) - Request/response transformation and validation using JSON Schema - Routing to downstream services based on URL path patterns - CORS handling for web clients - Request/response logging and correlation ID injection **Critical Endpoints**: - `POST /api/v1/workflows` - Create new workflow - `GET /api/v1/workflows/:id` - Retrieve workflow status - `POST /api/v1/workflows/:id/execute` - Trigger workflow execution - `GET /api/v1/workflows/:id/history` - Get execution history **Dependencies**: - Auth Service (for token validation) - Redis (for rate limiting counters) - All downstream microservices **Performance Targets**: - P50 latency: < 50ms - P99 latency: < 200ms - Throughput: 10,000 RPS sustained --- ### Auth Service **Purpose**: Centralized authentication and authorization service handling user identity, token generation, and permission validation. **Technology**: Go with gRPC for internal communication, REST for external **Replicas**: 8 pods (production), auto-scaling 6-12 **Resource Allocation**: 1 vCPU, 2GB RAM per pod **Key Responsibilities**: - User authentication via multiple providers (OAuth2, SAML, local credentials) - JWT token generation and validation (RS256 algorithm) - Role-based access control (RBAC) with fine-grained permissions - Session management with Redis-backed storage - API key generation and validation for service accounts - MFA enforcement for administrative operations **Authentication Flow**: ``` Client Request \u2192 API Gateway \u2192 Auth Service \u2502 \u251c\u2500 Validate credentials \u251c\u2500 Check MFA if required \u251c\u2500 Generate JWT (15min expiry) \u251c\u2500 Generate refresh token (7 days) \u2514\u2500 Store session in Redis ``` **Token Structure**: - Access Token: JWT with 15-minute expiry - Refresh Token: Opaque token with 7-day expiry, stored in PostgreSQL - Claims: user_id, email, roles[], permissions[], tenant_id **Security Features**: - Password hashing: Argon2id with 64MB memory, 4 iterations - Token rotation on refresh to prevent replay attacks - Brute force protection: 5 failed attempts \u2192 15-minute lockout - Secrets stored in HashiCorp Vault, rotated every 90 days **Performance Targets**: - Token validation: < 10ms (P99) - Token generation: < 50ms (P99) - Throughput: 5,000 RPS for validation operations --- ### Workflow Engine **Purpose**: Core orchestration service that executes workflow definitions, manages state transitions, and coordinates task execution across distributed systems.",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_2",
              "content": "**Technology**: Node.js with TypeScript, Bull queue library **Replicas**: 16 pods (production), auto-scaling 12-24 **Resource Allocation**: 4 vCPU, 8GB RAM per pod **Key Responsibilities**: - Parse and validate workflow definitions (JSON-based DSL) - Execute workflow steps with state machine pattern - Handle retries with exponential backoff (max 3 retries, backoff: 2^n seconds) - Coordinate parallel and sequential task execution - Manage workflow state persistence and recovery - Support for conditional branching and loops - Sub-workflow invocation and composition **Workflow Execution Model**: ``` Workflow Submitted \u2192 Validation \u2192 Queue in Kafka \u2502 \u25bc Workflow Engine picks up \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u25bc \u25bc \u25bc [Task 1 Exec] [Task 2 Exec] [Task 3 Exec] \u2502 \u2502 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u25bc State Update \u2192 PostgreSQL \u2502 \u25bc Publish Event \u2192 Kafka ``` **Workflow DSL Example**: ```json { \"workflow_id\": \"customer-onboarding-v2\", \"version\": \"2.1.0\", \"steps\": [ { \"id\": \"validate-customer\", \"type\": \"http_request\", \"config\": { \"url\": \"https://validation.internal/verify\", \"method\": \"POST\", \"timeout\": 5000, \"retry\": {\"max_attempts\": 3, \"backoff\": \"exponential\"} } }, { \"id\": \"create-account\", \"type\": \"database_operation\", \"depends_on\": [\"validate-customer\"] } ] } ``` **State Management**: - Workflow states: PENDING, RUNNING, COMPLETED, FAILED, CANCELLED - Step states: QUEUED, EXECUTING, SUCCESS, FAILED, SKIPPED - State transitions stored in PostgreSQL with event sourcing pattern - Checkpointing every 10 steps for long-running workflows **Performance Targets**: - Simple workflow (3-5 steps): < 2 seconds (P99) - Complex workflow (20+ steps): < 5 seconds (P99) - Throughput: 500 concurrent workflow executions per pod --- ### Scheduler Service **Purpose**: Time-based workflow triggering system supporting cron-like schedules and one-time delayed executions.",
              "score": null
            },
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_1",
              "content": "## Rate Limiting To ensure fair usage and system stability, CloudFlow enforces rate limits on all API endpoints. **Default Limits:** - 100 requests per minute per authenticated user - 20 requests per minute for unauthenticated requests - Burst allowance: 150 requests in a 10-second window ### Rate Limit Headers Every API response includes rate limit information: ``` X-RateLimit-Limit: 100 X-RateLimit-Remaining: 87 X-RateLimit-Reset: 1640995200 ``` When you exceed the rate limit, you'll receive a `429 Too Many Requests` response: ```json { \"error\": { \"code\": \"rate_limit_exceeded\", \"message\": \"Rate limit exceeded. Please retry after 42 seconds.\", \"retry_after\": 42 } } ``` **Best Practices:** - Monitor `X-RateLimit-Remaining` header values - Implement exponential backoff when receiving 429 responses - Cache responses when appropriate to reduce API calls - Consider upgrading to Enterprise tier for higher limits (1000 req/min) ## Pagination List endpoints return paginated results to optimize performance. Use `limit` and `offset` parameters to navigate through result sets. **Parameters:** - `limit`: Number of items per page (default: 20, max: 100) - `offset`: Number of items to skip (default: 0) **Example Request:** ```bash curl -H \"X-API-Key: YOUR_API_KEY\" \\ \"https://api.cloudflow.io/v2/workflows?limit=50&offset=100\" ``` **Response Structure:** ```json { \"data\": [...], \"pagination\": { \"total\": 347, \"limit\": 50, \"offset\": 100, \"has_more\": true } } ``` **Python Example:** ```python import requests def fetch_all_workflows(api_key): base_url = \"https://api.cloudflow.io/v2/workflows\" headers = {\"X-API-Key\": api_key} all_workflows = [] offset = 0 limit = 100 while True: response = requests.get( base_url, headers=headers, params={\"limit\": limit, \"offset\": offset} ) data = response.json() all_workflows.extend(data['data']) if not data['pagination']['has_more']: break offset += limit return all_workflows ``` ## Endpoints ### Workflows #### List Workflows Retrieve a paginated list of all workflows in your account. **Endpoint:** `GET /workflows` **Query Parameters:** - `status` (optional): Filter by status (`active`, `paused`, `archived`) - `created_after` (optional): ISO 8601 timestamp - `limit` (optional): Items per page (max 100) - `offset` (optional): Pagination offset **Example Request:** ```bash curl -H \"X-API-Key: YOUR_API_KEY\" \\ \"https://api.cloudflow.io/v2/workflows?status=active&limit=25\" ``` **Example Response (200 OK):** ```json { \"data\": [ { \"id\": \"wf_8x7k2m9p\", \"name\": \"Data Processing Pipeline\", \"description\": \"Processes customer data every hour\", \"status\": \"active\", \"trigger\": { \"type\": \"schedule\", \"cron\": \"0 * * * *\" }, \"steps\": 12, \"created_at\": \"2026-01-15T10:30:00Z\", \"updated_at\": \"2026-01-23T14:22:00Z\", \"last_run\": \"2026-01-24T09:00:00Z\" } ], \"pagination\": { \"total\": 1, \"limit\": 25, \"offset\": 0, \"has_more\": false } } ``` #### Create Workflow Create a new workflow with specified configuration.",
              "score": null
            }
          ],
          "latency_ms": 11.515080001117894,
          "query_id": "realistic_006",
          "dimension": "negation",
          "query": "Why is 1GB RAM not enough for API Gateway?"
        },
        {
          "key_facts": [
            "/health",
            "/ready"
          ],
          "found_facts": [
            "/health",
            "/ready"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "deployment_guide"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_10",
              "content": "database nc -zv cloudflow-db.internal.company.com 5432 # Trace route traceroute api.cloudflow.io # Capture packets tcpdump -i any -w /tmp/capture.pcap port 5432 ``` --- ## Escalation Procedures ### Severity Levels CloudFlow incidents are classified into four severity levels: #### SEV-1: Critical (P1) - **Definition:** Complete service outage or severe degradation affecting all users - **Examples:** - API returns 5xx errors for > 5 minutes - Database completely unavailable - Authentication system down - Data loss or corruption - **Response Time:** Immediate (< 15 minutes) - **Escalation:** Page on-call engineer immediately #### SEV-2: High (P2) - **Definition:** Major functionality impaired affecting multiple users - **Examples:** - Workflow execution success rate < 90% - Significant performance degradation (p95 latency > 10s) - Rate limiting affecting large customer segment - **Response Time:** < 1 hour - **Escalation:** Create incident ticket and notify on-call #### SEV-3: Medium (P3) - **Definition:** Partial functionality degraded affecting some users - **Examples:** - Intermittent failures for specific workflow types - Minor performance issues - Non-critical feature unavailable - **Response Time:** < 4 hours - **Escalation:** Create ticket, normal business hours support #### SEV-4: Low (P4) - **Definition:** Minor issues with minimal user impact - **Examples:** - Cosmetic issues - Documentation errors - Feature requests - **Response Time:** < 2 business days - **Escalation:** Standard support ticket ### Escalation Steps #### Step 1: Initial Assessment (0-5 minutes) ```bash # Run health check cloudflow health check --comprehensive # Check status page curl https://status.cloudflow.io/api/v1/status.json # Review recent changes cloudflow audit log --last 2h --event-type \"deployment,configuration\" # Check metrics dashboard cloudflow metrics dashboard --incident-mode ``` #### Step 2: Gather Information (5-15 minutes) Create incident document with: - Incident timestamp and duration - Affected services and endpoints - Error rates and user impact - Recent changes or deployments - Relevant log excerpts - Correlation IDs for failed requests ```bash # Generate incident report cloudflow debug incident-report \\ --start \"2026-01-24T10:30:00Z\" \\ --end \"2026-01-24T11:00:00Z\" \\ --output incident-report.md # Capture system snapshot cloudflow debug snapshot --output snapshot-$(date +%Y%m%d-%H%M%S).tar.gz ``` #### Step 3: Escalate Based on Severity **For SEV-1 (Critical):** ```bash # Page on-call engineer cloudflow incident create \\ --severity SEV-1 \\ --title \"Complete API outage\" \\ --description \"All API requests returning 503\" \\ --page-oncall # Notify status page cloudflow status update \\ --status \"major_outage\" \\ --message \"We are investigating a complete service",
              "score": null
            },
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_2",
              "content": "- secretName: cloudflow-tls hosts: - api.cloudflow.io healthCheck: enabled: true livenessProbe: path: /health initialDelaySeconds: 30 periodSeconds: 10 timeoutSeconds: 5 failureThreshold: 3 readinessProbe: path: /ready initialDelaySeconds: 10 periodSeconds: 5 timeoutSeconds: 3 failureThreshold: 3 env: - name: NODE_ENV value: \"production\" - name: LOG_LEVEL value: \"info\" - name: API_PORT value: \"3000\" - name: WORKER_CONCURRENCY value: \"10\" - name: AWS_REGION value: \"us-east-1\" - name: METRICS_ENABLED value: \"true\" envFrom: - secretRef: name: cloudflow-secrets podAnnotations: prometheus.io/scrape: \"true\" prometheus.io/port: \"9090\" prometheus.io/path: \"/metrics\" podSecurityContext: runAsNonRoot: true runAsUser: 1000 fsGroup: 1000 securityContext: allowPrivilegeEscalation: false capabilities: drop: - ALL readOnlyRootFilesystem: true persistence: enabled: true storageClass: gp3 accessMode: ReadWriteOnce size: 50Gi redis: enabled: true architecture: replication auth: enabled: true master: persistence: size: 20Gi replica: replicaCount: 2 persistence: size: 20Gi ``` Deploy CloudFlow using Helm: ```bash helm install cloudflow cloudflow/cloudflow \\ --namespace cloudflow-prod \\ --values values-production.yaml \\ --wait \\ --timeout 10m ``` ### Deployment Verification Verify the deployment status: ```bash # Check pod status kubectl get pods -n cloudflow-prod # Expected output: # NAME READY STATUS RESTARTS AGE # cloudflow-api-7d8f9c5b6d-4xk2p 1/1 Running 0 2m # cloudflow-api-7d8f9c5b6d-9hj5m 1/1 Running 0 2m # cloudflow-api-7d8f9c5b6d-tn8wq 1/1 Running 0 2m # Check service endpoints kubectl get svc -n cloudflow-prod # Check ingress kubectl get ingress -n cloudflow-prod ``` Test health endpoints: ```bash # Port forward for testing kubectl port-forward -n cloudflow-prod svc/cloudflow-api 8080:80 # Test health endpoint curl http://localhost:8080/health # Expected: {\"status\":\"healthy\",\"timestamp\":\"2026-01-24T10:30:00Z\"} # Test readiness endpoint curl http://localhost:8080/ready # Expected: {\"status\":\"ready\",\"dependencies\":{\"database\":\"connected\",\"redis\":\"connected\"}} ``` --- ## Database Configuration ### PostgreSQL Setup CloudFlow uses PostgreSQL 14 as its primary data store.",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_11",
              "content": "Service (2) - PostgreSQL Primary - PostgreSQL Replica - PostgreSQL Replica - Redis Primary (2) - Redis Replica (2) - Redis Replica (2) - Kafka Broker (2) - Kafka Broker (2) - Kafka Broker (1) ``` **Automatic Failover**: - Database: 30-60 seconds (automatic promotion of replica) - Redis: < 10 seconds (Sentinel-based failover) - Kafka: < 30 seconds (controller election) - Services: Kubernetes health checks with 10-second liveness probes ### Backup Strategy **Database Backups**: - Automated snapshots: Daily at 02:00 UTC - Retention: 30 days for daily, 90 days for monthly - Cross-region replication: Async replication to us-west-2 (15-minute lag) - Backup verification: Weekly automated restore test in staging environment **Backup Schedule**: ``` Daily: Full snapshot \u2192 S3 (encrypted) Hourly: WAL archives \u2192 S3 (point-in-time recovery) Weekly: Backup validation test Monthly: Long-term archive to Glacier ``` **Configuration Backups**: - Kubernetes manifests: Stored in Git (GitOps with ArgoCD) - Vault secrets: Automated snapshot every 6 hours - Kafka topic configurations: Exported daily to S3 ### Disaster Recovery Procedures **Scenario 1: Single AZ Failure** - Detection: Health checks fail for entire AZ (< 30 seconds) - Action: Traffic automatically routed to healthy AZs by ALB - Recovery time: < 5 minutes (no manual intervention) - Data loss: None (multi-AZ replication) **Scenario 2: Database Primary Failure** - Detection: Health check fails for primary database (< 30 seconds) - Action: Automatic promotion of read replica to primary - Recovery time: 30-60 seconds - Data loss: Minimal (< 1 second due to synchronous replication) **Scenario 3: Full Region Failure** - Detection: Multiple health check failures across all AZs (< 2 minutes) - Action: Manual failover to DR region (us-west-2) - Procedure: 1. Update DNS to point to DR region (TTL: 60 seconds) 2. Promote DR database replica to primary 3. Scale up DR region services to production capacity 4. Verify data consistency and integrity 5. Update monitoring dashboards - Recovery time: 2-4 hours (includes verification) - Data loss: < 1 hour (last cross-region replication) **Scenario 4: Data Corruption** - Detection: Data validation checks or user report - Action: Point-in-time recovery from WAL archives - Procedure: 1. Identify corruption time window 2. Restore from snapshot prior to corruption 3. Replay WAL logs up to corruption point 4. Verify data integrity 5.",
              "score": null
            },
            {
              "doc_id": "user_guide",
              "chunk_id": "user_guide_fix_8",
              "content": "Version Control For critical workflows: - Export YAML definitions regularly - Store in version control (Git) - Use pull requests for changes - Tag releases with semantic versioning ## Common Workflow Patterns Here are proven patterns for common automation scenarios: ### Pattern 1: Form to Database Capture form submissions and store in database: ```yaml name: \"Contact Form to Database\" trigger: type: webhook method: POST steps: - id: validate_submission action: javascript code: | if (!input.email || !input.message) { throw new Error(\"Email and message required\"); } return input; - id: check_duplicate action: database_query config: connection: \"{{secrets.DB_CONNECTION}}\" query: \"SELECT id FROM contacts WHERE email = $1 AND created_at > NOW() - INTERVAL '1 hour'\" parameters: - \"{{trigger.body.email}}\" - id: insert_contact action: database_query condition: \"{{length(steps.check_duplicate.rows) == 0}}\" config: connection: \"{{secrets.DB_CONNECTION}}\" query: | INSERT INTO contacts (name, email, message, source, created_at) VALUES ($1, $2, $3, $4, NOW()) parameters: - \"{{trigger.body.name}}\" - \"{{trigger.body.email}}\" - \"{{trigger.body.message}}\" - \"website_form\" - id: notify_sales action: slack_message condition: \"{{steps.insert_contact.affected_rows > 0}}\" config: channel: \"#leads\" text: \"New contact form submission from {{trigger.body.email}}\" ``` ### Pattern 2: API Polling Periodically check an API and take action on new items: ```yaml name: \"Poll GitHub Issues\" schedule: cron: \"*/15 * * * *\" # Every 15 minutes timezone: \"UTC\" steps: - id: get_last_check action: database_query config: query: \"SELECT last_checked_at FROM workflow_state WHERE workflow_id = $1\" parameters: - \"{{workflow.id}}\" - id: fetch_issues action: http_request config: method: GET url: \"https://api.github.com/repos/company/project/issues\" params: since: \"{{steps.get_last_check.rows[0].last_checked_at}}\" state: \"open\" headers: Authorization: \"token {{secrets.GITHUB_TOKEN}}\" - id: process_new_issues action: javascript code: | const issues = input.body; return { count: issues.length, issues: issues.map(i => ({ number: i.number, title: i.title, url: i.html_url })) }; - id: notify_team action: slack_message condition: \"{{steps.process_new_issues.output.count > 0}}\" config: channel: \"#engineering\" text: \"{{steps.process_new_issues.output.count}} new GitHub issues\" - id: update_last_check action: database_query config: query: | UPDATE workflow_state SET last_checked_at = $1 WHERE workflow_id = $2 parameters: - \"{{now()}}\" - \"{{workflow.id}}\" ``` ### Pattern 3: Multi-Step Approval Implement approval workflows with timeouts: ```yaml name: \"Expense Approval Workflow\" trigger: type: webhook method: POST steps: - id: create_approval_request action: database_query config: query: | INSERT INTO approvals (expense_id, amount, requester, status, created_at) VALUES ($1, $2, $3, 'pending', NOW()) RETURNING id parameters: - \"{{trigger.body.expense_id}}\" - \"{{trigger.body.amount}}\" - \"{{trigger.body.requester}}\" - id: notify_manager action: email config: to: \"{{trigger.body.manager_email}}\" subject: \"Expense Approval Required: ${{trigger.body.amount}}\" body: | An expense requires your approval: Amount: ${{trigger.body.amount}}",
              "score": null
            },
            {
              "doc_id": "user_guide",
              "chunk_id": "user_guide_fix_6",
              "content": "Filter by error type, date range, or execution ID 3. Click an execution to view full details 4. Click **\"Retry\"** to reprocess with the same input data ### Error Notifications Get notified when workflows fail: ```yaml workflow: notifications: on_failure: - type: email to: \"ops-team@company.com\" - type: slack channel: \"#alerts\" message: \"Workflow {{workflow.name}} failed: {{error.message}}\" on_success_after_retry: - type: slack channel: \"#monitoring\" message: \"Workflow recovered after {{error.attempts}} attempts\" ``` ## Workflow Limits CloudFlow enforces the following limits to ensure platform stability and performance: ### Steps Per Workflow - **Maximum**: 50 steps per workflow - **Recommendation**: Keep workflows focused and modular. If you need more steps, consider splitting into multiple workflows connected via webhooks. ### Execution Timeout - **Default**: 3600 seconds (60 minutes) - **Behavior**: Workflows exceeding this timeout are automatically terminated - **Custom Timeouts**: Enterprise plans can request custom timeout limits **Setting Step-Level Timeouts:** ```yaml - id: long_running_task action: http_request config: url: \"https://api.example.com/process\" timeout: 300 # 5 minutes for this specific step ``` ### Execution Limits - **Maximum**: 1000 executions per day (per workflow) - **Rate Limiting**: 100 concurrent executions per workflow - **Burst Limit**: 10 executions per second **What happens when limits are reached:** - New executions are queued automatically - Webhook triggers return HTTP 429 (Too Many Requests) - Scheduled executions are skipped (logged in audit trail) - Email notifications sent to workflow owner **Monitoring Usage:** View real-time metrics in your workflow dashboard: - Executions today: 847 / 1000 - Average duration: 12.3s - Success rate: 98.2% - Current queue depth: 3 ### Data Limits - **Maximum request/response size**: 10MB per action - **Maximum execution payload**: 50MB total - **Variable value size**: 1MB per variable ### Enterprise Plan Limits Enterprise customers can request increased limits: - Up to 100 steps per workflow - Up to 10,000 executions per day - Up to 7200 second timeout (2 hours) - Priority execution queue - Dedicated capacity allocation Contact sales@cloudflow.io for Enterprise pricing and custom limits. ## Best Practices Follow these best practices to build reliable, maintainable workflows: ### 1. Use Descriptive Names **Good:** - Workflow: \"Sync Customer Data from Salesforce to Database\" - Step: \"validate_customer_email\" **Bad:** - Workflow: \"Workflow 1\" - Step: \"step3\" ### 2.",
              "score": null
            }
          ],
          "latency_ms": 11.3709219986049,
          "query_id": "realistic_007",
          "dimension": "original",
          "query": "What are the health check endpoints?"
        },
        {
          "key_facts": [
            "/health",
            "/ready"
          ],
          "found_facts": [
            "/health",
            "/ready"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "deployment_guide"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_10",
              "content": "database nc -zv cloudflow-db.internal.company.com 5432 # Trace route traceroute api.cloudflow.io # Capture packets tcpdump -i any -w /tmp/capture.pcap port 5432 ``` --- ## Escalation Procedures ### Severity Levels CloudFlow incidents are classified into four severity levels: #### SEV-1: Critical (P1) - **Definition:** Complete service outage or severe degradation affecting all users - **Examples:** - API returns 5xx errors for > 5 minutes - Database completely unavailable - Authentication system down - Data loss or corruption - **Response Time:** Immediate (< 15 minutes) - **Escalation:** Page on-call engineer immediately #### SEV-2: High (P2) - **Definition:** Major functionality impaired affecting multiple users - **Examples:** - Workflow execution success rate < 90% - Significant performance degradation (p95 latency > 10s) - Rate limiting affecting large customer segment - **Response Time:** < 1 hour - **Escalation:** Create incident ticket and notify on-call #### SEV-3: Medium (P3) - **Definition:** Partial functionality degraded affecting some users - **Examples:** - Intermittent failures for specific workflow types - Minor performance issues - Non-critical feature unavailable - **Response Time:** < 4 hours - **Escalation:** Create ticket, normal business hours support #### SEV-4: Low (P4) - **Definition:** Minor issues with minimal user impact - **Examples:** - Cosmetic issues - Documentation errors - Feature requests - **Response Time:** < 2 business days - **Escalation:** Standard support ticket ### Escalation Steps #### Step 1: Initial Assessment (0-5 minutes) ```bash # Run health check cloudflow health check --comprehensive # Check status page curl https://status.cloudflow.io/api/v1/status.json # Review recent changes cloudflow audit log --last 2h --event-type \"deployment,configuration\" # Check metrics dashboard cloudflow metrics dashboard --incident-mode ``` #### Step 2: Gather Information (5-15 minutes) Create incident document with: - Incident timestamp and duration - Affected services and endpoints - Error rates and user impact - Recent changes or deployments - Relevant log excerpts - Correlation IDs for failed requests ```bash # Generate incident report cloudflow debug incident-report \\ --start \"2026-01-24T10:30:00Z\" \\ --end \"2026-01-24T11:00:00Z\" \\ --output incident-report.md # Capture system snapshot cloudflow debug snapshot --output snapshot-$(date +%Y%m%d-%H%M%S).tar.gz ``` #### Step 3: Escalate Based on Severity **For SEV-1 (Critical):** ```bash # Page on-call engineer cloudflow incident create \\ --severity SEV-1 \\ --title \"Complete API outage\" \\ --description \"All API requests returning 503\" \\ --page-oncall # Notify status page cloudflow status update \\ --status \"major_outage\" \\ --message \"We are investigating a complete service",
              "score": null
            },
            {
              "doc_id": "user_guide",
              "chunk_id": "user_guide_fix_2",
              "content": "### HTTP Requests Make HTTP requests to any API endpoint: **Configuration:** - **Method**: GET, POST, PUT, PATCH, DELETE, HEAD - **URL**: Full endpoint URL (supports variable interpolation) - **Headers**: Custom headers as key-value pairs - **Query Parameters**: URL parameters - **Body**: JSON, form data, or raw text - **Authentication**: Basic Auth, Bearer Token, API Key, OAuth 2.0 **Example:** ```yaml - id: fetch_user action: http_request config: method: GET url: \"https://api.example.com/users/{{user_id}}\" headers: Authorization: \"Bearer {{secrets.API_TOKEN}}\" Content-Type: \"application/json\" timeout: 30 ``` **Response Handling:** Access the response in subsequent steps: - `{{steps.fetch_user.status}}` - HTTP status code - `{{steps.fetch_user.body}}` - Response body - `{{steps.fetch_user.headers}}` - Response headers ### Database Queries Execute queries against supported databases (PostgreSQL, MySQL, MongoDB, Redis): **SQL Databases (PostgreSQL, MySQL):** ```yaml - id: get_orders action: database_query config: connection: \"{{secrets.DB_CONNECTION_STRING}}\" query: | SELECT * FROM orders WHERE customer_id = $1 AND status = $2 ORDER BY created_at DESC LIMIT 10 parameters: - \"{{trigger.customer_id}}\" - \"pending\" ``` **MongoDB:** ```yaml - id: find_documents action: mongodb_query config: connection: \"{{secrets.MONGO_URI}}\" database: \"production\" collection: \"users\" operation: find filter: email: \"{{trigger.email}}\" options: limit: 1 ``` **Important Notes:** - Always use parameterized queries to prevent SQL injection - Connection strings should be stored in secrets - Query timeout is 30 seconds by default - Maximum result set size is 10MB ### Email Notifications Send emails via SMTP or integrated email providers: **Configuration:** ```yaml - id: send_notification action: email config: provider: \"smtp\" # or \"sendgrid\", \"mailgun\", \"ses\" from: \"notifications@cloudflow.io\" to: \"{{trigger.recipient}}\" cc: \"manager@company.com\" subject: \"Alert: {{alert_type}}\" body: | Hello {{user.name}}, This is an automated notification about {{event_description}}. Details: - Time: {{timestamp}} - Type: {{alert_type}} - Priority: {{priority}} Best regards, CloudFlow Automation html: true attachments: - name: \"report.pdf\" content: \"{{steps.generate_report.output}}\" encoding: \"base64\" ``` **Email Templates:** CloudFlow supports dynamic templates with conditional content: ``` {{#if priority == 'high'}} \u26a0\ufe0f URGENT: Immediate attention required {{else}} \u2139\ufe0f Informational notification {{/if}} ``` ### Slack Messages Send messages to Slack channels or users: **Channel Messages:** ```yaml - id: notify_team action: slack_message config: channel: \"#deployments\" text: \"Deployment completed successfully!\" blocks: - type: \"section\" text: type: \"mrkdwn\" text: \"*Deployment Summary*\\nEnvironment: {{environment}}\\nVersion: {{version}}\\nDuration: {{duration}}s\" - type: \"actions\" elements: - type: \"button\" text: \"View Logs\" url: \"{{logs_url}}\" thread_ts: \"{{trigger.thread_ts}}\" # Reply to thread ``` **Direct Messages:** ```yaml - id: dm_user action: slack_message config: user: \"{{user_slack_id}}\" text: \"Your report is ready for review.\"",
              "score": null
            },
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_2",
              "content": "- secretName: cloudflow-tls hosts: - api.cloudflow.io healthCheck: enabled: true livenessProbe: path: /health initialDelaySeconds: 30 periodSeconds: 10 timeoutSeconds: 5 failureThreshold: 3 readinessProbe: path: /ready initialDelaySeconds: 10 periodSeconds: 5 timeoutSeconds: 3 failureThreshold: 3 env: - name: NODE_ENV value: \"production\" - name: LOG_LEVEL value: \"info\" - name: API_PORT value: \"3000\" - name: WORKER_CONCURRENCY value: \"10\" - name: AWS_REGION value: \"us-east-1\" - name: METRICS_ENABLED value: \"true\" envFrom: - secretRef: name: cloudflow-secrets podAnnotations: prometheus.io/scrape: \"true\" prometheus.io/port: \"9090\" prometheus.io/path: \"/metrics\" podSecurityContext: runAsNonRoot: true runAsUser: 1000 fsGroup: 1000 securityContext: allowPrivilegeEscalation: false capabilities: drop: - ALL readOnlyRootFilesystem: true persistence: enabled: true storageClass: gp3 accessMode: ReadWriteOnce size: 50Gi redis: enabled: true architecture: replication auth: enabled: true master: persistence: size: 20Gi replica: replicaCount: 2 persistence: size: 20Gi ``` Deploy CloudFlow using Helm: ```bash helm install cloudflow cloudflow/cloudflow \\ --namespace cloudflow-prod \\ --values values-production.yaml \\ --wait \\ --timeout 10m ``` ### Deployment Verification Verify the deployment status: ```bash # Check pod status kubectl get pods -n cloudflow-prod # Expected output: # NAME READY STATUS RESTARTS AGE # cloudflow-api-7d8f9c5b6d-4xk2p 1/1 Running 0 2m # cloudflow-api-7d8f9c5b6d-9hj5m 1/1 Running 0 2m # cloudflow-api-7d8f9c5b6d-tn8wq 1/1 Running 0 2m # Check service endpoints kubectl get svc -n cloudflow-prod # Check ingress kubectl get ingress -n cloudflow-prod ``` Test health endpoints: ```bash # Port forward for testing kubectl port-forward -n cloudflow-prod svc/cloudflow-api 8080:80 # Test health endpoint curl http://localhost:8080/health # Expected: {\"status\":\"healthy\",\"timestamp\":\"2026-01-24T10:30:00Z\"} # Test readiness endpoint curl http://localhost:8080/ready # Expected: {\"status\":\"ready\",\"dependencies\":{\"database\":\"connected\",\"redis\":\"connected\"}} ``` --- ## Database Configuration ### PostgreSQL Setup CloudFlow uses PostgreSQL 14 as its primary data store.",
              "score": null
            },
            {
              "doc_id": "user_guide",
              "chunk_id": "user_guide_fix_5",
              "content": "Save and activate **Testing Schedules:** Use the built-in schedule calculator to preview upcoming executions: ``` Next 5 executions: 1. 2026-01-24 14:00:00 UTC 2. 2026-01-25 14:00:00 UTC 3. 2026-01-26 14:00:00 UTC 4. 2026-01-27 14:00:00 UTC 5. 2026-01-28 14:00:00 UTC ``` ## Error Handling Robust error handling ensures your workflows are resilient and reliable. ### Retry Policies Configure automatic retries for failed actions: ```yaml - id: api_call action: http_request config: url: \"https://api.example.com/data\" retry: max_attempts: 3 backoff_type: \"exponential\" # or \"fixed\", \"linear\" initial_interval: 1000 # milliseconds max_interval: 30000 multiplier: 2.0 retry_on: - timeout - network_error - status: [500, 502, 503, 504] ``` **Backoff Strategies:** - **Fixed**: Wait the same amount of time between retries - Attempt 1: Wait 1s - Attempt 2: Wait 1s - Attempt 3: Wait 1s - **Linear**: Increase wait time by a fixed amount - Attempt 1: Wait 1s - Attempt 2: Wait 2s - Attempt 3: Wait 3s - **Exponential**: Double the wait time with each retry (recommended) - Attempt 1: Wait 1s - Attempt 2: Wait 2s - Attempt 3: Wait 4s **Retry Conditions:** Control which errors trigger retries: - `timeout`: Request timeout - `network_error`: Connection failures - `status`: Specific HTTP status codes - `error_code`: Application-specific error codes ### Fallback Actions Execute alternative actions when the primary action fails: ```yaml - id: primary_payment action: http_request config: url: \"https://primary-payment-gateway.com/charge\" method: POST body: amount: \"{{amount}}\" on_error: - id: fallback_payment action: http_request config: url: \"https://backup-payment-gateway.com/charge\" method: POST body: amount: \"{{amount}}\" - id: notify_admin action: email config: to: \"admin@company.com\" subject: \"Payment Gateway Failure\" body: \"Primary gateway failed, switched to backup\" ``` **Error Object:** Access error details in fallback actions: ``` {{error.message}} # Error message {{error.code}} # Error code {{error.step_id}} # Failed step ID {{error.timestamp}} # When the error occurred {{error.attempts}} # Number of retry attempts ``` ### Dead Letter Queue When all retries and fallbacks fail, CloudFlow can route failed executions to a Dead Letter Queue (DLQ) for manual review: **Enable DLQ:** ```yaml workflow: error_handling: dead_letter_queue: enabled: true retain_days: 30 ``` **DLQ Features:** - View failed executions in the dashboard - Inspect complete execution context and error details - Retry individual executions after fixing issues - Export failed executions for analysis - Set up alerts for DLQ threshold breaches **Accessing the DLQ:** 1. Navigate to **Workflows** > **[Your Workflow]** > **Dead Letter Queue** 2.",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_11",
              "content": "outage\" # Create war room cloudflow incident war-room create incident-2024012401 ``` **For SEV-2 (High):** ```bash # Create incident and notify cloudflow incident create \\ --severity SEV-2 \\ --title \"High rate of workflow failures\" \\ --description \"Workflow success rate dropped to 75%\" \\ --notify-oncall # Update status page cloudflow status update \\ --status \"partial_outage\" \\ --message \"We are experiencing elevated error rates\" ``` **For SEV-3/SEV-4:** ```bash # Create support ticket cloudflow support ticket create \\ --priority MEDIUM \\ --subject \"Intermittent timeout errors\" \\ --description \"See attached logs and reproduction steps\" ``` ### Contact Information - **On-call hotline:** +1-888-CLOUDFLOW (24/7) - **Support email:** support@cloudflow.io - **Incident Slack channel:** #cloudflow-incidents - **Status page:** https://status.cloudflow.io - **Documentation:** https://docs.cloudflow.io ### Post-Incident Review After resolving incidents, complete a postmortem: ```bash # Generate postmortem template cloudflow incident postmortem incident-2024012401 \\ --template standard \\ --output postmortem-2024012401.md # Required sections: # 1. Incident summary # 2. Impact assessment # 3. Timeline of events # 4. Root cause analysis # 5. Resolution steps # 6. Action items # 7. Lessons learned ``` --- ## Additional Resources - **CloudFlow Documentation:** https://docs.cloudflow.io - **API Reference:** https://api-docs.cloudflow.io - **Community Forum:** https://community.cloudflow.io - **GitHub Issues:** https://github.com/cloudflow/platform/issues - **Training Portal:** https://training.cloudflow.io - **Monitoring Dashboard:** https://monitoring.cloudflow.io ### Getting Help If this troubleshooting guide doesn't resolve your issue: 1. Search the knowledge base: `cloudflow kb search \"your issue\"` 2. Check community forum for similar issues 3. Contact support with detailed logs and reproduction steps 4. For urgent issues, use emergency escalation procedures **Remember:** Always capture logs, metrics, and reproduction steps before escalating! --- *Last updated: January 24, 2026* *Document version: 3.2.1* *Feedback: docs-feedback@cloudflow.io*",
              "score": null
            }
          ],
          "latency_ms": 11.424661002820358,
          "query_id": "realistic_007",
          "dimension": "synonym",
          "query": "Which URLs should I use for health monitoring?"
        },
        {
          "key_facts": [
            "/health",
            "/ready"
          ],
          "found_facts": [
            "/health"
          ],
          "missed_facts": [
            "/ready"
          ],
          "coverage": 0.5,
          "expected_docs": [
            "deployment_guide"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_6",
              "content": "**Verify Service Health**: ```bash kubectl get pods -n cloudflow-prod curl https://api.cloudflow.io/health ``` **Recovery Time Objective (RTO)**: 4 hours **Recovery Point Objective (RPO)**: 24 hours --- ## Scaling and Performance ### Horizontal Pod Autoscaling CloudFlow is configured with Horizontal Pod Autoscaler (HPA) to automatically scale based on resource utilization: ```yaml # hpa.yaml apiVersion: autoscaling/v2 kind: HorizontalPodAutoscaler metadata: name: cloudflow-api-hpa namespace: cloudflow-prod spec: scaleTargetRef: apiVersion: apps/v1 kind: Deployment name: cloudflow-api minReplicas: 3 maxReplicas: 10 metrics: - type: Resource resource: name: cpu target: type: Utilization averageUtilization: 70 - type: Resource resource: name: memory target: type: Utilization averageUtilization: 80 behavior: scaleDown: stabilizationWindowSeconds: 300 policies: - type: Percent value: 50 periodSeconds: 60 scaleUp: stabilizationWindowSeconds: 60 policies: - type: Percent value: 100 periodSeconds: 30 - type: Pods value: 2 periodSeconds: 30 selectPolicy: Max ``` Monitor autoscaling events: ```bash kubectl get hpa -n cloudflow-prod -w kubectl describe hpa cloudflow-api-hpa -n cloudflow-prod ``` ### Performance Optimization #### Connection Pooling Ensure optimal database connection pooling settings in PgBouncer: - **Pool Mode**: Transaction (optimal for microservices) - **Default Pool Size**: 25 connections per user - **Max DB Connections**: 100 (matches PostgreSQL `max_connections`) #### Redis Caching Implement Redis caching for frequently accessed data: ```javascript // Example caching strategy const CACHE_TTL = { workflows: 300, // 5 minutes userSessions: 3600, // 1 hour apiResults: 60 // 1 minute }; ``` #### Load Testing Perform regular load testing to validate scaling configuration: ```bash # Install k6 for load testing kubectl apply -f - <<EOF apiVersion: v1 kind: ConfigMap metadata: name: loadtest-script namespace: cloudflow-prod data: script.js: | import http from 'k6/http'; import { check, sleep } from 'k6'; export let options = { stages: [ { duration: '2m', target: 100 }, { duration: '5m', target: 100 }, { duration: '2m', target: 200 }, { duration: '5m', target: 200 }, { duration: '2m', target: 0 }, ], thresholds: { http_req_duration: ['p(95)<500'], }, }; export default function () { let res = http.get('https://api.cloudflow.io/api/workflows'); check(res, { 'status is 200': (r) => r.status === 200 }); sleep(1); } EOF ``` **Performance Targets**: - P95 API latency: < 500ms - P99 API latency: < 1000ms - Error rate: < 0.1% - Throughput: > 1000 requests/second --- ## Security and Compliance ### Network Policies Implement network policies to restrict pod-to-pod communication: ```yaml # network-policy.yaml apiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: cloudflow-network-policy namespace: cloudflow-prod spec:",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_6",
              "content": "Data Validation Errors** ``` ValidationError: Field 'customer_id' is required but missing in 234 records ``` **Resolution:** ```bash # Add data quality checks cloudflow workflows update wf_9k2n4m8p1q \\ --step data_ingestion \\ --add-validator required_fields \\ --validator-config '{\"fields\": [\"customer_id\", \"timestamp\", \"amount\"]}' # Configure error handling cloudflow workflows update wf_9k2n4m8p1q \\ --step data_validation \\ --on-error continue \\ --error-threshold 5% # Fail if > 5% of records invalid ``` **2. External API Failures** ``` ExternalAPIError: API request to https://partner-api.example.com failed with status 502 ``` **Resolution:** ```bash # Add circuit breaker cloudflow workflows update wf_9k2n4m8p1q \\ --step external_api_call \\ --circuit-breaker-enabled true \\ --circuit-breaker-threshold 5 \\ --circuit-breaker-timeout 30000 # Configure fallback behavior cloudflow workflows update wf_9k2n4m8p1q \\ --step external_api_call \\ --fallback-action use_cached_data \\ --cache-ttl 3600 ``` --- ## Rate Limiting & Throttling ### 429 Too Many Requests #### Error Response ```json { \"error\": \"rate_limit_exceeded\", \"message\": \"API rate limit exceeded. Retry after 45 seconds.\", \"status\": 429, \"headers\": { \"X-RateLimit-Limit\": \"1000\", \"X-RateLimit-Remaining\": \"0\", \"X-RateLimit-Reset\": \"1706112345\", \"Retry-After\": \"45\" } } ``` #### Rate Limit Tiers CloudFlow enforces the following rate limits per workspace: | Tier | Requests/Minute | Requests/Hour | Concurrent Workflows | |------|-----------------|---------------|----------------------| | Free | 60 | 1,000 | 5 | | Standard | 1,000 | 50,000 | 50 | | Premium | 5,000 | 250,000 | 200 | | Enterprise | Custom | Custom | Unlimited | #### Checking Rate Limit Status ```bash # Check current rate limit status curl -I https://api.cloudflow.io/api/v1/workflows \\ -H \"Authorization: Bearer $CF_ACCESS_TOKEN\" # Extract rate limit headers curl -s -I https://api.cloudflow.io/api/v1/workflows \\ -H \"Authorization: Bearer $CF_ACCESS_TOKEN\" | \\ grep -i \"x-ratelimit\" # Output: # X-RateLimit-Limit: 1000 # X-RateLimit-Remaining: 247 # X-RateLimit-Reset: 1706112400 # Monitor rate limit usage cloudflow metrics query \\ --metric rate_limit_remaining \\ --workspace ws_abc123 \\ --last 1h \\ --interval 1m ``` #### Handling Rate Limits in Code **Python example with retry logic:** ```python import time import requests def cloudflow_api_call_with_retry(url, headers, max_retries=3): for attempt in range(max_retries): response = requests.get(url, headers=headers) if response.status_code == 429: retry_after = int(response.headers.get('Retry-After', 60)) print(f\"Rate limited.",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_10",
              "content": "database nc -zv cloudflow-db.internal.company.com 5432 # Trace route traceroute api.cloudflow.io # Capture packets tcpdump -i any -w /tmp/capture.pcap port 5432 ``` --- ## Escalation Procedures ### Severity Levels CloudFlow incidents are classified into four severity levels: #### SEV-1: Critical (P1) - **Definition:** Complete service outage or severe degradation affecting all users - **Examples:** - API returns 5xx errors for > 5 minutes - Database completely unavailable - Authentication system down - Data loss or corruption - **Response Time:** Immediate (< 15 minutes) - **Escalation:** Page on-call engineer immediately #### SEV-2: High (P2) - **Definition:** Major functionality impaired affecting multiple users - **Examples:** - Workflow execution success rate < 90% - Significant performance degradation (p95 latency > 10s) - Rate limiting affecting large customer segment - **Response Time:** < 1 hour - **Escalation:** Create incident ticket and notify on-call #### SEV-3: Medium (P3) - **Definition:** Partial functionality degraded affecting some users - **Examples:** - Intermittent failures for specific workflow types - Minor performance issues - Non-critical feature unavailable - **Response Time:** < 4 hours - **Escalation:** Create ticket, normal business hours support #### SEV-4: Low (P4) - **Definition:** Minor issues with minimal user impact - **Examples:** - Cosmetic issues - Documentation errors - Feature requests - **Response Time:** < 2 business days - **Escalation:** Standard support ticket ### Escalation Steps #### Step 1: Initial Assessment (0-5 minutes) ```bash # Run health check cloudflow health check --comprehensive # Check status page curl https://status.cloudflow.io/api/v1/status.json # Review recent changes cloudflow audit log --last 2h --event-type \"deployment,configuration\" # Check metrics dashboard cloudflow metrics dashboard --incident-mode ``` #### Step 2: Gather Information (5-15 minutes) Create incident document with: - Incident timestamp and duration - Affected services and endpoints - Error rates and user impact - Recent changes or deployments - Relevant log excerpts - Correlation IDs for failed requests ```bash # Generate incident report cloudflow debug incident-report \\ --start \"2026-01-24T10:30:00Z\" \\ --end \"2026-01-24T11:00:00Z\" \\ --output incident-report.md # Capture system snapshot cloudflow debug snapshot --output snapshot-$(date +%Y%m%d-%H%M%S).tar.gz ``` #### Step 3: Escalate Based on Severity **For SEV-1 (Critical):** ```bash # Page on-call engineer cloudflow incident create \\ --severity SEV-1 \\ --title \"Complete API outage\" \\ --description \"All API requests returning 503\" \\ --page-oncall # Notify status page cloudflow status update \\ --status \"major_outage\" \\ --message \"We are investigating a complete service",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_0",
              "content": "# CloudFlow Platform Troubleshooting Guide **Version:** 3.2.1 **Last Updated:** January 2026 **Audience:** Platform Engineers, DevOps, Support Teams ## Table of Contents 1. [Overview](#overview) 2. [Authentication & Authorization Issues](#authentication--authorization-issues) 3. [Performance Problems](#performance-problems) 4. [Database Connection Issues](#database-connection-issues) 5. [Workflow Execution Failures](#workflow-execution-failures) 6. [Rate Limiting & Throttling](#rate-limiting--throttling) 7. [Log Analysis & Debugging](#log-analysis--debugging) 8. [Escalation Procedures](#escalation-procedures) --- ## Overview This guide provides comprehensive troubleshooting steps for common CloudFlow platform issues encountered in production environments. Each section includes error symptoms, root cause analysis, resolution steps, and preventive measures. ### Quick Diagnostic Checklist Before diving into specific issues, perform these initial checks: - Verify service health: `cloudflow status --all` - Check API connectivity: `curl -I https://api.cloudflow.io/health` - Review recent deployments: `kubectl get deployments -n cloudflow --sort-by=.metadata.creationTimestamp` - Inspect platform metrics: `cloudflow metrics --last 1h` --- ## Authentication & Authorization Issues ### 401 Unauthorized Errors #### Symptoms - API requests return `401 Unauthorized` - Error message: `Authentication credentials were not provided or are invalid` - Frontend displays \"Session expired\" message #### Common Causes **1. Token Expiration** CloudFlow access tokens expire after 3600 seconds (1 hour) by default. Refresh tokens are valid for 30 days. **Verification:** ```bash # Decode JWT to check expiration echo $CF_ACCESS_TOKEN | cut -d'.' -f2 | base64 -d | jq '.exp' # Compare with current time date +%s ``` **Resolution:** ```bash # Refresh the access token curl -X POST https://api.cloudflow.io/auth/refresh \\ -H \"Content-Type: application/json\" \\ -d '{\"refresh_token\": \"'$CF_REFRESH_TOKEN'\"}' # Update environment variable export CF_ACCESS_TOKEN=\"<new_token>\" ``` **2. Invalid JWT Signature** Error: `JWT signature verification failed` **Causes:** - Token was modified or corrupted - Using wrong signing key - Token generated with different secret **Resolution:** ```bash # Validate token structure cloudflow auth validate --token $CF_ACCESS_TOKEN # Generate new token cloudflow auth login --username <user> --password <pass> # For service accounts cloudflow auth service-account --create --name <sa-name> --scopes \"workflow:read,workflow:write\" ``` **3. Clock Skew Issues** JWT validation fails when system clocks are out of sync (tolerance: \u00b1300 seconds).",
              "score": null
            },
            {
              "doc_id": "user_guide",
              "chunk_id": "user_guide_fix_6",
              "content": "Filter by error type, date range, or execution ID 3. Click an execution to view full details 4. Click **\"Retry\"** to reprocess with the same input data ### Error Notifications Get notified when workflows fail: ```yaml workflow: notifications: on_failure: - type: email to: \"ops-team@company.com\" - type: slack channel: \"#alerts\" message: \"Workflow {{workflow.name}} failed: {{error.message}}\" on_success_after_retry: - type: slack channel: \"#monitoring\" message: \"Workflow recovered after {{error.attempts}} attempts\" ``` ## Workflow Limits CloudFlow enforces the following limits to ensure platform stability and performance: ### Steps Per Workflow - **Maximum**: 50 steps per workflow - **Recommendation**: Keep workflows focused and modular. If you need more steps, consider splitting into multiple workflows connected via webhooks. ### Execution Timeout - **Default**: 3600 seconds (60 minutes) - **Behavior**: Workflows exceeding this timeout are automatically terminated - **Custom Timeouts**: Enterprise plans can request custom timeout limits **Setting Step-Level Timeouts:** ```yaml - id: long_running_task action: http_request config: url: \"https://api.example.com/process\" timeout: 300 # 5 minutes for this specific step ``` ### Execution Limits - **Maximum**: 1000 executions per day (per workflow) - **Rate Limiting**: 100 concurrent executions per workflow - **Burst Limit**: 10 executions per second **What happens when limits are reached:** - New executions are queued automatically - Webhook triggers return HTTP 429 (Too Many Requests) - Scheduled executions are skipped (logged in audit trail) - Email notifications sent to workflow owner **Monitoring Usage:** View real-time metrics in your workflow dashboard: - Executions today: 847 / 1000 - Average duration: 12.3s - Success rate: 98.2% - Current queue depth: 3 ### Data Limits - **Maximum request/response size**: 10MB per action - **Maximum execution payload**: 50MB total - **Variable value size**: 1MB per variable ### Enterprise Plan Limits Enterprise customers can request increased limits: - Up to 100 steps per workflow - Up to 10,000 executions per day - Up to 7200 second timeout (2 hours) - Priority execution queue - Dedicated capacity allocation Contact sales@cloudflow.io for Enterprise pricing and custom limits. ## Best Practices Follow these best practices to build reliable, maintainable workflows: ### 1. Use Descriptive Names **Good:** - Workflow: \"Sync Customer Data from Salesforce to Database\" - Step: \"validate_customer_email\" **Bad:** - Workflow: \"Workflow 1\" - Step: \"step3\" ### 2.",
              "score": null
            }
          ],
          "latency_ms": 11.512043998664012,
          "query_id": "realistic_007",
          "dimension": "problem",
          "query": "My load balancer health checks are failing"
        },
        {
          "key_facts": [
            "/health",
            "/ready"
          ],
          "found_facts": [
            "/health",
            "/ready"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "deployment_guide"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_10",
              "content": "database nc -zv cloudflow-db.internal.company.com 5432 # Trace route traceroute api.cloudflow.io # Capture packets tcpdump -i any -w /tmp/capture.pcap port 5432 ``` --- ## Escalation Procedures ### Severity Levels CloudFlow incidents are classified into four severity levels: #### SEV-1: Critical (P1) - **Definition:** Complete service outage or severe degradation affecting all users - **Examples:** - API returns 5xx errors for > 5 minutes - Database completely unavailable - Authentication system down - Data loss or corruption - **Response Time:** Immediate (< 15 minutes) - **Escalation:** Page on-call engineer immediately #### SEV-2: High (P2) - **Definition:** Major functionality impaired affecting multiple users - **Examples:** - Workflow execution success rate < 90% - Significant performance degradation (p95 latency > 10s) - Rate limiting affecting large customer segment - **Response Time:** < 1 hour - **Escalation:** Create incident ticket and notify on-call #### SEV-3: Medium (P3) - **Definition:** Partial functionality degraded affecting some users - **Examples:** - Intermittent failures for specific workflow types - Minor performance issues - Non-critical feature unavailable - **Response Time:** < 4 hours - **Escalation:** Create ticket, normal business hours support #### SEV-4: Low (P4) - **Definition:** Minor issues with minimal user impact - **Examples:** - Cosmetic issues - Documentation errors - Feature requests - **Response Time:** < 2 business days - **Escalation:** Standard support ticket ### Escalation Steps #### Step 1: Initial Assessment (0-5 minutes) ```bash # Run health check cloudflow health check --comprehensive # Check status page curl https://status.cloudflow.io/api/v1/status.json # Review recent changes cloudflow audit log --last 2h --event-type \"deployment,configuration\" # Check metrics dashboard cloudflow metrics dashboard --incident-mode ``` #### Step 2: Gather Information (5-15 minutes) Create incident document with: - Incident timestamp and duration - Affected services and endpoints - Error rates and user impact - Recent changes or deployments - Relevant log excerpts - Correlation IDs for failed requests ```bash # Generate incident report cloudflow debug incident-report \\ --start \"2026-01-24T10:30:00Z\" \\ --end \"2026-01-24T11:00:00Z\" \\ --output incident-report.md # Capture system snapshot cloudflow debug snapshot --output snapshot-$(date +%Y%m%d-%H%M%S).tar.gz ``` #### Step 3: Escalate Based on Severity **For SEV-1 (Critical):** ```bash # Page on-call engineer cloudflow incident create \\ --severity SEV-1 \\ --title \"Complete API outage\" \\ --description \"All API requests returning 503\" \\ --page-oncall # Notify status page cloudflow status update \\ --status \"major_outage\" \\ --message \"We are investigating a complete service",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_11",
              "content": "Service (2) - PostgreSQL Primary - PostgreSQL Replica - PostgreSQL Replica - Redis Primary (2) - Redis Replica (2) - Redis Replica (2) - Kafka Broker (2) - Kafka Broker (2) - Kafka Broker (1) ``` **Automatic Failover**: - Database: 30-60 seconds (automatic promotion of replica) - Redis: < 10 seconds (Sentinel-based failover) - Kafka: < 30 seconds (controller election) - Services: Kubernetes health checks with 10-second liveness probes ### Backup Strategy **Database Backups**: - Automated snapshots: Daily at 02:00 UTC - Retention: 30 days for daily, 90 days for monthly - Cross-region replication: Async replication to us-west-2 (15-minute lag) - Backup verification: Weekly automated restore test in staging environment **Backup Schedule**: ``` Daily: Full snapshot \u2192 S3 (encrypted) Hourly: WAL archives \u2192 S3 (point-in-time recovery) Weekly: Backup validation test Monthly: Long-term archive to Glacier ``` **Configuration Backups**: - Kubernetes manifests: Stored in Git (GitOps with ArgoCD) - Vault secrets: Automated snapshot every 6 hours - Kafka topic configurations: Exported daily to S3 ### Disaster Recovery Procedures **Scenario 1: Single AZ Failure** - Detection: Health checks fail for entire AZ (< 30 seconds) - Action: Traffic automatically routed to healthy AZs by ALB - Recovery time: < 5 minutes (no manual intervention) - Data loss: None (multi-AZ replication) **Scenario 2: Database Primary Failure** - Detection: Health check fails for primary database (< 30 seconds) - Action: Automatic promotion of read replica to primary - Recovery time: 30-60 seconds - Data loss: Minimal (< 1 second due to synchronous replication) **Scenario 3: Full Region Failure** - Detection: Multiple health check failures across all AZs (< 2 minutes) - Action: Manual failover to DR region (us-west-2) - Procedure: 1. Update DNS to point to DR region (TTL: 60 seconds) 2. Promote DR database replica to primary 3. Scale up DR region services to production capacity 4. Verify data consistency and integrity 5. Update monitoring dashboards - Recovery time: 2-4 hours (includes verification) - Data loss: < 1 hour (last cross-region replication) **Scenario 4: Data Corruption** - Detection: Data validation checks or user report - Action: Point-in-time recovery from WAL archives - Procedure: 1. Identify corruption time window 2. Restore from snapshot prior to corruption 3. Replay WAL logs up to corruption point 4. Verify data integrity 5.",
              "score": null
            },
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_2",
              "content": "- secretName: cloudflow-tls hosts: - api.cloudflow.io healthCheck: enabled: true livenessProbe: path: /health initialDelaySeconds: 30 periodSeconds: 10 timeoutSeconds: 5 failureThreshold: 3 readinessProbe: path: /ready initialDelaySeconds: 10 periodSeconds: 5 timeoutSeconds: 3 failureThreshold: 3 env: - name: NODE_ENV value: \"production\" - name: LOG_LEVEL value: \"info\" - name: API_PORT value: \"3000\" - name: WORKER_CONCURRENCY value: \"10\" - name: AWS_REGION value: \"us-east-1\" - name: METRICS_ENABLED value: \"true\" envFrom: - secretRef: name: cloudflow-secrets podAnnotations: prometheus.io/scrape: \"true\" prometheus.io/port: \"9090\" prometheus.io/path: \"/metrics\" podSecurityContext: runAsNonRoot: true runAsUser: 1000 fsGroup: 1000 securityContext: allowPrivilegeEscalation: false capabilities: drop: - ALL readOnlyRootFilesystem: true persistence: enabled: true storageClass: gp3 accessMode: ReadWriteOnce size: 50Gi redis: enabled: true architecture: replication auth: enabled: true master: persistence: size: 20Gi replica: replicaCount: 2 persistence: size: 20Gi ``` Deploy CloudFlow using Helm: ```bash helm install cloudflow cloudflow/cloudflow \\ --namespace cloudflow-prod \\ --values values-production.yaml \\ --wait \\ --timeout 10m ``` ### Deployment Verification Verify the deployment status: ```bash # Check pod status kubectl get pods -n cloudflow-prod # Expected output: # NAME READY STATUS RESTARTS AGE # cloudflow-api-7d8f9c5b6d-4xk2p 1/1 Running 0 2m # cloudflow-api-7d8f9c5b6d-9hj5m 1/1 Running 0 2m # cloudflow-api-7d8f9c5b6d-tn8wq 1/1 Running 0 2m # Check service endpoints kubectl get svc -n cloudflow-prod # Check ingress kubectl get ingress -n cloudflow-prod ``` Test health endpoints: ```bash # Port forward for testing kubectl port-forward -n cloudflow-prod svc/cloudflow-api 8080:80 # Test health endpoint curl http://localhost:8080/health # Expected: {\"status\":\"healthy\",\"timestamp\":\"2026-01-24T10:30:00Z\"} # Test readiness endpoint curl http://localhost:8080/ready # Expected: {\"status\":\"ready\",\"dependencies\":{\"database\":\"connected\",\"redis\":\"connected\"}} ``` --- ## Database Configuration ### PostgreSQL Setup CloudFlow uses PostgreSQL 14 as its primary data store.",
              "score": null
            },
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_1",
              "content": "## Rate Limiting To ensure fair usage and system stability, CloudFlow enforces rate limits on all API endpoints. **Default Limits:** - 100 requests per minute per authenticated user - 20 requests per minute for unauthenticated requests - Burst allowance: 150 requests in a 10-second window ### Rate Limit Headers Every API response includes rate limit information: ``` X-RateLimit-Limit: 100 X-RateLimit-Remaining: 87 X-RateLimit-Reset: 1640995200 ``` When you exceed the rate limit, you'll receive a `429 Too Many Requests` response: ```json { \"error\": { \"code\": \"rate_limit_exceeded\", \"message\": \"Rate limit exceeded. Please retry after 42 seconds.\", \"retry_after\": 42 } } ``` **Best Practices:** - Monitor `X-RateLimit-Remaining` header values - Implement exponential backoff when receiving 429 responses - Cache responses when appropriate to reduce API calls - Consider upgrading to Enterprise tier for higher limits (1000 req/min) ## Pagination List endpoints return paginated results to optimize performance. Use `limit` and `offset` parameters to navigate through result sets. **Parameters:** - `limit`: Number of items per page (default: 20, max: 100) - `offset`: Number of items to skip (default: 0) **Example Request:** ```bash curl -H \"X-API-Key: YOUR_API_KEY\" \\ \"https://api.cloudflow.io/v2/workflows?limit=50&offset=100\" ``` **Response Structure:** ```json { \"data\": [...], \"pagination\": { \"total\": 347, \"limit\": 50, \"offset\": 100, \"has_more\": true } } ``` **Python Example:** ```python import requests def fetch_all_workflows(api_key): base_url = \"https://api.cloudflow.io/v2/workflows\" headers = {\"X-API-Key\": api_key} all_workflows = [] offset = 0 limit = 100 while True: response = requests.get( base_url, headers=headers, params={\"limit\": limit, \"offset\": offset} ) data = response.json() all_workflows.extend(data['data']) if not data['pagination']['has_more']: break offset += limit return all_workflows ``` ## Endpoints ### Workflows #### List Workflows Retrieve a paginated list of all workflows in your account. **Endpoint:** `GET /workflows` **Query Parameters:** - `status` (optional): Filter by status (`active`, `paused`, `archived`) - `created_after` (optional): ISO 8601 timestamp - `limit` (optional): Items per page (max 100) - `offset` (optional): Pagination offset **Example Request:** ```bash curl -H \"X-API-Key: YOUR_API_KEY\" \\ \"https://api.cloudflow.io/v2/workflows?status=active&limit=25\" ``` **Example Response (200 OK):** ```json { \"data\": [ { \"id\": \"wf_8x7k2m9p\", \"name\": \"Data Processing Pipeline\", \"description\": \"Processes customer data every hour\", \"status\": \"active\", \"trigger\": { \"type\": \"schedule\", \"cron\": \"0 * * * *\" }, \"steps\": 12, \"created_at\": \"2026-01-15T10:30:00Z\", \"updated_at\": \"2026-01-23T14:22:00Z\", \"last_run\": \"2026-01-24T09:00:00Z\" } ], \"pagination\": { \"total\": 1, \"limit\": 25, \"offset\": 0, \"has_more\": false } } ``` #### Create Workflow Create a new workflow with specified configuration.",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_5",
              "content": "Success/failure events published back to Kafka ``` **Event Schema**: ```json { \"event_id\": \"uuid-v4\", \"event_type\": \"workflow.execution.completed\", \"timestamp\": \"2026-01-15T10:30:00.000Z\", \"correlation_id\": \"request-trace-id\", \"payload\": { \"workflow_id\": \"wf-12345\", \"execution_id\": \"exec-67890\", \"status\": \"COMPLETED\", \"duration_ms\": 4230 }, \"metadata\": { \"source_service\": \"workflow-engine\", \"schema_version\": \"1.0\" } } ``` ### Inter-Service Communication Services communicate using two primary patterns: **Synchronous (gRPC)**: - Auth Service validation calls from API Gateway - User Service profile lookups - Low latency requirement (< 50ms) - Request-response pattern with circuit breaker **Asynchronous (Kafka)**: - Workflow execution events - Notification triggers - Audit log events - Eventual consistency acceptable --- ## Database Architecture ### PostgreSQL Primary Database **Cluster Configuration**: - Primary-replica setup with 1 primary + 2 read replicas - Instance type: db.r6g.2xlarge (8 vCPU, 64GB RAM) - Storage: 2TB gp3 SSD with 12,000 IOPS - Multi-AZ deployment for high availability - Automated backups: Daily snapshots, 30-day retention - Point-in-time recovery: 5-minute granularity **Database Schema Design**: ``` Core Tables: - users (5M rows): User accounts and profiles - workflows (2M rows): Workflow definitions - workflow_executions (500M rows, partitioned): Execution history - workflow_steps (2B rows, partitioned): Individual step records - schedules (100K rows): Scheduled workflow triggers - notifications (1B rows, partitioned): Notification delivery log Partitioning Strategy: - workflow_executions: Monthly partitions by created_at - workflow_steps: Monthly partitions by created_at - notifications: Weekly partitions by created_at - Automatic partition creation via cron job - Retention policy: Drop partitions older than 12 months ``` **Indexing Strategy**: - Primary keys: UUIDs with B-tree indexes - Frequently queried columns: Compound indexes (e.g., user_id + status + created_at) - JSONB columns: GIN indexes for workflow definitions - Full-text search: GiST indexes on description fields **Connection Pooling**: - PgBouncer in transaction mode - Pool size: 100 connections per service - Max client connections: 2000 - Connection timeout: 30 seconds **Performance Characteristics**: - Read query P99: < 20ms - Write query P99: < 50ms - Transaction throughput: 15,000 TPS - Replication lag: < 500ms --- ### Redis Caching Layer **Cluster Configuration**: - Redis Cluster with 6 nodes (3 primary + 3 replica) - Instance type: cache.r6g.xlarge (4 vCPU, 26GB RAM) - Total memory: 78GB usable cache space - Persistence: RDB snapshots every 5 minutes + AOF - Eviction policy: allkeys-lru **Cache Usage Patterns**: 1.",
              "score": null
            }
          ],
          "latency_ms": 11.150922000524588,
          "query_id": "realistic_007",
          "dimension": "casual",
          "query": "health endpoints"
        },
        {
          "key_facts": [
            "/health",
            "/ready"
          ],
          "found_facts": [
            "/health",
            "/ready"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "deployment_guide"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_10",
              "content": "database nc -zv cloudflow-db.internal.company.com 5432 # Trace route traceroute api.cloudflow.io # Capture packets tcpdump -i any -w /tmp/capture.pcap port 5432 ``` --- ## Escalation Procedures ### Severity Levels CloudFlow incidents are classified into four severity levels: #### SEV-1: Critical (P1) - **Definition:** Complete service outage or severe degradation affecting all users - **Examples:** - API returns 5xx errors for > 5 minutes - Database completely unavailable - Authentication system down - Data loss or corruption - **Response Time:** Immediate (< 15 minutes) - **Escalation:** Page on-call engineer immediately #### SEV-2: High (P2) - **Definition:** Major functionality impaired affecting multiple users - **Examples:** - Workflow execution success rate < 90% - Significant performance degradation (p95 latency > 10s) - Rate limiting affecting large customer segment - **Response Time:** < 1 hour - **Escalation:** Create incident ticket and notify on-call #### SEV-3: Medium (P3) - **Definition:** Partial functionality degraded affecting some users - **Examples:** - Intermittent failures for specific workflow types - Minor performance issues - Non-critical feature unavailable - **Response Time:** < 4 hours - **Escalation:** Create ticket, normal business hours support #### SEV-4: Low (P4) - **Definition:** Minor issues with minimal user impact - **Examples:** - Cosmetic issues - Documentation errors - Feature requests - **Response Time:** < 2 business days - **Escalation:** Standard support ticket ### Escalation Steps #### Step 1: Initial Assessment (0-5 minutes) ```bash # Run health check cloudflow health check --comprehensive # Check status page curl https://status.cloudflow.io/api/v1/status.json # Review recent changes cloudflow audit log --last 2h --event-type \"deployment,configuration\" # Check metrics dashboard cloudflow metrics dashboard --incident-mode ``` #### Step 2: Gather Information (5-15 minutes) Create incident document with: - Incident timestamp and duration - Affected services and endpoints - Error rates and user impact - Recent changes or deployments - Relevant log excerpts - Correlation IDs for failed requests ```bash # Generate incident report cloudflow debug incident-report \\ --start \"2026-01-24T10:30:00Z\" \\ --end \"2026-01-24T11:00:00Z\" \\ --output incident-report.md # Capture system snapshot cloudflow debug snapshot --output snapshot-$(date +%Y%m%d-%H%M%S).tar.gz ``` #### Step 3: Escalate Based on Severity **For SEV-1 (Critical):** ```bash # Page on-call engineer cloudflow incident create \\ --severity SEV-1 \\ --title \"Complete API outage\" \\ --description \"All API requests returning 503\" \\ --page-oncall # Notify status page cloudflow status update \\ --status \"major_outage\" \\ --message \"We are investigating a complete service",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_2",
              "content": "**Technology**: Node.js with TypeScript, Bull queue library **Replicas**: 16 pods (production), auto-scaling 12-24 **Resource Allocation**: 4 vCPU, 8GB RAM per pod **Key Responsibilities**: - Parse and validate workflow definitions (JSON-based DSL) - Execute workflow steps with state machine pattern - Handle retries with exponential backoff (max 3 retries, backoff: 2^n seconds) - Coordinate parallel and sequential task execution - Manage workflow state persistence and recovery - Support for conditional branching and loops - Sub-workflow invocation and composition **Workflow Execution Model**: ``` Workflow Submitted \u2192 Validation \u2192 Queue in Kafka \u2502 \u25bc Workflow Engine picks up \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u25bc \u25bc \u25bc [Task 1 Exec] [Task 2 Exec] [Task 3 Exec] \u2502 \u2502 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u25bc State Update \u2192 PostgreSQL \u2502 \u25bc Publish Event \u2192 Kafka ``` **Workflow DSL Example**: ```json { \"workflow_id\": \"customer-onboarding-v2\", \"version\": \"2.1.0\", \"steps\": [ { \"id\": \"validate-customer\", \"type\": \"http_request\", \"config\": { \"url\": \"https://validation.internal/verify\", \"method\": \"POST\", \"timeout\": 5000, \"retry\": {\"max_attempts\": 3, \"backoff\": \"exponential\"} } }, { \"id\": \"create-account\", \"type\": \"database_operation\", \"depends_on\": [\"validate-customer\"] } ] } ``` **State Management**: - Workflow states: PENDING, RUNNING, COMPLETED, FAILED, CANCELLED - Step states: QUEUED, EXECUTING, SUCCESS, FAILED, SKIPPED - State transitions stored in PostgreSQL with event sourcing pattern - Checkpointing every 10 steps for long-running workflows **Performance Targets**: - Simple workflow (3-5 steps): < 2 seconds (P99) - Complex workflow (20+ steps): < 5 seconds (P99) - Throughput: 500 concurrent workflow executions per pod --- ### Scheduler Service **Purpose**: Time-based workflow triggering system supporting cron-like schedules and one-time delayed executions.",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_7",
              "content": "records: 500 - Session timeout: 30 seconds --- ## Message Queue Patterns ### Pub/Sub Pattern Used for broadcasting events to multiple interested consumers: ``` Workflow Engine publishes workflow.execution.completed \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u25bc \u25bc \u25bc \u25bc Analytics Notification Audit Logger Billing Service Service Service Service ``` Each service maintains its own consumer group and processes events independently. Failures in one consumer don't affect others. ### Request-Reply Pattern Synchronous communication over async messaging (used sparingly): ``` API Gateway publishes request \u2192 Kafka (reply-to: temp-queue-123) \u2502 \u25bc Service processes request \u2502 \u25bc Service publishes response \u2192 temp-queue-123 \u2502 \u25bc API Gateway receives response ``` Timeout: 5 seconds, fallback to direct HTTP call if no response. ### Saga Pattern Distributed transaction management for multi-service workflows: ``` Step 1: Create Order \u2192 SUCCESS \u2192 Step 2: Reserve Inventory \u2502 FAILURE \u2502 \u25bc Compensating Transaction \u2502 \u25bc Cancel Order (Rollback) ``` Orchestrated by Workflow Engine with compensation logic defined in workflow DSL. ### Dead Letter Queue (DLQ) Failed messages after all retry attempts are routed to DLQ: ``` Message processing fails (3 retries with exponential backoff) \u2502 \u25bc Publish to dead-letter-queue topic \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u25bc \u25bc Alert to PagerDuty Store in PostgreSQL for analysis \u2502 \u2502 \u25bc \u25bc Manual investigation Automated pattern detection ``` DLQ Processing SLA: < 4 hours for critical events, < 24 hours for non-critical. --- ## Caching Strategy ### Cache-Aside Pattern (Lazy Loading) Primary caching pattern used across all services: ``` 1. Application checks cache (Redis GET) 2. If HIT \u2192 Return cached data 3. If MISS: a. Query database (PostgreSQL) b. Store result in cache with TTL c. Return data to application ``` **Implementation Example** (Workflow Definition): ```javascript async function getWorkflowDefinition(workflowId) { const cacheKey = `workflow:def:${workflowId}`; // Step 1: Check cache let workflow = await redis.get(cacheKey); if (workflow) { metrics.increment('cache.hit.workflow_def'); return JSON.parse(workflow); } // Step 2: Cache miss - query database metrics.increment('cache.miss.workflow_def'); workflow = await db.query( 'SELECT * FROM workflows WHERE id = $1', [workflowId] ); // Step 3: Store in cache (1 hour TTL) await redis.setex(cacheKey, 3600, JSON.stringify(workflow)); return workflow; } ``` ### Write-Through Pattern Used for critical data where cache consistency is paramount: ``` 1. Application writes to database 2. Database transaction commits 3. Application updates cache 4. Return success to client ``` Applied to: User profiles, authentication sessions, system configuration.",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_11",
              "content": "Service (2) - PostgreSQL Primary - PostgreSQL Replica - PostgreSQL Replica - Redis Primary (2) - Redis Replica (2) - Redis Replica (2) - Kafka Broker (2) - Kafka Broker (2) - Kafka Broker (1) ``` **Automatic Failover**: - Database: 30-60 seconds (automatic promotion of replica) - Redis: < 10 seconds (Sentinel-based failover) - Kafka: < 30 seconds (controller election) - Services: Kubernetes health checks with 10-second liveness probes ### Backup Strategy **Database Backups**: - Automated snapshots: Daily at 02:00 UTC - Retention: 30 days for daily, 90 days for monthly - Cross-region replication: Async replication to us-west-2 (15-minute lag) - Backup verification: Weekly automated restore test in staging environment **Backup Schedule**: ``` Daily: Full snapshot \u2192 S3 (encrypted) Hourly: WAL archives \u2192 S3 (point-in-time recovery) Weekly: Backup validation test Monthly: Long-term archive to Glacier ``` **Configuration Backups**: - Kubernetes manifests: Stored in Git (GitOps with ArgoCD) - Vault secrets: Automated snapshot every 6 hours - Kafka topic configurations: Exported daily to S3 ### Disaster Recovery Procedures **Scenario 1: Single AZ Failure** - Detection: Health checks fail for entire AZ (< 30 seconds) - Action: Traffic automatically routed to healthy AZs by ALB - Recovery time: < 5 minutes (no manual intervention) - Data loss: None (multi-AZ replication) **Scenario 2: Database Primary Failure** - Detection: Health check fails for primary database (< 30 seconds) - Action: Automatic promotion of read replica to primary - Recovery time: 30-60 seconds - Data loss: Minimal (< 1 second due to synchronous replication) **Scenario 3: Full Region Failure** - Detection: Multiple health check failures across all AZs (< 2 minutes) - Action: Manual failover to DR region (us-west-2) - Procedure: 1. Update DNS to point to DR region (TTL: 60 seconds) 2. Promote DR database replica to primary 3. Scale up DR region services to production capacity 4. Verify data consistency and integrity 5. Update monitoring dashboards - Recovery time: 2-4 hours (includes verification) - Data loss: < 1 hour (last cross-region replication) **Scenario 4: Data Corruption** - Detection: Data validation checks or user report - Action: Point-in-time recovery from WAL archives - Procedure: 1. Identify corruption time window 2. Restore from snapshot prior to corruption 3. Replay WAL logs up to corruption point 4. Verify data integrity 5.",
              "score": null
            },
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_2",
              "content": "- secretName: cloudflow-tls hosts: - api.cloudflow.io healthCheck: enabled: true livenessProbe: path: /health initialDelaySeconds: 30 periodSeconds: 10 timeoutSeconds: 5 failureThreshold: 3 readinessProbe: path: /ready initialDelaySeconds: 10 periodSeconds: 5 timeoutSeconds: 3 failureThreshold: 3 env: - name: NODE_ENV value: \"production\" - name: LOG_LEVEL value: \"info\" - name: API_PORT value: \"3000\" - name: WORKER_CONCURRENCY value: \"10\" - name: AWS_REGION value: \"us-east-1\" - name: METRICS_ENABLED value: \"true\" envFrom: - secretRef: name: cloudflow-secrets podAnnotations: prometheus.io/scrape: \"true\" prometheus.io/port: \"9090\" prometheus.io/path: \"/metrics\" podSecurityContext: runAsNonRoot: true runAsUser: 1000 fsGroup: 1000 securityContext: allowPrivilegeEscalation: false capabilities: drop: - ALL readOnlyRootFilesystem: true persistence: enabled: true storageClass: gp3 accessMode: ReadWriteOnce size: 50Gi redis: enabled: true architecture: replication auth: enabled: true master: persistence: size: 20Gi replica: replicaCount: 2 persistence: size: 20Gi ``` Deploy CloudFlow using Helm: ```bash helm install cloudflow cloudflow/cloudflow \\ --namespace cloudflow-prod \\ --values values-production.yaml \\ --wait \\ --timeout 10m ``` ### Deployment Verification Verify the deployment status: ```bash # Check pod status kubectl get pods -n cloudflow-prod # Expected output: # NAME READY STATUS RESTARTS AGE # cloudflow-api-7d8f9c5b6d-4xk2p 1/1 Running 0 2m # cloudflow-api-7d8f9c5b6d-9hj5m 1/1 Running 0 2m # cloudflow-api-7d8f9c5b6d-tn8wq 1/1 Running 0 2m # Check service endpoints kubectl get svc -n cloudflow-prod # Check ingress kubectl get ingress -n cloudflow-prod ``` Test health endpoints: ```bash # Port forward for testing kubectl port-forward -n cloudflow-prod svc/cloudflow-api 8080:80 # Test health endpoint curl http://localhost:8080/health # Expected: {\"status\":\"healthy\",\"timestamp\":\"2026-01-24T10:30:00Z\"} # Test readiness endpoint curl http://localhost:8080/ready # Expected: {\"status\":\"ready\",\"dependencies\":{\"database\":\"connected\",\"redis\":\"connected\"}} ``` --- ## Database Configuration ### PostgreSQL Setup CloudFlow uses PostgreSQL 14 as its primary data store.",
              "score": null
            }
          ],
          "latency_ms": 11.442364000686212,
          "query_id": "realistic_007",
          "dimension": "contextual",
          "query": "Setting up monitoring, what endpoints indicate service health?"
        },
        {
          "key_facts": [
            "/health",
            "/ready"
          ],
          "found_facts": [
            "/health",
            "/ready"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "deployment_guide"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_11",
              "content": "outage\" # Create war room cloudflow incident war-room create incident-2024012401 ``` **For SEV-2 (High):** ```bash # Create incident and notify cloudflow incident create \\ --severity SEV-2 \\ --title \"High rate of workflow failures\" \\ --description \"Workflow success rate dropped to 75%\" \\ --notify-oncall # Update status page cloudflow status update \\ --status \"partial_outage\" \\ --message \"We are experiencing elevated error rates\" ``` **For SEV-3/SEV-4:** ```bash # Create support ticket cloudflow support ticket create \\ --priority MEDIUM \\ --subject \"Intermittent timeout errors\" \\ --description \"See attached logs and reproduction steps\" ``` ### Contact Information - **On-call hotline:** +1-888-CLOUDFLOW (24/7) - **Support email:** support@cloudflow.io - **Incident Slack channel:** #cloudflow-incidents - **Status page:** https://status.cloudflow.io - **Documentation:** https://docs.cloudflow.io ### Post-Incident Review After resolving incidents, complete a postmortem: ```bash # Generate postmortem template cloudflow incident postmortem incident-2024012401 \\ --template standard \\ --output postmortem-2024012401.md # Required sections: # 1. Incident summary # 2. Impact assessment # 3. Timeline of events # 4. Root cause analysis # 5. Resolution steps # 6. Action items # 7. Lessons learned ``` --- ## Additional Resources - **CloudFlow Documentation:** https://docs.cloudflow.io - **API Reference:** https://api-docs.cloudflow.io - **Community Forum:** https://community.cloudflow.io - **GitHub Issues:** https://github.com/cloudflow/platform/issues - **Training Portal:** https://training.cloudflow.io - **Monitoring Dashboard:** https://monitoring.cloudflow.io ### Getting Help If this troubleshooting guide doesn't resolve your issue: 1. Search the knowledge base: `cloudflow kb search \"your issue\"` 2. Check community forum for similar issues 3. Contact support with detailed logs and reproduction steps 4. For urgent issues, use emergency escalation procedures **Remember:** Always capture logs, metrics, and reproduction steps before escalating! --- *Last updated: January 24, 2026* *Document version: 3.2.1* *Feedback: docs-feedback@cloudflow.io*",
              "score": null
            },
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_4",
              "content": "### Error Response Format ```json { \"error\": { \"code\": \"invalid_parameter\", \"message\": \"The 'limit' parameter must be between 1 and 100\", \"field\": \"limit\", \"request_id\": \"req_8k3m9x2p\" } } ``` ### HTTP Status Codes | Status Code | Description | Common Causes | |------------|-------------|---------------| | 400 | Bad Request | Invalid parameters, malformed JSON, validation errors | | 401 | Unauthorized | Missing or invalid authentication credentials | | 403 | Forbidden | Insufficient permissions for requested resource | | 404 | Not Found | Resource does not exist or has been deleted | | 429 | Too Many Requests | Rate limit exceeded, retry after specified period | | 500 | Internal Server Error | Unexpected server error, contact support if persists | | 502 | Bad Gateway | Temporary service issue, retry with exponential backoff | | 503 | Service Unavailable | Scheduled maintenance or temporary outage | ### Error Codes CloudFlow returns specific error codes to help you identify and resolve issues: - `invalid_parameter`: One or more request parameters are invalid - `missing_required_field`: Required field is missing from request body - `authentication_failed`: Invalid API key or token - `insufficient_permissions`: User lacks required scope or permission - `resource_not_found`: Requested resource does not exist - `rate_limit_exceeded`: Too many requests, see rate limiting section - `workflow_execution_failed`: Workflow execution encountered an error - `invalid_json`: Request body contains malformed JSON - `duplicate_resource`: Resource with same identifier already exists - `quota_exceeded`: Account quota limit reached ### Error Handling Best Practices **Python Example:** ```python import requests import time def make_api_request(url, headers, max_retries=3): for attempt in range(max_retries): try: response = requests.get(url, headers=headers) if response.status_code == 200: return response.json() elif response.status_code == 429: # Rate limit exceeded retry_after = int(response.headers.get('X-RateLimit-Reset', 60)) print(f\"Rate limited. Waiting {retry_after} seconds...\") time.sleep(retry_after) continue elif response.status_code >= 500: # Server error - retry with exponential backoff wait_time = 2 ** attempt print(f\"Server error. Retrying in {wait_time} seconds...\") time.sleep(wait_time) continue else: # Client error - don't retry error_data = response.json() raise Exception(f\"API Error: {error_data['error']['message']}\") except requests.exceptions.RequestException as e: print(f\"Request failed: {e}\") if attempt < max_retries - 1: time.sleep(2 ** attempt) else: raise raise Exception(\"Max retries exceeded\") ``` ## Webhooks CloudFlow can send webhook notifications when specific events occur in your workflows.",
              "score": null
            },
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_2",
              "content": "- secretName: cloudflow-tls hosts: - api.cloudflow.io healthCheck: enabled: true livenessProbe: path: /health initialDelaySeconds: 30 periodSeconds: 10 timeoutSeconds: 5 failureThreshold: 3 readinessProbe: path: /ready initialDelaySeconds: 10 periodSeconds: 5 timeoutSeconds: 3 failureThreshold: 3 env: - name: NODE_ENV value: \"production\" - name: LOG_LEVEL value: \"info\" - name: API_PORT value: \"3000\" - name: WORKER_CONCURRENCY value: \"10\" - name: AWS_REGION value: \"us-east-1\" - name: METRICS_ENABLED value: \"true\" envFrom: - secretRef: name: cloudflow-secrets podAnnotations: prometheus.io/scrape: \"true\" prometheus.io/port: \"9090\" prometheus.io/path: \"/metrics\" podSecurityContext: runAsNonRoot: true runAsUser: 1000 fsGroup: 1000 securityContext: allowPrivilegeEscalation: false capabilities: drop: - ALL readOnlyRootFilesystem: true persistence: enabled: true storageClass: gp3 accessMode: ReadWriteOnce size: 50Gi redis: enabled: true architecture: replication auth: enabled: true master: persistence: size: 20Gi replica: replicaCount: 2 persistence: size: 20Gi ``` Deploy CloudFlow using Helm: ```bash helm install cloudflow cloudflow/cloudflow \\ --namespace cloudflow-prod \\ --values values-production.yaml \\ --wait \\ --timeout 10m ``` ### Deployment Verification Verify the deployment status: ```bash # Check pod status kubectl get pods -n cloudflow-prod # Expected output: # NAME READY STATUS RESTARTS AGE # cloudflow-api-7d8f9c5b6d-4xk2p 1/1 Running 0 2m # cloudflow-api-7d8f9c5b6d-9hj5m 1/1 Running 0 2m # cloudflow-api-7d8f9c5b6d-tn8wq 1/1 Running 0 2m # Check service endpoints kubectl get svc -n cloudflow-prod # Check ingress kubectl get ingress -n cloudflow-prod ``` Test health endpoints: ```bash # Port forward for testing kubectl port-forward -n cloudflow-prod svc/cloudflow-api 8080:80 # Test health endpoint curl http://localhost:8080/health # Expected: {\"status\":\"healthy\",\"timestamp\":\"2026-01-24T10:30:00Z\"} # Test readiness endpoint curl http://localhost:8080/ready # Expected: {\"status\":\"ready\",\"dependencies\":{\"database\":\"connected\",\"redis\":\"connected\"}} ``` --- ## Database Configuration ### PostgreSQL Setup CloudFlow uses PostgreSQL 14 as its primary data store.",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_7",
              "content": "Waiting {retry_after} seconds...\") time.sleep(retry_after) continue remaining = int(response.headers.get('X-RateLimit-Remaining', 0)) if remaining < 10: print(f\"Warning: Only {remaining} requests remaining\") return response raise Exception(\"Max retries exceeded due to rate limiting\") ``` **Bash script with rate limit checking:** ```bash #!/bin/bash check_rate_limit() { local remaining=$(curl -s -I https://api.cloudflow.io/api/v1/workflows \\ -H \"Authorization: Bearer $CF_ACCESS_TOKEN\" | \\ grep -i \"x-ratelimit-remaining\" | \\ awk '{print $2}' | tr -d '\\r') if [ \"$remaining\" -lt 10 ]; then echo \"Warning: Only $remaining requests remaining\" local reset=$(curl -s -I https://api.cloudflow.io/api/v1/workflows \\ -H \"Authorization: Bearer $CF_ACCESS_TOKEN\" | \\ grep -i \"x-ratelimit-reset\" | \\ awk '{print $2}' | tr -d '\\r') local wait_time=$((reset - $(date +%s))) echo \"Rate limit resets in $wait_time seconds\" sleep $wait_time fi } # Use before API calls check_rate_limit cloudflow workflows execute wf_9k2n4m8p1q ``` #### Optimization Strategies **1. Implement request batching:** ```bash # Batch multiple workflow executions cloudflow workflows execute-batch \\ --workflow-ids \"wf_id1,wf_id2,wf_id3,wf_id4,wf_id5\" \\ --batch-size 5 # This counts as 1 API request instead of 5 ``` **2. Use webhooks instead of polling:** ```bash # Configure webhook for workflow completion cloudflow webhooks create \\ --event workflow.completed \\ --url https://your-service.com/webhooks/cloudflow \\ --secret $WEBHOOK_SECRET # Verify webhook cloudflow webhooks test webhook_abc123 ``` **3.",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_10",
              "content": "database nc -zv cloudflow-db.internal.company.com 5432 # Trace route traceroute api.cloudflow.io # Capture packets tcpdump -i any -w /tmp/capture.pcap port 5432 ``` --- ## Escalation Procedures ### Severity Levels CloudFlow incidents are classified into four severity levels: #### SEV-1: Critical (P1) - **Definition:** Complete service outage or severe degradation affecting all users - **Examples:** - API returns 5xx errors for > 5 minutes - Database completely unavailable - Authentication system down - Data loss or corruption - **Response Time:** Immediate (< 15 minutes) - **Escalation:** Page on-call engineer immediately #### SEV-2: High (P2) - **Definition:** Major functionality impaired affecting multiple users - **Examples:** - Workflow execution success rate < 90% - Significant performance degradation (p95 latency > 10s) - Rate limiting affecting large customer segment - **Response Time:** < 1 hour - **Escalation:** Create incident ticket and notify on-call #### SEV-3: Medium (P3) - **Definition:** Partial functionality degraded affecting some users - **Examples:** - Intermittent failures for specific workflow types - Minor performance issues - Non-critical feature unavailable - **Response Time:** < 4 hours - **Escalation:** Create ticket, normal business hours support #### SEV-4: Low (P4) - **Definition:** Minor issues with minimal user impact - **Examples:** - Cosmetic issues - Documentation errors - Feature requests - **Response Time:** < 2 business days - **Escalation:** Standard support ticket ### Escalation Steps #### Step 1: Initial Assessment (0-5 minutes) ```bash # Run health check cloudflow health check --comprehensive # Check status page curl https://status.cloudflow.io/api/v1/status.json # Review recent changes cloudflow audit log --last 2h --event-type \"deployment,configuration\" # Check metrics dashboard cloudflow metrics dashboard --incident-mode ``` #### Step 2: Gather Information (5-15 minutes) Create incident document with: - Incident timestamp and duration - Affected services and endpoints - Error rates and user impact - Recent changes or deployments - Relevant log excerpts - Correlation IDs for failed requests ```bash # Generate incident report cloudflow debug incident-report \\ --start \"2026-01-24T10:30:00Z\" \\ --end \"2026-01-24T11:00:00Z\" \\ --output incident-report.md # Capture system snapshot cloudflow debug snapshot --output snapshot-$(date +%Y%m%d-%H%M%S).tar.gz ``` #### Step 3: Escalate Based on Severity **For SEV-1 (Critical):** ```bash # Page on-call engineer cloudflow incident create \\ --severity SEV-1 \\ --title \"Complete API outage\" \\ --description \"All API requests returning 503\" \\ --page-oncall # Notify status page cloudflow status update \\ --status \"major_outage\" \\ --message \"We are investigating a complete service",
              "score": null
            }
          ],
          "latency_ms": 11.370691001502564,
          "query_id": "realistic_007",
          "dimension": "negation",
          "query": "Why doesn't /status return health information?"
        },
        {
          "key_facts": [
            "minReplicas: 3",
            "maxReplicas: 10",
            "targetCPUUtilizationPercentage: 70"
          ],
          "found_facts": [
            "minReplicas: 3",
            "maxReplicas: 10"
          ],
          "missed_facts": [
            "targetCPUUtilizationPercentage: 70"
          ],
          "coverage": 0.6666666666666666,
          "expected_docs": [
            "deployment_guide"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_6",
              "content": "**Verify Service Health**: ```bash kubectl get pods -n cloudflow-prod curl https://api.cloudflow.io/health ``` **Recovery Time Objective (RTO)**: 4 hours **Recovery Point Objective (RPO)**: 24 hours --- ## Scaling and Performance ### Horizontal Pod Autoscaling CloudFlow is configured with Horizontal Pod Autoscaler (HPA) to automatically scale based on resource utilization: ```yaml # hpa.yaml apiVersion: autoscaling/v2 kind: HorizontalPodAutoscaler metadata: name: cloudflow-api-hpa namespace: cloudflow-prod spec: scaleTargetRef: apiVersion: apps/v1 kind: Deployment name: cloudflow-api minReplicas: 3 maxReplicas: 10 metrics: - type: Resource resource: name: cpu target: type: Utilization averageUtilization: 70 - type: Resource resource: name: memory target: type: Utilization averageUtilization: 80 behavior: scaleDown: stabilizationWindowSeconds: 300 policies: - type: Percent value: 50 periodSeconds: 60 scaleUp: stabilizationWindowSeconds: 60 policies: - type: Percent value: 100 periodSeconds: 30 - type: Pods value: 2 periodSeconds: 30 selectPolicy: Max ``` Monitor autoscaling events: ```bash kubectl get hpa -n cloudflow-prod -w kubectl describe hpa cloudflow-api-hpa -n cloudflow-prod ``` ### Performance Optimization #### Connection Pooling Ensure optimal database connection pooling settings in PgBouncer: - **Pool Mode**: Transaction (optimal for microservices) - **Default Pool Size**: 25 connections per user - **Max DB Connections**: 100 (matches PostgreSQL `max_connections`) #### Redis Caching Implement Redis caching for frequently accessed data: ```javascript // Example caching strategy const CACHE_TTL = { workflows: 300, // 5 minutes userSessions: 3600, // 1 hour apiResults: 60 // 1 minute }; ``` #### Load Testing Perform regular load testing to validate scaling configuration: ```bash # Install k6 for load testing kubectl apply -f - <<EOF apiVersion: v1 kind: ConfigMap metadata: name: loadtest-script namespace: cloudflow-prod data: script.js: | import http from 'k6/http'; import { check, sleep } from 'k6'; export let options = { stages: [ { duration: '2m', target: 100 }, { duration: '5m', target: 100 }, { duration: '2m', target: 200 }, { duration: '5m', target: 200 }, { duration: '2m', target: 0 }, ], thresholds: { http_req_duration: ['p(95)<500'], }, }; export default function () { let res = http.get('https://api.cloudflow.io/api/workflows'); check(res, { 'status is 200': (r) => r.status === 200 }); sleep(1); } EOF ``` **Performance Targets**: - P95 API latency: < 500ms - P99 API latency: < 1000ms - Error rate: < 0.1% - Throughput: > 1000 requests/second --- ## Security and Compliance ### Network Policies Implement network policies to restrict pod-to-pod communication: ```yaml # network-policy.yaml apiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: cloudflow-network-policy namespace: cloudflow-prod spec:",
              "score": null
            },
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_4",
              "content": "--set prometheus.prometheusSpec.retention=30d \\ --set prometheus.prometheusSpec.storageSpec.volumeClaimTemplate.spec.resources.requests.storage=100Gi ``` CloudFlow exposes Prometheus metrics on port 9090 at the `/metrics` endpoint. The metrics include: - **HTTP Metrics**: Request rate, latency, error rate - **Database Metrics**: Connection pool status, query duration - **Worker Metrics**: Job queue size, processing time, failure rate - **System Metrics**: CPU usage, memory usage, heap statistics Example metrics exposed: ``` # HELP cloudflow_http_requests_total Total number of HTTP requests # TYPE cloudflow_http_requests_total counter cloudflow_http_requests_total{method=\"GET\",route=\"/api/workflows\",status=\"200\"} 15420 # HELP cloudflow_http_request_duration_seconds HTTP request duration in seconds # TYPE cloudflow_http_request_duration_seconds histogram cloudflow_http_request_duration_seconds_bucket{method=\"POST\",route=\"/api/workflows\",le=\"0.1\"} 8234 # HELP cloudflow_worker_jobs_processed_total Total number of jobs processed # TYPE cloudflow_worker_jobs_processed_total counter cloudflow_worker_jobs_processed_total{status=\"success\"} 45230 # HELP cloudflow_database_connections Current database connections # TYPE cloudflow_database_connections gauge cloudflow_database_connections{state=\"active\"} 23 ``` ### Grafana Dashboards Access Grafana to view CloudFlow dashboards: ```bash kubectl port-forward -n monitoring svc/prometheus-grafana 3000:80 ``` Import the CloudFlow dashboard (ID: 15847) or use the provided JSON template. Key dashboard panels include: 1. **API Performance**: Request rate, P95/P99 latency, error rate 2. **Resource Usage**: CPU, memory, disk I/O per pod 3. **Database Health**: Connection pool utilization, query performance 4. **Worker Status**: Queue depth, processing rate, job success/failure ratio 5. **System Overview**: Pod status, replica count, autoscaling events ### Logging with CloudWatch CloudFlow logs are automatically shipped to CloudWatch Logs.",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_0",
              "content": "# CloudFlow Platform - System Architecture Overview **Document Version:** 2.3.1 **Last Updated:** January 15, 2026 **Owner:** Platform Architecture Team **Status:** Production ## Executive Summary CloudFlow is a distributed, cloud-native workflow automation platform designed to orchestrate complex business processes at scale. The platform processes over 2.5 million workflow executions daily with an average P99 latency of 180ms for API operations and 4.2 seconds for workflow execution. This document provides a comprehensive overview of the system architecture, including microservices design, data flow patterns, infrastructure decisions, and operational considerations. ## Table of Contents 1. [High-Level Architecture](#high-level-architecture) 2. [Microservices Breakdown](#microservices-breakdown) 3. [Data Flow Architecture](#data-flow-architecture) 4. [Database Architecture](#database-architecture) 5. [Message Queue Patterns](#message-queue-patterns) 6. [Caching Strategy](#caching-strategy) 7. [Security Architecture](#security-architecture) 8. [Performance Characteristics](#performance-characteristics) 9. [Disaster Recovery](#disaster-recovery) --- ## High-Level Architecture CloudFlow follows a microservices architecture pattern deployed across multiple availability zones in AWS. The platform is designed with the following core principles: - **Scalability**: Horizontal scaling for all services with auto-scaling groups - **Resilience**: Circuit breakers, bulkheads, and graceful degradation - **Observability**: Distributed tracing, centralized logging, and comprehensive metrics - **Security**: Zero-trust network architecture with mTLS encryption ### Architecture Diagram (Conceptual) ``` \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 Load Balancer (ALB) \u2502 \u2502 (TLS Termination - 443) \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u25bc \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 API Gateway Layer \u2502 \u2502 (Rate Limiting, Auth, Request Routing) \u2502 \u2514\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u2502 \u2502 \u2502 \u2502 \u25bc \u25bc \u25bc \u25bc \u25bc \u250c\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502Auth\u2502 \u2502Workflow\u2502 \u2502Scheduler\u2502 \u2502Notification\u2502 \u2502User Service\u2502 \u2502Svc \u2502 \u2502 Engine \u2502 \u2502 Service \u2502 \u2502 Service \u2502 \u2502 \u2502 \u2514\u2500\u252c\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u2502 \u2502 \u2502 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 \u2502 \u2502 \u25bc \u25bc \u25bc \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502PostgreSQL\u2502 \u2502 Redis \u2502 \u2502 Kafka \u2502 \u2502 Cluster \u2502 \u2502 Cluster \u2502 \u2502 Cluster \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 ``` ### Technology Stack - **Runtime**: Node.js 20.x (API Gateway, Workflow Engine), Go 1.21 (Auth Service, Scheduler) - **Container Orchestration**: Kubernetes 1.28 (EKS) - **Service Mesh**: Istio 1.20 for service-to-service communication - **Databases**: PostgreSQL 15.4, Redis 7.2 - **Message Broker**: Apache Kafka 3.6 - **Monitoring**: Prometheus, Grafana, Jaeger for distributed tracing - **Secrets Management**: HashiCorp Vault 1.15 --- ## Microservices Breakdown ### API Gateway **Purpose**: Single entry point for all client requests, providing authentication, rate limiting, request routing, and protocol translation.",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_10",
              "content": "database nc -zv cloudflow-db.internal.company.com 5432 # Trace route traceroute api.cloudflow.io # Capture packets tcpdump -i any -w /tmp/capture.pcap port 5432 ``` --- ## Escalation Procedures ### Severity Levels CloudFlow incidents are classified into four severity levels: #### SEV-1: Critical (P1) - **Definition:** Complete service outage or severe degradation affecting all users - **Examples:** - API returns 5xx errors for > 5 minutes - Database completely unavailable - Authentication system down - Data loss or corruption - **Response Time:** Immediate (< 15 minutes) - **Escalation:** Page on-call engineer immediately #### SEV-2: High (P2) - **Definition:** Major functionality impaired affecting multiple users - **Examples:** - Workflow execution success rate < 90% - Significant performance degradation (p95 latency > 10s) - Rate limiting affecting large customer segment - **Response Time:** < 1 hour - **Escalation:** Create incident ticket and notify on-call #### SEV-3: Medium (P3) - **Definition:** Partial functionality degraded affecting some users - **Examples:** - Intermittent failures for specific workflow types - Minor performance issues - Non-critical feature unavailable - **Response Time:** < 4 hours - **Escalation:** Create ticket, normal business hours support #### SEV-4: Low (P4) - **Definition:** Minor issues with minimal user impact - **Examples:** - Cosmetic issues - Documentation errors - Feature requests - **Response Time:** < 2 business days - **Escalation:** Standard support ticket ### Escalation Steps #### Step 1: Initial Assessment (0-5 minutes) ```bash # Run health check cloudflow health check --comprehensive # Check status page curl https://status.cloudflow.io/api/v1/status.json # Review recent changes cloudflow audit log --last 2h --event-type \"deployment,configuration\" # Check metrics dashboard cloudflow metrics dashboard --incident-mode ``` #### Step 2: Gather Information (5-15 minutes) Create incident document with: - Incident timestamp and duration - Affected services and endpoints - Error rates and user impact - Recent changes or deployments - Relevant log excerpts - Correlation IDs for failed requests ```bash # Generate incident report cloudflow debug incident-report \\ --start \"2026-01-24T10:30:00Z\" \\ --end \"2026-01-24T11:00:00Z\" \\ --output incident-report.md # Capture system snapshot cloudflow debug snapshot --output snapshot-$(date +%Y%m%d-%H%M%S).tar.gz ``` #### Step 3: Escalate Based on Severity **For SEV-1 (Critical):** ```bash # Page on-call engineer cloudflow incident create \\ --severity SEV-1 \\ --title \"Complete API outage\" \\ --description \"All API requests returning 503\" \\ --page-oncall # Notify status page cloudflow status update \\ --status \"major_outage\" \\ --message \"We are investigating a complete service",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_9",
              "content": "per API key (default: 1000 RPM) ### Secrets Management **HashiCorp Vault Integration**: - Dynamic database credentials: Generated on-demand, 1-hour TTL - Encryption keys: Transit secrets engine for encryption-as-a-service - API keys for external services: Stored with versioning - Rotation policy: Automated rotation every 90 days with notification **Secret Access Pattern**: ``` Service starts \u2192 Vault authentication (Kubernetes service account) \u2502 \u25bc Request secret lease (1-hour TTL) \u2502 \u25bc Vault returns dynamic credentials \u2502 \u25bc Service connects to PostgreSQL \u2502 Renew lease every 30 minutes ``` ### Data Protection **Encryption at Rest**: - PostgreSQL: AES-256 encryption enabled at volume level - Redis: Encryption enabled using AWS KMS - Kafka: Encryption at rest for all data on brokers - S3 backups: Server-side encryption with KMS (SSE-KMS) **Encryption in Transit**: - External traffic: TLS 1.3 only (TLS 1.2 deprecated) - Internal traffic: mTLS via Istio service mesh - Database connections: SSL/TLS enforced (sslmode=require) **Data Masking**: - Sensitive fields (email, phone) masked in logs - PII redacted from error messages and stack traces - Audit logs: Full data retention with access controls **Compliance**: - GDPR: User data export and deletion workflows - SOC 2 Type II: Audit logging for all data access - HIPAA: PHI isolation in dedicated tenants with enhanced encryption --- ## Performance Characteristics ### Latency Targets **API Operations** (P99 latency): - `GET /workflows/{id}`: < 50ms (cache hit), < 150ms (cache miss) - `POST /workflows`: < 200ms (includes validation and database write) - `POST /workflows/{id}/execute`: < 100ms (async, returns execution ID) - `GET /workflows/{id}/history`: < 300ms (paginated, 50 records per page) **Workflow Execution** (P99 latency): - Simple workflow (< 5 steps): < 2 seconds - Medium workflow (5-15 steps): < 5 seconds - Complex workflow (> 15 steps): < 15 seconds - Parallel execution: Linear scaling up to 10 concurrent branches **Database Operations** (P99 latency): - Simple SELECT: < 5ms - JOIN query: < 20ms - INSERT/UPDATE: < 10ms - Batch operations: < 100ms (batch size: 100 records) **Cache Operations** (P99 latency): - Redis GET: < 2ms - Redis SET: < 3ms - Redis DEL: < 2ms ### Throughput Capacity **API Gateway**: - Sustained: 10,000 requests per second - Peak: 25,000 requests per second (5-minute burst) - Rate limiting: 1,000 requests per minute per API key **Workflow Engine**: - Concurrent executions: 8,000 workflows (across 16 pods)",
              "score": null
            }
          ],
          "latency_ms": 11.410455001168884,
          "query_id": "realistic_008",
          "dimension": "original",
          "query": "What are the HPA scaling parameters?"
        },
        {
          "key_facts": [
            "minReplicas: 3",
            "maxReplicas: 10",
            "targetCPUUtilizationPercentage: 70"
          ],
          "found_facts": [
            "minReplicas: 3",
            "maxReplicas: 10"
          ],
          "missed_facts": [
            "targetCPUUtilizationPercentage: 70"
          ],
          "coverage": 0.6666666666666666,
          "expected_docs": [
            "deployment_guide"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_6",
              "content": "**Verify Service Health**: ```bash kubectl get pods -n cloudflow-prod curl https://api.cloudflow.io/health ``` **Recovery Time Objective (RTO)**: 4 hours **Recovery Point Objective (RPO)**: 24 hours --- ## Scaling and Performance ### Horizontal Pod Autoscaling CloudFlow is configured with Horizontal Pod Autoscaler (HPA) to automatically scale based on resource utilization: ```yaml # hpa.yaml apiVersion: autoscaling/v2 kind: HorizontalPodAutoscaler metadata: name: cloudflow-api-hpa namespace: cloudflow-prod spec: scaleTargetRef: apiVersion: apps/v1 kind: Deployment name: cloudflow-api minReplicas: 3 maxReplicas: 10 metrics: - type: Resource resource: name: cpu target: type: Utilization averageUtilization: 70 - type: Resource resource: name: memory target: type: Utilization averageUtilization: 80 behavior: scaleDown: stabilizationWindowSeconds: 300 policies: - type: Percent value: 50 periodSeconds: 60 scaleUp: stabilizationWindowSeconds: 60 policies: - type: Percent value: 100 periodSeconds: 30 - type: Pods value: 2 periodSeconds: 30 selectPolicy: Max ``` Monitor autoscaling events: ```bash kubectl get hpa -n cloudflow-prod -w kubectl describe hpa cloudflow-api-hpa -n cloudflow-prod ``` ### Performance Optimization #### Connection Pooling Ensure optimal database connection pooling settings in PgBouncer: - **Pool Mode**: Transaction (optimal for microservices) - **Default Pool Size**: 25 connections per user - **Max DB Connections**: 100 (matches PostgreSQL `max_connections`) #### Redis Caching Implement Redis caching for frequently accessed data: ```javascript // Example caching strategy const CACHE_TTL = { workflows: 300, // 5 minutes userSessions: 3600, // 1 hour apiResults: 60 // 1 minute }; ``` #### Load Testing Perform regular load testing to validate scaling configuration: ```bash # Install k6 for load testing kubectl apply -f - <<EOF apiVersion: v1 kind: ConfigMap metadata: name: loadtest-script namespace: cloudflow-prod data: script.js: | import http from 'k6/http'; import { check, sleep } from 'k6'; export let options = { stages: [ { duration: '2m', target: 100 }, { duration: '5m', target: 100 }, { duration: '2m', target: 200 }, { duration: '5m', target: 200 }, { duration: '2m', target: 0 }, ], thresholds: { http_req_duration: ['p(95)<500'], }, }; export default function () { let res = http.get('https://api.cloudflow.io/api/workflows'); check(res, { 'status is 200': (r) => r.status === 200 }); sleep(1); } EOF ``` **Performance Targets**: - P95 API latency: < 500ms - P99 API latency: < 1000ms - Error rate: < 0.1% - Throughput: > 1000 requests/second --- ## Security and Compliance ### Network Policies Implement network policies to restrict pod-to-pod communication: ```yaml # network-policy.yaml apiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: cloudflow-network-policy namespace: cloudflow-prod spec:",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_1",
              "content": "**Technology**: Node.js with Express.js framework **Replicas**: 12 pods (production), auto-scaling 8-20 based on CPU **Resource Allocation**: 2 vCPU, 4GB RAM per pod **Key Responsibilities**: - JWT token validation (delegated to Auth Service for initial validation) - Rate limiting: 1000 requests per minute per API key (sliding window) - Request/response transformation and validation using JSON Schema - Routing to downstream services based on URL path patterns - CORS handling for web clients - Request/response logging and correlation ID injection **Critical Endpoints**: - `POST /api/v1/workflows` - Create new workflow - `GET /api/v1/workflows/:id` - Retrieve workflow status - `POST /api/v1/workflows/:id/execute` - Trigger workflow execution - `GET /api/v1/workflows/:id/history` - Get execution history **Dependencies**: - Auth Service (for token validation) - Redis (for rate limiting counters) - All downstream microservices **Performance Targets**: - P50 latency: < 50ms - P99 latency: < 200ms - Throughput: 10,000 RPS sustained --- ### Auth Service **Purpose**: Centralized authentication and authorization service handling user identity, token generation, and permission validation. **Technology**: Go with gRPC for internal communication, REST for external **Replicas**: 8 pods (production), auto-scaling 6-12 **Resource Allocation**: 1 vCPU, 2GB RAM per pod **Key Responsibilities**: - User authentication via multiple providers (OAuth2, SAML, local credentials) - JWT token generation and validation (RS256 algorithm) - Role-based access control (RBAC) with fine-grained permissions - Session management with Redis-backed storage - API key generation and validation for service accounts - MFA enforcement for administrative operations **Authentication Flow**: ``` Client Request \u2192 API Gateway \u2192 Auth Service \u2502 \u251c\u2500 Validate credentials \u251c\u2500 Check MFA if required \u251c\u2500 Generate JWT (15min expiry) \u251c\u2500 Generate refresh token (7 days) \u2514\u2500 Store session in Redis ``` **Token Structure**: - Access Token: JWT with 15-minute expiry - Refresh Token: Opaque token with 7-day expiry, stored in PostgreSQL - Claims: user_id, email, roles[], permissions[], tenant_id **Security Features**: - Password hashing: Argon2id with 64MB memory, 4 iterations - Token rotation on refresh to prevent replay attacks - Brute force protection: 5 failed attempts \u2192 15-minute lockout - Secrets stored in HashiCorp Vault, rotated every 90 days **Performance Targets**: - Token validation: < 10ms (P99) - Token generation: < 50ms (P99) - Throughput: 5,000 RPS for validation operations --- ### Workflow Engine **Purpose**: Core orchestration service that executes workflow definitions, manages state transitions, and coordinates task execution across distributed systems.",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_2",
              "content": "**Technology**: Node.js with TypeScript, Bull queue library **Replicas**: 16 pods (production), auto-scaling 12-24 **Resource Allocation**: 4 vCPU, 8GB RAM per pod **Key Responsibilities**: - Parse and validate workflow definitions (JSON-based DSL) - Execute workflow steps with state machine pattern - Handle retries with exponential backoff (max 3 retries, backoff: 2^n seconds) - Coordinate parallel and sequential task execution - Manage workflow state persistence and recovery - Support for conditional branching and loops - Sub-workflow invocation and composition **Workflow Execution Model**: ``` Workflow Submitted \u2192 Validation \u2192 Queue in Kafka \u2502 \u25bc Workflow Engine picks up \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u25bc \u25bc \u25bc [Task 1 Exec] [Task 2 Exec] [Task 3 Exec] \u2502 \u2502 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u25bc State Update \u2192 PostgreSQL \u2502 \u25bc Publish Event \u2192 Kafka ``` **Workflow DSL Example**: ```json { \"workflow_id\": \"customer-onboarding-v2\", \"version\": \"2.1.0\", \"steps\": [ { \"id\": \"validate-customer\", \"type\": \"http_request\", \"config\": { \"url\": \"https://validation.internal/verify\", \"method\": \"POST\", \"timeout\": 5000, \"retry\": {\"max_attempts\": 3, \"backoff\": \"exponential\"} } }, { \"id\": \"create-account\", \"type\": \"database_operation\", \"depends_on\": [\"validate-customer\"] } ] } ``` **State Management**: - Workflow states: PENDING, RUNNING, COMPLETED, FAILED, CANCELLED - Step states: QUEUED, EXECUTING, SUCCESS, FAILED, SKIPPED - State transitions stored in PostgreSQL with event sourcing pattern - Checkpointing every 10 steps for long-running workflows **Performance Targets**: - Simple workflow (3-5 steps): < 2 seconds (P99) - Complex workflow (20+ steps): < 5 seconds (P99) - Throughput: 500 concurrent workflow executions per pod --- ### Scheduler Service **Purpose**: Time-based workflow triggering system supporting cron-like schedules and one-time delayed executions.",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_3",
              "content": "**Technology**: Go with distributed locking via Redis **Replicas**: 4 pods (production), active-passive with leader election **Resource Allocation**: 2 vCPU, 4GB RAM per pod **Key Responsibilities**: - Parse and validate cron expressions (extended format supporting seconds) - Maintain schedule registry in PostgreSQL - Distributed scheduling with leader election (one active scheduler) - Missed execution handling with configurable catch-up policy - Schedule conflict detection and resolution - Time zone support for international schedules **Scheduling Architecture**: ``` PostgreSQL Schedules Table \u2502 \u25bc Leader Scheduler (elected via Redis) \u2502 \u251c\u2500 Scan for due schedules (every 10 seconds) \u251c\u2500 Acquire lock per schedule (prevents duplicates) \u251c\u2500 Publish to Kafka \u2192 workflow.events topic \u2514\u2500 Update last_run timestamp ``` **Schedule Types**: - **Cron-based**: `0 */5 * * * *` (every 5 minutes) - **One-time**: Specific timestamp for delayed execution - **Interval-based**: Every N seconds/minutes/hours **Reliability Features**: - Leader election using Redis with 30-second lease - Heartbeat mechanism to detect leader failure (5-second interval) - Automatic failover to standby scheduler (< 10 seconds) - Schedule versioning to handle updates during execution - Missed execution policy: SKIP, RUN_ONCE, or RUN_ALL **Performance Targets**: - Schedule evaluation: < 100ms per cycle - Accuracy: \u00b1 1 second for schedule triggers - Capacity: 100,000 active schedules --- ### Notification Service **Purpose**: Multi-channel notification delivery system supporting email, SMS, webhooks, and in-app notifications.",
              "score": null
            },
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_7",
              "content": "podSelector: matchLabels: app: cloudflow policyTypes: - Ingress - Egress ingress: - from: - namespaceSelector: matchLabels: name: cloudflow-prod - podSelector: matchLabels: app: nginx-ingress ports: - protocol: TCP port: 3000 egress: - to: - podSelector: matchLabels: app: postgresql ports: - protocol: TCP port: 5432 - to: - podSelector: matchLabels: app: redis ports: - protocol: TCP port: 6379 ``` ### Pod Security Standards Enforce Pod Security Standards at the namespace level: ```bash kubectl label namespace cloudflow-prod \\ pod-security.kubernetes.io/enforce=restricted \\ pod-security.kubernetes.io/audit=restricted \\ pod-security.kubernetes.io/warn=restricted ``` ### Secrets Management Use AWS Secrets Manager for sensitive credentials: ```bash # Install External Secrets Operator helm install external-secrets external-secrets/external-secrets \\ --namespace external-secrets-system \\ --create-namespace # Create SecretStore kubectl apply -f - <<EOF apiVersion: external-secrets.io/v1beta1 kind: SecretStore metadata: name: aws-secrets-manager namespace: cloudflow-prod spec: provider: aws: service: SecretsManager region: us-east-1 auth: jwt: serviceAccountRef: name: external-secrets-sa EOF ``` --- ## Troubleshooting ### Common Issues #### Pods in CrashLoopBackOff **Symptoms**: Pods continuously restart **Diagnosis**: ```bash kubectl logs -n cloudflow-prod <pod-name> --previous kubectl describe pod -n cloudflow-prod <pod-name> ``` **Common Causes**: - Database connection failure - Invalid environment variables - Insufficient resources #### High Memory Usage **Symptoms**: Pods being OOMKilled **Diagnosis**: ```bash kubectl top pods -n cloudflow-prod ``` **Resolution**: - Increase memory limits in deployment - Check for memory leaks in application logs - Review heap dump for Node.js processes #### Database Connection Pool Exhausted **Symptoms**: \"Too many connections\" errors **Diagnosis**: ```bash # Check PgBouncer stats kubectl exec -it pgbouncer-xxx -n cloudflow-prod -- \\ psql -p 5432 -U pgbouncer pgbouncer -c \"SHOW POOLS;\" ``` **Resolution**: - Increase `max_db_connections` in PgBouncer - Optimize application connection usage - Add more PostgreSQL replicas ### Support Contacts - **On-call Engineer**: pagerduty.com/cloudflow-oncall - **Slack Channel**: #cloudflow-operations - **Documentation**: https://docs.cloudflow.io - **Status Page**: https://status.cloudflow.io --- ## Appendix ### Useful Commands ```bash # View all resources in namespace kubectl get all -n cloudflow-prod # Check resource usage kubectl top pods -n cloudflow-prod kubectl top nodes # View logs kubectl logs -f deployment/cloudflow-api -n cloudflow-prod # Execute command in pod kubectl exec -it <pod-name> -n cloudflow-prod -- /bin/sh # Port forward for local testing kubectl port-forward svc/cloudflow-api 8080:80 -n cloudflow-prod # Rollback deployment helm rollback cloudflow -n cloudflow-prod # Update deployment helm upgrade cloudflow cloudflow/cloudflow \\ --namespace cloudflow-prod \\ --values values-production.yaml ``` ### Maintenance Windows Scheduled maintenance occurs during the following windows: - **Primary**: Sunday 02:00-06:00",
              "score": null
            }
          ],
          "latency_ms": 11.384858000383247,
          "query_id": "realistic_008",
          "dimension": "synonym",
          "query": "How does horizontal pod autoscaling work?"
        },
        {
          "key_facts": [
            "minReplicas: 3",
            "maxReplicas: 10",
            "targetCPUUtilizationPercentage: 70"
          ],
          "found_facts": [
            "minReplicas: 3",
            "maxReplicas: 10"
          ],
          "missed_facts": [
            "targetCPUUtilizationPercentage: 70"
          ],
          "coverage": 0.6666666666666666,
          "expected_docs": [
            "deployment_guide"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_7",
              "content": "podSelector: matchLabels: app: cloudflow policyTypes: - Ingress - Egress ingress: - from: - namespaceSelector: matchLabels: name: cloudflow-prod - podSelector: matchLabels: app: nginx-ingress ports: - protocol: TCP port: 3000 egress: - to: - podSelector: matchLabels: app: postgresql ports: - protocol: TCP port: 5432 - to: - podSelector: matchLabels: app: redis ports: - protocol: TCP port: 6379 ``` ### Pod Security Standards Enforce Pod Security Standards at the namespace level: ```bash kubectl label namespace cloudflow-prod \\ pod-security.kubernetes.io/enforce=restricted \\ pod-security.kubernetes.io/audit=restricted \\ pod-security.kubernetes.io/warn=restricted ``` ### Secrets Management Use AWS Secrets Manager for sensitive credentials: ```bash # Install External Secrets Operator helm install external-secrets external-secrets/external-secrets \\ --namespace external-secrets-system \\ --create-namespace # Create SecretStore kubectl apply -f - <<EOF apiVersion: external-secrets.io/v1beta1 kind: SecretStore metadata: name: aws-secrets-manager namespace: cloudflow-prod spec: provider: aws: service: SecretsManager region: us-east-1 auth: jwt: serviceAccountRef: name: external-secrets-sa EOF ``` --- ## Troubleshooting ### Common Issues #### Pods in CrashLoopBackOff **Symptoms**: Pods continuously restart **Diagnosis**: ```bash kubectl logs -n cloudflow-prod <pod-name> --previous kubectl describe pod -n cloudflow-prod <pod-name> ``` **Common Causes**: - Database connection failure - Invalid environment variables - Insufficient resources #### High Memory Usage **Symptoms**: Pods being OOMKilled **Diagnosis**: ```bash kubectl top pods -n cloudflow-prod ``` **Resolution**: - Increase memory limits in deployment - Check for memory leaks in application logs - Review heap dump for Node.js processes #### Database Connection Pool Exhausted **Symptoms**: \"Too many connections\" errors **Diagnosis**: ```bash # Check PgBouncer stats kubectl exec -it pgbouncer-xxx -n cloudflow-prod -- \\ psql -p 5432 -U pgbouncer pgbouncer -c \"SHOW POOLS;\" ``` **Resolution**: - Increase `max_db_connections` in PgBouncer - Optimize application connection usage - Add more PostgreSQL replicas ### Support Contacts - **On-call Engineer**: pagerduty.com/cloudflow-oncall - **Slack Channel**: #cloudflow-operations - **Documentation**: https://docs.cloudflow.io - **Status Page**: https://status.cloudflow.io --- ## Appendix ### Useful Commands ```bash # View all resources in namespace kubectl get all -n cloudflow-prod # Check resource usage kubectl top pods -n cloudflow-prod kubectl top nodes # View logs kubectl logs -f deployment/cloudflow-api -n cloudflow-prod # Execute command in pod kubectl exec -it <pod-name> -n cloudflow-prod -- /bin/sh # Port forward for local testing kubectl port-forward svc/cloudflow-api 8080:80 -n cloudflow-prod # Rollback deployment helm rollback cloudflow -n cloudflow-prod # Update deployment helm upgrade cloudflow cloudflow/cloudflow \\ --namespace cloudflow-prod \\ --values values-production.yaml ``` ### Maintenance Windows Scheduled maintenance occurs during the following windows: - **Primary**: Sunday 02:00-06:00",
              "score": null
            },
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_6",
              "content": "**Verify Service Health**: ```bash kubectl get pods -n cloudflow-prod curl https://api.cloudflow.io/health ``` **Recovery Time Objective (RTO)**: 4 hours **Recovery Point Objective (RPO)**: 24 hours --- ## Scaling and Performance ### Horizontal Pod Autoscaling CloudFlow is configured with Horizontal Pod Autoscaler (HPA) to automatically scale based on resource utilization: ```yaml # hpa.yaml apiVersion: autoscaling/v2 kind: HorizontalPodAutoscaler metadata: name: cloudflow-api-hpa namespace: cloudflow-prod spec: scaleTargetRef: apiVersion: apps/v1 kind: Deployment name: cloudflow-api minReplicas: 3 maxReplicas: 10 metrics: - type: Resource resource: name: cpu target: type: Utilization averageUtilization: 70 - type: Resource resource: name: memory target: type: Utilization averageUtilization: 80 behavior: scaleDown: stabilizationWindowSeconds: 300 policies: - type: Percent value: 50 periodSeconds: 60 scaleUp: stabilizationWindowSeconds: 60 policies: - type: Percent value: 100 periodSeconds: 30 - type: Pods value: 2 periodSeconds: 30 selectPolicy: Max ``` Monitor autoscaling events: ```bash kubectl get hpa -n cloudflow-prod -w kubectl describe hpa cloudflow-api-hpa -n cloudflow-prod ``` ### Performance Optimization #### Connection Pooling Ensure optimal database connection pooling settings in PgBouncer: - **Pool Mode**: Transaction (optimal for microservices) - **Default Pool Size**: 25 connections per user - **Max DB Connections**: 100 (matches PostgreSQL `max_connections`) #### Redis Caching Implement Redis caching for frequently accessed data: ```javascript // Example caching strategy const CACHE_TTL = { workflows: 300, // 5 minutes userSessions: 3600, // 1 hour apiResults: 60 // 1 minute }; ``` #### Load Testing Perform regular load testing to validate scaling configuration: ```bash # Install k6 for load testing kubectl apply -f - <<EOF apiVersion: v1 kind: ConfigMap metadata: name: loadtest-script namespace: cloudflow-prod data: script.js: | import http from 'k6/http'; import { check, sleep } from 'k6'; export let options = { stages: [ { duration: '2m', target: 100 }, { duration: '5m', target: 100 }, { duration: '2m', target: 200 }, { duration: '5m', target: 200 }, { duration: '2m', target: 0 }, ], thresholds: { http_req_duration: ['p(95)<500'], }, }; export default function () { let res = http.get('https://api.cloudflow.io/api/workflows'); check(res, { 'status is 200': (r) => r.status === 200 }); sleep(1); } EOF ``` **Performance Targets**: - P95 API latency: < 500ms - P99 API latency: < 1000ms - Error rate: < 0.1% - Throughput: > 1000 requests/second --- ## Security and Compliance ### Network Policies Implement network policies to restrict pod-to-pod communication: ```yaml # network-policy.yaml apiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: cloudflow-network-policy namespace: cloudflow-prod spec:",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_2",
              "content": "**Technology**: Node.js with TypeScript, Bull queue library **Replicas**: 16 pods (production), auto-scaling 12-24 **Resource Allocation**: 4 vCPU, 8GB RAM per pod **Key Responsibilities**: - Parse and validate workflow definitions (JSON-based DSL) - Execute workflow steps with state machine pattern - Handle retries with exponential backoff (max 3 retries, backoff: 2^n seconds) - Coordinate parallel and sequential task execution - Manage workflow state persistence and recovery - Support for conditional branching and loops - Sub-workflow invocation and composition **Workflow Execution Model**: ``` Workflow Submitted \u2192 Validation \u2192 Queue in Kafka \u2502 \u25bc Workflow Engine picks up \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u25bc \u25bc \u25bc [Task 1 Exec] [Task 2 Exec] [Task 3 Exec] \u2502 \u2502 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u25bc State Update \u2192 PostgreSQL \u2502 \u25bc Publish Event \u2192 Kafka ``` **Workflow DSL Example**: ```json { \"workflow_id\": \"customer-onboarding-v2\", \"version\": \"2.1.0\", \"steps\": [ { \"id\": \"validate-customer\", \"type\": \"http_request\", \"config\": { \"url\": \"https://validation.internal/verify\", \"method\": \"POST\", \"timeout\": 5000, \"retry\": {\"max_attempts\": 3, \"backoff\": \"exponential\"} } }, { \"id\": \"create-account\", \"type\": \"database_operation\", \"depends_on\": [\"validate-customer\"] } ] } ``` **State Management**: - Workflow states: PENDING, RUNNING, COMPLETED, FAILED, CANCELLED - Step states: QUEUED, EXECUTING, SUCCESS, FAILED, SKIPPED - State transitions stored in PostgreSQL with event sourcing pattern - Checkpointing every 10 steps for long-running workflows **Performance Targets**: - Simple workflow (3-5 steps): < 2 seconds (P99) - Complex workflow (20+ steps): < 5 seconds (P99) - Throughput: 500 concurrent workflow executions per pod --- ### Scheduler Service **Purpose**: Time-based workflow triggering system supporting cron-like schedules and one-time delayed executions.",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_3",
              "content": "**Technology**: Go with distributed locking via Redis **Replicas**: 4 pods (production), active-passive with leader election **Resource Allocation**: 2 vCPU, 4GB RAM per pod **Key Responsibilities**: - Parse and validate cron expressions (extended format supporting seconds) - Maintain schedule registry in PostgreSQL - Distributed scheduling with leader election (one active scheduler) - Missed execution handling with configurable catch-up policy - Schedule conflict detection and resolution - Time zone support for international schedules **Scheduling Architecture**: ``` PostgreSQL Schedules Table \u2502 \u25bc Leader Scheduler (elected via Redis) \u2502 \u251c\u2500 Scan for due schedules (every 10 seconds) \u251c\u2500 Acquire lock per schedule (prevents duplicates) \u251c\u2500 Publish to Kafka \u2192 workflow.events topic \u2514\u2500 Update last_run timestamp ``` **Schedule Types**: - **Cron-based**: `0 */5 * * * *` (every 5 minutes) - **One-time**: Specific timestamp for delayed execution - **Interval-based**: Every N seconds/minutes/hours **Reliability Features**: - Leader election using Redis with 30-second lease - Heartbeat mechanism to detect leader failure (5-second interval) - Automatic failover to standby scheduler (< 10 seconds) - Schedule versioning to handle updates during execution - Missed execution policy: SKIP, RUN_ONCE, or RUN_ALL **Performance Targets**: - Schedule evaluation: < 100ms per cycle - Accuracy: \u00b1 1 second for schedule triggers - Capacity: 100,000 active schedules --- ### Notification Service **Purpose**: Multi-channel notification delivery system supporting email, SMS, webhooks, and in-app notifications.",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_4",
              "content": "**Technology**: Node.js with worker pool pattern **Replicas**: 8 pods (production), auto-scaling 6-16 **Resource Allocation**: 2 vCPU, 4GB RAM per pod **Key Responsibilities**: - Consume notification events from Kafka - Template rendering with Handlebars (cached in Redis) - Multi-channel delivery (Email via SendGrid, SMS via Twilio, Webhooks) - Retry logic with dead letter queue for failed deliveries - Delivery status tracking and analytics - User preference management (opt-in/opt-out) **Notification Flow**: ``` Event Source \u2192 Kafka (notifications.email topic) \u2502 \u25bc Notification Service Consumer \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u25bc \u25bc \u25bc [Email Queue] [SMS Queue] [Webhook Queue] \u2502 \u2502 \u2502 \u25bc \u25bc \u25bc [SendGrid] [Twilio] [HTTP POST] \u2502 \u2502 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u25bc Delivery Status \u2192 PostgreSQL ``` **Channel Configuration**: - Email: SendGrid with IP warming, DKIM/SPF configured - SMS: Twilio with fallback to AWS SNS - Webhook: Signed payloads (HMAC-SHA256), retry 3 times with exponential backoff - In-app: WebSocket connections via Socket.io **Template Management**: - Templates stored in PostgreSQL with versioning - Compiled templates cached in Redis (1-hour TTL) - Support for localization (i18n) with 12 languages - A/B testing capability for email campaigns **Performance Targets**: - Email delivery: < 5 seconds (P99) from event to SendGrid - SMS delivery: < 3 seconds (P99) - Throughput: 10,000 notifications per minute --- ## Data Flow Architecture ### Synchronous Request Flow Client requests follow a synchronous path through the API Gateway to backend services: ``` 1. Client \u2192 Load Balancer (TLS termination) 2. Load Balancer \u2192 API Gateway (HTTP/2) 3. API Gateway \u2192 Auth Service (JWT validation via gRPC) 4. API Gateway \u2192 Target Service (HTTP/REST or gRPC) 5. Target Service \u2192 Database/Cache (data retrieval) 6. Response propagates back through the chain ``` **Timeout Configuration**: - Client \u2192 Load Balancer: 60 seconds - Load Balancer \u2192 API Gateway: 55 seconds - API Gateway \u2192 Services: 45 seconds - Service \u2192 Database: 10 seconds ### Asynchronous Event Flow Long-running operations and inter-service communication use event-driven patterns: ``` 1. Service publishes event \u2192 Kafka topic 2. Kafka persists event (replication factor: 3) 3. Consumer groups subscribe to topics 4. Consumers process events with at-least-once delivery 5. State updates written to PostgreSQL 6.",
              "score": null
            }
          ],
          "latency_ms": 11.377622999134474,
          "query_id": "realistic_008",
          "dimension": "problem",
          "query": "Pods aren't scaling up during traffic spikes"
        },
        {
          "key_facts": [
            "minReplicas: 3",
            "maxReplicas: 10",
            "targetCPUUtilizationPercentage: 70"
          ],
          "found_facts": [
            "minReplicas: 3",
            "maxReplicas: 10",
            "targetCPUUtilizationPercentage: 70"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "deployment_guide"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_6",
              "content": "**Verify Service Health**: ```bash kubectl get pods -n cloudflow-prod curl https://api.cloudflow.io/health ``` **Recovery Time Objective (RTO)**: 4 hours **Recovery Point Objective (RPO)**: 24 hours --- ## Scaling and Performance ### Horizontal Pod Autoscaling CloudFlow is configured with Horizontal Pod Autoscaler (HPA) to automatically scale based on resource utilization: ```yaml # hpa.yaml apiVersion: autoscaling/v2 kind: HorizontalPodAutoscaler metadata: name: cloudflow-api-hpa namespace: cloudflow-prod spec: scaleTargetRef: apiVersion: apps/v1 kind: Deployment name: cloudflow-api minReplicas: 3 maxReplicas: 10 metrics: - type: Resource resource: name: cpu target: type: Utilization averageUtilization: 70 - type: Resource resource: name: memory target: type: Utilization averageUtilization: 80 behavior: scaleDown: stabilizationWindowSeconds: 300 policies: - type: Percent value: 50 periodSeconds: 60 scaleUp: stabilizationWindowSeconds: 60 policies: - type: Percent value: 100 periodSeconds: 30 - type: Pods value: 2 periodSeconds: 30 selectPolicy: Max ``` Monitor autoscaling events: ```bash kubectl get hpa -n cloudflow-prod -w kubectl describe hpa cloudflow-api-hpa -n cloudflow-prod ``` ### Performance Optimization #### Connection Pooling Ensure optimal database connection pooling settings in PgBouncer: - **Pool Mode**: Transaction (optimal for microservices) - **Default Pool Size**: 25 connections per user - **Max DB Connections**: 100 (matches PostgreSQL `max_connections`) #### Redis Caching Implement Redis caching for frequently accessed data: ```javascript // Example caching strategy const CACHE_TTL = { workflows: 300, // 5 minutes userSessions: 3600, // 1 hour apiResults: 60 // 1 minute }; ``` #### Load Testing Perform regular load testing to validate scaling configuration: ```bash # Install k6 for load testing kubectl apply -f - <<EOF apiVersion: v1 kind: ConfigMap metadata: name: loadtest-script namespace: cloudflow-prod data: script.js: | import http from 'k6/http'; import { check, sleep } from 'k6'; export let options = { stages: [ { duration: '2m', target: 100 }, { duration: '5m', target: 100 }, { duration: '2m', target: 200 }, { duration: '5m', target: 200 }, { duration: '2m', target: 0 }, ], thresholds: { http_req_duration: ['p(95)<500'], }, }; export default function () { let res = http.get('https://api.cloudflow.io/api/workflows'); check(res, { 'status is 200': (r) => r.status === 200 }); sleep(1); } EOF ``` **Performance Targets**: - P95 API latency: < 500ms - P99 API latency: < 1000ms - Error rate: < 0.1% - Throughput: > 1000 requests/second --- ## Security and Compliance ### Network Policies Implement network policies to restrict pod-to-pod communication: ```yaml # network-policy.yaml apiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: cloudflow-network-policy namespace: cloudflow-prod spec:",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_3",
              "content": "Workflow Context Accumulation** Large workflow executions may accumulate state in memory. **Solution:** ```bash # Configure context cleanup cloudflow config set workflow.context.max_size_mb 100 cloudflow config set workflow.context.cleanup_threshold 0.8 # Enable context persistence to disk cloudflow config set workflow.context.persistence.enabled true cloudflow config set workflow.context.persistence.backend s3 ``` **2. Connection Pool Leaks** **Diagnosis:** ```bash # Check active connections cloudflow db connections --verbose # Expected output: # Active: 45/100 # Idle: 23 # Waiting: 2 # Average age: 245s ``` **Resolution:** ```bash # Adjust connection pool settings cloudflow config set db.pool.max_connections 100 cloudflow config set db.pool.min_connections 10 cloudflow config set db.pool.idle_timeout 300 cloudflow config set db.pool.max_lifetime 1800 # Force connection pool reset cloudflow db pool reset --confirm ``` **3. Event Stream Buffers** Unbounded event buffers can cause memory exhaustion. ```bash # Configure event buffer limits cloudflow config set events.buffer.max_size 10000 cloudflow config set events.buffer.overflow_strategy drop_oldest # Enable event streaming to external sink cloudflow config set events.sink.type kafka cloudflow config set events.sink.kafka.brokers \"kafka-1:9092,kafka-2:9092\" cloudflow config set events.sink.kafka.topic cloudflow-events ``` --- ## Database Connection Issues ### Connection Pool Exhaustion #### Symptoms - Error: `could not obtain connection from pool within 5000ms` - Error: `connection pool exhausted (100/100 connections in use)` - API requests fail with `503 Service Unavailable` - Database CPU usage normal, but connection count at maximum #### Investigation ```bash # Check current connection pool status cloudflow db pool status --detailed # Output example: # Pool Statistics: # Total Connections: 100/100 (100%) # Active: 87 # Idle: 13 # Waiting Requests: 45 # Average Wait Time: 3420ms # Max Wait Time: 8234ms # Identify long-running queries cloudflow db queries --status running --min-duration 30000 # Check connection distribution by client SELECT application_name, state, COUNT(*) as conn_count, AVG(EXTRACT(EPOCH FROM (NOW() - state_change))) as avg_duration_sec FROM pg_stat_activity WHERE datname = 'cloudflow_production' GROUP BY application_name, state ORDER BY conn_count DESC; ``` #### Resolution **Immediate mitigation:** ```bash # Temporarily increase connection limit (requires database restart) cloudflow db config set max_connections 150 # Kill idle connections older than 5 minutes cloudflow db connections kill --idle-timeout 300 # Restart connection pool without downtime kubectl rollout restart deployment/cloudflow-api -n cloudflow kubectl rollout status deployment/cloudflow-api -n cloudflow ``` **Long-term solutions:** 1.",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_8",
              "content": "Leverage caching:** ```bash # Enable client-side caching export CLOUDFLOW_CACHE_ENABLED=true export CLOUDFLOW_CACHE_TTL=300 # Cache workflow metadata cloudflow workflows list --use-cache --cache-ttl 600 ``` --- ## Log Analysis & Debugging ### Accessing CloudFlow Logs #### Kubernetes Deployments ```bash # List all CloudFlow pods kubectl get pods -n cloudflow # Tail logs from API server kubectl logs -f -n cloudflow deployment/cloudflow-api --tail=100 # Get logs from specific pod kubectl logs -n cloudflow cloudflow-api-7d4f6b8c9d-x7k2m # Get logs from previous crashed pod kubectl logs -n cloudflow cloudflow-api-7d4f6b8c9d-x7k2m --previous # Get logs from all pods in deployment kubectl logs -n cloudflow deployment/cloudflow-api --all-containers=true # Stream logs from multiple pods kubectl logs -n cloudflow -l app=cloudflow-api -f --max-log-requests=10 ``` #### Log Levels CloudFlow supports the following log levels: - `TRACE` - Very detailed debugging information - `DEBUG` - Detailed debugging information - `INFO` - Informational messages (default) - `WARN` - Warning messages - `ERROR` - Error messages - `FATAL` - Fatal errors causing shutdown **Changing log levels:** ```bash # Set global log level cloudflow config set logging.level DEBUG # Set log level for specific component cloudflow config set logging.components.database DEBUG cloudflow config set logging.components.auth INFO cloudflow config set logging.components.workflows TRACE # Temporary log level increase (resets after 1 hour) cloudflow debug set-log-level DEBUG --duration 3600 # View current log configuration cloudflow config get logging --format json ``` ### Grep Patterns for Common Issues #### Authentication Failures ```bash # Find all authentication errors kubectl logs -n cloudflow deployment/cloudflow-api | \\ grep -i \"authentication\\|401\\|unauthorized\" | \\ tail -n 50 # Find JWT validation failures kubectl logs -n cloudflow deployment/cloudflow-api | \\ grep -E \"JWT|token.*invalid|signature.*failed\" # Find clock skew issues kubectl logs -n cloudflow deployment/cloudflow-api | \\ grep -i \"clock skew\\|time.*sync\\|nbf\\|exp\" ``` #### Database Errors ```bash # Find database connection errors kubectl logs -n cloudflow deployment/cloudflow-api | \\ grep -E \"connection.*pool|could not connect|database.*timeout\" # Find slow queries kubectl logs -n cloudflow deployment/cloudflow-api | \\ grep \"slow query\" | \\ awk '{print $NF}' | \\ sort -n | \\ tail -n 20 # Find deadlock errors kubectl logs -n cloudflow deployment/cloudflow-api | \\ grep -i \"deadlock detected\" ``` #### Workflow Execution Errors ```bash # Find workflow timeout errors kubectl logs -n cloudflow deployment/cloudflow-workflow-engine | \\ grep -E \"timeout|exceeded.*3600\" # Find workflow retry attempts kubectl logs -n cloudflow deployment/cloudflow-workflow-engine | \\ grep -E \"retry attempt [0-9]|retrying",
              "score": null
            },
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_1",
              "content": "create cluster -f cluster-config.yaml ``` ### Storage Configuration Install the EBS CSI driver for persistent volumes: ```bash eksctl create addon --name aws-ebs-csi-driver \\ --cluster cloudflow-production \\ --service-account-role-arn arn:aws:iam::ACCOUNT_ID:role/AmazonEKS_EBS_CSI_DriverRole \\ --force ``` --- ## Kubernetes Deployment ### Namespace Setup Create the CloudFlow namespace and configure resource quotas: ```yaml # namespace.yaml apiVersion: v1 kind: Namespace metadata: name: cloudflow-prod labels: name: cloudflow-prod environment: production --- apiVersion: v1 kind: ResourceQuota metadata: name: cloudflow-quota namespace: cloudflow-prod spec: hard: requests.cpu: \"50\" requests.memory: 100Gi persistentvolumeclaims: \"10\" services.loadbalancers: \"2\" ``` Apply the namespace configuration: ```bash kubectl apply -f namespace.yaml ``` ### Environment Configuration CloudFlow requires the following environment variables: | Variable | Description | Example | Required | |----------|-------------|---------|----------| | `DATABASE_URL` | PostgreSQL connection string | `postgresql://user:pass@host:5432/cloudflow` | Yes | | `REDIS_URL` | Redis connection string | `redis://redis-master.cloudflow-prod.svc.cluster.local:6379` | Yes | | `JWT_SECRET` | Secret key for JWT token signing | `<generated-secret-256-bit>` | Yes | | `LOG_LEVEL` | Application log verbosity | `info`, `debug`, `warn`, `error` | No (default: `info`) | | `API_PORT` | API server port | `3000` | No (default: `3000`) | | `WORKER_CONCURRENCY` | Number of concurrent workers | `10` | No (default: `5`) | | `SESSION_SECRET` | Session encryption key | `<generated-secret-256-bit>` | Yes | | `AWS_REGION` | AWS region for services | `us-east-1` | Yes | | `S3_BUCKET` | S3 bucket for file storage | `cloudflow-prod-storage` | Yes | | `SMTP_HOST` | Email server hostname | `smtp.sendgrid.net` | No | | `SMTP_PORT` | Email server port | `587` | No | | `METRICS_ENABLED` | Enable Prometheus metrics | `true` | No (default: `false`) | Create a Kubernetes secret for sensitive variables: ```bash kubectl create secret generic cloudflow-secrets \\ --namespace cloudflow-prod \\ --from-literal=DATABASE_URL=\"postgresql://cloudflow:$(cat db-password.txt)@pgbouncer.cloudflow-prod.svc.cluster.local:5432/cloudflow\" \\ --from-literal=REDIS_URL=\"redis://redis-master.cloudflow-prod.svc.cluster.local:6379\" \\ --from-literal=JWT_SECRET=\"$(openssl rand -base64 32)\" \\ --from-literal=SESSION_SECRET=\"$(openssl rand -base64 32)\" ``` ### Helm Chart Deployment Add the CloudFlow Helm repository: ```bash helm repo add cloudflow https://charts.cloudflow.io helm repo update ``` Create a values file for production configuration: ```yaml # values-production.yaml replicaCount: 3 image: repository: 123456789012.dkr.ecr.us-east-1.amazonaws.com/cloudflow tag: \"2.4.0\" pullPolicy: IfNotPresent resources: limits: cpu: 2000m memory: 4Gi requests: cpu: 2000m memory: 4Gi autoscaling: enabled: true minReplicas: 3 maxReplicas: 10 targetCPUUtilizationPercentage: 70 targetMemoryUtilizationPercentage: 80 service: type: ClusterIP port: 80 targetPort: 3000 annotations: service.beta.kubernetes.io/aws-load-balancer-type: \"nlb\" ingress: enabled: true className: nginx annotations: cert-manager.io/cluster-issuer: \"letsencrypt-prod\" nginx.ingress.kubernetes.io/ssl-redirect: \"true\" hosts: - host: api.cloudflow.io paths: - path: / pathType: Prefix tls:",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_6",
              "content": "**Session Storage**: - Key pattern: `session:{user_id}` - TTL: 15 minutes (aligned with JWT expiry) - Data: User session state, preferences - Invalidation: On logout or password change 2. **Rate Limiting Counters**: - Key pattern: `ratelimit:{api_key}:{window}` - TTL: 60 seconds (sliding window) - Data: Request count per window - Algorithm: Token bucket with Redis INCR 3. **Compiled Templates**: - Key pattern: `template:{template_id}:{version}` - TTL: 1 hour - Data: Compiled Handlebars template - Invalidation: On template update 4. **Workflow Definitions**: - Key pattern: `workflow:def:{workflow_id}` - TTL: 1 hour - Data: Parsed workflow JSON - Invalidation: On workflow update or manual flush 5. **User Profiles**: - Key pattern: `user:profile:{user_id}` - TTL: 30 minutes - Data: User metadata (name, email, roles) - Invalidation: On profile update **Cache Hit Rates** (Production Metrics): - Session lookups: 98.5% - Workflow definitions: 94.2% - Template cache: 99.1% - User profiles: 91.8% **Performance Characteristics**: - GET operation P99: < 2ms - SET operation P99: < 3ms - Throughput: 100,000 operations per second - Network latency: < 1ms (same AZ) --- ### Kafka Event Streaming **Cluster Configuration**: - 5 broker nodes (distributed across 3 AZs) - Instance type: kafka.m5.2xlarge (8 vCPU, 32GB RAM) - Storage: 10TB per broker (gp3 SSD) - ZooKeeper ensemble: 3 nodes for cluster coordination - Replication factor: 3 (min in-sync replicas: 2) **Topic Architecture**: ``` workflow.events (32 partitions): - Workflow lifecycle events (created, started, completed, failed) - Retention: 7 days - Message rate: 5,000/sec peak - Consumer groups: workflow-engine, analytics-pipeline notifications.email (16 partitions): - Email notification triggers - Retention: 3 days - Message rate: 2,000/sec peak - Consumer groups: notification-service notifications.sms (8 partitions): - SMS notification triggers - Retention: 3 days - Message rate: 500/sec peak - Consumer groups: notification-service audit.logs (24 partitions): - Security and compliance audit events - Retention: 90 days (compliance requirement) - Message rate: 3,000/sec peak - Consumer groups: audit-processor, security-monitor dead-letter-queue (8 partitions): - Failed message processing - Retention: 30 days - Manual intervention required - Consumer groups: ops-team-alerts ``` **Producer Configuration**: - Acknowledgment: `acks=all` (wait for all in-sync replicas) - Compression: LZ4 (reduces network bandwidth by 60%) - Batching: 100ms linger time, 100KB batch size - Idempotence: Enabled to prevent duplicates **Consumer Configuration**: - Auto-commit: Disabled (manual commit after processing) - Offset reset: Earliest (replay from beginning on new consumer group) - Max poll",
              "score": null
            }
          ],
          "latency_ms": 11.363908000930678,
          "query_id": "realistic_008",
          "dimension": "casual",
          "query": "autoscaling config"
        },
        {
          "key_facts": [
            "minReplicas: 3",
            "maxReplicas: 10",
            "targetCPUUtilizationPercentage: 70"
          ],
          "found_facts": [],
          "missed_facts": [
            "minReplicas: 3",
            "maxReplicas: 10",
            "targetCPUUtilizationPercentage: 70"
          ],
          "coverage": 0.0,
          "expected_docs": [
            "deployment_guide"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_4",
              "content": "**Implement connection pooling optimization:** ```bash # Use PgBouncer for connection pooling kubectl apply -f cloudflow-pgbouncer.yaml # Configure CloudFlow to use PgBouncer cloudflow config set db.host pgbouncer.cloudflow.svc.cluster.local cloudflow config set db.port 6432 cloudflow config set db.pool.mode transaction ``` 2. **Add read replicas:** ```bash # Route read-only queries to replicas cloudflow db replicas add --count 2 cloudflow config set db.read_replicas \"replica-1:5432,replica-2:5432\" cloudflow config set db.read_write_split true ``` ### Connection Timeout Errors #### Error Messages - `connection timeout after 30000ms` - `could not connect to database server at 10.0.2.45:5432` - `database server unreachable` #### Troubleshooting Steps ```bash # Test network connectivity telnet cloudflow-db.internal.company.com 5432 # Check DNS resolution nslookup cloudflow-db.internal.company.com # Verify database is accepting connections pg_isready -h cloudflow-db.internal.company.com -p 5432 -U cloudflow # Check firewall rules sudo iptables -L -n | grep 5432 # Test from CloudFlow pod network kubectl run -n cloudflow debug-pod --rm -i --tty \\ --image=postgres:14 -- \\ psql -h cloudflow-db.internal.company.com -U cloudflow -d cloudflow_production # Review database logs for connection rejections kubectl logs -n cloudflow statefulset/cloudflow-db --tail=100 | \\ grep -i \"connection\\|reject\\|authentication\" ``` ### Maximum Connection Limit (100) Reached This is a hard limit in CloudFlow's database tier. #### Permanent Solutions **Option 1: Upgrade database tier** ```bash # Check available tiers cloudflow db tiers list # Upgrade to higher tier (supports 200 connections) cloudflow db upgrade --tier standard-plus --confirm # Monitor migration progress cloudflow db migration status ``` **Option 2: Implement aggressive connection reuse** ```bash # Reduce connection lifetime cloudflow config set db.pool.max_lifetime 600 # 10 minutes # Enable prepared statement caching cloudflow config set db.prepared_statements.cache true cloudflow config set db.prepared_statements.max_size 250 # Reduce idle connection timeout cloudflow config set db.pool.idle_timeout 120 # 2 minutes ``` --- ## Workflow Execution Failures ### Timeout Errors (3600 second limit) #### Error Message ``` WorkflowExecutionError: Workflow exceeded maximum execution time of 3600 seconds Status: TIMEOUT Workflow ID: wf_9k2n4m8p1q Execution ID: exec_7h3j6k9m2n ``` #### Analysis ```bash # Get workflow execution details cloudflow workflows executions get exec_7h3j6k9m2n --verbose # Check step-by-step breakdown cloudflow workflows executions steps exec_7h3j6k9m2n # Sample output: # Step 1: data_ingestion - Duration: 245s - Status: SUCCESS # Step 2: data_validation - Duration: 123s - Status: SUCCESS # Step 3: data_transformation - Duration: 3189s - Status: TIMEOUT # Step 4: data_export - Duration: 0s - Status: SKIPPED # Identify bottleneck step cloudflow workflows analyze",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_1",
              "content": "**Technology**: Node.js with Express.js framework **Replicas**: 12 pods (production), auto-scaling 8-20 based on CPU **Resource Allocation**: 2 vCPU, 4GB RAM per pod **Key Responsibilities**: - JWT token validation (delegated to Auth Service for initial validation) - Rate limiting: 1000 requests per minute per API key (sliding window) - Request/response transformation and validation using JSON Schema - Routing to downstream services based on URL path patterns - CORS handling for web clients - Request/response logging and correlation ID injection **Critical Endpoints**: - `POST /api/v1/workflows` - Create new workflow - `GET /api/v1/workflows/:id` - Retrieve workflow status - `POST /api/v1/workflows/:id/execute` - Trigger workflow execution - `GET /api/v1/workflows/:id/history` - Get execution history **Dependencies**: - Auth Service (for token validation) - Redis (for rate limiting counters) - All downstream microservices **Performance Targets**: - P50 latency: < 50ms - P99 latency: < 200ms - Throughput: 10,000 RPS sustained --- ### Auth Service **Purpose**: Centralized authentication and authorization service handling user identity, token generation, and permission validation. **Technology**: Go with gRPC for internal communication, REST for external **Replicas**: 8 pods (production), auto-scaling 6-12 **Resource Allocation**: 1 vCPU, 2GB RAM per pod **Key Responsibilities**: - User authentication via multiple providers (OAuth2, SAML, local credentials) - JWT token generation and validation (RS256 algorithm) - Role-based access control (RBAC) with fine-grained permissions - Session management with Redis-backed storage - API key generation and validation for service accounts - MFA enforcement for administrative operations **Authentication Flow**: ``` Client Request \u2192 API Gateway \u2192 Auth Service \u2502 \u251c\u2500 Validate credentials \u251c\u2500 Check MFA if required \u251c\u2500 Generate JWT (15min expiry) \u251c\u2500 Generate refresh token (7 days) \u2514\u2500 Store session in Redis ``` **Token Structure**: - Access Token: JWT with 15-minute expiry - Refresh Token: Opaque token with 7-day expiry, stored in PostgreSQL - Claims: user_id, email, roles[], permissions[], tenant_id **Security Features**: - Password hashing: Argon2id with 64MB memory, 4 iterations - Token rotation on refresh to prevent replay attacks - Brute force protection: 5 failed attempts \u2192 15-minute lockout - Secrets stored in HashiCorp Vault, rotated every 90 days **Performance Targets**: - Token validation: < 10ms (P99) - Token generation: < 50ms (P99) - Throughput: 5,000 RPS for validation operations --- ### Workflow Engine **Purpose**: Core orchestration service that executes workflow definitions, manages state transitions, and coordinates task execution across distributed systems.",
              "score": null
            },
            {
              "doc_id": "user_guide",
              "chunk_id": "user_guide_fix_5",
              "content": "Save and activate **Testing Schedules:** Use the built-in schedule calculator to preview upcoming executions: ``` Next 5 executions: 1. 2026-01-24 14:00:00 UTC 2. 2026-01-25 14:00:00 UTC 3. 2026-01-26 14:00:00 UTC 4. 2026-01-27 14:00:00 UTC 5. 2026-01-28 14:00:00 UTC ``` ## Error Handling Robust error handling ensures your workflows are resilient and reliable. ### Retry Policies Configure automatic retries for failed actions: ```yaml - id: api_call action: http_request config: url: \"https://api.example.com/data\" retry: max_attempts: 3 backoff_type: \"exponential\" # or \"fixed\", \"linear\" initial_interval: 1000 # milliseconds max_interval: 30000 multiplier: 2.0 retry_on: - timeout - network_error - status: [500, 502, 503, 504] ``` **Backoff Strategies:** - **Fixed**: Wait the same amount of time between retries - Attempt 1: Wait 1s - Attempt 2: Wait 1s - Attempt 3: Wait 1s - **Linear**: Increase wait time by a fixed amount - Attempt 1: Wait 1s - Attempt 2: Wait 2s - Attempt 3: Wait 3s - **Exponential**: Double the wait time with each retry (recommended) - Attempt 1: Wait 1s - Attempt 2: Wait 2s - Attempt 3: Wait 4s **Retry Conditions:** Control which errors trigger retries: - `timeout`: Request timeout - `network_error`: Connection failures - `status`: Specific HTTP status codes - `error_code`: Application-specific error codes ### Fallback Actions Execute alternative actions when the primary action fails: ```yaml - id: primary_payment action: http_request config: url: \"https://primary-payment-gateway.com/charge\" method: POST body: amount: \"{{amount}}\" on_error: - id: fallback_payment action: http_request config: url: \"https://backup-payment-gateway.com/charge\" method: POST body: amount: \"{{amount}}\" - id: notify_admin action: email config: to: \"admin@company.com\" subject: \"Payment Gateway Failure\" body: \"Primary gateway failed, switched to backup\" ``` **Error Object:** Access error details in fallback actions: ``` {{error.message}} # Error message {{error.code}} # Error code {{error.step_id}} # Failed step ID {{error.timestamp}} # When the error occurred {{error.attempts}} # Number of retry attempts ``` ### Dead Letter Queue When all retries and fallbacks fail, CloudFlow can route failed executions to a Dead Letter Queue (DLQ) for manual review: **Enable DLQ:** ```yaml workflow: error_handling: dead_letter_queue: enabled: true retain_days: 30 ``` **DLQ Features:** - View failed executions in the dashboard - Inspect complete execution context and error details - Retry individual executions after fixing issues - Export failed executions for analysis - Set up alerts for DLQ threshold breaches **Accessing the DLQ:** 1. Navigate to **Workflows** > **[Your Workflow]** > **Dead Letter Queue** 2.",
              "score": null
            },
            {
              "doc_id": "user_guide",
              "chunk_id": "user_guide_fix_6",
              "content": "Filter by error type, date range, or execution ID 3. Click an execution to view full details 4. Click **\"Retry\"** to reprocess with the same input data ### Error Notifications Get notified when workflows fail: ```yaml workflow: notifications: on_failure: - type: email to: \"ops-team@company.com\" - type: slack channel: \"#alerts\" message: \"Workflow {{workflow.name}} failed: {{error.message}}\" on_success_after_retry: - type: slack channel: \"#monitoring\" message: \"Workflow recovered after {{error.attempts}} attempts\" ``` ## Workflow Limits CloudFlow enforces the following limits to ensure platform stability and performance: ### Steps Per Workflow - **Maximum**: 50 steps per workflow - **Recommendation**: Keep workflows focused and modular. If you need more steps, consider splitting into multiple workflows connected via webhooks. ### Execution Timeout - **Default**: 3600 seconds (60 minutes) - **Behavior**: Workflows exceeding this timeout are automatically terminated - **Custom Timeouts**: Enterprise plans can request custom timeout limits **Setting Step-Level Timeouts:** ```yaml - id: long_running_task action: http_request config: url: \"https://api.example.com/process\" timeout: 300 # 5 minutes for this specific step ``` ### Execution Limits - **Maximum**: 1000 executions per day (per workflow) - **Rate Limiting**: 100 concurrent executions per workflow - **Burst Limit**: 10 executions per second **What happens when limits are reached:** - New executions are queued automatically - Webhook triggers return HTTP 429 (Too Many Requests) - Scheduled executions are skipped (logged in audit trail) - Email notifications sent to workflow owner **Monitoring Usage:** View real-time metrics in your workflow dashboard: - Executions today: 847 / 1000 - Average duration: 12.3s - Success rate: 98.2% - Current queue depth: 3 ### Data Limits - **Maximum request/response size**: 10MB per action - **Maximum execution payload**: 50MB total - **Variable value size**: 1MB per variable ### Enterprise Plan Limits Enterprise customers can request increased limits: - Up to 100 steps per workflow - Up to 10,000 executions per day - Up to 7200 second timeout (2 hours) - Priority execution queue - Dedicated capacity allocation Contact sales@cloudflow.io for Enterprise pricing and custom limits. ## Best Practices Follow these best practices to build reliable, maintainable workflows: ### 1. Use Descriptive Names **Good:** - Workflow: \"Sync Customer Data from Salesforce to Database\" - Step: \"validate_customer_email\" **Bad:** - Workflow: \"Workflow 1\" - Step: \"step3\" ### 2.",
              "score": null
            },
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_1",
              "content": "## Rate Limiting To ensure fair usage and system stability, CloudFlow enforces rate limits on all API endpoints. **Default Limits:** - 100 requests per minute per authenticated user - 20 requests per minute for unauthenticated requests - Burst allowance: 150 requests in a 10-second window ### Rate Limit Headers Every API response includes rate limit information: ``` X-RateLimit-Limit: 100 X-RateLimit-Remaining: 87 X-RateLimit-Reset: 1640995200 ``` When you exceed the rate limit, you'll receive a `429 Too Many Requests` response: ```json { \"error\": { \"code\": \"rate_limit_exceeded\", \"message\": \"Rate limit exceeded. Please retry after 42 seconds.\", \"retry_after\": 42 } } ``` **Best Practices:** - Monitor `X-RateLimit-Remaining` header values - Implement exponential backoff when receiving 429 responses - Cache responses when appropriate to reduce API calls - Consider upgrading to Enterprise tier for higher limits (1000 req/min) ## Pagination List endpoints return paginated results to optimize performance. Use `limit` and `offset` parameters to navigate through result sets. **Parameters:** - `limit`: Number of items per page (default: 20, max: 100) - `offset`: Number of items to skip (default: 0) **Example Request:** ```bash curl -H \"X-API-Key: YOUR_API_KEY\" \\ \"https://api.cloudflow.io/v2/workflows?limit=50&offset=100\" ``` **Response Structure:** ```json { \"data\": [...], \"pagination\": { \"total\": 347, \"limit\": 50, \"offset\": 100, \"has_more\": true } } ``` **Python Example:** ```python import requests def fetch_all_workflows(api_key): base_url = \"https://api.cloudflow.io/v2/workflows\" headers = {\"X-API-Key\": api_key} all_workflows = [] offset = 0 limit = 100 while True: response = requests.get( base_url, headers=headers, params={\"limit\": limit, \"offset\": offset} ) data = response.json() all_workflows.extend(data['data']) if not data['pagination']['has_more']: break offset += limit return all_workflows ``` ## Endpoints ### Workflows #### List Workflows Retrieve a paginated list of all workflows in your account. **Endpoint:** `GET /workflows` **Query Parameters:** - `status` (optional): Filter by status (`active`, `paused`, `archived`) - `created_after` (optional): ISO 8601 timestamp - `limit` (optional): Items per page (max 100) - `offset` (optional): Pagination offset **Example Request:** ```bash curl -H \"X-API-Key: YOUR_API_KEY\" \\ \"https://api.cloudflow.io/v2/workflows?status=active&limit=25\" ``` **Example Response (200 OK):** ```json { \"data\": [ { \"id\": \"wf_8x7k2m9p\", \"name\": \"Data Processing Pipeline\", \"description\": \"Processes customer data every hour\", \"status\": \"active\", \"trigger\": { \"type\": \"schedule\", \"cron\": \"0 * * * *\" }, \"steps\": 12, \"created_at\": \"2026-01-15T10:30:00Z\", \"updated_at\": \"2026-01-23T14:22:00Z\", \"last_run\": \"2026-01-24T09:00:00Z\" } ], \"pagination\": { \"total\": 1, \"limit\": 25, \"offset\": 0, \"has_more\": false } } ``` #### Create Workflow Create a new workflow with specified configuration.",
              "score": null
            }
          ],
          "latency_ms": 11.705654000252252,
          "query_id": "realistic_008",
          "dimension": "contextual",
          "query": "I need to configure autoscaling, what are the min/max replicas and thresholds?"
        },
        {
          "key_facts": [
            "minReplicas: 3",
            "maxReplicas: 10",
            "targetCPUUtilizationPercentage: 70"
          ],
          "found_facts": [],
          "missed_facts": [
            "minReplicas: 3",
            "maxReplicas: 10",
            "targetCPUUtilizationPercentage: 70"
          ],
          "coverage": 0.0,
          "expected_docs": [
            "deployment_guide"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_4",
              "content": "**Implement connection pooling optimization:** ```bash # Use PgBouncer for connection pooling kubectl apply -f cloudflow-pgbouncer.yaml # Configure CloudFlow to use PgBouncer cloudflow config set db.host pgbouncer.cloudflow.svc.cluster.local cloudflow config set db.port 6432 cloudflow config set db.pool.mode transaction ``` 2. **Add read replicas:** ```bash # Route read-only queries to replicas cloudflow db replicas add --count 2 cloudflow config set db.read_replicas \"replica-1:5432,replica-2:5432\" cloudflow config set db.read_write_split true ``` ### Connection Timeout Errors #### Error Messages - `connection timeout after 30000ms` - `could not connect to database server at 10.0.2.45:5432` - `database server unreachable` #### Troubleshooting Steps ```bash # Test network connectivity telnet cloudflow-db.internal.company.com 5432 # Check DNS resolution nslookup cloudflow-db.internal.company.com # Verify database is accepting connections pg_isready -h cloudflow-db.internal.company.com -p 5432 -U cloudflow # Check firewall rules sudo iptables -L -n | grep 5432 # Test from CloudFlow pod network kubectl run -n cloudflow debug-pod --rm -i --tty \\ --image=postgres:14 -- \\ psql -h cloudflow-db.internal.company.com -U cloudflow -d cloudflow_production # Review database logs for connection rejections kubectl logs -n cloudflow statefulset/cloudflow-db --tail=100 | \\ grep -i \"connection\\|reject\\|authentication\" ``` ### Maximum Connection Limit (100) Reached This is a hard limit in CloudFlow's database tier. #### Permanent Solutions **Option 1: Upgrade database tier** ```bash # Check available tiers cloudflow db tiers list # Upgrade to higher tier (supports 200 connections) cloudflow db upgrade --tier standard-plus --confirm # Monitor migration progress cloudflow db migration status ``` **Option 2: Implement aggressive connection reuse** ```bash # Reduce connection lifetime cloudflow config set db.pool.max_lifetime 600 # 10 minutes # Enable prepared statement caching cloudflow config set db.prepared_statements.cache true cloudflow config set db.prepared_statements.max_size 250 # Reduce idle connection timeout cloudflow config set db.pool.idle_timeout 120 # 2 minutes ``` --- ## Workflow Execution Failures ### Timeout Errors (3600 second limit) #### Error Message ``` WorkflowExecutionError: Workflow exceeded maximum execution time of 3600 seconds Status: TIMEOUT Workflow ID: wf_9k2n4m8p1q Execution ID: exec_7h3j6k9m2n ``` #### Analysis ```bash # Get workflow execution details cloudflow workflows executions get exec_7h3j6k9m2n --verbose # Check step-by-step breakdown cloudflow workflows executions steps exec_7h3j6k9m2n # Sample output: # Step 1: data_ingestion - Duration: 245s - Status: SUCCESS # Step 2: data_validation - Duration: 123s - Status: SUCCESS # Step 3: data_transformation - Duration: 3189s - Status: TIMEOUT # Step 4: data_export - Duration: 0s - Status: SKIPPED # Identify bottleneck step cloudflow workflows analyze",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_2",
              "content": "**Technology**: Node.js with TypeScript, Bull queue library **Replicas**: 16 pods (production), auto-scaling 12-24 **Resource Allocation**: 4 vCPU, 8GB RAM per pod **Key Responsibilities**: - Parse and validate workflow definitions (JSON-based DSL) - Execute workflow steps with state machine pattern - Handle retries with exponential backoff (max 3 retries, backoff: 2^n seconds) - Coordinate parallel and sequential task execution - Manage workflow state persistence and recovery - Support for conditional branching and loops - Sub-workflow invocation and composition **Workflow Execution Model**: ``` Workflow Submitted \u2192 Validation \u2192 Queue in Kafka \u2502 \u25bc Workflow Engine picks up \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u25bc \u25bc \u25bc [Task 1 Exec] [Task 2 Exec] [Task 3 Exec] \u2502 \u2502 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u25bc State Update \u2192 PostgreSQL \u2502 \u25bc Publish Event \u2192 Kafka ``` **Workflow DSL Example**: ```json { \"workflow_id\": \"customer-onboarding-v2\", \"version\": \"2.1.0\", \"steps\": [ { \"id\": \"validate-customer\", \"type\": \"http_request\", \"config\": { \"url\": \"https://validation.internal/verify\", \"method\": \"POST\", \"timeout\": 5000, \"retry\": {\"max_attempts\": 3, \"backoff\": \"exponential\"} } }, { \"id\": \"create-account\", \"type\": \"database_operation\", \"depends_on\": [\"validate-customer\"] } ] } ``` **State Management**: - Workflow states: PENDING, RUNNING, COMPLETED, FAILED, CANCELLED - Step states: QUEUED, EXECUTING, SUCCESS, FAILED, SKIPPED - State transitions stored in PostgreSQL with event sourcing pattern - Checkpointing every 10 steps for long-running workflows **Performance Targets**: - Simple workflow (3-5 steps): < 2 seconds (P99) - Complex workflow (20+ steps): < 5 seconds (P99) - Throughput: 500 concurrent workflow executions per pod --- ### Scheduler Service **Purpose**: Time-based workflow triggering system supporting cron-like schedules and one-time delayed executions.",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_4",
              "content": "**Technology**: Node.js with worker pool pattern **Replicas**: 8 pods (production), auto-scaling 6-16 **Resource Allocation**: 2 vCPU, 4GB RAM per pod **Key Responsibilities**: - Consume notification events from Kafka - Template rendering with Handlebars (cached in Redis) - Multi-channel delivery (Email via SendGrid, SMS via Twilio, Webhooks) - Retry logic with dead letter queue for failed deliveries - Delivery status tracking and analytics - User preference management (opt-in/opt-out) **Notification Flow**: ``` Event Source \u2192 Kafka (notifications.email topic) \u2502 \u25bc Notification Service Consumer \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u25bc \u25bc \u25bc [Email Queue] [SMS Queue] [Webhook Queue] \u2502 \u2502 \u2502 \u25bc \u25bc \u25bc [SendGrid] [Twilio] [HTTP POST] \u2502 \u2502 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u25bc Delivery Status \u2192 PostgreSQL ``` **Channel Configuration**: - Email: SendGrid with IP warming, DKIM/SPF configured - SMS: Twilio with fallback to AWS SNS - Webhook: Signed payloads (HMAC-SHA256), retry 3 times with exponential backoff - In-app: WebSocket connections via Socket.io **Template Management**: - Templates stored in PostgreSQL with versioning - Compiled templates cached in Redis (1-hour TTL) - Support for localization (i18n) with 12 languages - A/B testing capability for email campaigns **Performance Targets**: - Email delivery: < 5 seconds (P99) from event to SendGrid - SMS delivery: < 3 seconds (P99) - Throughput: 10,000 notifications per minute --- ## Data Flow Architecture ### Synchronous Request Flow Client requests follow a synchronous path through the API Gateway to backend services: ``` 1. Client \u2192 Load Balancer (TLS termination) 2. Load Balancer \u2192 API Gateway (HTTP/2) 3. API Gateway \u2192 Auth Service (JWT validation via gRPC) 4. API Gateway \u2192 Target Service (HTTP/REST or gRPC) 5. Target Service \u2192 Database/Cache (data retrieval) 6. Response propagates back through the chain ``` **Timeout Configuration**: - Client \u2192 Load Balancer: 60 seconds - Load Balancer \u2192 API Gateway: 55 seconds - API Gateway \u2192 Services: 45 seconds - Service \u2192 Database: 10 seconds ### Asynchronous Event Flow Long-running operations and inter-service communication use event-driven patterns: ``` 1. Service publishes event \u2192 Kafka topic 2. Kafka persists event (replication factor: 3) 3. Consumer groups subscribe to topics 4. Consumers process events with at-least-once delivery 5. State updates written to PostgreSQL 6.",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_5",
              "content": "Success/failure events published back to Kafka ``` **Event Schema**: ```json { \"event_id\": \"uuid-v4\", \"event_type\": \"workflow.execution.completed\", \"timestamp\": \"2026-01-15T10:30:00.000Z\", \"correlation_id\": \"request-trace-id\", \"payload\": { \"workflow_id\": \"wf-12345\", \"execution_id\": \"exec-67890\", \"status\": \"COMPLETED\", \"duration_ms\": 4230 }, \"metadata\": { \"source_service\": \"workflow-engine\", \"schema_version\": \"1.0\" } } ``` ### Inter-Service Communication Services communicate using two primary patterns: **Synchronous (gRPC)**: - Auth Service validation calls from API Gateway - User Service profile lookups - Low latency requirement (< 50ms) - Request-response pattern with circuit breaker **Asynchronous (Kafka)**: - Workflow execution events - Notification triggers - Audit log events - Eventual consistency acceptable --- ## Database Architecture ### PostgreSQL Primary Database **Cluster Configuration**: - Primary-replica setup with 1 primary + 2 read replicas - Instance type: db.r6g.2xlarge (8 vCPU, 64GB RAM) - Storage: 2TB gp3 SSD with 12,000 IOPS - Multi-AZ deployment for high availability - Automated backups: Daily snapshots, 30-day retention - Point-in-time recovery: 5-minute granularity **Database Schema Design**: ``` Core Tables: - users (5M rows): User accounts and profiles - workflows (2M rows): Workflow definitions - workflow_executions (500M rows, partitioned): Execution history - workflow_steps (2B rows, partitioned): Individual step records - schedules (100K rows): Scheduled workflow triggers - notifications (1B rows, partitioned): Notification delivery log Partitioning Strategy: - workflow_executions: Monthly partitions by created_at - workflow_steps: Monthly partitions by created_at - notifications: Weekly partitions by created_at - Automatic partition creation via cron job - Retention policy: Drop partitions older than 12 months ``` **Indexing Strategy**: - Primary keys: UUIDs with B-tree indexes - Frequently queried columns: Compound indexes (e.g., user_id + status + created_at) - JSONB columns: GIN indexes for workflow definitions - Full-text search: GiST indexes on description fields **Connection Pooling**: - PgBouncer in transaction mode - Pool size: 100 connections per service - Max client connections: 2000 - Connection timeout: 30 seconds **Performance Characteristics**: - Read query P99: < 20ms - Write query P99: < 50ms - Transaction throughput: 15,000 TPS - Replication lag: < 500ms --- ### Redis Caching Layer **Cluster Configuration**: - Redis Cluster with 6 nodes (3 primary + 3 replica) - Instance type: cache.r6g.xlarge (4 vCPU, 26GB RAM) - Total memory: 78GB usable cache space - Persistence: RDB snapshots every 5 minutes + AOF - Eviction policy: allkeys-lru **Cache Usage Patterns**: 1.",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_10",
              "content": "database nc -zv cloudflow-db.internal.company.com 5432 # Trace route traceroute api.cloudflow.io # Capture packets tcpdump -i any -w /tmp/capture.pcap port 5432 ``` --- ## Escalation Procedures ### Severity Levels CloudFlow incidents are classified into four severity levels: #### SEV-1: Critical (P1) - **Definition:** Complete service outage or severe degradation affecting all users - **Examples:** - API returns 5xx errors for > 5 minutes - Database completely unavailable - Authentication system down - Data loss or corruption - **Response Time:** Immediate (< 15 minutes) - **Escalation:** Page on-call engineer immediately #### SEV-2: High (P2) - **Definition:** Major functionality impaired affecting multiple users - **Examples:** - Workflow execution success rate < 90% - Significant performance degradation (p95 latency > 10s) - Rate limiting affecting large customer segment - **Response Time:** < 1 hour - **Escalation:** Create incident ticket and notify on-call #### SEV-3: Medium (P3) - **Definition:** Partial functionality degraded affecting some users - **Examples:** - Intermittent failures for specific workflow types - Minor performance issues - Non-critical feature unavailable - **Response Time:** < 4 hours - **Escalation:** Create ticket, normal business hours support #### SEV-4: Low (P4) - **Definition:** Minor issues with minimal user impact - **Examples:** - Cosmetic issues - Documentation errors - Feature requests - **Response Time:** < 2 business days - **Escalation:** Standard support ticket ### Escalation Steps #### Step 1: Initial Assessment (0-5 minutes) ```bash # Run health check cloudflow health check --comprehensive # Check status page curl https://status.cloudflow.io/api/v1/status.json # Review recent changes cloudflow audit log --last 2h --event-type \"deployment,configuration\" # Check metrics dashboard cloudflow metrics dashboard --incident-mode ``` #### Step 2: Gather Information (5-15 minutes) Create incident document with: - Incident timestamp and duration - Affected services and endpoints - Error rates and user impact - Recent changes or deployments - Relevant log excerpts - Correlation IDs for failed requests ```bash # Generate incident report cloudflow debug incident-report \\ --start \"2026-01-24T10:30:00Z\" \\ --end \"2026-01-24T11:00:00Z\" \\ --output incident-report.md # Capture system snapshot cloudflow debug snapshot --output snapshot-$(date +%Y%m%d-%H%M%S).tar.gz ``` #### Step 3: Escalate Based on Severity **For SEV-1 (Critical):** ```bash # Page on-call engineer cloudflow incident create \\ --severity SEV-1 \\ --title \"Complete API outage\" \\ --description \"All API requests returning 503\" \\ --page-oncall # Notify status page cloudflow status update \\ --status \"major_outage\" \\ --message \"We are investigating a complete service",
              "score": null
            }
          ],
          "latency_ms": 11.562777999643004,
          "query_id": "realistic_008",
          "dimension": "negation",
          "query": "Why do we have 3 replicas even with low traffic?"
        },
        {
          "key_facts": [
            "P99 latency: < 200ms",
            "average P99 latency of 180ms for API operations"
          ],
          "found_facts": [
            "average P99 latency of 180ms for API operations"
          ],
          "missed_facts": [
            "P99 latency: < 200ms"
          ],
          "coverage": 0.5,
          "expected_docs": [
            "architecture_overview"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_0",
              "content": "# CloudFlow Platform - System Architecture Overview **Document Version:** 2.3.1 **Last Updated:** January 15, 2026 **Owner:** Platform Architecture Team **Status:** Production ## Executive Summary CloudFlow is a distributed, cloud-native workflow automation platform designed to orchestrate complex business processes at scale. The platform processes over 2.5 million workflow executions daily with an average P99 latency of 180ms for API operations and 4.2 seconds for workflow execution. This document provides a comprehensive overview of the system architecture, including microservices design, data flow patterns, infrastructure decisions, and operational considerations. ## Table of Contents 1. [High-Level Architecture](#high-level-architecture) 2. [Microservices Breakdown](#microservices-breakdown) 3. [Data Flow Architecture](#data-flow-architecture) 4. [Database Architecture](#database-architecture) 5. [Message Queue Patterns](#message-queue-patterns) 6. [Caching Strategy](#caching-strategy) 7. [Security Architecture](#security-architecture) 8. [Performance Characteristics](#performance-characteristics) 9. [Disaster Recovery](#disaster-recovery) --- ## High-Level Architecture CloudFlow follows a microservices architecture pattern deployed across multiple availability zones in AWS. The platform is designed with the following core principles: - **Scalability**: Horizontal scaling for all services with auto-scaling groups - **Resilience**: Circuit breakers, bulkheads, and graceful degradation - **Observability**: Distributed tracing, centralized logging, and comprehensive metrics - **Security**: Zero-trust network architecture with mTLS encryption ### Architecture Diagram (Conceptual) ``` \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 Load Balancer (ALB) \u2502 \u2502 (TLS Termination - 443) \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u25bc \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 API Gateway Layer \u2502 \u2502 (Rate Limiting, Auth, Request Routing) \u2502 \u2514\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u2502 \u2502 \u2502 \u2502 \u25bc \u25bc \u25bc \u25bc \u25bc \u250c\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502Auth\u2502 \u2502Workflow\u2502 \u2502Scheduler\u2502 \u2502Notification\u2502 \u2502User Service\u2502 \u2502Svc \u2502 \u2502 Engine \u2502 \u2502 Service \u2502 \u2502 Service \u2502 \u2502 \u2502 \u2514\u2500\u252c\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u2502 \u2502 \u2502 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 \u2502 \u2502 \u25bc \u25bc \u25bc \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502PostgreSQL\u2502 \u2502 Redis \u2502 \u2502 Kafka \u2502 \u2502 Cluster \u2502 \u2502 Cluster \u2502 \u2502 Cluster \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 ``` ### Technology Stack - **Runtime**: Node.js 20.x (API Gateway, Workflow Engine), Go 1.21 (Auth Service, Scheduler) - **Container Orchestration**: Kubernetes 1.28 (EKS) - **Service Mesh**: Istio 1.20 for service-to-service communication - **Databases**: PostgreSQL 15.4, Redis 7.2 - **Message Broker**: Apache Kafka 3.6 - **Monitoring**: Prometheus, Grafana, Jaeger for distributed tracing - **Secrets Management**: HashiCorp Vault 1.15 --- ## Microservices Breakdown ### API Gateway **Purpose**: Single entry point for all client requests, providing authentication, rate limiting, request routing, and protocol translation.",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_10",
              "content": "- Execution start rate: 500 per second - Completion rate: 450 per second (average 2-second execution time) **Database**: - Read throughput: 50,000 queries per second (across replicas) - Write throughput: 15,000 transactions per second - Connection capacity: 2,000 concurrent connections **Kafka**: - Message ingestion: 100,000 messages per second - Consumer throughput: 80,000 messages per second (aggregated) - End-to-end latency: < 100ms (P99) ### Resource Utilization **CPU Utilization** (Target: 60-70% average): - API Gateway: 55% average, 80% peak - Workflow Engine: 65% average, 85% peak - Auth Service: 40% average, 60% peak **Memory Utilization** (Target: < 80%): - API Gateway: 2.5GB average per pod (4GB allocated) - Workflow Engine: 6GB average per pod (8GB allocated) - Notification Service: 2.8GB average per pod (4GB allocated) **Network Throughput**: - Ingress: 2 Gbps average, 5 Gbps peak - Egress: 1.5 Gbps average, 4 Gbps peak - Internal (service mesh): 8 Gbps average ### Monitoring & Alerting **Key Metrics** (Monitored via Prometheus): - Request rate (per service, per endpoint) - Error rate (4xx, 5xx responses) - Latency percentiles (P50, P95, P99) - Resource utilization (CPU, memory, disk) - Cache hit/miss ratios - Database connection pool saturation - Kafka consumer lag **Alerts** (via PagerDuty): - P1 (Immediate): API error rate > 1%, database connection failure - P2 (< 15 min): P99 latency > 500ms, cache hit rate < 80% - P3 (< 1 hour): Resource utilization > 85%, replica count < minimum **SLI/SLO/SLA**: - SLI: API success rate (non-5xx responses) - SLO: 99.9% uptime per month (43 minutes downtime allowance) - SLA: 99.5% uptime guarantee (customer-facing SLA with credits) --- ## Disaster Recovery ### Recovery Objectives **RPO (Recovery Point Objective): 1 hour** - Maximum acceptable data loss: 1 hour of transactions - Achieved through: Continuous database replication + hourly snapshots - Kafka retention: 7 days allows event replay **RTO (Recovery Time Objective): 4 hours** - Maximum acceptable downtime: 4 hours for full system recovery - Includes: Failover, data verification, and service restoration - Automated runbooks reduce RTO to < 2 hours for common scenarios ### High Availability Architecture **Multi-AZ Deployment**: ``` Region: us-east-1 AZ-1a: AZ-1b: AZ-1c: - API Gateway (4) - API Gateway (4) - API Gateway (4) - Workflow Engine (6) - Workflow Engine (5) - Workflow Engine (5) - Auth Service (3) - Auth Service (3) - Auth",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_2",
              "content": "Review Query Execution Plans** ```sql -- Connect to CloudFlow database cloudflow db connect --readonly -- Explain slow query EXPLAIN ANALYZE SELECT w.*, e.status, e.error_message FROM workflows w LEFT JOIN executions e ON w.id = e.workflow_id WHERE w.workspace_id = 'ws_abc123' AND e.created_at > NOW() - INTERVAL '7 days' ORDER BY e.created_at DESC; -- Check for missing indexes SELECT schemaname, tablename, indexname, indexdef FROM pg_indexes WHERE tablename IN ('workflows', 'executions', 'workflow_steps') ORDER BY tablename, indexname; ``` **3. Optimize Queries** Common optimization techniques: **Add missing indexes:** ```sql -- Index for workflow lookup by workspace CREATE INDEX CONCURRENTLY idx_workflows_workspace_created ON workflows(workspace_id, created_at DESC); -- Index for execution status queries CREATE INDEX CONCURRENTLY idx_executions_status_created ON executions(workflow_id, status, created_at DESC) WHERE status IN ('running', 'pending'); -- Composite index for common filter combinations CREATE INDEX CONCURRENTLY idx_executions_workspace_status ON executions(workspace_id, status, created_at) INCLUDE (error_message, retry_count); ``` **Use query result caching:** ```bash # Enable query cache for workspace metadata cloudflow config set cache.workspace.ttl 3600 # Configure Redis cache backend cloudflow config set cache.backend redis cloudflow config set cache.redis.host redis.cloudflow.svc.cluster.local cloudflow config set cache.redis.port 6379 ``` ### High API Latency #### Latency Breakdown Analysis ```bash # Generate latency report cloudflow metrics latency-report --endpoint \"/api/v1/workflows\" --last 24h # Sample output: # Endpoint: POST /api/v1/workflows/execute # P50: 245ms | P95: 1823ms | P99: 4521ms # Breakdown: # - Auth: 45ms (18%) # - DB Query: 156ms (64%) # - Business Logic: 32ms (13%) # - Response Serialization: 12ms (5%) ``` **Network latency issues:** ```bash # Test connectivity to CloudFlow API time curl -w \"@curl-format.txt\" -o /dev/null -s https://api.cloudflow.io/health # Create curl-format.txt: cat > curl-format.txt << EOF time_namelookup: %{time_namelookup}s\\n time_connect: %{time_connect}s\\n time_appconnect: %{time_appconnect}s\\n time_pretransfer: %{time_pretransfer}s\\n time_redirect: %{time_redirect}s\\n time_starttransfer: %{time_starttransfer}s\\n ----------\\n time_total: %{time_total}s\\n EOF # Trace route to API endpoint traceroute api.cloudflow.io # Check DNS resolution time dig api.cloudflow.io | grep \"Query time\" ``` ### Memory Leaks #### Detection ```bash # Monitor CloudFlow service memory usage kubectl top pods -n cloudflow --sort-by=memory # Get detailed memory metrics for specific pod kubectl exec -n cloudflow deploy/cloudflow-api -- \\ curl localhost:9090/metrics | grep memory # Check for OOMKilled pods kubectl get pods -n cloudflow --field-selector=status.phase=Failed | \\ grep OOMKilled # Review memory limits and requests kubectl describe deployment cloudflow-api -n cloudflow | \\ grep -A 5 \"Limits\\|Requests\" ``` #### Common Causes **1.",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_4",
              "content": "**Technology**: Node.js with worker pool pattern **Replicas**: 8 pods (production), auto-scaling 6-16 **Resource Allocation**: 2 vCPU, 4GB RAM per pod **Key Responsibilities**: - Consume notification events from Kafka - Template rendering with Handlebars (cached in Redis) - Multi-channel delivery (Email via SendGrid, SMS via Twilio, Webhooks) - Retry logic with dead letter queue for failed deliveries - Delivery status tracking and analytics - User preference management (opt-in/opt-out) **Notification Flow**: ``` Event Source \u2192 Kafka (notifications.email topic) \u2502 \u25bc Notification Service Consumer \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u25bc \u25bc \u25bc [Email Queue] [SMS Queue] [Webhook Queue] \u2502 \u2502 \u2502 \u25bc \u25bc \u25bc [SendGrid] [Twilio] [HTTP POST] \u2502 \u2502 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u25bc Delivery Status \u2192 PostgreSQL ``` **Channel Configuration**: - Email: SendGrid with IP warming, DKIM/SPF configured - SMS: Twilio with fallback to AWS SNS - Webhook: Signed payloads (HMAC-SHA256), retry 3 times with exponential backoff - In-app: WebSocket connections via Socket.io **Template Management**: - Templates stored in PostgreSQL with versioning - Compiled templates cached in Redis (1-hour TTL) - Support for localization (i18n) with 12 languages - A/B testing capability for email campaigns **Performance Targets**: - Email delivery: < 5 seconds (P99) from event to SendGrid - SMS delivery: < 3 seconds (P99) - Throughput: 10,000 notifications per minute --- ## Data Flow Architecture ### Synchronous Request Flow Client requests follow a synchronous path through the API Gateway to backend services: ``` 1. Client \u2192 Load Balancer (TLS termination) 2. Load Balancer \u2192 API Gateway (HTTP/2) 3. API Gateway \u2192 Auth Service (JWT validation via gRPC) 4. API Gateway \u2192 Target Service (HTTP/REST or gRPC) 5. Target Service \u2192 Database/Cache (data retrieval) 6. Response propagates back through the chain ``` **Timeout Configuration**: - Client \u2192 Load Balancer: 60 seconds - Load Balancer \u2192 API Gateway: 55 seconds - API Gateway \u2192 Services: 45 seconds - Service \u2192 Database: 10 seconds ### Asynchronous Event Flow Long-running operations and inter-service communication use event-driven patterns: ``` 1. Service publishes event \u2192 Kafka topic 2. Kafka persists event (replication factor: 3) 3. Consumer groups subscribe to topics 4. Consumers process events with at-least-once delivery 5. State updates written to PostgreSQL 6.",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_9",
              "content": "per API key (default: 1000 RPM) ### Secrets Management **HashiCorp Vault Integration**: - Dynamic database credentials: Generated on-demand, 1-hour TTL - Encryption keys: Transit secrets engine for encryption-as-a-service - API keys for external services: Stored with versioning - Rotation policy: Automated rotation every 90 days with notification **Secret Access Pattern**: ``` Service starts \u2192 Vault authentication (Kubernetes service account) \u2502 \u25bc Request secret lease (1-hour TTL) \u2502 \u25bc Vault returns dynamic credentials \u2502 \u25bc Service connects to PostgreSQL \u2502 Renew lease every 30 minutes ``` ### Data Protection **Encryption at Rest**: - PostgreSQL: AES-256 encryption enabled at volume level - Redis: Encryption enabled using AWS KMS - Kafka: Encryption at rest for all data on brokers - S3 backups: Server-side encryption with KMS (SSE-KMS) **Encryption in Transit**: - External traffic: TLS 1.3 only (TLS 1.2 deprecated) - Internal traffic: mTLS via Istio service mesh - Database connections: SSL/TLS enforced (sslmode=require) **Data Masking**: - Sensitive fields (email, phone) masked in logs - PII redacted from error messages and stack traces - Audit logs: Full data retention with access controls **Compliance**: - GDPR: User data export and deletion workflows - SOC 2 Type II: Audit logging for all data access - HIPAA: PHI isolation in dedicated tenants with enhanced encryption --- ## Performance Characteristics ### Latency Targets **API Operations** (P99 latency): - `GET /workflows/{id}`: < 50ms (cache hit), < 150ms (cache miss) - `POST /workflows`: < 200ms (includes validation and database write) - `POST /workflows/{id}/execute`: < 100ms (async, returns execution ID) - `GET /workflows/{id}/history`: < 300ms (paginated, 50 records per page) **Workflow Execution** (P99 latency): - Simple workflow (< 5 steps): < 2 seconds - Medium workflow (5-15 steps): < 5 seconds - Complex workflow (> 15 steps): < 15 seconds - Parallel execution: Linear scaling up to 10 concurrent branches **Database Operations** (P99 latency): - Simple SELECT: < 5ms - JOIN query: < 20ms - INSERT/UPDATE: < 10ms - Batch operations: < 100ms (batch size: 100 records) **Cache Operations** (P99 latency): - Redis GET: < 2ms - Redis SET: < 3ms - Redis DEL: < 2ms ### Throughput Capacity **API Gateway**: - Sustained: 10,000 requests per second - Peak: 25,000 requests per second (5-minute burst) - Rate limiting: 1,000 requests per minute per API key **Workflow Engine**: - Concurrent executions: 8,000 workflows (across 16 pods)",
              "score": null
            }
          ],
          "latency_ms": 11.588635999942198,
          "query_id": "realistic_009",
          "dimension": "original",
          "query": "What is the P99 latency target for API operations?"
        },
        {
          "key_facts": [
            "P99 latency: < 200ms",
            "average P99 latency of 180ms for API operations"
          ],
          "found_facts": [],
          "missed_facts": [
            "P99 latency: < 200ms",
            "average P99 latency of 180ms for API operations"
          ],
          "coverage": 0.0,
          "expected_docs": [
            "architecture_overview"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_7",
              "content": "Waiting {retry_after} seconds...\") time.sleep(retry_after) continue remaining = int(response.headers.get('X-RateLimit-Remaining', 0)) if remaining < 10: print(f\"Warning: Only {remaining} requests remaining\") return response raise Exception(\"Max retries exceeded due to rate limiting\") ``` **Bash script with rate limit checking:** ```bash #!/bin/bash check_rate_limit() { local remaining=$(curl -s -I https://api.cloudflow.io/api/v1/workflows \\ -H \"Authorization: Bearer $CF_ACCESS_TOKEN\" | \\ grep -i \"x-ratelimit-remaining\" | \\ awk '{print $2}' | tr -d '\\r') if [ \"$remaining\" -lt 10 ]; then echo \"Warning: Only $remaining requests remaining\" local reset=$(curl -s -I https://api.cloudflow.io/api/v1/workflows \\ -H \"Authorization: Bearer $CF_ACCESS_TOKEN\" | \\ grep -i \"x-ratelimit-reset\" | \\ awk '{print $2}' | tr -d '\\r') local wait_time=$((reset - $(date +%s))) echo \"Rate limit resets in $wait_time seconds\" sleep $wait_time fi } # Use before API calls check_rate_limit cloudflow workflows execute wf_9k2n4m8p1q ``` #### Optimization Strategies **1. Implement request batching:** ```bash # Batch multiple workflow executions cloudflow workflows execute-batch \\ --workflow-ids \"wf_id1,wf_id2,wf_id3,wf_id4,wf_id5\" \\ --batch-size 5 # This counts as 1 API request instead of 5 ``` **2. Use webhooks instead of polling:** ```bash # Configure webhook for workflow completion cloudflow webhooks create \\ --event workflow.completed \\ --url https://your-service.com/webhooks/cloudflow \\ --secret $WEBHOOK_SECRET # Verify webhook cloudflow webhooks test webhook_abc123 ``` **3.",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_1",
              "content": "**Diagnosis:** ```bash # Check system time timedatectl status # Compare with NTP server ntpdate -q pool.ntp.org # Check JWT issued time vs current time jwt_iat=$(echo $CF_ACCESS_TOKEN | cut -d'.' -f2 | base64 -d | jq '.iat') current_time=$(date +%s) skew=$((current_time - jwt_iat)) echo \"Clock skew: $skew seconds\" ``` **Resolution:** ```bash # Sync with NTP server sudo ntpdate -s pool.ntp.org # Enable automatic time sync sudo timedatectl set-ntp true # Restart CloudFlow client cloudflow restart ``` **4. Insufficient Permissions** Error: `User does not have required permissions for this operation` **Check user roles:** ```bash cloudflow auth whoami --verbose # Expected output: # User: john.doe@company.com # Roles: developer, workflow-admin # Scopes: workflow:*, data:read, metrics:read ``` **Request permission elevation:** ```bash # Submit access request cloudflow auth request-access \\ --resource \"workflow:production:deploy\" \\ --justification \"Deploy critical hotfix for TICKET-1234\" # Check pending approvals cloudflow auth list-requests --status pending ``` ### 403 Forbidden Errors #### RBAC Policy Violations CloudFlow uses role-based access control (RBAC) with the following hierarchy: - `viewer` - Read-only access - `developer` - Create and modify workflows (non-production) - `operator` - Execute workflows, view logs - `admin` - Full access to workspace - `platform-admin` - Cross-workspace administration **Verify resource permissions:** ```bash # Check effective permissions cloudflow rbac check \\ --user john.doe@company.com \\ --resource workflow:prod-pipeline \\ --action execute # List all policies affecting user cloudflow rbac policies --user john.doe@company.com --verbose ``` --- ## Performance Problems ### Slow Query Performance #### Symptoms - API response times > 5000ms - Database query latency warnings in logs - CloudFlow UI becomes unresponsive - Timeout errors: `Request exceeded maximum duration of 30000ms` #### Diagnosis Steps **1. Identify Slow Queries** ```bash # Query CloudFlow metrics cloudflow metrics query --metric api_request_duration_ms \\ --filter \"p95 > 5000\" \\ --last 1h # Check database slow query log kubectl logs -n cloudflow deploy/cloudflow-db-primary | \\ grep \"slow query\" | \\ tail -n 50 # Analyze query patterns cloudflow db analyze-queries --min-duration 5000 --limit 20 ``` **2.",
              "score": null
            },
            {
              "doc_id": "user_guide",
              "chunk_id": "user_guide_fix_5",
              "content": "Save and activate **Testing Schedules:** Use the built-in schedule calculator to preview upcoming executions: ``` Next 5 executions: 1. 2026-01-24 14:00:00 UTC 2. 2026-01-25 14:00:00 UTC 3. 2026-01-26 14:00:00 UTC 4. 2026-01-27 14:00:00 UTC 5. 2026-01-28 14:00:00 UTC ``` ## Error Handling Robust error handling ensures your workflows are resilient and reliable. ### Retry Policies Configure automatic retries for failed actions: ```yaml - id: api_call action: http_request config: url: \"https://api.example.com/data\" retry: max_attempts: 3 backoff_type: \"exponential\" # or \"fixed\", \"linear\" initial_interval: 1000 # milliseconds max_interval: 30000 multiplier: 2.0 retry_on: - timeout - network_error - status: [500, 502, 503, 504] ``` **Backoff Strategies:** - **Fixed**: Wait the same amount of time between retries - Attempt 1: Wait 1s - Attempt 2: Wait 1s - Attempt 3: Wait 1s - **Linear**: Increase wait time by a fixed amount - Attempt 1: Wait 1s - Attempt 2: Wait 2s - Attempt 3: Wait 3s - **Exponential**: Double the wait time with each retry (recommended) - Attempt 1: Wait 1s - Attempt 2: Wait 2s - Attempt 3: Wait 4s **Retry Conditions:** Control which errors trigger retries: - `timeout`: Request timeout - `network_error`: Connection failures - `status`: Specific HTTP status codes - `error_code`: Application-specific error codes ### Fallback Actions Execute alternative actions when the primary action fails: ```yaml - id: primary_payment action: http_request config: url: \"https://primary-payment-gateway.com/charge\" method: POST body: amount: \"{{amount}}\" on_error: - id: fallback_payment action: http_request config: url: \"https://backup-payment-gateway.com/charge\" method: POST body: amount: \"{{amount}}\" - id: notify_admin action: email config: to: \"admin@company.com\" subject: \"Payment Gateway Failure\" body: \"Primary gateway failed, switched to backup\" ``` **Error Object:** Access error details in fallback actions: ``` {{error.message}} # Error message {{error.code}} # Error code {{error.step_id}} # Failed step ID {{error.timestamp}} # When the error occurred {{error.attempts}} # Number of retry attempts ``` ### Dead Letter Queue When all retries and fallbacks fail, CloudFlow can route failed executions to a Dead Letter Queue (DLQ) for manual review: **Enable DLQ:** ```yaml workflow: error_handling: dead_letter_queue: enabled: true retain_days: 30 ``` **DLQ Features:** - View failed executions in the dashboard - Inspect complete execution context and error details - Retry individual executions after fixing issues - Export failed executions for analysis - Set up alerts for DLQ threshold breaches **Accessing the DLQ:** 1. Navigate to **Workflows** > **[Your Workflow]** > **Dead Letter Queue** 2.",
              "score": null
            },
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_3",
              "content": "**Endpoint:** `GET /pipelines/{pipeline_id}/executions` **Query Parameters:** - `status` (optional): Filter by status (`running`, `completed`, `failed`) - `start_date` (optional): ISO 8601 timestamp - `end_date` (optional): ISO 8601 timestamp - `limit` (optional): Items per page (max 100) - `offset` (optional): Pagination offset **Example Request:** ```python import requests api_key = \"YOUR_API_KEY\" pipeline_id = \"pipe_4x9k2m\" url = f\"https://api.cloudflow.io/v2/pipelines/{pipeline_id}/executions\" headers = {\"X-API-Key\": api_key} params = { \"status\": \"completed\", \"start_date\": \"2026-01-20T00:00:00Z\", \"limit\": 50 } response = requests.get(url, headers=headers, params=params) executions = response.json() for execution in executions['data']: print(f\"Execution {execution['id']}: {execution['status']} - {execution['duration_ms']}ms\") ``` **Example Response (200 OK):** ```json { \"data\": [ { \"id\": \"exec_7m3k9x2p\", \"pipeline_id\": \"pipe_4x9k2m\", \"status\": \"completed\", \"started_at\": \"2026-01-23T15:00:00Z\", \"completed_at\": \"2026-01-23T15:03:42Z\", \"duration_ms\": 222000, \"records_processed\": 15420, \"records_failed\": 3, \"error_rate\": 0.02 } ], \"pagination\": { \"total\": 156, \"limit\": 50, \"offset\": 0, \"has_more\": true } } ``` ### Analytics #### Get Workflow Metrics Retrieve performance metrics and analytics for workflows. **Endpoint:** `GET /analytics/workflows/{workflow_id}` **Query Parameters:** - `period` (required): Time period (`1h`, `24h`, `7d`, `30d`) - `metrics` (optional): Comma-separated list of metrics **Available Metrics:** - `execution_count`: Total number of executions - `success_rate`: Percentage of successful executions - `avg_duration`: Average execution duration in milliseconds - `error_count`: Total number of errors - `throughput`: Records processed per hour **Example Request:** ```bash curl -H \"X-API-Key: YOUR_API_KEY\" \\ \"https://api.cloudflow.io/v2/analytics/workflows/wf_8x7k2m9p?period=7d&metrics=execution_count,success_rate,avg_duration\" ``` **Example Response (200 OK):** ```json { \"workflow_id\": \"wf_8x7k2m9p\", \"period\": \"7d\", \"start_date\": \"2026-01-17T10:30:00Z\", \"end_date\": \"2026-01-24T10:30:00Z\", \"metrics\": { \"execution_count\": 168, \"success_rate\": 97.6, \"avg_duration\": 3285, \"error_count\": 4, \"throughput\": 2847 }, \"timeseries\": [ { \"timestamp\": \"2026-01-17T00:00:00Z\", \"execution_count\": 24, \"success_rate\": 100.0 }, { \"timestamp\": \"2026-01-18T00:00:00Z\", \"execution_count\": 24, \"success_rate\": 95.8 } ] } ``` ## Error Handling CloudFlow uses standard HTTP status codes and returns detailed error information in JSON format.",
              "score": null
            },
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_4",
              "content": "### Error Response Format ```json { \"error\": { \"code\": \"invalid_parameter\", \"message\": \"The 'limit' parameter must be between 1 and 100\", \"field\": \"limit\", \"request_id\": \"req_8k3m9x2p\" } } ``` ### HTTP Status Codes | Status Code | Description | Common Causes | |------------|-------------|---------------| | 400 | Bad Request | Invalid parameters, malformed JSON, validation errors | | 401 | Unauthorized | Missing or invalid authentication credentials | | 403 | Forbidden | Insufficient permissions for requested resource | | 404 | Not Found | Resource does not exist or has been deleted | | 429 | Too Many Requests | Rate limit exceeded, retry after specified period | | 500 | Internal Server Error | Unexpected server error, contact support if persists | | 502 | Bad Gateway | Temporary service issue, retry with exponential backoff | | 503 | Service Unavailable | Scheduled maintenance or temporary outage | ### Error Codes CloudFlow returns specific error codes to help you identify and resolve issues: - `invalid_parameter`: One or more request parameters are invalid - `missing_required_field`: Required field is missing from request body - `authentication_failed`: Invalid API key or token - `insufficient_permissions`: User lacks required scope or permission - `resource_not_found`: Requested resource does not exist - `rate_limit_exceeded`: Too many requests, see rate limiting section - `workflow_execution_failed`: Workflow execution encountered an error - `invalid_json`: Request body contains malformed JSON - `duplicate_resource`: Resource with same identifier already exists - `quota_exceeded`: Account quota limit reached ### Error Handling Best Practices **Python Example:** ```python import requests import time def make_api_request(url, headers, max_retries=3): for attempt in range(max_retries): try: response = requests.get(url, headers=headers) if response.status_code == 200: return response.json() elif response.status_code == 429: # Rate limit exceeded retry_after = int(response.headers.get('X-RateLimit-Reset', 60)) print(f\"Rate limited. Waiting {retry_after} seconds...\") time.sleep(retry_after) continue elif response.status_code >= 500: # Server error - retry with exponential backoff wait_time = 2 ** attempt print(f\"Server error. Retrying in {wait_time} seconds...\") time.sleep(wait_time) continue else: # Client error - don't retry error_data = response.json() raise Exception(f\"API Error: {error_data['error']['message']}\") except requests.exceptions.RequestException as e: print(f\"Request failed: {e}\") if attempt < max_retries - 1: time.sleep(2 ** attempt) else: raise raise Exception(\"Max retries exceeded\") ``` ## Webhooks CloudFlow can send webhook notifications when specific events occur in your workflows.",
              "score": null
            }
          ],
          "latency_ms": 11.431123002694221,
          "query_id": "realistic_009",
          "dimension": "synonym",
          "query": "What's the 99th percentile response time target?"
        },
        {
          "key_facts": [
            "P99 latency: < 200ms",
            "average P99 latency of 180ms for API operations"
          ],
          "found_facts": [
            "average P99 latency of 180ms for API operations"
          ],
          "missed_facts": [
            "P99 latency: < 200ms"
          ],
          "coverage": 0.5,
          "expected_docs": [
            "architecture_overview"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_10",
              "content": "- Execution start rate: 500 per second - Completion rate: 450 per second (average 2-second execution time) **Database**: - Read throughput: 50,000 queries per second (across replicas) - Write throughput: 15,000 transactions per second - Connection capacity: 2,000 concurrent connections **Kafka**: - Message ingestion: 100,000 messages per second - Consumer throughput: 80,000 messages per second (aggregated) - End-to-end latency: < 100ms (P99) ### Resource Utilization **CPU Utilization** (Target: 60-70% average): - API Gateway: 55% average, 80% peak - Workflow Engine: 65% average, 85% peak - Auth Service: 40% average, 60% peak **Memory Utilization** (Target: < 80%): - API Gateway: 2.5GB average per pod (4GB allocated) - Workflow Engine: 6GB average per pod (8GB allocated) - Notification Service: 2.8GB average per pod (4GB allocated) **Network Throughput**: - Ingress: 2 Gbps average, 5 Gbps peak - Egress: 1.5 Gbps average, 4 Gbps peak - Internal (service mesh): 8 Gbps average ### Monitoring & Alerting **Key Metrics** (Monitored via Prometheus): - Request rate (per service, per endpoint) - Error rate (4xx, 5xx responses) - Latency percentiles (P50, P95, P99) - Resource utilization (CPU, memory, disk) - Cache hit/miss ratios - Database connection pool saturation - Kafka consumer lag **Alerts** (via PagerDuty): - P1 (Immediate): API error rate > 1%, database connection failure - P2 (< 15 min): P99 latency > 500ms, cache hit rate < 80% - P3 (< 1 hour): Resource utilization > 85%, replica count < minimum **SLI/SLO/SLA**: - SLI: API success rate (non-5xx responses) - SLO: 99.9% uptime per month (43 minutes downtime allowance) - SLA: 99.5% uptime guarantee (customer-facing SLA with credits) --- ## Disaster Recovery ### Recovery Objectives **RPO (Recovery Point Objective): 1 hour** - Maximum acceptable data loss: 1 hour of transactions - Achieved through: Continuous database replication + hourly snapshots - Kafka retention: 7 days allows event replay **RTO (Recovery Time Objective): 4 hours** - Maximum acceptable downtime: 4 hours for full system recovery - Includes: Failover, data verification, and service restoration - Automated runbooks reduce RTO to < 2 hours for common scenarios ### High Availability Architecture **Multi-AZ Deployment**: ``` Region: us-east-1 AZ-1a: AZ-1b: AZ-1c: - API Gateway (4) - API Gateway (4) - API Gateway (4) - Workflow Engine (6) - Workflow Engine (5) - Workflow Engine (5) - Auth Service (3) - Auth Service (3) - Auth",
              "score": null
            },
            {
              "doc_id": "user_guide",
              "chunk_id": "user_guide_fix_2",
              "content": "### HTTP Requests Make HTTP requests to any API endpoint: **Configuration:** - **Method**: GET, POST, PUT, PATCH, DELETE, HEAD - **URL**: Full endpoint URL (supports variable interpolation) - **Headers**: Custom headers as key-value pairs - **Query Parameters**: URL parameters - **Body**: JSON, form data, or raw text - **Authentication**: Basic Auth, Bearer Token, API Key, OAuth 2.0 **Example:** ```yaml - id: fetch_user action: http_request config: method: GET url: \"https://api.example.com/users/{{user_id}}\" headers: Authorization: \"Bearer {{secrets.API_TOKEN}}\" Content-Type: \"application/json\" timeout: 30 ``` **Response Handling:** Access the response in subsequent steps: - `{{steps.fetch_user.status}}` - HTTP status code - `{{steps.fetch_user.body}}` - Response body - `{{steps.fetch_user.headers}}` - Response headers ### Database Queries Execute queries against supported databases (PostgreSQL, MySQL, MongoDB, Redis): **SQL Databases (PostgreSQL, MySQL):** ```yaml - id: get_orders action: database_query config: connection: \"{{secrets.DB_CONNECTION_STRING}}\" query: | SELECT * FROM orders WHERE customer_id = $1 AND status = $2 ORDER BY created_at DESC LIMIT 10 parameters: - \"{{trigger.customer_id}}\" - \"pending\" ``` **MongoDB:** ```yaml - id: find_documents action: mongodb_query config: connection: \"{{secrets.MONGO_URI}}\" database: \"production\" collection: \"users\" operation: find filter: email: \"{{trigger.email}}\" options: limit: 1 ``` **Important Notes:** - Always use parameterized queries to prevent SQL injection - Connection strings should be stored in secrets - Query timeout is 30 seconds by default - Maximum result set size is 10MB ### Email Notifications Send emails via SMTP or integrated email providers: **Configuration:** ```yaml - id: send_notification action: email config: provider: \"smtp\" # or \"sendgrid\", \"mailgun\", \"ses\" from: \"notifications@cloudflow.io\" to: \"{{trigger.recipient}}\" cc: \"manager@company.com\" subject: \"Alert: {{alert_type}}\" body: | Hello {{user.name}}, This is an automated notification about {{event_description}}. Details: - Time: {{timestamp}} - Type: {{alert_type}} - Priority: {{priority}} Best regards, CloudFlow Automation html: true attachments: - name: \"report.pdf\" content: \"{{steps.generate_report.output}}\" encoding: \"base64\" ``` **Email Templates:** CloudFlow supports dynamic templates with conditional content: ``` {{#if priority == 'high'}} \u26a0\ufe0f URGENT: Immediate attention required {{else}} \u2139\ufe0f Informational notification {{/if}} ``` ### Slack Messages Send messages to Slack channels or users: **Channel Messages:** ```yaml - id: notify_team action: slack_message config: channel: \"#deployments\" text: \"Deployment completed successfully!\" blocks: - type: \"section\" text: type: \"mrkdwn\" text: \"*Deployment Summary*\\nEnvironment: {{environment}}\\nVersion: {{version}}\\nDuration: {{duration}}s\" - type: \"actions\" elements: - type: \"button\" text: \"View Logs\" url: \"{{logs_url}}\" thread_ts: \"{{trigger.thread_ts}}\" # Reply to thread ``` **Direct Messages:** ```yaml - id: dm_user action: slack_message config: user: \"{{user_slack_id}}\" text: \"Your report is ready for review.\"",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_7",
              "content": "records: 500 - Session timeout: 30 seconds --- ## Message Queue Patterns ### Pub/Sub Pattern Used for broadcasting events to multiple interested consumers: ``` Workflow Engine publishes workflow.execution.completed \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u25bc \u25bc \u25bc \u25bc Analytics Notification Audit Logger Billing Service Service Service Service ``` Each service maintains its own consumer group and processes events independently. Failures in one consumer don't affect others. ### Request-Reply Pattern Synchronous communication over async messaging (used sparingly): ``` API Gateway publishes request \u2192 Kafka (reply-to: temp-queue-123) \u2502 \u25bc Service processes request \u2502 \u25bc Service publishes response \u2192 temp-queue-123 \u2502 \u25bc API Gateway receives response ``` Timeout: 5 seconds, fallback to direct HTTP call if no response. ### Saga Pattern Distributed transaction management for multi-service workflows: ``` Step 1: Create Order \u2192 SUCCESS \u2192 Step 2: Reserve Inventory \u2502 FAILURE \u2502 \u25bc Compensating Transaction \u2502 \u25bc Cancel Order (Rollback) ``` Orchestrated by Workflow Engine with compensation logic defined in workflow DSL. ### Dead Letter Queue (DLQ) Failed messages after all retry attempts are routed to DLQ: ``` Message processing fails (3 retries with exponential backoff) \u2502 \u25bc Publish to dead-letter-queue topic \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u25bc \u25bc Alert to PagerDuty Store in PostgreSQL for analysis \u2502 \u2502 \u25bc \u25bc Manual investigation Automated pattern detection ``` DLQ Processing SLA: < 4 hours for critical events, < 24 hours for non-critical. --- ## Caching Strategy ### Cache-Aside Pattern (Lazy Loading) Primary caching pattern used across all services: ``` 1. Application checks cache (Redis GET) 2. If HIT \u2192 Return cached data 3. If MISS: a. Query database (PostgreSQL) b. Store result in cache with TTL c. Return data to application ``` **Implementation Example** (Workflow Definition): ```javascript async function getWorkflowDefinition(workflowId) { const cacheKey = `workflow:def:${workflowId}`; // Step 1: Check cache let workflow = await redis.get(cacheKey); if (workflow) { metrics.increment('cache.hit.workflow_def'); return JSON.parse(workflow); } // Step 2: Cache miss - query database metrics.increment('cache.miss.workflow_def'); workflow = await db.query( 'SELECT * FROM workflows WHERE id = $1', [workflowId] ); // Step 3: Store in cache (1 hour TTL) await redis.setex(cacheKey, 3600, JSON.stringify(workflow)); return workflow; } ``` ### Write-Through Pattern Used for critical data where cache consistency is paramount: ``` 1. Application writes to database 2. Database transaction commits 3. Application updates cache 4. Return success to client ``` Applied to: User profiles, authentication sessions, system configuration.",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_0",
              "content": "# CloudFlow Platform - System Architecture Overview **Document Version:** 2.3.1 **Last Updated:** January 15, 2026 **Owner:** Platform Architecture Team **Status:** Production ## Executive Summary CloudFlow is a distributed, cloud-native workflow automation platform designed to orchestrate complex business processes at scale. The platform processes over 2.5 million workflow executions daily with an average P99 latency of 180ms for API operations and 4.2 seconds for workflow execution. This document provides a comprehensive overview of the system architecture, including microservices design, data flow patterns, infrastructure decisions, and operational considerations. ## Table of Contents 1. [High-Level Architecture](#high-level-architecture) 2. [Microservices Breakdown](#microservices-breakdown) 3. [Data Flow Architecture](#data-flow-architecture) 4. [Database Architecture](#database-architecture) 5. [Message Queue Patterns](#message-queue-patterns) 6. [Caching Strategy](#caching-strategy) 7. [Security Architecture](#security-architecture) 8. [Performance Characteristics](#performance-characteristics) 9. [Disaster Recovery](#disaster-recovery) --- ## High-Level Architecture CloudFlow follows a microservices architecture pattern deployed across multiple availability zones in AWS. The platform is designed with the following core principles: - **Scalability**: Horizontal scaling for all services with auto-scaling groups - **Resilience**: Circuit breakers, bulkheads, and graceful degradation - **Observability**: Distributed tracing, centralized logging, and comprehensive metrics - **Security**: Zero-trust network architecture with mTLS encryption ### Architecture Diagram (Conceptual) ``` \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 Load Balancer (ALB) \u2502 \u2502 (TLS Termination - 443) \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u25bc \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 API Gateway Layer \u2502 \u2502 (Rate Limiting, Auth, Request Routing) \u2502 \u2514\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u2502 \u2502 \u2502 \u2502 \u25bc \u25bc \u25bc \u25bc \u25bc \u250c\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502Auth\u2502 \u2502Workflow\u2502 \u2502Scheduler\u2502 \u2502Notification\u2502 \u2502User Service\u2502 \u2502Svc \u2502 \u2502 Engine \u2502 \u2502 Service \u2502 \u2502 Service \u2502 \u2502 \u2502 \u2514\u2500\u252c\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u2502 \u2502 \u2502 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 \u2502 \u2502 \u25bc \u25bc \u25bc \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502PostgreSQL\u2502 \u2502 Redis \u2502 \u2502 Kafka \u2502 \u2502 Cluster \u2502 \u2502 Cluster \u2502 \u2502 Cluster \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 ``` ### Technology Stack - **Runtime**: Node.js 20.x (API Gateway, Workflow Engine), Go 1.21 (Auth Service, Scheduler) - **Container Orchestration**: Kubernetes 1.28 (EKS) - **Service Mesh**: Istio 1.20 for service-to-service communication - **Databases**: PostgreSQL 15.4, Redis 7.2 - **Message Broker**: Apache Kafka 3.6 - **Monitoring**: Prometheus, Grafana, Jaeger for distributed tracing - **Secrets Management**: HashiCorp Vault 1.15 --- ## Microservices Breakdown ### API Gateway **Purpose**: Single entry point for all client requests, providing authentication, rate limiting, request routing, and protocol translation.",
              "score": null
            },
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_4",
              "content": "### Error Response Format ```json { \"error\": { \"code\": \"invalid_parameter\", \"message\": \"The 'limit' parameter must be between 1 and 100\", \"field\": \"limit\", \"request_id\": \"req_8k3m9x2p\" } } ``` ### HTTP Status Codes | Status Code | Description | Common Causes | |------------|-------------|---------------| | 400 | Bad Request | Invalid parameters, malformed JSON, validation errors | | 401 | Unauthorized | Missing or invalid authentication credentials | | 403 | Forbidden | Insufficient permissions for requested resource | | 404 | Not Found | Resource does not exist or has been deleted | | 429 | Too Many Requests | Rate limit exceeded, retry after specified period | | 500 | Internal Server Error | Unexpected server error, contact support if persists | | 502 | Bad Gateway | Temporary service issue, retry with exponential backoff | | 503 | Service Unavailable | Scheduled maintenance or temporary outage | ### Error Codes CloudFlow returns specific error codes to help you identify and resolve issues: - `invalid_parameter`: One or more request parameters are invalid - `missing_required_field`: Required field is missing from request body - `authentication_failed`: Invalid API key or token - `insufficient_permissions`: User lacks required scope or permission - `resource_not_found`: Requested resource does not exist - `rate_limit_exceeded`: Too many requests, see rate limiting section - `workflow_execution_failed`: Workflow execution encountered an error - `invalid_json`: Request body contains malformed JSON - `duplicate_resource`: Resource with same identifier already exists - `quota_exceeded`: Account quota limit reached ### Error Handling Best Practices **Python Example:** ```python import requests import time def make_api_request(url, headers, max_retries=3): for attempt in range(max_retries): try: response = requests.get(url, headers=headers) if response.status_code == 200: return response.json() elif response.status_code == 429: # Rate limit exceeded retry_after = int(response.headers.get('X-RateLimit-Reset', 60)) print(f\"Rate limited. Waiting {retry_after} seconds...\") time.sleep(retry_after) continue elif response.status_code >= 500: # Server error - retry with exponential backoff wait_time = 2 ** attempt print(f\"Server error. Retrying in {wait_time} seconds...\") time.sleep(wait_time) continue else: # Client error - don't retry error_data = response.json() raise Exception(f\"API Error: {error_data['error']['message']}\") except requests.exceptions.RequestException as e: print(f\"Request failed: {e}\") if attempt < max_retries - 1: time.sleep(2 ** attempt) else: raise raise Exception(\"Max retries exceeded\") ``` ## Webhooks CloudFlow can send webhook notifications when specific events occur in your workflows.",
              "score": null
            }
          ],
          "latency_ms": 11.460588000772987,
          "query_id": "realistic_009",
          "dimension": "problem",
          "query": "Our API latency is 500ms, is that acceptable?"
        },
        {
          "key_facts": [
            "P99 latency: < 200ms",
            "average P99 latency of 180ms for API operations"
          ],
          "found_facts": [],
          "missed_facts": [
            "P99 latency: < 200ms",
            "average P99 latency of 180ms for API operations"
          ],
          "coverage": 0.0,
          "expected_docs": [
            "architecture_overview"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_10",
              "content": "- Execution start rate: 500 per second - Completion rate: 450 per second (average 2-second execution time) **Database**: - Read throughput: 50,000 queries per second (across replicas) - Write throughput: 15,000 transactions per second - Connection capacity: 2,000 concurrent connections **Kafka**: - Message ingestion: 100,000 messages per second - Consumer throughput: 80,000 messages per second (aggregated) - End-to-end latency: < 100ms (P99) ### Resource Utilization **CPU Utilization** (Target: 60-70% average): - API Gateway: 55% average, 80% peak - Workflow Engine: 65% average, 85% peak - Auth Service: 40% average, 60% peak **Memory Utilization** (Target: < 80%): - API Gateway: 2.5GB average per pod (4GB allocated) - Workflow Engine: 6GB average per pod (8GB allocated) - Notification Service: 2.8GB average per pod (4GB allocated) **Network Throughput**: - Ingress: 2 Gbps average, 5 Gbps peak - Egress: 1.5 Gbps average, 4 Gbps peak - Internal (service mesh): 8 Gbps average ### Monitoring & Alerting **Key Metrics** (Monitored via Prometheus): - Request rate (per service, per endpoint) - Error rate (4xx, 5xx responses) - Latency percentiles (P50, P95, P99) - Resource utilization (CPU, memory, disk) - Cache hit/miss ratios - Database connection pool saturation - Kafka consumer lag **Alerts** (via PagerDuty): - P1 (Immediate): API error rate > 1%, database connection failure - P2 (< 15 min): P99 latency > 500ms, cache hit rate < 80% - P3 (< 1 hour): Resource utilization > 85%, replica count < minimum **SLI/SLO/SLA**: - SLI: API success rate (non-5xx responses) - SLO: 99.9% uptime per month (43 minutes downtime allowance) - SLA: 99.5% uptime guarantee (customer-facing SLA with credits) --- ## Disaster Recovery ### Recovery Objectives **RPO (Recovery Point Objective): 1 hour** - Maximum acceptable data loss: 1 hour of transactions - Achieved through: Continuous database replication + hourly snapshots - Kafka retention: 7 days allows event replay **RTO (Recovery Time Objective): 4 hours** - Maximum acceptable downtime: 4 hours for full system recovery - Includes: Failover, data verification, and service restoration - Automated runbooks reduce RTO to < 2 hours for common scenarios ### High Availability Architecture **Multi-AZ Deployment**: ``` Region: us-east-1 AZ-1a: AZ-1b: AZ-1c: - API Gateway (4) - API Gateway (4) - API Gateway (4) - Workflow Engine (6) - Workflow Engine (5) - Workflow Engine (5) - Auth Service (3) - Auth Service (3) - Auth",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_4",
              "content": "**Technology**: Node.js with worker pool pattern **Replicas**: 8 pods (production), auto-scaling 6-16 **Resource Allocation**: 2 vCPU, 4GB RAM per pod **Key Responsibilities**: - Consume notification events from Kafka - Template rendering with Handlebars (cached in Redis) - Multi-channel delivery (Email via SendGrid, SMS via Twilio, Webhooks) - Retry logic with dead letter queue for failed deliveries - Delivery status tracking and analytics - User preference management (opt-in/opt-out) **Notification Flow**: ``` Event Source \u2192 Kafka (notifications.email topic) \u2502 \u25bc Notification Service Consumer \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u25bc \u25bc \u25bc [Email Queue] [SMS Queue] [Webhook Queue] \u2502 \u2502 \u2502 \u25bc \u25bc \u25bc [SendGrid] [Twilio] [HTTP POST] \u2502 \u2502 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u25bc Delivery Status \u2192 PostgreSQL ``` **Channel Configuration**: - Email: SendGrid with IP warming, DKIM/SPF configured - SMS: Twilio with fallback to AWS SNS - Webhook: Signed payloads (HMAC-SHA256), retry 3 times with exponential backoff - In-app: WebSocket connections via Socket.io **Template Management**: - Templates stored in PostgreSQL with versioning - Compiled templates cached in Redis (1-hour TTL) - Support for localization (i18n) with 12 languages - A/B testing capability for email campaigns **Performance Targets**: - Email delivery: < 5 seconds (P99) from event to SendGrid - SMS delivery: < 3 seconds (P99) - Throughput: 10,000 notifications per minute --- ## Data Flow Architecture ### Synchronous Request Flow Client requests follow a synchronous path through the API Gateway to backend services: ``` 1. Client \u2192 Load Balancer (TLS termination) 2. Load Balancer \u2192 API Gateway (HTTP/2) 3. API Gateway \u2192 Auth Service (JWT validation via gRPC) 4. API Gateway \u2192 Target Service (HTTP/REST or gRPC) 5. Target Service \u2192 Database/Cache (data retrieval) 6. Response propagates back through the chain ``` **Timeout Configuration**: - Client \u2192 Load Balancer: 60 seconds - Load Balancer \u2192 API Gateway: 55 seconds - API Gateway \u2192 Services: 45 seconds - Service \u2192 Database: 10 seconds ### Asynchronous Event Flow Long-running operations and inter-service communication use event-driven patterns: ``` 1. Service publishes event \u2192 Kafka topic 2. Kafka persists event (replication factor: 3) 3. Consumer groups subscribe to topics 4. Consumers process events with at-least-once delivery 5. State updates written to PostgreSQL 6.",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_2",
              "content": "Review Query Execution Plans** ```sql -- Connect to CloudFlow database cloudflow db connect --readonly -- Explain slow query EXPLAIN ANALYZE SELECT w.*, e.status, e.error_message FROM workflows w LEFT JOIN executions e ON w.id = e.workflow_id WHERE w.workspace_id = 'ws_abc123' AND e.created_at > NOW() - INTERVAL '7 days' ORDER BY e.created_at DESC; -- Check for missing indexes SELECT schemaname, tablename, indexname, indexdef FROM pg_indexes WHERE tablename IN ('workflows', 'executions', 'workflow_steps') ORDER BY tablename, indexname; ``` **3. Optimize Queries** Common optimization techniques: **Add missing indexes:** ```sql -- Index for workflow lookup by workspace CREATE INDEX CONCURRENTLY idx_workflows_workspace_created ON workflows(workspace_id, created_at DESC); -- Index for execution status queries CREATE INDEX CONCURRENTLY idx_executions_status_created ON executions(workflow_id, status, created_at DESC) WHERE status IN ('running', 'pending'); -- Composite index for common filter combinations CREATE INDEX CONCURRENTLY idx_executions_workspace_status ON executions(workspace_id, status, created_at) INCLUDE (error_message, retry_count); ``` **Use query result caching:** ```bash # Enable query cache for workspace metadata cloudflow config set cache.workspace.ttl 3600 # Configure Redis cache backend cloudflow config set cache.backend redis cloudflow config set cache.redis.host redis.cloudflow.svc.cluster.local cloudflow config set cache.redis.port 6379 ``` ### High API Latency #### Latency Breakdown Analysis ```bash # Generate latency report cloudflow metrics latency-report --endpoint \"/api/v1/workflows\" --last 24h # Sample output: # Endpoint: POST /api/v1/workflows/execute # P50: 245ms | P95: 1823ms | P99: 4521ms # Breakdown: # - Auth: 45ms (18%) # - DB Query: 156ms (64%) # - Business Logic: 32ms (13%) # - Response Serialization: 12ms (5%) ``` **Network latency issues:** ```bash # Test connectivity to CloudFlow API time curl -w \"@curl-format.txt\" -o /dev/null -s https://api.cloudflow.io/health # Create curl-format.txt: cat > curl-format.txt << EOF time_namelookup: %{time_namelookup}s\\n time_connect: %{time_connect}s\\n time_appconnect: %{time_appconnect}s\\n time_pretransfer: %{time_pretransfer}s\\n time_redirect: %{time_redirect}s\\n time_starttransfer: %{time_starttransfer}s\\n ----------\\n time_total: %{time_total}s\\n EOF # Trace route to API endpoint traceroute api.cloudflow.io # Check DNS resolution time dig api.cloudflow.io | grep \"Query time\" ``` ### Memory Leaks #### Detection ```bash # Monitor CloudFlow service memory usage kubectl top pods -n cloudflow --sort-by=memory # Get detailed memory metrics for specific pod kubectl exec -n cloudflow deploy/cloudflow-api -- \\ curl localhost:9090/metrics | grep memory # Check for OOMKilled pods kubectl get pods -n cloudflow --field-selector=status.phase=Failed | \\ grep OOMKilled # Review memory limits and requests kubectl describe deployment cloudflow-api -n cloudflow | \\ grep -A 5 \"Limits\\|Requests\" ``` #### Common Causes **1.",
              "score": null
            },
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_8",
              "content": "UTC - **Secondary**: Wednesday 10:00-12:00 UTC All deployments should target these windows to minimize user impact. --- **Document Version**: 2.4.0 **Maintained by**: Platform Engineering Team **Last Review**: January 24, 2026",
              "score": null
            },
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_1",
              "content": "## Rate Limiting To ensure fair usage and system stability, CloudFlow enforces rate limits on all API endpoints. **Default Limits:** - 100 requests per minute per authenticated user - 20 requests per minute for unauthenticated requests - Burst allowance: 150 requests in a 10-second window ### Rate Limit Headers Every API response includes rate limit information: ``` X-RateLimit-Limit: 100 X-RateLimit-Remaining: 87 X-RateLimit-Reset: 1640995200 ``` When you exceed the rate limit, you'll receive a `429 Too Many Requests` response: ```json { \"error\": { \"code\": \"rate_limit_exceeded\", \"message\": \"Rate limit exceeded. Please retry after 42 seconds.\", \"retry_after\": 42 } } ``` **Best Practices:** - Monitor `X-RateLimit-Remaining` header values - Implement exponential backoff when receiving 429 responses - Cache responses when appropriate to reduce API calls - Consider upgrading to Enterprise tier for higher limits (1000 req/min) ## Pagination List endpoints return paginated results to optimize performance. Use `limit` and `offset` parameters to navigate through result sets. **Parameters:** - `limit`: Number of items per page (default: 20, max: 100) - `offset`: Number of items to skip (default: 0) **Example Request:** ```bash curl -H \"X-API-Key: YOUR_API_KEY\" \\ \"https://api.cloudflow.io/v2/workflows?limit=50&offset=100\" ``` **Response Structure:** ```json { \"data\": [...], \"pagination\": { \"total\": 347, \"limit\": 50, \"offset\": 100, \"has_more\": true } } ``` **Python Example:** ```python import requests def fetch_all_workflows(api_key): base_url = \"https://api.cloudflow.io/v2/workflows\" headers = {\"X-API-Key\": api_key} all_workflows = [] offset = 0 limit = 100 while True: response = requests.get( base_url, headers=headers, params={\"limit\": limit, \"offset\": offset} ) data = response.json() all_workflows.extend(data['data']) if not data['pagination']['has_more']: break offset += limit return all_workflows ``` ## Endpoints ### Workflows #### List Workflows Retrieve a paginated list of all workflows in your account. **Endpoint:** `GET /workflows` **Query Parameters:** - `status` (optional): Filter by status (`active`, `paused`, `archived`) - `created_after` (optional): ISO 8601 timestamp - `limit` (optional): Items per page (max 100) - `offset` (optional): Pagination offset **Example Request:** ```bash curl -H \"X-API-Key: YOUR_API_KEY\" \\ \"https://api.cloudflow.io/v2/workflows?status=active&limit=25\" ``` **Example Response (200 OK):** ```json { \"data\": [ { \"id\": \"wf_8x7k2m9p\", \"name\": \"Data Processing Pipeline\", \"description\": \"Processes customer data every hour\", \"status\": \"active\", \"trigger\": { \"type\": \"schedule\", \"cron\": \"0 * * * *\" }, \"steps\": 12, \"created_at\": \"2026-01-15T10:30:00Z\", \"updated_at\": \"2026-01-23T14:22:00Z\", \"last_run\": \"2026-01-24T09:00:00Z\" } ], \"pagination\": { \"total\": 1, \"limit\": 25, \"offset\": 0, \"has_more\": false } } ``` #### Create Workflow Create a new workflow with specified configuration.",
              "score": null
            }
          ],
          "latency_ms": 11.244686000281945,
          "query_id": "realistic_009",
          "dimension": "casual",
          "query": "api latency target"
        },
        {
          "key_facts": [
            "P99 latency: < 200ms",
            "average P99 latency of 180ms for API operations"
          ],
          "found_facts": [
            "average P99 latency of 180ms for API operations"
          ],
          "missed_facts": [
            "P99 latency: < 200ms"
          ],
          "coverage": 0.5,
          "expected_docs": [
            "architecture_overview"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_0",
              "content": "# CloudFlow Platform - System Architecture Overview **Document Version:** 2.3.1 **Last Updated:** January 15, 2026 **Owner:** Platform Architecture Team **Status:** Production ## Executive Summary CloudFlow is a distributed, cloud-native workflow automation platform designed to orchestrate complex business processes at scale. The platform processes over 2.5 million workflow executions daily with an average P99 latency of 180ms for API operations and 4.2 seconds for workflow execution. This document provides a comprehensive overview of the system architecture, including microservices design, data flow patterns, infrastructure decisions, and operational considerations. ## Table of Contents 1. [High-Level Architecture](#high-level-architecture) 2. [Microservices Breakdown](#microservices-breakdown) 3. [Data Flow Architecture](#data-flow-architecture) 4. [Database Architecture](#database-architecture) 5. [Message Queue Patterns](#message-queue-patterns) 6. [Caching Strategy](#caching-strategy) 7. [Security Architecture](#security-architecture) 8. [Performance Characteristics](#performance-characteristics) 9. [Disaster Recovery](#disaster-recovery) --- ## High-Level Architecture CloudFlow follows a microservices architecture pattern deployed across multiple availability zones in AWS. The platform is designed with the following core principles: - **Scalability**: Horizontal scaling for all services with auto-scaling groups - **Resilience**: Circuit breakers, bulkheads, and graceful degradation - **Observability**: Distributed tracing, centralized logging, and comprehensive metrics - **Security**: Zero-trust network architecture with mTLS encryption ### Architecture Diagram (Conceptual) ``` \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 Load Balancer (ALB) \u2502 \u2502 (TLS Termination - 443) \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u25bc \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 API Gateway Layer \u2502 \u2502 (Rate Limiting, Auth, Request Routing) \u2502 \u2514\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u2502 \u2502 \u2502 \u2502 \u25bc \u25bc \u25bc \u25bc \u25bc \u250c\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502Auth\u2502 \u2502Workflow\u2502 \u2502Scheduler\u2502 \u2502Notification\u2502 \u2502User Service\u2502 \u2502Svc \u2502 \u2502 Engine \u2502 \u2502 Service \u2502 \u2502 Service \u2502 \u2502 \u2502 \u2514\u2500\u252c\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u2502 \u2502 \u2502 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 \u2502 \u2502 \u25bc \u25bc \u25bc \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502PostgreSQL\u2502 \u2502 Redis \u2502 \u2502 Kafka \u2502 \u2502 Cluster \u2502 \u2502 Cluster \u2502 \u2502 Cluster \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 ``` ### Technology Stack - **Runtime**: Node.js 20.x (API Gateway, Workflow Engine), Go 1.21 (Auth Service, Scheduler) - **Container Orchestration**: Kubernetes 1.28 (EKS) - **Service Mesh**: Istio 1.20 for service-to-service communication - **Databases**: PostgreSQL 15.4, Redis 7.2 - **Message Broker**: Apache Kafka 3.6 - **Monitoring**: Prometheus, Grafana, Jaeger for distributed tracing - **Secrets Management**: HashiCorp Vault 1.15 --- ## Microservices Breakdown ### API Gateway **Purpose**: Single entry point for all client requests, providing authentication, rate limiting, request routing, and protocol translation.",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_2",
              "content": "Review Query Execution Plans** ```sql -- Connect to CloudFlow database cloudflow db connect --readonly -- Explain slow query EXPLAIN ANALYZE SELECT w.*, e.status, e.error_message FROM workflows w LEFT JOIN executions e ON w.id = e.workflow_id WHERE w.workspace_id = 'ws_abc123' AND e.created_at > NOW() - INTERVAL '7 days' ORDER BY e.created_at DESC; -- Check for missing indexes SELECT schemaname, tablename, indexname, indexdef FROM pg_indexes WHERE tablename IN ('workflows', 'executions', 'workflow_steps') ORDER BY tablename, indexname; ``` **3. Optimize Queries** Common optimization techniques: **Add missing indexes:** ```sql -- Index for workflow lookup by workspace CREATE INDEX CONCURRENTLY idx_workflows_workspace_created ON workflows(workspace_id, created_at DESC); -- Index for execution status queries CREATE INDEX CONCURRENTLY idx_executions_status_created ON executions(workflow_id, status, created_at DESC) WHERE status IN ('running', 'pending'); -- Composite index for common filter combinations CREATE INDEX CONCURRENTLY idx_executions_workspace_status ON executions(workspace_id, status, created_at) INCLUDE (error_message, retry_count); ``` **Use query result caching:** ```bash # Enable query cache for workspace metadata cloudflow config set cache.workspace.ttl 3600 # Configure Redis cache backend cloudflow config set cache.backend redis cloudflow config set cache.redis.host redis.cloudflow.svc.cluster.local cloudflow config set cache.redis.port 6379 ``` ### High API Latency #### Latency Breakdown Analysis ```bash # Generate latency report cloudflow metrics latency-report --endpoint \"/api/v1/workflows\" --last 24h # Sample output: # Endpoint: POST /api/v1/workflows/execute # P50: 245ms | P95: 1823ms | P99: 4521ms # Breakdown: # - Auth: 45ms (18%) # - DB Query: 156ms (64%) # - Business Logic: 32ms (13%) # - Response Serialization: 12ms (5%) ``` **Network latency issues:** ```bash # Test connectivity to CloudFlow API time curl -w \"@curl-format.txt\" -o /dev/null -s https://api.cloudflow.io/health # Create curl-format.txt: cat > curl-format.txt << EOF time_namelookup: %{time_namelookup}s\\n time_connect: %{time_connect}s\\n time_appconnect: %{time_appconnect}s\\n time_pretransfer: %{time_pretransfer}s\\n time_redirect: %{time_redirect}s\\n time_starttransfer: %{time_starttransfer}s\\n ----------\\n time_total: %{time_total}s\\n EOF # Trace route to API endpoint traceroute api.cloudflow.io # Check DNS resolution time dig api.cloudflow.io | grep \"Query time\" ``` ### Memory Leaks #### Detection ```bash # Monitor CloudFlow service memory usage kubectl top pods -n cloudflow --sort-by=memory # Get detailed memory metrics for specific pod kubectl exec -n cloudflow deploy/cloudflow-api -- \\ curl localhost:9090/metrics | grep memory # Check for OOMKilled pods kubectl get pods -n cloudflow --field-selector=status.phase=Failed | \\ grep OOMKilled # Review memory limits and requests kubectl describe deployment cloudflow-api -n cloudflow | \\ grep -A 5 \"Limits\\|Requests\" ``` #### Common Causes **1.",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_10",
              "content": "database nc -zv cloudflow-db.internal.company.com 5432 # Trace route traceroute api.cloudflow.io # Capture packets tcpdump -i any -w /tmp/capture.pcap port 5432 ``` --- ## Escalation Procedures ### Severity Levels CloudFlow incidents are classified into four severity levels: #### SEV-1: Critical (P1) - **Definition:** Complete service outage or severe degradation affecting all users - **Examples:** - API returns 5xx errors for > 5 minutes - Database completely unavailable - Authentication system down - Data loss or corruption - **Response Time:** Immediate (< 15 minutes) - **Escalation:** Page on-call engineer immediately #### SEV-2: High (P2) - **Definition:** Major functionality impaired affecting multiple users - **Examples:** - Workflow execution success rate < 90% - Significant performance degradation (p95 latency > 10s) - Rate limiting affecting large customer segment - **Response Time:** < 1 hour - **Escalation:** Create incident ticket and notify on-call #### SEV-3: Medium (P3) - **Definition:** Partial functionality degraded affecting some users - **Examples:** - Intermittent failures for specific workflow types - Minor performance issues - Non-critical feature unavailable - **Response Time:** < 4 hours - **Escalation:** Create ticket, normal business hours support #### SEV-4: Low (P4) - **Definition:** Minor issues with minimal user impact - **Examples:** - Cosmetic issues - Documentation errors - Feature requests - **Response Time:** < 2 business days - **Escalation:** Standard support ticket ### Escalation Steps #### Step 1: Initial Assessment (0-5 minutes) ```bash # Run health check cloudflow health check --comprehensive # Check status page curl https://status.cloudflow.io/api/v1/status.json # Review recent changes cloudflow audit log --last 2h --event-type \"deployment,configuration\" # Check metrics dashboard cloudflow metrics dashboard --incident-mode ``` #### Step 2: Gather Information (5-15 minutes) Create incident document with: - Incident timestamp and duration - Affected services and endpoints - Error rates and user impact - Recent changes or deployments - Relevant log excerpts - Correlation IDs for failed requests ```bash # Generate incident report cloudflow debug incident-report \\ --start \"2026-01-24T10:30:00Z\" \\ --end \"2026-01-24T11:00:00Z\" \\ --output incident-report.md # Capture system snapshot cloudflow debug snapshot --output snapshot-$(date +%Y%m%d-%H%M%S).tar.gz ``` #### Step 3: Escalate Based on Severity **For SEV-1 (Critical):** ```bash # Page on-call engineer cloudflow incident create \\ --severity SEV-1 \\ --title \"Complete API outage\" \\ --description \"All API requests returning 503\" \\ --page-oncall # Notify status page cloudflow status update \\ --status \"major_outage\" \\ --message \"We are investigating a complete service",
              "score": null
            },
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_6",
              "content": "**Verify Service Health**: ```bash kubectl get pods -n cloudflow-prod curl https://api.cloudflow.io/health ``` **Recovery Time Objective (RTO)**: 4 hours **Recovery Point Objective (RPO)**: 24 hours --- ## Scaling and Performance ### Horizontal Pod Autoscaling CloudFlow is configured with Horizontal Pod Autoscaler (HPA) to automatically scale based on resource utilization: ```yaml # hpa.yaml apiVersion: autoscaling/v2 kind: HorizontalPodAutoscaler metadata: name: cloudflow-api-hpa namespace: cloudflow-prod spec: scaleTargetRef: apiVersion: apps/v1 kind: Deployment name: cloudflow-api minReplicas: 3 maxReplicas: 10 metrics: - type: Resource resource: name: cpu target: type: Utilization averageUtilization: 70 - type: Resource resource: name: memory target: type: Utilization averageUtilization: 80 behavior: scaleDown: stabilizationWindowSeconds: 300 policies: - type: Percent value: 50 periodSeconds: 60 scaleUp: stabilizationWindowSeconds: 60 policies: - type: Percent value: 100 periodSeconds: 30 - type: Pods value: 2 periodSeconds: 30 selectPolicy: Max ``` Monitor autoscaling events: ```bash kubectl get hpa -n cloudflow-prod -w kubectl describe hpa cloudflow-api-hpa -n cloudflow-prod ``` ### Performance Optimization #### Connection Pooling Ensure optimal database connection pooling settings in PgBouncer: - **Pool Mode**: Transaction (optimal for microservices) - **Default Pool Size**: 25 connections per user - **Max DB Connections**: 100 (matches PostgreSQL `max_connections`) #### Redis Caching Implement Redis caching for frequently accessed data: ```javascript // Example caching strategy const CACHE_TTL = { workflows: 300, // 5 minutes userSessions: 3600, // 1 hour apiResults: 60 // 1 minute }; ``` #### Load Testing Perform regular load testing to validate scaling configuration: ```bash # Install k6 for load testing kubectl apply -f - <<EOF apiVersion: v1 kind: ConfigMap metadata: name: loadtest-script namespace: cloudflow-prod data: script.js: | import http from 'k6/http'; import { check, sleep } from 'k6'; export let options = { stages: [ { duration: '2m', target: 100 }, { duration: '5m', target: 100 }, { duration: '2m', target: 200 }, { duration: '5m', target: 200 }, { duration: '2m', target: 0 }, ], thresholds: { http_req_duration: ['p(95)<500'], }, }; export default function () { let res = http.get('https://api.cloudflow.io/api/workflows'); check(res, { 'status is 200': (r) => r.status === 200 }); sleep(1); } EOF ``` **Performance Targets**: - P95 API latency: < 500ms - P99 API latency: < 1000ms - Error rate: < 0.1% - Throughput: > 1000 requests/second --- ## Security and Compliance ### Network Policies Implement network policies to restrict pod-to-pod communication: ```yaml # network-policy.yaml apiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: cloudflow-network-policy namespace: cloudflow-prod spec:",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_1",
              "content": "**Diagnosis:** ```bash # Check system time timedatectl status # Compare with NTP server ntpdate -q pool.ntp.org # Check JWT issued time vs current time jwt_iat=$(echo $CF_ACCESS_TOKEN | cut -d'.' -f2 | base64 -d | jq '.iat') current_time=$(date +%s) skew=$((current_time - jwt_iat)) echo \"Clock skew: $skew seconds\" ``` **Resolution:** ```bash # Sync with NTP server sudo ntpdate -s pool.ntp.org # Enable automatic time sync sudo timedatectl set-ntp true # Restart CloudFlow client cloudflow restart ``` **4. Insufficient Permissions** Error: `User does not have required permissions for this operation` **Check user roles:** ```bash cloudflow auth whoami --verbose # Expected output: # User: john.doe@company.com # Roles: developer, workflow-admin # Scopes: workflow:*, data:read, metrics:read ``` **Request permission elevation:** ```bash # Submit access request cloudflow auth request-access \\ --resource \"workflow:production:deploy\" \\ --justification \"Deploy critical hotfix for TICKET-1234\" # Check pending approvals cloudflow auth list-requests --status pending ``` ### 403 Forbidden Errors #### RBAC Policy Violations CloudFlow uses role-based access control (RBAC) with the following hierarchy: - `viewer` - Read-only access - `developer` - Create and modify workflows (non-production) - `operator` - Execute workflows, view logs - `admin` - Full access to workspace - `platform-admin` - Cross-workspace administration **Verify resource permissions:** ```bash # Check effective permissions cloudflow rbac check \\ --user john.doe@company.com \\ --resource workflow:prod-pipeline \\ --action execute # List all policies affecting user cloudflow rbac policies --user john.doe@company.com --verbose ``` --- ## Performance Problems ### Slow Query Performance #### Symptoms - API response times > 5000ms - Database query latency warnings in logs - CloudFlow UI becomes unresponsive - Timeout errors: `Request exceeded maximum duration of 30000ms` #### Diagnosis Steps **1. Identify Slow Queries** ```bash # Query CloudFlow metrics cloudflow metrics query --metric api_request_duration_ms \\ --filter \"p95 > 5000\" \\ --last 1h # Check database slow query log kubectl logs -n cloudflow deploy/cloudflow-db-primary | \\ grep \"slow query\" | \\ tail -n 50 # Analyze query patterns cloudflow db analyze-queries --min-duration 5000 --limit 20 ``` **2.",
              "score": null
            }
          ],
          "latency_ms": 11.654199999611592,
          "query_id": "realistic_009",
          "dimension": "contextual",
          "query": "Setting SLOs for our service, what P99 latency does CloudFlow guarantee?"
        },
        {
          "key_facts": [
            "P99 latency: < 200ms",
            "average P99 latency of 180ms for API operations"
          ],
          "found_facts": [],
          "missed_facts": [
            "P99 latency: < 200ms",
            "average P99 latency of 180ms for API operations"
          ],
          "coverage": 0.0,
          "expected_docs": [
            "architecture_overview"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_11",
              "content": "outage\" # Create war room cloudflow incident war-room create incident-2024012401 ``` **For SEV-2 (High):** ```bash # Create incident and notify cloudflow incident create \\ --severity SEV-2 \\ --title \"High rate of workflow failures\" \\ --description \"Workflow success rate dropped to 75%\" \\ --notify-oncall # Update status page cloudflow status update \\ --status \"partial_outage\" \\ --message \"We are experiencing elevated error rates\" ``` **For SEV-3/SEV-4:** ```bash # Create support ticket cloudflow support ticket create \\ --priority MEDIUM \\ --subject \"Intermittent timeout errors\" \\ --description \"See attached logs and reproduction steps\" ``` ### Contact Information - **On-call hotline:** +1-888-CLOUDFLOW (24/7) - **Support email:** support@cloudflow.io - **Incident Slack channel:** #cloudflow-incidents - **Status page:** https://status.cloudflow.io - **Documentation:** https://docs.cloudflow.io ### Post-Incident Review After resolving incidents, complete a postmortem: ```bash # Generate postmortem template cloudflow incident postmortem incident-2024012401 \\ --template standard \\ --output postmortem-2024012401.md # Required sections: # 1. Incident summary # 2. Impact assessment # 3. Timeline of events # 4. Root cause analysis # 5. Resolution steps # 6. Action items # 7. Lessons learned ``` --- ## Additional Resources - **CloudFlow Documentation:** https://docs.cloudflow.io - **API Reference:** https://api-docs.cloudflow.io - **Community Forum:** https://community.cloudflow.io - **GitHub Issues:** https://github.com/cloudflow/platform/issues - **Training Portal:** https://training.cloudflow.io - **Monitoring Dashboard:** https://monitoring.cloudflow.io ### Getting Help If this troubleshooting guide doesn't resolve your issue: 1. Search the knowledge base: `cloudflow kb search \"your issue\"` 2. Check community forum for similar issues 3. Contact support with detailed logs and reproduction steps 4. For urgent issues, use emergency escalation procedures **Remember:** Always capture logs, metrics, and reproduction steps before escalating! --- *Last updated: January 24, 2026* *Document version: 3.2.1* *Feedback: docs-feedback@cloudflow.io*",
              "score": null
            },
            {
              "doc_id": "user_guide",
              "chunk_id": "user_guide_fix_6",
              "content": "Filter by error type, date range, or execution ID 3. Click an execution to view full details 4. Click **\"Retry\"** to reprocess with the same input data ### Error Notifications Get notified when workflows fail: ```yaml workflow: notifications: on_failure: - type: email to: \"ops-team@company.com\" - type: slack channel: \"#alerts\" message: \"Workflow {{workflow.name}} failed: {{error.message}}\" on_success_after_retry: - type: slack channel: \"#monitoring\" message: \"Workflow recovered after {{error.attempts}} attempts\" ``` ## Workflow Limits CloudFlow enforces the following limits to ensure platform stability and performance: ### Steps Per Workflow - **Maximum**: 50 steps per workflow - **Recommendation**: Keep workflows focused and modular. If you need more steps, consider splitting into multiple workflows connected via webhooks. ### Execution Timeout - **Default**: 3600 seconds (60 minutes) - **Behavior**: Workflows exceeding this timeout are automatically terminated - **Custom Timeouts**: Enterprise plans can request custom timeout limits **Setting Step-Level Timeouts:** ```yaml - id: long_running_task action: http_request config: url: \"https://api.example.com/process\" timeout: 300 # 5 minutes for this specific step ``` ### Execution Limits - **Maximum**: 1000 executions per day (per workflow) - **Rate Limiting**: 100 concurrent executions per workflow - **Burst Limit**: 10 executions per second **What happens when limits are reached:** - New executions are queued automatically - Webhook triggers return HTTP 429 (Too Many Requests) - Scheduled executions are skipped (logged in audit trail) - Email notifications sent to workflow owner **Monitoring Usage:** View real-time metrics in your workflow dashboard: - Executions today: 847 / 1000 - Average duration: 12.3s - Success rate: 98.2% - Current queue depth: 3 ### Data Limits - **Maximum request/response size**: 10MB per action - **Maximum execution payload**: 50MB total - **Variable value size**: 1MB per variable ### Enterprise Plan Limits Enterprise customers can request increased limits: - Up to 100 steps per workflow - Up to 10,000 executions per day - Up to 7200 second timeout (2 hours) - Priority execution queue - Dedicated capacity allocation Contact sales@cloudflow.io for Enterprise pricing and custom limits. ## Best Practices Follow these best practices to build reliable, maintainable workflows: ### 1. Use Descriptive Names **Good:** - Workflow: \"Sync Customer Data from Salesforce to Database\" - Step: \"validate_customer_email\" **Bad:** - Workflow: \"Workflow 1\" - Step: \"step3\" ### 2.",
              "score": null
            },
            {
              "doc_id": "user_guide",
              "chunk_id": "user_guide_fix_5",
              "content": "Save and activate **Testing Schedules:** Use the built-in schedule calculator to preview upcoming executions: ``` Next 5 executions: 1. 2026-01-24 14:00:00 UTC 2. 2026-01-25 14:00:00 UTC 3. 2026-01-26 14:00:00 UTC 4. 2026-01-27 14:00:00 UTC 5. 2026-01-28 14:00:00 UTC ``` ## Error Handling Robust error handling ensures your workflows are resilient and reliable. ### Retry Policies Configure automatic retries for failed actions: ```yaml - id: api_call action: http_request config: url: \"https://api.example.com/data\" retry: max_attempts: 3 backoff_type: \"exponential\" # or \"fixed\", \"linear\" initial_interval: 1000 # milliseconds max_interval: 30000 multiplier: 2.0 retry_on: - timeout - network_error - status: [500, 502, 503, 504] ``` **Backoff Strategies:** - **Fixed**: Wait the same amount of time between retries - Attempt 1: Wait 1s - Attempt 2: Wait 1s - Attempt 3: Wait 1s - **Linear**: Increase wait time by a fixed amount - Attempt 1: Wait 1s - Attempt 2: Wait 2s - Attempt 3: Wait 3s - **Exponential**: Double the wait time with each retry (recommended) - Attempt 1: Wait 1s - Attempt 2: Wait 2s - Attempt 3: Wait 4s **Retry Conditions:** Control which errors trigger retries: - `timeout`: Request timeout - `network_error`: Connection failures - `status`: Specific HTTP status codes - `error_code`: Application-specific error codes ### Fallback Actions Execute alternative actions when the primary action fails: ```yaml - id: primary_payment action: http_request config: url: \"https://primary-payment-gateway.com/charge\" method: POST body: amount: \"{{amount}}\" on_error: - id: fallback_payment action: http_request config: url: \"https://backup-payment-gateway.com/charge\" method: POST body: amount: \"{{amount}}\" - id: notify_admin action: email config: to: \"admin@company.com\" subject: \"Payment Gateway Failure\" body: \"Primary gateway failed, switched to backup\" ``` **Error Object:** Access error details in fallback actions: ``` {{error.message}} # Error message {{error.code}} # Error code {{error.step_id}} # Failed step ID {{error.timestamp}} # When the error occurred {{error.attempts}} # Number of retry attempts ``` ### Dead Letter Queue When all retries and fallbacks fail, CloudFlow can route failed executions to a Dead Letter Queue (DLQ) for manual review: **Enable DLQ:** ```yaml workflow: error_handling: dead_letter_queue: enabled: true retain_days: 30 ``` **DLQ Features:** - View failed executions in the dashboard - Inspect complete execution context and error details - Retry individual executions after fixing issues - Export failed executions for analysis - Set up alerts for DLQ threshold breaches **Accessing the DLQ:** 1. Navigate to **Workflows** > **[Your Workflow]** > **Dead Letter Queue** 2.",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_7",
              "content": "records: 500 - Session timeout: 30 seconds --- ## Message Queue Patterns ### Pub/Sub Pattern Used for broadcasting events to multiple interested consumers: ``` Workflow Engine publishes workflow.execution.completed \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u25bc \u25bc \u25bc \u25bc Analytics Notification Audit Logger Billing Service Service Service Service ``` Each service maintains its own consumer group and processes events independently. Failures in one consumer don't affect others. ### Request-Reply Pattern Synchronous communication over async messaging (used sparingly): ``` API Gateway publishes request \u2192 Kafka (reply-to: temp-queue-123) \u2502 \u25bc Service processes request \u2502 \u25bc Service publishes response \u2192 temp-queue-123 \u2502 \u25bc API Gateway receives response ``` Timeout: 5 seconds, fallback to direct HTTP call if no response. ### Saga Pattern Distributed transaction management for multi-service workflows: ``` Step 1: Create Order \u2192 SUCCESS \u2192 Step 2: Reserve Inventory \u2502 FAILURE \u2502 \u25bc Compensating Transaction \u2502 \u25bc Cancel Order (Rollback) ``` Orchestrated by Workflow Engine with compensation logic defined in workflow DSL. ### Dead Letter Queue (DLQ) Failed messages after all retry attempts are routed to DLQ: ``` Message processing fails (3 retries with exponential backoff) \u2502 \u25bc Publish to dead-letter-queue topic \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u25bc \u25bc Alert to PagerDuty Store in PostgreSQL for analysis \u2502 \u2502 \u25bc \u25bc Manual investigation Automated pattern detection ``` DLQ Processing SLA: < 4 hours for critical events, < 24 hours for non-critical. --- ## Caching Strategy ### Cache-Aside Pattern (Lazy Loading) Primary caching pattern used across all services: ``` 1. Application checks cache (Redis GET) 2. If HIT \u2192 Return cached data 3. If MISS: a. Query database (PostgreSQL) b. Store result in cache with TTL c. Return data to application ``` **Implementation Example** (Workflow Definition): ```javascript async function getWorkflowDefinition(workflowId) { const cacheKey = `workflow:def:${workflowId}`; // Step 1: Check cache let workflow = await redis.get(cacheKey); if (workflow) { metrics.increment('cache.hit.workflow_def'); return JSON.parse(workflow); } // Step 2: Cache miss - query database metrics.increment('cache.miss.workflow_def'); workflow = await db.query( 'SELECT * FROM workflows WHERE id = $1', [workflowId] ); // Step 3: Store in cache (1 hour TTL) await redis.setex(cacheKey, 3600, JSON.stringify(workflow)); return workflow; } ``` ### Write-Through Pattern Used for critical data where cache consistency is paramount: ``` 1. Application writes to database 2. Database transaction commits 3. Application updates cache 4. Return success to client ``` Applied to: User profiles, authentication sessions, system configuration.",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_10",
              "content": "database nc -zv cloudflow-db.internal.company.com 5432 # Trace route traceroute api.cloudflow.io # Capture packets tcpdump -i any -w /tmp/capture.pcap port 5432 ``` --- ## Escalation Procedures ### Severity Levels CloudFlow incidents are classified into four severity levels: #### SEV-1: Critical (P1) - **Definition:** Complete service outage or severe degradation affecting all users - **Examples:** - API returns 5xx errors for > 5 minutes - Database completely unavailable - Authentication system down - Data loss or corruption - **Response Time:** Immediate (< 15 minutes) - **Escalation:** Page on-call engineer immediately #### SEV-2: High (P2) - **Definition:** Major functionality impaired affecting multiple users - **Examples:** - Workflow execution success rate < 90% - Significant performance degradation (p95 latency > 10s) - Rate limiting affecting large customer segment - **Response Time:** < 1 hour - **Escalation:** Create incident ticket and notify on-call #### SEV-3: Medium (P3) - **Definition:** Partial functionality degraded affecting some users - **Examples:** - Intermittent failures for specific workflow types - Minor performance issues - Non-critical feature unavailable - **Response Time:** < 4 hours - **Escalation:** Create ticket, normal business hours support #### SEV-4: Low (P4) - **Definition:** Minor issues with minimal user impact - **Examples:** - Cosmetic issues - Documentation errors - Feature requests - **Response Time:** < 2 business days - **Escalation:** Standard support ticket ### Escalation Steps #### Step 1: Initial Assessment (0-5 minutes) ```bash # Run health check cloudflow health check --comprehensive # Check status page curl https://status.cloudflow.io/api/v1/status.json # Review recent changes cloudflow audit log --last 2h --event-type \"deployment,configuration\" # Check metrics dashboard cloudflow metrics dashboard --incident-mode ``` #### Step 2: Gather Information (5-15 minutes) Create incident document with: - Incident timestamp and duration - Affected services and endpoints - Error rates and user impact - Recent changes or deployments - Relevant log excerpts - Correlation IDs for failed requests ```bash # Generate incident report cloudflow debug incident-report \\ --start \"2026-01-24T10:30:00Z\" \\ --end \"2026-01-24T11:00:00Z\" \\ --output incident-report.md # Capture system snapshot cloudflow debug snapshot --output snapshot-$(date +%Y%m%d-%H%M%S).tar.gz ``` #### Step 3: Escalate Based on Severity **For SEV-1 (Critical):** ```bash # Page on-call engineer cloudflow incident create \\ --severity SEV-1 \\ --title \"Complete API outage\" \\ --description \"All API requests returning 503\" \\ --page-oncall # Notify status page cloudflow status update \\ --status \"major_outage\" \\ --message \"We are investigating a complete service",
              "score": null
            }
          ],
          "latency_ms": 11.43270600005053,
          "query_id": "realistic_009",
          "dimension": "negation",
          "query": "Why are we getting alerts at 200ms latency?"
        },
        {
          "key_facts": [
            "RPO (Recovery Point Objective): 1 hour",
            "RTO (Recovery Time Objective): 4 hours"
          ],
          "found_facts": [],
          "missed_facts": [
            "RPO (Recovery Point Objective): 1 hour",
            "RTO (Recovery Time Objective): 4 hours"
          ],
          "coverage": 0.0,
          "expected_docs": [
            "architecture_overview",
            "deployment_guide"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_12",
              "content": "Resume normal operations - Recovery time: 1-3 hours depending on data volume - Data loss: None if corruption detected quickly ### Testing & Validation **DR Drill Schedule**: - Monthly: Automated failover test (single AZ failure simulation) - Quarterly: Full DR region failover (non-production hours) - Annually: Complete disaster simulation with all stakeholders **Last DR Test Results** (Dec 15, 2025): - Scenario: Full region failover - Actual RTO: 2 hours 23 minutes (target: 4 hours) - Actual RPO: 42 minutes (target: 1 hour) - Issues identified: DNS propagation slower than expected (resolved) - Success criteria: Met all recovery objectives ### Business Continuity **Communication Plan**: - Status page: status.cloudflow.com (updated every 15 minutes during incident) - Customer notifications: Email + SMS for all P1 incidents - Internal escalation: PagerDuty \u2192 Incident Commander \u2192 Engineering Manager \u2192 CTO **Data Retention Policy**: - Active data: 12 months in hot storage (PostgreSQL) - Archived data: 7 years in cold storage (S3 Glacier) - Audit logs: 7 years (compliance requirement) - Backup retention: 30 days standard, 90 days monthly snapshots --- ## Appendix ### Service Dependency Matrix | Service | Depends On | Critical Path | |---------|-----------|---------------| | API Gateway | Auth Service, Redis | Yes | | Auth Service | PostgreSQL, Redis, Vault | Yes | | Workflow Engine | PostgreSQL, Kafka, Redis | Yes | | Scheduler | PostgreSQL, Redis, Kafka | No | | Notification Service | Kafka, PostgreSQL, SendGrid, Twilio | No | ### Capacity Planning **Current Capacity** (Jan 2026): - Daily workflow executions: 2.5M - Active users: 150,000 - API requests per day: 50M - Database size: 1.2TB - Kafka throughput: 100K msg/sec peak **6-Month Projection** (Jul 2026): - Daily workflow executions: 4M (+60%) - Active users: 225,000 (+50%) - API requests per day: 80M (+60%) - Database size: 2TB (+67%) - Required scaling: +30% compute, +50% storage ### Contact Information - **Architecture Team**: architecture@cloudflow.internal - **On-Call Engineer**: PagerDuty escalation - **Security Team**: security@cloudflow.internal - **Documentation**: https://wiki.cloudflow.internal/architecture --- **Document Revision History**: - v2.3.1 (Jan 15, 2026): Updated performance metrics, added DR test results - v2.3.0 (Dec 1, 2025): Added multi-AZ deployment details - v2.2.0 (Oct 15, 2025): Security architecture overhaul - v2.1.0 (Sep 1, 2025): Initial Kafka migration documentation - v2.0.0 (Jul 1, 2025): Complete microservices rewrite",
              "score": null
            },
            {
              "doc_id": "user_guide",
              "chunk_id": "user_guide_fix_6",
              "content": "Filter by error type, date range, or execution ID 3. Click an execution to view full details 4. Click **\"Retry\"** to reprocess with the same input data ### Error Notifications Get notified when workflows fail: ```yaml workflow: notifications: on_failure: - type: email to: \"ops-team@company.com\" - type: slack channel: \"#alerts\" message: \"Workflow {{workflow.name}} failed: {{error.message}}\" on_success_after_retry: - type: slack channel: \"#monitoring\" message: \"Workflow recovered after {{error.attempts}} attempts\" ``` ## Workflow Limits CloudFlow enforces the following limits to ensure platform stability and performance: ### Steps Per Workflow - **Maximum**: 50 steps per workflow - **Recommendation**: Keep workflows focused and modular. If you need more steps, consider splitting into multiple workflows connected via webhooks. ### Execution Timeout - **Default**: 3600 seconds (60 minutes) - **Behavior**: Workflows exceeding this timeout are automatically terminated - **Custom Timeouts**: Enterprise plans can request custom timeout limits **Setting Step-Level Timeouts:** ```yaml - id: long_running_task action: http_request config: url: \"https://api.example.com/process\" timeout: 300 # 5 minutes for this specific step ``` ### Execution Limits - **Maximum**: 1000 executions per day (per workflow) - **Rate Limiting**: 100 concurrent executions per workflow - **Burst Limit**: 10 executions per second **What happens when limits are reached:** - New executions are queued automatically - Webhook triggers return HTTP 429 (Too Many Requests) - Scheduled executions are skipped (logged in audit trail) - Email notifications sent to workflow owner **Monitoring Usage:** View real-time metrics in your workflow dashboard: - Executions today: 847 / 1000 - Average duration: 12.3s - Success rate: 98.2% - Current queue depth: 3 ### Data Limits - **Maximum request/response size**: 10MB per action - **Maximum execution payload**: 50MB total - **Variable value size**: 1MB per variable ### Enterprise Plan Limits Enterprise customers can request increased limits: - Up to 100 steps per workflow - Up to 10,000 executions per day - Up to 7200 second timeout (2 hours) - Priority execution queue - Dedicated capacity allocation Contact sales@cloudflow.io for Enterprise pricing and custom limits. ## Best Practices Follow these best practices to build reliable, maintainable workflows: ### 1. Use Descriptive Names **Good:** - Workflow: \"Sync Customer Data from Salesforce to Database\" - Step: \"validate_customer_email\" **Bad:** - Workflow: \"Workflow 1\" - Step: \"step3\" ### 2.",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_11",
              "content": "outage\" # Create war room cloudflow incident war-room create incident-2024012401 ``` **For SEV-2 (High):** ```bash # Create incident and notify cloudflow incident create \\ --severity SEV-2 \\ --title \"High rate of workflow failures\" \\ --description \"Workflow success rate dropped to 75%\" \\ --notify-oncall # Update status page cloudflow status update \\ --status \"partial_outage\" \\ --message \"We are experiencing elevated error rates\" ``` **For SEV-3/SEV-4:** ```bash # Create support ticket cloudflow support ticket create \\ --priority MEDIUM \\ --subject \"Intermittent timeout errors\" \\ --description \"See attached logs and reproduction steps\" ``` ### Contact Information - **On-call hotline:** +1-888-CLOUDFLOW (24/7) - **Support email:** support@cloudflow.io - **Incident Slack channel:** #cloudflow-incidents - **Status page:** https://status.cloudflow.io - **Documentation:** https://docs.cloudflow.io ### Post-Incident Review After resolving incidents, complete a postmortem: ```bash # Generate postmortem template cloudflow incident postmortem incident-2024012401 \\ --template standard \\ --output postmortem-2024012401.md # Required sections: # 1. Incident summary # 2. Impact assessment # 3. Timeline of events # 4. Root cause analysis # 5. Resolution steps # 6. Action items # 7. Lessons learned ``` --- ## Additional Resources - **CloudFlow Documentation:** https://docs.cloudflow.io - **API Reference:** https://api-docs.cloudflow.io - **Community Forum:** https://community.cloudflow.io - **GitHub Issues:** https://github.com/cloudflow/platform/issues - **Training Portal:** https://training.cloudflow.io - **Monitoring Dashboard:** https://monitoring.cloudflow.io ### Getting Help If this troubleshooting guide doesn't resolve your issue: 1. Search the knowledge base: `cloudflow kb search \"your issue\"` 2. Check community forum for similar issues 3. Contact support with detailed logs and reproduction steps 4. For urgent issues, use emergency escalation procedures **Remember:** Always capture logs, metrics, and reproduction steps before escalating! --- *Last updated: January 24, 2026* *Document version: 3.2.1* *Feedback: docs-feedback@cloudflow.io*",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_10",
              "content": "database nc -zv cloudflow-db.internal.company.com 5432 # Trace route traceroute api.cloudflow.io # Capture packets tcpdump -i any -w /tmp/capture.pcap port 5432 ``` --- ## Escalation Procedures ### Severity Levels CloudFlow incidents are classified into four severity levels: #### SEV-1: Critical (P1) - **Definition:** Complete service outage or severe degradation affecting all users - **Examples:** - API returns 5xx errors for > 5 minutes - Database completely unavailable - Authentication system down - Data loss or corruption - **Response Time:** Immediate (< 15 minutes) - **Escalation:** Page on-call engineer immediately #### SEV-2: High (P2) - **Definition:** Major functionality impaired affecting multiple users - **Examples:** - Workflow execution success rate < 90% - Significant performance degradation (p95 latency > 10s) - Rate limiting affecting large customer segment - **Response Time:** < 1 hour - **Escalation:** Create incident ticket and notify on-call #### SEV-3: Medium (P3) - **Definition:** Partial functionality degraded affecting some users - **Examples:** - Intermittent failures for specific workflow types - Minor performance issues - Non-critical feature unavailable - **Response Time:** < 4 hours - **Escalation:** Create ticket, normal business hours support #### SEV-4: Low (P4) - **Definition:** Minor issues with minimal user impact - **Examples:** - Cosmetic issues - Documentation errors - Feature requests - **Response Time:** < 2 business days - **Escalation:** Standard support ticket ### Escalation Steps #### Step 1: Initial Assessment (0-5 minutes) ```bash # Run health check cloudflow health check --comprehensive # Check status page curl https://status.cloudflow.io/api/v1/status.json # Review recent changes cloudflow audit log --last 2h --event-type \"deployment,configuration\" # Check metrics dashboard cloudflow metrics dashboard --incident-mode ``` #### Step 2: Gather Information (5-15 minutes) Create incident document with: - Incident timestamp and duration - Affected services and endpoints - Error rates and user impact - Recent changes or deployments - Relevant log excerpts - Correlation IDs for failed requests ```bash # Generate incident report cloudflow debug incident-report \\ --start \"2026-01-24T10:30:00Z\" \\ --end \"2026-01-24T11:00:00Z\" \\ --output incident-report.md # Capture system snapshot cloudflow debug snapshot --output snapshot-$(date +%Y%m%d-%H%M%S).tar.gz ``` #### Step 3: Escalate Based on Severity **For SEV-1 (Critical):** ```bash # Page on-call engineer cloudflow incident create \\ --severity SEV-1 \\ --title \"Complete API outage\" \\ --description \"All API requests returning 503\" \\ --page-oncall # Notify status page cloudflow status update \\ --status \"major_outage\" \\ --message \"We are investigating a complete service",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_11",
              "content": "Service (2) - PostgreSQL Primary - PostgreSQL Replica - PostgreSQL Replica - Redis Primary (2) - Redis Replica (2) - Redis Replica (2) - Kafka Broker (2) - Kafka Broker (2) - Kafka Broker (1) ``` **Automatic Failover**: - Database: 30-60 seconds (automatic promotion of replica) - Redis: < 10 seconds (Sentinel-based failover) - Kafka: < 30 seconds (controller election) - Services: Kubernetes health checks with 10-second liveness probes ### Backup Strategy **Database Backups**: - Automated snapshots: Daily at 02:00 UTC - Retention: 30 days for daily, 90 days for monthly - Cross-region replication: Async replication to us-west-2 (15-minute lag) - Backup verification: Weekly automated restore test in staging environment **Backup Schedule**: ``` Daily: Full snapshot \u2192 S3 (encrypted) Hourly: WAL archives \u2192 S3 (point-in-time recovery) Weekly: Backup validation test Monthly: Long-term archive to Glacier ``` **Configuration Backups**: - Kubernetes manifests: Stored in Git (GitOps with ArgoCD) - Vault secrets: Automated snapshot every 6 hours - Kafka topic configurations: Exported daily to S3 ### Disaster Recovery Procedures **Scenario 1: Single AZ Failure** - Detection: Health checks fail for entire AZ (< 30 seconds) - Action: Traffic automatically routed to healthy AZs by ALB - Recovery time: < 5 minutes (no manual intervention) - Data loss: None (multi-AZ replication) **Scenario 2: Database Primary Failure** - Detection: Health check fails for primary database (< 30 seconds) - Action: Automatic promotion of read replica to primary - Recovery time: 30-60 seconds - Data loss: Minimal (< 1 second due to synchronous replication) **Scenario 3: Full Region Failure** - Detection: Multiple health check failures across all AZs (< 2 minutes) - Action: Manual failover to DR region (us-west-2) - Procedure: 1. Update DNS to point to DR region (TTL: 60 seconds) 2. Promote DR database replica to primary 3. Scale up DR region services to production capacity 4. Verify data consistency and integrity 5. Update monitoring dashboards - Recovery time: 2-4 hours (includes verification) - Data loss: < 1 hour (last cross-region replication) **Scenario 4: Data Corruption** - Detection: Data validation checks or user report - Action: Point-in-time recovery from WAL archives - Procedure: 1. Identify corruption time window 2. Restore from snapshot prior to corruption 3. Replay WAL logs up to corruption point 4. Verify data integrity 5.",
              "score": null
            }
          ],
          "latency_ms": 11.492397999973036,
          "query_id": "realistic_010",
          "dimension": "original",
          "query": "What are the disaster recovery RPO and RTO values?"
        },
        {
          "key_facts": [
            "RPO (Recovery Point Objective): 1 hour",
            "RTO (Recovery Time Objective): 4 hours"
          ],
          "found_facts": [
            "RPO (Recovery Point Objective): 1 hour",
            "RTO (Recovery Time Objective): 4 hours"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "architecture_overview",
            "deployment_guide"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_10",
              "content": "- Execution start rate: 500 per second - Completion rate: 450 per second (average 2-second execution time) **Database**: - Read throughput: 50,000 queries per second (across replicas) - Write throughput: 15,000 transactions per second - Connection capacity: 2,000 concurrent connections **Kafka**: - Message ingestion: 100,000 messages per second - Consumer throughput: 80,000 messages per second (aggregated) - End-to-end latency: < 100ms (P99) ### Resource Utilization **CPU Utilization** (Target: 60-70% average): - API Gateway: 55% average, 80% peak - Workflow Engine: 65% average, 85% peak - Auth Service: 40% average, 60% peak **Memory Utilization** (Target: < 80%): - API Gateway: 2.5GB average per pod (4GB allocated) - Workflow Engine: 6GB average per pod (8GB allocated) - Notification Service: 2.8GB average per pod (4GB allocated) **Network Throughput**: - Ingress: 2 Gbps average, 5 Gbps peak - Egress: 1.5 Gbps average, 4 Gbps peak - Internal (service mesh): 8 Gbps average ### Monitoring & Alerting **Key Metrics** (Monitored via Prometheus): - Request rate (per service, per endpoint) - Error rate (4xx, 5xx responses) - Latency percentiles (P50, P95, P99) - Resource utilization (CPU, memory, disk) - Cache hit/miss ratios - Database connection pool saturation - Kafka consumer lag **Alerts** (via PagerDuty): - P1 (Immediate): API error rate > 1%, database connection failure - P2 (< 15 min): P99 latency > 500ms, cache hit rate < 80% - P3 (< 1 hour): Resource utilization > 85%, replica count < minimum **SLI/SLO/SLA**: - SLI: API success rate (non-5xx responses) - SLO: 99.9% uptime per month (43 minutes downtime allowance) - SLA: 99.5% uptime guarantee (customer-facing SLA with credits) --- ## Disaster Recovery ### Recovery Objectives **RPO (Recovery Point Objective): 1 hour** - Maximum acceptable data loss: 1 hour of transactions - Achieved through: Continuous database replication + hourly snapshots - Kafka retention: 7 days allows event replay **RTO (Recovery Time Objective): 4 hours** - Maximum acceptable downtime: 4 hours for full system recovery - Includes: Failover, data verification, and service restoration - Automated runbooks reduce RTO to < 2 hours for common scenarios ### High Availability Architecture **Multi-AZ Deployment**: ``` Region: us-east-1 AZ-1a: AZ-1b: AZ-1c: - API Gateway (4) - API Gateway (4) - API Gateway (4) - Workflow Engine (6) - Workflow Engine (5) - Workflow Engine (5) - Auth Service (3) - Auth Service (3) - Auth",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_10",
              "content": "database nc -zv cloudflow-db.internal.company.com 5432 # Trace route traceroute api.cloudflow.io # Capture packets tcpdump -i any -w /tmp/capture.pcap port 5432 ``` --- ## Escalation Procedures ### Severity Levels CloudFlow incidents are classified into four severity levels: #### SEV-1: Critical (P1) - **Definition:** Complete service outage or severe degradation affecting all users - **Examples:** - API returns 5xx errors for > 5 minutes - Database completely unavailable - Authentication system down - Data loss or corruption - **Response Time:** Immediate (< 15 minutes) - **Escalation:** Page on-call engineer immediately #### SEV-2: High (P2) - **Definition:** Major functionality impaired affecting multiple users - **Examples:** - Workflow execution success rate < 90% - Significant performance degradation (p95 latency > 10s) - Rate limiting affecting large customer segment - **Response Time:** < 1 hour - **Escalation:** Create incident ticket and notify on-call #### SEV-3: Medium (P3) - **Definition:** Partial functionality degraded affecting some users - **Examples:** - Intermittent failures for specific workflow types - Minor performance issues - Non-critical feature unavailable - **Response Time:** < 4 hours - **Escalation:** Create ticket, normal business hours support #### SEV-4: Low (P4) - **Definition:** Minor issues with minimal user impact - **Examples:** - Cosmetic issues - Documentation errors - Feature requests - **Response Time:** < 2 business days - **Escalation:** Standard support ticket ### Escalation Steps #### Step 1: Initial Assessment (0-5 minutes) ```bash # Run health check cloudflow health check --comprehensive # Check status page curl https://status.cloudflow.io/api/v1/status.json # Review recent changes cloudflow audit log --last 2h --event-type \"deployment,configuration\" # Check metrics dashboard cloudflow metrics dashboard --incident-mode ``` #### Step 2: Gather Information (5-15 minutes) Create incident document with: - Incident timestamp and duration - Affected services and endpoints - Error rates and user impact - Recent changes or deployments - Relevant log excerpts - Correlation IDs for failed requests ```bash # Generate incident report cloudflow debug incident-report \\ --start \"2026-01-24T10:30:00Z\" \\ --end \"2026-01-24T11:00:00Z\" \\ --output incident-report.md # Capture system snapshot cloudflow debug snapshot --output snapshot-$(date +%Y%m%d-%H%M%S).tar.gz ``` #### Step 3: Escalate Based on Severity **For SEV-1 (Critical):** ```bash # Page on-call engineer cloudflow incident create \\ --severity SEV-1 \\ --title \"Complete API outage\" \\ --description \"All API requests returning 503\" \\ --page-oncall # Notify status page cloudflow status update \\ --status \"major_outage\" \\ --message \"We are investigating a complete service",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_12",
              "content": "Resume normal operations - Recovery time: 1-3 hours depending on data volume - Data loss: None if corruption detected quickly ### Testing & Validation **DR Drill Schedule**: - Monthly: Automated failover test (single AZ failure simulation) - Quarterly: Full DR region failover (non-production hours) - Annually: Complete disaster simulation with all stakeholders **Last DR Test Results** (Dec 15, 2025): - Scenario: Full region failover - Actual RTO: 2 hours 23 minutes (target: 4 hours) - Actual RPO: 42 minutes (target: 1 hour) - Issues identified: DNS propagation slower than expected (resolved) - Success criteria: Met all recovery objectives ### Business Continuity **Communication Plan**: - Status page: status.cloudflow.com (updated every 15 minutes during incident) - Customer notifications: Email + SMS for all P1 incidents - Internal escalation: PagerDuty \u2192 Incident Commander \u2192 Engineering Manager \u2192 CTO **Data Retention Policy**: - Active data: 12 months in hot storage (PostgreSQL) - Archived data: 7 years in cold storage (S3 Glacier) - Audit logs: 7 years (compliance requirement) - Backup retention: 30 days standard, 90 days monthly snapshots --- ## Appendix ### Service Dependency Matrix | Service | Depends On | Critical Path | |---------|-----------|---------------| | API Gateway | Auth Service, Redis | Yes | | Auth Service | PostgreSQL, Redis, Vault | Yes | | Workflow Engine | PostgreSQL, Kafka, Redis | Yes | | Scheduler | PostgreSQL, Redis, Kafka | No | | Notification Service | Kafka, PostgreSQL, SendGrid, Twilio | No | ### Capacity Planning **Current Capacity** (Jan 2026): - Daily workflow executions: 2.5M - Active users: 150,000 - API requests per day: 50M - Database size: 1.2TB - Kafka throughput: 100K msg/sec peak **6-Month Projection** (Jul 2026): - Daily workflow executions: 4M (+60%) - Active users: 225,000 (+50%) - API requests per day: 80M (+60%) - Database size: 2TB (+67%) - Required scaling: +30% compute, +50% storage ### Contact Information - **Architecture Team**: architecture@cloudflow.internal - **On-Call Engineer**: PagerDuty escalation - **Security Team**: security@cloudflow.internal - **Documentation**: https://wiki.cloudflow.internal/architecture --- **Document Revision History**: - v2.3.1 (Jan 15, 2026): Updated performance metrics, added DR test results - v2.3.0 (Dec 1, 2025): Added multi-AZ deployment details - v2.2.0 (Oct 15, 2025): Security architecture overhaul - v2.1.0 (Sep 1, 2025): Initial Kafka migration documentation - v2.0.0 (Jul 1, 2025): Complete microservices rewrite",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_11",
              "content": "Service (2) - PostgreSQL Primary - PostgreSQL Replica - PostgreSQL Replica - Redis Primary (2) - Redis Replica (2) - Redis Replica (2) - Kafka Broker (2) - Kafka Broker (2) - Kafka Broker (1) ``` **Automatic Failover**: - Database: 30-60 seconds (automatic promotion of replica) - Redis: < 10 seconds (Sentinel-based failover) - Kafka: < 30 seconds (controller election) - Services: Kubernetes health checks with 10-second liveness probes ### Backup Strategy **Database Backups**: - Automated snapshots: Daily at 02:00 UTC - Retention: 30 days for daily, 90 days for monthly - Cross-region replication: Async replication to us-west-2 (15-minute lag) - Backup verification: Weekly automated restore test in staging environment **Backup Schedule**: ``` Daily: Full snapshot \u2192 S3 (encrypted) Hourly: WAL archives \u2192 S3 (point-in-time recovery) Weekly: Backup validation test Monthly: Long-term archive to Glacier ``` **Configuration Backups**: - Kubernetes manifests: Stored in Git (GitOps with ArgoCD) - Vault secrets: Automated snapshot every 6 hours - Kafka topic configurations: Exported daily to S3 ### Disaster Recovery Procedures **Scenario 1: Single AZ Failure** - Detection: Health checks fail for entire AZ (< 30 seconds) - Action: Traffic automatically routed to healthy AZs by ALB - Recovery time: < 5 minutes (no manual intervention) - Data loss: None (multi-AZ replication) **Scenario 2: Database Primary Failure** - Detection: Health check fails for primary database (< 30 seconds) - Action: Automatic promotion of read replica to primary - Recovery time: 30-60 seconds - Data loss: Minimal (< 1 second due to synchronous replication) **Scenario 3: Full Region Failure** - Detection: Multiple health check failures across all AZs (< 2 minutes) - Action: Manual failover to DR region (us-west-2) - Procedure: 1. Update DNS to point to DR region (TTL: 60 seconds) 2. Promote DR database replica to primary 3. Scale up DR region services to production capacity 4. Verify data consistency and integrity 5. Update monitoring dashboards - Recovery time: 2-4 hours (includes verification) - Data loss: < 1 hour (last cross-region replication) **Scenario 4: Data Corruption** - Detection: Data validation checks or user report - Action: Point-in-time recovery from WAL archives - Procedure: 1. Identify corruption time window 2. Restore from snapshot prior to corruption 3. Replay WAL logs up to corruption point 4. Verify data integrity 5.",
              "score": null
            },
            {
              "doc_id": "user_guide",
              "chunk_id": "user_guide_fix_6",
              "content": "Filter by error type, date range, or execution ID 3. Click an execution to view full details 4. Click **\"Retry\"** to reprocess with the same input data ### Error Notifications Get notified when workflows fail: ```yaml workflow: notifications: on_failure: - type: email to: \"ops-team@company.com\" - type: slack channel: \"#alerts\" message: \"Workflow {{workflow.name}} failed: {{error.message}}\" on_success_after_retry: - type: slack channel: \"#monitoring\" message: \"Workflow recovered after {{error.attempts}} attempts\" ``` ## Workflow Limits CloudFlow enforces the following limits to ensure platform stability and performance: ### Steps Per Workflow - **Maximum**: 50 steps per workflow - **Recommendation**: Keep workflows focused and modular. If you need more steps, consider splitting into multiple workflows connected via webhooks. ### Execution Timeout - **Default**: 3600 seconds (60 minutes) - **Behavior**: Workflows exceeding this timeout are automatically terminated - **Custom Timeouts**: Enterprise plans can request custom timeout limits **Setting Step-Level Timeouts:** ```yaml - id: long_running_task action: http_request config: url: \"https://api.example.com/process\" timeout: 300 # 5 minutes for this specific step ``` ### Execution Limits - **Maximum**: 1000 executions per day (per workflow) - **Rate Limiting**: 100 concurrent executions per workflow - **Burst Limit**: 10 executions per second **What happens when limits are reached:** - New executions are queued automatically - Webhook triggers return HTTP 429 (Too Many Requests) - Scheduled executions are skipped (logged in audit trail) - Email notifications sent to workflow owner **Monitoring Usage:** View real-time metrics in your workflow dashboard: - Executions today: 847 / 1000 - Average duration: 12.3s - Success rate: 98.2% - Current queue depth: 3 ### Data Limits - **Maximum request/response size**: 10MB per action - **Maximum execution payload**: 50MB total - **Variable value size**: 1MB per variable ### Enterprise Plan Limits Enterprise customers can request increased limits: - Up to 100 steps per workflow - Up to 10,000 executions per day - Up to 7200 second timeout (2 hours) - Priority execution queue - Dedicated capacity allocation Contact sales@cloudflow.io for Enterprise pricing and custom limits. ## Best Practices Follow these best practices to build reliable, maintainable workflows: ### 1. Use Descriptive Names **Good:** - Workflow: \"Sync Customer Data from Salesforce to Database\" - Step: \"validate_customer_email\" **Bad:** - Workflow: \"Workflow 1\" - Step: \"step3\" ### 2.",
              "score": null
            }
          ],
          "latency_ms": 11.644771999272052,
          "query_id": "realistic_010",
          "dimension": "synonym",
          "query": "What's the maximum data loss and recovery time?"
        },
        {
          "key_facts": [
            "RPO (Recovery Point Objective): 1 hour",
            "RTO (Recovery Time Objective): 4 hours"
          ],
          "found_facts": [],
          "missed_facts": [
            "RPO (Recovery Point Objective): 1 hour",
            "RTO (Recovery Time Objective): 4 hours"
          ],
          "coverage": 0.0,
          "expected_docs": [
            "architecture_overview",
            "deployment_guide"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_5",
              "content": "exec_7h3j6k9m2n --show-bottlenecks ``` #### Solutions **1. Increase workflow timeout (if justified):** ```bash # Update workflow configuration cloudflow workflows update wf_9k2n4m8p1q \\ --timeout 7200 \\ --reason \"Large dataset processing requires extended time\" # Verify update cloudflow workflows get wf_9k2n4m8p1q | grep timeout ``` **2. Optimize slow steps:** ```bash # Enable parallel processing cloudflow workflows update wf_9k2n4m8p1q \\ --step data_transformation \\ --parallel-workers 8 \\ --batch-size 1000 # Add checkpointing for long operations cloudflow workflows update wf_9k2n4m8p1q \\ --step data_transformation \\ --enable-checkpointing \\ --checkpoint-interval 300 ``` **3. Split workflow into smaller workflows:** ```bash # Create sub-workflows cloudflow workflows create data-pipeline-part1 \\ --steps \"data_ingestion,data_validation\" \\ --timeout 1800 cloudflow workflows create data-pipeline-part2 \\ --steps \"data_transformation,data_export\" \\ --timeout 3600 \\ --trigger workflow_completed \\ --trigger-workflow data-pipeline-part1 ``` ### Retry Logic and Exponential Backoff CloudFlow implements automatic retry with exponential backoff for transient failures: - Max retries: 3 - Initial delay: 1 second - Backoff multiplier: 2 - Max delay: 60 seconds #### Retry Sequence ``` Attempt 1: Immediate Attempt 2: Wait 1s (2^0 * 1s) Attempt 3: Wait 2s (2^1 * 1s) Attempt 4: Wait 4s (2^2 * 1s) ``` #### Configuration ```bash # View current retry settings cloudflow workflows get wf_9k2n4m8p1q --format json | jq '.retry_policy' # Customize retry behavior cloudflow workflows update wf_9k2n4m8p1q \\ --retry-max-attempts 5 \\ --retry-initial-delay 2000 \\ --retry-backoff-multiplier 2 \\ --retry-max-delay 120000 \\ --retry-on-errors \"NETWORK_ERROR,TIMEOUT,RATE_LIMIT\" # Disable retry for specific step cloudflow workflows update wf_9k2n4m8p1q \\ --step data_export \\ --retry-enabled false ``` #### Monitoring Retries ```bash # List failed executions with retry information cloudflow workflows executions list \\ --status FAILED \\ --show-retries \\ --last 7d # Get retry history for specific execution cloudflow workflows executions retries exec_7h3j6k9m2n # Output: # Execution: exec_7h3j6k9m2n # Attempt 1: FAILED - NetworkError: Connection refused (delay: 0ms) # Attempt 2: FAILED - NetworkError: Connection timeout (delay: 1000ms) # Attempt 3: FAILED - NetworkError: Connection timeout (delay: 2000ms) # Attempt 4: FAILED - NetworkError: Connection timeout (delay: 4000ms) # Final Status: FAILED_AFTER_RETRIES ``` ### Workflow Step Failures #### Common Error Patterns **1.",
              "score": null
            },
            {
              "doc_id": "user_guide",
              "chunk_id": "user_guide_fix_5",
              "content": "Save and activate **Testing Schedules:** Use the built-in schedule calculator to preview upcoming executions: ``` Next 5 executions: 1. 2026-01-24 14:00:00 UTC 2. 2026-01-25 14:00:00 UTC 3. 2026-01-26 14:00:00 UTC 4. 2026-01-27 14:00:00 UTC 5. 2026-01-28 14:00:00 UTC ``` ## Error Handling Robust error handling ensures your workflows are resilient and reliable. ### Retry Policies Configure automatic retries for failed actions: ```yaml - id: api_call action: http_request config: url: \"https://api.example.com/data\" retry: max_attempts: 3 backoff_type: \"exponential\" # or \"fixed\", \"linear\" initial_interval: 1000 # milliseconds max_interval: 30000 multiplier: 2.0 retry_on: - timeout - network_error - status: [500, 502, 503, 504] ``` **Backoff Strategies:** - **Fixed**: Wait the same amount of time between retries - Attempt 1: Wait 1s - Attempt 2: Wait 1s - Attempt 3: Wait 1s - **Linear**: Increase wait time by a fixed amount - Attempt 1: Wait 1s - Attempt 2: Wait 2s - Attempt 3: Wait 3s - **Exponential**: Double the wait time with each retry (recommended) - Attempt 1: Wait 1s - Attempt 2: Wait 2s - Attempt 3: Wait 4s **Retry Conditions:** Control which errors trigger retries: - `timeout`: Request timeout - `network_error`: Connection failures - `status`: Specific HTTP status codes - `error_code`: Application-specific error codes ### Fallback Actions Execute alternative actions when the primary action fails: ```yaml - id: primary_payment action: http_request config: url: \"https://primary-payment-gateway.com/charge\" method: POST body: amount: \"{{amount}}\" on_error: - id: fallback_payment action: http_request config: url: \"https://backup-payment-gateway.com/charge\" method: POST body: amount: \"{{amount}}\" - id: notify_admin action: email config: to: \"admin@company.com\" subject: \"Payment Gateway Failure\" body: \"Primary gateway failed, switched to backup\" ``` **Error Object:** Access error details in fallback actions: ``` {{error.message}} # Error message {{error.code}} # Error code {{error.step_id}} # Failed step ID {{error.timestamp}} # When the error occurred {{error.attempts}} # Number of retry attempts ``` ### Dead Letter Queue When all retries and fallbacks fail, CloudFlow can route failed executions to a Dead Letter Queue (DLQ) for manual review: **Enable DLQ:** ```yaml workflow: error_handling: dead_letter_queue: enabled: true retain_days: 30 ``` **DLQ Features:** - View failed executions in the dashboard - Inspect complete execution context and error details - Retry individual executions after fixing issues - Export failed executions for analysis - Set up alerts for DLQ threshold breaches **Accessing the DLQ:** 1. Navigate to **Workflows** > **[Your Workflow]** > **Dead Letter Queue** 2.",
              "score": null
            },
            {
              "doc_id": "user_guide",
              "chunk_id": "user_guide_fix_6",
              "content": "Filter by error type, date range, or execution ID 3. Click an execution to view full details 4. Click **\"Retry\"** to reprocess with the same input data ### Error Notifications Get notified when workflows fail: ```yaml workflow: notifications: on_failure: - type: email to: \"ops-team@company.com\" - type: slack channel: \"#alerts\" message: \"Workflow {{workflow.name}} failed: {{error.message}}\" on_success_after_retry: - type: slack channel: \"#monitoring\" message: \"Workflow recovered after {{error.attempts}} attempts\" ``` ## Workflow Limits CloudFlow enforces the following limits to ensure platform stability and performance: ### Steps Per Workflow - **Maximum**: 50 steps per workflow - **Recommendation**: Keep workflows focused and modular. If you need more steps, consider splitting into multiple workflows connected via webhooks. ### Execution Timeout - **Default**: 3600 seconds (60 minutes) - **Behavior**: Workflows exceeding this timeout are automatically terminated - **Custom Timeouts**: Enterprise plans can request custom timeout limits **Setting Step-Level Timeouts:** ```yaml - id: long_running_task action: http_request config: url: \"https://api.example.com/process\" timeout: 300 # 5 minutes for this specific step ``` ### Execution Limits - **Maximum**: 1000 executions per day (per workflow) - **Rate Limiting**: 100 concurrent executions per workflow - **Burst Limit**: 10 executions per second **What happens when limits are reached:** - New executions are queued automatically - Webhook triggers return HTTP 429 (Too Many Requests) - Scheduled executions are skipped (logged in audit trail) - Email notifications sent to workflow owner **Monitoring Usage:** View real-time metrics in your workflow dashboard: - Executions today: 847 / 1000 - Average duration: 12.3s - Success rate: 98.2% - Current queue depth: 3 ### Data Limits - **Maximum request/response size**: 10MB per action - **Maximum execution payload**: 50MB total - **Variable value size**: 1MB per variable ### Enterprise Plan Limits Enterprise customers can request increased limits: - Up to 100 steps per workflow - Up to 10,000 executions per day - Up to 7200 second timeout (2 hours) - Priority execution queue - Dedicated capacity allocation Contact sales@cloudflow.io for Enterprise pricing and custom limits. ## Best Practices Follow these best practices to build reliable, maintainable workflows: ### 1. Use Descriptive Names **Good:** - Workflow: \"Sync Customer Data from Salesforce to Database\" - Step: \"validate_customer_email\" **Bad:** - Workflow: \"Workflow 1\" - Step: \"step3\" ### 2.",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_11",
              "content": "Service (2) - PostgreSQL Primary - PostgreSQL Replica - PostgreSQL Replica - Redis Primary (2) - Redis Replica (2) - Redis Replica (2) - Kafka Broker (2) - Kafka Broker (2) - Kafka Broker (1) ``` **Automatic Failover**: - Database: 30-60 seconds (automatic promotion of replica) - Redis: < 10 seconds (Sentinel-based failover) - Kafka: < 30 seconds (controller election) - Services: Kubernetes health checks with 10-second liveness probes ### Backup Strategy **Database Backups**: - Automated snapshots: Daily at 02:00 UTC - Retention: 30 days for daily, 90 days for monthly - Cross-region replication: Async replication to us-west-2 (15-minute lag) - Backup verification: Weekly automated restore test in staging environment **Backup Schedule**: ``` Daily: Full snapshot \u2192 S3 (encrypted) Hourly: WAL archives \u2192 S3 (point-in-time recovery) Weekly: Backup validation test Monthly: Long-term archive to Glacier ``` **Configuration Backups**: - Kubernetes manifests: Stored in Git (GitOps with ArgoCD) - Vault secrets: Automated snapshot every 6 hours - Kafka topic configurations: Exported daily to S3 ### Disaster Recovery Procedures **Scenario 1: Single AZ Failure** - Detection: Health checks fail for entire AZ (< 30 seconds) - Action: Traffic automatically routed to healthy AZs by ALB - Recovery time: < 5 minutes (no manual intervention) - Data loss: None (multi-AZ replication) **Scenario 2: Database Primary Failure** - Detection: Health check fails for primary database (< 30 seconds) - Action: Automatic promotion of read replica to primary - Recovery time: 30-60 seconds - Data loss: Minimal (< 1 second due to synchronous replication) **Scenario 3: Full Region Failure** - Detection: Multiple health check failures across all AZs (< 2 minutes) - Action: Manual failover to DR region (us-west-2) - Procedure: 1. Update DNS to point to DR region (TTL: 60 seconds) 2. Promote DR database replica to primary 3. Scale up DR region services to production capacity 4. Verify data consistency and integrity 5. Update monitoring dashboards - Recovery time: 2-4 hours (includes verification) - Data loss: < 1 hour (last cross-region replication) **Scenario 4: Data Corruption** - Detection: Data validation checks or user report - Action: Point-in-time recovery from WAL archives - Procedure: 1. Identify corruption time window 2. Restore from snapshot prior to corruption 3. Replay WAL logs up to corruption point 4. Verify data integrity 5.",
              "score": null
            },
            {
              "doc_id": "user_guide",
              "chunk_id": "user_guide_fix_7",
              "content": "Handle Errors Gracefully Always implement error handling for external API calls and database operations: ```yaml - id: fetch_data action: http_request config: url: \"https://api.example.com/data\" retry: max_attempts: 3 backoff_type: exponential on_error: - id: log_error action: database_query config: query: \"INSERT INTO error_log (workflow_id, error) VALUES ($1, $2)\" parameters: - \"{{workflow.id}}\" - \"{{error.message}}\" ``` ### 3. Use Secrets for Sensitive Data Never hardcode API keys, passwords, or tokens in workflows: **Bad:** ```yaml headers: Authorization: \"Bearer sk_live_abc123xyz789\" ``` **Good:** ```yaml headers: Authorization: \"Bearer {{secrets.API_TOKEN}}\" ``` Store secrets in **Settings** > **Secrets** with encryption at rest. ### 4. Validate Input Data Always validate trigger data before processing: ```yaml - id: validate_input action: javascript code: | const required_fields = ['email', 'name', 'order_id']; for (const field of required_fields) { if (!input[field]) { throw new Error(`Missing required field: ${field}`); } } // Validate email format const email_regex = /^[^\\s@]+@[^\\s@]+\\.[^\\s@]+$/; if (!email_regex.test(input.email)) { throw new Error('Invalid email format'); } return { validated: true }; ``` ### 5. Use Idempotency Keys For operations that shouldn't be repeated (payments, record creation), use idempotency keys: ```yaml - id: create_charge action: http_request config: url: \"https://api.stripe.com/v1/charges\" method: POST headers: Idempotency-Key: \"{{workflow.id}}-{{execution.id}}\" body: amount: \"{{amount}}\" ``` ### 6. Monitor and Log Add logging steps for important workflow milestones: ```yaml - id: log_start action: database_query config: query: \"INSERT INTO workflow_audit (execution_id, step, timestamp) VALUES ($1, $2, $3)\" parameters: - \"{{execution.id}}\" - \"workflow_started\" - \"{{now()}}\" ``` ### 7. Keep Workflows Modular Break complex workflows into smaller, reusable components: - Use sub-workflows for repeated logic - Trigger child workflows via webhooks - Share common configurations via templates ### 8. Test Thoroughly Before activating a workflow: 1. Use test mode with sample data 2. Verify all actions execute correctly 3. Test error handling paths 4. Review execution logs 5. Start with a limited scope (e.g., test channel, small dataset) ### 9. Document Your Workflows Add descriptions to workflows and steps: ```yaml workflow: name: \"Daily Sales Report\" description: | Generates a daily sales report and distributes it to the sales team. Runs at 8:00 AM EST Monday-Friday. Data source: PostgreSQL sales database Recipients: sales-team@company.com steps: - id: fetch_sales name: \"Fetch Yesterday's Sales\" description: \"Query database for all completed orders from previous day\" ``` ### 10.",
              "score": null
            }
          ],
          "latency_ms": 12.071606997778872,
          "query_id": "realistic_010",
          "dimension": "problem",
          "query": "How long will it take to recover from a disaster?"
        },
        {
          "key_facts": [
            "RPO (Recovery Point Objective): 1 hour",
            "RTO (Recovery Time Objective): 4 hours"
          ],
          "found_facts": [],
          "missed_facts": [
            "RPO (Recovery Point Objective): 1 hour",
            "RTO (Recovery Time Objective): 4 hours"
          ],
          "coverage": 0.0,
          "expected_docs": [
            "architecture_overview",
            "deployment_guide"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_12",
              "content": "Resume normal operations - Recovery time: 1-3 hours depending on data volume - Data loss: None if corruption detected quickly ### Testing & Validation **DR Drill Schedule**: - Monthly: Automated failover test (single AZ failure simulation) - Quarterly: Full DR region failover (non-production hours) - Annually: Complete disaster simulation with all stakeholders **Last DR Test Results** (Dec 15, 2025): - Scenario: Full region failover - Actual RTO: 2 hours 23 minutes (target: 4 hours) - Actual RPO: 42 minutes (target: 1 hour) - Issues identified: DNS propagation slower than expected (resolved) - Success criteria: Met all recovery objectives ### Business Continuity **Communication Plan**: - Status page: status.cloudflow.com (updated every 15 minutes during incident) - Customer notifications: Email + SMS for all P1 incidents - Internal escalation: PagerDuty \u2192 Incident Commander \u2192 Engineering Manager \u2192 CTO **Data Retention Policy**: - Active data: 12 months in hot storage (PostgreSQL) - Archived data: 7 years in cold storage (S3 Glacier) - Audit logs: 7 years (compliance requirement) - Backup retention: 30 days standard, 90 days monthly snapshots --- ## Appendix ### Service Dependency Matrix | Service | Depends On | Critical Path | |---------|-----------|---------------| | API Gateway | Auth Service, Redis | Yes | | Auth Service | PostgreSQL, Redis, Vault | Yes | | Workflow Engine | PostgreSQL, Kafka, Redis | Yes | | Scheduler | PostgreSQL, Redis, Kafka | No | | Notification Service | Kafka, PostgreSQL, SendGrid, Twilio | No | ### Capacity Planning **Current Capacity** (Jan 2026): - Daily workflow executions: 2.5M - Active users: 150,000 - API requests per day: 50M - Database size: 1.2TB - Kafka throughput: 100K msg/sec peak **6-Month Projection** (Jul 2026): - Daily workflow executions: 4M (+60%) - Active users: 225,000 (+50%) - API requests per day: 80M (+60%) - Database size: 2TB (+67%) - Required scaling: +30% compute, +50% storage ### Contact Information - **Architecture Team**: architecture@cloudflow.internal - **On-Call Engineer**: PagerDuty escalation - **Security Team**: security@cloudflow.internal - **Documentation**: https://wiki.cloudflow.internal/architecture --- **Document Revision History**: - v2.3.1 (Jan 15, 2026): Updated performance metrics, added DR test results - v2.3.0 (Dec 1, 2025): Added multi-AZ deployment details - v2.2.0 (Oct 15, 2025): Security architecture overhaul - v2.1.0 (Sep 1, 2025): Initial Kafka migration documentation - v2.0.0 (Jul 1, 2025): Complete microservices rewrite",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_3",
              "content": "**Technology**: Go with distributed locking via Redis **Replicas**: 4 pods (production), active-passive with leader election **Resource Allocation**: 2 vCPU, 4GB RAM per pod **Key Responsibilities**: - Parse and validate cron expressions (extended format supporting seconds) - Maintain schedule registry in PostgreSQL - Distributed scheduling with leader election (one active scheduler) - Missed execution handling with configurable catch-up policy - Schedule conflict detection and resolution - Time zone support for international schedules **Scheduling Architecture**: ``` PostgreSQL Schedules Table \u2502 \u25bc Leader Scheduler (elected via Redis) \u2502 \u251c\u2500 Scan for due schedules (every 10 seconds) \u251c\u2500 Acquire lock per schedule (prevents duplicates) \u251c\u2500 Publish to Kafka \u2192 workflow.events topic \u2514\u2500 Update last_run timestamp ``` **Schedule Types**: - **Cron-based**: `0 */5 * * * *` (every 5 minutes) - **One-time**: Specific timestamp for delayed execution - **Interval-based**: Every N seconds/minutes/hours **Reliability Features**: - Leader election using Redis with 30-second lease - Heartbeat mechanism to detect leader failure (5-second interval) - Automatic failover to standby scheduler (< 10 seconds) - Schedule versioning to handle updates during execution - Missed execution policy: SKIP, RUN_ONCE, or RUN_ALL **Performance Targets**: - Schedule evaluation: < 100ms per cycle - Accuracy: \u00b1 1 second for schedule triggers - Capacity: 100,000 active schedules --- ### Notification Service **Purpose**: Multi-channel notification delivery system supporting email, SMS, webhooks, and in-app notifications.",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_9",
              "content": "per API key (default: 1000 RPM) ### Secrets Management **HashiCorp Vault Integration**: - Dynamic database credentials: Generated on-demand, 1-hour TTL - Encryption keys: Transit secrets engine for encryption-as-a-service - API keys for external services: Stored with versioning - Rotation policy: Automated rotation every 90 days with notification **Secret Access Pattern**: ``` Service starts \u2192 Vault authentication (Kubernetes service account) \u2502 \u25bc Request secret lease (1-hour TTL) \u2502 \u25bc Vault returns dynamic credentials \u2502 \u25bc Service connects to PostgreSQL \u2502 Renew lease every 30 minutes ``` ### Data Protection **Encryption at Rest**: - PostgreSQL: AES-256 encryption enabled at volume level - Redis: Encryption enabled using AWS KMS - Kafka: Encryption at rest for all data on brokers - S3 backups: Server-side encryption with KMS (SSE-KMS) **Encryption in Transit**: - External traffic: TLS 1.3 only (TLS 1.2 deprecated) - Internal traffic: mTLS via Istio service mesh - Database connections: SSL/TLS enforced (sslmode=require) **Data Masking**: - Sensitive fields (email, phone) masked in logs - PII redacted from error messages and stack traces - Audit logs: Full data retention with access controls **Compliance**: - GDPR: User data export and deletion workflows - SOC 2 Type II: Audit logging for all data access - HIPAA: PHI isolation in dedicated tenants with enhanced encryption --- ## Performance Characteristics ### Latency Targets **API Operations** (P99 latency): - `GET /workflows/{id}`: < 50ms (cache hit), < 150ms (cache miss) - `POST /workflows`: < 200ms (includes validation and database write) - `POST /workflows/{id}/execute`: < 100ms (async, returns execution ID) - `GET /workflows/{id}/history`: < 300ms (paginated, 50 records per page) **Workflow Execution** (P99 latency): - Simple workflow (< 5 steps): < 2 seconds - Medium workflow (5-15 steps): < 5 seconds - Complex workflow (> 15 steps): < 15 seconds - Parallel execution: Linear scaling up to 10 concurrent branches **Database Operations** (P99 latency): - Simple SELECT: < 5ms - JOIN query: < 20ms - INSERT/UPDATE: < 10ms - Batch operations: < 100ms (batch size: 100 records) **Cache Operations** (P99 latency): - Redis GET: < 2ms - Redis SET: < 3ms - Redis DEL: < 2ms ### Throughput Capacity **API Gateway**: - Sustained: 10,000 requests per second - Peak: 25,000 requests per second (5-minute burst) - Rate limiting: 1,000 requests per minute per API key **Workflow Engine**: - Concurrent executions: 8,000 workflows (across 16 pods)",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_4",
              "content": "**Technology**: Node.js with worker pool pattern **Replicas**: 8 pods (production), auto-scaling 6-16 **Resource Allocation**: 2 vCPU, 4GB RAM per pod **Key Responsibilities**: - Consume notification events from Kafka - Template rendering with Handlebars (cached in Redis) - Multi-channel delivery (Email via SendGrid, SMS via Twilio, Webhooks) - Retry logic with dead letter queue for failed deliveries - Delivery status tracking and analytics - User preference management (opt-in/opt-out) **Notification Flow**: ``` Event Source \u2192 Kafka (notifications.email topic) \u2502 \u25bc Notification Service Consumer \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u25bc \u25bc \u25bc [Email Queue] [SMS Queue] [Webhook Queue] \u2502 \u2502 \u2502 \u25bc \u25bc \u25bc [SendGrid] [Twilio] [HTTP POST] \u2502 \u2502 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u25bc Delivery Status \u2192 PostgreSQL ``` **Channel Configuration**: - Email: SendGrid with IP warming, DKIM/SPF configured - SMS: Twilio with fallback to AWS SNS - Webhook: Signed payloads (HMAC-SHA256), retry 3 times with exponential backoff - In-app: WebSocket connections via Socket.io **Template Management**: - Templates stored in PostgreSQL with versioning - Compiled templates cached in Redis (1-hour TTL) - Support for localization (i18n) with 12 languages - A/B testing capability for email campaigns **Performance Targets**: - Email delivery: < 5 seconds (P99) from event to SendGrid - SMS delivery: < 3 seconds (P99) - Throughput: 10,000 notifications per minute --- ## Data Flow Architecture ### Synchronous Request Flow Client requests follow a synchronous path through the API Gateway to backend services: ``` 1. Client \u2192 Load Balancer (TLS termination) 2. Load Balancer \u2192 API Gateway (HTTP/2) 3. API Gateway \u2192 Auth Service (JWT validation via gRPC) 4. API Gateway \u2192 Target Service (HTTP/REST or gRPC) 5. Target Service \u2192 Database/Cache (data retrieval) 6. Response propagates back through the chain ``` **Timeout Configuration**: - Client \u2192 Load Balancer: 60 seconds - Load Balancer \u2192 API Gateway: 55 seconds - API Gateway \u2192 Services: 45 seconds - Service \u2192 Database: 10 seconds ### Asynchronous Event Flow Long-running operations and inter-service communication use event-driven patterns: ``` 1. Service publishes event \u2192 Kafka topic 2. Kafka persists event (replication factor: 3) 3. Consumer groups subscribe to topics 4. Consumers process events with at-least-once delivery 5. State updates written to PostgreSQL 6.",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_1",
              "content": "**Technology**: Node.js with Express.js framework **Replicas**: 12 pods (production), auto-scaling 8-20 based on CPU **Resource Allocation**: 2 vCPU, 4GB RAM per pod **Key Responsibilities**: - JWT token validation (delegated to Auth Service for initial validation) - Rate limiting: 1000 requests per minute per API key (sliding window) - Request/response transformation and validation using JSON Schema - Routing to downstream services based on URL path patterns - CORS handling for web clients - Request/response logging and correlation ID injection **Critical Endpoints**: - `POST /api/v1/workflows` - Create new workflow - `GET /api/v1/workflows/:id` - Retrieve workflow status - `POST /api/v1/workflows/:id/execute` - Trigger workflow execution - `GET /api/v1/workflows/:id/history` - Get execution history **Dependencies**: - Auth Service (for token validation) - Redis (for rate limiting counters) - All downstream microservices **Performance Targets**: - P50 latency: < 50ms - P99 latency: < 200ms - Throughput: 10,000 RPS sustained --- ### Auth Service **Purpose**: Centralized authentication and authorization service handling user identity, token generation, and permission validation. **Technology**: Go with gRPC for internal communication, REST for external **Replicas**: 8 pods (production), auto-scaling 6-12 **Resource Allocation**: 1 vCPU, 2GB RAM per pod **Key Responsibilities**: - User authentication via multiple providers (OAuth2, SAML, local credentials) - JWT token generation and validation (RS256 algorithm) - Role-based access control (RBAC) with fine-grained permissions - Session management with Redis-backed storage - API key generation and validation for service accounts - MFA enforcement for administrative operations **Authentication Flow**: ``` Client Request \u2192 API Gateway \u2192 Auth Service \u2502 \u251c\u2500 Validate credentials \u251c\u2500 Check MFA if required \u251c\u2500 Generate JWT (15min expiry) \u251c\u2500 Generate refresh token (7 days) \u2514\u2500 Store session in Redis ``` **Token Structure**: - Access Token: JWT with 15-minute expiry - Refresh Token: Opaque token with 7-day expiry, stored in PostgreSQL - Claims: user_id, email, roles[], permissions[], tenant_id **Security Features**: - Password hashing: Argon2id with 64MB memory, 4 iterations - Token rotation on refresh to prevent replay attacks - Brute force protection: 5 failed attempts \u2192 15-minute lockout - Secrets stored in HashiCorp Vault, rotated every 90 days **Performance Targets**: - Token validation: < 10ms (P99) - Token generation: < 50ms (P99) - Throughput: 5,000 RPS for validation operations --- ### Workflow Engine **Purpose**: Core orchestration service that executes workflow definitions, manages state transitions, and coordinates task execution across distributed systems.",
              "score": null
            }
          ],
          "latency_ms": 12.24701299724984,
          "query_id": "realistic_010",
          "dimension": "casual",
          "query": "RPO RTO"
        },
        {
          "key_facts": [
            "RPO (Recovery Point Objective): 1 hour",
            "RTO (Recovery Time Objective): 4 hours"
          ],
          "found_facts": [],
          "missed_facts": [
            "RPO (Recovery Point Objective): 1 hour",
            "RTO (Recovery Time Objective): 4 hours"
          ],
          "coverage": 0.0,
          "expected_docs": [
            "architecture_overview",
            "deployment_guide"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_10",
              "content": "database nc -zv cloudflow-db.internal.company.com 5432 # Trace route traceroute api.cloudflow.io # Capture packets tcpdump -i any -w /tmp/capture.pcap port 5432 ``` --- ## Escalation Procedures ### Severity Levels CloudFlow incidents are classified into four severity levels: #### SEV-1: Critical (P1) - **Definition:** Complete service outage or severe degradation affecting all users - **Examples:** - API returns 5xx errors for > 5 minutes - Database completely unavailable - Authentication system down - Data loss or corruption - **Response Time:** Immediate (< 15 minutes) - **Escalation:** Page on-call engineer immediately #### SEV-2: High (P2) - **Definition:** Major functionality impaired affecting multiple users - **Examples:** - Workflow execution success rate < 90% - Significant performance degradation (p95 latency > 10s) - Rate limiting affecting large customer segment - **Response Time:** < 1 hour - **Escalation:** Create incident ticket and notify on-call #### SEV-3: Medium (P3) - **Definition:** Partial functionality degraded affecting some users - **Examples:** - Intermittent failures for specific workflow types - Minor performance issues - Non-critical feature unavailable - **Response Time:** < 4 hours - **Escalation:** Create ticket, normal business hours support #### SEV-4: Low (P4) - **Definition:** Minor issues with minimal user impact - **Examples:** - Cosmetic issues - Documentation errors - Feature requests - **Response Time:** < 2 business days - **Escalation:** Standard support ticket ### Escalation Steps #### Step 1: Initial Assessment (0-5 minutes) ```bash # Run health check cloudflow health check --comprehensive # Check status page curl https://status.cloudflow.io/api/v1/status.json # Review recent changes cloudflow audit log --last 2h --event-type \"deployment,configuration\" # Check metrics dashboard cloudflow metrics dashboard --incident-mode ``` #### Step 2: Gather Information (5-15 minutes) Create incident document with: - Incident timestamp and duration - Affected services and endpoints - Error rates and user impact - Recent changes or deployments - Relevant log excerpts - Correlation IDs for failed requests ```bash # Generate incident report cloudflow debug incident-report \\ --start \"2026-01-24T10:30:00Z\" \\ --end \"2026-01-24T11:00:00Z\" \\ --output incident-report.md # Capture system snapshot cloudflow debug snapshot --output snapshot-$(date +%Y%m%d-%H%M%S).tar.gz ``` #### Step 3: Escalate Based on Severity **For SEV-1 (Critical):** ```bash # Page on-call engineer cloudflow incident create \\ --severity SEV-1 \\ --title \"Complete API outage\" \\ --description \"All API requests returning 503\" \\ --page-oncall # Notify status page cloudflow status update \\ --status \"major_outage\" \\ --message \"We are investigating a complete service",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_0",
              "content": "# CloudFlow Platform - System Architecture Overview **Document Version:** 2.3.1 **Last Updated:** January 15, 2026 **Owner:** Platform Architecture Team **Status:** Production ## Executive Summary CloudFlow is a distributed, cloud-native workflow automation platform designed to orchestrate complex business processes at scale. The platform processes over 2.5 million workflow executions daily with an average P99 latency of 180ms for API operations and 4.2 seconds for workflow execution. This document provides a comprehensive overview of the system architecture, including microservices design, data flow patterns, infrastructure decisions, and operational considerations. ## Table of Contents 1. [High-Level Architecture](#high-level-architecture) 2. [Microservices Breakdown](#microservices-breakdown) 3. [Data Flow Architecture](#data-flow-architecture) 4. [Database Architecture](#database-architecture) 5. [Message Queue Patterns](#message-queue-patterns) 6. [Caching Strategy](#caching-strategy) 7. [Security Architecture](#security-architecture) 8. [Performance Characteristics](#performance-characteristics) 9. [Disaster Recovery](#disaster-recovery) --- ## High-Level Architecture CloudFlow follows a microservices architecture pattern deployed across multiple availability zones in AWS. The platform is designed with the following core principles: - **Scalability**: Horizontal scaling for all services with auto-scaling groups - **Resilience**: Circuit breakers, bulkheads, and graceful degradation - **Observability**: Distributed tracing, centralized logging, and comprehensive metrics - **Security**: Zero-trust network architecture with mTLS encryption ### Architecture Diagram (Conceptual) ``` \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 Load Balancer (ALB) \u2502 \u2502 (TLS Termination - 443) \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u25bc \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 API Gateway Layer \u2502 \u2502 (Rate Limiting, Auth, Request Routing) \u2502 \u2514\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u2502 \u2502 \u2502 \u2502 \u25bc \u25bc \u25bc \u25bc \u25bc \u250c\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502Auth\u2502 \u2502Workflow\u2502 \u2502Scheduler\u2502 \u2502Notification\u2502 \u2502User Service\u2502 \u2502Svc \u2502 \u2502 Engine \u2502 \u2502 Service \u2502 \u2502 Service \u2502 \u2502 \u2502 \u2514\u2500\u252c\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u2502 \u2502 \u2502 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 \u2502 \u2502 \u25bc \u25bc \u25bc \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502PostgreSQL\u2502 \u2502 Redis \u2502 \u2502 Kafka \u2502 \u2502 Cluster \u2502 \u2502 Cluster \u2502 \u2502 Cluster \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 ``` ### Technology Stack - **Runtime**: Node.js 20.x (API Gateway, Workflow Engine), Go 1.21 (Auth Service, Scheduler) - **Container Orchestration**: Kubernetes 1.28 (EKS) - **Service Mesh**: Istio 1.20 for service-to-service communication - **Databases**: PostgreSQL 15.4, Redis 7.2 - **Message Broker**: Apache Kafka 3.6 - **Monitoring**: Prometheus, Grafana, Jaeger for distributed tracing - **Secrets Management**: HashiCorp Vault 1.15 --- ## Microservices Breakdown ### API Gateway **Purpose**: Single entry point for all client requests, providing authentication, rate limiting, request routing, and protocol translation.",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_12",
              "content": "Resume normal operations - Recovery time: 1-3 hours depending on data volume - Data loss: None if corruption detected quickly ### Testing & Validation **DR Drill Schedule**: - Monthly: Automated failover test (single AZ failure simulation) - Quarterly: Full DR region failover (non-production hours) - Annually: Complete disaster simulation with all stakeholders **Last DR Test Results** (Dec 15, 2025): - Scenario: Full region failover - Actual RTO: 2 hours 23 minutes (target: 4 hours) - Actual RPO: 42 minutes (target: 1 hour) - Issues identified: DNS propagation slower than expected (resolved) - Success criteria: Met all recovery objectives ### Business Continuity **Communication Plan**: - Status page: status.cloudflow.com (updated every 15 minutes during incident) - Customer notifications: Email + SMS for all P1 incidents - Internal escalation: PagerDuty \u2192 Incident Commander \u2192 Engineering Manager \u2192 CTO **Data Retention Policy**: - Active data: 12 months in hot storage (PostgreSQL) - Archived data: 7 years in cold storage (S3 Glacier) - Audit logs: 7 years (compliance requirement) - Backup retention: 30 days standard, 90 days monthly snapshots --- ## Appendix ### Service Dependency Matrix | Service | Depends On | Critical Path | |---------|-----------|---------------| | API Gateway | Auth Service, Redis | Yes | | Auth Service | PostgreSQL, Redis, Vault | Yes | | Workflow Engine | PostgreSQL, Kafka, Redis | Yes | | Scheduler | PostgreSQL, Redis, Kafka | No | | Notification Service | Kafka, PostgreSQL, SendGrid, Twilio | No | ### Capacity Planning **Current Capacity** (Jan 2026): - Daily workflow executions: 2.5M - Active users: 150,000 - API requests per day: 50M - Database size: 1.2TB - Kafka throughput: 100K msg/sec peak **6-Month Projection** (Jul 2026): - Daily workflow executions: 4M (+60%) - Active users: 225,000 (+50%) - API requests per day: 80M (+60%) - Database size: 2TB (+67%) - Required scaling: +30% compute, +50% storage ### Contact Information - **Architecture Team**: architecture@cloudflow.internal - **On-Call Engineer**: PagerDuty escalation - **Security Team**: security@cloudflow.internal - **Documentation**: https://wiki.cloudflow.internal/architecture --- **Document Revision History**: - v2.3.1 (Jan 15, 2026): Updated performance metrics, added DR test results - v2.3.0 (Dec 1, 2025): Added multi-AZ deployment details - v2.2.0 (Oct 15, 2025): Security architecture overhaul - v2.1.0 (Sep 1, 2025): Initial Kafka migration documentation - v2.0.0 (Jul 1, 2025): Complete microservices rewrite",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_4",
              "content": "**Implement connection pooling optimization:** ```bash # Use PgBouncer for connection pooling kubectl apply -f cloudflow-pgbouncer.yaml # Configure CloudFlow to use PgBouncer cloudflow config set db.host pgbouncer.cloudflow.svc.cluster.local cloudflow config set db.port 6432 cloudflow config set db.pool.mode transaction ``` 2. **Add read replicas:** ```bash # Route read-only queries to replicas cloudflow db replicas add --count 2 cloudflow config set db.read_replicas \"replica-1:5432,replica-2:5432\" cloudflow config set db.read_write_split true ``` ### Connection Timeout Errors #### Error Messages - `connection timeout after 30000ms` - `could not connect to database server at 10.0.2.45:5432` - `database server unreachable` #### Troubleshooting Steps ```bash # Test network connectivity telnet cloudflow-db.internal.company.com 5432 # Check DNS resolution nslookup cloudflow-db.internal.company.com # Verify database is accepting connections pg_isready -h cloudflow-db.internal.company.com -p 5432 -U cloudflow # Check firewall rules sudo iptables -L -n | grep 5432 # Test from CloudFlow pod network kubectl run -n cloudflow debug-pod --rm -i --tty \\ --image=postgres:14 -- \\ psql -h cloudflow-db.internal.company.com -U cloudflow -d cloudflow_production # Review database logs for connection rejections kubectl logs -n cloudflow statefulset/cloudflow-db --tail=100 | \\ grep -i \"connection\\|reject\\|authentication\" ``` ### Maximum Connection Limit (100) Reached This is a hard limit in CloudFlow's database tier. #### Permanent Solutions **Option 1: Upgrade database tier** ```bash # Check available tiers cloudflow db tiers list # Upgrade to higher tier (supports 200 connections) cloudflow db upgrade --tier standard-plus --confirm # Monitor migration progress cloudflow db migration status ``` **Option 2: Implement aggressive connection reuse** ```bash # Reduce connection lifetime cloudflow config set db.pool.max_lifetime 600 # 10 minutes # Enable prepared statement caching cloudflow config set db.prepared_statements.cache true cloudflow config set db.prepared_statements.max_size 250 # Reduce idle connection timeout cloudflow config set db.pool.idle_timeout 120 # 2 minutes ``` --- ## Workflow Execution Failures ### Timeout Errors (3600 second limit) #### Error Message ``` WorkflowExecutionError: Workflow exceeded maximum execution time of 3600 seconds Status: TIMEOUT Workflow ID: wf_9k2n4m8p1q Execution ID: exec_7h3j6k9m2n ``` #### Analysis ```bash # Get workflow execution details cloudflow workflows executions get exec_7h3j6k9m2n --verbose # Check step-by-step breakdown cloudflow workflows executions steps exec_7h3j6k9m2n # Sample output: # Step 1: data_ingestion - Duration: 245s - Status: SUCCESS # Step 2: data_validation - Duration: 123s - Status: SUCCESS # Step 3: data_transformation - Duration: 3189s - Status: TIMEOUT # Step 4: data_export - Duration: 0s - Status: SKIPPED # Identify bottleneck step cloudflow workflows analyze",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_0",
              "content": "# CloudFlow Platform Troubleshooting Guide **Version:** 3.2.1 **Last Updated:** January 2026 **Audience:** Platform Engineers, DevOps, Support Teams ## Table of Contents 1. [Overview](#overview) 2. [Authentication & Authorization Issues](#authentication--authorization-issues) 3. [Performance Problems](#performance-problems) 4. [Database Connection Issues](#database-connection-issues) 5. [Workflow Execution Failures](#workflow-execution-failures) 6. [Rate Limiting & Throttling](#rate-limiting--throttling) 7. [Log Analysis & Debugging](#log-analysis--debugging) 8. [Escalation Procedures](#escalation-procedures) --- ## Overview This guide provides comprehensive troubleshooting steps for common CloudFlow platform issues encountered in production environments. Each section includes error symptoms, root cause analysis, resolution steps, and preventive measures. ### Quick Diagnostic Checklist Before diving into specific issues, perform these initial checks: - Verify service health: `cloudflow status --all` - Check API connectivity: `curl -I https://api.cloudflow.io/health` - Review recent deployments: `kubectl get deployments -n cloudflow --sort-by=.metadata.creationTimestamp` - Inspect platform metrics: `cloudflow metrics --last 1h` --- ## Authentication & Authorization Issues ### 401 Unauthorized Errors #### Symptoms - API requests return `401 Unauthorized` - Error message: `Authentication credentials were not provided or are invalid` - Frontend displays \"Session expired\" message #### Common Causes **1. Token Expiration** CloudFlow access tokens expire after 3600 seconds (1 hour) by default. Refresh tokens are valid for 30 days. **Verification:** ```bash # Decode JWT to check expiration echo $CF_ACCESS_TOKEN | cut -d'.' -f2 | base64 -d | jq '.exp' # Compare with current time date +%s ``` **Resolution:** ```bash # Refresh the access token curl -X POST https://api.cloudflow.io/auth/refresh \\ -H \"Content-Type: application/json\" \\ -d '{\"refresh_token\": \"'$CF_REFRESH_TOKEN'\"}' # Update environment variable export CF_ACCESS_TOKEN=\"<new_token>\" ``` **2. Invalid JWT Signature** Error: `JWT signature verification failed` **Causes:** - Token was modified or corrupted - Using wrong signing key - Token generated with different secret **Resolution:** ```bash # Validate token structure cloudflow auth validate --token $CF_ACCESS_TOKEN # Generate new token cloudflow auth login --username <user> --password <pass> # For service accounts cloudflow auth service-account --create --name <sa-name> --scopes \"workflow:read,workflow:write\" ``` **3. Clock Skew Issues** JWT validation fails when system clocks are out of sync (tolerance: \u00b1300 seconds).",
              "score": null
            }
          ],
          "latency_ms": 12.871497998276027,
          "query_id": "realistic_010",
          "dimension": "contextual",
          "query": "For our business continuity plan, what are CloudFlow's recovery objectives?"
        },
        {
          "key_facts": [
            "RPO (Recovery Point Objective): 1 hour",
            "RTO (Recovery Time Objective): 4 hours"
          ],
          "found_facts": [],
          "missed_facts": [
            "RPO (Recovery Point Objective): 1 hour",
            "RTO (Recovery Time Objective): 4 hours"
          ],
          "coverage": 0.0,
          "expected_docs": [
            "architecture_overview",
            "deployment_guide"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "user_guide",
              "chunk_id": "user_guide_fix_7",
              "content": "Handle Errors Gracefully Always implement error handling for external API calls and database operations: ```yaml - id: fetch_data action: http_request config: url: \"https://api.example.com/data\" retry: max_attempts: 3 backoff_type: exponential on_error: - id: log_error action: database_query config: query: \"INSERT INTO error_log (workflow_id, error) VALUES ($1, $2)\" parameters: - \"{{workflow.id}}\" - \"{{error.message}}\" ``` ### 3. Use Secrets for Sensitive Data Never hardcode API keys, passwords, or tokens in workflows: **Bad:** ```yaml headers: Authorization: \"Bearer sk_live_abc123xyz789\" ``` **Good:** ```yaml headers: Authorization: \"Bearer {{secrets.API_TOKEN}}\" ``` Store secrets in **Settings** > **Secrets** with encryption at rest. ### 4. Validate Input Data Always validate trigger data before processing: ```yaml - id: validate_input action: javascript code: | const required_fields = ['email', 'name', 'order_id']; for (const field of required_fields) { if (!input[field]) { throw new Error(`Missing required field: ${field}`); } } // Validate email format const email_regex = /^[^\\s@]+@[^\\s@]+\\.[^\\s@]+$/; if (!email_regex.test(input.email)) { throw new Error('Invalid email format'); } return { validated: true }; ``` ### 5. Use Idempotency Keys For operations that shouldn't be repeated (payments, record creation), use idempotency keys: ```yaml - id: create_charge action: http_request config: url: \"https://api.stripe.com/v1/charges\" method: POST headers: Idempotency-Key: \"{{workflow.id}}-{{execution.id}}\" body: amount: \"{{amount}}\" ``` ### 6. Monitor and Log Add logging steps for important workflow milestones: ```yaml - id: log_start action: database_query config: query: \"INSERT INTO workflow_audit (execution_id, step, timestamp) VALUES ($1, $2, $3)\" parameters: - \"{{execution.id}}\" - \"workflow_started\" - \"{{now()}}\" ``` ### 7. Keep Workflows Modular Break complex workflows into smaller, reusable components: - Use sub-workflows for repeated logic - Trigger child workflows via webhooks - Share common configurations via templates ### 8. Test Thoroughly Before activating a workflow: 1. Use test mode with sample data 2. Verify all actions execute correctly 3. Test error handling paths 4. Review execution logs 5. Start with a limited scope (e.g., test channel, small dataset) ### 9. Document Your Workflows Add descriptions to workflows and steps: ```yaml workflow: name: \"Daily Sales Report\" description: | Generates a daily sales report and distributes it to the sales team. Runs at 8:00 AM EST Monday-Friday. Data source: PostgreSQL sales database Recipients: sales-team@company.com steps: - id: fetch_sales name: \"Fetch Yesterday's Sales\" description: \"Query database for all completed orders from previous day\" ``` ### 10.",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_6",
              "content": "Data Validation Errors** ``` ValidationError: Field 'customer_id' is required but missing in 234 records ``` **Resolution:** ```bash # Add data quality checks cloudflow workflows update wf_9k2n4m8p1q \\ --step data_ingestion \\ --add-validator required_fields \\ --validator-config '{\"fields\": [\"customer_id\", \"timestamp\", \"amount\"]}' # Configure error handling cloudflow workflows update wf_9k2n4m8p1q \\ --step data_validation \\ --on-error continue \\ --error-threshold 5% # Fail if > 5% of records invalid ``` **2. External API Failures** ``` ExternalAPIError: API request to https://partner-api.example.com failed with status 502 ``` **Resolution:** ```bash # Add circuit breaker cloudflow workflows update wf_9k2n4m8p1q \\ --step external_api_call \\ --circuit-breaker-enabled true \\ --circuit-breaker-threshold 5 \\ --circuit-breaker-timeout 30000 # Configure fallback behavior cloudflow workflows update wf_9k2n4m8p1q \\ --step external_api_call \\ --fallback-action use_cached_data \\ --cache-ttl 3600 ``` --- ## Rate Limiting & Throttling ### 429 Too Many Requests #### Error Response ```json { \"error\": \"rate_limit_exceeded\", \"message\": \"API rate limit exceeded. Retry after 45 seconds.\", \"status\": 429, \"headers\": { \"X-RateLimit-Limit\": \"1000\", \"X-RateLimit-Remaining\": \"0\", \"X-RateLimit-Reset\": \"1706112345\", \"Retry-After\": \"45\" } } ``` #### Rate Limit Tiers CloudFlow enforces the following rate limits per workspace: | Tier | Requests/Minute | Requests/Hour | Concurrent Workflows | |------|-----------------|---------------|----------------------| | Free | 60 | 1,000 | 5 | | Standard | 1,000 | 50,000 | 50 | | Premium | 5,000 | 250,000 | 200 | | Enterprise | Custom | Custom | Unlimited | #### Checking Rate Limit Status ```bash # Check current rate limit status curl -I https://api.cloudflow.io/api/v1/workflows \\ -H \"Authorization: Bearer $CF_ACCESS_TOKEN\" # Extract rate limit headers curl -s -I https://api.cloudflow.io/api/v1/workflows \\ -H \"Authorization: Bearer $CF_ACCESS_TOKEN\" | \\ grep -i \"x-ratelimit\" # Output: # X-RateLimit-Limit: 1000 # X-RateLimit-Remaining: 247 # X-RateLimit-Reset: 1706112400 # Monitor rate limit usage cloudflow metrics query \\ --metric rate_limit_remaining \\ --workspace ws_abc123 \\ --last 1h \\ --interval 1m ``` #### Handling Rate Limits in Code **Python example with retry logic:** ```python import time import requests def cloudflow_api_call_with_retry(url, headers, max_retries=3): for attempt in range(max_retries): response = requests.get(url, headers=headers) if response.status_code == 429: retry_after = int(response.headers.get('Retry-After', 60)) print(f\"Rate limited.",
              "score": null
            },
            {
              "doc_id": "user_guide",
              "chunk_id": "user_guide_fix_6",
              "content": "Filter by error type, date range, or execution ID 3. Click an execution to view full details 4. Click **\"Retry\"** to reprocess with the same input data ### Error Notifications Get notified when workflows fail: ```yaml workflow: notifications: on_failure: - type: email to: \"ops-team@company.com\" - type: slack channel: \"#alerts\" message: \"Workflow {{workflow.name}} failed: {{error.message}}\" on_success_after_retry: - type: slack channel: \"#monitoring\" message: \"Workflow recovered after {{error.attempts}} attempts\" ``` ## Workflow Limits CloudFlow enforces the following limits to ensure platform stability and performance: ### Steps Per Workflow - **Maximum**: 50 steps per workflow - **Recommendation**: Keep workflows focused and modular. If you need more steps, consider splitting into multiple workflows connected via webhooks. ### Execution Timeout - **Default**: 3600 seconds (60 minutes) - **Behavior**: Workflows exceeding this timeout are automatically terminated - **Custom Timeouts**: Enterprise plans can request custom timeout limits **Setting Step-Level Timeouts:** ```yaml - id: long_running_task action: http_request config: url: \"https://api.example.com/process\" timeout: 300 # 5 minutes for this specific step ``` ### Execution Limits - **Maximum**: 1000 executions per day (per workflow) - **Rate Limiting**: 100 concurrent executions per workflow - **Burst Limit**: 10 executions per second **What happens when limits are reached:** - New executions are queued automatically - Webhook triggers return HTTP 429 (Too Many Requests) - Scheduled executions are skipped (logged in audit trail) - Email notifications sent to workflow owner **Monitoring Usage:** View real-time metrics in your workflow dashboard: - Executions today: 847 / 1000 - Average duration: 12.3s - Success rate: 98.2% - Current queue depth: 3 ### Data Limits - **Maximum request/response size**: 10MB per action - **Maximum execution payload**: 50MB total - **Variable value size**: 1MB per variable ### Enterprise Plan Limits Enterprise customers can request increased limits: - Up to 100 steps per workflow - Up to 10,000 executions per day - Up to 7200 second timeout (2 hours) - Priority execution queue - Dedicated capacity allocation Contact sales@cloudflow.io for Enterprise pricing and custom limits. ## Best Practices Follow these best practices to build reliable, maintainable workflows: ### 1. Use Descriptive Names **Good:** - Workflow: \"Sync Customer Data from Salesforce to Database\" - Step: \"validate_customer_email\" **Bad:** - Workflow: \"Workflow 1\" - Step: \"step3\" ### 2.",
              "score": null
            },
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_1",
              "content": "## Rate Limiting To ensure fair usage and system stability, CloudFlow enforces rate limits on all API endpoints. **Default Limits:** - 100 requests per minute per authenticated user - 20 requests per minute for unauthenticated requests - Burst allowance: 150 requests in a 10-second window ### Rate Limit Headers Every API response includes rate limit information: ``` X-RateLimit-Limit: 100 X-RateLimit-Remaining: 87 X-RateLimit-Reset: 1640995200 ``` When you exceed the rate limit, you'll receive a `429 Too Many Requests` response: ```json { \"error\": { \"code\": \"rate_limit_exceeded\", \"message\": \"Rate limit exceeded. Please retry after 42 seconds.\", \"retry_after\": 42 } } ``` **Best Practices:** - Monitor `X-RateLimit-Remaining` header values - Implement exponential backoff when receiving 429 responses - Cache responses when appropriate to reduce API calls - Consider upgrading to Enterprise tier for higher limits (1000 req/min) ## Pagination List endpoints return paginated results to optimize performance. Use `limit` and `offset` parameters to navigate through result sets. **Parameters:** - `limit`: Number of items per page (default: 20, max: 100) - `offset`: Number of items to skip (default: 0) **Example Request:** ```bash curl -H \"X-API-Key: YOUR_API_KEY\" \\ \"https://api.cloudflow.io/v2/workflows?limit=50&offset=100\" ``` **Response Structure:** ```json { \"data\": [...], \"pagination\": { \"total\": 347, \"limit\": 50, \"offset\": 100, \"has_more\": true } } ``` **Python Example:** ```python import requests def fetch_all_workflows(api_key): base_url = \"https://api.cloudflow.io/v2/workflows\" headers = {\"X-API-Key\": api_key} all_workflows = [] offset = 0 limit = 100 while True: response = requests.get( base_url, headers=headers, params={\"limit\": limit, \"offset\": offset} ) data = response.json() all_workflows.extend(data['data']) if not data['pagination']['has_more']: break offset += limit return all_workflows ``` ## Endpoints ### Workflows #### List Workflows Retrieve a paginated list of all workflows in your account. **Endpoint:** `GET /workflows` **Query Parameters:** - `status` (optional): Filter by status (`active`, `paused`, `archived`) - `created_after` (optional): ISO 8601 timestamp - `limit` (optional): Items per page (max 100) - `offset` (optional): Pagination offset **Example Request:** ```bash curl -H \"X-API-Key: YOUR_API_KEY\" \\ \"https://api.cloudflow.io/v2/workflows?status=active&limit=25\" ``` **Example Response (200 OK):** ```json { \"data\": [ { \"id\": \"wf_8x7k2m9p\", \"name\": \"Data Processing Pipeline\", \"description\": \"Processes customer data every hour\", \"status\": \"active\", \"trigger\": { \"type\": \"schedule\", \"cron\": \"0 * * * *\" }, \"steps\": 12, \"created_at\": \"2026-01-15T10:30:00Z\", \"updated_at\": \"2026-01-23T14:22:00Z\", \"last_run\": \"2026-01-24T09:00:00Z\" } ], \"pagination\": { \"total\": 1, \"limit\": 25, \"offset\": 0, \"has_more\": false } } ``` #### Create Workflow Create a new workflow with specified configuration.",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_12",
              "content": "Resume normal operations - Recovery time: 1-3 hours depending on data volume - Data loss: None if corruption detected quickly ### Testing & Validation **DR Drill Schedule**: - Monthly: Automated failover test (single AZ failure simulation) - Quarterly: Full DR region failover (non-production hours) - Annually: Complete disaster simulation with all stakeholders **Last DR Test Results** (Dec 15, 2025): - Scenario: Full region failover - Actual RTO: 2 hours 23 minutes (target: 4 hours) - Actual RPO: 42 minutes (target: 1 hour) - Issues identified: DNS propagation slower than expected (resolved) - Success criteria: Met all recovery objectives ### Business Continuity **Communication Plan**: - Status page: status.cloudflow.com (updated every 15 minutes during incident) - Customer notifications: Email + SMS for all P1 incidents - Internal escalation: PagerDuty \u2192 Incident Commander \u2192 Engineering Manager \u2192 CTO **Data Retention Policy**: - Active data: 12 months in hot storage (PostgreSQL) - Archived data: 7 years in cold storage (S3 Glacier) - Audit logs: 7 years (compliance requirement) - Backup retention: 30 days standard, 90 days monthly snapshots --- ## Appendix ### Service Dependency Matrix | Service | Depends On | Critical Path | |---------|-----------|---------------| | API Gateway | Auth Service, Redis | Yes | | Auth Service | PostgreSQL, Redis, Vault | Yes | | Workflow Engine | PostgreSQL, Kafka, Redis | Yes | | Scheduler | PostgreSQL, Redis, Kafka | No | | Notification Service | Kafka, PostgreSQL, SendGrid, Twilio | No | ### Capacity Planning **Current Capacity** (Jan 2026): - Daily workflow executions: 2.5M - Active users: 150,000 - API requests per day: 50M - Database size: 1.2TB - Kafka throughput: 100K msg/sec peak **6-Month Projection** (Jul 2026): - Daily workflow executions: 4M (+60%) - Active users: 225,000 (+50%) - API requests per day: 80M (+60%) - Database size: 2TB (+67%) - Required scaling: +30% compute, +50% storage ### Contact Information - **Architecture Team**: architecture@cloudflow.internal - **On-Call Engineer**: PagerDuty escalation - **Security Team**: security@cloudflow.internal - **Documentation**: https://wiki.cloudflow.internal/architecture --- **Document Revision History**: - v2.3.1 (Jan 15, 2026): Updated performance metrics, added DR test results - v2.3.0 (Dec 1, 2025): Added multi-AZ deployment details - v2.2.0 (Oct 15, 2025): Security architecture overhaul - v2.1.0 (Sep 1, 2025): Initial Kafka migration documentation - v2.0.0 (Jul 1, 2025): Complete microservices rewrite",
              "score": null
            }
          ],
          "latency_ms": 12.70759200269822,
          "query_id": "realistic_010",
          "dimension": "negation",
          "query": "Why can't we guarantee zero data loss?"
        },
        {
          "key_facts": [
            "3600 seconds",
            "exceeded maximum execution time of 3600 seconds"
          ],
          "found_facts": [
            "3600 seconds"
          ],
          "missed_facts": [
            "exceeded maximum execution time of 3600 seconds"
          ],
          "coverage": 0.5,
          "expected_docs": [
            "troubleshooting_guide",
            "user_guide"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_7",
              "content": "records: 500 - Session timeout: 30 seconds --- ## Message Queue Patterns ### Pub/Sub Pattern Used for broadcasting events to multiple interested consumers: ``` Workflow Engine publishes workflow.execution.completed \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u25bc \u25bc \u25bc \u25bc Analytics Notification Audit Logger Billing Service Service Service Service ``` Each service maintains its own consumer group and processes events independently. Failures in one consumer don't affect others. ### Request-Reply Pattern Synchronous communication over async messaging (used sparingly): ``` API Gateway publishes request \u2192 Kafka (reply-to: temp-queue-123) \u2502 \u25bc Service processes request \u2502 \u25bc Service publishes response \u2192 temp-queue-123 \u2502 \u25bc API Gateway receives response ``` Timeout: 5 seconds, fallback to direct HTTP call if no response. ### Saga Pattern Distributed transaction management for multi-service workflows: ``` Step 1: Create Order \u2192 SUCCESS \u2192 Step 2: Reserve Inventory \u2502 FAILURE \u2502 \u25bc Compensating Transaction \u2502 \u25bc Cancel Order (Rollback) ``` Orchestrated by Workflow Engine with compensation logic defined in workflow DSL. ### Dead Letter Queue (DLQ) Failed messages after all retry attempts are routed to DLQ: ``` Message processing fails (3 retries with exponential backoff) \u2502 \u25bc Publish to dead-letter-queue topic \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u25bc \u25bc Alert to PagerDuty Store in PostgreSQL for analysis \u2502 \u2502 \u25bc \u25bc Manual investigation Automated pattern detection ``` DLQ Processing SLA: < 4 hours for critical events, < 24 hours for non-critical. --- ## Caching Strategy ### Cache-Aside Pattern (Lazy Loading) Primary caching pattern used across all services: ``` 1. Application checks cache (Redis GET) 2. If HIT \u2192 Return cached data 3. If MISS: a. Query database (PostgreSQL) b. Store result in cache with TTL c. Return data to application ``` **Implementation Example** (Workflow Definition): ```javascript async function getWorkflowDefinition(workflowId) { const cacheKey = `workflow:def:${workflowId}`; // Step 1: Check cache let workflow = await redis.get(cacheKey); if (workflow) { metrics.increment('cache.hit.workflow_def'); return JSON.parse(workflow); } // Step 2: Cache miss - query database metrics.increment('cache.miss.workflow_def'); workflow = await db.query( 'SELECT * FROM workflows WHERE id = $1', [workflowId] ); // Step 3: Store in cache (1 hour TTL) await redis.setex(cacheKey, 3600, JSON.stringify(workflow)); return workflow; } ``` ### Write-Through Pattern Used for critical data where cache consistency is paramount: ``` 1. Application writes to database 2. Database transaction commits 3. Application updates cache 4. Return success to client ``` Applied to: User profiles, authentication sessions, system configuration.",
              "score": null
            },
            {
              "doc_id": "user_guide",
              "chunk_id": "user_guide_fix_6",
              "content": "Filter by error type, date range, or execution ID 3. Click an execution to view full details 4. Click **\"Retry\"** to reprocess with the same input data ### Error Notifications Get notified when workflows fail: ```yaml workflow: notifications: on_failure: - type: email to: \"ops-team@company.com\" - type: slack channel: \"#alerts\" message: \"Workflow {{workflow.name}} failed: {{error.message}}\" on_success_after_retry: - type: slack channel: \"#monitoring\" message: \"Workflow recovered after {{error.attempts}} attempts\" ``` ## Workflow Limits CloudFlow enforces the following limits to ensure platform stability and performance: ### Steps Per Workflow - **Maximum**: 50 steps per workflow - **Recommendation**: Keep workflows focused and modular. If you need more steps, consider splitting into multiple workflows connected via webhooks. ### Execution Timeout - **Default**: 3600 seconds (60 minutes) - **Behavior**: Workflows exceeding this timeout are automatically terminated - **Custom Timeouts**: Enterprise plans can request custom timeout limits **Setting Step-Level Timeouts:** ```yaml - id: long_running_task action: http_request config: url: \"https://api.example.com/process\" timeout: 300 # 5 minutes for this specific step ``` ### Execution Limits - **Maximum**: 1000 executions per day (per workflow) - **Rate Limiting**: 100 concurrent executions per workflow - **Burst Limit**: 10 executions per second **What happens when limits are reached:** - New executions are queued automatically - Webhook triggers return HTTP 429 (Too Many Requests) - Scheduled executions are skipped (logged in audit trail) - Email notifications sent to workflow owner **Monitoring Usage:** View real-time metrics in your workflow dashboard: - Executions today: 847 / 1000 - Average duration: 12.3s - Success rate: 98.2% - Current queue depth: 3 ### Data Limits - **Maximum request/response size**: 10MB per action - **Maximum execution payload**: 50MB total - **Variable value size**: 1MB per variable ### Enterprise Plan Limits Enterprise customers can request increased limits: - Up to 100 steps per workflow - Up to 10,000 executions per day - Up to 7200 second timeout (2 hours) - Priority execution queue - Dedicated capacity allocation Contact sales@cloudflow.io for Enterprise pricing and custom limits. ## Best Practices Follow these best practices to build reliable, maintainable workflows: ### 1. Use Descriptive Names **Good:** - Workflow: \"Sync Customer Data from Salesforce to Database\" - Step: \"validate_customer_email\" **Bad:** - Workflow: \"Workflow 1\" - Step: \"step3\" ### 2.",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_3",
              "content": "Workflow Context Accumulation** Large workflow executions may accumulate state in memory. **Solution:** ```bash # Configure context cleanup cloudflow config set workflow.context.max_size_mb 100 cloudflow config set workflow.context.cleanup_threshold 0.8 # Enable context persistence to disk cloudflow config set workflow.context.persistence.enabled true cloudflow config set workflow.context.persistence.backend s3 ``` **2. Connection Pool Leaks** **Diagnosis:** ```bash # Check active connections cloudflow db connections --verbose # Expected output: # Active: 45/100 # Idle: 23 # Waiting: 2 # Average age: 245s ``` **Resolution:** ```bash # Adjust connection pool settings cloudflow config set db.pool.max_connections 100 cloudflow config set db.pool.min_connections 10 cloudflow config set db.pool.idle_timeout 300 cloudflow config set db.pool.max_lifetime 1800 # Force connection pool reset cloudflow db pool reset --confirm ``` **3. Event Stream Buffers** Unbounded event buffers can cause memory exhaustion. ```bash # Configure event buffer limits cloudflow config set events.buffer.max_size 10000 cloudflow config set events.buffer.overflow_strategy drop_oldest # Enable event streaming to external sink cloudflow config set events.sink.type kafka cloudflow config set events.sink.kafka.brokers \"kafka-1:9092,kafka-2:9092\" cloudflow config set events.sink.kafka.topic cloudflow-events ``` --- ## Database Connection Issues ### Connection Pool Exhaustion #### Symptoms - Error: `could not obtain connection from pool within 5000ms` - Error: `connection pool exhausted (100/100 connections in use)` - API requests fail with `503 Service Unavailable` - Database CPU usage normal, but connection count at maximum #### Investigation ```bash # Check current connection pool status cloudflow db pool status --detailed # Output example: # Pool Statistics: # Total Connections: 100/100 (100%) # Active: 87 # Idle: 13 # Waiting Requests: 45 # Average Wait Time: 3420ms # Max Wait Time: 8234ms # Identify long-running queries cloudflow db queries --status running --min-duration 30000 # Check connection distribution by client SELECT application_name, state, COUNT(*) as conn_count, AVG(EXTRACT(EPOCH FROM (NOW() - state_change))) as avg_duration_sec FROM pg_stat_activity WHERE datname = 'cloudflow_production' GROUP BY application_name, state ORDER BY conn_count DESC; ``` #### Resolution **Immediate mitigation:** ```bash # Temporarily increase connection limit (requires database restart) cloudflow db config set max_connections 150 # Kill idle connections older than 5 minutes cloudflow db connections kill --idle-timeout 300 # Restart connection pool without downtime kubectl rollout restart deployment/cloudflow-api -n cloudflow kubectl rollout status deployment/cloudflow-api -n cloudflow ``` **Long-term solutions:** 1.",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_2",
              "content": "**Technology**: Node.js with TypeScript, Bull queue library **Replicas**: 16 pods (production), auto-scaling 12-24 **Resource Allocation**: 4 vCPU, 8GB RAM per pod **Key Responsibilities**: - Parse and validate workflow definitions (JSON-based DSL) - Execute workflow steps with state machine pattern - Handle retries with exponential backoff (max 3 retries, backoff: 2^n seconds) - Coordinate parallel and sequential task execution - Manage workflow state persistence and recovery - Support for conditional branching and loops - Sub-workflow invocation and composition **Workflow Execution Model**: ``` Workflow Submitted \u2192 Validation \u2192 Queue in Kafka \u2502 \u25bc Workflow Engine picks up \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u25bc \u25bc \u25bc [Task 1 Exec] [Task 2 Exec] [Task 3 Exec] \u2502 \u2502 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u25bc State Update \u2192 PostgreSQL \u2502 \u25bc Publish Event \u2192 Kafka ``` **Workflow DSL Example**: ```json { \"workflow_id\": \"customer-onboarding-v2\", \"version\": \"2.1.0\", \"steps\": [ { \"id\": \"validate-customer\", \"type\": \"http_request\", \"config\": { \"url\": \"https://validation.internal/verify\", \"method\": \"POST\", \"timeout\": 5000, \"retry\": {\"max_attempts\": 3, \"backoff\": \"exponential\"} } }, { \"id\": \"create-account\", \"type\": \"database_operation\", \"depends_on\": [\"validate-customer\"] } ] } ``` **State Management**: - Workflow states: PENDING, RUNNING, COMPLETED, FAILED, CANCELLED - Step states: QUEUED, EXECUTING, SUCCESS, FAILED, SKIPPED - State transitions stored in PostgreSQL with event sourcing pattern - Checkpointing every 10 steps for long-running workflows **Performance Targets**: - Simple workflow (3-5 steps): < 2 seconds (P99) - Complex workflow (20+ steps): < 5 seconds (P99) - Throughput: 500 concurrent workflow executions per pod --- ### Scheduler Service **Purpose**: Time-based workflow triggering system supporting cron-like schedules and one-time delayed executions.",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_5",
              "content": "exec_7h3j6k9m2n --show-bottlenecks ``` #### Solutions **1. Increase workflow timeout (if justified):** ```bash # Update workflow configuration cloudflow workflows update wf_9k2n4m8p1q \\ --timeout 7200 \\ --reason \"Large dataset processing requires extended time\" # Verify update cloudflow workflows get wf_9k2n4m8p1q | grep timeout ``` **2. Optimize slow steps:** ```bash # Enable parallel processing cloudflow workflows update wf_9k2n4m8p1q \\ --step data_transformation \\ --parallel-workers 8 \\ --batch-size 1000 # Add checkpointing for long operations cloudflow workflows update wf_9k2n4m8p1q \\ --step data_transformation \\ --enable-checkpointing \\ --checkpoint-interval 300 ``` **3. Split workflow into smaller workflows:** ```bash # Create sub-workflows cloudflow workflows create data-pipeline-part1 \\ --steps \"data_ingestion,data_validation\" \\ --timeout 1800 cloudflow workflows create data-pipeline-part2 \\ --steps \"data_transformation,data_export\" \\ --timeout 3600 \\ --trigger workflow_completed \\ --trigger-workflow data-pipeline-part1 ``` ### Retry Logic and Exponential Backoff CloudFlow implements automatic retry with exponential backoff for transient failures: - Max retries: 3 - Initial delay: 1 second - Backoff multiplier: 2 - Max delay: 60 seconds #### Retry Sequence ``` Attempt 1: Immediate Attempt 2: Wait 1s (2^0 * 1s) Attempt 3: Wait 2s (2^1 * 1s) Attempt 4: Wait 4s (2^2 * 1s) ``` #### Configuration ```bash # View current retry settings cloudflow workflows get wf_9k2n4m8p1q --format json | jq '.retry_policy' # Customize retry behavior cloudflow workflows update wf_9k2n4m8p1q \\ --retry-max-attempts 5 \\ --retry-initial-delay 2000 \\ --retry-backoff-multiplier 2 \\ --retry-max-delay 120000 \\ --retry-on-errors \"NETWORK_ERROR,TIMEOUT,RATE_LIMIT\" # Disable retry for specific step cloudflow workflows update wf_9k2n4m8p1q \\ --step data_export \\ --retry-enabled false ``` #### Monitoring Retries ```bash # List failed executions with retry information cloudflow workflows executions list \\ --status FAILED \\ --show-retries \\ --last 7d # Get retry history for specific execution cloudflow workflows executions retries exec_7h3j6k9m2n # Output: # Execution: exec_7h3j6k9m2n # Attempt 1: FAILED - NetworkError: Connection refused (delay: 0ms) # Attempt 2: FAILED - NetworkError: Connection timeout (delay: 1000ms) # Attempt 3: FAILED - NetworkError: Connection timeout (delay: 2000ms) # Attempt 4: FAILED - NetworkError: Connection timeout (delay: 4000ms) # Final Status: FAILED_AFTER_RETRIES ``` ### Workflow Step Failures #### Common Error Patterns **1.",
              "score": null
            }
          ],
          "latency_ms": 12.679550000029849,
          "query_id": "realistic_011",
          "dimension": "original",
          "query": "What is the maximum workflow execution timeout?"
        },
        {
          "key_facts": [
            "3600 seconds",
            "exceeded maximum execution time of 3600 seconds"
          ],
          "found_facts": [
            "3600 seconds"
          ],
          "missed_facts": [
            "exceeded maximum execution time of 3600 seconds"
          ],
          "coverage": 0.5,
          "expected_docs": [
            "troubleshooting_guide",
            "user_guide"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "user_guide",
              "chunk_id": "user_guide_fix_6",
              "content": "Filter by error type, date range, or execution ID 3. Click an execution to view full details 4. Click **\"Retry\"** to reprocess with the same input data ### Error Notifications Get notified when workflows fail: ```yaml workflow: notifications: on_failure: - type: email to: \"ops-team@company.com\" - type: slack channel: \"#alerts\" message: \"Workflow {{workflow.name}} failed: {{error.message}}\" on_success_after_retry: - type: slack channel: \"#monitoring\" message: \"Workflow recovered after {{error.attempts}} attempts\" ``` ## Workflow Limits CloudFlow enforces the following limits to ensure platform stability and performance: ### Steps Per Workflow - **Maximum**: 50 steps per workflow - **Recommendation**: Keep workflows focused and modular. If you need more steps, consider splitting into multiple workflows connected via webhooks. ### Execution Timeout - **Default**: 3600 seconds (60 minutes) - **Behavior**: Workflows exceeding this timeout are automatically terminated - **Custom Timeouts**: Enterprise plans can request custom timeout limits **Setting Step-Level Timeouts:** ```yaml - id: long_running_task action: http_request config: url: \"https://api.example.com/process\" timeout: 300 # 5 minutes for this specific step ``` ### Execution Limits - **Maximum**: 1000 executions per day (per workflow) - **Rate Limiting**: 100 concurrent executions per workflow - **Burst Limit**: 10 executions per second **What happens when limits are reached:** - New executions are queued automatically - Webhook triggers return HTTP 429 (Too Many Requests) - Scheduled executions are skipped (logged in audit trail) - Email notifications sent to workflow owner **Monitoring Usage:** View real-time metrics in your workflow dashboard: - Executions today: 847 / 1000 - Average duration: 12.3s - Success rate: 98.2% - Current queue depth: 3 ### Data Limits - **Maximum request/response size**: 10MB per action - **Maximum execution payload**: 50MB total - **Variable value size**: 1MB per variable ### Enterprise Plan Limits Enterprise customers can request increased limits: - Up to 100 steps per workflow - Up to 10,000 executions per day - Up to 7200 second timeout (2 hours) - Priority execution queue - Dedicated capacity allocation Contact sales@cloudflow.io for Enterprise pricing and custom limits. ## Best Practices Follow these best practices to build reliable, maintainable workflows: ### 1. Use Descriptive Names **Good:** - Workflow: \"Sync Customer Data from Salesforce to Database\" - Step: \"validate_customer_email\" **Bad:** - Workflow: \"Workflow 1\" - Step: \"step3\" ### 2.",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_5",
              "content": "exec_7h3j6k9m2n --show-bottlenecks ``` #### Solutions **1. Increase workflow timeout (if justified):** ```bash # Update workflow configuration cloudflow workflows update wf_9k2n4m8p1q \\ --timeout 7200 \\ --reason \"Large dataset processing requires extended time\" # Verify update cloudflow workflows get wf_9k2n4m8p1q | grep timeout ``` **2. Optimize slow steps:** ```bash # Enable parallel processing cloudflow workflows update wf_9k2n4m8p1q \\ --step data_transformation \\ --parallel-workers 8 \\ --batch-size 1000 # Add checkpointing for long operations cloudflow workflows update wf_9k2n4m8p1q \\ --step data_transformation \\ --enable-checkpointing \\ --checkpoint-interval 300 ``` **3. Split workflow into smaller workflows:** ```bash # Create sub-workflows cloudflow workflows create data-pipeline-part1 \\ --steps \"data_ingestion,data_validation\" \\ --timeout 1800 cloudflow workflows create data-pipeline-part2 \\ --steps \"data_transformation,data_export\" \\ --timeout 3600 \\ --trigger workflow_completed \\ --trigger-workflow data-pipeline-part1 ``` ### Retry Logic and Exponential Backoff CloudFlow implements automatic retry with exponential backoff for transient failures: - Max retries: 3 - Initial delay: 1 second - Backoff multiplier: 2 - Max delay: 60 seconds #### Retry Sequence ``` Attempt 1: Immediate Attempt 2: Wait 1s (2^0 * 1s) Attempt 3: Wait 2s (2^1 * 1s) Attempt 4: Wait 4s (2^2 * 1s) ``` #### Configuration ```bash # View current retry settings cloudflow workflows get wf_9k2n4m8p1q --format json | jq '.retry_policy' # Customize retry behavior cloudflow workflows update wf_9k2n4m8p1q \\ --retry-max-attempts 5 \\ --retry-initial-delay 2000 \\ --retry-backoff-multiplier 2 \\ --retry-max-delay 120000 \\ --retry-on-errors \"NETWORK_ERROR,TIMEOUT,RATE_LIMIT\" # Disable retry for specific step cloudflow workflows update wf_9k2n4m8p1q \\ --step data_export \\ --retry-enabled false ``` #### Monitoring Retries ```bash # List failed executions with retry information cloudflow workflows executions list \\ --status FAILED \\ --show-retries \\ --last 7d # Get retry history for specific execution cloudflow workflows executions retries exec_7h3j6k9m2n # Output: # Execution: exec_7h3j6k9m2n # Attempt 1: FAILED - NetworkError: Connection refused (delay: 0ms) # Attempt 2: FAILED - NetworkError: Connection timeout (delay: 1000ms) # Attempt 3: FAILED - NetworkError: Connection timeout (delay: 2000ms) # Attempt 4: FAILED - NetworkError: Connection timeout (delay: 4000ms) # Final Status: FAILED_AFTER_RETRIES ``` ### Workflow Step Failures #### Common Error Patterns **1.",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_7",
              "content": "Waiting {retry_after} seconds...\") time.sleep(retry_after) continue remaining = int(response.headers.get('X-RateLimit-Remaining', 0)) if remaining < 10: print(f\"Warning: Only {remaining} requests remaining\") return response raise Exception(\"Max retries exceeded due to rate limiting\") ``` **Bash script with rate limit checking:** ```bash #!/bin/bash check_rate_limit() { local remaining=$(curl -s -I https://api.cloudflow.io/api/v1/workflows \\ -H \"Authorization: Bearer $CF_ACCESS_TOKEN\" | \\ grep -i \"x-ratelimit-remaining\" | \\ awk '{print $2}' | tr -d '\\r') if [ \"$remaining\" -lt 10 ]; then echo \"Warning: Only $remaining requests remaining\" local reset=$(curl -s -I https://api.cloudflow.io/api/v1/workflows \\ -H \"Authorization: Bearer $CF_ACCESS_TOKEN\" | \\ grep -i \"x-ratelimit-reset\" | \\ awk '{print $2}' | tr -d '\\r') local wait_time=$((reset - $(date +%s))) echo \"Rate limit resets in $wait_time seconds\" sleep $wait_time fi } # Use before API calls check_rate_limit cloudflow workflows execute wf_9k2n4m8p1q ``` #### Optimization Strategies **1. Implement request batching:** ```bash # Batch multiple workflow executions cloudflow workflows execute-batch \\ --workflow-ids \"wf_id1,wf_id2,wf_id3,wf_id4,wf_id5\" \\ --batch-size 5 # This counts as 1 API request instead of 5 ``` **2. Use webhooks instead of polling:** ```bash # Configure webhook for workflow completion cloudflow webhooks create \\ --event workflow.completed \\ --url https://your-service.com/webhooks/cloudflow \\ --secret $WEBHOOK_SECRET # Verify webhook cloudflow webhooks test webhook_abc123 ``` **3.",
              "score": null
            },
            {
              "doc_id": "user_guide",
              "chunk_id": "user_guide_fix_0",
              "content": "# CloudFlow User Guide Welcome to CloudFlow, the modern workflow automation platform that helps you connect your apps, automate repetitive tasks, and build powerful integrations without writing code. ## Table of Contents 1. [Getting Started](#getting-started) 2. [Workflow Creation](#workflow-creation) 3. [Available Actions](#available-actions) 4. [Variables and Expressions](#variables-and-expressions) 5. [Scheduling](#scheduling) 6. [Error Handling](#error-handling) 7. [Workflow Limits](#workflow-limits) 8. [Best Practices](#best-practices) 9. [Common Workflow Patterns](#common-workflow-patterns) ## Getting Started ### Account Setup Creating your CloudFlow account takes just a few minutes: 1. Navigate to https://app.cloudflow.io/signup 2. Enter your email address and create a strong password 3. Verify your email address by clicking the link sent to your inbox 4. Complete your profile by providing your organization name and use case 5. Choose your plan (Free, Professional, or Enterprise) Once your account is created, you'll be redirected to your CloudFlow dashboard where you can start building workflows immediately. ### Your First Workflow Let's create a simple workflow that sends you a Slack notification when a new file is uploaded to your Google Drive: 1. Click the **\"Create Workflow\"** button in the top-right corner 2. Give your workflow a name: \"Google Drive to Slack Notifier\" 3. Select **\"Google Drive - New File\"** as your trigger 4. Authenticate your Google Drive account when prompted 5. Choose the folder you want to monitor 6. Click **\"Add Action\"** and select **\"Slack - Send Message\"** 7. Authenticate your Slack account 8. Select the channel where you want to receive notifications 9. Customize your message using variables: `New file uploaded: {{trigger.file.name}}` 10. Click **\"Save & Activate\"** to enable your workflow Congratulations! Your first workflow is now live. Every time a file is added to the specified Google Drive folder, you'll receive a Slack notification. ## Workflow Creation CloudFlow offers multiple ways to create and manage workflows, giving you the flexibility to work in the way that suits you best. ### Visual Editor The Visual Editor is CloudFlow's drag-and-drop interface for building workflows without code. It's perfect for users who prefer a graphical approach: **Key Features:** - Drag-and-drop action blocks from the sidebar - Visual connections between steps show your workflow logic - Inline configuration for each action - Real-time validation and error checking - Test mode to run your workflow with sample data **Using the Visual Editor:** 1.",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_3",
              "content": "Workflow Context Accumulation** Large workflow executions may accumulate state in memory. **Solution:** ```bash # Configure context cleanup cloudflow config set workflow.context.max_size_mb 100 cloudflow config set workflow.context.cleanup_threshold 0.8 # Enable context persistence to disk cloudflow config set workflow.context.persistence.enabled true cloudflow config set workflow.context.persistence.backend s3 ``` **2. Connection Pool Leaks** **Diagnosis:** ```bash # Check active connections cloudflow db connections --verbose # Expected output: # Active: 45/100 # Idle: 23 # Waiting: 2 # Average age: 245s ``` **Resolution:** ```bash # Adjust connection pool settings cloudflow config set db.pool.max_connections 100 cloudflow config set db.pool.min_connections 10 cloudflow config set db.pool.idle_timeout 300 cloudflow config set db.pool.max_lifetime 1800 # Force connection pool reset cloudflow db pool reset --confirm ``` **3. Event Stream Buffers** Unbounded event buffers can cause memory exhaustion. ```bash # Configure event buffer limits cloudflow config set events.buffer.max_size 10000 cloudflow config set events.buffer.overflow_strategy drop_oldest # Enable event streaming to external sink cloudflow config set events.sink.type kafka cloudflow config set events.sink.kafka.brokers \"kafka-1:9092,kafka-2:9092\" cloudflow config set events.sink.kafka.topic cloudflow-events ``` --- ## Database Connection Issues ### Connection Pool Exhaustion #### Symptoms - Error: `could not obtain connection from pool within 5000ms` - Error: `connection pool exhausted (100/100 connections in use)` - API requests fail with `503 Service Unavailable` - Database CPU usage normal, but connection count at maximum #### Investigation ```bash # Check current connection pool status cloudflow db pool status --detailed # Output example: # Pool Statistics: # Total Connections: 100/100 (100%) # Active: 87 # Idle: 13 # Waiting Requests: 45 # Average Wait Time: 3420ms # Max Wait Time: 8234ms # Identify long-running queries cloudflow db queries --status running --min-duration 30000 # Check connection distribution by client SELECT application_name, state, COUNT(*) as conn_count, AVG(EXTRACT(EPOCH FROM (NOW() - state_change))) as avg_duration_sec FROM pg_stat_activity WHERE datname = 'cloudflow_production' GROUP BY application_name, state ORDER BY conn_count DESC; ``` #### Resolution **Immediate mitigation:** ```bash # Temporarily increase connection limit (requires database restart) cloudflow db config set max_connections 150 # Kill idle connections older than 5 minutes cloudflow db connections kill --idle-timeout 300 # Restart connection pool without downtime kubectl rollout restart deployment/cloudflow-api -n cloudflow kubectl rollout status deployment/cloudflow-api -n cloudflow ``` **Long-term solutions:** 1.",
              "score": null
            }
          ],
          "latency_ms": 13.1831990001956,
          "query_id": "realistic_011",
          "dimension": "synonym",
          "query": "How long can a workflow run before timing out?"
        },
        {
          "key_facts": [
            "3600 seconds",
            "exceeded maximum execution time of 3600 seconds"
          ],
          "found_facts": [
            "3600 seconds"
          ],
          "missed_facts": [
            "exceeded maximum execution time of 3600 seconds"
          ],
          "coverage": 0.5,
          "expected_docs": [
            "troubleshooting_guide",
            "user_guide"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_7",
              "content": "records: 500 - Session timeout: 30 seconds --- ## Message Queue Patterns ### Pub/Sub Pattern Used for broadcasting events to multiple interested consumers: ``` Workflow Engine publishes workflow.execution.completed \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u25bc \u25bc \u25bc \u25bc Analytics Notification Audit Logger Billing Service Service Service Service ``` Each service maintains its own consumer group and processes events independently. Failures in one consumer don't affect others. ### Request-Reply Pattern Synchronous communication over async messaging (used sparingly): ``` API Gateway publishes request \u2192 Kafka (reply-to: temp-queue-123) \u2502 \u25bc Service processes request \u2502 \u25bc Service publishes response \u2192 temp-queue-123 \u2502 \u25bc API Gateway receives response ``` Timeout: 5 seconds, fallback to direct HTTP call if no response. ### Saga Pattern Distributed transaction management for multi-service workflows: ``` Step 1: Create Order \u2192 SUCCESS \u2192 Step 2: Reserve Inventory \u2502 FAILURE \u2502 \u25bc Compensating Transaction \u2502 \u25bc Cancel Order (Rollback) ``` Orchestrated by Workflow Engine with compensation logic defined in workflow DSL. ### Dead Letter Queue (DLQ) Failed messages after all retry attempts are routed to DLQ: ``` Message processing fails (3 retries with exponential backoff) \u2502 \u25bc Publish to dead-letter-queue topic \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u25bc \u25bc Alert to PagerDuty Store in PostgreSQL for analysis \u2502 \u2502 \u25bc \u25bc Manual investigation Automated pattern detection ``` DLQ Processing SLA: < 4 hours for critical events, < 24 hours for non-critical. --- ## Caching Strategy ### Cache-Aside Pattern (Lazy Loading) Primary caching pattern used across all services: ``` 1. Application checks cache (Redis GET) 2. If HIT \u2192 Return cached data 3. If MISS: a. Query database (PostgreSQL) b. Store result in cache with TTL c. Return data to application ``` **Implementation Example** (Workflow Definition): ```javascript async function getWorkflowDefinition(workflowId) { const cacheKey = `workflow:def:${workflowId}`; // Step 1: Check cache let workflow = await redis.get(cacheKey); if (workflow) { metrics.increment('cache.hit.workflow_def'); return JSON.parse(workflow); } // Step 2: Cache miss - query database metrics.increment('cache.miss.workflow_def'); workflow = await db.query( 'SELECT * FROM workflows WHERE id = $1', [workflowId] ); // Step 3: Store in cache (1 hour TTL) await redis.setex(cacheKey, 3600, JSON.stringify(workflow)); return workflow; } ``` ### Write-Through Pattern Used for critical data where cache consistency is paramount: ``` 1. Application writes to database 2. Database transaction commits 3. Application updates cache 4. Return success to client ``` Applied to: User profiles, authentication sessions, system configuration.",
              "score": null
            },
            {
              "doc_id": "user_guide",
              "chunk_id": "user_guide_fix_6",
              "content": "Filter by error type, date range, or execution ID 3. Click an execution to view full details 4. Click **\"Retry\"** to reprocess with the same input data ### Error Notifications Get notified when workflows fail: ```yaml workflow: notifications: on_failure: - type: email to: \"ops-team@company.com\" - type: slack channel: \"#alerts\" message: \"Workflow {{workflow.name}} failed: {{error.message}}\" on_success_after_retry: - type: slack channel: \"#monitoring\" message: \"Workflow recovered after {{error.attempts}} attempts\" ``` ## Workflow Limits CloudFlow enforces the following limits to ensure platform stability and performance: ### Steps Per Workflow - **Maximum**: 50 steps per workflow - **Recommendation**: Keep workflows focused and modular. If you need more steps, consider splitting into multiple workflows connected via webhooks. ### Execution Timeout - **Default**: 3600 seconds (60 minutes) - **Behavior**: Workflows exceeding this timeout are automatically terminated - **Custom Timeouts**: Enterprise plans can request custom timeout limits **Setting Step-Level Timeouts:** ```yaml - id: long_running_task action: http_request config: url: \"https://api.example.com/process\" timeout: 300 # 5 minutes for this specific step ``` ### Execution Limits - **Maximum**: 1000 executions per day (per workflow) - **Rate Limiting**: 100 concurrent executions per workflow - **Burst Limit**: 10 executions per second **What happens when limits are reached:** - New executions are queued automatically - Webhook triggers return HTTP 429 (Too Many Requests) - Scheduled executions are skipped (logged in audit trail) - Email notifications sent to workflow owner **Monitoring Usage:** View real-time metrics in your workflow dashboard: - Executions today: 847 / 1000 - Average duration: 12.3s - Success rate: 98.2% - Current queue depth: 3 ### Data Limits - **Maximum request/response size**: 10MB per action - **Maximum execution payload**: 50MB total - **Variable value size**: 1MB per variable ### Enterprise Plan Limits Enterprise customers can request increased limits: - Up to 100 steps per workflow - Up to 10,000 executions per day - Up to 7200 second timeout (2 hours) - Priority execution queue - Dedicated capacity allocation Contact sales@cloudflow.io for Enterprise pricing and custom limits. ## Best Practices Follow these best practices to build reliable, maintainable workflows: ### 1. Use Descriptive Names **Good:** - Workflow: \"Sync Customer Data from Salesforce to Database\" - Step: \"validate_customer_email\" **Bad:** - Workflow: \"Workflow 1\" - Step: \"step3\" ### 2.",
              "score": null
            },
            {
              "doc_id": "user_guide",
              "chunk_id": "user_guide_fix_0",
              "content": "# CloudFlow User Guide Welcome to CloudFlow, the modern workflow automation platform that helps you connect your apps, automate repetitive tasks, and build powerful integrations without writing code. ## Table of Contents 1. [Getting Started](#getting-started) 2. [Workflow Creation](#workflow-creation) 3. [Available Actions](#available-actions) 4. [Variables and Expressions](#variables-and-expressions) 5. [Scheduling](#scheduling) 6. [Error Handling](#error-handling) 7. [Workflow Limits](#workflow-limits) 8. [Best Practices](#best-practices) 9. [Common Workflow Patterns](#common-workflow-patterns) ## Getting Started ### Account Setup Creating your CloudFlow account takes just a few minutes: 1. Navigate to https://app.cloudflow.io/signup 2. Enter your email address and create a strong password 3. Verify your email address by clicking the link sent to your inbox 4. Complete your profile by providing your organization name and use case 5. Choose your plan (Free, Professional, or Enterprise) Once your account is created, you'll be redirected to your CloudFlow dashboard where you can start building workflows immediately. ### Your First Workflow Let's create a simple workflow that sends you a Slack notification when a new file is uploaded to your Google Drive: 1. Click the **\"Create Workflow\"** button in the top-right corner 2. Give your workflow a name: \"Google Drive to Slack Notifier\" 3. Select **\"Google Drive - New File\"** as your trigger 4. Authenticate your Google Drive account when prompted 5. Choose the folder you want to monitor 6. Click **\"Add Action\"** and select **\"Slack - Send Message\"** 7. Authenticate your Slack account 8. Select the channel where you want to receive notifications 9. Customize your message using variables: `New file uploaded: {{trigger.file.name}}` 10. Click **\"Save & Activate\"** to enable your workflow Congratulations! Your first workflow is now live. Every time a file is added to the specified Google Drive folder, you'll receive a Slack notification. ## Workflow Creation CloudFlow offers multiple ways to create and manage workflows, giving you the flexibility to work in the way that suits you best. ### Visual Editor The Visual Editor is CloudFlow's drag-and-drop interface for building workflows without code. It's perfect for users who prefer a graphical approach: **Key Features:** - Drag-and-drop action blocks from the sidebar - Visual connections between steps show your workflow logic - Inline configuration for each action - Real-time validation and error checking - Test mode to run your workflow with sample data **Using the Visual Editor:** 1.",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_0",
              "content": "# CloudFlow Platform - System Architecture Overview **Document Version:** 2.3.1 **Last Updated:** January 15, 2026 **Owner:** Platform Architecture Team **Status:** Production ## Executive Summary CloudFlow is a distributed, cloud-native workflow automation platform designed to orchestrate complex business processes at scale. The platform processes over 2.5 million workflow executions daily with an average P99 latency of 180ms for API operations and 4.2 seconds for workflow execution. This document provides a comprehensive overview of the system architecture, including microservices design, data flow patterns, infrastructure decisions, and operational considerations. ## Table of Contents 1. [High-Level Architecture](#high-level-architecture) 2. [Microservices Breakdown](#microservices-breakdown) 3. [Data Flow Architecture](#data-flow-architecture) 4. [Database Architecture](#database-architecture) 5. [Message Queue Patterns](#message-queue-patterns) 6. [Caching Strategy](#caching-strategy) 7. [Security Architecture](#security-architecture) 8. [Performance Characteristics](#performance-characteristics) 9. [Disaster Recovery](#disaster-recovery) --- ## High-Level Architecture CloudFlow follows a microservices architecture pattern deployed across multiple availability zones in AWS. The platform is designed with the following core principles: - **Scalability**: Horizontal scaling for all services with auto-scaling groups - **Resilience**: Circuit breakers, bulkheads, and graceful degradation - **Observability**: Distributed tracing, centralized logging, and comprehensive metrics - **Security**: Zero-trust network architecture with mTLS encryption ### Architecture Diagram (Conceptual) ``` \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 Load Balancer (ALB) \u2502 \u2502 (TLS Termination - 443) \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u25bc \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 API Gateway Layer \u2502 \u2502 (Rate Limiting, Auth, Request Routing) \u2502 \u2514\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u2502 \u2502 \u2502 \u2502 \u25bc \u25bc \u25bc \u25bc \u25bc \u250c\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502Auth\u2502 \u2502Workflow\u2502 \u2502Scheduler\u2502 \u2502Notification\u2502 \u2502User Service\u2502 \u2502Svc \u2502 \u2502 Engine \u2502 \u2502 Service \u2502 \u2502 Service \u2502 \u2502 \u2502 \u2514\u2500\u252c\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u2502 \u2502 \u2502 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 \u2502 \u2502 \u25bc \u25bc \u25bc \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502PostgreSQL\u2502 \u2502 Redis \u2502 \u2502 Kafka \u2502 \u2502 Cluster \u2502 \u2502 Cluster \u2502 \u2502 Cluster \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 ``` ### Technology Stack - **Runtime**: Node.js 20.x (API Gateway, Workflow Engine), Go 1.21 (Auth Service, Scheduler) - **Container Orchestration**: Kubernetes 1.28 (EKS) - **Service Mesh**: Istio 1.20 for service-to-service communication - **Databases**: PostgreSQL 15.4, Redis 7.2 - **Message Broker**: Apache Kafka 3.6 - **Monitoring**: Prometheus, Grafana, Jaeger for distributed tracing - **Secrets Management**: HashiCorp Vault 1.15 --- ## Microservices Breakdown ### API Gateway **Purpose**: Single entry point for all client requests, providing authentication, rate limiting, request routing, and protocol translation.",
              "score": null
            },
            {
              "doc_id": "user_guide",
              "chunk_id": "user_guide_fix_8",
              "content": "Version Control For critical workflows: - Export YAML definitions regularly - Store in version control (Git) - Use pull requests for changes - Tag releases with semantic versioning ## Common Workflow Patterns Here are proven patterns for common automation scenarios: ### Pattern 1: Form to Database Capture form submissions and store in database: ```yaml name: \"Contact Form to Database\" trigger: type: webhook method: POST steps: - id: validate_submission action: javascript code: | if (!input.email || !input.message) { throw new Error(\"Email and message required\"); } return input; - id: check_duplicate action: database_query config: connection: \"{{secrets.DB_CONNECTION}}\" query: \"SELECT id FROM contacts WHERE email = $1 AND created_at > NOW() - INTERVAL '1 hour'\" parameters: - \"{{trigger.body.email}}\" - id: insert_contact action: database_query condition: \"{{length(steps.check_duplicate.rows) == 0}}\" config: connection: \"{{secrets.DB_CONNECTION}}\" query: | INSERT INTO contacts (name, email, message, source, created_at) VALUES ($1, $2, $3, $4, NOW()) parameters: - \"{{trigger.body.name}}\" - \"{{trigger.body.email}}\" - \"{{trigger.body.message}}\" - \"website_form\" - id: notify_sales action: slack_message condition: \"{{steps.insert_contact.affected_rows > 0}}\" config: channel: \"#leads\" text: \"New contact form submission from {{trigger.body.email}}\" ``` ### Pattern 2: API Polling Periodically check an API and take action on new items: ```yaml name: \"Poll GitHub Issues\" schedule: cron: \"*/15 * * * *\" # Every 15 minutes timezone: \"UTC\" steps: - id: get_last_check action: database_query config: query: \"SELECT last_checked_at FROM workflow_state WHERE workflow_id = $1\" parameters: - \"{{workflow.id}}\" - id: fetch_issues action: http_request config: method: GET url: \"https://api.github.com/repos/company/project/issues\" params: since: \"{{steps.get_last_check.rows[0].last_checked_at}}\" state: \"open\" headers: Authorization: \"token {{secrets.GITHUB_TOKEN}}\" - id: process_new_issues action: javascript code: | const issues = input.body; return { count: issues.length, issues: issues.map(i => ({ number: i.number, title: i.title, url: i.html_url })) }; - id: notify_team action: slack_message condition: \"{{steps.process_new_issues.output.count > 0}}\" config: channel: \"#engineering\" text: \"{{steps.process_new_issues.output.count}} new GitHub issues\" - id: update_last_check action: database_query config: query: | UPDATE workflow_state SET last_checked_at = $1 WHERE workflow_id = $2 parameters: - \"{{now()}}\" - \"{{workflow.id}}\" ``` ### Pattern 3: Multi-Step Approval Implement approval workflows with timeouts: ```yaml name: \"Expense Approval Workflow\" trigger: type: webhook method: POST steps: - id: create_approval_request action: database_query config: query: | INSERT INTO approvals (expense_id, amount, requester, status, created_at) VALUES ($1, $2, $3, 'pending', NOW()) RETURNING id parameters: - \"{{trigger.body.expense_id}}\" - \"{{trigger.body.amount}}\" - \"{{trigger.body.requester}}\" - id: notify_manager action: email config: to: \"{{trigger.body.manager_email}}\" subject: \"Expense Approval Required: ${{trigger.body.amount}}\" body: | An expense requires your approval: Amount: ${{trigger.body.amount}}",
              "score": null
            }
          ],
          "latency_ms": 13.363795002078405,
          "query_id": "realistic_011",
          "dimension": "problem",
          "query": "My workflow is being killed after an hour"
        },
        {
          "key_facts": [
            "3600 seconds",
            "exceeded maximum execution time of 3600 seconds"
          ],
          "found_facts": [
            "3600 seconds"
          ],
          "missed_facts": [
            "exceeded maximum execution time of 3600 seconds"
          ],
          "coverage": 0.5,
          "expected_docs": [
            "troubleshooting_guide",
            "user_guide"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "user_guide",
              "chunk_id": "user_guide_fix_6",
              "content": "Filter by error type, date range, or execution ID 3. Click an execution to view full details 4. Click **\"Retry\"** to reprocess with the same input data ### Error Notifications Get notified when workflows fail: ```yaml workflow: notifications: on_failure: - type: email to: \"ops-team@company.com\" - type: slack channel: \"#alerts\" message: \"Workflow {{workflow.name}} failed: {{error.message}}\" on_success_after_retry: - type: slack channel: \"#monitoring\" message: \"Workflow recovered after {{error.attempts}} attempts\" ``` ## Workflow Limits CloudFlow enforces the following limits to ensure platform stability and performance: ### Steps Per Workflow - **Maximum**: 50 steps per workflow - **Recommendation**: Keep workflows focused and modular. If you need more steps, consider splitting into multiple workflows connected via webhooks. ### Execution Timeout - **Default**: 3600 seconds (60 minutes) - **Behavior**: Workflows exceeding this timeout are automatically terminated - **Custom Timeouts**: Enterprise plans can request custom timeout limits **Setting Step-Level Timeouts:** ```yaml - id: long_running_task action: http_request config: url: \"https://api.example.com/process\" timeout: 300 # 5 minutes for this specific step ``` ### Execution Limits - **Maximum**: 1000 executions per day (per workflow) - **Rate Limiting**: 100 concurrent executions per workflow - **Burst Limit**: 10 executions per second **What happens when limits are reached:** - New executions are queued automatically - Webhook triggers return HTTP 429 (Too Many Requests) - Scheduled executions are skipped (logged in audit trail) - Email notifications sent to workflow owner **Monitoring Usage:** View real-time metrics in your workflow dashboard: - Executions today: 847 / 1000 - Average duration: 12.3s - Success rate: 98.2% - Current queue depth: 3 ### Data Limits - **Maximum request/response size**: 10MB per action - **Maximum execution payload**: 50MB total - **Variable value size**: 1MB per variable ### Enterprise Plan Limits Enterprise customers can request increased limits: - Up to 100 steps per workflow - Up to 10,000 executions per day - Up to 7200 second timeout (2 hours) - Priority execution queue - Dedicated capacity allocation Contact sales@cloudflow.io for Enterprise pricing and custom limits. ## Best Practices Follow these best practices to build reliable, maintainable workflows: ### 1. Use Descriptive Names **Good:** - Workflow: \"Sync Customer Data from Salesforce to Database\" - Step: \"validate_customer_email\" **Bad:** - Workflow: \"Workflow 1\" - Step: \"step3\" ### 2.",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_5",
              "content": "exec_7h3j6k9m2n --show-bottlenecks ``` #### Solutions **1. Increase workflow timeout (if justified):** ```bash # Update workflow configuration cloudflow workflows update wf_9k2n4m8p1q \\ --timeout 7200 \\ --reason \"Large dataset processing requires extended time\" # Verify update cloudflow workflows get wf_9k2n4m8p1q | grep timeout ``` **2. Optimize slow steps:** ```bash # Enable parallel processing cloudflow workflows update wf_9k2n4m8p1q \\ --step data_transformation \\ --parallel-workers 8 \\ --batch-size 1000 # Add checkpointing for long operations cloudflow workflows update wf_9k2n4m8p1q \\ --step data_transformation \\ --enable-checkpointing \\ --checkpoint-interval 300 ``` **3. Split workflow into smaller workflows:** ```bash # Create sub-workflows cloudflow workflows create data-pipeline-part1 \\ --steps \"data_ingestion,data_validation\" \\ --timeout 1800 cloudflow workflows create data-pipeline-part2 \\ --steps \"data_transformation,data_export\" \\ --timeout 3600 \\ --trigger workflow_completed \\ --trigger-workflow data-pipeline-part1 ``` ### Retry Logic and Exponential Backoff CloudFlow implements automatic retry with exponential backoff for transient failures: - Max retries: 3 - Initial delay: 1 second - Backoff multiplier: 2 - Max delay: 60 seconds #### Retry Sequence ``` Attempt 1: Immediate Attempt 2: Wait 1s (2^0 * 1s) Attempt 3: Wait 2s (2^1 * 1s) Attempt 4: Wait 4s (2^2 * 1s) ``` #### Configuration ```bash # View current retry settings cloudflow workflows get wf_9k2n4m8p1q --format json | jq '.retry_policy' # Customize retry behavior cloudflow workflows update wf_9k2n4m8p1q \\ --retry-max-attempts 5 \\ --retry-initial-delay 2000 \\ --retry-backoff-multiplier 2 \\ --retry-max-delay 120000 \\ --retry-on-errors \"NETWORK_ERROR,TIMEOUT,RATE_LIMIT\" # Disable retry for specific step cloudflow workflows update wf_9k2n4m8p1q \\ --step data_export \\ --retry-enabled false ``` #### Monitoring Retries ```bash # List failed executions with retry information cloudflow workflows executions list \\ --status FAILED \\ --show-retries \\ --last 7d # Get retry history for specific execution cloudflow workflows executions retries exec_7h3j6k9m2n # Output: # Execution: exec_7h3j6k9m2n # Attempt 1: FAILED - NetworkError: Connection refused (delay: 0ms) # Attempt 2: FAILED - NetworkError: Connection timeout (delay: 1000ms) # Attempt 3: FAILED - NetworkError: Connection timeout (delay: 2000ms) # Attempt 4: FAILED - NetworkError: Connection timeout (delay: 4000ms) # Final Status: FAILED_AFTER_RETRIES ``` ### Workflow Step Failures #### Common Error Patterns **1.",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_11",
              "content": "outage\" # Create war room cloudflow incident war-room create incident-2024012401 ``` **For SEV-2 (High):** ```bash # Create incident and notify cloudflow incident create \\ --severity SEV-2 \\ --title \"High rate of workflow failures\" \\ --description \"Workflow success rate dropped to 75%\" \\ --notify-oncall # Update status page cloudflow status update \\ --status \"partial_outage\" \\ --message \"We are experiencing elevated error rates\" ``` **For SEV-3/SEV-4:** ```bash # Create support ticket cloudflow support ticket create \\ --priority MEDIUM \\ --subject \"Intermittent timeout errors\" \\ --description \"See attached logs and reproduction steps\" ``` ### Contact Information - **On-call hotline:** +1-888-CLOUDFLOW (24/7) - **Support email:** support@cloudflow.io - **Incident Slack channel:** #cloudflow-incidents - **Status page:** https://status.cloudflow.io - **Documentation:** https://docs.cloudflow.io ### Post-Incident Review After resolving incidents, complete a postmortem: ```bash # Generate postmortem template cloudflow incident postmortem incident-2024012401 \\ --template standard \\ --output postmortem-2024012401.md # Required sections: # 1. Incident summary # 2. Impact assessment # 3. Timeline of events # 4. Root cause analysis # 5. Resolution steps # 6. Action items # 7. Lessons learned ``` --- ## Additional Resources - **CloudFlow Documentation:** https://docs.cloudflow.io - **API Reference:** https://api-docs.cloudflow.io - **Community Forum:** https://community.cloudflow.io - **GitHub Issues:** https://github.com/cloudflow/platform/issues - **Training Portal:** https://training.cloudflow.io - **Monitoring Dashboard:** https://monitoring.cloudflow.io ### Getting Help If this troubleshooting guide doesn't resolve your issue: 1. Search the knowledge base: `cloudflow kb search \"your issue\"` 2. Check community forum for similar issues 3. Contact support with detailed logs and reproduction steps 4. For urgent issues, use emergency escalation procedures **Remember:** Always capture logs, metrics, and reproduction steps before escalating! --- *Last updated: January 24, 2026* *Document version: 3.2.1* *Feedback: docs-feedback@cloudflow.io*",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_7",
              "content": "records: 500 - Session timeout: 30 seconds --- ## Message Queue Patterns ### Pub/Sub Pattern Used for broadcasting events to multiple interested consumers: ``` Workflow Engine publishes workflow.execution.completed \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u25bc \u25bc \u25bc \u25bc Analytics Notification Audit Logger Billing Service Service Service Service ``` Each service maintains its own consumer group and processes events independently. Failures in one consumer don't affect others. ### Request-Reply Pattern Synchronous communication over async messaging (used sparingly): ``` API Gateway publishes request \u2192 Kafka (reply-to: temp-queue-123) \u2502 \u25bc Service processes request \u2502 \u25bc Service publishes response \u2192 temp-queue-123 \u2502 \u25bc API Gateway receives response ``` Timeout: 5 seconds, fallback to direct HTTP call if no response. ### Saga Pattern Distributed transaction management for multi-service workflows: ``` Step 1: Create Order \u2192 SUCCESS \u2192 Step 2: Reserve Inventory \u2502 FAILURE \u2502 \u25bc Compensating Transaction \u2502 \u25bc Cancel Order (Rollback) ``` Orchestrated by Workflow Engine with compensation logic defined in workflow DSL. ### Dead Letter Queue (DLQ) Failed messages after all retry attempts are routed to DLQ: ``` Message processing fails (3 retries with exponential backoff) \u2502 \u25bc Publish to dead-letter-queue topic \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u25bc \u25bc Alert to PagerDuty Store in PostgreSQL for analysis \u2502 \u2502 \u25bc \u25bc Manual investigation Automated pattern detection ``` DLQ Processing SLA: < 4 hours for critical events, < 24 hours for non-critical. --- ## Caching Strategy ### Cache-Aside Pattern (Lazy Loading) Primary caching pattern used across all services: ``` 1. Application checks cache (Redis GET) 2. If HIT \u2192 Return cached data 3. If MISS: a. Query database (PostgreSQL) b. Store result in cache with TTL c. Return data to application ``` **Implementation Example** (Workflow Definition): ```javascript async function getWorkflowDefinition(workflowId) { const cacheKey = `workflow:def:${workflowId}`; // Step 1: Check cache let workflow = await redis.get(cacheKey); if (workflow) { metrics.increment('cache.hit.workflow_def'); return JSON.parse(workflow); } // Step 2: Cache miss - query database metrics.increment('cache.miss.workflow_def'); workflow = await db.query( 'SELECT * FROM workflows WHERE id = $1', [workflowId] ); // Step 3: Store in cache (1 hour TTL) await redis.setex(cacheKey, 3600, JSON.stringify(workflow)); return workflow; } ``` ### Write-Through Pattern Used for critical data where cache consistency is paramount: ``` 1. Application writes to database 2. Database transaction commits 3. Application updates cache 4. Return success to client ``` Applied to: User profiles, authentication sessions, system configuration.",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_2",
              "content": "**Technology**: Node.js with TypeScript, Bull queue library **Replicas**: 16 pods (production), auto-scaling 12-24 **Resource Allocation**: 4 vCPU, 8GB RAM per pod **Key Responsibilities**: - Parse and validate workflow definitions (JSON-based DSL) - Execute workflow steps with state machine pattern - Handle retries with exponential backoff (max 3 retries, backoff: 2^n seconds) - Coordinate parallel and sequential task execution - Manage workflow state persistence and recovery - Support for conditional branching and loops - Sub-workflow invocation and composition **Workflow Execution Model**: ``` Workflow Submitted \u2192 Validation \u2192 Queue in Kafka \u2502 \u25bc Workflow Engine picks up \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u25bc \u25bc \u25bc [Task 1 Exec] [Task 2 Exec] [Task 3 Exec] \u2502 \u2502 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u25bc State Update \u2192 PostgreSQL \u2502 \u25bc Publish Event \u2192 Kafka ``` **Workflow DSL Example**: ```json { \"workflow_id\": \"customer-onboarding-v2\", \"version\": \"2.1.0\", \"steps\": [ { \"id\": \"validate-customer\", \"type\": \"http_request\", \"config\": { \"url\": \"https://validation.internal/verify\", \"method\": \"POST\", \"timeout\": 5000, \"retry\": {\"max_attempts\": 3, \"backoff\": \"exponential\"} } }, { \"id\": \"create-account\", \"type\": \"database_operation\", \"depends_on\": [\"validate-customer\"] } ] } ``` **State Management**: - Workflow states: PENDING, RUNNING, COMPLETED, FAILED, CANCELLED - Step states: QUEUED, EXECUTING, SUCCESS, FAILED, SKIPPED - State transitions stored in PostgreSQL with event sourcing pattern - Checkpointing every 10 steps for long-running workflows **Performance Targets**: - Simple workflow (3-5 steps): < 2 seconds (P99) - Complex workflow (20+ steps): < 5 seconds (P99) - Throughput: 500 concurrent workflow executions per pod --- ### Scheduler Service **Purpose**: Time-based workflow triggering system supporting cron-like schedules and one-time delayed executions.",
              "score": null
            }
          ],
          "latency_ms": 12.714465003227815,
          "query_id": "realistic_011",
          "dimension": "casual",
          "query": "workflow timeout"
        },
        {
          "key_facts": [
            "3600 seconds",
            "exceeded maximum execution time of 3600 seconds"
          ],
          "found_facts": [],
          "missed_facts": [
            "3600 seconds",
            "exceeded maximum execution time of 3600 seconds"
          ],
          "coverage": 0.0,
          "expected_docs": [
            "troubleshooting_guide",
            "user_guide"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_5",
              "content": "exec_7h3j6k9m2n --show-bottlenecks ``` #### Solutions **1. Increase workflow timeout (if justified):** ```bash # Update workflow configuration cloudflow workflows update wf_9k2n4m8p1q \\ --timeout 7200 \\ --reason \"Large dataset processing requires extended time\" # Verify update cloudflow workflows get wf_9k2n4m8p1q | grep timeout ``` **2. Optimize slow steps:** ```bash # Enable parallel processing cloudflow workflows update wf_9k2n4m8p1q \\ --step data_transformation \\ --parallel-workers 8 \\ --batch-size 1000 # Add checkpointing for long operations cloudflow workflows update wf_9k2n4m8p1q \\ --step data_transformation \\ --enable-checkpointing \\ --checkpoint-interval 300 ``` **3. Split workflow into smaller workflows:** ```bash # Create sub-workflows cloudflow workflows create data-pipeline-part1 \\ --steps \"data_ingestion,data_validation\" \\ --timeout 1800 cloudflow workflows create data-pipeline-part2 \\ --steps \"data_transformation,data_export\" \\ --timeout 3600 \\ --trigger workflow_completed \\ --trigger-workflow data-pipeline-part1 ``` ### Retry Logic and Exponential Backoff CloudFlow implements automatic retry with exponential backoff for transient failures: - Max retries: 3 - Initial delay: 1 second - Backoff multiplier: 2 - Max delay: 60 seconds #### Retry Sequence ``` Attempt 1: Immediate Attempt 2: Wait 1s (2^0 * 1s) Attempt 3: Wait 2s (2^1 * 1s) Attempt 4: Wait 4s (2^2 * 1s) ``` #### Configuration ```bash # View current retry settings cloudflow workflows get wf_9k2n4m8p1q --format json | jq '.retry_policy' # Customize retry behavior cloudflow workflows update wf_9k2n4m8p1q \\ --retry-max-attempts 5 \\ --retry-initial-delay 2000 \\ --retry-backoff-multiplier 2 \\ --retry-max-delay 120000 \\ --retry-on-errors \"NETWORK_ERROR,TIMEOUT,RATE_LIMIT\" # Disable retry for specific step cloudflow workflows update wf_9k2n4m8p1q \\ --step data_export \\ --retry-enabled false ``` #### Monitoring Retries ```bash # List failed executions with retry information cloudflow workflows executions list \\ --status FAILED \\ --show-retries \\ --last 7d # Get retry history for specific execution cloudflow workflows executions retries exec_7h3j6k9m2n # Output: # Execution: exec_7h3j6k9m2n # Attempt 1: FAILED - NetworkError: Connection refused (delay: 0ms) # Attempt 2: FAILED - NetworkError: Connection timeout (delay: 1000ms) # Attempt 3: FAILED - NetworkError: Connection timeout (delay: 2000ms) # Attempt 4: FAILED - NetworkError: Connection timeout (delay: 4000ms) # Final Status: FAILED_AFTER_RETRIES ``` ### Workflow Step Failures #### Common Error Patterns **1.",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_7",
              "content": "records: 500 - Session timeout: 30 seconds --- ## Message Queue Patterns ### Pub/Sub Pattern Used for broadcasting events to multiple interested consumers: ``` Workflow Engine publishes workflow.execution.completed \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u25bc \u25bc \u25bc \u25bc Analytics Notification Audit Logger Billing Service Service Service Service ``` Each service maintains its own consumer group and processes events independently. Failures in one consumer don't affect others. ### Request-Reply Pattern Synchronous communication over async messaging (used sparingly): ``` API Gateway publishes request \u2192 Kafka (reply-to: temp-queue-123) \u2502 \u25bc Service processes request \u2502 \u25bc Service publishes response \u2192 temp-queue-123 \u2502 \u25bc API Gateway receives response ``` Timeout: 5 seconds, fallback to direct HTTP call if no response. ### Saga Pattern Distributed transaction management for multi-service workflows: ``` Step 1: Create Order \u2192 SUCCESS \u2192 Step 2: Reserve Inventory \u2502 FAILURE \u2502 \u25bc Compensating Transaction \u2502 \u25bc Cancel Order (Rollback) ``` Orchestrated by Workflow Engine with compensation logic defined in workflow DSL. ### Dead Letter Queue (DLQ) Failed messages after all retry attempts are routed to DLQ: ``` Message processing fails (3 retries with exponential backoff) \u2502 \u25bc Publish to dead-letter-queue topic \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u25bc \u25bc Alert to PagerDuty Store in PostgreSQL for analysis \u2502 \u2502 \u25bc \u25bc Manual investigation Automated pattern detection ``` DLQ Processing SLA: < 4 hours for critical events, < 24 hours for non-critical. --- ## Caching Strategy ### Cache-Aside Pattern (Lazy Loading) Primary caching pattern used across all services: ``` 1. Application checks cache (Redis GET) 2. If HIT \u2192 Return cached data 3. If MISS: a. Query database (PostgreSQL) b. Store result in cache with TTL c. Return data to application ``` **Implementation Example** (Workflow Definition): ```javascript async function getWorkflowDefinition(workflowId) { const cacheKey = `workflow:def:${workflowId}`; // Step 1: Check cache let workflow = await redis.get(cacheKey); if (workflow) { metrics.increment('cache.hit.workflow_def'); return JSON.parse(workflow); } // Step 2: Cache miss - query database metrics.increment('cache.miss.workflow_def'); workflow = await db.query( 'SELECT * FROM workflows WHERE id = $1', [workflowId] ); // Step 3: Store in cache (1 hour TTL) await redis.setex(cacheKey, 3600, JSON.stringify(workflow)); return workflow; } ``` ### Write-Through Pattern Used for critical data where cache consistency is paramount: ``` 1. Application writes to database 2. Database transaction commits 3. Application updates cache 4. Return success to client ``` Applied to: User profiles, authentication sessions, system configuration.",
              "score": null
            },
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_1",
              "content": "## Rate Limiting To ensure fair usage and system stability, CloudFlow enforces rate limits on all API endpoints. **Default Limits:** - 100 requests per minute per authenticated user - 20 requests per minute for unauthenticated requests - Burst allowance: 150 requests in a 10-second window ### Rate Limit Headers Every API response includes rate limit information: ``` X-RateLimit-Limit: 100 X-RateLimit-Remaining: 87 X-RateLimit-Reset: 1640995200 ``` When you exceed the rate limit, you'll receive a `429 Too Many Requests` response: ```json { \"error\": { \"code\": \"rate_limit_exceeded\", \"message\": \"Rate limit exceeded. Please retry after 42 seconds.\", \"retry_after\": 42 } } ``` **Best Practices:** - Monitor `X-RateLimit-Remaining` header values - Implement exponential backoff when receiving 429 responses - Cache responses when appropriate to reduce API calls - Consider upgrading to Enterprise tier for higher limits (1000 req/min) ## Pagination List endpoints return paginated results to optimize performance. Use `limit` and `offset` parameters to navigate through result sets. **Parameters:** - `limit`: Number of items per page (default: 20, max: 100) - `offset`: Number of items to skip (default: 0) **Example Request:** ```bash curl -H \"X-API-Key: YOUR_API_KEY\" \\ \"https://api.cloudflow.io/v2/workflows?limit=50&offset=100\" ``` **Response Structure:** ```json { \"data\": [...], \"pagination\": { \"total\": 347, \"limit\": 50, \"offset\": 100, \"has_more\": true } } ``` **Python Example:** ```python import requests def fetch_all_workflows(api_key): base_url = \"https://api.cloudflow.io/v2/workflows\" headers = {\"X-API-Key\": api_key} all_workflows = [] offset = 0 limit = 100 while True: response = requests.get( base_url, headers=headers, params={\"limit\": limit, \"offset\": offset} ) data = response.json() all_workflows.extend(data['data']) if not data['pagination']['has_more']: break offset += limit return all_workflows ``` ## Endpoints ### Workflows #### List Workflows Retrieve a paginated list of all workflows in your account. **Endpoint:** `GET /workflows` **Query Parameters:** - `status` (optional): Filter by status (`active`, `paused`, `archived`) - `created_after` (optional): ISO 8601 timestamp - `limit` (optional): Items per page (max 100) - `offset` (optional): Pagination offset **Example Request:** ```bash curl -H \"X-API-Key: YOUR_API_KEY\" \\ \"https://api.cloudflow.io/v2/workflows?status=active&limit=25\" ``` **Example Response (200 OK):** ```json { \"data\": [ { \"id\": \"wf_8x7k2m9p\", \"name\": \"Data Processing Pipeline\", \"description\": \"Processes customer data every hour\", \"status\": \"active\", \"trigger\": { \"type\": \"schedule\", \"cron\": \"0 * * * *\" }, \"steps\": 12, \"created_at\": \"2026-01-15T10:30:00Z\", \"updated_at\": \"2026-01-23T14:22:00Z\", \"last_run\": \"2026-01-24T09:00:00Z\" } ], \"pagination\": { \"total\": 1, \"limit\": 25, \"offset\": 0, \"has_more\": false } } ``` #### Create Workflow Create a new workflow with specified configuration.",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_2",
              "content": "**Technology**: Node.js with TypeScript, Bull queue library **Replicas**: 16 pods (production), auto-scaling 12-24 **Resource Allocation**: 4 vCPU, 8GB RAM per pod **Key Responsibilities**: - Parse and validate workflow definitions (JSON-based DSL) - Execute workflow steps with state machine pattern - Handle retries with exponential backoff (max 3 retries, backoff: 2^n seconds) - Coordinate parallel and sequential task execution - Manage workflow state persistence and recovery - Support for conditional branching and loops - Sub-workflow invocation and composition **Workflow Execution Model**: ``` Workflow Submitted \u2192 Validation \u2192 Queue in Kafka \u2502 \u25bc Workflow Engine picks up \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u25bc \u25bc \u25bc [Task 1 Exec] [Task 2 Exec] [Task 3 Exec] \u2502 \u2502 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u25bc State Update \u2192 PostgreSQL \u2502 \u25bc Publish Event \u2192 Kafka ``` **Workflow DSL Example**: ```json { \"workflow_id\": \"customer-onboarding-v2\", \"version\": \"2.1.0\", \"steps\": [ { \"id\": \"validate-customer\", \"type\": \"http_request\", \"config\": { \"url\": \"https://validation.internal/verify\", \"method\": \"POST\", \"timeout\": 5000, \"retry\": {\"max_attempts\": 3, \"backoff\": \"exponential\"} } }, { \"id\": \"create-account\", \"type\": \"database_operation\", \"depends_on\": [\"validate-customer\"] } ] } ``` **State Management**: - Workflow states: PENDING, RUNNING, COMPLETED, FAILED, CANCELLED - Step states: QUEUED, EXECUTING, SUCCESS, FAILED, SKIPPED - State transitions stored in PostgreSQL with event sourcing pattern - Checkpointing every 10 steps for long-running workflows **Performance Targets**: - Simple workflow (3-5 steps): < 2 seconds (P99) - Complex workflow (20+ steps): < 5 seconds (P99) - Throughput: 500 concurrent workflow executions per pod --- ### Scheduler Service **Purpose**: Time-based workflow triggering system supporting cron-like schedules and one-time delayed executions.",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_3",
              "content": "Workflow Context Accumulation** Large workflow executions may accumulate state in memory. **Solution:** ```bash # Configure context cleanup cloudflow config set workflow.context.max_size_mb 100 cloudflow config set workflow.context.cleanup_threshold 0.8 # Enable context persistence to disk cloudflow config set workflow.context.persistence.enabled true cloudflow config set workflow.context.persistence.backend s3 ``` **2. Connection Pool Leaks** **Diagnosis:** ```bash # Check active connections cloudflow db connections --verbose # Expected output: # Active: 45/100 # Idle: 23 # Waiting: 2 # Average age: 245s ``` **Resolution:** ```bash # Adjust connection pool settings cloudflow config set db.pool.max_connections 100 cloudflow config set db.pool.min_connections 10 cloudflow config set db.pool.idle_timeout 300 cloudflow config set db.pool.max_lifetime 1800 # Force connection pool reset cloudflow db pool reset --confirm ``` **3. Event Stream Buffers** Unbounded event buffers can cause memory exhaustion. ```bash # Configure event buffer limits cloudflow config set events.buffer.max_size 10000 cloudflow config set events.buffer.overflow_strategy drop_oldest # Enable event streaming to external sink cloudflow config set events.sink.type kafka cloudflow config set events.sink.kafka.brokers \"kafka-1:9092,kafka-2:9092\" cloudflow config set events.sink.kafka.topic cloudflow-events ``` --- ## Database Connection Issues ### Connection Pool Exhaustion #### Symptoms - Error: `could not obtain connection from pool within 5000ms` - Error: `connection pool exhausted (100/100 connections in use)` - API requests fail with `503 Service Unavailable` - Database CPU usage normal, but connection count at maximum #### Investigation ```bash # Check current connection pool status cloudflow db pool status --detailed # Output example: # Pool Statistics: # Total Connections: 100/100 (100%) # Active: 87 # Idle: 13 # Waiting Requests: 45 # Average Wait Time: 3420ms # Max Wait Time: 8234ms # Identify long-running queries cloudflow db queries --status running --min-duration 30000 # Check connection distribution by client SELECT application_name, state, COUNT(*) as conn_count, AVG(EXTRACT(EPOCH FROM (NOW() - state_change))) as avg_duration_sec FROM pg_stat_activity WHERE datname = 'cloudflow_production' GROUP BY application_name, state ORDER BY conn_count DESC; ``` #### Resolution **Immediate mitigation:** ```bash # Temporarily increase connection limit (requires database restart) cloudflow db config set max_connections 150 # Kill idle connections older than 5 minutes cloudflow db connections kill --idle-timeout 300 # Restart connection pool without downtime kubectl rollout restart deployment/cloudflow-api -n cloudflow kubectl rollout status deployment/cloudflow-api -n cloudflow ``` **Long-term solutions:** 1.",
              "score": null
            }
          ],
          "latency_ms": 13.075427999865497,
          "query_id": "realistic_011",
          "dimension": "contextual",
          "query": "I have a long-running data processing workflow, what's the time limit?"
        },
        {
          "key_facts": [
            "3600 seconds",
            "exceeded maximum execution time of 3600 seconds"
          ],
          "found_facts": [
            "3600 seconds"
          ],
          "missed_facts": [
            "exceeded maximum execution time of 3600 seconds"
          ],
          "coverage": 0.5,
          "expected_docs": [
            "troubleshooting_guide",
            "user_guide"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "user_guide",
              "chunk_id": "user_guide_fix_6",
              "content": "Filter by error type, date range, or execution ID 3. Click an execution to view full details 4. Click **\"Retry\"** to reprocess with the same input data ### Error Notifications Get notified when workflows fail: ```yaml workflow: notifications: on_failure: - type: email to: \"ops-team@company.com\" - type: slack channel: \"#alerts\" message: \"Workflow {{workflow.name}} failed: {{error.message}}\" on_success_after_retry: - type: slack channel: \"#monitoring\" message: \"Workflow recovered after {{error.attempts}} attempts\" ``` ## Workflow Limits CloudFlow enforces the following limits to ensure platform stability and performance: ### Steps Per Workflow - **Maximum**: 50 steps per workflow - **Recommendation**: Keep workflows focused and modular. If you need more steps, consider splitting into multiple workflows connected via webhooks. ### Execution Timeout - **Default**: 3600 seconds (60 minutes) - **Behavior**: Workflows exceeding this timeout are automatically terminated - **Custom Timeouts**: Enterprise plans can request custom timeout limits **Setting Step-Level Timeouts:** ```yaml - id: long_running_task action: http_request config: url: \"https://api.example.com/process\" timeout: 300 # 5 minutes for this specific step ``` ### Execution Limits - **Maximum**: 1000 executions per day (per workflow) - **Rate Limiting**: 100 concurrent executions per workflow - **Burst Limit**: 10 executions per second **What happens when limits are reached:** - New executions are queued automatically - Webhook triggers return HTTP 429 (Too Many Requests) - Scheduled executions are skipped (logged in audit trail) - Email notifications sent to workflow owner **Monitoring Usage:** View real-time metrics in your workflow dashboard: - Executions today: 847 / 1000 - Average duration: 12.3s - Success rate: 98.2% - Current queue depth: 3 ### Data Limits - **Maximum request/response size**: 10MB per action - **Maximum execution payload**: 50MB total - **Variable value size**: 1MB per variable ### Enterprise Plan Limits Enterprise customers can request increased limits: - Up to 100 steps per workflow - Up to 10,000 executions per day - Up to 7200 second timeout (2 hours) - Priority execution queue - Dedicated capacity allocation Contact sales@cloudflow.io for Enterprise pricing and custom limits. ## Best Practices Follow these best practices to build reliable, maintainable workflows: ### 1. Use Descriptive Names **Good:** - Workflow: \"Sync Customer Data from Salesforce to Database\" - Step: \"validate_customer_email\" **Bad:** - Workflow: \"Workflow 1\" - Step: \"step3\" ### 2.",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_6",
              "content": "Data Validation Errors** ``` ValidationError: Field 'customer_id' is required but missing in 234 records ``` **Resolution:** ```bash # Add data quality checks cloudflow workflows update wf_9k2n4m8p1q \\ --step data_ingestion \\ --add-validator required_fields \\ --validator-config '{\"fields\": [\"customer_id\", \"timestamp\", \"amount\"]}' # Configure error handling cloudflow workflows update wf_9k2n4m8p1q \\ --step data_validation \\ --on-error continue \\ --error-threshold 5% # Fail if > 5% of records invalid ``` **2. External API Failures** ``` ExternalAPIError: API request to https://partner-api.example.com failed with status 502 ``` **Resolution:** ```bash # Add circuit breaker cloudflow workflows update wf_9k2n4m8p1q \\ --step external_api_call \\ --circuit-breaker-enabled true \\ --circuit-breaker-threshold 5 \\ --circuit-breaker-timeout 30000 # Configure fallback behavior cloudflow workflows update wf_9k2n4m8p1q \\ --step external_api_call \\ --fallback-action use_cached_data \\ --cache-ttl 3600 ``` --- ## Rate Limiting & Throttling ### 429 Too Many Requests #### Error Response ```json { \"error\": \"rate_limit_exceeded\", \"message\": \"API rate limit exceeded. Retry after 45 seconds.\", \"status\": 429, \"headers\": { \"X-RateLimit-Limit\": \"1000\", \"X-RateLimit-Remaining\": \"0\", \"X-RateLimit-Reset\": \"1706112345\", \"Retry-After\": \"45\" } } ``` #### Rate Limit Tiers CloudFlow enforces the following rate limits per workspace: | Tier | Requests/Minute | Requests/Hour | Concurrent Workflows | |------|-----------------|---------------|----------------------| | Free | 60 | 1,000 | 5 | | Standard | 1,000 | 50,000 | 50 | | Premium | 5,000 | 250,000 | 200 | | Enterprise | Custom | Custom | Unlimited | #### Checking Rate Limit Status ```bash # Check current rate limit status curl -I https://api.cloudflow.io/api/v1/workflows \\ -H \"Authorization: Bearer $CF_ACCESS_TOKEN\" # Extract rate limit headers curl -s -I https://api.cloudflow.io/api/v1/workflows \\ -H \"Authorization: Bearer $CF_ACCESS_TOKEN\" | \\ grep -i \"x-ratelimit\" # Output: # X-RateLimit-Limit: 1000 # X-RateLimit-Remaining: 247 # X-RateLimit-Reset: 1706112400 # Monitor rate limit usage cloudflow metrics query \\ --metric rate_limit_remaining \\ --workspace ws_abc123 \\ --last 1h \\ --interval 1m ``` #### Handling Rate Limits in Code **Python example with retry logic:** ```python import time import requests def cloudflow_api_call_with_retry(url, headers, max_retries=3): for attempt in range(max_retries): response = requests.get(url, headers=headers) if response.status_code == 429: retry_after = int(response.headers.get('Retry-After', 60)) print(f\"Rate limited.",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_5",
              "content": "exec_7h3j6k9m2n --show-bottlenecks ``` #### Solutions **1. Increase workflow timeout (if justified):** ```bash # Update workflow configuration cloudflow workflows update wf_9k2n4m8p1q \\ --timeout 7200 \\ --reason \"Large dataset processing requires extended time\" # Verify update cloudflow workflows get wf_9k2n4m8p1q | grep timeout ``` **2. Optimize slow steps:** ```bash # Enable parallel processing cloudflow workflows update wf_9k2n4m8p1q \\ --step data_transformation \\ --parallel-workers 8 \\ --batch-size 1000 # Add checkpointing for long operations cloudflow workflows update wf_9k2n4m8p1q \\ --step data_transformation \\ --enable-checkpointing \\ --checkpoint-interval 300 ``` **3. Split workflow into smaller workflows:** ```bash # Create sub-workflows cloudflow workflows create data-pipeline-part1 \\ --steps \"data_ingestion,data_validation\" \\ --timeout 1800 cloudflow workflows create data-pipeline-part2 \\ --steps \"data_transformation,data_export\" \\ --timeout 3600 \\ --trigger workflow_completed \\ --trigger-workflow data-pipeline-part1 ``` ### Retry Logic and Exponential Backoff CloudFlow implements automatic retry with exponential backoff for transient failures: - Max retries: 3 - Initial delay: 1 second - Backoff multiplier: 2 - Max delay: 60 seconds #### Retry Sequence ``` Attempt 1: Immediate Attempt 2: Wait 1s (2^0 * 1s) Attempt 3: Wait 2s (2^1 * 1s) Attempt 4: Wait 4s (2^2 * 1s) ``` #### Configuration ```bash # View current retry settings cloudflow workflows get wf_9k2n4m8p1q --format json | jq '.retry_policy' # Customize retry behavior cloudflow workflows update wf_9k2n4m8p1q \\ --retry-max-attempts 5 \\ --retry-initial-delay 2000 \\ --retry-backoff-multiplier 2 \\ --retry-max-delay 120000 \\ --retry-on-errors \"NETWORK_ERROR,TIMEOUT,RATE_LIMIT\" # Disable retry for specific step cloudflow workflows update wf_9k2n4m8p1q \\ --step data_export \\ --retry-enabled false ``` #### Monitoring Retries ```bash # List failed executions with retry information cloudflow workflows executions list \\ --status FAILED \\ --show-retries \\ --last 7d # Get retry history for specific execution cloudflow workflows executions retries exec_7h3j6k9m2n # Output: # Execution: exec_7h3j6k9m2n # Attempt 1: FAILED - NetworkError: Connection refused (delay: 0ms) # Attempt 2: FAILED - NetworkError: Connection timeout (delay: 1000ms) # Attempt 3: FAILED - NetworkError: Connection timeout (delay: 2000ms) # Attempt 4: FAILED - NetworkError: Connection timeout (delay: 4000ms) # Final Status: FAILED_AFTER_RETRIES ``` ### Workflow Step Failures #### Common Error Patterns **1.",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_7",
              "content": "records: 500 - Session timeout: 30 seconds --- ## Message Queue Patterns ### Pub/Sub Pattern Used for broadcasting events to multiple interested consumers: ``` Workflow Engine publishes workflow.execution.completed \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u25bc \u25bc \u25bc \u25bc Analytics Notification Audit Logger Billing Service Service Service Service ``` Each service maintains its own consumer group and processes events independently. Failures in one consumer don't affect others. ### Request-Reply Pattern Synchronous communication over async messaging (used sparingly): ``` API Gateway publishes request \u2192 Kafka (reply-to: temp-queue-123) \u2502 \u25bc Service processes request \u2502 \u25bc Service publishes response \u2192 temp-queue-123 \u2502 \u25bc API Gateway receives response ``` Timeout: 5 seconds, fallback to direct HTTP call if no response. ### Saga Pattern Distributed transaction management for multi-service workflows: ``` Step 1: Create Order \u2192 SUCCESS \u2192 Step 2: Reserve Inventory \u2502 FAILURE \u2502 \u25bc Compensating Transaction \u2502 \u25bc Cancel Order (Rollback) ``` Orchestrated by Workflow Engine with compensation logic defined in workflow DSL. ### Dead Letter Queue (DLQ) Failed messages after all retry attempts are routed to DLQ: ``` Message processing fails (3 retries with exponential backoff) \u2502 \u25bc Publish to dead-letter-queue topic \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u25bc \u25bc Alert to PagerDuty Store in PostgreSQL for analysis \u2502 \u2502 \u25bc \u25bc Manual investigation Automated pattern detection ``` DLQ Processing SLA: < 4 hours for critical events, < 24 hours for non-critical. --- ## Caching Strategy ### Cache-Aside Pattern (Lazy Loading) Primary caching pattern used across all services: ``` 1. Application checks cache (Redis GET) 2. If HIT \u2192 Return cached data 3. If MISS: a. Query database (PostgreSQL) b. Store result in cache with TTL c. Return data to application ``` **Implementation Example** (Workflow Definition): ```javascript async function getWorkflowDefinition(workflowId) { const cacheKey = `workflow:def:${workflowId}`; // Step 1: Check cache let workflow = await redis.get(cacheKey); if (workflow) { metrics.increment('cache.hit.workflow_def'); return JSON.parse(workflow); } // Step 2: Cache miss - query database metrics.increment('cache.miss.workflow_def'); workflow = await db.query( 'SELECT * FROM workflows WHERE id = $1', [workflowId] ); // Step 3: Store in cache (1 hour TTL) await redis.setex(cacheKey, 3600, JSON.stringify(workflow)); return workflow; } ``` ### Write-Through Pattern Used for critical data where cache consistency is paramount: ``` 1. Application writes to database 2. Database transaction commits 3. Application updates cache 4. Return success to client ``` Applied to: User profiles, authentication sessions, system configuration.",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_0",
              "content": "# CloudFlow Platform Troubleshooting Guide **Version:** 3.2.1 **Last Updated:** January 2026 **Audience:** Platform Engineers, DevOps, Support Teams ## Table of Contents 1. [Overview](#overview) 2. [Authentication & Authorization Issues](#authentication--authorization-issues) 3. [Performance Problems](#performance-problems) 4. [Database Connection Issues](#database-connection-issues) 5. [Workflow Execution Failures](#workflow-execution-failures) 6. [Rate Limiting & Throttling](#rate-limiting--throttling) 7. [Log Analysis & Debugging](#log-analysis--debugging) 8. [Escalation Procedures](#escalation-procedures) --- ## Overview This guide provides comprehensive troubleshooting steps for common CloudFlow platform issues encountered in production environments. Each section includes error symptoms, root cause analysis, resolution steps, and preventive measures. ### Quick Diagnostic Checklist Before diving into specific issues, perform these initial checks: - Verify service health: `cloudflow status --all` - Check API connectivity: `curl -I https://api.cloudflow.io/health` - Review recent deployments: `kubectl get deployments -n cloudflow --sort-by=.metadata.creationTimestamp` - Inspect platform metrics: `cloudflow metrics --last 1h` --- ## Authentication & Authorization Issues ### 401 Unauthorized Errors #### Symptoms - API requests return `401 Unauthorized` - Error message: `Authentication credentials were not provided or are invalid` - Frontend displays \"Session expired\" message #### Common Causes **1. Token Expiration** CloudFlow access tokens expire after 3600 seconds (1 hour) by default. Refresh tokens are valid for 30 days. **Verification:** ```bash # Decode JWT to check expiration echo $CF_ACCESS_TOKEN | cut -d'.' -f2 | base64 -d | jq '.exp' # Compare with current time date +%s ``` **Resolution:** ```bash # Refresh the access token curl -X POST https://api.cloudflow.io/auth/refresh \\ -H \"Content-Type: application/json\" \\ -d '{\"refresh_token\": \"'$CF_REFRESH_TOKEN'\"}' # Update environment variable export CF_ACCESS_TOKEN=\"<new_token>\" ``` **2. Invalid JWT Signature** Error: `JWT signature verification failed` **Causes:** - Token was modified or corrupted - Using wrong signing key - Token generated with different secret **Resolution:** ```bash # Validate token structure cloudflow auth validate --token $CF_ACCESS_TOKEN # Generate new token cloudflow auth login --username <user> --password <pass> # For service accounts cloudflow auth service-account --create --name <sa-name> --scopes \"workflow:read,workflow:write\" ``` **3. Clock Skew Issues** JWT validation fails when system clocks are out of sync (tolerance: \u00b1300 seconds).",
              "score": null
            }
          ],
          "latency_ms": 13.018722002016148,
          "query_id": "realistic_011",
          "dimension": "negation",
          "query": "Why did my workflow fail after 3600 seconds?"
        },
        {
          "key_facts": [
            "RS256",
            "RS256 algorithm",
            "RS256 signing algorithm"
          ],
          "found_facts": [
            "RS256",
            "RS256 algorithm",
            "RS256 signing algorithm"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "api_reference"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_1",
              "content": "**Technology**: Node.js with Express.js framework **Replicas**: 12 pods (production), auto-scaling 8-20 based on CPU **Resource Allocation**: 2 vCPU, 4GB RAM per pod **Key Responsibilities**: - JWT token validation (delegated to Auth Service for initial validation) - Rate limiting: 1000 requests per minute per API key (sliding window) - Request/response transformation and validation using JSON Schema - Routing to downstream services based on URL path patterns - CORS handling for web clients - Request/response logging and correlation ID injection **Critical Endpoints**: - `POST /api/v1/workflows` - Create new workflow - `GET /api/v1/workflows/:id` - Retrieve workflow status - `POST /api/v1/workflows/:id/execute` - Trigger workflow execution - `GET /api/v1/workflows/:id/history` - Get execution history **Dependencies**: - Auth Service (for token validation) - Redis (for rate limiting counters) - All downstream microservices **Performance Targets**: - P50 latency: < 50ms - P99 latency: < 200ms - Throughput: 10,000 RPS sustained --- ### Auth Service **Purpose**: Centralized authentication and authorization service handling user identity, token generation, and permission validation. **Technology**: Go with gRPC for internal communication, REST for external **Replicas**: 8 pods (production), auto-scaling 6-12 **Resource Allocation**: 1 vCPU, 2GB RAM per pod **Key Responsibilities**: - User authentication via multiple providers (OAuth2, SAML, local credentials) - JWT token generation and validation (RS256 algorithm) - Role-based access control (RBAC) with fine-grained permissions - Session management with Redis-backed storage - API key generation and validation for service accounts - MFA enforcement for administrative operations **Authentication Flow**: ``` Client Request \u2192 API Gateway \u2192 Auth Service \u2502 \u251c\u2500 Validate credentials \u251c\u2500 Check MFA if required \u251c\u2500 Generate JWT (15min expiry) \u251c\u2500 Generate refresh token (7 days) \u2514\u2500 Store session in Redis ``` **Token Structure**: - Access Token: JWT with 15-minute expiry - Refresh Token: Opaque token with 7-day expiry, stored in PostgreSQL - Claims: user_id, email, roles[], permissions[], tenant_id **Security Features**: - Password hashing: Argon2id with 64MB memory, 4 iterations - Token rotation on refresh to prevent replay attacks - Brute force protection: 5 failed attempts \u2192 15-minute lockout - Secrets stored in HashiCorp Vault, rotated every 90 days **Performance Targets**: - Token validation: < 10ms (P99) - Token generation: < 50ms (P99) - Throughput: 5,000 RPS for validation operations --- ### Workflow Engine **Purpose**: Core orchestration service that executes workflow definitions, manages state transitions, and coordinates task execution across distributed systems.",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_6",
              "content": "**Session Storage**: - Key pattern: `session:{user_id}` - TTL: 15 minutes (aligned with JWT expiry) - Data: User session state, preferences - Invalidation: On logout or password change 2. **Rate Limiting Counters**: - Key pattern: `ratelimit:{api_key}:{window}` - TTL: 60 seconds (sliding window) - Data: Request count per window - Algorithm: Token bucket with Redis INCR 3. **Compiled Templates**: - Key pattern: `template:{template_id}:{version}` - TTL: 1 hour - Data: Compiled Handlebars template - Invalidation: On template update 4. **Workflow Definitions**: - Key pattern: `workflow:def:{workflow_id}` - TTL: 1 hour - Data: Parsed workflow JSON - Invalidation: On workflow update or manual flush 5. **User Profiles**: - Key pattern: `user:profile:{user_id}` - TTL: 30 minutes - Data: User metadata (name, email, roles) - Invalidation: On profile update **Cache Hit Rates** (Production Metrics): - Session lookups: 98.5% - Workflow definitions: 94.2% - Template cache: 99.1% - User profiles: 91.8% **Performance Characteristics**: - GET operation P99: < 2ms - SET operation P99: < 3ms - Throughput: 100,000 operations per second - Network latency: < 1ms (same AZ) --- ### Kafka Event Streaming **Cluster Configuration**: - 5 broker nodes (distributed across 3 AZs) - Instance type: kafka.m5.2xlarge (8 vCPU, 32GB RAM) - Storage: 10TB per broker (gp3 SSD) - ZooKeeper ensemble: 3 nodes for cluster coordination - Replication factor: 3 (min in-sync replicas: 2) **Topic Architecture**: ``` workflow.events (32 partitions): - Workflow lifecycle events (created, started, completed, failed) - Retention: 7 days - Message rate: 5,000/sec peak - Consumer groups: workflow-engine, analytics-pipeline notifications.email (16 partitions): - Email notification triggers - Retention: 3 days - Message rate: 2,000/sec peak - Consumer groups: notification-service notifications.sms (8 partitions): - SMS notification triggers - Retention: 3 days - Message rate: 500/sec peak - Consumer groups: notification-service audit.logs (24 partitions): - Security and compliance audit events - Retention: 90 days (compliance requirement) - Message rate: 3,000/sec peak - Consumer groups: audit-processor, security-monitor dead-letter-queue (8 partitions): - Failed message processing - Retention: 30 days - Manual intervention required - Consumer groups: ops-team-alerts ``` **Producer Configuration**: - Acknowledgment: `acks=all` (wait for all in-sync replicas) - Compression: LZ4 (reduces network bandwidth by 60%) - Batching: 100ms linger time, 100KB batch size - Idempotence: Enabled to prevent duplicates **Consumer Configuration**: - Auto-commit: Disabled (manual commit after processing) - Offset reset: Earliest (replay from beginning on new consumer group) - Max poll",
              "score": null
            },
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_0",
              "content": "# CloudFlow API Reference Version 2.1.0 | Last Updated: January 2026 ## Overview The CloudFlow API is a RESTful service that enables developers to programmatically manage cloud workflows, data pipelines, and automation tasks. This documentation provides comprehensive details on authentication, endpoints, request/response formats, error handling, and best practices. **Base URL:** `https://api.cloudflow.io/v2` **API Status:** https://status.cloudflow.io ## Authentication CloudFlow supports three authentication methods to suit different use cases and security requirements. ### API Keys API keys provide simple authentication for server-to-server communication. Include your API key in the request header: ```bash curl -H \"X-API-Key: cf_live_a1b2c3d4e5f6g7h8i9j0\" \\ https://api.cloudflow.io/v2/workflows ``` **Security Notes:** - Never expose API keys in client-side code - Rotate keys every 90 days - Use separate keys for development and production environments ### OAuth 2.0 OAuth 2.0 is recommended for applications that access CloudFlow on behalf of users. We support the Authorization Code flow with PKCE. **Authorization Endpoint:** `https://auth.cloudflow.io/oauth/authorize` **Token Endpoint:** `https://auth.cloudflow.io/oauth/token` **Supported Scopes:** - `workflows:read` - Read workflow configurations - `workflows:write` - Create and modify workflows - `pipelines:read` - Read pipeline data - `pipelines:write` - Create and manage pipelines - `analytics:read` - Access analytics and metrics - `admin:full` - Full administrative access Example authorization request: ```bash curl -X POST https://auth.cloudflow.io/oauth/token \\ -H \"Content-Type: application/x-www-form-urlencoded\" \\ -d \"grant_type=authorization_code\" \\ -d \"code=AUTH_CODE_HERE\" \\ -d \"client_id=YOUR_CLIENT_ID\" \\ -d \"client_secret=YOUR_CLIENT_SECRET\" \\ -d \"redirect_uri=https://yourapp.com/callback\" ``` ### JWT Tokens For advanced use cases, CloudFlow supports JSON Web Tokens (JWT) with RS256 signing algorithm. JWTs must include the following claims: - `iss` (issuer): Your application identifier - `sub` (subject): User or service account ID - `aud` (audience): `https://api.cloudflow.io` - `exp` (expiration): Unix timestamp (max 3600 seconds from `iat`) - `iat` (issued at): Unix timestamp - `scope`: Space-separated list of requested scopes Example JWT header: ```python import jwt import time from cryptography.hazmat.primitives import serialization from cryptography.hazmat.backends import default_backend # Load your private key with open('private_key.pem', 'rb') as key_file: private_key = serialization.load_pem_private_key( key_file.read(), password=None, backend=default_backend() ) # Create JWT payload payload = { 'iss': 'your-app-id', 'sub': 'user-12345', 'aud': 'https://api.cloudflow.io', 'exp': int(time.time()) + 3600, 'iat': int(time.time()), 'scope': 'workflows:read workflows:write' } # Generate token token = jwt.encode(payload, private_key, algorithm='RS256') # Use in API request headers = {'Authorization': f'Bearer {token}'} ``` **Token Expiration:** All tokens expire after 3600 seconds (1 hour). Implement token refresh logic in your application.",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_0",
              "content": "# CloudFlow Platform Troubleshooting Guide **Version:** 3.2.1 **Last Updated:** January 2026 **Audience:** Platform Engineers, DevOps, Support Teams ## Table of Contents 1. [Overview](#overview) 2. [Authentication & Authorization Issues](#authentication--authorization-issues) 3. [Performance Problems](#performance-problems) 4. [Database Connection Issues](#database-connection-issues) 5. [Workflow Execution Failures](#workflow-execution-failures) 6. [Rate Limiting & Throttling](#rate-limiting--throttling) 7. [Log Analysis & Debugging](#log-analysis--debugging) 8. [Escalation Procedures](#escalation-procedures) --- ## Overview This guide provides comprehensive troubleshooting steps for common CloudFlow platform issues encountered in production environments. Each section includes error symptoms, root cause analysis, resolution steps, and preventive measures. ### Quick Diagnostic Checklist Before diving into specific issues, perform these initial checks: - Verify service health: `cloudflow status --all` - Check API connectivity: `curl -I https://api.cloudflow.io/health` - Review recent deployments: `kubectl get deployments -n cloudflow --sort-by=.metadata.creationTimestamp` - Inspect platform metrics: `cloudflow metrics --last 1h` --- ## Authentication & Authorization Issues ### 401 Unauthorized Errors #### Symptoms - API requests return `401 Unauthorized` - Error message: `Authentication credentials were not provided or are invalid` - Frontend displays \"Session expired\" message #### Common Causes **1. Token Expiration** CloudFlow access tokens expire after 3600 seconds (1 hour) by default. Refresh tokens are valid for 30 days. **Verification:** ```bash # Decode JWT to check expiration echo $CF_ACCESS_TOKEN | cut -d'.' -f2 | base64 -d | jq '.exp' # Compare with current time date +%s ``` **Resolution:** ```bash # Refresh the access token curl -X POST https://api.cloudflow.io/auth/refresh \\ -H \"Content-Type: application/json\" \\ -d '{\"refresh_token\": \"'$CF_REFRESH_TOKEN'\"}' # Update environment variable export CF_ACCESS_TOKEN=\"<new_token>\" ``` **2. Invalid JWT Signature** Error: `JWT signature verification failed` **Causes:** - Token was modified or corrupted - Using wrong signing key - Token generated with different secret **Resolution:** ```bash # Validate token structure cloudflow auth validate --token $CF_ACCESS_TOKEN # Generate new token cloudflow auth login --username <user> --password <pass> # For service accounts cloudflow auth service-account --create --name <sa-name> --scopes \"workflow:read,workflow:write\" ``` **3. Clock Skew Issues** JWT validation fails when system clocks are out of sync (tolerance: \u00b1300 seconds).",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_8",
              "content": "### Cache Invalidation Strategies **Time-Based Expiration (TTL)**: - Short-lived data: 5-15 minutes (session tokens, rate limit counters) - Medium-lived data: 1 hour (workflow definitions, templates) - Long-lived data: 24 hours (static configuration) **Event-Based Invalidation**: ``` Database Update Event \u2192 Kafka (cache.invalidation topic) \u2502 \u25bc All service instances consume event \u2502 \u25bc Redis DEL for affected keys ``` **Pattern-Based Invalidation**: - Use Redis SCAN + DEL for wildcard patterns - Example: Invalidate all user caches: `user:*:{user_id}` **Cache Warming**: - Scheduled job runs every hour to pre-populate frequently accessed data - Targets: Top 1000 workflows, system configuration, active user profiles - Reduces cache miss rate during traffic spikes ### Multi-Level Caching Application implements L1 (in-memory) and L2 (Redis) caching: ``` Request \u2192 L1 Cache (Node.js Map, 1000 entry LRU) \u2502 MISS \u2502 \u25bc L2 Cache (Redis, distributed) \u2502 MISS \u2502 \u25bc Database (PostgreSQL) ``` L1 cache reduces Redis network calls by 40% for hot data. --- ## Security Architecture ### Network Security **Zero-Trust Network Model**: - All service-to-service communication encrypted with mTLS - Certificate rotation: Every 90 days (automated via cert-manager) - Certificate authority: Internal PKI with HashiCorp Vault - Network segmentation: Private subnets for services, public subnet for ALB only **Service Mesh (Istio)**: ``` Service A \u2192 Envoy Sidecar (mTLS client cert) \u2502 Encrypted channel \u2502 Envoy Sidecar (mTLS server cert) \u2192 Service B ``` **Firewall Rules**: - Security groups: Deny all by default, explicit allow rules - Ingress: Only ALB can reach API Gateway (port 8080) - Egress: Services can only reach specific dependencies - Database access: Limited to application subnets only ### Authentication & Authorization **JWT Token Validation**: - Algorithm: RS256 (asymmetric signing) - Key rotation: Every 30 days with 7-day overlap period - Public key distribution: JWKS endpoint cached in Redis - Validation: Signature, expiry, issuer, audience claims - Token revocation: Blacklist in Redis for compromised tokens **Permission Model**: ``` User \u2192 Roles \u2192 Permissions \u2198 \u2197 Tenants (Multi-tenancy isolation) ``` Example permissions: - `workflow:read` - View workflows - `workflow:write` - Create/update workflows - `workflow:execute` - Trigger workflow execution - `workflow:delete` - Delete workflows - `admin:*` - All administrative operations **API Key Management**: - Format: `cfk_live_<32-char-random>` (production), `cfk_test_<32-char-random>` (sandbox) - Hashing: SHA-256 before storage in PostgreSQL - Scoping: API keys can be scoped to specific workflows or operations - Rate limits: Configurable",
              "score": null
            }
          ],
          "latency_ms": 12.880835001851665,
          "query_id": "realistic_012",
          "dimension": "original",
          "query": "What JWT algorithm is used for token signing?"
        },
        {
          "key_facts": [
            "RS256",
            "RS256 algorithm",
            "RS256 signing algorithm"
          ],
          "found_facts": [
            "RS256",
            "RS256 signing algorithm"
          ],
          "missed_facts": [
            "RS256 algorithm"
          ],
          "coverage": 0.6666666666666666,
          "expected_docs": [
            "api_reference"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_0",
              "content": "# CloudFlow API Reference Version 2.1.0 | Last Updated: January 2026 ## Overview The CloudFlow API is a RESTful service that enables developers to programmatically manage cloud workflows, data pipelines, and automation tasks. This documentation provides comprehensive details on authentication, endpoints, request/response formats, error handling, and best practices. **Base URL:** `https://api.cloudflow.io/v2` **API Status:** https://status.cloudflow.io ## Authentication CloudFlow supports three authentication methods to suit different use cases and security requirements. ### API Keys API keys provide simple authentication for server-to-server communication. Include your API key in the request header: ```bash curl -H \"X-API-Key: cf_live_a1b2c3d4e5f6g7h8i9j0\" \\ https://api.cloudflow.io/v2/workflows ``` **Security Notes:** - Never expose API keys in client-side code - Rotate keys every 90 days - Use separate keys for development and production environments ### OAuth 2.0 OAuth 2.0 is recommended for applications that access CloudFlow on behalf of users. We support the Authorization Code flow with PKCE. **Authorization Endpoint:** `https://auth.cloudflow.io/oauth/authorize` **Token Endpoint:** `https://auth.cloudflow.io/oauth/token` **Supported Scopes:** - `workflows:read` - Read workflow configurations - `workflows:write` - Create and modify workflows - `pipelines:read` - Read pipeline data - `pipelines:write` - Create and manage pipelines - `analytics:read` - Access analytics and metrics - `admin:full` - Full administrative access Example authorization request: ```bash curl -X POST https://auth.cloudflow.io/oauth/token \\ -H \"Content-Type: application/x-www-form-urlencoded\" \\ -d \"grant_type=authorization_code\" \\ -d \"code=AUTH_CODE_HERE\" \\ -d \"client_id=YOUR_CLIENT_ID\" \\ -d \"client_secret=YOUR_CLIENT_SECRET\" \\ -d \"redirect_uri=https://yourapp.com/callback\" ``` ### JWT Tokens For advanced use cases, CloudFlow supports JSON Web Tokens (JWT) with RS256 signing algorithm. JWTs must include the following claims: - `iss` (issuer): Your application identifier - `sub` (subject): User or service account ID - `aud` (audience): `https://api.cloudflow.io` - `exp` (expiration): Unix timestamp (max 3600 seconds from `iat`) - `iat` (issued at): Unix timestamp - `scope`: Space-separated list of requested scopes Example JWT header: ```python import jwt import time from cryptography.hazmat.primitives import serialization from cryptography.hazmat.backends import default_backend # Load your private key with open('private_key.pem', 'rb') as key_file: private_key = serialization.load_pem_private_key( key_file.read(), password=None, backend=default_backend() ) # Create JWT payload payload = { 'iss': 'your-app-id', 'sub': 'user-12345', 'aud': 'https://api.cloudflow.io', 'exp': int(time.time()) + 3600, 'iat': int(time.time()), 'scope': 'workflows:read workflows:write' } # Generate token token = jwt.encode(payload, private_key, algorithm='RS256') # Use in API request headers = {'Authorization': f'Bearer {token}'} ``` **Token Expiration:** All tokens expire after 3600 seconds (1 hour). Implement token refresh logic in your application.",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_0",
              "content": "# CloudFlow Platform Troubleshooting Guide **Version:** 3.2.1 **Last Updated:** January 2026 **Audience:** Platform Engineers, DevOps, Support Teams ## Table of Contents 1. [Overview](#overview) 2. [Authentication & Authorization Issues](#authentication--authorization-issues) 3. [Performance Problems](#performance-problems) 4. [Database Connection Issues](#database-connection-issues) 5. [Workflow Execution Failures](#workflow-execution-failures) 6. [Rate Limiting & Throttling](#rate-limiting--throttling) 7. [Log Analysis & Debugging](#log-analysis--debugging) 8. [Escalation Procedures](#escalation-procedures) --- ## Overview This guide provides comprehensive troubleshooting steps for common CloudFlow platform issues encountered in production environments. Each section includes error symptoms, root cause analysis, resolution steps, and preventive measures. ### Quick Diagnostic Checklist Before diving into specific issues, perform these initial checks: - Verify service health: `cloudflow status --all` - Check API connectivity: `curl -I https://api.cloudflow.io/health` - Review recent deployments: `kubectl get deployments -n cloudflow --sort-by=.metadata.creationTimestamp` - Inspect platform metrics: `cloudflow metrics --last 1h` --- ## Authentication & Authorization Issues ### 401 Unauthorized Errors #### Symptoms - API requests return `401 Unauthorized` - Error message: `Authentication credentials were not provided or are invalid` - Frontend displays \"Session expired\" message #### Common Causes **1. Token Expiration** CloudFlow access tokens expire after 3600 seconds (1 hour) by default. Refresh tokens are valid for 30 days. **Verification:** ```bash # Decode JWT to check expiration echo $CF_ACCESS_TOKEN | cut -d'.' -f2 | base64 -d | jq '.exp' # Compare with current time date +%s ``` **Resolution:** ```bash # Refresh the access token curl -X POST https://api.cloudflow.io/auth/refresh \\ -H \"Content-Type: application/json\" \\ -d '{\"refresh_token\": \"'$CF_REFRESH_TOKEN'\"}' # Update environment variable export CF_ACCESS_TOKEN=\"<new_token>\" ``` **2. Invalid JWT Signature** Error: `JWT signature verification failed` **Causes:** - Token was modified or corrupted - Using wrong signing key - Token generated with different secret **Resolution:** ```bash # Validate token structure cloudflow auth validate --token $CF_ACCESS_TOKEN # Generate new token cloudflow auth login --username <user> --password <pass> # For service accounts cloudflow auth service-account --create --name <sa-name> --scopes \"workflow:read,workflow:write\" ``` **3. Clock Skew Issues** JWT validation fails when system clocks are out of sync (tolerance: \u00b1300 seconds).",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_1",
              "content": "**Diagnosis:** ```bash # Check system time timedatectl status # Compare with NTP server ntpdate -q pool.ntp.org # Check JWT issued time vs current time jwt_iat=$(echo $CF_ACCESS_TOKEN | cut -d'.' -f2 | base64 -d | jq '.iat') current_time=$(date +%s) skew=$((current_time - jwt_iat)) echo \"Clock skew: $skew seconds\" ``` **Resolution:** ```bash # Sync with NTP server sudo ntpdate -s pool.ntp.org # Enable automatic time sync sudo timedatectl set-ntp true # Restart CloudFlow client cloudflow restart ``` **4. Insufficient Permissions** Error: `User does not have required permissions for this operation` **Check user roles:** ```bash cloudflow auth whoami --verbose # Expected output: # User: john.doe@company.com # Roles: developer, workflow-admin # Scopes: workflow:*, data:read, metrics:read ``` **Request permission elevation:** ```bash # Submit access request cloudflow auth request-access \\ --resource \"workflow:production:deploy\" \\ --justification \"Deploy critical hotfix for TICKET-1234\" # Check pending approvals cloudflow auth list-requests --status pending ``` ### 403 Forbidden Errors #### RBAC Policy Violations CloudFlow uses role-based access control (RBAC) with the following hierarchy: - `viewer` - Read-only access - `developer` - Create and modify workflows (non-production) - `operator` - Execute workflows, view logs - `admin` - Full access to workspace - `platform-admin` - Cross-workspace administration **Verify resource permissions:** ```bash # Check effective permissions cloudflow rbac check \\ --user john.doe@company.com \\ --resource workflow:prod-pipeline \\ --action execute # List all policies affecting user cloudflow rbac policies --user john.doe@company.com --verbose ``` --- ## Performance Problems ### Slow Query Performance #### Symptoms - API response times > 5000ms - Database query latency warnings in logs - CloudFlow UI becomes unresponsive - Timeout errors: `Request exceeded maximum duration of 30000ms` #### Diagnosis Steps **1. Identify Slow Queries** ```bash # Query CloudFlow metrics cloudflow metrics query --metric api_request_duration_ms \\ --filter \"p95 > 5000\" \\ --last 1h # Check database slow query log kubectl logs -n cloudflow deploy/cloudflow-db-primary | \\ grep \"slow query\" | \\ tail -n 50 # Analyze query patterns cloudflow db analyze-queries --min-duration 5000 --limit 20 ``` **2.",
              "score": null
            },
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_0",
              "content": "# CloudFlow Platform - Deployment and Operations Guide **Version:** 2.4.0 **Last Updated:** January 2026 **Target Environment:** Production (AWS EKS) ## Table of Contents 1. [Overview](#overview) 2. [Prerequisites](#prerequisites) 3. [Infrastructure Setup](#infrastructure-setup) 4. [Kubernetes Deployment](#kubernetes-deployment) 5. [Database Configuration](#database-configuration) 6. [Monitoring and Observability](#monitoring-and-observability) 7. [Backup and Disaster Recovery](#backup-and-disaster-recovery) 8. [Scaling and Performance](#scaling-and-performance) 9. [Security and Compliance](#security-and-compliance) 10. [Troubleshooting](#troubleshooting) --- ## Overview CloudFlow is a cloud-native workflow orchestration platform designed for high-availability production environments. This guide provides comprehensive instructions for deploying and operating CloudFlow on Amazon EKS (Elastic Kubernetes Service). ### Architecture Summary CloudFlow consists of the following components: - **API Server**: REST API for workflow management (Node.js/Express) - **Worker Service**: Background job processor (Node.js) - **Scheduler**: Cron-based task scheduler (Node.js) - **PostgreSQL**: Primary data store (version 14) - **Redis**: Cache and message queue (version 7.0) - **PgBouncer**: Database connection pooler ### Deployment Model - **Namespace**: `cloudflow-prod` - **Cluster**: AWS EKS 1.28 - **Region**: us-east-1 (primary), us-west-2 (disaster recovery) - **High Availability**: Multi-AZ deployment across 3 availability zones --- ## Prerequisites Before beginning the deployment process, ensure you have the following: ### Required Tools - `kubectl` (v1.28 or later) - `helm` (v3.12 or later) - `aws-cli` (v2.13 or later) - `eksctl` (v0.165 or later) - `terraform` (v1.6 or later) - for infrastructure provisioning ### Access Requirements - AWS account with appropriate IAM permissions - EKS cluster admin access - Container registry access (ECR) - Domain name and SSL certificates - Secrets management access (AWS Secrets Manager or Vault) ### Network Requirements - VPC with at least 3 public and 3 private subnets - NAT Gateway configured - Security groups allowing: - Ingress: HTTPS (443), HTTP (80) - Internal: PostgreSQL (5432), Redis (6379) - Monitoring: Prometheus (9090), Grafana (3000) --- ## Infrastructure Setup ### EKS Cluster Creation Use the following `eksctl` configuration to create the EKS cluster: ```yaml # cluster-config.yaml apiVersion: eksctl.io/v1alpha5 kind: ClusterConfig metadata: name: cloudflow-production region: us-east-1 version: \"1.28\" availabilityZones: - us-east-1a - us-east-1b - us-east-1c vpc: cidr: 10.0.0.0/16 nat: gateway: HighlyAvailable managedNodeGroups: - name: cloudflow-workers instanceType: m5.xlarge minSize: 3 maxSize: 10 desiredCapacity: 5 volumeSize: 100 privateNetworking: true labels: role: worker environment: production tags: Environment: production Application: cloudflow iam: withAddonPolicies: ebs: true efs: true albIngress: true cloudWatch: true cloudWatch: clusterLogging: enableTypes: - api - audit - authenticator - controllerManager - scheduler ``` Create the cluster: ```bash eksctl",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_0",
              "content": "# CloudFlow Platform - System Architecture Overview **Document Version:** 2.3.1 **Last Updated:** January 15, 2026 **Owner:** Platform Architecture Team **Status:** Production ## Executive Summary CloudFlow is a distributed, cloud-native workflow automation platform designed to orchestrate complex business processes at scale. The platform processes over 2.5 million workflow executions daily with an average P99 latency of 180ms for API operations and 4.2 seconds for workflow execution. This document provides a comprehensive overview of the system architecture, including microservices design, data flow patterns, infrastructure decisions, and operational considerations. ## Table of Contents 1. [High-Level Architecture](#high-level-architecture) 2. [Microservices Breakdown](#microservices-breakdown) 3. [Data Flow Architecture](#data-flow-architecture) 4. [Database Architecture](#database-architecture) 5. [Message Queue Patterns](#message-queue-patterns) 6. [Caching Strategy](#caching-strategy) 7. [Security Architecture](#security-architecture) 8. [Performance Characteristics](#performance-characteristics) 9. [Disaster Recovery](#disaster-recovery) --- ## High-Level Architecture CloudFlow follows a microservices architecture pattern deployed across multiple availability zones in AWS. The platform is designed with the following core principles: - **Scalability**: Horizontal scaling for all services with auto-scaling groups - **Resilience**: Circuit breakers, bulkheads, and graceful degradation - **Observability**: Distributed tracing, centralized logging, and comprehensive metrics - **Security**: Zero-trust network architecture with mTLS encryption ### Architecture Diagram (Conceptual) ``` \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 Load Balancer (ALB) \u2502 \u2502 (TLS Termination - 443) \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u25bc \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 API Gateway Layer \u2502 \u2502 (Rate Limiting, Auth, Request Routing) \u2502 \u2514\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u2502 \u2502 \u2502 \u2502 \u25bc \u25bc \u25bc \u25bc \u25bc \u250c\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502Auth\u2502 \u2502Workflow\u2502 \u2502Scheduler\u2502 \u2502Notification\u2502 \u2502User Service\u2502 \u2502Svc \u2502 \u2502 Engine \u2502 \u2502 Service \u2502 \u2502 Service \u2502 \u2502 \u2502 \u2514\u2500\u252c\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u2502 \u2502 \u2502 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 \u2502 \u2502 \u25bc \u25bc \u25bc \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502PostgreSQL\u2502 \u2502 Redis \u2502 \u2502 Kafka \u2502 \u2502 Cluster \u2502 \u2502 Cluster \u2502 \u2502 Cluster \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 ``` ### Technology Stack - **Runtime**: Node.js 20.x (API Gateway, Workflow Engine), Go 1.21 (Auth Service, Scheduler) - **Container Orchestration**: Kubernetes 1.28 (EKS) - **Service Mesh**: Istio 1.20 for service-to-service communication - **Databases**: PostgreSQL 15.4, Redis 7.2 - **Message Broker**: Apache Kafka 3.6 - **Monitoring**: Prometheus, Grafana, Jaeger for distributed tracing - **Secrets Management**: HashiCorp Vault 1.15 --- ## Microservices Breakdown ### API Gateway **Purpose**: Single entry point for all client requests, providing authentication, rate limiting, request routing, and protocol translation.",
              "score": null
            }
          ],
          "latency_ms": 12.80266899993876,
          "query_id": "realistic_012",
          "dimension": "synonym",
          "query": "Which signing algorithm does CloudFlow use for JWTs?"
        },
        {
          "key_facts": [
            "RS256",
            "RS256 algorithm",
            "RS256 signing algorithm"
          ],
          "found_facts": [
            "RS256",
            "RS256 algorithm"
          ],
          "missed_facts": [
            "RS256 signing algorithm"
          ],
          "coverage": 0.6666666666666666,
          "expected_docs": [
            "api_reference"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_1",
              "content": "**Technology**: Node.js with Express.js framework **Replicas**: 12 pods (production), auto-scaling 8-20 based on CPU **Resource Allocation**: 2 vCPU, 4GB RAM per pod **Key Responsibilities**: - JWT token validation (delegated to Auth Service for initial validation) - Rate limiting: 1000 requests per minute per API key (sliding window) - Request/response transformation and validation using JSON Schema - Routing to downstream services based on URL path patterns - CORS handling for web clients - Request/response logging and correlation ID injection **Critical Endpoints**: - `POST /api/v1/workflows` - Create new workflow - `GET /api/v1/workflows/:id` - Retrieve workflow status - `POST /api/v1/workflows/:id/execute` - Trigger workflow execution - `GET /api/v1/workflows/:id/history` - Get execution history **Dependencies**: - Auth Service (for token validation) - Redis (for rate limiting counters) - All downstream microservices **Performance Targets**: - P50 latency: < 50ms - P99 latency: < 200ms - Throughput: 10,000 RPS sustained --- ### Auth Service **Purpose**: Centralized authentication and authorization service handling user identity, token generation, and permission validation. **Technology**: Go with gRPC for internal communication, REST for external **Replicas**: 8 pods (production), auto-scaling 6-12 **Resource Allocation**: 1 vCPU, 2GB RAM per pod **Key Responsibilities**: - User authentication via multiple providers (OAuth2, SAML, local credentials) - JWT token generation and validation (RS256 algorithm) - Role-based access control (RBAC) with fine-grained permissions - Session management with Redis-backed storage - API key generation and validation for service accounts - MFA enforcement for administrative operations **Authentication Flow**: ``` Client Request \u2192 API Gateway \u2192 Auth Service \u2502 \u251c\u2500 Validate credentials \u251c\u2500 Check MFA if required \u251c\u2500 Generate JWT (15min expiry) \u251c\u2500 Generate refresh token (7 days) \u2514\u2500 Store session in Redis ``` **Token Structure**: - Access Token: JWT with 15-minute expiry - Refresh Token: Opaque token with 7-day expiry, stored in PostgreSQL - Claims: user_id, email, roles[], permissions[], tenant_id **Security Features**: - Password hashing: Argon2id with 64MB memory, 4 iterations - Token rotation on refresh to prevent replay attacks - Brute force protection: 5 failed attempts \u2192 15-minute lockout - Secrets stored in HashiCorp Vault, rotated every 90 days **Performance Targets**: - Token validation: < 10ms (P99) - Token generation: < 50ms (P99) - Throughput: 5,000 RPS for validation operations --- ### Workflow Engine **Purpose**: Core orchestration service that executes workflow definitions, manages state transitions, and coordinates task execution across distributed systems.",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_6",
              "content": "Data Validation Errors** ``` ValidationError: Field 'customer_id' is required but missing in 234 records ``` **Resolution:** ```bash # Add data quality checks cloudflow workflows update wf_9k2n4m8p1q \\ --step data_ingestion \\ --add-validator required_fields \\ --validator-config '{\"fields\": [\"customer_id\", \"timestamp\", \"amount\"]}' # Configure error handling cloudflow workflows update wf_9k2n4m8p1q \\ --step data_validation \\ --on-error continue \\ --error-threshold 5% # Fail if > 5% of records invalid ``` **2. External API Failures** ``` ExternalAPIError: API request to https://partner-api.example.com failed with status 502 ``` **Resolution:** ```bash # Add circuit breaker cloudflow workflows update wf_9k2n4m8p1q \\ --step external_api_call \\ --circuit-breaker-enabled true \\ --circuit-breaker-threshold 5 \\ --circuit-breaker-timeout 30000 # Configure fallback behavior cloudflow workflows update wf_9k2n4m8p1q \\ --step external_api_call \\ --fallback-action use_cached_data \\ --cache-ttl 3600 ``` --- ## Rate Limiting & Throttling ### 429 Too Many Requests #### Error Response ```json { \"error\": \"rate_limit_exceeded\", \"message\": \"API rate limit exceeded. Retry after 45 seconds.\", \"status\": 429, \"headers\": { \"X-RateLimit-Limit\": \"1000\", \"X-RateLimit-Remaining\": \"0\", \"X-RateLimit-Reset\": \"1706112345\", \"Retry-After\": \"45\" } } ``` #### Rate Limit Tiers CloudFlow enforces the following rate limits per workspace: | Tier | Requests/Minute | Requests/Hour | Concurrent Workflows | |------|-----------------|---------------|----------------------| | Free | 60 | 1,000 | 5 | | Standard | 1,000 | 50,000 | 50 | | Premium | 5,000 | 250,000 | 200 | | Enterprise | Custom | Custom | Unlimited | #### Checking Rate Limit Status ```bash # Check current rate limit status curl -I https://api.cloudflow.io/api/v1/workflows \\ -H \"Authorization: Bearer $CF_ACCESS_TOKEN\" # Extract rate limit headers curl -s -I https://api.cloudflow.io/api/v1/workflows \\ -H \"Authorization: Bearer $CF_ACCESS_TOKEN\" | \\ grep -i \"x-ratelimit\" # Output: # X-RateLimit-Limit: 1000 # X-RateLimit-Remaining: 247 # X-RateLimit-Reset: 1706112400 # Monitor rate limit usage cloudflow metrics query \\ --metric rate_limit_remaining \\ --workspace ws_abc123 \\ --last 1h \\ --interval 1m ``` #### Handling Rate Limits in Code **Python example with retry logic:** ```python import time import requests def cloudflow_api_call_with_retry(url, headers, max_retries=3): for attempt in range(max_retries): response = requests.get(url, headers=headers) if response.status_code == 429: retry_after = int(response.headers.get('Retry-After', 60)) print(f\"Rate limited.",
              "score": null
            },
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_4",
              "content": "### Error Response Format ```json { \"error\": { \"code\": \"invalid_parameter\", \"message\": \"The 'limit' parameter must be between 1 and 100\", \"field\": \"limit\", \"request_id\": \"req_8k3m9x2p\" } } ``` ### HTTP Status Codes | Status Code | Description | Common Causes | |------------|-------------|---------------| | 400 | Bad Request | Invalid parameters, malformed JSON, validation errors | | 401 | Unauthorized | Missing or invalid authentication credentials | | 403 | Forbidden | Insufficient permissions for requested resource | | 404 | Not Found | Resource does not exist or has been deleted | | 429 | Too Many Requests | Rate limit exceeded, retry after specified period | | 500 | Internal Server Error | Unexpected server error, contact support if persists | | 502 | Bad Gateway | Temporary service issue, retry with exponential backoff | | 503 | Service Unavailable | Scheduled maintenance or temporary outage | ### Error Codes CloudFlow returns specific error codes to help you identify and resolve issues: - `invalid_parameter`: One or more request parameters are invalid - `missing_required_field`: Required field is missing from request body - `authentication_failed`: Invalid API key or token - `insufficient_permissions`: User lacks required scope or permission - `resource_not_found`: Requested resource does not exist - `rate_limit_exceeded`: Too many requests, see rate limiting section - `workflow_execution_failed`: Workflow execution encountered an error - `invalid_json`: Request body contains malformed JSON - `duplicate_resource`: Resource with same identifier already exists - `quota_exceeded`: Account quota limit reached ### Error Handling Best Practices **Python Example:** ```python import requests import time def make_api_request(url, headers, max_retries=3): for attempt in range(max_retries): try: response = requests.get(url, headers=headers) if response.status_code == 200: return response.json() elif response.status_code == 429: # Rate limit exceeded retry_after = int(response.headers.get('X-RateLimit-Reset', 60)) print(f\"Rate limited. Waiting {retry_after} seconds...\") time.sleep(retry_after) continue elif response.status_code >= 500: # Server error - retry with exponential backoff wait_time = 2 ** attempt print(f\"Server error. Retrying in {wait_time} seconds...\") time.sleep(wait_time) continue else: # Client error - don't retry error_data = response.json() raise Exception(f\"API Error: {error_data['error']['message']}\") except requests.exceptions.RequestException as e: print(f\"Request failed: {e}\") if attempt < max_retries - 1: time.sleep(2 ** attempt) else: raise raise Exception(\"Max retries exceeded\") ``` ## Webhooks CloudFlow can send webhook notifications when specific events occur in your workflows.",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_0",
              "content": "# CloudFlow Platform Troubleshooting Guide **Version:** 3.2.1 **Last Updated:** January 2026 **Audience:** Platform Engineers, DevOps, Support Teams ## Table of Contents 1. [Overview](#overview) 2. [Authentication & Authorization Issues](#authentication--authorization-issues) 3. [Performance Problems](#performance-problems) 4. [Database Connection Issues](#database-connection-issues) 5. [Workflow Execution Failures](#workflow-execution-failures) 6. [Rate Limiting & Throttling](#rate-limiting--throttling) 7. [Log Analysis & Debugging](#log-analysis--debugging) 8. [Escalation Procedures](#escalation-procedures) --- ## Overview This guide provides comprehensive troubleshooting steps for common CloudFlow platform issues encountered in production environments. Each section includes error symptoms, root cause analysis, resolution steps, and preventive measures. ### Quick Diagnostic Checklist Before diving into specific issues, perform these initial checks: - Verify service health: `cloudflow status --all` - Check API connectivity: `curl -I https://api.cloudflow.io/health` - Review recent deployments: `kubectl get deployments -n cloudflow --sort-by=.metadata.creationTimestamp` - Inspect platform metrics: `cloudflow metrics --last 1h` --- ## Authentication & Authorization Issues ### 401 Unauthorized Errors #### Symptoms - API requests return `401 Unauthorized` - Error message: `Authentication credentials were not provided or are invalid` - Frontend displays \"Session expired\" message #### Common Causes **1. Token Expiration** CloudFlow access tokens expire after 3600 seconds (1 hour) by default. Refresh tokens are valid for 30 days. **Verification:** ```bash # Decode JWT to check expiration echo $CF_ACCESS_TOKEN | cut -d'.' -f2 | base64 -d | jq '.exp' # Compare with current time date +%s ``` **Resolution:** ```bash # Refresh the access token curl -X POST https://api.cloudflow.io/auth/refresh \\ -H \"Content-Type: application/json\" \\ -d '{\"refresh_token\": \"'$CF_REFRESH_TOKEN'\"}' # Update environment variable export CF_ACCESS_TOKEN=\"<new_token>\" ``` **2. Invalid JWT Signature** Error: `JWT signature verification failed` **Causes:** - Token was modified or corrupted - Using wrong signing key - Token generated with different secret **Resolution:** ```bash # Validate token structure cloudflow auth validate --token $CF_ACCESS_TOKEN # Generate new token cloudflow auth login --username <user> --password <pass> # For service accounts cloudflow auth service-account --create --name <sa-name> --scopes \"workflow:read,workflow:write\" ``` **3. Clock Skew Issues** JWT validation fails when system clocks are out of sync (tolerance: \u00b1300 seconds).",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_1",
              "content": "**Diagnosis:** ```bash # Check system time timedatectl status # Compare with NTP server ntpdate -q pool.ntp.org # Check JWT issued time vs current time jwt_iat=$(echo $CF_ACCESS_TOKEN | cut -d'.' -f2 | base64 -d | jq '.iat') current_time=$(date +%s) skew=$((current_time - jwt_iat)) echo \"Clock skew: $skew seconds\" ``` **Resolution:** ```bash # Sync with NTP server sudo ntpdate -s pool.ntp.org # Enable automatic time sync sudo timedatectl set-ntp true # Restart CloudFlow client cloudflow restart ``` **4. Insufficient Permissions** Error: `User does not have required permissions for this operation` **Check user roles:** ```bash cloudflow auth whoami --verbose # Expected output: # User: john.doe@company.com # Roles: developer, workflow-admin # Scopes: workflow:*, data:read, metrics:read ``` **Request permission elevation:** ```bash # Submit access request cloudflow auth request-access \\ --resource \"workflow:production:deploy\" \\ --justification \"Deploy critical hotfix for TICKET-1234\" # Check pending approvals cloudflow auth list-requests --status pending ``` ### 403 Forbidden Errors #### RBAC Policy Violations CloudFlow uses role-based access control (RBAC) with the following hierarchy: - `viewer` - Read-only access - `developer` - Create and modify workflows (non-production) - `operator` - Execute workflows, view logs - `admin` - Full access to workspace - `platform-admin` - Cross-workspace administration **Verify resource permissions:** ```bash # Check effective permissions cloudflow rbac check \\ --user john.doe@company.com \\ --resource workflow:prod-pipeline \\ --action execute # List all policies affecting user cloudflow rbac policies --user john.doe@company.com --verbose ``` --- ## Performance Problems ### Slow Query Performance #### Symptoms - API response times > 5000ms - Database query latency warnings in logs - CloudFlow UI becomes unresponsive - Timeout errors: `Request exceeded maximum duration of 30000ms` #### Diagnosis Steps **1. Identify Slow Queries** ```bash # Query CloudFlow metrics cloudflow metrics query --metric api_request_duration_ms \\ --filter \"p95 > 5000\" \\ --last 1h # Check database slow query log kubectl logs -n cloudflow deploy/cloudflow-db-primary | \\ grep \"slow query\" | \\ tail -n 50 # Analyze query patterns cloudflow db analyze-queries --min-duration 5000 --limit 20 ``` **2.",
              "score": null
            }
          ],
          "latency_ms": 12.929986001836369,
          "query_id": "realistic_012",
          "dimension": "problem",
          "query": "My JWT validation is failing with algorithm mismatch"
        },
        {
          "key_facts": [
            "RS256",
            "RS256 algorithm",
            "RS256 signing algorithm"
          ],
          "found_facts": [
            "RS256",
            "RS256 algorithm",
            "RS256 signing algorithm"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "api_reference"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_1",
              "content": "**Technology**: Node.js with Express.js framework **Replicas**: 12 pods (production), auto-scaling 8-20 based on CPU **Resource Allocation**: 2 vCPU, 4GB RAM per pod **Key Responsibilities**: - JWT token validation (delegated to Auth Service for initial validation) - Rate limiting: 1000 requests per minute per API key (sliding window) - Request/response transformation and validation using JSON Schema - Routing to downstream services based on URL path patterns - CORS handling for web clients - Request/response logging and correlation ID injection **Critical Endpoints**: - `POST /api/v1/workflows` - Create new workflow - `GET /api/v1/workflows/:id` - Retrieve workflow status - `POST /api/v1/workflows/:id/execute` - Trigger workflow execution - `GET /api/v1/workflows/:id/history` - Get execution history **Dependencies**: - Auth Service (for token validation) - Redis (for rate limiting counters) - All downstream microservices **Performance Targets**: - P50 latency: < 50ms - P99 latency: < 200ms - Throughput: 10,000 RPS sustained --- ### Auth Service **Purpose**: Centralized authentication and authorization service handling user identity, token generation, and permission validation. **Technology**: Go with gRPC for internal communication, REST for external **Replicas**: 8 pods (production), auto-scaling 6-12 **Resource Allocation**: 1 vCPU, 2GB RAM per pod **Key Responsibilities**: - User authentication via multiple providers (OAuth2, SAML, local credentials) - JWT token generation and validation (RS256 algorithm) - Role-based access control (RBAC) with fine-grained permissions - Session management with Redis-backed storage - API key generation and validation for service accounts - MFA enforcement for administrative operations **Authentication Flow**: ``` Client Request \u2192 API Gateway \u2192 Auth Service \u2502 \u251c\u2500 Validate credentials \u251c\u2500 Check MFA if required \u251c\u2500 Generate JWT (15min expiry) \u251c\u2500 Generate refresh token (7 days) \u2514\u2500 Store session in Redis ``` **Token Structure**: - Access Token: JWT with 15-minute expiry - Refresh Token: Opaque token with 7-day expiry, stored in PostgreSQL - Claims: user_id, email, roles[], permissions[], tenant_id **Security Features**: - Password hashing: Argon2id with 64MB memory, 4 iterations - Token rotation on refresh to prevent replay attacks - Brute force protection: 5 failed attempts \u2192 15-minute lockout - Secrets stored in HashiCorp Vault, rotated every 90 days **Performance Targets**: - Token validation: < 10ms (P99) - Token generation: < 50ms (P99) - Throughput: 5,000 RPS for validation operations --- ### Workflow Engine **Purpose**: Core orchestration service that executes workflow definitions, manages state transitions, and coordinates task execution across distributed systems.",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_6",
              "content": "**Session Storage**: - Key pattern: `session:{user_id}` - TTL: 15 minutes (aligned with JWT expiry) - Data: User session state, preferences - Invalidation: On logout or password change 2. **Rate Limiting Counters**: - Key pattern: `ratelimit:{api_key}:{window}` - TTL: 60 seconds (sliding window) - Data: Request count per window - Algorithm: Token bucket with Redis INCR 3. **Compiled Templates**: - Key pattern: `template:{template_id}:{version}` - TTL: 1 hour - Data: Compiled Handlebars template - Invalidation: On template update 4. **Workflow Definitions**: - Key pattern: `workflow:def:{workflow_id}` - TTL: 1 hour - Data: Parsed workflow JSON - Invalidation: On workflow update or manual flush 5. **User Profiles**: - Key pattern: `user:profile:{user_id}` - TTL: 30 minutes - Data: User metadata (name, email, roles) - Invalidation: On profile update **Cache Hit Rates** (Production Metrics): - Session lookups: 98.5% - Workflow definitions: 94.2% - Template cache: 99.1% - User profiles: 91.8% **Performance Characteristics**: - GET operation P99: < 2ms - SET operation P99: < 3ms - Throughput: 100,000 operations per second - Network latency: < 1ms (same AZ) --- ### Kafka Event Streaming **Cluster Configuration**: - 5 broker nodes (distributed across 3 AZs) - Instance type: kafka.m5.2xlarge (8 vCPU, 32GB RAM) - Storage: 10TB per broker (gp3 SSD) - ZooKeeper ensemble: 3 nodes for cluster coordination - Replication factor: 3 (min in-sync replicas: 2) **Topic Architecture**: ``` workflow.events (32 partitions): - Workflow lifecycle events (created, started, completed, failed) - Retention: 7 days - Message rate: 5,000/sec peak - Consumer groups: workflow-engine, analytics-pipeline notifications.email (16 partitions): - Email notification triggers - Retention: 3 days - Message rate: 2,000/sec peak - Consumer groups: notification-service notifications.sms (8 partitions): - SMS notification triggers - Retention: 3 days - Message rate: 500/sec peak - Consumer groups: notification-service audit.logs (24 partitions): - Security and compliance audit events - Retention: 90 days (compliance requirement) - Message rate: 3,000/sec peak - Consumer groups: audit-processor, security-monitor dead-letter-queue (8 partitions): - Failed message processing - Retention: 30 days - Manual intervention required - Consumer groups: ops-team-alerts ``` **Producer Configuration**: - Acknowledgment: `acks=all` (wait for all in-sync replicas) - Compression: LZ4 (reduces network bandwidth by 60%) - Batching: 100ms linger time, 100KB batch size - Idempotence: Enabled to prevent duplicates **Consumer Configuration**: - Auto-commit: Disabled (manual commit after processing) - Offset reset: Earliest (replay from beginning on new consumer group) - Max poll",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_1",
              "content": "**Diagnosis:** ```bash # Check system time timedatectl status # Compare with NTP server ntpdate -q pool.ntp.org # Check JWT issued time vs current time jwt_iat=$(echo $CF_ACCESS_TOKEN | cut -d'.' -f2 | base64 -d | jq '.iat') current_time=$(date +%s) skew=$((current_time - jwt_iat)) echo \"Clock skew: $skew seconds\" ``` **Resolution:** ```bash # Sync with NTP server sudo ntpdate -s pool.ntp.org # Enable automatic time sync sudo timedatectl set-ntp true # Restart CloudFlow client cloudflow restart ``` **4. Insufficient Permissions** Error: `User does not have required permissions for this operation` **Check user roles:** ```bash cloudflow auth whoami --verbose # Expected output: # User: john.doe@company.com # Roles: developer, workflow-admin # Scopes: workflow:*, data:read, metrics:read ``` **Request permission elevation:** ```bash # Submit access request cloudflow auth request-access \\ --resource \"workflow:production:deploy\" \\ --justification \"Deploy critical hotfix for TICKET-1234\" # Check pending approvals cloudflow auth list-requests --status pending ``` ### 403 Forbidden Errors #### RBAC Policy Violations CloudFlow uses role-based access control (RBAC) with the following hierarchy: - `viewer` - Read-only access - `developer` - Create and modify workflows (non-production) - `operator` - Execute workflows, view logs - `admin` - Full access to workspace - `platform-admin` - Cross-workspace administration **Verify resource permissions:** ```bash # Check effective permissions cloudflow rbac check \\ --user john.doe@company.com \\ --resource workflow:prod-pipeline \\ --action execute # List all policies affecting user cloudflow rbac policies --user john.doe@company.com --verbose ``` --- ## Performance Problems ### Slow Query Performance #### Symptoms - API response times > 5000ms - Database query latency warnings in logs - CloudFlow UI becomes unresponsive - Timeout errors: `Request exceeded maximum duration of 30000ms` #### Diagnosis Steps **1. Identify Slow Queries** ```bash # Query CloudFlow metrics cloudflow metrics query --metric api_request_duration_ms \\ --filter \"p95 > 5000\" \\ --last 1h # Check database slow query log kubectl logs -n cloudflow deploy/cloudflow-db-primary | \\ grep \"slow query\" | \\ tail -n 50 # Analyze query patterns cloudflow db analyze-queries --min-duration 5000 --limit 20 ``` **2.",
              "score": null
            },
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_0",
              "content": "# CloudFlow API Reference Version 2.1.0 | Last Updated: January 2026 ## Overview The CloudFlow API is a RESTful service that enables developers to programmatically manage cloud workflows, data pipelines, and automation tasks. This documentation provides comprehensive details on authentication, endpoints, request/response formats, error handling, and best practices. **Base URL:** `https://api.cloudflow.io/v2` **API Status:** https://status.cloudflow.io ## Authentication CloudFlow supports three authentication methods to suit different use cases and security requirements. ### API Keys API keys provide simple authentication for server-to-server communication. Include your API key in the request header: ```bash curl -H \"X-API-Key: cf_live_a1b2c3d4e5f6g7h8i9j0\" \\ https://api.cloudflow.io/v2/workflows ``` **Security Notes:** - Never expose API keys in client-side code - Rotate keys every 90 days - Use separate keys for development and production environments ### OAuth 2.0 OAuth 2.0 is recommended for applications that access CloudFlow on behalf of users. We support the Authorization Code flow with PKCE. **Authorization Endpoint:** `https://auth.cloudflow.io/oauth/authorize` **Token Endpoint:** `https://auth.cloudflow.io/oauth/token` **Supported Scopes:** - `workflows:read` - Read workflow configurations - `workflows:write` - Create and modify workflows - `pipelines:read` - Read pipeline data - `pipelines:write` - Create and manage pipelines - `analytics:read` - Access analytics and metrics - `admin:full` - Full administrative access Example authorization request: ```bash curl -X POST https://auth.cloudflow.io/oauth/token \\ -H \"Content-Type: application/x-www-form-urlencoded\" \\ -d \"grant_type=authorization_code\" \\ -d \"code=AUTH_CODE_HERE\" \\ -d \"client_id=YOUR_CLIENT_ID\" \\ -d \"client_secret=YOUR_CLIENT_SECRET\" \\ -d \"redirect_uri=https://yourapp.com/callback\" ``` ### JWT Tokens For advanced use cases, CloudFlow supports JSON Web Tokens (JWT) with RS256 signing algorithm. JWTs must include the following claims: - `iss` (issuer): Your application identifier - `sub` (subject): User or service account ID - `aud` (audience): `https://api.cloudflow.io` - `exp` (expiration): Unix timestamp (max 3600 seconds from `iat`) - `iat` (issued at): Unix timestamp - `scope`: Space-separated list of requested scopes Example JWT header: ```python import jwt import time from cryptography.hazmat.primitives import serialization from cryptography.hazmat.backends import default_backend # Load your private key with open('private_key.pem', 'rb') as key_file: private_key = serialization.load_pem_private_key( key_file.read(), password=None, backend=default_backend() ) # Create JWT payload payload = { 'iss': 'your-app-id', 'sub': 'user-12345', 'aud': 'https://api.cloudflow.io', 'exp': int(time.time()) + 3600, 'iat': int(time.time()), 'scope': 'workflows:read workflows:write' } # Generate token token = jwt.encode(payload, private_key, algorithm='RS256') # Use in API request headers = {'Authorization': f'Bearer {token}'} ``` **Token Expiration:** All tokens expire after 3600 seconds (1 hour). Implement token refresh logic in your application.",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_0",
              "content": "# CloudFlow Platform Troubleshooting Guide **Version:** 3.2.1 **Last Updated:** January 2026 **Audience:** Platform Engineers, DevOps, Support Teams ## Table of Contents 1. [Overview](#overview) 2. [Authentication & Authorization Issues](#authentication--authorization-issues) 3. [Performance Problems](#performance-problems) 4. [Database Connection Issues](#database-connection-issues) 5. [Workflow Execution Failures](#workflow-execution-failures) 6. [Rate Limiting & Throttling](#rate-limiting--throttling) 7. [Log Analysis & Debugging](#log-analysis--debugging) 8. [Escalation Procedures](#escalation-procedures) --- ## Overview This guide provides comprehensive troubleshooting steps for common CloudFlow platform issues encountered in production environments. Each section includes error symptoms, root cause analysis, resolution steps, and preventive measures. ### Quick Diagnostic Checklist Before diving into specific issues, perform these initial checks: - Verify service health: `cloudflow status --all` - Check API connectivity: `curl -I https://api.cloudflow.io/health` - Review recent deployments: `kubectl get deployments -n cloudflow --sort-by=.metadata.creationTimestamp` - Inspect platform metrics: `cloudflow metrics --last 1h` --- ## Authentication & Authorization Issues ### 401 Unauthorized Errors #### Symptoms - API requests return `401 Unauthorized` - Error message: `Authentication credentials were not provided or are invalid` - Frontend displays \"Session expired\" message #### Common Causes **1. Token Expiration** CloudFlow access tokens expire after 3600 seconds (1 hour) by default. Refresh tokens are valid for 30 days. **Verification:** ```bash # Decode JWT to check expiration echo $CF_ACCESS_TOKEN | cut -d'.' -f2 | base64 -d | jq '.exp' # Compare with current time date +%s ``` **Resolution:** ```bash # Refresh the access token curl -X POST https://api.cloudflow.io/auth/refresh \\ -H \"Content-Type: application/json\" \\ -d '{\"refresh_token\": \"'$CF_REFRESH_TOKEN'\"}' # Update environment variable export CF_ACCESS_TOKEN=\"<new_token>\" ``` **2. Invalid JWT Signature** Error: `JWT signature verification failed` **Causes:** - Token was modified or corrupted - Using wrong signing key - Token generated with different secret **Resolution:** ```bash # Validate token structure cloudflow auth validate --token $CF_ACCESS_TOKEN # Generate new token cloudflow auth login --username <user> --password <pass> # For service accounts cloudflow auth service-account --create --name <sa-name> --scopes \"workflow:read,workflow:write\" ``` **3. Clock Skew Issues** JWT validation fails when system clocks are out of sync (tolerance: \u00b1300 seconds).",
              "score": null
            }
          ],
          "latency_ms": 12.537606002297252,
          "query_id": "realistic_012",
          "dimension": "casual",
          "query": "jwt algorithm"
        },
        {
          "key_facts": [
            "RS256",
            "RS256 algorithm",
            "RS256 signing algorithm"
          ],
          "found_facts": [
            "RS256",
            "RS256 algorithm",
            "RS256 signing algorithm"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "api_reference"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_1",
              "content": "**Technology**: Node.js with Express.js framework **Replicas**: 12 pods (production), auto-scaling 8-20 based on CPU **Resource Allocation**: 2 vCPU, 4GB RAM per pod **Key Responsibilities**: - JWT token validation (delegated to Auth Service for initial validation) - Rate limiting: 1000 requests per minute per API key (sliding window) - Request/response transformation and validation using JSON Schema - Routing to downstream services based on URL path patterns - CORS handling for web clients - Request/response logging and correlation ID injection **Critical Endpoints**: - `POST /api/v1/workflows` - Create new workflow - `GET /api/v1/workflows/:id` - Retrieve workflow status - `POST /api/v1/workflows/:id/execute` - Trigger workflow execution - `GET /api/v1/workflows/:id/history` - Get execution history **Dependencies**: - Auth Service (for token validation) - Redis (for rate limiting counters) - All downstream microservices **Performance Targets**: - P50 latency: < 50ms - P99 latency: < 200ms - Throughput: 10,000 RPS sustained --- ### Auth Service **Purpose**: Centralized authentication and authorization service handling user identity, token generation, and permission validation. **Technology**: Go with gRPC for internal communication, REST for external **Replicas**: 8 pods (production), auto-scaling 6-12 **Resource Allocation**: 1 vCPU, 2GB RAM per pod **Key Responsibilities**: - User authentication via multiple providers (OAuth2, SAML, local credentials) - JWT token generation and validation (RS256 algorithm) - Role-based access control (RBAC) with fine-grained permissions - Session management with Redis-backed storage - API key generation and validation for service accounts - MFA enforcement for administrative operations **Authentication Flow**: ``` Client Request \u2192 API Gateway \u2192 Auth Service \u2502 \u251c\u2500 Validate credentials \u251c\u2500 Check MFA if required \u251c\u2500 Generate JWT (15min expiry) \u251c\u2500 Generate refresh token (7 days) \u2514\u2500 Store session in Redis ``` **Token Structure**: - Access Token: JWT with 15-minute expiry - Refresh Token: Opaque token with 7-day expiry, stored in PostgreSQL - Claims: user_id, email, roles[], permissions[], tenant_id **Security Features**: - Password hashing: Argon2id with 64MB memory, 4 iterations - Token rotation on refresh to prevent replay attacks - Brute force protection: 5 failed attempts \u2192 15-minute lockout - Secrets stored in HashiCorp Vault, rotated every 90 days **Performance Targets**: - Token validation: < 10ms (P99) - Token generation: < 50ms (P99) - Throughput: 5,000 RPS for validation operations --- ### Workflow Engine **Purpose**: Core orchestration service that executes workflow definitions, manages state transitions, and coordinates task execution across distributed systems.",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_2",
              "content": "**Technology**: Node.js with TypeScript, Bull queue library **Replicas**: 16 pods (production), auto-scaling 12-24 **Resource Allocation**: 4 vCPU, 8GB RAM per pod **Key Responsibilities**: - Parse and validate workflow definitions (JSON-based DSL) - Execute workflow steps with state machine pattern - Handle retries with exponential backoff (max 3 retries, backoff: 2^n seconds) - Coordinate parallel and sequential task execution - Manage workflow state persistence and recovery - Support for conditional branching and loops - Sub-workflow invocation and composition **Workflow Execution Model**: ``` Workflow Submitted \u2192 Validation \u2192 Queue in Kafka \u2502 \u25bc Workflow Engine picks up \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u25bc \u25bc \u25bc [Task 1 Exec] [Task 2 Exec] [Task 3 Exec] \u2502 \u2502 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u25bc State Update \u2192 PostgreSQL \u2502 \u25bc Publish Event \u2192 Kafka ``` **Workflow DSL Example**: ```json { \"workflow_id\": \"customer-onboarding-v2\", \"version\": \"2.1.0\", \"steps\": [ { \"id\": \"validate-customer\", \"type\": \"http_request\", \"config\": { \"url\": \"https://validation.internal/verify\", \"method\": \"POST\", \"timeout\": 5000, \"retry\": {\"max_attempts\": 3, \"backoff\": \"exponential\"} } }, { \"id\": \"create-account\", \"type\": \"database_operation\", \"depends_on\": [\"validate-customer\"] } ] } ``` **State Management**: - Workflow states: PENDING, RUNNING, COMPLETED, FAILED, CANCELLED - Step states: QUEUED, EXECUTING, SUCCESS, FAILED, SKIPPED - State transitions stored in PostgreSQL with event sourcing pattern - Checkpointing every 10 steps for long-running workflows **Performance Targets**: - Simple workflow (3-5 steps): < 2 seconds (P99) - Complex workflow (20+ steps): < 5 seconds (P99) - Throughput: 500 concurrent workflow executions per pod --- ### Scheduler Service **Purpose**: Time-based workflow triggering system supporting cron-like schedules and one-time delayed executions.",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_6",
              "content": "**Session Storage**: - Key pattern: `session:{user_id}` - TTL: 15 minutes (aligned with JWT expiry) - Data: User session state, preferences - Invalidation: On logout or password change 2. **Rate Limiting Counters**: - Key pattern: `ratelimit:{api_key}:{window}` - TTL: 60 seconds (sliding window) - Data: Request count per window - Algorithm: Token bucket with Redis INCR 3. **Compiled Templates**: - Key pattern: `template:{template_id}:{version}` - TTL: 1 hour - Data: Compiled Handlebars template - Invalidation: On template update 4. **Workflow Definitions**: - Key pattern: `workflow:def:{workflow_id}` - TTL: 1 hour - Data: Parsed workflow JSON - Invalidation: On workflow update or manual flush 5. **User Profiles**: - Key pattern: `user:profile:{user_id}` - TTL: 30 minutes - Data: User metadata (name, email, roles) - Invalidation: On profile update **Cache Hit Rates** (Production Metrics): - Session lookups: 98.5% - Workflow definitions: 94.2% - Template cache: 99.1% - User profiles: 91.8% **Performance Characteristics**: - GET operation P99: < 2ms - SET operation P99: < 3ms - Throughput: 100,000 operations per second - Network latency: < 1ms (same AZ) --- ### Kafka Event Streaming **Cluster Configuration**: - 5 broker nodes (distributed across 3 AZs) - Instance type: kafka.m5.2xlarge (8 vCPU, 32GB RAM) - Storage: 10TB per broker (gp3 SSD) - ZooKeeper ensemble: 3 nodes for cluster coordination - Replication factor: 3 (min in-sync replicas: 2) **Topic Architecture**: ``` workflow.events (32 partitions): - Workflow lifecycle events (created, started, completed, failed) - Retention: 7 days - Message rate: 5,000/sec peak - Consumer groups: workflow-engine, analytics-pipeline notifications.email (16 partitions): - Email notification triggers - Retention: 3 days - Message rate: 2,000/sec peak - Consumer groups: notification-service notifications.sms (8 partitions): - SMS notification triggers - Retention: 3 days - Message rate: 500/sec peak - Consumer groups: notification-service audit.logs (24 partitions): - Security and compliance audit events - Retention: 90 days (compliance requirement) - Message rate: 3,000/sec peak - Consumer groups: audit-processor, security-monitor dead-letter-queue (8 partitions): - Failed message processing - Retention: 30 days - Manual intervention required - Consumer groups: ops-team-alerts ``` **Producer Configuration**: - Acknowledgment: `acks=all` (wait for all in-sync replicas) - Compression: LZ4 (reduces network bandwidth by 60%) - Batching: 100ms linger time, 100KB batch size - Idempotence: Enabled to prevent duplicates **Consumer Configuration**: - Auto-commit: Disabled (manual commit after processing) - Offset reset: Earliest (replay from beginning on new consumer group) - Max poll",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_1",
              "content": "**Diagnosis:** ```bash # Check system time timedatectl status # Compare with NTP server ntpdate -q pool.ntp.org # Check JWT issued time vs current time jwt_iat=$(echo $CF_ACCESS_TOKEN | cut -d'.' -f2 | base64 -d | jq '.iat') current_time=$(date +%s) skew=$((current_time - jwt_iat)) echo \"Clock skew: $skew seconds\" ``` **Resolution:** ```bash # Sync with NTP server sudo ntpdate -s pool.ntp.org # Enable automatic time sync sudo timedatectl set-ntp true # Restart CloudFlow client cloudflow restart ``` **4. Insufficient Permissions** Error: `User does not have required permissions for this operation` **Check user roles:** ```bash cloudflow auth whoami --verbose # Expected output: # User: john.doe@company.com # Roles: developer, workflow-admin # Scopes: workflow:*, data:read, metrics:read ``` **Request permission elevation:** ```bash # Submit access request cloudflow auth request-access \\ --resource \"workflow:production:deploy\" \\ --justification \"Deploy critical hotfix for TICKET-1234\" # Check pending approvals cloudflow auth list-requests --status pending ``` ### 403 Forbidden Errors #### RBAC Policy Violations CloudFlow uses role-based access control (RBAC) with the following hierarchy: - `viewer` - Read-only access - `developer` - Create and modify workflows (non-production) - `operator` - Execute workflows, view logs - `admin` - Full access to workspace - `platform-admin` - Cross-workspace administration **Verify resource permissions:** ```bash # Check effective permissions cloudflow rbac check \\ --user john.doe@company.com \\ --resource workflow:prod-pipeline \\ --action execute # List all policies affecting user cloudflow rbac policies --user john.doe@company.com --verbose ``` --- ## Performance Problems ### Slow Query Performance #### Symptoms - API response times > 5000ms - Database query latency warnings in logs - CloudFlow UI becomes unresponsive - Timeout errors: `Request exceeded maximum duration of 30000ms` #### Diagnosis Steps **1. Identify Slow Queries** ```bash # Query CloudFlow metrics cloudflow metrics query --metric api_request_duration_ms \\ --filter \"p95 > 5000\" \\ --last 1h # Check database slow query log kubectl logs -n cloudflow deploy/cloudflow-db-primary | \\ grep \"slow query\" | \\ tail -n 50 # Analyze query patterns cloudflow db analyze-queries --min-duration 5000 --limit 20 ``` **2.",
              "score": null
            },
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_0",
              "content": "# CloudFlow API Reference Version 2.1.0 | Last Updated: January 2026 ## Overview The CloudFlow API is a RESTful service that enables developers to programmatically manage cloud workflows, data pipelines, and automation tasks. This documentation provides comprehensive details on authentication, endpoints, request/response formats, error handling, and best practices. **Base URL:** `https://api.cloudflow.io/v2` **API Status:** https://status.cloudflow.io ## Authentication CloudFlow supports three authentication methods to suit different use cases and security requirements. ### API Keys API keys provide simple authentication for server-to-server communication. Include your API key in the request header: ```bash curl -H \"X-API-Key: cf_live_a1b2c3d4e5f6g7h8i9j0\" \\ https://api.cloudflow.io/v2/workflows ``` **Security Notes:** - Never expose API keys in client-side code - Rotate keys every 90 days - Use separate keys for development and production environments ### OAuth 2.0 OAuth 2.0 is recommended for applications that access CloudFlow on behalf of users. We support the Authorization Code flow with PKCE. **Authorization Endpoint:** `https://auth.cloudflow.io/oauth/authorize` **Token Endpoint:** `https://auth.cloudflow.io/oauth/token` **Supported Scopes:** - `workflows:read` - Read workflow configurations - `workflows:write` - Create and modify workflows - `pipelines:read` - Read pipeline data - `pipelines:write` - Create and manage pipelines - `analytics:read` - Access analytics and metrics - `admin:full` - Full administrative access Example authorization request: ```bash curl -X POST https://auth.cloudflow.io/oauth/token \\ -H \"Content-Type: application/x-www-form-urlencoded\" \\ -d \"grant_type=authorization_code\" \\ -d \"code=AUTH_CODE_HERE\" \\ -d \"client_id=YOUR_CLIENT_ID\" \\ -d \"client_secret=YOUR_CLIENT_SECRET\" \\ -d \"redirect_uri=https://yourapp.com/callback\" ``` ### JWT Tokens For advanced use cases, CloudFlow supports JSON Web Tokens (JWT) with RS256 signing algorithm. JWTs must include the following claims: - `iss` (issuer): Your application identifier - `sub` (subject): User or service account ID - `aud` (audience): `https://api.cloudflow.io` - `exp` (expiration): Unix timestamp (max 3600 seconds from `iat`) - `iat` (issued at): Unix timestamp - `scope`: Space-separated list of requested scopes Example JWT header: ```python import jwt import time from cryptography.hazmat.primitives import serialization from cryptography.hazmat.backends import default_backend # Load your private key with open('private_key.pem', 'rb') as key_file: private_key = serialization.load_pem_private_key( key_file.read(), password=None, backend=default_backend() ) # Create JWT payload payload = { 'iss': 'your-app-id', 'sub': 'user-12345', 'aud': 'https://api.cloudflow.io', 'exp': int(time.time()) + 3600, 'iat': int(time.time()), 'scope': 'workflows:read workflows:write' } # Generate token token = jwt.encode(payload, private_key, algorithm='RS256') # Use in API request headers = {'Authorization': f'Bearer {token}'} ``` **Token Expiration:** All tokens expire after 3600 seconds (1 hour). Implement token refresh logic in your application.",
              "score": null
            }
          ],
          "latency_ms": 12.868982998043066,
          "query_id": "realistic_012",
          "dimension": "contextual",
          "query": "I'm implementing JWT verification, what algorithm should I expect?"
        },
        {
          "key_facts": [
            "RS256",
            "RS256 algorithm",
            "RS256 signing algorithm"
          ],
          "found_facts": [
            "RS256",
            "RS256 algorithm"
          ],
          "missed_facts": [
            "RS256 signing algorithm"
          ],
          "coverage": 0.6666666666666666,
          "expected_docs": [
            "api_reference"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_1",
              "content": "**Technology**: Node.js with Express.js framework **Replicas**: 12 pods (production), auto-scaling 8-20 based on CPU **Resource Allocation**: 2 vCPU, 4GB RAM per pod **Key Responsibilities**: - JWT token validation (delegated to Auth Service for initial validation) - Rate limiting: 1000 requests per minute per API key (sliding window) - Request/response transformation and validation using JSON Schema - Routing to downstream services based on URL path patterns - CORS handling for web clients - Request/response logging and correlation ID injection **Critical Endpoints**: - `POST /api/v1/workflows` - Create new workflow - `GET /api/v1/workflows/:id` - Retrieve workflow status - `POST /api/v1/workflows/:id/execute` - Trigger workflow execution - `GET /api/v1/workflows/:id/history` - Get execution history **Dependencies**: - Auth Service (for token validation) - Redis (for rate limiting counters) - All downstream microservices **Performance Targets**: - P50 latency: < 50ms - P99 latency: < 200ms - Throughput: 10,000 RPS sustained --- ### Auth Service **Purpose**: Centralized authentication and authorization service handling user identity, token generation, and permission validation. **Technology**: Go with gRPC for internal communication, REST for external **Replicas**: 8 pods (production), auto-scaling 6-12 **Resource Allocation**: 1 vCPU, 2GB RAM per pod **Key Responsibilities**: - User authentication via multiple providers (OAuth2, SAML, local credentials) - JWT token generation and validation (RS256 algorithm) - Role-based access control (RBAC) with fine-grained permissions - Session management with Redis-backed storage - API key generation and validation for service accounts - MFA enforcement for administrative operations **Authentication Flow**: ``` Client Request \u2192 API Gateway \u2192 Auth Service \u2502 \u251c\u2500 Validate credentials \u251c\u2500 Check MFA if required \u251c\u2500 Generate JWT (15min expiry) \u251c\u2500 Generate refresh token (7 days) \u2514\u2500 Store session in Redis ``` **Token Structure**: - Access Token: JWT with 15-minute expiry - Refresh Token: Opaque token with 7-day expiry, stored in PostgreSQL - Claims: user_id, email, roles[], permissions[], tenant_id **Security Features**: - Password hashing: Argon2id with 64MB memory, 4 iterations - Token rotation on refresh to prevent replay attacks - Brute force protection: 5 failed attempts \u2192 15-minute lockout - Secrets stored in HashiCorp Vault, rotated every 90 days **Performance Targets**: - Token validation: < 10ms (P99) - Token generation: < 50ms (P99) - Throughput: 5,000 RPS for validation operations --- ### Workflow Engine **Purpose**: Core orchestration service that executes workflow definitions, manages state transitions, and coordinates task execution across distributed systems.",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_0",
              "content": "# CloudFlow Platform Troubleshooting Guide **Version:** 3.2.1 **Last Updated:** January 2026 **Audience:** Platform Engineers, DevOps, Support Teams ## Table of Contents 1. [Overview](#overview) 2. [Authentication & Authorization Issues](#authentication--authorization-issues) 3. [Performance Problems](#performance-problems) 4. [Database Connection Issues](#database-connection-issues) 5. [Workflow Execution Failures](#workflow-execution-failures) 6. [Rate Limiting & Throttling](#rate-limiting--throttling) 7. [Log Analysis & Debugging](#log-analysis--debugging) 8. [Escalation Procedures](#escalation-procedures) --- ## Overview This guide provides comprehensive troubleshooting steps for common CloudFlow platform issues encountered in production environments. Each section includes error symptoms, root cause analysis, resolution steps, and preventive measures. ### Quick Diagnostic Checklist Before diving into specific issues, perform these initial checks: - Verify service health: `cloudflow status --all` - Check API connectivity: `curl -I https://api.cloudflow.io/health` - Review recent deployments: `kubectl get deployments -n cloudflow --sort-by=.metadata.creationTimestamp` - Inspect platform metrics: `cloudflow metrics --last 1h` --- ## Authentication & Authorization Issues ### 401 Unauthorized Errors #### Symptoms - API requests return `401 Unauthorized` - Error message: `Authentication credentials were not provided or are invalid` - Frontend displays \"Session expired\" message #### Common Causes **1. Token Expiration** CloudFlow access tokens expire after 3600 seconds (1 hour) by default. Refresh tokens are valid for 30 days. **Verification:** ```bash # Decode JWT to check expiration echo $CF_ACCESS_TOKEN | cut -d'.' -f2 | base64 -d | jq '.exp' # Compare with current time date +%s ``` **Resolution:** ```bash # Refresh the access token curl -X POST https://api.cloudflow.io/auth/refresh \\ -H \"Content-Type: application/json\" \\ -d '{\"refresh_token\": \"'$CF_REFRESH_TOKEN'\"}' # Update environment variable export CF_ACCESS_TOKEN=\"<new_token>\" ``` **2. Invalid JWT Signature** Error: `JWT signature verification failed` **Causes:** - Token was modified or corrupted - Using wrong signing key - Token generated with different secret **Resolution:** ```bash # Validate token structure cloudflow auth validate --token $CF_ACCESS_TOKEN # Generate new token cloudflow auth login --username <user> --password <pass> # For service accounts cloudflow auth service-account --create --name <sa-name> --scopes \"workflow:read,workflow:write\" ``` **3. Clock Skew Issues** JWT validation fails when system clocks are out of sync (tolerance: \u00b1300 seconds).",
              "score": null
            },
            {
              "doc_id": "user_guide",
              "chunk_id": "user_guide_fix_7",
              "content": "Handle Errors Gracefully Always implement error handling for external API calls and database operations: ```yaml - id: fetch_data action: http_request config: url: \"https://api.example.com/data\" retry: max_attempts: 3 backoff_type: exponential on_error: - id: log_error action: database_query config: query: \"INSERT INTO error_log (workflow_id, error) VALUES ($1, $2)\" parameters: - \"{{workflow.id}}\" - \"{{error.message}}\" ``` ### 3. Use Secrets for Sensitive Data Never hardcode API keys, passwords, or tokens in workflows: **Bad:** ```yaml headers: Authorization: \"Bearer sk_live_abc123xyz789\" ``` **Good:** ```yaml headers: Authorization: \"Bearer {{secrets.API_TOKEN}}\" ``` Store secrets in **Settings** > **Secrets** with encryption at rest. ### 4. Validate Input Data Always validate trigger data before processing: ```yaml - id: validate_input action: javascript code: | const required_fields = ['email', 'name', 'order_id']; for (const field of required_fields) { if (!input[field]) { throw new Error(`Missing required field: ${field}`); } } // Validate email format const email_regex = /^[^\\s@]+@[^\\s@]+\\.[^\\s@]+$/; if (!email_regex.test(input.email)) { throw new Error('Invalid email format'); } return { validated: true }; ``` ### 5. Use Idempotency Keys For operations that shouldn't be repeated (payments, record creation), use idempotency keys: ```yaml - id: create_charge action: http_request config: url: \"https://api.stripe.com/v1/charges\" method: POST headers: Idempotency-Key: \"{{workflow.id}}-{{execution.id}}\" body: amount: \"{{amount}}\" ``` ### 6. Monitor and Log Add logging steps for important workflow milestones: ```yaml - id: log_start action: database_query config: query: \"INSERT INTO workflow_audit (execution_id, step, timestamp) VALUES ($1, $2, $3)\" parameters: - \"{{execution.id}}\" - \"workflow_started\" - \"{{now()}}\" ``` ### 7. Keep Workflows Modular Break complex workflows into smaller, reusable components: - Use sub-workflows for repeated logic - Trigger child workflows via webhooks - Share common configurations via templates ### 8. Test Thoroughly Before activating a workflow: 1. Use test mode with sample data 2. Verify all actions execute correctly 3. Test error handling paths 4. Review execution logs 5. Start with a limited scope (e.g., test channel, small dataset) ### 9. Document Your Workflows Add descriptions to workflows and steps: ```yaml workflow: name: \"Daily Sales Report\" description: | Generates a daily sales report and distributes it to the sales team. Runs at 8:00 AM EST Monday-Friday. Data source: PostgreSQL sales database Recipients: sales-team@company.com steps: - id: fetch_sales name: \"Fetch Yesterday's Sales\" description: \"Query database for all completed orders from previous day\" ``` ### 10.",
              "score": null
            },
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_4",
              "content": "### Error Response Format ```json { \"error\": { \"code\": \"invalid_parameter\", \"message\": \"The 'limit' parameter must be between 1 and 100\", \"field\": \"limit\", \"request_id\": \"req_8k3m9x2p\" } } ``` ### HTTP Status Codes | Status Code | Description | Common Causes | |------------|-------------|---------------| | 400 | Bad Request | Invalid parameters, malformed JSON, validation errors | | 401 | Unauthorized | Missing or invalid authentication credentials | | 403 | Forbidden | Insufficient permissions for requested resource | | 404 | Not Found | Resource does not exist or has been deleted | | 429 | Too Many Requests | Rate limit exceeded, retry after specified period | | 500 | Internal Server Error | Unexpected server error, contact support if persists | | 502 | Bad Gateway | Temporary service issue, retry with exponential backoff | | 503 | Service Unavailable | Scheduled maintenance or temporary outage | ### Error Codes CloudFlow returns specific error codes to help you identify and resolve issues: - `invalid_parameter`: One or more request parameters are invalid - `missing_required_field`: Required field is missing from request body - `authentication_failed`: Invalid API key or token - `insufficient_permissions`: User lacks required scope or permission - `resource_not_found`: Requested resource does not exist - `rate_limit_exceeded`: Too many requests, see rate limiting section - `workflow_execution_failed`: Workflow execution encountered an error - `invalid_json`: Request body contains malformed JSON - `duplicate_resource`: Resource with same identifier already exists - `quota_exceeded`: Account quota limit reached ### Error Handling Best Practices **Python Example:** ```python import requests import time def make_api_request(url, headers, max_retries=3): for attempt in range(max_retries): try: response = requests.get(url, headers=headers) if response.status_code == 200: return response.json() elif response.status_code == 429: # Rate limit exceeded retry_after = int(response.headers.get('X-RateLimit-Reset', 60)) print(f\"Rate limited. Waiting {retry_after} seconds...\") time.sleep(retry_after) continue elif response.status_code >= 500: # Server error - retry with exponential backoff wait_time = 2 ** attempt print(f\"Server error. Retrying in {wait_time} seconds...\") time.sleep(wait_time) continue else: # Client error - don't retry error_data = response.json() raise Exception(f\"API Error: {error_data['error']['message']}\") except requests.exceptions.RequestException as e: print(f\"Request failed: {e}\") if attempt < max_retries - 1: time.sleep(2 ** attempt) else: raise raise Exception(\"Max retries exceeded\") ``` ## Webhooks CloudFlow can send webhook notifications when specific events occur in your workflows.",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_8",
              "content": "### Cache Invalidation Strategies **Time-Based Expiration (TTL)**: - Short-lived data: 5-15 minutes (session tokens, rate limit counters) - Medium-lived data: 1 hour (workflow definitions, templates) - Long-lived data: 24 hours (static configuration) **Event-Based Invalidation**: ``` Database Update Event \u2192 Kafka (cache.invalidation topic) \u2502 \u25bc All service instances consume event \u2502 \u25bc Redis DEL for affected keys ``` **Pattern-Based Invalidation**: - Use Redis SCAN + DEL for wildcard patterns - Example: Invalidate all user caches: `user:*:{user_id}` **Cache Warming**: - Scheduled job runs every hour to pre-populate frequently accessed data - Targets: Top 1000 workflows, system configuration, active user profiles - Reduces cache miss rate during traffic spikes ### Multi-Level Caching Application implements L1 (in-memory) and L2 (Redis) caching: ``` Request \u2192 L1 Cache (Node.js Map, 1000 entry LRU) \u2502 MISS \u2502 \u25bc L2 Cache (Redis, distributed) \u2502 MISS \u2502 \u25bc Database (PostgreSQL) ``` L1 cache reduces Redis network calls by 40% for hot data. --- ## Security Architecture ### Network Security **Zero-Trust Network Model**: - All service-to-service communication encrypted with mTLS - Certificate rotation: Every 90 days (automated via cert-manager) - Certificate authority: Internal PKI with HashiCorp Vault - Network segmentation: Private subnets for services, public subnet for ALB only **Service Mesh (Istio)**: ``` Service A \u2192 Envoy Sidecar (mTLS client cert) \u2502 Encrypted channel \u2502 Envoy Sidecar (mTLS server cert) \u2192 Service B ``` **Firewall Rules**: - Security groups: Deny all by default, explicit allow rules - Ingress: Only ALB can reach API Gateway (port 8080) - Egress: Services can only reach specific dependencies - Database access: Limited to application subnets only ### Authentication & Authorization **JWT Token Validation**: - Algorithm: RS256 (asymmetric signing) - Key rotation: Every 30 days with 7-day overlap period - Public key distribution: JWKS endpoint cached in Redis - Validation: Signature, expiry, issuer, audience claims - Token revocation: Blacklist in Redis for compromised tokens **Permission Model**: ``` User \u2192 Roles \u2192 Permissions \u2198 \u2197 Tenants (Multi-tenancy isolation) ``` Example permissions: - `workflow:read` - View workflows - `workflow:write` - Create/update workflows - `workflow:execute` - Trigger workflow execution - `workflow:delete` - Delete workflows - `admin:*` - All administrative operations **API Key Management**: - Format: `cfk_live_<32-char-random>` (production), `cfk_test_<32-char-random>` (sandbox) - Hashing: SHA-256 before storage in PostgreSQL - Scoping: API keys can be scoped to specific workflows or operations - Rate limits: Configurable",
              "score": null
            }
          ],
          "latency_ms": 12.905822000902845,
          "query_id": "realistic_012",
          "dimension": "negation",
          "query": "Why doesn't HS256 work for token validation?"
        },
        {
          "key_facts": [
            "TTL: 1 hour",
            "Workflow Definitions: TTL: 1 hour"
          ],
          "found_facts": [
            "TTL: 1 hour"
          ],
          "missed_facts": [
            "Workflow Definitions: TTL: 1 hour"
          ],
          "coverage": 0.5,
          "expected_docs": [
            "architecture_overview"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_8",
              "content": "### Cache Invalidation Strategies **Time-Based Expiration (TTL)**: - Short-lived data: 5-15 minutes (session tokens, rate limit counters) - Medium-lived data: 1 hour (workflow definitions, templates) - Long-lived data: 24 hours (static configuration) **Event-Based Invalidation**: ``` Database Update Event \u2192 Kafka (cache.invalidation topic) \u2502 \u25bc All service instances consume event \u2502 \u25bc Redis DEL for affected keys ``` **Pattern-Based Invalidation**: - Use Redis SCAN + DEL for wildcard patterns - Example: Invalidate all user caches: `user:*:{user_id}` **Cache Warming**: - Scheduled job runs every hour to pre-populate frequently accessed data - Targets: Top 1000 workflows, system configuration, active user profiles - Reduces cache miss rate during traffic spikes ### Multi-Level Caching Application implements L1 (in-memory) and L2 (Redis) caching: ``` Request \u2192 L1 Cache (Node.js Map, 1000 entry LRU) \u2502 MISS \u2502 \u25bc L2 Cache (Redis, distributed) \u2502 MISS \u2502 \u25bc Database (PostgreSQL) ``` L1 cache reduces Redis network calls by 40% for hot data. --- ## Security Architecture ### Network Security **Zero-Trust Network Model**: - All service-to-service communication encrypted with mTLS - Certificate rotation: Every 90 days (automated via cert-manager) - Certificate authority: Internal PKI with HashiCorp Vault - Network segmentation: Private subnets for services, public subnet for ALB only **Service Mesh (Istio)**: ``` Service A \u2192 Envoy Sidecar (mTLS client cert) \u2502 Encrypted channel \u2502 Envoy Sidecar (mTLS server cert) \u2192 Service B ``` **Firewall Rules**: - Security groups: Deny all by default, explicit allow rules - Ingress: Only ALB can reach API Gateway (port 8080) - Egress: Services can only reach specific dependencies - Database access: Limited to application subnets only ### Authentication & Authorization **JWT Token Validation**: - Algorithm: RS256 (asymmetric signing) - Key rotation: Every 30 days with 7-day overlap period - Public key distribution: JWKS endpoint cached in Redis - Validation: Signature, expiry, issuer, audience claims - Token revocation: Blacklist in Redis for compromised tokens **Permission Model**: ``` User \u2192 Roles \u2192 Permissions \u2198 \u2197 Tenants (Multi-tenancy isolation) ``` Example permissions: - `workflow:read` - View workflows - `workflow:write` - Create/update workflows - `workflow:execute` - Trigger workflow execution - `workflow:delete` - Delete workflows - `admin:*` - All administrative operations **API Key Management**: - Format: `cfk_live_<32-char-random>` (production), `cfk_test_<32-char-random>` (sandbox) - Hashing: SHA-256 before storage in PostgreSQL - Scoping: API keys can be scoped to specific workflows or operations - Rate limits: Configurable",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_7",
              "content": "records: 500 - Session timeout: 30 seconds --- ## Message Queue Patterns ### Pub/Sub Pattern Used for broadcasting events to multiple interested consumers: ``` Workflow Engine publishes workflow.execution.completed \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u25bc \u25bc \u25bc \u25bc Analytics Notification Audit Logger Billing Service Service Service Service ``` Each service maintains its own consumer group and processes events independently. Failures in one consumer don't affect others. ### Request-Reply Pattern Synchronous communication over async messaging (used sparingly): ``` API Gateway publishes request \u2192 Kafka (reply-to: temp-queue-123) \u2502 \u25bc Service processes request \u2502 \u25bc Service publishes response \u2192 temp-queue-123 \u2502 \u25bc API Gateway receives response ``` Timeout: 5 seconds, fallback to direct HTTP call if no response. ### Saga Pattern Distributed transaction management for multi-service workflows: ``` Step 1: Create Order \u2192 SUCCESS \u2192 Step 2: Reserve Inventory \u2502 FAILURE \u2502 \u25bc Compensating Transaction \u2502 \u25bc Cancel Order (Rollback) ``` Orchestrated by Workflow Engine with compensation logic defined in workflow DSL. ### Dead Letter Queue (DLQ) Failed messages after all retry attempts are routed to DLQ: ``` Message processing fails (3 retries with exponential backoff) \u2502 \u25bc Publish to dead-letter-queue topic \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u25bc \u25bc Alert to PagerDuty Store in PostgreSQL for analysis \u2502 \u2502 \u25bc \u25bc Manual investigation Automated pattern detection ``` DLQ Processing SLA: < 4 hours for critical events, < 24 hours for non-critical. --- ## Caching Strategy ### Cache-Aside Pattern (Lazy Loading) Primary caching pattern used across all services: ``` 1. Application checks cache (Redis GET) 2. If HIT \u2192 Return cached data 3. If MISS: a. Query database (PostgreSQL) b. Store result in cache with TTL c. Return data to application ``` **Implementation Example** (Workflow Definition): ```javascript async function getWorkflowDefinition(workflowId) { const cacheKey = `workflow:def:${workflowId}`; // Step 1: Check cache let workflow = await redis.get(cacheKey); if (workflow) { metrics.increment('cache.hit.workflow_def'); return JSON.parse(workflow); } // Step 2: Cache miss - query database metrics.increment('cache.miss.workflow_def'); workflow = await db.query( 'SELECT * FROM workflows WHERE id = $1', [workflowId] ); // Step 3: Store in cache (1 hour TTL) await redis.setex(cacheKey, 3600, JSON.stringify(workflow)); return workflow; } ``` ### Write-Through Pattern Used for critical data where cache consistency is paramount: ``` 1. Application writes to database 2. Database transaction commits 3. Application updates cache 4. Return success to client ``` Applied to: User profiles, authentication sessions, system configuration.",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_2",
              "content": "Review Query Execution Plans** ```sql -- Connect to CloudFlow database cloudflow db connect --readonly -- Explain slow query EXPLAIN ANALYZE SELECT w.*, e.status, e.error_message FROM workflows w LEFT JOIN executions e ON w.id = e.workflow_id WHERE w.workspace_id = 'ws_abc123' AND e.created_at > NOW() - INTERVAL '7 days' ORDER BY e.created_at DESC; -- Check for missing indexes SELECT schemaname, tablename, indexname, indexdef FROM pg_indexes WHERE tablename IN ('workflows', 'executions', 'workflow_steps') ORDER BY tablename, indexname; ``` **3. Optimize Queries** Common optimization techniques: **Add missing indexes:** ```sql -- Index for workflow lookup by workspace CREATE INDEX CONCURRENTLY idx_workflows_workspace_created ON workflows(workspace_id, created_at DESC); -- Index for execution status queries CREATE INDEX CONCURRENTLY idx_executions_status_created ON executions(workflow_id, status, created_at DESC) WHERE status IN ('running', 'pending'); -- Composite index for common filter combinations CREATE INDEX CONCURRENTLY idx_executions_workspace_status ON executions(workspace_id, status, created_at) INCLUDE (error_message, retry_count); ``` **Use query result caching:** ```bash # Enable query cache for workspace metadata cloudflow config set cache.workspace.ttl 3600 # Configure Redis cache backend cloudflow config set cache.backend redis cloudflow config set cache.redis.host redis.cloudflow.svc.cluster.local cloudflow config set cache.redis.port 6379 ``` ### High API Latency #### Latency Breakdown Analysis ```bash # Generate latency report cloudflow metrics latency-report --endpoint \"/api/v1/workflows\" --last 24h # Sample output: # Endpoint: POST /api/v1/workflows/execute # P50: 245ms | P95: 1823ms | P99: 4521ms # Breakdown: # - Auth: 45ms (18%) # - DB Query: 156ms (64%) # - Business Logic: 32ms (13%) # - Response Serialization: 12ms (5%) ``` **Network latency issues:** ```bash # Test connectivity to CloudFlow API time curl -w \"@curl-format.txt\" -o /dev/null -s https://api.cloudflow.io/health # Create curl-format.txt: cat > curl-format.txt << EOF time_namelookup: %{time_namelookup}s\\n time_connect: %{time_connect}s\\n time_appconnect: %{time_appconnect}s\\n time_pretransfer: %{time_pretransfer}s\\n time_redirect: %{time_redirect}s\\n time_starttransfer: %{time_starttransfer}s\\n ----------\\n time_total: %{time_total}s\\n EOF # Trace route to API endpoint traceroute api.cloudflow.io # Check DNS resolution time dig api.cloudflow.io | grep \"Query time\" ``` ### Memory Leaks #### Detection ```bash # Monitor CloudFlow service memory usage kubectl top pods -n cloudflow --sort-by=memory # Get detailed memory metrics for specific pod kubectl exec -n cloudflow deploy/cloudflow-api -- \\ curl localhost:9090/metrics | grep memory # Check for OOMKilled pods kubectl get pods -n cloudflow --field-selector=status.phase=Failed | \\ grep OOMKilled # Review memory limits and requests kubectl describe deployment cloudflow-api -n cloudflow | \\ grep -A 5 \"Limits\\|Requests\" ``` #### Common Causes **1.",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_9",
              "content": "per API key (default: 1000 RPM) ### Secrets Management **HashiCorp Vault Integration**: - Dynamic database credentials: Generated on-demand, 1-hour TTL - Encryption keys: Transit secrets engine for encryption-as-a-service - API keys for external services: Stored with versioning - Rotation policy: Automated rotation every 90 days with notification **Secret Access Pattern**: ``` Service starts \u2192 Vault authentication (Kubernetes service account) \u2502 \u25bc Request secret lease (1-hour TTL) \u2502 \u25bc Vault returns dynamic credentials \u2502 \u25bc Service connects to PostgreSQL \u2502 Renew lease every 30 minutes ``` ### Data Protection **Encryption at Rest**: - PostgreSQL: AES-256 encryption enabled at volume level - Redis: Encryption enabled using AWS KMS - Kafka: Encryption at rest for all data on brokers - S3 backups: Server-side encryption with KMS (SSE-KMS) **Encryption in Transit**: - External traffic: TLS 1.3 only (TLS 1.2 deprecated) - Internal traffic: mTLS via Istio service mesh - Database connections: SSL/TLS enforced (sslmode=require) **Data Masking**: - Sensitive fields (email, phone) masked in logs - PII redacted from error messages and stack traces - Audit logs: Full data retention with access controls **Compliance**: - GDPR: User data export and deletion workflows - SOC 2 Type II: Audit logging for all data access - HIPAA: PHI isolation in dedicated tenants with enhanced encryption --- ## Performance Characteristics ### Latency Targets **API Operations** (P99 latency): - `GET /workflows/{id}`: < 50ms (cache hit), < 150ms (cache miss) - `POST /workflows`: < 200ms (includes validation and database write) - `POST /workflows/{id}/execute`: < 100ms (async, returns execution ID) - `GET /workflows/{id}/history`: < 300ms (paginated, 50 records per page) **Workflow Execution** (P99 latency): - Simple workflow (< 5 steps): < 2 seconds - Medium workflow (5-15 steps): < 5 seconds - Complex workflow (> 15 steps): < 15 seconds - Parallel execution: Linear scaling up to 10 concurrent branches **Database Operations** (P99 latency): - Simple SELECT: < 5ms - JOIN query: < 20ms - INSERT/UPDATE: < 10ms - Batch operations: < 100ms (batch size: 100 records) **Cache Operations** (P99 latency): - Redis GET: < 2ms - Redis SET: < 3ms - Redis DEL: < 2ms ### Throughput Capacity **API Gateway**: - Sustained: 10,000 requests per second - Peak: 25,000 requests per second (5-minute burst) - Rate limiting: 1,000 requests per minute per API key **Workflow Engine**: - Concurrent executions: 8,000 workflows (across 16 pods)",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_6",
              "content": "**Session Storage**: - Key pattern: `session:{user_id}` - TTL: 15 minutes (aligned with JWT expiry) - Data: User session state, preferences - Invalidation: On logout or password change 2. **Rate Limiting Counters**: - Key pattern: `ratelimit:{api_key}:{window}` - TTL: 60 seconds (sliding window) - Data: Request count per window - Algorithm: Token bucket with Redis INCR 3. **Compiled Templates**: - Key pattern: `template:{template_id}:{version}` - TTL: 1 hour - Data: Compiled Handlebars template - Invalidation: On template update 4. **Workflow Definitions**: - Key pattern: `workflow:def:{workflow_id}` - TTL: 1 hour - Data: Parsed workflow JSON - Invalidation: On workflow update or manual flush 5. **User Profiles**: - Key pattern: `user:profile:{user_id}` - TTL: 30 minutes - Data: User metadata (name, email, roles) - Invalidation: On profile update **Cache Hit Rates** (Production Metrics): - Session lookups: 98.5% - Workflow definitions: 94.2% - Template cache: 99.1% - User profiles: 91.8% **Performance Characteristics**: - GET operation P99: < 2ms - SET operation P99: < 3ms - Throughput: 100,000 operations per second - Network latency: < 1ms (same AZ) --- ### Kafka Event Streaming **Cluster Configuration**: - 5 broker nodes (distributed across 3 AZs) - Instance type: kafka.m5.2xlarge (8 vCPU, 32GB RAM) - Storage: 10TB per broker (gp3 SSD) - ZooKeeper ensemble: 3 nodes for cluster coordination - Replication factor: 3 (min in-sync replicas: 2) **Topic Architecture**: ``` workflow.events (32 partitions): - Workflow lifecycle events (created, started, completed, failed) - Retention: 7 days - Message rate: 5,000/sec peak - Consumer groups: workflow-engine, analytics-pipeline notifications.email (16 partitions): - Email notification triggers - Retention: 3 days - Message rate: 2,000/sec peak - Consumer groups: notification-service notifications.sms (8 partitions): - SMS notification triggers - Retention: 3 days - Message rate: 500/sec peak - Consumer groups: notification-service audit.logs (24 partitions): - Security and compliance audit events - Retention: 90 days (compliance requirement) - Message rate: 3,000/sec peak - Consumer groups: audit-processor, security-monitor dead-letter-queue (8 partitions): - Failed message processing - Retention: 30 days - Manual intervention required - Consumer groups: ops-team-alerts ``` **Producer Configuration**: - Acknowledgment: `acks=all` (wait for all in-sync replicas) - Compression: LZ4 (reduces network bandwidth by 60%) - Batching: 100ms linger time, 100KB batch size - Idempotence: Enabled to prevent duplicates **Consumer Configuration**: - Auto-commit: Disabled (manual commit after processing) - Offset reset: Earliest (replay from beginning on new consumer group) - Max poll",
              "score": null
            }
          ],
          "latency_ms": 12.970823001523968,
          "query_id": "realistic_013",
          "dimension": "original",
          "query": "What is the Redis cache TTL for workflow definitions?"
        },
        {
          "key_facts": [
            "TTL: 1 hour",
            "Workflow Definitions: TTL: 1 hour"
          ],
          "found_facts": [],
          "missed_facts": [
            "TTL: 1 hour",
            "Workflow Definitions: TTL: 1 hour"
          ],
          "coverage": 0.0,
          "expected_docs": [
            "architecture_overview"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_5",
              "content": "exec_7h3j6k9m2n --show-bottlenecks ``` #### Solutions **1. Increase workflow timeout (if justified):** ```bash # Update workflow configuration cloudflow workflows update wf_9k2n4m8p1q \\ --timeout 7200 \\ --reason \"Large dataset processing requires extended time\" # Verify update cloudflow workflows get wf_9k2n4m8p1q | grep timeout ``` **2. Optimize slow steps:** ```bash # Enable parallel processing cloudflow workflows update wf_9k2n4m8p1q \\ --step data_transformation \\ --parallel-workers 8 \\ --batch-size 1000 # Add checkpointing for long operations cloudflow workflows update wf_9k2n4m8p1q \\ --step data_transformation \\ --enable-checkpointing \\ --checkpoint-interval 300 ``` **3. Split workflow into smaller workflows:** ```bash # Create sub-workflows cloudflow workflows create data-pipeline-part1 \\ --steps \"data_ingestion,data_validation\" \\ --timeout 1800 cloudflow workflows create data-pipeline-part2 \\ --steps \"data_transformation,data_export\" \\ --timeout 3600 \\ --trigger workflow_completed \\ --trigger-workflow data-pipeline-part1 ``` ### Retry Logic and Exponential Backoff CloudFlow implements automatic retry with exponential backoff for transient failures: - Max retries: 3 - Initial delay: 1 second - Backoff multiplier: 2 - Max delay: 60 seconds #### Retry Sequence ``` Attempt 1: Immediate Attempt 2: Wait 1s (2^0 * 1s) Attempt 3: Wait 2s (2^1 * 1s) Attempt 4: Wait 4s (2^2 * 1s) ``` #### Configuration ```bash # View current retry settings cloudflow workflows get wf_9k2n4m8p1q --format json | jq '.retry_policy' # Customize retry behavior cloudflow workflows update wf_9k2n4m8p1q \\ --retry-max-attempts 5 \\ --retry-initial-delay 2000 \\ --retry-backoff-multiplier 2 \\ --retry-max-delay 120000 \\ --retry-on-errors \"NETWORK_ERROR,TIMEOUT,RATE_LIMIT\" # Disable retry for specific step cloudflow workflows update wf_9k2n4m8p1q \\ --step data_export \\ --retry-enabled false ``` #### Monitoring Retries ```bash # List failed executions with retry information cloudflow workflows executions list \\ --status FAILED \\ --show-retries \\ --last 7d # Get retry history for specific execution cloudflow workflows executions retries exec_7h3j6k9m2n # Output: # Execution: exec_7h3j6k9m2n # Attempt 1: FAILED - NetworkError: Connection refused (delay: 0ms) # Attempt 2: FAILED - NetworkError: Connection timeout (delay: 1000ms) # Attempt 3: FAILED - NetworkError: Connection timeout (delay: 2000ms) # Attempt 4: FAILED - NetworkError: Connection timeout (delay: 4000ms) # Final Status: FAILED_AFTER_RETRIES ``` ### Workflow Step Failures #### Common Error Patterns **1.",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_7",
              "content": "records: 500 - Session timeout: 30 seconds --- ## Message Queue Patterns ### Pub/Sub Pattern Used for broadcasting events to multiple interested consumers: ``` Workflow Engine publishes workflow.execution.completed \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u25bc \u25bc \u25bc \u25bc Analytics Notification Audit Logger Billing Service Service Service Service ``` Each service maintains its own consumer group and processes events independently. Failures in one consumer don't affect others. ### Request-Reply Pattern Synchronous communication over async messaging (used sparingly): ``` API Gateway publishes request \u2192 Kafka (reply-to: temp-queue-123) \u2502 \u25bc Service processes request \u2502 \u25bc Service publishes response \u2192 temp-queue-123 \u2502 \u25bc API Gateway receives response ``` Timeout: 5 seconds, fallback to direct HTTP call if no response. ### Saga Pattern Distributed transaction management for multi-service workflows: ``` Step 1: Create Order \u2192 SUCCESS \u2192 Step 2: Reserve Inventory \u2502 FAILURE \u2502 \u25bc Compensating Transaction \u2502 \u25bc Cancel Order (Rollback) ``` Orchestrated by Workflow Engine with compensation logic defined in workflow DSL. ### Dead Letter Queue (DLQ) Failed messages after all retry attempts are routed to DLQ: ``` Message processing fails (3 retries with exponential backoff) \u2502 \u25bc Publish to dead-letter-queue topic \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u25bc \u25bc Alert to PagerDuty Store in PostgreSQL for analysis \u2502 \u2502 \u25bc \u25bc Manual investigation Automated pattern detection ``` DLQ Processing SLA: < 4 hours for critical events, < 24 hours for non-critical. --- ## Caching Strategy ### Cache-Aside Pattern (Lazy Loading) Primary caching pattern used across all services: ``` 1. Application checks cache (Redis GET) 2. If HIT \u2192 Return cached data 3. If MISS: a. Query database (PostgreSQL) b. Store result in cache with TTL c. Return data to application ``` **Implementation Example** (Workflow Definition): ```javascript async function getWorkflowDefinition(workflowId) { const cacheKey = `workflow:def:${workflowId}`; // Step 1: Check cache let workflow = await redis.get(cacheKey); if (workflow) { metrics.increment('cache.hit.workflow_def'); return JSON.parse(workflow); } // Step 2: Cache miss - query database metrics.increment('cache.miss.workflow_def'); workflow = await db.query( 'SELECT * FROM workflows WHERE id = $1', [workflowId] ); // Step 3: Store in cache (1 hour TTL) await redis.setex(cacheKey, 3600, JSON.stringify(workflow)); return workflow; } ``` ### Write-Through Pattern Used for critical data where cache consistency is paramount: ``` 1. Application writes to database 2. Database transaction commits 3. Application updates cache 4. Return success to client ``` Applied to: User profiles, authentication sessions, system configuration.",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_2",
              "content": "**Technology**: Node.js with TypeScript, Bull queue library **Replicas**: 16 pods (production), auto-scaling 12-24 **Resource Allocation**: 4 vCPU, 8GB RAM per pod **Key Responsibilities**: - Parse and validate workflow definitions (JSON-based DSL) - Execute workflow steps with state machine pattern - Handle retries with exponential backoff (max 3 retries, backoff: 2^n seconds) - Coordinate parallel and sequential task execution - Manage workflow state persistence and recovery - Support for conditional branching and loops - Sub-workflow invocation and composition **Workflow Execution Model**: ``` Workflow Submitted \u2192 Validation \u2192 Queue in Kafka \u2502 \u25bc Workflow Engine picks up \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u25bc \u25bc \u25bc [Task 1 Exec] [Task 2 Exec] [Task 3 Exec] \u2502 \u2502 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u25bc State Update \u2192 PostgreSQL \u2502 \u25bc Publish Event \u2192 Kafka ``` **Workflow DSL Example**: ```json { \"workflow_id\": \"customer-onboarding-v2\", \"version\": \"2.1.0\", \"steps\": [ { \"id\": \"validate-customer\", \"type\": \"http_request\", \"config\": { \"url\": \"https://validation.internal/verify\", \"method\": \"POST\", \"timeout\": 5000, \"retry\": {\"max_attempts\": 3, \"backoff\": \"exponential\"} } }, { \"id\": \"create-account\", \"type\": \"database_operation\", \"depends_on\": [\"validate-customer\"] } ] } ``` **State Management**: - Workflow states: PENDING, RUNNING, COMPLETED, FAILED, CANCELLED - Step states: QUEUED, EXECUTING, SUCCESS, FAILED, SKIPPED - State transitions stored in PostgreSQL with event sourcing pattern - Checkpointing every 10 steps for long-running workflows **Performance Targets**: - Simple workflow (3-5 steps): < 2 seconds (P99) - Complex workflow (20+ steps): < 5 seconds (P99) - Throughput: 500 concurrent workflow executions per pod --- ### Scheduler Service **Purpose**: Time-based workflow triggering system supporting cron-like schedules and one-time delayed executions.",
              "score": null
            },
            {
              "doc_id": "user_guide",
              "chunk_id": "user_guide_fix_6",
              "content": "Filter by error type, date range, or execution ID 3. Click an execution to view full details 4. Click **\"Retry\"** to reprocess with the same input data ### Error Notifications Get notified when workflows fail: ```yaml workflow: notifications: on_failure: - type: email to: \"ops-team@company.com\" - type: slack channel: \"#alerts\" message: \"Workflow {{workflow.name}} failed: {{error.message}}\" on_success_after_retry: - type: slack channel: \"#monitoring\" message: \"Workflow recovered after {{error.attempts}} attempts\" ``` ## Workflow Limits CloudFlow enforces the following limits to ensure platform stability and performance: ### Steps Per Workflow - **Maximum**: 50 steps per workflow - **Recommendation**: Keep workflows focused and modular. If you need more steps, consider splitting into multiple workflows connected via webhooks. ### Execution Timeout - **Default**: 3600 seconds (60 minutes) - **Behavior**: Workflows exceeding this timeout are automatically terminated - **Custom Timeouts**: Enterprise plans can request custom timeout limits **Setting Step-Level Timeouts:** ```yaml - id: long_running_task action: http_request config: url: \"https://api.example.com/process\" timeout: 300 # 5 minutes for this specific step ``` ### Execution Limits - **Maximum**: 1000 executions per day (per workflow) - **Rate Limiting**: 100 concurrent executions per workflow - **Burst Limit**: 10 executions per second **What happens when limits are reached:** - New executions are queued automatically - Webhook triggers return HTTP 429 (Too Many Requests) - Scheduled executions are skipped (logged in audit trail) - Email notifications sent to workflow owner **Monitoring Usage:** View real-time metrics in your workflow dashboard: - Executions today: 847 / 1000 - Average duration: 12.3s - Success rate: 98.2% - Current queue depth: 3 ### Data Limits - **Maximum request/response size**: 10MB per action - **Maximum execution payload**: 50MB total - **Variable value size**: 1MB per variable ### Enterprise Plan Limits Enterprise customers can request increased limits: - Up to 100 steps per workflow - Up to 10,000 executions per day - Up to 7200 second timeout (2 hours) - Priority execution queue - Dedicated capacity allocation Contact sales@cloudflow.io for Enterprise pricing and custom limits. ## Best Practices Follow these best practices to build reliable, maintainable workflows: ### 1. Use Descriptive Names **Good:** - Workflow: \"Sync Customer Data from Salesforce to Database\" - Step: \"validate_customer_email\" **Bad:** - Workflow: \"Workflow 1\" - Step: \"step3\" ### 2.",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_5",
              "content": "Success/failure events published back to Kafka ``` **Event Schema**: ```json { \"event_id\": \"uuid-v4\", \"event_type\": \"workflow.execution.completed\", \"timestamp\": \"2026-01-15T10:30:00.000Z\", \"correlation_id\": \"request-trace-id\", \"payload\": { \"workflow_id\": \"wf-12345\", \"execution_id\": \"exec-67890\", \"status\": \"COMPLETED\", \"duration_ms\": 4230 }, \"metadata\": { \"source_service\": \"workflow-engine\", \"schema_version\": \"1.0\" } } ``` ### Inter-Service Communication Services communicate using two primary patterns: **Synchronous (gRPC)**: - Auth Service validation calls from API Gateway - User Service profile lookups - Low latency requirement (< 50ms) - Request-response pattern with circuit breaker **Asynchronous (Kafka)**: - Workflow execution events - Notification triggers - Audit log events - Eventual consistency acceptable --- ## Database Architecture ### PostgreSQL Primary Database **Cluster Configuration**: - Primary-replica setup with 1 primary + 2 read replicas - Instance type: db.r6g.2xlarge (8 vCPU, 64GB RAM) - Storage: 2TB gp3 SSD with 12,000 IOPS - Multi-AZ deployment for high availability - Automated backups: Daily snapshots, 30-day retention - Point-in-time recovery: 5-minute granularity **Database Schema Design**: ``` Core Tables: - users (5M rows): User accounts and profiles - workflows (2M rows): Workflow definitions - workflow_executions (500M rows, partitioned): Execution history - workflow_steps (2B rows, partitioned): Individual step records - schedules (100K rows): Scheduled workflow triggers - notifications (1B rows, partitioned): Notification delivery log Partitioning Strategy: - workflow_executions: Monthly partitions by created_at - workflow_steps: Monthly partitions by created_at - notifications: Weekly partitions by created_at - Automatic partition creation via cron job - Retention policy: Drop partitions older than 12 months ``` **Indexing Strategy**: - Primary keys: UUIDs with B-tree indexes - Frequently queried columns: Compound indexes (e.g., user_id + status + created_at) - JSONB columns: GIN indexes for workflow definitions - Full-text search: GiST indexes on description fields **Connection Pooling**: - PgBouncer in transaction mode - Pool size: 100 connections per service - Max client connections: 2000 - Connection timeout: 30 seconds **Performance Characteristics**: - Read query P99: < 20ms - Write query P99: < 50ms - Transaction throughput: 15,000 TPS - Replication lag: < 500ms --- ### Redis Caching Layer **Cluster Configuration**: - Redis Cluster with 6 nodes (3 primary + 3 replica) - Instance type: cache.r6g.xlarge (4 vCPU, 26GB RAM) - Total memory: 78GB usable cache space - Persistence: RDB snapshots every 5 minutes + AOF - Eviction policy: allkeys-lru **Cache Usage Patterns**: 1.",
              "score": null
            }
          ],
          "latency_ms": 12.719554000796052,
          "query_id": "realistic_013",
          "dimension": "synonym",
          "query": "How long are workflow definitions cached?"
        },
        {
          "key_facts": [
            "TTL: 1 hour",
            "Workflow Definitions: TTL: 1 hour"
          ],
          "found_facts": [],
          "missed_facts": [
            "TTL: 1 hour",
            "Workflow Definitions: TTL: 1 hour"
          ],
          "coverage": 0.0,
          "expected_docs": [
            "architecture_overview"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_7",
              "content": "records: 500 - Session timeout: 30 seconds --- ## Message Queue Patterns ### Pub/Sub Pattern Used for broadcasting events to multiple interested consumers: ``` Workflow Engine publishes workflow.execution.completed \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u25bc \u25bc \u25bc \u25bc Analytics Notification Audit Logger Billing Service Service Service Service ``` Each service maintains its own consumer group and processes events independently. Failures in one consumer don't affect others. ### Request-Reply Pattern Synchronous communication over async messaging (used sparingly): ``` API Gateway publishes request \u2192 Kafka (reply-to: temp-queue-123) \u2502 \u25bc Service processes request \u2502 \u25bc Service publishes response \u2192 temp-queue-123 \u2502 \u25bc API Gateway receives response ``` Timeout: 5 seconds, fallback to direct HTTP call if no response. ### Saga Pattern Distributed transaction management for multi-service workflows: ``` Step 1: Create Order \u2192 SUCCESS \u2192 Step 2: Reserve Inventory \u2502 FAILURE \u2502 \u25bc Compensating Transaction \u2502 \u25bc Cancel Order (Rollback) ``` Orchestrated by Workflow Engine with compensation logic defined in workflow DSL. ### Dead Letter Queue (DLQ) Failed messages after all retry attempts are routed to DLQ: ``` Message processing fails (3 retries with exponential backoff) \u2502 \u25bc Publish to dead-letter-queue topic \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u25bc \u25bc Alert to PagerDuty Store in PostgreSQL for analysis \u2502 \u2502 \u25bc \u25bc Manual investigation Automated pattern detection ``` DLQ Processing SLA: < 4 hours for critical events, < 24 hours for non-critical. --- ## Caching Strategy ### Cache-Aside Pattern (Lazy Loading) Primary caching pattern used across all services: ``` 1. Application checks cache (Redis GET) 2. If HIT \u2192 Return cached data 3. If MISS: a. Query database (PostgreSQL) b. Store result in cache with TTL c. Return data to application ``` **Implementation Example** (Workflow Definition): ```javascript async function getWorkflowDefinition(workflowId) { const cacheKey = `workflow:def:${workflowId}`; // Step 1: Check cache let workflow = await redis.get(cacheKey); if (workflow) { metrics.increment('cache.hit.workflow_def'); return JSON.parse(workflow); } // Step 2: Cache miss - query database metrics.increment('cache.miss.workflow_def'); workflow = await db.query( 'SELECT * FROM workflows WHERE id = $1', [workflowId] ); // Step 3: Store in cache (1 hour TTL) await redis.setex(cacheKey, 3600, JSON.stringify(workflow)); return workflow; } ``` ### Write-Through Pattern Used for critical data where cache consistency is paramount: ``` 1. Application writes to database 2. Database transaction commits 3. Application updates cache 4. Return success to client ``` Applied to: User profiles, authentication sessions, system configuration.",
              "score": null
            },
            {
              "doc_id": "user_guide",
              "chunk_id": "user_guide_fix_6",
              "content": "Filter by error type, date range, or execution ID 3. Click an execution to view full details 4. Click **\"Retry\"** to reprocess with the same input data ### Error Notifications Get notified when workflows fail: ```yaml workflow: notifications: on_failure: - type: email to: \"ops-team@company.com\" - type: slack channel: \"#alerts\" message: \"Workflow {{workflow.name}} failed: {{error.message}}\" on_success_after_retry: - type: slack channel: \"#monitoring\" message: \"Workflow recovered after {{error.attempts}} attempts\" ``` ## Workflow Limits CloudFlow enforces the following limits to ensure platform stability and performance: ### Steps Per Workflow - **Maximum**: 50 steps per workflow - **Recommendation**: Keep workflows focused and modular. If you need more steps, consider splitting into multiple workflows connected via webhooks. ### Execution Timeout - **Default**: 3600 seconds (60 minutes) - **Behavior**: Workflows exceeding this timeout are automatically terminated - **Custom Timeouts**: Enterprise plans can request custom timeout limits **Setting Step-Level Timeouts:** ```yaml - id: long_running_task action: http_request config: url: \"https://api.example.com/process\" timeout: 300 # 5 minutes for this specific step ``` ### Execution Limits - **Maximum**: 1000 executions per day (per workflow) - **Rate Limiting**: 100 concurrent executions per workflow - **Burst Limit**: 10 executions per second **What happens when limits are reached:** - New executions are queued automatically - Webhook triggers return HTTP 429 (Too Many Requests) - Scheduled executions are skipped (logged in audit trail) - Email notifications sent to workflow owner **Monitoring Usage:** View real-time metrics in your workflow dashboard: - Executions today: 847 / 1000 - Average duration: 12.3s - Success rate: 98.2% - Current queue depth: 3 ### Data Limits - **Maximum request/response size**: 10MB per action - **Maximum execution payload**: 50MB total - **Variable value size**: 1MB per variable ### Enterprise Plan Limits Enterprise customers can request increased limits: - Up to 100 steps per workflow - Up to 10,000 executions per day - Up to 7200 second timeout (2 hours) - Priority execution queue - Dedicated capacity allocation Contact sales@cloudflow.io for Enterprise pricing and custom limits. ## Best Practices Follow these best practices to build reliable, maintainable workflows: ### 1. Use Descriptive Names **Good:** - Workflow: \"Sync Customer Data from Salesforce to Database\" - Step: \"validate_customer_email\" **Bad:** - Workflow: \"Workflow 1\" - Step: \"step3\" ### 2.",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_2",
              "content": "**Technology**: Node.js with TypeScript, Bull queue library **Replicas**: 16 pods (production), auto-scaling 12-24 **Resource Allocation**: 4 vCPU, 8GB RAM per pod **Key Responsibilities**: - Parse and validate workflow definitions (JSON-based DSL) - Execute workflow steps with state machine pattern - Handle retries with exponential backoff (max 3 retries, backoff: 2^n seconds) - Coordinate parallel and sequential task execution - Manage workflow state persistence and recovery - Support for conditional branching and loops - Sub-workflow invocation and composition **Workflow Execution Model**: ``` Workflow Submitted \u2192 Validation \u2192 Queue in Kafka \u2502 \u25bc Workflow Engine picks up \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u25bc \u25bc \u25bc [Task 1 Exec] [Task 2 Exec] [Task 3 Exec] \u2502 \u2502 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u25bc State Update \u2192 PostgreSQL \u2502 \u25bc Publish Event \u2192 Kafka ``` **Workflow DSL Example**: ```json { \"workflow_id\": \"customer-onboarding-v2\", \"version\": \"2.1.0\", \"steps\": [ { \"id\": \"validate-customer\", \"type\": \"http_request\", \"config\": { \"url\": \"https://validation.internal/verify\", \"method\": \"POST\", \"timeout\": 5000, \"retry\": {\"max_attempts\": 3, \"backoff\": \"exponential\"} } }, { \"id\": \"create-account\", \"type\": \"database_operation\", \"depends_on\": [\"validate-customer\"] } ] } ``` **State Management**: - Workflow states: PENDING, RUNNING, COMPLETED, FAILED, CANCELLED - Step states: QUEUED, EXECUTING, SUCCESS, FAILED, SKIPPED - State transitions stored in PostgreSQL with event sourcing pattern - Checkpointing every 10 steps for long-running workflows **Performance Targets**: - Simple workflow (3-5 steps): < 2 seconds (P99) - Complex workflow (20+ steps): < 5 seconds (P99) - Throughput: 500 concurrent workflow executions per pod --- ### Scheduler Service **Purpose**: Time-based workflow triggering system supporting cron-like schedules and one-time delayed executions.",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_5",
              "content": "exec_7h3j6k9m2n --show-bottlenecks ``` #### Solutions **1. Increase workflow timeout (if justified):** ```bash # Update workflow configuration cloudflow workflows update wf_9k2n4m8p1q \\ --timeout 7200 \\ --reason \"Large dataset processing requires extended time\" # Verify update cloudflow workflows get wf_9k2n4m8p1q | grep timeout ``` **2. Optimize slow steps:** ```bash # Enable parallel processing cloudflow workflows update wf_9k2n4m8p1q \\ --step data_transformation \\ --parallel-workers 8 \\ --batch-size 1000 # Add checkpointing for long operations cloudflow workflows update wf_9k2n4m8p1q \\ --step data_transformation \\ --enable-checkpointing \\ --checkpoint-interval 300 ``` **3. Split workflow into smaller workflows:** ```bash # Create sub-workflows cloudflow workflows create data-pipeline-part1 \\ --steps \"data_ingestion,data_validation\" \\ --timeout 1800 cloudflow workflows create data-pipeline-part2 \\ --steps \"data_transformation,data_export\" \\ --timeout 3600 \\ --trigger workflow_completed \\ --trigger-workflow data-pipeline-part1 ``` ### Retry Logic and Exponential Backoff CloudFlow implements automatic retry with exponential backoff for transient failures: - Max retries: 3 - Initial delay: 1 second - Backoff multiplier: 2 - Max delay: 60 seconds #### Retry Sequence ``` Attempt 1: Immediate Attempt 2: Wait 1s (2^0 * 1s) Attempt 3: Wait 2s (2^1 * 1s) Attempt 4: Wait 4s (2^2 * 1s) ``` #### Configuration ```bash # View current retry settings cloudflow workflows get wf_9k2n4m8p1q --format json | jq '.retry_policy' # Customize retry behavior cloudflow workflows update wf_9k2n4m8p1q \\ --retry-max-attempts 5 \\ --retry-initial-delay 2000 \\ --retry-backoff-multiplier 2 \\ --retry-max-delay 120000 \\ --retry-on-errors \"NETWORK_ERROR,TIMEOUT,RATE_LIMIT\" # Disable retry for specific step cloudflow workflows update wf_9k2n4m8p1q \\ --step data_export \\ --retry-enabled false ``` #### Monitoring Retries ```bash # List failed executions with retry information cloudflow workflows executions list \\ --status FAILED \\ --show-retries \\ --last 7d # Get retry history for specific execution cloudflow workflows executions retries exec_7h3j6k9m2n # Output: # Execution: exec_7h3j6k9m2n # Attempt 1: FAILED - NetworkError: Connection refused (delay: 0ms) # Attempt 2: FAILED - NetworkError: Connection timeout (delay: 1000ms) # Attempt 3: FAILED - NetworkError: Connection timeout (delay: 2000ms) # Attempt 4: FAILED - NetworkError: Connection timeout (delay: 4000ms) # Final Status: FAILED_AFTER_RETRIES ``` ### Workflow Step Failures #### Common Error Patterns **1.",
              "score": null
            },
            {
              "doc_id": "user_guide",
              "chunk_id": "user_guide_fix_0",
              "content": "# CloudFlow User Guide Welcome to CloudFlow, the modern workflow automation platform that helps you connect your apps, automate repetitive tasks, and build powerful integrations without writing code. ## Table of Contents 1. [Getting Started](#getting-started) 2. [Workflow Creation](#workflow-creation) 3. [Available Actions](#available-actions) 4. [Variables and Expressions](#variables-and-expressions) 5. [Scheduling](#scheduling) 6. [Error Handling](#error-handling) 7. [Workflow Limits](#workflow-limits) 8. [Best Practices](#best-practices) 9. [Common Workflow Patterns](#common-workflow-patterns) ## Getting Started ### Account Setup Creating your CloudFlow account takes just a few minutes: 1. Navigate to https://app.cloudflow.io/signup 2. Enter your email address and create a strong password 3. Verify your email address by clicking the link sent to your inbox 4. Complete your profile by providing your organization name and use case 5. Choose your plan (Free, Professional, or Enterprise) Once your account is created, you'll be redirected to your CloudFlow dashboard where you can start building workflows immediately. ### Your First Workflow Let's create a simple workflow that sends you a Slack notification when a new file is uploaded to your Google Drive: 1. Click the **\"Create Workflow\"** button in the top-right corner 2. Give your workflow a name: \"Google Drive to Slack Notifier\" 3. Select **\"Google Drive - New File\"** as your trigger 4. Authenticate your Google Drive account when prompted 5. Choose the folder you want to monitor 6. Click **\"Add Action\"** and select **\"Slack - Send Message\"** 7. Authenticate your Slack account 8. Select the channel where you want to receive notifications 9. Customize your message using variables: `New file uploaded: {{trigger.file.name}}` 10. Click **\"Save & Activate\"** to enable your workflow Congratulations! Your first workflow is now live. Every time a file is added to the specified Google Drive folder, you'll receive a Slack notification. ## Workflow Creation CloudFlow offers multiple ways to create and manage workflows, giving you the flexibility to work in the way that suits you best. ### Visual Editor The Visual Editor is CloudFlow's drag-and-drop interface for building workflows without code. It's perfect for users who prefer a graphical approach: **Key Features:** - Drag-and-drop action blocks from the sidebar - Visual connections between steps show your workflow logic - Inline configuration for each action - Real-time validation and error checking - Test mode to run your workflow with sample data **Using the Visual Editor:** 1.",
              "score": null
            }
          ],
          "latency_ms": 12.807938997866586,
          "query_id": "realistic_013",
          "dimension": "problem",
          "query": "My workflow updates aren't reflecting immediately"
        },
        {
          "key_facts": [
            "TTL: 1 hour",
            "Workflow Definitions: TTL: 1 hour"
          ],
          "found_facts": [],
          "missed_facts": [
            "TTL: 1 hour",
            "Workflow Definitions: TTL: 1 hour"
          ],
          "coverage": 0.0,
          "expected_docs": [
            "architecture_overview"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_8",
              "content": "### Cache Invalidation Strategies **Time-Based Expiration (TTL)**: - Short-lived data: 5-15 minutes (session tokens, rate limit counters) - Medium-lived data: 1 hour (workflow definitions, templates) - Long-lived data: 24 hours (static configuration) **Event-Based Invalidation**: ``` Database Update Event \u2192 Kafka (cache.invalidation topic) \u2502 \u25bc All service instances consume event \u2502 \u25bc Redis DEL for affected keys ``` **Pattern-Based Invalidation**: - Use Redis SCAN + DEL for wildcard patterns - Example: Invalidate all user caches: `user:*:{user_id}` **Cache Warming**: - Scheduled job runs every hour to pre-populate frequently accessed data - Targets: Top 1000 workflows, system configuration, active user profiles - Reduces cache miss rate during traffic spikes ### Multi-Level Caching Application implements L1 (in-memory) and L2 (Redis) caching: ``` Request \u2192 L1 Cache (Node.js Map, 1000 entry LRU) \u2502 MISS \u2502 \u25bc L2 Cache (Redis, distributed) \u2502 MISS \u2502 \u25bc Database (PostgreSQL) ``` L1 cache reduces Redis network calls by 40% for hot data. --- ## Security Architecture ### Network Security **Zero-Trust Network Model**: - All service-to-service communication encrypted with mTLS - Certificate rotation: Every 90 days (automated via cert-manager) - Certificate authority: Internal PKI with HashiCorp Vault - Network segmentation: Private subnets for services, public subnet for ALB only **Service Mesh (Istio)**: ``` Service A \u2192 Envoy Sidecar (mTLS client cert) \u2502 Encrypted channel \u2502 Envoy Sidecar (mTLS server cert) \u2192 Service B ``` **Firewall Rules**: - Security groups: Deny all by default, explicit allow rules - Ingress: Only ALB can reach API Gateway (port 8080) - Egress: Services can only reach specific dependencies - Database access: Limited to application subnets only ### Authentication & Authorization **JWT Token Validation**: - Algorithm: RS256 (asymmetric signing) - Key rotation: Every 30 days with 7-day overlap period - Public key distribution: JWKS endpoint cached in Redis - Validation: Signature, expiry, issuer, audience claims - Token revocation: Blacklist in Redis for compromised tokens **Permission Model**: ``` User \u2192 Roles \u2192 Permissions \u2198 \u2197 Tenants (Multi-tenancy isolation) ``` Example permissions: - `workflow:read` - View workflows - `workflow:write` - Create/update workflows - `workflow:execute` - Trigger workflow execution - `workflow:delete` - Delete workflows - `admin:*` - All administrative operations **API Key Management**: - Format: `cfk_live_<32-char-random>` (production), `cfk_test_<32-char-random>` (sandbox) - Hashing: SHA-256 before storage in PostgreSQL - Scoping: API keys can be scoped to specific workflows or operations - Rate limits: Configurable",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_7",
              "content": "records: 500 - Session timeout: 30 seconds --- ## Message Queue Patterns ### Pub/Sub Pattern Used for broadcasting events to multiple interested consumers: ``` Workflow Engine publishes workflow.execution.completed \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u25bc \u25bc \u25bc \u25bc Analytics Notification Audit Logger Billing Service Service Service Service ``` Each service maintains its own consumer group and processes events independently. Failures in one consumer don't affect others. ### Request-Reply Pattern Synchronous communication over async messaging (used sparingly): ``` API Gateway publishes request \u2192 Kafka (reply-to: temp-queue-123) \u2502 \u25bc Service processes request \u2502 \u25bc Service publishes response \u2192 temp-queue-123 \u2502 \u25bc API Gateway receives response ``` Timeout: 5 seconds, fallback to direct HTTP call if no response. ### Saga Pattern Distributed transaction management for multi-service workflows: ``` Step 1: Create Order \u2192 SUCCESS \u2192 Step 2: Reserve Inventory \u2502 FAILURE \u2502 \u25bc Compensating Transaction \u2502 \u25bc Cancel Order (Rollback) ``` Orchestrated by Workflow Engine with compensation logic defined in workflow DSL. ### Dead Letter Queue (DLQ) Failed messages after all retry attempts are routed to DLQ: ``` Message processing fails (3 retries with exponential backoff) \u2502 \u25bc Publish to dead-letter-queue topic \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u25bc \u25bc Alert to PagerDuty Store in PostgreSQL for analysis \u2502 \u2502 \u25bc \u25bc Manual investigation Automated pattern detection ``` DLQ Processing SLA: < 4 hours for critical events, < 24 hours for non-critical. --- ## Caching Strategy ### Cache-Aside Pattern (Lazy Loading) Primary caching pattern used across all services: ``` 1. Application checks cache (Redis GET) 2. If HIT \u2192 Return cached data 3. If MISS: a. Query database (PostgreSQL) b. Store result in cache with TTL c. Return data to application ``` **Implementation Example** (Workflow Definition): ```javascript async function getWorkflowDefinition(workflowId) { const cacheKey = `workflow:def:${workflowId}`; // Step 1: Check cache let workflow = await redis.get(cacheKey); if (workflow) { metrics.increment('cache.hit.workflow_def'); return JSON.parse(workflow); } // Step 2: Cache miss - query database metrics.increment('cache.miss.workflow_def'); workflow = await db.query( 'SELECT * FROM workflows WHERE id = $1', [workflowId] ); // Step 3: Store in cache (1 hour TTL) await redis.setex(cacheKey, 3600, JSON.stringify(workflow)); return workflow; } ``` ### Write-Through Pattern Used for critical data where cache consistency is paramount: ``` 1. Application writes to database 2. Database transaction commits 3. Application updates cache 4. Return success to client ``` Applied to: User profiles, authentication sessions, system configuration.",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_2",
              "content": "Review Query Execution Plans** ```sql -- Connect to CloudFlow database cloudflow db connect --readonly -- Explain slow query EXPLAIN ANALYZE SELECT w.*, e.status, e.error_message FROM workflows w LEFT JOIN executions e ON w.id = e.workflow_id WHERE w.workspace_id = 'ws_abc123' AND e.created_at > NOW() - INTERVAL '7 days' ORDER BY e.created_at DESC; -- Check for missing indexes SELECT schemaname, tablename, indexname, indexdef FROM pg_indexes WHERE tablename IN ('workflows', 'executions', 'workflow_steps') ORDER BY tablename, indexname; ``` **3. Optimize Queries** Common optimization techniques: **Add missing indexes:** ```sql -- Index for workflow lookup by workspace CREATE INDEX CONCURRENTLY idx_workflows_workspace_created ON workflows(workspace_id, created_at DESC); -- Index for execution status queries CREATE INDEX CONCURRENTLY idx_executions_status_created ON executions(workflow_id, status, created_at DESC) WHERE status IN ('running', 'pending'); -- Composite index for common filter combinations CREATE INDEX CONCURRENTLY idx_executions_workspace_status ON executions(workspace_id, status, created_at) INCLUDE (error_message, retry_count); ``` **Use query result caching:** ```bash # Enable query cache for workspace metadata cloudflow config set cache.workspace.ttl 3600 # Configure Redis cache backend cloudflow config set cache.backend redis cloudflow config set cache.redis.host redis.cloudflow.svc.cluster.local cloudflow config set cache.redis.port 6379 ``` ### High API Latency #### Latency Breakdown Analysis ```bash # Generate latency report cloudflow metrics latency-report --endpoint \"/api/v1/workflows\" --last 24h # Sample output: # Endpoint: POST /api/v1/workflows/execute # P50: 245ms | P95: 1823ms | P99: 4521ms # Breakdown: # - Auth: 45ms (18%) # - DB Query: 156ms (64%) # - Business Logic: 32ms (13%) # - Response Serialization: 12ms (5%) ``` **Network latency issues:** ```bash # Test connectivity to CloudFlow API time curl -w \"@curl-format.txt\" -o /dev/null -s https://api.cloudflow.io/health # Create curl-format.txt: cat > curl-format.txt << EOF time_namelookup: %{time_namelookup}s\\n time_connect: %{time_connect}s\\n time_appconnect: %{time_appconnect}s\\n time_pretransfer: %{time_pretransfer}s\\n time_redirect: %{time_redirect}s\\n time_starttransfer: %{time_starttransfer}s\\n ----------\\n time_total: %{time_total}s\\n EOF # Trace route to API endpoint traceroute api.cloudflow.io # Check DNS resolution time dig api.cloudflow.io | grep \"Query time\" ``` ### Memory Leaks #### Detection ```bash # Monitor CloudFlow service memory usage kubectl top pods -n cloudflow --sort-by=memory # Get detailed memory metrics for specific pod kubectl exec -n cloudflow deploy/cloudflow-api -- \\ curl localhost:9090/metrics | grep memory # Check for OOMKilled pods kubectl get pods -n cloudflow --field-selector=status.phase=Failed | \\ grep OOMKilled # Review memory limits and requests kubectl describe deployment cloudflow-api -n cloudflow | \\ grep -A 5 \"Limits\\|Requests\" ``` #### Common Causes **1.",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_9",
              "content": "per API key (default: 1000 RPM) ### Secrets Management **HashiCorp Vault Integration**: - Dynamic database credentials: Generated on-demand, 1-hour TTL - Encryption keys: Transit secrets engine for encryption-as-a-service - API keys for external services: Stored with versioning - Rotation policy: Automated rotation every 90 days with notification **Secret Access Pattern**: ``` Service starts \u2192 Vault authentication (Kubernetes service account) \u2502 \u25bc Request secret lease (1-hour TTL) \u2502 \u25bc Vault returns dynamic credentials \u2502 \u25bc Service connects to PostgreSQL \u2502 Renew lease every 30 minutes ``` ### Data Protection **Encryption at Rest**: - PostgreSQL: AES-256 encryption enabled at volume level - Redis: Encryption enabled using AWS KMS - Kafka: Encryption at rest for all data on brokers - S3 backups: Server-side encryption with KMS (SSE-KMS) **Encryption in Transit**: - External traffic: TLS 1.3 only (TLS 1.2 deprecated) - Internal traffic: mTLS via Istio service mesh - Database connections: SSL/TLS enforced (sslmode=require) **Data Masking**: - Sensitive fields (email, phone) masked in logs - PII redacted from error messages and stack traces - Audit logs: Full data retention with access controls **Compliance**: - GDPR: User data export and deletion workflows - SOC 2 Type II: Audit logging for all data access - HIPAA: PHI isolation in dedicated tenants with enhanced encryption --- ## Performance Characteristics ### Latency Targets **API Operations** (P99 latency): - `GET /workflows/{id}`: < 50ms (cache hit), < 150ms (cache miss) - `POST /workflows`: < 200ms (includes validation and database write) - `POST /workflows/{id}/execute`: < 100ms (async, returns execution ID) - `GET /workflows/{id}/history`: < 300ms (paginated, 50 records per page) **Workflow Execution** (P99 latency): - Simple workflow (< 5 steps): < 2 seconds - Medium workflow (5-15 steps): < 5 seconds - Complex workflow (> 15 steps): < 15 seconds - Parallel execution: Linear scaling up to 10 concurrent branches **Database Operations** (P99 latency): - Simple SELECT: < 5ms - JOIN query: < 20ms - INSERT/UPDATE: < 10ms - Batch operations: < 100ms (batch size: 100 records) **Cache Operations** (P99 latency): - Redis GET: < 2ms - Redis SET: < 3ms - Redis DEL: < 2ms ### Throughput Capacity **API Gateway**: - Sustained: 10,000 requests per second - Peak: 25,000 requests per second (5-minute burst) - Rate limiting: 1,000 requests per minute per API key **Workflow Engine**: - Concurrent executions: 8,000 workflows (across 16 pods)",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_8",
              "content": "Leverage caching:** ```bash # Enable client-side caching export CLOUDFLOW_CACHE_ENABLED=true export CLOUDFLOW_CACHE_TTL=300 # Cache workflow metadata cloudflow workflows list --use-cache --cache-ttl 600 ``` --- ## Log Analysis & Debugging ### Accessing CloudFlow Logs #### Kubernetes Deployments ```bash # List all CloudFlow pods kubectl get pods -n cloudflow # Tail logs from API server kubectl logs -f -n cloudflow deployment/cloudflow-api --tail=100 # Get logs from specific pod kubectl logs -n cloudflow cloudflow-api-7d4f6b8c9d-x7k2m # Get logs from previous crashed pod kubectl logs -n cloudflow cloudflow-api-7d4f6b8c9d-x7k2m --previous # Get logs from all pods in deployment kubectl logs -n cloudflow deployment/cloudflow-api --all-containers=true # Stream logs from multiple pods kubectl logs -n cloudflow -l app=cloudflow-api -f --max-log-requests=10 ``` #### Log Levels CloudFlow supports the following log levels: - `TRACE` - Very detailed debugging information - `DEBUG` - Detailed debugging information - `INFO` - Informational messages (default) - `WARN` - Warning messages - `ERROR` - Error messages - `FATAL` - Fatal errors causing shutdown **Changing log levels:** ```bash # Set global log level cloudflow config set logging.level DEBUG # Set log level for specific component cloudflow config set logging.components.database DEBUG cloudflow config set logging.components.auth INFO cloudflow config set logging.components.workflows TRACE # Temporary log level increase (resets after 1 hour) cloudflow debug set-log-level DEBUG --duration 3600 # View current log configuration cloudflow config get logging --format json ``` ### Grep Patterns for Common Issues #### Authentication Failures ```bash # Find all authentication errors kubectl logs -n cloudflow deployment/cloudflow-api | \\ grep -i \"authentication\\|401\\|unauthorized\" | \\ tail -n 50 # Find JWT validation failures kubectl logs -n cloudflow deployment/cloudflow-api | \\ grep -E \"JWT|token.*invalid|signature.*failed\" # Find clock skew issues kubectl logs -n cloudflow deployment/cloudflow-api | \\ grep -i \"clock skew\\|time.*sync\\|nbf\\|exp\" ``` #### Database Errors ```bash # Find database connection errors kubectl logs -n cloudflow deployment/cloudflow-api | \\ grep -E \"connection.*pool|could not connect|database.*timeout\" # Find slow queries kubectl logs -n cloudflow deployment/cloudflow-api | \\ grep \"slow query\" | \\ awk '{print $NF}' | \\ sort -n | \\ tail -n 20 # Find deadlock errors kubectl logs -n cloudflow deployment/cloudflow-api | \\ grep -i \"deadlock detected\" ``` #### Workflow Execution Errors ```bash # Find workflow timeout errors kubectl logs -n cloudflow deployment/cloudflow-workflow-engine | \\ grep -E \"timeout|exceeded.*3600\" # Find workflow retry attempts kubectl logs -n cloudflow deployment/cloudflow-workflow-engine | \\ grep -E \"retry attempt [0-9]|retrying",
              "score": null
            }
          ],
          "latency_ms": 12.61931800036109,
          "query_id": "realistic_013",
          "dimension": "casual",
          "query": "cache ttl workflows"
        },
        {
          "key_facts": [
            "TTL: 1 hour",
            "Workflow Definitions: TTL: 1 hour"
          ],
          "found_facts": [],
          "missed_facts": [
            "TTL: 1 hour",
            "Workflow Definitions: TTL: 1 hour"
          ],
          "coverage": 0.0,
          "expected_docs": [
            "architecture_overview"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_7",
              "content": "records: 500 - Session timeout: 30 seconds --- ## Message Queue Patterns ### Pub/Sub Pattern Used for broadcasting events to multiple interested consumers: ``` Workflow Engine publishes workflow.execution.completed \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u25bc \u25bc \u25bc \u25bc Analytics Notification Audit Logger Billing Service Service Service Service ``` Each service maintains its own consumer group and processes events independently. Failures in one consumer don't affect others. ### Request-Reply Pattern Synchronous communication over async messaging (used sparingly): ``` API Gateway publishes request \u2192 Kafka (reply-to: temp-queue-123) \u2502 \u25bc Service processes request \u2502 \u25bc Service publishes response \u2192 temp-queue-123 \u2502 \u25bc API Gateway receives response ``` Timeout: 5 seconds, fallback to direct HTTP call if no response. ### Saga Pattern Distributed transaction management for multi-service workflows: ``` Step 1: Create Order \u2192 SUCCESS \u2192 Step 2: Reserve Inventory \u2502 FAILURE \u2502 \u25bc Compensating Transaction \u2502 \u25bc Cancel Order (Rollback) ``` Orchestrated by Workflow Engine with compensation logic defined in workflow DSL. ### Dead Letter Queue (DLQ) Failed messages after all retry attempts are routed to DLQ: ``` Message processing fails (3 retries with exponential backoff) \u2502 \u25bc Publish to dead-letter-queue topic \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u25bc \u25bc Alert to PagerDuty Store in PostgreSQL for analysis \u2502 \u2502 \u25bc \u25bc Manual investigation Automated pattern detection ``` DLQ Processing SLA: < 4 hours for critical events, < 24 hours for non-critical. --- ## Caching Strategy ### Cache-Aside Pattern (Lazy Loading) Primary caching pattern used across all services: ``` 1. Application checks cache (Redis GET) 2. If HIT \u2192 Return cached data 3. If MISS: a. Query database (PostgreSQL) b. Store result in cache with TTL c. Return data to application ``` **Implementation Example** (Workflow Definition): ```javascript async function getWorkflowDefinition(workflowId) { const cacheKey = `workflow:def:${workflowId}`; // Step 1: Check cache let workflow = await redis.get(cacheKey); if (workflow) { metrics.increment('cache.hit.workflow_def'); return JSON.parse(workflow); } // Step 2: Cache miss - query database metrics.increment('cache.miss.workflow_def'); workflow = await db.query( 'SELECT * FROM workflows WHERE id = $1', [workflowId] ); // Step 3: Store in cache (1 hour TTL) await redis.setex(cacheKey, 3600, JSON.stringify(workflow)); return workflow; } ``` ### Write-Through Pattern Used for critical data where cache consistency is paramount: ``` 1. Application writes to database 2. Database transaction commits 3. Application updates cache 4. Return success to client ``` Applied to: User profiles, authentication sessions, system configuration.",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_5",
              "content": "exec_7h3j6k9m2n --show-bottlenecks ``` #### Solutions **1. Increase workflow timeout (if justified):** ```bash # Update workflow configuration cloudflow workflows update wf_9k2n4m8p1q \\ --timeout 7200 \\ --reason \"Large dataset processing requires extended time\" # Verify update cloudflow workflows get wf_9k2n4m8p1q | grep timeout ``` **2. Optimize slow steps:** ```bash # Enable parallel processing cloudflow workflows update wf_9k2n4m8p1q \\ --step data_transformation \\ --parallel-workers 8 \\ --batch-size 1000 # Add checkpointing for long operations cloudflow workflows update wf_9k2n4m8p1q \\ --step data_transformation \\ --enable-checkpointing \\ --checkpoint-interval 300 ``` **3. Split workflow into smaller workflows:** ```bash # Create sub-workflows cloudflow workflows create data-pipeline-part1 \\ --steps \"data_ingestion,data_validation\" \\ --timeout 1800 cloudflow workflows create data-pipeline-part2 \\ --steps \"data_transformation,data_export\" \\ --timeout 3600 \\ --trigger workflow_completed \\ --trigger-workflow data-pipeline-part1 ``` ### Retry Logic and Exponential Backoff CloudFlow implements automatic retry with exponential backoff for transient failures: - Max retries: 3 - Initial delay: 1 second - Backoff multiplier: 2 - Max delay: 60 seconds #### Retry Sequence ``` Attempt 1: Immediate Attempt 2: Wait 1s (2^0 * 1s) Attempt 3: Wait 2s (2^1 * 1s) Attempt 4: Wait 4s (2^2 * 1s) ``` #### Configuration ```bash # View current retry settings cloudflow workflows get wf_9k2n4m8p1q --format json | jq '.retry_policy' # Customize retry behavior cloudflow workflows update wf_9k2n4m8p1q \\ --retry-max-attempts 5 \\ --retry-initial-delay 2000 \\ --retry-backoff-multiplier 2 \\ --retry-max-delay 120000 \\ --retry-on-errors \"NETWORK_ERROR,TIMEOUT,RATE_LIMIT\" # Disable retry for specific step cloudflow workflows update wf_9k2n4m8p1q \\ --step data_export \\ --retry-enabled false ``` #### Monitoring Retries ```bash # List failed executions with retry information cloudflow workflows executions list \\ --status FAILED \\ --show-retries \\ --last 7d # Get retry history for specific execution cloudflow workflows executions retries exec_7h3j6k9m2n # Output: # Execution: exec_7h3j6k9m2n # Attempt 1: FAILED - NetworkError: Connection refused (delay: 0ms) # Attempt 2: FAILED - NetworkError: Connection timeout (delay: 1000ms) # Attempt 3: FAILED - NetworkError: Connection timeout (delay: 2000ms) # Attempt 4: FAILED - NetworkError: Connection timeout (delay: 4000ms) # Final Status: FAILED_AFTER_RETRIES ``` ### Workflow Step Failures #### Common Error Patterns **1.",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_8",
              "content": "### Cache Invalidation Strategies **Time-Based Expiration (TTL)**: - Short-lived data: 5-15 minutes (session tokens, rate limit counters) - Medium-lived data: 1 hour (workflow definitions, templates) - Long-lived data: 24 hours (static configuration) **Event-Based Invalidation**: ``` Database Update Event \u2192 Kafka (cache.invalidation topic) \u2502 \u25bc All service instances consume event \u2502 \u25bc Redis DEL for affected keys ``` **Pattern-Based Invalidation**: - Use Redis SCAN + DEL for wildcard patterns - Example: Invalidate all user caches: `user:*:{user_id}` **Cache Warming**: - Scheduled job runs every hour to pre-populate frequently accessed data - Targets: Top 1000 workflows, system configuration, active user profiles - Reduces cache miss rate during traffic spikes ### Multi-Level Caching Application implements L1 (in-memory) and L2 (Redis) caching: ``` Request \u2192 L1 Cache (Node.js Map, 1000 entry LRU) \u2502 MISS \u2502 \u25bc L2 Cache (Redis, distributed) \u2502 MISS \u2502 \u25bc Database (PostgreSQL) ``` L1 cache reduces Redis network calls by 40% for hot data. --- ## Security Architecture ### Network Security **Zero-Trust Network Model**: - All service-to-service communication encrypted with mTLS - Certificate rotation: Every 90 days (automated via cert-manager) - Certificate authority: Internal PKI with HashiCorp Vault - Network segmentation: Private subnets for services, public subnet for ALB only **Service Mesh (Istio)**: ``` Service A \u2192 Envoy Sidecar (mTLS client cert) \u2502 Encrypted channel \u2502 Envoy Sidecar (mTLS server cert) \u2192 Service B ``` **Firewall Rules**: - Security groups: Deny all by default, explicit allow rules - Ingress: Only ALB can reach API Gateway (port 8080) - Egress: Services can only reach specific dependencies - Database access: Limited to application subnets only ### Authentication & Authorization **JWT Token Validation**: - Algorithm: RS256 (asymmetric signing) - Key rotation: Every 30 days with 7-day overlap period - Public key distribution: JWKS endpoint cached in Redis - Validation: Signature, expiry, issuer, audience claims - Token revocation: Blacklist in Redis for compromised tokens **Permission Model**: ``` User \u2192 Roles \u2192 Permissions \u2198 \u2197 Tenants (Multi-tenancy isolation) ``` Example permissions: - `workflow:read` - View workflows - `workflow:write` - Create/update workflows - `workflow:execute` - Trigger workflow execution - `workflow:delete` - Delete workflows - `admin:*` - All administrative operations **API Key Management**: - Format: `cfk_live_<32-char-random>` (production), `cfk_test_<32-char-random>` (sandbox) - Hashing: SHA-256 before storage in PostgreSQL - Scoping: API keys can be scoped to specific workflows or operations - Rate limits: Configurable",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_2",
              "content": "Review Query Execution Plans** ```sql -- Connect to CloudFlow database cloudflow db connect --readonly -- Explain slow query EXPLAIN ANALYZE SELECT w.*, e.status, e.error_message FROM workflows w LEFT JOIN executions e ON w.id = e.workflow_id WHERE w.workspace_id = 'ws_abc123' AND e.created_at > NOW() - INTERVAL '7 days' ORDER BY e.created_at DESC; -- Check for missing indexes SELECT schemaname, tablename, indexname, indexdef FROM pg_indexes WHERE tablename IN ('workflows', 'executions', 'workflow_steps') ORDER BY tablename, indexname; ``` **3. Optimize Queries** Common optimization techniques: **Add missing indexes:** ```sql -- Index for workflow lookup by workspace CREATE INDEX CONCURRENTLY idx_workflows_workspace_created ON workflows(workspace_id, created_at DESC); -- Index for execution status queries CREATE INDEX CONCURRENTLY idx_executions_status_created ON executions(workflow_id, status, created_at DESC) WHERE status IN ('running', 'pending'); -- Composite index for common filter combinations CREATE INDEX CONCURRENTLY idx_executions_workspace_status ON executions(workspace_id, status, created_at) INCLUDE (error_message, retry_count); ``` **Use query result caching:** ```bash # Enable query cache for workspace metadata cloudflow config set cache.workspace.ttl 3600 # Configure Redis cache backend cloudflow config set cache.backend redis cloudflow config set cache.redis.host redis.cloudflow.svc.cluster.local cloudflow config set cache.redis.port 6379 ``` ### High API Latency #### Latency Breakdown Analysis ```bash # Generate latency report cloudflow metrics latency-report --endpoint \"/api/v1/workflows\" --last 24h # Sample output: # Endpoint: POST /api/v1/workflows/execute # P50: 245ms | P95: 1823ms | P99: 4521ms # Breakdown: # - Auth: 45ms (18%) # - DB Query: 156ms (64%) # - Business Logic: 32ms (13%) # - Response Serialization: 12ms (5%) ``` **Network latency issues:** ```bash # Test connectivity to CloudFlow API time curl -w \"@curl-format.txt\" -o /dev/null -s https://api.cloudflow.io/health # Create curl-format.txt: cat > curl-format.txt << EOF time_namelookup: %{time_namelookup}s\\n time_connect: %{time_connect}s\\n time_appconnect: %{time_appconnect}s\\n time_pretransfer: %{time_pretransfer}s\\n time_redirect: %{time_redirect}s\\n time_starttransfer: %{time_starttransfer}s\\n ----------\\n time_total: %{time_total}s\\n EOF # Trace route to API endpoint traceroute api.cloudflow.io # Check DNS resolution time dig api.cloudflow.io | grep \"Query time\" ``` ### Memory Leaks #### Detection ```bash # Monitor CloudFlow service memory usage kubectl top pods -n cloudflow --sort-by=memory # Get detailed memory metrics for specific pod kubectl exec -n cloudflow deploy/cloudflow-api -- \\ curl localhost:9090/metrics | grep memory # Check for OOMKilled pods kubectl get pods -n cloudflow --field-selector=status.phase=Failed | \\ grep OOMKilled # Review memory limits and requests kubectl describe deployment cloudflow-api -n cloudflow | \\ grep -A 5 \"Limits\\|Requests\" ``` #### Common Causes **1.",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_8",
              "content": "Leverage caching:** ```bash # Enable client-side caching export CLOUDFLOW_CACHE_ENABLED=true export CLOUDFLOW_CACHE_TTL=300 # Cache workflow metadata cloudflow workflows list --use-cache --cache-ttl 600 ``` --- ## Log Analysis & Debugging ### Accessing CloudFlow Logs #### Kubernetes Deployments ```bash # List all CloudFlow pods kubectl get pods -n cloudflow # Tail logs from API server kubectl logs -f -n cloudflow deployment/cloudflow-api --tail=100 # Get logs from specific pod kubectl logs -n cloudflow cloudflow-api-7d4f6b8c9d-x7k2m # Get logs from previous crashed pod kubectl logs -n cloudflow cloudflow-api-7d4f6b8c9d-x7k2m --previous # Get logs from all pods in deployment kubectl logs -n cloudflow deployment/cloudflow-api --all-containers=true # Stream logs from multiple pods kubectl logs -n cloudflow -l app=cloudflow-api -f --max-log-requests=10 ``` #### Log Levels CloudFlow supports the following log levels: - `TRACE` - Very detailed debugging information - `DEBUG` - Detailed debugging information - `INFO` - Informational messages (default) - `WARN` - Warning messages - `ERROR` - Error messages - `FATAL` - Fatal errors causing shutdown **Changing log levels:** ```bash # Set global log level cloudflow config set logging.level DEBUG # Set log level for specific component cloudflow config set logging.components.database DEBUG cloudflow config set logging.components.auth INFO cloudflow config set logging.components.workflows TRACE # Temporary log level increase (resets after 1 hour) cloudflow debug set-log-level DEBUG --duration 3600 # View current log configuration cloudflow config get logging --format json ``` ### Grep Patterns for Common Issues #### Authentication Failures ```bash # Find all authentication errors kubectl logs -n cloudflow deployment/cloudflow-api | \\ grep -i \"authentication\\|401\\|unauthorized\" | \\ tail -n 50 # Find JWT validation failures kubectl logs -n cloudflow deployment/cloudflow-api | \\ grep -E \"JWT|token.*invalid|signature.*failed\" # Find clock skew issues kubectl logs -n cloudflow deployment/cloudflow-api | \\ grep -i \"clock skew\\|time.*sync\\|nbf\\|exp\" ``` #### Database Errors ```bash # Find database connection errors kubectl logs -n cloudflow deployment/cloudflow-api | \\ grep -E \"connection.*pool|could not connect|database.*timeout\" # Find slow queries kubectl logs -n cloudflow deployment/cloudflow-api | \\ grep \"slow query\" | \\ awk '{print $NF}' | \\ sort -n | \\ tail -n 20 # Find deadlock errors kubectl logs -n cloudflow deployment/cloudflow-api | \\ grep -i \"deadlock detected\" ``` #### Workflow Execution Errors ```bash # Find workflow timeout errors kubectl logs -n cloudflow deployment/cloudflow-workflow-engine | \\ grep -E \"timeout|exceeded.*3600\" # Find workflow retry attempts kubectl logs -n cloudflow deployment/cloudflow-workflow-engine | \\ grep -E \"retry attempt [0-9]|retrying",
              "score": null
            }
          ],
          "latency_ms": 13.017680001212284,
          "query_id": "realistic_013",
          "dimension": "contextual",
          "query": "After updating a workflow, how long until the cache expires?"
        },
        {
          "key_facts": [
            "TTL: 1 hour",
            "Workflow Definitions: TTL: 1 hour"
          ],
          "found_facts": [],
          "missed_facts": [
            "TTL: 1 hour",
            "Workflow Definitions: TTL: 1 hour"
          ],
          "coverage": 0.0,
          "expected_docs": [
            "architecture_overview"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "user_guide",
              "chunk_id": "user_guide_fix_8",
              "content": "Version Control For critical workflows: - Export YAML definitions regularly - Store in version control (Git) - Use pull requests for changes - Tag releases with semantic versioning ## Common Workflow Patterns Here are proven patterns for common automation scenarios: ### Pattern 1: Form to Database Capture form submissions and store in database: ```yaml name: \"Contact Form to Database\" trigger: type: webhook method: POST steps: - id: validate_submission action: javascript code: | if (!input.email || !input.message) { throw new Error(\"Email and message required\"); } return input; - id: check_duplicate action: database_query config: connection: \"{{secrets.DB_CONNECTION}}\" query: \"SELECT id FROM contacts WHERE email = $1 AND created_at > NOW() - INTERVAL '1 hour'\" parameters: - \"{{trigger.body.email}}\" - id: insert_contact action: database_query condition: \"{{length(steps.check_duplicate.rows) == 0}}\" config: connection: \"{{secrets.DB_CONNECTION}}\" query: | INSERT INTO contacts (name, email, message, source, created_at) VALUES ($1, $2, $3, $4, NOW()) parameters: - \"{{trigger.body.name}}\" - \"{{trigger.body.email}}\" - \"{{trigger.body.message}}\" - \"website_form\" - id: notify_sales action: slack_message condition: \"{{steps.insert_contact.affected_rows > 0}}\" config: channel: \"#leads\" text: \"New contact form submission from {{trigger.body.email}}\" ``` ### Pattern 2: API Polling Periodically check an API and take action on new items: ```yaml name: \"Poll GitHub Issues\" schedule: cron: \"*/15 * * * *\" # Every 15 minutes timezone: \"UTC\" steps: - id: get_last_check action: database_query config: query: \"SELECT last_checked_at FROM workflow_state WHERE workflow_id = $1\" parameters: - \"{{workflow.id}}\" - id: fetch_issues action: http_request config: method: GET url: \"https://api.github.com/repos/company/project/issues\" params: since: \"{{steps.get_last_check.rows[0].last_checked_at}}\" state: \"open\" headers: Authorization: \"token {{secrets.GITHUB_TOKEN}}\" - id: process_new_issues action: javascript code: | const issues = input.body; return { count: issues.length, issues: issues.map(i => ({ number: i.number, title: i.title, url: i.html_url })) }; - id: notify_team action: slack_message condition: \"{{steps.process_new_issues.output.count > 0}}\" config: channel: \"#engineering\" text: \"{{steps.process_new_issues.output.count}} new GitHub issues\" - id: update_last_check action: database_query config: query: | UPDATE workflow_state SET last_checked_at = $1 WHERE workflow_id = $2 parameters: - \"{{now()}}\" - \"{{workflow.id}}\" ``` ### Pattern 3: Multi-Step Approval Implement approval workflows with timeouts: ```yaml name: \"Expense Approval Workflow\" trigger: type: webhook method: POST steps: - id: create_approval_request action: database_query config: query: | INSERT INTO approvals (expense_id, amount, requester, status, created_at) VALUES ($1, $2, $3, 'pending', NOW()) RETURNING id parameters: - \"{{trigger.body.expense_id}}\" - \"{{trigger.body.amount}}\" - \"{{trigger.body.requester}}\" - id: notify_manager action: email config: to: \"{{trigger.body.manager_email}}\" subject: \"Expense Approval Required: ${{trigger.body.amount}}\" body: | An expense requires your approval: Amount: ${{trigger.body.amount}}",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_7",
              "content": "records: 500 - Session timeout: 30 seconds --- ## Message Queue Patterns ### Pub/Sub Pattern Used for broadcasting events to multiple interested consumers: ``` Workflow Engine publishes workflow.execution.completed \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u25bc \u25bc \u25bc \u25bc Analytics Notification Audit Logger Billing Service Service Service Service ``` Each service maintains its own consumer group and processes events independently. Failures in one consumer don't affect others. ### Request-Reply Pattern Synchronous communication over async messaging (used sparingly): ``` API Gateway publishes request \u2192 Kafka (reply-to: temp-queue-123) \u2502 \u25bc Service processes request \u2502 \u25bc Service publishes response \u2192 temp-queue-123 \u2502 \u25bc API Gateway receives response ``` Timeout: 5 seconds, fallback to direct HTTP call if no response. ### Saga Pattern Distributed transaction management for multi-service workflows: ``` Step 1: Create Order \u2192 SUCCESS \u2192 Step 2: Reserve Inventory \u2502 FAILURE \u2502 \u25bc Compensating Transaction \u2502 \u25bc Cancel Order (Rollback) ``` Orchestrated by Workflow Engine with compensation logic defined in workflow DSL. ### Dead Letter Queue (DLQ) Failed messages after all retry attempts are routed to DLQ: ``` Message processing fails (3 retries with exponential backoff) \u2502 \u25bc Publish to dead-letter-queue topic \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u25bc \u25bc Alert to PagerDuty Store in PostgreSQL for analysis \u2502 \u2502 \u25bc \u25bc Manual investigation Automated pattern detection ``` DLQ Processing SLA: < 4 hours for critical events, < 24 hours for non-critical. --- ## Caching Strategy ### Cache-Aside Pattern (Lazy Loading) Primary caching pattern used across all services: ``` 1. Application checks cache (Redis GET) 2. If HIT \u2192 Return cached data 3. If MISS: a. Query database (PostgreSQL) b. Store result in cache with TTL c. Return data to application ``` **Implementation Example** (Workflow Definition): ```javascript async function getWorkflowDefinition(workflowId) { const cacheKey = `workflow:def:${workflowId}`; // Step 1: Check cache let workflow = await redis.get(cacheKey); if (workflow) { metrics.increment('cache.hit.workflow_def'); return JSON.parse(workflow); } // Step 2: Cache miss - query database metrics.increment('cache.miss.workflow_def'); workflow = await db.query( 'SELECT * FROM workflows WHERE id = $1', [workflowId] ); // Step 3: Store in cache (1 hour TTL) await redis.setex(cacheKey, 3600, JSON.stringify(workflow)); return workflow; } ``` ### Write-Through Pattern Used for critical data where cache consistency is paramount: ``` 1. Application writes to database 2. Database transaction commits 3. Application updates cache 4. Return success to client ``` Applied to: User profiles, authentication sessions, system configuration.",
              "score": null
            },
            {
              "doc_id": "user_guide",
              "chunk_id": "user_guide_fix_6",
              "content": "Filter by error type, date range, or execution ID 3. Click an execution to view full details 4. Click **\"Retry\"** to reprocess with the same input data ### Error Notifications Get notified when workflows fail: ```yaml workflow: notifications: on_failure: - type: email to: \"ops-team@company.com\" - type: slack channel: \"#alerts\" message: \"Workflow {{workflow.name}} failed: {{error.message}}\" on_success_after_retry: - type: slack channel: \"#monitoring\" message: \"Workflow recovered after {{error.attempts}} attempts\" ``` ## Workflow Limits CloudFlow enforces the following limits to ensure platform stability and performance: ### Steps Per Workflow - **Maximum**: 50 steps per workflow - **Recommendation**: Keep workflows focused and modular. If you need more steps, consider splitting into multiple workflows connected via webhooks. ### Execution Timeout - **Default**: 3600 seconds (60 minutes) - **Behavior**: Workflows exceeding this timeout are automatically terminated - **Custom Timeouts**: Enterprise plans can request custom timeout limits **Setting Step-Level Timeouts:** ```yaml - id: long_running_task action: http_request config: url: \"https://api.example.com/process\" timeout: 300 # 5 minutes for this specific step ``` ### Execution Limits - **Maximum**: 1000 executions per day (per workflow) - **Rate Limiting**: 100 concurrent executions per workflow - **Burst Limit**: 10 executions per second **What happens when limits are reached:** - New executions are queued automatically - Webhook triggers return HTTP 429 (Too Many Requests) - Scheduled executions are skipped (logged in audit trail) - Email notifications sent to workflow owner **Monitoring Usage:** View real-time metrics in your workflow dashboard: - Executions today: 847 / 1000 - Average duration: 12.3s - Success rate: 98.2% - Current queue depth: 3 ### Data Limits - **Maximum request/response size**: 10MB per action - **Maximum execution payload**: 50MB total - **Variable value size**: 1MB per variable ### Enterprise Plan Limits Enterprise customers can request increased limits: - Up to 100 steps per workflow - Up to 10,000 executions per day - Up to 7200 second timeout (2 hours) - Priority execution queue - Dedicated capacity allocation Contact sales@cloudflow.io for Enterprise pricing and custom limits. ## Best Practices Follow these best practices to build reliable, maintainable workflows: ### 1. Use Descriptive Names **Good:** - Workflow: \"Sync Customer Data from Salesforce to Database\" - Step: \"validate_customer_email\" **Bad:** - Workflow: \"Workflow 1\" - Step: \"step3\" ### 2.",
              "score": null
            },
            {
              "doc_id": "user_guide",
              "chunk_id": "user_guide_fix_4",
              "content": "### Cron Syntax Use standard cron expressions to define schedules: ``` * * * * * \u252c \u252c \u252c \u252c \u252c \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2514\u2500\u2500\u2500 Day of Week (0-6, Sunday=0) \u2502 \u2502 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Month (1-12) \u2502 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Day of Month (1-31) \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Hour (0-23) \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Minute (0-59) ``` **Common Cron Patterns:** | Pattern | Description | |---------|-------------| | `*/5 * * * *` | Every 5 minutes | | `0 * * * *` | Every hour at minute 0 | | `0 9 * * *` | Daily at 9:00 AM | | `0 9 * * 1` | Every Monday at 9:00 AM | | `0 0 1 * *` | First day of every month at midnight | | `0 0 * * 0` | Every Sunday at midnight | | `0 9-17 * * 1-5` | Every hour from 9 AM to 5 PM, Monday-Friday | | `*/15 9-17 * * 1-5` | Every 15 minutes during business hours | **Important:** The minimum scheduling interval is **1 minute**. Expressions that evaluate to more frequent executions will be rejected. ### Timezone Handling All scheduled workflows run in **UTC by default**. To account for your local timezone: **Option 1: Convert to UTC** If you want a workflow to run at 9:00 AM EST (UTC-5), schedule it for 14:00 UTC: ``` 0 14 * * * # 9:00 AM EST = 14:00 UTC ``` **Option 2: Use Timezone Configuration** Specify a timezone in your workflow configuration: ```yaml schedule: cron: \"0 9 * * *\" timezone: \"America/New_York\" # IANA timezone identifier ``` **Supported Timezones:** CloudFlow supports all IANA timezone identifiers, including: - `America/New_York` (Eastern Time) - `America/Chicago` (Central Time) - `America/Los_Angeles` (Pacific Time) - `Europe/London` (GMT/BST) - `Asia/Tokyo` (Japan Standard Time) - `Australia/Sydney` (Australian Eastern Time) **Daylight Saving Time:** When using timezone configuration, CloudFlow automatically handles DST transitions. A workflow scheduled for 9:00 AM will run at 9:00 AM local time regardless of DST changes. ### Schedule Management **Creating a Schedule:** 1. Open your workflow in the editor 2. Click the **\"Trigger\"** section 3. Select **\"Schedule\"** as the trigger type 4. Enter your cron expression or use the visual schedule builder 5. Select your timezone 6.",
              "score": null
            },
            {
              "doc_id": "user_guide",
              "chunk_id": "user_guide_fix_5",
              "content": "Save and activate **Testing Schedules:** Use the built-in schedule calculator to preview upcoming executions: ``` Next 5 executions: 1. 2026-01-24 14:00:00 UTC 2. 2026-01-25 14:00:00 UTC 3. 2026-01-26 14:00:00 UTC 4. 2026-01-27 14:00:00 UTC 5. 2026-01-28 14:00:00 UTC ``` ## Error Handling Robust error handling ensures your workflows are resilient and reliable. ### Retry Policies Configure automatic retries for failed actions: ```yaml - id: api_call action: http_request config: url: \"https://api.example.com/data\" retry: max_attempts: 3 backoff_type: \"exponential\" # or \"fixed\", \"linear\" initial_interval: 1000 # milliseconds max_interval: 30000 multiplier: 2.0 retry_on: - timeout - network_error - status: [500, 502, 503, 504] ``` **Backoff Strategies:** - **Fixed**: Wait the same amount of time between retries - Attempt 1: Wait 1s - Attempt 2: Wait 1s - Attempt 3: Wait 1s - **Linear**: Increase wait time by a fixed amount - Attempt 1: Wait 1s - Attempt 2: Wait 2s - Attempt 3: Wait 3s - **Exponential**: Double the wait time with each retry (recommended) - Attempt 1: Wait 1s - Attempt 2: Wait 2s - Attempt 3: Wait 4s **Retry Conditions:** Control which errors trigger retries: - `timeout`: Request timeout - `network_error`: Connection failures - `status`: Specific HTTP status codes - `error_code`: Application-specific error codes ### Fallback Actions Execute alternative actions when the primary action fails: ```yaml - id: primary_payment action: http_request config: url: \"https://primary-payment-gateway.com/charge\" method: POST body: amount: \"{{amount}}\" on_error: - id: fallback_payment action: http_request config: url: \"https://backup-payment-gateway.com/charge\" method: POST body: amount: \"{{amount}}\" - id: notify_admin action: email config: to: \"admin@company.com\" subject: \"Payment Gateway Failure\" body: \"Primary gateway failed, switched to backup\" ``` **Error Object:** Access error details in fallback actions: ``` {{error.message}} # Error message {{error.code}} # Error code {{error.step_id}} # Failed step ID {{error.timestamp}} # When the error occurred {{error.attempts}} # Number of retry attempts ``` ### Dead Letter Queue When all retries and fallbacks fail, CloudFlow can route failed executions to a Dead Letter Queue (DLQ) for manual review: **Enable DLQ:** ```yaml workflow: error_handling: dead_letter_queue: enabled: true retain_days: 30 ``` **DLQ Features:** - View failed executions in the dashboard - Inspect complete execution context and error details - Retry individual executions after fixing issues - Export failed executions for analysis - Set up alerts for DLQ threshold breaches **Accessing the DLQ:** 1. Navigate to **Workflows** > **[Your Workflow]** > **Dead Letter Queue** 2.",
              "score": null
            }
          ],
          "latency_ms": 12.985470002604416,
          "query_id": "realistic_013",
          "dimension": "negation",
          "query": "Why are changes taking an hour to appear?"
        },
        {
          "key_facts": [
            "Prometheus",
            "Grafana",
            "Jaeger for distributed tracing"
          ],
          "found_facts": [
            "Prometheus",
            "Grafana"
          ],
          "missed_facts": [
            "Jaeger for distributed tracing"
          ],
          "coverage": 0.6666666666666666,
          "expected_docs": [
            "architecture_overview",
            "deployment_guide"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_4",
              "content": "--set prometheus.prometheusSpec.retention=30d \\ --set prometheus.prometheusSpec.storageSpec.volumeClaimTemplate.spec.resources.requests.storage=100Gi ``` CloudFlow exposes Prometheus metrics on port 9090 at the `/metrics` endpoint. The metrics include: - **HTTP Metrics**: Request rate, latency, error rate - **Database Metrics**: Connection pool status, query duration - **Worker Metrics**: Job queue size, processing time, failure rate - **System Metrics**: CPU usage, memory usage, heap statistics Example metrics exposed: ``` # HELP cloudflow_http_requests_total Total number of HTTP requests # TYPE cloudflow_http_requests_total counter cloudflow_http_requests_total{method=\"GET\",route=\"/api/workflows\",status=\"200\"} 15420 # HELP cloudflow_http_request_duration_seconds HTTP request duration in seconds # TYPE cloudflow_http_request_duration_seconds histogram cloudflow_http_request_duration_seconds_bucket{method=\"POST\",route=\"/api/workflows\",le=\"0.1\"} 8234 # HELP cloudflow_worker_jobs_processed_total Total number of jobs processed # TYPE cloudflow_worker_jobs_processed_total counter cloudflow_worker_jobs_processed_total{status=\"success\"} 45230 # HELP cloudflow_database_connections Current database connections # TYPE cloudflow_database_connections gauge cloudflow_database_connections{state=\"active\"} 23 ``` ### Grafana Dashboards Access Grafana to view CloudFlow dashboards: ```bash kubectl port-forward -n monitoring svc/prometheus-grafana 3000:80 ``` Import the CloudFlow dashboard (ID: 15847) or use the provided JSON template. Key dashboard panels include: 1. **API Performance**: Request rate, P95/P99 latency, error rate 2. **Resource Usage**: CPU, memory, disk I/O per pod 3. **Database Health**: Connection pool utilization, query performance 4. **Worker Status**: Queue depth, processing rate, job success/failure ratio 5. **System Overview**: Pod status, replica count, autoscaling events ### Logging with CloudWatch CloudFlow logs are automatically shipped to CloudWatch Logs.",
              "score": null
            },
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_0",
              "content": "# CloudFlow Platform - Deployment and Operations Guide **Version:** 2.4.0 **Last Updated:** January 2026 **Target Environment:** Production (AWS EKS) ## Table of Contents 1. [Overview](#overview) 2. [Prerequisites](#prerequisites) 3. [Infrastructure Setup](#infrastructure-setup) 4. [Kubernetes Deployment](#kubernetes-deployment) 5. [Database Configuration](#database-configuration) 6. [Monitoring and Observability](#monitoring-and-observability) 7. [Backup and Disaster Recovery](#backup-and-disaster-recovery) 8. [Scaling and Performance](#scaling-and-performance) 9. [Security and Compliance](#security-and-compliance) 10. [Troubleshooting](#troubleshooting) --- ## Overview CloudFlow is a cloud-native workflow orchestration platform designed for high-availability production environments. This guide provides comprehensive instructions for deploying and operating CloudFlow on Amazon EKS (Elastic Kubernetes Service). ### Architecture Summary CloudFlow consists of the following components: - **API Server**: REST API for workflow management (Node.js/Express) - **Worker Service**: Background job processor (Node.js) - **Scheduler**: Cron-based task scheduler (Node.js) - **PostgreSQL**: Primary data store (version 14) - **Redis**: Cache and message queue (version 7.0) - **PgBouncer**: Database connection pooler ### Deployment Model - **Namespace**: `cloudflow-prod` - **Cluster**: AWS EKS 1.28 - **Region**: us-east-1 (primary), us-west-2 (disaster recovery) - **High Availability**: Multi-AZ deployment across 3 availability zones --- ## Prerequisites Before beginning the deployment process, ensure you have the following: ### Required Tools - `kubectl` (v1.28 or later) - `helm` (v3.12 or later) - `aws-cli` (v2.13 or later) - `eksctl` (v0.165 or later) - `terraform` (v1.6 or later) - for infrastructure provisioning ### Access Requirements - AWS account with appropriate IAM permissions - EKS cluster admin access - Container registry access (ECR) - Domain name and SSL certificates - Secrets management access (AWS Secrets Manager or Vault) ### Network Requirements - VPC with at least 3 public and 3 private subnets - NAT Gateway configured - Security groups allowing: - Ingress: HTTPS (443), HTTP (80) - Internal: PostgreSQL (5432), Redis (6379) - Monitoring: Prometheus (9090), Grafana (3000) --- ## Infrastructure Setup ### EKS Cluster Creation Use the following `eksctl` configuration to create the EKS cluster: ```yaml # cluster-config.yaml apiVersion: eksctl.io/v1alpha5 kind: ClusterConfig metadata: name: cloudflow-production region: us-east-1 version: \"1.28\" availabilityZones: - us-east-1a - us-east-1b - us-east-1c vpc: cidr: 10.0.0.0/16 nat: gateway: HighlyAvailable managedNodeGroups: - name: cloudflow-workers instanceType: m5.xlarge minSize: 3 maxSize: 10 desiredCapacity: 5 volumeSize: 100 privateNetworking: true labels: role: worker environment: production tags: Environment: production Application: cloudflow iam: withAddonPolicies: ebs: true efs: true albIngress: true cloudWatch: true cloudWatch: clusterLogging: enableTypes: - api - audit - authenticator - controllerManager - scheduler ``` Create the cluster: ```bash eksctl",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_1",
              "content": "**Diagnosis:** ```bash # Check system time timedatectl status # Compare with NTP server ntpdate -q pool.ntp.org # Check JWT issued time vs current time jwt_iat=$(echo $CF_ACCESS_TOKEN | cut -d'.' -f2 | base64 -d | jq '.iat') current_time=$(date +%s) skew=$((current_time - jwt_iat)) echo \"Clock skew: $skew seconds\" ``` **Resolution:** ```bash # Sync with NTP server sudo ntpdate -s pool.ntp.org # Enable automatic time sync sudo timedatectl set-ntp true # Restart CloudFlow client cloudflow restart ``` **4. Insufficient Permissions** Error: `User does not have required permissions for this operation` **Check user roles:** ```bash cloudflow auth whoami --verbose # Expected output: # User: john.doe@company.com # Roles: developer, workflow-admin # Scopes: workflow:*, data:read, metrics:read ``` **Request permission elevation:** ```bash # Submit access request cloudflow auth request-access \\ --resource \"workflow:production:deploy\" \\ --justification \"Deploy critical hotfix for TICKET-1234\" # Check pending approvals cloudflow auth list-requests --status pending ``` ### 403 Forbidden Errors #### RBAC Policy Violations CloudFlow uses role-based access control (RBAC) with the following hierarchy: - `viewer` - Read-only access - `developer` - Create and modify workflows (non-production) - `operator` - Execute workflows, view logs - `admin` - Full access to workspace - `platform-admin` - Cross-workspace administration **Verify resource permissions:** ```bash # Check effective permissions cloudflow rbac check \\ --user john.doe@company.com \\ --resource workflow:prod-pipeline \\ --action execute # List all policies affecting user cloudflow rbac policies --user john.doe@company.com --verbose ``` --- ## Performance Problems ### Slow Query Performance #### Symptoms - API response times > 5000ms - Database query latency warnings in logs - CloudFlow UI becomes unresponsive - Timeout errors: `Request exceeded maximum duration of 30000ms` #### Diagnosis Steps **1. Identify Slow Queries** ```bash # Query CloudFlow metrics cloudflow metrics query --metric api_request_duration_ms \\ --filter \"p95 > 5000\" \\ --last 1h # Check database slow query log kubectl logs -n cloudflow deploy/cloudflow-db-primary | \\ grep \"slow query\" | \\ tail -n 50 # Analyze query patterns cloudflow db analyze-queries --min-duration 5000 --limit 20 ``` **2.",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_8",
              "content": "Leverage caching:** ```bash # Enable client-side caching export CLOUDFLOW_CACHE_ENABLED=true export CLOUDFLOW_CACHE_TTL=300 # Cache workflow metadata cloudflow workflows list --use-cache --cache-ttl 600 ``` --- ## Log Analysis & Debugging ### Accessing CloudFlow Logs #### Kubernetes Deployments ```bash # List all CloudFlow pods kubectl get pods -n cloudflow # Tail logs from API server kubectl logs -f -n cloudflow deployment/cloudflow-api --tail=100 # Get logs from specific pod kubectl logs -n cloudflow cloudflow-api-7d4f6b8c9d-x7k2m # Get logs from previous crashed pod kubectl logs -n cloudflow cloudflow-api-7d4f6b8c9d-x7k2m --previous # Get logs from all pods in deployment kubectl logs -n cloudflow deployment/cloudflow-api --all-containers=true # Stream logs from multiple pods kubectl logs -n cloudflow -l app=cloudflow-api -f --max-log-requests=10 ``` #### Log Levels CloudFlow supports the following log levels: - `TRACE` - Very detailed debugging information - `DEBUG` - Detailed debugging information - `INFO` - Informational messages (default) - `WARN` - Warning messages - `ERROR` - Error messages - `FATAL` - Fatal errors causing shutdown **Changing log levels:** ```bash # Set global log level cloudflow config set logging.level DEBUG # Set log level for specific component cloudflow config set logging.components.database DEBUG cloudflow config set logging.components.auth INFO cloudflow config set logging.components.workflows TRACE # Temporary log level increase (resets after 1 hour) cloudflow debug set-log-level DEBUG --duration 3600 # View current log configuration cloudflow config get logging --format json ``` ### Grep Patterns for Common Issues #### Authentication Failures ```bash # Find all authentication errors kubectl logs -n cloudflow deployment/cloudflow-api | \\ grep -i \"authentication\\|401\\|unauthorized\" | \\ tail -n 50 # Find JWT validation failures kubectl logs -n cloudflow deployment/cloudflow-api | \\ grep -E \"JWT|token.*invalid|signature.*failed\" # Find clock skew issues kubectl logs -n cloudflow deployment/cloudflow-api | \\ grep -i \"clock skew\\|time.*sync\\|nbf\\|exp\" ``` #### Database Errors ```bash # Find database connection errors kubectl logs -n cloudflow deployment/cloudflow-api | \\ grep -E \"connection.*pool|could not connect|database.*timeout\" # Find slow queries kubectl logs -n cloudflow deployment/cloudflow-api | \\ grep \"slow query\" | \\ awk '{print $NF}' | \\ sort -n | \\ tail -n 20 # Find deadlock errors kubectl logs -n cloudflow deployment/cloudflow-api | \\ grep -i \"deadlock detected\" ``` #### Workflow Execution Errors ```bash # Find workflow timeout errors kubectl logs -n cloudflow deployment/cloudflow-workflow-engine | \\ grep -E \"timeout|exceeded.*3600\" # Find workflow retry attempts kubectl logs -n cloudflow deployment/cloudflow-workflow-engine | \\ grep -E \"retry attempt [0-9]|retrying",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_0",
              "content": "# CloudFlow Platform Troubleshooting Guide **Version:** 3.2.1 **Last Updated:** January 2026 **Audience:** Platform Engineers, DevOps, Support Teams ## Table of Contents 1. [Overview](#overview) 2. [Authentication & Authorization Issues](#authentication--authorization-issues) 3. [Performance Problems](#performance-problems) 4. [Database Connection Issues](#database-connection-issues) 5. [Workflow Execution Failures](#workflow-execution-failures) 6. [Rate Limiting & Throttling](#rate-limiting--throttling) 7. [Log Analysis & Debugging](#log-analysis--debugging) 8. [Escalation Procedures](#escalation-procedures) --- ## Overview This guide provides comprehensive troubleshooting steps for common CloudFlow platform issues encountered in production environments. Each section includes error symptoms, root cause analysis, resolution steps, and preventive measures. ### Quick Diagnostic Checklist Before diving into specific issues, perform these initial checks: - Verify service health: `cloudflow status --all` - Check API connectivity: `curl -I https://api.cloudflow.io/health` - Review recent deployments: `kubectl get deployments -n cloudflow --sort-by=.metadata.creationTimestamp` - Inspect platform metrics: `cloudflow metrics --last 1h` --- ## Authentication & Authorization Issues ### 401 Unauthorized Errors #### Symptoms - API requests return `401 Unauthorized` - Error message: `Authentication credentials were not provided or are invalid` - Frontend displays \"Session expired\" message #### Common Causes **1. Token Expiration** CloudFlow access tokens expire after 3600 seconds (1 hour) by default. Refresh tokens are valid for 30 days. **Verification:** ```bash # Decode JWT to check expiration echo $CF_ACCESS_TOKEN | cut -d'.' -f2 | base64 -d | jq '.exp' # Compare with current time date +%s ``` **Resolution:** ```bash # Refresh the access token curl -X POST https://api.cloudflow.io/auth/refresh \\ -H \"Content-Type: application/json\" \\ -d '{\"refresh_token\": \"'$CF_REFRESH_TOKEN'\"}' # Update environment variable export CF_ACCESS_TOKEN=\"<new_token>\" ``` **2. Invalid JWT Signature** Error: `JWT signature verification failed` **Causes:** - Token was modified or corrupted - Using wrong signing key - Token generated with different secret **Resolution:** ```bash # Validate token structure cloudflow auth validate --token $CF_ACCESS_TOKEN # Generate new token cloudflow auth login --username <user> --password <pass> # For service accounts cloudflow auth service-account --create --name <sa-name> --scopes \"workflow:read,workflow:write\" ``` **3. Clock Skew Issues** JWT validation fails when system clocks are out of sync (tolerance: \u00b1300 seconds).",
              "score": null
            }
          ],
          "latency_ms": 12.815173002309166,
          "query_id": "realistic_014",
          "dimension": "original",
          "query": "What monitoring tools does CloudFlow use?"
        },
        {
          "key_facts": [
            "Prometheus",
            "Grafana",
            "Jaeger for distributed tracing"
          ],
          "found_facts": [
            "Prometheus",
            "Grafana",
            "Jaeger for distributed tracing"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "architecture_overview",
            "deployment_guide"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_0",
              "content": "# CloudFlow Platform - System Architecture Overview **Document Version:** 2.3.1 **Last Updated:** January 15, 2026 **Owner:** Platform Architecture Team **Status:** Production ## Executive Summary CloudFlow is a distributed, cloud-native workflow automation platform designed to orchestrate complex business processes at scale. The platform processes over 2.5 million workflow executions daily with an average P99 latency of 180ms for API operations and 4.2 seconds for workflow execution. This document provides a comprehensive overview of the system architecture, including microservices design, data flow patterns, infrastructure decisions, and operational considerations. ## Table of Contents 1. [High-Level Architecture](#high-level-architecture) 2. [Microservices Breakdown](#microservices-breakdown) 3. [Data Flow Architecture](#data-flow-architecture) 4. [Database Architecture](#database-architecture) 5. [Message Queue Patterns](#message-queue-patterns) 6. [Caching Strategy](#caching-strategy) 7. [Security Architecture](#security-architecture) 8. [Performance Characteristics](#performance-characteristics) 9. [Disaster Recovery](#disaster-recovery) --- ## High-Level Architecture CloudFlow follows a microservices architecture pattern deployed across multiple availability zones in AWS. The platform is designed with the following core principles: - **Scalability**: Horizontal scaling for all services with auto-scaling groups - **Resilience**: Circuit breakers, bulkheads, and graceful degradation - **Observability**: Distributed tracing, centralized logging, and comprehensive metrics - **Security**: Zero-trust network architecture with mTLS encryption ### Architecture Diagram (Conceptual) ``` \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 Load Balancer (ALB) \u2502 \u2502 (TLS Termination - 443) \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u25bc \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 API Gateway Layer \u2502 \u2502 (Rate Limiting, Auth, Request Routing) \u2502 \u2514\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u2502 \u2502 \u2502 \u2502 \u25bc \u25bc \u25bc \u25bc \u25bc \u250c\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502Auth\u2502 \u2502Workflow\u2502 \u2502Scheduler\u2502 \u2502Notification\u2502 \u2502User Service\u2502 \u2502Svc \u2502 \u2502 Engine \u2502 \u2502 Service \u2502 \u2502 Service \u2502 \u2502 \u2502 \u2514\u2500\u252c\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u2502 \u2502 \u2502 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 \u2502 \u2502 \u25bc \u25bc \u25bc \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502PostgreSQL\u2502 \u2502 Redis \u2502 \u2502 Kafka \u2502 \u2502 Cluster \u2502 \u2502 Cluster \u2502 \u2502 Cluster \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 ``` ### Technology Stack - **Runtime**: Node.js 20.x (API Gateway, Workflow Engine), Go 1.21 (Auth Service, Scheduler) - **Container Orchestration**: Kubernetes 1.28 (EKS) - **Service Mesh**: Istio 1.20 for service-to-service communication - **Databases**: PostgreSQL 15.4, Redis 7.2 - **Message Broker**: Apache Kafka 3.6 - **Monitoring**: Prometheus, Grafana, Jaeger for distributed tracing - **Secrets Management**: HashiCorp Vault 1.15 --- ## Microservices Breakdown ### API Gateway **Purpose**: Single entry point for all client requests, providing authentication, rate limiting, request routing, and protocol translation.",
              "score": null
            },
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_0",
              "content": "# CloudFlow Platform - Deployment and Operations Guide **Version:** 2.4.0 **Last Updated:** January 2026 **Target Environment:** Production (AWS EKS) ## Table of Contents 1. [Overview](#overview) 2. [Prerequisites](#prerequisites) 3. [Infrastructure Setup](#infrastructure-setup) 4. [Kubernetes Deployment](#kubernetes-deployment) 5. [Database Configuration](#database-configuration) 6. [Monitoring and Observability](#monitoring-and-observability) 7. [Backup and Disaster Recovery](#backup-and-disaster-recovery) 8. [Scaling and Performance](#scaling-and-performance) 9. [Security and Compliance](#security-and-compliance) 10. [Troubleshooting](#troubleshooting) --- ## Overview CloudFlow is a cloud-native workflow orchestration platform designed for high-availability production environments. This guide provides comprehensive instructions for deploying and operating CloudFlow on Amazon EKS (Elastic Kubernetes Service). ### Architecture Summary CloudFlow consists of the following components: - **API Server**: REST API for workflow management (Node.js/Express) - **Worker Service**: Background job processor (Node.js) - **Scheduler**: Cron-based task scheduler (Node.js) - **PostgreSQL**: Primary data store (version 14) - **Redis**: Cache and message queue (version 7.0) - **PgBouncer**: Database connection pooler ### Deployment Model - **Namespace**: `cloudflow-prod` - **Cluster**: AWS EKS 1.28 - **Region**: us-east-1 (primary), us-west-2 (disaster recovery) - **High Availability**: Multi-AZ deployment across 3 availability zones --- ## Prerequisites Before beginning the deployment process, ensure you have the following: ### Required Tools - `kubectl` (v1.28 or later) - `helm` (v3.12 or later) - `aws-cli` (v2.13 or later) - `eksctl` (v0.165 or later) - `terraform` (v1.6 or later) - for infrastructure provisioning ### Access Requirements - AWS account with appropriate IAM permissions - EKS cluster admin access - Container registry access (ECR) - Domain name and SSL certificates - Secrets management access (AWS Secrets Manager or Vault) ### Network Requirements - VPC with at least 3 public and 3 private subnets - NAT Gateway configured - Security groups allowing: - Ingress: HTTPS (443), HTTP (80) - Internal: PostgreSQL (5432), Redis (6379) - Monitoring: Prometheus (9090), Grafana (3000) --- ## Infrastructure Setup ### EKS Cluster Creation Use the following `eksctl` configuration to create the EKS cluster: ```yaml # cluster-config.yaml apiVersion: eksctl.io/v1alpha5 kind: ClusterConfig metadata: name: cloudflow-production region: us-east-1 version: \"1.28\" availabilityZones: - us-east-1a - us-east-1b - us-east-1c vpc: cidr: 10.0.0.0/16 nat: gateway: HighlyAvailable managedNodeGroups: - name: cloudflow-workers instanceType: m5.xlarge minSize: 3 maxSize: 10 desiredCapacity: 5 volumeSize: 100 privateNetworking: true labels: role: worker environment: production tags: Environment: production Application: cloudflow iam: withAddonPolicies: ebs: true efs: true albIngress: true cloudWatch: true cloudWatch: clusterLogging: enableTypes: - api - audit - authenticator - controllerManager - scheduler ``` Create the cluster: ```bash eksctl",
              "score": null
            },
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_8",
              "content": "UTC - **Secondary**: Wednesday 10:00-12:00 UTC All deployments should target these windows to minimize user impact. --- **Document Version**: 2.4.0 **Maintained by**: Platform Engineering Team **Last Review**: January 24, 2026",
              "score": null
            },
            {
              "doc_id": "user_guide",
              "chunk_id": "user_guide_fix_0",
              "content": "# CloudFlow User Guide Welcome to CloudFlow, the modern workflow automation platform that helps you connect your apps, automate repetitive tasks, and build powerful integrations without writing code. ## Table of Contents 1. [Getting Started](#getting-started) 2. [Workflow Creation](#workflow-creation) 3. [Available Actions](#available-actions) 4. [Variables and Expressions](#variables-and-expressions) 5. [Scheduling](#scheduling) 6. [Error Handling](#error-handling) 7. [Workflow Limits](#workflow-limits) 8. [Best Practices](#best-practices) 9. [Common Workflow Patterns](#common-workflow-patterns) ## Getting Started ### Account Setup Creating your CloudFlow account takes just a few minutes: 1. Navigate to https://app.cloudflow.io/signup 2. Enter your email address and create a strong password 3. Verify your email address by clicking the link sent to your inbox 4. Complete your profile by providing your organization name and use case 5. Choose your plan (Free, Professional, or Enterprise) Once your account is created, you'll be redirected to your CloudFlow dashboard where you can start building workflows immediately. ### Your First Workflow Let's create a simple workflow that sends you a Slack notification when a new file is uploaded to your Google Drive: 1. Click the **\"Create Workflow\"** button in the top-right corner 2. Give your workflow a name: \"Google Drive to Slack Notifier\" 3. Select **\"Google Drive - New File\"** as your trigger 4. Authenticate your Google Drive account when prompted 5. Choose the folder you want to monitor 6. Click **\"Add Action\"** and select **\"Slack - Send Message\"** 7. Authenticate your Slack account 8. Select the channel where you want to receive notifications 9. Customize your message using variables: `New file uploaded: {{trigger.file.name}}` 10. Click **\"Save & Activate\"** to enable your workflow Congratulations! Your first workflow is now live. Every time a file is added to the specified Google Drive folder, you'll receive a Slack notification. ## Workflow Creation CloudFlow offers multiple ways to create and manage workflows, giving you the flexibility to work in the way that suits you best. ### Visual Editor The Visual Editor is CloudFlow's drag-and-drop interface for building workflows without code. It's perfect for users who prefer a graphical approach: **Key Features:** - Drag-and-drop action blocks from the sidebar - Visual connections between steps show your workflow logic - Inline configuration for each action - Real-time validation and error checking - Test mode to run your workflow with sample data **Using the Visual Editor:** 1.",
              "score": null
            },
            {
              "doc_id": "user_guide",
              "chunk_id": "user_guide_fix_5",
              "content": "Save and activate **Testing Schedules:** Use the built-in schedule calculator to preview upcoming executions: ``` Next 5 executions: 1. 2026-01-24 14:00:00 UTC 2. 2026-01-25 14:00:00 UTC 3. 2026-01-26 14:00:00 UTC 4. 2026-01-27 14:00:00 UTC 5. 2026-01-28 14:00:00 UTC ``` ## Error Handling Robust error handling ensures your workflows are resilient and reliable. ### Retry Policies Configure automatic retries for failed actions: ```yaml - id: api_call action: http_request config: url: \"https://api.example.com/data\" retry: max_attempts: 3 backoff_type: \"exponential\" # or \"fixed\", \"linear\" initial_interval: 1000 # milliseconds max_interval: 30000 multiplier: 2.0 retry_on: - timeout - network_error - status: [500, 502, 503, 504] ``` **Backoff Strategies:** - **Fixed**: Wait the same amount of time between retries - Attempt 1: Wait 1s - Attempt 2: Wait 1s - Attempt 3: Wait 1s - **Linear**: Increase wait time by a fixed amount - Attempt 1: Wait 1s - Attempt 2: Wait 2s - Attempt 3: Wait 3s - **Exponential**: Double the wait time with each retry (recommended) - Attempt 1: Wait 1s - Attempt 2: Wait 2s - Attempt 3: Wait 4s **Retry Conditions:** Control which errors trigger retries: - `timeout`: Request timeout - `network_error`: Connection failures - `status`: Specific HTTP status codes - `error_code`: Application-specific error codes ### Fallback Actions Execute alternative actions when the primary action fails: ```yaml - id: primary_payment action: http_request config: url: \"https://primary-payment-gateway.com/charge\" method: POST body: amount: \"{{amount}}\" on_error: - id: fallback_payment action: http_request config: url: \"https://backup-payment-gateway.com/charge\" method: POST body: amount: \"{{amount}}\" - id: notify_admin action: email config: to: \"admin@company.com\" subject: \"Payment Gateway Failure\" body: \"Primary gateway failed, switched to backup\" ``` **Error Object:** Access error details in fallback actions: ``` {{error.message}} # Error message {{error.code}} # Error code {{error.step_id}} # Failed step ID {{error.timestamp}} # When the error occurred {{error.attempts}} # Number of retry attempts ``` ### Dead Letter Queue When all retries and fallbacks fail, CloudFlow can route failed executions to a Dead Letter Queue (DLQ) for manual review: **Enable DLQ:** ```yaml workflow: error_handling: dead_letter_queue: enabled: true retain_days: 30 ``` **DLQ Features:** - View failed executions in the dashboard - Inspect complete execution context and error details - Retry individual executions after fixing issues - Export failed executions for analysis - Set up alerts for DLQ threshold breaches **Accessing the DLQ:** 1. Navigate to **Workflows** > **[Your Workflow]** > **Dead Letter Queue** 2.",
              "score": null
            }
          ],
          "latency_ms": 12.750461999530671,
          "query_id": "realistic_014",
          "dimension": "synonym",
          "query": "Which observability platform is integrated?"
        },
        {
          "key_facts": [
            "Prometheus",
            "Grafana",
            "Jaeger for distributed tracing"
          ],
          "found_facts": [
            "Prometheus",
            "Grafana"
          ],
          "missed_facts": [
            "Jaeger for distributed tracing"
          ],
          "coverage": 0.6666666666666666,
          "expected_docs": [
            "architecture_overview",
            "deployment_guide"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_4",
              "content": "--set prometheus.prometheusSpec.retention=30d \\ --set prometheus.prometheusSpec.storageSpec.volumeClaimTemplate.spec.resources.requests.storage=100Gi ``` CloudFlow exposes Prometheus metrics on port 9090 at the `/metrics` endpoint. The metrics include: - **HTTP Metrics**: Request rate, latency, error rate - **Database Metrics**: Connection pool status, query duration - **Worker Metrics**: Job queue size, processing time, failure rate - **System Metrics**: CPU usage, memory usage, heap statistics Example metrics exposed: ``` # HELP cloudflow_http_requests_total Total number of HTTP requests # TYPE cloudflow_http_requests_total counter cloudflow_http_requests_total{method=\"GET\",route=\"/api/workflows\",status=\"200\"} 15420 # HELP cloudflow_http_request_duration_seconds HTTP request duration in seconds # TYPE cloudflow_http_request_duration_seconds histogram cloudflow_http_request_duration_seconds_bucket{method=\"POST\",route=\"/api/workflows\",le=\"0.1\"} 8234 # HELP cloudflow_worker_jobs_processed_total Total number of jobs processed # TYPE cloudflow_worker_jobs_processed_total counter cloudflow_worker_jobs_processed_total{status=\"success\"} 45230 # HELP cloudflow_database_connections Current database connections # TYPE cloudflow_database_connections gauge cloudflow_database_connections{state=\"active\"} 23 ``` ### Grafana Dashboards Access Grafana to view CloudFlow dashboards: ```bash kubectl port-forward -n monitoring svc/prometheus-grafana 3000:80 ``` Import the CloudFlow dashboard (ID: 15847) or use the provided JSON template. Key dashboard panels include: 1. **API Performance**: Request rate, P95/P99 latency, error rate 2. **Resource Usage**: CPU, memory, disk I/O per pod 3. **Database Health**: Connection pool utilization, query performance 4. **Worker Status**: Queue depth, processing rate, job success/failure ratio 5. **System Overview**: Pod status, replica count, autoscaling events ### Logging with CloudWatch CloudFlow logs are automatically shipped to CloudWatch Logs.",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_2",
              "content": "Review Query Execution Plans** ```sql -- Connect to CloudFlow database cloudflow db connect --readonly -- Explain slow query EXPLAIN ANALYZE SELECT w.*, e.status, e.error_message FROM workflows w LEFT JOIN executions e ON w.id = e.workflow_id WHERE w.workspace_id = 'ws_abc123' AND e.created_at > NOW() - INTERVAL '7 days' ORDER BY e.created_at DESC; -- Check for missing indexes SELECT schemaname, tablename, indexname, indexdef FROM pg_indexes WHERE tablename IN ('workflows', 'executions', 'workflow_steps') ORDER BY tablename, indexname; ``` **3. Optimize Queries** Common optimization techniques: **Add missing indexes:** ```sql -- Index for workflow lookup by workspace CREATE INDEX CONCURRENTLY idx_workflows_workspace_created ON workflows(workspace_id, created_at DESC); -- Index for execution status queries CREATE INDEX CONCURRENTLY idx_executions_status_created ON executions(workflow_id, status, created_at DESC) WHERE status IN ('running', 'pending'); -- Composite index for common filter combinations CREATE INDEX CONCURRENTLY idx_executions_workspace_status ON executions(workspace_id, status, created_at) INCLUDE (error_message, retry_count); ``` **Use query result caching:** ```bash # Enable query cache for workspace metadata cloudflow config set cache.workspace.ttl 3600 # Configure Redis cache backend cloudflow config set cache.backend redis cloudflow config set cache.redis.host redis.cloudflow.svc.cluster.local cloudflow config set cache.redis.port 6379 ``` ### High API Latency #### Latency Breakdown Analysis ```bash # Generate latency report cloudflow metrics latency-report --endpoint \"/api/v1/workflows\" --last 24h # Sample output: # Endpoint: POST /api/v1/workflows/execute # P50: 245ms | P95: 1823ms | P99: 4521ms # Breakdown: # - Auth: 45ms (18%) # - DB Query: 156ms (64%) # - Business Logic: 32ms (13%) # - Response Serialization: 12ms (5%) ``` **Network latency issues:** ```bash # Test connectivity to CloudFlow API time curl -w \"@curl-format.txt\" -o /dev/null -s https://api.cloudflow.io/health # Create curl-format.txt: cat > curl-format.txt << EOF time_namelookup: %{time_namelookup}s\\n time_connect: %{time_connect}s\\n time_appconnect: %{time_appconnect}s\\n time_pretransfer: %{time_pretransfer}s\\n time_redirect: %{time_redirect}s\\n time_starttransfer: %{time_starttransfer}s\\n ----------\\n time_total: %{time_total}s\\n EOF # Trace route to API endpoint traceroute api.cloudflow.io # Check DNS resolution time dig api.cloudflow.io | grep \"Query time\" ``` ### Memory Leaks #### Detection ```bash # Monitor CloudFlow service memory usage kubectl top pods -n cloudflow --sort-by=memory # Get detailed memory metrics for specific pod kubectl exec -n cloudflow deploy/cloudflow-api -- \\ curl localhost:9090/metrics | grep memory # Check for OOMKilled pods kubectl get pods -n cloudflow --field-selector=status.phase=Failed | \\ grep OOMKilled # Review memory limits and requests kubectl describe deployment cloudflow-api -n cloudflow | \\ grep -A 5 \"Limits\\|Requests\" ``` #### Common Causes **1.",
              "score": null
            },
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_0",
              "content": "# CloudFlow API Reference Version 2.1.0 | Last Updated: January 2026 ## Overview The CloudFlow API is a RESTful service that enables developers to programmatically manage cloud workflows, data pipelines, and automation tasks. This documentation provides comprehensive details on authentication, endpoints, request/response formats, error handling, and best practices. **Base URL:** `https://api.cloudflow.io/v2` **API Status:** https://status.cloudflow.io ## Authentication CloudFlow supports three authentication methods to suit different use cases and security requirements. ### API Keys API keys provide simple authentication for server-to-server communication. Include your API key in the request header: ```bash curl -H \"X-API-Key: cf_live_a1b2c3d4e5f6g7h8i9j0\" \\ https://api.cloudflow.io/v2/workflows ``` **Security Notes:** - Never expose API keys in client-side code - Rotate keys every 90 days - Use separate keys for development and production environments ### OAuth 2.0 OAuth 2.0 is recommended for applications that access CloudFlow on behalf of users. We support the Authorization Code flow with PKCE. **Authorization Endpoint:** `https://auth.cloudflow.io/oauth/authorize` **Token Endpoint:** `https://auth.cloudflow.io/oauth/token` **Supported Scopes:** - `workflows:read` - Read workflow configurations - `workflows:write` - Create and modify workflows - `pipelines:read` - Read pipeline data - `pipelines:write` - Create and manage pipelines - `analytics:read` - Access analytics and metrics - `admin:full` - Full administrative access Example authorization request: ```bash curl -X POST https://auth.cloudflow.io/oauth/token \\ -H \"Content-Type: application/x-www-form-urlencoded\" \\ -d \"grant_type=authorization_code\" \\ -d \"code=AUTH_CODE_HERE\" \\ -d \"client_id=YOUR_CLIENT_ID\" \\ -d \"client_secret=YOUR_CLIENT_SECRET\" \\ -d \"redirect_uri=https://yourapp.com/callback\" ``` ### JWT Tokens For advanced use cases, CloudFlow supports JSON Web Tokens (JWT) with RS256 signing algorithm. JWTs must include the following claims: - `iss` (issuer): Your application identifier - `sub` (subject): User or service account ID - `aud` (audience): `https://api.cloudflow.io` - `exp` (expiration): Unix timestamp (max 3600 seconds from `iat`) - `iat` (issued at): Unix timestamp - `scope`: Space-separated list of requested scopes Example JWT header: ```python import jwt import time from cryptography.hazmat.primitives import serialization from cryptography.hazmat.backends import default_backend # Load your private key with open('private_key.pem', 'rb') as key_file: private_key = serialization.load_pem_private_key( key_file.read(), password=None, backend=default_backend() ) # Create JWT payload payload = { 'iss': 'your-app-id', 'sub': 'user-12345', 'aud': 'https://api.cloudflow.io', 'exp': int(time.time()) + 3600, 'iat': int(time.time()), 'scope': 'workflows:read workflows:write' } # Generate token token = jwt.encode(payload, private_key, algorithm='RS256') # Use in API request headers = {'Authorization': f'Bearer {token}'} ``` **Token Expiration:** All tokens expire after 3600 seconds (1 hour). Implement token refresh logic in your application.",
              "score": null
            },
            {
              "doc_id": "user_guide",
              "chunk_id": "user_guide_fix_0",
              "content": "# CloudFlow User Guide Welcome to CloudFlow, the modern workflow automation platform that helps you connect your apps, automate repetitive tasks, and build powerful integrations without writing code. ## Table of Contents 1. [Getting Started](#getting-started) 2. [Workflow Creation](#workflow-creation) 3. [Available Actions](#available-actions) 4. [Variables and Expressions](#variables-and-expressions) 5. [Scheduling](#scheduling) 6. [Error Handling](#error-handling) 7. [Workflow Limits](#workflow-limits) 8. [Best Practices](#best-practices) 9. [Common Workflow Patterns](#common-workflow-patterns) ## Getting Started ### Account Setup Creating your CloudFlow account takes just a few minutes: 1. Navigate to https://app.cloudflow.io/signup 2. Enter your email address and create a strong password 3. Verify your email address by clicking the link sent to your inbox 4. Complete your profile by providing your organization name and use case 5. Choose your plan (Free, Professional, or Enterprise) Once your account is created, you'll be redirected to your CloudFlow dashboard where you can start building workflows immediately. ### Your First Workflow Let's create a simple workflow that sends you a Slack notification when a new file is uploaded to your Google Drive: 1. Click the **\"Create Workflow\"** button in the top-right corner 2. Give your workflow a name: \"Google Drive to Slack Notifier\" 3. Select **\"Google Drive - New File\"** as your trigger 4. Authenticate your Google Drive account when prompted 5. Choose the folder you want to monitor 6. Click **\"Add Action\"** and select **\"Slack - Send Message\"** 7. Authenticate your Slack account 8. Select the channel where you want to receive notifications 9. Customize your message using variables: `New file uploaded: {{trigger.file.name}}` 10. Click **\"Save & Activate\"** to enable your workflow Congratulations! Your first workflow is now live. Every time a file is added to the specified Google Drive folder, you'll receive a Slack notification. ## Workflow Creation CloudFlow offers multiple ways to create and manage workflows, giving you the flexibility to work in the way that suits you best. ### Visual Editor The Visual Editor is CloudFlow's drag-and-drop interface for building workflows without code. It's perfect for users who prefer a graphical approach: **Key Features:** - Drag-and-drop action blocks from the sidebar - Visual connections between steps show your workflow logic - Inline configuration for each action - Real-time validation and error checking - Test mode to run your workflow with sample data **Using the Visual Editor:** 1.",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_1",
              "content": "**Diagnosis:** ```bash # Check system time timedatectl status # Compare with NTP server ntpdate -q pool.ntp.org # Check JWT issued time vs current time jwt_iat=$(echo $CF_ACCESS_TOKEN | cut -d'.' -f2 | base64 -d | jq '.iat') current_time=$(date +%s) skew=$((current_time - jwt_iat)) echo \"Clock skew: $skew seconds\" ``` **Resolution:** ```bash # Sync with NTP server sudo ntpdate -s pool.ntp.org # Enable automatic time sync sudo timedatectl set-ntp true # Restart CloudFlow client cloudflow restart ``` **4. Insufficient Permissions** Error: `User does not have required permissions for this operation` **Check user roles:** ```bash cloudflow auth whoami --verbose # Expected output: # User: john.doe@company.com # Roles: developer, workflow-admin # Scopes: workflow:*, data:read, metrics:read ``` **Request permission elevation:** ```bash # Submit access request cloudflow auth request-access \\ --resource \"workflow:production:deploy\" \\ --justification \"Deploy critical hotfix for TICKET-1234\" # Check pending approvals cloudflow auth list-requests --status pending ``` ### 403 Forbidden Errors #### RBAC Policy Violations CloudFlow uses role-based access control (RBAC) with the following hierarchy: - `viewer` - Read-only access - `developer` - Create and modify workflows (non-production) - `operator` - Execute workflows, view logs - `admin` - Full access to workspace - `platform-admin` - Cross-workspace administration **Verify resource permissions:** ```bash # Check effective permissions cloudflow rbac check \\ --user john.doe@company.com \\ --resource workflow:prod-pipeline \\ --action execute # List all policies affecting user cloudflow rbac policies --user john.doe@company.com --verbose ``` --- ## Performance Problems ### Slow Query Performance #### Symptoms - API response times > 5000ms - Database query latency warnings in logs - CloudFlow UI becomes unresponsive - Timeout errors: `Request exceeded maximum duration of 30000ms` #### Diagnosis Steps **1. Identify Slow Queries** ```bash # Query CloudFlow metrics cloudflow metrics query --metric api_request_duration_ms \\ --filter \"p95 > 5000\" \\ --last 1h # Check database slow query log kubectl logs -n cloudflow deploy/cloudflow-db-primary | \\ grep \"slow query\" | \\ tail -n 50 # Analyze query patterns cloudflow db analyze-queries --min-duration 5000 --limit 20 ``` **2.",
              "score": null
            }
          ],
          "latency_ms": 13.066450999758672,
          "query_id": "realistic_014",
          "dimension": "problem",
          "query": "Where can I view CloudFlow metrics and logs?"
        },
        {
          "key_facts": [
            "Prometheus",
            "Grafana",
            "Jaeger for distributed tracing"
          ],
          "found_facts": [
            "Prometheus",
            "Grafana"
          ],
          "missed_facts": [
            "Jaeger for distributed tracing"
          ],
          "coverage": 0.6666666666666666,
          "expected_docs": [
            "architecture_overview",
            "deployment_guide"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_4",
              "content": "**Technology**: Node.js with worker pool pattern **Replicas**: 8 pods (production), auto-scaling 6-16 **Resource Allocation**: 2 vCPU, 4GB RAM per pod **Key Responsibilities**: - Consume notification events from Kafka - Template rendering with Handlebars (cached in Redis) - Multi-channel delivery (Email via SendGrid, SMS via Twilio, Webhooks) - Retry logic with dead letter queue for failed deliveries - Delivery status tracking and analytics - User preference management (opt-in/opt-out) **Notification Flow**: ``` Event Source \u2192 Kafka (notifications.email topic) \u2502 \u25bc Notification Service Consumer \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u25bc \u25bc \u25bc [Email Queue] [SMS Queue] [Webhook Queue] \u2502 \u2502 \u2502 \u25bc \u25bc \u25bc [SendGrid] [Twilio] [HTTP POST] \u2502 \u2502 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u25bc Delivery Status \u2192 PostgreSQL ``` **Channel Configuration**: - Email: SendGrid with IP warming, DKIM/SPF configured - SMS: Twilio with fallback to AWS SNS - Webhook: Signed payloads (HMAC-SHA256), retry 3 times with exponential backoff - In-app: WebSocket connections via Socket.io **Template Management**: - Templates stored in PostgreSQL with versioning - Compiled templates cached in Redis (1-hour TTL) - Support for localization (i18n) with 12 languages - A/B testing capability for email campaigns **Performance Targets**: - Email delivery: < 5 seconds (P99) from event to SendGrid - SMS delivery: < 3 seconds (P99) - Throughput: 10,000 notifications per minute --- ## Data Flow Architecture ### Synchronous Request Flow Client requests follow a synchronous path through the API Gateway to backend services: ``` 1. Client \u2192 Load Balancer (TLS termination) 2. Load Balancer \u2192 API Gateway (HTTP/2) 3. API Gateway \u2192 Auth Service (JWT validation via gRPC) 4. API Gateway \u2192 Target Service (HTTP/REST or gRPC) 5. Target Service \u2192 Database/Cache (data retrieval) 6. Response propagates back through the chain ``` **Timeout Configuration**: - Client \u2192 Load Balancer: 60 seconds - Load Balancer \u2192 API Gateway: 55 seconds - API Gateway \u2192 Services: 45 seconds - Service \u2192 Database: 10 seconds ### Asynchronous Event Flow Long-running operations and inter-service communication use event-driven patterns: ``` 1. Service publishes event \u2192 Kafka topic 2. Kafka persists event (replication factor: 3) 3. Consumer groups subscribe to topics 4. Consumers process events with at-least-once delivery 5. State updates written to PostgreSQL 6.",
              "score": null
            },
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_5",
              "content": "**Supported Events:** - `workflow.started` - `workflow.completed` - `workflow.failed` - `pipeline.completed` - `pipeline.failed` **Webhook Payload Example:** ```json { \"event\": \"workflow.completed\", \"timestamp\": \"2026-01-24T10:45:00Z\", \"data\": { \"workflow_id\": \"wf_8x7k2m9p\", \"execution_id\": \"exec_9k3m7n2q\", \"status\": \"completed\", \"duration_ms\": 3420, \"records_processed\": 1247 } } ``` Configure webhooks in your account settings or via the API: ```bash curl -X POST https://api.cloudflow.io/v2/webhooks \\ -H \"X-API-Key: YOUR_API_KEY\" \\ -H \"Content-Type: application/json\" \\ -d '{ \"url\": \"https://your-app.com/cloudflow-webhook\", \"events\": [\"workflow.completed\", \"workflow.failed\"], \"secret\": \"whsec_your_webhook_secret\" }' ``` ## Support For additional help and resources: - **Documentation:** https://docs.cloudflow.io - **API Status:** https://status.cloudflow.io - **Support Email:** support@cloudflow.io - **Community Forum:** https://community.cloudflow.io Enterprise customers have access to 24/7 priority support via phone and dedicated Slack channels.",
              "score": null
            },
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_5",
              "content": "Configure log aggregation: ```bash # Install Fluent Bit for log forwarding helm install fluent-bit fluent/fluent-bit \\ --namespace logging \\ --create-namespace \\ --set cloudWatch.enabled=true \\ --set cloudWatch.region=us-east-1 \\ --set cloudWatch.logGroupName=/aws/eks/cloudflow-production/application ``` Query logs using CloudWatch Insights: ```sql fields @timestamp, @message, level, requestId, userId | filter namespace = \"cloudflow-prod\" | filter level = \"error\" | sort @timestamp desc | limit 100 ``` --- ## Backup and Disaster Recovery ### Database Backup Strategy CloudFlow implements a comprehensive backup strategy with the following retention policy: - **Daily snapshots**: Retained for 30 days - **Weekly backups**: Retained for 90 days - **Monthly backups**: Retained for 1 year #### Automated Backup with Velero Install Velero for cluster-wide backups: ```bash velero install \\ --provider aws \\ --plugins velero/velero-plugin-for-aws:v1.8.0 \\ --bucket cloudflow-velero-backups \\ --backup-location-config region=us-east-1 \\ --snapshot-location-config region=us-east-1 \\ --secret-file ./credentials-velero ``` Create a daily backup schedule: ```yaml # backup-schedule.yaml apiVersion: velero.io/v1 kind: Schedule metadata: name: cloudflow-daily-backup namespace: velero spec: schedule: \"0 2 * * *\" # 2 AM UTC daily template: includedNamespaces: - cloudflow-prod ttl: 720h0m0s # 30 days storageLocation: default snapshotVolumes: true ``` Apply the backup schedule: ```bash kubectl apply -f backup-schedule.yaml ``` #### PostgreSQL Backup Configure PostgreSQL continuous archiving with WAL-G: ```bash # Create backup cronjob kubectl apply -f - <<EOF apiVersion: batch/v1 kind: CronJob metadata: name: postgres-backup namespace: cloudflow-prod spec: schedule: \"0 1 * * *\" # 1 AM UTC daily jobTemplate: spec: template: spec: containers: - name: backup image: postgres:14 env: - name: PGPASSWORD valueFrom: secretKeyRef: name: postgres-credentials key: password command: - /bin/bash - -c - | TIMESTAMP=\\$(date +%Y%m%d_%H%M%S) pg_dump -h postgresql.cloudflow-prod.svc.cluster.local \\ -U cloudflow -d cloudflow \\ | gzip > /backups/cloudflow_\\${TIMESTAMP}.sql.gz aws s3 cp /backups/cloudflow_\\${TIMESTAMP}.sql.gz \\ s3://cloudflow-db-backups/daily/ volumeMounts: - name: backup-storage mountPath: /backups volumes: - name: backup-storage emptyDir: {} restartPolicy: OnFailure EOF ``` ### Disaster Recovery Procedure In case of catastrophic failure, follow these steps to restore CloudFlow: 1. **Restore EKS Cluster** (if necessary): ```bash eksctl create cluster -f cluster-config.yaml ``` 2. **Restore Velero Backup**: ```bash # List available backups velero backup get # Restore from backup velero restore create --from-backup cloudflow-daily-backup-20260124 ``` 3. **Restore Database**: ```bash # Download backup from S3 aws s3 cp s3://cloudflow-db-backups/daily/cloudflow_20260124_010000.sql.gz . # Restore database gunzip -c cloudflow_20260124_010000.sql.gz | \\ psql -h postgresql.cloudflow-prod.svc.cluster.local -U cloudflow -d cloudflow ``` 4.",
              "score": null
            },
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_4",
              "content": "--set prometheus.prometheusSpec.retention=30d \\ --set prometheus.prometheusSpec.storageSpec.volumeClaimTemplate.spec.resources.requests.storage=100Gi ``` CloudFlow exposes Prometheus metrics on port 9090 at the `/metrics` endpoint. The metrics include: - **HTTP Metrics**: Request rate, latency, error rate - **Database Metrics**: Connection pool status, query duration - **Worker Metrics**: Job queue size, processing time, failure rate - **System Metrics**: CPU usage, memory usage, heap statistics Example metrics exposed: ``` # HELP cloudflow_http_requests_total Total number of HTTP requests # TYPE cloudflow_http_requests_total counter cloudflow_http_requests_total{method=\"GET\",route=\"/api/workflows\",status=\"200\"} 15420 # HELP cloudflow_http_request_duration_seconds HTTP request duration in seconds # TYPE cloudflow_http_request_duration_seconds histogram cloudflow_http_request_duration_seconds_bucket{method=\"POST\",route=\"/api/workflows\",le=\"0.1\"} 8234 # HELP cloudflow_worker_jobs_processed_total Total number of jobs processed # TYPE cloudflow_worker_jobs_processed_total counter cloudflow_worker_jobs_processed_total{status=\"success\"} 45230 # HELP cloudflow_database_connections Current database connections # TYPE cloudflow_database_connections gauge cloudflow_database_connections{state=\"active\"} 23 ``` ### Grafana Dashboards Access Grafana to view CloudFlow dashboards: ```bash kubectl port-forward -n monitoring svc/prometheus-grafana 3000:80 ``` Import the CloudFlow dashboard (ID: 15847) or use the provided JSON template. Key dashboard panels include: 1. **API Performance**: Request rate, P95/P99 latency, error rate 2. **Resource Usage**: CPU, memory, disk I/O per pod 3. **Database Health**: Connection pool utilization, query performance 4. **Worker Status**: Queue depth, processing rate, job success/failure ratio 5. **System Overview**: Pod status, replica count, autoscaling events ### Logging with CloudWatch CloudFlow logs are automatically shipped to CloudWatch Logs.",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_2",
              "content": "**Technology**: Node.js with TypeScript, Bull queue library **Replicas**: 16 pods (production), auto-scaling 12-24 **Resource Allocation**: 4 vCPU, 8GB RAM per pod **Key Responsibilities**: - Parse and validate workflow definitions (JSON-based DSL) - Execute workflow steps with state machine pattern - Handle retries with exponential backoff (max 3 retries, backoff: 2^n seconds) - Coordinate parallel and sequential task execution - Manage workflow state persistence and recovery - Support for conditional branching and loops - Sub-workflow invocation and composition **Workflow Execution Model**: ``` Workflow Submitted \u2192 Validation \u2192 Queue in Kafka \u2502 \u25bc Workflow Engine picks up \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u25bc \u25bc \u25bc [Task 1 Exec] [Task 2 Exec] [Task 3 Exec] \u2502 \u2502 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u25bc State Update \u2192 PostgreSQL \u2502 \u25bc Publish Event \u2192 Kafka ``` **Workflow DSL Example**: ```json { \"workflow_id\": \"customer-onboarding-v2\", \"version\": \"2.1.0\", \"steps\": [ { \"id\": \"validate-customer\", \"type\": \"http_request\", \"config\": { \"url\": \"https://validation.internal/verify\", \"method\": \"POST\", \"timeout\": 5000, \"retry\": {\"max_attempts\": 3, \"backoff\": \"exponential\"} } }, { \"id\": \"create-account\", \"type\": \"database_operation\", \"depends_on\": [\"validate-customer\"] } ] } ``` **State Management**: - Workflow states: PENDING, RUNNING, COMPLETED, FAILED, CANCELLED - Step states: QUEUED, EXECUTING, SUCCESS, FAILED, SKIPPED - State transitions stored in PostgreSQL with event sourcing pattern - Checkpointing every 10 steps for long-running workflows **Performance Targets**: - Simple workflow (3-5 steps): < 2 seconds (P99) - Complex workflow (20+ steps): < 5 seconds (P99) - Throughput: 500 concurrent workflow executions per pod --- ### Scheduler Service **Purpose**: Time-based workflow triggering system supporting cron-like schedules and one-time delayed executions.",
              "score": null
            }
          ],
          "latency_ms": 12.77776300048572,
          "query_id": "realistic_014",
          "dimension": "casual",
          "query": "monitoring stack"
        },
        {
          "key_facts": [
            "Prometheus",
            "Grafana",
            "Jaeger for distributed tracing"
          ],
          "found_facts": [
            "Prometheus",
            "Grafana"
          ],
          "missed_facts": [
            "Jaeger for distributed tracing"
          ],
          "coverage": 0.6666666666666666,
          "expected_docs": [
            "architecture_overview",
            "deployment_guide"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "user_guide",
              "chunk_id": "user_guide_fix_6",
              "content": "Filter by error type, date range, or execution ID 3. Click an execution to view full details 4. Click **\"Retry\"** to reprocess with the same input data ### Error Notifications Get notified when workflows fail: ```yaml workflow: notifications: on_failure: - type: email to: \"ops-team@company.com\" - type: slack channel: \"#alerts\" message: \"Workflow {{workflow.name}} failed: {{error.message}}\" on_success_after_retry: - type: slack channel: \"#monitoring\" message: \"Workflow recovered after {{error.attempts}} attempts\" ``` ## Workflow Limits CloudFlow enforces the following limits to ensure platform stability and performance: ### Steps Per Workflow - **Maximum**: 50 steps per workflow - **Recommendation**: Keep workflows focused and modular. If you need more steps, consider splitting into multiple workflows connected via webhooks. ### Execution Timeout - **Default**: 3600 seconds (60 minutes) - **Behavior**: Workflows exceeding this timeout are automatically terminated - **Custom Timeouts**: Enterprise plans can request custom timeout limits **Setting Step-Level Timeouts:** ```yaml - id: long_running_task action: http_request config: url: \"https://api.example.com/process\" timeout: 300 # 5 minutes for this specific step ``` ### Execution Limits - **Maximum**: 1000 executions per day (per workflow) - **Rate Limiting**: 100 concurrent executions per workflow - **Burst Limit**: 10 executions per second **What happens when limits are reached:** - New executions are queued automatically - Webhook triggers return HTTP 429 (Too Many Requests) - Scheduled executions are skipped (logged in audit trail) - Email notifications sent to workflow owner **Monitoring Usage:** View real-time metrics in your workflow dashboard: - Executions today: 847 / 1000 - Average duration: 12.3s - Success rate: 98.2% - Current queue depth: 3 ### Data Limits - **Maximum request/response size**: 10MB per action - **Maximum execution payload**: 50MB total - **Variable value size**: 1MB per variable ### Enterprise Plan Limits Enterprise customers can request increased limits: - Up to 100 steps per workflow - Up to 10,000 executions per day - Up to 7200 second timeout (2 hours) - Priority execution queue - Dedicated capacity allocation Contact sales@cloudflow.io for Enterprise pricing and custom limits. ## Best Practices Follow these best practices to build reliable, maintainable workflows: ### 1. Use Descriptive Names **Good:** - Workflow: \"Sync Customer Data from Salesforce to Database\" - Step: \"validate_customer_email\" **Bad:** - Workflow: \"Workflow 1\" - Step: \"step3\" ### 2.",
              "score": null
            },
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_4",
              "content": "--set prometheus.prometheusSpec.retention=30d \\ --set prometheus.prometheusSpec.storageSpec.volumeClaimTemplate.spec.resources.requests.storage=100Gi ``` CloudFlow exposes Prometheus metrics on port 9090 at the `/metrics` endpoint. The metrics include: - **HTTP Metrics**: Request rate, latency, error rate - **Database Metrics**: Connection pool status, query duration - **Worker Metrics**: Job queue size, processing time, failure rate - **System Metrics**: CPU usage, memory usage, heap statistics Example metrics exposed: ``` # HELP cloudflow_http_requests_total Total number of HTTP requests # TYPE cloudflow_http_requests_total counter cloudflow_http_requests_total{method=\"GET\",route=\"/api/workflows\",status=\"200\"} 15420 # HELP cloudflow_http_request_duration_seconds HTTP request duration in seconds # TYPE cloudflow_http_request_duration_seconds histogram cloudflow_http_request_duration_seconds_bucket{method=\"POST\",route=\"/api/workflows\",le=\"0.1\"} 8234 # HELP cloudflow_worker_jobs_processed_total Total number of jobs processed # TYPE cloudflow_worker_jobs_processed_total counter cloudflow_worker_jobs_processed_total{status=\"success\"} 45230 # HELP cloudflow_database_connections Current database connections # TYPE cloudflow_database_connections gauge cloudflow_database_connections{state=\"active\"} 23 ``` ### Grafana Dashboards Access Grafana to view CloudFlow dashboards: ```bash kubectl port-forward -n monitoring svc/prometheus-grafana 3000:80 ``` Import the CloudFlow dashboard (ID: 15847) or use the provided JSON template. Key dashboard panels include: 1. **API Performance**: Request rate, P95/P99 latency, error rate 2. **Resource Usage**: CPU, memory, disk I/O per pod 3. **Database Health**: Connection pool utilization, query performance 4. **Worker Status**: Queue depth, processing rate, job success/failure ratio 5. **System Overview**: Pod status, replica count, autoscaling events ### Logging with CloudWatch CloudFlow logs are automatically shipped to CloudWatch Logs.",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_9",
              "content": "in\" # Find workflow failures by ID kubectl logs -n cloudflow deployment/cloudflow-workflow-engine | \\ grep \"exec_7h3j6k9m2n\" ``` #### Rate Limiting ```bash # Find rate limit events kubectl logs -n cloudflow deployment/cloudflow-api | \\ grep -E \"429|rate.*limit|throttle\" # Count rate limit errors by hour kubectl logs -n cloudflow deployment/cloudflow-api --since=24h | \\ grep \"rate_limit_exceeded\" | \\ awk '{print $1}' | \\ cut -d'T' -f1-2 | \\ sort | uniq -c ``` ### Advanced Log Analysis #### Using jq for JSON Logs ```bash # Parse JSON logs and filter by level kubectl logs -n cloudflow deployment/cloudflow-api | \\ jq 'select(.level == \"ERROR\")' # Extract specific fields kubectl logs -n cloudflow deployment/cloudflow-api | \\ jq '{timestamp: .timestamp, level: .level, message: .message, execution_id: .context.execution_id}' # Filter by workflow ID kubectl logs -n cloudflow deployment/cloudflow-workflow-engine | \\ jq 'select(.workflow_id == \"wf_9k2n4m8p1q\")' # Count errors by type kubectl logs -n cloudflow deployment/cloudflow-api --since=1h | \\ jq -r 'select(.level == \"ERROR\") | .error_type' | \\ sort | uniq -c | sort -rn ``` #### Correlation IDs CloudFlow uses correlation IDs to trace requests across services. ```bash # Extract correlation ID from error CORRELATION_ID=\"corr_8h4j9k2m5n\" # Trace request across all services for pod in $(kubectl get pods -n cloudflow -l tier=backend -o name); do echo \"=== $pod ===\" kubectl logs -n cloudflow $pod | grep $CORRELATION_ID done # Export full trace to file cloudflow debug trace $CORRELATION_ID --output trace-$CORRELATION_ID.json ``` ### Debugging Commands #### Enable Debug Mode for Workflow Execution ```bash # Execute workflow with debug logging cloudflow workflows execute wf_9k2n4m8p1q \\ --debug \\ --log-level TRACE \\ --output-logs /tmp/workflow-debug.log # Enable step-by-step execution cloudflow workflows execute wf_9k2n4m8p1q \\ --step-mode interactive \\ --breakpoint-on-error # Capture full execution context cloudflow workflows execute wf_9k2n4m8p1q \\ --capture-context \\ --context-output /tmp/execution-context.json ``` #### Database Query Debugging ```bash # Enable query logging cloudflow db config set log_statement all cloudflow db config set log_duration on cloudflow db config set log_min_duration_statement 1000 # Log queries > 1s # Capture query plan for slow endpoint cloudflow debug capture-queries \\ --endpoint \"/api/v1/workflows/list\" \\ --duration 60 \\ --output query-analysis.txt # Analyze query performance cloudflow db analyze-performance --last 1h ``` #### Network Debugging ```bash # Test connectivity from CloudFlow pod kubectl run -n cloudflow netdebug --rm -i --tty \\ --image=nicolaka/netshoot -- /bin/bash # Inside pod: # Check DNS resolution nslookup api.cloudflow.io # Check connectivity to",
              "score": null
            },
            {
              "doc_id": "user_guide",
              "chunk_id": "user_guide_fix_0",
              "content": "# CloudFlow User Guide Welcome to CloudFlow, the modern workflow automation platform that helps you connect your apps, automate repetitive tasks, and build powerful integrations without writing code. ## Table of Contents 1. [Getting Started](#getting-started) 2. [Workflow Creation](#workflow-creation) 3. [Available Actions](#available-actions) 4. [Variables and Expressions](#variables-and-expressions) 5. [Scheduling](#scheduling) 6. [Error Handling](#error-handling) 7. [Workflow Limits](#workflow-limits) 8. [Best Practices](#best-practices) 9. [Common Workflow Patterns](#common-workflow-patterns) ## Getting Started ### Account Setup Creating your CloudFlow account takes just a few minutes: 1. Navigate to https://app.cloudflow.io/signup 2. Enter your email address and create a strong password 3. Verify your email address by clicking the link sent to your inbox 4. Complete your profile by providing your organization name and use case 5. Choose your plan (Free, Professional, or Enterprise) Once your account is created, you'll be redirected to your CloudFlow dashboard where you can start building workflows immediately. ### Your First Workflow Let's create a simple workflow that sends you a Slack notification when a new file is uploaded to your Google Drive: 1. Click the **\"Create Workflow\"** button in the top-right corner 2. Give your workflow a name: \"Google Drive to Slack Notifier\" 3. Select **\"Google Drive - New File\"** as your trigger 4. Authenticate your Google Drive account when prompted 5. Choose the folder you want to monitor 6. Click **\"Add Action\"** and select **\"Slack - Send Message\"** 7. Authenticate your Slack account 8. Select the channel where you want to receive notifications 9. Customize your message using variables: `New file uploaded: {{trigger.file.name}}` 10. Click **\"Save & Activate\"** to enable your workflow Congratulations! Your first workflow is now live. Every time a file is added to the specified Google Drive folder, you'll receive a Slack notification. ## Workflow Creation CloudFlow offers multiple ways to create and manage workflows, giving you the flexibility to work in the way that suits you best. ### Visual Editor The Visual Editor is CloudFlow's drag-and-drop interface for building workflows without code. It's perfect for users who prefer a graphical approach: **Key Features:** - Drag-and-drop action blocks from the sidebar - Visual connections between steps show your workflow logic - Inline configuration for each action - Real-time validation and error checking - Test mode to run your workflow with sample data **Using the Visual Editor:** 1.",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_11",
              "content": "outage\" # Create war room cloudflow incident war-room create incident-2024012401 ``` **For SEV-2 (High):** ```bash # Create incident and notify cloudflow incident create \\ --severity SEV-2 \\ --title \"High rate of workflow failures\" \\ --description \"Workflow success rate dropped to 75%\" \\ --notify-oncall # Update status page cloudflow status update \\ --status \"partial_outage\" \\ --message \"We are experiencing elevated error rates\" ``` **For SEV-3/SEV-4:** ```bash # Create support ticket cloudflow support ticket create \\ --priority MEDIUM \\ --subject \"Intermittent timeout errors\" \\ --description \"See attached logs and reproduction steps\" ``` ### Contact Information - **On-call hotline:** +1-888-CLOUDFLOW (24/7) - **Support email:** support@cloudflow.io - **Incident Slack channel:** #cloudflow-incidents - **Status page:** https://status.cloudflow.io - **Documentation:** https://docs.cloudflow.io ### Post-Incident Review After resolving incidents, complete a postmortem: ```bash # Generate postmortem template cloudflow incident postmortem incident-2024012401 \\ --template standard \\ --output postmortem-2024012401.md # Required sections: # 1. Incident summary # 2. Impact assessment # 3. Timeline of events # 4. Root cause analysis # 5. Resolution steps # 6. Action items # 7. Lessons learned ``` --- ## Additional Resources - **CloudFlow Documentation:** https://docs.cloudflow.io - **API Reference:** https://api-docs.cloudflow.io - **Community Forum:** https://community.cloudflow.io - **GitHub Issues:** https://github.com/cloudflow/platform/issues - **Training Portal:** https://training.cloudflow.io - **Monitoring Dashboard:** https://monitoring.cloudflow.io ### Getting Help If this troubleshooting guide doesn't resolve your issue: 1. Search the knowledge base: `cloudflow kb search \"your issue\"` 2. Check community forum for similar issues 3. Contact support with detailed logs and reproduction steps 4. For urgent issues, use emergency escalation procedures **Remember:** Always capture logs, metrics, and reproduction steps before escalating! --- *Last updated: January 24, 2026* *Document version: 3.2.1* *Feedback: docs-feedback@cloudflow.io*",
              "score": null
            }
          ],
          "latency_ms": 13.219235999713419,
          "query_id": "realistic_014",
          "dimension": "contextual",
          "query": "I need to set up dashboards, what monitoring systems are available?"
        },
        {
          "key_facts": [
            "Prometheus",
            "Grafana",
            "Jaeger for distributed tracing"
          ],
          "found_facts": [
            "Prometheus",
            "Grafana"
          ],
          "missed_facts": [
            "Jaeger for distributed tracing"
          ],
          "coverage": 0.6666666666666666,
          "expected_docs": [
            "architecture_overview",
            "deployment_guide"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_4",
              "content": "--set prometheus.prometheusSpec.retention=30d \\ --set prometheus.prometheusSpec.storageSpec.volumeClaimTemplate.spec.resources.requests.storage=100Gi ``` CloudFlow exposes Prometheus metrics on port 9090 at the `/metrics` endpoint. The metrics include: - **HTTP Metrics**: Request rate, latency, error rate - **Database Metrics**: Connection pool status, query duration - **Worker Metrics**: Job queue size, processing time, failure rate - **System Metrics**: CPU usage, memory usage, heap statistics Example metrics exposed: ``` # HELP cloudflow_http_requests_total Total number of HTTP requests # TYPE cloudflow_http_requests_total counter cloudflow_http_requests_total{method=\"GET\",route=\"/api/workflows\",status=\"200\"} 15420 # HELP cloudflow_http_request_duration_seconds HTTP request duration in seconds # TYPE cloudflow_http_request_duration_seconds histogram cloudflow_http_request_duration_seconds_bucket{method=\"POST\",route=\"/api/workflows\",le=\"0.1\"} 8234 # HELP cloudflow_worker_jobs_processed_total Total number of jobs processed # TYPE cloudflow_worker_jobs_processed_total counter cloudflow_worker_jobs_processed_total{status=\"success\"} 45230 # HELP cloudflow_database_connections Current database connections # TYPE cloudflow_database_connections gauge cloudflow_database_connections{state=\"active\"} 23 ``` ### Grafana Dashboards Access Grafana to view CloudFlow dashboards: ```bash kubectl port-forward -n monitoring svc/prometheus-grafana 3000:80 ``` Import the CloudFlow dashboard (ID: 15847) or use the provided JSON template. Key dashboard panels include: 1. **API Performance**: Request rate, P95/P99 latency, error rate 2. **Resource Usage**: CPU, memory, disk I/O per pod 3. **Database Health**: Connection pool utilization, query performance 4. **Worker Status**: Queue depth, processing rate, job success/failure ratio 5. **System Overview**: Pod status, replica count, autoscaling events ### Logging with CloudWatch CloudFlow logs are automatically shipped to CloudWatch Logs.",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_6",
              "content": "Data Validation Errors** ``` ValidationError: Field 'customer_id' is required but missing in 234 records ``` **Resolution:** ```bash # Add data quality checks cloudflow workflows update wf_9k2n4m8p1q \\ --step data_ingestion \\ --add-validator required_fields \\ --validator-config '{\"fields\": [\"customer_id\", \"timestamp\", \"amount\"]}' # Configure error handling cloudflow workflows update wf_9k2n4m8p1q \\ --step data_validation \\ --on-error continue \\ --error-threshold 5% # Fail if > 5% of records invalid ``` **2. External API Failures** ``` ExternalAPIError: API request to https://partner-api.example.com failed with status 502 ``` **Resolution:** ```bash # Add circuit breaker cloudflow workflows update wf_9k2n4m8p1q \\ --step external_api_call \\ --circuit-breaker-enabled true \\ --circuit-breaker-threshold 5 \\ --circuit-breaker-timeout 30000 # Configure fallback behavior cloudflow workflows update wf_9k2n4m8p1q \\ --step external_api_call \\ --fallback-action use_cached_data \\ --cache-ttl 3600 ``` --- ## Rate Limiting & Throttling ### 429 Too Many Requests #### Error Response ```json { \"error\": \"rate_limit_exceeded\", \"message\": \"API rate limit exceeded. Retry after 45 seconds.\", \"status\": 429, \"headers\": { \"X-RateLimit-Limit\": \"1000\", \"X-RateLimit-Remaining\": \"0\", \"X-RateLimit-Reset\": \"1706112345\", \"Retry-After\": \"45\" } } ``` #### Rate Limit Tiers CloudFlow enforces the following rate limits per workspace: | Tier | Requests/Minute | Requests/Hour | Concurrent Workflows | |------|-----------------|---------------|----------------------| | Free | 60 | 1,000 | 5 | | Standard | 1,000 | 50,000 | 50 | | Premium | 5,000 | 250,000 | 200 | | Enterprise | Custom | Custom | Unlimited | #### Checking Rate Limit Status ```bash # Check current rate limit status curl -I https://api.cloudflow.io/api/v1/workflows \\ -H \"Authorization: Bearer $CF_ACCESS_TOKEN\" # Extract rate limit headers curl -s -I https://api.cloudflow.io/api/v1/workflows \\ -H \"Authorization: Bearer $CF_ACCESS_TOKEN\" | \\ grep -i \"x-ratelimit\" # Output: # X-RateLimit-Limit: 1000 # X-RateLimit-Remaining: 247 # X-RateLimit-Reset: 1706112400 # Monitor rate limit usage cloudflow metrics query \\ --metric rate_limit_remaining \\ --workspace ws_abc123 \\ --last 1h \\ --interval 1m ``` #### Handling Rate Limits in Code **Python example with retry logic:** ```python import time import requests def cloudflow_api_call_with_retry(url, headers, max_retries=3): for attempt in range(max_retries): response = requests.get(url, headers=headers) if response.status_code == 429: retry_after = int(response.headers.get('Retry-After', 60)) print(f\"Rate limited.",
              "score": null
            },
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_3",
              "content": "**Endpoint:** `GET /pipelines/{pipeline_id}/executions` **Query Parameters:** - `status` (optional): Filter by status (`running`, `completed`, `failed`) - `start_date` (optional): ISO 8601 timestamp - `end_date` (optional): ISO 8601 timestamp - `limit` (optional): Items per page (max 100) - `offset` (optional): Pagination offset **Example Request:** ```python import requests api_key = \"YOUR_API_KEY\" pipeline_id = \"pipe_4x9k2m\" url = f\"https://api.cloudflow.io/v2/pipelines/{pipeline_id}/executions\" headers = {\"X-API-Key\": api_key} params = { \"status\": \"completed\", \"start_date\": \"2026-01-20T00:00:00Z\", \"limit\": 50 } response = requests.get(url, headers=headers, params=params) executions = response.json() for execution in executions['data']: print(f\"Execution {execution['id']}: {execution['status']} - {execution['duration_ms']}ms\") ``` **Example Response (200 OK):** ```json { \"data\": [ { \"id\": \"exec_7m3k9x2p\", \"pipeline_id\": \"pipe_4x9k2m\", \"status\": \"completed\", \"started_at\": \"2026-01-23T15:00:00Z\", \"completed_at\": \"2026-01-23T15:03:42Z\", \"duration_ms\": 222000, \"records_processed\": 15420, \"records_failed\": 3, \"error_rate\": 0.02 } ], \"pagination\": { \"total\": 156, \"limit\": 50, \"offset\": 0, \"has_more\": true } } ``` ### Analytics #### Get Workflow Metrics Retrieve performance metrics and analytics for workflows. **Endpoint:** `GET /analytics/workflows/{workflow_id}` **Query Parameters:** - `period` (required): Time period (`1h`, `24h`, `7d`, `30d`) - `metrics` (optional): Comma-separated list of metrics **Available Metrics:** - `execution_count`: Total number of executions - `success_rate`: Percentage of successful executions - `avg_duration`: Average execution duration in milliseconds - `error_count`: Total number of errors - `throughput`: Records processed per hour **Example Request:** ```bash curl -H \"X-API-Key: YOUR_API_KEY\" \\ \"https://api.cloudflow.io/v2/analytics/workflows/wf_8x7k2m9p?period=7d&metrics=execution_count,success_rate,avg_duration\" ``` **Example Response (200 OK):** ```json { \"workflow_id\": \"wf_8x7k2m9p\", \"period\": \"7d\", \"start_date\": \"2026-01-17T10:30:00Z\", \"end_date\": \"2026-01-24T10:30:00Z\", \"metrics\": { \"execution_count\": 168, \"success_rate\": 97.6, \"avg_duration\": 3285, \"error_count\": 4, \"throughput\": 2847 }, \"timeseries\": [ { \"timestamp\": \"2026-01-17T00:00:00Z\", \"execution_count\": 24, \"success_rate\": 100.0 }, { \"timestamp\": \"2026-01-18T00:00:00Z\", \"execution_count\": 24, \"success_rate\": 95.8 } ] } ``` ## Error Handling CloudFlow uses standard HTTP status codes and returns detailed error information in JSON format.",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_2",
              "content": "Review Query Execution Plans** ```sql -- Connect to CloudFlow database cloudflow db connect --readonly -- Explain slow query EXPLAIN ANALYZE SELECT w.*, e.status, e.error_message FROM workflows w LEFT JOIN executions e ON w.id = e.workflow_id WHERE w.workspace_id = 'ws_abc123' AND e.created_at > NOW() - INTERVAL '7 days' ORDER BY e.created_at DESC; -- Check for missing indexes SELECT schemaname, tablename, indexname, indexdef FROM pg_indexes WHERE tablename IN ('workflows', 'executions', 'workflow_steps') ORDER BY tablename, indexname; ``` **3. Optimize Queries** Common optimization techniques: **Add missing indexes:** ```sql -- Index for workflow lookup by workspace CREATE INDEX CONCURRENTLY idx_workflows_workspace_created ON workflows(workspace_id, created_at DESC); -- Index for execution status queries CREATE INDEX CONCURRENTLY idx_executions_status_created ON executions(workflow_id, status, created_at DESC) WHERE status IN ('running', 'pending'); -- Composite index for common filter combinations CREATE INDEX CONCURRENTLY idx_executions_workspace_status ON executions(workspace_id, status, created_at) INCLUDE (error_message, retry_count); ``` **Use query result caching:** ```bash # Enable query cache for workspace metadata cloudflow config set cache.workspace.ttl 3600 # Configure Redis cache backend cloudflow config set cache.backend redis cloudflow config set cache.redis.host redis.cloudflow.svc.cluster.local cloudflow config set cache.redis.port 6379 ``` ### High API Latency #### Latency Breakdown Analysis ```bash # Generate latency report cloudflow metrics latency-report --endpoint \"/api/v1/workflows\" --last 24h # Sample output: # Endpoint: POST /api/v1/workflows/execute # P50: 245ms | P95: 1823ms | P99: 4521ms # Breakdown: # - Auth: 45ms (18%) # - DB Query: 156ms (64%) # - Business Logic: 32ms (13%) # - Response Serialization: 12ms (5%) ``` **Network latency issues:** ```bash # Test connectivity to CloudFlow API time curl -w \"@curl-format.txt\" -o /dev/null -s https://api.cloudflow.io/health # Create curl-format.txt: cat > curl-format.txt << EOF time_namelookup: %{time_namelookup}s\\n time_connect: %{time_connect}s\\n time_appconnect: %{time_appconnect}s\\n time_pretransfer: %{time_pretransfer}s\\n time_redirect: %{time_redirect}s\\n time_starttransfer: %{time_starttransfer}s\\n ----------\\n time_total: %{time_total}s\\n EOF # Trace route to API endpoint traceroute api.cloudflow.io # Check DNS resolution time dig api.cloudflow.io | grep \"Query time\" ``` ### Memory Leaks #### Detection ```bash # Monitor CloudFlow service memory usage kubectl top pods -n cloudflow --sort-by=memory # Get detailed memory metrics for specific pod kubectl exec -n cloudflow deploy/cloudflow-api -- \\ curl localhost:9090/metrics | grep memory # Check for OOMKilled pods kubectl get pods -n cloudflow --field-selector=status.phase=Failed | \\ grep OOMKilled # Review memory limits and requests kubectl describe deployment cloudflow-api -n cloudflow | \\ grep -A 5 \"Limits\\|Requests\" ``` #### Common Causes **1.",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_0",
              "content": "# CloudFlow Platform Troubleshooting Guide **Version:** 3.2.1 **Last Updated:** January 2026 **Audience:** Platform Engineers, DevOps, Support Teams ## Table of Contents 1. [Overview](#overview) 2. [Authentication & Authorization Issues](#authentication--authorization-issues) 3. [Performance Problems](#performance-problems) 4. [Database Connection Issues](#database-connection-issues) 5. [Workflow Execution Failures](#workflow-execution-failures) 6. [Rate Limiting & Throttling](#rate-limiting--throttling) 7. [Log Analysis & Debugging](#log-analysis--debugging) 8. [Escalation Procedures](#escalation-procedures) --- ## Overview This guide provides comprehensive troubleshooting steps for common CloudFlow platform issues encountered in production environments. Each section includes error symptoms, root cause analysis, resolution steps, and preventive measures. ### Quick Diagnostic Checklist Before diving into specific issues, perform these initial checks: - Verify service health: `cloudflow status --all` - Check API connectivity: `curl -I https://api.cloudflow.io/health` - Review recent deployments: `kubectl get deployments -n cloudflow --sort-by=.metadata.creationTimestamp` - Inspect platform metrics: `cloudflow metrics --last 1h` --- ## Authentication & Authorization Issues ### 401 Unauthorized Errors #### Symptoms - API requests return `401 Unauthorized` - Error message: `Authentication credentials were not provided or are invalid` - Frontend displays \"Session expired\" message #### Common Causes **1. Token Expiration** CloudFlow access tokens expire after 3600 seconds (1 hour) by default. Refresh tokens are valid for 30 days. **Verification:** ```bash # Decode JWT to check expiration echo $CF_ACCESS_TOKEN | cut -d'.' -f2 | base64 -d | jq '.exp' # Compare with current time date +%s ``` **Resolution:** ```bash # Refresh the access token curl -X POST https://api.cloudflow.io/auth/refresh \\ -H \"Content-Type: application/json\" \\ -d '{\"refresh_token\": \"'$CF_REFRESH_TOKEN'\"}' # Update environment variable export CF_ACCESS_TOKEN=\"<new_token>\" ``` **2. Invalid JWT Signature** Error: `JWT signature verification failed` **Causes:** - Token was modified or corrupted - Using wrong signing key - Token generated with different secret **Resolution:** ```bash # Validate token structure cloudflow auth validate --token $CF_ACCESS_TOKEN # Generate new token cloudflow auth login --username <user> --password <pass> # For service accounts cloudflow auth service-account --create --name <sa-name> --scopes \"workflow:read,workflow:write\" ``` **3. Clock Skew Issues** JWT validation fails when system clocks are out of sync (tolerance: \u00b1300 seconds).",
              "score": null
            }
          ],
          "latency_ms": 13.035412997851381,
          "query_id": "realistic_014",
          "dimension": "negation",
          "query": "Why can't I see metrics in Datadog?"
        },
        {
          "key_facts": [
            "could not obtain connection from pool within 5000ms",
            "connection pool exhausted (100/100 connections in use)",
            "PgBouncer",
            "max_db_connections = 100"
          ],
          "found_facts": [
            "could not obtain connection from pool within 5000ms",
            "connection pool exhausted (100/100 connections in use)",
            "PgBouncer"
          ],
          "missed_facts": [
            "max_db_connections = 100"
          ],
          "coverage": 0.75,
          "expected_docs": [
            "troubleshooting_guide",
            "deployment_guide"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_3",
              "content": "Workflow Context Accumulation** Large workflow executions may accumulate state in memory. **Solution:** ```bash # Configure context cleanup cloudflow config set workflow.context.max_size_mb 100 cloudflow config set workflow.context.cleanup_threshold 0.8 # Enable context persistence to disk cloudflow config set workflow.context.persistence.enabled true cloudflow config set workflow.context.persistence.backend s3 ``` **2. Connection Pool Leaks** **Diagnosis:** ```bash # Check active connections cloudflow db connections --verbose # Expected output: # Active: 45/100 # Idle: 23 # Waiting: 2 # Average age: 245s ``` **Resolution:** ```bash # Adjust connection pool settings cloudflow config set db.pool.max_connections 100 cloudflow config set db.pool.min_connections 10 cloudflow config set db.pool.idle_timeout 300 cloudflow config set db.pool.max_lifetime 1800 # Force connection pool reset cloudflow db pool reset --confirm ``` **3. Event Stream Buffers** Unbounded event buffers can cause memory exhaustion. ```bash # Configure event buffer limits cloudflow config set events.buffer.max_size 10000 cloudflow config set events.buffer.overflow_strategy drop_oldest # Enable event streaming to external sink cloudflow config set events.sink.type kafka cloudflow config set events.sink.kafka.brokers \"kafka-1:9092,kafka-2:9092\" cloudflow config set events.sink.kafka.topic cloudflow-events ``` --- ## Database Connection Issues ### Connection Pool Exhaustion #### Symptoms - Error: `could not obtain connection from pool within 5000ms` - Error: `connection pool exhausted (100/100 connections in use)` - API requests fail with `503 Service Unavailable` - Database CPU usage normal, but connection count at maximum #### Investigation ```bash # Check current connection pool status cloudflow db pool status --detailed # Output example: # Pool Statistics: # Total Connections: 100/100 (100%) # Active: 87 # Idle: 13 # Waiting Requests: 45 # Average Wait Time: 3420ms # Max Wait Time: 8234ms # Identify long-running queries cloudflow db queries --status running --min-duration 30000 # Check connection distribution by client SELECT application_name, state, COUNT(*) as conn_count, AVG(EXTRACT(EPOCH FROM (NOW() - state_change))) as avg_duration_sec FROM pg_stat_activity WHERE datname = 'cloudflow_production' GROUP BY application_name, state ORDER BY conn_count DESC; ``` #### Resolution **Immediate mitigation:** ```bash # Temporarily increase connection limit (requires database restart) cloudflow db config set max_connections 150 # Kill idle connections older than 5 minutes cloudflow db connections kill --idle-timeout 300 # Restart connection pool without downtime kubectl rollout restart deployment/cloudflow-api -n cloudflow kubectl rollout status deployment/cloudflow-api -n cloudflow ``` **Long-term solutions:** 1.",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_4",
              "content": "**Implement connection pooling optimization:** ```bash # Use PgBouncer for connection pooling kubectl apply -f cloudflow-pgbouncer.yaml # Configure CloudFlow to use PgBouncer cloudflow config set db.host pgbouncer.cloudflow.svc.cluster.local cloudflow config set db.port 6432 cloudflow config set db.pool.mode transaction ``` 2. **Add read replicas:** ```bash # Route read-only queries to replicas cloudflow db replicas add --count 2 cloudflow config set db.read_replicas \"replica-1:5432,replica-2:5432\" cloudflow config set db.read_write_split true ``` ### Connection Timeout Errors #### Error Messages - `connection timeout after 30000ms` - `could not connect to database server at 10.0.2.45:5432` - `database server unreachable` #### Troubleshooting Steps ```bash # Test network connectivity telnet cloudflow-db.internal.company.com 5432 # Check DNS resolution nslookup cloudflow-db.internal.company.com # Verify database is accepting connections pg_isready -h cloudflow-db.internal.company.com -p 5432 -U cloudflow # Check firewall rules sudo iptables -L -n | grep 5432 # Test from CloudFlow pod network kubectl run -n cloudflow debug-pod --rm -i --tty \\ --image=postgres:14 -- \\ psql -h cloudflow-db.internal.company.com -U cloudflow -d cloudflow_production # Review database logs for connection rejections kubectl logs -n cloudflow statefulset/cloudflow-db --tail=100 | \\ grep -i \"connection\\|reject\\|authentication\" ``` ### Maximum Connection Limit (100) Reached This is a hard limit in CloudFlow's database tier. #### Permanent Solutions **Option 1: Upgrade database tier** ```bash # Check available tiers cloudflow db tiers list # Upgrade to higher tier (supports 200 connections) cloudflow db upgrade --tier standard-plus --confirm # Monitor migration progress cloudflow db migration status ``` **Option 2: Implement aggressive connection reuse** ```bash # Reduce connection lifetime cloudflow config set db.pool.max_lifetime 600 # 10 minutes # Enable prepared statement caching cloudflow config set db.prepared_statements.cache true cloudflow config set db.prepared_statements.max_size 250 # Reduce idle connection timeout cloudflow config set db.pool.idle_timeout 120 # 2 minutes ``` --- ## Workflow Execution Failures ### Timeout Errors (3600 second limit) #### Error Message ``` WorkflowExecutionError: Workflow exceeded maximum execution time of 3600 seconds Status: TIMEOUT Workflow ID: wf_9k2n4m8p1q Execution ID: exec_7h3j6k9m2n ``` #### Analysis ```bash # Get workflow execution details cloudflow workflows executions get exec_7h3j6k9m2n --verbose # Check step-by-step breakdown cloudflow workflows executions steps exec_7h3j6k9m2n # Sample output: # Step 1: data_ingestion - Duration: 245s - Status: SUCCESS # Step 2: data_validation - Duration: 123s - Status: SUCCESS # Step 3: data_transformation - Duration: 3189s - Status: TIMEOUT # Step 4: data_export - Duration: 0s - Status: SKIPPED # Identify bottleneck step cloudflow workflows analyze",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_10",
              "content": "- Execution start rate: 500 per second - Completion rate: 450 per second (average 2-second execution time) **Database**: - Read throughput: 50,000 queries per second (across replicas) - Write throughput: 15,000 transactions per second - Connection capacity: 2,000 concurrent connections **Kafka**: - Message ingestion: 100,000 messages per second - Consumer throughput: 80,000 messages per second (aggregated) - End-to-end latency: < 100ms (P99) ### Resource Utilization **CPU Utilization** (Target: 60-70% average): - API Gateway: 55% average, 80% peak - Workflow Engine: 65% average, 85% peak - Auth Service: 40% average, 60% peak **Memory Utilization** (Target: < 80%): - API Gateway: 2.5GB average per pod (4GB allocated) - Workflow Engine: 6GB average per pod (8GB allocated) - Notification Service: 2.8GB average per pod (4GB allocated) **Network Throughput**: - Ingress: 2 Gbps average, 5 Gbps peak - Egress: 1.5 Gbps average, 4 Gbps peak - Internal (service mesh): 8 Gbps average ### Monitoring & Alerting **Key Metrics** (Monitored via Prometheus): - Request rate (per service, per endpoint) - Error rate (4xx, 5xx responses) - Latency percentiles (P50, P95, P99) - Resource utilization (CPU, memory, disk) - Cache hit/miss ratios - Database connection pool saturation - Kafka consumer lag **Alerts** (via PagerDuty): - P1 (Immediate): API error rate > 1%, database connection failure - P2 (< 15 min): P99 latency > 500ms, cache hit rate < 80% - P3 (< 1 hour): Resource utilization > 85%, replica count < minimum **SLI/SLO/SLA**: - SLI: API success rate (non-5xx responses) - SLO: 99.9% uptime per month (43 minutes downtime allowance) - SLA: 99.5% uptime guarantee (customer-facing SLA with credits) --- ## Disaster Recovery ### Recovery Objectives **RPO (Recovery Point Objective): 1 hour** - Maximum acceptable data loss: 1 hour of transactions - Achieved through: Continuous database replication + hourly snapshots - Kafka retention: 7 days allows event replay **RTO (Recovery Time Objective): 4 hours** - Maximum acceptable downtime: 4 hours for full system recovery - Includes: Failover, data verification, and service restoration - Automated runbooks reduce RTO to < 2 hours for common scenarios ### High Availability Architecture **Multi-AZ Deployment**: ``` Region: us-east-1 AZ-1a: AZ-1b: AZ-1c: - API Gateway (4) - API Gateway (4) - API Gateway (4) - Workflow Engine (6) - Workflow Engine (5) - Workflow Engine (5) - Auth Service (3) - Auth Service (3) - Auth",
              "score": null
            },
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_4",
              "content": "--set prometheus.prometheusSpec.retention=30d \\ --set prometheus.prometheusSpec.storageSpec.volumeClaimTemplate.spec.resources.requests.storage=100Gi ``` CloudFlow exposes Prometheus metrics on port 9090 at the `/metrics` endpoint. The metrics include: - **HTTP Metrics**: Request rate, latency, error rate - **Database Metrics**: Connection pool status, query duration - **Worker Metrics**: Job queue size, processing time, failure rate - **System Metrics**: CPU usage, memory usage, heap statistics Example metrics exposed: ``` # HELP cloudflow_http_requests_total Total number of HTTP requests # TYPE cloudflow_http_requests_total counter cloudflow_http_requests_total{method=\"GET\",route=\"/api/workflows\",status=\"200\"} 15420 # HELP cloudflow_http_request_duration_seconds HTTP request duration in seconds # TYPE cloudflow_http_request_duration_seconds histogram cloudflow_http_request_duration_seconds_bucket{method=\"POST\",route=\"/api/workflows\",le=\"0.1\"} 8234 # HELP cloudflow_worker_jobs_processed_total Total number of jobs processed # TYPE cloudflow_worker_jobs_processed_total counter cloudflow_worker_jobs_processed_total{status=\"success\"} 45230 # HELP cloudflow_database_connections Current database connections # TYPE cloudflow_database_connections gauge cloudflow_database_connections{state=\"active\"} 23 ``` ### Grafana Dashboards Access Grafana to view CloudFlow dashboards: ```bash kubectl port-forward -n monitoring svc/prometheus-grafana 3000:80 ``` Import the CloudFlow dashboard (ID: 15847) or use the provided JSON template. Key dashboard panels include: 1. **API Performance**: Request rate, P95/P99 latency, error rate 2. **Resource Usage**: CPU, memory, disk I/O per pod 3. **Database Health**: Connection pool utilization, query performance 4. **Worker Status**: Queue depth, processing rate, job success/failure ratio 5. **System Overview**: Pod status, replica count, autoscaling events ### Logging with CloudWatch CloudFlow logs are automatically shipped to CloudWatch Logs.",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_0",
              "content": "# CloudFlow Platform Troubleshooting Guide **Version:** 3.2.1 **Last Updated:** January 2026 **Audience:** Platform Engineers, DevOps, Support Teams ## Table of Contents 1. [Overview](#overview) 2. [Authentication & Authorization Issues](#authentication--authorization-issues) 3. [Performance Problems](#performance-problems) 4. [Database Connection Issues](#database-connection-issues) 5. [Workflow Execution Failures](#workflow-execution-failures) 6. [Rate Limiting & Throttling](#rate-limiting--throttling) 7. [Log Analysis & Debugging](#log-analysis--debugging) 8. [Escalation Procedures](#escalation-procedures) --- ## Overview This guide provides comprehensive troubleshooting steps for common CloudFlow platform issues encountered in production environments. Each section includes error symptoms, root cause analysis, resolution steps, and preventive measures. ### Quick Diagnostic Checklist Before diving into specific issues, perform these initial checks: - Verify service health: `cloudflow status --all` - Check API connectivity: `curl -I https://api.cloudflow.io/health` - Review recent deployments: `kubectl get deployments -n cloudflow --sort-by=.metadata.creationTimestamp` - Inspect platform metrics: `cloudflow metrics --last 1h` --- ## Authentication & Authorization Issues ### 401 Unauthorized Errors #### Symptoms - API requests return `401 Unauthorized` - Error message: `Authentication credentials were not provided or are invalid` - Frontend displays \"Session expired\" message #### Common Causes **1. Token Expiration** CloudFlow access tokens expire after 3600 seconds (1 hour) by default. Refresh tokens are valid for 30 days. **Verification:** ```bash # Decode JWT to check expiration echo $CF_ACCESS_TOKEN | cut -d'.' -f2 | base64 -d | jq '.exp' # Compare with current time date +%s ``` **Resolution:** ```bash # Refresh the access token curl -X POST https://api.cloudflow.io/auth/refresh \\ -H \"Content-Type: application/json\" \\ -d '{\"refresh_token\": \"'$CF_REFRESH_TOKEN'\"}' # Update environment variable export CF_ACCESS_TOKEN=\"<new_token>\" ``` **2. Invalid JWT Signature** Error: `JWT signature verification failed` **Causes:** - Token was modified or corrupted - Using wrong signing key - Token generated with different secret **Resolution:** ```bash # Validate token structure cloudflow auth validate --token $CF_ACCESS_TOKEN # Generate new token cloudflow auth login --username <user> --password <pass> # For service accounts cloudflow auth service-account --create --name <sa-name> --scopes \"workflow:read,workflow:write\" ``` **3. Clock Skew Issues** JWT validation fails when system clocks are out of sync (tolerance: \u00b1300 seconds).",
              "score": null
            }
          ],
          "latency_ms": 12.984477998543298,
          "query_id": "realistic_015",
          "dimension": "original",
          "query": "How do I diagnose database connection pool exhaustion?"
        },
        {
          "key_facts": [
            "could not obtain connection from pool within 5000ms",
            "connection pool exhausted (100/100 connections in use)",
            "PgBouncer",
            "max_db_connections = 100"
          ],
          "found_facts": [
            "could not obtain connection from pool within 5000ms",
            "connection pool exhausted (100/100 connections in use)",
            "PgBouncer"
          ],
          "missed_facts": [
            "max_db_connections = 100"
          ],
          "coverage": 0.75,
          "expected_docs": [
            "troubleshooting_guide",
            "deployment_guide"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_4",
              "content": "**Implement connection pooling optimization:** ```bash # Use PgBouncer for connection pooling kubectl apply -f cloudflow-pgbouncer.yaml # Configure CloudFlow to use PgBouncer cloudflow config set db.host pgbouncer.cloudflow.svc.cluster.local cloudflow config set db.port 6432 cloudflow config set db.pool.mode transaction ``` 2. **Add read replicas:** ```bash # Route read-only queries to replicas cloudflow db replicas add --count 2 cloudflow config set db.read_replicas \"replica-1:5432,replica-2:5432\" cloudflow config set db.read_write_split true ``` ### Connection Timeout Errors #### Error Messages - `connection timeout after 30000ms` - `could not connect to database server at 10.0.2.45:5432` - `database server unreachable` #### Troubleshooting Steps ```bash # Test network connectivity telnet cloudflow-db.internal.company.com 5432 # Check DNS resolution nslookup cloudflow-db.internal.company.com # Verify database is accepting connections pg_isready -h cloudflow-db.internal.company.com -p 5432 -U cloudflow # Check firewall rules sudo iptables -L -n | grep 5432 # Test from CloudFlow pod network kubectl run -n cloudflow debug-pod --rm -i --tty \\ --image=postgres:14 -- \\ psql -h cloudflow-db.internal.company.com -U cloudflow -d cloudflow_production # Review database logs for connection rejections kubectl logs -n cloudflow statefulset/cloudflow-db --tail=100 | \\ grep -i \"connection\\|reject\\|authentication\" ``` ### Maximum Connection Limit (100) Reached This is a hard limit in CloudFlow's database tier. #### Permanent Solutions **Option 1: Upgrade database tier** ```bash # Check available tiers cloudflow db tiers list # Upgrade to higher tier (supports 200 connections) cloudflow db upgrade --tier standard-plus --confirm # Monitor migration progress cloudflow db migration status ``` **Option 2: Implement aggressive connection reuse** ```bash # Reduce connection lifetime cloudflow config set db.pool.max_lifetime 600 # 10 minutes # Enable prepared statement caching cloudflow config set db.prepared_statements.cache true cloudflow config set db.prepared_statements.max_size 250 # Reduce idle connection timeout cloudflow config set db.pool.idle_timeout 120 # 2 minutes ``` --- ## Workflow Execution Failures ### Timeout Errors (3600 second limit) #### Error Message ``` WorkflowExecutionError: Workflow exceeded maximum execution time of 3600 seconds Status: TIMEOUT Workflow ID: wf_9k2n4m8p1q Execution ID: exec_7h3j6k9m2n ``` #### Analysis ```bash # Get workflow execution details cloudflow workflows executions get exec_7h3j6k9m2n --verbose # Check step-by-step breakdown cloudflow workflows executions steps exec_7h3j6k9m2n # Sample output: # Step 1: data_ingestion - Duration: 245s - Status: SUCCESS # Step 2: data_validation - Duration: 123s - Status: SUCCESS # Step 3: data_transformation - Duration: 3189s - Status: TIMEOUT # Step 4: data_export - Duration: 0s - Status: SKIPPED # Identify bottleneck step cloudflow workflows analyze",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_0",
              "content": "# CloudFlow Platform Troubleshooting Guide **Version:** 3.2.1 **Last Updated:** January 2026 **Audience:** Platform Engineers, DevOps, Support Teams ## Table of Contents 1. [Overview](#overview) 2. [Authentication & Authorization Issues](#authentication--authorization-issues) 3. [Performance Problems](#performance-problems) 4. [Database Connection Issues](#database-connection-issues) 5. [Workflow Execution Failures](#workflow-execution-failures) 6. [Rate Limiting & Throttling](#rate-limiting--throttling) 7. [Log Analysis & Debugging](#log-analysis--debugging) 8. [Escalation Procedures](#escalation-procedures) --- ## Overview This guide provides comprehensive troubleshooting steps for common CloudFlow platform issues encountered in production environments. Each section includes error symptoms, root cause analysis, resolution steps, and preventive measures. ### Quick Diagnostic Checklist Before diving into specific issues, perform these initial checks: - Verify service health: `cloudflow status --all` - Check API connectivity: `curl -I https://api.cloudflow.io/health` - Review recent deployments: `kubectl get deployments -n cloudflow --sort-by=.metadata.creationTimestamp` - Inspect platform metrics: `cloudflow metrics --last 1h` --- ## Authentication & Authorization Issues ### 401 Unauthorized Errors #### Symptoms - API requests return `401 Unauthorized` - Error message: `Authentication credentials were not provided or are invalid` - Frontend displays \"Session expired\" message #### Common Causes **1. Token Expiration** CloudFlow access tokens expire after 3600 seconds (1 hour) by default. Refresh tokens are valid for 30 days. **Verification:** ```bash # Decode JWT to check expiration echo $CF_ACCESS_TOKEN | cut -d'.' -f2 | base64 -d | jq '.exp' # Compare with current time date +%s ``` **Resolution:** ```bash # Refresh the access token curl -X POST https://api.cloudflow.io/auth/refresh \\ -H \"Content-Type: application/json\" \\ -d '{\"refresh_token\": \"'$CF_REFRESH_TOKEN'\"}' # Update environment variable export CF_ACCESS_TOKEN=\"<new_token>\" ``` **2. Invalid JWT Signature** Error: `JWT signature verification failed` **Causes:** - Token was modified or corrupted - Using wrong signing key - Token generated with different secret **Resolution:** ```bash # Validate token structure cloudflow auth validate --token $CF_ACCESS_TOKEN # Generate new token cloudflow auth login --username <user> --password <pass> # For service accounts cloudflow auth service-account --create --name <sa-name> --scopes \"workflow:read,workflow:write\" ``` **3. Clock Skew Issues** JWT validation fails when system clocks are out of sync (tolerance: \u00b1300 seconds).",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_10",
              "content": "database nc -zv cloudflow-db.internal.company.com 5432 # Trace route traceroute api.cloudflow.io # Capture packets tcpdump -i any -w /tmp/capture.pcap port 5432 ``` --- ## Escalation Procedures ### Severity Levels CloudFlow incidents are classified into four severity levels: #### SEV-1: Critical (P1) - **Definition:** Complete service outage or severe degradation affecting all users - **Examples:** - API returns 5xx errors for > 5 minutes - Database completely unavailable - Authentication system down - Data loss or corruption - **Response Time:** Immediate (< 15 minutes) - **Escalation:** Page on-call engineer immediately #### SEV-2: High (P2) - **Definition:** Major functionality impaired affecting multiple users - **Examples:** - Workflow execution success rate < 90% - Significant performance degradation (p95 latency > 10s) - Rate limiting affecting large customer segment - **Response Time:** < 1 hour - **Escalation:** Create incident ticket and notify on-call #### SEV-3: Medium (P3) - **Definition:** Partial functionality degraded affecting some users - **Examples:** - Intermittent failures for specific workflow types - Minor performance issues - Non-critical feature unavailable - **Response Time:** < 4 hours - **Escalation:** Create ticket, normal business hours support #### SEV-4: Low (P4) - **Definition:** Minor issues with minimal user impact - **Examples:** - Cosmetic issues - Documentation errors - Feature requests - **Response Time:** < 2 business days - **Escalation:** Standard support ticket ### Escalation Steps #### Step 1: Initial Assessment (0-5 minutes) ```bash # Run health check cloudflow health check --comprehensive # Check status page curl https://status.cloudflow.io/api/v1/status.json # Review recent changes cloudflow audit log --last 2h --event-type \"deployment,configuration\" # Check metrics dashboard cloudflow metrics dashboard --incident-mode ``` #### Step 2: Gather Information (5-15 minutes) Create incident document with: - Incident timestamp and duration - Affected services and endpoints - Error rates and user impact - Recent changes or deployments - Relevant log excerpts - Correlation IDs for failed requests ```bash # Generate incident report cloudflow debug incident-report \\ --start \"2026-01-24T10:30:00Z\" \\ --end \"2026-01-24T11:00:00Z\" \\ --output incident-report.md # Capture system snapshot cloudflow debug snapshot --output snapshot-$(date +%Y%m%d-%H%M%S).tar.gz ``` #### Step 3: Escalate Based on Severity **For SEV-1 (Critical):** ```bash # Page on-call engineer cloudflow incident create \\ --severity SEV-1 \\ --title \"Complete API outage\" \\ --description \"All API requests returning 503\" \\ --page-oncall # Notify status page cloudflow status update \\ --status \"major_outage\" \\ --message \"We are investigating a complete service",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_3",
              "content": "Workflow Context Accumulation** Large workflow executions may accumulate state in memory. **Solution:** ```bash # Configure context cleanup cloudflow config set workflow.context.max_size_mb 100 cloudflow config set workflow.context.cleanup_threshold 0.8 # Enable context persistence to disk cloudflow config set workflow.context.persistence.enabled true cloudflow config set workflow.context.persistence.backend s3 ``` **2. Connection Pool Leaks** **Diagnosis:** ```bash # Check active connections cloudflow db connections --verbose # Expected output: # Active: 45/100 # Idle: 23 # Waiting: 2 # Average age: 245s ``` **Resolution:** ```bash # Adjust connection pool settings cloudflow config set db.pool.max_connections 100 cloudflow config set db.pool.min_connections 10 cloudflow config set db.pool.idle_timeout 300 cloudflow config set db.pool.max_lifetime 1800 # Force connection pool reset cloudflow db pool reset --confirm ``` **3. Event Stream Buffers** Unbounded event buffers can cause memory exhaustion. ```bash # Configure event buffer limits cloudflow config set events.buffer.max_size 10000 cloudflow config set events.buffer.overflow_strategy drop_oldest # Enable event streaming to external sink cloudflow config set events.sink.type kafka cloudflow config set events.sink.kafka.brokers \"kafka-1:9092,kafka-2:9092\" cloudflow config set events.sink.kafka.topic cloudflow-events ``` --- ## Database Connection Issues ### Connection Pool Exhaustion #### Symptoms - Error: `could not obtain connection from pool within 5000ms` - Error: `connection pool exhausted (100/100 connections in use)` - API requests fail with `503 Service Unavailable` - Database CPU usage normal, but connection count at maximum #### Investigation ```bash # Check current connection pool status cloudflow db pool status --detailed # Output example: # Pool Statistics: # Total Connections: 100/100 (100%) # Active: 87 # Idle: 13 # Waiting Requests: 45 # Average Wait Time: 3420ms # Max Wait Time: 8234ms # Identify long-running queries cloudflow db queries --status running --min-duration 30000 # Check connection distribution by client SELECT application_name, state, COUNT(*) as conn_count, AVG(EXTRACT(EPOCH FROM (NOW() - state_change))) as avg_duration_sec FROM pg_stat_activity WHERE datname = 'cloudflow_production' GROUP BY application_name, state ORDER BY conn_count DESC; ``` #### Resolution **Immediate mitigation:** ```bash # Temporarily increase connection limit (requires database restart) cloudflow db config set max_connections 150 # Kill idle connections older than 5 minutes cloudflow db connections kill --idle-timeout 300 # Restart connection pool without downtime kubectl rollout restart deployment/cloudflow-api -n cloudflow kubectl rollout status deployment/cloudflow-api -n cloudflow ``` **Long-term solutions:** 1.",
              "score": null
            },
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_4",
              "content": "### Error Response Format ```json { \"error\": { \"code\": \"invalid_parameter\", \"message\": \"The 'limit' parameter must be between 1 and 100\", \"field\": \"limit\", \"request_id\": \"req_8k3m9x2p\" } } ``` ### HTTP Status Codes | Status Code | Description | Common Causes | |------------|-------------|---------------| | 400 | Bad Request | Invalid parameters, malformed JSON, validation errors | | 401 | Unauthorized | Missing or invalid authentication credentials | | 403 | Forbidden | Insufficient permissions for requested resource | | 404 | Not Found | Resource does not exist or has been deleted | | 429 | Too Many Requests | Rate limit exceeded, retry after specified period | | 500 | Internal Server Error | Unexpected server error, contact support if persists | | 502 | Bad Gateway | Temporary service issue, retry with exponential backoff | | 503 | Service Unavailable | Scheduled maintenance or temporary outage | ### Error Codes CloudFlow returns specific error codes to help you identify and resolve issues: - `invalid_parameter`: One or more request parameters are invalid - `missing_required_field`: Required field is missing from request body - `authentication_failed`: Invalid API key or token - `insufficient_permissions`: User lacks required scope or permission - `resource_not_found`: Requested resource does not exist - `rate_limit_exceeded`: Too many requests, see rate limiting section - `workflow_execution_failed`: Workflow execution encountered an error - `invalid_json`: Request body contains malformed JSON - `duplicate_resource`: Resource with same identifier already exists - `quota_exceeded`: Account quota limit reached ### Error Handling Best Practices **Python Example:** ```python import requests import time def make_api_request(url, headers, max_retries=3): for attempt in range(max_retries): try: response = requests.get(url, headers=headers) if response.status_code == 200: return response.json() elif response.status_code == 429: # Rate limit exceeded retry_after = int(response.headers.get('X-RateLimit-Reset', 60)) print(f\"Rate limited. Waiting {retry_after} seconds...\") time.sleep(retry_after) continue elif response.status_code >= 500: # Server error - retry with exponential backoff wait_time = 2 ** attempt print(f\"Server error. Retrying in {wait_time} seconds...\") time.sleep(wait_time) continue else: # Client error - don't retry error_data = response.json() raise Exception(f\"API Error: {error_data['error']['message']}\") except requests.exceptions.RequestException as e: print(f\"Request failed: {e}\") if attempt < max_retries - 1: time.sleep(2 ** attempt) else: raise raise Exception(\"Max retries exceeded\") ``` ## Webhooks CloudFlow can send webhook notifications when specific events occur in your workflows.",
              "score": null
            }
          ],
          "latency_ms": 13.067111998680048,
          "query_id": "realistic_015",
          "dimension": "synonym",
          "query": "What should I check when I run out of database connections?"
        },
        {
          "key_facts": [
            "could not obtain connection from pool within 5000ms",
            "connection pool exhausted (100/100 connections in use)",
            "PgBouncer",
            "max_db_connections = 100"
          ],
          "found_facts": [
            "could not obtain connection from pool within 5000ms",
            "connection pool exhausted (100/100 connections in use)",
            "PgBouncer"
          ],
          "missed_facts": [
            "max_db_connections = 100"
          ],
          "coverage": 0.75,
          "expected_docs": [
            "troubleshooting_guide",
            "deployment_guide"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_4",
              "content": "**Implement connection pooling optimization:** ```bash # Use PgBouncer for connection pooling kubectl apply -f cloudflow-pgbouncer.yaml # Configure CloudFlow to use PgBouncer cloudflow config set db.host pgbouncer.cloudflow.svc.cluster.local cloudflow config set db.port 6432 cloudflow config set db.pool.mode transaction ``` 2. **Add read replicas:** ```bash # Route read-only queries to replicas cloudflow db replicas add --count 2 cloudflow config set db.read_replicas \"replica-1:5432,replica-2:5432\" cloudflow config set db.read_write_split true ``` ### Connection Timeout Errors #### Error Messages - `connection timeout after 30000ms` - `could not connect to database server at 10.0.2.45:5432` - `database server unreachable` #### Troubleshooting Steps ```bash # Test network connectivity telnet cloudflow-db.internal.company.com 5432 # Check DNS resolution nslookup cloudflow-db.internal.company.com # Verify database is accepting connections pg_isready -h cloudflow-db.internal.company.com -p 5432 -U cloudflow # Check firewall rules sudo iptables -L -n | grep 5432 # Test from CloudFlow pod network kubectl run -n cloudflow debug-pod --rm -i --tty \\ --image=postgres:14 -- \\ psql -h cloudflow-db.internal.company.com -U cloudflow -d cloudflow_production # Review database logs for connection rejections kubectl logs -n cloudflow statefulset/cloudflow-db --tail=100 | \\ grep -i \"connection\\|reject\\|authentication\" ``` ### Maximum Connection Limit (100) Reached This is a hard limit in CloudFlow's database tier. #### Permanent Solutions **Option 1: Upgrade database tier** ```bash # Check available tiers cloudflow db tiers list # Upgrade to higher tier (supports 200 connections) cloudflow db upgrade --tier standard-plus --confirm # Monitor migration progress cloudflow db migration status ``` **Option 2: Implement aggressive connection reuse** ```bash # Reduce connection lifetime cloudflow config set db.pool.max_lifetime 600 # 10 minutes # Enable prepared statement caching cloudflow config set db.prepared_statements.cache true cloudflow config set db.prepared_statements.max_size 250 # Reduce idle connection timeout cloudflow config set db.pool.idle_timeout 120 # 2 minutes ``` --- ## Workflow Execution Failures ### Timeout Errors (3600 second limit) #### Error Message ``` WorkflowExecutionError: Workflow exceeded maximum execution time of 3600 seconds Status: TIMEOUT Workflow ID: wf_9k2n4m8p1q Execution ID: exec_7h3j6k9m2n ``` #### Analysis ```bash # Get workflow execution details cloudflow workflows executions get exec_7h3j6k9m2n --verbose # Check step-by-step breakdown cloudflow workflows executions steps exec_7h3j6k9m2n # Sample output: # Step 1: data_ingestion - Duration: 245s - Status: SUCCESS # Step 2: data_validation - Duration: 123s - Status: SUCCESS # Step 3: data_transformation - Duration: 3189s - Status: TIMEOUT # Step 4: data_export - Duration: 0s - Status: SKIPPED # Identify bottleneck step cloudflow workflows analyze",
              "score": null
            },
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_4",
              "content": "### Error Response Format ```json { \"error\": { \"code\": \"invalid_parameter\", \"message\": \"The 'limit' parameter must be between 1 and 100\", \"field\": \"limit\", \"request_id\": \"req_8k3m9x2p\" } } ``` ### HTTP Status Codes | Status Code | Description | Common Causes | |------------|-------------|---------------| | 400 | Bad Request | Invalid parameters, malformed JSON, validation errors | | 401 | Unauthorized | Missing or invalid authentication credentials | | 403 | Forbidden | Insufficient permissions for requested resource | | 404 | Not Found | Resource does not exist or has been deleted | | 429 | Too Many Requests | Rate limit exceeded, retry after specified period | | 500 | Internal Server Error | Unexpected server error, contact support if persists | | 502 | Bad Gateway | Temporary service issue, retry with exponential backoff | | 503 | Service Unavailable | Scheduled maintenance or temporary outage | ### Error Codes CloudFlow returns specific error codes to help you identify and resolve issues: - `invalid_parameter`: One or more request parameters are invalid - `missing_required_field`: Required field is missing from request body - `authentication_failed`: Invalid API key or token - `insufficient_permissions`: User lacks required scope or permission - `resource_not_found`: Requested resource does not exist - `rate_limit_exceeded`: Too many requests, see rate limiting section - `workflow_execution_failed`: Workflow execution encountered an error - `invalid_json`: Request body contains malformed JSON - `duplicate_resource`: Resource with same identifier already exists - `quota_exceeded`: Account quota limit reached ### Error Handling Best Practices **Python Example:** ```python import requests import time def make_api_request(url, headers, max_retries=3): for attempt in range(max_retries): try: response = requests.get(url, headers=headers) if response.status_code == 200: return response.json() elif response.status_code == 429: # Rate limit exceeded retry_after = int(response.headers.get('X-RateLimit-Reset', 60)) print(f\"Rate limited. Waiting {retry_after} seconds...\") time.sleep(retry_after) continue elif response.status_code >= 500: # Server error - retry with exponential backoff wait_time = 2 ** attempt print(f\"Server error. Retrying in {wait_time} seconds...\") time.sleep(wait_time) continue else: # Client error - don't retry error_data = response.json() raise Exception(f\"API Error: {error_data['error']['message']}\") except requests.exceptions.RequestException as e: print(f\"Request failed: {e}\") if attempt < max_retries - 1: time.sleep(2 ** attempt) else: raise raise Exception(\"Max retries exceeded\") ``` ## Webhooks CloudFlow can send webhook notifications when specific events occur in your workflows.",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_0",
              "content": "# CloudFlow Platform Troubleshooting Guide **Version:** 3.2.1 **Last Updated:** January 2026 **Audience:** Platform Engineers, DevOps, Support Teams ## Table of Contents 1. [Overview](#overview) 2. [Authentication & Authorization Issues](#authentication--authorization-issues) 3. [Performance Problems](#performance-problems) 4. [Database Connection Issues](#database-connection-issues) 5. [Workflow Execution Failures](#workflow-execution-failures) 6. [Rate Limiting & Throttling](#rate-limiting--throttling) 7. [Log Analysis & Debugging](#log-analysis--debugging) 8. [Escalation Procedures](#escalation-procedures) --- ## Overview This guide provides comprehensive troubleshooting steps for common CloudFlow platform issues encountered in production environments. Each section includes error symptoms, root cause analysis, resolution steps, and preventive measures. ### Quick Diagnostic Checklist Before diving into specific issues, perform these initial checks: - Verify service health: `cloudflow status --all` - Check API connectivity: `curl -I https://api.cloudflow.io/health` - Review recent deployments: `kubectl get deployments -n cloudflow --sort-by=.metadata.creationTimestamp` - Inspect platform metrics: `cloudflow metrics --last 1h` --- ## Authentication & Authorization Issues ### 401 Unauthorized Errors #### Symptoms - API requests return `401 Unauthorized` - Error message: `Authentication credentials were not provided or are invalid` - Frontend displays \"Session expired\" message #### Common Causes **1. Token Expiration** CloudFlow access tokens expire after 3600 seconds (1 hour) by default. Refresh tokens are valid for 30 days. **Verification:** ```bash # Decode JWT to check expiration echo $CF_ACCESS_TOKEN | cut -d'.' -f2 | base64 -d | jq '.exp' # Compare with current time date +%s ``` **Resolution:** ```bash # Refresh the access token curl -X POST https://api.cloudflow.io/auth/refresh \\ -H \"Content-Type: application/json\" \\ -d '{\"refresh_token\": \"'$CF_REFRESH_TOKEN'\"}' # Update environment variable export CF_ACCESS_TOKEN=\"<new_token>\" ``` **2. Invalid JWT Signature** Error: `JWT signature verification failed` **Causes:** - Token was modified or corrupted - Using wrong signing key - Token generated with different secret **Resolution:** ```bash # Validate token structure cloudflow auth validate --token $CF_ACCESS_TOKEN # Generate new token cloudflow auth login --username <user> --password <pass> # For service accounts cloudflow auth service-account --create --name <sa-name> --scopes \"workflow:read,workflow:write\" ``` **3. Clock Skew Issues** JWT validation fails when system clocks are out of sync (tolerance: \u00b1300 seconds).",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_3",
              "content": "Workflow Context Accumulation** Large workflow executions may accumulate state in memory. **Solution:** ```bash # Configure context cleanup cloudflow config set workflow.context.max_size_mb 100 cloudflow config set workflow.context.cleanup_threshold 0.8 # Enable context persistence to disk cloudflow config set workflow.context.persistence.enabled true cloudflow config set workflow.context.persistence.backend s3 ``` **2. Connection Pool Leaks** **Diagnosis:** ```bash # Check active connections cloudflow db connections --verbose # Expected output: # Active: 45/100 # Idle: 23 # Waiting: 2 # Average age: 245s ``` **Resolution:** ```bash # Adjust connection pool settings cloudflow config set db.pool.max_connections 100 cloudflow config set db.pool.min_connections 10 cloudflow config set db.pool.idle_timeout 300 cloudflow config set db.pool.max_lifetime 1800 # Force connection pool reset cloudflow db pool reset --confirm ``` **3. Event Stream Buffers** Unbounded event buffers can cause memory exhaustion. ```bash # Configure event buffer limits cloudflow config set events.buffer.max_size 10000 cloudflow config set events.buffer.overflow_strategy drop_oldest # Enable event streaming to external sink cloudflow config set events.sink.type kafka cloudflow config set events.sink.kafka.brokers \"kafka-1:9092,kafka-2:9092\" cloudflow config set events.sink.kafka.topic cloudflow-events ``` --- ## Database Connection Issues ### Connection Pool Exhaustion #### Symptoms - Error: `could not obtain connection from pool within 5000ms` - Error: `connection pool exhausted (100/100 connections in use)` - API requests fail with `503 Service Unavailable` - Database CPU usage normal, but connection count at maximum #### Investigation ```bash # Check current connection pool status cloudflow db pool status --detailed # Output example: # Pool Statistics: # Total Connections: 100/100 (100%) # Active: 87 # Idle: 13 # Waiting Requests: 45 # Average Wait Time: 3420ms # Max Wait Time: 8234ms # Identify long-running queries cloudflow db queries --status running --min-duration 30000 # Check connection distribution by client SELECT application_name, state, COUNT(*) as conn_count, AVG(EXTRACT(EPOCH FROM (NOW() - state_change))) as avg_duration_sec FROM pg_stat_activity WHERE datname = 'cloudflow_production' GROUP BY application_name, state ORDER BY conn_count DESC; ``` #### Resolution **Immediate mitigation:** ```bash # Temporarily increase connection limit (requires database restart) cloudflow db config set max_connections 150 # Kill idle connections older than 5 minutes cloudflow db connections kill --idle-timeout 300 # Restart connection pool without downtime kubectl rollout restart deployment/cloudflow-api -n cloudflow kubectl rollout status deployment/cloudflow-api -n cloudflow ``` **Long-term solutions:** 1.",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_1",
              "content": "**Diagnosis:** ```bash # Check system time timedatectl status # Compare with NTP server ntpdate -q pool.ntp.org # Check JWT issued time vs current time jwt_iat=$(echo $CF_ACCESS_TOKEN | cut -d'.' -f2 | base64 -d | jq '.iat') current_time=$(date +%s) skew=$((current_time - jwt_iat)) echo \"Clock skew: $skew seconds\" ``` **Resolution:** ```bash # Sync with NTP server sudo ntpdate -s pool.ntp.org # Enable automatic time sync sudo timedatectl set-ntp true # Restart CloudFlow client cloudflow restart ``` **4. Insufficient Permissions** Error: `User does not have required permissions for this operation` **Check user roles:** ```bash cloudflow auth whoami --verbose # Expected output: # User: john.doe@company.com # Roles: developer, workflow-admin # Scopes: workflow:*, data:read, metrics:read ``` **Request permission elevation:** ```bash # Submit access request cloudflow auth request-access \\ --resource \"workflow:production:deploy\" \\ --justification \"Deploy critical hotfix for TICKET-1234\" # Check pending approvals cloudflow auth list-requests --status pending ``` ### 403 Forbidden Errors #### RBAC Policy Violations CloudFlow uses role-based access control (RBAC) with the following hierarchy: - `viewer` - Read-only access - `developer` - Create and modify workflows (non-production) - `operator` - Execute workflows, view logs - `admin` - Full access to workspace - `platform-admin` - Cross-workspace administration **Verify resource permissions:** ```bash # Check effective permissions cloudflow rbac check \\ --user john.doe@company.com \\ --resource workflow:prod-pipeline \\ --action execute # List all policies affecting user cloudflow rbac policies --user john.doe@company.com --verbose ``` --- ## Performance Problems ### Slow Query Performance #### Symptoms - API response times > 5000ms - Database query latency warnings in logs - CloudFlow UI becomes unresponsive - Timeout errors: `Request exceeded maximum duration of 30000ms` #### Diagnosis Steps **1. Identify Slow Queries** ```bash # Query CloudFlow metrics cloudflow metrics query --metric api_request_duration_ms \\ --filter \"p95 > 5000\" \\ --last 1h # Check database slow query log kubectl logs -n cloudflow deploy/cloudflow-db-primary | \\ grep \"slow query\" | \\ tail -n 50 # Analyze query patterns cloudflow db analyze-queries --min-duration 5000 --limit 20 ``` **2.",
              "score": null
            }
          ],
          "latency_ms": 13.07003799956874,
          "query_id": "realistic_015",
          "dimension": "problem",
          "query": "Getting 'could not obtain connection from pool' errors"
        },
        {
          "key_facts": [
            "could not obtain connection from pool within 5000ms",
            "connection pool exhausted (100/100 connections in use)",
            "PgBouncer",
            "max_db_connections = 100"
          ],
          "found_facts": [
            "could not obtain connection from pool within 5000ms",
            "connection pool exhausted (100/100 connections in use)",
            "PgBouncer"
          ],
          "missed_facts": [
            "max_db_connections = 100"
          ],
          "coverage": 0.75,
          "expected_docs": [
            "troubleshooting_guide",
            "deployment_guide"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_3",
              "content": "Workflow Context Accumulation** Large workflow executions may accumulate state in memory. **Solution:** ```bash # Configure context cleanup cloudflow config set workflow.context.max_size_mb 100 cloudflow config set workflow.context.cleanup_threshold 0.8 # Enable context persistence to disk cloudflow config set workflow.context.persistence.enabled true cloudflow config set workflow.context.persistence.backend s3 ``` **2. Connection Pool Leaks** **Diagnosis:** ```bash # Check active connections cloudflow db connections --verbose # Expected output: # Active: 45/100 # Idle: 23 # Waiting: 2 # Average age: 245s ``` **Resolution:** ```bash # Adjust connection pool settings cloudflow config set db.pool.max_connections 100 cloudflow config set db.pool.min_connections 10 cloudflow config set db.pool.idle_timeout 300 cloudflow config set db.pool.max_lifetime 1800 # Force connection pool reset cloudflow db pool reset --confirm ``` **3. Event Stream Buffers** Unbounded event buffers can cause memory exhaustion. ```bash # Configure event buffer limits cloudflow config set events.buffer.max_size 10000 cloudflow config set events.buffer.overflow_strategy drop_oldest # Enable event streaming to external sink cloudflow config set events.sink.type kafka cloudflow config set events.sink.kafka.brokers \"kafka-1:9092,kafka-2:9092\" cloudflow config set events.sink.kafka.topic cloudflow-events ``` --- ## Database Connection Issues ### Connection Pool Exhaustion #### Symptoms - Error: `could not obtain connection from pool within 5000ms` - Error: `connection pool exhausted (100/100 connections in use)` - API requests fail with `503 Service Unavailable` - Database CPU usage normal, but connection count at maximum #### Investigation ```bash # Check current connection pool status cloudflow db pool status --detailed # Output example: # Pool Statistics: # Total Connections: 100/100 (100%) # Active: 87 # Idle: 13 # Waiting Requests: 45 # Average Wait Time: 3420ms # Max Wait Time: 8234ms # Identify long-running queries cloudflow db queries --status running --min-duration 30000 # Check connection distribution by client SELECT application_name, state, COUNT(*) as conn_count, AVG(EXTRACT(EPOCH FROM (NOW() - state_change))) as avg_duration_sec FROM pg_stat_activity WHERE datname = 'cloudflow_production' GROUP BY application_name, state ORDER BY conn_count DESC; ``` #### Resolution **Immediate mitigation:** ```bash # Temporarily increase connection limit (requires database restart) cloudflow db config set max_connections 150 # Kill idle connections older than 5 minutes cloudflow db connections kill --idle-timeout 300 # Restart connection pool without downtime kubectl rollout restart deployment/cloudflow-api -n cloudflow kubectl rollout status deployment/cloudflow-api -n cloudflow ``` **Long-term solutions:** 1.",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_4",
              "content": "**Implement connection pooling optimization:** ```bash # Use PgBouncer for connection pooling kubectl apply -f cloudflow-pgbouncer.yaml # Configure CloudFlow to use PgBouncer cloudflow config set db.host pgbouncer.cloudflow.svc.cluster.local cloudflow config set db.port 6432 cloudflow config set db.pool.mode transaction ``` 2. **Add read replicas:** ```bash # Route read-only queries to replicas cloudflow db replicas add --count 2 cloudflow config set db.read_replicas \"replica-1:5432,replica-2:5432\" cloudflow config set db.read_write_split true ``` ### Connection Timeout Errors #### Error Messages - `connection timeout after 30000ms` - `could not connect to database server at 10.0.2.45:5432` - `database server unreachable` #### Troubleshooting Steps ```bash # Test network connectivity telnet cloudflow-db.internal.company.com 5432 # Check DNS resolution nslookup cloudflow-db.internal.company.com # Verify database is accepting connections pg_isready -h cloudflow-db.internal.company.com -p 5432 -U cloudflow # Check firewall rules sudo iptables -L -n | grep 5432 # Test from CloudFlow pod network kubectl run -n cloudflow debug-pod --rm -i --tty \\ --image=postgres:14 -- \\ psql -h cloudflow-db.internal.company.com -U cloudflow -d cloudflow_production # Review database logs for connection rejections kubectl logs -n cloudflow statefulset/cloudflow-db --tail=100 | \\ grep -i \"connection\\|reject\\|authentication\" ``` ### Maximum Connection Limit (100) Reached This is a hard limit in CloudFlow's database tier. #### Permanent Solutions **Option 1: Upgrade database tier** ```bash # Check available tiers cloudflow db tiers list # Upgrade to higher tier (supports 200 connections) cloudflow db upgrade --tier standard-plus --confirm # Monitor migration progress cloudflow db migration status ``` **Option 2: Implement aggressive connection reuse** ```bash # Reduce connection lifetime cloudflow config set db.pool.max_lifetime 600 # 10 minutes # Enable prepared statement caching cloudflow config set db.prepared_statements.cache true cloudflow config set db.prepared_statements.max_size 250 # Reduce idle connection timeout cloudflow config set db.pool.idle_timeout 120 # 2 minutes ``` --- ## Workflow Execution Failures ### Timeout Errors (3600 second limit) #### Error Message ``` WorkflowExecutionError: Workflow exceeded maximum execution time of 3600 seconds Status: TIMEOUT Workflow ID: wf_9k2n4m8p1q Execution ID: exec_7h3j6k9m2n ``` #### Analysis ```bash # Get workflow execution details cloudflow workflows executions get exec_7h3j6k9m2n --verbose # Check step-by-step breakdown cloudflow workflows executions steps exec_7h3j6k9m2n # Sample output: # Step 1: data_ingestion - Duration: 245s - Status: SUCCESS # Step 2: data_validation - Duration: 123s - Status: SUCCESS # Step 3: data_transformation - Duration: 3189s - Status: TIMEOUT # Step 4: data_export - Duration: 0s - Status: SKIPPED # Identify bottleneck step cloudflow workflows analyze",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_12",
              "content": "Resume normal operations - Recovery time: 1-3 hours depending on data volume - Data loss: None if corruption detected quickly ### Testing & Validation **DR Drill Schedule**: - Monthly: Automated failover test (single AZ failure simulation) - Quarterly: Full DR region failover (non-production hours) - Annually: Complete disaster simulation with all stakeholders **Last DR Test Results** (Dec 15, 2025): - Scenario: Full region failover - Actual RTO: 2 hours 23 minutes (target: 4 hours) - Actual RPO: 42 minutes (target: 1 hour) - Issues identified: DNS propagation slower than expected (resolved) - Success criteria: Met all recovery objectives ### Business Continuity **Communication Plan**: - Status page: status.cloudflow.com (updated every 15 minutes during incident) - Customer notifications: Email + SMS for all P1 incidents - Internal escalation: PagerDuty \u2192 Incident Commander \u2192 Engineering Manager \u2192 CTO **Data Retention Policy**: - Active data: 12 months in hot storage (PostgreSQL) - Archived data: 7 years in cold storage (S3 Glacier) - Audit logs: 7 years (compliance requirement) - Backup retention: 30 days standard, 90 days monthly snapshots --- ## Appendix ### Service Dependency Matrix | Service | Depends On | Critical Path | |---------|-----------|---------------| | API Gateway | Auth Service, Redis | Yes | | Auth Service | PostgreSQL, Redis, Vault | Yes | | Workflow Engine | PostgreSQL, Kafka, Redis | Yes | | Scheduler | PostgreSQL, Redis, Kafka | No | | Notification Service | Kafka, PostgreSQL, SendGrid, Twilio | No | ### Capacity Planning **Current Capacity** (Jan 2026): - Daily workflow executions: 2.5M - Active users: 150,000 - API requests per day: 50M - Database size: 1.2TB - Kafka throughput: 100K msg/sec peak **6-Month Projection** (Jul 2026): - Daily workflow executions: 4M (+60%) - Active users: 225,000 (+50%) - API requests per day: 80M (+60%) - Database size: 2TB (+67%) - Required scaling: +30% compute, +50% storage ### Contact Information - **Architecture Team**: architecture@cloudflow.internal - **On-Call Engineer**: PagerDuty escalation - **Security Team**: security@cloudflow.internal - **Documentation**: https://wiki.cloudflow.internal/architecture --- **Document Revision History**: - v2.3.1 (Jan 15, 2026): Updated performance metrics, added DR test results - v2.3.0 (Dec 1, 2025): Added multi-AZ deployment details - v2.2.0 (Oct 15, 2025): Security architecture overhaul - v2.1.0 (Sep 1, 2025): Initial Kafka migration documentation - v2.0.0 (Jul 1, 2025): Complete microservices rewrite",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_4",
              "content": "**Technology**: Node.js with worker pool pattern **Replicas**: 8 pods (production), auto-scaling 6-16 **Resource Allocation**: 2 vCPU, 4GB RAM per pod **Key Responsibilities**: - Consume notification events from Kafka - Template rendering with Handlebars (cached in Redis) - Multi-channel delivery (Email via SendGrid, SMS via Twilio, Webhooks) - Retry logic with dead letter queue for failed deliveries - Delivery status tracking and analytics - User preference management (opt-in/opt-out) **Notification Flow**: ``` Event Source \u2192 Kafka (notifications.email topic) \u2502 \u25bc Notification Service Consumer \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u25bc \u25bc \u25bc [Email Queue] [SMS Queue] [Webhook Queue] \u2502 \u2502 \u2502 \u25bc \u25bc \u25bc [SendGrid] [Twilio] [HTTP POST] \u2502 \u2502 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u25bc Delivery Status \u2192 PostgreSQL ``` **Channel Configuration**: - Email: SendGrid with IP warming, DKIM/SPF configured - SMS: Twilio with fallback to AWS SNS - Webhook: Signed payloads (HMAC-SHA256), retry 3 times with exponential backoff - In-app: WebSocket connections via Socket.io **Template Management**: - Templates stored in PostgreSQL with versioning - Compiled templates cached in Redis (1-hour TTL) - Support for localization (i18n) with 12 languages - A/B testing capability for email campaigns **Performance Targets**: - Email delivery: < 5 seconds (P99) from event to SendGrid - SMS delivery: < 3 seconds (P99) - Throughput: 10,000 notifications per minute --- ## Data Flow Architecture ### Synchronous Request Flow Client requests follow a synchronous path through the API Gateway to backend services: ``` 1. Client \u2192 Load Balancer (TLS termination) 2. Load Balancer \u2192 API Gateway (HTTP/2) 3. API Gateway \u2192 Auth Service (JWT validation via gRPC) 4. API Gateway \u2192 Target Service (HTTP/REST or gRPC) 5. Target Service \u2192 Database/Cache (data retrieval) 6. Response propagates back through the chain ``` **Timeout Configuration**: - Client \u2192 Load Balancer: 60 seconds - Load Balancer \u2192 API Gateway: 55 seconds - API Gateway \u2192 Services: 45 seconds - Service \u2192 Database: 10 seconds ### Asynchronous Event Flow Long-running operations and inter-service communication use event-driven patterns: ``` 1. Service publishes event \u2192 Kafka topic 2. Kafka persists event (replication factor: 3) 3. Consumer groups subscribe to topics 4. Consumers process events with at-least-once delivery 5. State updates written to PostgreSQL 6.",
              "score": null
            },
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_6",
              "content": "**Verify Service Health**: ```bash kubectl get pods -n cloudflow-prod curl https://api.cloudflow.io/health ``` **Recovery Time Objective (RTO)**: 4 hours **Recovery Point Objective (RPO)**: 24 hours --- ## Scaling and Performance ### Horizontal Pod Autoscaling CloudFlow is configured with Horizontal Pod Autoscaler (HPA) to automatically scale based on resource utilization: ```yaml # hpa.yaml apiVersion: autoscaling/v2 kind: HorizontalPodAutoscaler metadata: name: cloudflow-api-hpa namespace: cloudflow-prod spec: scaleTargetRef: apiVersion: apps/v1 kind: Deployment name: cloudflow-api minReplicas: 3 maxReplicas: 10 metrics: - type: Resource resource: name: cpu target: type: Utilization averageUtilization: 70 - type: Resource resource: name: memory target: type: Utilization averageUtilization: 80 behavior: scaleDown: stabilizationWindowSeconds: 300 policies: - type: Percent value: 50 periodSeconds: 60 scaleUp: stabilizationWindowSeconds: 60 policies: - type: Percent value: 100 periodSeconds: 30 - type: Pods value: 2 periodSeconds: 30 selectPolicy: Max ``` Monitor autoscaling events: ```bash kubectl get hpa -n cloudflow-prod -w kubectl describe hpa cloudflow-api-hpa -n cloudflow-prod ``` ### Performance Optimization #### Connection Pooling Ensure optimal database connection pooling settings in PgBouncer: - **Pool Mode**: Transaction (optimal for microservices) - **Default Pool Size**: 25 connections per user - **Max DB Connections**: 100 (matches PostgreSQL `max_connections`) #### Redis Caching Implement Redis caching for frequently accessed data: ```javascript // Example caching strategy const CACHE_TTL = { workflows: 300, // 5 minutes userSessions: 3600, // 1 hour apiResults: 60 // 1 minute }; ``` #### Load Testing Perform regular load testing to validate scaling configuration: ```bash # Install k6 for load testing kubectl apply -f - <<EOF apiVersion: v1 kind: ConfigMap metadata: name: loadtest-script namespace: cloudflow-prod data: script.js: | import http from 'k6/http'; import { check, sleep } from 'k6'; export let options = { stages: [ { duration: '2m', target: 100 }, { duration: '5m', target: 100 }, { duration: '2m', target: 200 }, { duration: '5m', target: 200 }, { duration: '2m', target: 0 }, ], thresholds: { http_req_duration: ['p(95)<500'], }, }; export default function () { let res = http.get('https://api.cloudflow.io/api/workflows'); check(res, { 'status is 200': (r) => r.status === 200 }); sleep(1); } EOF ``` **Performance Targets**: - P95 API latency: < 500ms - P99 API latency: < 1000ms - Error rate: < 0.1% - Throughput: > 1000 requests/second --- ## Security and Compliance ### Network Policies Implement network policies to restrict pod-to-pod communication: ```yaml # network-policy.yaml apiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: cloudflow-network-policy namespace: cloudflow-prod spec:",
              "score": null
            }
          ],
          "latency_ms": 12.75969900234486,
          "query_id": "realistic_015",
          "dimension": "casual",
          "query": "connection pool full"
        },
        {
          "key_facts": [
            "could not obtain connection from pool within 5000ms",
            "connection pool exhausted (100/100 connections in use)",
            "PgBouncer",
            "max_db_connections = 100"
          ],
          "found_facts": [
            "could not obtain connection from pool within 5000ms",
            "connection pool exhausted (100/100 connections in use)",
            "PgBouncer"
          ],
          "missed_facts": [
            "max_db_connections = 100"
          ],
          "coverage": 0.75,
          "expected_docs": [
            "troubleshooting_guide",
            "deployment_guide"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_3",
              "content": "Workflow Context Accumulation** Large workflow executions may accumulate state in memory. **Solution:** ```bash # Configure context cleanup cloudflow config set workflow.context.max_size_mb 100 cloudflow config set workflow.context.cleanup_threshold 0.8 # Enable context persistence to disk cloudflow config set workflow.context.persistence.enabled true cloudflow config set workflow.context.persistence.backend s3 ``` **2. Connection Pool Leaks** **Diagnosis:** ```bash # Check active connections cloudflow db connections --verbose # Expected output: # Active: 45/100 # Idle: 23 # Waiting: 2 # Average age: 245s ``` **Resolution:** ```bash # Adjust connection pool settings cloudflow config set db.pool.max_connections 100 cloudflow config set db.pool.min_connections 10 cloudflow config set db.pool.idle_timeout 300 cloudflow config set db.pool.max_lifetime 1800 # Force connection pool reset cloudflow db pool reset --confirm ``` **3. Event Stream Buffers** Unbounded event buffers can cause memory exhaustion. ```bash # Configure event buffer limits cloudflow config set events.buffer.max_size 10000 cloudflow config set events.buffer.overflow_strategy drop_oldest # Enable event streaming to external sink cloudflow config set events.sink.type kafka cloudflow config set events.sink.kafka.brokers \"kafka-1:9092,kafka-2:9092\" cloudflow config set events.sink.kafka.topic cloudflow-events ``` --- ## Database Connection Issues ### Connection Pool Exhaustion #### Symptoms - Error: `could not obtain connection from pool within 5000ms` - Error: `connection pool exhausted (100/100 connections in use)` - API requests fail with `503 Service Unavailable` - Database CPU usage normal, but connection count at maximum #### Investigation ```bash # Check current connection pool status cloudflow db pool status --detailed # Output example: # Pool Statistics: # Total Connections: 100/100 (100%) # Active: 87 # Idle: 13 # Waiting Requests: 45 # Average Wait Time: 3420ms # Max Wait Time: 8234ms # Identify long-running queries cloudflow db queries --status running --min-duration 30000 # Check connection distribution by client SELECT application_name, state, COUNT(*) as conn_count, AVG(EXTRACT(EPOCH FROM (NOW() - state_change))) as avg_duration_sec FROM pg_stat_activity WHERE datname = 'cloudflow_production' GROUP BY application_name, state ORDER BY conn_count DESC; ``` #### Resolution **Immediate mitigation:** ```bash # Temporarily increase connection limit (requires database restart) cloudflow db config set max_connections 150 # Kill idle connections older than 5 minutes cloudflow db connections kill --idle-timeout 300 # Restart connection pool without downtime kubectl rollout restart deployment/cloudflow-api -n cloudflow kubectl rollout status deployment/cloudflow-api -n cloudflow ``` **Long-term solutions:** 1.",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_4",
              "content": "**Implement connection pooling optimization:** ```bash # Use PgBouncer for connection pooling kubectl apply -f cloudflow-pgbouncer.yaml # Configure CloudFlow to use PgBouncer cloudflow config set db.host pgbouncer.cloudflow.svc.cluster.local cloudflow config set db.port 6432 cloudflow config set db.pool.mode transaction ``` 2. **Add read replicas:** ```bash # Route read-only queries to replicas cloudflow db replicas add --count 2 cloudflow config set db.read_replicas \"replica-1:5432,replica-2:5432\" cloudflow config set db.read_write_split true ``` ### Connection Timeout Errors #### Error Messages - `connection timeout after 30000ms` - `could not connect to database server at 10.0.2.45:5432` - `database server unreachable` #### Troubleshooting Steps ```bash # Test network connectivity telnet cloudflow-db.internal.company.com 5432 # Check DNS resolution nslookup cloudflow-db.internal.company.com # Verify database is accepting connections pg_isready -h cloudflow-db.internal.company.com -p 5432 -U cloudflow # Check firewall rules sudo iptables -L -n | grep 5432 # Test from CloudFlow pod network kubectl run -n cloudflow debug-pod --rm -i --tty \\ --image=postgres:14 -- \\ psql -h cloudflow-db.internal.company.com -U cloudflow -d cloudflow_production # Review database logs for connection rejections kubectl logs -n cloudflow statefulset/cloudflow-db --tail=100 | \\ grep -i \"connection\\|reject\\|authentication\" ``` ### Maximum Connection Limit (100) Reached This is a hard limit in CloudFlow's database tier. #### Permanent Solutions **Option 1: Upgrade database tier** ```bash # Check available tiers cloudflow db tiers list # Upgrade to higher tier (supports 200 connections) cloudflow db upgrade --tier standard-plus --confirm # Monitor migration progress cloudflow db migration status ``` **Option 2: Implement aggressive connection reuse** ```bash # Reduce connection lifetime cloudflow config set db.pool.max_lifetime 600 # 10 minutes # Enable prepared statement caching cloudflow config set db.prepared_statements.cache true cloudflow config set db.prepared_statements.max_size 250 # Reduce idle connection timeout cloudflow config set db.pool.idle_timeout 120 # 2 minutes ``` --- ## Workflow Execution Failures ### Timeout Errors (3600 second limit) #### Error Message ``` WorkflowExecutionError: Workflow exceeded maximum execution time of 3600 seconds Status: TIMEOUT Workflow ID: wf_9k2n4m8p1q Execution ID: exec_7h3j6k9m2n ``` #### Analysis ```bash # Get workflow execution details cloudflow workflows executions get exec_7h3j6k9m2n --verbose # Check step-by-step breakdown cloudflow workflows executions steps exec_7h3j6k9m2n # Sample output: # Step 1: data_ingestion - Duration: 245s - Status: SUCCESS # Step 2: data_validation - Duration: 123s - Status: SUCCESS # Step 3: data_transformation - Duration: 3189s - Status: TIMEOUT # Step 4: data_export - Duration: 0s - Status: SKIPPED # Identify bottleneck step cloudflow workflows analyze",
              "score": null
            },
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_4",
              "content": "### Error Response Format ```json { \"error\": { \"code\": \"invalid_parameter\", \"message\": \"The 'limit' parameter must be between 1 and 100\", \"field\": \"limit\", \"request_id\": \"req_8k3m9x2p\" } } ``` ### HTTP Status Codes | Status Code | Description | Common Causes | |------------|-------------|---------------| | 400 | Bad Request | Invalid parameters, malformed JSON, validation errors | | 401 | Unauthorized | Missing or invalid authentication credentials | | 403 | Forbidden | Insufficient permissions for requested resource | | 404 | Not Found | Resource does not exist or has been deleted | | 429 | Too Many Requests | Rate limit exceeded, retry after specified period | | 500 | Internal Server Error | Unexpected server error, contact support if persists | | 502 | Bad Gateway | Temporary service issue, retry with exponential backoff | | 503 | Service Unavailable | Scheduled maintenance or temporary outage | ### Error Codes CloudFlow returns specific error codes to help you identify and resolve issues: - `invalid_parameter`: One or more request parameters are invalid - `missing_required_field`: Required field is missing from request body - `authentication_failed`: Invalid API key or token - `insufficient_permissions`: User lacks required scope or permission - `resource_not_found`: Requested resource does not exist - `rate_limit_exceeded`: Too many requests, see rate limiting section - `workflow_execution_failed`: Workflow execution encountered an error - `invalid_json`: Request body contains malformed JSON - `duplicate_resource`: Resource with same identifier already exists - `quota_exceeded`: Account quota limit reached ### Error Handling Best Practices **Python Example:** ```python import requests import time def make_api_request(url, headers, max_retries=3): for attempt in range(max_retries): try: response = requests.get(url, headers=headers) if response.status_code == 200: return response.json() elif response.status_code == 429: # Rate limit exceeded retry_after = int(response.headers.get('X-RateLimit-Reset', 60)) print(f\"Rate limited. Waiting {retry_after} seconds...\") time.sleep(retry_after) continue elif response.status_code >= 500: # Server error - retry with exponential backoff wait_time = 2 ** attempt print(f\"Server error. Retrying in {wait_time} seconds...\") time.sleep(wait_time) continue else: # Client error - don't retry error_data = response.json() raise Exception(f\"API Error: {error_data['error']['message']}\") except requests.exceptions.RequestException as e: print(f\"Request failed: {e}\") if attempt < max_retries - 1: time.sleep(2 ** attempt) else: raise raise Exception(\"Max retries exceeded\") ``` ## Webhooks CloudFlow can send webhook notifications when specific events occur in your workflows.",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_0",
              "content": "# CloudFlow Platform Troubleshooting Guide **Version:** 3.2.1 **Last Updated:** January 2026 **Audience:** Platform Engineers, DevOps, Support Teams ## Table of Contents 1. [Overview](#overview) 2. [Authentication & Authorization Issues](#authentication--authorization-issues) 3. [Performance Problems](#performance-problems) 4. [Database Connection Issues](#database-connection-issues) 5. [Workflow Execution Failures](#workflow-execution-failures) 6. [Rate Limiting & Throttling](#rate-limiting--throttling) 7. [Log Analysis & Debugging](#log-analysis--debugging) 8. [Escalation Procedures](#escalation-procedures) --- ## Overview This guide provides comprehensive troubleshooting steps for common CloudFlow platform issues encountered in production environments. Each section includes error symptoms, root cause analysis, resolution steps, and preventive measures. ### Quick Diagnostic Checklist Before diving into specific issues, perform these initial checks: - Verify service health: `cloudflow status --all` - Check API connectivity: `curl -I https://api.cloudflow.io/health` - Review recent deployments: `kubectl get deployments -n cloudflow --sort-by=.metadata.creationTimestamp` - Inspect platform metrics: `cloudflow metrics --last 1h` --- ## Authentication & Authorization Issues ### 401 Unauthorized Errors #### Symptoms - API requests return `401 Unauthorized` - Error message: `Authentication credentials were not provided or are invalid` - Frontend displays \"Session expired\" message #### Common Causes **1. Token Expiration** CloudFlow access tokens expire after 3600 seconds (1 hour) by default. Refresh tokens are valid for 30 days. **Verification:** ```bash # Decode JWT to check expiration echo $CF_ACCESS_TOKEN | cut -d'.' -f2 | base64 -d | jq '.exp' # Compare with current time date +%s ``` **Resolution:** ```bash # Refresh the access token curl -X POST https://api.cloudflow.io/auth/refresh \\ -H \"Content-Type: application/json\" \\ -d '{\"refresh_token\": \"'$CF_REFRESH_TOKEN'\"}' # Update environment variable export CF_ACCESS_TOKEN=\"<new_token>\" ``` **2. Invalid JWT Signature** Error: `JWT signature verification failed` **Causes:** - Token was modified or corrupted - Using wrong signing key - Token generated with different secret **Resolution:** ```bash # Validate token structure cloudflow auth validate --token $CF_ACCESS_TOKEN # Generate new token cloudflow auth login --username <user> --password <pass> # For service accounts cloudflow auth service-account --create --name <sa-name> --scopes \"workflow:read,workflow:write\" ``` **3. Clock Skew Issues** JWT validation fails when system clocks are out of sync (tolerance: \u00b1300 seconds).",
              "score": null
            },
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_7",
              "content": "podSelector: matchLabels: app: cloudflow policyTypes: - Ingress - Egress ingress: - from: - namespaceSelector: matchLabels: name: cloudflow-prod - podSelector: matchLabels: app: nginx-ingress ports: - protocol: TCP port: 3000 egress: - to: - podSelector: matchLabels: app: postgresql ports: - protocol: TCP port: 5432 - to: - podSelector: matchLabels: app: redis ports: - protocol: TCP port: 6379 ``` ### Pod Security Standards Enforce Pod Security Standards at the namespace level: ```bash kubectl label namespace cloudflow-prod \\ pod-security.kubernetes.io/enforce=restricted \\ pod-security.kubernetes.io/audit=restricted \\ pod-security.kubernetes.io/warn=restricted ``` ### Secrets Management Use AWS Secrets Manager for sensitive credentials: ```bash # Install External Secrets Operator helm install external-secrets external-secrets/external-secrets \\ --namespace external-secrets-system \\ --create-namespace # Create SecretStore kubectl apply -f - <<EOF apiVersion: external-secrets.io/v1beta1 kind: SecretStore metadata: name: aws-secrets-manager namespace: cloudflow-prod spec: provider: aws: service: SecretsManager region: us-east-1 auth: jwt: serviceAccountRef: name: external-secrets-sa EOF ``` --- ## Troubleshooting ### Common Issues #### Pods in CrashLoopBackOff **Symptoms**: Pods continuously restart **Diagnosis**: ```bash kubectl logs -n cloudflow-prod <pod-name> --previous kubectl describe pod -n cloudflow-prod <pod-name> ``` **Common Causes**: - Database connection failure - Invalid environment variables - Insufficient resources #### High Memory Usage **Symptoms**: Pods being OOMKilled **Diagnosis**: ```bash kubectl top pods -n cloudflow-prod ``` **Resolution**: - Increase memory limits in deployment - Check for memory leaks in application logs - Review heap dump for Node.js processes #### Database Connection Pool Exhausted **Symptoms**: \"Too many connections\" errors **Diagnosis**: ```bash # Check PgBouncer stats kubectl exec -it pgbouncer-xxx -n cloudflow-prod -- \\ psql -p 5432 -U pgbouncer pgbouncer -c \"SHOW POOLS;\" ``` **Resolution**: - Increase `max_db_connections` in PgBouncer - Optimize application connection usage - Add more PostgreSQL replicas ### Support Contacts - **On-call Engineer**: pagerduty.com/cloudflow-oncall - **Slack Channel**: #cloudflow-operations - **Documentation**: https://docs.cloudflow.io - **Status Page**: https://status.cloudflow.io --- ## Appendix ### Useful Commands ```bash # View all resources in namespace kubectl get all -n cloudflow-prod # Check resource usage kubectl top pods -n cloudflow-prod kubectl top nodes # View logs kubectl logs -f deployment/cloudflow-api -n cloudflow-prod # Execute command in pod kubectl exec -it <pod-name> -n cloudflow-prod -- /bin/sh # Port forward for local testing kubectl port-forward svc/cloudflow-api 8080:80 -n cloudflow-prod # Rollback deployment helm rollback cloudflow -n cloudflow-prod # Update deployment helm upgrade cloudflow cloudflow/cloudflow \\ --namespace cloudflow-prod \\ --values values-production.yaml ``` ### Maintenance Windows Scheduled maintenance occurs during the following windows: - **Primary**: Sunday 02:00-06:00",
              "score": null
            }
          ],
          "latency_ms": 13.198616998124635,
          "query_id": "realistic_015",
          "dimension": "contextual",
          "query": "My app is failing with connection pool errors, how do I troubleshoot?"
        },
        {
          "key_facts": [
            "could not obtain connection from pool within 5000ms",
            "connection pool exhausted (100/100 connections in use)",
            "PgBouncer",
            "max_db_connections = 100"
          ],
          "found_facts": [
            "could not obtain connection from pool within 5000ms",
            "connection pool exhausted (100/100 connections in use)",
            "PgBouncer"
          ],
          "missed_facts": [
            "max_db_connections = 100"
          ],
          "coverage": 0.75,
          "expected_docs": [
            "troubleshooting_guide",
            "deployment_guide"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_4",
              "content": "**Implement connection pooling optimization:** ```bash # Use PgBouncer for connection pooling kubectl apply -f cloudflow-pgbouncer.yaml # Configure CloudFlow to use PgBouncer cloudflow config set db.host pgbouncer.cloudflow.svc.cluster.local cloudflow config set db.port 6432 cloudflow config set db.pool.mode transaction ``` 2. **Add read replicas:** ```bash # Route read-only queries to replicas cloudflow db replicas add --count 2 cloudflow config set db.read_replicas \"replica-1:5432,replica-2:5432\" cloudflow config set db.read_write_split true ``` ### Connection Timeout Errors #### Error Messages - `connection timeout after 30000ms` - `could not connect to database server at 10.0.2.45:5432` - `database server unreachable` #### Troubleshooting Steps ```bash # Test network connectivity telnet cloudflow-db.internal.company.com 5432 # Check DNS resolution nslookup cloudflow-db.internal.company.com # Verify database is accepting connections pg_isready -h cloudflow-db.internal.company.com -p 5432 -U cloudflow # Check firewall rules sudo iptables -L -n | grep 5432 # Test from CloudFlow pod network kubectl run -n cloudflow debug-pod --rm -i --tty \\ --image=postgres:14 -- \\ psql -h cloudflow-db.internal.company.com -U cloudflow -d cloudflow_production # Review database logs for connection rejections kubectl logs -n cloudflow statefulset/cloudflow-db --tail=100 | \\ grep -i \"connection\\|reject\\|authentication\" ``` ### Maximum Connection Limit (100) Reached This is a hard limit in CloudFlow's database tier. #### Permanent Solutions **Option 1: Upgrade database tier** ```bash # Check available tiers cloudflow db tiers list # Upgrade to higher tier (supports 200 connections) cloudflow db upgrade --tier standard-plus --confirm # Monitor migration progress cloudflow db migration status ``` **Option 2: Implement aggressive connection reuse** ```bash # Reduce connection lifetime cloudflow config set db.pool.max_lifetime 600 # 10 minutes # Enable prepared statement caching cloudflow config set db.prepared_statements.cache true cloudflow config set db.prepared_statements.max_size 250 # Reduce idle connection timeout cloudflow config set db.pool.idle_timeout 120 # 2 minutes ``` --- ## Workflow Execution Failures ### Timeout Errors (3600 second limit) #### Error Message ``` WorkflowExecutionError: Workflow exceeded maximum execution time of 3600 seconds Status: TIMEOUT Workflow ID: wf_9k2n4m8p1q Execution ID: exec_7h3j6k9m2n ``` #### Analysis ```bash # Get workflow execution details cloudflow workflows executions get exec_7h3j6k9m2n --verbose # Check step-by-step breakdown cloudflow workflows executions steps exec_7h3j6k9m2n # Sample output: # Step 1: data_ingestion - Duration: 245s - Status: SUCCESS # Step 2: data_validation - Duration: 123s - Status: SUCCESS # Step 3: data_transformation - Duration: 3189s - Status: TIMEOUT # Step 4: data_export - Duration: 0s - Status: SKIPPED # Identify bottleneck step cloudflow workflows analyze",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_3",
              "content": "Workflow Context Accumulation** Large workflow executions may accumulate state in memory. **Solution:** ```bash # Configure context cleanup cloudflow config set workflow.context.max_size_mb 100 cloudflow config set workflow.context.cleanup_threshold 0.8 # Enable context persistence to disk cloudflow config set workflow.context.persistence.enabled true cloudflow config set workflow.context.persistence.backend s3 ``` **2. Connection Pool Leaks** **Diagnosis:** ```bash # Check active connections cloudflow db connections --verbose # Expected output: # Active: 45/100 # Idle: 23 # Waiting: 2 # Average age: 245s ``` **Resolution:** ```bash # Adjust connection pool settings cloudflow config set db.pool.max_connections 100 cloudflow config set db.pool.min_connections 10 cloudflow config set db.pool.idle_timeout 300 cloudflow config set db.pool.max_lifetime 1800 # Force connection pool reset cloudflow db pool reset --confirm ``` **3. Event Stream Buffers** Unbounded event buffers can cause memory exhaustion. ```bash # Configure event buffer limits cloudflow config set events.buffer.max_size 10000 cloudflow config set events.buffer.overflow_strategy drop_oldest # Enable event streaming to external sink cloudflow config set events.sink.type kafka cloudflow config set events.sink.kafka.brokers \"kafka-1:9092,kafka-2:9092\" cloudflow config set events.sink.kafka.topic cloudflow-events ``` --- ## Database Connection Issues ### Connection Pool Exhaustion #### Symptoms - Error: `could not obtain connection from pool within 5000ms` - Error: `connection pool exhausted (100/100 connections in use)` - API requests fail with `503 Service Unavailable` - Database CPU usage normal, but connection count at maximum #### Investigation ```bash # Check current connection pool status cloudflow db pool status --detailed # Output example: # Pool Statistics: # Total Connections: 100/100 (100%) # Active: 87 # Idle: 13 # Waiting Requests: 45 # Average Wait Time: 3420ms # Max Wait Time: 8234ms # Identify long-running queries cloudflow db queries --status running --min-duration 30000 # Check connection distribution by client SELECT application_name, state, COUNT(*) as conn_count, AVG(EXTRACT(EPOCH FROM (NOW() - state_change))) as avg_duration_sec FROM pg_stat_activity WHERE datname = 'cloudflow_production' GROUP BY application_name, state ORDER BY conn_count DESC; ``` #### Resolution **Immediate mitigation:** ```bash # Temporarily increase connection limit (requires database restart) cloudflow db config set max_connections 150 # Kill idle connections older than 5 minutes cloudflow db connections kill --idle-timeout 300 # Restart connection pool without downtime kubectl rollout restart deployment/cloudflow-api -n cloudflow kubectl rollout status deployment/cloudflow-api -n cloudflow ``` **Long-term solutions:** 1.",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_5",
              "content": "exec_7h3j6k9m2n --show-bottlenecks ``` #### Solutions **1. Increase workflow timeout (if justified):** ```bash # Update workflow configuration cloudflow workflows update wf_9k2n4m8p1q \\ --timeout 7200 \\ --reason \"Large dataset processing requires extended time\" # Verify update cloudflow workflows get wf_9k2n4m8p1q | grep timeout ``` **2. Optimize slow steps:** ```bash # Enable parallel processing cloudflow workflows update wf_9k2n4m8p1q \\ --step data_transformation \\ --parallel-workers 8 \\ --batch-size 1000 # Add checkpointing for long operations cloudflow workflows update wf_9k2n4m8p1q \\ --step data_transformation \\ --enable-checkpointing \\ --checkpoint-interval 300 ``` **3. Split workflow into smaller workflows:** ```bash # Create sub-workflows cloudflow workflows create data-pipeline-part1 \\ --steps \"data_ingestion,data_validation\" \\ --timeout 1800 cloudflow workflows create data-pipeline-part2 \\ --steps \"data_transformation,data_export\" \\ --timeout 3600 \\ --trigger workflow_completed \\ --trigger-workflow data-pipeline-part1 ``` ### Retry Logic and Exponential Backoff CloudFlow implements automatic retry with exponential backoff for transient failures: - Max retries: 3 - Initial delay: 1 second - Backoff multiplier: 2 - Max delay: 60 seconds #### Retry Sequence ``` Attempt 1: Immediate Attempt 2: Wait 1s (2^0 * 1s) Attempt 3: Wait 2s (2^1 * 1s) Attempt 4: Wait 4s (2^2 * 1s) ``` #### Configuration ```bash # View current retry settings cloudflow workflows get wf_9k2n4m8p1q --format json | jq '.retry_policy' # Customize retry behavior cloudflow workflows update wf_9k2n4m8p1q \\ --retry-max-attempts 5 \\ --retry-initial-delay 2000 \\ --retry-backoff-multiplier 2 \\ --retry-max-delay 120000 \\ --retry-on-errors \"NETWORK_ERROR,TIMEOUT,RATE_LIMIT\" # Disable retry for specific step cloudflow workflows update wf_9k2n4m8p1q \\ --step data_export \\ --retry-enabled false ``` #### Monitoring Retries ```bash # List failed executions with retry information cloudflow workflows executions list \\ --status FAILED \\ --show-retries \\ --last 7d # Get retry history for specific execution cloudflow workflows executions retries exec_7h3j6k9m2n # Output: # Execution: exec_7h3j6k9m2n # Attempt 1: FAILED - NetworkError: Connection refused (delay: 0ms) # Attempt 2: FAILED - NetworkError: Connection timeout (delay: 1000ms) # Attempt 3: FAILED - NetworkError: Connection timeout (delay: 2000ms) # Attempt 4: FAILED - NetworkError: Connection timeout (delay: 4000ms) # Final Status: FAILED_AFTER_RETRIES ``` ### Workflow Step Failures #### Common Error Patterns **1.",
              "score": null
            },
            {
              "doc_id": "user_guide",
              "chunk_id": "user_guide_fix_2",
              "content": "### HTTP Requests Make HTTP requests to any API endpoint: **Configuration:** - **Method**: GET, POST, PUT, PATCH, DELETE, HEAD - **URL**: Full endpoint URL (supports variable interpolation) - **Headers**: Custom headers as key-value pairs - **Query Parameters**: URL parameters - **Body**: JSON, form data, or raw text - **Authentication**: Basic Auth, Bearer Token, API Key, OAuth 2.0 **Example:** ```yaml - id: fetch_user action: http_request config: method: GET url: \"https://api.example.com/users/{{user_id}}\" headers: Authorization: \"Bearer {{secrets.API_TOKEN}}\" Content-Type: \"application/json\" timeout: 30 ``` **Response Handling:** Access the response in subsequent steps: - `{{steps.fetch_user.status}}` - HTTP status code - `{{steps.fetch_user.body}}` - Response body - `{{steps.fetch_user.headers}}` - Response headers ### Database Queries Execute queries against supported databases (PostgreSQL, MySQL, MongoDB, Redis): **SQL Databases (PostgreSQL, MySQL):** ```yaml - id: get_orders action: database_query config: connection: \"{{secrets.DB_CONNECTION_STRING}}\" query: | SELECT * FROM orders WHERE customer_id = $1 AND status = $2 ORDER BY created_at DESC LIMIT 10 parameters: - \"{{trigger.customer_id}}\" - \"pending\" ``` **MongoDB:** ```yaml - id: find_documents action: mongodb_query config: connection: \"{{secrets.MONGO_URI}}\" database: \"production\" collection: \"users\" operation: find filter: email: \"{{trigger.email}}\" options: limit: 1 ``` **Important Notes:** - Always use parameterized queries to prevent SQL injection - Connection strings should be stored in secrets - Query timeout is 30 seconds by default - Maximum result set size is 10MB ### Email Notifications Send emails via SMTP or integrated email providers: **Configuration:** ```yaml - id: send_notification action: email config: provider: \"smtp\" # or \"sendgrid\", \"mailgun\", \"ses\" from: \"notifications@cloudflow.io\" to: \"{{trigger.recipient}}\" cc: \"manager@company.com\" subject: \"Alert: {{alert_type}}\" body: | Hello {{user.name}}, This is an automated notification about {{event_description}}. Details: - Time: {{timestamp}} - Type: {{alert_type}} - Priority: {{priority}} Best regards, CloudFlow Automation html: true attachments: - name: \"report.pdf\" content: \"{{steps.generate_report.output}}\" encoding: \"base64\" ``` **Email Templates:** CloudFlow supports dynamic templates with conditional content: ``` {{#if priority == 'high'}} \u26a0\ufe0f URGENT: Immediate attention required {{else}} \u2139\ufe0f Informational notification {{/if}} ``` ### Slack Messages Send messages to Slack channels or users: **Channel Messages:** ```yaml - id: notify_team action: slack_message config: channel: \"#deployments\" text: \"Deployment completed successfully!\" blocks: - type: \"section\" text: type: \"mrkdwn\" text: \"*Deployment Summary*\\nEnvironment: {{environment}}\\nVersion: {{version}}\\nDuration: {{duration}}s\" - type: \"actions\" elements: - type: \"button\" text: \"View Logs\" url: \"{{logs_url}}\" thread_ts: \"{{trigger.thread_ts}}\" # Reply to thread ``` **Direct Messages:** ```yaml - id: dm_user action: slack_message config: user: \"{{user_slack_id}}\" text: \"Your report is ready for review.\"",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_10",
              "content": "- Execution start rate: 500 per second - Completion rate: 450 per second (average 2-second execution time) **Database**: - Read throughput: 50,000 queries per second (across replicas) - Write throughput: 15,000 transactions per second - Connection capacity: 2,000 concurrent connections **Kafka**: - Message ingestion: 100,000 messages per second - Consumer throughput: 80,000 messages per second (aggregated) - End-to-end latency: < 100ms (P99) ### Resource Utilization **CPU Utilization** (Target: 60-70% average): - API Gateway: 55% average, 80% peak - Workflow Engine: 65% average, 85% peak - Auth Service: 40% average, 60% peak **Memory Utilization** (Target: < 80%): - API Gateway: 2.5GB average per pod (4GB allocated) - Workflow Engine: 6GB average per pod (8GB allocated) - Notification Service: 2.8GB average per pod (4GB allocated) **Network Throughput**: - Ingress: 2 Gbps average, 5 Gbps peak - Egress: 1.5 Gbps average, 4 Gbps peak - Internal (service mesh): 8 Gbps average ### Monitoring & Alerting **Key Metrics** (Monitored via Prometheus): - Request rate (per service, per endpoint) - Error rate (4xx, 5xx responses) - Latency percentiles (P50, P95, P99) - Resource utilization (CPU, memory, disk) - Cache hit/miss ratios - Database connection pool saturation - Kafka consumer lag **Alerts** (via PagerDuty): - P1 (Immediate): API error rate > 1%, database connection failure - P2 (< 15 min): P99 latency > 500ms, cache hit rate < 80% - P3 (< 1 hour): Resource utilization > 85%, replica count < minimum **SLI/SLO/SLA**: - SLI: API success rate (non-5xx responses) - SLO: 99.9% uptime per month (43 minutes downtime allowance) - SLA: 99.5% uptime guarantee (customer-facing SLA with credits) --- ## Disaster Recovery ### Recovery Objectives **RPO (Recovery Point Objective): 1 hour** - Maximum acceptable data loss: 1 hour of transactions - Achieved through: Continuous database replication + hourly snapshots - Kafka retention: 7 days allows event replay **RTO (Recovery Time Objective): 4 hours** - Maximum acceptable downtime: 4 hours for full system recovery - Includes: Failover, data verification, and service restoration - Automated runbooks reduce RTO to < 2 hours for common scenarios ### High Availability Architecture **Multi-AZ Deployment**: ``` Region: us-east-1 AZ-1a: AZ-1b: AZ-1c: - API Gateway (4) - API Gateway (4) - API Gateway (4) - Workflow Engine (6) - Workflow Engine (5) - Workflow Engine (5) - Auth Service (3) - Auth Service (3) - Auth",
              "score": null
            }
          ],
          "latency_ms": 13.194900999224046,
          "query_id": "realistic_015",
          "dimension": "negation",
          "query": "Why can't I get a database connection even though CPU is low?"
        },
        {
          "key_facts": [
            "OAuth 2.0",
            "API keys",
            "JWT tokens"
          ],
          "found_facts": [
            "OAuth 2.0",
            "API keys",
            "JWT tokens"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "api_reference"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "user_guide",
              "chunk_id": "user_guide_fix_7",
              "content": "Handle Errors Gracefully Always implement error handling for external API calls and database operations: ```yaml - id: fetch_data action: http_request config: url: \"https://api.example.com/data\" retry: max_attempts: 3 backoff_type: exponential on_error: - id: log_error action: database_query config: query: \"INSERT INTO error_log (workflow_id, error) VALUES ($1, $2)\" parameters: - \"{{workflow.id}}\" - \"{{error.message}}\" ``` ### 3. Use Secrets for Sensitive Data Never hardcode API keys, passwords, or tokens in workflows: **Bad:** ```yaml headers: Authorization: \"Bearer sk_live_abc123xyz789\" ``` **Good:** ```yaml headers: Authorization: \"Bearer {{secrets.API_TOKEN}}\" ``` Store secrets in **Settings** > **Secrets** with encryption at rest. ### 4. Validate Input Data Always validate trigger data before processing: ```yaml - id: validate_input action: javascript code: | const required_fields = ['email', 'name', 'order_id']; for (const field of required_fields) { if (!input[field]) { throw new Error(`Missing required field: ${field}`); } } // Validate email format const email_regex = /^[^\\s@]+@[^\\s@]+\\.[^\\s@]+$/; if (!email_regex.test(input.email)) { throw new Error('Invalid email format'); } return { validated: true }; ``` ### 5. Use Idempotency Keys For operations that shouldn't be repeated (payments, record creation), use idempotency keys: ```yaml - id: create_charge action: http_request config: url: \"https://api.stripe.com/v1/charges\" method: POST headers: Idempotency-Key: \"{{workflow.id}}-{{execution.id}}\" body: amount: \"{{amount}}\" ``` ### 6. Monitor and Log Add logging steps for important workflow milestones: ```yaml - id: log_start action: database_query config: query: \"INSERT INTO workflow_audit (execution_id, step, timestamp) VALUES ($1, $2, $3)\" parameters: - \"{{execution.id}}\" - \"workflow_started\" - \"{{now()}}\" ``` ### 7. Keep Workflows Modular Break complex workflows into smaller, reusable components: - Use sub-workflows for repeated logic - Trigger child workflows via webhooks - Share common configurations via templates ### 8. Test Thoroughly Before activating a workflow: 1. Use test mode with sample data 2. Verify all actions execute correctly 3. Test error handling paths 4. Review execution logs 5. Start with a limited scope (e.g., test channel, small dataset) ### 9. Document Your Workflows Add descriptions to workflows and steps: ```yaml workflow: name: \"Daily Sales Report\" description: | Generates a daily sales report and distributes it to the sales team. Runs at 8:00 AM EST Monday-Friday. Data source: PostgreSQL sales database Recipients: sales-team@company.com steps: - id: fetch_sales name: \"Fetch Yesterday's Sales\" description: \"Query database for all completed orders from previous day\" ``` ### 10.",
              "score": null
            },
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_0",
              "content": "# CloudFlow API Reference Version 2.1.0 | Last Updated: January 2026 ## Overview The CloudFlow API is a RESTful service that enables developers to programmatically manage cloud workflows, data pipelines, and automation tasks. This documentation provides comprehensive details on authentication, endpoints, request/response formats, error handling, and best practices. **Base URL:** `https://api.cloudflow.io/v2` **API Status:** https://status.cloudflow.io ## Authentication CloudFlow supports three authentication methods to suit different use cases and security requirements. ### API Keys API keys provide simple authentication for server-to-server communication. Include your API key in the request header: ```bash curl -H \"X-API-Key: cf_live_a1b2c3d4e5f6g7h8i9j0\" \\ https://api.cloudflow.io/v2/workflows ``` **Security Notes:** - Never expose API keys in client-side code - Rotate keys every 90 days - Use separate keys for development and production environments ### OAuth 2.0 OAuth 2.0 is recommended for applications that access CloudFlow on behalf of users. We support the Authorization Code flow with PKCE. **Authorization Endpoint:** `https://auth.cloudflow.io/oauth/authorize` **Token Endpoint:** `https://auth.cloudflow.io/oauth/token` **Supported Scopes:** - `workflows:read` - Read workflow configurations - `workflows:write` - Create and modify workflows - `pipelines:read` - Read pipeline data - `pipelines:write` - Create and manage pipelines - `analytics:read` - Access analytics and metrics - `admin:full` - Full administrative access Example authorization request: ```bash curl -X POST https://auth.cloudflow.io/oauth/token \\ -H \"Content-Type: application/x-www-form-urlencoded\" \\ -d \"grant_type=authorization_code\" \\ -d \"code=AUTH_CODE_HERE\" \\ -d \"client_id=YOUR_CLIENT_ID\" \\ -d \"client_secret=YOUR_CLIENT_SECRET\" \\ -d \"redirect_uri=https://yourapp.com/callback\" ``` ### JWT Tokens For advanced use cases, CloudFlow supports JSON Web Tokens (JWT) with RS256 signing algorithm. JWTs must include the following claims: - `iss` (issuer): Your application identifier - `sub` (subject): User or service account ID - `aud` (audience): `https://api.cloudflow.io` - `exp` (expiration): Unix timestamp (max 3600 seconds from `iat`) - `iat` (issued at): Unix timestamp - `scope`: Space-separated list of requested scopes Example JWT header: ```python import jwt import time from cryptography.hazmat.primitives import serialization from cryptography.hazmat.backends import default_backend # Load your private key with open('private_key.pem', 'rb') as key_file: private_key = serialization.load_pem_private_key( key_file.read(), password=None, backend=default_backend() ) # Create JWT payload payload = { 'iss': 'your-app-id', 'sub': 'user-12345', 'aud': 'https://api.cloudflow.io', 'exp': int(time.time()) + 3600, 'iat': int(time.time()), 'scope': 'workflows:read workflows:write' } # Generate token token = jwt.encode(payload, private_key, algorithm='RS256') # Use in API request headers = {'Authorization': f'Bearer {token}'} ``` **Token Expiration:** All tokens expire after 3600 seconds (1 hour). Implement token refresh logic in your application.",
              "score": null
            },
            {
              "doc_id": "user_guide",
              "chunk_id": "user_guide_fix_9",
              "content": "Requester: {{trigger.body.requester}} Description: {{trigger.body.description}} Approve: https://app.company.com/approve/{{steps.create_approval_request.rows[0].id}} Reject: https://app.company.com/reject/{{steps.create_approval_request.rows[0].id}} - id: wait_for_approval action: wait_for_webhook config: timeout: 86400 # 24 hours webhook_path: \"/approval/{{steps.create_approval_request.rows[0].id}}\" - id: process_approval action: javascript code: | if (input.status === 'approved') { return { approved: true }; } else { throw new Error('Expense rejected'); } on_error: - id: notify_rejection action: email config: to: \"{{trigger.body.requester}}\" subject: \"Expense Rejected\" body: \"Your expense request has been rejected.\" - id: process_payment action: http_request config: method: POST url: \"https://api.accounting.com/payments\" body: amount: \"{{trigger.body.amount}}\" recipient: \"{{trigger.body.requester}}\" ``` ### Pattern 4: Data Synchronization Keep two systems in sync bidirectionally: ```yaml name: \"Sync Customers: CRM to Database\" trigger: type: event source: \"salesforce\" event: \"customer.updated\" steps: - id: fetch_customer action: http_request config: url: \"https://api.salesforce.com/customers/{{trigger.customer_id}}\" headers: Authorization: \"Bearer {{secrets.SALESFORCE_TOKEN}}\" - id: check_existing action: database_query config: query: \"SELECT id, last_updated FROM customers WHERE salesforce_id = $1\" parameters: - \"{{trigger.customer_id}}\" - id: update_or_insert action: database_query config: query: | INSERT INTO customers (salesforce_id, name, email, phone, last_updated) VALUES ($1, $2, $3, $4, NOW()) ON CONFLICT (salesforce_id) DO UPDATE SET name = $2, email = $3, phone = $4, last_updated = NOW() parameters: - \"{{trigger.customer_id}}\" - \"{{steps.fetch_customer.body.name}}\" - \"{{steps.fetch_customer.body.email}}\" - \"{{steps.fetch_customer.body.phone}}\" - id: log_sync action: database_query config: query: | INSERT INTO sync_log (source, target, record_id, synced_at) VALUES ('salesforce', 'postgresql', $1, NOW()) parameters: - \"{{trigger.customer_id}}\" ``` ### Pattern 5: Error Aggregation and Alerting Aggregate errors and send smart alerts: ```yaml name: \"Application Error Monitor\" schedule: cron: \"*/5 * * * *\" timezone: \"UTC\" steps: - id: fetch_recent_errors action: database_query config: query: | SELECT error_type, COUNT(*) as count, MAX(created_at) as last_seen FROM error_logs WHERE created_at > NOW() - INTERVAL '5 minutes' AND alerted = false GROUP BY error_type HAVING COUNT(*) > 5 - id: format_alert action: javascript condition: \"{{length(steps.fetch_recent_errors.rows) > 0}}\" code: | const errors = input.rows; let message = \"\u26a0\ufe0f Error Alert\\n\\n\"; for (const error of errors) { message += `\u2022 ${error.error_type}: ${error.count} occurrences (last: ${error.last_seen})\\n`; } return { message, total_errors: errors.reduce((sum, e) => sum + e.count, 0) }; - id: send_alert action: slack_message condition: \"{{steps.format_alert.output.total_errors > 10}}\" config: channel: \"#incidents\" text: \"{{steps.format_alert.output.message}}\" priority: \"high\" - id: mark_alerted action: database_query config: query: | UPDATE error_logs SET alerted = true WHERE created_at > NOW() - INTERVAL '5 minutes' ``` --- ## Getting Help Need assistance with CloudFlow?",
              "score": null
            },
            {
              "doc_id": "user_guide",
              "chunk_id": "user_guide_fix_2",
              "content": "### HTTP Requests Make HTTP requests to any API endpoint: **Configuration:** - **Method**: GET, POST, PUT, PATCH, DELETE, HEAD - **URL**: Full endpoint URL (supports variable interpolation) - **Headers**: Custom headers as key-value pairs - **Query Parameters**: URL parameters - **Body**: JSON, form data, or raw text - **Authentication**: Basic Auth, Bearer Token, API Key, OAuth 2.0 **Example:** ```yaml - id: fetch_user action: http_request config: method: GET url: \"https://api.example.com/users/{{user_id}}\" headers: Authorization: \"Bearer {{secrets.API_TOKEN}}\" Content-Type: \"application/json\" timeout: 30 ``` **Response Handling:** Access the response in subsequent steps: - `{{steps.fetch_user.status}}` - HTTP status code - `{{steps.fetch_user.body}}` - Response body - `{{steps.fetch_user.headers}}` - Response headers ### Database Queries Execute queries against supported databases (PostgreSQL, MySQL, MongoDB, Redis): **SQL Databases (PostgreSQL, MySQL):** ```yaml - id: get_orders action: database_query config: connection: \"{{secrets.DB_CONNECTION_STRING}}\" query: | SELECT * FROM orders WHERE customer_id = $1 AND status = $2 ORDER BY created_at DESC LIMIT 10 parameters: - \"{{trigger.customer_id}}\" - \"pending\" ``` **MongoDB:** ```yaml - id: find_documents action: mongodb_query config: connection: \"{{secrets.MONGO_URI}}\" database: \"production\" collection: \"users\" operation: find filter: email: \"{{trigger.email}}\" options: limit: 1 ``` **Important Notes:** - Always use parameterized queries to prevent SQL injection - Connection strings should be stored in secrets - Query timeout is 30 seconds by default - Maximum result set size is 10MB ### Email Notifications Send emails via SMTP or integrated email providers: **Configuration:** ```yaml - id: send_notification action: email config: provider: \"smtp\" # or \"sendgrid\", \"mailgun\", \"ses\" from: \"notifications@cloudflow.io\" to: \"{{trigger.recipient}}\" cc: \"manager@company.com\" subject: \"Alert: {{alert_type}}\" body: | Hello {{user.name}}, This is an automated notification about {{event_description}}. Details: - Time: {{timestamp}} - Type: {{alert_type}} - Priority: {{priority}} Best regards, CloudFlow Automation html: true attachments: - name: \"report.pdf\" content: \"{{steps.generate_report.output}}\" encoding: \"base64\" ``` **Email Templates:** CloudFlow supports dynamic templates with conditional content: ``` {{#if priority == 'high'}} \u26a0\ufe0f URGENT: Immediate attention required {{else}} \u2139\ufe0f Informational notification {{/if}} ``` ### Slack Messages Send messages to Slack channels or users: **Channel Messages:** ```yaml - id: notify_team action: slack_message config: channel: \"#deployments\" text: \"Deployment completed successfully!\" blocks: - type: \"section\" text: type: \"mrkdwn\" text: \"*Deployment Summary*\\nEnvironment: {{environment}}\\nVersion: {{version}}\\nDuration: {{duration}}s\" - type: \"actions\" elements: - type: \"button\" text: \"View Logs\" url: \"{{logs_url}}\" thread_ts: \"{{trigger.thread_ts}}\" # Reply to thread ``` **Direct Messages:** ```yaml - id: dm_user action: slack_message config: user: \"{{user_slack_id}}\" text: \"Your report is ready for review.\"",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_0",
              "content": "# CloudFlow Platform Troubleshooting Guide **Version:** 3.2.1 **Last Updated:** January 2026 **Audience:** Platform Engineers, DevOps, Support Teams ## Table of Contents 1. [Overview](#overview) 2. [Authentication & Authorization Issues](#authentication--authorization-issues) 3. [Performance Problems](#performance-problems) 4. [Database Connection Issues](#database-connection-issues) 5. [Workflow Execution Failures](#workflow-execution-failures) 6. [Rate Limiting & Throttling](#rate-limiting--throttling) 7. [Log Analysis & Debugging](#log-analysis--debugging) 8. [Escalation Procedures](#escalation-procedures) --- ## Overview This guide provides comprehensive troubleshooting steps for common CloudFlow platform issues encountered in production environments. Each section includes error symptoms, root cause analysis, resolution steps, and preventive measures. ### Quick Diagnostic Checklist Before diving into specific issues, perform these initial checks: - Verify service health: `cloudflow status --all` - Check API connectivity: `curl -I https://api.cloudflow.io/health` - Review recent deployments: `kubectl get deployments -n cloudflow --sort-by=.metadata.creationTimestamp` - Inspect platform metrics: `cloudflow metrics --last 1h` --- ## Authentication & Authorization Issues ### 401 Unauthorized Errors #### Symptoms - API requests return `401 Unauthorized` - Error message: `Authentication credentials were not provided or are invalid` - Frontend displays \"Session expired\" message #### Common Causes **1. Token Expiration** CloudFlow access tokens expire after 3600 seconds (1 hour) by default. Refresh tokens are valid for 30 days. **Verification:** ```bash # Decode JWT to check expiration echo $CF_ACCESS_TOKEN | cut -d'.' -f2 | base64 -d | jq '.exp' # Compare with current time date +%s ``` **Resolution:** ```bash # Refresh the access token curl -X POST https://api.cloudflow.io/auth/refresh \\ -H \"Content-Type: application/json\" \\ -d '{\"refresh_token\": \"'$CF_REFRESH_TOKEN'\"}' # Update environment variable export CF_ACCESS_TOKEN=\"<new_token>\" ``` **2. Invalid JWT Signature** Error: `JWT signature verification failed` **Causes:** - Token was modified or corrupted - Using wrong signing key - Token generated with different secret **Resolution:** ```bash # Validate token structure cloudflow auth validate --token $CF_ACCESS_TOKEN # Generate new token cloudflow auth login --username <user> --password <pass> # For service accounts cloudflow auth service-account --create --name <sa-name> --scopes \"workflow:read,workflow:write\" ``` **3. Clock Skew Issues** JWT validation fails when system clocks are out of sync (tolerance: \u00b1300 seconds).",
              "score": null
            }
          ],
          "latency_ms": 12.933732999954373,
          "query_id": "realistic_016",
          "dimension": "original",
          "query": "How do I handle API authentication?"
        },
        {
          "key_facts": [
            "OAuth 2.0",
            "API keys",
            "JWT tokens"
          ],
          "found_facts": [
            "OAuth 2.0",
            "API keys",
            "JWT tokens"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "api_reference"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_0",
              "content": "# CloudFlow API Reference Version 2.1.0 | Last Updated: January 2026 ## Overview The CloudFlow API is a RESTful service that enables developers to programmatically manage cloud workflows, data pipelines, and automation tasks. This documentation provides comprehensive details on authentication, endpoints, request/response formats, error handling, and best practices. **Base URL:** `https://api.cloudflow.io/v2` **API Status:** https://status.cloudflow.io ## Authentication CloudFlow supports three authentication methods to suit different use cases and security requirements. ### API Keys API keys provide simple authentication for server-to-server communication. Include your API key in the request header: ```bash curl -H \"X-API-Key: cf_live_a1b2c3d4e5f6g7h8i9j0\" \\ https://api.cloudflow.io/v2/workflows ``` **Security Notes:** - Never expose API keys in client-side code - Rotate keys every 90 days - Use separate keys for development and production environments ### OAuth 2.0 OAuth 2.0 is recommended for applications that access CloudFlow on behalf of users. We support the Authorization Code flow with PKCE. **Authorization Endpoint:** `https://auth.cloudflow.io/oauth/authorize` **Token Endpoint:** `https://auth.cloudflow.io/oauth/token` **Supported Scopes:** - `workflows:read` - Read workflow configurations - `workflows:write` - Create and modify workflows - `pipelines:read` - Read pipeline data - `pipelines:write` - Create and manage pipelines - `analytics:read` - Access analytics and metrics - `admin:full` - Full administrative access Example authorization request: ```bash curl -X POST https://auth.cloudflow.io/oauth/token \\ -H \"Content-Type: application/x-www-form-urlencoded\" \\ -d \"grant_type=authorization_code\" \\ -d \"code=AUTH_CODE_HERE\" \\ -d \"client_id=YOUR_CLIENT_ID\" \\ -d \"client_secret=YOUR_CLIENT_SECRET\" \\ -d \"redirect_uri=https://yourapp.com/callback\" ``` ### JWT Tokens For advanced use cases, CloudFlow supports JSON Web Tokens (JWT) with RS256 signing algorithm. JWTs must include the following claims: - `iss` (issuer): Your application identifier - `sub` (subject): User or service account ID - `aud` (audience): `https://api.cloudflow.io` - `exp` (expiration): Unix timestamp (max 3600 seconds from `iat`) - `iat` (issued at): Unix timestamp - `scope`: Space-separated list of requested scopes Example JWT header: ```python import jwt import time from cryptography.hazmat.primitives import serialization from cryptography.hazmat.backends import default_backend # Load your private key with open('private_key.pem', 'rb') as key_file: private_key = serialization.load_pem_private_key( key_file.read(), password=None, backend=default_backend() ) # Create JWT payload payload = { 'iss': 'your-app-id', 'sub': 'user-12345', 'aud': 'https://api.cloudflow.io', 'exp': int(time.time()) + 3600, 'iat': int(time.time()), 'scope': 'workflows:read workflows:write' } # Generate token token = jwt.encode(payload, private_key, algorithm='RS256') # Use in API request headers = {'Authorization': f'Bearer {token}'} ``` **Token Expiration:** All tokens expire after 3600 seconds (1 hour). Implement token refresh logic in your application.",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_0",
              "content": "# CloudFlow Platform Troubleshooting Guide **Version:** 3.2.1 **Last Updated:** January 2026 **Audience:** Platform Engineers, DevOps, Support Teams ## Table of Contents 1. [Overview](#overview) 2. [Authentication & Authorization Issues](#authentication--authorization-issues) 3. [Performance Problems](#performance-problems) 4. [Database Connection Issues](#database-connection-issues) 5. [Workflow Execution Failures](#workflow-execution-failures) 6. [Rate Limiting & Throttling](#rate-limiting--throttling) 7. [Log Analysis & Debugging](#log-analysis--debugging) 8. [Escalation Procedures](#escalation-procedures) --- ## Overview This guide provides comprehensive troubleshooting steps for common CloudFlow platform issues encountered in production environments. Each section includes error symptoms, root cause analysis, resolution steps, and preventive measures. ### Quick Diagnostic Checklist Before diving into specific issues, perform these initial checks: - Verify service health: `cloudflow status --all` - Check API connectivity: `curl -I https://api.cloudflow.io/health` - Review recent deployments: `kubectl get deployments -n cloudflow --sort-by=.metadata.creationTimestamp` - Inspect platform metrics: `cloudflow metrics --last 1h` --- ## Authentication & Authorization Issues ### 401 Unauthorized Errors #### Symptoms - API requests return `401 Unauthorized` - Error message: `Authentication credentials were not provided or are invalid` - Frontend displays \"Session expired\" message #### Common Causes **1. Token Expiration** CloudFlow access tokens expire after 3600 seconds (1 hour) by default. Refresh tokens are valid for 30 days. **Verification:** ```bash # Decode JWT to check expiration echo $CF_ACCESS_TOKEN | cut -d'.' -f2 | base64 -d | jq '.exp' # Compare with current time date +%s ``` **Resolution:** ```bash # Refresh the access token curl -X POST https://api.cloudflow.io/auth/refresh \\ -H \"Content-Type: application/json\" \\ -d '{\"refresh_token\": \"'$CF_REFRESH_TOKEN'\"}' # Update environment variable export CF_ACCESS_TOKEN=\"<new_token>\" ``` **2. Invalid JWT Signature** Error: `JWT signature verification failed` **Causes:** - Token was modified or corrupted - Using wrong signing key - Token generated with different secret **Resolution:** ```bash # Validate token structure cloudflow auth validate --token $CF_ACCESS_TOKEN # Generate new token cloudflow auth login --username <user> --password <pass> # For service accounts cloudflow auth service-account --create --name <sa-name> --scopes \"workflow:read,workflow:write\" ``` **3. Clock Skew Issues** JWT validation fails when system clocks are out of sync (tolerance: \u00b1300 seconds).",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_1",
              "content": "**Technology**: Node.js with Express.js framework **Replicas**: 12 pods (production), auto-scaling 8-20 based on CPU **Resource Allocation**: 2 vCPU, 4GB RAM per pod **Key Responsibilities**: - JWT token validation (delegated to Auth Service for initial validation) - Rate limiting: 1000 requests per minute per API key (sliding window) - Request/response transformation and validation using JSON Schema - Routing to downstream services based on URL path patterns - CORS handling for web clients - Request/response logging and correlation ID injection **Critical Endpoints**: - `POST /api/v1/workflows` - Create new workflow - `GET /api/v1/workflows/:id` - Retrieve workflow status - `POST /api/v1/workflows/:id/execute` - Trigger workflow execution - `GET /api/v1/workflows/:id/history` - Get execution history **Dependencies**: - Auth Service (for token validation) - Redis (for rate limiting counters) - All downstream microservices **Performance Targets**: - P50 latency: < 50ms - P99 latency: < 200ms - Throughput: 10,000 RPS sustained --- ### Auth Service **Purpose**: Centralized authentication and authorization service handling user identity, token generation, and permission validation. **Technology**: Go with gRPC for internal communication, REST for external **Replicas**: 8 pods (production), auto-scaling 6-12 **Resource Allocation**: 1 vCPU, 2GB RAM per pod **Key Responsibilities**: - User authentication via multiple providers (OAuth2, SAML, local credentials) - JWT token generation and validation (RS256 algorithm) - Role-based access control (RBAC) with fine-grained permissions - Session management with Redis-backed storage - API key generation and validation for service accounts - MFA enforcement for administrative operations **Authentication Flow**: ``` Client Request \u2192 API Gateway \u2192 Auth Service \u2502 \u251c\u2500 Validate credentials \u251c\u2500 Check MFA if required \u251c\u2500 Generate JWT (15min expiry) \u251c\u2500 Generate refresh token (7 days) \u2514\u2500 Store session in Redis ``` **Token Structure**: - Access Token: JWT with 15-minute expiry - Refresh Token: Opaque token with 7-day expiry, stored in PostgreSQL - Claims: user_id, email, roles[], permissions[], tenant_id **Security Features**: - Password hashing: Argon2id with 64MB memory, 4 iterations - Token rotation on refresh to prevent replay attacks - Brute force protection: 5 failed attempts \u2192 15-minute lockout - Secrets stored in HashiCorp Vault, rotated every 90 days **Performance Targets**: - Token validation: < 10ms (P99) - Token generation: < 50ms (P99) - Throughput: 5,000 RPS for validation operations --- ### Workflow Engine **Purpose**: Core orchestration service that executes workflow definitions, manages state transitions, and coordinates task execution across distributed systems.",
              "score": null
            },
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_4",
              "content": "### Error Response Format ```json { \"error\": { \"code\": \"invalid_parameter\", \"message\": \"The 'limit' parameter must be between 1 and 100\", \"field\": \"limit\", \"request_id\": \"req_8k3m9x2p\" } } ``` ### HTTP Status Codes | Status Code | Description | Common Causes | |------------|-------------|---------------| | 400 | Bad Request | Invalid parameters, malformed JSON, validation errors | | 401 | Unauthorized | Missing or invalid authentication credentials | | 403 | Forbidden | Insufficient permissions for requested resource | | 404 | Not Found | Resource does not exist or has been deleted | | 429 | Too Many Requests | Rate limit exceeded, retry after specified period | | 500 | Internal Server Error | Unexpected server error, contact support if persists | | 502 | Bad Gateway | Temporary service issue, retry with exponential backoff | | 503 | Service Unavailable | Scheduled maintenance or temporary outage | ### Error Codes CloudFlow returns specific error codes to help you identify and resolve issues: - `invalid_parameter`: One or more request parameters are invalid - `missing_required_field`: Required field is missing from request body - `authentication_failed`: Invalid API key or token - `insufficient_permissions`: User lacks required scope or permission - `resource_not_found`: Requested resource does not exist - `rate_limit_exceeded`: Too many requests, see rate limiting section - `workflow_execution_failed`: Workflow execution encountered an error - `invalid_json`: Request body contains malformed JSON - `duplicate_resource`: Resource with same identifier already exists - `quota_exceeded`: Account quota limit reached ### Error Handling Best Practices **Python Example:** ```python import requests import time def make_api_request(url, headers, max_retries=3): for attempt in range(max_retries): try: response = requests.get(url, headers=headers) if response.status_code == 200: return response.json() elif response.status_code == 429: # Rate limit exceeded retry_after = int(response.headers.get('X-RateLimit-Reset', 60)) print(f\"Rate limited. Waiting {retry_after} seconds...\") time.sleep(retry_after) continue elif response.status_code >= 500: # Server error - retry with exponential backoff wait_time = 2 ** attempt print(f\"Server error. Retrying in {wait_time} seconds...\") time.sleep(wait_time) continue else: # Client error - don't retry error_data = response.json() raise Exception(f\"API Error: {error_data['error']['message']}\") except requests.exceptions.RequestException as e: print(f\"Request failed: {e}\") if attempt < max_retries - 1: time.sleep(2 ** attempt) else: raise raise Exception(\"Max retries exceeded\") ``` ## Webhooks CloudFlow can send webhook notifications when specific events occur in your workflows.",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_8",
              "content": "### Cache Invalidation Strategies **Time-Based Expiration (TTL)**: - Short-lived data: 5-15 minutes (session tokens, rate limit counters) - Medium-lived data: 1 hour (workflow definitions, templates) - Long-lived data: 24 hours (static configuration) **Event-Based Invalidation**: ``` Database Update Event \u2192 Kafka (cache.invalidation topic) \u2502 \u25bc All service instances consume event \u2502 \u25bc Redis DEL for affected keys ``` **Pattern-Based Invalidation**: - Use Redis SCAN + DEL for wildcard patterns - Example: Invalidate all user caches: `user:*:{user_id}` **Cache Warming**: - Scheduled job runs every hour to pre-populate frequently accessed data - Targets: Top 1000 workflows, system configuration, active user profiles - Reduces cache miss rate during traffic spikes ### Multi-Level Caching Application implements L1 (in-memory) and L2 (Redis) caching: ``` Request \u2192 L1 Cache (Node.js Map, 1000 entry LRU) \u2502 MISS \u2502 \u25bc L2 Cache (Redis, distributed) \u2502 MISS \u2502 \u25bc Database (PostgreSQL) ``` L1 cache reduces Redis network calls by 40% for hot data. --- ## Security Architecture ### Network Security **Zero-Trust Network Model**: - All service-to-service communication encrypted with mTLS - Certificate rotation: Every 90 days (automated via cert-manager) - Certificate authority: Internal PKI with HashiCorp Vault - Network segmentation: Private subnets for services, public subnet for ALB only **Service Mesh (Istio)**: ``` Service A \u2192 Envoy Sidecar (mTLS client cert) \u2502 Encrypted channel \u2502 Envoy Sidecar (mTLS server cert) \u2192 Service B ``` **Firewall Rules**: - Security groups: Deny all by default, explicit allow rules - Ingress: Only ALB can reach API Gateway (port 8080) - Egress: Services can only reach specific dependencies - Database access: Limited to application subnets only ### Authentication & Authorization **JWT Token Validation**: - Algorithm: RS256 (asymmetric signing) - Key rotation: Every 30 days with 7-day overlap period - Public key distribution: JWKS endpoint cached in Redis - Validation: Signature, expiry, issuer, audience claims - Token revocation: Blacklist in Redis for compromised tokens **Permission Model**: ``` User \u2192 Roles \u2192 Permissions \u2198 \u2197 Tenants (Multi-tenancy isolation) ``` Example permissions: - `workflow:read` - View workflows - `workflow:write` - Create/update workflows - `workflow:execute` - Trigger workflow execution - `workflow:delete` - Delete workflows - `admin:*` - All administrative operations **API Key Management**: - Format: `cfk_live_<32-char-random>` (production), `cfk_test_<32-char-random>` (sandbox) - Hashing: SHA-256 before storage in PostgreSQL - Scoping: API keys can be scoped to specific workflows or operations - Rate limits: Configurable",
              "score": null
            }
          ],
          "latency_ms": 12.873871997726383,
          "query_id": "realistic_016",
          "dimension": "synonym",
          "query": "What authentication methods are supported?"
        },
        {
          "key_facts": [
            "OAuth 2.0",
            "API keys",
            "JWT tokens"
          ],
          "found_facts": [
            "OAuth 2.0",
            "API keys"
          ],
          "missed_facts": [
            "JWT tokens"
          ],
          "coverage": 0.6666666666666666,
          "expected_docs": [
            "api_reference"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_4",
              "content": "### Error Response Format ```json { \"error\": { \"code\": \"invalid_parameter\", \"message\": \"The 'limit' parameter must be between 1 and 100\", \"field\": \"limit\", \"request_id\": \"req_8k3m9x2p\" } } ``` ### HTTP Status Codes | Status Code | Description | Common Causes | |------------|-------------|---------------| | 400 | Bad Request | Invalid parameters, malformed JSON, validation errors | | 401 | Unauthorized | Missing or invalid authentication credentials | | 403 | Forbidden | Insufficient permissions for requested resource | | 404 | Not Found | Resource does not exist or has been deleted | | 429 | Too Many Requests | Rate limit exceeded, retry after specified period | | 500 | Internal Server Error | Unexpected server error, contact support if persists | | 502 | Bad Gateway | Temporary service issue, retry with exponential backoff | | 503 | Service Unavailable | Scheduled maintenance or temporary outage | ### Error Codes CloudFlow returns specific error codes to help you identify and resolve issues: - `invalid_parameter`: One or more request parameters are invalid - `missing_required_field`: Required field is missing from request body - `authentication_failed`: Invalid API key or token - `insufficient_permissions`: User lacks required scope or permission - `resource_not_found`: Requested resource does not exist - `rate_limit_exceeded`: Too many requests, see rate limiting section - `workflow_execution_failed`: Workflow execution encountered an error - `invalid_json`: Request body contains malformed JSON - `duplicate_resource`: Resource with same identifier already exists - `quota_exceeded`: Account quota limit reached ### Error Handling Best Practices **Python Example:** ```python import requests import time def make_api_request(url, headers, max_retries=3): for attempt in range(max_retries): try: response = requests.get(url, headers=headers) if response.status_code == 200: return response.json() elif response.status_code == 429: # Rate limit exceeded retry_after = int(response.headers.get('X-RateLimit-Reset', 60)) print(f\"Rate limited. Waiting {retry_after} seconds...\") time.sleep(retry_after) continue elif response.status_code >= 500: # Server error - retry with exponential backoff wait_time = 2 ** attempt print(f\"Server error. Retrying in {wait_time} seconds...\") time.sleep(wait_time) continue else: # Client error - don't retry error_data = response.json() raise Exception(f\"API Error: {error_data['error']['message']}\") except requests.exceptions.RequestException as e: print(f\"Request failed: {e}\") if attempt < max_retries - 1: time.sleep(2 ** attempt) else: raise raise Exception(\"Max retries exceeded\") ``` ## Webhooks CloudFlow can send webhook notifications when specific events occur in your workflows.",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_0",
              "content": "# CloudFlow Platform Troubleshooting Guide **Version:** 3.2.1 **Last Updated:** January 2026 **Audience:** Platform Engineers, DevOps, Support Teams ## Table of Contents 1. [Overview](#overview) 2. [Authentication & Authorization Issues](#authentication--authorization-issues) 3. [Performance Problems](#performance-problems) 4. [Database Connection Issues](#database-connection-issues) 5. [Workflow Execution Failures](#workflow-execution-failures) 6. [Rate Limiting & Throttling](#rate-limiting--throttling) 7. [Log Analysis & Debugging](#log-analysis--debugging) 8. [Escalation Procedures](#escalation-procedures) --- ## Overview This guide provides comprehensive troubleshooting steps for common CloudFlow platform issues encountered in production environments. Each section includes error symptoms, root cause analysis, resolution steps, and preventive measures. ### Quick Diagnostic Checklist Before diving into specific issues, perform these initial checks: - Verify service health: `cloudflow status --all` - Check API connectivity: `curl -I https://api.cloudflow.io/health` - Review recent deployments: `kubectl get deployments -n cloudflow --sort-by=.metadata.creationTimestamp` - Inspect platform metrics: `cloudflow metrics --last 1h` --- ## Authentication & Authorization Issues ### 401 Unauthorized Errors #### Symptoms - API requests return `401 Unauthorized` - Error message: `Authentication credentials were not provided or are invalid` - Frontend displays \"Session expired\" message #### Common Causes **1. Token Expiration** CloudFlow access tokens expire after 3600 seconds (1 hour) by default. Refresh tokens are valid for 30 days. **Verification:** ```bash # Decode JWT to check expiration echo $CF_ACCESS_TOKEN | cut -d'.' -f2 | base64 -d | jq '.exp' # Compare with current time date +%s ``` **Resolution:** ```bash # Refresh the access token curl -X POST https://api.cloudflow.io/auth/refresh \\ -H \"Content-Type: application/json\" \\ -d '{\"refresh_token\": \"'$CF_REFRESH_TOKEN'\"}' # Update environment variable export CF_ACCESS_TOKEN=\"<new_token>\" ``` **2. Invalid JWT Signature** Error: `JWT signature verification failed` **Causes:** - Token was modified or corrupted - Using wrong signing key - Token generated with different secret **Resolution:** ```bash # Validate token structure cloudflow auth validate --token $CF_ACCESS_TOKEN # Generate new token cloudflow auth login --username <user> --password <pass> # For service accounts cloudflow auth service-account --create --name <sa-name> --scopes \"workflow:read,workflow:write\" ``` **3. Clock Skew Issues** JWT validation fails when system clocks are out of sync (tolerance: \u00b1300 seconds).",
              "score": null
            },
            {
              "doc_id": "user_guide",
              "chunk_id": "user_guide_fix_9",
              "content": "Requester: {{trigger.body.requester}} Description: {{trigger.body.description}} Approve: https://app.company.com/approve/{{steps.create_approval_request.rows[0].id}} Reject: https://app.company.com/reject/{{steps.create_approval_request.rows[0].id}} - id: wait_for_approval action: wait_for_webhook config: timeout: 86400 # 24 hours webhook_path: \"/approval/{{steps.create_approval_request.rows[0].id}}\" - id: process_approval action: javascript code: | if (input.status === 'approved') { return { approved: true }; } else { throw new Error('Expense rejected'); } on_error: - id: notify_rejection action: email config: to: \"{{trigger.body.requester}}\" subject: \"Expense Rejected\" body: \"Your expense request has been rejected.\" - id: process_payment action: http_request config: method: POST url: \"https://api.accounting.com/payments\" body: amount: \"{{trigger.body.amount}}\" recipient: \"{{trigger.body.requester}}\" ``` ### Pattern 4: Data Synchronization Keep two systems in sync bidirectionally: ```yaml name: \"Sync Customers: CRM to Database\" trigger: type: event source: \"salesforce\" event: \"customer.updated\" steps: - id: fetch_customer action: http_request config: url: \"https://api.salesforce.com/customers/{{trigger.customer_id}}\" headers: Authorization: \"Bearer {{secrets.SALESFORCE_TOKEN}}\" - id: check_existing action: database_query config: query: \"SELECT id, last_updated FROM customers WHERE salesforce_id = $1\" parameters: - \"{{trigger.customer_id}}\" - id: update_or_insert action: database_query config: query: | INSERT INTO customers (salesforce_id, name, email, phone, last_updated) VALUES ($1, $2, $3, $4, NOW()) ON CONFLICT (salesforce_id) DO UPDATE SET name = $2, email = $3, phone = $4, last_updated = NOW() parameters: - \"{{trigger.customer_id}}\" - \"{{steps.fetch_customer.body.name}}\" - \"{{steps.fetch_customer.body.email}}\" - \"{{steps.fetch_customer.body.phone}}\" - id: log_sync action: database_query config: query: | INSERT INTO sync_log (source, target, record_id, synced_at) VALUES ('salesforce', 'postgresql', $1, NOW()) parameters: - \"{{trigger.customer_id}}\" ``` ### Pattern 5: Error Aggregation and Alerting Aggregate errors and send smart alerts: ```yaml name: \"Application Error Monitor\" schedule: cron: \"*/5 * * * *\" timezone: \"UTC\" steps: - id: fetch_recent_errors action: database_query config: query: | SELECT error_type, COUNT(*) as count, MAX(created_at) as last_seen FROM error_logs WHERE created_at > NOW() - INTERVAL '5 minutes' AND alerted = false GROUP BY error_type HAVING COUNT(*) > 5 - id: format_alert action: javascript condition: \"{{length(steps.fetch_recent_errors.rows) > 0}}\" code: | const errors = input.rows; let message = \"\u26a0\ufe0f Error Alert\\n\\n\"; for (const error of errors) { message += `\u2022 ${error.error_type}: ${error.count} occurrences (last: ${error.last_seen})\\n`; } return { message, total_errors: errors.reduce((sum, e) => sum + e.count, 0) }; - id: send_alert action: slack_message condition: \"{{steps.format_alert.output.total_errors > 10}}\" config: channel: \"#incidents\" text: \"{{steps.format_alert.output.message}}\" priority: \"high\" - id: mark_alerted action: database_query config: query: | UPDATE error_logs SET alerted = true WHERE created_at > NOW() - INTERVAL '5 minutes' ``` --- ## Getting Help Need assistance with CloudFlow?",
              "score": null
            },
            {
              "doc_id": "user_guide",
              "chunk_id": "user_guide_fix_7",
              "content": "Handle Errors Gracefully Always implement error handling for external API calls and database operations: ```yaml - id: fetch_data action: http_request config: url: \"https://api.example.com/data\" retry: max_attempts: 3 backoff_type: exponential on_error: - id: log_error action: database_query config: query: \"INSERT INTO error_log (workflow_id, error) VALUES ($1, $2)\" parameters: - \"{{workflow.id}}\" - \"{{error.message}}\" ``` ### 3. Use Secrets for Sensitive Data Never hardcode API keys, passwords, or tokens in workflows: **Bad:** ```yaml headers: Authorization: \"Bearer sk_live_abc123xyz789\" ``` **Good:** ```yaml headers: Authorization: \"Bearer {{secrets.API_TOKEN}}\" ``` Store secrets in **Settings** > **Secrets** with encryption at rest. ### 4. Validate Input Data Always validate trigger data before processing: ```yaml - id: validate_input action: javascript code: | const required_fields = ['email', 'name', 'order_id']; for (const field of required_fields) { if (!input[field]) { throw new Error(`Missing required field: ${field}`); } } // Validate email format const email_regex = /^[^\\s@]+@[^\\s@]+\\.[^\\s@]+$/; if (!email_regex.test(input.email)) { throw new Error('Invalid email format'); } return { validated: true }; ``` ### 5. Use Idempotency Keys For operations that shouldn't be repeated (payments, record creation), use idempotency keys: ```yaml - id: create_charge action: http_request config: url: \"https://api.stripe.com/v1/charges\" method: POST headers: Idempotency-Key: \"{{workflow.id}}-{{execution.id}}\" body: amount: \"{{amount}}\" ``` ### 6. Monitor and Log Add logging steps for important workflow milestones: ```yaml - id: log_start action: database_query config: query: \"INSERT INTO workflow_audit (execution_id, step, timestamp) VALUES ($1, $2, $3)\" parameters: - \"{{execution.id}}\" - \"workflow_started\" - \"{{now()}}\" ``` ### 7. Keep Workflows Modular Break complex workflows into smaller, reusable components: - Use sub-workflows for repeated logic - Trigger child workflows via webhooks - Share common configurations via templates ### 8. Test Thoroughly Before activating a workflow: 1. Use test mode with sample data 2. Verify all actions execute correctly 3. Test error handling paths 4. Review execution logs 5. Start with a limited scope (e.g., test channel, small dataset) ### 9. Document Your Workflows Add descriptions to workflows and steps: ```yaml workflow: name: \"Daily Sales Report\" description: | Generates a daily sales report and distributes it to the sales team. Runs at 8:00 AM EST Monday-Friday. Data source: PostgreSQL sales database Recipients: sales-team@company.com steps: - id: fetch_sales name: \"Fetch Yesterday's Sales\" description: \"Query database for all completed orders from previous day\" ``` ### 10.",
              "score": null
            },
            {
              "doc_id": "user_guide",
              "chunk_id": "user_guide_fix_2",
              "content": "### HTTP Requests Make HTTP requests to any API endpoint: **Configuration:** - **Method**: GET, POST, PUT, PATCH, DELETE, HEAD - **URL**: Full endpoint URL (supports variable interpolation) - **Headers**: Custom headers as key-value pairs - **Query Parameters**: URL parameters - **Body**: JSON, form data, or raw text - **Authentication**: Basic Auth, Bearer Token, API Key, OAuth 2.0 **Example:** ```yaml - id: fetch_user action: http_request config: method: GET url: \"https://api.example.com/users/{{user_id}}\" headers: Authorization: \"Bearer {{secrets.API_TOKEN}}\" Content-Type: \"application/json\" timeout: 30 ``` **Response Handling:** Access the response in subsequent steps: - `{{steps.fetch_user.status}}` - HTTP status code - `{{steps.fetch_user.body}}` - Response body - `{{steps.fetch_user.headers}}` - Response headers ### Database Queries Execute queries against supported databases (PostgreSQL, MySQL, MongoDB, Redis): **SQL Databases (PostgreSQL, MySQL):** ```yaml - id: get_orders action: database_query config: connection: \"{{secrets.DB_CONNECTION_STRING}}\" query: | SELECT * FROM orders WHERE customer_id = $1 AND status = $2 ORDER BY created_at DESC LIMIT 10 parameters: - \"{{trigger.customer_id}}\" - \"pending\" ``` **MongoDB:** ```yaml - id: find_documents action: mongodb_query config: connection: \"{{secrets.MONGO_URI}}\" database: \"production\" collection: \"users\" operation: find filter: email: \"{{trigger.email}}\" options: limit: 1 ``` **Important Notes:** - Always use parameterized queries to prevent SQL injection - Connection strings should be stored in secrets - Query timeout is 30 seconds by default - Maximum result set size is 10MB ### Email Notifications Send emails via SMTP or integrated email providers: **Configuration:** ```yaml - id: send_notification action: email config: provider: \"smtp\" # or \"sendgrid\", \"mailgun\", \"ses\" from: \"notifications@cloudflow.io\" to: \"{{trigger.recipient}}\" cc: \"manager@company.com\" subject: \"Alert: {{alert_type}}\" body: | Hello {{user.name}}, This is an automated notification about {{event_description}}. Details: - Time: {{timestamp}} - Type: {{alert_type}} - Priority: {{priority}} Best regards, CloudFlow Automation html: true attachments: - name: \"report.pdf\" content: \"{{steps.generate_report.output}}\" encoding: \"base64\" ``` **Email Templates:** CloudFlow supports dynamic templates with conditional content: ``` {{#if priority == 'high'}} \u26a0\ufe0f URGENT: Immediate attention required {{else}} \u2139\ufe0f Informational notification {{/if}} ``` ### Slack Messages Send messages to Slack channels or users: **Channel Messages:** ```yaml - id: notify_team action: slack_message config: channel: \"#deployments\" text: \"Deployment completed successfully!\" blocks: - type: \"section\" text: type: \"mrkdwn\" text: \"*Deployment Summary*\\nEnvironment: {{environment}}\\nVersion: {{version}}\\nDuration: {{duration}}s\" - type: \"actions\" elements: - type: \"button\" text: \"View Logs\" url: \"{{logs_url}}\" thread_ts: \"{{trigger.thread_ts}}\" # Reply to thread ``` **Direct Messages:** ```yaml - id: dm_user action: slack_message config: user: \"{{user_slack_id}}\" text: \"Your report is ready for review.\"",
              "score": null
            }
          ],
          "latency_ms": 12.970932999451179,
          "query_id": "realistic_016",
          "dimension": "problem",
          "query": "My API requests are getting 401 errors"
        },
        {
          "key_facts": [
            "OAuth 2.0",
            "API keys",
            "JWT tokens"
          ],
          "found_facts": [
            "OAuth 2.0",
            "API keys",
            "JWT tokens"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "api_reference"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_1",
              "content": "**Technology**: Node.js with Express.js framework **Replicas**: 12 pods (production), auto-scaling 8-20 based on CPU **Resource Allocation**: 2 vCPU, 4GB RAM per pod **Key Responsibilities**: - JWT token validation (delegated to Auth Service for initial validation) - Rate limiting: 1000 requests per minute per API key (sliding window) - Request/response transformation and validation using JSON Schema - Routing to downstream services based on URL path patterns - CORS handling for web clients - Request/response logging and correlation ID injection **Critical Endpoints**: - `POST /api/v1/workflows` - Create new workflow - `GET /api/v1/workflows/:id` - Retrieve workflow status - `POST /api/v1/workflows/:id/execute` - Trigger workflow execution - `GET /api/v1/workflows/:id/history` - Get execution history **Dependencies**: - Auth Service (for token validation) - Redis (for rate limiting counters) - All downstream microservices **Performance Targets**: - P50 latency: < 50ms - P99 latency: < 200ms - Throughput: 10,000 RPS sustained --- ### Auth Service **Purpose**: Centralized authentication and authorization service handling user identity, token generation, and permission validation. **Technology**: Go with gRPC for internal communication, REST for external **Replicas**: 8 pods (production), auto-scaling 6-12 **Resource Allocation**: 1 vCPU, 2GB RAM per pod **Key Responsibilities**: - User authentication via multiple providers (OAuth2, SAML, local credentials) - JWT token generation and validation (RS256 algorithm) - Role-based access control (RBAC) with fine-grained permissions - Session management with Redis-backed storage - API key generation and validation for service accounts - MFA enforcement for administrative operations **Authentication Flow**: ``` Client Request \u2192 API Gateway \u2192 Auth Service \u2502 \u251c\u2500 Validate credentials \u251c\u2500 Check MFA if required \u251c\u2500 Generate JWT (15min expiry) \u251c\u2500 Generate refresh token (7 days) \u2514\u2500 Store session in Redis ``` **Token Structure**: - Access Token: JWT with 15-minute expiry - Refresh Token: Opaque token with 7-day expiry, stored in PostgreSQL - Claims: user_id, email, roles[], permissions[], tenant_id **Security Features**: - Password hashing: Argon2id with 64MB memory, 4 iterations - Token rotation on refresh to prevent replay attacks - Brute force protection: 5 failed attempts \u2192 15-minute lockout - Secrets stored in HashiCorp Vault, rotated every 90 days **Performance Targets**: - Token validation: < 10ms (P99) - Token generation: < 50ms (P99) - Throughput: 5,000 RPS for validation operations --- ### Workflow Engine **Purpose**: Core orchestration service that executes workflow definitions, manages state transitions, and coordinates task execution across distributed systems.",
              "score": null
            },
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_0",
              "content": "# CloudFlow API Reference Version 2.1.0 | Last Updated: January 2026 ## Overview The CloudFlow API is a RESTful service that enables developers to programmatically manage cloud workflows, data pipelines, and automation tasks. This documentation provides comprehensive details on authentication, endpoints, request/response formats, error handling, and best practices. **Base URL:** `https://api.cloudflow.io/v2` **API Status:** https://status.cloudflow.io ## Authentication CloudFlow supports three authentication methods to suit different use cases and security requirements. ### API Keys API keys provide simple authentication for server-to-server communication. Include your API key in the request header: ```bash curl -H \"X-API-Key: cf_live_a1b2c3d4e5f6g7h8i9j0\" \\ https://api.cloudflow.io/v2/workflows ``` **Security Notes:** - Never expose API keys in client-side code - Rotate keys every 90 days - Use separate keys for development and production environments ### OAuth 2.0 OAuth 2.0 is recommended for applications that access CloudFlow on behalf of users. We support the Authorization Code flow with PKCE. **Authorization Endpoint:** `https://auth.cloudflow.io/oauth/authorize` **Token Endpoint:** `https://auth.cloudflow.io/oauth/token` **Supported Scopes:** - `workflows:read` - Read workflow configurations - `workflows:write` - Create and modify workflows - `pipelines:read` - Read pipeline data - `pipelines:write` - Create and manage pipelines - `analytics:read` - Access analytics and metrics - `admin:full` - Full administrative access Example authorization request: ```bash curl -X POST https://auth.cloudflow.io/oauth/token \\ -H \"Content-Type: application/x-www-form-urlencoded\" \\ -d \"grant_type=authorization_code\" \\ -d \"code=AUTH_CODE_HERE\" \\ -d \"client_id=YOUR_CLIENT_ID\" \\ -d \"client_secret=YOUR_CLIENT_SECRET\" \\ -d \"redirect_uri=https://yourapp.com/callback\" ``` ### JWT Tokens For advanced use cases, CloudFlow supports JSON Web Tokens (JWT) with RS256 signing algorithm. JWTs must include the following claims: - `iss` (issuer): Your application identifier - `sub` (subject): User or service account ID - `aud` (audience): `https://api.cloudflow.io` - `exp` (expiration): Unix timestamp (max 3600 seconds from `iat`) - `iat` (issued at): Unix timestamp - `scope`: Space-separated list of requested scopes Example JWT header: ```python import jwt import time from cryptography.hazmat.primitives import serialization from cryptography.hazmat.backends import default_backend # Load your private key with open('private_key.pem', 'rb') as key_file: private_key = serialization.load_pem_private_key( key_file.read(), password=None, backend=default_backend() ) # Create JWT payload payload = { 'iss': 'your-app-id', 'sub': 'user-12345', 'aud': 'https://api.cloudflow.io', 'exp': int(time.time()) + 3600, 'iat': int(time.time()), 'scope': 'workflows:read workflows:write' } # Generate token token = jwt.encode(payload, private_key, algorithm='RS256') # Use in API request headers = {'Authorization': f'Bearer {token}'} ``` **Token Expiration:** All tokens expire after 3600 seconds (1 hour). Implement token refresh logic in your application.",
              "score": null
            },
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_2",
              "content": "**Endpoint:** `POST /workflows` **Request Body:** ```json { \"name\": \"Email Campaign Automation\", \"description\": \"Sends personalized emails based on user behavior\", \"trigger\": { \"type\": \"webhook\", \"url\": \"https://your-app.com/webhook\" }, \"steps\": [ { \"type\": \"fetch_data\", \"source\": \"users_table\", \"filters\": {\"active\": true} }, { \"type\": \"transform\", \"script\": \"data.map(user => ({ ...user, segment: calculateSegment(user) }))\" }, { \"type\": \"send_email\", \"template_id\": \"tpl_welcome_v2\" } ] } ``` **Example Request:** ```bash curl -X POST https://api.cloudflow.io/v2/workflows \\ -H \"X-API-Key: YOUR_API_KEY\" \\ -H \"Content-Type: application/json\" \\ -d '{ \"name\": \"Email Campaign Automation\", \"trigger\": {\"type\": \"webhook\"}, \"steps\": [{\"type\": \"fetch_data\", \"source\": \"users_table\"}] }' ``` **Example Response (201 Created):** ```json { \"id\": \"wf_9k3m7n2q\", \"name\": \"Email Campaign Automation\", \"status\": \"active\", \"created_at\": \"2026-01-24T10:15:00Z\", \"webhook_url\": \"https://api.cloudflow.io/v2/webhooks/wf_9k3m7n2q/trigger\" } ``` #### Get Workflow Details Retrieve detailed information about a specific workflow. **Endpoint:** `GET /workflows/{workflow_id}` **Example Request:** ```bash curl -H \"X-API-Key: YOUR_API_KEY\" \\ https://api.cloudflow.io/v2/workflows/wf_8x7k2m9p ``` **Example Response (200 OK):** ```json { \"id\": \"wf_8x7k2m9p\", \"name\": \"Data Processing Pipeline\", \"description\": \"Processes customer data every hour\", \"status\": \"active\", \"trigger\": { \"type\": \"schedule\", \"cron\": \"0 * * * *\", \"timezone\": \"UTC\" }, \"steps\": [ { \"id\": \"step_1\", \"type\": \"fetch_data\", \"source\": \"customer_db\", \"query\": \"SELECT * FROM customers WHERE updated_at > :last_run\" }, { \"id\": \"step_2\", \"type\": \"transform\", \"script_id\": \"scr_transform_v3\" } ], \"metrics\": { \"total_runs\": 1247, \"success_rate\": 98.7, \"avg_duration_ms\": 3420, \"last_error\": null }, \"created_at\": \"2026-01-15T10:30:00Z\", \"updated_at\": \"2026-01-23T14:22:00Z\" } ``` #### Update Workflow Modify an existing workflow's configuration. **Endpoint:** `PATCH /workflows/{workflow_id}` **Example Request:** ```bash curl -X PATCH https://api.cloudflow.io/v2/workflows/wf_8x7k2m9p \\ -H \"X-API-Key: YOUR_API_KEY\" \\ -H \"Content-Type: application/json\" \\ -d '{\"status\": \"paused\"}' ``` **Example Response (200 OK):** ```json { \"id\": \"wf_8x7k2m9p\", \"status\": \"paused\", \"updated_at\": \"2026-01-24T10:30:00Z\" } ``` #### Delete Workflow Permanently delete a workflow. **Endpoint:** `DELETE /workflows/{workflow_id}` **Example Request:** ```bash curl -X DELETE https://api.cloudflow.io/v2/workflows/wf_8x7k2m9p \\ -H \"X-API-Key: YOUR_API_KEY\" ``` **Example Response (204 No Content)** ### Pipelines #### List Pipeline Executions Retrieve execution history for data pipelines.",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_1",
              "content": "**Diagnosis:** ```bash # Check system time timedatectl status # Compare with NTP server ntpdate -q pool.ntp.org # Check JWT issued time vs current time jwt_iat=$(echo $CF_ACCESS_TOKEN | cut -d'.' -f2 | base64 -d | jq '.iat') current_time=$(date +%s) skew=$((current_time - jwt_iat)) echo \"Clock skew: $skew seconds\" ``` **Resolution:** ```bash # Sync with NTP server sudo ntpdate -s pool.ntp.org # Enable automatic time sync sudo timedatectl set-ntp true # Restart CloudFlow client cloudflow restart ``` **4. Insufficient Permissions** Error: `User does not have required permissions for this operation` **Check user roles:** ```bash cloudflow auth whoami --verbose # Expected output: # User: john.doe@company.com # Roles: developer, workflow-admin # Scopes: workflow:*, data:read, metrics:read ``` **Request permission elevation:** ```bash # Submit access request cloudflow auth request-access \\ --resource \"workflow:production:deploy\" \\ --justification \"Deploy critical hotfix for TICKET-1234\" # Check pending approvals cloudflow auth list-requests --status pending ``` ### 403 Forbidden Errors #### RBAC Policy Violations CloudFlow uses role-based access control (RBAC) with the following hierarchy: - `viewer` - Read-only access - `developer` - Create and modify workflows (non-production) - `operator` - Execute workflows, view logs - `admin` - Full access to workspace - `platform-admin` - Cross-workspace administration **Verify resource permissions:** ```bash # Check effective permissions cloudflow rbac check \\ --user john.doe@company.com \\ --resource workflow:prod-pipeline \\ --action execute # List all policies affecting user cloudflow rbac policies --user john.doe@company.com --verbose ``` --- ## Performance Problems ### Slow Query Performance #### Symptoms - API response times > 5000ms - Database query latency warnings in logs - CloudFlow UI becomes unresponsive - Timeout errors: `Request exceeded maximum duration of 30000ms` #### Diagnosis Steps **1. Identify Slow Queries** ```bash # Query CloudFlow metrics cloudflow metrics query --metric api_request_duration_ms \\ --filter \"p95 > 5000\" \\ --last 1h # Check database slow query log kubectl logs -n cloudflow deploy/cloudflow-db-primary | \\ grep \"slow query\" | \\ tail -n 50 # Analyze query patterns cloudflow db analyze-queries --min-duration 5000 --limit 20 ``` **2.",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_0",
              "content": "# CloudFlow Platform Troubleshooting Guide **Version:** 3.2.1 **Last Updated:** January 2026 **Audience:** Platform Engineers, DevOps, Support Teams ## Table of Contents 1. [Overview](#overview) 2. [Authentication & Authorization Issues](#authentication--authorization-issues) 3. [Performance Problems](#performance-problems) 4. [Database Connection Issues](#database-connection-issues) 5. [Workflow Execution Failures](#workflow-execution-failures) 6. [Rate Limiting & Throttling](#rate-limiting--throttling) 7. [Log Analysis & Debugging](#log-analysis--debugging) 8. [Escalation Procedures](#escalation-procedures) --- ## Overview This guide provides comprehensive troubleshooting steps for common CloudFlow platform issues encountered in production environments. Each section includes error symptoms, root cause analysis, resolution steps, and preventive measures. ### Quick Diagnostic Checklist Before diving into specific issues, perform these initial checks: - Verify service health: `cloudflow status --all` - Check API connectivity: `curl -I https://api.cloudflow.io/health` - Review recent deployments: `kubectl get deployments -n cloudflow --sort-by=.metadata.creationTimestamp` - Inspect platform metrics: `cloudflow metrics --last 1h` --- ## Authentication & Authorization Issues ### 401 Unauthorized Errors #### Symptoms - API requests return `401 Unauthorized` - Error message: `Authentication credentials were not provided or are invalid` - Frontend displays \"Session expired\" message #### Common Causes **1. Token Expiration** CloudFlow access tokens expire after 3600 seconds (1 hour) by default. Refresh tokens are valid for 30 days. **Verification:** ```bash # Decode JWT to check expiration echo $CF_ACCESS_TOKEN | cut -d'.' -f2 | base64 -d | jq '.exp' # Compare with current time date +%s ``` **Resolution:** ```bash # Refresh the access token curl -X POST https://api.cloudflow.io/auth/refresh \\ -H \"Content-Type: application/json\" \\ -d '{\"refresh_token\": \"'$CF_REFRESH_TOKEN'\"}' # Update environment variable export CF_ACCESS_TOKEN=\"<new_token>\" ``` **2. Invalid JWT Signature** Error: `JWT signature verification failed` **Causes:** - Token was modified or corrupted - Using wrong signing key - Token generated with different secret **Resolution:** ```bash # Validate token structure cloudflow auth validate --token $CF_ACCESS_TOKEN # Generate new token cloudflow auth login --username <user> --password <pass> # For service accounts cloudflow auth service-account --create --name <sa-name> --scopes \"workflow:read,workflow:write\" ``` **3. Clock Skew Issues** JWT validation fails when system clocks are out of sync (tolerance: \u00b1300 seconds).",
              "score": null
            }
          ],
          "latency_ms": 12.815853999200044,
          "query_id": "realistic_016",
          "dimension": "casual",
          "query": "auth methods"
        },
        {
          "key_facts": [
            "OAuth 2.0",
            "API keys",
            "JWT tokens"
          ],
          "found_facts": [
            "OAuth 2.0",
            "API keys",
            "JWT tokens"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "api_reference"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_0",
              "content": "# CloudFlow API Reference Version 2.1.0 | Last Updated: January 2026 ## Overview The CloudFlow API is a RESTful service that enables developers to programmatically manage cloud workflows, data pipelines, and automation tasks. This documentation provides comprehensive details on authentication, endpoints, request/response formats, error handling, and best practices. **Base URL:** `https://api.cloudflow.io/v2` **API Status:** https://status.cloudflow.io ## Authentication CloudFlow supports three authentication methods to suit different use cases and security requirements. ### API Keys API keys provide simple authentication for server-to-server communication. Include your API key in the request header: ```bash curl -H \"X-API-Key: cf_live_a1b2c3d4e5f6g7h8i9j0\" \\ https://api.cloudflow.io/v2/workflows ``` **Security Notes:** - Never expose API keys in client-side code - Rotate keys every 90 days - Use separate keys for development and production environments ### OAuth 2.0 OAuth 2.0 is recommended for applications that access CloudFlow on behalf of users. We support the Authorization Code flow with PKCE. **Authorization Endpoint:** `https://auth.cloudflow.io/oauth/authorize` **Token Endpoint:** `https://auth.cloudflow.io/oauth/token` **Supported Scopes:** - `workflows:read` - Read workflow configurations - `workflows:write` - Create and modify workflows - `pipelines:read` - Read pipeline data - `pipelines:write` - Create and manage pipelines - `analytics:read` - Access analytics and metrics - `admin:full` - Full administrative access Example authorization request: ```bash curl -X POST https://auth.cloudflow.io/oauth/token \\ -H \"Content-Type: application/x-www-form-urlencoded\" \\ -d \"grant_type=authorization_code\" \\ -d \"code=AUTH_CODE_HERE\" \\ -d \"client_id=YOUR_CLIENT_ID\" \\ -d \"client_secret=YOUR_CLIENT_SECRET\" \\ -d \"redirect_uri=https://yourapp.com/callback\" ``` ### JWT Tokens For advanced use cases, CloudFlow supports JSON Web Tokens (JWT) with RS256 signing algorithm. JWTs must include the following claims: - `iss` (issuer): Your application identifier - `sub` (subject): User or service account ID - `aud` (audience): `https://api.cloudflow.io` - `exp` (expiration): Unix timestamp (max 3600 seconds from `iat`) - `iat` (issued at): Unix timestamp - `scope`: Space-separated list of requested scopes Example JWT header: ```python import jwt import time from cryptography.hazmat.primitives import serialization from cryptography.hazmat.backends import default_backend # Load your private key with open('private_key.pem', 'rb') as key_file: private_key = serialization.load_pem_private_key( key_file.read(), password=None, backend=default_backend() ) # Create JWT payload payload = { 'iss': 'your-app-id', 'sub': 'user-12345', 'aud': 'https://api.cloudflow.io', 'exp': int(time.time()) + 3600, 'iat': int(time.time()), 'scope': 'workflows:read workflows:write' } # Generate token token = jwt.encode(payload, private_key, algorithm='RS256') # Use in API request headers = {'Authorization': f'Bearer {token}'} ``` **Token Expiration:** All tokens expire after 3600 seconds (1 hour). Implement token refresh logic in your application.",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_0",
              "content": "# CloudFlow Platform Troubleshooting Guide **Version:** 3.2.1 **Last Updated:** January 2026 **Audience:** Platform Engineers, DevOps, Support Teams ## Table of Contents 1. [Overview](#overview) 2. [Authentication & Authorization Issues](#authentication--authorization-issues) 3. [Performance Problems](#performance-problems) 4. [Database Connection Issues](#database-connection-issues) 5. [Workflow Execution Failures](#workflow-execution-failures) 6. [Rate Limiting & Throttling](#rate-limiting--throttling) 7. [Log Analysis & Debugging](#log-analysis--debugging) 8. [Escalation Procedures](#escalation-procedures) --- ## Overview This guide provides comprehensive troubleshooting steps for common CloudFlow platform issues encountered in production environments. Each section includes error symptoms, root cause analysis, resolution steps, and preventive measures. ### Quick Diagnostic Checklist Before diving into specific issues, perform these initial checks: - Verify service health: `cloudflow status --all` - Check API connectivity: `curl -I https://api.cloudflow.io/health` - Review recent deployments: `kubectl get deployments -n cloudflow --sort-by=.metadata.creationTimestamp` - Inspect platform metrics: `cloudflow metrics --last 1h` --- ## Authentication & Authorization Issues ### 401 Unauthorized Errors #### Symptoms - API requests return `401 Unauthorized` - Error message: `Authentication credentials were not provided or are invalid` - Frontend displays \"Session expired\" message #### Common Causes **1. Token Expiration** CloudFlow access tokens expire after 3600 seconds (1 hour) by default. Refresh tokens are valid for 30 days. **Verification:** ```bash # Decode JWT to check expiration echo $CF_ACCESS_TOKEN | cut -d'.' -f2 | base64 -d | jq '.exp' # Compare with current time date +%s ``` **Resolution:** ```bash # Refresh the access token curl -X POST https://api.cloudflow.io/auth/refresh \\ -H \"Content-Type: application/json\" \\ -d '{\"refresh_token\": \"'$CF_REFRESH_TOKEN'\"}' # Update environment variable export CF_ACCESS_TOKEN=\"<new_token>\" ``` **2. Invalid JWT Signature** Error: `JWT signature verification failed` **Causes:** - Token was modified or corrupted - Using wrong signing key - Token generated with different secret **Resolution:** ```bash # Validate token structure cloudflow auth validate --token $CF_ACCESS_TOKEN # Generate new token cloudflow auth login --username <user> --password <pass> # For service accounts cloudflow auth service-account --create --name <sa-name> --scopes \"workflow:read,workflow:write\" ``` **3. Clock Skew Issues** JWT validation fails when system clocks are out of sync (tolerance: \u00b1300 seconds).",
              "score": null
            },
            {
              "doc_id": "user_guide",
              "chunk_id": "user_guide_fix_3",
              "content": "``` **Slack App Integration:** - Install the CloudFlow Slack app in your workspace - Authenticate once per workspace - Use @mentions, emojis, and rich formatting - Send messages as a bot or as yourself ## Variables and Expressions CloudFlow uses a powerful templating system to work with dynamic data in your workflows. ### Variable Syntax Variables are enclosed in double curly braces: `{{variable_name}}` **Accessing Trigger Data:** ``` {{trigger.body.order_id}} {{trigger.headers.user_agent}} {{trigger.query.page}} ``` **Accessing Step Outputs:** ``` {{steps.step_id.output}} {{steps.fetch_user.body.email}} {{steps.query_db.rows[0].name}} ``` **System Variables:** ``` {{workflow.id}} # Current workflow ID {{workflow.name}} # Workflow name {{execution.id}} # Current execution ID {{execution.started_at}} # Execution start timestamp {{env.ENVIRONMENT}} # Environment variable {{secrets.API_KEY}} # Secret value (encrypted at rest) ``` ### Built-in Functions CloudFlow provides built-in functions for common data transformations: **String Functions:** ``` {{upper(user.name)}} # Convert to uppercase {{lower(email)}} # Convert to lowercase {{trim(input)}} # Remove whitespace {{substring(text, 0, 10)}} # Extract substring {{replace(text, \"old\", \"new\")}} # Replace text {{split(csv_string, \",\")}} # Split string into array {{join(array, \", \")}} # Join array into string ``` **Date/Time Functions:** ``` {{now()}} # Current timestamp (ISO 8601) {{now(\"America/New_York\")}} # Current time in timezone {{format_date(timestamp, \"YYYY-MM-DD\")}} # Format date {{add_days(date, 7)}} # Add days to date {{diff_hours(date1, date2)}} # Difference in hours ``` **Math Functions:** ``` {{add(num1, num2)}} # Addition {{subtract(num1, num2)}} # Subtraction {{multiply(num1, num2)}} # Multiplication {{divide(num1, num2)}} # Division {{round(number, 2)}} # Round to decimal places {{max(array)}} # Maximum value {{min(array)}} # Minimum value ``` **Array Functions:** ``` {{length(array)}} # Array length {{first(array)}} # First element {{last(array)}} # Last element {{filter(array, \"status\", \"active\")}} # Filter array {{map(array, \"id\")}} # Extract property from objects {{unique(array)}} # Remove duplicates ``` **Conditional Functions:** ``` {{if(condition, \"true_value\", \"false_value\")}} {{default(value, \"fallback\")}} # Use fallback if value is null/undefined ``` ### Expression Examples **Complex nested expressions:** ``` {{upper(trim(steps.get_user.body.name))}} ``` **Conditional formatting:** ``` Order Status: {{if(steps.check_inventory.in_stock, \"\u2713 Available\", \"\u2717 Out of Stock\")}} ``` **Date calculations:** ``` Due Date: {{format_date(add_days(now(), 7), \"MMMM DD, YYYY\")}} ``` **Dynamic URLs:** ``` https://api.example.com/v1/users/{{user_id}}/orders?status={{status}}&limit={{default(limit, 10)}} ``` ## Scheduling CloudFlow supports powerful scheduling options for recurring workflows.",
              "score": null
            },
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_6",
              "content": "**Verify Service Health**: ```bash kubectl get pods -n cloudflow-prod curl https://api.cloudflow.io/health ``` **Recovery Time Objective (RTO)**: 4 hours **Recovery Point Objective (RPO)**: 24 hours --- ## Scaling and Performance ### Horizontal Pod Autoscaling CloudFlow is configured with Horizontal Pod Autoscaler (HPA) to automatically scale based on resource utilization: ```yaml # hpa.yaml apiVersion: autoscaling/v2 kind: HorizontalPodAutoscaler metadata: name: cloudflow-api-hpa namespace: cloudflow-prod spec: scaleTargetRef: apiVersion: apps/v1 kind: Deployment name: cloudflow-api minReplicas: 3 maxReplicas: 10 metrics: - type: Resource resource: name: cpu target: type: Utilization averageUtilization: 70 - type: Resource resource: name: memory target: type: Utilization averageUtilization: 80 behavior: scaleDown: stabilizationWindowSeconds: 300 policies: - type: Percent value: 50 periodSeconds: 60 scaleUp: stabilizationWindowSeconds: 60 policies: - type: Percent value: 100 periodSeconds: 30 - type: Pods value: 2 periodSeconds: 30 selectPolicy: Max ``` Monitor autoscaling events: ```bash kubectl get hpa -n cloudflow-prod -w kubectl describe hpa cloudflow-api-hpa -n cloudflow-prod ``` ### Performance Optimization #### Connection Pooling Ensure optimal database connection pooling settings in PgBouncer: - **Pool Mode**: Transaction (optimal for microservices) - **Default Pool Size**: 25 connections per user - **Max DB Connections**: 100 (matches PostgreSQL `max_connections`) #### Redis Caching Implement Redis caching for frequently accessed data: ```javascript // Example caching strategy const CACHE_TTL = { workflows: 300, // 5 minutes userSessions: 3600, // 1 hour apiResults: 60 // 1 minute }; ``` #### Load Testing Perform regular load testing to validate scaling configuration: ```bash # Install k6 for load testing kubectl apply -f - <<EOF apiVersion: v1 kind: ConfigMap metadata: name: loadtest-script namespace: cloudflow-prod data: script.js: | import http from 'k6/http'; import { check, sleep } from 'k6'; export let options = { stages: [ { duration: '2m', target: 100 }, { duration: '5m', target: 100 }, { duration: '2m', target: 200 }, { duration: '5m', target: 200 }, { duration: '2m', target: 0 }, ], thresholds: { http_req_duration: ['p(95)<500'], }, }; export default function () { let res = http.get('https://api.cloudflow.io/api/workflows'); check(res, { 'status is 200': (r) => r.status === 200 }); sleep(1); } EOF ``` **Performance Targets**: - P95 API latency: < 500ms - P99 API latency: < 1000ms - Error rate: < 0.1% - Throughput: > 1000 requests/second --- ## Security and Compliance ### Network Policies Implement network policies to restrict pod-to-pod communication: ```yaml # network-policy.yaml apiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: cloudflow-network-policy namespace: cloudflow-prod spec:",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_8",
              "content": "Leverage caching:** ```bash # Enable client-side caching export CLOUDFLOW_CACHE_ENABLED=true export CLOUDFLOW_CACHE_TTL=300 # Cache workflow metadata cloudflow workflows list --use-cache --cache-ttl 600 ``` --- ## Log Analysis & Debugging ### Accessing CloudFlow Logs #### Kubernetes Deployments ```bash # List all CloudFlow pods kubectl get pods -n cloudflow # Tail logs from API server kubectl logs -f -n cloudflow deployment/cloudflow-api --tail=100 # Get logs from specific pod kubectl logs -n cloudflow cloudflow-api-7d4f6b8c9d-x7k2m # Get logs from previous crashed pod kubectl logs -n cloudflow cloudflow-api-7d4f6b8c9d-x7k2m --previous # Get logs from all pods in deployment kubectl logs -n cloudflow deployment/cloudflow-api --all-containers=true # Stream logs from multiple pods kubectl logs -n cloudflow -l app=cloudflow-api -f --max-log-requests=10 ``` #### Log Levels CloudFlow supports the following log levels: - `TRACE` - Very detailed debugging information - `DEBUG` - Detailed debugging information - `INFO` - Informational messages (default) - `WARN` - Warning messages - `ERROR` - Error messages - `FATAL` - Fatal errors causing shutdown **Changing log levels:** ```bash # Set global log level cloudflow config set logging.level DEBUG # Set log level for specific component cloudflow config set logging.components.database DEBUG cloudflow config set logging.components.auth INFO cloudflow config set logging.components.workflows TRACE # Temporary log level increase (resets after 1 hour) cloudflow debug set-log-level DEBUG --duration 3600 # View current log configuration cloudflow config get logging --format json ``` ### Grep Patterns for Common Issues #### Authentication Failures ```bash # Find all authentication errors kubectl logs -n cloudflow deployment/cloudflow-api | \\ grep -i \"authentication\\|401\\|unauthorized\" | \\ tail -n 50 # Find JWT validation failures kubectl logs -n cloudflow deployment/cloudflow-api | \\ grep -E \"JWT|token.*invalid|signature.*failed\" # Find clock skew issues kubectl logs -n cloudflow deployment/cloudflow-api | \\ grep -i \"clock skew\\|time.*sync\\|nbf\\|exp\" ``` #### Database Errors ```bash # Find database connection errors kubectl logs -n cloudflow deployment/cloudflow-api | \\ grep -E \"connection.*pool|could not connect|database.*timeout\" # Find slow queries kubectl logs -n cloudflow deployment/cloudflow-api | \\ grep \"slow query\" | \\ awk '{print $NF}' | \\ sort -n | \\ tail -n 20 # Find deadlock errors kubectl logs -n cloudflow deployment/cloudflow-api | \\ grep -i \"deadlock detected\" ``` #### Workflow Execution Errors ```bash # Find workflow timeout errors kubectl logs -n cloudflow deployment/cloudflow-workflow-engine | \\ grep -E \"timeout|exceeded.*3600\" # Find workflow retry attempts kubectl logs -n cloudflow deployment/cloudflow-workflow-engine | \\ grep -E \"retry attempt [0-9]|retrying",
              "score": null
            }
          ],
          "latency_ms": 13.155676999303978,
          "query_id": "realistic_016",
          "dimension": "contextual",
          "query": "I'm integrating with CloudFlow API, what authentication options do I have?"
        },
        {
          "key_facts": [
            "OAuth 2.0",
            "API keys",
            "JWT tokens"
          ],
          "found_facts": [
            "OAuth 2.0"
          ],
          "missed_facts": [
            "API keys",
            "JWT tokens"
          ],
          "coverage": 0.3333333333333333,
          "expected_docs": [
            "api_reference"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_0",
              "content": "# CloudFlow Platform Troubleshooting Guide **Version:** 3.2.1 **Last Updated:** January 2026 **Audience:** Platform Engineers, DevOps, Support Teams ## Table of Contents 1. [Overview](#overview) 2. [Authentication & Authorization Issues](#authentication--authorization-issues) 3. [Performance Problems](#performance-problems) 4. [Database Connection Issues](#database-connection-issues) 5. [Workflow Execution Failures](#workflow-execution-failures) 6. [Rate Limiting & Throttling](#rate-limiting--throttling) 7. [Log Analysis & Debugging](#log-analysis--debugging) 8. [Escalation Procedures](#escalation-procedures) --- ## Overview This guide provides comprehensive troubleshooting steps for common CloudFlow platform issues encountered in production environments. Each section includes error symptoms, root cause analysis, resolution steps, and preventive measures. ### Quick Diagnostic Checklist Before diving into specific issues, perform these initial checks: - Verify service health: `cloudflow status --all` - Check API connectivity: `curl -I https://api.cloudflow.io/health` - Review recent deployments: `kubectl get deployments -n cloudflow --sort-by=.metadata.creationTimestamp` - Inspect platform metrics: `cloudflow metrics --last 1h` --- ## Authentication & Authorization Issues ### 401 Unauthorized Errors #### Symptoms - API requests return `401 Unauthorized` - Error message: `Authentication credentials were not provided or are invalid` - Frontend displays \"Session expired\" message #### Common Causes **1. Token Expiration** CloudFlow access tokens expire after 3600 seconds (1 hour) by default. Refresh tokens are valid for 30 days. **Verification:** ```bash # Decode JWT to check expiration echo $CF_ACCESS_TOKEN | cut -d'.' -f2 | base64 -d | jq '.exp' # Compare with current time date +%s ``` **Resolution:** ```bash # Refresh the access token curl -X POST https://api.cloudflow.io/auth/refresh \\ -H \"Content-Type: application/json\" \\ -d '{\"refresh_token\": \"'$CF_REFRESH_TOKEN'\"}' # Update environment variable export CF_ACCESS_TOKEN=\"<new_token>\" ``` **2. Invalid JWT Signature** Error: `JWT signature verification failed` **Causes:** - Token was modified or corrupted - Using wrong signing key - Token generated with different secret **Resolution:** ```bash # Validate token structure cloudflow auth validate --token $CF_ACCESS_TOKEN # Generate new token cloudflow auth login --username <user> --password <pass> # For service accounts cloudflow auth service-account --create --name <sa-name> --scopes \"workflow:read,workflow:write\" ``` **3. Clock Skew Issues** JWT validation fails when system clocks are out of sync (tolerance: \u00b1300 seconds).",
              "score": null
            },
            {
              "doc_id": "user_guide",
              "chunk_id": "user_guide_fix_2",
              "content": "### HTTP Requests Make HTTP requests to any API endpoint: **Configuration:** - **Method**: GET, POST, PUT, PATCH, DELETE, HEAD - **URL**: Full endpoint URL (supports variable interpolation) - **Headers**: Custom headers as key-value pairs - **Query Parameters**: URL parameters - **Body**: JSON, form data, or raw text - **Authentication**: Basic Auth, Bearer Token, API Key, OAuth 2.0 **Example:** ```yaml - id: fetch_user action: http_request config: method: GET url: \"https://api.example.com/users/{{user_id}}\" headers: Authorization: \"Bearer {{secrets.API_TOKEN}}\" Content-Type: \"application/json\" timeout: 30 ``` **Response Handling:** Access the response in subsequent steps: - `{{steps.fetch_user.status}}` - HTTP status code - `{{steps.fetch_user.body}}` - Response body - `{{steps.fetch_user.headers}}` - Response headers ### Database Queries Execute queries against supported databases (PostgreSQL, MySQL, MongoDB, Redis): **SQL Databases (PostgreSQL, MySQL):** ```yaml - id: get_orders action: database_query config: connection: \"{{secrets.DB_CONNECTION_STRING}}\" query: | SELECT * FROM orders WHERE customer_id = $1 AND status = $2 ORDER BY created_at DESC LIMIT 10 parameters: - \"{{trigger.customer_id}}\" - \"pending\" ``` **MongoDB:** ```yaml - id: find_documents action: mongodb_query config: connection: \"{{secrets.MONGO_URI}}\" database: \"production\" collection: \"users\" operation: find filter: email: \"{{trigger.email}}\" options: limit: 1 ``` **Important Notes:** - Always use parameterized queries to prevent SQL injection - Connection strings should be stored in secrets - Query timeout is 30 seconds by default - Maximum result set size is 10MB ### Email Notifications Send emails via SMTP or integrated email providers: **Configuration:** ```yaml - id: send_notification action: email config: provider: \"smtp\" # or \"sendgrid\", \"mailgun\", \"ses\" from: \"notifications@cloudflow.io\" to: \"{{trigger.recipient}}\" cc: \"manager@company.com\" subject: \"Alert: {{alert_type}}\" body: | Hello {{user.name}}, This is an automated notification about {{event_description}}. Details: - Time: {{timestamp}} - Type: {{alert_type}} - Priority: {{priority}} Best regards, CloudFlow Automation html: true attachments: - name: \"report.pdf\" content: \"{{steps.generate_report.output}}\" encoding: \"base64\" ``` **Email Templates:** CloudFlow supports dynamic templates with conditional content: ``` {{#if priority == 'high'}} \u26a0\ufe0f URGENT: Immediate attention required {{else}} \u2139\ufe0f Informational notification {{/if}} ``` ### Slack Messages Send messages to Slack channels or users: **Channel Messages:** ```yaml - id: notify_team action: slack_message config: channel: \"#deployments\" text: \"Deployment completed successfully!\" blocks: - type: \"section\" text: type: \"mrkdwn\" text: \"*Deployment Summary*\\nEnvironment: {{environment}}\\nVersion: {{version}}\\nDuration: {{duration}}s\" - type: \"actions\" elements: - type: \"button\" text: \"View Logs\" url: \"{{logs_url}}\" thread_ts: \"{{trigger.thread_ts}}\" # Reply to thread ``` **Direct Messages:** ```yaml - id: dm_user action: slack_message config: user: \"{{user_slack_id}}\" text: \"Your report is ready for review.\"",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_1",
              "content": "**Diagnosis:** ```bash # Check system time timedatectl status # Compare with NTP server ntpdate -q pool.ntp.org # Check JWT issued time vs current time jwt_iat=$(echo $CF_ACCESS_TOKEN | cut -d'.' -f2 | base64 -d | jq '.iat') current_time=$(date +%s) skew=$((current_time - jwt_iat)) echo \"Clock skew: $skew seconds\" ``` **Resolution:** ```bash # Sync with NTP server sudo ntpdate -s pool.ntp.org # Enable automatic time sync sudo timedatectl set-ntp true # Restart CloudFlow client cloudflow restart ``` **4. Insufficient Permissions** Error: `User does not have required permissions for this operation` **Check user roles:** ```bash cloudflow auth whoami --verbose # Expected output: # User: john.doe@company.com # Roles: developer, workflow-admin # Scopes: workflow:*, data:read, metrics:read ``` **Request permission elevation:** ```bash # Submit access request cloudflow auth request-access \\ --resource \"workflow:production:deploy\" \\ --justification \"Deploy critical hotfix for TICKET-1234\" # Check pending approvals cloudflow auth list-requests --status pending ``` ### 403 Forbidden Errors #### RBAC Policy Violations CloudFlow uses role-based access control (RBAC) with the following hierarchy: - `viewer` - Read-only access - `developer` - Create and modify workflows (non-production) - `operator` - Execute workflows, view logs - `admin` - Full access to workspace - `platform-admin` - Cross-workspace administration **Verify resource permissions:** ```bash # Check effective permissions cloudflow rbac check \\ --user john.doe@company.com \\ --resource workflow:prod-pipeline \\ --action execute # List all policies affecting user cloudflow rbac policies --user john.doe@company.com --verbose ``` --- ## Performance Problems ### Slow Query Performance #### Symptoms - API response times > 5000ms - Database query latency warnings in logs - CloudFlow UI becomes unresponsive - Timeout errors: `Request exceeded maximum duration of 30000ms` #### Diagnosis Steps **1. Identify Slow Queries** ```bash # Query CloudFlow metrics cloudflow metrics query --metric api_request_duration_ms \\ --filter \"p95 > 5000\" \\ --last 1h # Check database slow query log kubectl logs -n cloudflow deploy/cloudflow-db-primary | \\ grep \"slow query\" | \\ tail -n 50 # Analyze query patterns cloudflow db analyze-queries --min-duration 5000 --limit 20 ``` **2.",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_1",
              "content": "**Technology**: Node.js with Express.js framework **Replicas**: 12 pods (production), auto-scaling 8-20 based on CPU **Resource Allocation**: 2 vCPU, 4GB RAM per pod **Key Responsibilities**: - JWT token validation (delegated to Auth Service for initial validation) - Rate limiting: 1000 requests per minute per API key (sliding window) - Request/response transformation and validation using JSON Schema - Routing to downstream services based on URL path patterns - CORS handling for web clients - Request/response logging and correlation ID injection **Critical Endpoints**: - `POST /api/v1/workflows` - Create new workflow - `GET /api/v1/workflows/:id` - Retrieve workflow status - `POST /api/v1/workflows/:id/execute` - Trigger workflow execution - `GET /api/v1/workflows/:id/history` - Get execution history **Dependencies**: - Auth Service (for token validation) - Redis (for rate limiting counters) - All downstream microservices **Performance Targets**: - P50 latency: < 50ms - P99 latency: < 200ms - Throughput: 10,000 RPS sustained --- ### Auth Service **Purpose**: Centralized authentication and authorization service handling user identity, token generation, and permission validation. **Technology**: Go with gRPC for internal communication, REST for external **Replicas**: 8 pods (production), auto-scaling 6-12 **Resource Allocation**: 1 vCPU, 2GB RAM per pod **Key Responsibilities**: - User authentication via multiple providers (OAuth2, SAML, local credentials) - JWT token generation and validation (RS256 algorithm) - Role-based access control (RBAC) with fine-grained permissions - Session management with Redis-backed storage - API key generation and validation for service accounts - MFA enforcement for administrative operations **Authentication Flow**: ``` Client Request \u2192 API Gateway \u2192 Auth Service \u2502 \u251c\u2500 Validate credentials \u251c\u2500 Check MFA if required \u251c\u2500 Generate JWT (15min expiry) \u251c\u2500 Generate refresh token (7 days) \u2514\u2500 Store session in Redis ``` **Token Structure**: - Access Token: JWT with 15-minute expiry - Refresh Token: Opaque token with 7-day expiry, stored in PostgreSQL - Claims: user_id, email, roles[], permissions[], tenant_id **Security Features**: - Password hashing: Argon2id with 64MB memory, 4 iterations - Token rotation on refresh to prevent replay attacks - Brute force protection: 5 failed attempts \u2192 15-minute lockout - Secrets stored in HashiCorp Vault, rotated every 90 days **Performance Targets**: - Token validation: < 10ms (P99) - Token generation: < 50ms (P99) - Throughput: 5,000 RPS for validation operations --- ### Workflow Engine **Purpose**: Core orchestration service that executes workflow definitions, manages state transitions, and coordinates task execution across distributed systems.",
              "score": null
            },
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_2",
              "content": "**Endpoint:** `POST /workflows` **Request Body:** ```json { \"name\": \"Email Campaign Automation\", \"description\": \"Sends personalized emails based on user behavior\", \"trigger\": { \"type\": \"webhook\", \"url\": \"https://your-app.com/webhook\" }, \"steps\": [ { \"type\": \"fetch_data\", \"source\": \"users_table\", \"filters\": {\"active\": true} }, { \"type\": \"transform\", \"script\": \"data.map(user => ({ ...user, segment: calculateSegment(user) }))\" }, { \"type\": \"send_email\", \"template_id\": \"tpl_welcome_v2\" } ] } ``` **Example Request:** ```bash curl -X POST https://api.cloudflow.io/v2/workflows \\ -H \"X-API-Key: YOUR_API_KEY\" \\ -H \"Content-Type: application/json\" \\ -d '{ \"name\": \"Email Campaign Automation\", \"trigger\": {\"type\": \"webhook\"}, \"steps\": [{\"type\": \"fetch_data\", \"source\": \"users_table\"}] }' ``` **Example Response (201 Created):** ```json { \"id\": \"wf_9k3m7n2q\", \"name\": \"Email Campaign Automation\", \"status\": \"active\", \"created_at\": \"2026-01-24T10:15:00Z\", \"webhook_url\": \"https://api.cloudflow.io/v2/webhooks/wf_9k3m7n2q/trigger\" } ``` #### Get Workflow Details Retrieve detailed information about a specific workflow. **Endpoint:** `GET /workflows/{workflow_id}` **Example Request:** ```bash curl -H \"X-API-Key: YOUR_API_KEY\" \\ https://api.cloudflow.io/v2/workflows/wf_8x7k2m9p ``` **Example Response (200 OK):** ```json { \"id\": \"wf_8x7k2m9p\", \"name\": \"Data Processing Pipeline\", \"description\": \"Processes customer data every hour\", \"status\": \"active\", \"trigger\": { \"type\": \"schedule\", \"cron\": \"0 * * * *\", \"timezone\": \"UTC\" }, \"steps\": [ { \"id\": \"step_1\", \"type\": \"fetch_data\", \"source\": \"customer_db\", \"query\": \"SELECT * FROM customers WHERE updated_at > :last_run\" }, { \"id\": \"step_2\", \"type\": \"transform\", \"script_id\": \"scr_transform_v3\" } ], \"metrics\": { \"total_runs\": 1247, \"success_rate\": 98.7, \"avg_duration_ms\": 3420, \"last_error\": null }, \"created_at\": \"2026-01-15T10:30:00Z\", \"updated_at\": \"2026-01-23T14:22:00Z\" } ``` #### Update Workflow Modify an existing workflow's configuration. **Endpoint:** `PATCH /workflows/{workflow_id}` **Example Request:** ```bash curl -X PATCH https://api.cloudflow.io/v2/workflows/wf_8x7k2m9p \\ -H \"X-API-Key: YOUR_API_KEY\" \\ -H \"Content-Type: application/json\" \\ -d '{\"status\": \"paused\"}' ``` **Example Response (200 OK):** ```json { \"id\": \"wf_8x7k2m9p\", \"status\": \"paused\", \"updated_at\": \"2026-01-24T10:30:00Z\" } ``` #### Delete Workflow Permanently delete a workflow. **Endpoint:** `DELETE /workflows/{workflow_id}` **Example Request:** ```bash curl -X DELETE https://api.cloudflow.io/v2/workflows/wf_8x7k2m9p \\ -H \"X-API-Key: YOUR_API_KEY\" ``` **Example Response (204 No Content)** ### Pipelines #### List Pipeline Executions Retrieve execution history for data pipelines.",
              "score": null
            }
          ],
          "latency_ms": 12.800786000298103,
          "query_id": "realistic_016",
          "dimension": "negation",
          "query": "Why isn't basic auth working?"
        },
        {
          "key_facts": [
            "PgBouncer",
            "connection pooling",
            "max_db_connections = 100",
            "default_pool_size = 25",
            "pool_mode = transaction"
          ],
          "found_facts": [
            "PgBouncer",
            "connection pooling",
            "max_db_connections = 100",
            "default_pool_size = 25",
            "pool_mode = transaction"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "deployment_guide",
            "architecture_overview"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_4",
              "content": "**Implement connection pooling optimization:** ```bash # Use PgBouncer for connection pooling kubectl apply -f cloudflow-pgbouncer.yaml # Configure CloudFlow to use PgBouncer cloudflow config set db.host pgbouncer.cloudflow.svc.cluster.local cloudflow config set db.port 6432 cloudflow config set db.pool.mode transaction ``` 2. **Add read replicas:** ```bash # Route read-only queries to replicas cloudflow db replicas add --count 2 cloudflow config set db.read_replicas \"replica-1:5432,replica-2:5432\" cloudflow config set db.read_write_split true ``` ### Connection Timeout Errors #### Error Messages - `connection timeout after 30000ms` - `could not connect to database server at 10.0.2.45:5432` - `database server unreachable` #### Troubleshooting Steps ```bash # Test network connectivity telnet cloudflow-db.internal.company.com 5432 # Check DNS resolution nslookup cloudflow-db.internal.company.com # Verify database is accepting connections pg_isready -h cloudflow-db.internal.company.com -p 5432 -U cloudflow # Check firewall rules sudo iptables -L -n | grep 5432 # Test from CloudFlow pod network kubectl run -n cloudflow debug-pod --rm -i --tty \\ --image=postgres:14 -- \\ psql -h cloudflow-db.internal.company.com -U cloudflow -d cloudflow_production # Review database logs for connection rejections kubectl logs -n cloudflow statefulset/cloudflow-db --tail=100 | \\ grep -i \"connection\\|reject\\|authentication\" ``` ### Maximum Connection Limit (100) Reached This is a hard limit in CloudFlow's database tier. #### Permanent Solutions **Option 1: Upgrade database tier** ```bash # Check available tiers cloudflow db tiers list # Upgrade to higher tier (supports 200 connections) cloudflow db upgrade --tier standard-plus --confirm # Monitor migration progress cloudflow db migration status ``` **Option 2: Implement aggressive connection reuse** ```bash # Reduce connection lifetime cloudflow config set db.pool.max_lifetime 600 # 10 minutes # Enable prepared statement caching cloudflow config set db.prepared_statements.cache true cloudflow config set db.prepared_statements.max_size 250 # Reduce idle connection timeout cloudflow config set db.pool.idle_timeout 120 # 2 minutes ``` --- ## Workflow Execution Failures ### Timeout Errors (3600 second limit) #### Error Message ``` WorkflowExecutionError: Workflow exceeded maximum execution time of 3600 seconds Status: TIMEOUT Workflow ID: wf_9k2n4m8p1q Execution ID: exec_7h3j6k9m2n ``` #### Analysis ```bash # Get workflow execution details cloudflow workflows executions get exec_7h3j6k9m2n --verbose # Check step-by-step breakdown cloudflow workflows executions steps exec_7h3j6k9m2n # Sample output: # Step 1: data_ingestion - Duration: 245s - Status: SUCCESS # Step 2: data_validation - Duration: 123s - Status: SUCCESS # Step 3: data_transformation - Duration: 3189s - Status: TIMEOUT # Step 4: data_export - Duration: 0s - Status: SKIPPED # Identify bottleneck step cloudflow workflows analyze",
              "score": null
            },
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_0",
              "content": "# CloudFlow API Reference Version 2.1.0 | Last Updated: January 2026 ## Overview The CloudFlow API is a RESTful service that enables developers to programmatically manage cloud workflows, data pipelines, and automation tasks. This documentation provides comprehensive details on authentication, endpoints, request/response formats, error handling, and best practices. **Base URL:** `https://api.cloudflow.io/v2` **API Status:** https://status.cloudflow.io ## Authentication CloudFlow supports three authentication methods to suit different use cases and security requirements. ### API Keys API keys provide simple authentication for server-to-server communication. Include your API key in the request header: ```bash curl -H \"X-API-Key: cf_live_a1b2c3d4e5f6g7h8i9j0\" \\ https://api.cloudflow.io/v2/workflows ``` **Security Notes:** - Never expose API keys in client-side code - Rotate keys every 90 days - Use separate keys for development and production environments ### OAuth 2.0 OAuth 2.0 is recommended for applications that access CloudFlow on behalf of users. We support the Authorization Code flow with PKCE. **Authorization Endpoint:** `https://auth.cloudflow.io/oauth/authorize` **Token Endpoint:** `https://auth.cloudflow.io/oauth/token` **Supported Scopes:** - `workflows:read` - Read workflow configurations - `workflows:write` - Create and modify workflows - `pipelines:read` - Read pipeline data - `pipelines:write` - Create and manage pipelines - `analytics:read` - Access analytics and metrics - `admin:full` - Full administrative access Example authorization request: ```bash curl -X POST https://auth.cloudflow.io/oauth/token \\ -H \"Content-Type: application/x-www-form-urlencoded\" \\ -d \"grant_type=authorization_code\" \\ -d \"code=AUTH_CODE_HERE\" \\ -d \"client_id=YOUR_CLIENT_ID\" \\ -d \"client_secret=YOUR_CLIENT_SECRET\" \\ -d \"redirect_uri=https://yourapp.com/callback\" ``` ### JWT Tokens For advanced use cases, CloudFlow supports JSON Web Tokens (JWT) with RS256 signing algorithm. JWTs must include the following claims: - `iss` (issuer): Your application identifier - `sub` (subject): User or service account ID - `aud` (audience): `https://api.cloudflow.io` - `exp` (expiration): Unix timestamp (max 3600 seconds from `iat`) - `iat` (issued at): Unix timestamp - `scope`: Space-separated list of requested scopes Example JWT header: ```python import jwt import time from cryptography.hazmat.primitives import serialization from cryptography.hazmat.backends import default_backend # Load your private key with open('private_key.pem', 'rb') as key_file: private_key = serialization.load_pem_private_key( key_file.read(), password=None, backend=default_backend() ) # Create JWT payload payload = { 'iss': 'your-app-id', 'sub': 'user-12345', 'aud': 'https://api.cloudflow.io', 'exp': int(time.time()) + 3600, 'iat': int(time.time()), 'scope': 'workflows:read workflows:write' } # Generate token token = jwt.encode(payload, private_key, algorithm='RS256') # Use in API request headers = {'Authorization': f'Bearer {token}'} ``` **Token Expiration:** All tokens expire after 3600 seconds (1 hour). Implement token refresh logic in your application.",
              "score": null
            },
            {
              "doc_id": "user_guide",
              "chunk_id": "user_guide_fix_0",
              "content": "# CloudFlow User Guide Welcome to CloudFlow, the modern workflow automation platform that helps you connect your apps, automate repetitive tasks, and build powerful integrations without writing code. ## Table of Contents 1. [Getting Started](#getting-started) 2. [Workflow Creation](#workflow-creation) 3. [Available Actions](#available-actions) 4. [Variables and Expressions](#variables-and-expressions) 5. [Scheduling](#scheduling) 6. [Error Handling](#error-handling) 7. [Workflow Limits](#workflow-limits) 8. [Best Practices](#best-practices) 9. [Common Workflow Patterns](#common-workflow-patterns) ## Getting Started ### Account Setup Creating your CloudFlow account takes just a few minutes: 1. Navigate to https://app.cloudflow.io/signup 2. Enter your email address and create a strong password 3. Verify your email address by clicking the link sent to your inbox 4. Complete your profile by providing your organization name and use case 5. Choose your plan (Free, Professional, or Enterprise) Once your account is created, you'll be redirected to your CloudFlow dashboard where you can start building workflows immediately. ### Your First Workflow Let's create a simple workflow that sends you a Slack notification when a new file is uploaded to your Google Drive: 1. Click the **\"Create Workflow\"** button in the top-right corner 2. Give your workflow a name: \"Google Drive to Slack Notifier\" 3. Select **\"Google Drive - New File\"** as your trigger 4. Authenticate your Google Drive account when prompted 5. Choose the folder you want to monitor 6. Click **\"Add Action\"** and select **\"Slack - Send Message\"** 7. Authenticate your Slack account 8. Select the channel where you want to receive notifications 9. Customize your message using variables: `New file uploaded: {{trigger.file.name}}` 10. Click **\"Save & Activate\"** to enable your workflow Congratulations! Your first workflow is now live. Every time a file is added to the specified Google Drive folder, you'll receive a Slack notification. ## Workflow Creation CloudFlow offers multiple ways to create and manage workflows, giving you the flexibility to work in the way that suits you best. ### Visual Editor The Visual Editor is CloudFlow's drag-and-drop interface for building workflows without code. It's perfect for users who prefer a graphical approach: **Key Features:** - Drag-and-drop action blocks from the sidebar - Visual connections between steps show your workflow logic - Inline configuration for each action - Real-time validation and error checking - Test mode to run your workflow with sample data **Using the Visual Editor:** 1.",
              "score": null
            },
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_3",
              "content": "Deploy PostgreSQL using the Bitnami Helm chart: ```yaml # postgres-values.yaml global: postgresql: auth: username: cloudflow database: cloudflow existingSecret: postgres-credentials image: tag: \"14.10.0\" primary: resources: limits: cpu: 4000m memory: 8Gi requests: cpu: 2000m memory: 4Gi persistence: enabled: true size: 100Gi storageClass: gp3 extendedConfiguration: | max_connections = 100 shared_buffers = 2GB effective_cache_size = 6GB maintenance_work_mem = 512MB checkpoint_completion_target = 0.9 wal_buffers = 16MB default_statistics_target = 100 random_page_cost = 1.1 effective_io_concurrency = 200 work_mem = 10MB min_wal_size = 1GB max_wal_size = 4GB readReplicas: replicaCount: 2 resources: limits: cpu: 2000m memory: 4Gi requests: cpu: 1000m memory: 2Gi metrics: enabled: true serviceMonitor: enabled: true ``` Install PostgreSQL: ```bash # Create password secret kubectl create secret generic postgres-credentials \\ --namespace cloudflow-prod \\ --from-literal=postgres-password=\"$(openssl rand -base64 32)\" \\ --from-literal=password=\"$(openssl rand -base64 32)\" # Install PostgreSQL helm install postgresql bitnami/postgresql \\ --namespace cloudflow-prod \\ --values postgres-values.yaml \\ --wait ``` ### PgBouncer Configuration Deploy PgBouncer for connection pooling to handle up to 100 connections efficiently: ```yaml # pgbouncer.yaml apiVersion: v1 kind: ConfigMap metadata: name: pgbouncer-config namespace: cloudflow-prod data: pgbouncer.ini: | [databases] cloudflow = host=postgresql.cloudflow-prod.svc.cluster.local port=5432 dbname=cloudflow [pgbouncer] listen_addr = 0.0.0.0 listen_port = 5432 auth_type = md5 auth_file = /etc/pgbouncer/userlist.txt pool_mode = transaction max_client_conn = 1000 default_pool_size = 25 reserve_pool_size = 5 reserve_pool_timeout = 3 max_db_connections = 100 max_user_connections = 100 server_lifetime = 3600 server_idle_timeout = 600 log_connections = 1 log_disconnections = 1 log_pooler_errors = 1 --- apiVersion: apps/v1 kind: Deployment metadata: name: pgbouncer namespace: cloudflow-prod spec: replicas: 2 selector: matchLabels: app: pgbouncer template: metadata: labels: app: pgbouncer spec: containers: - name: pgbouncer image: pgbouncer/pgbouncer:1.21.0 ports: - containerPort: 5432 resources: requests: cpu: 500m memory: 512Mi limits: cpu: 1000m memory: 1Gi volumeMounts: - name: config mountPath: /etc/pgbouncer volumes: - name: config configMap: name: pgbouncer-config --- apiVersion: v1 kind: Service metadata: name: pgbouncer namespace: cloudflow-prod spec: selector: app: pgbouncer ports: - port: 5432 targetPort: 5432 type: ClusterIP ``` Apply the PgBouncer configuration: ```bash kubectl apply -f pgbouncer.yaml ``` ### Database Migrations Run database migrations before deploying new versions: ```bash # Create migration job kubectl create job cloudflow-migrate-$(date +%s) \\ --namespace cloudflow-prod \\ --image=123456789012.dkr.ecr.us-east-1.amazonaws.com/cloudflow:2.4.0 \\ -- npm run migrate # Monitor migration progress kubectl logs -f job/cloudflow-migrate-<timestamp> -n cloudflow-prod ``` --- ## Monitoring and Observability ### Prometheus Setup Install Prometheus using the kube-prometheus-stack: ```bash helm install prometheus prometheus-community/kube-prometheus-stack \\ --namespace monitoring \\ --create-namespace \\ --set prometheus.prometheusSpec.serviceMonitorSelectorNilUsesHelmValues=false \\",
              "score": null
            },
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_7",
              "content": "podSelector: matchLabels: app: cloudflow policyTypes: - Ingress - Egress ingress: - from: - namespaceSelector: matchLabels: name: cloudflow-prod - podSelector: matchLabels: app: nginx-ingress ports: - protocol: TCP port: 3000 egress: - to: - podSelector: matchLabels: app: postgresql ports: - protocol: TCP port: 5432 - to: - podSelector: matchLabels: app: redis ports: - protocol: TCP port: 6379 ``` ### Pod Security Standards Enforce Pod Security Standards at the namespace level: ```bash kubectl label namespace cloudflow-prod \\ pod-security.kubernetes.io/enforce=restricted \\ pod-security.kubernetes.io/audit=restricted \\ pod-security.kubernetes.io/warn=restricted ``` ### Secrets Management Use AWS Secrets Manager for sensitive credentials: ```bash # Install External Secrets Operator helm install external-secrets external-secrets/external-secrets \\ --namespace external-secrets-system \\ --create-namespace # Create SecretStore kubectl apply -f - <<EOF apiVersion: external-secrets.io/v1beta1 kind: SecretStore metadata: name: aws-secrets-manager namespace: cloudflow-prod spec: provider: aws: service: SecretsManager region: us-east-1 auth: jwt: serviceAccountRef: name: external-secrets-sa EOF ``` --- ## Troubleshooting ### Common Issues #### Pods in CrashLoopBackOff **Symptoms**: Pods continuously restart **Diagnosis**: ```bash kubectl logs -n cloudflow-prod <pod-name> --previous kubectl describe pod -n cloudflow-prod <pod-name> ``` **Common Causes**: - Database connection failure - Invalid environment variables - Insufficient resources #### High Memory Usage **Symptoms**: Pods being OOMKilled **Diagnosis**: ```bash kubectl top pods -n cloudflow-prod ``` **Resolution**: - Increase memory limits in deployment - Check for memory leaks in application logs - Review heap dump for Node.js processes #### Database Connection Pool Exhausted **Symptoms**: \"Too many connections\" errors **Diagnosis**: ```bash # Check PgBouncer stats kubectl exec -it pgbouncer-xxx -n cloudflow-prod -- \\ psql -p 5432 -U pgbouncer pgbouncer -c \"SHOW POOLS;\" ``` **Resolution**: - Increase `max_db_connections` in PgBouncer - Optimize application connection usage - Add more PostgreSQL replicas ### Support Contacts - **On-call Engineer**: pagerduty.com/cloudflow-oncall - **Slack Channel**: #cloudflow-operations - **Documentation**: https://docs.cloudflow.io - **Status Page**: https://status.cloudflow.io --- ## Appendix ### Useful Commands ```bash # View all resources in namespace kubectl get all -n cloudflow-prod # Check resource usage kubectl top pods -n cloudflow-prod kubectl top nodes # View logs kubectl logs -f deployment/cloudflow-api -n cloudflow-prod # Execute command in pod kubectl exec -it <pod-name> -n cloudflow-prod -- /bin/sh # Port forward for local testing kubectl port-forward svc/cloudflow-api 8080:80 -n cloudflow-prod # Rollback deployment helm rollback cloudflow -n cloudflow-prod # Update deployment helm upgrade cloudflow cloudflow/cloudflow \\ --namespace cloudflow-prod \\ --values values-production.yaml ``` ### Maintenance Windows Scheduled maintenance occurs during the following windows: - **Primary**: Sunday 02:00-06:00",
              "score": null
            }
          ],
          "latency_ms": 13.0688259996532,
          "query_id": "realistic_017",
          "dimension": "original",
          "query": "What is PgBouncer and why is it used in CloudFlow?"
        },
        {
          "key_facts": [
            "PgBouncer",
            "connection pooling",
            "max_db_connections = 100",
            "default_pool_size = 25",
            "pool_mode = transaction"
          ],
          "found_facts": [
            "PgBouncer",
            "connection pooling"
          ],
          "missed_facts": [
            "max_db_connections = 100",
            "default_pool_size = 25",
            "pool_mode = transaction"
          ],
          "coverage": 0.4,
          "expected_docs": [
            "deployment_guide",
            "architecture_overview"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_4",
              "content": "**Implement connection pooling optimization:** ```bash # Use PgBouncer for connection pooling kubectl apply -f cloudflow-pgbouncer.yaml # Configure CloudFlow to use PgBouncer cloudflow config set db.host pgbouncer.cloudflow.svc.cluster.local cloudflow config set db.port 6432 cloudflow config set db.pool.mode transaction ``` 2. **Add read replicas:** ```bash # Route read-only queries to replicas cloudflow db replicas add --count 2 cloudflow config set db.read_replicas \"replica-1:5432,replica-2:5432\" cloudflow config set db.read_write_split true ``` ### Connection Timeout Errors #### Error Messages - `connection timeout after 30000ms` - `could not connect to database server at 10.0.2.45:5432` - `database server unreachable` #### Troubleshooting Steps ```bash # Test network connectivity telnet cloudflow-db.internal.company.com 5432 # Check DNS resolution nslookup cloudflow-db.internal.company.com # Verify database is accepting connections pg_isready -h cloudflow-db.internal.company.com -p 5432 -U cloudflow # Check firewall rules sudo iptables -L -n | grep 5432 # Test from CloudFlow pod network kubectl run -n cloudflow debug-pod --rm -i --tty \\ --image=postgres:14 -- \\ psql -h cloudflow-db.internal.company.com -U cloudflow -d cloudflow_production # Review database logs for connection rejections kubectl logs -n cloudflow statefulset/cloudflow-db --tail=100 | \\ grep -i \"connection\\|reject\\|authentication\" ``` ### Maximum Connection Limit (100) Reached This is a hard limit in CloudFlow's database tier. #### Permanent Solutions **Option 1: Upgrade database tier** ```bash # Check available tiers cloudflow db tiers list # Upgrade to higher tier (supports 200 connections) cloudflow db upgrade --tier standard-plus --confirm # Monitor migration progress cloudflow db migration status ``` **Option 2: Implement aggressive connection reuse** ```bash # Reduce connection lifetime cloudflow config set db.pool.max_lifetime 600 # 10 minutes # Enable prepared statement caching cloudflow config set db.prepared_statements.cache true cloudflow config set db.prepared_statements.max_size 250 # Reduce idle connection timeout cloudflow config set db.pool.idle_timeout 120 # 2 minutes ``` --- ## Workflow Execution Failures ### Timeout Errors (3600 second limit) #### Error Message ``` WorkflowExecutionError: Workflow exceeded maximum execution time of 3600 seconds Status: TIMEOUT Workflow ID: wf_9k2n4m8p1q Execution ID: exec_7h3j6k9m2n ``` #### Analysis ```bash # Get workflow execution details cloudflow workflows executions get exec_7h3j6k9m2n --verbose # Check step-by-step breakdown cloudflow workflows executions steps exec_7h3j6k9m2n # Sample output: # Step 1: data_ingestion - Duration: 245s - Status: SUCCESS # Step 2: data_validation - Duration: 123s - Status: SUCCESS # Step 3: data_transformation - Duration: 3189s - Status: TIMEOUT # Step 4: data_export - Duration: 0s - Status: SKIPPED # Identify bottleneck step cloudflow workflows analyze",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_3",
              "content": "Workflow Context Accumulation** Large workflow executions may accumulate state in memory. **Solution:** ```bash # Configure context cleanup cloudflow config set workflow.context.max_size_mb 100 cloudflow config set workflow.context.cleanup_threshold 0.8 # Enable context persistence to disk cloudflow config set workflow.context.persistence.enabled true cloudflow config set workflow.context.persistence.backend s3 ``` **2. Connection Pool Leaks** **Diagnosis:** ```bash # Check active connections cloudflow db connections --verbose # Expected output: # Active: 45/100 # Idle: 23 # Waiting: 2 # Average age: 245s ``` **Resolution:** ```bash # Adjust connection pool settings cloudflow config set db.pool.max_connections 100 cloudflow config set db.pool.min_connections 10 cloudflow config set db.pool.idle_timeout 300 cloudflow config set db.pool.max_lifetime 1800 # Force connection pool reset cloudflow db pool reset --confirm ``` **3. Event Stream Buffers** Unbounded event buffers can cause memory exhaustion. ```bash # Configure event buffer limits cloudflow config set events.buffer.max_size 10000 cloudflow config set events.buffer.overflow_strategy drop_oldest # Enable event streaming to external sink cloudflow config set events.sink.type kafka cloudflow config set events.sink.kafka.brokers \"kafka-1:9092,kafka-2:9092\" cloudflow config set events.sink.kafka.topic cloudflow-events ``` --- ## Database Connection Issues ### Connection Pool Exhaustion #### Symptoms - Error: `could not obtain connection from pool within 5000ms` - Error: `connection pool exhausted (100/100 connections in use)` - API requests fail with `503 Service Unavailable` - Database CPU usage normal, but connection count at maximum #### Investigation ```bash # Check current connection pool status cloudflow db pool status --detailed # Output example: # Pool Statistics: # Total Connections: 100/100 (100%) # Active: 87 # Idle: 13 # Waiting Requests: 45 # Average Wait Time: 3420ms # Max Wait Time: 8234ms # Identify long-running queries cloudflow db queries --status running --min-duration 30000 # Check connection distribution by client SELECT application_name, state, COUNT(*) as conn_count, AVG(EXTRACT(EPOCH FROM (NOW() - state_change))) as avg_duration_sec FROM pg_stat_activity WHERE datname = 'cloudflow_production' GROUP BY application_name, state ORDER BY conn_count DESC; ``` #### Resolution **Immediate mitigation:** ```bash # Temporarily increase connection limit (requires database restart) cloudflow db config set max_connections 150 # Kill idle connections older than 5 minutes cloudflow db connections kill --idle-timeout 300 # Restart connection pool without downtime kubectl rollout restart deployment/cloudflow-api -n cloudflow kubectl rollout status deployment/cloudflow-api -n cloudflow ``` **Long-term solutions:** 1.",
              "score": null
            },
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_4",
              "content": "--set prometheus.prometheusSpec.retention=30d \\ --set prometheus.prometheusSpec.storageSpec.volumeClaimTemplate.spec.resources.requests.storage=100Gi ``` CloudFlow exposes Prometheus metrics on port 9090 at the `/metrics` endpoint. The metrics include: - **HTTP Metrics**: Request rate, latency, error rate - **Database Metrics**: Connection pool status, query duration - **Worker Metrics**: Job queue size, processing time, failure rate - **System Metrics**: CPU usage, memory usage, heap statistics Example metrics exposed: ``` # HELP cloudflow_http_requests_total Total number of HTTP requests # TYPE cloudflow_http_requests_total counter cloudflow_http_requests_total{method=\"GET\",route=\"/api/workflows\",status=\"200\"} 15420 # HELP cloudflow_http_request_duration_seconds HTTP request duration in seconds # TYPE cloudflow_http_request_duration_seconds histogram cloudflow_http_request_duration_seconds_bucket{method=\"POST\",route=\"/api/workflows\",le=\"0.1\"} 8234 # HELP cloudflow_worker_jobs_processed_total Total number of jobs processed # TYPE cloudflow_worker_jobs_processed_total counter cloudflow_worker_jobs_processed_total{status=\"success\"} 45230 # HELP cloudflow_database_connections Current database connections # TYPE cloudflow_database_connections gauge cloudflow_database_connections{state=\"active\"} 23 ``` ### Grafana Dashboards Access Grafana to view CloudFlow dashboards: ```bash kubectl port-forward -n monitoring svc/prometheus-grafana 3000:80 ``` Import the CloudFlow dashboard (ID: 15847) or use the provided JSON template. Key dashboard panels include: 1. **API Performance**: Request rate, P95/P99 latency, error rate 2. **Resource Usage**: CPU, memory, disk I/O per pod 3. **Database Health**: Connection pool utilization, query performance 4. **Worker Status**: Queue depth, processing rate, job success/failure ratio 5. **System Overview**: Pod status, replica count, autoscaling events ### Logging with CloudWatch CloudFlow logs are automatically shipped to CloudWatch Logs.",
              "score": null
            },
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_7",
              "content": "podSelector: matchLabels: app: cloudflow policyTypes: - Ingress - Egress ingress: - from: - namespaceSelector: matchLabels: name: cloudflow-prod - podSelector: matchLabels: app: nginx-ingress ports: - protocol: TCP port: 3000 egress: - to: - podSelector: matchLabels: app: postgresql ports: - protocol: TCP port: 5432 - to: - podSelector: matchLabels: app: redis ports: - protocol: TCP port: 6379 ``` ### Pod Security Standards Enforce Pod Security Standards at the namespace level: ```bash kubectl label namespace cloudflow-prod \\ pod-security.kubernetes.io/enforce=restricted \\ pod-security.kubernetes.io/audit=restricted \\ pod-security.kubernetes.io/warn=restricted ``` ### Secrets Management Use AWS Secrets Manager for sensitive credentials: ```bash # Install External Secrets Operator helm install external-secrets external-secrets/external-secrets \\ --namespace external-secrets-system \\ --create-namespace # Create SecretStore kubectl apply -f - <<EOF apiVersion: external-secrets.io/v1beta1 kind: SecretStore metadata: name: aws-secrets-manager namespace: cloudflow-prod spec: provider: aws: service: SecretsManager region: us-east-1 auth: jwt: serviceAccountRef: name: external-secrets-sa EOF ``` --- ## Troubleshooting ### Common Issues #### Pods in CrashLoopBackOff **Symptoms**: Pods continuously restart **Diagnosis**: ```bash kubectl logs -n cloudflow-prod <pod-name> --previous kubectl describe pod -n cloudflow-prod <pod-name> ``` **Common Causes**: - Database connection failure - Invalid environment variables - Insufficient resources #### High Memory Usage **Symptoms**: Pods being OOMKilled **Diagnosis**: ```bash kubectl top pods -n cloudflow-prod ``` **Resolution**: - Increase memory limits in deployment - Check for memory leaks in application logs - Review heap dump for Node.js processes #### Database Connection Pool Exhausted **Symptoms**: \"Too many connections\" errors **Diagnosis**: ```bash # Check PgBouncer stats kubectl exec -it pgbouncer-xxx -n cloudflow-prod -- \\ psql -p 5432 -U pgbouncer pgbouncer -c \"SHOW POOLS;\" ``` **Resolution**: - Increase `max_db_connections` in PgBouncer - Optimize application connection usage - Add more PostgreSQL replicas ### Support Contacts - **On-call Engineer**: pagerduty.com/cloudflow-oncall - **Slack Channel**: #cloudflow-operations - **Documentation**: https://docs.cloudflow.io - **Status Page**: https://status.cloudflow.io --- ## Appendix ### Useful Commands ```bash # View all resources in namespace kubectl get all -n cloudflow-prod # Check resource usage kubectl top pods -n cloudflow-prod kubectl top nodes # View logs kubectl logs -f deployment/cloudflow-api -n cloudflow-prod # Execute command in pod kubectl exec -it <pod-name> -n cloudflow-prod -- /bin/sh # Port forward for local testing kubectl port-forward svc/cloudflow-api 8080:80 -n cloudflow-prod # Rollback deployment helm rollback cloudflow -n cloudflow-prod # Update deployment helm upgrade cloudflow cloudflow/cloudflow \\ --namespace cloudflow-prod \\ --values values-production.yaml ``` ### Maintenance Windows Scheduled maintenance occurs during the following windows: - **Primary**: Sunday 02:00-06:00",
              "score": null
            },
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_1",
              "content": "create cluster -f cluster-config.yaml ``` ### Storage Configuration Install the EBS CSI driver for persistent volumes: ```bash eksctl create addon --name aws-ebs-csi-driver \\ --cluster cloudflow-production \\ --service-account-role-arn arn:aws:iam::ACCOUNT_ID:role/AmazonEKS_EBS_CSI_DriverRole \\ --force ``` --- ## Kubernetes Deployment ### Namespace Setup Create the CloudFlow namespace and configure resource quotas: ```yaml # namespace.yaml apiVersion: v1 kind: Namespace metadata: name: cloudflow-prod labels: name: cloudflow-prod environment: production --- apiVersion: v1 kind: ResourceQuota metadata: name: cloudflow-quota namespace: cloudflow-prod spec: hard: requests.cpu: \"50\" requests.memory: 100Gi persistentvolumeclaims: \"10\" services.loadbalancers: \"2\" ``` Apply the namespace configuration: ```bash kubectl apply -f namespace.yaml ``` ### Environment Configuration CloudFlow requires the following environment variables: | Variable | Description | Example | Required | |----------|-------------|---------|----------| | `DATABASE_URL` | PostgreSQL connection string | `postgresql://user:pass@host:5432/cloudflow` | Yes | | `REDIS_URL` | Redis connection string | `redis://redis-master.cloudflow-prod.svc.cluster.local:6379` | Yes | | `JWT_SECRET` | Secret key for JWT token signing | `<generated-secret-256-bit>` | Yes | | `LOG_LEVEL` | Application log verbosity | `info`, `debug`, `warn`, `error` | No (default: `info`) | | `API_PORT` | API server port | `3000` | No (default: `3000`) | | `WORKER_CONCURRENCY` | Number of concurrent workers | `10` | No (default: `5`) | | `SESSION_SECRET` | Session encryption key | `<generated-secret-256-bit>` | Yes | | `AWS_REGION` | AWS region for services | `us-east-1` | Yes | | `S3_BUCKET` | S3 bucket for file storage | `cloudflow-prod-storage` | Yes | | `SMTP_HOST` | Email server hostname | `smtp.sendgrid.net` | No | | `SMTP_PORT` | Email server port | `587` | No | | `METRICS_ENABLED` | Enable Prometheus metrics | `true` | No (default: `false`) | Create a Kubernetes secret for sensitive variables: ```bash kubectl create secret generic cloudflow-secrets \\ --namespace cloudflow-prod \\ --from-literal=DATABASE_URL=\"postgresql://cloudflow:$(cat db-password.txt)@pgbouncer.cloudflow-prod.svc.cluster.local:5432/cloudflow\" \\ --from-literal=REDIS_URL=\"redis://redis-master.cloudflow-prod.svc.cluster.local:6379\" \\ --from-literal=JWT_SECRET=\"$(openssl rand -base64 32)\" \\ --from-literal=SESSION_SECRET=\"$(openssl rand -base64 32)\" ``` ### Helm Chart Deployment Add the CloudFlow Helm repository: ```bash helm repo add cloudflow https://charts.cloudflow.io helm repo update ``` Create a values file for production configuration: ```yaml # values-production.yaml replicaCount: 3 image: repository: 123456789012.dkr.ecr.us-east-1.amazonaws.com/cloudflow tag: \"2.4.0\" pullPolicy: IfNotPresent resources: limits: cpu: 2000m memory: 4Gi requests: cpu: 2000m memory: 4Gi autoscaling: enabled: true minReplicas: 3 maxReplicas: 10 targetCPUUtilizationPercentage: 70 targetMemoryUtilizationPercentage: 80 service: type: ClusterIP port: 80 targetPort: 3000 annotations: service.beta.kubernetes.io/aws-load-balancer-type: \"nlb\" ingress: enabled: true className: nginx annotations: cert-manager.io/cluster-issuer: \"letsencrypt-prod\" nginx.ingress.kubernetes.io/ssl-redirect: \"true\" hosts: - host: api.cloudflow.io paths: - path: / pathType: Prefix tls:",
              "score": null
            }
          ],
          "latency_ms": 12.912893998873187,
          "query_id": "realistic_017",
          "dimension": "synonym",
          "query": "What's the purpose of the connection pooler?"
        },
        {
          "key_facts": [
            "PgBouncer",
            "connection pooling",
            "max_db_connections = 100",
            "default_pool_size = 25",
            "pool_mode = transaction"
          ],
          "found_facts": [
            "PgBouncer",
            "connection pooling"
          ],
          "missed_facts": [
            "max_db_connections = 100",
            "default_pool_size = 25",
            "pool_mode = transaction"
          ],
          "coverage": 0.4,
          "expected_docs": [
            "deployment_guide",
            "architecture_overview"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_4",
              "content": "**Implement connection pooling optimization:** ```bash # Use PgBouncer for connection pooling kubectl apply -f cloudflow-pgbouncer.yaml # Configure CloudFlow to use PgBouncer cloudflow config set db.host pgbouncer.cloudflow.svc.cluster.local cloudflow config set db.port 6432 cloudflow config set db.pool.mode transaction ``` 2. **Add read replicas:** ```bash # Route read-only queries to replicas cloudflow db replicas add --count 2 cloudflow config set db.read_replicas \"replica-1:5432,replica-2:5432\" cloudflow config set db.read_write_split true ``` ### Connection Timeout Errors #### Error Messages - `connection timeout after 30000ms` - `could not connect to database server at 10.0.2.45:5432` - `database server unreachable` #### Troubleshooting Steps ```bash # Test network connectivity telnet cloudflow-db.internal.company.com 5432 # Check DNS resolution nslookup cloudflow-db.internal.company.com # Verify database is accepting connections pg_isready -h cloudflow-db.internal.company.com -p 5432 -U cloudflow # Check firewall rules sudo iptables -L -n | grep 5432 # Test from CloudFlow pod network kubectl run -n cloudflow debug-pod --rm -i --tty \\ --image=postgres:14 -- \\ psql -h cloudflow-db.internal.company.com -U cloudflow -d cloudflow_production # Review database logs for connection rejections kubectl logs -n cloudflow statefulset/cloudflow-db --tail=100 | \\ grep -i \"connection\\|reject\\|authentication\" ``` ### Maximum Connection Limit (100) Reached This is a hard limit in CloudFlow's database tier. #### Permanent Solutions **Option 1: Upgrade database tier** ```bash # Check available tiers cloudflow db tiers list # Upgrade to higher tier (supports 200 connections) cloudflow db upgrade --tier standard-plus --confirm # Monitor migration progress cloudflow db migration status ``` **Option 2: Implement aggressive connection reuse** ```bash # Reduce connection lifetime cloudflow config set db.pool.max_lifetime 600 # 10 minutes # Enable prepared statement caching cloudflow config set db.prepared_statements.cache true cloudflow config set db.prepared_statements.max_size 250 # Reduce idle connection timeout cloudflow config set db.pool.idle_timeout 120 # 2 minutes ``` --- ## Workflow Execution Failures ### Timeout Errors (3600 second limit) #### Error Message ``` WorkflowExecutionError: Workflow exceeded maximum execution time of 3600 seconds Status: TIMEOUT Workflow ID: wf_9k2n4m8p1q Execution ID: exec_7h3j6k9m2n ``` #### Analysis ```bash # Get workflow execution details cloudflow workflows executions get exec_7h3j6k9m2n --verbose # Check step-by-step breakdown cloudflow workflows executions steps exec_7h3j6k9m2n # Sample output: # Step 1: data_ingestion - Duration: 245s - Status: SUCCESS # Step 2: data_validation - Duration: 123s - Status: SUCCESS # Step 3: data_transformation - Duration: 3189s - Status: TIMEOUT # Step 4: data_export - Duration: 0s - Status: SKIPPED # Identify bottleneck step cloudflow workflows analyze",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_4",
              "content": "**Technology**: Node.js with worker pool pattern **Replicas**: 8 pods (production), auto-scaling 6-16 **Resource Allocation**: 2 vCPU, 4GB RAM per pod **Key Responsibilities**: - Consume notification events from Kafka - Template rendering with Handlebars (cached in Redis) - Multi-channel delivery (Email via SendGrid, SMS via Twilio, Webhooks) - Retry logic with dead letter queue for failed deliveries - Delivery status tracking and analytics - User preference management (opt-in/opt-out) **Notification Flow**: ``` Event Source \u2192 Kafka (notifications.email topic) \u2502 \u25bc Notification Service Consumer \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u25bc \u25bc \u25bc [Email Queue] [SMS Queue] [Webhook Queue] \u2502 \u2502 \u2502 \u25bc \u25bc \u25bc [SendGrid] [Twilio] [HTTP POST] \u2502 \u2502 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u25bc Delivery Status \u2192 PostgreSQL ``` **Channel Configuration**: - Email: SendGrid with IP warming, DKIM/SPF configured - SMS: Twilio with fallback to AWS SNS - Webhook: Signed payloads (HMAC-SHA256), retry 3 times with exponential backoff - In-app: WebSocket connections via Socket.io **Template Management**: - Templates stored in PostgreSQL with versioning - Compiled templates cached in Redis (1-hour TTL) - Support for localization (i18n) with 12 languages - A/B testing capability for email campaigns **Performance Targets**: - Email delivery: < 5 seconds (P99) from event to SendGrid - SMS delivery: < 3 seconds (P99) - Throughput: 10,000 notifications per minute --- ## Data Flow Architecture ### Synchronous Request Flow Client requests follow a synchronous path through the API Gateway to backend services: ``` 1. Client \u2192 Load Balancer (TLS termination) 2. Load Balancer \u2192 API Gateway (HTTP/2) 3. API Gateway \u2192 Auth Service (JWT validation via gRPC) 4. API Gateway \u2192 Target Service (HTTP/REST or gRPC) 5. Target Service \u2192 Database/Cache (data retrieval) 6. Response propagates back through the chain ``` **Timeout Configuration**: - Client \u2192 Load Balancer: 60 seconds - Load Balancer \u2192 API Gateway: 55 seconds - API Gateway \u2192 Services: 45 seconds - Service \u2192 Database: 10 seconds ### Asynchronous Event Flow Long-running operations and inter-service communication use event-driven patterns: ``` 1. Service publishes event \u2192 Kafka topic 2. Kafka persists event (replication factor: 3) 3. Consumer groups subscribe to topics 4. Consumers process events with at-least-once delivery 5. State updates written to PostgreSQL 6.",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_3",
              "content": "**Technology**: Go with distributed locking via Redis **Replicas**: 4 pods (production), active-passive with leader election **Resource Allocation**: 2 vCPU, 4GB RAM per pod **Key Responsibilities**: - Parse and validate cron expressions (extended format supporting seconds) - Maintain schedule registry in PostgreSQL - Distributed scheduling with leader election (one active scheduler) - Missed execution handling with configurable catch-up policy - Schedule conflict detection and resolution - Time zone support for international schedules **Scheduling Architecture**: ``` PostgreSQL Schedules Table \u2502 \u25bc Leader Scheduler (elected via Redis) \u2502 \u251c\u2500 Scan for due schedules (every 10 seconds) \u251c\u2500 Acquire lock per schedule (prevents duplicates) \u251c\u2500 Publish to Kafka \u2192 workflow.events topic \u2514\u2500 Update last_run timestamp ``` **Schedule Types**: - **Cron-based**: `0 */5 * * * *` (every 5 minutes) - **One-time**: Specific timestamp for delayed execution - **Interval-based**: Every N seconds/minutes/hours **Reliability Features**: - Leader election using Redis with 30-second lease - Heartbeat mechanism to detect leader failure (5-second interval) - Automatic failover to standby scheduler (< 10 seconds) - Schedule versioning to handle updates during execution - Missed execution policy: SKIP, RUN_ONCE, or RUN_ALL **Performance Targets**: - Schedule evaluation: < 100ms per cycle - Accuracy: \u00b1 1 second for schedule triggers - Capacity: 100,000 active schedules --- ### Notification Service **Purpose**: Multi-channel notification delivery system supporting email, SMS, webhooks, and in-app notifications.",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_11",
              "content": "Service (2) - PostgreSQL Primary - PostgreSQL Replica - PostgreSQL Replica - Redis Primary (2) - Redis Replica (2) - Redis Replica (2) - Kafka Broker (2) - Kafka Broker (2) - Kafka Broker (1) ``` **Automatic Failover**: - Database: 30-60 seconds (automatic promotion of replica) - Redis: < 10 seconds (Sentinel-based failover) - Kafka: < 30 seconds (controller election) - Services: Kubernetes health checks with 10-second liveness probes ### Backup Strategy **Database Backups**: - Automated snapshots: Daily at 02:00 UTC - Retention: 30 days for daily, 90 days for monthly - Cross-region replication: Async replication to us-west-2 (15-minute lag) - Backup verification: Weekly automated restore test in staging environment **Backup Schedule**: ``` Daily: Full snapshot \u2192 S3 (encrypted) Hourly: WAL archives \u2192 S3 (point-in-time recovery) Weekly: Backup validation test Monthly: Long-term archive to Glacier ``` **Configuration Backups**: - Kubernetes manifests: Stored in Git (GitOps with ArgoCD) - Vault secrets: Automated snapshot every 6 hours - Kafka topic configurations: Exported daily to S3 ### Disaster Recovery Procedures **Scenario 1: Single AZ Failure** - Detection: Health checks fail for entire AZ (< 30 seconds) - Action: Traffic automatically routed to healthy AZs by ALB - Recovery time: < 5 minutes (no manual intervention) - Data loss: None (multi-AZ replication) **Scenario 2: Database Primary Failure** - Detection: Health check fails for primary database (< 30 seconds) - Action: Automatic promotion of read replica to primary - Recovery time: 30-60 seconds - Data loss: Minimal (< 1 second due to synchronous replication) **Scenario 3: Full Region Failure** - Detection: Multiple health check failures across all AZs (< 2 minutes) - Action: Manual failover to DR region (us-west-2) - Procedure: 1. Update DNS to point to DR region (TTL: 60 seconds) 2. Promote DR database replica to primary 3. Scale up DR region services to production capacity 4. Verify data consistency and integrity 5. Update monitoring dashboards - Recovery time: 2-4 hours (includes verification) - Data loss: < 1 hour (last cross-region replication) **Scenario 4: Data Corruption** - Detection: Data validation checks or user report - Action: Point-in-time recovery from WAL archives - Procedure: 1. Identify corruption time window 2. Restore from snapshot prior to corruption 3. Replay WAL logs up to corruption point 4. Verify data integrity 5.",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_2",
              "content": "Review Query Execution Plans** ```sql -- Connect to CloudFlow database cloudflow db connect --readonly -- Explain slow query EXPLAIN ANALYZE SELECT w.*, e.status, e.error_message FROM workflows w LEFT JOIN executions e ON w.id = e.workflow_id WHERE w.workspace_id = 'ws_abc123' AND e.created_at > NOW() - INTERVAL '7 days' ORDER BY e.created_at DESC; -- Check for missing indexes SELECT schemaname, tablename, indexname, indexdef FROM pg_indexes WHERE tablename IN ('workflows', 'executions', 'workflow_steps') ORDER BY tablename, indexname; ``` **3. Optimize Queries** Common optimization techniques: **Add missing indexes:** ```sql -- Index for workflow lookup by workspace CREATE INDEX CONCURRENTLY idx_workflows_workspace_created ON workflows(workspace_id, created_at DESC); -- Index for execution status queries CREATE INDEX CONCURRENTLY idx_executions_status_created ON executions(workflow_id, status, created_at DESC) WHERE status IN ('running', 'pending'); -- Composite index for common filter combinations CREATE INDEX CONCURRENTLY idx_executions_workspace_status ON executions(workspace_id, status, created_at) INCLUDE (error_message, retry_count); ``` **Use query result caching:** ```bash # Enable query cache for workspace metadata cloudflow config set cache.workspace.ttl 3600 # Configure Redis cache backend cloudflow config set cache.backend redis cloudflow config set cache.redis.host redis.cloudflow.svc.cluster.local cloudflow config set cache.redis.port 6379 ``` ### High API Latency #### Latency Breakdown Analysis ```bash # Generate latency report cloudflow metrics latency-report --endpoint \"/api/v1/workflows\" --last 24h # Sample output: # Endpoint: POST /api/v1/workflows/execute # P50: 245ms | P95: 1823ms | P99: 4521ms # Breakdown: # - Auth: 45ms (18%) # - DB Query: 156ms (64%) # - Business Logic: 32ms (13%) # - Response Serialization: 12ms (5%) ``` **Network latency issues:** ```bash # Test connectivity to CloudFlow API time curl -w \"@curl-format.txt\" -o /dev/null -s https://api.cloudflow.io/health # Create curl-format.txt: cat > curl-format.txt << EOF time_namelookup: %{time_namelookup}s\\n time_connect: %{time_connect}s\\n time_appconnect: %{time_appconnect}s\\n time_pretransfer: %{time_pretransfer}s\\n time_redirect: %{time_redirect}s\\n time_starttransfer: %{time_starttransfer}s\\n ----------\\n time_total: %{time_total}s\\n EOF # Trace route to API endpoint traceroute api.cloudflow.io # Check DNS resolution time dig api.cloudflow.io | grep \"Query time\" ``` ### Memory Leaks #### Detection ```bash # Monitor CloudFlow service memory usage kubectl top pods -n cloudflow --sort-by=memory # Get detailed memory metrics for specific pod kubectl exec -n cloudflow deploy/cloudflow-api -- \\ curl localhost:9090/metrics | grep memory # Check for OOMKilled pods kubectl get pods -n cloudflow --field-selector=status.phase=Failed | \\ grep OOMKilled # Review memory limits and requests kubectl describe deployment cloudflow-api -n cloudflow | \\ grep -A 5 \"Limits\\|Requests\" ``` #### Common Causes **1.",
              "score": null
            }
          ],
          "latency_ms": 13.063626000075601,
          "query_id": "realistic_017",
          "dimension": "problem",
          "query": "Should I connect directly to PostgreSQL or through PgBouncer?"
        },
        {
          "key_facts": [
            "PgBouncer",
            "connection pooling",
            "max_db_connections = 100",
            "default_pool_size = 25",
            "pool_mode = transaction"
          ],
          "found_facts": [
            "PgBouncer",
            "connection pooling",
            "max_db_connections = 100",
            "default_pool_size = 25",
            "pool_mode = transaction"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "deployment_guide",
            "architecture_overview"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_3",
              "content": "Deploy PostgreSQL using the Bitnami Helm chart: ```yaml # postgres-values.yaml global: postgresql: auth: username: cloudflow database: cloudflow existingSecret: postgres-credentials image: tag: \"14.10.0\" primary: resources: limits: cpu: 4000m memory: 8Gi requests: cpu: 2000m memory: 4Gi persistence: enabled: true size: 100Gi storageClass: gp3 extendedConfiguration: | max_connections = 100 shared_buffers = 2GB effective_cache_size = 6GB maintenance_work_mem = 512MB checkpoint_completion_target = 0.9 wal_buffers = 16MB default_statistics_target = 100 random_page_cost = 1.1 effective_io_concurrency = 200 work_mem = 10MB min_wal_size = 1GB max_wal_size = 4GB readReplicas: replicaCount: 2 resources: limits: cpu: 2000m memory: 4Gi requests: cpu: 1000m memory: 2Gi metrics: enabled: true serviceMonitor: enabled: true ``` Install PostgreSQL: ```bash # Create password secret kubectl create secret generic postgres-credentials \\ --namespace cloudflow-prod \\ --from-literal=postgres-password=\"$(openssl rand -base64 32)\" \\ --from-literal=password=\"$(openssl rand -base64 32)\" # Install PostgreSQL helm install postgresql bitnami/postgresql \\ --namespace cloudflow-prod \\ --values postgres-values.yaml \\ --wait ``` ### PgBouncer Configuration Deploy PgBouncer for connection pooling to handle up to 100 connections efficiently: ```yaml # pgbouncer.yaml apiVersion: v1 kind: ConfigMap metadata: name: pgbouncer-config namespace: cloudflow-prod data: pgbouncer.ini: | [databases] cloudflow = host=postgresql.cloudflow-prod.svc.cluster.local port=5432 dbname=cloudflow [pgbouncer] listen_addr = 0.0.0.0 listen_port = 5432 auth_type = md5 auth_file = /etc/pgbouncer/userlist.txt pool_mode = transaction max_client_conn = 1000 default_pool_size = 25 reserve_pool_size = 5 reserve_pool_timeout = 3 max_db_connections = 100 max_user_connections = 100 server_lifetime = 3600 server_idle_timeout = 600 log_connections = 1 log_disconnections = 1 log_pooler_errors = 1 --- apiVersion: apps/v1 kind: Deployment metadata: name: pgbouncer namespace: cloudflow-prod spec: replicas: 2 selector: matchLabels: app: pgbouncer template: metadata: labels: app: pgbouncer spec: containers: - name: pgbouncer image: pgbouncer/pgbouncer:1.21.0 ports: - containerPort: 5432 resources: requests: cpu: 500m memory: 512Mi limits: cpu: 1000m memory: 1Gi volumeMounts: - name: config mountPath: /etc/pgbouncer volumes: - name: config configMap: name: pgbouncer-config --- apiVersion: v1 kind: Service metadata: name: pgbouncer namespace: cloudflow-prod spec: selector: app: pgbouncer ports: - port: 5432 targetPort: 5432 type: ClusterIP ``` Apply the PgBouncer configuration: ```bash kubectl apply -f pgbouncer.yaml ``` ### Database Migrations Run database migrations before deploying new versions: ```bash # Create migration job kubectl create job cloudflow-migrate-$(date +%s) \\ --namespace cloudflow-prod \\ --image=123456789012.dkr.ecr.us-east-1.amazonaws.com/cloudflow:2.4.0 \\ -- npm run migrate # Monitor migration progress kubectl logs -f job/cloudflow-migrate-<timestamp> -n cloudflow-prod ``` --- ## Monitoring and Observability ### Prometheus Setup Install Prometheus using the kube-prometheus-stack: ```bash helm install prometheus prometheus-community/kube-prometheus-stack \\ --namespace monitoring \\ --create-namespace \\ --set prometheus.prometheusSpec.serviceMonitorSelectorNilUsesHelmValues=false \\",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_4",
              "content": "**Implement connection pooling optimization:** ```bash # Use PgBouncer for connection pooling kubectl apply -f cloudflow-pgbouncer.yaml # Configure CloudFlow to use PgBouncer cloudflow config set db.host pgbouncer.cloudflow.svc.cluster.local cloudflow config set db.port 6432 cloudflow config set db.pool.mode transaction ``` 2. **Add read replicas:** ```bash # Route read-only queries to replicas cloudflow db replicas add --count 2 cloudflow config set db.read_replicas \"replica-1:5432,replica-2:5432\" cloudflow config set db.read_write_split true ``` ### Connection Timeout Errors #### Error Messages - `connection timeout after 30000ms` - `could not connect to database server at 10.0.2.45:5432` - `database server unreachable` #### Troubleshooting Steps ```bash # Test network connectivity telnet cloudflow-db.internal.company.com 5432 # Check DNS resolution nslookup cloudflow-db.internal.company.com # Verify database is accepting connections pg_isready -h cloudflow-db.internal.company.com -p 5432 -U cloudflow # Check firewall rules sudo iptables -L -n | grep 5432 # Test from CloudFlow pod network kubectl run -n cloudflow debug-pod --rm -i --tty \\ --image=postgres:14 -- \\ psql -h cloudflow-db.internal.company.com -U cloudflow -d cloudflow_production # Review database logs for connection rejections kubectl logs -n cloudflow statefulset/cloudflow-db --tail=100 | \\ grep -i \"connection\\|reject\\|authentication\" ``` ### Maximum Connection Limit (100) Reached This is a hard limit in CloudFlow's database tier. #### Permanent Solutions **Option 1: Upgrade database tier** ```bash # Check available tiers cloudflow db tiers list # Upgrade to higher tier (supports 200 connections) cloudflow db upgrade --tier standard-plus --confirm # Monitor migration progress cloudflow db migration status ``` **Option 2: Implement aggressive connection reuse** ```bash # Reduce connection lifetime cloudflow config set db.pool.max_lifetime 600 # 10 minutes # Enable prepared statement caching cloudflow config set db.prepared_statements.cache true cloudflow config set db.prepared_statements.max_size 250 # Reduce idle connection timeout cloudflow config set db.pool.idle_timeout 120 # 2 minutes ``` --- ## Workflow Execution Failures ### Timeout Errors (3600 second limit) #### Error Message ``` WorkflowExecutionError: Workflow exceeded maximum execution time of 3600 seconds Status: TIMEOUT Workflow ID: wf_9k2n4m8p1q Execution ID: exec_7h3j6k9m2n ``` #### Analysis ```bash # Get workflow execution details cloudflow workflows executions get exec_7h3j6k9m2n --verbose # Check step-by-step breakdown cloudflow workflows executions steps exec_7h3j6k9m2n # Sample output: # Step 1: data_ingestion - Duration: 245s - Status: SUCCESS # Step 2: data_validation - Duration: 123s - Status: SUCCESS # Step 3: data_transformation - Duration: 3189s - Status: TIMEOUT # Step 4: data_export - Duration: 0s - Status: SKIPPED # Identify bottleneck step cloudflow workflows analyze",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_7",
              "content": "records: 500 - Session timeout: 30 seconds --- ## Message Queue Patterns ### Pub/Sub Pattern Used for broadcasting events to multiple interested consumers: ``` Workflow Engine publishes workflow.execution.completed \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u25bc \u25bc \u25bc \u25bc Analytics Notification Audit Logger Billing Service Service Service Service ``` Each service maintains its own consumer group and processes events independently. Failures in one consumer don't affect others. ### Request-Reply Pattern Synchronous communication over async messaging (used sparingly): ``` API Gateway publishes request \u2192 Kafka (reply-to: temp-queue-123) \u2502 \u25bc Service processes request \u2502 \u25bc Service publishes response \u2192 temp-queue-123 \u2502 \u25bc API Gateway receives response ``` Timeout: 5 seconds, fallback to direct HTTP call if no response. ### Saga Pattern Distributed transaction management for multi-service workflows: ``` Step 1: Create Order \u2192 SUCCESS \u2192 Step 2: Reserve Inventory \u2502 FAILURE \u2502 \u25bc Compensating Transaction \u2502 \u25bc Cancel Order (Rollback) ``` Orchestrated by Workflow Engine with compensation logic defined in workflow DSL. ### Dead Letter Queue (DLQ) Failed messages after all retry attempts are routed to DLQ: ``` Message processing fails (3 retries with exponential backoff) \u2502 \u25bc Publish to dead-letter-queue topic \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u25bc \u25bc Alert to PagerDuty Store in PostgreSQL for analysis \u2502 \u2502 \u25bc \u25bc Manual investigation Automated pattern detection ``` DLQ Processing SLA: < 4 hours for critical events, < 24 hours for non-critical. --- ## Caching Strategy ### Cache-Aside Pattern (Lazy Loading) Primary caching pattern used across all services: ``` 1. Application checks cache (Redis GET) 2. If HIT \u2192 Return cached data 3. If MISS: a. Query database (PostgreSQL) b. Store result in cache with TTL c. Return data to application ``` **Implementation Example** (Workflow Definition): ```javascript async function getWorkflowDefinition(workflowId) { const cacheKey = `workflow:def:${workflowId}`; // Step 1: Check cache let workflow = await redis.get(cacheKey); if (workflow) { metrics.increment('cache.hit.workflow_def'); return JSON.parse(workflow); } // Step 2: Cache miss - query database metrics.increment('cache.miss.workflow_def'); workflow = await db.query( 'SELECT * FROM workflows WHERE id = $1', [workflowId] ); // Step 3: Store in cache (1 hour TTL) await redis.setex(cacheKey, 3600, JSON.stringify(workflow)); return workflow; } ``` ### Write-Through Pattern Used for critical data where cache consistency is paramount: ``` 1. Application writes to database 2. Database transaction commits 3. Application updates cache 4. Return success to client ``` Applied to: User profiles, authentication sessions, system configuration.",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_5",
              "content": "Success/failure events published back to Kafka ``` **Event Schema**: ```json { \"event_id\": \"uuid-v4\", \"event_type\": \"workflow.execution.completed\", \"timestamp\": \"2026-01-15T10:30:00.000Z\", \"correlation_id\": \"request-trace-id\", \"payload\": { \"workflow_id\": \"wf-12345\", \"execution_id\": \"exec-67890\", \"status\": \"COMPLETED\", \"duration_ms\": 4230 }, \"metadata\": { \"source_service\": \"workflow-engine\", \"schema_version\": \"1.0\" } } ``` ### Inter-Service Communication Services communicate using two primary patterns: **Synchronous (gRPC)**: - Auth Service validation calls from API Gateway - User Service profile lookups - Low latency requirement (< 50ms) - Request-response pattern with circuit breaker **Asynchronous (Kafka)**: - Workflow execution events - Notification triggers - Audit log events - Eventual consistency acceptable --- ## Database Architecture ### PostgreSQL Primary Database **Cluster Configuration**: - Primary-replica setup with 1 primary + 2 read replicas - Instance type: db.r6g.2xlarge (8 vCPU, 64GB RAM) - Storage: 2TB gp3 SSD with 12,000 IOPS - Multi-AZ deployment for high availability - Automated backups: Daily snapshots, 30-day retention - Point-in-time recovery: 5-minute granularity **Database Schema Design**: ``` Core Tables: - users (5M rows): User accounts and profiles - workflows (2M rows): Workflow definitions - workflow_executions (500M rows, partitioned): Execution history - workflow_steps (2B rows, partitioned): Individual step records - schedules (100K rows): Scheduled workflow triggers - notifications (1B rows, partitioned): Notification delivery log Partitioning Strategy: - workflow_executions: Monthly partitions by created_at - workflow_steps: Monthly partitions by created_at - notifications: Weekly partitions by created_at - Automatic partition creation via cron job - Retention policy: Drop partitions older than 12 months ``` **Indexing Strategy**: - Primary keys: UUIDs with B-tree indexes - Frequently queried columns: Compound indexes (e.g., user_id + status + created_at) - JSONB columns: GIN indexes for workflow definitions - Full-text search: GiST indexes on description fields **Connection Pooling**: - PgBouncer in transaction mode - Pool size: 100 connections per service - Max client connections: 2000 - Connection timeout: 30 seconds **Performance Characteristics**: - Read query P99: < 20ms - Write query P99: < 50ms - Transaction throughput: 15,000 TPS - Replication lag: < 500ms --- ### Redis Caching Layer **Cluster Configuration**: - Redis Cluster with 6 nodes (3 primary + 3 replica) - Instance type: cache.r6g.xlarge (4 vCPU, 26GB RAM) - Total memory: 78GB usable cache space - Persistence: RDB snapshots every 5 minutes + AOF - Eviction policy: allkeys-lru **Cache Usage Patterns**: 1.",
              "score": null
            },
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_7",
              "content": "podSelector: matchLabels: app: cloudflow policyTypes: - Ingress - Egress ingress: - from: - namespaceSelector: matchLabels: name: cloudflow-prod - podSelector: matchLabels: app: nginx-ingress ports: - protocol: TCP port: 3000 egress: - to: - podSelector: matchLabels: app: postgresql ports: - protocol: TCP port: 5432 - to: - podSelector: matchLabels: app: redis ports: - protocol: TCP port: 6379 ``` ### Pod Security Standards Enforce Pod Security Standards at the namespace level: ```bash kubectl label namespace cloudflow-prod \\ pod-security.kubernetes.io/enforce=restricted \\ pod-security.kubernetes.io/audit=restricted \\ pod-security.kubernetes.io/warn=restricted ``` ### Secrets Management Use AWS Secrets Manager for sensitive credentials: ```bash # Install External Secrets Operator helm install external-secrets external-secrets/external-secrets \\ --namespace external-secrets-system \\ --create-namespace # Create SecretStore kubectl apply -f - <<EOF apiVersion: external-secrets.io/v1beta1 kind: SecretStore metadata: name: aws-secrets-manager namespace: cloudflow-prod spec: provider: aws: service: SecretsManager region: us-east-1 auth: jwt: serviceAccountRef: name: external-secrets-sa EOF ``` --- ## Troubleshooting ### Common Issues #### Pods in CrashLoopBackOff **Symptoms**: Pods continuously restart **Diagnosis**: ```bash kubectl logs -n cloudflow-prod <pod-name> --previous kubectl describe pod -n cloudflow-prod <pod-name> ``` **Common Causes**: - Database connection failure - Invalid environment variables - Insufficient resources #### High Memory Usage **Symptoms**: Pods being OOMKilled **Diagnosis**: ```bash kubectl top pods -n cloudflow-prod ``` **Resolution**: - Increase memory limits in deployment - Check for memory leaks in application logs - Review heap dump for Node.js processes #### Database Connection Pool Exhausted **Symptoms**: \"Too many connections\" errors **Diagnosis**: ```bash # Check PgBouncer stats kubectl exec -it pgbouncer-xxx -n cloudflow-prod -- \\ psql -p 5432 -U pgbouncer pgbouncer -c \"SHOW POOLS;\" ``` **Resolution**: - Increase `max_db_connections` in PgBouncer - Optimize application connection usage - Add more PostgreSQL replicas ### Support Contacts - **On-call Engineer**: pagerduty.com/cloudflow-oncall - **Slack Channel**: #cloudflow-operations - **Documentation**: https://docs.cloudflow.io - **Status Page**: https://status.cloudflow.io --- ## Appendix ### Useful Commands ```bash # View all resources in namespace kubectl get all -n cloudflow-prod # Check resource usage kubectl top pods -n cloudflow-prod kubectl top nodes # View logs kubectl logs -f deployment/cloudflow-api -n cloudflow-prod # Execute command in pod kubectl exec -it <pod-name> -n cloudflow-prod -- /bin/sh # Port forward for local testing kubectl port-forward svc/cloudflow-api 8080:80 -n cloudflow-prod # Rollback deployment helm rollback cloudflow -n cloudflow-prod # Update deployment helm upgrade cloudflow cloudflow/cloudflow \\ --namespace cloudflow-prod \\ --values values-production.yaml ``` ### Maintenance Windows Scheduled maintenance occurs during the following windows: - **Primary**: Sunday 02:00-06:00",
              "score": null
            }
          ],
          "latency_ms": 12.751905000186525,
          "query_id": "realistic_017",
          "dimension": "casual",
          "query": "pgbouncer purpose"
        },
        {
          "key_facts": [
            "PgBouncer",
            "connection pooling",
            "max_db_connections = 100",
            "default_pool_size = 25",
            "pool_mode = transaction"
          ],
          "found_facts": [
            "PgBouncer",
            "connection pooling",
            "max_db_connections = 100",
            "default_pool_size = 25",
            "pool_mode = transaction"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "deployment_guide",
            "architecture_overview"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_3",
              "content": "Deploy PostgreSQL using the Bitnami Helm chart: ```yaml # postgres-values.yaml global: postgresql: auth: username: cloudflow database: cloudflow existingSecret: postgres-credentials image: tag: \"14.10.0\" primary: resources: limits: cpu: 4000m memory: 8Gi requests: cpu: 2000m memory: 4Gi persistence: enabled: true size: 100Gi storageClass: gp3 extendedConfiguration: | max_connections = 100 shared_buffers = 2GB effective_cache_size = 6GB maintenance_work_mem = 512MB checkpoint_completion_target = 0.9 wal_buffers = 16MB default_statistics_target = 100 random_page_cost = 1.1 effective_io_concurrency = 200 work_mem = 10MB min_wal_size = 1GB max_wal_size = 4GB readReplicas: replicaCount: 2 resources: limits: cpu: 2000m memory: 4Gi requests: cpu: 1000m memory: 2Gi metrics: enabled: true serviceMonitor: enabled: true ``` Install PostgreSQL: ```bash # Create password secret kubectl create secret generic postgres-credentials \\ --namespace cloudflow-prod \\ --from-literal=postgres-password=\"$(openssl rand -base64 32)\" \\ --from-literal=password=\"$(openssl rand -base64 32)\" # Install PostgreSQL helm install postgresql bitnami/postgresql \\ --namespace cloudflow-prod \\ --values postgres-values.yaml \\ --wait ``` ### PgBouncer Configuration Deploy PgBouncer for connection pooling to handle up to 100 connections efficiently: ```yaml # pgbouncer.yaml apiVersion: v1 kind: ConfigMap metadata: name: pgbouncer-config namespace: cloudflow-prod data: pgbouncer.ini: | [databases] cloudflow = host=postgresql.cloudflow-prod.svc.cluster.local port=5432 dbname=cloudflow [pgbouncer] listen_addr = 0.0.0.0 listen_port = 5432 auth_type = md5 auth_file = /etc/pgbouncer/userlist.txt pool_mode = transaction max_client_conn = 1000 default_pool_size = 25 reserve_pool_size = 5 reserve_pool_timeout = 3 max_db_connections = 100 max_user_connections = 100 server_lifetime = 3600 server_idle_timeout = 600 log_connections = 1 log_disconnections = 1 log_pooler_errors = 1 --- apiVersion: apps/v1 kind: Deployment metadata: name: pgbouncer namespace: cloudflow-prod spec: replicas: 2 selector: matchLabels: app: pgbouncer template: metadata: labels: app: pgbouncer spec: containers: - name: pgbouncer image: pgbouncer/pgbouncer:1.21.0 ports: - containerPort: 5432 resources: requests: cpu: 500m memory: 512Mi limits: cpu: 1000m memory: 1Gi volumeMounts: - name: config mountPath: /etc/pgbouncer volumes: - name: config configMap: name: pgbouncer-config --- apiVersion: v1 kind: Service metadata: name: pgbouncer namespace: cloudflow-prod spec: selector: app: pgbouncer ports: - port: 5432 targetPort: 5432 type: ClusterIP ``` Apply the PgBouncer configuration: ```bash kubectl apply -f pgbouncer.yaml ``` ### Database Migrations Run database migrations before deploying new versions: ```bash # Create migration job kubectl create job cloudflow-migrate-$(date +%s) \\ --namespace cloudflow-prod \\ --image=123456789012.dkr.ecr.us-east-1.amazonaws.com/cloudflow:2.4.0 \\ -- npm run migrate # Monitor migration progress kubectl logs -f job/cloudflow-migrate-<timestamp> -n cloudflow-prod ``` --- ## Monitoring and Observability ### Prometheus Setup Install Prometheus using the kube-prometheus-stack: ```bash helm install prometheus prometheus-community/kube-prometheus-stack \\ --namespace monitoring \\ --create-namespace \\ --set prometheus.prometheusSpec.serviceMonitorSelectorNilUsesHelmValues=false \\",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_4",
              "content": "**Implement connection pooling optimization:** ```bash # Use PgBouncer for connection pooling kubectl apply -f cloudflow-pgbouncer.yaml # Configure CloudFlow to use PgBouncer cloudflow config set db.host pgbouncer.cloudflow.svc.cluster.local cloudflow config set db.port 6432 cloudflow config set db.pool.mode transaction ``` 2. **Add read replicas:** ```bash # Route read-only queries to replicas cloudflow db replicas add --count 2 cloudflow config set db.read_replicas \"replica-1:5432,replica-2:5432\" cloudflow config set db.read_write_split true ``` ### Connection Timeout Errors #### Error Messages - `connection timeout after 30000ms` - `could not connect to database server at 10.0.2.45:5432` - `database server unreachable` #### Troubleshooting Steps ```bash # Test network connectivity telnet cloudflow-db.internal.company.com 5432 # Check DNS resolution nslookup cloudflow-db.internal.company.com # Verify database is accepting connections pg_isready -h cloudflow-db.internal.company.com -p 5432 -U cloudflow # Check firewall rules sudo iptables -L -n | grep 5432 # Test from CloudFlow pod network kubectl run -n cloudflow debug-pod --rm -i --tty \\ --image=postgres:14 -- \\ psql -h cloudflow-db.internal.company.com -U cloudflow -d cloudflow_production # Review database logs for connection rejections kubectl logs -n cloudflow statefulset/cloudflow-db --tail=100 | \\ grep -i \"connection\\|reject\\|authentication\" ``` ### Maximum Connection Limit (100) Reached This is a hard limit in CloudFlow's database tier. #### Permanent Solutions **Option 1: Upgrade database tier** ```bash # Check available tiers cloudflow db tiers list # Upgrade to higher tier (supports 200 connections) cloudflow db upgrade --tier standard-plus --confirm # Monitor migration progress cloudflow db migration status ``` **Option 2: Implement aggressive connection reuse** ```bash # Reduce connection lifetime cloudflow config set db.pool.max_lifetime 600 # 10 minutes # Enable prepared statement caching cloudflow config set db.prepared_statements.cache true cloudflow config set db.prepared_statements.max_size 250 # Reduce idle connection timeout cloudflow config set db.pool.idle_timeout 120 # 2 minutes ``` --- ## Workflow Execution Failures ### Timeout Errors (3600 second limit) #### Error Message ``` WorkflowExecutionError: Workflow exceeded maximum execution time of 3600 seconds Status: TIMEOUT Workflow ID: wf_9k2n4m8p1q Execution ID: exec_7h3j6k9m2n ``` #### Analysis ```bash # Get workflow execution details cloudflow workflows executions get exec_7h3j6k9m2n --verbose # Check step-by-step breakdown cloudflow workflows executions steps exec_7h3j6k9m2n # Sample output: # Step 1: data_ingestion - Duration: 245s - Status: SUCCESS # Step 2: data_validation - Duration: 123s - Status: SUCCESS # Step 3: data_transformation - Duration: 3189s - Status: TIMEOUT # Step 4: data_export - Duration: 0s - Status: SKIPPED # Identify bottleneck step cloudflow workflows analyze",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_11",
              "content": "Service (2) - PostgreSQL Primary - PostgreSQL Replica - PostgreSQL Replica - Redis Primary (2) - Redis Replica (2) - Redis Replica (2) - Kafka Broker (2) - Kafka Broker (2) - Kafka Broker (1) ``` **Automatic Failover**: - Database: 30-60 seconds (automatic promotion of replica) - Redis: < 10 seconds (Sentinel-based failover) - Kafka: < 30 seconds (controller election) - Services: Kubernetes health checks with 10-second liveness probes ### Backup Strategy **Database Backups**: - Automated snapshots: Daily at 02:00 UTC - Retention: 30 days for daily, 90 days for monthly - Cross-region replication: Async replication to us-west-2 (15-minute lag) - Backup verification: Weekly automated restore test in staging environment **Backup Schedule**: ``` Daily: Full snapshot \u2192 S3 (encrypted) Hourly: WAL archives \u2192 S3 (point-in-time recovery) Weekly: Backup validation test Monthly: Long-term archive to Glacier ``` **Configuration Backups**: - Kubernetes manifests: Stored in Git (GitOps with ArgoCD) - Vault secrets: Automated snapshot every 6 hours - Kafka topic configurations: Exported daily to S3 ### Disaster Recovery Procedures **Scenario 1: Single AZ Failure** - Detection: Health checks fail for entire AZ (< 30 seconds) - Action: Traffic automatically routed to healthy AZs by ALB - Recovery time: < 5 minutes (no manual intervention) - Data loss: None (multi-AZ replication) **Scenario 2: Database Primary Failure** - Detection: Health check fails for primary database (< 30 seconds) - Action: Automatic promotion of read replica to primary - Recovery time: 30-60 seconds - Data loss: Minimal (< 1 second due to synchronous replication) **Scenario 3: Full Region Failure** - Detection: Multiple health check failures across all AZs (< 2 minutes) - Action: Manual failover to DR region (us-west-2) - Procedure: 1. Update DNS to point to DR region (TTL: 60 seconds) 2. Promote DR database replica to primary 3. Scale up DR region services to production capacity 4. Verify data consistency and integrity 5. Update monitoring dashboards - Recovery time: 2-4 hours (includes verification) - Data loss: < 1 hour (last cross-region replication) **Scenario 4: Data Corruption** - Detection: Data validation checks or user report - Action: Point-in-time recovery from WAL archives - Procedure: 1. Identify corruption time window 2. Restore from snapshot prior to corruption 3. Replay WAL logs up to corruption point 4. Verify data integrity 5.",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_7",
              "content": "records: 500 - Session timeout: 30 seconds --- ## Message Queue Patterns ### Pub/Sub Pattern Used for broadcasting events to multiple interested consumers: ``` Workflow Engine publishes workflow.execution.completed \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u25bc \u25bc \u25bc \u25bc Analytics Notification Audit Logger Billing Service Service Service Service ``` Each service maintains its own consumer group and processes events independently. Failures in one consumer don't affect others. ### Request-Reply Pattern Synchronous communication over async messaging (used sparingly): ``` API Gateway publishes request \u2192 Kafka (reply-to: temp-queue-123) \u2502 \u25bc Service processes request \u2502 \u25bc Service publishes response \u2192 temp-queue-123 \u2502 \u25bc API Gateway receives response ``` Timeout: 5 seconds, fallback to direct HTTP call if no response. ### Saga Pattern Distributed transaction management for multi-service workflows: ``` Step 1: Create Order \u2192 SUCCESS \u2192 Step 2: Reserve Inventory \u2502 FAILURE \u2502 \u25bc Compensating Transaction \u2502 \u25bc Cancel Order (Rollback) ``` Orchestrated by Workflow Engine with compensation logic defined in workflow DSL. ### Dead Letter Queue (DLQ) Failed messages after all retry attempts are routed to DLQ: ``` Message processing fails (3 retries with exponential backoff) \u2502 \u25bc Publish to dead-letter-queue topic \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u25bc \u25bc Alert to PagerDuty Store in PostgreSQL for analysis \u2502 \u2502 \u25bc \u25bc Manual investigation Automated pattern detection ``` DLQ Processing SLA: < 4 hours for critical events, < 24 hours for non-critical. --- ## Caching Strategy ### Cache-Aside Pattern (Lazy Loading) Primary caching pattern used across all services: ``` 1. Application checks cache (Redis GET) 2. If HIT \u2192 Return cached data 3. If MISS: a. Query database (PostgreSQL) b. Store result in cache with TTL c. Return data to application ``` **Implementation Example** (Workflow Definition): ```javascript async function getWorkflowDefinition(workflowId) { const cacheKey = `workflow:def:${workflowId}`; // Step 1: Check cache let workflow = await redis.get(cacheKey); if (workflow) { metrics.increment('cache.hit.workflow_def'); return JSON.parse(workflow); } // Step 2: Cache miss - query database metrics.increment('cache.miss.workflow_def'); workflow = await db.query( 'SELECT * FROM workflows WHERE id = $1', [workflowId] ); // Step 3: Store in cache (1 hour TTL) await redis.setex(cacheKey, 3600, JSON.stringify(workflow)); return workflow; } ``` ### Write-Through Pattern Used for critical data where cache consistency is paramount: ``` 1. Application writes to database 2. Database transaction commits 3. Application updates cache 4. Return success to client ``` Applied to: User profiles, authentication sessions, system configuration.",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_10",
              "content": "- Execution start rate: 500 per second - Completion rate: 450 per second (average 2-second execution time) **Database**: - Read throughput: 50,000 queries per second (across replicas) - Write throughput: 15,000 transactions per second - Connection capacity: 2,000 concurrent connections **Kafka**: - Message ingestion: 100,000 messages per second - Consumer throughput: 80,000 messages per second (aggregated) - End-to-end latency: < 100ms (P99) ### Resource Utilization **CPU Utilization** (Target: 60-70% average): - API Gateway: 55% average, 80% peak - Workflow Engine: 65% average, 85% peak - Auth Service: 40% average, 60% peak **Memory Utilization** (Target: < 80%): - API Gateway: 2.5GB average per pod (4GB allocated) - Workflow Engine: 6GB average per pod (8GB allocated) - Notification Service: 2.8GB average per pod (4GB allocated) **Network Throughput**: - Ingress: 2 Gbps average, 5 Gbps peak - Egress: 1.5 Gbps average, 4 Gbps peak - Internal (service mesh): 8 Gbps average ### Monitoring & Alerting **Key Metrics** (Monitored via Prometheus): - Request rate (per service, per endpoint) - Error rate (4xx, 5xx responses) - Latency percentiles (P50, P95, P99) - Resource utilization (CPU, memory, disk) - Cache hit/miss ratios - Database connection pool saturation - Kafka consumer lag **Alerts** (via PagerDuty): - P1 (Immediate): API error rate > 1%, database connection failure - P2 (< 15 min): P99 latency > 500ms, cache hit rate < 80% - P3 (< 1 hour): Resource utilization > 85%, replica count < minimum **SLI/SLO/SLA**: - SLI: API success rate (non-5xx responses) - SLO: 99.9% uptime per month (43 minutes downtime allowance) - SLA: 99.5% uptime guarantee (customer-facing SLA with credits) --- ## Disaster Recovery ### Recovery Objectives **RPO (Recovery Point Objective): 1 hour** - Maximum acceptable data loss: 1 hour of transactions - Achieved through: Continuous database replication + hourly snapshots - Kafka retention: 7 days allows event replay **RTO (Recovery Time Objective): 4 hours** - Maximum acceptable downtime: 4 hours for full system recovery - Includes: Failover, data verification, and service restoration - Automated runbooks reduce RTO to < 2 hours for common scenarios ### High Availability Architecture **Multi-AZ Deployment**: ``` Region: us-east-1 AZ-1a: AZ-1b: AZ-1c: - API Gateway (4) - API Gateway (4) - API Gateway (4) - Workflow Engine (6) - Workflow Engine (5) - Workflow Engine (5) - Auth Service (3) - Auth Service (3) - Auth",
              "score": null
            }
          ],
          "latency_ms": 13.018181998631917,
          "query_id": "realistic_017",
          "dimension": "contextual",
          "query": "Optimizing database connections, what role does PgBouncer play?"
        },
        {
          "key_facts": [
            "PgBouncer",
            "connection pooling",
            "max_db_connections = 100",
            "default_pool_size = 25",
            "pool_mode = transaction"
          ],
          "found_facts": [
            "PgBouncer",
            "connection pooling"
          ],
          "missed_facts": [
            "max_db_connections = 100",
            "default_pool_size = 25",
            "pool_mode = transaction"
          ],
          "coverage": 0.4,
          "expected_docs": [
            "deployment_guide",
            "architecture_overview"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_4",
              "content": "**Implement connection pooling optimization:** ```bash # Use PgBouncer for connection pooling kubectl apply -f cloudflow-pgbouncer.yaml # Configure CloudFlow to use PgBouncer cloudflow config set db.host pgbouncer.cloudflow.svc.cluster.local cloudflow config set db.port 6432 cloudflow config set db.pool.mode transaction ``` 2. **Add read replicas:** ```bash # Route read-only queries to replicas cloudflow db replicas add --count 2 cloudflow config set db.read_replicas \"replica-1:5432,replica-2:5432\" cloudflow config set db.read_write_split true ``` ### Connection Timeout Errors #### Error Messages - `connection timeout after 30000ms` - `could not connect to database server at 10.0.2.45:5432` - `database server unreachable` #### Troubleshooting Steps ```bash # Test network connectivity telnet cloudflow-db.internal.company.com 5432 # Check DNS resolution nslookup cloudflow-db.internal.company.com # Verify database is accepting connections pg_isready -h cloudflow-db.internal.company.com -p 5432 -U cloudflow # Check firewall rules sudo iptables -L -n | grep 5432 # Test from CloudFlow pod network kubectl run -n cloudflow debug-pod --rm -i --tty \\ --image=postgres:14 -- \\ psql -h cloudflow-db.internal.company.com -U cloudflow -d cloudflow_production # Review database logs for connection rejections kubectl logs -n cloudflow statefulset/cloudflow-db --tail=100 | \\ grep -i \"connection\\|reject\\|authentication\" ``` ### Maximum Connection Limit (100) Reached This is a hard limit in CloudFlow's database tier. #### Permanent Solutions **Option 1: Upgrade database tier** ```bash # Check available tiers cloudflow db tiers list # Upgrade to higher tier (supports 200 connections) cloudflow db upgrade --tier standard-plus --confirm # Monitor migration progress cloudflow db migration status ``` **Option 2: Implement aggressive connection reuse** ```bash # Reduce connection lifetime cloudflow config set db.pool.max_lifetime 600 # 10 minutes # Enable prepared statement caching cloudflow config set db.prepared_statements.cache true cloudflow config set db.prepared_statements.max_size 250 # Reduce idle connection timeout cloudflow config set db.pool.idle_timeout 120 # 2 minutes ``` --- ## Workflow Execution Failures ### Timeout Errors (3600 second limit) #### Error Message ``` WorkflowExecutionError: Workflow exceeded maximum execution time of 3600 seconds Status: TIMEOUT Workflow ID: wf_9k2n4m8p1q Execution ID: exec_7h3j6k9m2n ``` #### Analysis ```bash # Get workflow execution details cloudflow workflows executions get exec_7h3j6k9m2n --verbose # Check step-by-step breakdown cloudflow workflows executions steps exec_7h3j6k9m2n # Sample output: # Step 1: data_ingestion - Duration: 245s - Status: SUCCESS # Step 2: data_validation - Duration: 123s - Status: SUCCESS # Step 3: data_transformation - Duration: 3189s - Status: TIMEOUT # Step 4: data_export - Duration: 0s - Status: SKIPPED # Identify bottleneck step cloudflow workflows analyze",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_2",
              "content": "Review Query Execution Plans** ```sql -- Connect to CloudFlow database cloudflow db connect --readonly -- Explain slow query EXPLAIN ANALYZE SELECT w.*, e.status, e.error_message FROM workflows w LEFT JOIN executions e ON w.id = e.workflow_id WHERE w.workspace_id = 'ws_abc123' AND e.created_at > NOW() - INTERVAL '7 days' ORDER BY e.created_at DESC; -- Check for missing indexes SELECT schemaname, tablename, indexname, indexdef FROM pg_indexes WHERE tablename IN ('workflows', 'executions', 'workflow_steps') ORDER BY tablename, indexname; ``` **3. Optimize Queries** Common optimization techniques: **Add missing indexes:** ```sql -- Index for workflow lookup by workspace CREATE INDEX CONCURRENTLY idx_workflows_workspace_created ON workflows(workspace_id, created_at DESC); -- Index for execution status queries CREATE INDEX CONCURRENTLY idx_executions_status_created ON executions(workflow_id, status, created_at DESC) WHERE status IN ('running', 'pending'); -- Composite index for common filter combinations CREATE INDEX CONCURRENTLY idx_executions_workspace_status ON executions(workspace_id, status, created_at) INCLUDE (error_message, retry_count); ``` **Use query result caching:** ```bash # Enable query cache for workspace metadata cloudflow config set cache.workspace.ttl 3600 # Configure Redis cache backend cloudflow config set cache.backend redis cloudflow config set cache.redis.host redis.cloudflow.svc.cluster.local cloudflow config set cache.redis.port 6379 ``` ### High API Latency #### Latency Breakdown Analysis ```bash # Generate latency report cloudflow metrics latency-report --endpoint \"/api/v1/workflows\" --last 24h # Sample output: # Endpoint: POST /api/v1/workflows/execute # P50: 245ms | P95: 1823ms | P99: 4521ms # Breakdown: # - Auth: 45ms (18%) # - DB Query: 156ms (64%) # - Business Logic: 32ms (13%) # - Response Serialization: 12ms (5%) ``` **Network latency issues:** ```bash # Test connectivity to CloudFlow API time curl -w \"@curl-format.txt\" -o /dev/null -s https://api.cloudflow.io/health # Create curl-format.txt: cat > curl-format.txt << EOF time_namelookup: %{time_namelookup}s\\n time_connect: %{time_connect}s\\n time_appconnect: %{time_appconnect}s\\n time_pretransfer: %{time_pretransfer}s\\n time_redirect: %{time_redirect}s\\n time_starttransfer: %{time_starttransfer}s\\n ----------\\n time_total: %{time_total}s\\n EOF # Trace route to API endpoint traceroute api.cloudflow.io # Check DNS resolution time dig api.cloudflow.io | grep \"Query time\" ``` ### Memory Leaks #### Detection ```bash # Monitor CloudFlow service memory usage kubectl top pods -n cloudflow --sort-by=memory # Get detailed memory metrics for specific pod kubectl exec -n cloudflow deploy/cloudflow-api -- \\ curl localhost:9090/metrics | grep memory # Check for OOMKilled pods kubectl get pods -n cloudflow --field-selector=status.phase=Failed | \\ grep OOMKilled # Review memory limits and requests kubectl describe deployment cloudflow-api -n cloudflow | \\ grep -A 5 \"Limits\\|Requests\" ``` #### Common Causes **1.",
              "score": null
            },
            {
              "doc_id": "user_guide",
              "chunk_id": "user_guide_fix_2",
              "content": "### HTTP Requests Make HTTP requests to any API endpoint: **Configuration:** - **Method**: GET, POST, PUT, PATCH, DELETE, HEAD - **URL**: Full endpoint URL (supports variable interpolation) - **Headers**: Custom headers as key-value pairs - **Query Parameters**: URL parameters - **Body**: JSON, form data, or raw text - **Authentication**: Basic Auth, Bearer Token, API Key, OAuth 2.0 **Example:** ```yaml - id: fetch_user action: http_request config: method: GET url: \"https://api.example.com/users/{{user_id}}\" headers: Authorization: \"Bearer {{secrets.API_TOKEN}}\" Content-Type: \"application/json\" timeout: 30 ``` **Response Handling:** Access the response in subsequent steps: - `{{steps.fetch_user.status}}` - HTTP status code - `{{steps.fetch_user.body}}` - Response body - `{{steps.fetch_user.headers}}` - Response headers ### Database Queries Execute queries against supported databases (PostgreSQL, MySQL, MongoDB, Redis): **SQL Databases (PostgreSQL, MySQL):** ```yaml - id: get_orders action: database_query config: connection: \"{{secrets.DB_CONNECTION_STRING}}\" query: | SELECT * FROM orders WHERE customer_id = $1 AND status = $2 ORDER BY created_at DESC LIMIT 10 parameters: - \"{{trigger.customer_id}}\" - \"pending\" ``` **MongoDB:** ```yaml - id: find_documents action: mongodb_query config: connection: \"{{secrets.MONGO_URI}}\" database: \"production\" collection: \"users\" operation: find filter: email: \"{{trigger.email}}\" options: limit: 1 ``` **Important Notes:** - Always use parameterized queries to prevent SQL injection - Connection strings should be stored in secrets - Query timeout is 30 seconds by default - Maximum result set size is 10MB ### Email Notifications Send emails via SMTP or integrated email providers: **Configuration:** ```yaml - id: send_notification action: email config: provider: \"smtp\" # or \"sendgrid\", \"mailgun\", \"ses\" from: \"notifications@cloudflow.io\" to: \"{{trigger.recipient}}\" cc: \"manager@company.com\" subject: \"Alert: {{alert_type}}\" body: | Hello {{user.name}}, This is an automated notification about {{event_description}}. Details: - Time: {{timestamp}} - Type: {{alert_type}} - Priority: {{priority}} Best regards, CloudFlow Automation html: true attachments: - name: \"report.pdf\" content: \"{{steps.generate_report.output}}\" encoding: \"base64\" ``` **Email Templates:** CloudFlow supports dynamic templates with conditional content: ``` {{#if priority == 'high'}} \u26a0\ufe0f URGENT: Immediate attention required {{else}} \u2139\ufe0f Informational notification {{/if}} ``` ### Slack Messages Send messages to Slack channels or users: **Channel Messages:** ```yaml - id: notify_team action: slack_message config: channel: \"#deployments\" text: \"Deployment completed successfully!\" blocks: - type: \"section\" text: type: \"mrkdwn\" text: \"*Deployment Summary*\\nEnvironment: {{environment}}\\nVersion: {{version}}\\nDuration: {{duration}}s\" - type: \"actions\" elements: - type: \"button\" text: \"View Logs\" url: \"{{logs_url}}\" thread_ts: \"{{trigger.thread_ts}}\" # Reply to thread ``` **Direct Messages:** ```yaml - id: dm_user action: slack_message config: user: \"{{user_slack_id}}\" text: \"Your report is ready for review.\"",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_7",
              "content": "records: 500 - Session timeout: 30 seconds --- ## Message Queue Patterns ### Pub/Sub Pattern Used for broadcasting events to multiple interested consumers: ``` Workflow Engine publishes workflow.execution.completed \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u25bc \u25bc \u25bc \u25bc Analytics Notification Audit Logger Billing Service Service Service Service ``` Each service maintains its own consumer group and processes events independently. Failures in one consumer don't affect others. ### Request-Reply Pattern Synchronous communication over async messaging (used sparingly): ``` API Gateway publishes request \u2192 Kafka (reply-to: temp-queue-123) \u2502 \u25bc Service processes request \u2502 \u25bc Service publishes response \u2192 temp-queue-123 \u2502 \u25bc API Gateway receives response ``` Timeout: 5 seconds, fallback to direct HTTP call if no response. ### Saga Pattern Distributed transaction management for multi-service workflows: ``` Step 1: Create Order \u2192 SUCCESS \u2192 Step 2: Reserve Inventory \u2502 FAILURE \u2502 \u25bc Compensating Transaction \u2502 \u25bc Cancel Order (Rollback) ``` Orchestrated by Workflow Engine with compensation logic defined in workflow DSL. ### Dead Letter Queue (DLQ) Failed messages after all retry attempts are routed to DLQ: ``` Message processing fails (3 retries with exponential backoff) \u2502 \u25bc Publish to dead-letter-queue topic \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u25bc \u25bc Alert to PagerDuty Store in PostgreSQL for analysis \u2502 \u2502 \u25bc \u25bc Manual investigation Automated pattern detection ``` DLQ Processing SLA: < 4 hours for critical events, < 24 hours for non-critical. --- ## Caching Strategy ### Cache-Aside Pattern (Lazy Loading) Primary caching pattern used across all services: ``` 1. Application checks cache (Redis GET) 2. If HIT \u2192 Return cached data 3. If MISS: a. Query database (PostgreSQL) b. Store result in cache with TTL c. Return data to application ``` **Implementation Example** (Workflow Definition): ```javascript async function getWorkflowDefinition(workflowId) { const cacheKey = `workflow:def:${workflowId}`; // Step 1: Check cache let workflow = await redis.get(cacheKey); if (workflow) { metrics.increment('cache.hit.workflow_def'); return JSON.parse(workflow); } // Step 2: Cache miss - query database metrics.increment('cache.miss.workflow_def'); workflow = await db.query( 'SELECT * FROM workflows WHERE id = $1', [workflowId] ); // Step 3: Store in cache (1 hour TTL) await redis.setex(cacheKey, 3600, JSON.stringify(workflow)); return workflow; } ``` ### Write-Through Pattern Used for critical data where cache consistency is paramount: ``` 1. Application writes to database 2. Database transaction commits 3. Application updates cache 4. Return success to client ``` Applied to: User profiles, authentication sessions, system configuration.",
              "score": null
            },
            {
              "doc_id": "user_guide",
              "chunk_id": "user_guide_fix_7",
              "content": "Handle Errors Gracefully Always implement error handling for external API calls and database operations: ```yaml - id: fetch_data action: http_request config: url: \"https://api.example.com/data\" retry: max_attempts: 3 backoff_type: exponential on_error: - id: log_error action: database_query config: query: \"INSERT INTO error_log (workflow_id, error) VALUES ($1, $2)\" parameters: - \"{{workflow.id}}\" - \"{{error.message}}\" ``` ### 3. Use Secrets for Sensitive Data Never hardcode API keys, passwords, or tokens in workflows: **Bad:** ```yaml headers: Authorization: \"Bearer sk_live_abc123xyz789\" ``` **Good:** ```yaml headers: Authorization: \"Bearer {{secrets.API_TOKEN}}\" ``` Store secrets in **Settings** > **Secrets** with encryption at rest. ### 4. Validate Input Data Always validate trigger data before processing: ```yaml - id: validate_input action: javascript code: | const required_fields = ['email', 'name', 'order_id']; for (const field of required_fields) { if (!input[field]) { throw new Error(`Missing required field: ${field}`); } } // Validate email format const email_regex = /^[^\\s@]+@[^\\s@]+\\.[^\\s@]+$/; if (!email_regex.test(input.email)) { throw new Error('Invalid email format'); } return { validated: true }; ``` ### 5. Use Idempotency Keys For operations that shouldn't be repeated (payments, record creation), use idempotency keys: ```yaml - id: create_charge action: http_request config: url: \"https://api.stripe.com/v1/charges\" method: POST headers: Idempotency-Key: \"{{workflow.id}}-{{execution.id}}\" body: amount: \"{{amount}}\" ``` ### 6. Monitor and Log Add logging steps for important workflow milestones: ```yaml - id: log_start action: database_query config: query: \"INSERT INTO workflow_audit (execution_id, step, timestamp) VALUES ($1, $2, $3)\" parameters: - \"{{execution.id}}\" - \"workflow_started\" - \"{{now()}}\" ``` ### 7. Keep Workflows Modular Break complex workflows into smaller, reusable components: - Use sub-workflows for repeated logic - Trigger child workflows via webhooks - Share common configurations via templates ### 8. Test Thoroughly Before activating a workflow: 1. Use test mode with sample data 2. Verify all actions execute correctly 3. Test error handling paths 4. Review execution logs 5. Start with a limited scope (e.g., test channel, small dataset) ### 9. Document Your Workflows Add descriptions to workflows and steps: ```yaml workflow: name: \"Daily Sales Report\" description: | Generates a daily sales report and distributes it to the sales team. Runs at 8:00 AM EST Monday-Friday. Data source: PostgreSQL sales database Recipients: sales-team@company.com steps: - id: fetch_sales name: \"Fetch Yesterday's Sales\" description: \"Query database for all completed orders from previous day\" ``` ### 10.",
              "score": null
            }
          ],
          "latency_ms": 13.310785998328356,
          "query_id": "realistic_017",
          "dimension": "negation",
          "query": "Why can't I connect directly to the database?"
        },
        {
          "key_facts": [
            "max_attempts: 3",
            "backoff_type: exponential",
            "initial_interval: 1000",
            "max retries: 3"
          ],
          "found_facts": [
            "max_attempts: 3",
            "backoff_type: exponential",
            "initial_interval: 1000",
            "max retries: 3"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "troubleshooting_guide",
            "user_guide"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "user_guide",
              "chunk_id": "user_guide_fix_5",
              "content": "Save and activate **Testing Schedules:** Use the built-in schedule calculator to preview upcoming executions: ``` Next 5 executions: 1. 2026-01-24 14:00:00 UTC 2. 2026-01-25 14:00:00 UTC 3. 2026-01-26 14:00:00 UTC 4. 2026-01-27 14:00:00 UTC 5. 2026-01-28 14:00:00 UTC ``` ## Error Handling Robust error handling ensures your workflows are resilient and reliable. ### Retry Policies Configure automatic retries for failed actions: ```yaml - id: api_call action: http_request config: url: \"https://api.example.com/data\" retry: max_attempts: 3 backoff_type: \"exponential\" # or \"fixed\", \"linear\" initial_interval: 1000 # milliseconds max_interval: 30000 multiplier: 2.0 retry_on: - timeout - network_error - status: [500, 502, 503, 504] ``` **Backoff Strategies:** - **Fixed**: Wait the same amount of time between retries - Attempt 1: Wait 1s - Attempt 2: Wait 1s - Attempt 3: Wait 1s - **Linear**: Increase wait time by a fixed amount - Attempt 1: Wait 1s - Attempt 2: Wait 2s - Attempt 3: Wait 3s - **Exponential**: Double the wait time with each retry (recommended) - Attempt 1: Wait 1s - Attempt 2: Wait 2s - Attempt 3: Wait 4s **Retry Conditions:** Control which errors trigger retries: - `timeout`: Request timeout - `network_error`: Connection failures - `status`: Specific HTTP status codes - `error_code`: Application-specific error codes ### Fallback Actions Execute alternative actions when the primary action fails: ```yaml - id: primary_payment action: http_request config: url: \"https://primary-payment-gateway.com/charge\" method: POST body: amount: \"{{amount}}\" on_error: - id: fallback_payment action: http_request config: url: \"https://backup-payment-gateway.com/charge\" method: POST body: amount: \"{{amount}}\" - id: notify_admin action: email config: to: \"admin@company.com\" subject: \"Payment Gateway Failure\" body: \"Primary gateway failed, switched to backup\" ``` **Error Object:** Access error details in fallback actions: ``` {{error.message}} # Error message {{error.code}} # Error code {{error.step_id}} # Failed step ID {{error.timestamp}} # When the error occurred {{error.attempts}} # Number of retry attempts ``` ### Dead Letter Queue When all retries and fallbacks fail, CloudFlow can route failed executions to a Dead Letter Queue (DLQ) for manual review: **Enable DLQ:** ```yaml workflow: error_handling: dead_letter_queue: enabled: true retain_days: 30 ``` **DLQ Features:** - View failed executions in the dashboard - Inspect complete execution context and error details - Retry individual executions after fixing issues - Export failed executions for analysis - Set up alerts for DLQ threshold breaches **Accessing the DLQ:** 1. Navigate to **Workflows** > **[Your Workflow]** > **Dead Letter Queue** 2.",
              "score": null
            },
            {
              "doc_id": "user_guide",
              "chunk_id": "user_guide_fix_7",
              "content": "Handle Errors Gracefully Always implement error handling for external API calls and database operations: ```yaml - id: fetch_data action: http_request config: url: \"https://api.example.com/data\" retry: max_attempts: 3 backoff_type: exponential on_error: - id: log_error action: database_query config: query: \"INSERT INTO error_log (workflow_id, error) VALUES ($1, $2)\" parameters: - \"{{workflow.id}}\" - \"{{error.message}}\" ``` ### 3. Use Secrets for Sensitive Data Never hardcode API keys, passwords, or tokens in workflows: **Bad:** ```yaml headers: Authorization: \"Bearer sk_live_abc123xyz789\" ``` **Good:** ```yaml headers: Authorization: \"Bearer {{secrets.API_TOKEN}}\" ``` Store secrets in **Settings** > **Secrets** with encryption at rest. ### 4. Validate Input Data Always validate trigger data before processing: ```yaml - id: validate_input action: javascript code: | const required_fields = ['email', 'name', 'order_id']; for (const field of required_fields) { if (!input[field]) { throw new Error(`Missing required field: ${field}`); } } // Validate email format const email_regex = /^[^\\s@]+@[^\\s@]+\\.[^\\s@]+$/; if (!email_regex.test(input.email)) { throw new Error('Invalid email format'); } return { validated: true }; ``` ### 5. Use Idempotency Keys For operations that shouldn't be repeated (payments, record creation), use idempotency keys: ```yaml - id: create_charge action: http_request config: url: \"https://api.stripe.com/v1/charges\" method: POST headers: Idempotency-Key: \"{{workflow.id}}-{{execution.id}}\" body: amount: \"{{amount}}\" ``` ### 6. Monitor and Log Add logging steps for important workflow milestones: ```yaml - id: log_start action: database_query config: query: \"INSERT INTO workflow_audit (execution_id, step, timestamp) VALUES ($1, $2, $3)\" parameters: - \"{{execution.id}}\" - \"workflow_started\" - \"{{now()}}\" ``` ### 7. Keep Workflows Modular Break complex workflows into smaller, reusable components: - Use sub-workflows for repeated logic - Trigger child workflows via webhooks - Share common configurations via templates ### 8. Test Thoroughly Before activating a workflow: 1. Use test mode with sample data 2. Verify all actions execute correctly 3. Test error handling paths 4. Review execution logs 5. Start with a limited scope (e.g., test channel, small dataset) ### 9. Document Your Workflows Add descriptions to workflows and steps: ```yaml workflow: name: \"Daily Sales Report\" description: | Generates a daily sales report and distributes it to the sales team. Runs at 8:00 AM EST Monday-Friday. Data source: PostgreSQL sales database Recipients: sales-team@company.com steps: - id: fetch_sales name: \"Fetch Yesterday's Sales\" description: \"Query database for all completed orders from previous day\" ``` ### 10.",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_7",
              "content": "records: 500 - Session timeout: 30 seconds --- ## Message Queue Patterns ### Pub/Sub Pattern Used for broadcasting events to multiple interested consumers: ``` Workflow Engine publishes workflow.execution.completed \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u25bc \u25bc \u25bc \u25bc Analytics Notification Audit Logger Billing Service Service Service Service ``` Each service maintains its own consumer group and processes events independently. Failures in one consumer don't affect others. ### Request-Reply Pattern Synchronous communication over async messaging (used sparingly): ``` API Gateway publishes request \u2192 Kafka (reply-to: temp-queue-123) \u2502 \u25bc Service processes request \u2502 \u25bc Service publishes response \u2192 temp-queue-123 \u2502 \u25bc API Gateway receives response ``` Timeout: 5 seconds, fallback to direct HTTP call if no response. ### Saga Pattern Distributed transaction management for multi-service workflows: ``` Step 1: Create Order \u2192 SUCCESS \u2192 Step 2: Reserve Inventory \u2502 FAILURE \u2502 \u25bc Compensating Transaction \u2502 \u25bc Cancel Order (Rollback) ``` Orchestrated by Workflow Engine with compensation logic defined in workflow DSL. ### Dead Letter Queue (DLQ) Failed messages after all retry attempts are routed to DLQ: ``` Message processing fails (3 retries with exponential backoff) \u2502 \u25bc Publish to dead-letter-queue topic \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u25bc \u25bc Alert to PagerDuty Store in PostgreSQL for analysis \u2502 \u2502 \u25bc \u25bc Manual investigation Automated pattern detection ``` DLQ Processing SLA: < 4 hours for critical events, < 24 hours for non-critical. --- ## Caching Strategy ### Cache-Aside Pattern (Lazy Loading) Primary caching pattern used across all services: ``` 1. Application checks cache (Redis GET) 2. If HIT \u2192 Return cached data 3. If MISS: a. Query database (PostgreSQL) b. Store result in cache with TTL c. Return data to application ``` **Implementation Example** (Workflow Definition): ```javascript async function getWorkflowDefinition(workflowId) { const cacheKey = `workflow:def:${workflowId}`; // Step 1: Check cache let workflow = await redis.get(cacheKey); if (workflow) { metrics.increment('cache.hit.workflow_def'); return JSON.parse(workflow); } // Step 2: Cache miss - query database metrics.increment('cache.miss.workflow_def'); workflow = await db.query( 'SELECT * FROM workflows WHERE id = $1', [workflowId] ); // Step 3: Store in cache (1 hour TTL) await redis.setex(cacheKey, 3600, JSON.stringify(workflow)); return workflow; } ``` ### Write-Through Pattern Used for critical data where cache consistency is paramount: ``` 1. Application writes to database 2. Database transaction commits 3. Application updates cache 4. Return success to client ``` Applied to: User profiles, authentication sessions, system configuration.",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_5",
              "content": "exec_7h3j6k9m2n --show-bottlenecks ``` #### Solutions **1. Increase workflow timeout (if justified):** ```bash # Update workflow configuration cloudflow workflows update wf_9k2n4m8p1q \\ --timeout 7200 \\ --reason \"Large dataset processing requires extended time\" # Verify update cloudflow workflows get wf_9k2n4m8p1q | grep timeout ``` **2. Optimize slow steps:** ```bash # Enable parallel processing cloudflow workflows update wf_9k2n4m8p1q \\ --step data_transformation \\ --parallel-workers 8 \\ --batch-size 1000 # Add checkpointing for long operations cloudflow workflows update wf_9k2n4m8p1q \\ --step data_transformation \\ --enable-checkpointing \\ --checkpoint-interval 300 ``` **3. Split workflow into smaller workflows:** ```bash # Create sub-workflows cloudflow workflows create data-pipeline-part1 \\ --steps \"data_ingestion,data_validation\" \\ --timeout 1800 cloudflow workflows create data-pipeline-part2 \\ --steps \"data_transformation,data_export\" \\ --timeout 3600 \\ --trigger workflow_completed \\ --trigger-workflow data-pipeline-part1 ``` ### Retry Logic and Exponential Backoff CloudFlow implements automatic retry with exponential backoff for transient failures: - Max retries: 3 - Initial delay: 1 second - Backoff multiplier: 2 - Max delay: 60 seconds #### Retry Sequence ``` Attempt 1: Immediate Attempt 2: Wait 1s (2^0 * 1s) Attempt 3: Wait 2s (2^1 * 1s) Attempt 4: Wait 4s (2^2 * 1s) ``` #### Configuration ```bash # View current retry settings cloudflow workflows get wf_9k2n4m8p1q --format json | jq '.retry_policy' # Customize retry behavior cloudflow workflows update wf_9k2n4m8p1q \\ --retry-max-attempts 5 \\ --retry-initial-delay 2000 \\ --retry-backoff-multiplier 2 \\ --retry-max-delay 120000 \\ --retry-on-errors \"NETWORK_ERROR,TIMEOUT,RATE_LIMIT\" # Disable retry for specific step cloudflow workflows update wf_9k2n4m8p1q \\ --step data_export \\ --retry-enabled false ``` #### Monitoring Retries ```bash # List failed executions with retry information cloudflow workflows executions list \\ --status FAILED \\ --show-retries \\ --last 7d # Get retry history for specific execution cloudflow workflows executions retries exec_7h3j6k9m2n # Output: # Execution: exec_7h3j6k9m2n # Attempt 1: FAILED - NetworkError: Connection refused (delay: 0ms) # Attempt 2: FAILED - NetworkError: Connection timeout (delay: 1000ms) # Attempt 3: FAILED - NetworkError: Connection timeout (delay: 2000ms) # Attempt 4: FAILED - NetworkError: Connection timeout (delay: 4000ms) # Final Status: FAILED_AFTER_RETRIES ``` ### Workflow Step Failures #### Common Error Patterns **1.",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_7",
              "content": "Waiting {retry_after} seconds...\") time.sleep(retry_after) continue remaining = int(response.headers.get('X-RateLimit-Remaining', 0)) if remaining < 10: print(f\"Warning: Only {remaining} requests remaining\") return response raise Exception(\"Max retries exceeded due to rate limiting\") ``` **Bash script with rate limit checking:** ```bash #!/bin/bash check_rate_limit() { local remaining=$(curl -s -I https://api.cloudflow.io/api/v1/workflows \\ -H \"Authorization: Bearer $CF_ACCESS_TOKEN\" | \\ grep -i \"x-ratelimit-remaining\" | \\ awk '{print $2}' | tr -d '\\r') if [ \"$remaining\" -lt 10 ]; then echo \"Warning: Only $remaining requests remaining\" local reset=$(curl -s -I https://api.cloudflow.io/api/v1/workflows \\ -H \"Authorization: Bearer $CF_ACCESS_TOKEN\" | \\ grep -i \"x-ratelimit-reset\" | \\ awk '{print $2}' | tr -d '\\r') local wait_time=$((reset - $(date +%s))) echo \"Rate limit resets in $wait_time seconds\" sleep $wait_time fi } # Use before API calls check_rate_limit cloudflow workflows execute wf_9k2n4m8p1q ``` #### Optimization Strategies **1. Implement request batching:** ```bash # Batch multiple workflow executions cloudflow workflows execute-batch \\ --workflow-ids \"wf_id1,wf_id2,wf_id3,wf_id4,wf_id5\" \\ --batch-size 5 # This counts as 1 API request instead of 5 ``` **2. Use webhooks instead of polling:** ```bash # Configure webhook for workflow completion cloudflow webhooks create \\ --event workflow.completed \\ --url https://your-service.com/webhooks/cloudflow \\ --secret $WEBHOOK_SECRET # Verify webhook cloudflow webhooks test webhook_abc123 ``` **3.",
              "score": null
            }
          ],
          "latency_ms": 13.30207999853883,
          "query_id": "realistic_018",
          "dimension": "original",
          "query": "How do I implement retry logic for failed workflow steps?"
        },
        {
          "key_facts": [
            "max_attempts: 3",
            "backoff_type: exponential",
            "initial_interval: 1000",
            "max retries: 3"
          ],
          "found_facts": [
            "max_attempts: 3",
            "backoff_type: exponential",
            "initial_interval: 1000",
            "max retries: 3"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "troubleshooting_guide",
            "user_guide"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "user_guide",
              "chunk_id": "user_guide_fix_5",
              "content": "Save and activate **Testing Schedules:** Use the built-in schedule calculator to preview upcoming executions: ``` Next 5 executions: 1. 2026-01-24 14:00:00 UTC 2. 2026-01-25 14:00:00 UTC 3. 2026-01-26 14:00:00 UTC 4. 2026-01-27 14:00:00 UTC 5. 2026-01-28 14:00:00 UTC ``` ## Error Handling Robust error handling ensures your workflows are resilient and reliable. ### Retry Policies Configure automatic retries for failed actions: ```yaml - id: api_call action: http_request config: url: \"https://api.example.com/data\" retry: max_attempts: 3 backoff_type: \"exponential\" # or \"fixed\", \"linear\" initial_interval: 1000 # milliseconds max_interval: 30000 multiplier: 2.0 retry_on: - timeout - network_error - status: [500, 502, 503, 504] ``` **Backoff Strategies:** - **Fixed**: Wait the same amount of time between retries - Attempt 1: Wait 1s - Attempt 2: Wait 1s - Attempt 3: Wait 1s - **Linear**: Increase wait time by a fixed amount - Attempt 1: Wait 1s - Attempt 2: Wait 2s - Attempt 3: Wait 3s - **Exponential**: Double the wait time with each retry (recommended) - Attempt 1: Wait 1s - Attempt 2: Wait 2s - Attempt 3: Wait 4s **Retry Conditions:** Control which errors trigger retries: - `timeout`: Request timeout - `network_error`: Connection failures - `status`: Specific HTTP status codes - `error_code`: Application-specific error codes ### Fallback Actions Execute alternative actions when the primary action fails: ```yaml - id: primary_payment action: http_request config: url: \"https://primary-payment-gateway.com/charge\" method: POST body: amount: \"{{amount}}\" on_error: - id: fallback_payment action: http_request config: url: \"https://backup-payment-gateway.com/charge\" method: POST body: amount: \"{{amount}}\" - id: notify_admin action: email config: to: \"admin@company.com\" subject: \"Payment Gateway Failure\" body: \"Primary gateway failed, switched to backup\" ``` **Error Object:** Access error details in fallback actions: ``` {{error.message}} # Error message {{error.code}} # Error code {{error.step_id}} # Failed step ID {{error.timestamp}} # When the error occurred {{error.attempts}} # Number of retry attempts ``` ### Dead Letter Queue When all retries and fallbacks fail, CloudFlow can route failed executions to a Dead Letter Queue (DLQ) for manual review: **Enable DLQ:** ```yaml workflow: error_handling: dead_letter_queue: enabled: true retain_days: 30 ``` **DLQ Features:** - View failed executions in the dashboard - Inspect complete execution context and error details - Retry individual executions after fixing issues - Export failed executions for analysis - Set up alerts for DLQ threshold breaches **Accessing the DLQ:** 1. Navigate to **Workflows** > **[Your Workflow]** > **Dead Letter Queue** 2.",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_7",
              "content": "records: 500 - Session timeout: 30 seconds --- ## Message Queue Patterns ### Pub/Sub Pattern Used for broadcasting events to multiple interested consumers: ``` Workflow Engine publishes workflow.execution.completed \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u25bc \u25bc \u25bc \u25bc Analytics Notification Audit Logger Billing Service Service Service Service ``` Each service maintains its own consumer group and processes events independently. Failures in one consumer don't affect others. ### Request-Reply Pattern Synchronous communication over async messaging (used sparingly): ``` API Gateway publishes request \u2192 Kafka (reply-to: temp-queue-123) \u2502 \u25bc Service processes request \u2502 \u25bc Service publishes response \u2192 temp-queue-123 \u2502 \u25bc API Gateway receives response ``` Timeout: 5 seconds, fallback to direct HTTP call if no response. ### Saga Pattern Distributed transaction management for multi-service workflows: ``` Step 1: Create Order \u2192 SUCCESS \u2192 Step 2: Reserve Inventory \u2502 FAILURE \u2502 \u25bc Compensating Transaction \u2502 \u25bc Cancel Order (Rollback) ``` Orchestrated by Workflow Engine with compensation logic defined in workflow DSL. ### Dead Letter Queue (DLQ) Failed messages after all retry attempts are routed to DLQ: ``` Message processing fails (3 retries with exponential backoff) \u2502 \u25bc Publish to dead-letter-queue topic \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u25bc \u25bc Alert to PagerDuty Store in PostgreSQL for analysis \u2502 \u2502 \u25bc \u25bc Manual investigation Automated pattern detection ``` DLQ Processing SLA: < 4 hours for critical events, < 24 hours for non-critical. --- ## Caching Strategy ### Cache-Aside Pattern (Lazy Loading) Primary caching pattern used across all services: ``` 1. Application checks cache (Redis GET) 2. If HIT \u2192 Return cached data 3. If MISS: a. Query database (PostgreSQL) b. Store result in cache with TTL c. Return data to application ``` **Implementation Example** (Workflow Definition): ```javascript async function getWorkflowDefinition(workflowId) { const cacheKey = `workflow:def:${workflowId}`; // Step 1: Check cache let workflow = await redis.get(cacheKey); if (workflow) { metrics.increment('cache.hit.workflow_def'); return JSON.parse(workflow); } // Step 2: Cache miss - query database metrics.increment('cache.miss.workflow_def'); workflow = await db.query( 'SELECT * FROM workflows WHERE id = $1', [workflowId] ); // Step 3: Store in cache (1 hour TTL) await redis.setex(cacheKey, 3600, JSON.stringify(workflow)); return workflow; } ``` ### Write-Through Pattern Used for critical data where cache consistency is paramount: ``` 1. Application writes to database 2. Database transaction commits 3. Application updates cache 4. Return success to client ``` Applied to: User profiles, authentication sessions, system configuration.",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_5",
              "content": "exec_7h3j6k9m2n --show-bottlenecks ``` #### Solutions **1. Increase workflow timeout (if justified):** ```bash # Update workflow configuration cloudflow workflows update wf_9k2n4m8p1q \\ --timeout 7200 \\ --reason \"Large dataset processing requires extended time\" # Verify update cloudflow workflows get wf_9k2n4m8p1q | grep timeout ``` **2. Optimize slow steps:** ```bash # Enable parallel processing cloudflow workflows update wf_9k2n4m8p1q \\ --step data_transformation \\ --parallel-workers 8 \\ --batch-size 1000 # Add checkpointing for long operations cloudflow workflows update wf_9k2n4m8p1q \\ --step data_transformation \\ --enable-checkpointing \\ --checkpoint-interval 300 ``` **3. Split workflow into smaller workflows:** ```bash # Create sub-workflows cloudflow workflows create data-pipeline-part1 \\ --steps \"data_ingestion,data_validation\" \\ --timeout 1800 cloudflow workflows create data-pipeline-part2 \\ --steps \"data_transformation,data_export\" \\ --timeout 3600 \\ --trigger workflow_completed \\ --trigger-workflow data-pipeline-part1 ``` ### Retry Logic and Exponential Backoff CloudFlow implements automatic retry with exponential backoff for transient failures: - Max retries: 3 - Initial delay: 1 second - Backoff multiplier: 2 - Max delay: 60 seconds #### Retry Sequence ``` Attempt 1: Immediate Attempt 2: Wait 1s (2^0 * 1s) Attempt 3: Wait 2s (2^1 * 1s) Attempt 4: Wait 4s (2^2 * 1s) ``` #### Configuration ```bash # View current retry settings cloudflow workflows get wf_9k2n4m8p1q --format json | jq '.retry_policy' # Customize retry behavior cloudflow workflows update wf_9k2n4m8p1q \\ --retry-max-attempts 5 \\ --retry-initial-delay 2000 \\ --retry-backoff-multiplier 2 \\ --retry-max-delay 120000 \\ --retry-on-errors \"NETWORK_ERROR,TIMEOUT,RATE_LIMIT\" # Disable retry for specific step cloudflow workflows update wf_9k2n4m8p1q \\ --step data_export \\ --retry-enabled false ``` #### Monitoring Retries ```bash # List failed executions with retry information cloudflow workflows executions list \\ --status FAILED \\ --show-retries \\ --last 7d # Get retry history for specific execution cloudflow workflows executions retries exec_7h3j6k9m2n # Output: # Execution: exec_7h3j6k9m2n # Attempt 1: FAILED - NetworkError: Connection refused (delay: 0ms) # Attempt 2: FAILED - NetworkError: Connection timeout (delay: 1000ms) # Attempt 3: FAILED - NetworkError: Connection timeout (delay: 2000ms) # Attempt 4: FAILED - NetworkError: Connection timeout (delay: 4000ms) # Final Status: FAILED_AFTER_RETRIES ``` ### Workflow Step Failures #### Common Error Patterns **1.",
              "score": null
            },
            {
              "doc_id": "user_guide",
              "chunk_id": "user_guide_fix_7",
              "content": "Handle Errors Gracefully Always implement error handling for external API calls and database operations: ```yaml - id: fetch_data action: http_request config: url: \"https://api.example.com/data\" retry: max_attempts: 3 backoff_type: exponential on_error: - id: log_error action: database_query config: query: \"INSERT INTO error_log (workflow_id, error) VALUES ($1, $2)\" parameters: - \"{{workflow.id}}\" - \"{{error.message}}\" ``` ### 3. Use Secrets for Sensitive Data Never hardcode API keys, passwords, or tokens in workflows: **Bad:** ```yaml headers: Authorization: \"Bearer sk_live_abc123xyz789\" ``` **Good:** ```yaml headers: Authorization: \"Bearer {{secrets.API_TOKEN}}\" ``` Store secrets in **Settings** > **Secrets** with encryption at rest. ### 4. Validate Input Data Always validate trigger data before processing: ```yaml - id: validate_input action: javascript code: | const required_fields = ['email', 'name', 'order_id']; for (const field of required_fields) { if (!input[field]) { throw new Error(`Missing required field: ${field}`); } } // Validate email format const email_regex = /^[^\\s@]+@[^\\s@]+\\.[^\\s@]+$/; if (!email_regex.test(input.email)) { throw new Error('Invalid email format'); } return { validated: true }; ``` ### 5. Use Idempotency Keys For operations that shouldn't be repeated (payments, record creation), use idempotency keys: ```yaml - id: create_charge action: http_request config: url: \"https://api.stripe.com/v1/charges\" method: POST headers: Idempotency-Key: \"{{workflow.id}}-{{execution.id}}\" body: amount: \"{{amount}}\" ``` ### 6. Monitor and Log Add logging steps for important workflow milestones: ```yaml - id: log_start action: database_query config: query: \"INSERT INTO workflow_audit (execution_id, step, timestamp) VALUES ($1, $2, $3)\" parameters: - \"{{execution.id}}\" - \"workflow_started\" - \"{{now()}}\" ``` ### 7. Keep Workflows Modular Break complex workflows into smaller, reusable components: - Use sub-workflows for repeated logic - Trigger child workflows via webhooks - Share common configurations via templates ### 8. Test Thoroughly Before activating a workflow: 1. Use test mode with sample data 2. Verify all actions execute correctly 3. Test error handling paths 4. Review execution logs 5. Start with a limited scope (e.g., test channel, small dataset) ### 9. Document Your Workflows Add descriptions to workflows and steps: ```yaml workflow: name: \"Daily Sales Report\" description: | Generates a daily sales report and distributes it to the sales team. Runs at 8:00 AM EST Monday-Friday. Data source: PostgreSQL sales database Recipients: sales-team@company.com steps: - id: fetch_sales name: \"Fetch Yesterday's Sales\" description: \"Query database for all completed orders from previous day\" ``` ### 10.",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_6",
              "content": "Data Validation Errors** ``` ValidationError: Field 'customer_id' is required but missing in 234 records ``` **Resolution:** ```bash # Add data quality checks cloudflow workflows update wf_9k2n4m8p1q \\ --step data_ingestion \\ --add-validator required_fields \\ --validator-config '{\"fields\": [\"customer_id\", \"timestamp\", \"amount\"]}' # Configure error handling cloudflow workflows update wf_9k2n4m8p1q \\ --step data_validation \\ --on-error continue \\ --error-threshold 5% # Fail if > 5% of records invalid ``` **2. External API Failures** ``` ExternalAPIError: API request to https://partner-api.example.com failed with status 502 ``` **Resolution:** ```bash # Add circuit breaker cloudflow workflows update wf_9k2n4m8p1q \\ --step external_api_call \\ --circuit-breaker-enabled true \\ --circuit-breaker-threshold 5 \\ --circuit-breaker-timeout 30000 # Configure fallback behavior cloudflow workflows update wf_9k2n4m8p1q \\ --step external_api_call \\ --fallback-action use_cached_data \\ --cache-ttl 3600 ``` --- ## Rate Limiting & Throttling ### 429 Too Many Requests #### Error Response ```json { \"error\": \"rate_limit_exceeded\", \"message\": \"API rate limit exceeded. Retry after 45 seconds.\", \"status\": 429, \"headers\": { \"X-RateLimit-Limit\": \"1000\", \"X-RateLimit-Remaining\": \"0\", \"X-RateLimit-Reset\": \"1706112345\", \"Retry-After\": \"45\" } } ``` #### Rate Limit Tiers CloudFlow enforces the following rate limits per workspace: | Tier | Requests/Minute | Requests/Hour | Concurrent Workflows | |------|-----------------|---------------|----------------------| | Free | 60 | 1,000 | 5 | | Standard | 1,000 | 50,000 | 50 | | Premium | 5,000 | 250,000 | 200 | | Enterprise | Custom | Custom | Unlimited | #### Checking Rate Limit Status ```bash # Check current rate limit status curl -I https://api.cloudflow.io/api/v1/workflows \\ -H \"Authorization: Bearer $CF_ACCESS_TOKEN\" # Extract rate limit headers curl -s -I https://api.cloudflow.io/api/v1/workflows \\ -H \"Authorization: Bearer $CF_ACCESS_TOKEN\" | \\ grep -i \"x-ratelimit\" # Output: # X-RateLimit-Limit: 1000 # X-RateLimit-Remaining: 247 # X-RateLimit-Reset: 1706112400 # Monitor rate limit usage cloudflow metrics query \\ --metric rate_limit_remaining \\ --workspace ws_abc123 \\ --last 1h \\ --interval 1m ``` #### Handling Rate Limits in Code **Python example with retry logic:** ```python import time import requests def cloudflow_api_call_with_retry(url, headers, max_retries=3): for attempt in range(max_retries): response = requests.get(url, headers=headers) if response.status_code == 429: retry_after = int(response.headers.get('Retry-After', 60)) print(f\"Rate limited.",
              "score": null
            }
          ],
          "latency_ms": 13.006670000322629,
          "query_id": "realistic_018",
          "dimension": "synonym",
          "query": "What's the retry strategy for transient failures?"
        },
        {
          "key_facts": [
            "max_attempts: 3",
            "backoff_type: exponential",
            "initial_interval: 1000",
            "max retries: 3"
          ],
          "found_facts": [],
          "missed_facts": [
            "max_attempts: 3",
            "backoff_type: exponential",
            "initial_interval: 1000",
            "max retries: 3"
          ],
          "coverage": 0.0,
          "expected_docs": [
            "troubleshooting_guide",
            "user_guide"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_4",
              "content": "### Error Response Format ```json { \"error\": { \"code\": \"invalid_parameter\", \"message\": \"The 'limit' parameter must be between 1 and 100\", \"field\": \"limit\", \"request_id\": \"req_8k3m9x2p\" } } ``` ### HTTP Status Codes | Status Code | Description | Common Causes | |------------|-------------|---------------| | 400 | Bad Request | Invalid parameters, malformed JSON, validation errors | | 401 | Unauthorized | Missing or invalid authentication credentials | | 403 | Forbidden | Insufficient permissions for requested resource | | 404 | Not Found | Resource does not exist or has been deleted | | 429 | Too Many Requests | Rate limit exceeded, retry after specified period | | 500 | Internal Server Error | Unexpected server error, contact support if persists | | 502 | Bad Gateway | Temporary service issue, retry with exponential backoff | | 503 | Service Unavailable | Scheduled maintenance or temporary outage | ### Error Codes CloudFlow returns specific error codes to help you identify and resolve issues: - `invalid_parameter`: One or more request parameters are invalid - `missing_required_field`: Required field is missing from request body - `authentication_failed`: Invalid API key or token - `insufficient_permissions`: User lacks required scope or permission - `resource_not_found`: Requested resource does not exist - `rate_limit_exceeded`: Too many requests, see rate limiting section - `workflow_execution_failed`: Workflow execution encountered an error - `invalid_json`: Request body contains malformed JSON - `duplicate_resource`: Resource with same identifier already exists - `quota_exceeded`: Account quota limit reached ### Error Handling Best Practices **Python Example:** ```python import requests import time def make_api_request(url, headers, max_retries=3): for attempt in range(max_retries): try: response = requests.get(url, headers=headers) if response.status_code == 200: return response.json() elif response.status_code == 429: # Rate limit exceeded retry_after = int(response.headers.get('X-RateLimit-Reset', 60)) print(f\"Rate limited. Waiting {retry_after} seconds...\") time.sleep(retry_after) continue elif response.status_code >= 500: # Server error - retry with exponential backoff wait_time = 2 ** attempt print(f\"Server error. Retrying in {wait_time} seconds...\") time.sleep(wait_time) continue else: # Client error - don't retry error_data = response.json() raise Exception(f\"API Error: {error_data['error']['message']}\") except requests.exceptions.RequestException as e: print(f\"Request failed: {e}\") if attempt < max_retries - 1: time.sleep(2 ** attempt) else: raise raise Exception(\"Max retries exceeded\") ``` ## Webhooks CloudFlow can send webhook notifications when specific events occur in your workflows.",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_7",
              "content": "records: 500 - Session timeout: 30 seconds --- ## Message Queue Patterns ### Pub/Sub Pattern Used for broadcasting events to multiple interested consumers: ``` Workflow Engine publishes workflow.execution.completed \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u25bc \u25bc \u25bc \u25bc Analytics Notification Audit Logger Billing Service Service Service Service ``` Each service maintains its own consumer group and processes events independently. Failures in one consumer don't affect others. ### Request-Reply Pattern Synchronous communication over async messaging (used sparingly): ``` API Gateway publishes request \u2192 Kafka (reply-to: temp-queue-123) \u2502 \u25bc Service processes request \u2502 \u25bc Service publishes response \u2192 temp-queue-123 \u2502 \u25bc API Gateway receives response ``` Timeout: 5 seconds, fallback to direct HTTP call if no response. ### Saga Pattern Distributed transaction management for multi-service workflows: ``` Step 1: Create Order \u2192 SUCCESS \u2192 Step 2: Reserve Inventory \u2502 FAILURE \u2502 \u25bc Compensating Transaction \u2502 \u25bc Cancel Order (Rollback) ``` Orchestrated by Workflow Engine with compensation logic defined in workflow DSL. ### Dead Letter Queue (DLQ) Failed messages after all retry attempts are routed to DLQ: ``` Message processing fails (3 retries with exponential backoff) \u2502 \u25bc Publish to dead-letter-queue topic \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u25bc \u25bc Alert to PagerDuty Store in PostgreSQL for analysis \u2502 \u2502 \u25bc \u25bc Manual investigation Automated pattern detection ``` DLQ Processing SLA: < 4 hours for critical events, < 24 hours for non-critical. --- ## Caching Strategy ### Cache-Aside Pattern (Lazy Loading) Primary caching pattern used across all services: ``` 1. Application checks cache (Redis GET) 2. If HIT \u2192 Return cached data 3. If MISS: a. Query database (PostgreSQL) b. Store result in cache with TTL c. Return data to application ``` **Implementation Example** (Workflow Definition): ```javascript async function getWorkflowDefinition(workflowId) { const cacheKey = `workflow:def:${workflowId}`; // Step 1: Check cache let workflow = await redis.get(cacheKey); if (workflow) { metrics.increment('cache.hit.workflow_def'); return JSON.parse(workflow); } // Step 2: Cache miss - query database metrics.increment('cache.miss.workflow_def'); workflow = await db.query( 'SELECT * FROM workflows WHERE id = $1', [workflowId] ); // Step 3: Store in cache (1 hour TTL) await redis.setex(cacheKey, 3600, JSON.stringify(workflow)); return workflow; } ``` ### Write-Through Pattern Used for critical data where cache consistency is paramount: ``` 1. Application writes to database 2. Database transaction commits 3. Application updates cache 4. Return success to client ``` Applied to: User profiles, authentication sessions, system configuration.",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_9",
              "content": "in\" # Find workflow failures by ID kubectl logs -n cloudflow deployment/cloudflow-workflow-engine | \\ grep \"exec_7h3j6k9m2n\" ``` #### Rate Limiting ```bash # Find rate limit events kubectl logs -n cloudflow deployment/cloudflow-api | \\ grep -E \"429|rate.*limit|throttle\" # Count rate limit errors by hour kubectl logs -n cloudflow deployment/cloudflow-api --since=24h | \\ grep \"rate_limit_exceeded\" | \\ awk '{print $1}' | \\ cut -d'T' -f1-2 | \\ sort | uniq -c ``` ### Advanced Log Analysis #### Using jq for JSON Logs ```bash # Parse JSON logs and filter by level kubectl logs -n cloudflow deployment/cloudflow-api | \\ jq 'select(.level == \"ERROR\")' # Extract specific fields kubectl logs -n cloudflow deployment/cloudflow-api | \\ jq '{timestamp: .timestamp, level: .level, message: .message, execution_id: .context.execution_id}' # Filter by workflow ID kubectl logs -n cloudflow deployment/cloudflow-workflow-engine | \\ jq 'select(.workflow_id == \"wf_9k2n4m8p1q\")' # Count errors by type kubectl logs -n cloudflow deployment/cloudflow-api --since=1h | \\ jq -r 'select(.level == \"ERROR\") | .error_type' | \\ sort | uniq -c | sort -rn ``` #### Correlation IDs CloudFlow uses correlation IDs to trace requests across services. ```bash # Extract correlation ID from error CORRELATION_ID=\"corr_8h4j9k2m5n\" # Trace request across all services for pod in $(kubectl get pods -n cloudflow -l tier=backend -o name); do echo \"=== $pod ===\" kubectl logs -n cloudflow $pod | grep $CORRELATION_ID done # Export full trace to file cloudflow debug trace $CORRELATION_ID --output trace-$CORRELATION_ID.json ``` ### Debugging Commands #### Enable Debug Mode for Workflow Execution ```bash # Execute workflow with debug logging cloudflow workflows execute wf_9k2n4m8p1q \\ --debug \\ --log-level TRACE \\ --output-logs /tmp/workflow-debug.log # Enable step-by-step execution cloudflow workflows execute wf_9k2n4m8p1q \\ --step-mode interactive \\ --breakpoint-on-error # Capture full execution context cloudflow workflows execute wf_9k2n4m8p1q \\ --capture-context \\ --context-output /tmp/execution-context.json ``` #### Database Query Debugging ```bash # Enable query logging cloudflow db config set log_statement all cloudflow db config set log_duration on cloudflow db config set log_min_duration_statement 1000 # Log queries > 1s # Capture query plan for slow endpoint cloudflow debug capture-queries \\ --endpoint \"/api/v1/workflows/list\" \\ --duration 60 \\ --output query-analysis.txt # Analyze query performance cloudflow db analyze-performance --last 1h ``` #### Network Debugging ```bash # Test connectivity from CloudFlow pod kubectl run -n cloudflow netdebug --rm -i --tty \\ --image=nicolaka/netshoot -- /bin/bash # Inside pod: # Check DNS resolution nslookup api.cloudflow.io # Check connectivity to",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_8",
              "content": "Leverage caching:** ```bash # Enable client-side caching export CLOUDFLOW_CACHE_ENABLED=true export CLOUDFLOW_CACHE_TTL=300 # Cache workflow metadata cloudflow workflows list --use-cache --cache-ttl 600 ``` --- ## Log Analysis & Debugging ### Accessing CloudFlow Logs #### Kubernetes Deployments ```bash # List all CloudFlow pods kubectl get pods -n cloudflow # Tail logs from API server kubectl logs -f -n cloudflow deployment/cloudflow-api --tail=100 # Get logs from specific pod kubectl logs -n cloudflow cloudflow-api-7d4f6b8c9d-x7k2m # Get logs from previous crashed pod kubectl logs -n cloudflow cloudflow-api-7d4f6b8c9d-x7k2m --previous # Get logs from all pods in deployment kubectl logs -n cloudflow deployment/cloudflow-api --all-containers=true # Stream logs from multiple pods kubectl logs -n cloudflow -l app=cloudflow-api -f --max-log-requests=10 ``` #### Log Levels CloudFlow supports the following log levels: - `TRACE` - Very detailed debugging information - `DEBUG` - Detailed debugging information - `INFO` - Informational messages (default) - `WARN` - Warning messages - `ERROR` - Error messages - `FATAL` - Fatal errors causing shutdown **Changing log levels:** ```bash # Set global log level cloudflow config set logging.level DEBUG # Set log level for specific component cloudflow config set logging.components.database DEBUG cloudflow config set logging.components.auth INFO cloudflow config set logging.components.workflows TRACE # Temporary log level increase (resets after 1 hour) cloudflow debug set-log-level DEBUG --duration 3600 # View current log configuration cloudflow config get logging --format json ``` ### Grep Patterns for Common Issues #### Authentication Failures ```bash # Find all authentication errors kubectl logs -n cloudflow deployment/cloudflow-api | \\ grep -i \"authentication\\|401\\|unauthorized\" | \\ tail -n 50 # Find JWT validation failures kubectl logs -n cloudflow deployment/cloudflow-api | \\ grep -E \"JWT|token.*invalid|signature.*failed\" # Find clock skew issues kubectl logs -n cloudflow deployment/cloudflow-api | \\ grep -i \"clock skew\\|time.*sync\\|nbf\\|exp\" ``` #### Database Errors ```bash # Find database connection errors kubectl logs -n cloudflow deployment/cloudflow-api | \\ grep -E \"connection.*pool|could not connect|database.*timeout\" # Find slow queries kubectl logs -n cloudflow deployment/cloudflow-api | \\ grep \"slow query\" | \\ awk '{print $NF}' | \\ sort -n | \\ tail -n 20 # Find deadlock errors kubectl logs -n cloudflow deployment/cloudflow-api | \\ grep -i \"deadlock detected\" ``` #### Workflow Execution Errors ```bash # Find workflow timeout errors kubectl logs -n cloudflow deployment/cloudflow-workflow-engine | \\ grep -E \"timeout|exceeded.*3600\" # Find workflow retry attempts kubectl logs -n cloudflow deployment/cloudflow-workflow-engine | \\ grep -E \"retry attempt [0-9]|retrying",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_0",
              "content": "# CloudFlow Platform Troubleshooting Guide **Version:** 3.2.1 **Last Updated:** January 2026 **Audience:** Platform Engineers, DevOps, Support Teams ## Table of Contents 1. [Overview](#overview) 2. [Authentication & Authorization Issues](#authentication--authorization-issues) 3. [Performance Problems](#performance-problems) 4. [Database Connection Issues](#database-connection-issues) 5. [Workflow Execution Failures](#workflow-execution-failures) 6. [Rate Limiting & Throttling](#rate-limiting--throttling) 7. [Log Analysis & Debugging](#log-analysis--debugging) 8. [Escalation Procedures](#escalation-procedures) --- ## Overview This guide provides comprehensive troubleshooting steps for common CloudFlow platform issues encountered in production environments. Each section includes error symptoms, root cause analysis, resolution steps, and preventive measures. ### Quick Diagnostic Checklist Before diving into specific issues, perform these initial checks: - Verify service health: `cloudflow status --all` - Check API connectivity: `curl -I https://api.cloudflow.io/health` - Review recent deployments: `kubectl get deployments -n cloudflow --sort-by=.metadata.creationTimestamp` - Inspect platform metrics: `cloudflow metrics --last 1h` --- ## Authentication & Authorization Issues ### 401 Unauthorized Errors #### Symptoms - API requests return `401 Unauthorized` - Error message: `Authentication credentials were not provided or are invalid` - Frontend displays \"Session expired\" message #### Common Causes **1. Token Expiration** CloudFlow access tokens expire after 3600 seconds (1 hour) by default. Refresh tokens are valid for 30 days. **Verification:** ```bash # Decode JWT to check expiration echo $CF_ACCESS_TOKEN | cut -d'.' -f2 | base64 -d | jq '.exp' # Compare with current time date +%s ``` **Resolution:** ```bash # Refresh the access token curl -X POST https://api.cloudflow.io/auth/refresh \\ -H \"Content-Type: application/json\" \\ -d '{\"refresh_token\": \"'$CF_REFRESH_TOKEN'\"}' # Update environment variable export CF_ACCESS_TOKEN=\"<new_token>\" ``` **2. Invalid JWT Signature** Error: `JWT signature verification failed` **Causes:** - Token was modified or corrupted - Using wrong signing key - Token generated with different secret **Resolution:** ```bash # Validate token structure cloudflow auth validate --token $CF_ACCESS_TOKEN # Generate new token cloudflow auth login --username <user> --password <pass> # For service accounts cloudflow auth service-account --create --name <sa-name> --scopes \"workflow:read,workflow:write\" ``` **3. Clock Skew Issues** JWT validation fails when system clocks are out of sync (tolerance: \u00b1300 seconds).",
              "score": null
            }
          ],
          "latency_ms": 12.984768000023905,
          "query_id": "realistic_018",
          "dimension": "problem",
          "query": "My workflow fails on temporary network errors"
        },
        {
          "key_facts": [
            "max_attempts: 3",
            "backoff_type: exponential",
            "initial_interval: 1000",
            "max retries: 3"
          ],
          "found_facts": [
            "max_attempts: 3",
            "initial_interval: 1000",
            "max retries: 3"
          ],
          "missed_facts": [
            "backoff_type: exponential"
          ],
          "coverage": 0.75,
          "expected_docs": [
            "troubleshooting_guide",
            "user_guide"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "user_guide",
              "chunk_id": "user_guide_fix_5",
              "content": "Save and activate **Testing Schedules:** Use the built-in schedule calculator to preview upcoming executions: ``` Next 5 executions: 1. 2026-01-24 14:00:00 UTC 2. 2026-01-25 14:00:00 UTC 3. 2026-01-26 14:00:00 UTC 4. 2026-01-27 14:00:00 UTC 5. 2026-01-28 14:00:00 UTC ``` ## Error Handling Robust error handling ensures your workflows are resilient and reliable. ### Retry Policies Configure automatic retries for failed actions: ```yaml - id: api_call action: http_request config: url: \"https://api.example.com/data\" retry: max_attempts: 3 backoff_type: \"exponential\" # or \"fixed\", \"linear\" initial_interval: 1000 # milliseconds max_interval: 30000 multiplier: 2.0 retry_on: - timeout - network_error - status: [500, 502, 503, 504] ``` **Backoff Strategies:** - **Fixed**: Wait the same amount of time between retries - Attempt 1: Wait 1s - Attempt 2: Wait 1s - Attempt 3: Wait 1s - **Linear**: Increase wait time by a fixed amount - Attempt 1: Wait 1s - Attempt 2: Wait 2s - Attempt 3: Wait 3s - **Exponential**: Double the wait time with each retry (recommended) - Attempt 1: Wait 1s - Attempt 2: Wait 2s - Attempt 3: Wait 4s **Retry Conditions:** Control which errors trigger retries: - `timeout`: Request timeout - `network_error`: Connection failures - `status`: Specific HTTP status codes - `error_code`: Application-specific error codes ### Fallback Actions Execute alternative actions when the primary action fails: ```yaml - id: primary_payment action: http_request config: url: \"https://primary-payment-gateway.com/charge\" method: POST body: amount: \"{{amount}}\" on_error: - id: fallback_payment action: http_request config: url: \"https://backup-payment-gateway.com/charge\" method: POST body: amount: \"{{amount}}\" - id: notify_admin action: email config: to: \"admin@company.com\" subject: \"Payment Gateway Failure\" body: \"Primary gateway failed, switched to backup\" ``` **Error Object:** Access error details in fallback actions: ``` {{error.message}} # Error message {{error.code}} # Error code {{error.step_id}} # Failed step ID {{error.timestamp}} # When the error occurred {{error.attempts}} # Number of retry attempts ``` ### Dead Letter Queue When all retries and fallbacks fail, CloudFlow can route failed executions to a Dead Letter Queue (DLQ) for manual review: **Enable DLQ:** ```yaml workflow: error_handling: dead_letter_queue: enabled: true retain_days: 30 ``` **DLQ Features:** - View failed executions in the dashboard - Inspect complete execution context and error details - Retry individual executions after fixing issues - Export failed executions for analysis - Set up alerts for DLQ threshold breaches **Accessing the DLQ:** 1. Navigate to **Workflows** > **[Your Workflow]** > **Dead Letter Queue** 2.",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_5",
              "content": "exec_7h3j6k9m2n --show-bottlenecks ``` #### Solutions **1. Increase workflow timeout (if justified):** ```bash # Update workflow configuration cloudflow workflows update wf_9k2n4m8p1q \\ --timeout 7200 \\ --reason \"Large dataset processing requires extended time\" # Verify update cloudflow workflows get wf_9k2n4m8p1q | grep timeout ``` **2. Optimize slow steps:** ```bash # Enable parallel processing cloudflow workflows update wf_9k2n4m8p1q \\ --step data_transformation \\ --parallel-workers 8 \\ --batch-size 1000 # Add checkpointing for long operations cloudflow workflows update wf_9k2n4m8p1q \\ --step data_transformation \\ --enable-checkpointing \\ --checkpoint-interval 300 ``` **3. Split workflow into smaller workflows:** ```bash # Create sub-workflows cloudflow workflows create data-pipeline-part1 \\ --steps \"data_ingestion,data_validation\" \\ --timeout 1800 cloudflow workflows create data-pipeline-part2 \\ --steps \"data_transformation,data_export\" \\ --timeout 3600 \\ --trigger workflow_completed \\ --trigger-workflow data-pipeline-part1 ``` ### Retry Logic and Exponential Backoff CloudFlow implements automatic retry with exponential backoff for transient failures: - Max retries: 3 - Initial delay: 1 second - Backoff multiplier: 2 - Max delay: 60 seconds #### Retry Sequence ``` Attempt 1: Immediate Attempt 2: Wait 1s (2^0 * 1s) Attempt 3: Wait 2s (2^1 * 1s) Attempt 4: Wait 4s (2^2 * 1s) ``` #### Configuration ```bash # View current retry settings cloudflow workflows get wf_9k2n4m8p1q --format json | jq '.retry_policy' # Customize retry behavior cloudflow workflows update wf_9k2n4m8p1q \\ --retry-max-attempts 5 \\ --retry-initial-delay 2000 \\ --retry-backoff-multiplier 2 \\ --retry-max-delay 120000 \\ --retry-on-errors \"NETWORK_ERROR,TIMEOUT,RATE_LIMIT\" # Disable retry for specific step cloudflow workflows update wf_9k2n4m8p1q \\ --step data_export \\ --retry-enabled false ``` #### Monitoring Retries ```bash # List failed executions with retry information cloudflow workflows executions list \\ --status FAILED \\ --show-retries \\ --last 7d # Get retry history for specific execution cloudflow workflows executions retries exec_7h3j6k9m2n # Output: # Execution: exec_7h3j6k9m2n # Attempt 1: FAILED - NetworkError: Connection refused (delay: 0ms) # Attempt 2: FAILED - NetworkError: Connection timeout (delay: 1000ms) # Attempt 3: FAILED - NetworkError: Connection timeout (delay: 2000ms) # Attempt 4: FAILED - NetworkError: Connection timeout (delay: 4000ms) # Final Status: FAILED_AFTER_RETRIES ``` ### Workflow Step Failures #### Common Error Patterns **1.",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_9",
              "content": "in\" # Find workflow failures by ID kubectl logs -n cloudflow deployment/cloudflow-workflow-engine | \\ grep \"exec_7h3j6k9m2n\" ``` #### Rate Limiting ```bash # Find rate limit events kubectl logs -n cloudflow deployment/cloudflow-api | \\ grep -E \"429|rate.*limit|throttle\" # Count rate limit errors by hour kubectl logs -n cloudflow deployment/cloudflow-api --since=24h | \\ grep \"rate_limit_exceeded\" | \\ awk '{print $1}' | \\ cut -d'T' -f1-2 | \\ sort | uniq -c ``` ### Advanced Log Analysis #### Using jq for JSON Logs ```bash # Parse JSON logs and filter by level kubectl logs -n cloudflow deployment/cloudflow-api | \\ jq 'select(.level == \"ERROR\")' # Extract specific fields kubectl logs -n cloudflow deployment/cloudflow-api | \\ jq '{timestamp: .timestamp, level: .level, message: .message, execution_id: .context.execution_id}' # Filter by workflow ID kubectl logs -n cloudflow deployment/cloudflow-workflow-engine | \\ jq 'select(.workflow_id == \"wf_9k2n4m8p1q\")' # Count errors by type kubectl logs -n cloudflow deployment/cloudflow-api --since=1h | \\ jq -r 'select(.level == \"ERROR\") | .error_type' | \\ sort | uniq -c | sort -rn ``` #### Correlation IDs CloudFlow uses correlation IDs to trace requests across services. ```bash # Extract correlation ID from error CORRELATION_ID=\"corr_8h4j9k2m5n\" # Trace request across all services for pod in $(kubectl get pods -n cloudflow -l tier=backend -o name); do echo \"=== $pod ===\" kubectl logs -n cloudflow $pod | grep $CORRELATION_ID done # Export full trace to file cloudflow debug trace $CORRELATION_ID --output trace-$CORRELATION_ID.json ``` ### Debugging Commands #### Enable Debug Mode for Workflow Execution ```bash # Execute workflow with debug logging cloudflow workflows execute wf_9k2n4m8p1q \\ --debug \\ --log-level TRACE \\ --output-logs /tmp/workflow-debug.log # Enable step-by-step execution cloudflow workflows execute wf_9k2n4m8p1q \\ --step-mode interactive \\ --breakpoint-on-error # Capture full execution context cloudflow workflows execute wf_9k2n4m8p1q \\ --capture-context \\ --context-output /tmp/execution-context.json ``` #### Database Query Debugging ```bash # Enable query logging cloudflow db config set log_statement all cloudflow db config set log_duration on cloudflow db config set log_min_duration_statement 1000 # Log queries > 1s # Capture query plan for slow endpoint cloudflow debug capture-queries \\ --endpoint \"/api/v1/workflows/list\" \\ --duration 60 \\ --output query-analysis.txt # Analyze query performance cloudflow db analyze-performance --last 1h ``` #### Network Debugging ```bash # Test connectivity from CloudFlow pod kubectl run -n cloudflow netdebug --rm -i --tty \\ --image=nicolaka/netshoot -- /bin/bash # Inside pod: # Check DNS resolution nslookup api.cloudflow.io # Check connectivity to",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_8",
              "content": "Leverage caching:** ```bash # Enable client-side caching export CLOUDFLOW_CACHE_ENABLED=true export CLOUDFLOW_CACHE_TTL=300 # Cache workflow metadata cloudflow workflows list --use-cache --cache-ttl 600 ``` --- ## Log Analysis & Debugging ### Accessing CloudFlow Logs #### Kubernetes Deployments ```bash # List all CloudFlow pods kubectl get pods -n cloudflow # Tail logs from API server kubectl logs -f -n cloudflow deployment/cloudflow-api --tail=100 # Get logs from specific pod kubectl logs -n cloudflow cloudflow-api-7d4f6b8c9d-x7k2m # Get logs from previous crashed pod kubectl logs -n cloudflow cloudflow-api-7d4f6b8c9d-x7k2m --previous # Get logs from all pods in deployment kubectl logs -n cloudflow deployment/cloudflow-api --all-containers=true # Stream logs from multiple pods kubectl logs -n cloudflow -l app=cloudflow-api -f --max-log-requests=10 ``` #### Log Levels CloudFlow supports the following log levels: - `TRACE` - Very detailed debugging information - `DEBUG` - Detailed debugging information - `INFO` - Informational messages (default) - `WARN` - Warning messages - `ERROR` - Error messages - `FATAL` - Fatal errors causing shutdown **Changing log levels:** ```bash # Set global log level cloudflow config set logging.level DEBUG # Set log level for specific component cloudflow config set logging.components.database DEBUG cloudflow config set logging.components.auth INFO cloudflow config set logging.components.workflows TRACE # Temporary log level increase (resets after 1 hour) cloudflow debug set-log-level DEBUG --duration 3600 # View current log configuration cloudflow config get logging --format json ``` ### Grep Patterns for Common Issues #### Authentication Failures ```bash # Find all authentication errors kubectl logs -n cloudflow deployment/cloudflow-api | \\ grep -i \"authentication\\|401\\|unauthorized\" | \\ tail -n 50 # Find JWT validation failures kubectl logs -n cloudflow deployment/cloudflow-api | \\ grep -E \"JWT|token.*invalid|signature.*failed\" # Find clock skew issues kubectl logs -n cloudflow deployment/cloudflow-api | \\ grep -i \"clock skew\\|time.*sync\\|nbf\\|exp\" ``` #### Database Errors ```bash # Find database connection errors kubectl logs -n cloudflow deployment/cloudflow-api | \\ grep -E \"connection.*pool|could not connect|database.*timeout\" # Find slow queries kubectl logs -n cloudflow deployment/cloudflow-api | \\ grep \"slow query\" | \\ awk '{print $NF}' | \\ sort -n | \\ tail -n 20 # Find deadlock errors kubectl logs -n cloudflow deployment/cloudflow-api | \\ grep -i \"deadlock detected\" ``` #### Workflow Execution Errors ```bash # Find workflow timeout errors kubectl logs -n cloudflow deployment/cloudflow-workflow-engine | \\ grep -E \"timeout|exceeded.*3600\" # Find workflow retry attempts kubectl logs -n cloudflow deployment/cloudflow-workflow-engine | \\ grep -E \"retry attempt [0-9]|retrying",
              "score": null
            },
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_4",
              "content": "### Error Response Format ```json { \"error\": { \"code\": \"invalid_parameter\", \"message\": \"The 'limit' parameter must be between 1 and 100\", \"field\": \"limit\", \"request_id\": \"req_8k3m9x2p\" } } ``` ### HTTP Status Codes | Status Code | Description | Common Causes | |------------|-------------|---------------| | 400 | Bad Request | Invalid parameters, malformed JSON, validation errors | | 401 | Unauthorized | Missing or invalid authentication credentials | | 403 | Forbidden | Insufficient permissions for requested resource | | 404 | Not Found | Resource does not exist or has been deleted | | 429 | Too Many Requests | Rate limit exceeded, retry after specified period | | 500 | Internal Server Error | Unexpected server error, contact support if persists | | 502 | Bad Gateway | Temporary service issue, retry with exponential backoff | | 503 | Service Unavailable | Scheduled maintenance or temporary outage | ### Error Codes CloudFlow returns specific error codes to help you identify and resolve issues: - `invalid_parameter`: One or more request parameters are invalid - `missing_required_field`: Required field is missing from request body - `authentication_failed`: Invalid API key or token - `insufficient_permissions`: User lacks required scope or permission - `resource_not_found`: Requested resource does not exist - `rate_limit_exceeded`: Too many requests, see rate limiting section - `workflow_execution_failed`: Workflow execution encountered an error - `invalid_json`: Request body contains malformed JSON - `duplicate_resource`: Resource with same identifier already exists - `quota_exceeded`: Account quota limit reached ### Error Handling Best Practices **Python Example:** ```python import requests import time def make_api_request(url, headers, max_retries=3): for attempt in range(max_retries): try: response = requests.get(url, headers=headers) if response.status_code == 200: return response.json() elif response.status_code == 429: # Rate limit exceeded retry_after = int(response.headers.get('X-RateLimit-Reset', 60)) print(f\"Rate limited. Waiting {retry_after} seconds...\") time.sleep(retry_after) continue elif response.status_code >= 500: # Server error - retry with exponential backoff wait_time = 2 ** attempt print(f\"Server error. Retrying in {wait_time} seconds...\") time.sleep(wait_time) continue else: # Client error - don't retry error_data = response.json() raise Exception(f\"API Error: {error_data['error']['message']}\") except requests.exceptions.RequestException as e: print(f\"Request failed: {e}\") if attempt < max_retries - 1: time.sleep(2 ** attempt) else: raise raise Exception(\"Max retries exceeded\") ``` ## Webhooks CloudFlow can send webhook notifications when specific events occur in your workflows.",
              "score": null
            }
          ],
          "latency_ms": 12.720406000880757,
          "query_id": "realistic_018",
          "dimension": "casual",
          "query": "retry config"
        },
        {
          "key_facts": [
            "max_attempts: 3",
            "backoff_type: exponential",
            "initial_interval: 1000",
            "max retries: 3"
          ],
          "found_facts": [
            "max_attempts: 3",
            "initial_interval: 1000",
            "max retries: 3"
          ],
          "missed_facts": [
            "backoff_type: exponential"
          ],
          "coverage": 0.75,
          "expected_docs": [
            "troubleshooting_guide",
            "user_guide"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "user_guide",
              "chunk_id": "user_guide_fix_6",
              "content": "Filter by error type, date range, or execution ID 3. Click an execution to view full details 4. Click **\"Retry\"** to reprocess with the same input data ### Error Notifications Get notified when workflows fail: ```yaml workflow: notifications: on_failure: - type: email to: \"ops-team@company.com\" - type: slack channel: \"#alerts\" message: \"Workflow {{workflow.name}} failed: {{error.message}}\" on_success_after_retry: - type: slack channel: \"#monitoring\" message: \"Workflow recovered after {{error.attempts}} attempts\" ``` ## Workflow Limits CloudFlow enforces the following limits to ensure platform stability and performance: ### Steps Per Workflow - **Maximum**: 50 steps per workflow - **Recommendation**: Keep workflows focused and modular. If you need more steps, consider splitting into multiple workflows connected via webhooks. ### Execution Timeout - **Default**: 3600 seconds (60 minutes) - **Behavior**: Workflows exceeding this timeout are automatically terminated - **Custom Timeouts**: Enterprise plans can request custom timeout limits **Setting Step-Level Timeouts:** ```yaml - id: long_running_task action: http_request config: url: \"https://api.example.com/process\" timeout: 300 # 5 minutes for this specific step ``` ### Execution Limits - **Maximum**: 1000 executions per day (per workflow) - **Rate Limiting**: 100 concurrent executions per workflow - **Burst Limit**: 10 executions per second **What happens when limits are reached:** - New executions are queued automatically - Webhook triggers return HTTP 429 (Too Many Requests) - Scheduled executions are skipped (logged in audit trail) - Email notifications sent to workflow owner **Monitoring Usage:** View real-time metrics in your workflow dashboard: - Executions today: 847 / 1000 - Average duration: 12.3s - Success rate: 98.2% - Current queue depth: 3 ### Data Limits - **Maximum request/response size**: 10MB per action - **Maximum execution payload**: 50MB total - **Variable value size**: 1MB per variable ### Enterprise Plan Limits Enterprise customers can request increased limits: - Up to 100 steps per workflow - Up to 10,000 executions per day - Up to 7200 second timeout (2 hours) - Priority execution queue - Dedicated capacity allocation Contact sales@cloudflow.io for Enterprise pricing and custom limits. ## Best Practices Follow these best practices to build reliable, maintainable workflows: ### 1. Use Descriptive Names **Good:** - Workflow: \"Sync Customer Data from Salesforce to Database\" - Step: \"validate_customer_email\" **Bad:** - Workflow: \"Workflow 1\" - Step: \"step3\" ### 2.",
              "score": null
            },
            {
              "doc_id": "user_guide",
              "chunk_id": "user_guide_fix_5",
              "content": "Save and activate **Testing Schedules:** Use the built-in schedule calculator to preview upcoming executions: ``` Next 5 executions: 1. 2026-01-24 14:00:00 UTC 2. 2026-01-25 14:00:00 UTC 3. 2026-01-26 14:00:00 UTC 4. 2026-01-27 14:00:00 UTC 5. 2026-01-28 14:00:00 UTC ``` ## Error Handling Robust error handling ensures your workflows are resilient and reliable. ### Retry Policies Configure automatic retries for failed actions: ```yaml - id: api_call action: http_request config: url: \"https://api.example.com/data\" retry: max_attempts: 3 backoff_type: \"exponential\" # or \"fixed\", \"linear\" initial_interval: 1000 # milliseconds max_interval: 30000 multiplier: 2.0 retry_on: - timeout - network_error - status: [500, 502, 503, 504] ``` **Backoff Strategies:** - **Fixed**: Wait the same amount of time between retries - Attempt 1: Wait 1s - Attempt 2: Wait 1s - Attempt 3: Wait 1s - **Linear**: Increase wait time by a fixed amount - Attempt 1: Wait 1s - Attempt 2: Wait 2s - Attempt 3: Wait 3s - **Exponential**: Double the wait time with each retry (recommended) - Attempt 1: Wait 1s - Attempt 2: Wait 2s - Attempt 3: Wait 4s **Retry Conditions:** Control which errors trigger retries: - `timeout`: Request timeout - `network_error`: Connection failures - `status`: Specific HTTP status codes - `error_code`: Application-specific error codes ### Fallback Actions Execute alternative actions when the primary action fails: ```yaml - id: primary_payment action: http_request config: url: \"https://primary-payment-gateway.com/charge\" method: POST body: amount: \"{{amount}}\" on_error: - id: fallback_payment action: http_request config: url: \"https://backup-payment-gateway.com/charge\" method: POST body: amount: \"{{amount}}\" - id: notify_admin action: email config: to: \"admin@company.com\" subject: \"Payment Gateway Failure\" body: \"Primary gateway failed, switched to backup\" ``` **Error Object:** Access error details in fallback actions: ``` {{error.message}} # Error message {{error.code}} # Error code {{error.step_id}} # Failed step ID {{error.timestamp}} # When the error occurred {{error.attempts}} # Number of retry attempts ``` ### Dead Letter Queue When all retries and fallbacks fail, CloudFlow can route failed executions to a Dead Letter Queue (DLQ) for manual review: **Enable DLQ:** ```yaml workflow: error_handling: dead_letter_queue: enabled: true retain_days: 30 ``` **DLQ Features:** - View failed executions in the dashboard - Inspect complete execution context and error details - Retry individual executions after fixing issues - Export failed executions for analysis - Set up alerts for DLQ threshold breaches **Accessing the DLQ:** 1. Navigate to **Workflows** > **[Your Workflow]** > **Dead Letter Queue** 2.",
              "score": null
            },
            {
              "doc_id": "user_guide",
              "chunk_id": "user_guide_fix_0",
              "content": "# CloudFlow User Guide Welcome to CloudFlow, the modern workflow automation platform that helps you connect your apps, automate repetitive tasks, and build powerful integrations without writing code. ## Table of Contents 1. [Getting Started](#getting-started) 2. [Workflow Creation](#workflow-creation) 3. [Available Actions](#available-actions) 4. [Variables and Expressions](#variables-and-expressions) 5. [Scheduling](#scheduling) 6. [Error Handling](#error-handling) 7. [Workflow Limits](#workflow-limits) 8. [Best Practices](#best-practices) 9. [Common Workflow Patterns](#common-workflow-patterns) ## Getting Started ### Account Setup Creating your CloudFlow account takes just a few minutes: 1. Navigate to https://app.cloudflow.io/signup 2. Enter your email address and create a strong password 3. Verify your email address by clicking the link sent to your inbox 4. Complete your profile by providing your organization name and use case 5. Choose your plan (Free, Professional, or Enterprise) Once your account is created, you'll be redirected to your CloudFlow dashboard where you can start building workflows immediately. ### Your First Workflow Let's create a simple workflow that sends you a Slack notification when a new file is uploaded to your Google Drive: 1. Click the **\"Create Workflow\"** button in the top-right corner 2. Give your workflow a name: \"Google Drive to Slack Notifier\" 3. Select **\"Google Drive - New File\"** as your trigger 4. Authenticate your Google Drive account when prompted 5. Choose the folder you want to monitor 6. Click **\"Add Action\"** and select **\"Slack - Send Message\"** 7. Authenticate your Slack account 8. Select the channel where you want to receive notifications 9. Customize your message using variables: `New file uploaded: {{trigger.file.name}}` 10. Click **\"Save & Activate\"** to enable your workflow Congratulations! Your first workflow is now live. Every time a file is added to the specified Google Drive folder, you'll receive a Slack notification. ## Workflow Creation CloudFlow offers multiple ways to create and manage workflows, giving you the flexibility to work in the way that suits you best. ### Visual Editor The Visual Editor is CloudFlow's drag-and-drop interface for building workflows without code. It's perfect for users who prefer a graphical approach: **Key Features:** - Drag-and-drop action blocks from the sidebar - Visual connections between steps show your workflow logic - Inline configuration for each action - Real-time validation and error checking - Test mode to run your workflow with sample data **Using the Visual Editor:** 1.",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_5",
              "content": "exec_7h3j6k9m2n --show-bottlenecks ``` #### Solutions **1. Increase workflow timeout (if justified):** ```bash # Update workflow configuration cloudflow workflows update wf_9k2n4m8p1q \\ --timeout 7200 \\ --reason \"Large dataset processing requires extended time\" # Verify update cloudflow workflows get wf_9k2n4m8p1q | grep timeout ``` **2. Optimize slow steps:** ```bash # Enable parallel processing cloudflow workflows update wf_9k2n4m8p1q \\ --step data_transformation \\ --parallel-workers 8 \\ --batch-size 1000 # Add checkpointing for long operations cloudflow workflows update wf_9k2n4m8p1q \\ --step data_transformation \\ --enable-checkpointing \\ --checkpoint-interval 300 ``` **3. Split workflow into smaller workflows:** ```bash # Create sub-workflows cloudflow workflows create data-pipeline-part1 \\ --steps \"data_ingestion,data_validation\" \\ --timeout 1800 cloudflow workflows create data-pipeline-part2 \\ --steps \"data_transformation,data_export\" \\ --timeout 3600 \\ --trigger workflow_completed \\ --trigger-workflow data-pipeline-part1 ``` ### Retry Logic and Exponential Backoff CloudFlow implements automatic retry with exponential backoff for transient failures: - Max retries: 3 - Initial delay: 1 second - Backoff multiplier: 2 - Max delay: 60 seconds #### Retry Sequence ``` Attempt 1: Immediate Attempt 2: Wait 1s (2^0 * 1s) Attempt 3: Wait 2s (2^1 * 1s) Attempt 4: Wait 4s (2^2 * 1s) ``` #### Configuration ```bash # View current retry settings cloudflow workflows get wf_9k2n4m8p1q --format json | jq '.retry_policy' # Customize retry behavior cloudflow workflows update wf_9k2n4m8p1q \\ --retry-max-attempts 5 \\ --retry-initial-delay 2000 \\ --retry-backoff-multiplier 2 \\ --retry-max-delay 120000 \\ --retry-on-errors \"NETWORK_ERROR,TIMEOUT,RATE_LIMIT\" # Disable retry for specific step cloudflow workflows update wf_9k2n4m8p1q \\ --step data_export \\ --retry-enabled false ``` #### Monitoring Retries ```bash # List failed executions with retry information cloudflow workflows executions list \\ --status FAILED \\ --show-retries \\ --last 7d # Get retry history for specific execution cloudflow workflows executions retries exec_7h3j6k9m2n # Output: # Execution: exec_7h3j6k9m2n # Attempt 1: FAILED - NetworkError: Connection refused (delay: 0ms) # Attempt 2: FAILED - NetworkError: Connection timeout (delay: 1000ms) # Attempt 3: FAILED - NetworkError: Connection timeout (delay: 2000ms) # Attempt 4: FAILED - NetworkError: Connection timeout (delay: 4000ms) # Final Status: FAILED_AFTER_RETRIES ``` ### Workflow Step Failures #### Common Error Patterns **1.",
              "score": null
            },
            {
              "doc_id": "user_guide",
              "chunk_id": "user_guide_fix_8",
              "content": "Version Control For critical workflows: - Export YAML definitions regularly - Store in version control (Git) - Use pull requests for changes - Tag releases with semantic versioning ## Common Workflow Patterns Here are proven patterns for common automation scenarios: ### Pattern 1: Form to Database Capture form submissions and store in database: ```yaml name: \"Contact Form to Database\" trigger: type: webhook method: POST steps: - id: validate_submission action: javascript code: | if (!input.email || !input.message) { throw new Error(\"Email and message required\"); } return input; - id: check_duplicate action: database_query config: connection: \"{{secrets.DB_CONNECTION}}\" query: \"SELECT id FROM contacts WHERE email = $1 AND created_at > NOW() - INTERVAL '1 hour'\" parameters: - \"{{trigger.body.email}}\" - id: insert_contact action: database_query condition: \"{{length(steps.check_duplicate.rows) == 0}}\" config: connection: \"{{secrets.DB_CONNECTION}}\" query: | INSERT INTO contacts (name, email, message, source, created_at) VALUES ($1, $2, $3, $4, NOW()) parameters: - \"{{trigger.body.name}}\" - \"{{trigger.body.email}}\" - \"{{trigger.body.message}}\" - \"website_form\" - id: notify_sales action: slack_message condition: \"{{steps.insert_contact.affected_rows > 0}}\" config: channel: \"#leads\" text: \"New contact form submission from {{trigger.body.email}}\" ``` ### Pattern 2: API Polling Periodically check an API and take action on new items: ```yaml name: \"Poll GitHub Issues\" schedule: cron: \"*/15 * * * *\" # Every 15 minutes timezone: \"UTC\" steps: - id: get_last_check action: database_query config: query: \"SELECT last_checked_at FROM workflow_state WHERE workflow_id = $1\" parameters: - \"{{workflow.id}}\" - id: fetch_issues action: http_request config: method: GET url: \"https://api.github.com/repos/company/project/issues\" params: since: \"{{steps.get_last_check.rows[0].last_checked_at}}\" state: \"open\" headers: Authorization: \"token {{secrets.GITHUB_TOKEN}}\" - id: process_new_issues action: javascript code: | const issues = input.body; return { count: issues.length, issues: issues.map(i => ({ number: i.number, title: i.title, url: i.html_url })) }; - id: notify_team action: slack_message condition: \"{{steps.process_new_issues.output.count > 0}}\" config: channel: \"#engineering\" text: \"{{steps.process_new_issues.output.count}} new GitHub issues\" - id: update_last_check action: database_query config: query: | UPDATE workflow_state SET last_checked_at = $1 WHERE workflow_id = $2 parameters: - \"{{now()}}\" - \"{{workflow.id}}\" ``` ### Pattern 3: Multi-Step Approval Implement approval workflows with timeouts: ```yaml name: \"Expense Approval Workflow\" trigger: type: webhook method: POST steps: - id: create_approval_request action: database_query config: query: | INSERT INTO approvals (expense_id, amount, requester, status, created_at) VALUES ($1, $2, $3, 'pending', NOW()) RETURNING id parameters: - \"{{trigger.body.expense_id}}\" - \"{{trigger.body.amount}}\" - \"{{trigger.body.requester}}\" - id: notify_manager action: email config: to: \"{{trigger.body.manager_email}}\" subject: \"Expense Approval Required: ${{trigger.body.amount}}\" body: | An expense requires your approval: Amount: ${{trigger.body.amount}}",
              "score": null
            }
          ],
          "latency_ms": 13.142372998117935,
          "query_id": "realistic_018",
          "dimension": "contextual",
          "query": "I want workflows to automatically retry on errors, what are the options?"
        },
        {
          "key_facts": [
            "max_attempts: 3",
            "backoff_type: exponential",
            "initial_interval: 1000",
            "max retries: 3"
          ],
          "found_facts": [
            "max_attempts: 3",
            "initial_interval: 1000",
            "max retries: 3"
          ],
          "missed_facts": [
            "backoff_type: exponential"
          ],
          "coverage": 0.75,
          "expected_docs": [
            "troubleshooting_guide",
            "user_guide"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "user_guide",
              "chunk_id": "user_guide_fix_5",
              "content": "Save and activate **Testing Schedules:** Use the built-in schedule calculator to preview upcoming executions: ``` Next 5 executions: 1. 2026-01-24 14:00:00 UTC 2. 2026-01-25 14:00:00 UTC 3. 2026-01-26 14:00:00 UTC 4. 2026-01-27 14:00:00 UTC 5. 2026-01-28 14:00:00 UTC ``` ## Error Handling Robust error handling ensures your workflows are resilient and reliable. ### Retry Policies Configure automatic retries for failed actions: ```yaml - id: api_call action: http_request config: url: \"https://api.example.com/data\" retry: max_attempts: 3 backoff_type: \"exponential\" # or \"fixed\", \"linear\" initial_interval: 1000 # milliseconds max_interval: 30000 multiplier: 2.0 retry_on: - timeout - network_error - status: [500, 502, 503, 504] ``` **Backoff Strategies:** - **Fixed**: Wait the same amount of time between retries - Attempt 1: Wait 1s - Attempt 2: Wait 1s - Attempt 3: Wait 1s - **Linear**: Increase wait time by a fixed amount - Attempt 1: Wait 1s - Attempt 2: Wait 2s - Attempt 3: Wait 3s - **Exponential**: Double the wait time with each retry (recommended) - Attempt 1: Wait 1s - Attempt 2: Wait 2s - Attempt 3: Wait 4s **Retry Conditions:** Control which errors trigger retries: - `timeout`: Request timeout - `network_error`: Connection failures - `status`: Specific HTTP status codes - `error_code`: Application-specific error codes ### Fallback Actions Execute alternative actions when the primary action fails: ```yaml - id: primary_payment action: http_request config: url: \"https://primary-payment-gateway.com/charge\" method: POST body: amount: \"{{amount}}\" on_error: - id: fallback_payment action: http_request config: url: \"https://backup-payment-gateway.com/charge\" method: POST body: amount: \"{{amount}}\" - id: notify_admin action: email config: to: \"admin@company.com\" subject: \"Payment Gateway Failure\" body: \"Primary gateway failed, switched to backup\" ``` **Error Object:** Access error details in fallback actions: ``` {{error.message}} # Error message {{error.code}} # Error code {{error.step_id}} # Failed step ID {{error.timestamp}} # When the error occurred {{error.attempts}} # Number of retry attempts ``` ### Dead Letter Queue When all retries and fallbacks fail, CloudFlow can route failed executions to a Dead Letter Queue (DLQ) for manual review: **Enable DLQ:** ```yaml workflow: error_handling: dead_letter_queue: enabled: true retain_days: 30 ``` **DLQ Features:** - View failed executions in the dashboard - Inspect complete execution context and error details - Retry individual executions after fixing issues - Export failed executions for analysis - Set up alerts for DLQ threshold breaches **Accessing the DLQ:** 1. Navigate to **Workflows** > **[Your Workflow]** > **Dead Letter Queue** 2.",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_5",
              "content": "exec_7h3j6k9m2n --show-bottlenecks ``` #### Solutions **1. Increase workflow timeout (if justified):** ```bash # Update workflow configuration cloudflow workflows update wf_9k2n4m8p1q \\ --timeout 7200 \\ --reason \"Large dataset processing requires extended time\" # Verify update cloudflow workflows get wf_9k2n4m8p1q | grep timeout ``` **2. Optimize slow steps:** ```bash # Enable parallel processing cloudflow workflows update wf_9k2n4m8p1q \\ --step data_transformation \\ --parallel-workers 8 \\ --batch-size 1000 # Add checkpointing for long operations cloudflow workflows update wf_9k2n4m8p1q \\ --step data_transformation \\ --enable-checkpointing \\ --checkpoint-interval 300 ``` **3. Split workflow into smaller workflows:** ```bash # Create sub-workflows cloudflow workflows create data-pipeline-part1 \\ --steps \"data_ingestion,data_validation\" \\ --timeout 1800 cloudflow workflows create data-pipeline-part2 \\ --steps \"data_transformation,data_export\" \\ --timeout 3600 \\ --trigger workflow_completed \\ --trigger-workflow data-pipeline-part1 ``` ### Retry Logic and Exponential Backoff CloudFlow implements automatic retry with exponential backoff for transient failures: - Max retries: 3 - Initial delay: 1 second - Backoff multiplier: 2 - Max delay: 60 seconds #### Retry Sequence ``` Attempt 1: Immediate Attempt 2: Wait 1s (2^0 * 1s) Attempt 3: Wait 2s (2^1 * 1s) Attempt 4: Wait 4s (2^2 * 1s) ``` #### Configuration ```bash # View current retry settings cloudflow workflows get wf_9k2n4m8p1q --format json | jq '.retry_policy' # Customize retry behavior cloudflow workflows update wf_9k2n4m8p1q \\ --retry-max-attempts 5 \\ --retry-initial-delay 2000 \\ --retry-backoff-multiplier 2 \\ --retry-max-delay 120000 \\ --retry-on-errors \"NETWORK_ERROR,TIMEOUT,RATE_LIMIT\" # Disable retry for specific step cloudflow workflows update wf_9k2n4m8p1q \\ --step data_export \\ --retry-enabled false ``` #### Monitoring Retries ```bash # List failed executions with retry information cloudflow workflows executions list \\ --status FAILED \\ --show-retries \\ --last 7d # Get retry history for specific execution cloudflow workflows executions retries exec_7h3j6k9m2n # Output: # Execution: exec_7h3j6k9m2n # Attempt 1: FAILED - NetworkError: Connection refused (delay: 0ms) # Attempt 2: FAILED - NetworkError: Connection timeout (delay: 1000ms) # Attempt 3: FAILED - NetworkError: Connection timeout (delay: 2000ms) # Attempt 4: FAILED - NetworkError: Connection timeout (delay: 4000ms) # Final Status: FAILED_AFTER_RETRIES ``` ### Workflow Step Failures #### Common Error Patterns **1.",
              "score": null
            },
            {
              "doc_id": "user_guide",
              "chunk_id": "user_guide_fix_6",
              "content": "Filter by error type, date range, or execution ID 3. Click an execution to view full details 4. Click **\"Retry\"** to reprocess with the same input data ### Error Notifications Get notified when workflows fail: ```yaml workflow: notifications: on_failure: - type: email to: \"ops-team@company.com\" - type: slack channel: \"#alerts\" message: \"Workflow {{workflow.name}} failed: {{error.message}}\" on_success_after_retry: - type: slack channel: \"#monitoring\" message: \"Workflow recovered after {{error.attempts}} attempts\" ``` ## Workflow Limits CloudFlow enforces the following limits to ensure platform stability and performance: ### Steps Per Workflow - **Maximum**: 50 steps per workflow - **Recommendation**: Keep workflows focused and modular. If you need more steps, consider splitting into multiple workflows connected via webhooks. ### Execution Timeout - **Default**: 3600 seconds (60 minutes) - **Behavior**: Workflows exceeding this timeout are automatically terminated - **Custom Timeouts**: Enterprise plans can request custom timeout limits **Setting Step-Level Timeouts:** ```yaml - id: long_running_task action: http_request config: url: \"https://api.example.com/process\" timeout: 300 # 5 minutes for this specific step ``` ### Execution Limits - **Maximum**: 1000 executions per day (per workflow) - **Rate Limiting**: 100 concurrent executions per workflow - **Burst Limit**: 10 executions per second **What happens when limits are reached:** - New executions are queued automatically - Webhook triggers return HTTP 429 (Too Many Requests) - Scheduled executions are skipped (logged in audit trail) - Email notifications sent to workflow owner **Monitoring Usage:** View real-time metrics in your workflow dashboard: - Executions today: 847 / 1000 - Average duration: 12.3s - Success rate: 98.2% - Current queue depth: 3 ### Data Limits - **Maximum request/response size**: 10MB per action - **Maximum execution payload**: 50MB total - **Variable value size**: 1MB per variable ### Enterprise Plan Limits Enterprise customers can request increased limits: - Up to 100 steps per workflow - Up to 10,000 executions per day - Up to 7200 second timeout (2 hours) - Priority execution queue - Dedicated capacity allocation Contact sales@cloudflow.io for Enterprise pricing and custom limits. ## Best Practices Follow these best practices to build reliable, maintainable workflows: ### 1. Use Descriptive Names **Good:** - Workflow: \"Sync Customer Data from Salesforce to Database\" - Step: \"validate_customer_email\" **Bad:** - Workflow: \"Workflow 1\" - Step: \"step3\" ### 2.",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_11",
              "content": "outage\" # Create war room cloudflow incident war-room create incident-2024012401 ``` **For SEV-2 (High):** ```bash # Create incident and notify cloudflow incident create \\ --severity SEV-2 \\ --title \"High rate of workflow failures\" \\ --description \"Workflow success rate dropped to 75%\" \\ --notify-oncall # Update status page cloudflow status update \\ --status \"partial_outage\" \\ --message \"We are experiencing elevated error rates\" ``` **For SEV-3/SEV-4:** ```bash # Create support ticket cloudflow support ticket create \\ --priority MEDIUM \\ --subject \"Intermittent timeout errors\" \\ --description \"See attached logs and reproduction steps\" ``` ### Contact Information - **On-call hotline:** +1-888-CLOUDFLOW (24/7) - **Support email:** support@cloudflow.io - **Incident Slack channel:** #cloudflow-incidents - **Status page:** https://status.cloudflow.io - **Documentation:** https://docs.cloudflow.io ### Post-Incident Review After resolving incidents, complete a postmortem: ```bash # Generate postmortem template cloudflow incident postmortem incident-2024012401 \\ --template standard \\ --output postmortem-2024012401.md # Required sections: # 1. Incident summary # 2. Impact assessment # 3. Timeline of events # 4. Root cause analysis # 5. Resolution steps # 6. Action items # 7. Lessons learned ``` --- ## Additional Resources - **CloudFlow Documentation:** https://docs.cloudflow.io - **API Reference:** https://api-docs.cloudflow.io - **Community Forum:** https://community.cloudflow.io - **GitHub Issues:** https://github.com/cloudflow/platform/issues - **Training Portal:** https://training.cloudflow.io - **Monitoring Dashboard:** https://monitoring.cloudflow.io ### Getting Help If this troubleshooting guide doesn't resolve your issue: 1. Search the knowledge base: `cloudflow kb search \"your issue\"` 2. Check community forum for similar issues 3. Contact support with detailed logs and reproduction steps 4. For urgent issues, use emergency escalation procedures **Remember:** Always capture logs, metrics, and reproduction steps before escalating! --- *Last updated: January 24, 2026* *Document version: 3.2.1* *Feedback: docs-feedback@cloudflow.io*",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_7",
              "content": "records: 500 - Session timeout: 30 seconds --- ## Message Queue Patterns ### Pub/Sub Pattern Used for broadcasting events to multiple interested consumers: ``` Workflow Engine publishes workflow.execution.completed \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u25bc \u25bc \u25bc \u25bc Analytics Notification Audit Logger Billing Service Service Service Service ``` Each service maintains its own consumer group and processes events independently. Failures in one consumer don't affect others. ### Request-Reply Pattern Synchronous communication over async messaging (used sparingly): ``` API Gateway publishes request \u2192 Kafka (reply-to: temp-queue-123) \u2502 \u25bc Service processes request \u2502 \u25bc Service publishes response \u2192 temp-queue-123 \u2502 \u25bc API Gateway receives response ``` Timeout: 5 seconds, fallback to direct HTTP call if no response. ### Saga Pattern Distributed transaction management for multi-service workflows: ``` Step 1: Create Order \u2192 SUCCESS \u2192 Step 2: Reserve Inventory \u2502 FAILURE \u2502 \u25bc Compensating Transaction \u2502 \u25bc Cancel Order (Rollback) ``` Orchestrated by Workflow Engine with compensation logic defined in workflow DSL. ### Dead Letter Queue (DLQ) Failed messages after all retry attempts are routed to DLQ: ``` Message processing fails (3 retries with exponential backoff) \u2502 \u25bc Publish to dead-letter-queue topic \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u25bc \u25bc Alert to PagerDuty Store in PostgreSQL for analysis \u2502 \u2502 \u25bc \u25bc Manual investigation Automated pattern detection ``` DLQ Processing SLA: < 4 hours for critical events, < 24 hours for non-critical. --- ## Caching Strategy ### Cache-Aside Pattern (Lazy Loading) Primary caching pattern used across all services: ``` 1. Application checks cache (Redis GET) 2. If HIT \u2192 Return cached data 3. If MISS: a. Query database (PostgreSQL) b. Store result in cache with TTL c. Return data to application ``` **Implementation Example** (Workflow Definition): ```javascript async function getWorkflowDefinition(workflowId) { const cacheKey = `workflow:def:${workflowId}`; // Step 1: Check cache let workflow = await redis.get(cacheKey); if (workflow) { metrics.increment('cache.hit.workflow_def'); return JSON.parse(workflow); } // Step 2: Cache miss - query database metrics.increment('cache.miss.workflow_def'); workflow = await db.query( 'SELECT * FROM workflows WHERE id = $1', [workflowId] ); // Step 3: Store in cache (1 hour TTL) await redis.setex(cacheKey, 3600, JSON.stringify(workflow)); return workflow; } ``` ### Write-Through Pattern Used for critical data where cache consistency is paramount: ``` 1. Application writes to database 2. Database transaction commits 3. Application updates cache 4. Return success to client ``` Applied to: User profiles, authentication sessions, system configuration.",
              "score": null
            }
          ],
          "latency_ms": 12.939904998347629,
          "query_id": "realistic_018",
          "dimension": "negation",
          "query": "Why doesn't my workflow retry after failing?"
        },
        {
          "key_facts": [
            "helm repo add cloudflow https://charts.cloudflow.io"
          ],
          "found_facts": [
            "helm repo add cloudflow https://charts.cloudflow.io"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "deployment_guide"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_3",
              "content": "Deploy PostgreSQL using the Bitnami Helm chart: ```yaml # postgres-values.yaml global: postgresql: auth: username: cloudflow database: cloudflow existingSecret: postgres-credentials image: tag: \"14.10.0\" primary: resources: limits: cpu: 4000m memory: 8Gi requests: cpu: 2000m memory: 4Gi persistence: enabled: true size: 100Gi storageClass: gp3 extendedConfiguration: | max_connections = 100 shared_buffers = 2GB effective_cache_size = 6GB maintenance_work_mem = 512MB checkpoint_completion_target = 0.9 wal_buffers = 16MB default_statistics_target = 100 random_page_cost = 1.1 effective_io_concurrency = 200 work_mem = 10MB min_wal_size = 1GB max_wal_size = 4GB readReplicas: replicaCount: 2 resources: limits: cpu: 2000m memory: 4Gi requests: cpu: 1000m memory: 2Gi metrics: enabled: true serviceMonitor: enabled: true ``` Install PostgreSQL: ```bash # Create password secret kubectl create secret generic postgres-credentials \\ --namespace cloudflow-prod \\ --from-literal=postgres-password=\"$(openssl rand -base64 32)\" \\ --from-literal=password=\"$(openssl rand -base64 32)\" # Install PostgreSQL helm install postgresql bitnami/postgresql \\ --namespace cloudflow-prod \\ --values postgres-values.yaml \\ --wait ``` ### PgBouncer Configuration Deploy PgBouncer for connection pooling to handle up to 100 connections efficiently: ```yaml # pgbouncer.yaml apiVersion: v1 kind: ConfigMap metadata: name: pgbouncer-config namespace: cloudflow-prod data: pgbouncer.ini: | [databases] cloudflow = host=postgresql.cloudflow-prod.svc.cluster.local port=5432 dbname=cloudflow [pgbouncer] listen_addr = 0.0.0.0 listen_port = 5432 auth_type = md5 auth_file = /etc/pgbouncer/userlist.txt pool_mode = transaction max_client_conn = 1000 default_pool_size = 25 reserve_pool_size = 5 reserve_pool_timeout = 3 max_db_connections = 100 max_user_connections = 100 server_lifetime = 3600 server_idle_timeout = 600 log_connections = 1 log_disconnections = 1 log_pooler_errors = 1 --- apiVersion: apps/v1 kind: Deployment metadata: name: pgbouncer namespace: cloudflow-prod spec: replicas: 2 selector: matchLabels: app: pgbouncer template: metadata: labels: app: pgbouncer spec: containers: - name: pgbouncer image: pgbouncer/pgbouncer:1.21.0 ports: - containerPort: 5432 resources: requests: cpu: 500m memory: 512Mi limits: cpu: 1000m memory: 1Gi volumeMounts: - name: config mountPath: /etc/pgbouncer volumes: - name: config configMap: name: pgbouncer-config --- apiVersion: v1 kind: Service metadata: name: pgbouncer namespace: cloudflow-prod spec: selector: app: pgbouncer ports: - port: 5432 targetPort: 5432 type: ClusterIP ``` Apply the PgBouncer configuration: ```bash kubectl apply -f pgbouncer.yaml ``` ### Database Migrations Run database migrations before deploying new versions: ```bash # Create migration job kubectl create job cloudflow-migrate-$(date +%s) \\ --namespace cloudflow-prod \\ --image=123456789012.dkr.ecr.us-east-1.amazonaws.com/cloudflow:2.4.0 \\ -- npm run migrate # Monitor migration progress kubectl logs -f job/cloudflow-migrate-<timestamp> -n cloudflow-prod ``` --- ## Monitoring and Observability ### Prometheus Setup Install Prometheus using the kube-prometheus-stack: ```bash helm install prometheus prometheus-community/kube-prometheus-stack \\ --namespace monitoring \\ --create-namespace \\ --set prometheus.prometheusSpec.serviceMonitorSelectorNilUsesHelmValues=false \\",
              "score": null
            },
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_5",
              "content": "Configure log aggregation: ```bash # Install Fluent Bit for log forwarding helm install fluent-bit fluent/fluent-bit \\ --namespace logging \\ --create-namespace \\ --set cloudWatch.enabled=true \\ --set cloudWatch.region=us-east-1 \\ --set cloudWatch.logGroupName=/aws/eks/cloudflow-production/application ``` Query logs using CloudWatch Insights: ```sql fields @timestamp, @message, level, requestId, userId | filter namespace = \"cloudflow-prod\" | filter level = \"error\" | sort @timestamp desc | limit 100 ``` --- ## Backup and Disaster Recovery ### Database Backup Strategy CloudFlow implements a comprehensive backup strategy with the following retention policy: - **Daily snapshots**: Retained for 30 days - **Weekly backups**: Retained for 90 days - **Monthly backups**: Retained for 1 year #### Automated Backup with Velero Install Velero for cluster-wide backups: ```bash velero install \\ --provider aws \\ --plugins velero/velero-plugin-for-aws:v1.8.0 \\ --bucket cloudflow-velero-backups \\ --backup-location-config region=us-east-1 \\ --snapshot-location-config region=us-east-1 \\ --secret-file ./credentials-velero ``` Create a daily backup schedule: ```yaml # backup-schedule.yaml apiVersion: velero.io/v1 kind: Schedule metadata: name: cloudflow-daily-backup namespace: velero spec: schedule: \"0 2 * * *\" # 2 AM UTC daily template: includedNamespaces: - cloudflow-prod ttl: 720h0m0s # 30 days storageLocation: default snapshotVolumes: true ``` Apply the backup schedule: ```bash kubectl apply -f backup-schedule.yaml ``` #### PostgreSQL Backup Configure PostgreSQL continuous archiving with WAL-G: ```bash # Create backup cronjob kubectl apply -f - <<EOF apiVersion: batch/v1 kind: CronJob metadata: name: postgres-backup namespace: cloudflow-prod spec: schedule: \"0 1 * * *\" # 1 AM UTC daily jobTemplate: spec: template: spec: containers: - name: backup image: postgres:14 env: - name: PGPASSWORD valueFrom: secretKeyRef: name: postgres-credentials key: password command: - /bin/bash - -c - | TIMESTAMP=\\$(date +%Y%m%d_%H%M%S) pg_dump -h postgresql.cloudflow-prod.svc.cluster.local \\ -U cloudflow -d cloudflow \\ | gzip > /backups/cloudflow_\\${TIMESTAMP}.sql.gz aws s3 cp /backups/cloudflow_\\${TIMESTAMP}.sql.gz \\ s3://cloudflow-db-backups/daily/ volumeMounts: - name: backup-storage mountPath: /backups volumes: - name: backup-storage emptyDir: {} restartPolicy: OnFailure EOF ``` ### Disaster Recovery Procedure In case of catastrophic failure, follow these steps to restore CloudFlow: 1. **Restore EKS Cluster** (if necessary): ```bash eksctl create cluster -f cluster-config.yaml ``` 2. **Restore Velero Backup**: ```bash # List available backups velero backup get # Restore from backup velero restore create --from-backup cloudflow-daily-backup-20260124 ``` 3. **Restore Database**: ```bash # Download backup from S3 aws s3 cp s3://cloudflow-db-backups/daily/cloudflow_20260124_010000.sql.gz . # Restore database gunzip -c cloudflow_20260124_010000.sql.gz | \\ psql -h postgresql.cloudflow-prod.svc.cluster.local -U cloudflow -d cloudflow ``` 4.",
              "score": null
            },
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_8",
              "content": "UTC - **Secondary**: Wednesday 10:00-12:00 UTC All deployments should target these windows to minimize user impact. --- **Document Version**: 2.4.0 **Maintained by**: Platform Engineering Team **Last Review**: January 24, 2026",
              "score": null
            },
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_1",
              "content": "create cluster -f cluster-config.yaml ``` ### Storage Configuration Install the EBS CSI driver for persistent volumes: ```bash eksctl create addon --name aws-ebs-csi-driver \\ --cluster cloudflow-production \\ --service-account-role-arn arn:aws:iam::ACCOUNT_ID:role/AmazonEKS_EBS_CSI_DriverRole \\ --force ``` --- ## Kubernetes Deployment ### Namespace Setup Create the CloudFlow namespace and configure resource quotas: ```yaml # namespace.yaml apiVersion: v1 kind: Namespace metadata: name: cloudflow-prod labels: name: cloudflow-prod environment: production --- apiVersion: v1 kind: ResourceQuota metadata: name: cloudflow-quota namespace: cloudflow-prod spec: hard: requests.cpu: \"50\" requests.memory: 100Gi persistentvolumeclaims: \"10\" services.loadbalancers: \"2\" ``` Apply the namespace configuration: ```bash kubectl apply -f namespace.yaml ``` ### Environment Configuration CloudFlow requires the following environment variables: | Variable | Description | Example | Required | |----------|-------------|---------|----------| | `DATABASE_URL` | PostgreSQL connection string | `postgresql://user:pass@host:5432/cloudflow` | Yes | | `REDIS_URL` | Redis connection string | `redis://redis-master.cloudflow-prod.svc.cluster.local:6379` | Yes | | `JWT_SECRET` | Secret key for JWT token signing | `<generated-secret-256-bit>` | Yes | | `LOG_LEVEL` | Application log verbosity | `info`, `debug`, `warn`, `error` | No (default: `info`) | | `API_PORT` | API server port | `3000` | No (default: `3000`) | | `WORKER_CONCURRENCY` | Number of concurrent workers | `10` | No (default: `5`) | | `SESSION_SECRET` | Session encryption key | `<generated-secret-256-bit>` | Yes | | `AWS_REGION` | AWS region for services | `us-east-1` | Yes | | `S3_BUCKET` | S3 bucket for file storage | `cloudflow-prod-storage` | Yes | | `SMTP_HOST` | Email server hostname | `smtp.sendgrid.net` | No | | `SMTP_PORT` | Email server port | `587` | No | | `METRICS_ENABLED` | Enable Prometheus metrics | `true` | No (default: `false`) | Create a Kubernetes secret for sensitive variables: ```bash kubectl create secret generic cloudflow-secrets \\ --namespace cloudflow-prod \\ --from-literal=DATABASE_URL=\"postgresql://cloudflow:$(cat db-password.txt)@pgbouncer.cloudflow-prod.svc.cluster.local:5432/cloudflow\" \\ --from-literal=REDIS_URL=\"redis://redis-master.cloudflow-prod.svc.cluster.local:6379\" \\ --from-literal=JWT_SECRET=\"$(openssl rand -base64 32)\" \\ --from-literal=SESSION_SECRET=\"$(openssl rand -base64 32)\" ``` ### Helm Chart Deployment Add the CloudFlow Helm repository: ```bash helm repo add cloudflow https://charts.cloudflow.io helm repo update ``` Create a values file for production configuration: ```yaml # values-production.yaml replicaCount: 3 image: repository: 123456789012.dkr.ecr.us-east-1.amazonaws.com/cloudflow tag: \"2.4.0\" pullPolicy: IfNotPresent resources: limits: cpu: 2000m memory: 4Gi requests: cpu: 2000m memory: 4Gi autoscaling: enabled: true minReplicas: 3 maxReplicas: 10 targetCPUUtilizationPercentage: 70 targetMemoryUtilizationPercentage: 80 service: type: ClusterIP port: 80 targetPort: 3000 annotations: service.beta.kubernetes.io/aws-load-balancer-type: \"nlb\" ingress: enabled: true className: nginx annotations: cert-manager.io/cluster-issuer: \"letsencrypt-prod\" nginx.ingress.kubernetes.io/ssl-redirect: \"true\" hosts: - host: api.cloudflow.io paths: - path: / pathType: Prefix tls:",
              "score": null
            },
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_0",
              "content": "# CloudFlow Platform - Deployment and Operations Guide **Version:** 2.4.0 **Last Updated:** January 2026 **Target Environment:** Production (AWS EKS) ## Table of Contents 1. [Overview](#overview) 2. [Prerequisites](#prerequisites) 3. [Infrastructure Setup](#infrastructure-setup) 4. [Kubernetes Deployment](#kubernetes-deployment) 5. [Database Configuration](#database-configuration) 6. [Monitoring and Observability](#monitoring-and-observability) 7. [Backup and Disaster Recovery](#backup-and-disaster-recovery) 8. [Scaling and Performance](#scaling-and-performance) 9. [Security and Compliance](#security-and-compliance) 10. [Troubleshooting](#troubleshooting) --- ## Overview CloudFlow is a cloud-native workflow orchestration platform designed for high-availability production environments. This guide provides comprehensive instructions for deploying and operating CloudFlow on Amazon EKS (Elastic Kubernetes Service). ### Architecture Summary CloudFlow consists of the following components: - **API Server**: REST API for workflow management (Node.js/Express) - **Worker Service**: Background job processor (Node.js) - **Scheduler**: Cron-based task scheduler (Node.js) - **PostgreSQL**: Primary data store (version 14) - **Redis**: Cache and message queue (version 7.0) - **PgBouncer**: Database connection pooler ### Deployment Model - **Namespace**: `cloudflow-prod` - **Cluster**: AWS EKS 1.28 - **Region**: us-east-1 (primary), us-west-2 (disaster recovery) - **High Availability**: Multi-AZ deployment across 3 availability zones --- ## Prerequisites Before beginning the deployment process, ensure you have the following: ### Required Tools - `kubectl` (v1.28 or later) - `helm` (v3.12 or later) - `aws-cli` (v2.13 or later) - `eksctl` (v0.165 or later) - `terraform` (v1.6 or later) - for infrastructure provisioning ### Access Requirements - AWS account with appropriate IAM permissions - EKS cluster admin access - Container registry access (ECR) - Domain name and SSL certificates - Secrets management access (AWS Secrets Manager or Vault) ### Network Requirements - VPC with at least 3 public and 3 private subnets - NAT Gateway configured - Security groups allowing: - Ingress: HTTPS (443), HTTP (80) - Internal: PostgreSQL (5432), Redis (6379) - Monitoring: Prometheus (9090), Grafana (3000) --- ## Infrastructure Setup ### EKS Cluster Creation Use the following `eksctl` configuration to create the EKS cluster: ```yaml # cluster-config.yaml apiVersion: eksctl.io/v1alpha5 kind: ClusterConfig metadata: name: cloudflow-production region: us-east-1 version: \"1.28\" availabilityZones: - us-east-1a - us-east-1b - us-east-1c vpc: cidr: 10.0.0.0/16 nat: gateway: HighlyAvailable managedNodeGroups: - name: cloudflow-workers instanceType: m5.xlarge minSize: 3 maxSize: 10 desiredCapacity: 5 volumeSize: 100 privateNetworking: true labels: role: worker environment: production tags: Environment: production Application: cloudflow iam: withAddonPolicies: ebs: true efs: true albIngress: true cloudWatch: true cloudWatch: clusterLogging: enableTypes: - api - audit - authenticator - controllerManager - scheduler ``` Create the cluster: ```bash eksctl",
              "score": null
            }
          ],
          "latency_ms": 12.978406997717684,
          "query_id": "realistic_019",
          "dimension": "original",
          "query": "What Helm chart repository should I use for deployment?"
        },
        {
          "key_facts": [
            "helm repo add cloudflow https://charts.cloudflow.io"
          ],
          "found_facts": [
            "helm repo add cloudflow https://charts.cloudflow.io"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "deployment_guide"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_3",
              "content": "Deploy PostgreSQL using the Bitnami Helm chart: ```yaml # postgres-values.yaml global: postgresql: auth: username: cloudflow database: cloudflow existingSecret: postgres-credentials image: tag: \"14.10.0\" primary: resources: limits: cpu: 4000m memory: 8Gi requests: cpu: 2000m memory: 4Gi persistence: enabled: true size: 100Gi storageClass: gp3 extendedConfiguration: | max_connections = 100 shared_buffers = 2GB effective_cache_size = 6GB maintenance_work_mem = 512MB checkpoint_completion_target = 0.9 wal_buffers = 16MB default_statistics_target = 100 random_page_cost = 1.1 effective_io_concurrency = 200 work_mem = 10MB min_wal_size = 1GB max_wal_size = 4GB readReplicas: replicaCount: 2 resources: limits: cpu: 2000m memory: 4Gi requests: cpu: 1000m memory: 2Gi metrics: enabled: true serviceMonitor: enabled: true ``` Install PostgreSQL: ```bash # Create password secret kubectl create secret generic postgres-credentials \\ --namespace cloudflow-prod \\ --from-literal=postgres-password=\"$(openssl rand -base64 32)\" \\ --from-literal=password=\"$(openssl rand -base64 32)\" # Install PostgreSQL helm install postgresql bitnami/postgresql \\ --namespace cloudflow-prod \\ --values postgres-values.yaml \\ --wait ``` ### PgBouncer Configuration Deploy PgBouncer for connection pooling to handle up to 100 connections efficiently: ```yaml # pgbouncer.yaml apiVersion: v1 kind: ConfigMap metadata: name: pgbouncer-config namespace: cloudflow-prod data: pgbouncer.ini: | [databases] cloudflow = host=postgresql.cloudflow-prod.svc.cluster.local port=5432 dbname=cloudflow [pgbouncer] listen_addr = 0.0.0.0 listen_port = 5432 auth_type = md5 auth_file = /etc/pgbouncer/userlist.txt pool_mode = transaction max_client_conn = 1000 default_pool_size = 25 reserve_pool_size = 5 reserve_pool_timeout = 3 max_db_connections = 100 max_user_connections = 100 server_lifetime = 3600 server_idle_timeout = 600 log_connections = 1 log_disconnections = 1 log_pooler_errors = 1 --- apiVersion: apps/v1 kind: Deployment metadata: name: pgbouncer namespace: cloudflow-prod spec: replicas: 2 selector: matchLabels: app: pgbouncer template: metadata: labels: app: pgbouncer spec: containers: - name: pgbouncer image: pgbouncer/pgbouncer:1.21.0 ports: - containerPort: 5432 resources: requests: cpu: 500m memory: 512Mi limits: cpu: 1000m memory: 1Gi volumeMounts: - name: config mountPath: /etc/pgbouncer volumes: - name: config configMap: name: pgbouncer-config --- apiVersion: v1 kind: Service metadata: name: pgbouncer namespace: cloudflow-prod spec: selector: app: pgbouncer ports: - port: 5432 targetPort: 5432 type: ClusterIP ``` Apply the PgBouncer configuration: ```bash kubectl apply -f pgbouncer.yaml ``` ### Database Migrations Run database migrations before deploying new versions: ```bash # Create migration job kubectl create job cloudflow-migrate-$(date +%s) \\ --namespace cloudflow-prod \\ --image=123456789012.dkr.ecr.us-east-1.amazonaws.com/cloudflow:2.4.0 \\ -- npm run migrate # Monitor migration progress kubectl logs -f job/cloudflow-migrate-<timestamp> -n cloudflow-prod ``` --- ## Monitoring and Observability ### Prometheus Setup Install Prometheus using the kube-prometheus-stack: ```bash helm install prometheus prometheus-community/kube-prometheus-stack \\ --namespace monitoring \\ --create-namespace \\ --set prometheus.prometheusSpec.serviceMonitorSelectorNilUsesHelmValues=false \\",
              "score": null
            },
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_2",
              "content": "- secretName: cloudflow-tls hosts: - api.cloudflow.io healthCheck: enabled: true livenessProbe: path: /health initialDelaySeconds: 30 periodSeconds: 10 timeoutSeconds: 5 failureThreshold: 3 readinessProbe: path: /ready initialDelaySeconds: 10 periodSeconds: 5 timeoutSeconds: 3 failureThreshold: 3 env: - name: NODE_ENV value: \"production\" - name: LOG_LEVEL value: \"info\" - name: API_PORT value: \"3000\" - name: WORKER_CONCURRENCY value: \"10\" - name: AWS_REGION value: \"us-east-1\" - name: METRICS_ENABLED value: \"true\" envFrom: - secretRef: name: cloudflow-secrets podAnnotations: prometheus.io/scrape: \"true\" prometheus.io/port: \"9090\" prometheus.io/path: \"/metrics\" podSecurityContext: runAsNonRoot: true runAsUser: 1000 fsGroup: 1000 securityContext: allowPrivilegeEscalation: false capabilities: drop: - ALL readOnlyRootFilesystem: true persistence: enabled: true storageClass: gp3 accessMode: ReadWriteOnce size: 50Gi redis: enabled: true architecture: replication auth: enabled: true master: persistence: size: 20Gi replica: replicaCount: 2 persistence: size: 20Gi ``` Deploy CloudFlow using Helm: ```bash helm install cloudflow cloudflow/cloudflow \\ --namespace cloudflow-prod \\ --values values-production.yaml \\ --wait \\ --timeout 10m ``` ### Deployment Verification Verify the deployment status: ```bash # Check pod status kubectl get pods -n cloudflow-prod # Expected output: # NAME READY STATUS RESTARTS AGE # cloudflow-api-7d8f9c5b6d-4xk2p 1/1 Running 0 2m # cloudflow-api-7d8f9c5b6d-9hj5m 1/1 Running 0 2m # cloudflow-api-7d8f9c5b6d-tn8wq 1/1 Running 0 2m # Check service endpoints kubectl get svc -n cloudflow-prod # Check ingress kubectl get ingress -n cloudflow-prod ``` Test health endpoints: ```bash # Port forward for testing kubectl port-forward -n cloudflow-prod svc/cloudflow-api 8080:80 # Test health endpoint curl http://localhost:8080/health # Expected: {\"status\":\"healthy\",\"timestamp\":\"2026-01-24T10:30:00Z\"} # Test readiness endpoint curl http://localhost:8080/ready # Expected: {\"status\":\"ready\",\"dependencies\":{\"database\":\"connected\",\"redis\":\"connected\"}} ``` --- ## Database Configuration ### PostgreSQL Setup CloudFlow uses PostgreSQL 14 as its primary data store.",
              "score": null
            },
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_1",
              "content": "create cluster -f cluster-config.yaml ``` ### Storage Configuration Install the EBS CSI driver for persistent volumes: ```bash eksctl create addon --name aws-ebs-csi-driver \\ --cluster cloudflow-production \\ --service-account-role-arn arn:aws:iam::ACCOUNT_ID:role/AmazonEKS_EBS_CSI_DriverRole \\ --force ``` --- ## Kubernetes Deployment ### Namespace Setup Create the CloudFlow namespace and configure resource quotas: ```yaml # namespace.yaml apiVersion: v1 kind: Namespace metadata: name: cloudflow-prod labels: name: cloudflow-prod environment: production --- apiVersion: v1 kind: ResourceQuota metadata: name: cloudflow-quota namespace: cloudflow-prod spec: hard: requests.cpu: \"50\" requests.memory: 100Gi persistentvolumeclaims: \"10\" services.loadbalancers: \"2\" ``` Apply the namespace configuration: ```bash kubectl apply -f namespace.yaml ``` ### Environment Configuration CloudFlow requires the following environment variables: | Variable | Description | Example | Required | |----------|-------------|---------|----------| | `DATABASE_URL` | PostgreSQL connection string | `postgresql://user:pass@host:5432/cloudflow` | Yes | | `REDIS_URL` | Redis connection string | `redis://redis-master.cloudflow-prod.svc.cluster.local:6379` | Yes | | `JWT_SECRET` | Secret key for JWT token signing | `<generated-secret-256-bit>` | Yes | | `LOG_LEVEL` | Application log verbosity | `info`, `debug`, `warn`, `error` | No (default: `info`) | | `API_PORT` | API server port | `3000` | No (default: `3000`) | | `WORKER_CONCURRENCY` | Number of concurrent workers | `10` | No (default: `5`) | | `SESSION_SECRET` | Session encryption key | `<generated-secret-256-bit>` | Yes | | `AWS_REGION` | AWS region for services | `us-east-1` | Yes | | `S3_BUCKET` | S3 bucket for file storage | `cloudflow-prod-storage` | Yes | | `SMTP_HOST` | Email server hostname | `smtp.sendgrid.net` | No | | `SMTP_PORT` | Email server port | `587` | No | | `METRICS_ENABLED` | Enable Prometheus metrics | `true` | No (default: `false`) | Create a Kubernetes secret for sensitive variables: ```bash kubectl create secret generic cloudflow-secrets \\ --namespace cloudflow-prod \\ --from-literal=DATABASE_URL=\"postgresql://cloudflow:$(cat db-password.txt)@pgbouncer.cloudflow-prod.svc.cluster.local:5432/cloudflow\" \\ --from-literal=REDIS_URL=\"redis://redis-master.cloudflow-prod.svc.cluster.local:6379\" \\ --from-literal=JWT_SECRET=\"$(openssl rand -base64 32)\" \\ --from-literal=SESSION_SECRET=\"$(openssl rand -base64 32)\" ``` ### Helm Chart Deployment Add the CloudFlow Helm repository: ```bash helm repo add cloudflow https://charts.cloudflow.io helm repo update ``` Create a values file for production configuration: ```yaml # values-production.yaml replicaCount: 3 image: repository: 123456789012.dkr.ecr.us-east-1.amazonaws.com/cloudflow tag: \"2.4.0\" pullPolicy: IfNotPresent resources: limits: cpu: 2000m memory: 4Gi requests: cpu: 2000m memory: 4Gi autoscaling: enabled: true minReplicas: 3 maxReplicas: 10 targetCPUUtilizationPercentage: 70 targetMemoryUtilizationPercentage: 80 service: type: ClusterIP port: 80 targetPort: 3000 annotations: service.beta.kubernetes.io/aws-load-balancer-type: \"nlb\" ingress: enabled: true className: nginx annotations: cert-manager.io/cluster-issuer: \"letsencrypt-prod\" nginx.ingress.kubernetes.io/ssl-redirect: \"true\" hosts: - host: api.cloudflow.io paths: - path: / pathType: Prefix tls:",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_0",
              "content": "# CloudFlow Platform - System Architecture Overview **Document Version:** 2.3.1 **Last Updated:** January 15, 2026 **Owner:** Platform Architecture Team **Status:** Production ## Executive Summary CloudFlow is a distributed, cloud-native workflow automation platform designed to orchestrate complex business processes at scale. The platform processes over 2.5 million workflow executions daily with an average P99 latency of 180ms for API operations and 4.2 seconds for workflow execution. This document provides a comprehensive overview of the system architecture, including microservices design, data flow patterns, infrastructure decisions, and operational considerations. ## Table of Contents 1. [High-Level Architecture](#high-level-architecture) 2. [Microservices Breakdown](#microservices-breakdown) 3. [Data Flow Architecture](#data-flow-architecture) 4. [Database Architecture](#database-architecture) 5. [Message Queue Patterns](#message-queue-patterns) 6. [Caching Strategy](#caching-strategy) 7. [Security Architecture](#security-architecture) 8. [Performance Characteristics](#performance-characteristics) 9. [Disaster Recovery](#disaster-recovery) --- ## High-Level Architecture CloudFlow follows a microservices architecture pattern deployed across multiple availability zones in AWS. The platform is designed with the following core principles: - **Scalability**: Horizontal scaling for all services with auto-scaling groups - **Resilience**: Circuit breakers, bulkheads, and graceful degradation - **Observability**: Distributed tracing, centralized logging, and comprehensive metrics - **Security**: Zero-trust network architecture with mTLS encryption ### Architecture Diagram (Conceptual) ``` \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 Load Balancer (ALB) \u2502 \u2502 (TLS Termination - 443) \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u25bc \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 API Gateway Layer \u2502 \u2502 (Rate Limiting, Auth, Request Routing) \u2502 \u2514\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u2502 \u2502 \u2502 \u2502 \u25bc \u25bc \u25bc \u25bc \u25bc \u250c\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502Auth\u2502 \u2502Workflow\u2502 \u2502Scheduler\u2502 \u2502Notification\u2502 \u2502User Service\u2502 \u2502Svc \u2502 \u2502 Engine \u2502 \u2502 Service \u2502 \u2502 Service \u2502 \u2502 \u2502 \u2514\u2500\u252c\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u2502 \u2502 \u2502 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 \u2502 \u2502 \u25bc \u25bc \u25bc \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502PostgreSQL\u2502 \u2502 Redis \u2502 \u2502 Kafka \u2502 \u2502 Cluster \u2502 \u2502 Cluster \u2502 \u2502 Cluster \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 ``` ### Technology Stack - **Runtime**: Node.js 20.x (API Gateway, Workflow Engine), Go 1.21 (Auth Service, Scheduler) - **Container Orchestration**: Kubernetes 1.28 (EKS) - **Service Mesh**: Istio 1.20 for service-to-service communication - **Databases**: PostgreSQL 15.4, Redis 7.2 - **Message Broker**: Apache Kafka 3.6 - **Monitoring**: Prometheus, Grafana, Jaeger for distributed tracing - **Secrets Management**: HashiCorp Vault 1.15 --- ## Microservices Breakdown ### API Gateway **Purpose**: Single entry point for all client requests, providing authentication, rate limiting, request routing, and protocol translation.",
              "score": null
            },
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_0",
              "content": "# CloudFlow Platform - Deployment and Operations Guide **Version:** 2.4.0 **Last Updated:** January 2026 **Target Environment:** Production (AWS EKS) ## Table of Contents 1. [Overview](#overview) 2. [Prerequisites](#prerequisites) 3. [Infrastructure Setup](#infrastructure-setup) 4. [Kubernetes Deployment](#kubernetes-deployment) 5. [Database Configuration](#database-configuration) 6. [Monitoring and Observability](#monitoring-and-observability) 7. [Backup and Disaster Recovery](#backup-and-disaster-recovery) 8. [Scaling and Performance](#scaling-and-performance) 9. [Security and Compliance](#security-and-compliance) 10. [Troubleshooting](#troubleshooting) --- ## Overview CloudFlow is a cloud-native workflow orchestration platform designed for high-availability production environments. This guide provides comprehensive instructions for deploying and operating CloudFlow on Amazon EKS (Elastic Kubernetes Service). ### Architecture Summary CloudFlow consists of the following components: - **API Server**: REST API for workflow management (Node.js/Express) - **Worker Service**: Background job processor (Node.js) - **Scheduler**: Cron-based task scheduler (Node.js) - **PostgreSQL**: Primary data store (version 14) - **Redis**: Cache and message queue (version 7.0) - **PgBouncer**: Database connection pooler ### Deployment Model - **Namespace**: `cloudflow-prod` - **Cluster**: AWS EKS 1.28 - **Region**: us-east-1 (primary), us-west-2 (disaster recovery) - **High Availability**: Multi-AZ deployment across 3 availability zones --- ## Prerequisites Before beginning the deployment process, ensure you have the following: ### Required Tools - `kubectl` (v1.28 or later) - `helm` (v3.12 or later) - `aws-cli` (v2.13 or later) - `eksctl` (v0.165 or later) - `terraform` (v1.6 or later) - for infrastructure provisioning ### Access Requirements - AWS account with appropriate IAM permissions - EKS cluster admin access - Container registry access (ECR) - Domain name and SSL certificates - Secrets management access (AWS Secrets Manager or Vault) ### Network Requirements - VPC with at least 3 public and 3 private subnets - NAT Gateway configured - Security groups allowing: - Ingress: HTTPS (443), HTTP (80) - Internal: PostgreSQL (5432), Redis (6379) - Monitoring: Prometheus (9090), Grafana (3000) --- ## Infrastructure Setup ### EKS Cluster Creation Use the following `eksctl` configuration to create the EKS cluster: ```yaml # cluster-config.yaml apiVersion: eksctl.io/v1alpha5 kind: ClusterConfig metadata: name: cloudflow-production region: us-east-1 version: \"1.28\" availabilityZones: - us-east-1a - us-east-1b - us-east-1c vpc: cidr: 10.0.0.0/16 nat: gateway: HighlyAvailable managedNodeGroups: - name: cloudflow-workers instanceType: m5.xlarge minSize: 3 maxSize: 10 desiredCapacity: 5 volumeSize: 100 privateNetworking: true labels: role: worker environment: production tags: Environment: production Application: cloudflow iam: withAddonPolicies: ebs: true efs: true albIngress: true cloudWatch: true cloudWatch: clusterLogging: enableTypes: - api - audit - authenticator - controllerManager - scheduler ``` Create the cluster: ```bash eksctl",
              "score": null
            }
          ],
          "latency_ms": 12.922784000693355,
          "query_id": "realistic_019",
          "dimension": "synonym",
          "query": "Where is the CloudFlow Helm chart hosted?"
        },
        {
          "key_facts": [
            "helm repo add cloudflow https://charts.cloudflow.io"
          ],
          "found_facts": [
            "helm repo add cloudflow https://charts.cloudflow.io"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "deployment_guide"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_1",
              "content": "create cluster -f cluster-config.yaml ``` ### Storage Configuration Install the EBS CSI driver for persistent volumes: ```bash eksctl create addon --name aws-ebs-csi-driver \\ --cluster cloudflow-production \\ --service-account-role-arn arn:aws:iam::ACCOUNT_ID:role/AmazonEKS_EBS_CSI_DriverRole \\ --force ``` --- ## Kubernetes Deployment ### Namespace Setup Create the CloudFlow namespace and configure resource quotas: ```yaml # namespace.yaml apiVersion: v1 kind: Namespace metadata: name: cloudflow-prod labels: name: cloudflow-prod environment: production --- apiVersion: v1 kind: ResourceQuota metadata: name: cloudflow-quota namespace: cloudflow-prod spec: hard: requests.cpu: \"50\" requests.memory: 100Gi persistentvolumeclaims: \"10\" services.loadbalancers: \"2\" ``` Apply the namespace configuration: ```bash kubectl apply -f namespace.yaml ``` ### Environment Configuration CloudFlow requires the following environment variables: | Variable | Description | Example | Required | |----------|-------------|---------|----------| | `DATABASE_URL` | PostgreSQL connection string | `postgresql://user:pass@host:5432/cloudflow` | Yes | | `REDIS_URL` | Redis connection string | `redis://redis-master.cloudflow-prod.svc.cluster.local:6379` | Yes | | `JWT_SECRET` | Secret key for JWT token signing | `<generated-secret-256-bit>` | Yes | | `LOG_LEVEL` | Application log verbosity | `info`, `debug`, `warn`, `error` | No (default: `info`) | | `API_PORT` | API server port | `3000` | No (default: `3000`) | | `WORKER_CONCURRENCY` | Number of concurrent workers | `10` | No (default: `5`) | | `SESSION_SECRET` | Session encryption key | `<generated-secret-256-bit>` | Yes | | `AWS_REGION` | AWS region for services | `us-east-1` | Yes | | `S3_BUCKET` | S3 bucket for file storage | `cloudflow-prod-storage` | Yes | | `SMTP_HOST` | Email server hostname | `smtp.sendgrid.net` | No | | `SMTP_PORT` | Email server port | `587` | No | | `METRICS_ENABLED` | Enable Prometheus metrics | `true` | No (default: `false`) | Create a Kubernetes secret for sensitive variables: ```bash kubectl create secret generic cloudflow-secrets \\ --namespace cloudflow-prod \\ --from-literal=DATABASE_URL=\"postgresql://cloudflow:$(cat db-password.txt)@pgbouncer.cloudflow-prod.svc.cluster.local:5432/cloudflow\" \\ --from-literal=REDIS_URL=\"redis://redis-master.cloudflow-prod.svc.cluster.local:6379\" \\ --from-literal=JWT_SECRET=\"$(openssl rand -base64 32)\" \\ --from-literal=SESSION_SECRET=\"$(openssl rand -base64 32)\" ``` ### Helm Chart Deployment Add the CloudFlow Helm repository: ```bash helm repo add cloudflow https://charts.cloudflow.io helm repo update ``` Create a values file for production configuration: ```yaml # values-production.yaml replicaCount: 3 image: repository: 123456789012.dkr.ecr.us-east-1.amazonaws.com/cloudflow tag: \"2.4.0\" pullPolicy: IfNotPresent resources: limits: cpu: 2000m memory: 4Gi requests: cpu: 2000m memory: 4Gi autoscaling: enabled: true minReplicas: 3 maxReplicas: 10 targetCPUUtilizationPercentage: 70 targetMemoryUtilizationPercentage: 80 service: type: ClusterIP port: 80 targetPort: 3000 annotations: service.beta.kubernetes.io/aws-load-balancer-type: \"nlb\" ingress: enabled: true className: nginx annotations: cert-manager.io/cluster-issuer: \"letsencrypt-prod\" nginx.ingress.kubernetes.io/ssl-redirect: \"true\" hosts: - host: api.cloudflow.io paths: - path: / pathType: Prefix tls:",
              "score": null
            },
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_3",
              "content": "Deploy PostgreSQL using the Bitnami Helm chart: ```yaml # postgres-values.yaml global: postgresql: auth: username: cloudflow database: cloudflow existingSecret: postgres-credentials image: tag: \"14.10.0\" primary: resources: limits: cpu: 4000m memory: 8Gi requests: cpu: 2000m memory: 4Gi persistence: enabled: true size: 100Gi storageClass: gp3 extendedConfiguration: | max_connections = 100 shared_buffers = 2GB effective_cache_size = 6GB maintenance_work_mem = 512MB checkpoint_completion_target = 0.9 wal_buffers = 16MB default_statistics_target = 100 random_page_cost = 1.1 effective_io_concurrency = 200 work_mem = 10MB min_wal_size = 1GB max_wal_size = 4GB readReplicas: replicaCount: 2 resources: limits: cpu: 2000m memory: 4Gi requests: cpu: 1000m memory: 2Gi metrics: enabled: true serviceMonitor: enabled: true ``` Install PostgreSQL: ```bash # Create password secret kubectl create secret generic postgres-credentials \\ --namespace cloudflow-prod \\ --from-literal=postgres-password=\"$(openssl rand -base64 32)\" \\ --from-literal=password=\"$(openssl rand -base64 32)\" # Install PostgreSQL helm install postgresql bitnami/postgresql \\ --namespace cloudflow-prod \\ --values postgres-values.yaml \\ --wait ``` ### PgBouncer Configuration Deploy PgBouncer for connection pooling to handle up to 100 connections efficiently: ```yaml # pgbouncer.yaml apiVersion: v1 kind: ConfigMap metadata: name: pgbouncer-config namespace: cloudflow-prod data: pgbouncer.ini: | [databases] cloudflow = host=postgresql.cloudflow-prod.svc.cluster.local port=5432 dbname=cloudflow [pgbouncer] listen_addr = 0.0.0.0 listen_port = 5432 auth_type = md5 auth_file = /etc/pgbouncer/userlist.txt pool_mode = transaction max_client_conn = 1000 default_pool_size = 25 reserve_pool_size = 5 reserve_pool_timeout = 3 max_db_connections = 100 max_user_connections = 100 server_lifetime = 3600 server_idle_timeout = 600 log_connections = 1 log_disconnections = 1 log_pooler_errors = 1 --- apiVersion: apps/v1 kind: Deployment metadata: name: pgbouncer namespace: cloudflow-prod spec: replicas: 2 selector: matchLabels: app: pgbouncer template: metadata: labels: app: pgbouncer spec: containers: - name: pgbouncer image: pgbouncer/pgbouncer:1.21.0 ports: - containerPort: 5432 resources: requests: cpu: 500m memory: 512Mi limits: cpu: 1000m memory: 1Gi volumeMounts: - name: config mountPath: /etc/pgbouncer volumes: - name: config configMap: name: pgbouncer-config --- apiVersion: v1 kind: Service metadata: name: pgbouncer namespace: cloudflow-prod spec: selector: app: pgbouncer ports: - port: 5432 targetPort: 5432 type: ClusterIP ``` Apply the PgBouncer configuration: ```bash kubectl apply -f pgbouncer.yaml ``` ### Database Migrations Run database migrations before deploying new versions: ```bash # Create migration job kubectl create job cloudflow-migrate-$(date +%s) \\ --namespace cloudflow-prod \\ --image=123456789012.dkr.ecr.us-east-1.amazonaws.com/cloudflow:2.4.0 \\ -- npm run migrate # Monitor migration progress kubectl logs -f job/cloudflow-migrate-<timestamp> -n cloudflow-prod ``` --- ## Monitoring and Observability ### Prometheus Setup Install Prometheus using the kube-prometheus-stack: ```bash helm install prometheus prometheus-community/kube-prometheus-stack \\ --namespace monitoring \\ --create-namespace \\ --set prometheus.prometheusSpec.serviceMonitorSelectorNilUsesHelmValues=false \\",
              "score": null
            },
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_5",
              "content": "Configure log aggregation: ```bash # Install Fluent Bit for log forwarding helm install fluent-bit fluent/fluent-bit \\ --namespace logging \\ --create-namespace \\ --set cloudWatch.enabled=true \\ --set cloudWatch.region=us-east-1 \\ --set cloudWatch.logGroupName=/aws/eks/cloudflow-production/application ``` Query logs using CloudWatch Insights: ```sql fields @timestamp, @message, level, requestId, userId | filter namespace = \"cloudflow-prod\" | filter level = \"error\" | sort @timestamp desc | limit 100 ``` --- ## Backup and Disaster Recovery ### Database Backup Strategy CloudFlow implements a comprehensive backup strategy with the following retention policy: - **Daily snapshots**: Retained for 30 days - **Weekly backups**: Retained for 90 days - **Monthly backups**: Retained for 1 year #### Automated Backup with Velero Install Velero for cluster-wide backups: ```bash velero install \\ --provider aws \\ --plugins velero/velero-plugin-for-aws:v1.8.0 \\ --bucket cloudflow-velero-backups \\ --backup-location-config region=us-east-1 \\ --snapshot-location-config region=us-east-1 \\ --secret-file ./credentials-velero ``` Create a daily backup schedule: ```yaml # backup-schedule.yaml apiVersion: velero.io/v1 kind: Schedule metadata: name: cloudflow-daily-backup namespace: velero spec: schedule: \"0 2 * * *\" # 2 AM UTC daily template: includedNamespaces: - cloudflow-prod ttl: 720h0m0s # 30 days storageLocation: default snapshotVolumes: true ``` Apply the backup schedule: ```bash kubectl apply -f backup-schedule.yaml ``` #### PostgreSQL Backup Configure PostgreSQL continuous archiving with WAL-G: ```bash # Create backup cronjob kubectl apply -f - <<EOF apiVersion: batch/v1 kind: CronJob metadata: name: postgres-backup namespace: cloudflow-prod spec: schedule: \"0 1 * * *\" # 1 AM UTC daily jobTemplate: spec: template: spec: containers: - name: backup image: postgres:14 env: - name: PGPASSWORD valueFrom: secretKeyRef: name: postgres-credentials key: password command: - /bin/bash - -c - | TIMESTAMP=\\$(date +%Y%m%d_%H%M%S) pg_dump -h postgresql.cloudflow-prod.svc.cluster.local \\ -U cloudflow -d cloudflow \\ | gzip > /backups/cloudflow_\\${TIMESTAMP}.sql.gz aws s3 cp /backups/cloudflow_\\${TIMESTAMP}.sql.gz \\ s3://cloudflow-db-backups/daily/ volumeMounts: - name: backup-storage mountPath: /backups volumes: - name: backup-storage emptyDir: {} restartPolicy: OnFailure EOF ``` ### Disaster Recovery Procedure In case of catastrophic failure, follow these steps to restore CloudFlow: 1. **Restore EKS Cluster** (if necessary): ```bash eksctl create cluster -f cluster-config.yaml ``` 2. **Restore Velero Backup**: ```bash # List available backups velero backup get # Restore from backup velero restore create --from-backup cloudflow-daily-backup-20260124 ``` 3. **Restore Database**: ```bash # Download backup from S3 aws s3 cp s3://cloudflow-db-backups/daily/cloudflow_20260124_010000.sql.gz . # Restore database gunzip -c cloudflow_20260124_010000.sql.gz | \\ psql -h postgresql.cloudflow-prod.svc.cluster.local -U cloudflow -d cloudflow ``` 4.",
              "score": null
            },
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_7",
              "content": "podSelector: matchLabels: app: cloudflow policyTypes: - Ingress - Egress ingress: - from: - namespaceSelector: matchLabels: name: cloudflow-prod - podSelector: matchLabels: app: nginx-ingress ports: - protocol: TCP port: 3000 egress: - to: - podSelector: matchLabels: app: postgresql ports: - protocol: TCP port: 5432 - to: - podSelector: matchLabels: app: redis ports: - protocol: TCP port: 6379 ``` ### Pod Security Standards Enforce Pod Security Standards at the namespace level: ```bash kubectl label namespace cloudflow-prod \\ pod-security.kubernetes.io/enforce=restricted \\ pod-security.kubernetes.io/audit=restricted \\ pod-security.kubernetes.io/warn=restricted ``` ### Secrets Management Use AWS Secrets Manager for sensitive credentials: ```bash # Install External Secrets Operator helm install external-secrets external-secrets/external-secrets \\ --namespace external-secrets-system \\ --create-namespace # Create SecretStore kubectl apply -f - <<EOF apiVersion: external-secrets.io/v1beta1 kind: SecretStore metadata: name: aws-secrets-manager namespace: cloudflow-prod spec: provider: aws: service: SecretsManager region: us-east-1 auth: jwt: serviceAccountRef: name: external-secrets-sa EOF ``` --- ## Troubleshooting ### Common Issues #### Pods in CrashLoopBackOff **Symptoms**: Pods continuously restart **Diagnosis**: ```bash kubectl logs -n cloudflow-prod <pod-name> --previous kubectl describe pod -n cloudflow-prod <pod-name> ``` **Common Causes**: - Database connection failure - Invalid environment variables - Insufficient resources #### High Memory Usage **Symptoms**: Pods being OOMKilled **Diagnosis**: ```bash kubectl top pods -n cloudflow-prod ``` **Resolution**: - Increase memory limits in deployment - Check for memory leaks in application logs - Review heap dump for Node.js processes #### Database Connection Pool Exhausted **Symptoms**: \"Too many connections\" errors **Diagnosis**: ```bash # Check PgBouncer stats kubectl exec -it pgbouncer-xxx -n cloudflow-prod -- \\ psql -p 5432 -U pgbouncer pgbouncer -c \"SHOW POOLS;\" ``` **Resolution**: - Increase `max_db_connections` in PgBouncer - Optimize application connection usage - Add more PostgreSQL replicas ### Support Contacts - **On-call Engineer**: pagerduty.com/cloudflow-oncall - **Slack Channel**: #cloudflow-operations - **Documentation**: https://docs.cloudflow.io - **Status Page**: https://status.cloudflow.io --- ## Appendix ### Useful Commands ```bash # View all resources in namespace kubectl get all -n cloudflow-prod # Check resource usage kubectl top pods -n cloudflow-prod kubectl top nodes # View logs kubectl logs -f deployment/cloudflow-api -n cloudflow-prod # Execute command in pod kubectl exec -it <pod-name> -n cloudflow-prod -- /bin/sh # Port forward for local testing kubectl port-forward svc/cloudflow-api 8080:80 -n cloudflow-prod # Rollback deployment helm rollback cloudflow -n cloudflow-prod # Update deployment helm upgrade cloudflow cloudflow/cloudflow \\ --namespace cloudflow-prod \\ --values values-production.yaml ``` ### Maintenance Windows Scheduled maintenance occurs during the following windows: - **Primary**: Sunday 02:00-06:00",
              "score": null
            },
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_2",
              "content": "- secretName: cloudflow-tls hosts: - api.cloudflow.io healthCheck: enabled: true livenessProbe: path: /health initialDelaySeconds: 30 periodSeconds: 10 timeoutSeconds: 5 failureThreshold: 3 readinessProbe: path: /ready initialDelaySeconds: 10 periodSeconds: 5 timeoutSeconds: 3 failureThreshold: 3 env: - name: NODE_ENV value: \"production\" - name: LOG_LEVEL value: \"info\" - name: API_PORT value: \"3000\" - name: WORKER_CONCURRENCY value: \"10\" - name: AWS_REGION value: \"us-east-1\" - name: METRICS_ENABLED value: \"true\" envFrom: - secretRef: name: cloudflow-secrets podAnnotations: prometheus.io/scrape: \"true\" prometheus.io/port: \"9090\" prometheus.io/path: \"/metrics\" podSecurityContext: runAsNonRoot: true runAsUser: 1000 fsGroup: 1000 securityContext: allowPrivilegeEscalation: false capabilities: drop: - ALL readOnlyRootFilesystem: true persistence: enabled: true storageClass: gp3 accessMode: ReadWriteOnce size: 50Gi redis: enabled: true architecture: replication auth: enabled: true master: persistence: size: 20Gi replica: replicaCount: 2 persistence: size: 20Gi ``` Deploy CloudFlow using Helm: ```bash helm install cloudflow cloudflow/cloudflow \\ --namespace cloudflow-prod \\ --values values-production.yaml \\ --wait \\ --timeout 10m ``` ### Deployment Verification Verify the deployment status: ```bash # Check pod status kubectl get pods -n cloudflow-prod # Expected output: # NAME READY STATUS RESTARTS AGE # cloudflow-api-7d8f9c5b6d-4xk2p 1/1 Running 0 2m # cloudflow-api-7d8f9c5b6d-9hj5m 1/1 Running 0 2m # cloudflow-api-7d8f9c5b6d-tn8wq 1/1 Running 0 2m # Check service endpoints kubectl get svc -n cloudflow-prod # Check ingress kubectl get ingress -n cloudflow-prod ``` Test health endpoints: ```bash # Port forward for testing kubectl port-forward -n cloudflow-prod svc/cloudflow-api 8080:80 # Test health endpoint curl http://localhost:8080/health # Expected: {\"status\":\"healthy\",\"timestamp\":\"2026-01-24T10:30:00Z\"} # Test readiness endpoint curl http://localhost:8080/ready # Expected: {\"status\":\"ready\",\"dependencies\":{\"database\":\"connected\",\"redis\":\"connected\"}} ``` --- ## Database Configuration ### PostgreSQL Setup CloudFlow uses PostgreSQL 14 as its primary data store.",
              "score": null
            }
          ],
          "latency_ms": 13.041414000326768,
          "query_id": "realistic_019",
          "dimension": "problem",
          "query": "helm repo add is failing, what's the correct URL?"
        },
        {
          "key_facts": [
            "helm repo add cloudflow https://charts.cloudflow.io"
          ],
          "found_facts": [
            "helm repo add cloudflow https://charts.cloudflow.io"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "deployment_guide"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_5",
              "content": "Configure log aggregation: ```bash # Install Fluent Bit for log forwarding helm install fluent-bit fluent/fluent-bit \\ --namespace logging \\ --create-namespace \\ --set cloudWatch.enabled=true \\ --set cloudWatch.region=us-east-1 \\ --set cloudWatch.logGroupName=/aws/eks/cloudflow-production/application ``` Query logs using CloudWatch Insights: ```sql fields @timestamp, @message, level, requestId, userId | filter namespace = \"cloudflow-prod\" | filter level = \"error\" | sort @timestamp desc | limit 100 ``` --- ## Backup and Disaster Recovery ### Database Backup Strategy CloudFlow implements a comprehensive backup strategy with the following retention policy: - **Daily snapshots**: Retained for 30 days - **Weekly backups**: Retained for 90 days - **Monthly backups**: Retained for 1 year #### Automated Backup with Velero Install Velero for cluster-wide backups: ```bash velero install \\ --provider aws \\ --plugins velero/velero-plugin-for-aws:v1.8.0 \\ --bucket cloudflow-velero-backups \\ --backup-location-config region=us-east-1 \\ --snapshot-location-config region=us-east-1 \\ --secret-file ./credentials-velero ``` Create a daily backup schedule: ```yaml # backup-schedule.yaml apiVersion: velero.io/v1 kind: Schedule metadata: name: cloudflow-daily-backup namespace: velero spec: schedule: \"0 2 * * *\" # 2 AM UTC daily template: includedNamespaces: - cloudflow-prod ttl: 720h0m0s # 30 days storageLocation: default snapshotVolumes: true ``` Apply the backup schedule: ```bash kubectl apply -f backup-schedule.yaml ``` #### PostgreSQL Backup Configure PostgreSQL continuous archiving with WAL-G: ```bash # Create backup cronjob kubectl apply -f - <<EOF apiVersion: batch/v1 kind: CronJob metadata: name: postgres-backup namespace: cloudflow-prod spec: schedule: \"0 1 * * *\" # 1 AM UTC daily jobTemplate: spec: template: spec: containers: - name: backup image: postgres:14 env: - name: PGPASSWORD valueFrom: secretKeyRef: name: postgres-credentials key: password command: - /bin/bash - -c - | TIMESTAMP=\\$(date +%Y%m%d_%H%M%S) pg_dump -h postgresql.cloudflow-prod.svc.cluster.local \\ -U cloudflow -d cloudflow \\ | gzip > /backups/cloudflow_\\${TIMESTAMP}.sql.gz aws s3 cp /backups/cloudflow_\\${TIMESTAMP}.sql.gz \\ s3://cloudflow-db-backups/daily/ volumeMounts: - name: backup-storage mountPath: /backups volumes: - name: backup-storage emptyDir: {} restartPolicy: OnFailure EOF ``` ### Disaster Recovery Procedure In case of catastrophic failure, follow these steps to restore CloudFlow: 1. **Restore EKS Cluster** (if necessary): ```bash eksctl create cluster -f cluster-config.yaml ``` 2. **Restore Velero Backup**: ```bash # List available backups velero backup get # Restore from backup velero restore create --from-backup cloudflow-daily-backup-20260124 ``` 3. **Restore Database**: ```bash # Download backup from S3 aws s3 cp s3://cloudflow-db-backups/daily/cloudflow_20260124_010000.sql.gz . # Restore database gunzip -c cloudflow_20260124_010000.sql.gz | \\ psql -h postgresql.cloudflow-prod.svc.cluster.local -U cloudflow -d cloudflow ``` 4.",
              "score": null
            },
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_2",
              "content": "- secretName: cloudflow-tls hosts: - api.cloudflow.io healthCheck: enabled: true livenessProbe: path: /health initialDelaySeconds: 30 periodSeconds: 10 timeoutSeconds: 5 failureThreshold: 3 readinessProbe: path: /ready initialDelaySeconds: 10 periodSeconds: 5 timeoutSeconds: 3 failureThreshold: 3 env: - name: NODE_ENV value: \"production\" - name: LOG_LEVEL value: \"info\" - name: API_PORT value: \"3000\" - name: WORKER_CONCURRENCY value: \"10\" - name: AWS_REGION value: \"us-east-1\" - name: METRICS_ENABLED value: \"true\" envFrom: - secretRef: name: cloudflow-secrets podAnnotations: prometheus.io/scrape: \"true\" prometheus.io/port: \"9090\" prometheus.io/path: \"/metrics\" podSecurityContext: runAsNonRoot: true runAsUser: 1000 fsGroup: 1000 securityContext: allowPrivilegeEscalation: false capabilities: drop: - ALL readOnlyRootFilesystem: true persistence: enabled: true storageClass: gp3 accessMode: ReadWriteOnce size: 50Gi redis: enabled: true architecture: replication auth: enabled: true master: persistence: size: 20Gi replica: replicaCount: 2 persistence: size: 20Gi ``` Deploy CloudFlow using Helm: ```bash helm install cloudflow cloudflow/cloudflow \\ --namespace cloudflow-prod \\ --values values-production.yaml \\ --wait \\ --timeout 10m ``` ### Deployment Verification Verify the deployment status: ```bash # Check pod status kubectl get pods -n cloudflow-prod # Expected output: # NAME READY STATUS RESTARTS AGE # cloudflow-api-7d8f9c5b6d-4xk2p 1/1 Running 0 2m # cloudflow-api-7d8f9c5b6d-9hj5m 1/1 Running 0 2m # cloudflow-api-7d8f9c5b6d-tn8wq 1/1 Running 0 2m # Check service endpoints kubectl get svc -n cloudflow-prod # Check ingress kubectl get ingress -n cloudflow-prod ``` Test health endpoints: ```bash # Port forward for testing kubectl port-forward -n cloudflow-prod svc/cloudflow-api 8080:80 # Test health endpoint curl http://localhost:8080/health # Expected: {\"status\":\"healthy\",\"timestamp\":\"2026-01-24T10:30:00Z\"} # Test readiness endpoint curl http://localhost:8080/ready # Expected: {\"status\":\"ready\",\"dependencies\":{\"database\":\"connected\",\"redis\":\"connected\"}} ``` --- ## Database Configuration ### PostgreSQL Setup CloudFlow uses PostgreSQL 14 as its primary data store.",
              "score": null
            },
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_0",
              "content": "# CloudFlow Platform - Deployment and Operations Guide **Version:** 2.4.0 **Last Updated:** January 2026 **Target Environment:** Production (AWS EKS) ## Table of Contents 1. [Overview](#overview) 2. [Prerequisites](#prerequisites) 3. [Infrastructure Setup](#infrastructure-setup) 4. [Kubernetes Deployment](#kubernetes-deployment) 5. [Database Configuration](#database-configuration) 6. [Monitoring and Observability](#monitoring-and-observability) 7. [Backup and Disaster Recovery](#backup-and-disaster-recovery) 8. [Scaling and Performance](#scaling-and-performance) 9. [Security and Compliance](#security-and-compliance) 10. [Troubleshooting](#troubleshooting) --- ## Overview CloudFlow is a cloud-native workflow orchestration platform designed for high-availability production environments. This guide provides comprehensive instructions for deploying and operating CloudFlow on Amazon EKS (Elastic Kubernetes Service). ### Architecture Summary CloudFlow consists of the following components: - **API Server**: REST API for workflow management (Node.js/Express) - **Worker Service**: Background job processor (Node.js) - **Scheduler**: Cron-based task scheduler (Node.js) - **PostgreSQL**: Primary data store (version 14) - **Redis**: Cache and message queue (version 7.0) - **PgBouncer**: Database connection pooler ### Deployment Model - **Namespace**: `cloudflow-prod` - **Cluster**: AWS EKS 1.28 - **Region**: us-east-1 (primary), us-west-2 (disaster recovery) - **High Availability**: Multi-AZ deployment across 3 availability zones --- ## Prerequisites Before beginning the deployment process, ensure you have the following: ### Required Tools - `kubectl` (v1.28 or later) - `helm` (v3.12 or later) - `aws-cli` (v2.13 or later) - `eksctl` (v0.165 or later) - `terraform` (v1.6 or later) - for infrastructure provisioning ### Access Requirements - AWS account with appropriate IAM permissions - EKS cluster admin access - Container registry access (ECR) - Domain name and SSL certificates - Secrets management access (AWS Secrets Manager or Vault) ### Network Requirements - VPC with at least 3 public and 3 private subnets - NAT Gateway configured - Security groups allowing: - Ingress: HTTPS (443), HTTP (80) - Internal: PostgreSQL (5432), Redis (6379) - Monitoring: Prometheus (9090), Grafana (3000) --- ## Infrastructure Setup ### EKS Cluster Creation Use the following `eksctl` configuration to create the EKS cluster: ```yaml # cluster-config.yaml apiVersion: eksctl.io/v1alpha5 kind: ClusterConfig metadata: name: cloudflow-production region: us-east-1 version: \"1.28\" availabilityZones: - us-east-1a - us-east-1b - us-east-1c vpc: cidr: 10.0.0.0/16 nat: gateway: HighlyAvailable managedNodeGroups: - name: cloudflow-workers instanceType: m5.xlarge minSize: 3 maxSize: 10 desiredCapacity: 5 volumeSize: 100 privateNetworking: true labels: role: worker environment: production tags: Environment: production Application: cloudflow iam: withAddonPolicies: ebs: true efs: true albIngress: true cloudWatch: true cloudWatch: clusterLogging: enableTypes: - api - audit - authenticator - controllerManager - scheduler ``` Create the cluster: ```bash eksctl",
              "score": null
            },
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_3",
              "content": "Deploy PostgreSQL using the Bitnami Helm chart: ```yaml # postgres-values.yaml global: postgresql: auth: username: cloudflow database: cloudflow existingSecret: postgres-credentials image: tag: \"14.10.0\" primary: resources: limits: cpu: 4000m memory: 8Gi requests: cpu: 2000m memory: 4Gi persistence: enabled: true size: 100Gi storageClass: gp3 extendedConfiguration: | max_connections = 100 shared_buffers = 2GB effective_cache_size = 6GB maintenance_work_mem = 512MB checkpoint_completion_target = 0.9 wal_buffers = 16MB default_statistics_target = 100 random_page_cost = 1.1 effective_io_concurrency = 200 work_mem = 10MB min_wal_size = 1GB max_wal_size = 4GB readReplicas: replicaCount: 2 resources: limits: cpu: 2000m memory: 4Gi requests: cpu: 1000m memory: 2Gi metrics: enabled: true serviceMonitor: enabled: true ``` Install PostgreSQL: ```bash # Create password secret kubectl create secret generic postgres-credentials \\ --namespace cloudflow-prod \\ --from-literal=postgres-password=\"$(openssl rand -base64 32)\" \\ --from-literal=password=\"$(openssl rand -base64 32)\" # Install PostgreSQL helm install postgresql bitnami/postgresql \\ --namespace cloudflow-prod \\ --values postgres-values.yaml \\ --wait ``` ### PgBouncer Configuration Deploy PgBouncer for connection pooling to handle up to 100 connections efficiently: ```yaml # pgbouncer.yaml apiVersion: v1 kind: ConfigMap metadata: name: pgbouncer-config namespace: cloudflow-prod data: pgbouncer.ini: | [databases] cloudflow = host=postgresql.cloudflow-prod.svc.cluster.local port=5432 dbname=cloudflow [pgbouncer] listen_addr = 0.0.0.0 listen_port = 5432 auth_type = md5 auth_file = /etc/pgbouncer/userlist.txt pool_mode = transaction max_client_conn = 1000 default_pool_size = 25 reserve_pool_size = 5 reserve_pool_timeout = 3 max_db_connections = 100 max_user_connections = 100 server_lifetime = 3600 server_idle_timeout = 600 log_connections = 1 log_disconnections = 1 log_pooler_errors = 1 --- apiVersion: apps/v1 kind: Deployment metadata: name: pgbouncer namespace: cloudflow-prod spec: replicas: 2 selector: matchLabels: app: pgbouncer template: metadata: labels: app: pgbouncer spec: containers: - name: pgbouncer image: pgbouncer/pgbouncer:1.21.0 ports: - containerPort: 5432 resources: requests: cpu: 500m memory: 512Mi limits: cpu: 1000m memory: 1Gi volumeMounts: - name: config mountPath: /etc/pgbouncer volumes: - name: config configMap: name: pgbouncer-config --- apiVersion: v1 kind: Service metadata: name: pgbouncer namespace: cloudflow-prod spec: selector: app: pgbouncer ports: - port: 5432 targetPort: 5432 type: ClusterIP ``` Apply the PgBouncer configuration: ```bash kubectl apply -f pgbouncer.yaml ``` ### Database Migrations Run database migrations before deploying new versions: ```bash # Create migration job kubectl create job cloudflow-migrate-$(date +%s) \\ --namespace cloudflow-prod \\ --image=123456789012.dkr.ecr.us-east-1.amazonaws.com/cloudflow:2.4.0 \\ -- npm run migrate # Monitor migration progress kubectl logs -f job/cloudflow-migrate-<timestamp> -n cloudflow-prod ``` --- ## Monitoring and Observability ### Prometheus Setup Install Prometheus using the kube-prometheus-stack: ```bash helm install prometheus prometheus-community/kube-prometheus-stack \\ --namespace monitoring \\ --create-namespace \\ --set prometheus.prometheusSpec.serviceMonitorSelectorNilUsesHelmValues=false \\",
              "score": null
            },
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_1",
              "content": "create cluster -f cluster-config.yaml ``` ### Storage Configuration Install the EBS CSI driver for persistent volumes: ```bash eksctl create addon --name aws-ebs-csi-driver \\ --cluster cloudflow-production \\ --service-account-role-arn arn:aws:iam::ACCOUNT_ID:role/AmazonEKS_EBS_CSI_DriverRole \\ --force ``` --- ## Kubernetes Deployment ### Namespace Setup Create the CloudFlow namespace and configure resource quotas: ```yaml # namespace.yaml apiVersion: v1 kind: Namespace metadata: name: cloudflow-prod labels: name: cloudflow-prod environment: production --- apiVersion: v1 kind: ResourceQuota metadata: name: cloudflow-quota namespace: cloudflow-prod spec: hard: requests.cpu: \"50\" requests.memory: 100Gi persistentvolumeclaims: \"10\" services.loadbalancers: \"2\" ``` Apply the namespace configuration: ```bash kubectl apply -f namespace.yaml ``` ### Environment Configuration CloudFlow requires the following environment variables: | Variable | Description | Example | Required | |----------|-------------|---------|----------| | `DATABASE_URL` | PostgreSQL connection string | `postgresql://user:pass@host:5432/cloudflow` | Yes | | `REDIS_URL` | Redis connection string | `redis://redis-master.cloudflow-prod.svc.cluster.local:6379` | Yes | | `JWT_SECRET` | Secret key for JWT token signing | `<generated-secret-256-bit>` | Yes | | `LOG_LEVEL` | Application log verbosity | `info`, `debug`, `warn`, `error` | No (default: `info`) | | `API_PORT` | API server port | `3000` | No (default: `3000`) | | `WORKER_CONCURRENCY` | Number of concurrent workers | `10` | No (default: `5`) | | `SESSION_SECRET` | Session encryption key | `<generated-secret-256-bit>` | Yes | | `AWS_REGION` | AWS region for services | `us-east-1` | Yes | | `S3_BUCKET` | S3 bucket for file storage | `cloudflow-prod-storage` | Yes | | `SMTP_HOST` | Email server hostname | `smtp.sendgrid.net` | No | | `SMTP_PORT` | Email server port | `587` | No | | `METRICS_ENABLED` | Enable Prometheus metrics | `true` | No (default: `false`) | Create a Kubernetes secret for sensitive variables: ```bash kubectl create secret generic cloudflow-secrets \\ --namespace cloudflow-prod \\ --from-literal=DATABASE_URL=\"postgresql://cloudflow:$(cat db-password.txt)@pgbouncer.cloudflow-prod.svc.cluster.local:5432/cloudflow\" \\ --from-literal=REDIS_URL=\"redis://redis-master.cloudflow-prod.svc.cluster.local:6379\" \\ --from-literal=JWT_SECRET=\"$(openssl rand -base64 32)\" \\ --from-literal=SESSION_SECRET=\"$(openssl rand -base64 32)\" ``` ### Helm Chart Deployment Add the CloudFlow Helm repository: ```bash helm repo add cloudflow https://charts.cloudflow.io helm repo update ``` Create a values file for production configuration: ```yaml # values-production.yaml replicaCount: 3 image: repository: 123456789012.dkr.ecr.us-east-1.amazonaws.com/cloudflow tag: \"2.4.0\" pullPolicy: IfNotPresent resources: limits: cpu: 2000m memory: 4Gi requests: cpu: 2000m memory: 4Gi autoscaling: enabled: true minReplicas: 3 maxReplicas: 10 targetCPUUtilizationPercentage: 70 targetMemoryUtilizationPercentage: 80 service: type: ClusterIP port: 80 targetPort: 3000 annotations: service.beta.kubernetes.io/aws-load-balancer-type: \"nlb\" ingress: enabled: true className: nginx annotations: cert-manager.io/cluster-issuer: \"letsencrypt-prod\" nginx.ingress.kubernetes.io/ssl-redirect: \"true\" hosts: - host: api.cloudflow.io paths: - path: / pathType: Prefix tls:",
              "score": null
            }
          ],
          "latency_ms": 12.707341000350425,
          "query_id": "realistic_019",
          "dimension": "casual",
          "query": "helm repo"
        },
        {
          "key_facts": [
            "helm repo add cloudflow https://charts.cloudflow.io"
          ],
          "found_facts": [
            "helm repo add cloudflow https://charts.cloudflow.io"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "deployment_guide"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_3",
              "content": "Deploy PostgreSQL using the Bitnami Helm chart: ```yaml # postgres-values.yaml global: postgresql: auth: username: cloudflow database: cloudflow existingSecret: postgres-credentials image: tag: \"14.10.0\" primary: resources: limits: cpu: 4000m memory: 8Gi requests: cpu: 2000m memory: 4Gi persistence: enabled: true size: 100Gi storageClass: gp3 extendedConfiguration: | max_connections = 100 shared_buffers = 2GB effective_cache_size = 6GB maintenance_work_mem = 512MB checkpoint_completion_target = 0.9 wal_buffers = 16MB default_statistics_target = 100 random_page_cost = 1.1 effective_io_concurrency = 200 work_mem = 10MB min_wal_size = 1GB max_wal_size = 4GB readReplicas: replicaCount: 2 resources: limits: cpu: 2000m memory: 4Gi requests: cpu: 1000m memory: 2Gi metrics: enabled: true serviceMonitor: enabled: true ``` Install PostgreSQL: ```bash # Create password secret kubectl create secret generic postgres-credentials \\ --namespace cloudflow-prod \\ --from-literal=postgres-password=\"$(openssl rand -base64 32)\" \\ --from-literal=password=\"$(openssl rand -base64 32)\" # Install PostgreSQL helm install postgresql bitnami/postgresql \\ --namespace cloudflow-prod \\ --values postgres-values.yaml \\ --wait ``` ### PgBouncer Configuration Deploy PgBouncer for connection pooling to handle up to 100 connections efficiently: ```yaml # pgbouncer.yaml apiVersion: v1 kind: ConfigMap metadata: name: pgbouncer-config namespace: cloudflow-prod data: pgbouncer.ini: | [databases] cloudflow = host=postgresql.cloudflow-prod.svc.cluster.local port=5432 dbname=cloudflow [pgbouncer] listen_addr = 0.0.0.0 listen_port = 5432 auth_type = md5 auth_file = /etc/pgbouncer/userlist.txt pool_mode = transaction max_client_conn = 1000 default_pool_size = 25 reserve_pool_size = 5 reserve_pool_timeout = 3 max_db_connections = 100 max_user_connections = 100 server_lifetime = 3600 server_idle_timeout = 600 log_connections = 1 log_disconnections = 1 log_pooler_errors = 1 --- apiVersion: apps/v1 kind: Deployment metadata: name: pgbouncer namespace: cloudflow-prod spec: replicas: 2 selector: matchLabels: app: pgbouncer template: metadata: labels: app: pgbouncer spec: containers: - name: pgbouncer image: pgbouncer/pgbouncer:1.21.0 ports: - containerPort: 5432 resources: requests: cpu: 500m memory: 512Mi limits: cpu: 1000m memory: 1Gi volumeMounts: - name: config mountPath: /etc/pgbouncer volumes: - name: config configMap: name: pgbouncer-config --- apiVersion: v1 kind: Service metadata: name: pgbouncer namespace: cloudflow-prod spec: selector: app: pgbouncer ports: - port: 5432 targetPort: 5432 type: ClusterIP ``` Apply the PgBouncer configuration: ```bash kubectl apply -f pgbouncer.yaml ``` ### Database Migrations Run database migrations before deploying new versions: ```bash # Create migration job kubectl create job cloudflow-migrate-$(date +%s) \\ --namespace cloudflow-prod \\ --image=123456789012.dkr.ecr.us-east-1.amazonaws.com/cloudflow:2.4.0 \\ -- npm run migrate # Monitor migration progress kubectl logs -f job/cloudflow-migrate-<timestamp> -n cloudflow-prod ``` --- ## Monitoring and Observability ### Prometheus Setup Install Prometheus using the kube-prometheus-stack: ```bash helm install prometheus prometheus-community/kube-prometheus-stack \\ --namespace monitoring \\ --create-namespace \\ --set prometheus.prometheusSpec.serviceMonitorSelectorNilUsesHelmValues=false \\",
              "score": null
            },
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_0",
              "content": "# CloudFlow Platform - Deployment and Operations Guide **Version:** 2.4.0 **Last Updated:** January 2026 **Target Environment:** Production (AWS EKS) ## Table of Contents 1. [Overview](#overview) 2. [Prerequisites](#prerequisites) 3. [Infrastructure Setup](#infrastructure-setup) 4. [Kubernetes Deployment](#kubernetes-deployment) 5. [Database Configuration](#database-configuration) 6. [Monitoring and Observability](#monitoring-and-observability) 7. [Backup and Disaster Recovery](#backup-and-disaster-recovery) 8. [Scaling and Performance](#scaling-and-performance) 9. [Security and Compliance](#security-and-compliance) 10. [Troubleshooting](#troubleshooting) --- ## Overview CloudFlow is a cloud-native workflow orchestration platform designed for high-availability production environments. This guide provides comprehensive instructions for deploying and operating CloudFlow on Amazon EKS (Elastic Kubernetes Service). ### Architecture Summary CloudFlow consists of the following components: - **API Server**: REST API for workflow management (Node.js/Express) - **Worker Service**: Background job processor (Node.js) - **Scheduler**: Cron-based task scheduler (Node.js) - **PostgreSQL**: Primary data store (version 14) - **Redis**: Cache and message queue (version 7.0) - **PgBouncer**: Database connection pooler ### Deployment Model - **Namespace**: `cloudflow-prod` - **Cluster**: AWS EKS 1.28 - **Region**: us-east-1 (primary), us-west-2 (disaster recovery) - **High Availability**: Multi-AZ deployment across 3 availability zones --- ## Prerequisites Before beginning the deployment process, ensure you have the following: ### Required Tools - `kubectl` (v1.28 or later) - `helm` (v3.12 or later) - `aws-cli` (v2.13 or later) - `eksctl` (v0.165 or later) - `terraform` (v1.6 or later) - for infrastructure provisioning ### Access Requirements - AWS account with appropriate IAM permissions - EKS cluster admin access - Container registry access (ECR) - Domain name and SSL certificates - Secrets management access (AWS Secrets Manager or Vault) ### Network Requirements - VPC with at least 3 public and 3 private subnets - NAT Gateway configured - Security groups allowing: - Ingress: HTTPS (443), HTTP (80) - Internal: PostgreSQL (5432), Redis (6379) - Monitoring: Prometheus (9090), Grafana (3000) --- ## Infrastructure Setup ### EKS Cluster Creation Use the following `eksctl` configuration to create the EKS cluster: ```yaml # cluster-config.yaml apiVersion: eksctl.io/v1alpha5 kind: ClusterConfig metadata: name: cloudflow-production region: us-east-1 version: \"1.28\" availabilityZones: - us-east-1a - us-east-1b - us-east-1c vpc: cidr: 10.0.0.0/16 nat: gateway: HighlyAvailable managedNodeGroups: - name: cloudflow-workers instanceType: m5.xlarge minSize: 3 maxSize: 10 desiredCapacity: 5 volumeSize: 100 privateNetworking: true labels: role: worker environment: production tags: Environment: production Application: cloudflow iam: withAddonPolicies: ebs: true efs: true albIngress: true cloudWatch: true cloudWatch: clusterLogging: enableTypes: - api - audit - authenticator - controllerManager - scheduler ``` Create the cluster: ```bash eksctl",
              "score": null
            },
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_2",
              "content": "- secretName: cloudflow-tls hosts: - api.cloudflow.io healthCheck: enabled: true livenessProbe: path: /health initialDelaySeconds: 30 periodSeconds: 10 timeoutSeconds: 5 failureThreshold: 3 readinessProbe: path: /ready initialDelaySeconds: 10 periodSeconds: 5 timeoutSeconds: 3 failureThreshold: 3 env: - name: NODE_ENV value: \"production\" - name: LOG_LEVEL value: \"info\" - name: API_PORT value: \"3000\" - name: WORKER_CONCURRENCY value: \"10\" - name: AWS_REGION value: \"us-east-1\" - name: METRICS_ENABLED value: \"true\" envFrom: - secretRef: name: cloudflow-secrets podAnnotations: prometheus.io/scrape: \"true\" prometheus.io/port: \"9090\" prometheus.io/path: \"/metrics\" podSecurityContext: runAsNonRoot: true runAsUser: 1000 fsGroup: 1000 securityContext: allowPrivilegeEscalation: false capabilities: drop: - ALL readOnlyRootFilesystem: true persistence: enabled: true storageClass: gp3 accessMode: ReadWriteOnce size: 50Gi redis: enabled: true architecture: replication auth: enabled: true master: persistence: size: 20Gi replica: replicaCount: 2 persistence: size: 20Gi ``` Deploy CloudFlow using Helm: ```bash helm install cloudflow cloudflow/cloudflow \\ --namespace cloudflow-prod \\ --values values-production.yaml \\ --wait \\ --timeout 10m ``` ### Deployment Verification Verify the deployment status: ```bash # Check pod status kubectl get pods -n cloudflow-prod # Expected output: # NAME READY STATUS RESTARTS AGE # cloudflow-api-7d8f9c5b6d-4xk2p 1/1 Running 0 2m # cloudflow-api-7d8f9c5b6d-9hj5m 1/1 Running 0 2m # cloudflow-api-7d8f9c5b6d-tn8wq 1/1 Running 0 2m # Check service endpoints kubectl get svc -n cloudflow-prod # Check ingress kubectl get ingress -n cloudflow-prod ``` Test health endpoints: ```bash # Port forward for testing kubectl port-forward -n cloudflow-prod svc/cloudflow-api 8080:80 # Test health endpoint curl http://localhost:8080/health # Expected: {\"status\":\"healthy\",\"timestamp\":\"2026-01-24T10:30:00Z\"} # Test readiness endpoint curl http://localhost:8080/ready # Expected: {\"status\":\"ready\",\"dependencies\":{\"database\":\"connected\",\"redis\":\"connected\"}} ``` --- ## Database Configuration ### PostgreSQL Setup CloudFlow uses PostgreSQL 14 as its primary data store.",
              "score": null
            },
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_1",
              "content": "create cluster -f cluster-config.yaml ``` ### Storage Configuration Install the EBS CSI driver for persistent volumes: ```bash eksctl create addon --name aws-ebs-csi-driver \\ --cluster cloudflow-production \\ --service-account-role-arn arn:aws:iam::ACCOUNT_ID:role/AmazonEKS_EBS_CSI_DriverRole \\ --force ``` --- ## Kubernetes Deployment ### Namespace Setup Create the CloudFlow namespace and configure resource quotas: ```yaml # namespace.yaml apiVersion: v1 kind: Namespace metadata: name: cloudflow-prod labels: name: cloudflow-prod environment: production --- apiVersion: v1 kind: ResourceQuota metadata: name: cloudflow-quota namespace: cloudflow-prod spec: hard: requests.cpu: \"50\" requests.memory: 100Gi persistentvolumeclaims: \"10\" services.loadbalancers: \"2\" ``` Apply the namespace configuration: ```bash kubectl apply -f namespace.yaml ``` ### Environment Configuration CloudFlow requires the following environment variables: | Variable | Description | Example | Required | |----------|-------------|---------|----------| | `DATABASE_URL` | PostgreSQL connection string | `postgresql://user:pass@host:5432/cloudflow` | Yes | | `REDIS_URL` | Redis connection string | `redis://redis-master.cloudflow-prod.svc.cluster.local:6379` | Yes | | `JWT_SECRET` | Secret key for JWT token signing | `<generated-secret-256-bit>` | Yes | | `LOG_LEVEL` | Application log verbosity | `info`, `debug`, `warn`, `error` | No (default: `info`) | | `API_PORT` | API server port | `3000` | No (default: `3000`) | | `WORKER_CONCURRENCY` | Number of concurrent workers | `10` | No (default: `5`) | | `SESSION_SECRET` | Session encryption key | `<generated-secret-256-bit>` | Yes | | `AWS_REGION` | AWS region for services | `us-east-1` | Yes | | `S3_BUCKET` | S3 bucket for file storage | `cloudflow-prod-storage` | Yes | | `SMTP_HOST` | Email server hostname | `smtp.sendgrid.net` | No | | `SMTP_PORT` | Email server port | `587` | No | | `METRICS_ENABLED` | Enable Prometheus metrics | `true` | No (default: `false`) | Create a Kubernetes secret for sensitive variables: ```bash kubectl create secret generic cloudflow-secrets \\ --namespace cloudflow-prod \\ --from-literal=DATABASE_URL=\"postgresql://cloudflow:$(cat db-password.txt)@pgbouncer.cloudflow-prod.svc.cluster.local:5432/cloudflow\" \\ --from-literal=REDIS_URL=\"redis://redis-master.cloudflow-prod.svc.cluster.local:6379\" \\ --from-literal=JWT_SECRET=\"$(openssl rand -base64 32)\" \\ --from-literal=SESSION_SECRET=\"$(openssl rand -base64 32)\" ``` ### Helm Chart Deployment Add the CloudFlow Helm repository: ```bash helm repo add cloudflow https://charts.cloudflow.io helm repo update ``` Create a values file for production configuration: ```yaml # values-production.yaml replicaCount: 3 image: repository: 123456789012.dkr.ecr.us-east-1.amazonaws.com/cloudflow tag: \"2.4.0\" pullPolicy: IfNotPresent resources: limits: cpu: 2000m memory: 4Gi requests: cpu: 2000m memory: 4Gi autoscaling: enabled: true minReplicas: 3 maxReplicas: 10 targetCPUUtilizationPercentage: 70 targetMemoryUtilizationPercentage: 80 service: type: ClusterIP port: 80 targetPort: 3000 annotations: service.beta.kubernetes.io/aws-load-balancer-type: \"nlb\" ingress: enabled: true className: nginx annotations: cert-manager.io/cluster-issuer: \"letsencrypt-prod\" nginx.ingress.kubernetes.io/ssl-redirect: \"true\" hosts: - host: api.cloudflow.io paths: - path: / pathType: Prefix tls:",
              "score": null
            },
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_5",
              "content": "Configure log aggregation: ```bash # Install Fluent Bit for log forwarding helm install fluent-bit fluent/fluent-bit \\ --namespace logging \\ --create-namespace \\ --set cloudWatch.enabled=true \\ --set cloudWatch.region=us-east-1 \\ --set cloudWatch.logGroupName=/aws/eks/cloudflow-production/application ``` Query logs using CloudWatch Insights: ```sql fields @timestamp, @message, level, requestId, userId | filter namespace = \"cloudflow-prod\" | filter level = \"error\" | sort @timestamp desc | limit 100 ``` --- ## Backup and Disaster Recovery ### Database Backup Strategy CloudFlow implements a comprehensive backup strategy with the following retention policy: - **Daily snapshots**: Retained for 30 days - **Weekly backups**: Retained for 90 days - **Monthly backups**: Retained for 1 year #### Automated Backup with Velero Install Velero for cluster-wide backups: ```bash velero install \\ --provider aws \\ --plugins velero/velero-plugin-for-aws:v1.8.0 \\ --bucket cloudflow-velero-backups \\ --backup-location-config region=us-east-1 \\ --snapshot-location-config region=us-east-1 \\ --secret-file ./credentials-velero ``` Create a daily backup schedule: ```yaml # backup-schedule.yaml apiVersion: velero.io/v1 kind: Schedule metadata: name: cloudflow-daily-backup namespace: velero spec: schedule: \"0 2 * * *\" # 2 AM UTC daily template: includedNamespaces: - cloudflow-prod ttl: 720h0m0s # 30 days storageLocation: default snapshotVolumes: true ``` Apply the backup schedule: ```bash kubectl apply -f backup-schedule.yaml ``` #### PostgreSQL Backup Configure PostgreSQL continuous archiving with WAL-G: ```bash # Create backup cronjob kubectl apply -f - <<EOF apiVersion: batch/v1 kind: CronJob metadata: name: postgres-backup namespace: cloudflow-prod spec: schedule: \"0 1 * * *\" # 1 AM UTC daily jobTemplate: spec: template: spec: containers: - name: backup image: postgres:14 env: - name: PGPASSWORD valueFrom: secretKeyRef: name: postgres-credentials key: password command: - /bin/bash - -c - | TIMESTAMP=\\$(date +%Y%m%d_%H%M%S) pg_dump -h postgresql.cloudflow-prod.svc.cluster.local \\ -U cloudflow -d cloudflow \\ | gzip > /backups/cloudflow_\\${TIMESTAMP}.sql.gz aws s3 cp /backups/cloudflow_\\${TIMESTAMP}.sql.gz \\ s3://cloudflow-db-backups/daily/ volumeMounts: - name: backup-storage mountPath: /backups volumes: - name: backup-storage emptyDir: {} restartPolicy: OnFailure EOF ``` ### Disaster Recovery Procedure In case of catastrophic failure, follow these steps to restore CloudFlow: 1. **Restore EKS Cluster** (if necessary): ```bash eksctl create cluster -f cluster-config.yaml ``` 2. **Restore Velero Backup**: ```bash # List available backups velero backup get # Restore from backup velero restore create --from-backup cloudflow-daily-backup-20260124 ``` 3. **Restore Database**: ```bash # Download backup from S3 aws s3 cp s3://cloudflow-db-backups/daily/cloudflow_20260124_010000.sql.gz . # Restore database gunzip -c cloudflow_20260124_010000.sql.gz | \\ psql -h postgresql.cloudflow-prod.svc.cluster.local -U cloudflow -d cloudflow ``` 4.",
              "score": null
            }
          ],
          "latency_ms": 13.01463400159264,
          "query_id": "realistic_019",
          "dimension": "contextual",
          "query": "Setting up deployment pipeline, which Helm repository has CloudFlow charts?"
        },
        {
          "key_facts": [
            "helm repo add cloudflow https://charts.cloudflow.io"
          ],
          "found_facts": [
            "helm repo add cloudflow https://charts.cloudflow.io"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "deployment_guide"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_1",
              "content": "create cluster -f cluster-config.yaml ``` ### Storage Configuration Install the EBS CSI driver for persistent volumes: ```bash eksctl create addon --name aws-ebs-csi-driver \\ --cluster cloudflow-production \\ --service-account-role-arn arn:aws:iam::ACCOUNT_ID:role/AmazonEKS_EBS_CSI_DriverRole \\ --force ``` --- ## Kubernetes Deployment ### Namespace Setup Create the CloudFlow namespace and configure resource quotas: ```yaml # namespace.yaml apiVersion: v1 kind: Namespace metadata: name: cloudflow-prod labels: name: cloudflow-prod environment: production --- apiVersion: v1 kind: ResourceQuota metadata: name: cloudflow-quota namespace: cloudflow-prod spec: hard: requests.cpu: \"50\" requests.memory: 100Gi persistentvolumeclaims: \"10\" services.loadbalancers: \"2\" ``` Apply the namespace configuration: ```bash kubectl apply -f namespace.yaml ``` ### Environment Configuration CloudFlow requires the following environment variables: | Variable | Description | Example | Required | |----------|-------------|---------|----------| | `DATABASE_URL` | PostgreSQL connection string | `postgresql://user:pass@host:5432/cloudflow` | Yes | | `REDIS_URL` | Redis connection string | `redis://redis-master.cloudflow-prod.svc.cluster.local:6379` | Yes | | `JWT_SECRET` | Secret key for JWT token signing | `<generated-secret-256-bit>` | Yes | | `LOG_LEVEL` | Application log verbosity | `info`, `debug`, `warn`, `error` | No (default: `info`) | | `API_PORT` | API server port | `3000` | No (default: `3000`) | | `WORKER_CONCURRENCY` | Number of concurrent workers | `10` | No (default: `5`) | | `SESSION_SECRET` | Session encryption key | `<generated-secret-256-bit>` | Yes | | `AWS_REGION` | AWS region for services | `us-east-1` | Yes | | `S3_BUCKET` | S3 bucket for file storage | `cloudflow-prod-storage` | Yes | | `SMTP_HOST` | Email server hostname | `smtp.sendgrid.net` | No | | `SMTP_PORT` | Email server port | `587` | No | | `METRICS_ENABLED` | Enable Prometheus metrics | `true` | No (default: `false`) | Create a Kubernetes secret for sensitive variables: ```bash kubectl create secret generic cloudflow-secrets \\ --namespace cloudflow-prod \\ --from-literal=DATABASE_URL=\"postgresql://cloudflow:$(cat db-password.txt)@pgbouncer.cloudflow-prod.svc.cluster.local:5432/cloudflow\" \\ --from-literal=REDIS_URL=\"redis://redis-master.cloudflow-prod.svc.cluster.local:6379\" \\ --from-literal=JWT_SECRET=\"$(openssl rand -base64 32)\" \\ --from-literal=SESSION_SECRET=\"$(openssl rand -base64 32)\" ``` ### Helm Chart Deployment Add the CloudFlow Helm repository: ```bash helm repo add cloudflow https://charts.cloudflow.io helm repo update ``` Create a values file for production configuration: ```yaml # values-production.yaml replicaCount: 3 image: repository: 123456789012.dkr.ecr.us-east-1.amazonaws.com/cloudflow tag: \"2.4.0\" pullPolicy: IfNotPresent resources: limits: cpu: 2000m memory: 4Gi requests: cpu: 2000m memory: 4Gi autoscaling: enabled: true minReplicas: 3 maxReplicas: 10 targetCPUUtilizationPercentage: 70 targetMemoryUtilizationPercentage: 80 service: type: ClusterIP port: 80 targetPort: 3000 annotations: service.beta.kubernetes.io/aws-load-balancer-type: \"nlb\" ingress: enabled: true className: nginx annotations: cert-manager.io/cluster-issuer: \"letsencrypt-prod\" nginx.ingress.kubernetes.io/ssl-redirect: \"true\" hosts: - host: api.cloudflow.io paths: - path: / pathType: Prefix tls:",
              "score": null
            },
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_2",
              "content": "- secretName: cloudflow-tls hosts: - api.cloudflow.io healthCheck: enabled: true livenessProbe: path: /health initialDelaySeconds: 30 periodSeconds: 10 timeoutSeconds: 5 failureThreshold: 3 readinessProbe: path: /ready initialDelaySeconds: 10 periodSeconds: 5 timeoutSeconds: 3 failureThreshold: 3 env: - name: NODE_ENV value: \"production\" - name: LOG_LEVEL value: \"info\" - name: API_PORT value: \"3000\" - name: WORKER_CONCURRENCY value: \"10\" - name: AWS_REGION value: \"us-east-1\" - name: METRICS_ENABLED value: \"true\" envFrom: - secretRef: name: cloudflow-secrets podAnnotations: prometheus.io/scrape: \"true\" prometheus.io/port: \"9090\" prometheus.io/path: \"/metrics\" podSecurityContext: runAsNonRoot: true runAsUser: 1000 fsGroup: 1000 securityContext: allowPrivilegeEscalation: false capabilities: drop: - ALL readOnlyRootFilesystem: true persistence: enabled: true storageClass: gp3 accessMode: ReadWriteOnce size: 50Gi redis: enabled: true architecture: replication auth: enabled: true master: persistence: size: 20Gi replica: replicaCount: 2 persistence: size: 20Gi ``` Deploy CloudFlow using Helm: ```bash helm install cloudflow cloudflow/cloudflow \\ --namespace cloudflow-prod \\ --values values-production.yaml \\ --wait \\ --timeout 10m ``` ### Deployment Verification Verify the deployment status: ```bash # Check pod status kubectl get pods -n cloudflow-prod # Expected output: # NAME READY STATUS RESTARTS AGE # cloudflow-api-7d8f9c5b6d-4xk2p 1/1 Running 0 2m # cloudflow-api-7d8f9c5b6d-9hj5m 1/1 Running 0 2m # cloudflow-api-7d8f9c5b6d-tn8wq 1/1 Running 0 2m # Check service endpoints kubectl get svc -n cloudflow-prod # Check ingress kubectl get ingress -n cloudflow-prod ``` Test health endpoints: ```bash # Port forward for testing kubectl port-forward -n cloudflow-prod svc/cloudflow-api 8080:80 # Test health endpoint curl http://localhost:8080/health # Expected: {\"status\":\"healthy\",\"timestamp\":\"2026-01-24T10:30:00Z\"} # Test readiness endpoint curl http://localhost:8080/ready # Expected: {\"status\":\"ready\",\"dependencies\":{\"database\":\"connected\",\"redis\":\"connected\"}} ``` --- ## Database Configuration ### PostgreSQL Setup CloudFlow uses PostgreSQL 14 as its primary data store.",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_8",
              "content": "Leverage caching:** ```bash # Enable client-side caching export CLOUDFLOW_CACHE_ENABLED=true export CLOUDFLOW_CACHE_TTL=300 # Cache workflow metadata cloudflow workflows list --use-cache --cache-ttl 600 ``` --- ## Log Analysis & Debugging ### Accessing CloudFlow Logs #### Kubernetes Deployments ```bash # List all CloudFlow pods kubectl get pods -n cloudflow # Tail logs from API server kubectl logs -f -n cloudflow deployment/cloudflow-api --tail=100 # Get logs from specific pod kubectl logs -n cloudflow cloudflow-api-7d4f6b8c9d-x7k2m # Get logs from previous crashed pod kubectl logs -n cloudflow cloudflow-api-7d4f6b8c9d-x7k2m --previous # Get logs from all pods in deployment kubectl logs -n cloudflow deployment/cloudflow-api --all-containers=true # Stream logs from multiple pods kubectl logs -n cloudflow -l app=cloudflow-api -f --max-log-requests=10 ``` #### Log Levels CloudFlow supports the following log levels: - `TRACE` - Very detailed debugging information - `DEBUG` - Detailed debugging information - `INFO` - Informational messages (default) - `WARN` - Warning messages - `ERROR` - Error messages - `FATAL` - Fatal errors causing shutdown **Changing log levels:** ```bash # Set global log level cloudflow config set logging.level DEBUG # Set log level for specific component cloudflow config set logging.components.database DEBUG cloudflow config set logging.components.auth INFO cloudflow config set logging.components.workflows TRACE # Temporary log level increase (resets after 1 hour) cloudflow debug set-log-level DEBUG --duration 3600 # View current log configuration cloudflow config get logging --format json ``` ### Grep Patterns for Common Issues #### Authentication Failures ```bash # Find all authentication errors kubectl logs -n cloudflow deployment/cloudflow-api | \\ grep -i \"authentication\\|401\\|unauthorized\" | \\ tail -n 50 # Find JWT validation failures kubectl logs -n cloudflow deployment/cloudflow-api | \\ grep -E \"JWT|token.*invalid|signature.*failed\" # Find clock skew issues kubectl logs -n cloudflow deployment/cloudflow-api | \\ grep -i \"clock skew\\|time.*sync\\|nbf\\|exp\" ``` #### Database Errors ```bash # Find database connection errors kubectl logs -n cloudflow deployment/cloudflow-api | \\ grep -E \"connection.*pool|could not connect|database.*timeout\" # Find slow queries kubectl logs -n cloudflow deployment/cloudflow-api | \\ grep \"slow query\" | \\ awk '{print $NF}' | \\ sort -n | \\ tail -n 20 # Find deadlock errors kubectl logs -n cloudflow deployment/cloudflow-api | \\ grep -i \"deadlock detected\" ``` #### Workflow Execution Errors ```bash # Find workflow timeout errors kubectl logs -n cloudflow deployment/cloudflow-workflow-engine | \\ grep -E \"timeout|exceeded.*3600\" # Find workflow retry attempts kubectl logs -n cloudflow deployment/cloudflow-workflow-engine | \\ grep -E \"retry attempt [0-9]|retrying",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_9",
              "content": "in\" # Find workflow failures by ID kubectl logs -n cloudflow deployment/cloudflow-workflow-engine | \\ grep \"exec_7h3j6k9m2n\" ``` #### Rate Limiting ```bash # Find rate limit events kubectl logs -n cloudflow deployment/cloudflow-api | \\ grep -E \"429|rate.*limit|throttle\" # Count rate limit errors by hour kubectl logs -n cloudflow deployment/cloudflow-api --since=24h | \\ grep \"rate_limit_exceeded\" | \\ awk '{print $1}' | \\ cut -d'T' -f1-2 | \\ sort | uniq -c ``` ### Advanced Log Analysis #### Using jq for JSON Logs ```bash # Parse JSON logs and filter by level kubectl logs -n cloudflow deployment/cloudflow-api | \\ jq 'select(.level == \"ERROR\")' # Extract specific fields kubectl logs -n cloudflow deployment/cloudflow-api | \\ jq '{timestamp: .timestamp, level: .level, message: .message, execution_id: .context.execution_id}' # Filter by workflow ID kubectl logs -n cloudflow deployment/cloudflow-workflow-engine | \\ jq 'select(.workflow_id == \"wf_9k2n4m8p1q\")' # Count errors by type kubectl logs -n cloudflow deployment/cloudflow-api --since=1h | \\ jq -r 'select(.level == \"ERROR\") | .error_type' | \\ sort | uniq -c | sort -rn ``` #### Correlation IDs CloudFlow uses correlation IDs to trace requests across services. ```bash # Extract correlation ID from error CORRELATION_ID=\"corr_8h4j9k2m5n\" # Trace request across all services for pod in $(kubectl get pods -n cloudflow -l tier=backend -o name); do echo \"=== $pod ===\" kubectl logs -n cloudflow $pod | grep $CORRELATION_ID done # Export full trace to file cloudflow debug trace $CORRELATION_ID --output trace-$CORRELATION_ID.json ``` ### Debugging Commands #### Enable Debug Mode for Workflow Execution ```bash # Execute workflow with debug logging cloudflow workflows execute wf_9k2n4m8p1q \\ --debug \\ --log-level TRACE \\ --output-logs /tmp/workflow-debug.log # Enable step-by-step execution cloudflow workflows execute wf_9k2n4m8p1q \\ --step-mode interactive \\ --breakpoint-on-error # Capture full execution context cloudflow workflows execute wf_9k2n4m8p1q \\ --capture-context \\ --context-output /tmp/execution-context.json ``` #### Database Query Debugging ```bash # Enable query logging cloudflow db config set log_statement all cloudflow db config set log_duration on cloudflow db config set log_min_duration_statement 1000 # Log queries > 1s # Capture query plan for slow endpoint cloudflow debug capture-queries \\ --endpoint \"/api/v1/workflows/list\" \\ --duration 60 \\ --output query-analysis.txt # Analyze query performance cloudflow db analyze-performance --last 1h ``` #### Network Debugging ```bash # Test connectivity from CloudFlow pod kubectl run -n cloudflow netdebug --rm -i --tty \\ --image=nicolaka/netshoot -- /bin/bash # Inside pod: # Check DNS resolution nslookup api.cloudflow.io # Check connectivity to",
              "score": null
            },
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_7",
              "content": "podSelector: matchLabels: app: cloudflow policyTypes: - Ingress - Egress ingress: - from: - namespaceSelector: matchLabels: name: cloudflow-prod - podSelector: matchLabels: app: nginx-ingress ports: - protocol: TCP port: 3000 egress: - to: - podSelector: matchLabels: app: postgresql ports: - protocol: TCP port: 5432 - to: - podSelector: matchLabels: app: redis ports: - protocol: TCP port: 6379 ``` ### Pod Security Standards Enforce Pod Security Standards at the namespace level: ```bash kubectl label namespace cloudflow-prod \\ pod-security.kubernetes.io/enforce=restricted \\ pod-security.kubernetes.io/audit=restricted \\ pod-security.kubernetes.io/warn=restricted ``` ### Secrets Management Use AWS Secrets Manager for sensitive credentials: ```bash # Install External Secrets Operator helm install external-secrets external-secrets/external-secrets \\ --namespace external-secrets-system \\ --create-namespace # Create SecretStore kubectl apply -f - <<EOF apiVersion: external-secrets.io/v1beta1 kind: SecretStore metadata: name: aws-secrets-manager namespace: cloudflow-prod spec: provider: aws: service: SecretsManager region: us-east-1 auth: jwt: serviceAccountRef: name: external-secrets-sa EOF ``` --- ## Troubleshooting ### Common Issues #### Pods in CrashLoopBackOff **Symptoms**: Pods continuously restart **Diagnosis**: ```bash kubectl logs -n cloudflow-prod <pod-name> --previous kubectl describe pod -n cloudflow-prod <pod-name> ``` **Common Causes**: - Database connection failure - Invalid environment variables - Insufficient resources #### High Memory Usage **Symptoms**: Pods being OOMKilled **Diagnosis**: ```bash kubectl top pods -n cloudflow-prod ``` **Resolution**: - Increase memory limits in deployment - Check for memory leaks in application logs - Review heap dump for Node.js processes #### Database Connection Pool Exhausted **Symptoms**: \"Too many connections\" errors **Diagnosis**: ```bash # Check PgBouncer stats kubectl exec -it pgbouncer-xxx -n cloudflow-prod -- \\ psql -p 5432 -U pgbouncer pgbouncer -c \"SHOW POOLS;\" ``` **Resolution**: - Increase `max_db_connections` in PgBouncer - Optimize application connection usage - Add more PostgreSQL replicas ### Support Contacts - **On-call Engineer**: pagerduty.com/cloudflow-oncall - **Slack Channel**: #cloudflow-operations - **Documentation**: https://docs.cloudflow.io - **Status Page**: https://status.cloudflow.io --- ## Appendix ### Useful Commands ```bash # View all resources in namespace kubectl get all -n cloudflow-prod # Check resource usage kubectl top pods -n cloudflow-prod kubectl top nodes # View logs kubectl logs -f deployment/cloudflow-api -n cloudflow-prod # Execute command in pod kubectl exec -it <pod-name> -n cloudflow-prod -- /bin/sh # Port forward for local testing kubectl port-forward svc/cloudflow-api 8080:80 -n cloudflow-prod # Rollback deployment helm rollback cloudflow -n cloudflow-prod # Update deployment helm upgrade cloudflow cloudflow/cloudflow \\ --namespace cloudflow-prod \\ --values values-production.yaml ``` ### Maintenance Windows Scheduled maintenance occurs during the following windows: - **Primary**: Sunday 02:00-06:00",
              "score": null
            }
          ],
          "latency_ms": 12.954301997524453,
          "query_id": "realistic_019",
          "dimension": "negation",
          "query": "Why can't I find CloudFlow in the official Helm hub?"
        },
        {
          "key_facts": [
            "minimum scheduling interval is 1 minute",
            "The minimum scheduling interval is **1 minute**"
          ],
          "found_facts": [
            "The minimum scheduling interval is **1 minute**"
          ],
          "missed_facts": [
            "minimum scheduling interval is 1 minute"
          ],
          "coverage": 0.5,
          "expected_docs": [
            "user_guide"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "user_guide",
              "chunk_id": "user_guide_fix_4",
              "content": "### Cron Syntax Use standard cron expressions to define schedules: ``` * * * * * \u252c \u252c \u252c \u252c \u252c \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2514\u2500\u2500\u2500 Day of Week (0-6, Sunday=0) \u2502 \u2502 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Month (1-12) \u2502 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Day of Month (1-31) \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Hour (0-23) \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Minute (0-59) ``` **Common Cron Patterns:** | Pattern | Description | |---------|-------------| | `*/5 * * * *` | Every 5 minutes | | `0 * * * *` | Every hour at minute 0 | | `0 9 * * *` | Daily at 9:00 AM | | `0 9 * * 1` | Every Monday at 9:00 AM | | `0 0 1 * *` | First day of every month at midnight | | `0 0 * * 0` | Every Sunday at midnight | | `0 9-17 * * 1-5` | Every hour from 9 AM to 5 PM, Monday-Friday | | `*/15 9-17 * * 1-5` | Every 15 minutes during business hours | **Important:** The minimum scheduling interval is **1 minute**. Expressions that evaluate to more frequent executions will be rejected. ### Timezone Handling All scheduled workflows run in **UTC by default**. To account for your local timezone: **Option 1: Convert to UTC** If you want a workflow to run at 9:00 AM EST (UTC-5), schedule it for 14:00 UTC: ``` 0 14 * * * # 9:00 AM EST = 14:00 UTC ``` **Option 2: Use Timezone Configuration** Specify a timezone in your workflow configuration: ```yaml schedule: cron: \"0 9 * * *\" timezone: \"America/New_York\" # IANA timezone identifier ``` **Supported Timezones:** CloudFlow supports all IANA timezone identifiers, including: - `America/New_York` (Eastern Time) - `America/Chicago` (Central Time) - `America/Los_Angeles` (Pacific Time) - `Europe/London` (GMT/BST) - `Asia/Tokyo` (Japan Standard Time) - `Australia/Sydney` (Australian Eastern Time) **Daylight Saving Time:** When using timezone configuration, CloudFlow automatically handles DST transitions. A workflow scheduled for 9:00 AM will run at 9:00 AM local time regardless of DST changes. ### Schedule Management **Creating a Schedule:** 1. Open your workflow in the editor 2. Click the **\"Trigger\"** section 3. Select **\"Schedule\"** as the trigger type 4. Enter your cron expression or use the visual schedule builder 5. Select your timezone 6.",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_2",
              "content": "Review Query Execution Plans** ```sql -- Connect to CloudFlow database cloudflow db connect --readonly -- Explain slow query EXPLAIN ANALYZE SELECT w.*, e.status, e.error_message FROM workflows w LEFT JOIN executions e ON w.id = e.workflow_id WHERE w.workspace_id = 'ws_abc123' AND e.created_at > NOW() - INTERVAL '7 days' ORDER BY e.created_at DESC; -- Check for missing indexes SELECT schemaname, tablename, indexname, indexdef FROM pg_indexes WHERE tablename IN ('workflows', 'executions', 'workflow_steps') ORDER BY tablename, indexname; ``` **3. Optimize Queries** Common optimization techniques: **Add missing indexes:** ```sql -- Index for workflow lookup by workspace CREATE INDEX CONCURRENTLY idx_workflows_workspace_created ON workflows(workspace_id, created_at DESC); -- Index for execution status queries CREATE INDEX CONCURRENTLY idx_executions_status_created ON executions(workflow_id, status, created_at DESC) WHERE status IN ('running', 'pending'); -- Composite index for common filter combinations CREATE INDEX CONCURRENTLY idx_executions_workspace_status ON executions(workspace_id, status, created_at) INCLUDE (error_message, retry_count); ``` **Use query result caching:** ```bash # Enable query cache for workspace metadata cloudflow config set cache.workspace.ttl 3600 # Configure Redis cache backend cloudflow config set cache.backend redis cloudflow config set cache.redis.host redis.cloudflow.svc.cluster.local cloudflow config set cache.redis.port 6379 ``` ### High API Latency #### Latency Breakdown Analysis ```bash # Generate latency report cloudflow metrics latency-report --endpoint \"/api/v1/workflows\" --last 24h # Sample output: # Endpoint: POST /api/v1/workflows/execute # P50: 245ms | P95: 1823ms | P99: 4521ms # Breakdown: # - Auth: 45ms (18%) # - DB Query: 156ms (64%) # - Business Logic: 32ms (13%) # - Response Serialization: 12ms (5%) ``` **Network latency issues:** ```bash # Test connectivity to CloudFlow API time curl -w \"@curl-format.txt\" -o /dev/null -s https://api.cloudflow.io/health # Create curl-format.txt: cat > curl-format.txt << EOF time_namelookup: %{time_namelookup}s\\n time_connect: %{time_connect}s\\n time_appconnect: %{time_appconnect}s\\n time_pretransfer: %{time_pretransfer}s\\n time_redirect: %{time_redirect}s\\n time_starttransfer: %{time_starttransfer}s\\n ----------\\n time_total: %{time_total}s\\n EOF # Trace route to API endpoint traceroute api.cloudflow.io # Check DNS resolution time dig api.cloudflow.io | grep \"Query time\" ``` ### Memory Leaks #### Detection ```bash # Monitor CloudFlow service memory usage kubectl top pods -n cloudflow --sort-by=memory # Get detailed memory metrics for specific pod kubectl exec -n cloudflow deploy/cloudflow-api -- \\ curl localhost:9090/metrics | grep memory # Check for OOMKilled pods kubectl get pods -n cloudflow --field-selector=status.phase=Failed | \\ grep OOMKilled # Review memory limits and requests kubectl describe deployment cloudflow-api -n cloudflow | \\ grep -A 5 \"Limits\\|Requests\" ``` #### Common Causes **1.",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_0",
              "content": "# CloudFlow Platform - System Architecture Overview **Document Version:** 2.3.1 **Last Updated:** January 15, 2026 **Owner:** Platform Architecture Team **Status:** Production ## Executive Summary CloudFlow is a distributed, cloud-native workflow automation platform designed to orchestrate complex business processes at scale. The platform processes over 2.5 million workflow executions daily with an average P99 latency of 180ms for API operations and 4.2 seconds for workflow execution. This document provides a comprehensive overview of the system architecture, including microservices design, data flow patterns, infrastructure decisions, and operational considerations. ## Table of Contents 1. [High-Level Architecture](#high-level-architecture) 2. [Microservices Breakdown](#microservices-breakdown) 3. [Data Flow Architecture](#data-flow-architecture) 4. [Database Architecture](#database-architecture) 5. [Message Queue Patterns](#message-queue-patterns) 6. [Caching Strategy](#caching-strategy) 7. [Security Architecture](#security-architecture) 8. [Performance Characteristics](#performance-characteristics) 9. [Disaster Recovery](#disaster-recovery) --- ## High-Level Architecture CloudFlow follows a microservices architecture pattern deployed across multiple availability zones in AWS. The platform is designed with the following core principles: - **Scalability**: Horizontal scaling for all services with auto-scaling groups - **Resilience**: Circuit breakers, bulkheads, and graceful degradation - **Observability**: Distributed tracing, centralized logging, and comprehensive metrics - **Security**: Zero-trust network architecture with mTLS encryption ### Architecture Diagram (Conceptual) ``` \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 Load Balancer (ALB) \u2502 \u2502 (TLS Termination - 443) \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u25bc \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 API Gateway Layer \u2502 \u2502 (Rate Limiting, Auth, Request Routing) \u2502 \u2514\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u2502 \u2502 \u2502 \u2502 \u25bc \u25bc \u25bc \u25bc \u25bc \u250c\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502Auth\u2502 \u2502Workflow\u2502 \u2502Scheduler\u2502 \u2502Notification\u2502 \u2502User Service\u2502 \u2502Svc \u2502 \u2502 Engine \u2502 \u2502 Service \u2502 \u2502 Service \u2502 \u2502 \u2502 \u2514\u2500\u252c\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u2502 \u2502 \u2502 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 \u2502 \u2502 \u25bc \u25bc \u25bc \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502PostgreSQL\u2502 \u2502 Redis \u2502 \u2502 Kafka \u2502 \u2502 Cluster \u2502 \u2502 Cluster \u2502 \u2502 Cluster \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 ``` ### Technology Stack - **Runtime**: Node.js 20.x (API Gateway, Workflow Engine), Go 1.21 (Auth Service, Scheduler) - **Container Orchestration**: Kubernetes 1.28 (EKS) - **Service Mesh**: Istio 1.20 for service-to-service communication - **Databases**: PostgreSQL 15.4, Redis 7.2 - **Message Broker**: Apache Kafka 3.6 - **Monitoring**: Prometheus, Grafana, Jaeger for distributed tracing - **Secrets Management**: HashiCorp Vault 1.15 --- ## Microservices Breakdown ### API Gateway **Purpose**: Single entry point for all client requests, providing authentication, rate limiting, request routing, and protocol translation.",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_3",
              "content": "**Technology**: Go with distributed locking via Redis **Replicas**: 4 pods (production), active-passive with leader election **Resource Allocation**: 2 vCPU, 4GB RAM per pod **Key Responsibilities**: - Parse and validate cron expressions (extended format supporting seconds) - Maintain schedule registry in PostgreSQL - Distributed scheduling with leader election (one active scheduler) - Missed execution handling with configurable catch-up policy - Schedule conflict detection and resolution - Time zone support for international schedules **Scheduling Architecture**: ``` PostgreSQL Schedules Table \u2502 \u25bc Leader Scheduler (elected via Redis) \u2502 \u251c\u2500 Scan for due schedules (every 10 seconds) \u251c\u2500 Acquire lock per schedule (prevents duplicates) \u251c\u2500 Publish to Kafka \u2192 workflow.events topic \u2514\u2500 Update last_run timestamp ``` **Schedule Types**: - **Cron-based**: `0 */5 * * * *` (every 5 minutes) - **One-time**: Specific timestamp for delayed execution - **Interval-based**: Every N seconds/minutes/hours **Reliability Features**: - Leader election using Redis with 30-second lease - Heartbeat mechanism to detect leader failure (5-second interval) - Automatic failover to standby scheduler (< 10 seconds) - Schedule versioning to handle updates during execution - Missed execution policy: SKIP, RUN_ONCE, or RUN_ALL **Performance Targets**: - Schedule evaluation: < 100ms per cycle - Accuracy: \u00b1 1 second for schedule triggers - Capacity: 100,000 active schedules --- ### Notification Service **Purpose**: Multi-channel notification delivery system supporting email, SMS, webhooks, and in-app notifications.",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_7",
              "content": "records: 500 - Session timeout: 30 seconds --- ## Message Queue Patterns ### Pub/Sub Pattern Used for broadcasting events to multiple interested consumers: ``` Workflow Engine publishes workflow.execution.completed \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u25bc \u25bc \u25bc \u25bc Analytics Notification Audit Logger Billing Service Service Service Service ``` Each service maintains its own consumer group and processes events independently. Failures in one consumer don't affect others. ### Request-Reply Pattern Synchronous communication over async messaging (used sparingly): ``` API Gateway publishes request \u2192 Kafka (reply-to: temp-queue-123) \u2502 \u25bc Service processes request \u2502 \u25bc Service publishes response \u2192 temp-queue-123 \u2502 \u25bc API Gateway receives response ``` Timeout: 5 seconds, fallback to direct HTTP call if no response. ### Saga Pattern Distributed transaction management for multi-service workflows: ``` Step 1: Create Order \u2192 SUCCESS \u2192 Step 2: Reserve Inventory \u2502 FAILURE \u2502 \u25bc Compensating Transaction \u2502 \u25bc Cancel Order (Rollback) ``` Orchestrated by Workflow Engine with compensation logic defined in workflow DSL. ### Dead Letter Queue (DLQ) Failed messages after all retry attempts are routed to DLQ: ``` Message processing fails (3 retries with exponential backoff) \u2502 \u25bc Publish to dead-letter-queue topic \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u25bc \u25bc Alert to PagerDuty Store in PostgreSQL for analysis \u2502 \u2502 \u25bc \u25bc Manual investigation Automated pattern detection ``` DLQ Processing SLA: < 4 hours for critical events, < 24 hours for non-critical. --- ## Caching Strategy ### Cache-Aside Pattern (Lazy Loading) Primary caching pattern used across all services: ``` 1. Application checks cache (Redis GET) 2. If HIT \u2192 Return cached data 3. If MISS: a. Query database (PostgreSQL) b. Store result in cache with TTL c. Return data to application ``` **Implementation Example** (Workflow Definition): ```javascript async function getWorkflowDefinition(workflowId) { const cacheKey = `workflow:def:${workflowId}`; // Step 1: Check cache let workflow = await redis.get(cacheKey); if (workflow) { metrics.increment('cache.hit.workflow_def'); return JSON.parse(workflow); } // Step 2: Cache miss - query database metrics.increment('cache.miss.workflow_def'); workflow = await db.query( 'SELECT * FROM workflows WHERE id = $1', [workflowId] ); // Step 3: Store in cache (1 hour TTL) await redis.setex(cacheKey, 3600, JSON.stringify(workflow)); return workflow; } ``` ### Write-Through Pattern Used for critical data where cache consistency is paramount: ``` 1. Application writes to database 2. Database transaction commits 3. Application updates cache 4. Return success to client ``` Applied to: User profiles, authentication sessions, system configuration.",
              "score": null
            }
          ],
          "latency_ms": 12.992002000828506,
          "query_id": "realistic_020",
          "dimension": "original",
          "query": "What is the minimum scheduling interval for workflows?"
        },
        {
          "key_facts": [
            "minimum scheduling interval is 1 minute",
            "The minimum scheduling interval is **1 minute**"
          ],
          "found_facts": [
            "The minimum scheduling interval is **1 minute**"
          ],
          "missed_facts": [
            "minimum scheduling interval is 1 minute"
          ],
          "coverage": 0.5,
          "expected_docs": [
            "user_guide"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "user_guide",
              "chunk_id": "user_guide_fix_5",
              "content": "Save and activate **Testing Schedules:** Use the built-in schedule calculator to preview upcoming executions: ``` Next 5 executions: 1. 2026-01-24 14:00:00 UTC 2. 2026-01-25 14:00:00 UTC 3. 2026-01-26 14:00:00 UTC 4. 2026-01-27 14:00:00 UTC 5. 2026-01-28 14:00:00 UTC ``` ## Error Handling Robust error handling ensures your workflows are resilient and reliable. ### Retry Policies Configure automatic retries for failed actions: ```yaml - id: api_call action: http_request config: url: \"https://api.example.com/data\" retry: max_attempts: 3 backoff_type: \"exponential\" # or \"fixed\", \"linear\" initial_interval: 1000 # milliseconds max_interval: 30000 multiplier: 2.0 retry_on: - timeout - network_error - status: [500, 502, 503, 504] ``` **Backoff Strategies:** - **Fixed**: Wait the same amount of time between retries - Attempt 1: Wait 1s - Attempt 2: Wait 1s - Attempt 3: Wait 1s - **Linear**: Increase wait time by a fixed amount - Attempt 1: Wait 1s - Attempt 2: Wait 2s - Attempt 3: Wait 3s - **Exponential**: Double the wait time with each retry (recommended) - Attempt 1: Wait 1s - Attempt 2: Wait 2s - Attempt 3: Wait 4s **Retry Conditions:** Control which errors trigger retries: - `timeout`: Request timeout - `network_error`: Connection failures - `status`: Specific HTTP status codes - `error_code`: Application-specific error codes ### Fallback Actions Execute alternative actions when the primary action fails: ```yaml - id: primary_payment action: http_request config: url: \"https://primary-payment-gateway.com/charge\" method: POST body: amount: \"{{amount}}\" on_error: - id: fallback_payment action: http_request config: url: \"https://backup-payment-gateway.com/charge\" method: POST body: amount: \"{{amount}}\" - id: notify_admin action: email config: to: \"admin@company.com\" subject: \"Payment Gateway Failure\" body: \"Primary gateway failed, switched to backup\" ``` **Error Object:** Access error details in fallback actions: ``` {{error.message}} # Error message {{error.code}} # Error code {{error.step_id}} # Failed step ID {{error.timestamp}} # When the error occurred {{error.attempts}} # Number of retry attempts ``` ### Dead Letter Queue When all retries and fallbacks fail, CloudFlow can route failed executions to a Dead Letter Queue (DLQ) for manual review: **Enable DLQ:** ```yaml workflow: error_handling: dead_letter_queue: enabled: true retain_days: 30 ``` **DLQ Features:** - View failed executions in the dashboard - Inspect complete execution context and error details - Retry individual executions after fixing issues - Export failed executions for analysis - Set up alerts for DLQ threshold breaches **Accessing the DLQ:** 1. Navigate to **Workflows** > **[Your Workflow]** > **Dead Letter Queue** 2.",
              "score": null
            },
            {
              "doc_id": "user_guide",
              "chunk_id": "user_guide_fix_4",
              "content": "### Cron Syntax Use standard cron expressions to define schedules: ``` * * * * * \u252c \u252c \u252c \u252c \u252c \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2514\u2500\u2500\u2500 Day of Week (0-6, Sunday=0) \u2502 \u2502 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Month (1-12) \u2502 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Day of Month (1-31) \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Hour (0-23) \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Minute (0-59) ``` **Common Cron Patterns:** | Pattern | Description | |---------|-------------| | `*/5 * * * *` | Every 5 minutes | | `0 * * * *` | Every hour at minute 0 | | `0 9 * * *` | Daily at 9:00 AM | | `0 9 * * 1` | Every Monday at 9:00 AM | | `0 0 1 * *` | First day of every month at midnight | | `0 0 * * 0` | Every Sunday at midnight | | `0 9-17 * * 1-5` | Every hour from 9 AM to 5 PM, Monday-Friday | | `*/15 9-17 * * 1-5` | Every 15 minutes during business hours | **Important:** The minimum scheduling interval is **1 minute**. Expressions that evaluate to more frequent executions will be rejected. ### Timezone Handling All scheduled workflows run in **UTC by default**. To account for your local timezone: **Option 1: Convert to UTC** If you want a workflow to run at 9:00 AM EST (UTC-5), schedule it for 14:00 UTC: ``` 0 14 * * * # 9:00 AM EST = 14:00 UTC ``` **Option 2: Use Timezone Configuration** Specify a timezone in your workflow configuration: ```yaml schedule: cron: \"0 9 * * *\" timezone: \"America/New_York\" # IANA timezone identifier ``` **Supported Timezones:** CloudFlow supports all IANA timezone identifiers, including: - `America/New_York` (Eastern Time) - `America/Chicago` (Central Time) - `America/Los_Angeles` (Pacific Time) - `Europe/London` (GMT/BST) - `Asia/Tokyo` (Japan Standard Time) - `Australia/Sydney` (Australian Eastern Time) **Daylight Saving Time:** When using timezone configuration, CloudFlow automatically handles DST transitions. A workflow scheduled for 9:00 AM will run at 9:00 AM local time regardless of DST changes. ### Schedule Management **Creating a Schedule:** 1. Open your workflow in the editor 2. Click the **\"Trigger\"** section 3. Select **\"Schedule\"** as the trigger type 4. Enter your cron expression or use the visual schedule builder 5. Select your timezone 6.",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_3",
              "content": "**Technology**: Go with distributed locking via Redis **Replicas**: 4 pods (production), active-passive with leader election **Resource Allocation**: 2 vCPU, 4GB RAM per pod **Key Responsibilities**: - Parse and validate cron expressions (extended format supporting seconds) - Maintain schedule registry in PostgreSQL - Distributed scheduling with leader election (one active scheduler) - Missed execution handling with configurable catch-up policy - Schedule conflict detection and resolution - Time zone support for international schedules **Scheduling Architecture**: ``` PostgreSQL Schedules Table \u2502 \u25bc Leader Scheduler (elected via Redis) \u2502 \u251c\u2500 Scan for due schedules (every 10 seconds) \u251c\u2500 Acquire lock per schedule (prevents duplicates) \u251c\u2500 Publish to Kafka \u2192 workflow.events topic \u2514\u2500 Update last_run timestamp ``` **Schedule Types**: - **Cron-based**: `0 */5 * * * *` (every 5 minutes) - **One-time**: Specific timestamp for delayed execution - **Interval-based**: Every N seconds/minutes/hours **Reliability Features**: - Leader election using Redis with 30-second lease - Heartbeat mechanism to detect leader failure (5-second interval) - Automatic failover to standby scheduler (< 10 seconds) - Schedule versioning to handle updates during execution - Missed execution policy: SKIP, RUN_ONCE, or RUN_ALL **Performance Targets**: - Schedule evaluation: < 100ms per cycle - Accuracy: \u00b1 1 second for schedule triggers - Capacity: 100,000 active schedules --- ### Notification Service **Purpose**: Multi-channel notification delivery system supporting email, SMS, webhooks, and in-app notifications.",
              "score": null
            },
            {
              "doc_id": "user_guide",
              "chunk_id": "user_guide_fix_6",
              "content": "Filter by error type, date range, or execution ID 3. Click an execution to view full details 4. Click **\"Retry\"** to reprocess with the same input data ### Error Notifications Get notified when workflows fail: ```yaml workflow: notifications: on_failure: - type: email to: \"ops-team@company.com\" - type: slack channel: \"#alerts\" message: \"Workflow {{workflow.name}} failed: {{error.message}}\" on_success_after_retry: - type: slack channel: \"#monitoring\" message: \"Workflow recovered after {{error.attempts}} attempts\" ``` ## Workflow Limits CloudFlow enforces the following limits to ensure platform stability and performance: ### Steps Per Workflow - **Maximum**: 50 steps per workflow - **Recommendation**: Keep workflows focused and modular. If you need more steps, consider splitting into multiple workflows connected via webhooks. ### Execution Timeout - **Default**: 3600 seconds (60 minutes) - **Behavior**: Workflows exceeding this timeout are automatically terminated - **Custom Timeouts**: Enterprise plans can request custom timeout limits **Setting Step-Level Timeouts:** ```yaml - id: long_running_task action: http_request config: url: \"https://api.example.com/process\" timeout: 300 # 5 minutes for this specific step ``` ### Execution Limits - **Maximum**: 1000 executions per day (per workflow) - **Rate Limiting**: 100 concurrent executions per workflow - **Burst Limit**: 10 executions per second **What happens when limits are reached:** - New executions are queued automatically - Webhook triggers return HTTP 429 (Too Many Requests) - Scheduled executions are skipped (logged in audit trail) - Email notifications sent to workflow owner **Monitoring Usage:** View real-time metrics in your workflow dashboard: - Executions today: 847 / 1000 - Average duration: 12.3s - Success rate: 98.2% - Current queue depth: 3 ### Data Limits - **Maximum request/response size**: 10MB per action - **Maximum execution payload**: 50MB total - **Variable value size**: 1MB per variable ### Enterprise Plan Limits Enterprise customers can request increased limits: - Up to 100 steps per workflow - Up to 10,000 executions per day - Up to 7200 second timeout (2 hours) - Priority execution queue - Dedicated capacity allocation Contact sales@cloudflow.io for Enterprise pricing and custom limits. ## Best Practices Follow these best practices to build reliable, maintainable workflows: ### 1. Use Descriptive Names **Good:** - Workflow: \"Sync Customer Data from Salesforce to Database\" - Step: \"validate_customer_email\" **Bad:** - Workflow: \"Workflow 1\" - Step: \"step3\" ### 2.",
              "score": null
            },
            {
              "doc_id": "user_guide",
              "chunk_id": "user_guide_fix_0",
              "content": "# CloudFlow User Guide Welcome to CloudFlow, the modern workflow automation platform that helps you connect your apps, automate repetitive tasks, and build powerful integrations without writing code. ## Table of Contents 1. [Getting Started](#getting-started) 2. [Workflow Creation](#workflow-creation) 3. [Available Actions](#available-actions) 4. [Variables and Expressions](#variables-and-expressions) 5. [Scheduling](#scheduling) 6. [Error Handling](#error-handling) 7. [Workflow Limits](#workflow-limits) 8. [Best Practices](#best-practices) 9. [Common Workflow Patterns](#common-workflow-patterns) ## Getting Started ### Account Setup Creating your CloudFlow account takes just a few minutes: 1. Navigate to https://app.cloudflow.io/signup 2. Enter your email address and create a strong password 3. Verify your email address by clicking the link sent to your inbox 4. Complete your profile by providing your organization name and use case 5. Choose your plan (Free, Professional, or Enterprise) Once your account is created, you'll be redirected to your CloudFlow dashboard where you can start building workflows immediately. ### Your First Workflow Let's create a simple workflow that sends you a Slack notification when a new file is uploaded to your Google Drive: 1. Click the **\"Create Workflow\"** button in the top-right corner 2. Give your workflow a name: \"Google Drive to Slack Notifier\" 3. Select **\"Google Drive - New File\"** as your trigger 4. Authenticate your Google Drive account when prompted 5. Choose the folder you want to monitor 6. Click **\"Add Action\"** and select **\"Slack - Send Message\"** 7. Authenticate your Slack account 8. Select the channel where you want to receive notifications 9. Customize your message using variables: `New file uploaded: {{trigger.file.name}}` 10. Click **\"Save & Activate\"** to enable your workflow Congratulations! Your first workflow is now live. Every time a file is added to the specified Google Drive folder, you'll receive a Slack notification. ## Workflow Creation CloudFlow offers multiple ways to create and manage workflows, giving you the flexibility to work in the way that suits you best. ### Visual Editor The Visual Editor is CloudFlow's drag-and-drop interface for building workflows without code. It's perfect for users who prefer a graphical approach: **Key Features:** - Drag-and-drop action blocks from the sidebar - Visual connections between steps show your workflow logic - Inline configuration for each action - Real-time validation and error checking - Test mode to run your workflow with sample data **Using the Visual Editor:** 1.",
              "score": null
            }
          ],
          "latency_ms": 12.923474998387974,
          "query_id": "realistic_020",
          "dimension": "synonym",
          "query": "How frequently can I schedule a workflow?"
        },
        {
          "key_facts": [
            "minimum scheduling interval is 1 minute",
            "The minimum scheduling interval is **1 minute**"
          ],
          "found_facts": [
            "The minimum scheduling interval is **1 minute**"
          ],
          "missed_facts": [
            "minimum scheduling interval is 1 minute"
          ],
          "coverage": 0.5,
          "expected_docs": [
            "user_guide"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "user_guide",
              "chunk_id": "user_guide_fix_4",
              "content": "### Cron Syntax Use standard cron expressions to define schedules: ``` * * * * * \u252c \u252c \u252c \u252c \u252c \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2514\u2500\u2500\u2500 Day of Week (0-6, Sunday=0) \u2502 \u2502 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Month (1-12) \u2502 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Day of Month (1-31) \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Hour (0-23) \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Minute (0-59) ``` **Common Cron Patterns:** | Pattern | Description | |---------|-------------| | `*/5 * * * *` | Every 5 minutes | | `0 * * * *` | Every hour at minute 0 | | `0 9 * * *` | Daily at 9:00 AM | | `0 9 * * 1` | Every Monday at 9:00 AM | | `0 0 1 * *` | First day of every month at midnight | | `0 0 * * 0` | Every Sunday at midnight | | `0 9-17 * * 1-5` | Every hour from 9 AM to 5 PM, Monday-Friday | | `*/15 9-17 * * 1-5` | Every 15 minutes during business hours | **Important:** The minimum scheduling interval is **1 minute**. Expressions that evaluate to more frequent executions will be rejected. ### Timezone Handling All scheduled workflows run in **UTC by default**. To account for your local timezone: **Option 1: Convert to UTC** If you want a workflow to run at 9:00 AM EST (UTC-5), schedule it for 14:00 UTC: ``` 0 14 * * * # 9:00 AM EST = 14:00 UTC ``` **Option 2: Use Timezone Configuration** Specify a timezone in your workflow configuration: ```yaml schedule: cron: \"0 9 * * *\" timezone: \"America/New_York\" # IANA timezone identifier ``` **Supported Timezones:** CloudFlow supports all IANA timezone identifiers, including: - `America/New_York` (Eastern Time) - `America/Chicago` (Central Time) - `America/Los_Angeles` (Pacific Time) - `Europe/London` (GMT/BST) - `Asia/Tokyo` (Japan Standard Time) - `Australia/Sydney` (Australian Eastern Time) **Daylight Saving Time:** When using timezone configuration, CloudFlow automatically handles DST transitions. A workflow scheduled for 9:00 AM will run at 9:00 AM local time regardless of DST changes. ### Schedule Management **Creating a Schedule:** 1. Open your workflow in the editor 2. Click the **\"Trigger\"** section 3. Select **\"Schedule\"** as the trigger type 4. Enter your cron expression or use the visual schedule builder 5. Select your timezone 6.",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_3",
              "content": "**Technology**: Go with distributed locking via Redis **Replicas**: 4 pods (production), active-passive with leader election **Resource Allocation**: 2 vCPU, 4GB RAM per pod **Key Responsibilities**: - Parse and validate cron expressions (extended format supporting seconds) - Maintain schedule registry in PostgreSQL - Distributed scheduling with leader election (one active scheduler) - Missed execution handling with configurable catch-up policy - Schedule conflict detection and resolution - Time zone support for international schedules **Scheduling Architecture**: ``` PostgreSQL Schedules Table \u2502 \u25bc Leader Scheduler (elected via Redis) \u2502 \u251c\u2500 Scan for due schedules (every 10 seconds) \u251c\u2500 Acquire lock per schedule (prevents duplicates) \u251c\u2500 Publish to Kafka \u2192 workflow.events topic \u2514\u2500 Update last_run timestamp ``` **Schedule Types**: - **Cron-based**: `0 */5 * * * *` (every 5 minutes) - **One-time**: Specific timestamp for delayed execution - **Interval-based**: Every N seconds/minutes/hours **Reliability Features**: - Leader election using Redis with 30-second lease - Heartbeat mechanism to detect leader failure (5-second interval) - Automatic failover to standby scheduler (< 10 seconds) - Schedule versioning to handle updates during execution - Missed execution policy: SKIP, RUN_ONCE, or RUN_ALL **Performance Targets**: - Schedule evaluation: < 100ms per cycle - Accuracy: \u00b1 1 second for schedule triggers - Capacity: 100,000 active schedules --- ### Notification Service **Purpose**: Multi-channel notification delivery system supporting email, SMS, webhooks, and in-app notifications.",
              "score": null
            },
            {
              "doc_id": "user_guide",
              "chunk_id": "user_guide_fix_5",
              "content": "Save and activate **Testing Schedules:** Use the built-in schedule calculator to preview upcoming executions: ``` Next 5 executions: 1. 2026-01-24 14:00:00 UTC 2. 2026-01-25 14:00:00 UTC 3. 2026-01-26 14:00:00 UTC 4. 2026-01-27 14:00:00 UTC 5. 2026-01-28 14:00:00 UTC ``` ## Error Handling Robust error handling ensures your workflows are resilient and reliable. ### Retry Policies Configure automatic retries for failed actions: ```yaml - id: api_call action: http_request config: url: \"https://api.example.com/data\" retry: max_attempts: 3 backoff_type: \"exponential\" # or \"fixed\", \"linear\" initial_interval: 1000 # milliseconds max_interval: 30000 multiplier: 2.0 retry_on: - timeout - network_error - status: [500, 502, 503, 504] ``` **Backoff Strategies:** - **Fixed**: Wait the same amount of time between retries - Attempt 1: Wait 1s - Attempt 2: Wait 1s - Attempt 3: Wait 1s - **Linear**: Increase wait time by a fixed amount - Attempt 1: Wait 1s - Attempt 2: Wait 2s - Attempt 3: Wait 3s - **Exponential**: Double the wait time with each retry (recommended) - Attempt 1: Wait 1s - Attempt 2: Wait 2s - Attempt 3: Wait 4s **Retry Conditions:** Control which errors trigger retries: - `timeout`: Request timeout - `network_error`: Connection failures - `status`: Specific HTTP status codes - `error_code`: Application-specific error codes ### Fallback Actions Execute alternative actions when the primary action fails: ```yaml - id: primary_payment action: http_request config: url: \"https://primary-payment-gateway.com/charge\" method: POST body: amount: \"{{amount}}\" on_error: - id: fallback_payment action: http_request config: url: \"https://backup-payment-gateway.com/charge\" method: POST body: amount: \"{{amount}}\" - id: notify_admin action: email config: to: \"admin@company.com\" subject: \"Payment Gateway Failure\" body: \"Primary gateway failed, switched to backup\" ``` **Error Object:** Access error details in fallback actions: ``` {{error.message}} # Error message {{error.code}} # Error code {{error.step_id}} # Failed step ID {{error.timestamp}} # When the error occurred {{error.attempts}} # Number of retry attempts ``` ### Dead Letter Queue When all retries and fallbacks fail, CloudFlow can route failed executions to a Dead Letter Queue (DLQ) for manual review: **Enable DLQ:** ```yaml workflow: error_handling: dead_letter_queue: enabled: true retain_days: 30 ``` **DLQ Features:** - View failed executions in the dashboard - Inspect complete execution context and error details - Retry individual executions after fixing issues - Export failed executions for analysis - Set up alerts for DLQ threshold breaches **Accessing the DLQ:** 1. Navigate to **Workflows** > **[Your Workflow]** > **Dead Letter Queue** 2.",
              "score": null
            },
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_5",
              "content": "Configure log aggregation: ```bash # Install Fluent Bit for log forwarding helm install fluent-bit fluent/fluent-bit \\ --namespace logging \\ --create-namespace \\ --set cloudWatch.enabled=true \\ --set cloudWatch.region=us-east-1 \\ --set cloudWatch.logGroupName=/aws/eks/cloudflow-production/application ``` Query logs using CloudWatch Insights: ```sql fields @timestamp, @message, level, requestId, userId | filter namespace = \"cloudflow-prod\" | filter level = \"error\" | sort @timestamp desc | limit 100 ``` --- ## Backup and Disaster Recovery ### Database Backup Strategy CloudFlow implements a comprehensive backup strategy with the following retention policy: - **Daily snapshots**: Retained for 30 days - **Weekly backups**: Retained for 90 days - **Monthly backups**: Retained for 1 year #### Automated Backup with Velero Install Velero for cluster-wide backups: ```bash velero install \\ --provider aws \\ --plugins velero/velero-plugin-for-aws:v1.8.0 \\ --bucket cloudflow-velero-backups \\ --backup-location-config region=us-east-1 \\ --snapshot-location-config region=us-east-1 \\ --secret-file ./credentials-velero ``` Create a daily backup schedule: ```yaml # backup-schedule.yaml apiVersion: velero.io/v1 kind: Schedule metadata: name: cloudflow-daily-backup namespace: velero spec: schedule: \"0 2 * * *\" # 2 AM UTC daily template: includedNamespaces: - cloudflow-prod ttl: 720h0m0s # 30 days storageLocation: default snapshotVolumes: true ``` Apply the backup schedule: ```bash kubectl apply -f backup-schedule.yaml ``` #### PostgreSQL Backup Configure PostgreSQL continuous archiving with WAL-G: ```bash # Create backup cronjob kubectl apply -f - <<EOF apiVersion: batch/v1 kind: CronJob metadata: name: postgres-backup namespace: cloudflow-prod spec: schedule: \"0 1 * * *\" # 1 AM UTC daily jobTemplate: spec: template: spec: containers: - name: backup image: postgres:14 env: - name: PGPASSWORD valueFrom: secretKeyRef: name: postgres-credentials key: password command: - /bin/bash - -c - | TIMESTAMP=\\$(date +%Y%m%d_%H%M%S) pg_dump -h postgresql.cloudflow-prod.svc.cluster.local \\ -U cloudflow -d cloudflow \\ | gzip > /backups/cloudflow_\\${TIMESTAMP}.sql.gz aws s3 cp /backups/cloudflow_\\${TIMESTAMP}.sql.gz \\ s3://cloudflow-db-backups/daily/ volumeMounts: - name: backup-storage mountPath: /backups volumes: - name: backup-storage emptyDir: {} restartPolicy: OnFailure EOF ``` ### Disaster Recovery Procedure In case of catastrophic failure, follow these steps to restore CloudFlow: 1. **Restore EKS Cluster** (if necessary): ```bash eksctl create cluster -f cluster-config.yaml ``` 2. **Restore Velero Backup**: ```bash # List available backups velero backup get # Restore from backup velero restore create --from-backup cloudflow-daily-backup-20260124 ``` 3. **Restore Database**: ```bash # Download backup from S3 aws s3 cp s3://cloudflow-db-backups/daily/cloudflow_20260124_010000.sql.gz . # Restore database gunzip -c cloudflow_20260124_010000.sql.gz | \\ psql -h postgresql.cloudflow-prod.svc.cluster.local -U cloudflow -d cloudflow ``` 4.",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_7",
              "content": "records: 500 - Session timeout: 30 seconds --- ## Message Queue Patterns ### Pub/Sub Pattern Used for broadcasting events to multiple interested consumers: ``` Workflow Engine publishes workflow.execution.completed \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u25bc \u25bc \u25bc \u25bc Analytics Notification Audit Logger Billing Service Service Service Service ``` Each service maintains its own consumer group and processes events independently. Failures in one consumer don't affect others. ### Request-Reply Pattern Synchronous communication over async messaging (used sparingly): ``` API Gateway publishes request \u2192 Kafka (reply-to: temp-queue-123) \u2502 \u25bc Service processes request \u2502 \u25bc Service publishes response \u2192 temp-queue-123 \u2502 \u25bc API Gateway receives response ``` Timeout: 5 seconds, fallback to direct HTTP call if no response. ### Saga Pattern Distributed transaction management for multi-service workflows: ``` Step 1: Create Order \u2192 SUCCESS \u2192 Step 2: Reserve Inventory \u2502 FAILURE \u2502 \u25bc Compensating Transaction \u2502 \u25bc Cancel Order (Rollback) ``` Orchestrated by Workflow Engine with compensation logic defined in workflow DSL. ### Dead Letter Queue (DLQ) Failed messages after all retry attempts are routed to DLQ: ``` Message processing fails (3 retries with exponential backoff) \u2502 \u25bc Publish to dead-letter-queue topic \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u25bc \u25bc Alert to PagerDuty Store in PostgreSQL for analysis \u2502 \u2502 \u25bc \u25bc Manual investigation Automated pattern detection ``` DLQ Processing SLA: < 4 hours for critical events, < 24 hours for non-critical. --- ## Caching Strategy ### Cache-Aside Pattern (Lazy Loading) Primary caching pattern used across all services: ``` 1. Application checks cache (Redis GET) 2. If HIT \u2192 Return cached data 3. If MISS: a. Query database (PostgreSQL) b. Store result in cache with TTL c. Return data to application ``` **Implementation Example** (Workflow Definition): ```javascript async function getWorkflowDefinition(workflowId) { const cacheKey = `workflow:def:${workflowId}`; // Step 1: Check cache let workflow = await redis.get(cacheKey); if (workflow) { metrics.increment('cache.hit.workflow_def'); return JSON.parse(workflow); } // Step 2: Cache miss - query database metrics.increment('cache.miss.workflow_def'); workflow = await db.query( 'SELECT * FROM workflows WHERE id = $1', [workflowId] ); // Step 3: Store in cache (1 hour TTL) await redis.setex(cacheKey, 3600, JSON.stringify(workflow)); return workflow; } ``` ### Write-Through Pattern Used for critical data where cache consistency is paramount: ``` 1. Application writes to database 2. Database transaction commits 3. Application updates cache 4. Return success to client ``` Applied to: User profiles, authentication sessions, system configuration.",
              "score": null
            }
          ],
          "latency_ms": 12.851931998739019,
          "query_id": "realistic_020",
          "dimension": "problem",
          "query": "My every-30-seconds schedule is being rejected"
        },
        {
          "key_facts": [
            "minimum scheduling interval is 1 minute",
            "The minimum scheduling interval is **1 minute**"
          ],
          "found_facts": [
            "The minimum scheduling interval is **1 minute**"
          ],
          "missed_facts": [
            "minimum scheduling interval is 1 minute"
          ],
          "coverage": 0.5,
          "expected_docs": [
            "user_guide"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "user_guide",
              "chunk_id": "user_guide_fix_4",
              "content": "### Cron Syntax Use standard cron expressions to define schedules: ``` * * * * * \u252c \u252c \u252c \u252c \u252c \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2514\u2500\u2500\u2500 Day of Week (0-6, Sunday=0) \u2502 \u2502 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Month (1-12) \u2502 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Day of Month (1-31) \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Hour (0-23) \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Minute (0-59) ``` **Common Cron Patterns:** | Pattern | Description | |---------|-------------| | `*/5 * * * *` | Every 5 minutes | | `0 * * * *` | Every hour at minute 0 | | `0 9 * * *` | Daily at 9:00 AM | | `0 9 * * 1` | Every Monday at 9:00 AM | | `0 0 1 * *` | First day of every month at midnight | | `0 0 * * 0` | Every Sunday at midnight | | `0 9-17 * * 1-5` | Every hour from 9 AM to 5 PM, Monday-Friday | | `*/15 9-17 * * 1-5` | Every 15 minutes during business hours | **Important:** The minimum scheduling interval is **1 minute**. Expressions that evaluate to more frequent executions will be rejected. ### Timezone Handling All scheduled workflows run in **UTC by default**. To account for your local timezone: **Option 1: Convert to UTC** If you want a workflow to run at 9:00 AM EST (UTC-5), schedule it for 14:00 UTC: ``` 0 14 * * * # 9:00 AM EST = 14:00 UTC ``` **Option 2: Use Timezone Configuration** Specify a timezone in your workflow configuration: ```yaml schedule: cron: \"0 9 * * *\" timezone: \"America/New_York\" # IANA timezone identifier ``` **Supported Timezones:** CloudFlow supports all IANA timezone identifiers, including: - `America/New_York` (Eastern Time) - `America/Chicago` (Central Time) - `America/Los_Angeles` (Pacific Time) - `Europe/London` (GMT/BST) - `Asia/Tokyo` (Japan Standard Time) - `Australia/Sydney` (Australian Eastern Time) **Daylight Saving Time:** When using timezone configuration, CloudFlow automatically handles DST transitions. A workflow scheduled for 9:00 AM will run at 9:00 AM local time regardless of DST changes. ### Schedule Management **Creating a Schedule:** 1. Open your workflow in the editor 2. Click the **\"Trigger\"** section 3. Select **\"Schedule\"** as the trigger type 4. Enter your cron expression or use the visual schedule builder 5. Select your timezone 6.",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_3",
              "content": "**Technology**: Go with distributed locking via Redis **Replicas**: 4 pods (production), active-passive with leader election **Resource Allocation**: 2 vCPU, 4GB RAM per pod **Key Responsibilities**: - Parse and validate cron expressions (extended format supporting seconds) - Maintain schedule registry in PostgreSQL - Distributed scheduling with leader election (one active scheduler) - Missed execution handling with configurable catch-up policy - Schedule conflict detection and resolution - Time zone support for international schedules **Scheduling Architecture**: ``` PostgreSQL Schedules Table \u2502 \u25bc Leader Scheduler (elected via Redis) \u2502 \u251c\u2500 Scan for due schedules (every 10 seconds) \u251c\u2500 Acquire lock per schedule (prevents duplicates) \u251c\u2500 Publish to Kafka \u2192 workflow.events topic \u2514\u2500 Update last_run timestamp ``` **Schedule Types**: - **Cron-based**: `0 */5 * * * *` (every 5 minutes) - **One-time**: Specific timestamp for delayed execution - **Interval-based**: Every N seconds/minutes/hours **Reliability Features**: - Leader election using Redis with 30-second lease - Heartbeat mechanism to detect leader failure (5-second interval) - Automatic failover to standby scheduler (< 10 seconds) - Schedule versioning to handle updates during execution - Missed execution policy: SKIP, RUN_ONCE, or RUN_ALL **Performance Targets**: - Schedule evaluation: < 100ms per cycle - Accuracy: \u00b1 1 second for schedule triggers - Capacity: 100,000 active schedules --- ### Notification Service **Purpose**: Multi-channel notification delivery system supporting email, SMS, webhooks, and in-app notifications.",
              "score": null
            },
            {
              "doc_id": "user_guide",
              "chunk_id": "user_guide_fix_5",
              "content": "Save and activate **Testing Schedules:** Use the built-in schedule calculator to preview upcoming executions: ``` Next 5 executions: 1. 2026-01-24 14:00:00 UTC 2. 2026-01-25 14:00:00 UTC 3. 2026-01-26 14:00:00 UTC 4. 2026-01-27 14:00:00 UTC 5. 2026-01-28 14:00:00 UTC ``` ## Error Handling Robust error handling ensures your workflows are resilient and reliable. ### Retry Policies Configure automatic retries for failed actions: ```yaml - id: api_call action: http_request config: url: \"https://api.example.com/data\" retry: max_attempts: 3 backoff_type: \"exponential\" # or \"fixed\", \"linear\" initial_interval: 1000 # milliseconds max_interval: 30000 multiplier: 2.0 retry_on: - timeout - network_error - status: [500, 502, 503, 504] ``` **Backoff Strategies:** - **Fixed**: Wait the same amount of time between retries - Attempt 1: Wait 1s - Attempt 2: Wait 1s - Attempt 3: Wait 1s - **Linear**: Increase wait time by a fixed amount - Attempt 1: Wait 1s - Attempt 2: Wait 2s - Attempt 3: Wait 3s - **Exponential**: Double the wait time with each retry (recommended) - Attempt 1: Wait 1s - Attempt 2: Wait 2s - Attempt 3: Wait 4s **Retry Conditions:** Control which errors trigger retries: - `timeout`: Request timeout - `network_error`: Connection failures - `status`: Specific HTTP status codes - `error_code`: Application-specific error codes ### Fallback Actions Execute alternative actions when the primary action fails: ```yaml - id: primary_payment action: http_request config: url: \"https://primary-payment-gateway.com/charge\" method: POST body: amount: \"{{amount}}\" on_error: - id: fallback_payment action: http_request config: url: \"https://backup-payment-gateway.com/charge\" method: POST body: amount: \"{{amount}}\" - id: notify_admin action: email config: to: \"admin@company.com\" subject: \"Payment Gateway Failure\" body: \"Primary gateway failed, switched to backup\" ``` **Error Object:** Access error details in fallback actions: ``` {{error.message}} # Error message {{error.code}} # Error code {{error.step_id}} # Failed step ID {{error.timestamp}} # When the error occurred {{error.attempts}} # Number of retry attempts ``` ### Dead Letter Queue When all retries and fallbacks fail, CloudFlow can route failed executions to a Dead Letter Queue (DLQ) for manual review: **Enable DLQ:** ```yaml workflow: error_handling: dead_letter_queue: enabled: true retain_days: 30 ``` **DLQ Features:** - View failed executions in the dashboard - Inspect complete execution context and error details - Retry individual executions after fixing issues - Export failed executions for analysis - Set up alerts for DLQ threshold breaches **Accessing the DLQ:** 1. Navigate to **Workflows** > **[Your Workflow]** > **Dead Letter Queue** 2.",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_12",
              "content": "Resume normal operations - Recovery time: 1-3 hours depending on data volume - Data loss: None if corruption detected quickly ### Testing & Validation **DR Drill Schedule**: - Monthly: Automated failover test (single AZ failure simulation) - Quarterly: Full DR region failover (non-production hours) - Annually: Complete disaster simulation with all stakeholders **Last DR Test Results** (Dec 15, 2025): - Scenario: Full region failover - Actual RTO: 2 hours 23 minutes (target: 4 hours) - Actual RPO: 42 minutes (target: 1 hour) - Issues identified: DNS propagation slower than expected (resolved) - Success criteria: Met all recovery objectives ### Business Continuity **Communication Plan**: - Status page: status.cloudflow.com (updated every 15 minutes during incident) - Customer notifications: Email + SMS for all P1 incidents - Internal escalation: PagerDuty \u2192 Incident Commander \u2192 Engineering Manager \u2192 CTO **Data Retention Policy**: - Active data: 12 months in hot storage (PostgreSQL) - Archived data: 7 years in cold storage (S3 Glacier) - Audit logs: 7 years (compliance requirement) - Backup retention: 30 days standard, 90 days monthly snapshots --- ## Appendix ### Service Dependency Matrix | Service | Depends On | Critical Path | |---------|-----------|---------------| | API Gateway | Auth Service, Redis | Yes | | Auth Service | PostgreSQL, Redis, Vault | Yes | | Workflow Engine | PostgreSQL, Kafka, Redis | Yes | | Scheduler | PostgreSQL, Redis, Kafka | No | | Notification Service | Kafka, PostgreSQL, SendGrid, Twilio | No | ### Capacity Planning **Current Capacity** (Jan 2026): - Daily workflow executions: 2.5M - Active users: 150,000 - API requests per day: 50M - Database size: 1.2TB - Kafka throughput: 100K msg/sec peak **6-Month Projection** (Jul 2026): - Daily workflow executions: 4M (+60%) - Active users: 225,000 (+50%) - API requests per day: 80M (+60%) - Database size: 2TB (+67%) - Required scaling: +30% compute, +50% storage ### Contact Information - **Architecture Team**: architecture@cloudflow.internal - **On-Call Engineer**: PagerDuty escalation - **Security Team**: security@cloudflow.internal - **Documentation**: https://wiki.cloudflow.internal/architecture --- **Document Revision History**: - v2.3.1 (Jan 15, 2026): Updated performance metrics, added DR test results - v2.3.0 (Dec 1, 2025): Added multi-AZ deployment details - v2.2.0 (Oct 15, 2025): Security architecture overhaul - v2.1.0 (Sep 1, 2025): Initial Kafka migration documentation - v2.0.0 (Jul 1, 2025): Complete microservices rewrite",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_2",
              "content": "Review Query Execution Plans** ```sql -- Connect to CloudFlow database cloudflow db connect --readonly -- Explain slow query EXPLAIN ANALYZE SELECT w.*, e.status, e.error_message FROM workflows w LEFT JOIN executions e ON w.id = e.workflow_id WHERE w.workspace_id = 'ws_abc123' AND e.created_at > NOW() - INTERVAL '7 days' ORDER BY e.created_at DESC; -- Check for missing indexes SELECT schemaname, tablename, indexname, indexdef FROM pg_indexes WHERE tablename IN ('workflows', 'executions', 'workflow_steps') ORDER BY tablename, indexname; ``` **3. Optimize Queries** Common optimization techniques: **Add missing indexes:** ```sql -- Index for workflow lookup by workspace CREATE INDEX CONCURRENTLY idx_workflows_workspace_created ON workflows(workspace_id, created_at DESC); -- Index for execution status queries CREATE INDEX CONCURRENTLY idx_executions_status_created ON executions(workflow_id, status, created_at DESC) WHERE status IN ('running', 'pending'); -- Composite index for common filter combinations CREATE INDEX CONCURRENTLY idx_executions_workspace_status ON executions(workspace_id, status, created_at) INCLUDE (error_message, retry_count); ``` **Use query result caching:** ```bash # Enable query cache for workspace metadata cloudflow config set cache.workspace.ttl 3600 # Configure Redis cache backend cloudflow config set cache.backend redis cloudflow config set cache.redis.host redis.cloudflow.svc.cluster.local cloudflow config set cache.redis.port 6379 ``` ### High API Latency #### Latency Breakdown Analysis ```bash # Generate latency report cloudflow metrics latency-report --endpoint \"/api/v1/workflows\" --last 24h # Sample output: # Endpoint: POST /api/v1/workflows/execute # P50: 245ms | P95: 1823ms | P99: 4521ms # Breakdown: # - Auth: 45ms (18%) # - DB Query: 156ms (64%) # - Business Logic: 32ms (13%) # - Response Serialization: 12ms (5%) ``` **Network latency issues:** ```bash # Test connectivity to CloudFlow API time curl -w \"@curl-format.txt\" -o /dev/null -s https://api.cloudflow.io/health # Create curl-format.txt: cat > curl-format.txt << EOF time_namelookup: %{time_namelookup}s\\n time_connect: %{time_connect}s\\n time_appconnect: %{time_appconnect}s\\n time_pretransfer: %{time_pretransfer}s\\n time_redirect: %{time_redirect}s\\n time_starttransfer: %{time_starttransfer}s\\n ----------\\n time_total: %{time_total}s\\n EOF # Trace route to API endpoint traceroute api.cloudflow.io # Check DNS resolution time dig api.cloudflow.io | grep \"Query time\" ``` ### Memory Leaks #### Detection ```bash # Monitor CloudFlow service memory usage kubectl top pods -n cloudflow --sort-by=memory # Get detailed memory metrics for specific pod kubectl exec -n cloudflow deploy/cloudflow-api -- \\ curl localhost:9090/metrics | grep memory # Check for OOMKilled pods kubectl get pods -n cloudflow --field-selector=status.phase=Failed | \\ grep OOMKilled # Review memory limits and requests kubectl describe deployment cloudflow-api -n cloudflow | \\ grep -A 5 \"Limits\\|Requests\" ``` #### Common Causes **1.",
              "score": null
            }
          ],
          "latency_ms": 12.928805001138244,
          "query_id": "realistic_020",
          "dimension": "casual",
          "query": "min schedule interval"
        },
        {
          "key_facts": [
            "minimum scheduling interval is 1 minute",
            "The minimum scheduling interval is **1 minute**"
          ],
          "found_facts": [
            "The minimum scheduling interval is **1 minute**"
          ],
          "missed_facts": [
            "minimum scheduling interval is 1 minute"
          ],
          "coverage": 0.5,
          "expected_docs": [
            "user_guide"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_3",
              "content": "**Technology**: Go with distributed locking via Redis **Replicas**: 4 pods (production), active-passive with leader election **Resource Allocation**: 2 vCPU, 4GB RAM per pod **Key Responsibilities**: - Parse and validate cron expressions (extended format supporting seconds) - Maintain schedule registry in PostgreSQL - Distributed scheduling with leader election (one active scheduler) - Missed execution handling with configurable catch-up policy - Schedule conflict detection and resolution - Time zone support for international schedules **Scheduling Architecture**: ``` PostgreSQL Schedules Table \u2502 \u25bc Leader Scheduler (elected via Redis) \u2502 \u251c\u2500 Scan for due schedules (every 10 seconds) \u251c\u2500 Acquire lock per schedule (prevents duplicates) \u251c\u2500 Publish to Kafka \u2192 workflow.events topic \u2514\u2500 Update last_run timestamp ``` **Schedule Types**: - **Cron-based**: `0 */5 * * * *` (every 5 minutes) - **One-time**: Specific timestamp for delayed execution - **Interval-based**: Every N seconds/minutes/hours **Reliability Features**: - Leader election using Redis with 30-second lease - Heartbeat mechanism to detect leader failure (5-second interval) - Automatic failover to standby scheduler (< 10 seconds) - Schedule versioning to handle updates during execution - Missed execution policy: SKIP, RUN_ONCE, or RUN_ALL **Performance Targets**: - Schedule evaluation: < 100ms per cycle - Accuracy: \u00b1 1 second for schedule triggers - Capacity: 100,000 active schedules --- ### Notification Service **Purpose**: Multi-channel notification delivery system supporting email, SMS, webhooks, and in-app notifications.",
              "score": null
            },
            {
              "doc_id": "user_guide",
              "chunk_id": "user_guide_fix_6",
              "content": "Filter by error type, date range, or execution ID 3. Click an execution to view full details 4. Click **\"Retry\"** to reprocess with the same input data ### Error Notifications Get notified when workflows fail: ```yaml workflow: notifications: on_failure: - type: email to: \"ops-team@company.com\" - type: slack channel: \"#alerts\" message: \"Workflow {{workflow.name}} failed: {{error.message}}\" on_success_after_retry: - type: slack channel: \"#monitoring\" message: \"Workflow recovered after {{error.attempts}} attempts\" ``` ## Workflow Limits CloudFlow enforces the following limits to ensure platform stability and performance: ### Steps Per Workflow - **Maximum**: 50 steps per workflow - **Recommendation**: Keep workflows focused and modular. If you need more steps, consider splitting into multiple workflows connected via webhooks. ### Execution Timeout - **Default**: 3600 seconds (60 minutes) - **Behavior**: Workflows exceeding this timeout are automatically terminated - **Custom Timeouts**: Enterprise plans can request custom timeout limits **Setting Step-Level Timeouts:** ```yaml - id: long_running_task action: http_request config: url: \"https://api.example.com/process\" timeout: 300 # 5 minutes for this specific step ``` ### Execution Limits - **Maximum**: 1000 executions per day (per workflow) - **Rate Limiting**: 100 concurrent executions per workflow - **Burst Limit**: 10 executions per second **What happens when limits are reached:** - New executions are queued automatically - Webhook triggers return HTTP 429 (Too Many Requests) - Scheduled executions are skipped (logged in audit trail) - Email notifications sent to workflow owner **Monitoring Usage:** View real-time metrics in your workflow dashboard: - Executions today: 847 / 1000 - Average duration: 12.3s - Success rate: 98.2% - Current queue depth: 3 ### Data Limits - **Maximum request/response size**: 10MB per action - **Maximum execution payload**: 50MB total - **Variable value size**: 1MB per variable ### Enterprise Plan Limits Enterprise customers can request increased limits: - Up to 100 steps per workflow - Up to 10,000 executions per day - Up to 7200 second timeout (2 hours) - Priority execution queue - Dedicated capacity allocation Contact sales@cloudflow.io for Enterprise pricing and custom limits. ## Best Practices Follow these best practices to build reliable, maintainable workflows: ### 1. Use Descriptive Names **Good:** - Workflow: \"Sync Customer Data from Salesforce to Database\" - Step: \"validate_customer_email\" **Bad:** - Workflow: \"Workflow 1\" - Step: \"step3\" ### 2.",
              "score": null
            },
            {
              "doc_id": "user_guide",
              "chunk_id": "user_guide_fix_5",
              "content": "Save and activate **Testing Schedules:** Use the built-in schedule calculator to preview upcoming executions: ``` Next 5 executions: 1. 2026-01-24 14:00:00 UTC 2. 2026-01-25 14:00:00 UTC 3. 2026-01-26 14:00:00 UTC 4. 2026-01-27 14:00:00 UTC 5. 2026-01-28 14:00:00 UTC ``` ## Error Handling Robust error handling ensures your workflows are resilient and reliable. ### Retry Policies Configure automatic retries for failed actions: ```yaml - id: api_call action: http_request config: url: \"https://api.example.com/data\" retry: max_attempts: 3 backoff_type: \"exponential\" # or \"fixed\", \"linear\" initial_interval: 1000 # milliseconds max_interval: 30000 multiplier: 2.0 retry_on: - timeout - network_error - status: [500, 502, 503, 504] ``` **Backoff Strategies:** - **Fixed**: Wait the same amount of time between retries - Attempt 1: Wait 1s - Attempt 2: Wait 1s - Attempt 3: Wait 1s - **Linear**: Increase wait time by a fixed amount - Attempt 1: Wait 1s - Attempt 2: Wait 2s - Attempt 3: Wait 3s - **Exponential**: Double the wait time with each retry (recommended) - Attempt 1: Wait 1s - Attempt 2: Wait 2s - Attempt 3: Wait 4s **Retry Conditions:** Control which errors trigger retries: - `timeout`: Request timeout - `network_error`: Connection failures - `status`: Specific HTTP status codes - `error_code`: Application-specific error codes ### Fallback Actions Execute alternative actions when the primary action fails: ```yaml - id: primary_payment action: http_request config: url: \"https://primary-payment-gateway.com/charge\" method: POST body: amount: \"{{amount}}\" on_error: - id: fallback_payment action: http_request config: url: \"https://backup-payment-gateway.com/charge\" method: POST body: amount: \"{{amount}}\" - id: notify_admin action: email config: to: \"admin@company.com\" subject: \"Payment Gateway Failure\" body: \"Primary gateway failed, switched to backup\" ``` **Error Object:** Access error details in fallback actions: ``` {{error.message}} # Error message {{error.code}} # Error code {{error.step_id}} # Failed step ID {{error.timestamp}} # When the error occurred {{error.attempts}} # Number of retry attempts ``` ### Dead Letter Queue When all retries and fallbacks fail, CloudFlow can route failed executions to a Dead Letter Queue (DLQ) for manual review: **Enable DLQ:** ```yaml workflow: error_handling: dead_letter_queue: enabled: true retain_days: 30 ``` **DLQ Features:** - View failed executions in the dashboard - Inspect complete execution context and error details - Retry individual executions after fixing issues - Export failed executions for analysis - Set up alerts for DLQ threshold breaches **Accessing the DLQ:** 1. Navigate to **Workflows** > **[Your Workflow]** > **Dead Letter Queue** 2.",
              "score": null
            },
            {
              "doc_id": "user_guide",
              "chunk_id": "user_guide_fix_4",
              "content": "### Cron Syntax Use standard cron expressions to define schedules: ``` * * * * * \u252c \u252c \u252c \u252c \u252c \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2514\u2500\u2500\u2500 Day of Week (0-6, Sunday=0) \u2502 \u2502 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Month (1-12) \u2502 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Day of Month (1-31) \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Hour (0-23) \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Minute (0-59) ``` **Common Cron Patterns:** | Pattern | Description | |---------|-------------| | `*/5 * * * *` | Every 5 minutes | | `0 * * * *` | Every hour at minute 0 | | `0 9 * * *` | Daily at 9:00 AM | | `0 9 * * 1` | Every Monday at 9:00 AM | | `0 0 1 * *` | First day of every month at midnight | | `0 0 * * 0` | Every Sunday at midnight | | `0 9-17 * * 1-5` | Every hour from 9 AM to 5 PM, Monday-Friday | | `*/15 9-17 * * 1-5` | Every 15 minutes during business hours | **Important:** The minimum scheduling interval is **1 minute**. Expressions that evaluate to more frequent executions will be rejected. ### Timezone Handling All scheduled workflows run in **UTC by default**. To account for your local timezone: **Option 1: Convert to UTC** If you want a workflow to run at 9:00 AM EST (UTC-5), schedule it for 14:00 UTC: ``` 0 14 * * * # 9:00 AM EST = 14:00 UTC ``` **Option 2: Use Timezone Configuration** Specify a timezone in your workflow configuration: ```yaml schedule: cron: \"0 9 * * *\" timezone: \"America/New_York\" # IANA timezone identifier ``` **Supported Timezones:** CloudFlow supports all IANA timezone identifiers, including: - `America/New_York` (Eastern Time) - `America/Chicago` (Central Time) - `America/Los_Angeles` (Pacific Time) - `Europe/London` (GMT/BST) - `Asia/Tokyo` (Japan Standard Time) - `Australia/Sydney` (Australian Eastern Time) **Daylight Saving Time:** When using timezone configuration, CloudFlow automatically handles DST transitions. A workflow scheduled for 9:00 AM will run at 9:00 AM local time regardless of DST changes. ### Schedule Management **Creating a Schedule:** 1. Open your workflow in the editor 2. Click the **\"Trigger\"** section 3. Select **\"Schedule\"** as the trigger type 4. Enter your cron expression or use the visual schedule builder 5. Select your timezone 6.",
              "score": null
            },
            {
              "doc_id": "user_guide",
              "chunk_id": "user_guide_fix_0",
              "content": "# CloudFlow User Guide Welcome to CloudFlow, the modern workflow automation platform that helps you connect your apps, automate repetitive tasks, and build powerful integrations without writing code. ## Table of Contents 1. [Getting Started](#getting-started) 2. [Workflow Creation](#workflow-creation) 3. [Available Actions](#available-actions) 4. [Variables and Expressions](#variables-and-expressions) 5. [Scheduling](#scheduling) 6. [Error Handling](#error-handling) 7. [Workflow Limits](#workflow-limits) 8. [Best Practices](#best-practices) 9. [Common Workflow Patterns](#common-workflow-patterns) ## Getting Started ### Account Setup Creating your CloudFlow account takes just a few minutes: 1. Navigate to https://app.cloudflow.io/signup 2. Enter your email address and create a strong password 3. Verify your email address by clicking the link sent to your inbox 4. Complete your profile by providing your organization name and use case 5. Choose your plan (Free, Professional, or Enterprise) Once your account is created, you'll be redirected to your CloudFlow dashboard where you can start building workflows immediately. ### Your First Workflow Let's create a simple workflow that sends you a Slack notification when a new file is uploaded to your Google Drive: 1. Click the **\"Create Workflow\"** button in the top-right corner 2. Give your workflow a name: \"Google Drive to Slack Notifier\" 3. Select **\"Google Drive - New File\"** as your trigger 4. Authenticate your Google Drive account when prompted 5. Choose the folder you want to monitor 6. Click **\"Add Action\"** and select **\"Slack - Send Message\"** 7. Authenticate your Slack account 8. Select the channel where you want to receive notifications 9. Customize your message using variables: `New file uploaded: {{trigger.file.name}}` 10. Click **\"Save & Activate\"** to enable your workflow Congratulations! Your first workflow is now live. Every time a file is added to the specified Google Drive folder, you'll receive a Slack notification. ## Workflow Creation CloudFlow offers multiple ways to create and manage workflows, giving you the flexibility to work in the way that suits you best. ### Visual Editor The Visual Editor is CloudFlow's drag-and-drop interface for building workflows without code. It's perfect for users who prefer a graphical approach: **Key Features:** - Drag-and-drop action blocks from the sidebar - Visual connections between steps show your workflow logic - Inline configuration for each action - Real-time validation and error checking - Test mode to run your workflow with sample data **Using the Visual Editor:** 1.",
              "score": null
            }
          ],
          "latency_ms": 13.854168999387184,
          "query_id": "realistic_020",
          "dimension": "contextual",
          "query": "I need near real-time execution, what's the fastest schedule I can set?"
        },
        {
          "key_facts": [
            "minimum scheduling interval is 1 minute",
            "The minimum scheduling interval is **1 minute**"
          ],
          "found_facts": [
            "The minimum scheduling interval is **1 minute**"
          ],
          "missed_facts": [
            "minimum scheduling interval is 1 minute"
          ],
          "coverage": 0.5,
          "expected_docs": [
            "user_guide"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "user_guide",
              "chunk_id": "user_guide_fix_4",
              "content": "### Cron Syntax Use standard cron expressions to define schedules: ``` * * * * * \u252c \u252c \u252c \u252c \u252c \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2514\u2500\u2500\u2500 Day of Week (0-6, Sunday=0) \u2502 \u2502 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Month (1-12) \u2502 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Day of Month (1-31) \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Hour (0-23) \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Minute (0-59) ``` **Common Cron Patterns:** | Pattern | Description | |---------|-------------| | `*/5 * * * *` | Every 5 minutes | | `0 * * * *` | Every hour at minute 0 | | `0 9 * * *` | Daily at 9:00 AM | | `0 9 * * 1` | Every Monday at 9:00 AM | | `0 0 1 * *` | First day of every month at midnight | | `0 0 * * 0` | Every Sunday at midnight | | `0 9-17 * * 1-5` | Every hour from 9 AM to 5 PM, Monday-Friday | | `*/15 9-17 * * 1-5` | Every 15 minutes during business hours | **Important:** The minimum scheduling interval is **1 minute**. Expressions that evaluate to more frequent executions will be rejected. ### Timezone Handling All scheduled workflows run in **UTC by default**. To account for your local timezone: **Option 1: Convert to UTC** If you want a workflow to run at 9:00 AM EST (UTC-5), schedule it for 14:00 UTC: ``` 0 14 * * * # 9:00 AM EST = 14:00 UTC ``` **Option 2: Use Timezone Configuration** Specify a timezone in your workflow configuration: ```yaml schedule: cron: \"0 9 * * *\" timezone: \"America/New_York\" # IANA timezone identifier ``` **Supported Timezones:** CloudFlow supports all IANA timezone identifiers, including: - `America/New_York` (Eastern Time) - `America/Chicago` (Central Time) - `America/Los_Angeles` (Pacific Time) - `Europe/London` (GMT/BST) - `Asia/Tokyo` (Japan Standard Time) - `Australia/Sydney` (Australian Eastern Time) **Daylight Saving Time:** When using timezone configuration, CloudFlow automatically handles DST transitions. A workflow scheduled for 9:00 AM will run at 9:00 AM local time regardless of DST changes. ### Schedule Management **Creating a Schedule:** 1. Open your workflow in the editor 2. Click the **\"Trigger\"** section 3. Select **\"Schedule\"** as the trigger type 4. Enter your cron expression or use the visual schedule builder 5. Select your timezone 6.",
              "score": null
            },
            {
              "doc_id": "user_guide",
              "chunk_id": "user_guide_fix_5",
              "content": "Save and activate **Testing Schedules:** Use the built-in schedule calculator to preview upcoming executions: ``` Next 5 executions: 1. 2026-01-24 14:00:00 UTC 2. 2026-01-25 14:00:00 UTC 3. 2026-01-26 14:00:00 UTC 4. 2026-01-27 14:00:00 UTC 5. 2026-01-28 14:00:00 UTC ``` ## Error Handling Robust error handling ensures your workflows are resilient and reliable. ### Retry Policies Configure automatic retries for failed actions: ```yaml - id: api_call action: http_request config: url: \"https://api.example.com/data\" retry: max_attempts: 3 backoff_type: \"exponential\" # or \"fixed\", \"linear\" initial_interval: 1000 # milliseconds max_interval: 30000 multiplier: 2.0 retry_on: - timeout - network_error - status: [500, 502, 503, 504] ``` **Backoff Strategies:** - **Fixed**: Wait the same amount of time between retries - Attempt 1: Wait 1s - Attempt 2: Wait 1s - Attempt 3: Wait 1s - **Linear**: Increase wait time by a fixed amount - Attempt 1: Wait 1s - Attempt 2: Wait 2s - Attempt 3: Wait 3s - **Exponential**: Double the wait time with each retry (recommended) - Attempt 1: Wait 1s - Attempt 2: Wait 2s - Attempt 3: Wait 4s **Retry Conditions:** Control which errors trigger retries: - `timeout`: Request timeout - `network_error`: Connection failures - `status`: Specific HTTP status codes - `error_code`: Application-specific error codes ### Fallback Actions Execute alternative actions when the primary action fails: ```yaml - id: primary_payment action: http_request config: url: \"https://primary-payment-gateway.com/charge\" method: POST body: amount: \"{{amount}}\" on_error: - id: fallback_payment action: http_request config: url: \"https://backup-payment-gateway.com/charge\" method: POST body: amount: \"{{amount}}\" - id: notify_admin action: email config: to: \"admin@company.com\" subject: \"Payment Gateway Failure\" body: \"Primary gateway failed, switched to backup\" ``` **Error Object:** Access error details in fallback actions: ``` {{error.message}} # Error message {{error.code}} # Error code {{error.step_id}} # Failed step ID {{error.timestamp}} # When the error occurred {{error.attempts}} # Number of retry attempts ``` ### Dead Letter Queue When all retries and fallbacks fail, CloudFlow can route failed executions to a Dead Letter Queue (DLQ) for manual review: **Enable DLQ:** ```yaml workflow: error_handling: dead_letter_queue: enabled: true retain_days: 30 ``` **DLQ Features:** - View failed executions in the dashboard - Inspect complete execution context and error details - Retry individual executions after fixing issues - Export failed executions for analysis - Set up alerts for DLQ threshold breaches **Accessing the DLQ:** 1. Navigate to **Workflows** > **[Your Workflow]** > **Dead Letter Queue** 2.",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_3",
              "content": "**Technology**: Go with distributed locking via Redis **Replicas**: 4 pods (production), active-passive with leader election **Resource Allocation**: 2 vCPU, 4GB RAM per pod **Key Responsibilities**: - Parse and validate cron expressions (extended format supporting seconds) - Maintain schedule registry in PostgreSQL - Distributed scheduling with leader election (one active scheduler) - Missed execution handling with configurable catch-up policy - Schedule conflict detection and resolution - Time zone support for international schedules **Scheduling Architecture**: ``` PostgreSQL Schedules Table \u2502 \u25bc Leader Scheduler (elected via Redis) \u2502 \u251c\u2500 Scan for due schedules (every 10 seconds) \u251c\u2500 Acquire lock per schedule (prevents duplicates) \u251c\u2500 Publish to Kafka \u2192 workflow.events topic \u2514\u2500 Update last_run timestamp ``` **Schedule Types**: - **Cron-based**: `0 */5 * * * *` (every 5 minutes) - **One-time**: Specific timestamp for delayed execution - **Interval-based**: Every N seconds/minutes/hours **Reliability Features**: - Leader election using Redis with 30-second lease - Heartbeat mechanism to detect leader failure (5-second interval) - Automatic failover to standby scheduler (< 10 seconds) - Schedule versioning to handle updates during execution - Missed execution policy: SKIP, RUN_ONCE, or RUN_ALL **Performance Targets**: - Schedule evaluation: < 100ms per cycle - Accuracy: \u00b1 1 second for schedule triggers - Capacity: 100,000 active schedules --- ### Notification Service **Purpose**: Multi-channel notification delivery system supporting email, SMS, webhooks, and in-app notifications.",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_7",
              "content": "records: 500 - Session timeout: 30 seconds --- ## Message Queue Patterns ### Pub/Sub Pattern Used for broadcasting events to multiple interested consumers: ``` Workflow Engine publishes workflow.execution.completed \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u25bc \u25bc \u25bc \u25bc Analytics Notification Audit Logger Billing Service Service Service Service ``` Each service maintains its own consumer group and processes events independently. Failures in one consumer don't affect others. ### Request-Reply Pattern Synchronous communication over async messaging (used sparingly): ``` API Gateway publishes request \u2192 Kafka (reply-to: temp-queue-123) \u2502 \u25bc Service processes request \u2502 \u25bc Service publishes response \u2192 temp-queue-123 \u2502 \u25bc API Gateway receives response ``` Timeout: 5 seconds, fallback to direct HTTP call if no response. ### Saga Pattern Distributed transaction management for multi-service workflows: ``` Step 1: Create Order \u2192 SUCCESS \u2192 Step 2: Reserve Inventory \u2502 FAILURE \u2502 \u25bc Compensating Transaction \u2502 \u25bc Cancel Order (Rollback) ``` Orchestrated by Workflow Engine with compensation logic defined in workflow DSL. ### Dead Letter Queue (DLQ) Failed messages after all retry attempts are routed to DLQ: ``` Message processing fails (3 retries with exponential backoff) \u2502 \u25bc Publish to dead-letter-queue topic \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u25bc \u25bc Alert to PagerDuty Store in PostgreSQL for analysis \u2502 \u2502 \u25bc \u25bc Manual investigation Automated pattern detection ``` DLQ Processing SLA: < 4 hours for critical events, < 24 hours for non-critical. --- ## Caching Strategy ### Cache-Aside Pattern (Lazy Loading) Primary caching pattern used across all services: ``` 1. Application checks cache (Redis GET) 2. If HIT \u2192 Return cached data 3. If MISS: a. Query database (PostgreSQL) b. Store result in cache with TTL c. Return data to application ``` **Implementation Example** (Workflow Definition): ```javascript async function getWorkflowDefinition(workflowId) { const cacheKey = `workflow:def:${workflowId}`; // Step 1: Check cache let workflow = await redis.get(cacheKey); if (workflow) { metrics.increment('cache.hit.workflow_def'); return JSON.parse(workflow); } // Step 2: Cache miss - query database metrics.increment('cache.miss.workflow_def'); workflow = await db.query( 'SELECT * FROM workflows WHERE id = $1', [workflowId] ); // Step 3: Store in cache (1 hour TTL) await redis.setex(cacheKey, 3600, JSON.stringify(workflow)); return workflow; } ``` ### Write-Through Pattern Used for critical data where cache consistency is paramount: ``` 1. Application writes to database 2. Database transaction commits 3. Application updates cache 4. Return success to client ``` Applied to: User profiles, authentication sessions, system configuration.",
              "score": null
            },
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_5",
              "content": "Configure log aggregation: ```bash # Install Fluent Bit for log forwarding helm install fluent-bit fluent/fluent-bit \\ --namespace logging \\ --create-namespace \\ --set cloudWatch.enabled=true \\ --set cloudWatch.region=us-east-1 \\ --set cloudWatch.logGroupName=/aws/eks/cloudflow-production/application ``` Query logs using CloudWatch Insights: ```sql fields @timestamp, @message, level, requestId, userId | filter namespace = \"cloudflow-prod\" | filter level = \"error\" | sort @timestamp desc | limit 100 ``` --- ## Backup and Disaster Recovery ### Database Backup Strategy CloudFlow implements a comprehensive backup strategy with the following retention policy: - **Daily snapshots**: Retained for 30 days - **Weekly backups**: Retained for 90 days - **Monthly backups**: Retained for 1 year #### Automated Backup with Velero Install Velero for cluster-wide backups: ```bash velero install \\ --provider aws \\ --plugins velero/velero-plugin-for-aws:v1.8.0 \\ --bucket cloudflow-velero-backups \\ --backup-location-config region=us-east-1 \\ --snapshot-location-config region=us-east-1 \\ --secret-file ./credentials-velero ``` Create a daily backup schedule: ```yaml # backup-schedule.yaml apiVersion: velero.io/v1 kind: Schedule metadata: name: cloudflow-daily-backup namespace: velero spec: schedule: \"0 2 * * *\" # 2 AM UTC daily template: includedNamespaces: - cloudflow-prod ttl: 720h0m0s # 30 days storageLocation: default snapshotVolumes: true ``` Apply the backup schedule: ```bash kubectl apply -f backup-schedule.yaml ``` #### PostgreSQL Backup Configure PostgreSQL continuous archiving with WAL-G: ```bash # Create backup cronjob kubectl apply -f - <<EOF apiVersion: batch/v1 kind: CronJob metadata: name: postgres-backup namespace: cloudflow-prod spec: schedule: \"0 1 * * *\" # 1 AM UTC daily jobTemplate: spec: template: spec: containers: - name: backup image: postgres:14 env: - name: PGPASSWORD valueFrom: secretKeyRef: name: postgres-credentials key: password command: - /bin/bash - -c - | TIMESTAMP=\\$(date +%Y%m%d_%H%M%S) pg_dump -h postgresql.cloudflow-prod.svc.cluster.local \\ -U cloudflow -d cloudflow \\ | gzip > /backups/cloudflow_\\${TIMESTAMP}.sql.gz aws s3 cp /backups/cloudflow_\\${TIMESTAMP}.sql.gz \\ s3://cloudflow-db-backups/daily/ volumeMounts: - name: backup-storage mountPath: /backups volumes: - name: backup-storage emptyDir: {} restartPolicy: OnFailure EOF ``` ### Disaster Recovery Procedure In case of catastrophic failure, follow these steps to restore CloudFlow: 1. **Restore EKS Cluster** (if necessary): ```bash eksctl create cluster -f cluster-config.yaml ``` 2. **Restore Velero Backup**: ```bash # List available backups velero backup get # Restore from backup velero restore create --from-backup cloudflow-daily-backup-20260124 ``` 3. **Restore Database**: ```bash # Download backup from S3 aws s3 cp s3://cloudflow-db-backups/daily/cloudflow_20260124_010000.sql.gz . # Restore database gunzip -c cloudflow_20260124_010000.sql.gz | \\ psql -h postgresql.cloudflow-prod.svc.cluster.local -U cloudflow -d cloudflow ``` 4.",
              "score": null
            }
          ],
          "latency_ms": 13.87272300053155,
          "query_id": "realistic_020",
          "dimension": "negation",
          "query": "Why can't I schedule workflows every 30 seconds?"
        }
      ]
    },
    {
      "strategy": "enriched_hybrid_llm",
      "chunking": "fixed_512_0pct",
      "embedder": "BAAI/bge-base-en-v1.5",
      "reranker": null,
      "llm": "fast",
      "k": 5,
      "metric": "exact_match",
      "num_chunks": 51,
      "index_time_s": 0.9768022219977865,
      "aggregate": {
        "original": {
          "coverage": 0.8867924528301887,
          "found": 47,
          "total": 53,
          "avg_latency_ms": 957.3219371995947,
          "p95_latency_ms": 1159.42970019878
        },
        "synonym": {
          "coverage": 0.7924528301886793,
          "found": 42,
          "total": 53,
          "avg_latency_ms": 1210.299437499998,
          "p95_latency_ms": 1739.4558661009821
        },
        "problem": {
          "coverage": 0.7924528301886793,
          "found": 42,
          "total": 53,
          "avg_latency_ms": 983.6807307998242,
          "p95_latency_ms": 1115.8086666497184
        },
        "casual": {
          "coverage": 0.8113207547169812,
          "found": 43,
          "total": 53,
          "avg_latency_ms": 933.1588117498541,
          "p95_latency_ms": 1111.4295497513012
        },
        "contextual": {
          "coverage": 0.7169811320754716,
          "found": 38,
          "total": 53,
          "avg_latency_ms": 1004.6754300998145,
          "p95_latency_ms": 1235.1260844996432
        },
        "negation": {
          "coverage": 0.7735849056603774,
          "found": 41,
          "total": 53,
          "avg_latency_ms": 991.9001975506035,
          "p95_latency_ms": 1220.7547866999448
        }
      },
      "per_query": [
        {
          "key_facts": [
            "100 requests per minute per authenticated user",
            "20 requests per minute for unauthenticated requests"
          ],
          "found_facts": [
            "100 requests per minute per authenticated user",
            "20 requests per minute for unauthenticated requests"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "api_reference"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_1",
              "content": "## Rate Limiting To ensure fair usage and system stability, CloudFlow enforces rate limits on all API endpoints. **Default Limits:** - 100 requests per minute per authenticated user - 20 requests per minute for unauthenticated requests - Burst allowance: 150 requests in a 10-second window ### Rate Limit Headers Every API response includes rate limit information: ``` X-RateLimit-Limit: 100 X-RateLimit-Remaining: 87 X-RateLimit-Reset: 1640995200 ``` When you exceed the rate limit, you'll receive a `429 Too Many Requests` response: ```json { \"error\": { \"code\": \"rate_limit_exceeded\", \"message\": \"Rate limit exceeded. Please retry after 42 seconds.\", \"retry_after\": 42 } } ``` **Best Practices:** - Monitor `X-RateLimit-Remaining` header values - Implement exponential backoff when receiving 429 responses - Cache responses when appropriate to reduce API calls - Consider upgrading to Enterprise tier for higher limits (1000 req/min) ## Pagination List endpoints return paginated results to optimize performance. Use `limit` and `offset` parameters to navigate through result sets. **Parameters:** - `limit`: Number of items per page (default: 20, max: 100) - `offset`: Number of items to skip (default: 0) **Example Request:** ```bash curl -H \"X-API-Key: YOUR_API_KEY\" \\ \"https://api.cloudflow.io/v2/workflows?limit=50&offset=100\" ``` **Response Structure:** ```json { \"data\": [...], \"pagination\": { \"total\": 347, \"limit\": 50, \"offset\": 100, \"has_more\": true } } ``` **Python Example:** ```python import requests def fetch_all_workflows(api_key): base_url = \"https://api.cloudflow.io/v2/workflows\" headers = {\"X-API-Key\": api_key} all_workflows = [] offset = 0 limit = 100 while True: response = requests.get( base_url, headers=headers, params={\"limit\": limit, \"offset\": offset} ) data = response.json() all_workflows.extend(data['data']) if not data['pagination']['has_more']: break offset += limit return all_workflows ``` ## Endpoints ### Workflows #### List Workflows Retrieve a paginated list of all workflows in your account. **Endpoint:** `GET /workflows` **Query Parameters:** - `status` (optional): Filter by status (`active`, `paused`, `archived`) - `created_after` (optional): ISO 8601 timestamp - `limit` (optional): Items per page (max 100) - `offset` (optional): Pagination offset **Example Request:** ```bash curl -H \"X-API-Key: YOUR_API_KEY\" \\ \"https://api.cloudflow.io/v2/workflows?status=active&limit=25\" ``` **Example Response (200 OK):** ```json { \"data\": [ { \"id\": \"wf_8x7k2m9p\", \"name\": \"Data Processing Pipeline\", \"description\": \"Processes customer data every hour\", \"status\": \"active\", \"trigger\": { \"type\": \"schedule\", \"cron\": \"0 * * * *\" }, \"steps\": 12, \"created_at\": \"2026-01-15T10:30:00Z\", \"updated_at\": \"2026-01-23T14:22:00Z\", \"last_run\": \"2026-01-24T09:00:00Z\" } ], \"pagination\": { \"total\": 1, \"limit\": 25, \"offset\": 0, \"has_more\": false } } ``` #### Create Workflow Create a new workflow with specified configuration.",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_7",
              "content": "Waiting {retry_after} seconds...\") time.sleep(retry_after) continue remaining = int(response.headers.get('X-RateLimit-Remaining', 0)) if remaining < 10: print(f\"Warning: Only {remaining} requests remaining\") return response raise Exception(\"Max retries exceeded due to rate limiting\") ``` **Bash script with rate limit checking:** ```bash #!/bin/bash check_rate_limit() { local remaining=$(curl -s -I https://api.cloudflow.io/api/v1/workflows \\ -H \"Authorization: Bearer $CF_ACCESS_TOKEN\" | \\ grep -i \"x-ratelimit-remaining\" | \\ awk '{print $2}' | tr -d '\\r') if [ \"$remaining\" -lt 10 ]; then echo \"Warning: Only $remaining requests remaining\" local reset=$(curl -s -I https://api.cloudflow.io/api/v1/workflows \\ -H \"Authorization: Bearer $CF_ACCESS_TOKEN\" | \\ grep -i \"x-ratelimit-reset\" | \\ awk '{print $2}' | tr -d '\\r') local wait_time=$((reset - $(date +%s))) echo \"Rate limit resets in $wait_time seconds\" sleep $wait_time fi } # Use before API calls check_rate_limit cloudflow workflows execute wf_9k2n4m8p1q ``` #### Optimization Strategies **1. Implement request batching:** ```bash # Batch multiple workflow executions cloudflow workflows execute-batch \\ --workflow-ids \"wf_id1,wf_id2,wf_id3,wf_id4,wf_id5\" \\ --batch-size 5 # This counts as 1 API request instead of 5 ``` **2. Use webhooks instead of polling:** ```bash # Configure webhook for workflow completion cloudflow webhooks create \\ --event workflow.completed \\ --url https://your-service.com/webhooks/cloudflow \\ --secret $WEBHOOK_SECRET # Verify webhook cloudflow webhooks test webhook_abc123 ``` **3.",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_6",
              "content": "Data Validation Errors** ``` ValidationError: Field 'customer_id' is required but missing in 234 records ``` **Resolution:** ```bash # Add data quality checks cloudflow workflows update wf_9k2n4m8p1q \\ --step data_ingestion \\ --add-validator required_fields \\ --validator-config '{\"fields\": [\"customer_id\", \"timestamp\", \"amount\"]}' # Configure error handling cloudflow workflows update wf_9k2n4m8p1q \\ --step data_validation \\ --on-error continue \\ --error-threshold 5% # Fail if > 5% of records invalid ``` **2. External API Failures** ``` ExternalAPIError: API request to https://partner-api.example.com failed with status 502 ``` **Resolution:** ```bash # Add circuit breaker cloudflow workflows update wf_9k2n4m8p1q \\ --step external_api_call \\ --circuit-breaker-enabled true \\ --circuit-breaker-threshold 5 \\ --circuit-breaker-timeout 30000 # Configure fallback behavior cloudflow workflows update wf_9k2n4m8p1q \\ --step external_api_call \\ --fallback-action use_cached_data \\ --cache-ttl 3600 ``` --- ## Rate Limiting & Throttling ### 429 Too Many Requests #### Error Response ```json { \"error\": \"rate_limit_exceeded\", \"message\": \"API rate limit exceeded. Retry after 45 seconds.\", \"status\": 429, \"headers\": { \"X-RateLimit-Limit\": \"1000\", \"X-RateLimit-Remaining\": \"0\", \"X-RateLimit-Reset\": \"1706112345\", \"Retry-After\": \"45\" } } ``` #### Rate Limit Tiers CloudFlow enforces the following rate limits per workspace: | Tier | Requests/Minute | Requests/Hour | Concurrent Workflows | |------|-----------------|---------------|----------------------| | Free | 60 | 1,000 | 5 | | Standard | 1,000 | 50,000 | 50 | | Premium | 5,000 | 250,000 | 200 | | Enterprise | Custom | Custom | Unlimited | #### Checking Rate Limit Status ```bash # Check current rate limit status curl -I https://api.cloudflow.io/api/v1/workflows \\ -H \"Authorization: Bearer $CF_ACCESS_TOKEN\" # Extract rate limit headers curl -s -I https://api.cloudflow.io/api/v1/workflows \\ -H \"Authorization: Bearer $CF_ACCESS_TOKEN\" | \\ grep -i \"x-ratelimit\" # Output: # X-RateLimit-Limit: 1000 # X-RateLimit-Remaining: 247 # X-RateLimit-Reset: 1706112400 # Monitor rate limit usage cloudflow metrics query \\ --metric rate_limit_remaining \\ --workspace ws_abc123 \\ --last 1h \\ --interval 1m ``` #### Handling Rate Limits in Code **Python example with retry logic:** ```python import time import requests def cloudflow_api_call_with_retry(url, headers, max_retries=3): for attempt in range(max_retries): response = requests.get(url, headers=headers) if response.status_code == 429: retry_after = int(response.headers.get('Retry-After', 60)) print(f\"Rate limited.",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_10",
              "content": "- Execution start rate: 500 per second - Completion rate: 450 per second (average 2-second execution time) **Database**: - Read throughput: 50,000 queries per second (across replicas) - Write throughput: 15,000 transactions per second - Connection capacity: 2,000 concurrent connections **Kafka**: - Message ingestion: 100,000 messages per second - Consumer throughput: 80,000 messages per second (aggregated) - End-to-end latency: < 100ms (P99) ### Resource Utilization **CPU Utilization** (Target: 60-70% average): - API Gateway: 55% average, 80% peak - Workflow Engine: 65% average, 85% peak - Auth Service: 40% average, 60% peak **Memory Utilization** (Target: < 80%): - API Gateway: 2.5GB average per pod (4GB allocated) - Workflow Engine: 6GB average per pod (8GB allocated) - Notification Service: 2.8GB average per pod (4GB allocated) **Network Throughput**: - Ingress: 2 Gbps average, 5 Gbps peak - Egress: 1.5 Gbps average, 4 Gbps peak - Internal (service mesh): 8 Gbps average ### Monitoring & Alerting **Key Metrics** (Monitored via Prometheus): - Request rate (per service, per endpoint) - Error rate (4xx, 5xx responses) - Latency percentiles (P50, P95, P99) - Resource utilization (CPU, memory, disk) - Cache hit/miss ratios - Database connection pool saturation - Kafka consumer lag **Alerts** (via PagerDuty): - P1 (Immediate): API error rate > 1%, database connection failure - P2 (< 15 min): P99 latency > 500ms, cache hit rate < 80% - P3 (< 1 hour): Resource utilization > 85%, replica count < minimum **SLI/SLO/SLA**: - SLI: API success rate (non-5xx responses) - SLO: 99.9% uptime per month (43 minutes downtime allowance) - SLA: 99.5% uptime guarantee (customer-facing SLA with credits) --- ## Disaster Recovery ### Recovery Objectives **RPO (Recovery Point Objective): 1 hour** - Maximum acceptable data loss: 1 hour of transactions - Achieved through: Continuous database replication + hourly snapshots - Kafka retention: 7 days allows event replay **RTO (Recovery Time Objective): 4 hours** - Maximum acceptable downtime: 4 hours for full system recovery - Includes: Failover, data verification, and service restoration - Automated runbooks reduce RTO to < 2 hours for common scenarios ### High Availability Architecture **Multi-AZ Deployment**: ``` Region: us-east-1 AZ-1a: AZ-1b: AZ-1c: - API Gateway (4) - API Gateway (4) - API Gateway (4) - Workflow Engine (6) - Workflow Engine (5) - Workflow Engine (5) - Auth Service (3) - Auth Service (3) - Auth",
              "score": null
            },
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_4",
              "content": "### Error Response Format ```json { \"error\": { \"code\": \"invalid_parameter\", \"message\": \"The 'limit' parameter must be between 1 and 100\", \"field\": \"limit\", \"request_id\": \"req_8k3m9x2p\" } } ``` ### HTTP Status Codes | Status Code | Description | Common Causes | |------------|-------------|---------------| | 400 | Bad Request | Invalid parameters, malformed JSON, validation errors | | 401 | Unauthorized | Missing or invalid authentication credentials | | 403 | Forbidden | Insufficient permissions for requested resource | | 404 | Not Found | Resource does not exist or has been deleted | | 429 | Too Many Requests | Rate limit exceeded, retry after specified period | | 500 | Internal Server Error | Unexpected server error, contact support if persists | | 502 | Bad Gateway | Temporary service issue, retry with exponential backoff | | 503 | Service Unavailable | Scheduled maintenance or temporary outage | ### Error Codes CloudFlow returns specific error codes to help you identify and resolve issues: - `invalid_parameter`: One or more request parameters are invalid - `missing_required_field`: Required field is missing from request body - `authentication_failed`: Invalid API key or token - `insufficient_permissions`: User lacks required scope or permission - `resource_not_found`: Requested resource does not exist - `rate_limit_exceeded`: Too many requests, see rate limiting section - `workflow_execution_failed`: Workflow execution encountered an error - `invalid_json`: Request body contains malformed JSON - `duplicate_resource`: Resource with same identifier already exists - `quota_exceeded`: Account quota limit reached ### Error Handling Best Practices **Python Example:** ```python import requests import time def make_api_request(url, headers, max_retries=3): for attempt in range(max_retries): try: response = requests.get(url, headers=headers) if response.status_code == 200: return response.json() elif response.status_code == 429: # Rate limit exceeded retry_after = int(response.headers.get('X-RateLimit-Reset', 60)) print(f\"Rate limited. Waiting {retry_after} seconds...\") time.sleep(retry_after) continue elif response.status_code >= 500: # Server error - retry with exponential backoff wait_time = 2 ** attempt print(f\"Server error. Retrying in {wait_time} seconds...\") time.sleep(wait_time) continue else: # Client error - don't retry error_data = response.json() raise Exception(f\"API Error: {error_data['error']['message']}\") except requests.exceptions.RequestException as e: print(f\"Request failed: {e}\") if attempt < max_retries - 1: time.sleep(2 ** attempt) else: raise raise Exception(\"Max retries exceeded\") ``` ## Webhooks CloudFlow can send webhook notifications when specific events occur in your workflows.",
              "score": null
            }
          ],
          "latency_ms": 861.7902730002243,
          "query_id": "realistic_001",
          "dimension": "original",
          "query": "What is the API rate limit per minute?"
        },
        {
          "key_facts": [
            "100 requests per minute per authenticated user",
            "20 requests per minute for unauthenticated requests"
          ],
          "found_facts": [
            "100 requests per minute per authenticated user",
            "20 requests per minute for unauthenticated requests"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "api_reference"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_1",
              "content": "## Rate Limiting To ensure fair usage and system stability, CloudFlow enforces rate limits on all API endpoints. **Default Limits:** - 100 requests per minute per authenticated user - 20 requests per minute for unauthenticated requests - Burst allowance: 150 requests in a 10-second window ### Rate Limit Headers Every API response includes rate limit information: ``` X-RateLimit-Limit: 100 X-RateLimit-Remaining: 87 X-RateLimit-Reset: 1640995200 ``` When you exceed the rate limit, you'll receive a `429 Too Many Requests` response: ```json { \"error\": { \"code\": \"rate_limit_exceeded\", \"message\": \"Rate limit exceeded. Please retry after 42 seconds.\", \"retry_after\": 42 } } ``` **Best Practices:** - Monitor `X-RateLimit-Remaining` header values - Implement exponential backoff when receiving 429 responses - Cache responses when appropriate to reduce API calls - Consider upgrading to Enterprise tier for higher limits (1000 req/min) ## Pagination List endpoints return paginated results to optimize performance. Use `limit` and `offset` parameters to navigate through result sets. **Parameters:** - `limit`: Number of items per page (default: 20, max: 100) - `offset`: Number of items to skip (default: 0) **Example Request:** ```bash curl -H \"X-API-Key: YOUR_API_KEY\" \\ \"https://api.cloudflow.io/v2/workflows?limit=50&offset=100\" ``` **Response Structure:** ```json { \"data\": [...], \"pagination\": { \"total\": 347, \"limit\": 50, \"offset\": 100, \"has_more\": true } } ``` **Python Example:** ```python import requests def fetch_all_workflows(api_key): base_url = \"https://api.cloudflow.io/v2/workflows\" headers = {\"X-API-Key\": api_key} all_workflows = [] offset = 0 limit = 100 while True: response = requests.get( base_url, headers=headers, params={\"limit\": limit, \"offset\": offset} ) data = response.json() all_workflows.extend(data['data']) if not data['pagination']['has_more']: break offset += limit return all_workflows ``` ## Endpoints ### Workflows #### List Workflows Retrieve a paginated list of all workflows in your account. **Endpoint:** `GET /workflows` **Query Parameters:** - `status` (optional): Filter by status (`active`, `paused`, `archived`) - `created_after` (optional): ISO 8601 timestamp - `limit` (optional): Items per page (max 100) - `offset` (optional): Pagination offset **Example Request:** ```bash curl -H \"X-API-Key: YOUR_API_KEY\" \\ \"https://api.cloudflow.io/v2/workflows?status=active&limit=25\" ``` **Example Response (200 OK):** ```json { \"data\": [ { \"id\": \"wf_8x7k2m9p\", \"name\": \"Data Processing Pipeline\", \"description\": \"Processes customer data every hour\", \"status\": \"active\", \"trigger\": { \"type\": \"schedule\", \"cron\": \"0 * * * *\" }, \"steps\": 12, \"created_at\": \"2026-01-15T10:30:00Z\", \"updated_at\": \"2026-01-23T14:22:00Z\", \"last_run\": \"2026-01-24T09:00:00Z\" } ], \"pagination\": { \"total\": 1, \"limit\": 25, \"offset\": 0, \"has_more\": false } } ``` #### Create Workflow Create a new workflow with specified configuration.",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_10",
              "content": "- Execution start rate: 500 per second - Completion rate: 450 per second (average 2-second execution time) **Database**: - Read throughput: 50,000 queries per second (across replicas) - Write throughput: 15,000 transactions per second - Connection capacity: 2,000 concurrent connections **Kafka**: - Message ingestion: 100,000 messages per second - Consumer throughput: 80,000 messages per second (aggregated) - End-to-end latency: < 100ms (P99) ### Resource Utilization **CPU Utilization** (Target: 60-70% average): - API Gateway: 55% average, 80% peak - Workflow Engine: 65% average, 85% peak - Auth Service: 40% average, 60% peak **Memory Utilization** (Target: < 80%): - API Gateway: 2.5GB average per pod (4GB allocated) - Workflow Engine: 6GB average per pod (8GB allocated) - Notification Service: 2.8GB average per pod (4GB allocated) **Network Throughput**: - Ingress: 2 Gbps average, 5 Gbps peak - Egress: 1.5 Gbps average, 4 Gbps peak - Internal (service mesh): 8 Gbps average ### Monitoring & Alerting **Key Metrics** (Monitored via Prometheus): - Request rate (per service, per endpoint) - Error rate (4xx, 5xx responses) - Latency percentiles (P50, P95, P99) - Resource utilization (CPU, memory, disk) - Cache hit/miss ratios - Database connection pool saturation - Kafka consumer lag **Alerts** (via PagerDuty): - P1 (Immediate): API error rate > 1%, database connection failure - P2 (< 15 min): P99 latency > 500ms, cache hit rate < 80% - P3 (< 1 hour): Resource utilization > 85%, replica count < minimum **SLI/SLO/SLA**: - SLI: API success rate (non-5xx responses) - SLO: 99.9% uptime per month (43 minutes downtime allowance) - SLA: 99.5% uptime guarantee (customer-facing SLA with credits) --- ## Disaster Recovery ### Recovery Objectives **RPO (Recovery Point Objective): 1 hour** - Maximum acceptable data loss: 1 hour of transactions - Achieved through: Continuous database replication + hourly snapshots - Kafka retention: 7 days allows event replay **RTO (Recovery Time Objective): 4 hours** - Maximum acceptable downtime: 4 hours for full system recovery - Includes: Failover, data verification, and service restoration - Automated runbooks reduce RTO to < 2 hours for common scenarios ### High Availability Architecture **Multi-AZ Deployment**: ``` Region: us-east-1 AZ-1a: AZ-1b: AZ-1c: - API Gateway (4) - API Gateway (4) - API Gateway (4) - Workflow Engine (6) - Workflow Engine (5) - Workflow Engine (5) - Auth Service (3) - Auth Service (3) - Auth",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_1",
              "content": "**Technology**: Node.js with Express.js framework **Replicas**: 12 pods (production), auto-scaling 8-20 based on CPU **Resource Allocation**: 2 vCPU, 4GB RAM per pod **Key Responsibilities**: - JWT token validation (delegated to Auth Service for initial validation) - Rate limiting: 1000 requests per minute per API key (sliding window) - Request/response transformation and validation using JSON Schema - Routing to downstream services based on URL path patterns - CORS handling for web clients - Request/response logging and correlation ID injection **Critical Endpoints**: - `POST /api/v1/workflows` - Create new workflow - `GET /api/v1/workflows/:id` - Retrieve workflow status - `POST /api/v1/workflows/:id/execute` - Trigger workflow execution - `GET /api/v1/workflows/:id/history` - Get execution history **Dependencies**: - Auth Service (for token validation) - Redis (for rate limiting counters) - All downstream microservices **Performance Targets**: - P50 latency: < 50ms - P99 latency: < 200ms - Throughput: 10,000 RPS sustained --- ### Auth Service **Purpose**: Centralized authentication and authorization service handling user identity, token generation, and permission validation. **Technology**: Go with gRPC for internal communication, REST for external **Replicas**: 8 pods (production), auto-scaling 6-12 **Resource Allocation**: 1 vCPU, 2GB RAM per pod **Key Responsibilities**: - User authentication via multiple providers (OAuth2, SAML, local credentials) - JWT token generation and validation (RS256 algorithm) - Role-based access control (RBAC) with fine-grained permissions - Session management with Redis-backed storage - API key generation and validation for service accounts - MFA enforcement for administrative operations **Authentication Flow**: ``` Client Request \u2192 API Gateway \u2192 Auth Service \u2502 \u251c\u2500 Validate credentials \u251c\u2500 Check MFA if required \u251c\u2500 Generate JWT (15min expiry) \u251c\u2500 Generate refresh token (7 days) \u2514\u2500 Store session in Redis ``` **Token Structure**: - Access Token: JWT with 15-minute expiry - Refresh Token: Opaque token with 7-day expiry, stored in PostgreSQL - Claims: user_id, email, roles[], permissions[], tenant_id **Security Features**: - Password hashing: Argon2id with 64MB memory, 4 iterations - Token rotation on refresh to prevent replay attacks - Brute force protection: 5 failed attempts \u2192 15-minute lockout - Secrets stored in HashiCorp Vault, rotated every 90 days **Performance Targets**: - Token validation: < 10ms (P99) - Token generation: < 50ms (P99) - Throughput: 5,000 RPS for validation operations --- ### Workflow Engine **Purpose**: Core orchestration service that executes workflow definitions, manages state transitions, and coordinates task execution across distributed systems.",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_7",
              "content": "Waiting {retry_after} seconds...\") time.sleep(retry_after) continue remaining = int(response.headers.get('X-RateLimit-Remaining', 0)) if remaining < 10: print(f\"Warning: Only {remaining} requests remaining\") return response raise Exception(\"Max retries exceeded due to rate limiting\") ``` **Bash script with rate limit checking:** ```bash #!/bin/bash check_rate_limit() { local remaining=$(curl -s -I https://api.cloudflow.io/api/v1/workflows \\ -H \"Authorization: Bearer $CF_ACCESS_TOKEN\" | \\ grep -i \"x-ratelimit-remaining\" | \\ awk '{print $2}' | tr -d '\\r') if [ \"$remaining\" -lt 10 ]; then echo \"Warning: Only $remaining requests remaining\" local reset=$(curl -s -I https://api.cloudflow.io/api/v1/workflows \\ -H \"Authorization: Bearer $CF_ACCESS_TOKEN\" | \\ grep -i \"x-ratelimit-reset\" | \\ awk '{print $2}' | tr -d '\\r') local wait_time=$((reset - $(date +%s))) echo \"Rate limit resets in $wait_time seconds\" sleep $wait_time fi } # Use before API calls check_rate_limit cloudflow workflows execute wf_9k2n4m8p1q ``` #### Optimization Strategies **1. Implement request batching:** ```bash # Batch multiple workflow executions cloudflow workflows execute-batch \\ --workflow-ids \"wf_id1,wf_id2,wf_id3,wf_id4,wf_id5\" \\ --batch-size 5 # This counts as 1 API request instead of 5 ``` **2. Use webhooks instead of polling:** ```bash # Configure webhook for workflow completion cloudflow webhooks create \\ --event workflow.completed \\ --url https://your-service.com/webhooks/cloudflow \\ --secret $WEBHOOK_SECRET # Verify webhook cloudflow webhooks test webhook_abc123 ``` **3.",
              "score": null
            },
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_4",
              "content": "### Error Response Format ```json { \"error\": { \"code\": \"invalid_parameter\", \"message\": \"The 'limit' parameter must be between 1 and 100\", \"field\": \"limit\", \"request_id\": \"req_8k3m9x2p\" } } ``` ### HTTP Status Codes | Status Code | Description | Common Causes | |------------|-------------|---------------| | 400 | Bad Request | Invalid parameters, malformed JSON, validation errors | | 401 | Unauthorized | Missing or invalid authentication credentials | | 403 | Forbidden | Insufficient permissions for requested resource | | 404 | Not Found | Resource does not exist or has been deleted | | 429 | Too Many Requests | Rate limit exceeded, retry after specified period | | 500 | Internal Server Error | Unexpected server error, contact support if persists | | 502 | Bad Gateway | Temporary service issue, retry with exponential backoff | | 503 | Service Unavailable | Scheduled maintenance or temporary outage | ### Error Codes CloudFlow returns specific error codes to help you identify and resolve issues: - `invalid_parameter`: One or more request parameters are invalid - `missing_required_field`: Required field is missing from request body - `authentication_failed`: Invalid API key or token - `insufficient_permissions`: User lacks required scope or permission - `resource_not_found`: Requested resource does not exist - `rate_limit_exceeded`: Too many requests, see rate limiting section - `workflow_execution_failed`: Workflow execution encountered an error - `invalid_json`: Request body contains malformed JSON - `duplicate_resource`: Resource with same identifier already exists - `quota_exceeded`: Account quota limit reached ### Error Handling Best Practices **Python Example:** ```python import requests import time def make_api_request(url, headers, max_retries=3): for attempt in range(max_retries): try: response = requests.get(url, headers=headers) if response.status_code == 200: return response.json() elif response.status_code == 429: # Rate limit exceeded retry_after = int(response.headers.get('X-RateLimit-Reset', 60)) print(f\"Rate limited. Waiting {retry_after} seconds...\") time.sleep(retry_after) continue elif response.status_code >= 500: # Server error - retry with exponential backoff wait_time = 2 ** attempt print(f\"Server error. Retrying in {wait_time} seconds...\") time.sleep(wait_time) continue else: # Client error - don't retry error_data = response.json() raise Exception(f\"API Error: {error_data['error']['message']}\") except requests.exceptions.RequestException as e: print(f\"Request failed: {e}\") if attempt < max_retries - 1: time.sleep(2 ** attempt) else: raise raise Exception(\"Max retries exceeded\") ``` ## Webhooks CloudFlow can send webhook notifications when specific events occur in your workflows.",
              "score": null
            }
          ],
          "latency_ms": 1015.7799199987494,
          "query_id": "realistic_001",
          "dimension": "synonym",
          "query": "How many requests can I make per minute to the API?"
        },
        {
          "key_facts": [
            "100 requests per minute per authenticated user",
            "20 requests per minute for unauthenticated requests"
          ],
          "found_facts": [
            "100 requests per minute per authenticated user",
            "20 requests per minute for unauthenticated requests"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "api_reference"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_4",
              "content": "### Error Response Format ```json { \"error\": { \"code\": \"invalid_parameter\", \"message\": \"The 'limit' parameter must be between 1 and 100\", \"field\": \"limit\", \"request_id\": \"req_8k3m9x2p\" } } ``` ### HTTP Status Codes | Status Code | Description | Common Causes | |------------|-------------|---------------| | 400 | Bad Request | Invalid parameters, malformed JSON, validation errors | | 401 | Unauthorized | Missing or invalid authentication credentials | | 403 | Forbidden | Insufficient permissions for requested resource | | 404 | Not Found | Resource does not exist or has been deleted | | 429 | Too Many Requests | Rate limit exceeded, retry after specified period | | 500 | Internal Server Error | Unexpected server error, contact support if persists | | 502 | Bad Gateway | Temporary service issue, retry with exponential backoff | | 503 | Service Unavailable | Scheduled maintenance or temporary outage | ### Error Codes CloudFlow returns specific error codes to help you identify and resolve issues: - `invalid_parameter`: One or more request parameters are invalid - `missing_required_field`: Required field is missing from request body - `authentication_failed`: Invalid API key or token - `insufficient_permissions`: User lacks required scope or permission - `resource_not_found`: Requested resource does not exist - `rate_limit_exceeded`: Too many requests, see rate limiting section - `workflow_execution_failed`: Workflow execution encountered an error - `invalid_json`: Request body contains malformed JSON - `duplicate_resource`: Resource with same identifier already exists - `quota_exceeded`: Account quota limit reached ### Error Handling Best Practices **Python Example:** ```python import requests import time def make_api_request(url, headers, max_retries=3): for attempt in range(max_retries): try: response = requests.get(url, headers=headers) if response.status_code == 200: return response.json() elif response.status_code == 429: # Rate limit exceeded retry_after = int(response.headers.get('X-RateLimit-Reset', 60)) print(f\"Rate limited. Waiting {retry_after} seconds...\") time.sleep(retry_after) continue elif response.status_code >= 500: # Server error - retry with exponential backoff wait_time = 2 ** attempt print(f\"Server error. Retrying in {wait_time} seconds...\") time.sleep(wait_time) continue else: # Client error - don't retry error_data = response.json() raise Exception(f\"API Error: {error_data['error']['message']}\") except requests.exceptions.RequestException as e: print(f\"Request failed: {e}\") if attempt < max_retries - 1: time.sleep(2 ** attempt) else: raise raise Exception(\"Max retries exceeded\") ``` ## Webhooks CloudFlow can send webhook notifications when specific events occur in your workflows.",
              "score": null
            },
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_1",
              "content": "## Rate Limiting To ensure fair usage and system stability, CloudFlow enforces rate limits on all API endpoints. **Default Limits:** - 100 requests per minute per authenticated user - 20 requests per minute for unauthenticated requests - Burst allowance: 150 requests in a 10-second window ### Rate Limit Headers Every API response includes rate limit information: ``` X-RateLimit-Limit: 100 X-RateLimit-Remaining: 87 X-RateLimit-Reset: 1640995200 ``` When you exceed the rate limit, you'll receive a `429 Too Many Requests` response: ```json { \"error\": { \"code\": \"rate_limit_exceeded\", \"message\": \"Rate limit exceeded. Please retry after 42 seconds.\", \"retry_after\": 42 } } ``` **Best Practices:** - Monitor `X-RateLimit-Remaining` header values - Implement exponential backoff when receiving 429 responses - Cache responses when appropriate to reduce API calls - Consider upgrading to Enterprise tier for higher limits (1000 req/min) ## Pagination List endpoints return paginated results to optimize performance. Use `limit` and `offset` parameters to navigate through result sets. **Parameters:** - `limit`: Number of items per page (default: 20, max: 100) - `offset`: Number of items to skip (default: 0) **Example Request:** ```bash curl -H \"X-API-Key: YOUR_API_KEY\" \\ \"https://api.cloudflow.io/v2/workflows?limit=50&offset=100\" ``` **Response Structure:** ```json { \"data\": [...], \"pagination\": { \"total\": 347, \"limit\": 50, \"offset\": 100, \"has_more\": true } } ``` **Python Example:** ```python import requests def fetch_all_workflows(api_key): base_url = \"https://api.cloudflow.io/v2/workflows\" headers = {\"X-API-Key\": api_key} all_workflows = [] offset = 0 limit = 100 while True: response = requests.get( base_url, headers=headers, params={\"limit\": limit, \"offset\": offset} ) data = response.json() all_workflows.extend(data['data']) if not data['pagination']['has_more']: break offset += limit return all_workflows ``` ## Endpoints ### Workflows #### List Workflows Retrieve a paginated list of all workflows in your account. **Endpoint:** `GET /workflows` **Query Parameters:** - `status` (optional): Filter by status (`active`, `paused`, `archived`) - `created_after` (optional): ISO 8601 timestamp - `limit` (optional): Items per page (max 100) - `offset` (optional): Pagination offset **Example Request:** ```bash curl -H \"X-API-Key: YOUR_API_KEY\" \\ \"https://api.cloudflow.io/v2/workflows?status=active&limit=25\" ``` **Example Response (200 OK):** ```json { \"data\": [ { \"id\": \"wf_8x7k2m9p\", \"name\": \"Data Processing Pipeline\", \"description\": \"Processes customer data every hour\", \"status\": \"active\", \"trigger\": { \"type\": \"schedule\", \"cron\": \"0 * * * *\" }, \"steps\": 12, \"created_at\": \"2026-01-15T10:30:00Z\", \"updated_at\": \"2026-01-23T14:22:00Z\", \"last_run\": \"2026-01-24T09:00:00Z\" } ], \"pagination\": { \"total\": 1, \"limit\": 25, \"offset\": 0, \"has_more\": false } } ``` #### Create Workflow Create a new workflow with specified configuration.",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_6",
              "content": "Data Validation Errors** ``` ValidationError: Field 'customer_id' is required but missing in 234 records ``` **Resolution:** ```bash # Add data quality checks cloudflow workflows update wf_9k2n4m8p1q \\ --step data_ingestion \\ --add-validator required_fields \\ --validator-config '{\"fields\": [\"customer_id\", \"timestamp\", \"amount\"]}' # Configure error handling cloudflow workflows update wf_9k2n4m8p1q \\ --step data_validation \\ --on-error continue \\ --error-threshold 5% # Fail if > 5% of records invalid ``` **2. External API Failures** ``` ExternalAPIError: API request to https://partner-api.example.com failed with status 502 ``` **Resolution:** ```bash # Add circuit breaker cloudflow workflows update wf_9k2n4m8p1q \\ --step external_api_call \\ --circuit-breaker-enabled true \\ --circuit-breaker-threshold 5 \\ --circuit-breaker-timeout 30000 # Configure fallback behavior cloudflow workflows update wf_9k2n4m8p1q \\ --step external_api_call \\ --fallback-action use_cached_data \\ --cache-ttl 3600 ``` --- ## Rate Limiting & Throttling ### 429 Too Many Requests #### Error Response ```json { \"error\": \"rate_limit_exceeded\", \"message\": \"API rate limit exceeded. Retry after 45 seconds.\", \"status\": 429, \"headers\": { \"X-RateLimit-Limit\": \"1000\", \"X-RateLimit-Remaining\": \"0\", \"X-RateLimit-Reset\": \"1706112345\", \"Retry-After\": \"45\" } } ``` #### Rate Limit Tiers CloudFlow enforces the following rate limits per workspace: | Tier | Requests/Minute | Requests/Hour | Concurrent Workflows | |------|-----------------|---------------|----------------------| | Free | 60 | 1,000 | 5 | | Standard | 1,000 | 50,000 | 50 | | Premium | 5,000 | 250,000 | 200 | | Enterprise | Custom | Custom | Unlimited | #### Checking Rate Limit Status ```bash # Check current rate limit status curl -I https://api.cloudflow.io/api/v1/workflows \\ -H \"Authorization: Bearer $CF_ACCESS_TOKEN\" # Extract rate limit headers curl -s -I https://api.cloudflow.io/api/v1/workflows \\ -H \"Authorization: Bearer $CF_ACCESS_TOKEN\" | \\ grep -i \"x-ratelimit\" # Output: # X-RateLimit-Limit: 1000 # X-RateLimit-Remaining: 247 # X-RateLimit-Reset: 1706112400 # Monitor rate limit usage cloudflow metrics query \\ --metric rate_limit_remaining \\ --workspace ws_abc123 \\ --last 1h \\ --interval 1m ``` #### Handling Rate Limits in Code **Python example with retry logic:** ```python import time import requests def cloudflow_api_call_with_retry(url, headers, max_retries=3): for attempt in range(max_retries): response = requests.get(url, headers=headers) if response.status_code == 429: retry_after = int(response.headers.get('Retry-After', 60)) print(f\"Rate limited.",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_7",
              "content": "Waiting {retry_after} seconds...\") time.sleep(retry_after) continue remaining = int(response.headers.get('X-RateLimit-Remaining', 0)) if remaining < 10: print(f\"Warning: Only {remaining} requests remaining\") return response raise Exception(\"Max retries exceeded due to rate limiting\") ``` **Bash script with rate limit checking:** ```bash #!/bin/bash check_rate_limit() { local remaining=$(curl -s -I https://api.cloudflow.io/api/v1/workflows \\ -H \"Authorization: Bearer $CF_ACCESS_TOKEN\" | \\ grep -i \"x-ratelimit-remaining\" | \\ awk '{print $2}' | tr -d '\\r') if [ \"$remaining\" -lt 10 ]; then echo \"Warning: Only $remaining requests remaining\" local reset=$(curl -s -I https://api.cloudflow.io/api/v1/workflows \\ -H \"Authorization: Bearer $CF_ACCESS_TOKEN\" | \\ grep -i \"x-ratelimit-reset\" | \\ awk '{print $2}' | tr -d '\\r') local wait_time=$((reset - $(date +%s))) echo \"Rate limit resets in $wait_time seconds\" sleep $wait_time fi } # Use before API calls check_rate_limit cloudflow workflows execute wf_9k2n4m8p1q ``` #### Optimization Strategies **1. Implement request batching:** ```bash # Batch multiple workflow executions cloudflow workflows execute-batch \\ --workflow-ids \"wf_id1,wf_id2,wf_id3,wf_id4,wf_id5\" \\ --batch-size 5 # This counts as 1 API request instead of 5 ``` **2. Use webhooks instead of polling:** ```bash # Configure webhook for workflow completion cloudflow webhooks create \\ --event workflow.completed \\ --url https://your-service.com/webhooks/cloudflow \\ --secret $WEBHOOK_SECRET # Verify webhook cloudflow webhooks test webhook_abc123 ``` **3.",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_1",
              "content": "**Technology**: Node.js with Express.js framework **Replicas**: 12 pods (production), auto-scaling 8-20 based on CPU **Resource Allocation**: 2 vCPU, 4GB RAM per pod **Key Responsibilities**: - JWT token validation (delegated to Auth Service for initial validation) - Rate limiting: 1000 requests per minute per API key (sliding window) - Request/response transformation and validation using JSON Schema - Routing to downstream services based on URL path patterns - CORS handling for web clients - Request/response logging and correlation ID injection **Critical Endpoints**: - `POST /api/v1/workflows` - Create new workflow - `GET /api/v1/workflows/:id` - Retrieve workflow status - `POST /api/v1/workflows/:id/execute` - Trigger workflow execution - `GET /api/v1/workflows/:id/history` - Get execution history **Dependencies**: - Auth Service (for token validation) - Redis (for rate limiting counters) - All downstream microservices **Performance Targets**: - P50 latency: < 50ms - P99 latency: < 200ms - Throughput: 10,000 RPS sustained --- ### Auth Service **Purpose**: Centralized authentication and authorization service handling user identity, token generation, and permission validation. **Technology**: Go with gRPC for internal communication, REST for external **Replicas**: 8 pods (production), auto-scaling 6-12 **Resource Allocation**: 1 vCPU, 2GB RAM per pod **Key Responsibilities**: - User authentication via multiple providers (OAuth2, SAML, local credentials) - JWT token generation and validation (RS256 algorithm) - Role-based access control (RBAC) with fine-grained permissions - Session management with Redis-backed storage - API key generation and validation for service accounts - MFA enforcement for administrative operations **Authentication Flow**: ``` Client Request \u2192 API Gateway \u2192 Auth Service \u2502 \u251c\u2500 Validate credentials \u251c\u2500 Check MFA if required \u251c\u2500 Generate JWT (15min expiry) \u251c\u2500 Generate refresh token (7 days) \u2514\u2500 Store session in Redis ``` **Token Structure**: - Access Token: JWT with 15-minute expiry - Refresh Token: Opaque token with 7-day expiry, stored in PostgreSQL - Claims: user_id, email, roles[], permissions[], tenant_id **Security Features**: - Password hashing: Argon2id with 64MB memory, 4 iterations - Token rotation on refresh to prevent replay attacks - Brute force protection: 5 failed attempts \u2192 15-minute lockout - Secrets stored in HashiCorp Vault, rotated every 90 days **Performance Targets**: - Token validation: < 10ms (P99) - Token generation: < 50ms (P99) - Throughput: 5,000 RPS for validation operations --- ### Workflow Engine **Purpose**: Core orchestration service that executes workflow definitions, manages state transitions, and coordinates task execution across distributed systems.",
              "score": null
            }
          ],
          "latency_ms": 929.0837210028258,
          "query_id": "realistic_001",
          "dimension": "problem",
          "query": "My API calls are getting blocked, what's the limit?"
        },
        {
          "key_facts": [
            "100 requests per minute per authenticated user",
            "20 requests per minute for unauthenticated requests"
          ],
          "found_facts": [
            "100 requests per minute per authenticated user",
            "20 requests per minute for unauthenticated requests"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "api_reference"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_1",
              "content": "## Rate Limiting To ensure fair usage and system stability, CloudFlow enforces rate limits on all API endpoints. **Default Limits:** - 100 requests per minute per authenticated user - 20 requests per minute for unauthenticated requests - Burst allowance: 150 requests in a 10-second window ### Rate Limit Headers Every API response includes rate limit information: ``` X-RateLimit-Limit: 100 X-RateLimit-Remaining: 87 X-RateLimit-Reset: 1640995200 ``` When you exceed the rate limit, you'll receive a `429 Too Many Requests` response: ```json { \"error\": { \"code\": \"rate_limit_exceeded\", \"message\": \"Rate limit exceeded. Please retry after 42 seconds.\", \"retry_after\": 42 } } ``` **Best Practices:** - Monitor `X-RateLimit-Remaining` header values - Implement exponential backoff when receiving 429 responses - Cache responses when appropriate to reduce API calls - Consider upgrading to Enterprise tier for higher limits (1000 req/min) ## Pagination List endpoints return paginated results to optimize performance. Use `limit` and `offset` parameters to navigate through result sets. **Parameters:** - `limit`: Number of items per page (default: 20, max: 100) - `offset`: Number of items to skip (default: 0) **Example Request:** ```bash curl -H \"X-API-Key: YOUR_API_KEY\" \\ \"https://api.cloudflow.io/v2/workflows?limit=50&offset=100\" ``` **Response Structure:** ```json { \"data\": [...], \"pagination\": { \"total\": 347, \"limit\": 50, \"offset\": 100, \"has_more\": true } } ``` **Python Example:** ```python import requests def fetch_all_workflows(api_key): base_url = \"https://api.cloudflow.io/v2/workflows\" headers = {\"X-API-Key\": api_key} all_workflows = [] offset = 0 limit = 100 while True: response = requests.get( base_url, headers=headers, params={\"limit\": limit, \"offset\": offset} ) data = response.json() all_workflows.extend(data['data']) if not data['pagination']['has_more']: break offset += limit return all_workflows ``` ## Endpoints ### Workflows #### List Workflows Retrieve a paginated list of all workflows in your account. **Endpoint:** `GET /workflows` **Query Parameters:** - `status` (optional): Filter by status (`active`, `paused`, `archived`) - `created_after` (optional): ISO 8601 timestamp - `limit` (optional): Items per page (max 100) - `offset` (optional): Pagination offset **Example Request:** ```bash curl -H \"X-API-Key: YOUR_API_KEY\" \\ \"https://api.cloudflow.io/v2/workflows?status=active&limit=25\" ``` **Example Response (200 OK):** ```json { \"data\": [ { \"id\": \"wf_8x7k2m9p\", \"name\": \"Data Processing Pipeline\", \"description\": \"Processes customer data every hour\", \"status\": \"active\", \"trigger\": { \"type\": \"schedule\", \"cron\": \"0 * * * *\" }, \"steps\": 12, \"created_at\": \"2026-01-15T10:30:00Z\", \"updated_at\": \"2026-01-23T14:22:00Z\", \"last_run\": \"2026-01-24T09:00:00Z\" } ], \"pagination\": { \"total\": 1, \"limit\": 25, \"offset\": 0, \"has_more\": false } } ``` #### Create Workflow Create a new workflow with specified configuration.",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_6",
              "content": "Data Validation Errors** ``` ValidationError: Field 'customer_id' is required but missing in 234 records ``` **Resolution:** ```bash # Add data quality checks cloudflow workflows update wf_9k2n4m8p1q \\ --step data_ingestion \\ --add-validator required_fields \\ --validator-config '{\"fields\": [\"customer_id\", \"timestamp\", \"amount\"]}' # Configure error handling cloudflow workflows update wf_9k2n4m8p1q \\ --step data_validation \\ --on-error continue \\ --error-threshold 5% # Fail if > 5% of records invalid ``` **2. External API Failures** ``` ExternalAPIError: API request to https://partner-api.example.com failed with status 502 ``` **Resolution:** ```bash # Add circuit breaker cloudflow workflows update wf_9k2n4m8p1q \\ --step external_api_call \\ --circuit-breaker-enabled true \\ --circuit-breaker-threshold 5 \\ --circuit-breaker-timeout 30000 # Configure fallback behavior cloudflow workflows update wf_9k2n4m8p1q \\ --step external_api_call \\ --fallback-action use_cached_data \\ --cache-ttl 3600 ``` --- ## Rate Limiting & Throttling ### 429 Too Many Requests #### Error Response ```json { \"error\": \"rate_limit_exceeded\", \"message\": \"API rate limit exceeded. Retry after 45 seconds.\", \"status\": 429, \"headers\": { \"X-RateLimit-Limit\": \"1000\", \"X-RateLimit-Remaining\": \"0\", \"X-RateLimit-Reset\": \"1706112345\", \"Retry-After\": \"45\" } } ``` #### Rate Limit Tiers CloudFlow enforces the following rate limits per workspace: | Tier | Requests/Minute | Requests/Hour | Concurrent Workflows | |------|-----------------|---------------|----------------------| | Free | 60 | 1,000 | 5 | | Standard | 1,000 | 50,000 | 50 | | Premium | 5,000 | 250,000 | 200 | | Enterprise | Custom | Custom | Unlimited | #### Checking Rate Limit Status ```bash # Check current rate limit status curl -I https://api.cloudflow.io/api/v1/workflows \\ -H \"Authorization: Bearer $CF_ACCESS_TOKEN\" # Extract rate limit headers curl -s -I https://api.cloudflow.io/api/v1/workflows \\ -H \"Authorization: Bearer $CF_ACCESS_TOKEN\" | \\ grep -i \"x-ratelimit\" # Output: # X-RateLimit-Limit: 1000 # X-RateLimit-Remaining: 247 # X-RateLimit-Reset: 1706112400 # Monitor rate limit usage cloudflow metrics query \\ --metric rate_limit_remaining \\ --workspace ws_abc123 \\ --last 1h \\ --interval 1m ``` #### Handling Rate Limits in Code **Python example with retry logic:** ```python import time import requests def cloudflow_api_call_with_retry(url, headers, max_retries=3): for attempt in range(max_retries): response = requests.get(url, headers=headers) if response.status_code == 429: retry_after = int(response.headers.get('Retry-After', 60)) print(f\"Rate limited.",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_7",
              "content": "Waiting {retry_after} seconds...\") time.sleep(retry_after) continue remaining = int(response.headers.get('X-RateLimit-Remaining', 0)) if remaining < 10: print(f\"Warning: Only {remaining} requests remaining\") return response raise Exception(\"Max retries exceeded due to rate limiting\") ``` **Bash script with rate limit checking:** ```bash #!/bin/bash check_rate_limit() { local remaining=$(curl -s -I https://api.cloudflow.io/api/v1/workflows \\ -H \"Authorization: Bearer $CF_ACCESS_TOKEN\" | \\ grep -i \"x-ratelimit-remaining\" | \\ awk '{print $2}' | tr -d '\\r') if [ \"$remaining\" -lt 10 ]; then echo \"Warning: Only $remaining requests remaining\" local reset=$(curl -s -I https://api.cloudflow.io/api/v1/workflows \\ -H \"Authorization: Bearer $CF_ACCESS_TOKEN\" | \\ grep -i \"x-ratelimit-reset\" | \\ awk '{print $2}' | tr -d '\\r') local wait_time=$((reset - $(date +%s))) echo \"Rate limit resets in $wait_time seconds\" sleep $wait_time fi } # Use before API calls check_rate_limit cloudflow workflows execute wf_9k2n4m8p1q ``` #### Optimization Strategies **1. Implement request batching:** ```bash # Batch multiple workflow executions cloudflow workflows execute-batch \\ --workflow-ids \"wf_id1,wf_id2,wf_id3,wf_id4,wf_id5\" \\ --batch-size 5 # This counts as 1 API request instead of 5 ``` **2. Use webhooks instead of polling:** ```bash # Configure webhook for workflow completion cloudflow webhooks create \\ --event workflow.completed \\ --url https://your-service.com/webhooks/cloudflow \\ --secret $WEBHOOK_SECRET # Verify webhook cloudflow webhooks test webhook_abc123 ``` **3.",
              "score": null
            },
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_4",
              "content": "### Error Response Format ```json { \"error\": { \"code\": \"invalid_parameter\", \"message\": \"The 'limit' parameter must be between 1 and 100\", \"field\": \"limit\", \"request_id\": \"req_8k3m9x2p\" } } ``` ### HTTP Status Codes | Status Code | Description | Common Causes | |------------|-------------|---------------| | 400 | Bad Request | Invalid parameters, malformed JSON, validation errors | | 401 | Unauthorized | Missing or invalid authentication credentials | | 403 | Forbidden | Insufficient permissions for requested resource | | 404 | Not Found | Resource does not exist or has been deleted | | 429 | Too Many Requests | Rate limit exceeded, retry after specified period | | 500 | Internal Server Error | Unexpected server error, contact support if persists | | 502 | Bad Gateway | Temporary service issue, retry with exponential backoff | | 503 | Service Unavailable | Scheduled maintenance or temporary outage | ### Error Codes CloudFlow returns specific error codes to help you identify and resolve issues: - `invalid_parameter`: One or more request parameters are invalid - `missing_required_field`: Required field is missing from request body - `authentication_failed`: Invalid API key or token - `insufficient_permissions`: User lacks required scope or permission - `resource_not_found`: Requested resource does not exist - `rate_limit_exceeded`: Too many requests, see rate limiting section - `workflow_execution_failed`: Workflow execution encountered an error - `invalid_json`: Request body contains malformed JSON - `duplicate_resource`: Resource with same identifier already exists - `quota_exceeded`: Account quota limit reached ### Error Handling Best Practices **Python Example:** ```python import requests import time def make_api_request(url, headers, max_retries=3): for attempt in range(max_retries): try: response = requests.get(url, headers=headers) if response.status_code == 200: return response.json() elif response.status_code == 429: # Rate limit exceeded retry_after = int(response.headers.get('X-RateLimit-Reset', 60)) print(f\"Rate limited. Waiting {retry_after} seconds...\") time.sleep(retry_after) continue elif response.status_code >= 500: # Server error - retry with exponential backoff wait_time = 2 ** attempt print(f\"Server error. Retrying in {wait_time} seconds...\") time.sleep(wait_time) continue else: # Client error - don't retry error_data = response.json() raise Exception(f\"API Error: {error_data['error']['message']}\") except requests.exceptions.RequestException as e: print(f\"Request failed: {e}\") if attempt < max_retries - 1: time.sleep(2 ** attempt) else: raise raise Exception(\"Max retries exceeded\") ``` ## Webhooks CloudFlow can send webhook notifications when specific events occur in your workflows.",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_10",
              "content": "- Execution start rate: 500 per second - Completion rate: 450 per second (average 2-second execution time) **Database**: - Read throughput: 50,000 queries per second (across replicas) - Write throughput: 15,000 transactions per second - Connection capacity: 2,000 concurrent connections **Kafka**: - Message ingestion: 100,000 messages per second - Consumer throughput: 80,000 messages per second (aggregated) - End-to-end latency: < 100ms (P99) ### Resource Utilization **CPU Utilization** (Target: 60-70% average): - API Gateway: 55% average, 80% peak - Workflow Engine: 65% average, 85% peak - Auth Service: 40% average, 60% peak **Memory Utilization** (Target: < 80%): - API Gateway: 2.5GB average per pod (4GB allocated) - Workflow Engine: 6GB average per pod (8GB allocated) - Notification Service: 2.8GB average per pod (4GB allocated) **Network Throughput**: - Ingress: 2 Gbps average, 5 Gbps peak - Egress: 1.5 Gbps average, 4 Gbps peak - Internal (service mesh): 8 Gbps average ### Monitoring & Alerting **Key Metrics** (Monitored via Prometheus): - Request rate (per service, per endpoint) - Error rate (4xx, 5xx responses) - Latency percentiles (P50, P95, P99) - Resource utilization (CPU, memory, disk) - Cache hit/miss ratios - Database connection pool saturation - Kafka consumer lag **Alerts** (via PagerDuty): - P1 (Immediate): API error rate > 1%, database connection failure - P2 (< 15 min): P99 latency > 500ms, cache hit rate < 80% - P3 (< 1 hour): Resource utilization > 85%, replica count < minimum **SLI/SLO/SLA**: - SLI: API success rate (non-5xx responses) - SLO: 99.9% uptime per month (43 minutes downtime allowance) - SLA: 99.5% uptime guarantee (customer-facing SLA with credits) --- ## Disaster Recovery ### Recovery Objectives **RPO (Recovery Point Objective): 1 hour** - Maximum acceptable data loss: 1 hour of transactions - Achieved through: Continuous database replication + hourly snapshots - Kafka retention: 7 days allows event replay **RTO (Recovery Time Objective): 4 hours** - Maximum acceptable downtime: 4 hours for full system recovery - Includes: Failover, data verification, and service restoration - Automated runbooks reduce RTO to < 2 hours for common scenarios ### High Availability Architecture **Multi-AZ Deployment**: ``` Region: us-east-1 AZ-1a: AZ-1b: AZ-1c: - API Gateway (4) - API Gateway (4) - API Gateway (4) - Workflow Engine (6) - Workflow Engine (5) - Workflow Engine (5) - Auth Service (3) - Auth Service (3) - Auth",
              "score": null
            }
          ],
          "latency_ms": 929.0812740000547,
          "query_id": "realistic_001",
          "dimension": "casual",
          "query": "api rate limit"
        },
        {
          "key_facts": [
            "100 requests per minute per authenticated user",
            "20 requests per minute for unauthenticated requests"
          ],
          "found_facts": [
            "100 requests per minute per authenticated user",
            "20 requests per minute for unauthenticated requests"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "api_reference"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_7",
              "content": "Waiting {retry_after} seconds...\") time.sleep(retry_after) continue remaining = int(response.headers.get('X-RateLimit-Remaining', 0)) if remaining < 10: print(f\"Warning: Only {remaining} requests remaining\") return response raise Exception(\"Max retries exceeded due to rate limiting\") ``` **Bash script with rate limit checking:** ```bash #!/bin/bash check_rate_limit() { local remaining=$(curl -s -I https://api.cloudflow.io/api/v1/workflows \\ -H \"Authorization: Bearer $CF_ACCESS_TOKEN\" | \\ grep -i \"x-ratelimit-remaining\" | \\ awk '{print $2}' | tr -d '\\r') if [ \"$remaining\" -lt 10 ]; then echo \"Warning: Only $remaining requests remaining\" local reset=$(curl -s -I https://api.cloudflow.io/api/v1/workflows \\ -H \"Authorization: Bearer $CF_ACCESS_TOKEN\" | \\ grep -i \"x-ratelimit-reset\" | \\ awk '{print $2}' | tr -d '\\r') local wait_time=$((reset - $(date +%s))) echo \"Rate limit resets in $wait_time seconds\" sleep $wait_time fi } # Use before API calls check_rate_limit cloudflow workflows execute wf_9k2n4m8p1q ``` #### Optimization Strategies **1. Implement request batching:** ```bash # Batch multiple workflow executions cloudflow workflows execute-batch \\ --workflow-ids \"wf_id1,wf_id2,wf_id3,wf_id4,wf_id5\" \\ --batch-size 5 # This counts as 1 API request instead of 5 ``` **2. Use webhooks instead of polling:** ```bash # Configure webhook for workflow completion cloudflow webhooks create \\ --event workflow.completed \\ --url https://your-service.com/webhooks/cloudflow \\ --secret $WEBHOOK_SECRET # Verify webhook cloudflow webhooks test webhook_abc123 ``` **3.",
              "score": null
            },
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_1",
              "content": "## Rate Limiting To ensure fair usage and system stability, CloudFlow enforces rate limits on all API endpoints. **Default Limits:** - 100 requests per minute per authenticated user - 20 requests per minute for unauthenticated requests - Burst allowance: 150 requests in a 10-second window ### Rate Limit Headers Every API response includes rate limit information: ``` X-RateLimit-Limit: 100 X-RateLimit-Remaining: 87 X-RateLimit-Reset: 1640995200 ``` When you exceed the rate limit, you'll receive a `429 Too Many Requests` response: ```json { \"error\": { \"code\": \"rate_limit_exceeded\", \"message\": \"Rate limit exceeded. Please retry after 42 seconds.\", \"retry_after\": 42 } } ``` **Best Practices:** - Monitor `X-RateLimit-Remaining` header values - Implement exponential backoff when receiving 429 responses - Cache responses when appropriate to reduce API calls - Consider upgrading to Enterprise tier for higher limits (1000 req/min) ## Pagination List endpoints return paginated results to optimize performance. Use `limit` and `offset` parameters to navigate through result sets. **Parameters:** - `limit`: Number of items per page (default: 20, max: 100) - `offset`: Number of items to skip (default: 0) **Example Request:** ```bash curl -H \"X-API-Key: YOUR_API_KEY\" \\ \"https://api.cloudflow.io/v2/workflows?limit=50&offset=100\" ``` **Response Structure:** ```json { \"data\": [...], \"pagination\": { \"total\": 347, \"limit\": 50, \"offset\": 100, \"has_more\": true } } ``` **Python Example:** ```python import requests def fetch_all_workflows(api_key): base_url = \"https://api.cloudflow.io/v2/workflows\" headers = {\"X-API-Key\": api_key} all_workflows = [] offset = 0 limit = 100 while True: response = requests.get( base_url, headers=headers, params={\"limit\": limit, \"offset\": offset} ) data = response.json() all_workflows.extend(data['data']) if not data['pagination']['has_more']: break offset += limit return all_workflows ``` ## Endpoints ### Workflows #### List Workflows Retrieve a paginated list of all workflows in your account. **Endpoint:** `GET /workflows` **Query Parameters:** - `status` (optional): Filter by status (`active`, `paused`, `archived`) - `created_after` (optional): ISO 8601 timestamp - `limit` (optional): Items per page (max 100) - `offset` (optional): Pagination offset **Example Request:** ```bash curl -H \"X-API-Key: YOUR_API_KEY\" \\ \"https://api.cloudflow.io/v2/workflows?status=active&limit=25\" ``` **Example Response (200 OK):** ```json { \"data\": [ { \"id\": \"wf_8x7k2m9p\", \"name\": \"Data Processing Pipeline\", \"description\": \"Processes customer data every hour\", \"status\": \"active\", \"trigger\": { \"type\": \"schedule\", \"cron\": \"0 * * * *\" }, \"steps\": 12, \"created_at\": \"2026-01-15T10:30:00Z\", \"updated_at\": \"2026-01-23T14:22:00Z\", \"last_run\": \"2026-01-24T09:00:00Z\" } ], \"pagination\": { \"total\": 1, \"limit\": 25, \"offset\": 0, \"has_more\": false } } ``` #### Create Workflow Create a new workflow with specified configuration.",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_6",
              "content": "Data Validation Errors** ``` ValidationError: Field 'customer_id' is required but missing in 234 records ``` **Resolution:** ```bash # Add data quality checks cloudflow workflows update wf_9k2n4m8p1q \\ --step data_ingestion \\ --add-validator required_fields \\ --validator-config '{\"fields\": [\"customer_id\", \"timestamp\", \"amount\"]}' # Configure error handling cloudflow workflows update wf_9k2n4m8p1q \\ --step data_validation \\ --on-error continue \\ --error-threshold 5% # Fail if > 5% of records invalid ``` **2. External API Failures** ``` ExternalAPIError: API request to https://partner-api.example.com failed with status 502 ``` **Resolution:** ```bash # Add circuit breaker cloudflow workflows update wf_9k2n4m8p1q \\ --step external_api_call \\ --circuit-breaker-enabled true \\ --circuit-breaker-threshold 5 \\ --circuit-breaker-timeout 30000 # Configure fallback behavior cloudflow workflows update wf_9k2n4m8p1q \\ --step external_api_call \\ --fallback-action use_cached_data \\ --cache-ttl 3600 ``` --- ## Rate Limiting & Throttling ### 429 Too Many Requests #### Error Response ```json { \"error\": \"rate_limit_exceeded\", \"message\": \"API rate limit exceeded. Retry after 45 seconds.\", \"status\": 429, \"headers\": { \"X-RateLimit-Limit\": \"1000\", \"X-RateLimit-Remaining\": \"0\", \"X-RateLimit-Reset\": \"1706112345\", \"Retry-After\": \"45\" } } ``` #### Rate Limit Tiers CloudFlow enforces the following rate limits per workspace: | Tier | Requests/Minute | Requests/Hour | Concurrent Workflows | |------|-----------------|---------------|----------------------| | Free | 60 | 1,000 | 5 | | Standard | 1,000 | 50,000 | 50 | | Premium | 5,000 | 250,000 | 200 | | Enterprise | Custom | Custom | Unlimited | #### Checking Rate Limit Status ```bash # Check current rate limit status curl -I https://api.cloudflow.io/api/v1/workflows \\ -H \"Authorization: Bearer $CF_ACCESS_TOKEN\" # Extract rate limit headers curl -s -I https://api.cloudflow.io/api/v1/workflows \\ -H \"Authorization: Bearer $CF_ACCESS_TOKEN\" | \\ grep -i \"x-ratelimit\" # Output: # X-RateLimit-Limit: 1000 # X-RateLimit-Remaining: 247 # X-RateLimit-Reset: 1706112400 # Monitor rate limit usage cloudflow metrics query \\ --metric rate_limit_remaining \\ --workspace ws_abc123 \\ --last 1h \\ --interval 1m ``` #### Handling Rate Limits in Code **Python example with retry logic:** ```python import time import requests def cloudflow_api_call_with_retry(url, headers, max_retries=3): for attempt in range(max_retries): response = requests.get(url, headers=headers) if response.status_code == 429: retry_after = int(response.headers.get('Retry-After', 60)) print(f\"Rate limited.",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_9",
              "content": "in\" # Find workflow failures by ID kubectl logs -n cloudflow deployment/cloudflow-workflow-engine | \\ grep \"exec_7h3j6k9m2n\" ``` #### Rate Limiting ```bash # Find rate limit events kubectl logs -n cloudflow deployment/cloudflow-api | \\ grep -E \"429|rate.*limit|throttle\" # Count rate limit errors by hour kubectl logs -n cloudflow deployment/cloudflow-api --since=24h | \\ grep \"rate_limit_exceeded\" | \\ awk '{print $1}' | \\ cut -d'T' -f1-2 | \\ sort | uniq -c ``` ### Advanced Log Analysis #### Using jq for JSON Logs ```bash # Parse JSON logs and filter by level kubectl logs -n cloudflow deployment/cloudflow-api | \\ jq 'select(.level == \"ERROR\")' # Extract specific fields kubectl logs -n cloudflow deployment/cloudflow-api | \\ jq '{timestamp: .timestamp, level: .level, message: .message, execution_id: .context.execution_id}' # Filter by workflow ID kubectl logs -n cloudflow deployment/cloudflow-workflow-engine | \\ jq 'select(.workflow_id == \"wf_9k2n4m8p1q\")' # Count errors by type kubectl logs -n cloudflow deployment/cloudflow-api --since=1h | \\ jq -r 'select(.level == \"ERROR\") | .error_type' | \\ sort | uniq -c | sort -rn ``` #### Correlation IDs CloudFlow uses correlation IDs to trace requests across services. ```bash # Extract correlation ID from error CORRELATION_ID=\"corr_8h4j9k2m5n\" # Trace request across all services for pod in $(kubectl get pods -n cloudflow -l tier=backend -o name); do echo \"=== $pod ===\" kubectl logs -n cloudflow $pod | grep $CORRELATION_ID done # Export full trace to file cloudflow debug trace $CORRELATION_ID --output trace-$CORRELATION_ID.json ``` ### Debugging Commands #### Enable Debug Mode for Workflow Execution ```bash # Execute workflow with debug logging cloudflow workflows execute wf_9k2n4m8p1q \\ --debug \\ --log-level TRACE \\ --output-logs /tmp/workflow-debug.log # Enable step-by-step execution cloudflow workflows execute wf_9k2n4m8p1q \\ --step-mode interactive \\ --breakpoint-on-error # Capture full execution context cloudflow workflows execute wf_9k2n4m8p1q \\ --capture-context \\ --context-output /tmp/execution-context.json ``` #### Database Query Debugging ```bash # Enable query logging cloudflow db config set log_statement all cloudflow db config set log_duration on cloudflow db config set log_min_duration_statement 1000 # Log queries > 1s # Capture query plan for slow endpoint cloudflow debug capture-queries \\ --endpoint \"/api/v1/workflows/list\" \\ --duration 60 \\ --output query-analysis.txt # Analyze query performance cloudflow db analyze-performance --last 1h ``` #### Network Debugging ```bash # Test connectivity from CloudFlow pod kubectl run -n cloudflow netdebug --rm -i --tty \\ --image=nicolaka/netshoot -- /bin/bash # Inside pod: # Check DNS resolution nslookup api.cloudflow.io # Check connectivity to",
              "score": null
            },
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_4",
              "content": "--set prometheus.prometheusSpec.retention=30d \\ --set prometheus.prometheusSpec.storageSpec.volumeClaimTemplate.spec.resources.requests.storage=100Gi ``` CloudFlow exposes Prometheus metrics on port 9090 at the `/metrics` endpoint. The metrics include: - **HTTP Metrics**: Request rate, latency, error rate - **Database Metrics**: Connection pool status, query duration - **Worker Metrics**: Job queue size, processing time, failure rate - **System Metrics**: CPU usage, memory usage, heap statistics Example metrics exposed: ``` # HELP cloudflow_http_requests_total Total number of HTTP requests # TYPE cloudflow_http_requests_total counter cloudflow_http_requests_total{method=\"GET\",route=\"/api/workflows\",status=\"200\"} 15420 # HELP cloudflow_http_request_duration_seconds HTTP request duration in seconds # TYPE cloudflow_http_request_duration_seconds histogram cloudflow_http_request_duration_seconds_bucket{method=\"POST\",route=\"/api/workflows\",le=\"0.1\"} 8234 # HELP cloudflow_worker_jobs_processed_total Total number of jobs processed # TYPE cloudflow_worker_jobs_processed_total counter cloudflow_worker_jobs_processed_total{status=\"success\"} 45230 # HELP cloudflow_database_connections Current database connections # TYPE cloudflow_database_connections gauge cloudflow_database_connections{state=\"active\"} 23 ``` ### Grafana Dashboards Access Grafana to view CloudFlow dashboards: ```bash kubectl port-forward -n monitoring svc/prometheus-grafana 3000:80 ``` Import the CloudFlow dashboard (ID: 15847) or use the provided JSON template. Key dashboard panels include: 1. **API Performance**: Request rate, P95/P99 latency, error rate 2. **Resource Usage**: CPU, memory, disk I/O per pod 3. **Database Health**: Connection pool utilization, query performance 4. **Worker Status**: Queue depth, processing rate, job success/failure ratio 5. **System Overview**: Pod status, replica count, autoscaling events ### Logging with CloudWatch CloudFlow logs are automatically shipped to CloudWatch Logs.",
              "score": null
            }
          ],
          "latency_ms": 882.4164580000797,
          "query_id": "realistic_001",
          "dimension": "contextual",
          "query": "I'm building a batch job that calls CloudFlow API repeatedly, what rate limit should I expect?"
        },
        {
          "key_facts": [
            "100 requests per minute per authenticated user",
            "20 requests per minute for unauthenticated requests"
          ],
          "found_facts": [
            "100 requests per minute per authenticated user",
            "20 requests per minute for unauthenticated requests"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "api_reference"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_4",
              "content": "### Error Response Format ```json { \"error\": { \"code\": \"invalid_parameter\", \"message\": \"The 'limit' parameter must be between 1 and 100\", \"field\": \"limit\", \"request_id\": \"req_8k3m9x2p\" } } ``` ### HTTP Status Codes | Status Code | Description | Common Causes | |------------|-------------|---------------| | 400 | Bad Request | Invalid parameters, malformed JSON, validation errors | | 401 | Unauthorized | Missing or invalid authentication credentials | | 403 | Forbidden | Insufficient permissions for requested resource | | 404 | Not Found | Resource does not exist or has been deleted | | 429 | Too Many Requests | Rate limit exceeded, retry after specified period | | 500 | Internal Server Error | Unexpected server error, contact support if persists | | 502 | Bad Gateway | Temporary service issue, retry with exponential backoff | | 503 | Service Unavailable | Scheduled maintenance or temporary outage | ### Error Codes CloudFlow returns specific error codes to help you identify and resolve issues: - `invalid_parameter`: One or more request parameters are invalid - `missing_required_field`: Required field is missing from request body - `authentication_failed`: Invalid API key or token - `insufficient_permissions`: User lacks required scope or permission - `resource_not_found`: Requested resource does not exist - `rate_limit_exceeded`: Too many requests, see rate limiting section - `workflow_execution_failed`: Workflow execution encountered an error - `invalid_json`: Request body contains malformed JSON - `duplicate_resource`: Resource with same identifier already exists - `quota_exceeded`: Account quota limit reached ### Error Handling Best Practices **Python Example:** ```python import requests import time def make_api_request(url, headers, max_retries=3): for attempt in range(max_retries): try: response = requests.get(url, headers=headers) if response.status_code == 200: return response.json() elif response.status_code == 429: # Rate limit exceeded retry_after = int(response.headers.get('X-RateLimit-Reset', 60)) print(f\"Rate limited. Waiting {retry_after} seconds...\") time.sleep(retry_after) continue elif response.status_code >= 500: # Server error - retry with exponential backoff wait_time = 2 ** attempt print(f\"Server error. Retrying in {wait_time} seconds...\") time.sleep(wait_time) continue else: # Client error - don't retry error_data = response.json() raise Exception(f\"API Error: {error_data['error']['message']}\") except requests.exceptions.RequestException as e: print(f\"Request failed: {e}\") if attempt < max_retries - 1: time.sleep(2 ** attempt) else: raise raise Exception(\"Max retries exceeded\") ``` ## Webhooks CloudFlow can send webhook notifications when specific events occur in your workflows.",
              "score": null
            },
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_1",
              "content": "## Rate Limiting To ensure fair usage and system stability, CloudFlow enforces rate limits on all API endpoints. **Default Limits:** - 100 requests per minute per authenticated user - 20 requests per minute for unauthenticated requests - Burst allowance: 150 requests in a 10-second window ### Rate Limit Headers Every API response includes rate limit information: ``` X-RateLimit-Limit: 100 X-RateLimit-Remaining: 87 X-RateLimit-Reset: 1640995200 ``` When you exceed the rate limit, you'll receive a `429 Too Many Requests` response: ```json { \"error\": { \"code\": \"rate_limit_exceeded\", \"message\": \"Rate limit exceeded. Please retry after 42 seconds.\", \"retry_after\": 42 } } ``` **Best Practices:** - Monitor `X-RateLimit-Remaining` header values - Implement exponential backoff when receiving 429 responses - Cache responses when appropriate to reduce API calls - Consider upgrading to Enterprise tier for higher limits (1000 req/min) ## Pagination List endpoints return paginated results to optimize performance. Use `limit` and `offset` parameters to navigate through result sets. **Parameters:** - `limit`: Number of items per page (default: 20, max: 100) - `offset`: Number of items to skip (default: 0) **Example Request:** ```bash curl -H \"X-API-Key: YOUR_API_KEY\" \\ \"https://api.cloudflow.io/v2/workflows?limit=50&offset=100\" ``` **Response Structure:** ```json { \"data\": [...], \"pagination\": { \"total\": 347, \"limit\": 50, \"offset\": 100, \"has_more\": true } } ``` **Python Example:** ```python import requests def fetch_all_workflows(api_key): base_url = \"https://api.cloudflow.io/v2/workflows\" headers = {\"X-API-Key\": api_key} all_workflows = [] offset = 0 limit = 100 while True: response = requests.get( base_url, headers=headers, params={\"limit\": limit, \"offset\": offset} ) data = response.json() all_workflows.extend(data['data']) if not data['pagination']['has_more']: break offset += limit return all_workflows ``` ## Endpoints ### Workflows #### List Workflows Retrieve a paginated list of all workflows in your account. **Endpoint:** `GET /workflows` **Query Parameters:** - `status` (optional): Filter by status (`active`, `paused`, `archived`) - `created_after` (optional): ISO 8601 timestamp - `limit` (optional): Items per page (max 100) - `offset` (optional): Pagination offset **Example Request:** ```bash curl -H \"X-API-Key: YOUR_API_KEY\" \\ \"https://api.cloudflow.io/v2/workflows?status=active&limit=25\" ``` **Example Response (200 OK):** ```json { \"data\": [ { \"id\": \"wf_8x7k2m9p\", \"name\": \"Data Processing Pipeline\", \"description\": \"Processes customer data every hour\", \"status\": \"active\", \"trigger\": { \"type\": \"schedule\", \"cron\": \"0 * * * *\" }, \"steps\": 12, \"created_at\": \"2026-01-15T10:30:00Z\", \"updated_at\": \"2026-01-23T14:22:00Z\", \"last_run\": \"2026-01-24T09:00:00Z\" } ], \"pagination\": { \"total\": 1, \"limit\": 25, \"offset\": 0, \"has_more\": false } } ``` #### Create Workflow Create a new workflow with specified configuration.",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_6",
              "content": "Data Validation Errors** ``` ValidationError: Field 'customer_id' is required but missing in 234 records ``` **Resolution:** ```bash # Add data quality checks cloudflow workflows update wf_9k2n4m8p1q \\ --step data_ingestion \\ --add-validator required_fields \\ --validator-config '{\"fields\": [\"customer_id\", \"timestamp\", \"amount\"]}' # Configure error handling cloudflow workflows update wf_9k2n4m8p1q \\ --step data_validation \\ --on-error continue \\ --error-threshold 5% # Fail if > 5% of records invalid ``` **2. External API Failures** ``` ExternalAPIError: API request to https://partner-api.example.com failed with status 502 ``` **Resolution:** ```bash # Add circuit breaker cloudflow workflows update wf_9k2n4m8p1q \\ --step external_api_call \\ --circuit-breaker-enabled true \\ --circuit-breaker-threshold 5 \\ --circuit-breaker-timeout 30000 # Configure fallback behavior cloudflow workflows update wf_9k2n4m8p1q \\ --step external_api_call \\ --fallback-action use_cached_data \\ --cache-ttl 3600 ``` --- ## Rate Limiting & Throttling ### 429 Too Many Requests #### Error Response ```json { \"error\": \"rate_limit_exceeded\", \"message\": \"API rate limit exceeded. Retry after 45 seconds.\", \"status\": 429, \"headers\": { \"X-RateLimit-Limit\": \"1000\", \"X-RateLimit-Remaining\": \"0\", \"X-RateLimit-Reset\": \"1706112345\", \"Retry-After\": \"45\" } } ``` #### Rate Limit Tiers CloudFlow enforces the following rate limits per workspace: | Tier | Requests/Minute | Requests/Hour | Concurrent Workflows | |------|-----------------|---------------|----------------------| | Free | 60 | 1,000 | 5 | | Standard | 1,000 | 50,000 | 50 | | Premium | 5,000 | 250,000 | 200 | | Enterprise | Custom | Custom | Unlimited | #### Checking Rate Limit Status ```bash # Check current rate limit status curl -I https://api.cloudflow.io/api/v1/workflows \\ -H \"Authorization: Bearer $CF_ACCESS_TOKEN\" # Extract rate limit headers curl -s -I https://api.cloudflow.io/api/v1/workflows \\ -H \"Authorization: Bearer $CF_ACCESS_TOKEN\" | \\ grep -i \"x-ratelimit\" # Output: # X-RateLimit-Limit: 1000 # X-RateLimit-Remaining: 247 # X-RateLimit-Reset: 1706112400 # Monitor rate limit usage cloudflow metrics query \\ --metric rate_limit_remaining \\ --workspace ws_abc123 \\ --last 1h \\ --interval 1m ``` #### Handling Rate Limits in Code **Python example with retry logic:** ```python import time import requests def cloudflow_api_call_with_retry(url, headers, max_retries=3): for attempt in range(max_retries): response = requests.get(url, headers=headers) if response.status_code == 429: retry_after = int(response.headers.get('Retry-After', 60)) print(f\"Rate limited.",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_7",
              "content": "Waiting {retry_after} seconds...\") time.sleep(retry_after) continue remaining = int(response.headers.get('X-RateLimit-Remaining', 0)) if remaining < 10: print(f\"Warning: Only {remaining} requests remaining\") return response raise Exception(\"Max retries exceeded due to rate limiting\") ``` **Bash script with rate limit checking:** ```bash #!/bin/bash check_rate_limit() { local remaining=$(curl -s -I https://api.cloudflow.io/api/v1/workflows \\ -H \"Authorization: Bearer $CF_ACCESS_TOKEN\" | \\ grep -i \"x-ratelimit-remaining\" | \\ awk '{print $2}' | tr -d '\\r') if [ \"$remaining\" -lt 10 ]; then echo \"Warning: Only $remaining requests remaining\" local reset=$(curl -s -I https://api.cloudflow.io/api/v1/workflows \\ -H \"Authorization: Bearer $CF_ACCESS_TOKEN\" | \\ grep -i \"x-ratelimit-reset\" | \\ awk '{print $2}' | tr -d '\\r') local wait_time=$((reset - $(date +%s))) echo \"Rate limit resets in $wait_time seconds\" sleep $wait_time fi } # Use before API calls check_rate_limit cloudflow workflows execute wf_9k2n4m8p1q ``` #### Optimization Strategies **1. Implement request batching:** ```bash # Batch multiple workflow executions cloudflow workflows execute-batch \\ --workflow-ids \"wf_id1,wf_id2,wf_id3,wf_id4,wf_id5\" \\ --batch-size 5 # This counts as 1 API request instead of 5 ``` **2. Use webhooks instead of polling:** ```bash # Configure webhook for workflow completion cloudflow webhooks create \\ --event workflow.completed \\ --url https://your-service.com/webhooks/cloudflow \\ --secret $WEBHOOK_SECRET # Verify webhook cloudflow webhooks test webhook_abc123 ``` **3.",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_1",
              "content": "**Diagnosis:** ```bash # Check system time timedatectl status # Compare with NTP server ntpdate -q pool.ntp.org # Check JWT issued time vs current time jwt_iat=$(echo $CF_ACCESS_TOKEN | cut -d'.' -f2 | base64 -d | jq '.iat') current_time=$(date +%s) skew=$((current_time - jwt_iat)) echo \"Clock skew: $skew seconds\" ``` **Resolution:** ```bash # Sync with NTP server sudo ntpdate -s pool.ntp.org # Enable automatic time sync sudo timedatectl set-ntp true # Restart CloudFlow client cloudflow restart ``` **4. Insufficient Permissions** Error: `User does not have required permissions for this operation` **Check user roles:** ```bash cloudflow auth whoami --verbose # Expected output: # User: john.doe@company.com # Roles: developer, workflow-admin # Scopes: workflow:*, data:read, metrics:read ``` **Request permission elevation:** ```bash # Submit access request cloudflow auth request-access \\ --resource \"workflow:production:deploy\" \\ --justification \"Deploy critical hotfix for TICKET-1234\" # Check pending approvals cloudflow auth list-requests --status pending ``` ### 403 Forbidden Errors #### RBAC Policy Violations CloudFlow uses role-based access control (RBAC) with the following hierarchy: - `viewer` - Read-only access - `developer` - Create and modify workflows (non-production) - `operator` - Execute workflows, view logs - `admin` - Full access to workspace - `platform-admin` - Cross-workspace administration **Verify resource permissions:** ```bash # Check effective permissions cloudflow rbac check \\ --user john.doe@company.com \\ --resource workflow:prod-pipeline \\ --action execute # List all policies affecting user cloudflow rbac policies --user john.doe@company.com --verbose ``` --- ## Performance Problems ### Slow Query Performance #### Symptoms - API response times > 5000ms - Database query latency warnings in logs - CloudFlow UI becomes unresponsive - Timeout errors: `Request exceeded maximum duration of 30000ms` #### Diagnosis Steps **1. Identify Slow Queries** ```bash # Query CloudFlow metrics cloudflow metrics query --metric api_request_duration_ms \\ --filter \"p95 > 5000\" \\ --last 1h # Check database slow query log kubectl logs -n cloudflow deploy/cloudflow-db-primary | \\ grep \"slow query\" | \\ tail -n 50 # Analyze query patterns cloudflow db analyze-queries --min-duration 5000 --limit 20 ``` **2.",
              "score": null
            }
          ],
          "latency_ms": 937.8415179999138,
          "query_id": "realistic_001",
          "dimension": "negation",
          "query": "Why is the API rejecting my requests after 100 calls?"
        },
        {
          "key_facts": [
            "429 Too Many Requests",
            "X-RateLimit-Remaining",
            "Retry-After",
            "Monitor X-RateLimit-Remaining header values",
            "Implement exponential backoff when receiving 429 responses"
          ],
          "found_facts": [
            "429 Too Many Requests",
            "X-RateLimit-Remaining",
            "Retry-After",
            "Implement exponential backoff when receiving 429 responses"
          ],
          "missed_facts": [
            "Monitor X-RateLimit-Remaining header values"
          ],
          "coverage": 0.8,
          "expected_docs": [
            "api_reference",
            "troubleshooting_guide"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_6",
              "content": "Data Validation Errors** ``` ValidationError: Field 'customer_id' is required but missing in 234 records ``` **Resolution:** ```bash # Add data quality checks cloudflow workflows update wf_9k2n4m8p1q \\ --step data_ingestion \\ --add-validator required_fields \\ --validator-config '{\"fields\": [\"customer_id\", \"timestamp\", \"amount\"]}' # Configure error handling cloudflow workflows update wf_9k2n4m8p1q \\ --step data_validation \\ --on-error continue \\ --error-threshold 5% # Fail if > 5% of records invalid ``` **2. External API Failures** ``` ExternalAPIError: API request to https://partner-api.example.com failed with status 502 ``` **Resolution:** ```bash # Add circuit breaker cloudflow workflows update wf_9k2n4m8p1q \\ --step external_api_call \\ --circuit-breaker-enabled true \\ --circuit-breaker-threshold 5 \\ --circuit-breaker-timeout 30000 # Configure fallback behavior cloudflow workflows update wf_9k2n4m8p1q \\ --step external_api_call \\ --fallback-action use_cached_data \\ --cache-ttl 3600 ``` --- ## Rate Limiting & Throttling ### 429 Too Many Requests #### Error Response ```json { \"error\": \"rate_limit_exceeded\", \"message\": \"API rate limit exceeded. Retry after 45 seconds.\", \"status\": 429, \"headers\": { \"X-RateLimit-Limit\": \"1000\", \"X-RateLimit-Remaining\": \"0\", \"X-RateLimit-Reset\": \"1706112345\", \"Retry-After\": \"45\" } } ``` #### Rate Limit Tiers CloudFlow enforces the following rate limits per workspace: | Tier | Requests/Minute | Requests/Hour | Concurrent Workflows | |------|-----------------|---------------|----------------------| | Free | 60 | 1,000 | 5 | | Standard | 1,000 | 50,000 | 50 | | Premium | 5,000 | 250,000 | 200 | | Enterprise | Custom | Custom | Unlimited | #### Checking Rate Limit Status ```bash # Check current rate limit status curl -I https://api.cloudflow.io/api/v1/workflows \\ -H \"Authorization: Bearer $CF_ACCESS_TOKEN\" # Extract rate limit headers curl -s -I https://api.cloudflow.io/api/v1/workflows \\ -H \"Authorization: Bearer $CF_ACCESS_TOKEN\" | \\ grep -i \"x-ratelimit\" # Output: # X-RateLimit-Limit: 1000 # X-RateLimit-Remaining: 247 # X-RateLimit-Reset: 1706112400 # Monitor rate limit usage cloudflow metrics query \\ --metric rate_limit_remaining \\ --workspace ws_abc123 \\ --last 1h \\ --interval 1m ``` #### Handling Rate Limits in Code **Python example with retry logic:** ```python import time import requests def cloudflow_api_call_with_retry(url, headers, max_retries=3): for attempt in range(max_retries): response = requests.get(url, headers=headers) if response.status_code == 429: retry_after = int(response.headers.get('Retry-After', 60)) print(f\"Rate limited.",
              "score": null
            },
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_1",
              "content": "## Rate Limiting To ensure fair usage and system stability, CloudFlow enforces rate limits on all API endpoints. **Default Limits:** - 100 requests per minute per authenticated user - 20 requests per minute for unauthenticated requests - Burst allowance: 150 requests in a 10-second window ### Rate Limit Headers Every API response includes rate limit information: ``` X-RateLimit-Limit: 100 X-RateLimit-Remaining: 87 X-RateLimit-Reset: 1640995200 ``` When you exceed the rate limit, you'll receive a `429 Too Many Requests` response: ```json { \"error\": { \"code\": \"rate_limit_exceeded\", \"message\": \"Rate limit exceeded. Please retry after 42 seconds.\", \"retry_after\": 42 } } ``` **Best Practices:** - Monitor `X-RateLimit-Remaining` header values - Implement exponential backoff when receiving 429 responses - Cache responses when appropriate to reduce API calls - Consider upgrading to Enterprise tier for higher limits (1000 req/min) ## Pagination List endpoints return paginated results to optimize performance. Use `limit` and `offset` parameters to navigate through result sets. **Parameters:** - `limit`: Number of items per page (default: 20, max: 100) - `offset`: Number of items to skip (default: 0) **Example Request:** ```bash curl -H \"X-API-Key: YOUR_API_KEY\" \\ \"https://api.cloudflow.io/v2/workflows?limit=50&offset=100\" ``` **Response Structure:** ```json { \"data\": [...], \"pagination\": { \"total\": 347, \"limit\": 50, \"offset\": 100, \"has_more\": true } } ``` **Python Example:** ```python import requests def fetch_all_workflows(api_key): base_url = \"https://api.cloudflow.io/v2/workflows\" headers = {\"X-API-Key\": api_key} all_workflows = [] offset = 0 limit = 100 while True: response = requests.get( base_url, headers=headers, params={\"limit\": limit, \"offset\": offset} ) data = response.json() all_workflows.extend(data['data']) if not data['pagination']['has_more']: break offset += limit return all_workflows ``` ## Endpoints ### Workflows #### List Workflows Retrieve a paginated list of all workflows in your account. **Endpoint:** `GET /workflows` **Query Parameters:** - `status` (optional): Filter by status (`active`, `paused`, `archived`) - `created_after` (optional): ISO 8601 timestamp - `limit` (optional): Items per page (max 100) - `offset` (optional): Pagination offset **Example Request:** ```bash curl -H \"X-API-Key: YOUR_API_KEY\" \\ \"https://api.cloudflow.io/v2/workflows?status=active&limit=25\" ``` **Example Response (200 OK):** ```json { \"data\": [ { \"id\": \"wf_8x7k2m9p\", \"name\": \"Data Processing Pipeline\", \"description\": \"Processes customer data every hour\", \"status\": \"active\", \"trigger\": { \"type\": \"schedule\", \"cron\": \"0 * * * *\" }, \"steps\": 12, \"created_at\": \"2026-01-15T10:30:00Z\", \"updated_at\": \"2026-01-23T14:22:00Z\", \"last_run\": \"2026-01-24T09:00:00Z\" } ], \"pagination\": { \"total\": 1, \"limit\": 25, \"offset\": 0, \"has_more\": false } } ``` #### Create Workflow Create a new workflow with specified configuration.",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_7",
              "content": "Waiting {retry_after} seconds...\") time.sleep(retry_after) continue remaining = int(response.headers.get('X-RateLimit-Remaining', 0)) if remaining < 10: print(f\"Warning: Only {remaining} requests remaining\") return response raise Exception(\"Max retries exceeded due to rate limiting\") ``` **Bash script with rate limit checking:** ```bash #!/bin/bash check_rate_limit() { local remaining=$(curl -s -I https://api.cloudflow.io/api/v1/workflows \\ -H \"Authorization: Bearer $CF_ACCESS_TOKEN\" | \\ grep -i \"x-ratelimit-remaining\" | \\ awk '{print $2}' | tr -d '\\r') if [ \"$remaining\" -lt 10 ]; then echo \"Warning: Only $remaining requests remaining\" local reset=$(curl -s -I https://api.cloudflow.io/api/v1/workflows \\ -H \"Authorization: Bearer $CF_ACCESS_TOKEN\" | \\ grep -i \"x-ratelimit-reset\" | \\ awk '{print $2}' | tr -d '\\r') local wait_time=$((reset - $(date +%s))) echo \"Rate limit resets in $wait_time seconds\" sleep $wait_time fi } # Use before API calls check_rate_limit cloudflow workflows execute wf_9k2n4m8p1q ``` #### Optimization Strategies **1. Implement request batching:** ```bash # Batch multiple workflow executions cloudflow workflows execute-batch \\ --workflow-ids \"wf_id1,wf_id2,wf_id3,wf_id4,wf_id5\" \\ --batch-size 5 # This counts as 1 API request instead of 5 ``` **2. Use webhooks instead of polling:** ```bash # Configure webhook for workflow completion cloudflow webhooks create \\ --event workflow.completed \\ --url https://your-service.com/webhooks/cloudflow \\ --secret $WEBHOOK_SECRET # Verify webhook cloudflow webhooks test webhook_abc123 ``` **3.",
              "score": null
            },
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_4",
              "content": "### Error Response Format ```json { \"error\": { \"code\": \"invalid_parameter\", \"message\": \"The 'limit' parameter must be between 1 and 100\", \"field\": \"limit\", \"request_id\": \"req_8k3m9x2p\" } } ``` ### HTTP Status Codes | Status Code | Description | Common Causes | |------------|-------------|---------------| | 400 | Bad Request | Invalid parameters, malformed JSON, validation errors | | 401 | Unauthorized | Missing or invalid authentication credentials | | 403 | Forbidden | Insufficient permissions for requested resource | | 404 | Not Found | Resource does not exist or has been deleted | | 429 | Too Many Requests | Rate limit exceeded, retry after specified period | | 500 | Internal Server Error | Unexpected server error, contact support if persists | | 502 | Bad Gateway | Temporary service issue, retry with exponential backoff | | 503 | Service Unavailable | Scheduled maintenance or temporary outage | ### Error Codes CloudFlow returns specific error codes to help you identify and resolve issues: - `invalid_parameter`: One or more request parameters are invalid - `missing_required_field`: Required field is missing from request body - `authentication_failed`: Invalid API key or token - `insufficient_permissions`: User lacks required scope or permission - `resource_not_found`: Requested resource does not exist - `rate_limit_exceeded`: Too many requests, see rate limiting section - `workflow_execution_failed`: Workflow execution encountered an error - `invalid_json`: Request body contains malformed JSON - `duplicate_resource`: Resource with same identifier already exists - `quota_exceeded`: Account quota limit reached ### Error Handling Best Practices **Python Example:** ```python import requests import time def make_api_request(url, headers, max_retries=3): for attempt in range(max_retries): try: response = requests.get(url, headers=headers) if response.status_code == 200: return response.json() elif response.status_code == 429: # Rate limit exceeded retry_after = int(response.headers.get('X-RateLimit-Reset', 60)) print(f\"Rate limited. Waiting {retry_after} seconds...\") time.sleep(retry_after) continue elif response.status_code >= 500: # Server error - retry with exponential backoff wait_time = 2 ** attempt print(f\"Server error. Retrying in {wait_time} seconds...\") time.sleep(wait_time) continue else: # Client error - don't retry error_data = response.json() raise Exception(f\"API Error: {error_data['error']['message']}\") except requests.exceptions.RequestException as e: print(f\"Request failed: {e}\") if attempt < max_retries - 1: time.sleep(2 ** attempt) else: raise raise Exception(\"Max retries exceeded\") ``` ## Webhooks CloudFlow can send webhook notifications when specific events occur in your workflows.",
              "score": null
            },
            {
              "doc_id": "user_guide",
              "chunk_id": "user_guide_fix_5",
              "content": "Save and activate **Testing Schedules:** Use the built-in schedule calculator to preview upcoming executions: ``` Next 5 executions: 1. 2026-01-24 14:00:00 UTC 2. 2026-01-25 14:00:00 UTC 3. 2026-01-26 14:00:00 UTC 4. 2026-01-27 14:00:00 UTC 5. 2026-01-28 14:00:00 UTC ``` ## Error Handling Robust error handling ensures your workflows are resilient and reliable. ### Retry Policies Configure automatic retries for failed actions: ```yaml - id: api_call action: http_request config: url: \"https://api.example.com/data\" retry: max_attempts: 3 backoff_type: \"exponential\" # or \"fixed\", \"linear\" initial_interval: 1000 # milliseconds max_interval: 30000 multiplier: 2.0 retry_on: - timeout - network_error - status: [500, 502, 503, 504] ``` **Backoff Strategies:** - **Fixed**: Wait the same amount of time between retries - Attempt 1: Wait 1s - Attempt 2: Wait 1s - Attempt 3: Wait 1s - **Linear**: Increase wait time by a fixed amount - Attempt 1: Wait 1s - Attempt 2: Wait 2s - Attempt 3: Wait 3s - **Exponential**: Double the wait time with each retry (recommended) - Attempt 1: Wait 1s - Attempt 2: Wait 2s - Attempt 3: Wait 4s **Retry Conditions:** Control which errors trigger retries: - `timeout`: Request timeout - `network_error`: Connection failures - `status`: Specific HTTP status codes - `error_code`: Application-specific error codes ### Fallback Actions Execute alternative actions when the primary action fails: ```yaml - id: primary_payment action: http_request config: url: \"https://primary-payment-gateway.com/charge\" method: POST body: amount: \"{{amount}}\" on_error: - id: fallback_payment action: http_request config: url: \"https://backup-payment-gateway.com/charge\" method: POST body: amount: \"{{amount}}\" - id: notify_admin action: email config: to: \"admin@company.com\" subject: \"Payment Gateway Failure\" body: \"Primary gateway failed, switched to backup\" ``` **Error Object:** Access error details in fallback actions: ``` {{error.message}} # Error message {{error.code}} # Error code {{error.step_id}} # Failed step ID {{error.timestamp}} # When the error occurred {{error.attempts}} # Number of retry attempts ``` ### Dead Letter Queue When all retries and fallbacks fail, CloudFlow can route failed executions to a Dead Letter Queue (DLQ) for manual review: **Enable DLQ:** ```yaml workflow: error_handling: dead_letter_queue: enabled: true retain_days: 30 ``` **DLQ Features:** - View failed executions in the dashboard - Inspect complete execution context and error details - Retry individual executions after fixing issues - Export failed executions for analysis - Set up alerts for DLQ threshold breaches **Accessing the DLQ:** 1. Navigate to **Workflows** > **[Your Workflow]** > **Dead Letter Queue** 2.",
              "score": null
            }
          ],
          "latency_ms": 1030.247826998675,
          "query_id": "realistic_002",
          "dimension": "original",
          "query": "How do I fix 429 Too Many Requests errors?"
        },
        {
          "key_facts": [
            "429 Too Many Requests",
            "X-RateLimit-Remaining",
            "Retry-After",
            "Monitor X-RateLimit-Remaining header values",
            "Implement exponential backoff when receiving 429 responses"
          ],
          "found_facts": [
            "429 Too Many Requests",
            "X-RateLimit-Remaining",
            "Retry-After",
            "Implement exponential backoff when receiving 429 responses"
          ],
          "missed_facts": [
            "Monitor X-RateLimit-Remaining header values"
          ],
          "coverage": 0.8,
          "expected_docs": [
            "api_reference",
            "troubleshooting_guide"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "user_guide",
              "chunk_id": "user_guide_fix_5",
              "content": "Save and activate **Testing Schedules:** Use the built-in schedule calculator to preview upcoming executions: ``` Next 5 executions: 1. 2026-01-24 14:00:00 UTC 2. 2026-01-25 14:00:00 UTC 3. 2026-01-26 14:00:00 UTC 4. 2026-01-27 14:00:00 UTC 5. 2026-01-28 14:00:00 UTC ``` ## Error Handling Robust error handling ensures your workflows are resilient and reliable. ### Retry Policies Configure automatic retries for failed actions: ```yaml - id: api_call action: http_request config: url: \"https://api.example.com/data\" retry: max_attempts: 3 backoff_type: \"exponential\" # or \"fixed\", \"linear\" initial_interval: 1000 # milliseconds max_interval: 30000 multiplier: 2.0 retry_on: - timeout - network_error - status: [500, 502, 503, 504] ``` **Backoff Strategies:** - **Fixed**: Wait the same amount of time between retries - Attempt 1: Wait 1s - Attempt 2: Wait 1s - Attempt 3: Wait 1s - **Linear**: Increase wait time by a fixed amount - Attempt 1: Wait 1s - Attempt 2: Wait 2s - Attempt 3: Wait 3s - **Exponential**: Double the wait time with each retry (recommended) - Attempt 1: Wait 1s - Attempt 2: Wait 2s - Attempt 3: Wait 4s **Retry Conditions:** Control which errors trigger retries: - `timeout`: Request timeout - `network_error`: Connection failures - `status`: Specific HTTP status codes - `error_code`: Application-specific error codes ### Fallback Actions Execute alternative actions when the primary action fails: ```yaml - id: primary_payment action: http_request config: url: \"https://primary-payment-gateway.com/charge\" method: POST body: amount: \"{{amount}}\" on_error: - id: fallback_payment action: http_request config: url: \"https://backup-payment-gateway.com/charge\" method: POST body: amount: \"{{amount}}\" - id: notify_admin action: email config: to: \"admin@company.com\" subject: \"Payment Gateway Failure\" body: \"Primary gateway failed, switched to backup\" ``` **Error Object:** Access error details in fallback actions: ``` {{error.message}} # Error message {{error.code}} # Error code {{error.step_id}} # Failed step ID {{error.timestamp}} # When the error occurred {{error.attempts}} # Number of retry attempts ``` ### Dead Letter Queue When all retries and fallbacks fail, CloudFlow can route failed executions to a Dead Letter Queue (DLQ) for manual review: **Enable DLQ:** ```yaml workflow: error_handling: dead_letter_queue: enabled: true retain_days: 30 ``` **DLQ Features:** - View failed executions in the dashboard - Inspect complete execution context and error details - Retry individual executions after fixing issues - Export failed executions for analysis - Set up alerts for DLQ threshold breaches **Accessing the DLQ:** 1. Navigate to **Workflows** > **[Your Workflow]** > **Dead Letter Queue** 2.",
              "score": null
            },
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_1",
              "content": "## Rate Limiting To ensure fair usage and system stability, CloudFlow enforces rate limits on all API endpoints. **Default Limits:** - 100 requests per minute per authenticated user - 20 requests per minute for unauthenticated requests - Burst allowance: 150 requests in a 10-second window ### Rate Limit Headers Every API response includes rate limit information: ``` X-RateLimit-Limit: 100 X-RateLimit-Remaining: 87 X-RateLimit-Reset: 1640995200 ``` When you exceed the rate limit, you'll receive a `429 Too Many Requests` response: ```json { \"error\": { \"code\": \"rate_limit_exceeded\", \"message\": \"Rate limit exceeded. Please retry after 42 seconds.\", \"retry_after\": 42 } } ``` **Best Practices:** - Monitor `X-RateLimit-Remaining` header values - Implement exponential backoff when receiving 429 responses - Cache responses when appropriate to reduce API calls - Consider upgrading to Enterprise tier for higher limits (1000 req/min) ## Pagination List endpoints return paginated results to optimize performance. Use `limit` and `offset` parameters to navigate through result sets. **Parameters:** - `limit`: Number of items per page (default: 20, max: 100) - `offset`: Number of items to skip (default: 0) **Example Request:** ```bash curl -H \"X-API-Key: YOUR_API_KEY\" \\ \"https://api.cloudflow.io/v2/workflows?limit=50&offset=100\" ``` **Response Structure:** ```json { \"data\": [...], \"pagination\": { \"total\": 347, \"limit\": 50, \"offset\": 100, \"has_more\": true } } ``` **Python Example:** ```python import requests def fetch_all_workflows(api_key): base_url = \"https://api.cloudflow.io/v2/workflows\" headers = {\"X-API-Key\": api_key} all_workflows = [] offset = 0 limit = 100 while True: response = requests.get( base_url, headers=headers, params={\"limit\": limit, \"offset\": offset} ) data = response.json() all_workflows.extend(data['data']) if not data['pagination']['has_more']: break offset += limit return all_workflows ``` ## Endpoints ### Workflows #### List Workflows Retrieve a paginated list of all workflows in your account. **Endpoint:** `GET /workflows` **Query Parameters:** - `status` (optional): Filter by status (`active`, `paused`, `archived`) - `created_after` (optional): ISO 8601 timestamp - `limit` (optional): Items per page (max 100) - `offset` (optional): Pagination offset **Example Request:** ```bash curl -H \"X-API-Key: YOUR_API_KEY\" \\ \"https://api.cloudflow.io/v2/workflows?status=active&limit=25\" ``` **Example Response (200 OK):** ```json { \"data\": [ { \"id\": \"wf_8x7k2m9p\", \"name\": \"Data Processing Pipeline\", \"description\": \"Processes customer data every hour\", \"status\": \"active\", \"trigger\": { \"type\": \"schedule\", \"cron\": \"0 * * * *\" }, \"steps\": 12, \"created_at\": \"2026-01-15T10:30:00Z\", \"updated_at\": \"2026-01-23T14:22:00Z\", \"last_run\": \"2026-01-24T09:00:00Z\" } ], \"pagination\": { \"total\": 1, \"limit\": 25, \"offset\": 0, \"has_more\": false } } ``` #### Create Workflow Create a new workflow with specified configuration.",
              "score": null
            },
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_4",
              "content": "### Error Response Format ```json { \"error\": { \"code\": \"invalid_parameter\", \"message\": \"The 'limit' parameter must be between 1 and 100\", \"field\": \"limit\", \"request_id\": \"req_8k3m9x2p\" } } ``` ### HTTP Status Codes | Status Code | Description | Common Causes | |------------|-------------|---------------| | 400 | Bad Request | Invalid parameters, malformed JSON, validation errors | | 401 | Unauthorized | Missing or invalid authentication credentials | | 403 | Forbidden | Insufficient permissions for requested resource | | 404 | Not Found | Resource does not exist or has been deleted | | 429 | Too Many Requests | Rate limit exceeded, retry after specified period | | 500 | Internal Server Error | Unexpected server error, contact support if persists | | 502 | Bad Gateway | Temporary service issue, retry with exponential backoff | | 503 | Service Unavailable | Scheduled maintenance or temporary outage | ### Error Codes CloudFlow returns specific error codes to help you identify and resolve issues: - `invalid_parameter`: One or more request parameters are invalid - `missing_required_field`: Required field is missing from request body - `authentication_failed`: Invalid API key or token - `insufficient_permissions`: User lacks required scope or permission - `resource_not_found`: Requested resource does not exist - `rate_limit_exceeded`: Too many requests, see rate limiting section - `workflow_execution_failed`: Workflow execution encountered an error - `invalid_json`: Request body contains malformed JSON - `duplicate_resource`: Resource with same identifier already exists - `quota_exceeded`: Account quota limit reached ### Error Handling Best Practices **Python Example:** ```python import requests import time def make_api_request(url, headers, max_retries=3): for attempt in range(max_retries): try: response = requests.get(url, headers=headers) if response.status_code == 200: return response.json() elif response.status_code == 429: # Rate limit exceeded retry_after = int(response.headers.get('X-RateLimit-Reset', 60)) print(f\"Rate limited. Waiting {retry_after} seconds...\") time.sleep(retry_after) continue elif response.status_code >= 500: # Server error - retry with exponential backoff wait_time = 2 ** attempt print(f\"Server error. Retrying in {wait_time} seconds...\") time.sleep(wait_time) continue else: # Client error - don't retry error_data = response.json() raise Exception(f\"API Error: {error_data['error']['message']}\") except requests.exceptions.RequestException as e: print(f\"Request failed: {e}\") if attempt < max_retries - 1: time.sleep(2 ** attempt) else: raise raise Exception(\"Max retries exceeded\") ``` ## Webhooks CloudFlow can send webhook notifications when specific events occur in your workflows.",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_6",
              "content": "Data Validation Errors** ``` ValidationError: Field 'customer_id' is required but missing in 234 records ``` **Resolution:** ```bash # Add data quality checks cloudflow workflows update wf_9k2n4m8p1q \\ --step data_ingestion \\ --add-validator required_fields \\ --validator-config '{\"fields\": [\"customer_id\", \"timestamp\", \"amount\"]}' # Configure error handling cloudflow workflows update wf_9k2n4m8p1q \\ --step data_validation \\ --on-error continue \\ --error-threshold 5% # Fail if > 5% of records invalid ``` **2. External API Failures** ``` ExternalAPIError: API request to https://partner-api.example.com failed with status 502 ``` **Resolution:** ```bash # Add circuit breaker cloudflow workflows update wf_9k2n4m8p1q \\ --step external_api_call \\ --circuit-breaker-enabled true \\ --circuit-breaker-threshold 5 \\ --circuit-breaker-timeout 30000 # Configure fallback behavior cloudflow workflows update wf_9k2n4m8p1q \\ --step external_api_call \\ --fallback-action use_cached_data \\ --cache-ttl 3600 ``` --- ## Rate Limiting & Throttling ### 429 Too Many Requests #### Error Response ```json { \"error\": \"rate_limit_exceeded\", \"message\": \"API rate limit exceeded. Retry after 45 seconds.\", \"status\": 429, \"headers\": { \"X-RateLimit-Limit\": \"1000\", \"X-RateLimit-Remaining\": \"0\", \"X-RateLimit-Reset\": \"1706112345\", \"Retry-After\": \"45\" } } ``` #### Rate Limit Tiers CloudFlow enforces the following rate limits per workspace: | Tier | Requests/Minute | Requests/Hour | Concurrent Workflows | |------|-----------------|---------------|----------------------| | Free | 60 | 1,000 | 5 | | Standard | 1,000 | 50,000 | 50 | | Premium | 5,000 | 250,000 | 200 | | Enterprise | Custom | Custom | Unlimited | #### Checking Rate Limit Status ```bash # Check current rate limit status curl -I https://api.cloudflow.io/api/v1/workflows \\ -H \"Authorization: Bearer $CF_ACCESS_TOKEN\" # Extract rate limit headers curl -s -I https://api.cloudflow.io/api/v1/workflows \\ -H \"Authorization: Bearer $CF_ACCESS_TOKEN\" | \\ grep -i \"x-ratelimit\" # Output: # X-RateLimit-Limit: 1000 # X-RateLimit-Remaining: 247 # X-RateLimit-Reset: 1706112400 # Monitor rate limit usage cloudflow metrics query \\ --metric rate_limit_remaining \\ --workspace ws_abc123 \\ --last 1h \\ --interval 1m ``` #### Handling Rate Limits in Code **Python example with retry logic:** ```python import time import requests def cloudflow_api_call_with_retry(url, headers, max_retries=3): for attempt in range(max_retries): response = requests.get(url, headers=headers) if response.status_code == 429: retry_after = int(response.headers.get('Retry-After', 60)) print(f\"Rate limited.",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_7",
              "content": "Waiting {retry_after} seconds...\") time.sleep(retry_after) continue remaining = int(response.headers.get('X-RateLimit-Remaining', 0)) if remaining < 10: print(f\"Warning: Only {remaining} requests remaining\") return response raise Exception(\"Max retries exceeded due to rate limiting\") ``` **Bash script with rate limit checking:** ```bash #!/bin/bash check_rate_limit() { local remaining=$(curl -s -I https://api.cloudflow.io/api/v1/workflows \\ -H \"Authorization: Bearer $CF_ACCESS_TOKEN\" | \\ grep -i \"x-ratelimit-remaining\" | \\ awk '{print $2}' | tr -d '\\r') if [ \"$remaining\" -lt 10 ]; then echo \"Warning: Only $remaining requests remaining\" local reset=$(curl -s -I https://api.cloudflow.io/api/v1/workflows \\ -H \"Authorization: Bearer $CF_ACCESS_TOKEN\" | \\ grep -i \"x-ratelimit-reset\" | \\ awk '{print $2}' | tr -d '\\r') local wait_time=$((reset - $(date +%s))) echo \"Rate limit resets in $wait_time seconds\" sleep $wait_time fi } # Use before API calls check_rate_limit cloudflow workflows execute wf_9k2n4m8p1q ``` #### Optimization Strategies **1. Implement request batching:** ```bash # Batch multiple workflow executions cloudflow workflows execute-batch \\ --workflow-ids \"wf_id1,wf_id2,wf_id3,wf_id4,wf_id5\" \\ --batch-size 5 # This counts as 1 API request instead of 5 ``` **2. Use webhooks instead of polling:** ```bash # Configure webhook for workflow completion cloudflow webhooks create \\ --event workflow.completed \\ --url https://your-service.com/webhooks/cloudflow \\ --secret $WEBHOOK_SECRET # Verify webhook cloudflow webhooks test webhook_abc123 ``` **3.",
              "score": null
            }
          ],
          "latency_ms": 1031.3343210000312,
          "query_id": "realistic_002",
          "dimension": "synonym",
          "query": "What should I do when I get rate limited?"
        },
        {
          "key_facts": [
            "429 Too Many Requests",
            "X-RateLimit-Remaining",
            "Retry-After",
            "Monitor X-RateLimit-Remaining header values",
            "Implement exponential backoff when receiving 429 responses"
          ],
          "found_facts": [
            "429 Too Many Requests",
            "X-RateLimit-Remaining",
            "Retry-After",
            "Implement exponential backoff when receiving 429 responses"
          ],
          "missed_facts": [
            "Monitor X-RateLimit-Remaining header values"
          ],
          "coverage": 0.8,
          "expected_docs": [
            "api_reference",
            "troubleshooting_guide"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_4",
              "content": "### Error Response Format ```json { \"error\": { \"code\": \"invalid_parameter\", \"message\": \"The 'limit' parameter must be between 1 and 100\", \"field\": \"limit\", \"request_id\": \"req_8k3m9x2p\" } } ``` ### HTTP Status Codes | Status Code | Description | Common Causes | |------------|-------------|---------------| | 400 | Bad Request | Invalid parameters, malformed JSON, validation errors | | 401 | Unauthorized | Missing or invalid authentication credentials | | 403 | Forbidden | Insufficient permissions for requested resource | | 404 | Not Found | Resource does not exist or has been deleted | | 429 | Too Many Requests | Rate limit exceeded, retry after specified period | | 500 | Internal Server Error | Unexpected server error, contact support if persists | | 502 | Bad Gateway | Temporary service issue, retry with exponential backoff | | 503 | Service Unavailable | Scheduled maintenance or temporary outage | ### Error Codes CloudFlow returns specific error codes to help you identify and resolve issues: - `invalid_parameter`: One or more request parameters are invalid - `missing_required_field`: Required field is missing from request body - `authentication_failed`: Invalid API key or token - `insufficient_permissions`: User lacks required scope or permission - `resource_not_found`: Requested resource does not exist - `rate_limit_exceeded`: Too many requests, see rate limiting section - `workflow_execution_failed`: Workflow execution encountered an error - `invalid_json`: Request body contains malformed JSON - `duplicate_resource`: Resource with same identifier already exists - `quota_exceeded`: Account quota limit reached ### Error Handling Best Practices **Python Example:** ```python import requests import time def make_api_request(url, headers, max_retries=3): for attempt in range(max_retries): try: response = requests.get(url, headers=headers) if response.status_code == 200: return response.json() elif response.status_code == 429: # Rate limit exceeded retry_after = int(response.headers.get('X-RateLimit-Reset', 60)) print(f\"Rate limited. Waiting {retry_after} seconds...\") time.sleep(retry_after) continue elif response.status_code >= 500: # Server error - retry with exponential backoff wait_time = 2 ** attempt print(f\"Server error. Retrying in {wait_time} seconds...\") time.sleep(wait_time) continue else: # Client error - don't retry error_data = response.json() raise Exception(f\"API Error: {error_data['error']['message']}\") except requests.exceptions.RequestException as e: print(f\"Request failed: {e}\") if attempt < max_retries - 1: time.sleep(2 ** attempt) else: raise raise Exception(\"Max retries exceeded\") ``` ## Webhooks CloudFlow can send webhook notifications when specific events occur in your workflows.",
              "score": null
            },
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_1",
              "content": "## Rate Limiting To ensure fair usage and system stability, CloudFlow enforces rate limits on all API endpoints. **Default Limits:** - 100 requests per minute per authenticated user - 20 requests per minute for unauthenticated requests - Burst allowance: 150 requests in a 10-second window ### Rate Limit Headers Every API response includes rate limit information: ``` X-RateLimit-Limit: 100 X-RateLimit-Remaining: 87 X-RateLimit-Reset: 1640995200 ``` When you exceed the rate limit, you'll receive a `429 Too Many Requests` response: ```json { \"error\": { \"code\": \"rate_limit_exceeded\", \"message\": \"Rate limit exceeded. Please retry after 42 seconds.\", \"retry_after\": 42 } } ``` **Best Practices:** - Monitor `X-RateLimit-Remaining` header values - Implement exponential backoff when receiving 429 responses - Cache responses when appropriate to reduce API calls - Consider upgrading to Enterprise tier for higher limits (1000 req/min) ## Pagination List endpoints return paginated results to optimize performance. Use `limit` and `offset` parameters to navigate through result sets. **Parameters:** - `limit`: Number of items per page (default: 20, max: 100) - `offset`: Number of items to skip (default: 0) **Example Request:** ```bash curl -H \"X-API-Key: YOUR_API_KEY\" \\ \"https://api.cloudflow.io/v2/workflows?limit=50&offset=100\" ``` **Response Structure:** ```json { \"data\": [...], \"pagination\": { \"total\": 347, \"limit\": 50, \"offset\": 100, \"has_more\": true } } ``` **Python Example:** ```python import requests def fetch_all_workflows(api_key): base_url = \"https://api.cloudflow.io/v2/workflows\" headers = {\"X-API-Key\": api_key} all_workflows = [] offset = 0 limit = 100 while True: response = requests.get( base_url, headers=headers, params={\"limit\": limit, \"offset\": offset} ) data = response.json() all_workflows.extend(data['data']) if not data['pagination']['has_more']: break offset += limit return all_workflows ``` ## Endpoints ### Workflows #### List Workflows Retrieve a paginated list of all workflows in your account. **Endpoint:** `GET /workflows` **Query Parameters:** - `status` (optional): Filter by status (`active`, `paused`, `archived`) - `created_after` (optional): ISO 8601 timestamp - `limit` (optional): Items per page (max 100) - `offset` (optional): Pagination offset **Example Request:** ```bash curl -H \"X-API-Key: YOUR_API_KEY\" \\ \"https://api.cloudflow.io/v2/workflows?status=active&limit=25\" ``` **Example Response (200 OK):** ```json { \"data\": [ { \"id\": \"wf_8x7k2m9p\", \"name\": \"Data Processing Pipeline\", \"description\": \"Processes customer data every hour\", \"status\": \"active\", \"trigger\": { \"type\": \"schedule\", \"cron\": \"0 * * * *\" }, \"steps\": 12, \"created_at\": \"2026-01-15T10:30:00Z\", \"updated_at\": \"2026-01-23T14:22:00Z\", \"last_run\": \"2026-01-24T09:00:00Z\" } ], \"pagination\": { \"total\": 1, \"limit\": 25, \"offset\": 0, \"has_more\": false } } ``` #### Create Workflow Create a new workflow with specified configuration.",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_6",
              "content": "Data Validation Errors** ``` ValidationError: Field 'customer_id' is required but missing in 234 records ``` **Resolution:** ```bash # Add data quality checks cloudflow workflows update wf_9k2n4m8p1q \\ --step data_ingestion \\ --add-validator required_fields \\ --validator-config '{\"fields\": [\"customer_id\", \"timestamp\", \"amount\"]}' # Configure error handling cloudflow workflows update wf_9k2n4m8p1q \\ --step data_validation \\ --on-error continue \\ --error-threshold 5% # Fail if > 5% of records invalid ``` **2. External API Failures** ``` ExternalAPIError: API request to https://partner-api.example.com failed with status 502 ``` **Resolution:** ```bash # Add circuit breaker cloudflow workflows update wf_9k2n4m8p1q \\ --step external_api_call \\ --circuit-breaker-enabled true \\ --circuit-breaker-threshold 5 \\ --circuit-breaker-timeout 30000 # Configure fallback behavior cloudflow workflows update wf_9k2n4m8p1q \\ --step external_api_call \\ --fallback-action use_cached_data \\ --cache-ttl 3600 ``` --- ## Rate Limiting & Throttling ### 429 Too Many Requests #### Error Response ```json { \"error\": \"rate_limit_exceeded\", \"message\": \"API rate limit exceeded. Retry after 45 seconds.\", \"status\": 429, \"headers\": { \"X-RateLimit-Limit\": \"1000\", \"X-RateLimit-Remaining\": \"0\", \"X-RateLimit-Reset\": \"1706112345\", \"Retry-After\": \"45\" } } ``` #### Rate Limit Tiers CloudFlow enforces the following rate limits per workspace: | Tier | Requests/Minute | Requests/Hour | Concurrent Workflows | |------|-----------------|---------------|----------------------| | Free | 60 | 1,000 | 5 | | Standard | 1,000 | 50,000 | 50 | | Premium | 5,000 | 250,000 | 200 | | Enterprise | Custom | Custom | Unlimited | #### Checking Rate Limit Status ```bash # Check current rate limit status curl -I https://api.cloudflow.io/api/v1/workflows \\ -H \"Authorization: Bearer $CF_ACCESS_TOKEN\" # Extract rate limit headers curl -s -I https://api.cloudflow.io/api/v1/workflows \\ -H \"Authorization: Bearer $CF_ACCESS_TOKEN\" | \\ grep -i \"x-ratelimit\" # Output: # X-RateLimit-Limit: 1000 # X-RateLimit-Remaining: 247 # X-RateLimit-Reset: 1706112400 # Monitor rate limit usage cloudflow metrics query \\ --metric rate_limit_remaining \\ --workspace ws_abc123 \\ --last 1h \\ --interval 1m ``` #### Handling Rate Limits in Code **Python example with retry logic:** ```python import time import requests def cloudflow_api_call_with_retry(url, headers, max_retries=3): for attempt in range(max_retries): response = requests.get(url, headers=headers) if response.status_code == 429: retry_after = int(response.headers.get('Retry-After', 60)) print(f\"Rate limited.",
              "score": null
            },
            {
              "doc_id": "user_guide",
              "chunk_id": "user_guide_fix_6",
              "content": "Filter by error type, date range, or execution ID 3. Click an execution to view full details 4. Click **\"Retry\"** to reprocess with the same input data ### Error Notifications Get notified when workflows fail: ```yaml workflow: notifications: on_failure: - type: email to: \"ops-team@company.com\" - type: slack channel: \"#alerts\" message: \"Workflow {{workflow.name}} failed: {{error.message}}\" on_success_after_retry: - type: slack channel: \"#monitoring\" message: \"Workflow recovered after {{error.attempts}} attempts\" ``` ## Workflow Limits CloudFlow enforces the following limits to ensure platform stability and performance: ### Steps Per Workflow - **Maximum**: 50 steps per workflow - **Recommendation**: Keep workflows focused and modular. If you need more steps, consider splitting into multiple workflows connected via webhooks. ### Execution Timeout - **Default**: 3600 seconds (60 minutes) - **Behavior**: Workflows exceeding this timeout are automatically terminated - **Custom Timeouts**: Enterprise plans can request custom timeout limits **Setting Step-Level Timeouts:** ```yaml - id: long_running_task action: http_request config: url: \"https://api.example.com/process\" timeout: 300 # 5 minutes for this specific step ``` ### Execution Limits - **Maximum**: 1000 executions per day (per workflow) - **Rate Limiting**: 100 concurrent executions per workflow - **Burst Limit**: 10 executions per second **What happens when limits are reached:** - New executions are queued automatically - Webhook triggers return HTTP 429 (Too Many Requests) - Scheduled executions are skipped (logged in audit trail) - Email notifications sent to workflow owner **Monitoring Usage:** View real-time metrics in your workflow dashboard: - Executions today: 847 / 1000 - Average duration: 12.3s - Success rate: 98.2% - Current queue depth: 3 ### Data Limits - **Maximum request/response size**: 10MB per action - **Maximum execution payload**: 50MB total - **Variable value size**: 1MB per variable ### Enterprise Plan Limits Enterprise customers can request increased limits: - Up to 100 steps per workflow - Up to 10,000 executions per day - Up to 7200 second timeout (2 hours) - Priority execution queue - Dedicated capacity allocation Contact sales@cloudflow.io for Enterprise pricing and custom limits. ## Best Practices Follow these best practices to build reliable, maintainable workflows: ### 1. Use Descriptive Names **Good:** - Workflow: \"Sync Customer Data from Salesforce to Database\" - Step: \"validate_customer_email\" **Bad:** - Workflow: \"Workflow 1\" - Step: \"step3\" ### 2.",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_7",
              "content": "Waiting {retry_after} seconds...\") time.sleep(retry_after) continue remaining = int(response.headers.get('X-RateLimit-Remaining', 0)) if remaining < 10: print(f\"Warning: Only {remaining} requests remaining\") return response raise Exception(\"Max retries exceeded due to rate limiting\") ``` **Bash script with rate limit checking:** ```bash #!/bin/bash check_rate_limit() { local remaining=$(curl -s -I https://api.cloudflow.io/api/v1/workflows \\ -H \"Authorization: Bearer $CF_ACCESS_TOKEN\" | \\ grep -i \"x-ratelimit-remaining\" | \\ awk '{print $2}' | tr -d '\\r') if [ \"$remaining\" -lt 10 ]; then echo \"Warning: Only $remaining requests remaining\" local reset=$(curl -s -I https://api.cloudflow.io/api/v1/workflows \\ -H \"Authorization: Bearer $CF_ACCESS_TOKEN\" | \\ grep -i \"x-ratelimit-reset\" | \\ awk '{print $2}' | tr -d '\\r') local wait_time=$((reset - $(date +%s))) echo \"Rate limit resets in $wait_time seconds\" sleep $wait_time fi } # Use before API calls check_rate_limit cloudflow workflows execute wf_9k2n4m8p1q ``` #### Optimization Strategies **1. Implement request batching:** ```bash # Batch multiple workflow executions cloudflow workflows execute-batch \\ --workflow-ids \"wf_id1,wf_id2,wf_id3,wf_id4,wf_id5\" \\ --batch-size 5 # This counts as 1 API request instead of 5 ``` **2. Use webhooks instead of polling:** ```bash # Configure webhook for workflow completion cloudflow webhooks create \\ --event workflow.completed \\ --url https://your-service.com/webhooks/cloudflow \\ --secret $WEBHOOK_SECRET # Verify webhook cloudflow webhooks test webhook_abc123 ``` **3.",
              "score": null
            }
          ],
          "latency_ms": 994.2880499984312,
          "query_id": "realistic_002",
          "dimension": "problem",
          "query": "My requests keep failing with 429 status code"
        },
        {
          "key_facts": [
            "429 Too Many Requests",
            "X-RateLimit-Remaining",
            "Retry-After",
            "Monitor X-RateLimit-Remaining header values",
            "Implement exponential backoff when receiving 429 responses"
          ],
          "found_facts": [
            "429 Too Many Requests",
            "X-RateLimit-Remaining",
            "Retry-After",
            "Implement exponential backoff when receiving 429 responses"
          ],
          "missed_facts": [
            "Monitor X-RateLimit-Remaining header values"
          ],
          "coverage": 0.8,
          "expected_docs": [
            "api_reference",
            "troubleshooting_guide"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_6",
              "content": "Data Validation Errors** ``` ValidationError: Field 'customer_id' is required but missing in 234 records ``` **Resolution:** ```bash # Add data quality checks cloudflow workflows update wf_9k2n4m8p1q \\ --step data_ingestion \\ --add-validator required_fields \\ --validator-config '{\"fields\": [\"customer_id\", \"timestamp\", \"amount\"]}' # Configure error handling cloudflow workflows update wf_9k2n4m8p1q \\ --step data_validation \\ --on-error continue \\ --error-threshold 5% # Fail if > 5% of records invalid ``` **2. External API Failures** ``` ExternalAPIError: API request to https://partner-api.example.com failed with status 502 ``` **Resolution:** ```bash # Add circuit breaker cloudflow workflows update wf_9k2n4m8p1q \\ --step external_api_call \\ --circuit-breaker-enabled true \\ --circuit-breaker-threshold 5 \\ --circuit-breaker-timeout 30000 # Configure fallback behavior cloudflow workflows update wf_9k2n4m8p1q \\ --step external_api_call \\ --fallback-action use_cached_data \\ --cache-ttl 3600 ``` --- ## Rate Limiting & Throttling ### 429 Too Many Requests #### Error Response ```json { \"error\": \"rate_limit_exceeded\", \"message\": \"API rate limit exceeded. Retry after 45 seconds.\", \"status\": 429, \"headers\": { \"X-RateLimit-Limit\": \"1000\", \"X-RateLimit-Remaining\": \"0\", \"X-RateLimit-Reset\": \"1706112345\", \"Retry-After\": \"45\" } } ``` #### Rate Limit Tiers CloudFlow enforces the following rate limits per workspace: | Tier | Requests/Minute | Requests/Hour | Concurrent Workflows | |------|-----------------|---------------|----------------------| | Free | 60 | 1,000 | 5 | | Standard | 1,000 | 50,000 | 50 | | Premium | 5,000 | 250,000 | 200 | | Enterprise | Custom | Custom | Unlimited | #### Checking Rate Limit Status ```bash # Check current rate limit status curl -I https://api.cloudflow.io/api/v1/workflows \\ -H \"Authorization: Bearer $CF_ACCESS_TOKEN\" # Extract rate limit headers curl -s -I https://api.cloudflow.io/api/v1/workflows \\ -H \"Authorization: Bearer $CF_ACCESS_TOKEN\" | \\ grep -i \"x-ratelimit\" # Output: # X-RateLimit-Limit: 1000 # X-RateLimit-Remaining: 247 # X-RateLimit-Reset: 1706112400 # Monitor rate limit usage cloudflow metrics query \\ --metric rate_limit_remaining \\ --workspace ws_abc123 \\ --last 1h \\ --interval 1m ``` #### Handling Rate Limits in Code **Python example with retry logic:** ```python import time import requests def cloudflow_api_call_with_retry(url, headers, max_retries=3): for attempt in range(max_retries): response = requests.get(url, headers=headers) if response.status_code == 429: retry_after = int(response.headers.get('Retry-After', 60)) print(f\"Rate limited.",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_7",
              "content": "Waiting {retry_after} seconds...\") time.sleep(retry_after) continue remaining = int(response.headers.get('X-RateLimit-Remaining', 0)) if remaining < 10: print(f\"Warning: Only {remaining} requests remaining\") return response raise Exception(\"Max retries exceeded due to rate limiting\") ``` **Bash script with rate limit checking:** ```bash #!/bin/bash check_rate_limit() { local remaining=$(curl -s -I https://api.cloudflow.io/api/v1/workflows \\ -H \"Authorization: Bearer $CF_ACCESS_TOKEN\" | \\ grep -i \"x-ratelimit-remaining\" | \\ awk '{print $2}' | tr -d '\\r') if [ \"$remaining\" -lt 10 ]; then echo \"Warning: Only $remaining requests remaining\" local reset=$(curl -s -I https://api.cloudflow.io/api/v1/workflows \\ -H \"Authorization: Bearer $CF_ACCESS_TOKEN\" | \\ grep -i \"x-ratelimit-reset\" | \\ awk '{print $2}' | tr -d '\\r') local wait_time=$((reset - $(date +%s))) echo \"Rate limit resets in $wait_time seconds\" sleep $wait_time fi } # Use before API calls check_rate_limit cloudflow workflows execute wf_9k2n4m8p1q ``` #### Optimization Strategies **1. Implement request batching:** ```bash # Batch multiple workflow executions cloudflow workflows execute-batch \\ --workflow-ids \"wf_id1,wf_id2,wf_id3,wf_id4,wf_id5\" \\ --batch-size 5 # This counts as 1 API request instead of 5 ``` **2. Use webhooks instead of polling:** ```bash # Configure webhook for workflow completion cloudflow webhooks create \\ --event workflow.completed \\ --url https://your-service.com/webhooks/cloudflow \\ --secret $WEBHOOK_SECRET # Verify webhook cloudflow webhooks test webhook_abc123 ``` **3.",
              "score": null
            },
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_1",
              "content": "## Rate Limiting To ensure fair usage and system stability, CloudFlow enforces rate limits on all API endpoints. **Default Limits:** - 100 requests per minute per authenticated user - 20 requests per minute for unauthenticated requests - Burst allowance: 150 requests in a 10-second window ### Rate Limit Headers Every API response includes rate limit information: ``` X-RateLimit-Limit: 100 X-RateLimit-Remaining: 87 X-RateLimit-Reset: 1640995200 ``` When you exceed the rate limit, you'll receive a `429 Too Many Requests` response: ```json { \"error\": { \"code\": \"rate_limit_exceeded\", \"message\": \"Rate limit exceeded. Please retry after 42 seconds.\", \"retry_after\": 42 } } ``` **Best Practices:** - Monitor `X-RateLimit-Remaining` header values - Implement exponential backoff when receiving 429 responses - Cache responses when appropriate to reduce API calls - Consider upgrading to Enterprise tier for higher limits (1000 req/min) ## Pagination List endpoints return paginated results to optimize performance. Use `limit` and `offset` parameters to navigate through result sets. **Parameters:** - `limit`: Number of items per page (default: 20, max: 100) - `offset`: Number of items to skip (default: 0) **Example Request:** ```bash curl -H \"X-API-Key: YOUR_API_KEY\" \\ \"https://api.cloudflow.io/v2/workflows?limit=50&offset=100\" ``` **Response Structure:** ```json { \"data\": [...], \"pagination\": { \"total\": 347, \"limit\": 50, \"offset\": 100, \"has_more\": true } } ``` **Python Example:** ```python import requests def fetch_all_workflows(api_key): base_url = \"https://api.cloudflow.io/v2/workflows\" headers = {\"X-API-Key\": api_key} all_workflows = [] offset = 0 limit = 100 while True: response = requests.get( base_url, headers=headers, params={\"limit\": limit, \"offset\": offset} ) data = response.json() all_workflows.extend(data['data']) if not data['pagination']['has_more']: break offset += limit return all_workflows ``` ## Endpoints ### Workflows #### List Workflows Retrieve a paginated list of all workflows in your account. **Endpoint:** `GET /workflows` **Query Parameters:** - `status` (optional): Filter by status (`active`, `paused`, `archived`) - `created_after` (optional): ISO 8601 timestamp - `limit` (optional): Items per page (max 100) - `offset` (optional): Pagination offset **Example Request:** ```bash curl -H \"X-API-Key: YOUR_API_KEY\" \\ \"https://api.cloudflow.io/v2/workflows?status=active&limit=25\" ``` **Example Response (200 OK):** ```json { \"data\": [ { \"id\": \"wf_8x7k2m9p\", \"name\": \"Data Processing Pipeline\", \"description\": \"Processes customer data every hour\", \"status\": \"active\", \"trigger\": { \"type\": \"schedule\", \"cron\": \"0 * * * *\" }, \"steps\": 12, \"created_at\": \"2026-01-15T10:30:00Z\", \"updated_at\": \"2026-01-23T14:22:00Z\", \"last_run\": \"2026-01-24T09:00:00Z\" } ], \"pagination\": { \"total\": 1, \"limit\": 25, \"offset\": 0, \"has_more\": false } } ``` #### Create Workflow Create a new workflow with specified configuration.",
              "score": null
            },
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_4",
              "content": "### Error Response Format ```json { \"error\": { \"code\": \"invalid_parameter\", \"message\": \"The 'limit' parameter must be between 1 and 100\", \"field\": \"limit\", \"request_id\": \"req_8k3m9x2p\" } } ``` ### HTTP Status Codes | Status Code | Description | Common Causes | |------------|-------------|---------------| | 400 | Bad Request | Invalid parameters, malformed JSON, validation errors | | 401 | Unauthorized | Missing or invalid authentication credentials | | 403 | Forbidden | Insufficient permissions for requested resource | | 404 | Not Found | Resource does not exist or has been deleted | | 429 | Too Many Requests | Rate limit exceeded, retry after specified period | | 500 | Internal Server Error | Unexpected server error, contact support if persists | | 502 | Bad Gateway | Temporary service issue, retry with exponential backoff | | 503 | Service Unavailable | Scheduled maintenance or temporary outage | ### Error Codes CloudFlow returns specific error codes to help you identify and resolve issues: - `invalid_parameter`: One or more request parameters are invalid - `missing_required_field`: Required field is missing from request body - `authentication_failed`: Invalid API key or token - `insufficient_permissions`: User lacks required scope or permission - `resource_not_found`: Requested resource does not exist - `rate_limit_exceeded`: Too many requests, see rate limiting section - `workflow_execution_failed`: Workflow execution encountered an error - `invalid_json`: Request body contains malformed JSON - `duplicate_resource`: Resource with same identifier already exists - `quota_exceeded`: Account quota limit reached ### Error Handling Best Practices **Python Example:** ```python import requests import time def make_api_request(url, headers, max_retries=3): for attempt in range(max_retries): try: response = requests.get(url, headers=headers) if response.status_code == 200: return response.json() elif response.status_code == 429: # Rate limit exceeded retry_after = int(response.headers.get('X-RateLimit-Reset', 60)) print(f\"Rate limited. Waiting {retry_after} seconds...\") time.sleep(retry_after) continue elif response.status_code >= 500: # Server error - retry with exponential backoff wait_time = 2 ** attempt print(f\"Server error. Retrying in {wait_time} seconds...\") time.sleep(wait_time) continue else: # Client error - don't retry error_data = response.json() raise Exception(f\"API Error: {error_data['error']['message']}\") except requests.exceptions.RequestException as e: print(f\"Request failed: {e}\") if attempt < max_retries - 1: time.sleep(2 ** attempt) else: raise raise Exception(\"Max retries exceeded\") ``` ## Webhooks CloudFlow can send webhook notifications when specific events occur in your workflows.",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_1",
              "content": "**Technology**: Node.js with Express.js framework **Replicas**: 12 pods (production), auto-scaling 8-20 based on CPU **Resource Allocation**: 2 vCPU, 4GB RAM per pod **Key Responsibilities**: - JWT token validation (delegated to Auth Service for initial validation) - Rate limiting: 1000 requests per minute per API key (sliding window) - Request/response transformation and validation using JSON Schema - Routing to downstream services based on URL path patterns - CORS handling for web clients - Request/response logging and correlation ID injection **Critical Endpoints**: - `POST /api/v1/workflows` - Create new workflow - `GET /api/v1/workflows/:id` - Retrieve workflow status - `POST /api/v1/workflows/:id/execute` - Trigger workflow execution - `GET /api/v1/workflows/:id/history` - Get execution history **Dependencies**: - Auth Service (for token validation) - Redis (for rate limiting counters) - All downstream microservices **Performance Targets**: - P50 latency: < 50ms - P99 latency: < 200ms - Throughput: 10,000 RPS sustained --- ### Auth Service **Purpose**: Centralized authentication and authorization service handling user identity, token generation, and permission validation. **Technology**: Go with gRPC for internal communication, REST for external **Replicas**: 8 pods (production), auto-scaling 6-12 **Resource Allocation**: 1 vCPU, 2GB RAM per pod **Key Responsibilities**: - User authentication via multiple providers (OAuth2, SAML, local credentials) - JWT token generation and validation (RS256 algorithm) - Role-based access control (RBAC) with fine-grained permissions - Session management with Redis-backed storage - API key generation and validation for service accounts - MFA enforcement for administrative operations **Authentication Flow**: ``` Client Request \u2192 API Gateway \u2192 Auth Service \u2502 \u251c\u2500 Validate credentials \u251c\u2500 Check MFA if required \u251c\u2500 Generate JWT (15min expiry) \u251c\u2500 Generate refresh token (7 days) \u2514\u2500 Store session in Redis ``` **Token Structure**: - Access Token: JWT with 15-minute expiry - Refresh Token: Opaque token with 7-day expiry, stored in PostgreSQL - Claims: user_id, email, roles[], permissions[], tenant_id **Security Features**: - Password hashing: Argon2id with 64MB memory, 4 iterations - Token rotation on refresh to prevent replay attacks - Brute force protection: 5 failed attempts \u2192 15-minute lockout - Secrets stored in HashiCorp Vault, rotated every 90 days **Performance Targets**: - Token validation: < 10ms (P99) - Token generation: < 50ms (P99) - Throughput: 5,000 RPS for validation operations --- ### Workflow Engine **Purpose**: Core orchestration service that executes workflow definitions, manages state transitions, and coordinates task execution across distributed systems.",
              "score": null
            }
          ],
          "latency_ms": 1048.9268349992926,
          "query_id": "realistic_002",
          "dimension": "casual",
          "query": "429 error fix"
        },
        {
          "key_facts": [
            "429 Too Many Requests",
            "X-RateLimit-Remaining",
            "Retry-After",
            "Monitor X-RateLimit-Remaining header values",
            "Implement exponential backoff when receiving 429 responses"
          ],
          "found_facts": [
            "429 Too Many Requests",
            "X-RateLimit-Remaining",
            "Retry-After",
            "Implement exponential backoff when receiving 429 responses"
          ],
          "missed_facts": [
            "Monitor X-RateLimit-Remaining header values"
          ],
          "coverage": 0.8,
          "expected_docs": [
            "api_reference",
            "troubleshooting_guide"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_1",
              "content": "## Rate Limiting To ensure fair usage and system stability, CloudFlow enforces rate limits on all API endpoints. **Default Limits:** - 100 requests per minute per authenticated user - 20 requests per minute for unauthenticated requests - Burst allowance: 150 requests in a 10-second window ### Rate Limit Headers Every API response includes rate limit information: ``` X-RateLimit-Limit: 100 X-RateLimit-Remaining: 87 X-RateLimit-Reset: 1640995200 ``` When you exceed the rate limit, you'll receive a `429 Too Many Requests` response: ```json { \"error\": { \"code\": \"rate_limit_exceeded\", \"message\": \"Rate limit exceeded. Please retry after 42 seconds.\", \"retry_after\": 42 } } ``` **Best Practices:** - Monitor `X-RateLimit-Remaining` header values - Implement exponential backoff when receiving 429 responses - Cache responses when appropriate to reduce API calls - Consider upgrading to Enterprise tier for higher limits (1000 req/min) ## Pagination List endpoints return paginated results to optimize performance. Use `limit` and `offset` parameters to navigate through result sets. **Parameters:** - `limit`: Number of items per page (default: 20, max: 100) - `offset`: Number of items to skip (default: 0) **Example Request:** ```bash curl -H \"X-API-Key: YOUR_API_KEY\" \\ \"https://api.cloudflow.io/v2/workflows?limit=50&offset=100\" ``` **Response Structure:** ```json { \"data\": [...], \"pagination\": { \"total\": 347, \"limit\": 50, \"offset\": 100, \"has_more\": true } } ``` **Python Example:** ```python import requests def fetch_all_workflows(api_key): base_url = \"https://api.cloudflow.io/v2/workflows\" headers = {\"X-API-Key\": api_key} all_workflows = [] offset = 0 limit = 100 while True: response = requests.get( base_url, headers=headers, params={\"limit\": limit, \"offset\": offset} ) data = response.json() all_workflows.extend(data['data']) if not data['pagination']['has_more']: break offset += limit return all_workflows ``` ## Endpoints ### Workflows #### List Workflows Retrieve a paginated list of all workflows in your account. **Endpoint:** `GET /workflows` **Query Parameters:** - `status` (optional): Filter by status (`active`, `paused`, `archived`) - `created_after` (optional): ISO 8601 timestamp - `limit` (optional): Items per page (max 100) - `offset` (optional): Pagination offset **Example Request:** ```bash curl -H \"X-API-Key: YOUR_API_KEY\" \\ \"https://api.cloudflow.io/v2/workflows?status=active&limit=25\" ``` **Example Response (200 OK):** ```json { \"data\": [ { \"id\": \"wf_8x7k2m9p\", \"name\": \"Data Processing Pipeline\", \"description\": \"Processes customer data every hour\", \"status\": \"active\", \"trigger\": { \"type\": \"schedule\", \"cron\": \"0 * * * *\" }, \"steps\": 12, \"created_at\": \"2026-01-15T10:30:00Z\", \"updated_at\": \"2026-01-23T14:22:00Z\", \"last_run\": \"2026-01-24T09:00:00Z\" } ], \"pagination\": { \"total\": 1, \"limit\": 25, \"offset\": 0, \"has_more\": false } } ``` #### Create Workflow Create a new workflow with specified configuration.",
              "score": null
            },
            {
              "doc_id": "user_guide",
              "chunk_id": "user_guide_fix_5",
              "content": "Save and activate **Testing Schedules:** Use the built-in schedule calculator to preview upcoming executions: ``` Next 5 executions: 1. 2026-01-24 14:00:00 UTC 2. 2026-01-25 14:00:00 UTC 3. 2026-01-26 14:00:00 UTC 4. 2026-01-27 14:00:00 UTC 5. 2026-01-28 14:00:00 UTC ``` ## Error Handling Robust error handling ensures your workflows are resilient and reliable. ### Retry Policies Configure automatic retries for failed actions: ```yaml - id: api_call action: http_request config: url: \"https://api.example.com/data\" retry: max_attempts: 3 backoff_type: \"exponential\" # or \"fixed\", \"linear\" initial_interval: 1000 # milliseconds max_interval: 30000 multiplier: 2.0 retry_on: - timeout - network_error - status: [500, 502, 503, 504] ``` **Backoff Strategies:** - **Fixed**: Wait the same amount of time between retries - Attempt 1: Wait 1s - Attempt 2: Wait 1s - Attempt 3: Wait 1s - **Linear**: Increase wait time by a fixed amount - Attempt 1: Wait 1s - Attempt 2: Wait 2s - Attempt 3: Wait 3s - **Exponential**: Double the wait time with each retry (recommended) - Attempt 1: Wait 1s - Attempt 2: Wait 2s - Attempt 3: Wait 4s **Retry Conditions:** Control which errors trigger retries: - `timeout`: Request timeout - `network_error`: Connection failures - `status`: Specific HTTP status codes - `error_code`: Application-specific error codes ### Fallback Actions Execute alternative actions when the primary action fails: ```yaml - id: primary_payment action: http_request config: url: \"https://primary-payment-gateway.com/charge\" method: POST body: amount: \"{{amount}}\" on_error: - id: fallback_payment action: http_request config: url: \"https://backup-payment-gateway.com/charge\" method: POST body: amount: \"{{amount}}\" - id: notify_admin action: email config: to: \"admin@company.com\" subject: \"Payment Gateway Failure\" body: \"Primary gateway failed, switched to backup\" ``` **Error Object:** Access error details in fallback actions: ``` {{error.message}} # Error message {{error.code}} # Error code {{error.step_id}} # Failed step ID {{error.timestamp}} # When the error occurred {{error.attempts}} # Number of retry attempts ``` ### Dead Letter Queue When all retries and fallbacks fail, CloudFlow can route failed executions to a Dead Letter Queue (DLQ) for manual review: **Enable DLQ:** ```yaml workflow: error_handling: dead_letter_queue: enabled: true retain_days: 30 ``` **DLQ Features:** - View failed executions in the dashboard - Inspect complete execution context and error details - Retry individual executions after fixing issues - Export failed executions for analysis - Set up alerts for DLQ threshold breaches **Accessing the DLQ:** 1. Navigate to **Workflows** > **[Your Workflow]** > **Dead Letter Queue** 2.",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_6",
              "content": "Data Validation Errors** ``` ValidationError: Field 'customer_id' is required but missing in 234 records ``` **Resolution:** ```bash # Add data quality checks cloudflow workflows update wf_9k2n4m8p1q \\ --step data_ingestion \\ --add-validator required_fields \\ --validator-config '{\"fields\": [\"customer_id\", \"timestamp\", \"amount\"]}' # Configure error handling cloudflow workflows update wf_9k2n4m8p1q \\ --step data_validation \\ --on-error continue \\ --error-threshold 5% # Fail if > 5% of records invalid ``` **2. External API Failures** ``` ExternalAPIError: API request to https://partner-api.example.com failed with status 502 ``` **Resolution:** ```bash # Add circuit breaker cloudflow workflows update wf_9k2n4m8p1q \\ --step external_api_call \\ --circuit-breaker-enabled true \\ --circuit-breaker-threshold 5 \\ --circuit-breaker-timeout 30000 # Configure fallback behavior cloudflow workflows update wf_9k2n4m8p1q \\ --step external_api_call \\ --fallback-action use_cached_data \\ --cache-ttl 3600 ``` --- ## Rate Limiting & Throttling ### 429 Too Many Requests #### Error Response ```json { \"error\": \"rate_limit_exceeded\", \"message\": \"API rate limit exceeded. Retry after 45 seconds.\", \"status\": 429, \"headers\": { \"X-RateLimit-Limit\": \"1000\", \"X-RateLimit-Remaining\": \"0\", \"X-RateLimit-Reset\": \"1706112345\", \"Retry-After\": \"45\" } } ``` #### Rate Limit Tiers CloudFlow enforces the following rate limits per workspace: | Tier | Requests/Minute | Requests/Hour | Concurrent Workflows | |------|-----------------|---------------|----------------------| | Free | 60 | 1,000 | 5 | | Standard | 1,000 | 50,000 | 50 | | Premium | 5,000 | 250,000 | 200 | | Enterprise | Custom | Custom | Unlimited | #### Checking Rate Limit Status ```bash # Check current rate limit status curl -I https://api.cloudflow.io/api/v1/workflows \\ -H \"Authorization: Bearer $CF_ACCESS_TOKEN\" # Extract rate limit headers curl -s -I https://api.cloudflow.io/api/v1/workflows \\ -H \"Authorization: Bearer $CF_ACCESS_TOKEN\" | \\ grep -i \"x-ratelimit\" # Output: # X-RateLimit-Limit: 1000 # X-RateLimit-Remaining: 247 # X-RateLimit-Reset: 1706112400 # Monitor rate limit usage cloudflow metrics query \\ --metric rate_limit_remaining \\ --workspace ws_abc123 \\ --last 1h \\ --interval 1m ``` #### Handling Rate Limits in Code **Python example with retry logic:** ```python import time import requests def cloudflow_api_call_with_retry(url, headers, max_retries=3): for attempt in range(max_retries): response = requests.get(url, headers=headers) if response.status_code == 429: retry_after = int(response.headers.get('Retry-After', 60)) print(f\"Rate limited.",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_5",
              "content": "exec_7h3j6k9m2n --show-bottlenecks ``` #### Solutions **1. Increase workflow timeout (if justified):** ```bash # Update workflow configuration cloudflow workflows update wf_9k2n4m8p1q \\ --timeout 7200 \\ --reason \"Large dataset processing requires extended time\" # Verify update cloudflow workflows get wf_9k2n4m8p1q | grep timeout ``` **2. Optimize slow steps:** ```bash # Enable parallel processing cloudflow workflows update wf_9k2n4m8p1q \\ --step data_transformation \\ --parallel-workers 8 \\ --batch-size 1000 # Add checkpointing for long operations cloudflow workflows update wf_9k2n4m8p1q \\ --step data_transformation \\ --enable-checkpointing \\ --checkpoint-interval 300 ``` **3. Split workflow into smaller workflows:** ```bash # Create sub-workflows cloudflow workflows create data-pipeline-part1 \\ --steps \"data_ingestion,data_validation\" \\ --timeout 1800 cloudflow workflows create data-pipeline-part2 \\ --steps \"data_transformation,data_export\" \\ --timeout 3600 \\ --trigger workflow_completed \\ --trigger-workflow data-pipeline-part1 ``` ### Retry Logic and Exponential Backoff CloudFlow implements automatic retry with exponential backoff for transient failures: - Max retries: 3 - Initial delay: 1 second - Backoff multiplier: 2 - Max delay: 60 seconds #### Retry Sequence ``` Attempt 1: Immediate Attempt 2: Wait 1s (2^0 * 1s) Attempt 3: Wait 2s (2^1 * 1s) Attempt 4: Wait 4s (2^2 * 1s) ``` #### Configuration ```bash # View current retry settings cloudflow workflows get wf_9k2n4m8p1q --format json | jq '.retry_policy' # Customize retry behavior cloudflow workflows update wf_9k2n4m8p1q \\ --retry-max-attempts 5 \\ --retry-initial-delay 2000 \\ --retry-backoff-multiplier 2 \\ --retry-max-delay 120000 \\ --retry-on-errors \"NETWORK_ERROR,TIMEOUT,RATE_LIMIT\" # Disable retry for specific step cloudflow workflows update wf_9k2n4m8p1q \\ --step data_export \\ --retry-enabled false ``` #### Monitoring Retries ```bash # List failed executions with retry information cloudflow workflows executions list \\ --status FAILED \\ --show-retries \\ --last 7d # Get retry history for specific execution cloudflow workflows executions retries exec_7h3j6k9m2n # Output: # Execution: exec_7h3j6k9m2n # Attempt 1: FAILED - NetworkError: Connection refused (delay: 0ms) # Attempt 2: FAILED - NetworkError: Connection timeout (delay: 1000ms) # Attempt 3: FAILED - NetworkError: Connection timeout (delay: 2000ms) # Attempt 4: FAILED - NetworkError: Connection timeout (delay: 4000ms) # Final Status: FAILED_AFTER_RETRIES ``` ### Workflow Step Failures #### Common Error Patterns **1.",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_7",
              "content": "Waiting {retry_after} seconds...\") time.sleep(retry_after) continue remaining = int(response.headers.get('X-RateLimit-Remaining', 0)) if remaining < 10: print(f\"Warning: Only {remaining} requests remaining\") return response raise Exception(\"Max retries exceeded due to rate limiting\") ``` **Bash script with rate limit checking:** ```bash #!/bin/bash check_rate_limit() { local remaining=$(curl -s -I https://api.cloudflow.io/api/v1/workflows \\ -H \"Authorization: Bearer $CF_ACCESS_TOKEN\" | \\ grep -i \"x-ratelimit-remaining\" | \\ awk '{print $2}' | tr -d '\\r') if [ \"$remaining\" -lt 10 ]; then echo \"Warning: Only $remaining requests remaining\" local reset=$(curl -s -I https://api.cloudflow.io/api/v1/workflows \\ -H \"Authorization: Bearer $CF_ACCESS_TOKEN\" | \\ grep -i \"x-ratelimit-reset\" | \\ awk '{print $2}' | tr -d '\\r') local wait_time=$((reset - $(date +%s))) echo \"Rate limit resets in $wait_time seconds\" sleep $wait_time fi } # Use before API calls check_rate_limit cloudflow workflows execute wf_9k2n4m8p1q ``` #### Optimization Strategies **1. Implement request batching:** ```bash # Batch multiple workflow executions cloudflow workflows execute-batch \\ --workflow-ids \"wf_id1,wf_id2,wf_id3,wf_id4,wf_id5\" \\ --batch-size 5 # This counts as 1 API request instead of 5 ``` **2. Use webhooks instead of polling:** ```bash # Configure webhook for workflow completion cloudflow webhooks create \\ --event workflow.completed \\ --url https://your-service.com/webhooks/cloudflow \\ --secret $WEBHOOK_SECRET # Verify webhook cloudflow webhooks test webhook_abc123 ``` **3.",
              "score": null
            }
          ],
          "latency_ms": 1227.5159769997117,
          "query_id": "realistic_002",
          "dimension": "contextual",
          "query": "I'm getting throttled by CloudFlow API, how can I handle this in my application?"
        },
        {
          "key_facts": [
            "429 Too Many Requests",
            "X-RateLimit-Remaining",
            "Retry-After",
            "Monitor X-RateLimit-Remaining header values",
            "Implement exponential backoff when receiving 429 responses"
          ],
          "found_facts": [
            "429 Too Many Requests",
            "X-RateLimit-Remaining",
            "Retry-After",
            "Implement exponential backoff when receiving 429 responses"
          ],
          "missed_facts": [
            "Monitor X-RateLimit-Remaining header values"
          ],
          "coverage": 0.8,
          "expected_docs": [
            "api_reference",
            "troubleshooting_guide"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_1",
              "content": "## Rate Limiting To ensure fair usage and system stability, CloudFlow enforces rate limits on all API endpoints. **Default Limits:** - 100 requests per minute per authenticated user - 20 requests per minute for unauthenticated requests - Burst allowance: 150 requests in a 10-second window ### Rate Limit Headers Every API response includes rate limit information: ``` X-RateLimit-Limit: 100 X-RateLimit-Remaining: 87 X-RateLimit-Reset: 1640995200 ``` When you exceed the rate limit, you'll receive a `429 Too Many Requests` response: ```json { \"error\": { \"code\": \"rate_limit_exceeded\", \"message\": \"Rate limit exceeded. Please retry after 42 seconds.\", \"retry_after\": 42 } } ``` **Best Practices:** - Monitor `X-RateLimit-Remaining` header values - Implement exponential backoff when receiving 429 responses - Cache responses when appropriate to reduce API calls - Consider upgrading to Enterprise tier for higher limits (1000 req/min) ## Pagination List endpoints return paginated results to optimize performance. Use `limit` and `offset` parameters to navigate through result sets. **Parameters:** - `limit`: Number of items per page (default: 20, max: 100) - `offset`: Number of items to skip (default: 0) **Example Request:** ```bash curl -H \"X-API-Key: YOUR_API_KEY\" \\ \"https://api.cloudflow.io/v2/workflows?limit=50&offset=100\" ``` **Response Structure:** ```json { \"data\": [...], \"pagination\": { \"total\": 347, \"limit\": 50, \"offset\": 100, \"has_more\": true } } ``` **Python Example:** ```python import requests def fetch_all_workflows(api_key): base_url = \"https://api.cloudflow.io/v2/workflows\" headers = {\"X-API-Key\": api_key} all_workflows = [] offset = 0 limit = 100 while True: response = requests.get( base_url, headers=headers, params={\"limit\": limit, \"offset\": offset} ) data = response.json() all_workflows.extend(data['data']) if not data['pagination']['has_more']: break offset += limit return all_workflows ``` ## Endpoints ### Workflows #### List Workflows Retrieve a paginated list of all workflows in your account. **Endpoint:** `GET /workflows` **Query Parameters:** - `status` (optional): Filter by status (`active`, `paused`, `archived`) - `created_after` (optional): ISO 8601 timestamp - `limit` (optional): Items per page (max 100) - `offset` (optional): Pagination offset **Example Request:** ```bash curl -H \"X-API-Key: YOUR_API_KEY\" \\ \"https://api.cloudflow.io/v2/workflows?status=active&limit=25\" ``` **Example Response (200 OK):** ```json { \"data\": [ { \"id\": \"wf_8x7k2m9p\", \"name\": \"Data Processing Pipeline\", \"description\": \"Processes customer data every hour\", \"status\": \"active\", \"trigger\": { \"type\": \"schedule\", \"cron\": \"0 * * * *\" }, \"steps\": 12, \"created_at\": \"2026-01-15T10:30:00Z\", \"updated_at\": \"2026-01-23T14:22:00Z\", \"last_run\": \"2026-01-24T09:00:00Z\" } ], \"pagination\": { \"total\": 1, \"limit\": 25, \"offset\": 0, \"has_more\": false } } ``` #### Create Workflow Create a new workflow with specified configuration.",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_6",
              "content": "Data Validation Errors** ``` ValidationError: Field 'customer_id' is required but missing in 234 records ``` **Resolution:** ```bash # Add data quality checks cloudflow workflows update wf_9k2n4m8p1q \\ --step data_ingestion \\ --add-validator required_fields \\ --validator-config '{\"fields\": [\"customer_id\", \"timestamp\", \"amount\"]}' # Configure error handling cloudflow workflows update wf_9k2n4m8p1q \\ --step data_validation \\ --on-error continue \\ --error-threshold 5% # Fail if > 5% of records invalid ``` **2. External API Failures** ``` ExternalAPIError: API request to https://partner-api.example.com failed with status 502 ``` **Resolution:** ```bash # Add circuit breaker cloudflow workflows update wf_9k2n4m8p1q \\ --step external_api_call \\ --circuit-breaker-enabled true \\ --circuit-breaker-threshold 5 \\ --circuit-breaker-timeout 30000 # Configure fallback behavior cloudflow workflows update wf_9k2n4m8p1q \\ --step external_api_call \\ --fallback-action use_cached_data \\ --cache-ttl 3600 ``` --- ## Rate Limiting & Throttling ### 429 Too Many Requests #### Error Response ```json { \"error\": \"rate_limit_exceeded\", \"message\": \"API rate limit exceeded. Retry after 45 seconds.\", \"status\": 429, \"headers\": { \"X-RateLimit-Limit\": \"1000\", \"X-RateLimit-Remaining\": \"0\", \"X-RateLimit-Reset\": \"1706112345\", \"Retry-After\": \"45\" } } ``` #### Rate Limit Tiers CloudFlow enforces the following rate limits per workspace: | Tier | Requests/Minute | Requests/Hour | Concurrent Workflows | |------|-----------------|---------------|----------------------| | Free | 60 | 1,000 | 5 | | Standard | 1,000 | 50,000 | 50 | | Premium | 5,000 | 250,000 | 200 | | Enterprise | Custom | Custom | Unlimited | #### Checking Rate Limit Status ```bash # Check current rate limit status curl -I https://api.cloudflow.io/api/v1/workflows \\ -H \"Authorization: Bearer $CF_ACCESS_TOKEN\" # Extract rate limit headers curl -s -I https://api.cloudflow.io/api/v1/workflows \\ -H \"Authorization: Bearer $CF_ACCESS_TOKEN\" | \\ grep -i \"x-ratelimit\" # Output: # X-RateLimit-Limit: 1000 # X-RateLimit-Remaining: 247 # X-RateLimit-Reset: 1706112400 # Monitor rate limit usage cloudflow metrics query \\ --metric rate_limit_remaining \\ --workspace ws_abc123 \\ --last 1h \\ --interval 1m ``` #### Handling Rate Limits in Code **Python example with retry logic:** ```python import time import requests def cloudflow_api_call_with_retry(url, headers, max_retries=3): for attempt in range(max_retries): response = requests.get(url, headers=headers) if response.status_code == 429: retry_after = int(response.headers.get('Retry-After', 60)) print(f\"Rate limited.",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_7",
              "content": "Waiting {retry_after} seconds...\") time.sleep(retry_after) continue remaining = int(response.headers.get('X-RateLimit-Remaining', 0)) if remaining < 10: print(f\"Warning: Only {remaining} requests remaining\") return response raise Exception(\"Max retries exceeded due to rate limiting\") ``` **Bash script with rate limit checking:** ```bash #!/bin/bash check_rate_limit() { local remaining=$(curl -s -I https://api.cloudflow.io/api/v1/workflows \\ -H \"Authorization: Bearer $CF_ACCESS_TOKEN\" | \\ grep -i \"x-ratelimit-remaining\" | \\ awk '{print $2}' | tr -d '\\r') if [ \"$remaining\" -lt 10 ]; then echo \"Warning: Only $remaining requests remaining\" local reset=$(curl -s -I https://api.cloudflow.io/api/v1/workflows \\ -H \"Authorization: Bearer $CF_ACCESS_TOKEN\" | \\ grep -i \"x-ratelimit-reset\" | \\ awk '{print $2}' | tr -d '\\r') local wait_time=$((reset - $(date +%s))) echo \"Rate limit resets in $wait_time seconds\" sleep $wait_time fi } # Use before API calls check_rate_limit cloudflow workflows execute wf_9k2n4m8p1q ``` #### Optimization Strategies **1. Implement request batching:** ```bash # Batch multiple workflow executions cloudflow workflows execute-batch \\ --workflow-ids \"wf_id1,wf_id2,wf_id3,wf_id4,wf_id5\" \\ --batch-size 5 # This counts as 1 API request instead of 5 ``` **2. Use webhooks instead of polling:** ```bash # Configure webhook for workflow completion cloudflow webhooks create \\ --event workflow.completed \\ --url https://your-service.com/webhooks/cloudflow \\ --secret $WEBHOOK_SECRET # Verify webhook cloudflow webhooks test webhook_abc123 ``` **3.",
              "score": null
            },
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_4",
              "content": "### Error Response Format ```json { \"error\": { \"code\": \"invalid_parameter\", \"message\": \"The 'limit' parameter must be between 1 and 100\", \"field\": \"limit\", \"request_id\": \"req_8k3m9x2p\" } } ``` ### HTTP Status Codes | Status Code | Description | Common Causes | |------------|-------------|---------------| | 400 | Bad Request | Invalid parameters, malformed JSON, validation errors | | 401 | Unauthorized | Missing or invalid authentication credentials | | 403 | Forbidden | Insufficient permissions for requested resource | | 404 | Not Found | Resource does not exist or has been deleted | | 429 | Too Many Requests | Rate limit exceeded, retry after specified period | | 500 | Internal Server Error | Unexpected server error, contact support if persists | | 502 | Bad Gateway | Temporary service issue, retry with exponential backoff | | 503 | Service Unavailable | Scheduled maintenance or temporary outage | ### Error Codes CloudFlow returns specific error codes to help you identify and resolve issues: - `invalid_parameter`: One or more request parameters are invalid - `missing_required_field`: Required field is missing from request body - `authentication_failed`: Invalid API key or token - `insufficient_permissions`: User lacks required scope or permission - `resource_not_found`: Requested resource does not exist - `rate_limit_exceeded`: Too many requests, see rate limiting section - `workflow_execution_failed`: Workflow execution encountered an error - `invalid_json`: Request body contains malformed JSON - `duplicate_resource`: Resource with same identifier already exists - `quota_exceeded`: Account quota limit reached ### Error Handling Best Practices **Python Example:** ```python import requests import time def make_api_request(url, headers, max_retries=3): for attempt in range(max_retries): try: response = requests.get(url, headers=headers) if response.status_code == 200: return response.json() elif response.status_code == 429: # Rate limit exceeded retry_after = int(response.headers.get('X-RateLimit-Reset', 60)) print(f\"Rate limited. Waiting {retry_after} seconds...\") time.sleep(retry_after) continue elif response.status_code >= 500: # Server error - retry with exponential backoff wait_time = 2 ** attempt print(f\"Server error. Retrying in {wait_time} seconds...\") time.sleep(wait_time) continue else: # Client error - don't retry error_data = response.json() raise Exception(f\"API Error: {error_data['error']['message']}\") except requests.exceptions.RequestException as e: print(f\"Request failed: {e}\") if attempt < max_retries - 1: time.sleep(2 ** attempt) else: raise raise Exception(\"Max retries exceeded\") ``` ## Webhooks CloudFlow can send webhook notifications when specific events occur in your workflows.",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_1",
              "content": "**Technology**: Node.js with Express.js framework **Replicas**: 12 pods (production), auto-scaling 8-20 based on CPU **Resource Allocation**: 2 vCPU, 4GB RAM per pod **Key Responsibilities**: - JWT token validation (delegated to Auth Service for initial validation) - Rate limiting: 1000 requests per minute per API key (sliding window) - Request/response transformation and validation using JSON Schema - Routing to downstream services based on URL path patterns - CORS handling for web clients - Request/response logging and correlation ID injection **Critical Endpoints**: - `POST /api/v1/workflows` - Create new workflow - `GET /api/v1/workflows/:id` - Retrieve workflow status - `POST /api/v1/workflows/:id/execute` - Trigger workflow execution - `GET /api/v1/workflows/:id/history` - Get execution history **Dependencies**: - Auth Service (for token validation) - Redis (for rate limiting counters) - All downstream microservices **Performance Targets**: - P50 latency: < 50ms - P99 latency: < 200ms - Throughput: 10,000 RPS sustained --- ### Auth Service **Purpose**: Centralized authentication and authorization service handling user identity, token generation, and permission validation. **Technology**: Go with gRPC for internal communication, REST for external **Replicas**: 8 pods (production), auto-scaling 6-12 **Resource Allocation**: 1 vCPU, 2GB RAM per pod **Key Responsibilities**: - User authentication via multiple providers (OAuth2, SAML, local credentials) - JWT token generation and validation (RS256 algorithm) - Role-based access control (RBAC) with fine-grained permissions - Session management with Redis-backed storage - API key generation and validation for service accounts - MFA enforcement for administrative operations **Authentication Flow**: ``` Client Request \u2192 API Gateway \u2192 Auth Service \u2502 \u251c\u2500 Validate credentials \u251c\u2500 Check MFA if required \u251c\u2500 Generate JWT (15min expiry) \u251c\u2500 Generate refresh token (7 days) \u2514\u2500 Store session in Redis ``` **Token Structure**: - Access Token: JWT with 15-minute expiry - Refresh Token: Opaque token with 7-day expiry, stored in PostgreSQL - Claims: user_id, email, roles[], permissions[], tenant_id **Security Features**: - Password hashing: Argon2id with 64MB memory, 4 iterations - Token rotation on refresh to prevent replay attacks - Brute force protection: 5 failed attempts \u2192 15-minute lockout - Secrets stored in HashiCorp Vault, rotated every 90 days **Performance Targets**: - Token validation: < 10ms (P99) - Token generation: < 50ms (P99) - Throughput: 5,000 RPS for validation operations --- ### Workflow Engine **Purpose**: Core orchestration service that executes workflow definitions, manages state transitions, and coordinates task execution across distributed systems.",
              "score": null
            }
          ],
          "latency_ms": 943.435707999015,
          "query_id": "realistic_002",
          "dimension": "negation",
          "query": "Why am I being blocked with rate limit errors?"
        },
        {
          "key_facts": [
            "3600 seconds",
            "max 3600 seconds from iat",
            "All tokens expire after 3600 seconds"
          ],
          "found_facts": [
            "3600 seconds",
            "All tokens expire after 3600 seconds"
          ],
          "missed_facts": [
            "max 3600 seconds from iat"
          ],
          "coverage": 0.6666666666666666,
          "expected_docs": [
            "api_reference"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_1",
              "content": "**Technology**: Node.js with Express.js framework **Replicas**: 12 pods (production), auto-scaling 8-20 based on CPU **Resource Allocation**: 2 vCPU, 4GB RAM per pod **Key Responsibilities**: - JWT token validation (delegated to Auth Service for initial validation) - Rate limiting: 1000 requests per minute per API key (sliding window) - Request/response transformation and validation using JSON Schema - Routing to downstream services based on URL path patterns - CORS handling for web clients - Request/response logging and correlation ID injection **Critical Endpoints**: - `POST /api/v1/workflows` - Create new workflow - `GET /api/v1/workflows/:id` - Retrieve workflow status - `POST /api/v1/workflows/:id/execute` - Trigger workflow execution - `GET /api/v1/workflows/:id/history` - Get execution history **Dependencies**: - Auth Service (for token validation) - Redis (for rate limiting counters) - All downstream microservices **Performance Targets**: - P50 latency: < 50ms - P99 latency: < 200ms - Throughput: 10,000 RPS sustained --- ### Auth Service **Purpose**: Centralized authentication and authorization service handling user identity, token generation, and permission validation. **Technology**: Go with gRPC for internal communication, REST for external **Replicas**: 8 pods (production), auto-scaling 6-12 **Resource Allocation**: 1 vCPU, 2GB RAM per pod **Key Responsibilities**: - User authentication via multiple providers (OAuth2, SAML, local credentials) - JWT token generation and validation (RS256 algorithm) - Role-based access control (RBAC) with fine-grained permissions - Session management with Redis-backed storage - API key generation and validation for service accounts - MFA enforcement for administrative operations **Authentication Flow**: ``` Client Request \u2192 API Gateway \u2192 Auth Service \u2502 \u251c\u2500 Validate credentials \u251c\u2500 Check MFA if required \u251c\u2500 Generate JWT (15min expiry) \u251c\u2500 Generate refresh token (7 days) \u2514\u2500 Store session in Redis ``` **Token Structure**: - Access Token: JWT with 15-minute expiry - Refresh Token: Opaque token with 7-day expiry, stored in PostgreSQL - Claims: user_id, email, roles[], permissions[], tenant_id **Security Features**: - Password hashing: Argon2id with 64MB memory, 4 iterations - Token rotation on refresh to prevent replay attacks - Brute force protection: 5 failed attempts \u2192 15-minute lockout - Secrets stored in HashiCorp Vault, rotated every 90 days **Performance Targets**: - Token validation: < 10ms (P99) - Token generation: < 50ms (P99) - Throughput: 5,000 RPS for validation operations --- ### Workflow Engine **Purpose**: Core orchestration service that executes workflow definitions, manages state transitions, and coordinates task execution across distributed systems.",
              "score": null
            },
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_0",
              "content": "# CloudFlow API Reference Version 2.1.0 | Last Updated: January 2026 ## Overview The CloudFlow API is a RESTful service that enables developers to programmatically manage cloud workflows, data pipelines, and automation tasks. This documentation provides comprehensive details on authentication, endpoints, request/response formats, error handling, and best practices. **Base URL:** `https://api.cloudflow.io/v2` **API Status:** https://status.cloudflow.io ## Authentication CloudFlow supports three authentication methods to suit different use cases and security requirements. ### API Keys API keys provide simple authentication for server-to-server communication. Include your API key in the request header: ```bash curl -H \"X-API-Key: cf_live_a1b2c3d4e5f6g7h8i9j0\" \\ https://api.cloudflow.io/v2/workflows ``` **Security Notes:** - Never expose API keys in client-side code - Rotate keys every 90 days - Use separate keys for development and production environments ### OAuth 2.0 OAuth 2.0 is recommended for applications that access CloudFlow on behalf of users. We support the Authorization Code flow with PKCE. **Authorization Endpoint:** `https://auth.cloudflow.io/oauth/authorize` **Token Endpoint:** `https://auth.cloudflow.io/oauth/token` **Supported Scopes:** - `workflows:read` - Read workflow configurations - `workflows:write` - Create and modify workflows - `pipelines:read` - Read pipeline data - `pipelines:write` - Create and manage pipelines - `analytics:read` - Access analytics and metrics - `admin:full` - Full administrative access Example authorization request: ```bash curl -X POST https://auth.cloudflow.io/oauth/token \\ -H \"Content-Type: application/x-www-form-urlencoded\" \\ -d \"grant_type=authorization_code\" \\ -d \"code=AUTH_CODE_HERE\" \\ -d \"client_id=YOUR_CLIENT_ID\" \\ -d \"client_secret=YOUR_CLIENT_SECRET\" \\ -d \"redirect_uri=https://yourapp.com/callback\" ``` ### JWT Tokens For advanced use cases, CloudFlow supports JSON Web Tokens (JWT) with RS256 signing algorithm. JWTs must include the following claims: - `iss` (issuer): Your application identifier - `sub` (subject): User or service account ID - `aud` (audience): `https://api.cloudflow.io` - `exp` (expiration): Unix timestamp (max 3600 seconds from `iat`) - `iat` (issued at): Unix timestamp - `scope`: Space-separated list of requested scopes Example JWT header: ```python import jwt import time from cryptography.hazmat.primitives import serialization from cryptography.hazmat.backends import default_backend # Load your private key with open('private_key.pem', 'rb') as key_file: private_key = serialization.load_pem_private_key( key_file.read(), password=None, backend=default_backend() ) # Create JWT payload payload = { 'iss': 'your-app-id', 'sub': 'user-12345', 'aud': 'https://api.cloudflow.io', 'exp': int(time.time()) + 3600, 'iat': int(time.time()), 'scope': 'workflows:read workflows:write' } # Generate token token = jwt.encode(payload, private_key, algorithm='RS256') # Use in API request headers = {'Authorization': f'Bearer {token}'} ``` **Token Expiration:** All tokens expire after 3600 seconds (1 hour). Implement token refresh logic in your application.",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_0",
              "content": "# CloudFlow Platform Troubleshooting Guide **Version:** 3.2.1 **Last Updated:** January 2026 **Audience:** Platform Engineers, DevOps, Support Teams ## Table of Contents 1. [Overview](#overview) 2. [Authentication & Authorization Issues](#authentication--authorization-issues) 3. [Performance Problems](#performance-problems) 4. [Database Connection Issues](#database-connection-issues) 5. [Workflow Execution Failures](#workflow-execution-failures) 6. [Rate Limiting & Throttling](#rate-limiting--throttling) 7. [Log Analysis & Debugging](#log-analysis--debugging) 8. [Escalation Procedures](#escalation-procedures) --- ## Overview This guide provides comprehensive troubleshooting steps for common CloudFlow platform issues encountered in production environments. Each section includes error symptoms, root cause analysis, resolution steps, and preventive measures. ### Quick Diagnostic Checklist Before diving into specific issues, perform these initial checks: - Verify service health: `cloudflow status --all` - Check API connectivity: `curl -I https://api.cloudflow.io/health` - Review recent deployments: `kubectl get deployments -n cloudflow --sort-by=.metadata.creationTimestamp` - Inspect platform metrics: `cloudflow metrics --last 1h` --- ## Authentication & Authorization Issues ### 401 Unauthorized Errors #### Symptoms - API requests return `401 Unauthorized` - Error message: `Authentication credentials were not provided or are invalid` - Frontend displays \"Session expired\" message #### Common Causes **1. Token Expiration** CloudFlow access tokens expire after 3600 seconds (1 hour) by default. Refresh tokens are valid for 30 days. **Verification:** ```bash # Decode JWT to check expiration echo $CF_ACCESS_TOKEN | cut -d'.' -f2 | base64 -d | jq '.exp' # Compare with current time date +%s ``` **Resolution:** ```bash # Refresh the access token curl -X POST https://api.cloudflow.io/auth/refresh \\ -H \"Content-Type: application/json\" \\ -d '{\"refresh_token\": \"'$CF_REFRESH_TOKEN'\"}' # Update environment variable export CF_ACCESS_TOKEN=\"<new_token>\" ``` **2. Invalid JWT Signature** Error: `JWT signature verification failed` **Causes:** - Token was modified or corrupted - Using wrong signing key - Token generated with different secret **Resolution:** ```bash # Validate token structure cloudflow auth validate --token $CF_ACCESS_TOKEN # Generate new token cloudflow auth login --username <user> --password <pass> # For service accounts cloudflow auth service-account --create --name <sa-name> --scopes \"workflow:read,workflow:write\" ``` **3. Clock Skew Issues** JWT validation fails when system clocks are out of sync (tolerance: \u00b1300 seconds).",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_1",
              "content": "**Diagnosis:** ```bash # Check system time timedatectl status # Compare with NTP server ntpdate -q pool.ntp.org # Check JWT issued time vs current time jwt_iat=$(echo $CF_ACCESS_TOKEN | cut -d'.' -f2 | base64 -d | jq '.iat') current_time=$(date +%s) skew=$((current_time - jwt_iat)) echo \"Clock skew: $skew seconds\" ``` **Resolution:** ```bash # Sync with NTP server sudo ntpdate -s pool.ntp.org # Enable automatic time sync sudo timedatectl set-ntp true # Restart CloudFlow client cloudflow restart ``` **4. Insufficient Permissions** Error: `User does not have required permissions for this operation` **Check user roles:** ```bash cloudflow auth whoami --verbose # Expected output: # User: john.doe@company.com # Roles: developer, workflow-admin # Scopes: workflow:*, data:read, metrics:read ``` **Request permission elevation:** ```bash # Submit access request cloudflow auth request-access \\ --resource \"workflow:production:deploy\" \\ --justification \"Deploy critical hotfix for TICKET-1234\" # Check pending approvals cloudflow auth list-requests --status pending ``` ### 403 Forbidden Errors #### RBAC Policy Violations CloudFlow uses role-based access control (RBAC) with the following hierarchy: - `viewer` - Read-only access - `developer` - Create and modify workflows (non-production) - `operator` - Execute workflows, view logs - `admin` - Full access to workspace - `platform-admin` - Cross-workspace administration **Verify resource permissions:** ```bash # Check effective permissions cloudflow rbac check \\ --user john.doe@company.com \\ --resource workflow:prod-pipeline \\ --action execute # List all policies affecting user cloudflow rbac policies --user john.doe@company.com --verbose ``` --- ## Performance Problems ### Slow Query Performance #### Symptoms - API response times > 5000ms - Database query latency warnings in logs - CloudFlow UI becomes unresponsive - Timeout errors: `Request exceeded maximum duration of 30000ms` #### Diagnosis Steps **1. Identify Slow Queries** ```bash # Query CloudFlow metrics cloudflow metrics query --metric api_request_duration_ms \\ --filter \"p95 > 5000\" \\ --last 1h # Check database slow query log kubectl logs -n cloudflow deploy/cloudflow-db-primary | \\ grep \"slow query\" | \\ tail -n 50 # Analyze query patterns cloudflow db analyze-queries --min-duration 5000 --limit 20 ``` **2.",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_8",
              "content": "### Cache Invalidation Strategies **Time-Based Expiration (TTL)**: - Short-lived data: 5-15 minutes (session tokens, rate limit counters) - Medium-lived data: 1 hour (workflow definitions, templates) - Long-lived data: 24 hours (static configuration) **Event-Based Invalidation**: ``` Database Update Event \u2192 Kafka (cache.invalidation topic) \u2502 \u25bc All service instances consume event \u2502 \u25bc Redis DEL for affected keys ``` **Pattern-Based Invalidation**: - Use Redis SCAN + DEL for wildcard patterns - Example: Invalidate all user caches: `user:*:{user_id}` **Cache Warming**: - Scheduled job runs every hour to pre-populate frequently accessed data - Targets: Top 1000 workflows, system configuration, active user profiles - Reduces cache miss rate during traffic spikes ### Multi-Level Caching Application implements L1 (in-memory) and L2 (Redis) caching: ``` Request \u2192 L1 Cache (Node.js Map, 1000 entry LRU) \u2502 MISS \u2502 \u25bc L2 Cache (Redis, distributed) \u2502 MISS \u2502 \u25bc Database (PostgreSQL) ``` L1 cache reduces Redis network calls by 40% for hot data. --- ## Security Architecture ### Network Security **Zero-Trust Network Model**: - All service-to-service communication encrypted with mTLS - Certificate rotation: Every 90 days (automated via cert-manager) - Certificate authority: Internal PKI with HashiCorp Vault - Network segmentation: Private subnets for services, public subnet for ALB only **Service Mesh (Istio)**: ``` Service A \u2192 Envoy Sidecar (mTLS client cert) \u2502 Encrypted channel \u2502 Envoy Sidecar (mTLS server cert) \u2192 Service B ``` **Firewall Rules**: - Security groups: Deny all by default, explicit allow rules - Ingress: Only ALB can reach API Gateway (port 8080) - Egress: Services can only reach specific dependencies - Database access: Limited to application subnets only ### Authentication & Authorization **JWT Token Validation**: - Algorithm: RS256 (asymmetric signing) - Key rotation: Every 30 days with 7-day overlap period - Public key distribution: JWKS endpoint cached in Redis - Validation: Signature, expiry, issuer, audience claims - Token revocation: Blacklist in Redis for compromised tokens **Permission Model**: ``` User \u2192 Roles \u2192 Permissions \u2198 \u2197 Tenants (Multi-tenancy isolation) ``` Example permissions: - `workflow:read` - View workflows - `workflow:write` - Create/update workflows - `workflow:execute` - Trigger workflow execution - `workflow:delete` - Delete workflows - `admin:*` - All administrative operations **API Key Management**: - Format: `cfk_live_<32-char-random>` (production), `cfk_test_<32-char-random>` (sandbox) - Hashing: SHA-256 before storage in PostgreSQL - Scoping: API keys can be scoped to specific workflows or operations - Rate limits: Configurable",
              "score": null
            }
          ],
          "latency_ms": 906.9581660005497,
          "query_id": "realistic_003",
          "dimension": "original",
          "query": "What is the JWT token expiration time?"
        },
        {
          "key_facts": [
            "3600 seconds",
            "max 3600 seconds from iat",
            "All tokens expire after 3600 seconds"
          ],
          "found_facts": [
            "3600 seconds",
            "All tokens expire after 3600 seconds"
          ],
          "missed_facts": [
            "max 3600 seconds from iat"
          ],
          "coverage": 0.6666666666666666,
          "expected_docs": [
            "api_reference"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_0",
              "content": "# CloudFlow Platform Troubleshooting Guide **Version:** 3.2.1 **Last Updated:** January 2026 **Audience:** Platform Engineers, DevOps, Support Teams ## Table of Contents 1. [Overview](#overview) 2. [Authentication & Authorization Issues](#authentication--authorization-issues) 3. [Performance Problems](#performance-problems) 4. [Database Connection Issues](#database-connection-issues) 5. [Workflow Execution Failures](#workflow-execution-failures) 6. [Rate Limiting & Throttling](#rate-limiting--throttling) 7. [Log Analysis & Debugging](#log-analysis--debugging) 8. [Escalation Procedures](#escalation-procedures) --- ## Overview This guide provides comprehensive troubleshooting steps for common CloudFlow platform issues encountered in production environments. Each section includes error symptoms, root cause analysis, resolution steps, and preventive measures. ### Quick Diagnostic Checklist Before diving into specific issues, perform these initial checks: - Verify service health: `cloudflow status --all` - Check API connectivity: `curl -I https://api.cloudflow.io/health` - Review recent deployments: `kubectl get deployments -n cloudflow --sort-by=.metadata.creationTimestamp` - Inspect platform metrics: `cloudflow metrics --last 1h` --- ## Authentication & Authorization Issues ### 401 Unauthorized Errors #### Symptoms - API requests return `401 Unauthorized` - Error message: `Authentication credentials were not provided or are invalid` - Frontend displays \"Session expired\" message #### Common Causes **1. Token Expiration** CloudFlow access tokens expire after 3600 seconds (1 hour) by default. Refresh tokens are valid for 30 days. **Verification:** ```bash # Decode JWT to check expiration echo $CF_ACCESS_TOKEN | cut -d'.' -f2 | base64 -d | jq '.exp' # Compare with current time date +%s ``` **Resolution:** ```bash # Refresh the access token curl -X POST https://api.cloudflow.io/auth/refresh \\ -H \"Content-Type: application/json\" \\ -d '{\"refresh_token\": \"'$CF_REFRESH_TOKEN'\"}' # Update environment variable export CF_ACCESS_TOKEN=\"<new_token>\" ``` **2. Invalid JWT Signature** Error: `JWT signature verification failed` **Causes:** - Token was modified or corrupted - Using wrong signing key - Token generated with different secret **Resolution:** ```bash # Validate token structure cloudflow auth validate --token $CF_ACCESS_TOKEN # Generate new token cloudflow auth login --username <user> --password <pass> # For service accounts cloudflow auth service-account --create --name <sa-name> --scopes \"workflow:read,workflow:write\" ``` **3. Clock Skew Issues** JWT validation fails when system clocks are out of sync (tolerance: \u00b1300 seconds).",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_8",
              "content": "### Cache Invalidation Strategies **Time-Based Expiration (TTL)**: - Short-lived data: 5-15 minutes (session tokens, rate limit counters) - Medium-lived data: 1 hour (workflow definitions, templates) - Long-lived data: 24 hours (static configuration) **Event-Based Invalidation**: ``` Database Update Event \u2192 Kafka (cache.invalidation topic) \u2502 \u25bc All service instances consume event \u2502 \u25bc Redis DEL for affected keys ``` **Pattern-Based Invalidation**: - Use Redis SCAN + DEL for wildcard patterns - Example: Invalidate all user caches: `user:*:{user_id}` **Cache Warming**: - Scheduled job runs every hour to pre-populate frequently accessed data - Targets: Top 1000 workflows, system configuration, active user profiles - Reduces cache miss rate during traffic spikes ### Multi-Level Caching Application implements L1 (in-memory) and L2 (Redis) caching: ``` Request \u2192 L1 Cache (Node.js Map, 1000 entry LRU) \u2502 MISS \u2502 \u25bc L2 Cache (Redis, distributed) \u2502 MISS \u2502 \u25bc Database (PostgreSQL) ``` L1 cache reduces Redis network calls by 40% for hot data. --- ## Security Architecture ### Network Security **Zero-Trust Network Model**: - All service-to-service communication encrypted with mTLS - Certificate rotation: Every 90 days (automated via cert-manager) - Certificate authority: Internal PKI with HashiCorp Vault - Network segmentation: Private subnets for services, public subnet for ALB only **Service Mesh (Istio)**: ``` Service A \u2192 Envoy Sidecar (mTLS client cert) \u2502 Encrypted channel \u2502 Envoy Sidecar (mTLS server cert) \u2192 Service B ``` **Firewall Rules**: - Security groups: Deny all by default, explicit allow rules - Ingress: Only ALB can reach API Gateway (port 8080) - Egress: Services can only reach specific dependencies - Database access: Limited to application subnets only ### Authentication & Authorization **JWT Token Validation**: - Algorithm: RS256 (asymmetric signing) - Key rotation: Every 30 days with 7-day overlap period - Public key distribution: JWKS endpoint cached in Redis - Validation: Signature, expiry, issuer, audience claims - Token revocation: Blacklist in Redis for compromised tokens **Permission Model**: ``` User \u2192 Roles \u2192 Permissions \u2198 \u2197 Tenants (Multi-tenancy isolation) ``` Example permissions: - `workflow:read` - View workflows - `workflow:write` - Create/update workflows - `workflow:execute` - Trigger workflow execution - `workflow:delete` - Delete workflows - `admin:*` - All administrative operations **API Key Management**: - Format: `cfk_live_<32-char-random>` (production), `cfk_test_<32-char-random>` (sandbox) - Hashing: SHA-256 before storage in PostgreSQL - Scoping: API keys can be scoped to specific workflows or operations - Rate limits: Configurable",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_1",
              "content": "**Technology**: Node.js with Express.js framework **Replicas**: 12 pods (production), auto-scaling 8-20 based on CPU **Resource Allocation**: 2 vCPU, 4GB RAM per pod **Key Responsibilities**: - JWT token validation (delegated to Auth Service for initial validation) - Rate limiting: 1000 requests per minute per API key (sliding window) - Request/response transformation and validation using JSON Schema - Routing to downstream services based on URL path patterns - CORS handling for web clients - Request/response logging and correlation ID injection **Critical Endpoints**: - `POST /api/v1/workflows` - Create new workflow - `GET /api/v1/workflows/:id` - Retrieve workflow status - `POST /api/v1/workflows/:id/execute` - Trigger workflow execution - `GET /api/v1/workflows/:id/history` - Get execution history **Dependencies**: - Auth Service (for token validation) - Redis (for rate limiting counters) - All downstream microservices **Performance Targets**: - P50 latency: < 50ms - P99 latency: < 200ms - Throughput: 10,000 RPS sustained --- ### Auth Service **Purpose**: Centralized authentication and authorization service handling user identity, token generation, and permission validation. **Technology**: Go with gRPC for internal communication, REST for external **Replicas**: 8 pods (production), auto-scaling 6-12 **Resource Allocation**: 1 vCPU, 2GB RAM per pod **Key Responsibilities**: - User authentication via multiple providers (OAuth2, SAML, local credentials) - JWT token generation and validation (RS256 algorithm) - Role-based access control (RBAC) with fine-grained permissions - Session management with Redis-backed storage - API key generation and validation for service accounts - MFA enforcement for administrative operations **Authentication Flow**: ``` Client Request \u2192 API Gateway \u2192 Auth Service \u2502 \u251c\u2500 Validate credentials \u251c\u2500 Check MFA if required \u251c\u2500 Generate JWT (15min expiry) \u251c\u2500 Generate refresh token (7 days) \u2514\u2500 Store session in Redis ``` **Token Structure**: - Access Token: JWT with 15-minute expiry - Refresh Token: Opaque token with 7-day expiry, stored in PostgreSQL - Claims: user_id, email, roles[], permissions[], tenant_id **Security Features**: - Password hashing: Argon2id with 64MB memory, 4 iterations - Token rotation on refresh to prevent replay attacks - Brute force protection: 5 failed attempts \u2192 15-minute lockout - Secrets stored in HashiCorp Vault, rotated every 90 days **Performance Targets**: - Token validation: < 10ms (P99) - Token generation: < 50ms (P99) - Throughput: 5,000 RPS for validation operations --- ### Workflow Engine **Purpose**: Core orchestration service that executes workflow definitions, manages state transitions, and coordinates task execution across distributed systems.",
              "score": null
            },
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_0",
              "content": "# CloudFlow API Reference Version 2.1.0 | Last Updated: January 2026 ## Overview The CloudFlow API is a RESTful service that enables developers to programmatically manage cloud workflows, data pipelines, and automation tasks. This documentation provides comprehensive details on authentication, endpoints, request/response formats, error handling, and best practices. **Base URL:** `https://api.cloudflow.io/v2` **API Status:** https://status.cloudflow.io ## Authentication CloudFlow supports three authentication methods to suit different use cases and security requirements. ### API Keys API keys provide simple authentication for server-to-server communication. Include your API key in the request header: ```bash curl -H \"X-API-Key: cf_live_a1b2c3d4e5f6g7h8i9j0\" \\ https://api.cloudflow.io/v2/workflows ``` **Security Notes:** - Never expose API keys in client-side code - Rotate keys every 90 days - Use separate keys for development and production environments ### OAuth 2.0 OAuth 2.0 is recommended for applications that access CloudFlow on behalf of users. We support the Authorization Code flow with PKCE. **Authorization Endpoint:** `https://auth.cloudflow.io/oauth/authorize` **Token Endpoint:** `https://auth.cloudflow.io/oauth/token` **Supported Scopes:** - `workflows:read` - Read workflow configurations - `workflows:write` - Create and modify workflows - `pipelines:read` - Read pipeline data - `pipelines:write` - Create and manage pipelines - `analytics:read` - Access analytics and metrics - `admin:full` - Full administrative access Example authorization request: ```bash curl -X POST https://auth.cloudflow.io/oauth/token \\ -H \"Content-Type: application/x-www-form-urlencoded\" \\ -d \"grant_type=authorization_code\" \\ -d \"code=AUTH_CODE_HERE\" \\ -d \"client_id=YOUR_CLIENT_ID\" \\ -d \"client_secret=YOUR_CLIENT_SECRET\" \\ -d \"redirect_uri=https://yourapp.com/callback\" ``` ### JWT Tokens For advanced use cases, CloudFlow supports JSON Web Tokens (JWT) with RS256 signing algorithm. JWTs must include the following claims: - `iss` (issuer): Your application identifier - `sub` (subject): User or service account ID - `aud` (audience): `https://api.cloudflow.io` - `exp` (expiration): Unix timestamp (max 3600 seconds from `iat`) - `iat` (issued at): Unix timestamp - `scope`: Space-separated list of requested scopes Example JWT header: ```python import jwt import time from cryptography.hazmat.primitives import serialization from cryptography.hazmat.backends import default_backend # Load your private key with open('private_key.pem', 'rb') as key_file: private_key = serialization.load_pem_private_key( key_file.read(), password=None, backend=default_backend() ) # Create JWT payload payload = { 'iss': 'your-app-id', 'sub': 'user-12345', 'aud': 'https://api.cloudflow.io', 'exp': int(time.time()) + 3600, 'iat': int(time.time()), 'scope': 'workflows:read workflows:write' } # Generate token token = jwt.encode(payload, private_key, algorithm='RS256') # Use in API request headers = {'Authorization': f'Bearer {token}'} ``` **Token Expiration:** All tokens expire after 3600 seconds (1 hour). Implement token refresh logic in your application.",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_1",
              "content": "**Diagnosis:** ```bash # Check system time timedatectl status # Compare with NTP server ntpdate -q pool.ntp.org # Check JWT issued time vs current time jwt_iat=$(echo $CF_ACCESS_TOKEN | cut -d'.' -f2 | base64 -d | jq '.iat') current_time=$(date +%s) skew=$((current_time - jwt_iat)) echo \"Clock skew: $skew seconds\" ``` **Resolution:** ```bash # Sync with NTP server sudo ntpdate -s pool.ntp.org # Enable automatic time sync sudo timedatectl set-ntp true # Restart CloudFlow client cloudflow restart ``` **4. Insufficient Permissions** Error: `User does not have required permissions for this operation` **Check user roles:** ```bash cloudflow auth whoami --verbose # Expected output: # User: john.doe@company.com # Roles: developer, workflow-admin # Scopes: workflow:*, data:read, metrics:read ``` **Request permission elevation:** ```bash # Submit access request cloudflow auth request-access \\ --resource \"workflow:production:deploy\" \\ --justification \"Deploy critical hotfix for TICKET-1234\" # Check pending approvals cloudflow auth list-requests --status pending ``` ### 403 Forbidden Errors #### RBAC Policy Violations CloudFlow uses role-based access control (RBAC) with the following hierarchy: - `viewer` - Read-only access - `developer` - Create and modify workflows (non-production) - `operator` - Execute workflows, view logs - `admin` - Full access to workspace - `platform-admin` - Cross-workspace administration **Verify resource permissions:** ```bash # Check effective permissions cloudflow rbac check \\ --user john.doe@company.com \\ --resource workflow:prod-pipeline \\ --action execute # List all policies affecting user cloudflow rbac policies --user john.doe@company.com --verbose ``` --- ## Performance Problems ### Slow Query Performance #### Symptoms - API response times > 5000ms - Database query latency warnings in logs - CloudFlow UI becomes unresponsive - Timeout errors: `Request exceeded maximum duration of 30000ms` #### Diagnosis Steps **1. Identify Slow Queries** ```bash # Query CloudFlow metrics cloudflow metrics query --metric api_request_duration_ms \\ --filter \"p95 > 5000\" \\ --last 1h # Check database slow query log kubectl logs -n cloudflow deploy/cloudflow-db-primary | \\ grep \"slow query\" | \\ tail -n 50 # Analyze query patterns cloudflow db analyze-queries --min-duration 5000 --limit 20 ``` **2.",
              "score": null
            }
          ],
          "latency_ms": 1157.783749000373,
          "query_id": "realistic_003",
          "dimension": "synonym",
          "query": "How long do access tokens last?"
        },
        {
          "key_facts": [
            "3600 seconds",
            "max 3600 seconds from iat",
            "All tokens expire after 3600 seconds"
          ],
          "found_facts": [
            "3600 seconds",
            "All tokens expire after 3600 seconds"
          ],
          "missed_facts": [
            "max 3600 seconds from iat"
          ],
          "coverage": 0.6666666666666666,
          "expected_docs": [
            "api_reference"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_1",
              "content": "**Technology**: Node.js with Express.js framework **Replicas**: 12 pods (production), auto-scaling 8-20 based on CPU **Resource Allocation**: 2 vCPU, 4GB RAM per pod **Key Responsibilities**: - JWT token validation (delegated to Auth Service for initial validation) - Rate limiting: 1000 requests per minute per API key (sliding window) - Request/response transformation and validation using JSON Schema - Routing to downstream services based on URL path patterns - CORS handling for web clients - Request/response logging and correlation ID injection **Critical Endpoints**: - `POST /api/v1/workflows` - Create new workflow - `GET /api/v1/workflows/:id` - Retrieve workflow status - `POST /api/v1/workflows/:id/execute` - Trigger workflow execution - `GET /api/v1/workflows/:id/history` - Get execution history **Dependencies**: - Auth Service (for token validation) - Redis (for rate limiting counters) - All downstream microservices **Performance Targets**: - P50 latency: < 50ms - P99 latency: < 200ms - Throughput: 10,000 RPS sustained --- ### Auth Service **Purpose**: Centralized authentication and authorization service handling user identity, token generation, and permission validation. **Technology**: Go with gRPC for internal communication, REST for external **Replicas**: 8 pods (production), auto-scaling 6-12 **Resource Allocation**: 1 vCPU, 2GB RAM per pod **Key Responsibilities**: - User authentication via multiple providers (OAuth2, SAML, local credentials) - JWT token generation and validation (RS256 algorithm) - Role-based access control (RBAC) with fine-grained permissions - Session management with Redis-backed storage - API key generation and validation for service accounts - MFA enforcement for administrative operations **Authentication Flow**: ``` Client Request \u2192 API Gateway \u2192 Auth Service \u2502 \u251c\u2500 Validate credentials \u251c\u2500 Check MFA if required \u251c\u2500 Generate JWT (15min expiry) \u251c\u2500 Generate refresh token (7 days) \u2514\u2500 Store session in Redis ``` **Token Structure**: - Access Token: JWT with 15-minute expiry - Refresh Token: Opaque token with 7-day expiry, stored in PostgreSQL - Claims: user_id, email, roles[], permissions[], tenant_id **Security Features**: - Password hashing: Argon2id with 64MB memory, 4 iterations - Token rotation on refresh to prevent replay attacks - Brute force protection: 5 failed attempts \u2192 15-minute lockout - Secrets stored in HashiCorp Vault, rotated every 90 days **Performance Targets**: - Token validation: < 10ms (P99) - Token generation: < 50ms (P99) - Throughput: 5,000 RPS for validation operations --- ### Workflow Engine **Purpose**: Core orchestration service that executes workflow definitions, manages state transitions, and coordinates task execution across distributed systems.",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_0",
              "content": "# CloudFlow Platform Troubleshooting Guide **Version:** 3.2.1 **Last Updated:** January 2026 **Audience:** Platform Engineers, DevOps, Support Teams ## Table of Contents 1. [Overview](#overview) 2. [Authentication & Authorization Issues](#authentication--authorization-issues) 3. [Performance Problems](#performance-problems) 4. [Database Connection Issues](#database-connection-issues) 5. [Workflow Execution Failures](#workflow-execution-failures) 6. [Rate Limiting & Throttling](#rate-limiting--throttling) 7. [Log Analysis & Debugging](#log-analysis--debugging) 8. [Escalation Procedures](#escalation-procedures) --- ## Overview This guide provides comprehensive troubleshooting steps for common CloudFlow platform issues encountered in production environments. Each section includes error symptoms, root cause analysis, resolution steps, and preventive measures. ### Quick Diagnostic Checklist Before diving into specific issues, perform these initial checks: - Verify service health: `cloudflow status --all` - Check API connectivity: `curl -I https://api.cloudflow.io/health` - Review recent deployments: `kubectl get deployments -n cloudflow --sort-by=.metadata.creationTimestamp` - Inspect platform metrics: `cloudflow metrics --last 1h` --- ## Authentication & Authorization Issues ### 401 Unauthorized Errors #### Symptoms - API requests return `401 Unauthorized` - Error message: `Authentication credentials were not provided or are invalid` - Frontend displays \"Session expired\" message #### Common Causes **1. Token Expiration** CloudFlow access tokens expire after 3600 seconds (1 hour) by default. Refresh tokens are valid for 30 days. **Verification:** ```bash # Decode JWT to check expiration echo $CF_ACCESS_TOKEN | cut -d'.' -f2 | base64 -d | jq '.exp' # Compare with current time date +%s ``` **Resolution:** ```bash # Refresh the access token curl -X POST https://api.cloudflow.io/auth/refresh \\ -H \"Content-Type: application/json\" \\ -d '{\"refresh_token\": \"'$CF_REFRESH_TOKEN'\"}' # Update environment variable export CF_ACCESS_TOKEN=\"<new_token>\" ``` **2. Invalid JWT Signature** Error: `JWT signature verification failed` **Causes:** - Token was modified or corrupted - Using wrong signing key - Token generated with different secret **Resolution:** ```bash # Validate token structure cloudflow auth validate --token $CF_ACCESS_TOKEN # Generate new token cloudflow auth login --username <user> --password <pass> # For service accounts cloudflow auth service-account --create --name <sa-name> --scopes \"workflow:read,workflow:write\" ``` **3. Clock Skew Issues** JWT validation fails when system clocks are out of sync (tolerance: \u00b1300 seconds).",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_1",
              "content": "**Diagnosis:** ```bash # Check system time timedatectl status # Compare with NTP server ntpdate -q pool.ntp.org # Check JWT issued time vs current time jwt_iat=$(echo $CF_ACCESS_TOKEN | cut -d'.' -f2 | base64 -d | jq '.iat') current_time=$(date +%s) skew=$((current_time - jwt_iat)) echo \"Clock skew: $skew seconds\" ``` **Resolution:** ```bash # Sync with NTP server sudo ntpdate -s pool.ntp.org # Enable automatic time sync sudo timedatectl set-ntp true # Restart CloudFlow client cloudflow restart ``` **4. Insufficient Permissions** Error: `User does not have required permissions for this operation` **Check user roles:** ```bash cloudflow auth whoami --verbose # Expected output: # User: john.doe@company.com # Roles: developer, workflow-admin # Scopes: workflow:*, data:read, metrics:read ``` **Request permission elevation:** ```bash # Submit access request cloudflow auth request-access \\ --resource \"workflow:production:deploy\" \\ --justification \"Deploy critical hotfix for TICKET-1234\" # Check pending approvals cloudflow auth list-requests --status pending ``` ### 403 Forbidden Errors #### RBAC Policy Violations CloudFlow uses role-based access control (RBAC) with the following hierarchy: - `viewer` - Read-only access - `developer` - Create and modify workflows (non-production) - `operator` - Execute workflows, view logs - `admin` - Full access to workspace - `platform-admin` - Cross-workspace administration **Verify resource permissions:** ```bash # Check effective permissions cloudflow rbac check \\ --user john.doe@company.com \\ --resource workflow:prod-pipeline \\ --action execute # List all policies affecting user cloudflow rbac policies --user john.doe@company.com --verbose ``` --- ## Performance Problems ### Slow Query Performance #### Symptoms - API response times > 5000ms - Database query latency warnings in logs - CloudFlow UI becomes unresponsive - Timeout errors: `Request exceeded maximum duration of 30000ms` #### Diagnosis Steps **1. Identify Slow Queries** ```bash # Query CloudFlow metrics cloudflow metrics query --metric api_request_duration_ms \\ --filter \"p95 > 5000\" \\ --last 1h # Check database slow query log kubectl logs -n cloudflow deploy/cloudflow-db-primary | \\ grep \"slow query\" | \\ tail -n 50 # Analyze query patterns cloudflow db analyze-queries --min-duration 5000 --limit 20 ``` **2.",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_8",
              "content": "### Cache Invalidation Strategies **Time-Based Expiration (TTL)**: - Short-lived data: 5-15 minutes (session tokens, rate limit counters) - Medium-lived data: 1 hour (workflow definitions, templates) - Long-lived data: 24 hours (static configuration) **Event-Based Invalidation**: ``` Database Update Event \u2192 Kafka (cache.invalidation topic) \u2502 \u25bc All service instances consume event \u2502 \u25bc Redis DEL for affected keys ``` **Pattern-Based Invalidation**: - Use Redis SCAN + DEL for wildcard patterns - Example: Invalidate all user caches: `user:*:{user_id}` **Cache Warming**: - Scheduled job runs every hour to pre-populate frequently accessed data - Targets: Top 1000 workflows, system configuration, active user profiles - Reduces cache miss rate during traffic spikes ### Multi-Level Caching Application implements L1 (in-memory) and L2 (Redis) caching: ``` Request \u2192 L1 Cache (Node.js Map, 1000 entry LRU) \u2502 MISS \u2502 \u25bc L2 Cache (Redis, distributed) \u2502 MISS \u2502 \u25bc Database (PostgreSQL) ``` L1 cache reduces Redis network calls by 40% for hot data. --- ## Security Architecture ### Network Security **Zero-Trust Network Model**: - All service-to-service communication encrypted with mTLS - Certificate rotation: Every 90 days (automated via cert-manager) - Certificate authority: Internal PKI with HashiCorp Vault - Network segmentation: Private subnets for services, public subnet for ALB only **Service Mesh (Istio)**: ``` Service A \u2192 Envoy Sidecar (mTLS client cert) \u2502 Encrypted channel \u2502 Envoy Sidecar (mTLS server cert) \u2192 Service B ``` **Firewall Rules**: - Security groups: Deny all by default, explicit allow rules - Ingress: Only ALB can reach API Gateway (port 8080) - Egress: Services can only reach specific dependencies - Database access: Limited to application subnets only ### Authentication & Authorization **JWT Token Validation**: - Algorithm: RS256 (asymmetric signing) - Key rotation: Every 30 days with 7-day overlap period - Public key distribution: JWKS endpoint cached in Redis - Validation: Signature, expiry, issuer, audience claims - Token revocation: Blacklist in Redis for compromised tokens **Permission Model**: ``` User \u2192 Roles \u2192 Permissions \u2198 \u2197 Tenants (Multi-tenancy isolation) ``` Example permissions: - `workflow:read` - View workflows - `workflow:write` - Create/update workflows - `workflow:execute` - Trigger workflow execution - `workflow:delete` - Delete workflows - `admin:*` - All administrative operations **API Key Management**: - Format: `cfk_live_<32-char-random>` (production), `cfk_test_<32-char-random>` (sandbox) - Hashing: SHA-256 before storage in PostgreSQL - Scoping: API keys can be scoped to specific workflows or operations - Rate limits: Configurable",
              "score": null
            },
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_0",
              "content": "# CloudFlow API Reference Version 2.1.0 | Last Updated: January 2026 ## Overview The CloudFlow API is a RESTful service that enables developers to programmatically manage cloud workflows, data pipelines, and automation tasks. This documentation provides comprehensive details on authentication, endpoints, request/response formats, error handling, and best practices. **Base URL:** `https://api.cloudflow.io/v2` **API Status:** https://status.cloudflow.io ## Authentication CloudFlow supports three authentication methods to suit different use cases and security requirements. ### API Keys API keys provide simple authentication for server-to-server communication. Include your API key in the request header: ```bash curl -H \"X-API-Key: cf_live_a1b2c3d4e5f6g7h8i9j0\" \\ https://api.cloudflow.io/v2/workflows ``` **Security Notes:** - Never expose API keys in client-side code - Rotate keys every 90 days - Use separate keys for development and production environments ### OAuth 2.0 OAuth 2.0 is recommended for applications that access CloudFlow on behalf of users. We support the Authorization Code flow with PKCE. **Authorization Endpoint:** `https://auth.cloudflow.io/oauth/authorize` **Token Endpoint:** `https://auth.cloudflow.io/oauth/token` **Supported Scopes:** - `workflows:read` - Read workflow configurations - `workflows:write` - Create and modify workflows - `pipelines:read` - Read pipeline data - `pipelines:write` - Create and manage pipelines - `analytics:read` - Access analytics and metrics - `admin:full` - Full administrative access Example authorization request: ```bash curl -X POST https://auth.cloudflow.io/oauth/token \\ -H \"Content-Type: application/x-www-form-urlencoded\" \\ -d \"grant_type=authorization_code\" \\ -d \"code=AUTH_CODE_HERE\" \\ -d \"client_id=YOUR_CLIENT_ID\" \\ -d \"client_secret=YOUR_CLIENT_SECRET\" \\ -d \"redirect_uri=https://yourapp.com/callback\" ``` ### JWT Tokens For advanced use cases, CloudFlow supports JSON Web Tokens (JWT) with RS256 signing algorithm. JWTs must include the following claims: - `iss` (issuer): Your application identifier - `sub` (subject): User or service account ID - `aud` (audience): `https://api.cloudflow.io` - `exp` (expiration): Unix timestamp (max 3600 seconds from `iat`) - `iat` (issued at): Unix timestamp - `scope`: Space-separated list of requested scopes Example JWT header: ```python import jwt import time from cryptography.hazmat.primitives import serialization from cryptography.hazmat.backends import default_backend # Load your private key with open('private_key.pem', 'rb') as key_file: private_key = serialization.load_pem_private_key( key_file.read(), password=None, backend=default_backend() ) # Create JWT payload payload = { 'iss': 'your-app-id', 'sub': 'user-12345', 'aud': 'https://api.cloudflow.io', 'exp': int(time.time()) + 3600, 'iat': int(time.time()), 'scope': 'workflows:read workflows:write' } # Generate token token = jwt.encode(payload, private_key, algorithm='RS256') # Use in API request headers = {'Authorization': f'Bearer {token}'} ``` **Token Expiration:** All tokens expire after 3600 seconds (1 hour). Implement token refresh logic in your application.",
              "score": null
            }
          ],
          "latency_ms": 1013.7116220030293,
          "query_id": "realistic_003",
          "dimension": "problem",
          "query": "My authentication keeps expiring after an hour"
        },
        {
          "key_facts": [
            "3600 seconds",
            "max 3600 seconds from iat",
            "All tokens expire after 3600 seconds"
          ],
          "found_facts": [
            "3600 seconds",
            "All tokens expire after 3600 seconds"
          ],
          "missed_facts": [
            "max 3600 seconds from iat"
          ],
          "coverage": 0.6666666666666666,
          "expected_docs": [
            "api_reference"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_0",
              "content": "# CloudFlow Platform Troubleshooting Guide **Version:** 3.2.1 **Last Updated:** January 2026 **Audience:** Platform Engineers, DevOps, Support Teams ## Table of Contents 1. [Overview](#overview) 2. [Authentication & Authorization Issues](#authentication--authorization-issues) 3. [Performance Problems](#performance-problems) 4. [Database Connection Issues](#database-connection-issues) 5. [Workflow Execution Failures](#workflow-execution-failures) 6. [Rate Limiting & Throttling](#rate-limiting--throttling) 7. [Log Analysis & Debugging](#log-analysis--debugging) 8. [Escalation Procedures](#escalation-procedures) --- ## Overview This guide provides comprehensive troubleshooting steps for common CloudFlow platform issues encountered in production environments. Each section includes error symptoms, root cause analysis, resolution steps, and preventive measures. ### Quick Diagnostic Checklist Before diving into specific issues, perform these initial checks: - Verify service health: `cloudflow status --all` - Check API connectivity: `curl -I https://api.cloudflow.io/health` - Review recent deployments: `kubectl get deployments -n cloudflow --sort-by=.metadata.creationTimestamp` - Inspect platform metrics: `cloudflow metrics --last 1h` --- ## Authentication & Authorization Issues ### 401 Unauthorized Errors #### Symptoms - API requests return `401 Unauthorized` - Error message: `Authentication credentials were not provided or are invalid` - Frontend displays \"Session expired\" message #### Common Causes **1. Token Expiration** CloudFlow access tokens expire after 3600 seconds (1 hour) by default. Refresh tokens are valid for 30 days. **Verification:** ```bash # Decode JWT to check expiration echo $CF_ACCESS_TOKEN | cut -d'.' -f2 | base64 -d | jq '.exp' # Compare with current time date +%s ``` **Resolution:** ```bash # Refresh the access token curl -X POST https://api.cloudflow.io/auth/refresh \\ -H \"Content-Type: application/json\" \\ -d '{\"refresh_token\": \"'$CF_REFRESH_TOKEN'\"}' # Update environment variable export CF_ACCESS_TOKEN=\"<new_token>\" ``` **2. Invalid JWT Signature** Error: `JWT signature verification failed` **Causes:** - Token was modified or corrupted - Using wrong signing key - Token generated with different secret **Resolution:** ```bash # Validate token structure cloudflow auth validate --token $CF_ACCESS_TOKEN # Generate new token cloudflow auth login --username <user> --password <pass> # For service accounts cloudflow auth service-account --create --name <sa-name> --scopes \"workflow:read,workflow:write\" ``` **3. Clock Skew Issues** JWT validation fails when system clocks are out of sync (tolerance: \u00b1300 seconds).",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_8",
              "content": "### Cache Invalidation Strategies **Time-Based Expiration (TTL)**: - Short-lived data: 5-15 minutes (session tokens, rate limit counters) - Medium-lived data: 1 hour (workflow definitions, templates) - Long-lived data: 24 hours (static configuration) **Event-Based Invalidation**: ``` Database Update Event \u2192 Kafka (cache.invalidation topic) \u2502 \u25bc All service instances consume event \u2502 \u25bc Redis DEL for affected keys ``` **Pattern-Based Invalidation**: - Use Redis SCAN + DEL for wildcard patterns - Example: Invalidate all user caches: `user:*:{user_id}` **Cache Warming**: - Scheduled job runs every hour to pre-populate frequently accessed data - Targets: Top 1000 workflows, system configuration, active user profiles - Reduces cache miss rate during traffic spikes ### Multi-Level Caching Application implements L1 (in-memory) and L2 (Redis) caching: ``` Request \u2192 L1 Cache (Node.js Map, 1000 entry LRU) \u2502 MISS \u2502 \u25bc L2 Cache (Redis, distributed) \u2502 MISS \u2502 \u25bc Database (PostgreSQL) ``` L1 cache reduces Redis network calls by 40% for hot data. --- ## Security Architecture ### Network Security **Zero-Trust Network Model**: - All service-to-service communication encrypted with mTLS - Certificate rotation: Every 90 days (automated via cert-manager) - Certificate authority: Internal PKI with HashiCorp Vault - Network segmentation: Private subnets for services, public subnet for ALB only **Service Mesh (Istio)**: ``` Service A \u2192 Envoy Sidecar (mTLS client cert) \u2502 Encrypted channel \u2502 Envoy Sidecar (mTLS server cert) \u2192 Service B ``` **Firewall Rules**: - Security groups: Deny all by default, explicit allow rules - Ingress: Only ALB can reach API Gateway (port 8080) - Egress: Services can only reach specific dependencies - Database access: Limited to application subnets only ### Authentication & Authorization **JWT Token Validation**: - Algorithm: RS256 (asymmetric signing) - Key rotation: Every 30 days with 7-day overlap period - Public key distribution: JWKS endpoint cached in Redis - Validation: Signature, expiry, issuer, audience claims - Token revocation: Blacklist in Redis for compromised tokens **Permission Model**: ``` User \u2192 Roles \u2192 Permissions \u2198 \u2197 Tenants (Multi-tenancy isolation) ``` Example permissions: - `workflow:read` - View workflows - `workflow:write` - Create/update workflows - `workflow:execute` - Trigger workflow execution - `workflow:delete` - Delete workflows - `admin:*` - All administrative operations **API Key Management**: - Format: `cfk_live_<32-char-random>` (production), `cfk_test_<32-char-random>` (sandbox) - Hashing: SHA-256 before storage in PostgreSQL - Scoping: API keys can be scoped to specific workflows or operations - Rate limits: Configurable",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_1",
              "content": "**Technology**: Node.js with Express.js framework **Replicas**: 12 pods (production), auto-scaling 8-20 based on CPU **Resource Allocation**: 2 vCPU, 4GB RAM per pod **Key Responsibilities**: - JWT token validation (delegated to Auth Service for initial validation) - Rate limiting: 1000 requests per minute per API key (sliding window) - Request/response transformation and validation using JSON Schema - Routing to downstream services based on URL path patterns - CORS handling for web clients - Request/response logging and correlation ID injection **Critical Endpoints**: - `POST /api/v1/workflows` - Create new workflow - `GET /api/v1/workflows/:id` - Retrieve workflow status - `POST /api/v1/workflows/:id/execute` - Trigger workflow execution - `GET /api/v1/workflows/:id/history` - Get execution history **Dependencies**: - Auth Service (for token validation) - Redis (for rate limiting counters) - All downstream microservices **Performance Targets**: - P50 latency: < 50ms - P99 latency: < 200ms - Throughput: 10,000 RPS sustained --- ### Auth Service **Purpose**: Centralized authentication and authorization service handling user identity, token generation, and permission validation. **Technology**: Go with gRPC for internal communication, REST for external **Replicas**: 8 pods (production), auto-scaling 6-12 **Resource Allocation**: 1 vCPU, 2GB RAM per pod **Key Responsibilities**: - User authentication via multiple providers (OAuth2, SAML, local credentials) - JWT token generation and validation (RS256 algorithm) - Role-based access control (RBAC) with fine-grained permissions - Session management with Redis-backed storage - API key generation and validation for service accounts - MFA enforcement for administrative operations **Authentication Flow**: ``` Client Request \u2192 API Gateway \u2192 Auth Service \u2502 \u251c\u2500 Validate credentials \u251c\u2500 Check MFA if required \u251c\u2500 Generate JWT (15min expiry) \u251c\u2500 Generate refresh token (7 days) \u2514\u2500 Store session in Redis ``` **Token Structure**: - Access Token: JWT with 15-minute expiry - Refresh Token: Opaque token with 7-day expiry, stored in PostgreSQL - Claims: user_id, email, roles[], permissions[], tenant_id **Security Features**: - Password hashing: Argon2id with 64MB memory, 4 iterations - Token rotation on refresh to prevent replay attacks - Brute force protection: 5 failed attempts \u2192 15-minute lockout - Secrets stored in HashiCorp Vault, rotated every 90 days **Performance Targets**: - Token validation: < 10ms (P99) - Token generation: < 50ms (P99) - Throughput: 5,000 RPS for validation operations --- ### Workflow Engine **Purpose**: Core orchestration service that executes workflow definitions, manages state transitions, and coordinates task execution across distributed systems.",
              "score": null
            },
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_0",
              "content": "# CloudFlow API Reference Version 2.1.0 | Last Updated: January 2026 ## Overview The CloudFlow API is a RESTful service that enables developers to programmatically manage cloud workflows, data pipelines, and automation tasks. This documentation provides comprehensive details on authentication, endpoints, request/response formats, error handling, and best practices. **Base URL:** `https://api.cloudflow.io/v2` **API Status:** https://status.cloudflow.io ## Authentication CloudFlow supports three authentication methods to suit different use cases and security requirements. ### API Keys API keys provide simple authentication for server-to-server communication. Include your API key in the request header: ```bash curl -H \"X-API-Key: cf_live_a1b2c3d4e5f6g7h8i9j0\" \\ https://api.cloudflow.io/v2/workflows ``` **Security Notes:** - Never expose API keys in client-side code - Rotate keys every 90 days - Use separate keys for development and production environments ### OAuth 2.0 OAuth 2.0 is recommended for applications that access CloudFlow on behalf of users. We support the Authorization Code flow with PKCE. **Authorization Endpoint:** `https://auth.cloudflow.io/oauth/authorize` **Token Endpoint:** `https://auth.cloudflow.io/oauth/token` **Supported Scopes:** - `workflows:read` - Read workflow configurations - `workflows:write` - Create and modify workflows - `pipelines:read` - Read pipeline data - `pipelines:write` - Create and manage pipelines - `analytics:read` - Access analytics and metrics - `admin:full` - Full administrative access Example authorization request: ```bash curl -X POST https://auth.cloudflow.io/oauth/token \\ -H \"Content-Type: application/x-www-form-urlencoded\" \\ -d \"grant_type=authorization_code\" \\ -d \"code=AUTH_CODE_HERE\" \\ -d \"client_id=YOUR_CLIENT_ID\" \\ -d \"client_secret=YOUR_CLIENT_SECRET\" \\ -d \"redirect_uri=https://yourapp.com/callback\" ``` ### JWT Tokens For advanced use cases, CloudFlow supports JSON Web Tokens (JWT) with RS256 signing algorithm. JWTs must include the following claims: - `iss` (issuer): Your application identifier - `sub` (subject): User or service account ID - `aud` (audience): `https://api.cloudflow.io` - `exp` (expiration): Unix timestamp (max 3600 seconds from `iat`) - `iat` (issued at): Unix timestamp - `scope`: Space-separated list of requested scopes Example JWT header: ```python import jwt import time from cryptography.hazmat.primitives import serialization from cryptography.hazmat.backends import default_backend # Load your private key with open('private_key.pem', 'rb') as key_file: private_key = serialization.load_pem_private_key( key_file.read(), password=None, backend=default_backend() ) # Create JWT payload payload = { 'iss': 'your-app-id', 'sub': 'user-12345', 'aud': 'https://api.cloudflow.io', 'exp': int(time.time()) + 3600, 'iat': int(time.time()), 'scope': 'workflows:read workflows:write' } # Generate token token = jwt.encode(payload, private_key, algorithm='RS256') # Use in API request headers = {'Authorization': f'Bearer {token}'} ``` **Token Expiration:** All tokens expire after 3600 seconds (1 hour). Implement token refresh logic in your application.",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_1",
              "content": "**Diagnosis:** ```bash # Check system time timedatectl status # Compare with NTP server ntpdate -q pool.ntp.org # Check JWT issued time vs current time jwt_iat=$(echo $CF_ACCESS_TOKEN | cut -d'.' -f2 | base64 -d | jq '.iat') current_time=$(date +%s) skew=$((current_time - jwt_iat)) echo \"Clock skew: $skew seconds\" ``` **Resolution:** ```bash # Sync with NTP server sudo ntpdate -s pool.ntp.org # Enable automatic time sync sudo timedatectl set-ntp true # Restart CloudFlow client cloudflow restart ``` **4. Insufficient Permissions** Error: `User does not have required permissions for this operation` **Check user roles:** ```bash cloudflow auth whoami --verbose # Expected output: # User: john.doe@company.com # Roles: developer, workflow-admin # Scopes: workflow:*, data:read, metrics:read ``` **Request permission elevation:** ```bash # Submit access request cloudflow auth request-access \\ --resource \"workflow:production:deploy\" \\ --justification \"Deploy critical hotfix for TICKET-1234\" # Check pending approvals cloudflow auth list-requests --status pending ``` ### 403 Forbidden Errors #### RBAC Policy Violations CloudFlow uses role-based access control (RBAC) with the following hierarchy: - `viewer` - Read-only access - `developer` - Create and modify workflows (non-production) - `operator` - Execute workflows, view logs - `admin` - Full access to workspace - `platform-admin` - Cross-workspace administration **Verify resource permissions:** ```bash # Check effective permissions cloudflow rbac check \\ --user john.doe@company.com \\ --resource workflow:prod-pipeline \\ --action execute # List all policies affecting user cloudflow rbac policies --user john.doe@company.com --verbose ``` --- ## Performance Problems ### Slow Query Performance #### Symptoms - API response times > 5000ms - Database query latency warnings in logs - CloudFlow UI becomes unresponsive - Timeout errors: `Request exceeded maximum duration of 30000ms` #### Diagnosis Steps **1. Identify Slow Queries** ```bash # Query CloudFlow metrics cloudflow metrics query --metric api_request_duration_ms \\ --filter \"p95 > 5000\" \\ --last 1h # Check database slow query log kubectl logs -n cloudflow deploy/cloudflow-db-primary | \\ grep \"slow query\" | \\ tail -n 50 # Analyze query patterns cloudflow db analyze-queries --min-duration 5000 --limit 20 ``` **2.",
              "score": null
            }
          ],
          "latency_ms": 1186.2573780017556,
          "query_id": "realistic_003",
          "dimension": "casual",
          "query": "token expiry time"
        },
        {
          "key_facts": [
            "3600 seconds",
            "max 3600 seconds from iat",
            "All tokens expire after 3600 seconds"
          ],
          "found_facts": [
            "3600 seconds",
            "All tokens expire after 3600 seconds"
          ],
          "missed_facts": [
            "max 3600 seconds from iat"
          ],
          "coverage": 0.6666666666666666,
          "expected_docs": [
            "api_reference"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_1",
              "content": "**Technology**: Node.js with Express.js framework **Replicas**: 12 pods (production), auto-scaling 8-20 based on CPU **Resource Allocation**: 2 vCPU, 4GB RAM per pod **Key Responsibilities**: - JWT token validation (delegated to Auth Service for initial validation) - Rate limiting: 1000 requests per minute per API key (sliding window) - Request/response transformation and validation using JSON Schema - Routing to downstream services based on URL path patterns - CORS handling for web clients - Request/response logging and correlation ID injection **Critical Endpoints**: - `POST /api/v1/workflows` - Create new workflow - `GET /api/v1/workflows/:id` - Retrieve workflow status - `POST /api/v1/workflows/:id/execute` - Trigger workflow execution - `GET /api/v1/workflows/:id/history` - Get execution history **Dependencies**: - Auth Service (for token validation) - Redis (for rate limiting counters) - All downstream microservices **Performance Targets**: - P50 latency: < 50ms - P99 latency: < 200ms - Throughput: 10,000 RPS sustained --- ### Auth Service **Purpose**: Centralized authentication and authorization service handling user identity, token generation, and permission validation. **Technology**: Go with gRPC for internal communication, REST for external **Replicas**: 8 pods (production), auto-scaling 6-12 **Resource Allocation**: 1 vCPU, 2GB RAM per pod **Key Responsibilities**: - User authentication via multiple providers (OAuth2, SAML, local credentials) - JWT token generation and validation (RS256 algorithm) - Role-based access control (RBAC) with fine-grained permissions - Session management with Redis-backed storage - API key generation and validation for service accounts - MFA enforcement for administrative operations **Authentication Flow**: ``` Client Request \u2192 API Gateway \u2192 Auth Service \u2502 \u251c\u2500 Validate credentials \u251c\u2500 Check MFA if required \u251c\u2500 Generate JWT (15min expiry) \u251c\u2500 Generate refresh token (7 days) \u2514\u2500 Store session in Redis ``` **Token Structure**: - Access Token: JWT with 15-minute expiry - Refresh Token: Opaque token with 7-day expiry, stored in PostgreSQL - Claims: user_id, email, roles[], permissions[], tenant_id **Security Features**: - Password hashing: Argon2id with 64MB memory, 4 iterations - Token rotation on refresh to prevent replay attacks - Brute force protection: 5 failed attempts \u2192 15-minute lockout - Secrets stored in HashiCorp Vault, rotated every 90 days **Performance Targets**: - Token validation: < 10ms (P99) - Token generation: < 50ms (P99) - Throughput: 5,000 RPS for validation operations --- ### Workflow Engine **Purpose**: Core orchestration service that executes workflow definitions, manages state transitions, and coordinates task execution across distributed systems.",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_0",
              "content": "# CloudFlow Platform Troubleshooting Guide **Version:** 3.2.1 **Last Updated:** January 2026 **Audience:** Platform Engineers, DevOps, Support Teams ## Table of Contents 1. [Overview](#overview) 2. [Authentication & Authorization Issues](#authentication--authorization-issues) 3. [Performance Problems](#performance-problems) 4. [Database Connection Issues](#database-connection-issues) 5. [Workflow Execution Failures](#workflow-execution-failures) 6. [Rate Limiting & Throttling](#rate-limiting--throttling) 7. [Log Analysis & Debugging](#log-analysis--debugging) 8. [Escalation Procedures](#escalation-procedures) --- ## Overview This guide provides comprehensive troubleshooting steps for common CloudFlow platform issues encountered in production environments. Each section includes error symptoms, root cause analysis, resolution steps, and preventive measures. ### Quick Diagnostic Checklist Before diving into specific issues, perform these initial checks: - Verify service health: `cloudflow status --all` - Check API connectivity: `curl -I https://api.cloudflow.io/health` - Review recent deployments: `kubectl get deployments -n cloudflow --sort-by=.metadata.creationTimestamp` - Inspect platform metrics: `cloudflow metrics --last 1h` --- ## Authentication & Authorization Issues ### 401 Unauthorized Errors #### Symptoms - API requests return `401 Unauthorized` - Error message: `Authentication credentials were not provided or are invalid` - Frontend displays \"Session expired\" message #### Common Causes **1. Token Expiration** CloudFlow access tokens expire after 3600 seconds (1 hour) by default. Refresh tokens are valid for 30 days. **Verification:** ```bash # Decode JWT to check expiration echo $CF_ACCESS_TOKEN | cut -d'.' -f2 | base64 -d | jq '.exp' # Compare with current time date +%s ``` **Resolution:** ```bash # Refresh the access token curl -X POST https://api.cloudflow.io/auth/refresh \\ -H \"Content-Type: application/json\" \\ -d '{\"refresh_token\": \"'$CF_REFRESH_TOKEN'\"}' # Update environment variable export CF_ACCESS_TOKEN=\"<new_token>\" ``` **2. Invalid JWT Signature** Error: `JWT signature verification failed` **Causes:** - Token was modified or corrupted - Using wrong signing key - Token generated with different secret **Resolution:** ```bash # Validate token structure cloudflow auth validate --token $CF_ACCESS_TOKEN # Generate new token cloudflow auth login --username <user> --password <pass> # For service accounts cloudflow auth service-account --create --name <sa-name> --scopes \"workflow:read,workflow:write\" ``` **3. Clock Skew Issues** JWT validation fails when system clocks are out of sync (tolerance: \u00b1300 seconds).",
              "score": null
            },
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_0",
              "content": "# CloudFlow API Reference Version 2.1.0 | Last Updated: January 2026 ## Overview The CloudFlow API is a RESTful service that enables developers to programmatically manage cloud workflows, data pipelines, and automation tasks. This documentation provides comprehensive details on authentication, endpoints, request/response formats, error handling, and best practices. **Base URL:** `https://api.cloudflow.io/v2` **API Status:** https://status.cloudflow.io ## Authentication CloudFlow supports three authentication methods to suit different use cases and security requirements. ### API Keys API keys provide simple authentication for server-to-server communication. Include your API key in the request header: ```bash curl -H \"X-API-Key: cf_live_a1b2c3d4e5f6g7h8i9j0\" \\ https://api.cloudflow.io/v2/workflows ``` **Security Notes:** - Never expose API keys in client-side code - Rotate keys every 90 days - Use separate keys for development and production environments ### OAuth 2.0 OAuth 2.0 is recommended for applications that access CloudFlow on behalf of users. We support the Authorization Code flow with PKCE. **Authorization Endpoint:** `https://auth.cloudflow.io/oauth/authorize` **Token Endpoint:** `https://auth.cloudflow.io/oauth/token` **Supported Scopes:** - `workflows:read` - Read workflow configurations - `workflows:write` - Create and modify workflows - `pipelines:read` - Read pipeline data - `pipelines:write` - Create and manage pipelines - `analytics:read` - Access analytics and metrics - `admin:full` - Full administrative access Example authorization request: ```bash curl -X POST https://auth.cloudflow.io/oauth/token \\ -H \"Content-Type: application/x-www-form-urlencoded\" \\ -d \"grant_type=authorization_code\" \\ -d \"code=AUTH_CODE_HERE\" \\ -d \"client_id=YOUR_CLIENT_ID\" \\ -d \"client_secret=YOUR_CLIENT_SECRET\" \\ -d \"redirect_uri=https://yourapp.com/callback\" ``` ### JWT Tokens For advanced use cases, CloudFlow supports JSON Web Tokens (JWT) with RS256 signing algorithm. JWTs must include the following claims: - `iss` (issuer): Your application identifier - `sub` (subject): User or service account ID - `aud` (audience): `https://api.cloudflow.io` - `exp` (expiration): Unix timestamp (max 3600 seconds from `iat`) - `iat` (issued at): Unix timestamp - `scope`: Space-separated list of requested scopes Example JWT header: ```python import jwt import time from cryptography.hazmat.primitives import serialization from cryptography.hazmat.backends import default_backend # Load your private key with open('private_key.pem', 'rb') as key_file: private_key = serialization.load_pem_private_key( key_file.read(), password=None, backend=default_backend() ) # Create JWT payload payload = { 'iss': 'your-app-id', 'sub': 'user-12345', 'aud': 'https://api.cloudflow.io', 'exp': int(time.time()) + 3600, 'iat': int(time.time()), 'scope': 'workflows:read workflows:write' } # Generate token token = jwt.encode(payload, private_key, algorithm='RS256') # Use in API request headers = {'Authorization': f'Bearer {token}'} ``` **Token Expiration:** All tokens expire after 3600 seconds (1 hour). Implement token refresh logic in your application.",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_6",
              "content": "**Session Storage**: - Key pattern: `session:{user_id}` - TTL: 15 minutes (aligned with JWT expiry) - Data: User session state, preferences - Invalidation: On logout or password change 2. **Rate Limiting Counters**: - Key pattern: `ratelimit:{api_key}:{window}` - TTL: 60 seconds (sliding window) - Data: Request count per window - Algorithm: Token bucket with Redis INCR 3. **Compiled Templates**: - Key pattern: `template:{template_id}:{version}` - TTL: 1 hour - Data: Compiled Handlebars template - Invalidation: On template update 4. **Workflow Definitions**: - Key pattern: `workflow:def:{workflow_id}` - TTL: 1 hour - Data: Parsed workflow JSON - Invalidation: On workflow update or manual flush 5. **User Profiles**: - Key pattern: `user:profile:{user_id}` - TTL: 30 minutes - Data: User metadata (name, email, roles) - Invalidation: On profile update **Cache Hit Rates** (Production Metrics): - Session lookups: 98.5% - Workflow definitions: 94.2% - Template cache: 99.1% - User profiles: 91.8% **Performance Characteristics**: - GET operation P99: < 2ms - SET operation P99: < 3ms - Throughput: 100,000 operations per second - Network latency: < 1ms (same AZ) --- ### Kafka Event Streaming **Cluster Configuration**: - 5 broker nodes (distributed across 3 AZs) - Instance type: kafka.m5.2xlarge (8 vCPU, 32GB RAM) - Storage: 10TB per broker (gp3 SSD) - ZooKeeper ensemble: 3 nodes for cluster coordination - Replication factor: 3 (min in-sync replicas: 2) **Topic Architecture**: ``` workflow.events (32 partitions): - Workflow lifecycle events (created, started, completed, failed) - Retention: 7 days - Message rate: 5,000/sec peak - Consumer groups: workflow-engine, analytics-pipeline notifications.email (16 partitions): - Email notification triggers - Retention: 3 days - Message rate: 2,000/sec peak - Consumer groups: notification-service notifications.sms (8 partitions): - SMS notification triggers - Retention: 3 days - Message rate: 500/sec peak - Consumer groups: notification-service audit.logs (24 partitions): - Security and compliance audit events - Retention: 90 days (compliance requirement) - Message rate: 3,000/sec peak - Consumer groups: audit-processor, security-monitor dead-letter-queue (8 partitions): - Failed message processing - Retention: 30 days - Manual intervention required - Consumer groups: ops-team-alerts ``` **Producer Configuration**: - Acknowledgment: `acks=all` (wait for all in-sync replicas) - Compression: LZ4 (reduces network bandwidth by 60%) - Batching: 100ms linger time, 100KB batch size - Idempotence: Enabled to prevent duplicates **Consumer Configuration**: - Auto-commit: Disabled (manual commit after processing) - Offset reset: Earliest (replay from beginning on new consumer group) - Max poll",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_8",
              "content": "### Cache Invalidation Strategies **Time-Based Expiration (TTL)**: - Short-lived data: 5-15 minutes (session tokens, rate limit counters) - Medium-lived data: 1 hour (workflow definitions, templates) - Long-lived data: 24 hours (static configuration) **Event-Based Invalidation**: ``` Database Update Event \u2192 Kafka (cache.invalidation topic) \u2502 \u25bc All service instances consume event \u2502 \u25bc Redis DEL for affected keys ``` **Pattern-Based Invalidation**: - Use Redis SCAN + DEL for wildcard patterns - Example: Invalidate all user caches: `user:*:{user_id}` **Cache Warming**: - Scheduled job runs every hour to pre-populate frequently accessed data - Targets: Top 1000 workflows, system configuration, active user profiles - Reduces cache miss rate during traffic spikes ### Multi-Level Caching Application implements L1 (in-memory) and L2 (Redis) caching: ``` Request \u2192 L1 Cache (Node.js Map, 1000 entry LRU) \u2502 MISS \u2502 \u25bc L2 Cache (Redis, distributed) \u2502 MISS \u2502 \u25bc Database (PostgreSQL) ``` L1 cache reduces Redis network calls by 40% for hot data. --- ## Security Architecture ### Network Security **Zero-Trust Network Model**: - All service-to-service communication encrypted with mTLS - Certificate rotation: Every 90 days (automated via cert-manager) - Certificate authority: Internal PKI with HashiCorp Vault - Network segmentation: Private subnets for services, public subnet for ALB only **Service Mesh (Istio)**: ``` Service A \u2192 Envoy Sidecar (mTLS client cert) \u2502 Encrypted channel \u2502 Envoy Sidecar (mTLS server cert) \u2192 Service B ``` **Firewall Rules**: - Security groups: Deny all by default, explicit allow rules - Ingress: Only ALB can reach API Gateway (port 8080) - Egress: Services can only reach specific dependencies - Database access: Limited to application subnets only ### Authentication & Authorization **JWT Token Validation**: - Algorithm: RS256 (asymmetric signing) - Key rotation: Every 30 days with 7-day overlap period - Public key distribution: JWKS endpoint cached in Redis - Validation: Signature, expiry, issuer, audience claims - Token revocation: Blacklist in Redis for compromised tokens **Permission Model**: ``` User \u2192 Roles \u2192 Permissions \u2198 \u2197 Tenants (Multi-tenancy isolation) ``` Example permissions: - `workflow:read` - View workflows - `workflow:write` - Create/update workflows - `workflow:execute` - Trigger workflow execution - `workflow:delete` - Delete workflows - `admin:*` - All administrative operations **API Key Management**: - Format: `cfk_live_<32-char-random>` (production), `cfk_test_<32-char-random>` (sandbox) - Hashing: SHA-256 before storage in PostgreSQL - Scoping: API keys can be scoped to specific workflows or operations - Rate limits: Configurable",
              "score": null
            }
          ],
          "latency_ms": 976.7343589992379,
          "query_id": "realistic_003",
          "dimension": "contextual",
          "query": "I need to implement token refresh logic, when do JWT tokens expire?"
        },
        {
          "key_facts": [
            "3600 seconds",
            "max 3600 seconds from iat",
            "All tokens expire after 3600 seconds"
          ],
          "found_facts": [
            "3600 seconds",
            "All tokens expire after 3600 seconds"
          ],
          "missed_facts": [
            "max 3600 seconds from iat"
          ],
          "coverage": 0.6666666666666666,
          "expected_docs": [
            "api_reference"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_1",
              "content": "**Technology**: Node.js with Express.js framework **Replicas**: 12 pods (production), auto-scaling 8-20 based on CPU **Resource Allocation**: 2 vCPU, 4GB RAM per pod **Key Responsibilities**: - JWT token validation (delegated to Auth Service for initial validation) - Rate limiting: 1000 requests per minute per API key (sliding window) - Request/response transformation and validation using JSON Schema - Routing to downstream services based on URL path patterns - CORS handling for web clients - Request/response logging and correlation ID injection **Critical Endpoints**: - `POST /api/v1/workflows` - Create new workflow - `GET /api/v1/workflows/:id` - Retrieve workflow status - `POST /api/v1/workflows/:id/execute` - Trigger workflow execution - `GET /api/v1/workflows/:id/history` - Get execution history **Dependencies**: - Auth Service (for token validation) - Redis (for rate limiting counters) - All downstream microservices **Performance Targets**: - P50 latency: < 50ms - P99 latency: < 200ms - Throughput: 10,000 RPS sustained --- ### Auth Service **Purpose**: Centralized authentication and authorization service handling user identity, token generation, and permission validation. **Technology**: Go with gRPC for internal communication, REST for external **Replicas**: 8 pods (production), auto-scaling 6-12 **Resource Allocation**: 1 vCPU, 2GB RAM per pod **Key Responsibilities**: - User authentication via multiple providers (OAuth2, SAML, local credentials) - JWT token generation and validation (RS256 algorithm) - Role-based access control (RBAC) with fine-grained permissions - Session management with Redis-backed storage - API key generation and validation for service accounts - MFA enforcement for administrative operations **Authentication Flow**: ``` Client Request \u2192 API Gateway \u2192 Auth Service \u2502 \u251c\u2500 Validate credentials \u251c\u2500 Check MFA if required \u251c\u2500 Generate JWT (15min expiry) \u251c\u2500 Generate refresh token (7 days) \u2514\u2500 Store session in Redis ``` **Token Structure**: - Access Token: JWT with 15-minute expiry - Refresh Token: Opaque token with 7-day expiry, stored in PostgreSQL - Claims: user_id, email, roles[], permissions[], tenant_id **Security Features**: - Password hashing: Argon2id with 64MB memory, 4 iterations - Token rotation on refresh to prevent replay attacks - Brute force protection: 5 failed attempts \u2192 15-minute lockout - Secrets stored in HashiCorp Vault, rotated every 90 days **Performance Targets**: - Token validation: < 10ms (P99) - Token generation: < 50ms (P99) - Throughput: 5,000 RPS for validation operations --- ### Workflow Engine **Purpose**: Core orchestration service that executes workflow definitions, manages state transitions, and coordinates task execution across distributed systems.",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_0",
              "content": "# CloudFlow Platform Troubleshooting Guide **Version:** 3.2.1 **Last Updated:** January 2026 **Audience:** Platform Engineers, DevOps, Support Teams ## Table of Contents 1. [Overview](#overview) 2. [Authentication & Authorization Issues](#authentication--authorization-issues) 3. [Performance Problems](#performance-problems) 4. [Database Connection Issues](#database-connection-issues) 5. [Workflow Execution Failures](#workflow-execution-failures) 6. [Rate Limiting & Throttling](#rate-limiting--throttling) 7. [Log Analysis & Debugging](#log-analysis--debugging) 8. [Escalation Procedures](#escalation-procedures) --- ## Overview This guide provides comprehensive troubleshooting steps for common CloudFlow platform issues encountered in production environments. Each section includes error symptoms, root cause analysis, resolution steps, and preventive measures. ### Quick Diagnostic Checklist Before diving into specific issues, perform these initial checks: - Verify service health: `cloudflow status --all` - Check API connectivity: `curl -I https://api.cloudflow.io/health` - Review recent deployments: `kubectl get deployments -n cloudflow --sort-by=.metadata.creationTimestamp` - Inspect platform metrics: `cloudflow metrics --last 1h` --- ## Authentication & Authorization Issues ### 401 Unauthorized Errors #### Symptoms - API requests return `401 Unauthorized` - Error message: `Authentication credentials were not provided or are invalid` - Frontend displays \"Session expired\" message #### Common Causes **1. Token Expiration** CloudFlow access tokens expire after 3600 seconds (1 hour) by default. Refresh tokens are valid for 30 days. **Verification:** ```bash # Decode JWT to check expiration echo $CF_ACCESS_TOKEN | cut -d'.' -f2 | base64 -d | jq '.exp' # Compare with current time date +%s ``` **Resolution:** ```bash # Refresh the access token curl -X POST https://api.cloudflow.io/auth/refresh \\ -H \"Content-Type: application/json\" \\ -d '{\"refresh_token\": \"'$CF_REFRESH_TOKEN'\"}' # Update environment variable export CF_ACCESS_TOKEN=\"<new_token>\" ``` **2. Invalid JWT Signature** Error: `JWT signature verification failed` **Causes:** - Token was modified or corrupted - Using wrong signing key - Token generated with different secret **Resolution:** ```bash # Validate token structure cloudflow auth validate --token $CF_ACCESS_TOKEN # Generate new token cloudflow auth login --username <user> --password <pass> # For service accounts cloudflow auth service-account --create --name <sa-name> --scopes \"workflow:read,workflow:write\" ``` **3. Clock Skew Issues** JWT validation fails when system clocks are out of sync (tolerance: \u00b1300 seconds).",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_1",
              "content": "**Diagnosis:** ```bash # Check system time timedatectl status # Compare with NTP server ntpdate -q pool.ntp.org # Check JWT issued time vs current time jwt_iat=$(echo $CF_ACCESS_TOKEN | cut -d'.' -f2 | base64 -d | jq '.iat') current_time=$(date +%s) skew=$((current_time - jwt_iat)) echo \"Clock skew: $skew seconds\" ``` **Resolution:** ```bash # Sync with NTP server sudo ntpdate -s pool.ntp.org # Enable automatic time sync sudo timedatectl set-ntp true # Restart CloudFlow client cloudflow restart ``` **4. Insufficient Permissions** Error: `User does not have required permissions for this operation` **Check user roles:** ```bash cloudflow auth whoami --verbose # Expected output: # User: john.doe@company.com # Roles: developer, workflow-admin # Scopes: workflow:*, data:read, metrics:read ``` **Request permission elevation:** ```bash # Submit access request cloudflow auth request-access \\ --resource \"workflow:production:deploy\" \\ --justification \"Deploy critical hotfix for TICKET-1234\" # Check pending approvals cloudflow auth list-requests --status pending ``` ### 403 Forbidden Errors #### RBAC Policy Violations CloudFlow uses role-based access control (RBAC) with the following hierarchy: - `viewer` - Read-only access - `developer` - Create and modify workflows (non-production) - `operator` - Execute workflows, view logs - `admin` - Full access to workspace - `platform-admin` - Cross-workspace administration **Verify resource permissions:** ```bash # Check effective permissions cloudflow rbac check \\ --user john.doe@company.com \\ --resource workflow:prod-pipeline \\ --action execute # List all policies affecting user cloudflow rbac policies --user john.doe@company.com --verbose ``` --- ## Performance Problems ### Slow Query Performance #### Symptoms - API response times > 5000ms - Database query latency warnings in logs - CloudFlow UI becomes unresponsive - Timeout errors: `Request exceeded maximum duration of 30000ms` #### Diagnosis Steps **1. Identify Slow Queries** ```bash # Query CloudFlow metrics cloudflow metrics query --metric api_request_duration_ms \\ --filter \"p95 > 5000\" \\ --last 1h # Check database slow query log kubectl logs -n cloudflow deploy/cloudflow-db-primary | \\ grep \"slow query\" | \\ tail -n 50 # Analyze query patterns cloudflow db analyze-queries --min-duration 5000 --limit 20 ``` **2.",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_8",
              "content": "### Cache Invalidation Strategies **Time-Based Expiration (TTL)**: - Short-lived data: 5-15 minutes (session tokens, rate limit counters) - Medium-lived data: 1 hour (workflow definitions, templates) - Long-lived data: 24 hours (static configuration) **Event-Based Invalidation**: ``` Database Update Event \u2192 Kafka (cache.invalidation topic) \u2502 \u25bc All service instances consume event \u2502 \u25bc Redis DEL for affected keys ``` **Pattern-Based Invalidation**: - Use Redis SCAN + DEL for wildcard patterns - Example: Invalidate all user caches: `user:*:{user_id}` **Cache Warming**: - Scheduled job runs every hour to pre-populate frequently accessed data - Targets: Top 1000 workflows, system configuration, active user profiles - Reduces cache miss rate during traffic spikes ### Multi-Level Caching Application implements L1 (in-memory) and L2 (Redis) caching: ``` Request \u2192 L1 Cache (Node.js Map, 1000 entry LRU) \u2502 MISS \u2502 \u25bc L2 Cache (Redis, distributed) \u2502 MISS \u2502 \u25bc Database (PostgreSQL) ``` L1 cache reduces Redis network calls by 40% for hot data. --- ## Security Architecture ### Network Security **Zero-Trust Network Model**: - All service-to-service communication encrypted with mTLS - Certificate rotation: Every 90 days (automated via cert-manager) - Certificate authority: Internal PKI with HashiCorp Vault - Network segmentation: Private subnets for services, public subnet for ALB only **Service Mesh (Istio)**: ``` Service A \u2192 Envoy Sidecar (mTLS client cert) \u2502 Encrypted channel \u2502 Envoy Sidecar (mTLS server cert) \u2192 Service B ``` **Firewall Rules**: - Security groups: Deny all by default, explicit allow rules - Ingress: Only ALB can reach API Gateway (port 8080) - Egress: Services can only reach specific dependencies - Database access: Limited to application subnets only ### Authentication & Authorization **JWT Token Validation**: - Algorithm: RS256 (asymmetric signing) - Key rotation: Every 30 days with 7-day overlap period - Public key distribution: JWKS endpoint cached in Redis - Validation: Signature, expiry, issuer, audience claims - Token revocation: Blacklist in Redis for compromised tokens **Permission Model**: ``` User \u2192 Roles \u2192 Permissions \u2198 \u2197 Tenants (Multi-tenancy isolation) ``` Example permissions: - `workflow:read` - View workflows - `workflow:write` - Create/update workflows - `workflow:execute` - Trigger workflow execution - `workflow:delete` - Delete workflows - `admin:*` - All administrative operations **API Key Management**: - Format: `cfk_live_<32-char-random>` (production), `cfk_test_<32-char-random>` (sandbox) - Hashing: SHA-256 before storage in PostgreSQL - Scoping: API keys can be scoped to specific workflows or operations - Rate limits: Configurable",
              "score": null
            },
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_0",
              "content": "# CloudFlow API Reference Version 2.1.0 | Last Updated: January 2026 ## Overview The CloudFlow API is a RESTful service that enables developers to programmatically manage cloud workflows, data pipelines, and automation tasks. This documentation provides comprehensive details on authentication, endpoints, request/response formats, error handling, and best practices. **Base URL:** `https://api.cloudflow.io/v2` **API Status:** https://status.cloudflow.io ## Authentication CloudFlow supports three authentication methods to suit different use cases and security requirements. ### API Keys API keys provide simple authentication for server-to-server communication. Include your API key in the request header: ```bash curl -H \"X-API-Key: cf_live_a1b2c3d4e5f6g7h8i9j0\" \\ https://api.cloudflow.io/v2/workflows ``` **Security Notes:** - Never expose API keys in client-side code - Rotate keys every 90 days - Use separate keys for development and production environments ### OAuth 2.0 OAuth 2.0 is recommended for applications that access CloudFlow on behalf of users. We support the Authorization Code flow with PKCE. **Authorization Endpoint:** `https://auth.cloudflow.io/oauth/authorize` **Token Endpoint:** `https://auth.cloudflow.io/oauth/token` **Supported Scopes:** - `workflows:read` - Read workflow configurations - `workflows:write` - Create and modify workflows - `pipelines:read` - Read pipeline data - `pipelines:write` - Create and manage pipelines - `analytics:read` - Access analytics and metrics - `admin:full` - Full administrative access Example authorization request: ```bash curl -X POST https://auth.cloudflow.io/oauth/token \\ -H \"Content-Type: application/x-www-form-urlencoded\" \\ -d \"grant_type=authorization_code\" \\ -d \"code=AUTH_CODE_HERE\" \\ -d \"client_id=YOUR_CLIENT_ID\" \\ -d \"client_secret=YOUR_CLIENT_SECRET\" \\ -d \"redirect_uri=https://yourapp.com/callback\" ``` ### JWT Tokens For advanced use cases, CloudFlow supports JSON Web Tokens (JWT) with RS256 signing algorithm. JWTs must include the following claims: - `iss` (issuer): Your application identifier - `sub` (subject): User or service account ID - `aud` (audience): `https://api.cloudflow.io` - `exp` (expiration): Unix timestamp (max 3600 seconds from `iat`) - `iat` (issued at): Unix timestamp - `scope`: Space-separated list of requested scopes Example JWT header: ```python import jwt import time from cryptography.hazmat.primitives import serialization from cryptography.hazmat.backends import default_backend # Load your private key with open('private_key.pem', 'rb') as key_file: private_key = serialization.load_pem_private_key( key_file.read(), password=None, backend=default_backend() ) # Create JWT payload payload = { 'iss': 'your-app-id', 'sub': 'user-12345', 'aud': 'https://api.cloudflow.io', 'exp': int(time.time()) + 3600, 'iat': int(time.time()), 'scope': 'workflows:read workflows:write' } # Generate token token = jwt.encode(payload, private_key, algorithm='RS256') # Use in API request headers = {'Authorization': f'Bearer {token}'} ``` **Token Expiration:** All tokens expire after 3600 seconds (1 hour). Implement token refresh logic in your application.",
              "score": null
            }
          ],
          "latency_ms": 912.8393120008695,
          "query_id": "realistic_003",
          "dimension": "negation",
          "query": "Why does my token stop working after 3600 seconds?"
        },
        {
          "key_facts": [
            "PostgreSQL 15.4",
            "Redis 7.2",
            "Apache Kafka 3.6"
          ],
          "found_facts": [
            "PostgreSQL 15.4",
            "Redis 7.2",
            "Apache Kafka 3.6"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "architecture_overview"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_0",
              "content": "# CloudFlow Platform - System Architecture Overview **Document Version:** 2.3.1 **Last Updated:** January 15, 2026 **Owner:** Platform Architecture Team **Status:** Production ## Executive Summary CloudFlow is a distributed, cloud-native workflow automation platform designed to orchestrate complex business processes at scale. The platform processes over 2.5 million workflow executions daily with an average P99 latency of 180ms for API operations and 4.2 seconds for workflow execution. This document provides a comprehensive overview of the system architecture, including microservices design, data flow patterns, infrastructure decisions, and operational considerations. ## Table of Contents 1. [High-Level Architecture](#high-level-architecture) 2. [Microservices Breakdown](#microservices-breakdown) 3. [Data Flow Architecture](#data-flow-architecture) 4. [Database Architecture](#database-architecture) 5. [Message Queue Patterns](#message-queue-patterns) 6. [Caching Strategy](#caching-strategy) 7. [Security Architecture](#security-architecture) 8. [Performance Characteristics](#performance-characteristics) 9. [Disaster Recovery](#disaster-recovery) --- ## High-Level Architecture CloudFlow follows a microservices architecture pattern deployed across multiple availability zones in AWS. The platform is designed with the following core principles: - **Scalability**: Horizontal scaling for all services with auto-scaling groups - **Resilience**: Circuit breakers, bulkheads, and graceful degradation - **Observability**: Distributed tracing, centralized logging, and comprehensive metrics - **Security**: Zero-trust network architecture with mTLS encryption ### Architecture Diagram (Conceptual) ``` \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 Load Balancer (ALB) \u2502 \u2502 (TLS Termination - 443) \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u25bc \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 API Gateway Layer \u2502 \u2502 (Rate Limiting, Auth, Request Routing) \u2502 \u2514\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u2502 \u2502 \u2502 \u2502 \u25bc \u25bc \u25bc \u25bc \u25bc \u250c\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502Auth\u2502 \u2502Workflow\u2502 \u2502Scheduler\u2502 \u2502Notification\u2502 \u2502User Service\u2502 \u2502Svc \u2502 \u2502 Engine \u2502 \u2502 Service \u2502 \u2502 Service \u2502 \u2502 \u2502 \u2514\u2500\u252c\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u2502 \u2502 \u2502 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 \u2502 \u2502 \u25bc \u25bc \u25bc \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502PostgreSQL\u2502 \u2502 Redis \u2502 \u2502 Kafka \u2502 \u2502 Cluster \u2502 \u2502 Cluster \u2502 \u2502 Cluster \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 ``` ### Technology Stack - **Runtime**: Node.js 20.x (API Gateway, Workflow Engine), Go 1.21 (Auth Service, Scheduler) - **Container Orchestration**: Kubernetes 1.28 (EKS) - **Service Mesh**: Istio 1.20 for service-to-service communication - **Databases**: PostgreSQL 15.4, Redis 7.2 - **Message Broker**: Apache Kafka 3.6 - **Monitoring**: Prometheus, Grafana, Jaeger for distributed tracing - **Secrets Management**: HashiCorp Vault 1.15 --- ## Microservices Breakdown ### API Gateway **Purpose**: Single entry point for all client requests, providing authentication, rate limiting, request routing, and protocol translation.",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_4",
              "content": "**Technology**: Node.js with worker pool pattern **Replicas**: 8 pods (production), auto-scaling 6-16 **Resource Allocation**: 2 vCPU, 4GB RAM per pod **Key Responsibilities**: - Consume notification events from Kafka - Template rendering with Handlebars (cached in Redis) - Multi-channel delivery (Email via SendGrid, SMS via Twilio, Webhooks) - Retry logic with dead letter queue for failed deliveries - Delivery status tracking and analytics - User preference management (opt-in/opt-out) **Notification Flow**: ``` Event Source \u2192 Kafka (notifications.email topic) \u2502 \u25bc Notification Service Consumer \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u25bc \u25bc \u25bc [Email Queue] [SMS Queue] [Webhook Queue] \u2502 \u2502 \u2502 \u25bc \u25bc \u25bc [SendGrid] [Twilio] [HTTP POST] \u2502 \u2502 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u25bc Delivery Status \u2192 PostgreSQL ``` **Channel Configuration**: - Email: SendGrid with IP warming, DKIM/SPF configured - SMS: Twilio with fallback to AWS SNS - Webhook: Signed payloads (HMAC-SHA256), retry 3 times with exponential backoff - In-app: WebSocket connections via Socket.io **Template Management**: - Templates stored in PostgreSQL with versioning - Compiled templates cached in Redis (1-hour TTL) - Support for localization (i18n) with 12 languages - A/B testing capability for email campaigns **Performance Targets**: - Email delivery: < 5 seconds (P99) from event to SendGrid - SMS delivery: < 3 seconds (P99) - Throughput: 10,000 notifications per minute --- ## Data Flow Architecture ### Synchronous Request Flow Client requests follow a synchronous path through the API Gateway to backend services: ``` 1. Client \u2192 Load Balancer (TLS termination) 2. Load Balancer \u2192 API Gateway (HTTP/2) 3. API Gateway \u2192 Auth Service (JWT validation via gRPC) 4. API Gateway \u2192 Target Service (HTTP/REST or gRPC) 5. Target Service \u2192 Database/Cache (data retrieval) 6. Response propagates back through the chain ``` **Timeout Configuration**: - Client \u2192 Load Balancer: 60 seconds - Load Balancer \u2192 API Gateway: 55 seconds - API Gateway \u2192 Services: 45 seconds - Service \u2192 Database: 10 seconds ### Asynchronous Event Flow Long-running operations and inter-service communication use event-driven patterns: ``` 1. Service publishes event \u2192 Kafka topic 2. Kafka persists event (replication factor: 3) 3. Consumer groups subscribe to topics 4. Consumers process events with at-least-once delivery 5. State updates written to PostgreSQL 6.",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_12",
              "content": "Resume normal operations - Recovery time: 1-3 hours depending on data volume - Data loss: None if corruption detected quickly ### Testing & Validation **DR Drill Schedule**: - Monthly: Automated failover test (single AZ failure simulation) - Quarterly: Full DR region failover (non-production hours) - Annually: Complete disaster simulation with all stakeholders **Last DR Test Results** (Dec 15, 2025): - Scenario: Full region failover - Actual RTO: 2 hours 23 minutes (target: 4 hours) - Actual RPO: 42 minutes (target: 1 hour) - Issues identified: DNS propagation slower than expected (resolved) - Success criteria: Met all recovery objectives ### Business Continuity **Communication Plan**: - Status page: status.cloudflow.com (updated every 15 minutes during incident) - Customer notifications: Email + SMS for all P1 incidents - Internal escalation: PagerDuty \u2192 Incident Commander \u2192 Engineering Manager \u2192 CTO **Data Retention Policy**: - Active data: 12 months in hot storage (PostgreSQL) - Archived data: 7 years in cold storage (S3 Glacier) - Audit logs: 7 years (compliance requirement) - Backup retention: 30 days standard, 90 days monthly snapshots --- ## Appendix ### Service Dependency Matrix | Service | Depends On | Critical Path | |---------|-----------|---------------| | API Gateway | Auth Service, Redis | Yes | | Auth Service | PostgreSQL, Redis, Vault | Yes | | Workflow Engine | PostgreSQL, Kafka, Redis | Yes | | Scheduler | PostgreSQL, Redis, Kafka | No | | Notification Service | Kafka, PostgreSQL, SendGrid, Twilio | No | ### Capacity Planning **Current Capacity** (Jan 2026): - Daily workflow executions: 2.5M - Active users: 150,000 - API requests per day: 50M - Database size: 1.2TB - Kafka throughput: 100K msg/sec peak **6-Month Projection** (Jul 2026): - Daily workflow executions: 4M (+60%) - Active users: 225,000 (+50%) - API requests per day: 80M (+60%) - Database size: 2TB (+67%) - Required scaling: +30% compute, +50% storage ### Contact Information - **Architecture Team**: architecture@cloudflow.internal - **On-Call Engineer**: PagerDuty escalation - **Security Team**: security@cloudflow.internal - **Documentation**: https://wiki.cloudflow.internal/architecture --- **Document Revision History**: - v2.3.1 (Jan 15, 2026): Updated performance metrics, added DR test results - v2.3.0 (Dec 1, 2025): Added multi-AZ deployment details - v2.2.0 (Oct 15, 2025): Security architecture overhaul - v2.1.0 (Sep 1, 2025): Initial Kafka migration documentation - v2.0.0 (Jul 1, 2025): Complete microservices rewrite",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_5",
              "content": "Success/failure events published back to Kafka ``` **Event Schema**: ```json { \"event_id\": \"uuid-v4\", \"event_type\": \"workflow.execution.completed\", \"timestamp\": \"2026-01-15T10:30:00.000Z\", \"correlation_id\": \"request-trace-id\", \"payload\": { \"workflow_id\": \"wf-12345\", \"execution_id\": \"exec-67890\", \"status\": \"COMPLETED\", \"duration_ms\": 4230 }, \"metadata\": { \"source_service\": \"workflow-engine\", \"schema_version\": \"1.0\" } } ``` ### Inter-Service Communication Services communicate using two primary patterns: **Synchronous (gRPC)**: - Auth Service validation calls from API Gateway - User Service profile lookups - Low latency requirement (< 50ms) - Request-response pattern with circuit breaker **Asynchronous (Kafka)**: - Workflow execution events - Notification triggers - Audit log events - Eventual consistency acceptable --- ## Database Architecture ### PostgreSQL Primary Database **Cluster Configuration**: - Primary-replica setup with 1 primary + 2 read replicas - Instance type: db.r6g.2xlarge (8 vCPU, 64GB RAM) - Storage: 2TB gp3 SSD with 12,000 IOPS - Multi-AZ deployment for high availability - Automated backups: Daily snapshots, 30-day retention - Point-in-time recovery: 5-minute granularity **Database Schema Design**: ``` Core Tables: - users (5M rows): User accounts and profiles - workflows (2M rows): Workflow definitions - workflow_executions (500M rows, partitioned): Execution history - workflow_steps (2B rows, partitioned): Individual step records - schedules (100K rows): Scheduled workflow triggers - notifications (1B rows, partitioned): Notification delivery log Partitioning Strategy: - workflow_executions: Monthly partitions by created_at - workflow_steps: Monthly partitions by created_at - notifications: Weekly partitions by created_at - Automatic partition creation via cron job - Retention policy: Drop partitions older than 12 months ``` **Indexing Strategy**: - Primary keys: UUIDs with B-tree indexes - Frequently queried columns: Compound indexes (e.g., user_id + status + created_at) - JSONB columns: GIN indexes for workflow definitions - Full-text search: GiST indexes on description fields **Connection Pooling**: - PgBouncer in transaction mode - Pool size: 100 connections per service - Max client connections: 2000 - Connection timeout: 30 seconds **Performance Characteristics**: - Read query P99: < 20ms - Write query P99: < 50ms - Transaction throughput: 15,000 TPS - Replication lag: < 500ms --- ### Redis Caching Layer **Cluster Configuration**: - Redis Cluster with 6 nodes (3 primary + 3 replica) - Instance type: cache.r6g.xlarge (4 vCPU, 26GB RAM) - Total memory: 78GB usable cache space - Persistence: RDB snapshots every 5 minutes + AOF - Eviction policy: allkeys-lru **Cache Usage Patterns**: 1.",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_8",
              "content": "### Cache Invalidation Strategies **Time-Based Expiration (TTL)**: - Short-lived data: 5-15 minutes (session tokens, rate limit counters) - Medium-lived data: 1 hour (workflow definitions, templates) - Long-lived data: 24 hours (static configuration) **Event-Based Invalidation**: ``` Database Update Event \u2192 Kafka (cache.invalidation topic) \u2502 \u25bc All service instances consume event \u2502 \u25bc Redis DEL for affected keys ``` **Pattern-Based Invalidation**: - Use Redis SCAN + DEL for wildcard patterns - Example: Invalidate all user caches: `user:*:{user_id}` **Cache Warming**: - Scheduled job runs every hour to pre-populate frequently accessed data - Targets: Top 1000 workflows, system configuration, active user profiles - Reduces cache miss rate during traffic spikes ### Multi-Level Caching Application implements L1 (in-memory) and L2 (Redis) caching: ``` Request \u2192 L1 Cache (Node.js Map, 1000 entry LRU) \u2502 MISS \u2502 \u25bc L2 Cache (Redis, distributed) \u2502 MISS \u2502 \u25bc Database (PostgreSQL) ``` L1 cache reduces Redis network calls by 40% for hot data. --- ## Security Architecture ### Network Security **Zero-Trust Network Model**: - All service-to-service communication encrypted with mTLS - Certificate rotation: Every 90 days (automated via cert-manager) - Certificate authority: Internal PKI with HashiCorp Vault - Network segmentation: Private subnets for services, public subnet for ALB only **Service Mesh (Istio)**: ``` Service A \u2192 Envoy Sidecar (mTLS client cert) \u2502 Encrypted channel \u2502 Envoy Sidecar (mTLS server cert) \u2192 Service B ``` **Firewall Rules**: - Security groups: Deny all by default, explicit allow rules - Ingress: Only ALB can reach API Gateway (port 8080) - Egress: Services can only reach specific dependencies - Database access: Limited to application subnets only ### Authentication & Authorization **JWT Token Validation**: - Algorithm: RS256 (asymmetric signing) - Key rotation: Every 30 days with 7-day overlap period - Public key distribution: JWKS endpoint cached in Redis - Validation: Signature, expiry, issuer, audience claims - Token revocation: Blacklist in Redis for compromised tokens **Permission Model**: ``` User \u2192 Roles \u2192 Permissions \u2198 \u2197 Tenants (Multi-tenancy isolation) ``` Example permissions: - `workflow:read` - View workflows - `workflow:write` - Create/update workflows - `workflow:execute` - Trigger workflow execution - `workflow:delete` - Delete workflows - `admin:*` - All administrative operations **API Key Management**: - Format: `cfk_live_<32-char-random>` (production), `cfk_test_<32-char-random>` (sandbox) - Hashing: SHA-256 before storage in PostgreSQL - Scoping: API keys can be scoped to specific workflows or operations - Rate limits: Configurable",
              "score": null
            }
          ],
          "latency_ms": 893.6875100007455,
          "query_id": "realistic_004",
          "dimension": "original",
          "query": "What database technology does CloudFlow use?"
        },
        {
          "key_facts": [
            "PostgreSQL 15.4",
            "Redis 7.2",
            "Apache Kafka 3.6"
          ],
          "found_facts": [
            "PostgreSQL 15.4",
            "Redis 7.2",
            "Apache Kafka 3.6"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "architecture_overview"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_0",
              "content": "# CloudFlow Platform - System Architecture Overview **Document Version:** 2.3.1 **Last Updated:** January 15, 2026 **Owner:** Platform Architecture Team **Status:** Production ## Executive Summary CloudFlow is a distributed, cloud-native workflow automation platform designed to orchestrate complex business processes at scale. The platform processes over 2.5 million workflow executions daily with an average P99 latency of 180ms for API operations and 4.2 seconds for workflow execution. This document provides a comprehensive overview of the system architecture, including microservices design, data flow patterns, infrastructure decisions, and operational considerations. ## Table of Contents 1. [High-Level Architecture](#high-level-architecture) 2. [Microservices Breakdown](#microservices-breakdown) 3. [Data Flow Architecture](#data-flow-architecture) 4. [Database Architecture](#database-architecture) 5. [Message Queue Patterns](#message-queue-patterns) 6. [Caching Strategy](#caching-strategy) 7. [Security Architecture](#security-architecture) 8. [Performance Characteristics](#performance-characteristics) 9. [Disaster Recovery](#disaster-recovery) --- ## High-Level Architecture CloudFlow follows a microservices architecture pattern deployed across multiple availability zones in AWS. The platform is designed with the following core principles: - **Scalability**: Horizontal scaling for all services with auto-scaling groups - **Resilience**: Circuit breakers, bulkheads, and graceful degradation - **Observability**: Distributed tracing, centralized logging, and comprehensive metrics - **Security**: Zero-trust network architecture with mTLS encryption ### Architecture Diagram (Conceptual) ``` \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 Load Balancer (ALB) \u2502 \u2502 (TLS Termination - 443) \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u25bc \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 API Gateway Layer \u2502 \u2502 (Rate Limiting, Auth, Request Routing) \u2502 \u2514\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u2502 \u2502 \u2502 \u2502 \u25bc \u25bc \u25bc \u25bc \u25bc \u250c\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502Auth\u2502 \u2502Workflow\u2502 \u2502Scheduler\u2502 \u2502Notification\u2502 \u2502User Service\u2502 \u2502Svc \u2502 \u2502 Engine \u2502 \u2502 Service \u2502 \u2502 Service \u2502 \u2502 \u2502 \u2514\u2500\u252c\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u2502 \u2502 \u2502 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 \u2502 \u2502 \u25bc \u25bc \u25bc \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502PostgreSQL\u2502 \u2502 Redis \u2502 \u2502 Kafka \u2502 \u2502 Cluster \u2502 \u2502 Cluster \u2502 \u2502 Cluster \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 ``` ### Technology Stack - **Runtime**: Node.js 20.x (API Gateway, Workflow Engine), Go 1.21 (Auth Service, Scheduler) - **Container Orchestration**: Kubernetes 1.28 (EKS) - **Service Mesh**: Istio 1.20 for service-to-service communication - **Databases**: PostgreSQL 15.4, Redis 7.2 - **Message Broker**: Apache Kafka 3.6 - **Monitoring**: Prometheus, Grafana, Jaeger for distributed tracing - **Secrets Management**: HashiCorp Vault 1.15 --- ## Microservices Breakdown ### API Gateway **Purpose**: Single entry point for all client requests, providing authentication, rate limiting, request routing, and protocol translation.",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_4",
              "content": "**Technology**: Node.js with worker pool pattern **Replicas**: 8 pods (production), auto-scaling 6-16 **Resource Allocation**: 2 vCPU, 4GB RAM per pod **Key Responsibilities**: - Consume notification events from Kafka - Template rendering with Handlebars (cached in Redis) - Multi-channel delivery (Email via SendGrid, SMS via Twilio, Webhooks) - Retry logic with dead letter queue for failed deliveries - Delivery status tracking and analytics - User preference management (opt-in/opt-out) **Notification Flow**: ``` Event Source \u2192 Kafka (notifications.email topic) \u2502 \u25bc Notification Service Consumer \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u25bc \u25bc \u25bc [Email Queue] [SMS Queue] [Webhook Queue] \u2502 \u2502 \u2502 \u25bc \u25bc \u25bc [SendGrid] [Twilio] [HTTP POST] \u2502 \u2502 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u25bc Delivery Status \u2192 PostgreSQL ``` **Channel Configuration**: - Email: SendGrid with IP warming, DKIM/SPF configured - SMS: Twilio with fallback to AWS SNS - Webhook: Signed payloads (HMAC-SHA256), retry 3 times with exponential backoff - In-app: WebSocket connections via Socket.io **Template Management**: - Templates stored in PostgreSQL with versioning - Compiled templates cached in Redis (1-hour TTL) - Support for localization (i18n) with 12 languages - A/B testing capability for email campaigns **Performance Targets**: - Email delivery: < 5 seconds (P99) from event to SendGrid - SMS delivery: < 3 seconds (P99) - Throughput: 10,000 notifications per minute --- ## Data Flow Architecture ### Synchronous Request Flow Client requests follow a synchronous path through the API Gateway to backend services: ``` 1. Client \u2192 Load Balancer (TLS termination) 2. Load Balancer \u2192 API Gateway (HTTP/2) 3. API Gateway \u2192 Auth Service (JWT validation via gRPC) 4. API Gateway \u2192 Target Service (HTTP/REST or gRPC) 5. Target Service \u2192 Database/Cache (data retrieval) 6. Response propagates back through the chain ``` **Timeout Configuration**: - Client \u2192 Load Balancer: 60 seconds - Load Balancer \u2192 API Gateway: 55 seconds - API Gateway \u2192 Services: 45 seconds - Service \u2192 Database: 10 seconds ### Asynchronous Event Flow Long-running operations and inter-service communication use event-driven patterns: ``` 1. Service publishes event \u2192 Kafka topic 2. Kafka persists event (replication factor: 3) 3. Consumer groups subscribe to topics 4. Consumers process events with at-least-once delivery 5. State updates written to PostgreSQL 6.",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_12",
              "content": "Resume normal operations - Recovery time: 1-3 hours depending on data volume - Data loss: None if corruption detected quickly ### Testing & Validation **DR Drill Schedule**: - Monthly: Automated failover test (single AZ failure simulation) - Quarterly: Full DR region failover (non-production hours) - Annually: Complete disaster simulation with all stakeholders **Last DR Test Results** (Dec 15, 2025): - Scenario: Full region failover - Actual RTO: 2 hours 23 minutes (target: 4 hours) - Actual RPO: 42 minutes (target: 1 hour) - Issues identified: DNS propagation slower than expected (resolved) - Success criteria: Met all recovery objectives ### Business Continuity **Communication Plan**: - Status page: status.cloudflow.com (updated every 15 minutes during incident) - Customer notifications: Email + SMS for all P1 incidents - Internal escalation: PagerDuty \u2192 Incident Commander \u2192 Engineering Manager \u2192 CTO **Data Retention Policy**: - Active data: 12 months in hot storage (PostgreSQL) - Archived data: 7 years in cold storage (S3 Glacier) - Audit logs: 7 years (compliance requirement) - Backup retention: 30 days standard, 90 days monthly snapshots --- ## Appendix ### Service Dependency Matrix | Service | Depends On | Critical Path | |---------|-----------|---------------| | API Gateway | Auth Service, Redis | Yes | | Auth Service | PostgreSQL, Redis, Vault | Yes | | Workflow Engine | PostgreSQL, Kafka, Redis | Yes | | Scheduler | PostgreSQL, Redis, Kafka | No | | Notification Service | Kafka, PostgreSQL, SendGrid, Twilio | No | ### Capacity Planning **Current Capacity** (Jan 2026): - Daily workflow executions: 2.5M - Active users: 150,000 - API requests per day: 50M - Database size: 1.2TB - Kafka throughput: 100K msg/sec peak **6-Month Projection** (Jul 2026): - Daily workflow executions: 4M (+60%) - Active users: 225,000 (+50%) - API requests per day: 80M (+60%) - Database size: 2TB (+67%) - Required scaling: +30% compute, +50% storage ### Contact Information - **Architecture Team**: architecture@cloudflow.internal - **On-Call Engineer**: PagerDuty escalation - **Security Team**: security@cloudflow.internal - **Documentation**: https://wiki.cloudflow.internal/architecture --- **Document Revision History**: - v2.3.1 (Jan 15, 2026): Updated performance metrics, added DR test results - v2.3.0 (Dec 1, 2025): Added multi-AZ deployment details - v2.2.0 (Oct 15, 2025): Security architecture overhaul - v2.1.0 (Sep 1, 2025): Initial Kafka migration documentation - v2.0.0 (Jul 1, 2025): Complete microservices rewrite",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_8",
              "content": "### Cache Invalidation Strategies **Time-Based Expiration (TTL)**: - Short-lived data: 5-15 minutes (session tokens, rate limit counters) - Medium-lived data: 1 hour (workflow definitions, templates) - Long-lived data: 24 hours (static configuration) **Event-Based Invalidation**: ``` Database Update Event \u2192 Kafka (cache.invalidation topic) \u2502 \u25bc All service instances consume event \u2502 \u25bc Redis DEL for affected keys ``` **Pattern-Based Invalidation**: - Use Redis SCAN + DEL for wildcard patterns - Example: Invalidate all user caches: `user:*:{user_id}` **Cache Warming**: - Scheduled job runs every hour to pre-populate frequently accessed data - Targets: Top 1000 workflows, system configuration, active user profiles - Reduces cache miss rate during traffic spikes ### Multi-Level Caching Application implements L1 (in-memory) and L2 (Redis) caching: ``` Request \u2192 L1 Cache (Node.js Map, 1000 entry LRU) \u2502 MISS \u2502 \u25bc L2 Cache (Redis, distributed) \u2502 MISS \u2502 \u25bc Database (PostgreSQL) ``` L1 cache reduces Redis network calls by 40% for hot data. --- ## Security Architecture ### Network Security **Zero-Trust Network Model**: - All service-to-service communication encrypted with mTLS - Certificate rotation: Every 90 days (automated via cert-manager) - Certificate authority: Internal PKI with HashiCorp Vault - Network segmentation: Private subnets for services, public subnet for ALB only **Service Mesh (Istio)**: ``` Service A \u2192 Envoy Sidecar (mTLS client cert) \u2502 Encrypted channel \u2502 Envoy Sidecar (mTLS server cert) \u2192 Service B ``` **Firewall Rules**: - Security groups: Deny all by default, explicit allow rules - Ingress: Only ALB can reach API Gateway (port 8080) - Egress: Services can only reach specific dependencies - Database access: Limited to application subnets only ### Authentication & Authorization **JWT Token Validation**: - Algorithm: RS256 (asymmetric signing) - Key rotation: Every 30 days with 7-day overlap period - Public key distribution: JWKS endpoint cached in Redis - Validation: Signature, expiry, issuer, audience claims - Token revocation: Blacklist in Redis for compromised tokens **Permission Model**: ``` User \u2192 Roles \u2192 Permissions \u2198 \u2197 Tenants (Multi-tenancy isolation) ``` Example permissions: - `workflow:read` - View workflows - `workflow:write` - Create/update workflows - `workflow:execute` - Trigger workflow execution - `workflow:delete` - Delete workflows - `admin:*` - All administrative operations **API Key Management**: - Format: `cfk_live_<32-char-random>` (production), `cfk_test_<32-char-random>` (sandbox) - Hashing: SHA-256 before storage in PostgreSQL - Scoping: API keys can be scoped to specific workflows or operations - Rate limits: Configurable",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_5",
              "content": "Success/failure events published back to Kafka ``` **Event Schema**: ```json { \"event_id\": \"uuid-v4\", \"event_type\": \"workflow.execution.completed\", \"timestamp\": \"2026-01-15T10:30:00.000Z\", \"correlation_id\": \"request-trace-id\", \"payload\": { \"workflow_id\": \"wf-12345\", \"execution_id\": \"exec-67890\", \"status\": \"COMPLETED\", \"duration_ms\": 4230 }, \"metadata\": { \"source_service\": \"workflow-engine\", \"schema_version\": \"1.0\" } } ``` ### Inter-Service Communication Services communicate using two primary patterns: **Synchronous (gRPC)**: - Auth Service validation calls from API Gateway - User Service profile lookups - Low latency requirement (< 50ms) - Request-response pattern with circuit breaker **Asynchronous (Kafka)**: - Workflow execution events - Notification triggers - Audit log events - Eventual consistency acceptable --- ## Database Architecture ### PostgreSQL Primary Database **Cluster Configuration**: - Primary-replica setup with 1 primary + 2 read replicas - Instance type: db.r6g.2xlarge (8 vCPU, 64GB RAM) - Storage: 2TB gp3 SSD with 12,000 IOPS - Multi-AZ deployment for high availability - Automated backups: Daily snapshots, 30-day retention - Point-in-time recovery: 5-minute granularity **Database Schema Design**: ``` Core Tables: - users (5M rows): User accounts and profiles - workflows (2M rows): Workflow definitions - workflow_executions (500M rows, partitioned): Execution history - workflow_steps (2B rows, partitioned): Individual step records - schedules (100K rows): Scheduled workflow triggers - notifications (1B rows, partitioned): Notification delivery log Partitioning Strategy: - workflow_executions: Monthly partitions by created_at - workflow_steps: Monthly partitions by created_at - notifications: Weekly partitions by created_at - Automatic partition creation via cron job - Retention policy: Drop partitions older than 12 months ``` **Indexing Strategy**: - Primary keys: UUIDs with B-tree indexes - Frequently queried columns: Compound indexes (e.g., user_id + status + created_at) - JSONB columns: GIN indexes for workflow definitions - Full-text search: GiST indexes on description fields **Connection Pooling**: - PgBouncer in transaction mode - Pool size: 100 connections per service - Max client connections: 2000 - Connection timeout: 30 seconds **Performance Characteristics**: - Read query P99: < 20ms - Write query P99: < 50ms - Transaction throughput: 15,000 TPS - Replication lag: < 500ms --- ### Redis Caching Layer **Cluster Configuration**: - Redis Cluster with 6 nodes (3 primary + 3 replica) - Instance type: cache.r6g.xlarge (4 vCPU, 26GB RAM) - Total memory: 78GB usable cache space - Persistence: RDB snapshots every 5 minutes + AOF - Eviction policy: allkeys-lru **Cache Usage Patterns**: 1.",
              "score": null
            }
          ],
          "latency_ms": 840.8216780007933,
          "query_id": "realistic_004",
          "dimension": "synonym",
          "query": "Which database system powers CloudFlow?"
        },
        {
          "key_facts": [
            "PostgreSQL 15.4",
            "Redis 7.2",
            "Apache Kafka 3.6"
          ],
          "found_facts": [
            "PostgreSQL 15.4",
            "Redis 7.2",
            "Apache Kafka 3.6"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "architecture_overview"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_0",
              "content": "# CloudFlow Platform - System Architecture Overview **Document Version:** 2.3.1 **Last Updated:** January 15, 2026 **Owner:** Platform Architecture Team **Status:** Production ## Executive Summary CloudFlow is a distributed, cloud-native workflow automation platform designed to orchestrate complex business processes at scale. The platform processes over 2.5 million workflow executions daily with an average P99 latency of 180ms for API operations and 4.2 seconds for workflow execution. This document provides a comprehensive overview of the system architecture, including microservices design, data flow patterns, infrastructure decisions, and operational considerations. ## Table of Contents 1. [High-Level Architecture](#high-level-architecture) 2. [Microservices Breakdown](#microservices-breakdown) 3. [Data Flow Architecture](#data-flow-architecture) 4. [Database Architecture](#database-architecture) 5. [Message Queue Patterns](#message-queue-patterns) 6. [Caching Strategy](#caching-strategy) 7. [Security Architecture](#security-architecture) 8. [Performance Characteristics](#performance-characteristics) 9. [Disaster Recovery](#disaster-recovery) --- ## High-Level Architecture CloudFlow follows a microservices architecture pattern deployed across multiple availability zones in AWS. The platform is designed with the following core principles: - **Scalability**: Horizontal scaling for all services with auto-scaling groups - **Resilience**: Circuit breakers, bulkheads, and graceful degradation - **Observability**: Distributed tracing, centralized logging, and comprehensive metrics - **Security**: Zero-trust network architecture with mTLS encryption ### Architecture Diagram (Conceptual) ``` \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 Load Balancer (ALB) \u2502 \u2502 (TLS Termination - 443) \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u25bc \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 API Gateway Layer \u2502 \u2502 (Rate Limiting, Auth, Request Routing) \u2502 \u2514\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u2502 \u2502 \u2502 \u2502 \u25bc \u25bc \u25bc \u25bc \u25bc \u250c\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502Auth\u2502 \u2502Workflow\u2502 \u2502Scheduler\u2502 \u2502Notification\u2502 \u2502User Service\u2502 \u2502Svc \u2502 \u2502 Engine \u2502 \u2502 Service \u2502 \u2502 Service \u2502 \u2502 \u2502 \u2514\u2500\u252c\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u2502 \u2502 \u2502 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 \u2502 \u2502 \u25bc \u25bc \u25bc \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502PostgreSQL\u2502 \u2502 Redis \u2502 \u2502 Kafka \u2502 \u2502 Cluster \u2502 \u2502 Cluster \u2502 \u2502 Cluster \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 ``` ### Technology Stack - **Runtime**: Node.js 20.x (API Gateway, Workflow Engine), Go 1.21 (Auth Service, Scheduler) - **Container Orchestration**: Kubernetes 1.28 (EKS) - **Service Mesh**: Istio 1.20 for service-to-service communication - **Databases**: PostgreSQL 15.4, Redis 7.2 - **Message Broker**: Apache Kafka 3.6 - **Monitoring**: Prometheus, Grafana, Jaeger for distributed tracing - **Secrets Management**: HashiCorp Vault 1.15 --- ## Microservices Breakdown ### API Gateway **Purpose**: Single entry point for all client requests, providing authentication, rate limiting, request routing, and protocol translation.",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_8",
              "content": "### Cache Invalidation Strategies **Time-Based Expiration (TTL)**: - Short-lived data: 5-15 minutes (session tokens, rate limit counters) - Medium-lived data: 1 hour (workflow definitions, templates) - Long-lived data: 24 hours (static configuration) **Event-Based Invalidation**: ``` Database Update Event \u2192 Kafka (cache.invalidation topic) \u2502 \u25bc All service instances consume event \u2502 \u25bc Redis DEL for affected keys ``` **Pattern-Based Invalidation**: - Use Redis SCAN + DEL for wildcard patterns - Example: Invalidate all user caches: `user:*:{user_id}` **Cache Warming**: - Scheduled job runs every hour to pre-populate frequently accessed data - Targets: Top 1000 workflows, system configuration, active user profiles - Reduces cache miss rate during traffic spikes ### Multi-Level Caching Application implements L1 (in-memory) and L2 (Redis) caching: ``` Request \u2192 L1 Cache (Node.js Map, 1000 entry LRU) \u2502 MISS \u2502 \u25bc L2 Cache (Redis, distributed) \u2502 MISS \u2502 \u25bc Database (PostgreSQL) ``` L1 cache reduces Redis network calls by 40% for hot data. --- ## Security Architecture ### Network Security **Zero-Trust Network Model**: - All service-to-service communication encrypted with mTLS - Certificate rotation: Every 90 days (automated via cert-manager) - Certificate authority: Internal PKI with HashiCorp Vault - Network segmentation: Private subnets for services, public subnet for ALB only **Service Mesh (Istio)**: ``` Service A \u2192 Envoy Sidecar (mTLS client cert) \u2502 Encrypted channel \u2502 Envoy Sidecar (mTLS server cert) \u2192 Service B ``` **Firewall Rules**: - Security groups: Deny all by default, explicit allow rules - Ingress: Only ALB can reach API Gateway (port 8080) - Egress: Services can only reach specific dependencies - Database access: Limited to application subnets only ### Authentication & Authorization **JWT Token Validation**: - Algorithm: RS256 (asymmetric signing) - Key rotation: Every 30 days with 7-day overlap period - Public key distribution: JWKS endpoint cached in Redis - Validation: Signature, expiry, issuer, audience claims - Token revocation: Blacklist in Redis for compromised tokens **Permission Model**: ``` User \u2192 Roles \u2192 Permissions \u2198 \u2197 Tenants (Multi-tenancy isolation) ``` Example permissions: - `workflow:read` - View workflows - `workflow:write` - Create/update workflows - `workflow:execute` - Trigger workflow execution - `workflow:delete` - Delete workflows - `admin:*` - All administrative operations **API Key Management**: - Format: `cfk_live_<32-char-random>` (production), `cfk_test_<32-char-random>` (sandbox) - Hashing: SHA-256 before storage in PostgreSQL - Scoping: API keys can be scoped to specific workflows or operations - Rate limits: Configurable",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_12",
              "content": "Resume normal operations - Recovery time: 1-3 hours depending on data volume - Data loss: None if corruption detected quickly ### Testing & Validation **DR Drill Schedule**: - Monthly: Automated failover test (single AZ failure simulation) - Quarterly: Full DR region failover (non-production hours) - Annually: Complete disaster simulation with all stakeholders **Last DR Test Results** (Dec 15, 2025): - Scenario: Full region failover - Actual RTO: 2 hours 23 minutes (target: 4 hours) - Actual RPO: 42 minutes (target: 1 hour) - Issues identified: DNS propagation slower than expected (resolved) - Success criteria: Met all recovery objectives ### Business Continuity **Communication Plan**: - Status page: status.cloudflow.com (updated every 15 minutes during incident) - Customer notifications: Email + SMS for all P1 incidents - Internal escalation: PagerDuty \u2192 Incident Commander \u2192 Engineering Manager \u2192 CTO **Data Retention Policy**: - Active data: 12 months in hot storage (PostgreSQL) - Archived data: 7 years in cold storage (S3 Glacier) - Audit logs: 7 years (compliance requirement) - Backup retention: 30 days standard, 90 days monthly snapshots --- ## Appendix ### Service Dependency Matrix | Service | Depends On | Critical Path | |---------|-----------|---------------| | API Gateway | Auth Service, Redis | Yes | | Auth Service | PostgreSQL, Redis, Vault | Yes | | Workflow Engine | PostgreSQL, Kafka, Redis | Yes | | Scheduler | PostgreSQL, Redis, Kafka | No | | Notification Service | Kafka, PostgreSQL, SendGrid, Twilio | No | ### Capacity Planning **Current Capacity** (Jan 2026): - Daily workflow executions: 2.5M - Active users: 150,000 - API requests per day: 50M - Database size: 1.2TB - Kafka throughput: 100K msg/sec peak **6-Month Projection** (Jul 2026): - Daily workflow executions: 4M (+60%) - Active users: 225,000 (+50%) - API requests per day: 80M (+60%) - Database size: 2TB (+67%) - Required scaling: +30% compute, +50% storage ### Contact Information - **Architecture Team**: architecture@cloudflow.internal - **On-Call Engineer**: PagerDuty escalation - **Security Team**: security@cloudflow.internal - **Documentation**: https://wiki.cloudflow.internal/architecture --- **Document Revision History**: - v2.3.1 (Jan 15, 2026): Updated performance metrics, added DR test results - v2.3.0 (Dec 1, 2025): Added multi-AZ deployment details - v2.2.0 (Oct 15, 2025): Security architecture overhaul - v2.1.0 (Sep 1, 2025): Initial Kafka migration documentation - v2.0.0 (Jul 1, 2025): Complete microservices rewrite",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_3",
              "content": "**Technology**: Go with distributed locking via Redis **Replicas**: 4 pods (production), active-passive with leader election **Resource Allocation**: 2 vCPU, 4GB RAM per pod **Key Responsibilities**: - Parse and validate cron expressions (extended format supporting seconds) - Maintain schedule registry in PostgreSQL - Distributed scheduling with leader election (one active scheduler) - Missed execution handling with configurable catch-up policy - Schedule conflict detection and resolution - Time zone support for international schedules **Scheduling Architecture**: ``` PostgreSQL Schedules Table \u2502 \u25bc Leader Scheduler (elected via Redis) \u2502 \u251c\u2500 Scan for due schedules (every 10 seconds) \u251c\u2500 Acquire lock per schedule (prevents duplicates) \u251c\u2500 Publish to Kafka \u2192 workflow.events topic \u2514\u2500 Update last_run timestamp ``` **Schedule Types**: - **Cron-based**: `0 */5 * * * *` (every 5 minutes) - **One-time**: Specific timestamp for delayed execution - **Interval-based**: Every N seconds/minutes/hours **Reliability Features**: - Leader election using Redis with 30-second lease - Heartbeat mechanism to detect leader failure (5-second interval) - Automatic failover to standby scheduler (< 10 seconds) - Schedule versioning to handle updates during execution - Missed execution policy: SKIP, RUN_ONCE, or RUN_ALL **Performance Targets**: - Schedule evaluation: < 100ms per cycle - Accuracy: \u00b1 1 second for schedule triggers - Capacity: 100,000 active schedules --- ### Notification Service **Purpose**: Multi-channel notification delivery system supporting email, SMS, webhooks, and in-app notifications.",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_2",
              "content": "**Technology**: Node.js with TypeScript, Bull queue library **Replicas**: 16 pods (production), auto-scaling 12-24 **Resource Allocation**: 4 vCPU, 8GB RAM per pod **Key Responsibilities**: - Parse and validate workflow definitions (JSON-based DSL) - Execute workflow steps with state machine pattern - Handle retries with exponential backoff (max 3 retries, backoff: 2^n seconds) - Coordinate parallel and sequential task execution - Manage workflow state persistence and recovery - Support for conditional branching and loops - Sub-workflow invocation and composition **Workflow Execution Model**: ``` Workflow Submitted \u2192 Validation \u2192 Queue in Kafka \u2502 \u25bc Workflow Engine picks up \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u25bc \u25bc \u25bc [Task 1 Exec] [Task 2 Exec] [Task 3 Exec] \u2502 \u2502 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u25bc State Update \u2192 PostgreSQL \u2502 \u25bc Publish Event \u2192 Kafka ``` **Workflow DSL Example**: ```json { \"workflow_id\": \"customer-onboarding-v2\", \"version\": \"2.1.0\", \"steps\": [ { \"id\": \"validate-customer\", \"type\": \"http_request\", \"config\": { \"url\": \"https://validation.internal/verify\", \"method\": \"POST\", \"timeout\": 5000, \"retry\": {\"max_attempts\": 3, \"backoff\": \"exponential\"} } }, { \"id\": \"create-account\", \"type\": \"database_operation\", \"depends_on\": [\"validate-customer\"] } ] } ``` **State Management**: - Workflow states: PENDING, RUNNING, COMPLETED, FAILED, CANCELLED - Step states: QUEUED, EXECUTING, SUCCESS, FAILED, SKIPPED - State transitions stored in PostgreSQL with event sourcing pattern - Checkpointing every 10 steps for long-running workflows **Performance Targets**: - Simple workflow (3-5 steps): < 2 seconds (P99) - Complex workflow (20+ steps): < 5 seconds (P99) - Throughput: 500 concurrent workflow executions per pod --- ### Scheduler Service **Purpose**: Time-based workflow triggering system supporting cron-like schedules and one-time delayed executions.",
              "score": null
            }
          ],
          "latency_ms": 1061.9834859971888,
          "query_id": "realistic_004",
          "dimension": "problem",
          "query": "I need to understand the data storage architecture"
        },
        {
          "key_facts": [
            "PostgreSQL 15.4",
            "Redis 7.2",
            "Apache Kafka 3.6"
          ],
          "found_facts": [
            "PostgreSQL 15.4",
            "Redis 7.2",
            "Apache Kafka 3.6"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "architecture_overview"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_0",
              "content": "# CloudFlow Platform - System Architecture Overview **Document Version:** 2.3.1 **Last Updated:** January 15, 2026 **Owner:** Platform Architecture Team **Status:** Production ## Executive Summary CloudFlow is a distributed, cloud-native workflow automation platform designed to orchestrate complex business processes at scale. The platform processes over 2.5 million workflow executions daily with an average P99 latency of 180ms for API operations and 4.2 seconds for workflow execution. This document provides a comprehensive overview of the system architecture, including microservices design, data flow patterns, infrastructure decisions, and operational considerations. ## Table of Contents 1. [High-Level Architecture](#high-level-architecture) 2. [Microservices Breakdown](#microservices-breakdown) 3. [Data Flow Architecture](#data-flow-architecture) 4. [Database Architecture](#database-architecture) 5. [Message Queue Patterns](#message-queue-patterns) 6. [Caching Strategy](#caching-strategy) 7. [Security Architecture](#security-architecture) 8. [Performance Characteristics](#performance-characteristics) 9. [Disaster Recovery](#disaster-recovery) --- ## High-Level Architecture CloudFlow follows a microservices architecture pattern deployed across multiple availability zones in AWS. The platform is designed with the following core principles: - **Scalability**: Horizontal scaling for all services with auto-scaling groups - **Resilience**: Circuit breakers, bulkheads, and graceful degradation - **Observability**: Distributed tracing, centralized logging, and comprehensive metrics - **Security**: Zero-trust network architecture with mTLS encryption ### Architecture Diagram (Conceptual) ``` \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 Load Balancer (ALB) \u2502 \u2502 (TLS Termination - 443) \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u25bc \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 API Gateway Layer \u2502 \u2502 (Rate Limiting, Auth, Request Routing) \u2502 \u2514\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u2502 \u2502 \u2502 \u2502 \u25bc \u25bc \u25bc \u25bc \u25bc \u250c\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502Auth\u2502 \u2502Workflow\u2502 \u2502Scheduler\u2502 \u2502Notification\u2502 \u2502User Service\u2502 \u2502Svc \u2502 \u2502 Engine \u2502 \u2502 Service \u2502 \u2502 Service \u2502 \u2502 \u2502 \u2514\u2500\u252c\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u2502 \u2502 \u2502 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 \u2502 \u2502 \u25bc \u25bc \u25bc \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502PostgreSQL\u2502 \u2502 Redis \u2502 \u2502 Kafka \u2502 \u2502 Cluster \u2502 \u2502 Cluster \u2502 \u2502 Cluster \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 ``` ### Technology Stack - **Runtime**: Node.js 20.x (API Gateway, Workflow Engine), Go 1.21 (Auth Service, Scheduler) - **Container Orchestration**: Kubernetes 1.28 (EKS) - **Service Mesh**: Istio 1.20 for service-to-service communication - **Databases**: PostgreSQL 15.4, Redis 7.2 - **Message Broker**: Apache Kafka 3.6 - **Monitoring**: Prometheus, Grafana, Jaeger for distributed tracing - **Secrets Management**: HashiCorp Vault 1.15 --- ## Microservices Breakdown ### API Gateway **Purpose**: Single entry point for all client requests, providing authentication, rate limiting, request routing, and protocol translation.",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_12",
              "content": "Resume normal operations - Recovery time: 1-3 hours depending on data volume - Data loss: None if corruption detected quickly ### Testing & Validation **DR Drill Schedule**: - Monthly: Automated failover test (single AZ failure simulation) - Quarterly: Full DR region failover (non-production hours) - Annually: Complete disaster simulation with all stakeholders **Last DR Test Results** (Dec 15, 2025): - Scenario: Full region failover - Actual RTO: 2 hours 23 minutes (target: 4 hours) - Actual RPO: 42 minutes (target: 1 hour) - Issues identified: DNS propagation slower than expected (resolved) - Success criteria: Met all recovery objectives ### Business Continuity **Communication Plan**: - Status page: status.cloudflow.com (updated every 15 minutes during incident) - Customer notifications: Email + SMS for all P1 incidents - Internal escalation: PagerDuty \u2192 Incident Commander \u2192 Engineering Manager \u2192 CTO **Data Retention Policy**: - Active data: 12 months in hot storage (PostgreSQL) - Archived data: 7 years in cold storage (S3 Glacier) - Audit logs: 7 years (compliance requirement) - Backup retention: 30 days standard, 90 days monthly snapshots --- ## Appendix ### Service Dependency Matrix | Service | Depends On | Critical Path | |---------|-----------|---------------| | API Gateway | Auth Service, Redis | Yes | | Auth Service | PostgreSQL, Redis, Vault | Yes | | Workflow Engine | PostgreSQL, Kafka, Redis | Yes | | Scheduler | PostgreSQL, Redis, Kafka | No | | Notification Service | Kafka, PostgreSQL, SendGrid, Twilio | No | ### Capacity Planning **Current Capacity** (Jan 2026): - Daily workflow executions: 2.5M - Active users: 150,000 - API requests per day: 50M - Database size: 1.2TB - Kafka throughput: 100K msg/sec peak **6-Month Projection** (Jul 2026): - Daily workflow executions: 4M (+60%) - Active users: 225,000 (+50%) - API requests per day: 80M (+60%) - Database size: 2TB (+67%) - Required scaling: +30% compute, +50% storage ### Contact Information - **Architecture Team**: architecture@cloudflow.internal - **On-Call Engineer**: PagerDuty escalation - **Security Team**: security@cloudflow.internal - **Documentation**: https://wiki.cloudflow.internal/architecture --- **Document Revision History**: - v2.3.1 (Jan 15, 2026): Updated performance metrics, added DR test results - v2.3.0 (Dec 1, 2025): Added multi-AZ deployment details - v2.2.0 (Oct 15, 2025): Security architecture overhaul - v2.1.0 (Sep 1, 2025): Initial Kafka migration documentation - v2.0.0 (Jul 1, 2025): Complete microservices rewrite",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_5",
              "content": "Success/failure events published back to Kafka ``` **Event Schema**: ```json { \"event_id\": \"uuid-v4\", \"event_type\": \"workflow.execution.completed\", \"timestamp\": \"2026-01-15T10:30:00.000Z\", \"correlation_id\": \"request-trace-id\", \"payload\": { \"workflow_id\": \"wf-12345\", \"execution_id\": \"exec-67890\", \"status\": \"COMPLETED\", \"duration_ms\": 4230 }, \"metadata\": { \"source_service\": \"workflow-engine\", \"schema_version\": \"1.0\" } } ``` ### Inter-Service Communication Services communicate using two primary patterns: **Synchronous (gRPC)**: - Auth Service validation calls from API Gateway - User Service profile lookups - Low latency requirement (< 50ms) - Request-response pattern with circuit breaker **Asynchronous (Kafka)**: - Workflow execution events - Notification triggers - Audit log events - Eventual consistency acceptable --- ## Database Architecture ### PostgreSQL Primary Database **Cluster Configuration**: - Primary-replica setup with 1 primary + 2 read replicas - Instance type: db.r6g.2xlarge (8 vCPU, 64GB RAM) - Storage: 2TB gp3 SSD with 12,000 IOPS - Multi-AZ deployment for high availability - Automated backups: Daily snapshots, 30-day retention - Point-in-time recovery: 5-minute granularity **Database Schema Design**: ``` Core Tables: - users (5M rows): User accounts and profiles - workflows (2M rows): Workflow definitions - workflow_executions (500M rows, partitioned): Execution history - workflow_steps (2B rows, partitioned): Individual step records - schedules (100K rows): Scheduled workflow triggers - notifications (1B rows, partitioned): Notification delivery log Partitioning Strategy: - workflow_executions: Monthly partitions by created_at - workflow_steps: Monthly partitions by created_at - notifications: Weekly partitions by created_at - Automatic partition creation via cron job - Retention policy: Drop partitions older than 12 months ``` **Indexing Strategy**: - Primary keys: UUIDs with B-tree indexes - Frequently queried columns: Compound indexes (e.g., user_id + status + created_at) - JSONB columns: GIN indexes for workflow definitions - Full-text search: GiST indexes on description fields **Connection Pooling**: - PgBouncer in transaction mode - Pool size: 100 connections per service - Max client connections: 2000 - Connection timeout: 30 seconds **Performance Characteristics**: - Read query P99: < 20ms - Write query P99: < 50ms - Transaction throughput: 15,000 TPS - Replication lag: < 500ms --- ### Redis Caching Layer **Cluster Configuration**: - Redis Cluster with 6 nodes (3 primary + 3 replica) - Instance type: cache.r6g.xlarge (4 vCPU, 26GB RAM) - Total memory: 78GB usable cache space - Persistence: RDB snapshots every 5 minutes + AOF - Eviction policy: allkeys-lru **Cache Usage Patterns**: 1.",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_8",
              "content": "### Cache Invalidation Strategies **Time-Based Expiration (TTL)**: - Short-lived data: 5-15 minutes (session tokens, rate limit counters) - Medium-lived data: 1 hour (workflow definitions, templates) - Long-lived data: 24 hours (static configuration) **Event-Based Invalidation**: ``` Database Update Event \u2192 Kafka (cache.invalidation topic) \u2502 \u25bc All service instances consume event \u2502 \u25bc Redis DEL for affected keys ``` **Pattern-Based Invalidation**: - Use Redis SCAN + DEL for wildcard patterns - Example: Invalidate all user caches: `user:*:{user_id}` **Cache Warming**: - Scheduled job runs every hour to pre-populate frequently accessed data - Targets: Top 1000 workflows, system configuration, active user profiles - Reduces cache miss rate during traffic spikes ### Multi-Level Caching Application implements L1 (in-memory) and L2 (Redis) caching: ``` Request \u2192 L1 Cache (Node.js Map, 1000 entry LRU) \u2502 MISS \u2502 \u25bc L2 Cache (Redis, distributed) \u2502 MISS \u2502 \u25bc Database (PostgreSQL) ``` L1 cache reduces Redis network calls by 40% for hot data. --- ## Security Architecture ### Network Security **Zero-Trust Network Model**: - All service-to-service communication encrypted with mTLS - Certificate rotation: Every 90 days (automated via cert-manager) - Certificate authority: Internal PKI with HashiCorp Vault - Network segmentation: Private subnets for services, public subnet for ALB only **Service Mesh (Istio)**: ``` Service A \u2192 Envoy Sidecar (mTLS client cert) \u2502 Encrypted channel \u2502 Envoy Sidecar (mTLS server cert) \u2192 Service B ``` **Firewall Rules**: - Security groups: Deny all by default, explicit allow rules - Ingress: Only ALB can reach API Gateway (port 8080) - Egress: Services can only reach specific dependencies - Database access: Limited to application subnets only ### Authentication & Authorization **JWT Token Validation**: - Algorithm: RS256 (asymmetric signing) - Key rotation: Every 30 days with 7-day overlap period - Public key distribution: JWKS endpoint cached in Redis - Validation: Signature, expiry, issuer, audience claims - Token revocation: Blacklist in Redis for compromised tokens **Permission Model**: ``` User \u2192 Roles \u2192 Permissions \u2198 \u2197 Tenants (Multi-tenancy isolation) ``` Example permissions: - `workflow:read` - View workflows - `workflow:write` - Create/update workflows - `workflow:execute` - Trigger workflow execution - `workflow:delete` - Delete workflows - `admin:*` - All administrative operations **API Key Management**: - Format: `cfk_live_<32-char-random>` (production), `cfk_test_<32-char-random>` (sandbox) - Hashing: SHA-256 before storage in PostgreSQL - Scoping: API keys can be scoped to specific workflows or operations - Rate limits: Configurable",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_4",
              "content": "**Technology**: Node.js with worker pool pattern **Replicas**: 8 pods (production), auto-scaling 6-16 **Resource Allocation**: 2 vCPU, 4GB RAM per pod **Key Responsibilities**: - Consume notification events from Kafka - Template rendering with Handlebars (cached in Redis) - Multi-channel delivery (Email via SendGrid, SMS via Twilio, Webhooks) - Retry logic with dead letter queue for failed deliveries - Delivery status tracking and analytics - User preference management (opt-in/opt-out) **Notification Flow**: ``` Event Source \u2192 Kafka (notifications.email topic) \u2502 \u25bc Notification Service Consumer \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u25bc \u25bc \u25bc [Email Queue] [SMS Queue] [Webhook Queue] \u2502 \u2502 \u2502 \u25bc \u25bc \u25bc [SendGrid] [Twilio] [HTTP POST] \u2502 \u2502 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u25bc Delivery Status \u2192 PostgreSQL ``` **Channel Configuration**: - Email: SendGrid with IP warming, DKIM/SPF configured - SMS: Twilio with fallback to AWS SNS - Webhook: Signed payloads (HMAC-SHA256), retry 3 times with exponential backoff - In-app: WebSocket connections via Socket.io **Template Management**: - Templates stored in PostgreSQL with versioning - Compiled templates cached in Redis (1-hour TTL) - Support for localization (i18n) with 12 languages - A/B testing capability for email campaigns **Performance Targets**: - Email delivery: < 5 seconds (P99) from event to SendGrid - SMS delivery: < 3 seconds (P99) - Throughput: 10,000 notifications per minute --- ## Data Flow Architecture ### Synchronous Request Flow Client requests follow a synchronous path through the API Gateway to backend services: ``` 1. Client \u2192 Load Balancer (TLS termination) 2. Load Balancer \u2192 API Gateway (HTTP/2) 3. API Gateway \u2192 Auth Service (JWT validation via gRPC) 4. API Gateway \u2192 Target Service (HTTP/REST or gRPC) 5. Target Service \u2192 Database/Cache (data retrieval) 6. Response propagates back through the chain ``` **Timeout Configuration**: - Client \u2192 Load Balancer: 60 seconds - Load Balancer \u2192 API Gateway: 55 seconds - API Gateway \u2192 Services: 45 seconds - Service \u2192 Database: 10 seconds ### Asynchronous Event Flow Long-running operations and inter-service communication use event-driven patterns: ``` 1. Service publishes event \u2192 Kafka topic 2. Kafka persists event (replication factor: 3) 3. Consumer groups subscribe to topics 4. Consumers process events with at-least-once delivery 5. State updates written to PostgreSQL 6.",
              "score": null
            }
          ],
          "latency_ms": 924.537008999323,
          "query_id": "realistic_004",
          "dimension": "casual",
          "query": "database stack"
        },
        {
          "key_facts": [
            "PostgreSQL 15.4",
            "Redis 7.2",
            "Apache Kafka 3.6"
          ],
          "found_facts": [
            "PostgreSQL 15.4",
            "Redis 7.2",
            "Apache Kafka 3.6"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "architecture_overview"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_12",
              "content": "Resume normal operations - Recovery time: 1-3 hours depending on data volume - Data loss: None if corruption detected quickly ### Testing & Validation **DR Drill Schedule**: - Monthly: Automated failover test (single AZ failure simulation) - Quarterly: Full DR region failover (non-production hours) - Annually: Complete disaster simulation with all stakeholders **Last DR Test Results** (Dec 15, 2025): - Scenario: Full region failover - Actual RTO: 2 hours 23 minutes (target: 4 hours) - Actual RPO: 42 minutes (target: 1 hour) - Issues identified: DNS propagation slower than expected (resolved) - Success criteria: Met all recovery objectives ### Business Continuity **Communication Plan**: - Status page: status.cloudflow.com (updated every 15 minutes during incident) - Customer notifications: Email + SMS for all P1 incidents - Internal escalation: PagerDuty \u2192 Incident Commander \u2192 Engineering Manager \u2192 CTO **Data Retention Policy**: - Active data: 12 months in hot storage (PostgreSQL) - Archived data: 7 years in cold storage (S3 Glacier) - Audit logs: 7 years (compliance requirement) - Backup retention: 30 days standard, 90 days monthly snapshots --- ## Appendix ### Service Dependency Matrix | Service | Depends On | Critical Path | |---------|-----------|---------------| | API Gateway | Auth Service, Redis | Yes | | Auth Service | PostgreSQL, Redis, Vault | Yes | | Workflow Engine | PostgreSQL, Kafka, Redis | Yes | | Scheduler | PostgreSQL, Redis, Kafka | No | | Notification Service | Kafka, PostgreSQL, SendGrid, Twilio | No | ### Capacity Planning **Current Capacity** (Jan 2026): - Daily workflow executions: 2.5M - Active users: 150,000 - API requests per day: 50M - Database size: 1.2TB - Kafka throughput: 100K msg/sec peak **6-Month Projection** (Jul 2026): - Daily workflow executions: 4M (+60%) - Active users: 225,000 (+50%) - API requests per day: 80M (+60%) - Database size: 2TB (+67%) - Required scaling: +30% compute, +50% storage ### Contact Information - **Architecture Team**: architecture@cloudflow.internal - **On-Call Engineer**: PagerDuty escalation - **Security Team**: security@cloudflow.internal - **Documentation**: https://wiki.cloudflow.internal/architecture --- **Document Revision History**: - v2.3.1 (Jan 15, 2026): Updated performance metrics, added DR test results - v2.3.0 (Dec 1, 2025): Added multi-AZ deployment details - v2.2.0 (Oct 15, 2025): Security architecture overhaul - v2.1.0 (Sep 1, 2025): Initial Kafka migration documentation - v2.0.0 (Jul 1, 2025): Complete microservices rewrite",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_11",
              "content": "Service (2) - PostgreSQL Primary - PostgreSQL Replica - PostgreSQL Replica - Redis Primary (2) - Redis Replica (2) - Redis Replica (2) - Kafka Broker (2) - Kafka Broker (2) - Kafka Broker (1) ``` **Automatic Failover**: - Database: 30-60 seconds (automatic promotion of replica) - Redis: < 10 seconds (Sentinel-based failover) - Kafka: < 30 seconds (controller election) - Services: Kubernetes health checks with 10-second liveness probes ### Backup Strategy **Database Backups**: - Automated snapshots: Daily at 02:00 UTC - Retention: 30 days for daily, 90 days for monthly - Cross-region replication: Async replication to us-west-2 (15-minute lag) - Backup verification: Weekly automated restore test in staging environment **Backup Schedule**: ``` Daily: Full snapshot \u2192 S3 (encrypted) Hourly: WAL archives \u2192 S3 (point-in-time recovery) Weekly: Backup validation test Monthly: Long-term archive to Glacier ``` **Configuration Backups**: - Kubernetes manifests: Stored in Git (GitOps with ArgoCD) - Vault secrets: Automated snapshot every 6 hours - Kafka topic configurations: Exported daily to S3 ### Disaster Recovery Procedures **Scenario 1: Single AZ Failure** - Detection: Health checks fail for entire AZ (< 30 seconds) - Action: Traffic automatically routed to healthy AZs by ALB - Recovery time: < 5 minutes (no manual intervention) - Data loss: None (multi-AZ replication) **Scenario 2: Database Primary Failure** - Detection: Health check fails for primary database (< 30 seconds) - Action: Automatic promotion of read replica to primary - Recovery time: 30-60 seconds - Data loss: Minimal (< 1 second due to synchronous replication) **Scenario 3: Full Region Failure** - Detection: Multiple health check failures across all AZs (< 2 minutes) - Action: Manual failover to DR region (us-west-2) - Procedure: 1. Update DNS to point to DR region (TTL: 60 seconds) 2. Promote DR database replica to primary 3. Scale up DR region services to production capacity 4. Verify data consistency and integrity 5. Update monitoring dashboards - Recovery time: 2-4 hours (includes verification) - Data loss: < 1 hour (last cross-region replication) **Scenario 4: Data Corruption** - Detection: Data validation checks or user report - Action: Point-in-time recovery from WAL archives - Procedure: 1. Identify corruption time window 2. Restore from snapshot prior to corruption 3. Replay WAL logs up to corruption point 4. Verify data integrity 5.",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_0",
              "content": "# CloudFlow Platform - System Architecture Overview **Document Version:** 2.3.1 **Last Updated:** January 15, 2026 **Owner:** Platform Architecture Team **Status:** Production ## Executive Summary CloudFlow is a distributed, cloud-native workflow automation platform designed to orchestrate complex business processes at scale. The platform processes over 2.5 million workflow executions daily with an average P99 latency of 180ms for API operations and 4.2 seconds for workflow execution. This document provides a comprehensive overview of the system architecture, including microservices design, data flow patterns, infrastructure decisions, and operational considerations. ## Table of Contents 1. [High-Level Architecture](#high-level-architecture) 2. [Microservices Breakdown](#microservices-breakdown) 3. [Data Flow Architecture](#data-flow-architecture) 4. [Database Architecture](#database-architecture) 5. [Message Queue Patterns](#message-queue-patterns) 6. [Caching Strategy](#caching-strategy) 7. [Security Architecture](#security-architecture) 8. [Performance Characteristics](#performance-characteristics) 9. [Disaster Recovery](#disaster-recovery) --- ## High-Level Architecture CloudFlow follows a microservices architecture pattern deployed across multiple availability zones in AWS. The platform is designed with the following core principles: - **Scalability**: Horizontal scaling for all services with auto-scaling groups - **Resilience**: Circuit breakers, bulkheads, and graceful degradation - **Observability**: Distributed tracing, centralized logging, and comprehensive metrics - **Security**: Zero-trust network architecture with mTLS encryption ### Architecture Diagram (Conceptual) ``` \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 Load Balancer (ALB) \u2502 \u2502 (TLS Termination - 443) \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u25bc \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 API Gateway Layer \u2502 \u2502 (Rate Limiting, Auth, Request Routing) \u2502 \u2514\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u2502 \u2502 \u2502 \u2502 \u25bc \u25bc \u25bc \u25bc \u25bc \u250c\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502Auth\u2502 \u2502Workflow\u2502 \u2502Scheduler\u2502 \u2502Notification\u2502 \u2502User Service\u2502 \u2502Svc \u2502 \u2502 Engine \u2502 \u2502 Service \u2502 \u2502 Service \u2502 \u2502 \u2502 \u2514\u2500\u252c\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u2502 \u2502 \u2502 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 \u2502 \u2502 \u25bc \u25bc \u25bc \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502PostgreSQL\u2502 \u2502 Redis \u2502 \u2502 Kafka \u2502 \u2502 Cluster \u2502 \u2502 Cluster \u2502 \u2502 Cluster \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 ``` ### Technology Stack - **Runtime**: Node.js 20.x (API Gateway, Workflow Engine), Go 1.21 (Auth Service, Scheduler) - **Container Orchestration**: Kubernetes 1.28 (EKS) - **Service Mesh**: Istio 1.20 for service-to-service communication - **Databases**: PostgreSQL 15.4, Redis 7.2 - **Message Broker**: Apache Kafka 3.6 - **Monitoring**: Prometheus, Grafana, Jaeger for distributed tracing - **Secrets Management**: HashiCorp Vault 1.15 --- ## Microservices Breakdown ### API Gateway **Purpose**: Single entry point for all client requests, providing authentication, rate limiting, request routing, and protocol translation.",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_5",
              "content": "Success/failure events published back to Kafka ``` **Event Schema**: ```json { \"event_id\": \"uuid-v4\", \"event_type\": \"workflow.execution.completed\", \"timestamp\": \"2026-01-15T10:30:00.000Z\", \"correlation_id\": \"request-trace-id\", \"payload\": { \"workflow_id\": \"wf-12345\", \"execution_id\": \"exec-67890\", \"status\": \"COMPLETED\", \"duration_ms\": 4230 }, \"metadata\": { \"source_service\": \"workflow-engine\", \"schema_version\": \"1.0\" } } ``` ### Inter-Service Communication Services communicate using two primary patterns: **Synchronous (gRPC)**: - Auth Service validation calls from API Gateway - User Service profile lookups - Low latency requirement (< 50ms) - Request-response pattern with circuit breaker **Asynchronous (Kafka)**: - Workflow execution events - Notification triggers - Audit log events - Eventual consistency acceptable --- ## Database Architecture ### PostgreSQL Primary Database **Cluster Configuration**: - Primary-replica setup with 1 primary + 2 read replicas - Instance type: db.r6g.2xlarge (8 vCPU, 64GB RAM) - Storage: 2TB gp3 SSD with 12,000 IOPS - Multi-AZ deployment for high availability - Automated backups: Daily snapshots, 30-day retention - Point-in-time recovery: 5-minute granularity **Database Schema Design**: ``` Core Tables: - users (5M rows): User accounts and profiles - workflows (2M rows): Workflow definitions - workflow_executions (500M rows, partitioned): Execution history - workflow_steps (2B rows, partitioned): Individual step records - schedules (100K rows): Scheduled workflow triggers - notifications (1B rows, partitioned): Notification delivery log Partitioning Strategy: - workflow_executions: Monthly partitions by created_at - workflow_steps: Monthly partitions by created_at - notifications: Weekly partitions by created_at - Automatic partition creation via cron job - Retention policy: Drop partitions older than 12 months ``` **Indexing Strategy**: - Primary keys: UUIDs with B-tree indexes - Frequently queried columns: Compound indexes (e.g., user_id + status + created_at) - JSONB columns: GIN indexes for workflow definitions - Full-text search: GiST indexes on description fields **Connection Pooling**: - PgBouncer in transaction mode - Pool size: 100 connections per service - Max client connections: 2000 - Connection timeout: 30 seconds **Performance Characteristics**: - Read query P99: < 20ms - Write query P99: < 50ms - Transaction throughput: 15,000 TPS - Replication lag: < 500ms --- ### Redis Caching Layer **Cluster Configuration**: - Redis Cluster with 6 nodes (3 primary + 3 replica) - Instance type: cache.r6g.xlarge (4 vCPU, 26GB RAM) - Total memory: 78GB usable cache space - Persistence: RDB snapshots every 5 minutes + AOF - Eviction policy: allkeys-lru **Cache Usage Patterns**: 1.",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_8",
              "content": "### Cache Invalidation Strategies **Time-Based Expiration (TTL)**: - Short-lived data: 5-15 minutes (session tokens, rate limit counters) - Medium-lived data: 1 hour (workflow definitions, templates) - Long-lived data: 24 hours (static configuration) **Event-Based Invalidation**: ``` Database Update Event \u2192 Kafka (cache.invalidation topic) \u2502 \u25bc All service instances consume event \u2502 \u25bc Redis DEL for affected keys ``` **Pattern-Based Invalidation**: - Use Redis SCAN + DEL for wildcard patterns - Example: Invalidate all user caches: `user:*:{user_id}` **Cache Warming**: - Scheduled job runs every hour to pre-populate frequently accessed data - Targets: Top 1000 workflows, system configuration, active user profiles - Reduces cache miss rate during traffic spikes ### Multi-Level Caching Application implements L1 (in-memory) and L2 (Redis) caching: ``` Request \u2192 L1 Cache (Node.js Map, 1000 entry LRU) \u2502 MISS \u2502 \u25bc L2 Cache (Redis, distributed) \u2502 MISS \u2502 \u25bc Database (PostgreSQL) ``` L1 cache reduces Redis network calls by 40% for hot data. --- ## Security Architecture ### Network Security **Zero-Trust Network Model**: - All service-to-service communication encrypted with mTLS - Certificate rotation: Every 90 days (automated via cert-manager) - Certificate authority: Internal PKI with HashiCorp Vault - Network segmentation: Private subnets for services, public subnet for ALB only **Service Mesh (Istio)**: ``` Service A \u2192 Envoy Sidecar (mTLS client cert) \u2502 Encrypted channel \u2502 Envoy Sidecar (mTLS server cert) \u2192 Service B ``` **Firewall Rules**: - Security groups: Deny all by default, explicit allow rules - Ingress: Only ALB can reach API Gateway (port 8080) - Egress: Services can only reach specific dependencies - Database access: Limited to application subnets only ### Authentication & Authorization **JWT Token Validation**: - Algorithm: RS256 (asymmetric signing) - Key rotation: Every 30 days with 7-day overlap period - Public key distribution: JWKS endpoint cached in Redis - Validation: Signature, expiry, issuer, audience claims - Token revocation: Blacklist in Redis for compromised tokens **Permission Model**: ``` User \u2192 Roles \u2192 Permissions \u2198 \u2197 Tenants (Multi-tenancy isolation) ``` Example permissions: - `workflow:read` - View workflows - `workflow:write` - Create/update workflows - `workflow:execute` - Trigger workflow execution - `workflow:delete` - Delete workflows - `admin:*` - All administrative operations **API Key Management**: - Format: `cfk_live_<32-char-random>` (production), `cfk_test_<32-char-random>` (sandbox) - Hashing: SHA-256 before storage in PostgreSQL - Scoping: API keys can be scoped to specific workflows or operations - Rate limits: Configurable",
              "score": null
            }
          ],
          "latency_ms": 1049.1977269994095,
          "query_id": "realistic_004",
          "dimension": "contextual",
          "query": "For capacity planning, what databases does CloudFlow rely on?"
        },
        {
          "key_facts": [
            "PostgreSQL 15.4",
            "Redis 7.2",
            "Apache Kafka 3.6"
          ],
          "found_facts": [
            "PostgreSQL 15.4",
            "Redis 7.2",
            "Apache Kafka 3.6"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "architecture_overview"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_12",
              "content": "Resume normal operations - Recovery time: 1-3 hours depending on data volume - Data loss: None if corruption detected quickly ### Testing & Validation **DR Drill Schedule**: - Monthly: Automated failover test (single AZ failure simulation) - Quarterly: Full DR region failover (non-production hours) - Annually: Complete disaster simulation with all stakeholders **Last DR Test Results** (Dec 15, 2025): - Scenario: Full region failover - Actual RTO: 2 hours 23 minutes (target: 4 hours) - Actual RPO: 42 minutes (target: 1 hour) - Issues identified: DNS propagation slower than expected (resolved) - Success criteria: Met all recovery objectives ### Business Continuity **Communication Plan**: - Status page: status.cloudflow.com (updated every 15 minutes during incident) - Customer notifications: Email + SMS for all P1 incidents - Internal escalation: PagerDuty \u2192 Incident Commander \u2192 Engineering Manager \u2192 CTO **Data Retention Policy**: - Active data: 12 months in hot storage (PostgreSQL) - Archived data: 7 years in cold storage (S3 Glacier) - Audit logs: 7 years (compliance requirement) - Backup retention: 30 days standard, 90 days monthly snapshots --- ## Appendix ### Service Dependency Matrix | Service | Depends On | Critical Path | |---------|-----------|---------------| | API Gateway | Auth Service, Redis | Yes | | Auth Service | PostgreSQL, Redis, Vault | Yes | | Workflow Engine | PostgreSQL, Kafka, Redis | Yes | | Scheduler | PostgreSQL, Redis, Kafka | No | | Notification Service | Kafka, PostgreSQL, SendGrid, Twilio | No | ### Capacity Planning **Current Capacity** (Jan 2026): - Daily workflow executions: 2.5M - Active users: 150,000 - API requests per day: 50M - Database size: 1.2TB - Kafka throughput: 100K msg/sec peak **6-Month Projection** (Jul 2026): - Daily workflow executions: 4M (+60%) - Active users: 225,000 (+50%) - API requests per day: 80M (+60%) - Database size: 2TB (+67%) - Required scaling: +30% compute, +50% storage ### Contact Information - **Architecture Team**: architecture@cloudflow.internal - **On-Call Engineer**: PagerDuty escalation - **Security Team**: security@cloudflow.internal - **Documentation**: https://wiki.cloudflow.internal/architecture --- **Document Revision History**: - v2.3.1 (Jan 15, 2026): Updated performance metrics, added DR test results - v2.3.0 (Dec 1, 2025): Added multi-AZ deployment details - v2.2.0 (Oct 15, 2025): Security architecture overhaul - v2.1.0 (Sep 1, 2025): Initial Kafka migration documentation - v2.0.0 (Jul 1, 2025): Complete microservices rewrite",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_0",
              "content": "# CloudFlow Platform - System Architecture Overview **Document Version:** 2.3.1 **Last Updated:** January 15, 2026 **Owner:** Platform Architecture Team **Status:** Production ## Executive Summary CloudFlow is a distributed, cloud-native workflow automation platform designed to orchestrate complex business processes at scale. The platform processes over 2.5 million workflow executions daily with an average P99 latency of 180ms for API operations and 4.2 seconds for workflow execution. This document provides a comprehensive overview of the system architecture, including microservices design, data flow patterns, infrastructure decisions, and operational considerations. ## Table of Contents 1. [High-Level Architecture](#high-level-architecture) 2. [Microservices Breakdown](#microservices-breakdown) 3. [Data Flow Architecture](#data-flow-architecture) 4. [Database Architecture](#database-architecture) 5. [Message Queue Patterns](#message-queue-patterns) 6. [Caching Strategy](#caching-strategy) 7. [Security Architecture](#security-architecture) 8. [Performance Characteristics](#performance-characteristics) 9. [Disaster Recovery](#disaster-recovery) --- ## High-Level Architecture CloudFlow follows a microservices architecture pattern deployed across multiple availability zones in AWS. The platform is designed with the following core principles: - **Scalability**: Horizontal scaling for all services with auto-scaling groups - **Resilience**: Circuit breakers, bulkheads, and graceful degradation - **Observability**: Distributed tracing, centralized logging, and comprehensive metrics - **Security**: Zero-trust network architecture with mTLS encryption ### Architecture Diagram (Conceptual) ``` \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 Load Balancer (ALB) \u2502 \u2502 (TLS Termination - 443) \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u25bc \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 API Gateway Layer \u2502 \u2502 (Rate Limiting, Auth, Request Routing) \u2502 \u2514\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u2502 \u2502 \u2502 \u2502 \u25bc \u25bc \u25bc \u25bc \u25bc \u250c\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502Auth\u2502 \u2502Workflow\u2502 \u2502Scheduler\u2502 \u2502Notification\u2502 \u2502User Service\u2502 \u2502Svc \u2502 \u2502 Engine \u2502 \u2502 Service \u2502 \u2502 Service \u2502 \u2502 \u2502 \u2514\u2500\u252c\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u2502 \u2502 \u2502 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 \u2502 \u2502 \u25bc \u25bc \u25bc \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502PostgreSQL\u2502 \u2502 Redis \u2502 \u2502 Kafka \u2502 \u2502 Cluster \u2502 \u2502 Cluster \u2502 \u2502 Cluster \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 ``` ### Technology Stack - **Runtime**: Node.js 20.x (API Gateway, Workflow Engine), Go 1.21 (Auth Service, Scheduler) - **Container Orchestration**: Kubernetes 1.28 (EKS) - **Service Mesh**: Istio 1.20 for service-to-service communication - **Databases**: PostgreSQL 15.4, Redis 7.2 - **Message Broker**: Apache Kafka 3.6 - **Monitoring**: Prometheus, Grafana, Jaeger for distributed tracing - **Secrets Management**: HashiCorp Vault 1.15 --- ## Microservices Breakdown ### API Gateway **Purpose**: Single entry point for all client requests, providing authentication, rate limiting, request routing, and protocol translation.",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_4",
              "content": "**Technology**: Node.js with worker pool pattern **Replicas**: 8 pods (production), auto-scaling 6-16 **Resource Allocation**: 2 vCPU, 4GB RAM per pod **Key Responsibilities**: - Consume notification events from Kafka - Template rendering with Handlebars (cached in Redis) - Multi-channel delivery (Email via SendGrid, SMS via Twilio, Webhooks) - Retry logic with dead letter queue for failed deliveries - Delivery status tracking and analytics - User preference management (opt-in/opt-out) **Notification Flow**: ``` Event Source \u2192 Kafka (notifications.email topic) \u2502 \u25bc Notification Service Consumer \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u25bc \u25bc \u25bc [Email Queue] [SMS Queue] [Webhook Queue] \u2502 \u2502 \u2502 \u25bc \u25bc \u25bc [SendGrid] [Twilio] [HTTP POST] \u2502 \u2502 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u25bc Delivery Status \u2192 PostgreSQL ``` **Channel Configuration**: - Email: SendGrid with IP warming, DKIM/SPF configured - SMS: Twilio with fallback to AWS SNS - Webhook: Signed payloads (HMAC-SHA256), retry 3 times with exponential backoff - In-app: WebSocket connections via Socket.io **Template Management**: - Templates stored in PostgreSQL with versioning - Compiled templates cached in Redis (1-hour TTL) - Support for localization (i18n) with 12 languages - A/B testing capability for email campaigns **Performance Targets**: - Email delivery: < 5 seconds (P99) from event to SendGrid - SMS delivery: < 3 seconds (P99) - Throughput: 10,000 notifications per minute --- ## Data Flow Architecture ### Synchronous Request Flow Client requests follow a synchronous path through the API Gateway to backend services: ``` 1. Client \u2192 Load Balancer (TLS termination) 2. Load Balancer \u2192 API Gateway (HTTP/2) 3. API Gateway \u2192 Auth Service (JWT validation via gRPC) 4. API Gateway \u2192 Target Service (HTTP/REST or gRPC) 5. Target Service \u2192 Database/Cache (data retrieval) 6. Response propagates back through the chain ``` **Timeout Configuration**: - Client \u2192 Load Balancer: 60 seconds - Load Balancer \u2192 API Gateway: 55 seconds - API Gateway \u2192 Services: 45 seconds - Service \u2192 Database: 10 seconds ### Asynchronous Event Flow Long-running operations and inter-service communication use event-driven patterns: ``` 1. Service publishes event \u2192 Kafka topic 2. Kafka persists event (replication factor: 3) 3. Consumer groups subscribe to topics 4. Consumers process events with at-least-once delivery 5. State updates written to PostgreSQL 6.",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_5",
              "content": "Success/failure events published back to Kafka ``` **Event Schema**: ```json { \"event_id\": \"uuid-v4\", \"event_type\": \"workflow.execution.completed\", \"timestamp\": \"2026-01-15T10:30:00.000Z\", \"correlation_id\": \"request-trace-id\", \"payload\": { \"workflow_id\": \"wf-12345\", \"execution_id\": \"exec-67890\", \"status\": \"COMPLETED\", \"duration_ms\": 4230 }, \"metadata\": { \"source_service\": \"workflow-engine\", \"schema_version\": \"1.0\" } } ``` ### Inter-Service Communication Services communicate using two primary patterns: **Synchronous (gRPC)**: - Auth Service validation calls from API Gateway - User Service profile lookups - Low latency requirement (< 50ms) - Request-response pattern with circuit breaker **Asynchronous (Kafka)**: - Workflow execution events - Notification triggers - Audit log events - Eventual consistency acceptable --- ## Database Architecture ### PostgreSQL Primary Database **Cluster Configuration**: - Primary-replica setup with 1 primary + 2 read replicas - Instance type: db.r6g.2xlarge (8 vCPU, 64GB RAM) - Storage: 2TB gp3 SSD with 12,000 IOPS - Multi-AZ deployment for high availability - Automated backups: Daily snapshots, 30-day retention - Point-in-time recovery: 5-minute granularity **Database Schema Design**: ``` Core Tables: - users (5M rows): User accounts and profiles - workflows (2M rows): Workflow definitions - workflow_executions (500M rows, partitioned): Execution history - workflow_steps (2B rows, partitioned): Individual step records - schedules (100K rows): Scheduled workflow triggers - notifications (1B rows, partitioned): Notification delivery log Partitioning Strategy: - workflow_executions: Monthly partitions by created_at - workflow_steps: Monthly partitions by created_at - notifications: Weekly partitions by created_at - Automatic partition creation via cron job - Retention policy: Drop partitions older than 12 months ``` **Indexing Strategy**: - Primary keys: UUIDs with B-tree indexes - Frequently queried columns: Compound indexes (e.g., user_id + status + created_at) - JSONB columns: GIN indexes for workflow definitions - Full-text search: GiST indexes on description fields **Connection Pooling**: - PgBouncer in transaction mode - Pool size: 100 connections per service - Max client connections: 2000 - Connection timeout: 30 seconds **Performance Characteristics**: - Read query P99: < 20ms - Write query P99: < 50ms - Transaction throughput: 15,000 TPS - Replication lag: < 500ms --- ### Redis Caching Layer **Cluster Configuration**: - Redis Cluster with 6 nodes (3 primary + 3 replica) - Instance type: cache.r6g.xlarge (4 vCPU, 26GB RAM) - Total memory: 78GB usable cache space - Persistence: RDB snapshots every 5 minutes + AOF - Eviction policy: allkeys-lru **Cache Usage Patterns**: 1.",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_8",
              "content": "### Cache Invalidation Strategies **Time-Based Expiration (TTL)**: - Short-lived data: 5-15 minutes (session tokens, rate limit counters) - Medium-lived data: 1 hour (workflow definitions, templates) - Long-lived data: 24 hours (static configuration) **Event-Based Invalidation**: ``` Database Update Event \u2192 Kafka (cache.invalidation topic) \u2502 \u25bc All service instances consume event \u2502 \u25bc Redis DEL for affected keys ``` **Pattern-Based Invalidation**: - Use Redis SCAN + DEL for wildcard patterns - Example: Invalidate all user caches: `user:*:{user_id}` **Cache Warming**: - Scheduled job runs every hour to pre-populate frequently accessed data - Targets: Top 1000 workflows, system configuration, active user profiles - Reduces cache miss rate during traffic spikes ### Multi-Level Caching Application implements L1 (in-memory) and L2 (Redis) caching: ``` Request \u2192 L1 Cache (Node.js Map, 1000 entry LRU) \u2502 MISS \u2502 \u25bc L2 Cache (Redis, distributed) \u2502 MISS \u2502 \u25bc Database (PostgreSQL) ``` L1 cache reduces Redis network calls by 40% for hot data. --- ## Security Architecture ### Network Security **Zero-Trust Network Model**: - All service-to-service communication encrypted with mTLS - Certificate rotation: Every 90 days (automated via cert-manager) - Certificate authority: Internal PKI with HashiCorp Vault - Network segmentation: Private subnets for services, public subnet for ALB only **Service Mesh (Istio)**: ``` Service A \u2192 Envoy Sidecar (mTLS client cert) \u2502 Encrypted channel \u2502 Envoy Sidecar (mTLS server cert) \u2192 Service B ``` **Firewall Rules**: - Security groups: Deny all by default, explicit allow rules - Ingress: Only ALB can reach API Gateway (port 8080) - Egress: Services can only reach specific dependencies - Database access: Limited to application subnets only ### Authentication & Authorization **JWT Token Validation**: - Algorithm: RS256 (asymmetric signing) - Key rotation: Every 30 days with 7-day overlap period - Public key distribution: JWKS endpoint cached in Redis - Validation: Signature, expiry, issuer, audience claims - Token revocation: Blacklist in Redis for compromised tokens **Permission Model**: ``` User \u2192 Roles \u2192 Permissions \u2198 \u2197 Tenants (Multi-tenancy isolation) ``` Example permissions: - `workflow:read` - View workflows - `workflow:write` - Create/update workflows - `workflow:execute` - Trigger workflow execution - `workflow:delete` - Delete workflows - `admin:*` - All administrative operations **API Key Management**: - Format: `cfk_live_<32-char-random>` (production), `cfk_test_<32-char-random>` (sandbox) - Hashing: SHA-256 before storage in PostgreSQL - Scoping: API keys can be scoped to specific workflows or operations - Rate limits: Configurable",
              "score": null
            }
          ],
          "latency_ms": 1028.6821439985943,
          "query_id": "realistic_004",
          "dimension": "negation",
          "query": "Is CloudFlow using MySQL or something else?"
        },
        {
          "key_facts": [
            "cloudflow-prod"
          ],
          "found_facts": [
            "cloudflow-prod"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "deployment_guide"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_0",
              "content": "# CloudFlow Platform - Deployment and Operations Guide **Version:** 2.4.0 **Last Updated:** January 2026 **Target Environment:** Production (AWS EKS) ## Table of Contents 1. [Overview](#overview) 2. [Prerequisites](#prerequisites) 3. [Infrastructure Setup](#infrastructure-setup) 4. [Kubernetes Deployment](#kubernetes-deployment) 5. [Database Configuration](#database-configuration) 6. [Monitoring and Observability](#monitoring-and-observability) 7. [Backup and Disaster Recovery](#backup-and-disaster-recovery) 8. [Scaling and Performance](#scaling-and-performance) 9. [Security and Compliance](#security-and-compliance) 10. [Troubleshooting](#troubleshooting) --- ## Overview CloudFlow is a cloud-native workflow orchestration platform designed for high-availability production environments. This guide provides comprehensive instructions for deploying and operating CloudFlow on Amazon EKS (Elastic Kubernetes Service). ### Architecture Summary CloudFlow consists of the following components: - **API Server**: REST API for workflow management (Node.js/Express) - **Worker Service**: Background job processor (Node.js) - **Scheduler**: Cron-based task scheduler (Node.js) - **PostgreSQL**: Primary data store (version 14) - **Redis**: Cache and message queue (version 7.0) - **PgBouncer**: Database connection pooler ### Deployment Model - **Namespace**: `cloudflow-prod` - **Cluster**: AWS EKS 1.28 - **Region**: us-east-1 (primary), us-west-2 (disaster recovery) - **High Availability**: Multi-AZ deployment across 3 availability zones --- ## Prerequisites Before beginning the deployment process, ensure you have the following: ### Required Tools - `kubectl` (v1.28 or later) - `helm` (v3.12 or later) - `aws-cli` (v2.13 or later) - `eksctl` (v0.165 or later) - `terraform` (v1.6 or later) - for infrastructure provisioning ### Access Requirements - AWS account with appropriate IAM permissions - EKS cluster admin access - Container registry access (ECR) - Domain name and SSL certificates - Secrets management access (AWS Secrets Manager or Vault) ### Network Requirements - VPC with at least 3 public and 3 private subnets - NAT Gateway configured - Security groups allowing: - Ingress: HTTPS (443), HTTP (80) - Internal: PostgreSQL (5432), Redis (6379) - Monitoring: Prometheus (9090), Grafana (3000) --- ## Infrastructure Setup ### EKS Cluster Creation Use the following `eksctl` configuration to create the EKS cluster: ```yaml # cluster-config.yaml apiVersion: eksctl.io/v1alpha5 kind: ClusterConfig metadata: name: cloudflow-production region: us-east-1 version: \"1.28\" availabilityZones: - us-east-1a - us-east-1b - us-east-1c vpc: cidr: 10.0.0.0/16 nat: gateway: HighlyAvailable managedNodeGroups: - name: cloudflow-workers instanceType: m5.xlarge minSize: 3 maxSize: 10 desiredCapacity: 5 volumeSize: 100 privateNetworking: true labels: role: worker environment: production tags: Environment: production Application: cloudflow iam: withAddonPolicies: ebs: true efs: true albIngress: true cloudWatch: true cloudWatch: clusterLogging: enableTypes: - api - audit - authenticator - controllerManager - scheduler ``` Create the cluster: ```bash eksctl",
              "score": null
            },
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_1",
              "content": "create cluster -f cluster-config.yaml ``` ### Storage Configuration Install the EBS CSI driver for persistent volumes: ```bash eksctl create addon --name aws-ebs-csi-driver \\ --cluster cloudflow-production \\ --service-account-role-arn arn:aws:iam::ACCOUNT_ID:role/AmazonEKS_EBS_CSI_DriverRole \\ --force ``` --- ## Kubernetes Deployment ### Namespace Setup Create the CloudFlow namespace and configure resource quotas: ```yaml # namespace.yaml apiVersion: v1 kind: Namespace metadata: name: cloudflow-prod labels: name: cloudflow-prod environment: production --- apiVersion: v1 kind: ResourceQuota metadata: name: cloudflow-quota namespace: cloudflow-prod spec: hard: requests.cpu: \"50\" requests.memory: 100Gi persistentvolumeclaims: \"10\" services.loadbalancers: \"2\" ``` Apply the namespace configuration: ```bash kubectl apply -f namespace.yaml ``` ### Environment Configuration CloudFlow requires the following environment variables: | Variable | Description | Example | Required | |----------|-------------|---------|----------| | `DATABASE_URL` | PostgreSQL connection string | `postgresql://user:pass@host:5432/cloudflow` | Yes | | `REDIS_URL` | Redis connection string | `redis://redis-master.cloudflow-prod.svc.cluster.local:6379` | Yes | | `JWT_SECRET` | Secret key for JWT token signing | `<generated-secret-256-bit>` | Yes | | `LOG_LEVEL` | Application log verbosity | `info`, `debug`, `warn`, `error` | No (default: `info`) | | `API_PORT` | API server port | `3000` | No (default: `3000`) | | `WORKER_CONCURRENCY` | Number of concurrent workers | `10` | No (default: `5`) | | `SESSION_SECRET` | Session encryption key | `<generated-secret-256-bit>` | Yes | | `AWS_REGION` | AWS region for services | `us-east-1` | Yes | | `S3_BUCKET` | S3 bucket for file storage | `cloudflow-prod-storage` | Yes | | `SMTP_HOST` | Email server hostname | `smtp.sendgrid.net` | No | | `SMTP_PORT` | Email server port | `587` | No | | `METRICS_ENABLED` | Enable Prometheus metrics | `true` | No (default: `false`) | Create a Kubernetes secret for sensitive variables: ```bash kubectl create secret generic cloudflow-secrets \\ --namespace cloudflow-prod \\ --from-literal=DATABASE_URL=\"postgresql://cloudflow:$(cat db-password.txt)@pgbouncer.cloudflow-prod.svc.cluster.local:5432/cloudflow\" \\ --from-literal=REDIS_URL=\"redis://redis-master.cloudflow-prod.svc.cluster.local:6379\" \\ --from-literal=JWT_SECRET=\"$(openssl rand -base64 32)\" \\ --from-literal=SESSION_SECRET=\"$(openssl rand -base64 32)\" ``` ### Helm Chart Deployment Add the CloudFlow Helm repository: ```bash helm repo add cloudflow https://charts.cloudflow.io helm repo update ``` Create a values file for production configuration: ```yaml # values-production.yaml replicaCount: 3 image: repository: 123456789012.dkr.ecr.us-east-1.amazonaws.com/cloudflow tag: \"2.4.0\" pullPolicy: IfNotPresent resources: limits: cpu: 2000m memory: 4Gi requests: cpu: 2000m memory: 4Gi autoscaling: enabled: true minReplicas: 3 maxReplicas: 10 targetCPUUtilizationPercentage: 70 targetMemoryUtilizationPercentage: 80 service: type: ClusterIP port: 80 targetPort: 3000 annotations: service.beta.kubernetes.io/aws-load-balancer-type: \"nlb\" ingress: enabled: true className: nginx annotations: cert-manager.io/cluster-issuer: \"letsencrypt-prod\" nginx.ingress.kubernetes.io/ssl-redirect: \"true\" hosts: - host: api.cloudflow.io paths: - path: / pathType: Prefix tls:",
              "score": null
            },
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_7",
              "content": "podSelector: matchLabels: app: cloudflow policyTypes: - Ingress - Egress ingress: - from: - namespaceSelector: matchLabels: name: cloudflow-prod - podSelector: matchLabels: app: nginx-ingress ports: - protocol: TCP port: 3000 egress: - to: - podSelector: matchLabels: app: postgresql ports: - protocol: TCP port: 5432 - to: - podSelector: matchLabels: app: redis ports: - protocol: TCP port: 6379 ``` ### Pod Security Standards Enforce Pod Security Standards at the namespace level: ```bash kubectl label namespace cloudflow-prod \\ pod-security.kubernetes.io/enforce=restricted \\ pod-security.kubernetes.io/audit=restricted \\ pod-security.kubernetes.io/warn=restricted ``` ### Secrets Management Use AWS Secrets Manager for sensitive credentials: ```bash # Install External Secrets Operator helm install external-secrets external-secrets/external-secrets \\ --namespace external-secrets-system \\ --create-namespace # Create SecretStore kubectl apply -f - <<EOF apiVersion: external-secrets.io/v1beta1 kind: SecretStore metadata: name: aws-secrets-manager namespace: cloudflow-prod spec: provider: aws: service: SecretsManager region: us-east-1 auth: jwt: serviceAccountRef: name: external-secrets-sa EOF ``` --- ## Troubleshooting ### Common Issues #### Pods in CrashLoopBackOff **Symptoms**: Pods continuously restart **Diagnosis**: ```bash kubectl logs -n cloudflow-prod <pod-name> --previous kubectl describe pod -n cloudflow-prod <pod-name> ``` **Common Causes**: - Database connection failure - Invalid environment variables - Insufficient resources #### High Memory Usage **Symptoms**: Pods being OOMKilled **Diagnosis**: ```bash kubectl top pods -n cloudflow-prod ``` **Resolution**: - Increase memory limits in deployment - Check for memory leaks in application logs - Review heap dump for Node.js processes #### Database Connection Pool Exhausted **Symptoms**: \"Too many connections\" errors **Diagnosis**: ```bash # Check PgBouncer stats kubectl exec -it pgbouncer-xxx -n cloudflow-prod -- \\ psql -p 5432 -U pgbouncer pgbouncer -c \"SHOW POOLS;\" ``` **Resolution**: - Increase `max_db_connections` in PgBouncer - Optimize application connection usage - Add more PostgreSQL replicas ### Support Contacts - **On-call Engineer**: pagerduty.com/cloudflow-oncall - **Slack Channel**: #cloudflow-operations - **Documentation**: https://docs.cloudflow.io - **Status Page**: https://status.cloudflow.io --- ## Appendix ### Useful Commands ```bash # View all resources in namespace kubectl get all -n cloudflow-prod # Check resource usage kubectl top pods -n cloudflow-prod kubectl top nodes # View logs kubectl logs -f deployment/cloudflow-api -n cloudflow-prod # Execute command in pod kubectl exec -it <pod-name> -n cloudflow-prod -- /bin/sh # Port forward for local testing kubectl port-forward svc/cloudflow-api 8080:80 -n cloudflow-prod # Rollback deployment helm rollback cloudflow -n cloudflow-prod # Update deployment helm upgrade cloudflow cloudflow/cloudflow \\ --namespace cloudflow-prod \\ --values values-production.yaml ``` ### Maintenance Windows Scheduled maintenance occurs during the following windows: - **Primary**: Sunday 02:00-06:00",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_8",
              "content": "Leverage caching:** ```bash # Enable client-side caching export CLOUDFLOW_CACHE_ENABLED=true export CLOUDFLOW_CACHE_TTL=300 # Cache workflow metadata cloudflow workflows list --use-cache --cache-ttl 600 ``` --- ## Log Analysis & Debugging ### Accessing CloudFlow Logs #### Kubernetes Deployments ```bash # List all CloudFlow pods kubectl get pods -n cloudflow # Tail logs from API server kubectl logs -f -n cloudflow deployment/cloudflow-api --tail=100 # Get logs from specific pod kubectl logs -n cloudflow cloudflow-api-7d4f6b8c9d-x7k2m # Get logs from previous crashed pod kubectl logs -n cloudflow cloudflow-api-7d4f6b8c9d-x7k2m --previous # Get logs from all pods in deployment kubectl logs -n cloudflow deployment/cloudflow-api --all-containers=true # Stream logs from multiple pods kubectl logs -n cloudflow -l app=cloudflow-api -f --max-log-requests=10 ``` #### Log Levels CloudFlow supports the following log levels: - `TRACE` - Very detailed debugging information - `DEBUG` - Detailed debugging information - `INFO` - Informational messages (default) - `WARN` - Warning messages - `ERROR` - Error messages - `FATAL` - Fatal errors causing shutdown **Changing log levels:** ```bash # Set global log level cloudflow config set logging.level DEBUG # Set log level for specific component cloudflow config set logging.components.database DEBUG cloudflow config set logging.components.auth INFO cloudflow config set logging.components.workflows TRACE # Temporary log level increase (resets after 1 hour) cloudflow debug set-log-level DEBUG --duration 3600 # View current log configuration cloudflow config get logging --format json ``` ### Grep Patterns for Common Issues #### Authentication Failures ```bash # Find all authentication errors kubectl logs -n cloudflow deployment/cloudflow-api | \\ grep -i \"authentication\\|401\\|unauthorized\" | \\ tail -n 50 # Find JWT validation failures kubectl logs -n cloudflow deployment/cloudflow-api | \\ grep -E \"JWT|token.*invalid|signature.*failed\" # Find clock skew issues kubectl logs -n cloudflow deployment/cloudflow-api | \\ grep -i \"clock skew\\|time.*sync\\|nbf\\|exp\" ``` #### Database Errors ```bash # Find database connection errors kubectl logs -n cloudflow deployment/cloudflow-api | \\ grep -E \"connection.*pool|could not connect|database.*timeout\" # Find slow queries kubectl logs -n cloudflow deployment/cloudflow-api | \\ grep \"slow query\" | \\ awk '{print $NF}' | \\ sort -n | \\ tail -n 20 # Find deadlock errors kubectl logs -n cloudflow deployment/cloudflow-api | \\ grep -i \"deadlock detected\" ``` #### Workflow Execution Errors ```bash # Find workflow timeout errors kubectl logs -n cloudflow deployment/cloudflow-workflow-engine | \\ grep -E \"timeout|exceeded.*3600\" # Find workflow retry attempts kubectl logs -n cloudflow deployment/cloudflow-workflow-engine | \\ grep -E \"retry attempt [0-9]|retrying",
              "score": null
            },
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_2",
              "content": "- secretName: cloudflow-tls hosts: - api.cloudflow.io healthCheck: enabled: true livenessProbe: path: /health initialDelaySeconds: 30 periodSeconds: 10 timeoutSeconds: 5 failureThreshold: 3 readinessProbe: path: /ready initialDelaySeconds: 10 periodSeconds: 5 timeoutSeconds: 3 failureThreshold: 3 env: - name: NODE_ENV value: \"production\" - name: LOG_LEVEL value: \"info\" - name: API_PORT value: \"3000\" - name: WORKER_CONCURRENCY value: \"10\" - name: AWS_REGION value: \"us-east-1\" - name: METRICS_ENABLED value: \"true\" envFrom: - secretRef: name: cloudflow-secrets podAnnotations: prometheus.io/scrape: \"true\" prometheus.io/port: \"9090\" prometheus.io/path: \"/metrics\" podSecurityContext: runAsNonRoot: true runAsUser: 1000 fsGroup: 1000 securityContext: allowPrivilegeEscalation: false capabilities: drop: - ALL readOnlyRootFilesystem: true persistence: enabled: true storageClass: gp3 accessMode: ReadWriteOnce size: 50Gi redis: enabled: true architecture: replication auth: enabled: true master: persistence: size: 20Gi replica: replicaCount: 2 persistence: size: 20Gi ``` Deploy CloudFlow using Helm: ```bash helm install cloudflow cloudflow/cloudflow \\ --namespace cloudflow-prod \\ --values values-production.yaml \\ --wait \\ --timeout 10m ``` ### Deployment Verification Verify the deployment status: ```bash # Check pod status kubectl get pods -n cloudflow-prod # Expected output: # NAME READY STATUS RESTARTS AGE # cloudflow-api-7d8f9c5b6d-4xk2p 1/1 Running 0 2m # cloudflow-api-7d8f9c5b6d-9hj5m 1/1 Running 0 2m # cloudflow-api-7d8f9c5b6d-tn8wq 1/1 Running 0 2m # Check service endpoints kubectl get svc -n cloudflow-prod # Check ingress kubectl get ingress -n cloudflow-prod ``` Test health endpoints: ```bash # Port forward for testing kubectl port-forward -n cloudflow-prod svc/cloudflow-api 8080:80 # Test health endpoint curl http://localhost:8080/health # Expected: {\"status\":\"healthy\",\"timestamp\":\"2026-01-24T10:30:00Z\"} # Test readiness endpoint curl http://localhost:8080/ready # Expected: {\"status\":\"ready\",\"dependencies\":{\"database\":\"connected\",\"redis\":\"connected\"}} ``` --- ## Database Configuration ### PostgreSQL Setup CloudFlow uses PostgreSQL 14 as its primary data store.",
              "score": null
            }
          ],
          "latency_ms": 885.8362749997468,
          "query_id": "realistic_005",
          "dimension": "original",
          "query": "What is the Kubernetes namespace for production deployment?"
        },
        {
          "key_facts": [
            "cloudflow-prod"
          ],
          "found_facts": [
            "cloudflow-prod"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "deployment_guide"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_1",
              "content": "create cluster -f cluster-config.yaml ``` ### Storage Configuration Install the EBS CSI driver for persistent volumes: ```bash eksctl create addon --name aws-ebs-csi-driver \\ --cluster cloudflow-production \\ --service-account-role-arn arn:aws:iam::ACCOUNT_ID:role/AmazonEKS_EBS_CSI_DriverRole \\ --force ``` --- ## Kubernetes Deployment ### Namespace Setup Create the CloudFlow namespace and configure resource quotas: ```yaml # namespace.yaml apiVersion: v1 kind: Namespace metadata: name: cloudflow-prod labels: name: cloudflow-prod environment: production --- apiVersion: v1 kind: ResourceQuota metadata: name: cloudflow-quota namespace: cloudflow-prod spec: hard: requests.cpu: \"50\" requests.memory: 100Gi persistentvolumeclaims: \"10\" services.loadbalancers: \"2\" ``` Apply the namespace configuration: ```bash kubectl apply -f namespace.yaml ``` ### Environment Configuration CloudFlow requires the following environment variables: | Variable | Description | Example | Required | |----------|-------------|---------|----------| | `DATABASE_URL` | PostgreSQL connection string | `postgresql://user:pass@host:5432/cloudflow` | Yes | | `REDIS_URL` | Redis connection string | `redis://redis-master.cloudflow-prod.svc.cluster.local:6379` | Yes | | `JWT_SECRET` | Secret key for JWT token signing | `<generated-secret-256-bit>` | Yes | | `LOG_LEVEL` | Application log verbosity | `info`, `debug`, `warn`, `error` | No (default: `info`) | | `API_PORT` | API server port | `3000` | No (default: `3000`) | | `WORKER_CONCURRENCY` | Number of concurrent workers | `10` | No (default: `5`) | | `SESSION_SECRET` | Session encryption key | `<generated-secret-256-bit>` | Yes | | `AWS_REGION` | AWS region for services | `us-east-1` | Yes | | `S3_BUCKET` | S3 bucket for file storage | `cloudflow-prod-storage` | Yes | | `SMTP_HOST` | Email server hostname | `smtp.sendgrid.net` | No | | `SMTP_PORT` | Email server port | `587` | No | | `METRICS_ENABLED` | Enable Prometheus metrics | `true` | No (default: `false`) | Create a Kubernetes secret for sensitive variables: ```bash kubectl create secret generic cloudflow-secrets \\ --namespace cloudflow-prod \\ --from-literal=DATABASE_URL=\"postgresql://cloudflow:$(cat db-password.txt)@pgbouncer.cloudflow-prod.svc.cluster.local:5432/cloudflow\" \\ --from-literal=REDIS_URL=\"redis://redis-master.cloudflow-prod.svc.cluster.local:6379\" \\ --from-literal=JWT_SECRET=\"$(openssl rand -base64 32)\" \\ --from-literal=SESSION_SECRET=\"$(openssl rand -base64 32)\" ``` ### Helm Chart Deployment Add the CloudFlow Helm repository: ```bash helm repo add cloudflow https://charts.cloudflow.io helm repo update ``` Create a values file for production configuration: ```yaml # values-production.yaml replicaCount: 3 image: repository: 123456789012.dkr.ecr.us-east-1.amazonaws.com/cloudflow tag: \"2.4.0\" pullPolicy: IfNotPresent resources: limits: cpu: 2000m memory: 4Gi requests: cpu: 2000m memory: 4Gi autoscaling: enabled: true minReplicas: 3 maxReplicas: 10 targetCPUUtilizationPercentage: 70 targetMemoryUtilizationPercentage: 80 service: type: ClusterIP port: 80 targetPort: 3000 annotations: service.beta.kubernetes.io/aws-load-balancer-type: \"nlb\" ingress: enabled: true className: nginx annotations: cert-manager.io/cluster-issuer: \"letsencrypt-prod\" nginx.ingress.kubernetes.io/ssl-redirect: \"true\" hosts: - host: api.cloudflow.io paths: - path: / pathType: Prefix tls:",
              "score": null
            },
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_0",
              "content": "# CloudFlow Platform - Deployment and Operations Guide **Version:** 2.4.0 **Last Updated:** January 2026 **Target Environment:** Production (AWS EKS) ## Table of Contents 1. [Overview](#overview) 2. [Prerequisites](#prerequisites) 3. [Infrastructure Setup](#infrastructure-setup) 4. [Kubernetes Deployment](#kubernetes-deployment) 5. [Database Configuration](#database-configuration) 6. [Monitoring and Observability](#monitoring-and-observability) 7. [Backup and Disaster Recovery](#backup-and-disaster-recovery) 8. [Scaling and Performance](#scaling-and-performance) 9. [Security and Compliance](#security-and-compliance) 10. [Troubleshooting](#troubleshooting) --- ## Overview CloudFlow is a cloud-native workflow orchestration platform designed for high-availability production environments. This guide provides comprehensive instructions for deploying and operating CloudFlow on Amazon EKS (Elastic Kubernetes Service). ### Architecture Summary CloudFlow consists of the following components: - **API Server**: REST API for workflow management (Node.js/Express) - **Worker Service**: Background job processor (Node.js) - **Scheduler**: Cron-based task scheduler (Node.js) - **PostgreSQL**: Primary data store (version 14) - **Redis**: Cache and message queue (version 7.0) - **PgBouncer**: Database connection pooler ### Deployment Model - **Namespace**: `cloudflow-prod` - **Cluster**: AWS EKS 1.28 - **Region**: us-east-1 (primary), us-west-2 (disaster recovery) - **High Availability**: Multi-AZ deployment across 3 availability zones --- ## Prerequisites Before beginning the deployment process, ensure you have the following: ### Required Tools - `kubectl` (v1.28 or later) - `helm` (v3.12 or later) - `aws-cli` (v2.13 or later) - `eksctl` (v0.165 or later) - `terraform` (v1.6 or later) - for infrastructure provisioning ### Access Requirements - AWS account with appropriate IAM permissions - EKS cluster admin access - Container registry access (ECR) - Domain name and SSL certificates - Secrets management access (AWS Secrets Manager or Vault) ### Network Requirements - VPC with at least 3 public and 3 private subnets - NAT Gateway configured - Security groups allowing: - Ingress: HTTPS (443), HTTP (80) - Internal: PostgreSQL (5432), Redis (6379) - Monitoring: Prometheus (9090), Grafana (3000) --- ## Infrastructure Setup ### EKS Cluster Creation Use the following `eksctl` configuration to create the EKS cluster: ```yaml # cluster-config.yaml apiVersion: eksctl.io/v1alpha5 kind: ClusterConfig metadata: name: cloudflow-production region: us-east-1 version: \"1.28\" availabilityZones: - us-east-1a - us-east-1b - us-east-1c vpc: cidr: 10.0.0.0/16 nat: gateway: HighlyAvailable managedNodeGroups: - name: cloudflow-workers instanceType: m5.xlarge minSize: 3 maxSize: 10 desiredCapacity: 5 volumeSize: 100 privateNetworking: true labels: role: worker environment: production tags: Environment: production Application: cloudflow iam: withAddonPolicies: ebs: true efs: true albIngress: true cloudWatch: true cloudWatch: clusterLogging: enableTypes: - api - audit - authenticator - controllerManager - scheduler ``` Create the cluster: ```bash eksctl",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_8",
              "content": "Leverage caching:** ```bash # Enable client-side caching export CLOUDFLOW_CACHE_ENABLED=true export CLOUDFLOW_CACHE_TTL=300 # Cache workflow metadata cloudflow workflows list --use-cache --cache-ttl 600 ``` --- ## Log Analysis & Debugging ### Accessing CloudFlow Logs #### Kubernetes Deployments ```bash # List all CloudFlow pods kubectl get pods -n cloudflow # Tail logs from API server kubectl logs -f -n cloudflow deployment/cloudflow-api --tail=100 # Get logs from specific pod kubectl logs -n cloudflow cloudflow-api-7d4f6b8c9d-x7k2m # Get logs from previous crashed pod kubectl logs -n cloudflow cloudflow-api-7d4f6b8c9d-x7k2m --previous # Get logs from all pods in deployment kubectl logs -n cloudflow deployment/cloudflow-api --all-containers=true # Stream logs from multiple pods kubectl logs -n cloudflow -l app=cloudflow-api -f --max-log-requests=10 ``` #### Log Levels CloudFlow supports the following log levels: - `TRACE` - Very detailed debugging information - `DEBUG` - Detailed debugging information - `INFO` - Informational messages (default) - `WARN` - Warning messages - `ERROR` - Error messages - `FATAL` - Fatal errors causing shutdown **Changing log levels:** ```bash # Set global log level cloudflow config set logging.level DEBUG # Set log level for specific component cloudflow config set logging.components.database DEBUG cloudflow config set logging.components.auth INFO cloudflow config set logging.components.workflows TRACE # Temporary log level increase (resets after 1 hour) cloudflow debug set-log-level DEBUG --duration 3600 # View current log configuration cloudflow config get logging --format json ``` ### Grep Patterns for Common Issues #### Authentication Failures ```bash # Find all authentication errors kubectl logs -n cloudflow deployment/cloudflow-api | \\ grep -i \"authentication\\|401\\|unauthorized\" | \\ tail -n 50 # Find JWT validation failures kubectl logs -n cloudflow deployment/cloudflow-api | \\ grep -E \"JWT|token.*invalid|signature.*failed\" # Find clock skew issues kubectl logs -n cloudflow deployment/cloudflow-api | \\ grep -i \"clock skew\\|time.*sync\\|nbf\\|exp\" ``` #### Database Errors ```bash # Find database connection errors kubectl logs -n cloudflow deployment/cloudflow-api | \\ grep -E \"connection.*pool|could not connect|database.*timeout\" # Find slow queries kubectl logs -n cloudflow deployment/cloudflow-api | \\ grep \"slow query\" | \\ awk '{print $NF}' | \\ sort -n | \\ tail -n 20 # Find deadlock errors kubectl logs -n cloudflow deployment/cloudflow-api | \\ grep -i \"deadlock detected\" ``` #### Workflow Execution Errors ```bash # Find workflow timeout errors kubectl logs -n cloudflow deployment/cloudflow-workflow-engine | \\ grep -E \"timeout|exceeded.*3600\" # Find workflow retry attempts kubectl logs -n cloudflow deployment/cloudflow-workflow-engine | \\ grep -E \"retry attempt [0-9]|retrying",
              "score": null
            },
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_7",
              "content": "podSelector: matchLabels: app: cloudflow policyTypes: - Ingress - Egress ingress: - from: - namespaceSelector: matchLabels: name: cloudflow-prod - podSelector: matchLabels: app: nginx-ingress ports: - protocol: TCP port: 3000 egress: - to: - podSelector: matchLabels: app: postgresql ports: - protocol: TCP port: 5432 - to: - podSelector: matchLabels: app: redis ports: - protocol: TCP port: 6379 ``` ### Pod Security Standards Enforce Pod Security Standards at the namespace level: ```bash kubectl label namespace cloudflow-prod \\ pod-security.kubernetes.io/enforce=restricted \\ pod-security.kubernetes.io/audit=restricted \\ pod-security.kubernetes.io/warn=restricted ``` ### Secrets Management Use AWS Secrets Manager for sensitive credentials: ```bash # Install External Secrets Operator helm install external-secrets external-secrets/external-secrets \\ --namespace external-secrets-system \\ --create-namespace # Create SecretStore kubectl apply -f - <<EOF apiVersion: external-secrets.io/v1beta1 kind: SecretStore metadata: name: aws-secrets-manager namespace: cloudflow-prod spec: provider: aws: service: SecretsManager region: us-east-1 auth: jwt: serviceAccountRef: name: external-secrets-sa EOF ``` --- ## Troubleshooting ### Common Issues #### Pods in CrashLoopBackOff **Symptoms**: Pods continuously restart **Diagnosis**: ```bash kubectl logs -n cloudflow-prod <pod-name> --previous kubectl describe pod -n cloudflow-prod <pod-name> ``` **Common Causes**: - Database connection failure - Invalid environment variables - Insufficient resources #### High Memory Usage **Symptoms**: Pods being OOMKilled **Diagnosis**: ```bash kubectl top pods -n cloudflow-prod ``` **Resolution**: - Increase memory limits in deployment - Check for memory leaks in application logs - Review heap dump for Node.js processes #### Database Connection Pool Exhausted **Symptoms**: \"Too many connections\" errors **Diagnosis**: ```bash # Check PgBouncer stats kubectl exec -it pgbouncer-xxx -n cloudflow-prod -- \\ psql -p 5432 -U pgbouncer pgbouncer -c \"SHOW POOLS;\" ``` **Resolution**: - Increase `max_db_connections` in PgBouncer - Optimize application connection usage - Add more PostgreSQL replicas ### Support Contacts - **On-call Engineer**: pagerduty.com/cloudflow-oncall - **Slack Channel**: #cloudflow-operations - **Documentation**: https://docs.cloudflow.io - **Status Page**: https://status.cloudflow.io --- ## Appendix ### Useful Commands ```bash # View all resources in namespace kubectl get all -n cloudflow-prod # Check resource usage kubectl top pods -n cloudflow-prod kubectl top nodes # View logs kubectl logs -f deployment/cloudflow-api -n cloudflow-prod # Execute command in pod kubectl exec -it <pod-name> -n cloudflow-prod -- /bin/sh # Port forward for local testing kubectl port-forward svc/cloudflow-api 8080:80 -n cloudflow-prod # Rollback deployment helm rollback cloudflow -n cloudflow-prod # Update deployment helm upgrade cloudflow cloudflow/cloudflow \\ --namespace cloudflow-prod \\ --values values-production.yaml ``` ### Maintenance Windows Scheduled maintenance occurs during the following windows: - **Primary**: Sunday 02:00-06:00",
              "score": null
            },
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_2",
              "content": "- secretName: cloudflow-tls hosts: - api.cloudflow.io healthCheck: enabled: true livenessProbe: path: /health initialDelaySeconds: 30 periodSeconds: 10 timeoutSeconds: 5 failureThreshold: 3 readinessProbe: path: /ready initialDelaySeconds: 10 periodSeconds: 5 timeoutSeconds: 3 failureThreshold: 3 env: - name: NODE_ENV value: \"production\" - name: LOG_LEVEL value: \"info\" - name: API_PORT value: \"3000\" - name: WORKER_CONCURRENCY value: \"10\" - name: AWS_REGION value: \"us-east-1\" - name: METRICS_ENABLED value: \"true\" envFrom: - secretRef: name: cloudflow-secrets podAnnotations: prometheus.io/scrape: \"true\" prometheus.io/port: \"9090\" prometheus.io/path: \"/metrics\" podSecurityContext: runAsNonRoot: true runAsUser: 1000 fsGroup: 1000 securityContext: allowPrivilegeEscalation: false capabilities: drop: - ALL readOnlyRootFilesystem: true persistence: enabled: true storageClass: gp3 accessMode: ReadWriteOnce size: 50Gi redis: enabled: true architecture: replication auth: enabled: true master: persistence: size: 20Gi replica: replicaCount: 2 persistence: size: 20Gi ``` Deploy CloudFlow using Helm: ```bash helm install cloudflow cloudflow/cloudflow \\ --namespace cloudflow-prod \\ --values values-production.yaml \\ --wait \\ --timeout 10m ``` ### Deployment Verification Verify the deployment status: ```bash # Check pod status kubectl get pods -n cloudflow-prod # Expected output: # NAME READY STATUS RESTARTS AGE # cloudflow-api-7d8f9c5b6d-4xk2p 1/1 Running 0 2m # cloudflow-api-7d8f9c5b6d-9hj5m 1/1 Running 0 2m # cloudflow-api-7d8f9c5b6d-tn8wq 1/1 Running 0 2m # Check service endpoints kubectl get svc -n cloudflow-prod # Check ingress kubectl get ingress -n cloudflow-prod ``` Test health endpoints: ```bash # Port forward for testing kubectl port-forward -n cloudflow-prod svc/cloudflow-api 8080:80 # Test health endpoint curl http://localhost:8080/health # Expected: {\"status\":\"healthy\",\"timestamp\":\"2026-01-24T10:30:00Z\"} # Test readiness endpoint curl http://localhost:8080/ready # Expected: {\"status\":\"ready\",\"dependencies\":{\"database\":\"connected\",\"redis\":\"connected\"}} ``` --- ## Database Configuration ### PostgreSQL Setup CloudFlow uses PostgreSQL 14 as its primary data store.",
              "score": null
            }
          ],
          "latency_ms": 1047.4338770000031,
          "query_id": "realistic_005",
          "dimension": "synonym",
          "query": "Which namespace is used for CloudFlow production?"
        },
        {
          "key_facts": [
            "cloudflow-prod"
          ],
          "found_facts": [
            "cloudflow-prod"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "deployment_guide"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_8",
              "content": "Leverage caching:** ```bash # Enable client-side caching export CLOUDFLOW_CACHE_ENABLED=true export CLOUDFLOW_CACHE_TTL=300 # Cache workflow metadata cloudflow workflows list --use-cache --cache-ttl 600 ``` --- ## Log Analysis & Debugging ### Accessing CloudFlow Logs #### Kubernetes Deployments ```bash # List all CloudFlow pods kubectl get pods -n cloudflow # Tail logs from API server kubectl logs -f -n cloudflow deployment/cloudflow-api --tail=100 # Get logs from specific pod kubectl logs -n cloudflow cloudflow-api-7d4f6b8c9d-x7k2m # Get logs from previous crashed pod kubectl logs -n cloudflow cloudflow-api-7d4f6b8c9d-x7k2m --previous # Get logs from all pods in deployment kubectl logs -n cloudflow deployment/cloudflow-api --all-containers=true # Stream logs from multiple pods kubectl logs -n cloudflow -l app=cloudflow-api -f --max-log-requests=10 ``` #### Log Levels CloudFlow supports the following log levels: - `TRACE` - Very detailed debugging information - `DEBUG` - Detailed debugging information - `INFO` - Informational messages (default) - `WARN` - Warning messages - `ERROR` - Error messages - `FATAL` - Fatal errors causing shutdown **Changing log levels:** ```bash # Set global log level cloudflow config set logging.level DEBUG # Set log level for specific component cloudflow config set logging.components.database DEBUG cloudflow config set logging.components.auth INFO cloudflow config set logging.components.workflows TRACE # Temporary log level increase (resets after 1 hour) cloudflow debug set-log-level DEBUG --duration 3600 # View current log configuration cloudflow config get logging --format json ``` ### Grep Patterns for Common Issues #### Authentication Failures ```bash # Find all authentication errors kubectl logs -n cloudflow deployment/cloudflow-api | \\ grep -i \"authentication\\|401\\|unauthorized\" | \\ tail -n 50 # Find JWT validation failures kubectl logs -n cloudflow deployment/cloudflow-api | \\ grep -E \"JWT|token.*invalid|signature.*failed\" # Find clock skew issues kubectl logs -n cloudflow deployment/cloudflow-api | \\ grep -i \"clock skew\\|time.*sync\\|nbf\\|exp\" ``` #### Database Errors ```bash # Find database connection errors kubectl logs -n cloudflow deployment/cloudflow-api | \\ grep -E \"connection.*pool|could not connect|database.*timeout\" # Find slow queries kubectl logs -n cloudflow deployment/cloudflow-api | \\ grep \"slow query\" | \\ awk '{print $NF}' | \\ sort -n | \\ tail -n 20 # Find deadlock errors kubectl logs -n cloudflow deployment/cloudflow-api | \\ grep -i \"deadlock detected\" ``` #### Workflow Execution Errors ```bash # Find workflow timeout errors kubectl logs -n cloudflow deployment/cloudflow-workflow-engine | \\ grep -E \"timeout|exceeded.*3600\" # Find workflow retry attempts kubectl logs -n cloudflow deployment/cloudflow-workflow-engine | \\ grep -E \"retry attempt [0-9]|retrying",
              "score": null
            },
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_0",
              "content": "# CloudFlow Platform - Deployment and Operations Guide **Version:** 2.4.0 **Last Updated:** January 2026 **Target Environment:** Production (AWS EKS) ## Table of Contents 1. [Overview](#overview) 2. [Prerequisites](#prerequisites) 3. [Infrastructure Setup](#infrastructure-setup) 4. [Kubernetes Deployment](#kubernetes-deployment) 5. [Database Configuration](#database-configuration) 6. [Monitoring and Observability](#monitoring-and-observability) 7. [Backup and Disaster Recovery](#backup-and-disaster-recovery) 8. [Scaling and Performance](#scaling-and-performance) 9. [Security and Compliance](#security-and-compliance) 10. [Troubleshooting](#troubleshooting) --- ## Overview CloudFlow is a cloud-native workflow orchestration platform designed for high-availability production environments. This guide provides comprehensive instructions for deploying and operating CloudFlow on Amazon EKS (Elastic Kubernetes Service). ### Architecture Summary CloudFlow consists of the following components: - **API Server**: REST API for workflow management (Node.js/Express) - **Worker Service**: Background job processor (Node.js) - **Scheduler**: Cron-based task scheduler (Node.js) - **PostgreSQL**: Primary data store (version 14) - **Redis**: Cache and message queue (version 7.0) - **PgBouncer**: Database connection pooler ### Deployment Model - **Namespace**: `cloudflow-prod` - **Cluster**: AWS EKS 1.28 - **Region**: us-east-1 (primary), us-west-2 (disaster recovery) - **High Availability**: Multi-AZ deployment across 3 availability zones --- ## Prerequisites Before beginning the deployment process, ensure you have the following: ### Required Tools - `kubectl` (v1.28 or later) - `helm` (v3.12 or later) - `aws-cli` (v2.13 or later) - `eksctl` (v0.165 or later) - `terraform` (v1.6 or later) - for infrastructure provisioning ### Access Requirements - AWS account with appropriate IAM permissions - EKS cluster admin access - Container registry access (ECR) - Domain name and SSL certificates - Secrets management access (AWS Secrets Manager or Vault) ### Network Requirements - VPC with at least 3 public and 3 private subnets - NAT Gateway configured - Security groups allowing: - Ingress: HTTPS (443), HTTP (80) - Internal: PostgreSQL (5432), Redis (6379) - Monitoring: Prometheus (9090), Grafana (3000) --- ## Infrastructure Setup ### EKS Cluster Creation Use the following `eksctl` configuration to create the EKS cluster: ```yaml # cluster-config.yaml apiVersion: eksctl.io/v1alpha5 kind: ClusterConfig metadata: name: cloudflow-production region: us-east-1 version: \"1.28\" availabilityZones: - us-east-1a - us-east-1b - us-east-1c vpc: cidr: 10.0.0.0/16 nat: gateway: HighlyAvailable managedNodeGroups: - name: cloudflow-workers instanceType: m5.xlarge minSize: 3 maxSize: 10 desiredCapacity: 5 volumeSize: 100 privateNetworking: true labels: role: worker environment: production tags: Environment: production Application: cloudflow iam: withAddonPolicies: ebs: true efs: true albIngress: true cloudWatch: true cloudWatch: clusterLogging: enableTypes: - api - audit - authenticator - controllerManager - scheduler ``` Create the cluster: ```bash eksctl",
              "score": null
            },
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_7",
              "content": "podSelector: matchLabels: app: cloudflow policyTypes: - Ingress - Egress ingress: - from: - namespaceSelector: matchLabels: name: cloudflow-prod - podSelector: matchLabels: app: nginx-ingress ports: - protocol: TCP port: 3000 egress: - to: - podSelector: matchLabels: app: postgresql ports: - protocol: TCP port: 5432 - to: - podSelector: matchLabels: app: redis ports: - protocol: TCP port: 6379 ``` ### Pod Security Standards Enforce Pod Security Standards at the namespace level: ```bash kubectl label namespace cloudflow-prod \\ pod-security.kubernetes.io/enforce=restricted \\ pod-security.kubernetes.io/audit=restricted \\ pod-security.kubernetes.io/warn=restricted ``` ### Secrets Management Use AWS Secrets Manager for sensitive credentials: ```bash # Install External Secrets Operator helm install external-secrets external-secrets/external-secrets \\ --namespace external-secrets-system \\ --create-namespace # Create SecretStore kubectl apply -f - <<EOF apiVersion: external-secrets.io/v1beta1 kind: SecretStore metadata: name: aws-secrets-manager namespace: cloudflow-prod spec: provider: aws: service: SecretsManager region: us-east-1 auth: jwt: serviceAccountRef: name: external-secrets-sa EOF ``` --- ## Troubleshooting ### Common Issues #### Pods in CrashLoopBackOff **Symptoms**: Pods continuously restart **Diagnosis**: ```bash kubectl logs -n cloudflow-prod <pod-name> --previous kubectl describe pod -n cloudflow-prod <pod-name> ``` **Common Causes**: - Database connection failure - Invalid environment variables - Insufficient resources #### High Memory Usage **Symptoms**: Pods being OOMKilled **Diagnosis**: ```bash kubectl top pods -n cloudflow-prod ``` **Resolution**: - Increase memory limits in deployment - Check for memory leaks in application logs - Review heap dump for Node.js processes #### Database Connection Pool Exhausted **Symptoms**: \"Too many connections\" errors **Diagnosis**: ```bash # Check PgBouncer stats kubectl exec -it pgbouncer-xxx -n cloudflow-prod -- \\ psql -p 5432 -U pgbouncer pgbouncer -c \"SHOW POOLS;\" ``` **Resolution**: - Increase `max_db_connections` in PgBouncer - Optimize application connection usage - Add more PostgreSQL replicas ### Support Contacts - **On-call Engineer**: pagerduty.com/cloudflow-oncall - **Slack Channel**: #cloudflow-operations - **Documentation**: https://docs.cloudflow.io - **Status Page**: https://status.cloudflow.io --- ## Appendix ### Useful Commands ```bash # View all resources in namespace kubectl get all -n cloudflow-prod # Check resource usage kubectl top pods -n cloudflow-prod kubectl top nodes # View logs kubectl logs -f deployment/cloudflow-api -n cloudflow-prod # Execute command in pod kubectl exec -it <pod-name> -n cloudflow-prod -- /bin/sh # Port forward for local testing kubectl port-forward svc/cloudflow-api 8080:80 -n cloudflow-prod # Rollback deployment helm rollback cloudflow -n cloudflow-prod # Update deployment helm upgrade cloudflow cloudflow/cloudflow \\ --namespace cloudflow-prod \\ --values values-production.yaml ``` ### Maintenance Windows Scheduled maintenance occurs during the following windows: - **Primary**: Sunday 02:00-06:00",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_0",
              "content": "# CloudFlow Platform Troubleshooting Guide **Version:** 3.2.1 **Last Updated:** January 2026 **Audience:** Platform Engineers, DevOps, Support Teams ## Table of Contents 1. [Overview](#overview) 2. [Authentication & Authorization Issues](#authentication--authorization-issues) 3. [Performance Problems](#performance-problems) 4. [Database Connection Issues](#database-connection-issues) 5. [Workflow Execution Failures](#workflow-execution-failures) 6. [Rate Limiting & Throttling](#rate-limiting--throttling) 7. [Log Analysis & Debugging](#log-analysis--debugging) 8. [Escalation Procedures](#escalation-procedures) --- ## Overview This guide provides comprehensive troubleshooting steps for common CloudFlow platform issues encountered in production environments. Each section includes error symptoms, root cause analysis, resolution steps, and preventive measures. ### Quick Diagnostic Checklist Before diving into specific issues, perform these initial checks: - Verify service health: `cloudflow status --all` - Check API connectivity: `curl -I https://api.cloudflow.io/health` - Review recent deployments: `kubectl get deployments -n cloudflow --sort-by=.metadata.creationTimestamp` - Inspect platform metrics: `cloudflow metrics --last 1h` --- ## Authentication & Authorization Issues ### 401 Unauthorized Errors #### Symptoms - API requests return `401 Unauthorized` - Error message: `Authentication credentials were not provided or are invalid` - Frontend displays \"Session expired\" message #### Common Causes **1. Token Expiration** CloudFlow access tokens expire after 3600 seconds (1 hour) by default. Refresh tokens are valid for 30 days. **Verification:** ```bash # Decode JWT to check expiration echo $CF_ACCESS_TOKEN | cut -d'.' -f2 | base64 -d | jq '.exp' # Compare with current time date +%s ``` **Resolution:** ```bash # Refresh the access token curl -X POST https://api.cloudflow.io/auth/refresh \\ -H \"Content-Type: application/json\" \\ -d '{\"refresh_token\": \"'$CF_REFRESH_TOKEN'\"}' # Update environment variable export CF_ACCESS_TOKEN=\"<new_token>\" ``` **2. Invalid JWT Signature** Error: `JWT signature verification failed` **Causes:** - Token was modified or corrupted - Using wrong signing key - Token generated with different secret **Resolution:** ```bash # Validate token structure cloudflow auth validate --token $CF_ACCESS_TOKEN # Generate new token cloudflow auth login --username <user> --password <pass> # For service accounts cloudflow auth service-account --create --name <sa-name> --scopes \"workflow:read,workflow:write\" ``` **3. Clock Skew Issues** JWT validation fails when system clocks are out of sync (tolerance: \u00b1300 seconds).",
              "score": null
            },
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_6",
              "content": "**Verify Service Health**: ```bash kubectl get pods -n cloudflow-prod curl https://api.cloudflow.io/health ``` **Recovery Time Objective (RTO)**: 4 hours **Recovery Point Objective (RPO)**: 24 hours --- ## Scaling and Performance ### Horizontal Pod Autoscaling CloudFlow is configured with Horizontal Pod Autoscaler (HPA) to automatically scale based on resource utilization: ```yaml # hpa.yaml apiVersion: autoscaling/v2 kind: HorizontalPodAutoscaler metadata: name: cloudflow-api-hpa namespace: cloudflow-prod spec: scaleTargetRef: apiVersion: apps/v1 kind: Deployment name: cloudflow-api minReplicas: 3 maxReplicas: 10 metrics: - type: Resource resource: name: cpu target: type: Utilization averageUtilization: 70 - type: Resource resource: name: memory target: type: Utilization averageUtilization: 80 behavior: scaleDown: stabilizationWindowSeconds: 300 policies: - type: Percent value: 50 periodSeconds: 60 scaleUp: stabilizationWindowSeconds: 60 policies: - type: Percent value: 100 periodSeconds: 30 - type: Pods value: 2 periodSeconds: 30 selectPolicy: Max ``` Monitor autoscaling events: ```bash kubectl get hpa -n cloudflow-prod -w kubectl describe hpa cloudflow-api-hpa -n cloudflow-prod ``` ### Performance Optimization #### Connection Pooling Ensure optimal database connection pooling settings in PgBouncer: - **Pool Mode**: Transaction (optimal for microservices) - **Default Pool Size**: 25 connections per user - **Max DB Connections**: 100 (matches PostgreSQL `max_connections`) #### Redis Caching Implement Redis caching for frequently accessed data: ```javascript // Example caching strategy const CACHE_TTL = { workflows: 300, // 5 minutes userSessions: 3600, // 1 hour apiResults: 60 // 1 minute }; ``` #### Load Testing Perform regular load testing to validate scaling configuration: ```bash # Install k6 for load testing kubectl apply -f - <<EOF apiVersion: v1 kind: ConfigMap metadata: name: loadtest-script namespace: cloudflow-prod data: script.js: | import http from 'k6/http'; import { check, sleep } from 'k6'; export let options = { stages: [ { duration: '2m', target: 100 }, { duration: '5m', target: 100 }, { duration: '2m', target: 200 }, { duration: '5m', target: 200 }, { duration: '2m', target: 0 }, ], thresholds: { http_req_duration: ['p(95)<500'], }, }; export default function () { let res = http.get('https://api.cloudflow.io/api/workflows'); check(res, { 'status is 200': (r) => r.status === 200 }); sleep(1); } EOF ``` **Performance Targets**: - P95 API latency: < 500ms - P99 API latency: < 1000ms - Error rate: < 0.1% - Throughput: > 1000 requests/second --- ## Security and Compliance ### Network Policies Implement network policies to restrict pod-to-pod communication: ```yaml # network-policy.yaml apiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: cloudflow-network-policy namespace: cloudflow-prod spec:",
              "score": null
            }
          ],
          "latency_ms": 960.1934559977963,
          "query_id": "realistic_005",
          "dimension": "problem",
          "query": "I can't find the production pods"
        },
        {
          "key_facts": [
            "cloudflow-prod"
          ],
          "found_facts": [
            "cloudflow-prod"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "deployment_guide"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_1",
              "content": "create cluster -f cluster-config.yaml ``` ### Storage Configuration Install the EBS CSI driver for persistent volumes: ```bash eksctl create addon --name aws-ebs-csi-driver \\ --cluster cloudflow-production \\ --service-account-role-arn arn:aws:iam::ACCOUNT_ID:role/AmazonEKS_EBS_CSI_DriverRole \\ --force ``` --- ## Kubernetes Deployment ### Namespace Setup Create the CloudFlow namespace and configure resource quotas: ```yaml # namespace.yaml apiVersion: v1 kind: Namespace metadata: name: cloudflow-prod labels: name: cloudflow-prod environment: production --- apiVersion: v1 kind: ResourceQuota metadata: name: cloudflow-quota namespace: cloudflow-prod spec: hard: requests.cpu: \"50\" requests.memory: 100Gi persistentvolumeclaims: \"10\" services.loadbalancers: \"2\" ``` Apply the namespace configuration: ```bash kubectl apply -f namespace.yaml ``` ### Environment Configuration CloudFlow requires the following environment variables: | Variable | Description | Example | Required | |----------|-------------|---------|----------| | `DATABASE_URL` | PostgreSQL connection string | `postgresql://user:pass@host:5432/cloudflow` | Yes | | `REDIS_URL` | Redis connection string | `redis://redis-master.cloudflow-prod.svc.cluster.local:6379` | Yes | | `JWT_SECRET` | Secret key for JWT token signing | `<generated-secret-256-bit>` | Yes | | `LOG_LEVEL` | Application log verbosity | `info`, `debug`, `warn`, `error` | No (default: `info`) | | `API_PORT` | API server port | `3000` | No (default: `3000`) | | `WORKER_CONCURRENCY` | Number of concurrent workers | `10` | No (default: `5`) | | `SESSION_SECRET` | Session encryption key | `<generated-secret-256-bit>` | Yes | | `AWS_REGION` | AWS region for services | `us-east-1` | Yes | | `S3_BUCKET` | S3 bucket for file storage | `cloudflow-prod-storage` | Yes | | `SMTP_HOST` | Email server hostname | `smtp.sendgrid.net` | No | | `SMTP_PORT` | Email server port | `587` | No | | `METRICS_ENABLED` | Enable Prometheus metrics | `true` | No (default: `false`) | Create a Kubernetes secret for sensitive variables: ```bash kubectl create secret generic cloudflow-secrets \\ --namespace cloudflow-prod \\ --from-literal=DATABASE_URL=\"postgresql://cloudflow:$(cat db-password.txt)@pgbouncer.cloudflow-prod.svc.cluster.local:5432/cloudflow\" \\ --from-literal=REDIS_URL=\"redis://redis-master.cloudflow-prod.svc.cluster.local:6379\" \\ --from-literal=JWT_SECRET=\"$(openssl rand -base64 32)\" \\ --from-literal=SESSION_SECRET=\"$(openssl rand -base64 32)\" ``` ### Helm Chart Deployment Add the CloudFlow Helm repository: ```bash helm repo add cloudflow https://charts.cloudflow.io helm repo update ``` Create a values file for production configuration: ```yaml # values-production.yaml replicaCount: 3 image: repository: 123456789012.dkr.ecr.us-east-1.amazonaws.com/cloudflow tag: \"2.4.0\" pullPolicy: IfNotPresent resources: limits: cpu: 2000m memory: 4Gi requests: cpu: 2000m memory: 4Gi autoscaling: enabled: true minReplicas: 3 maxReplicas: 10 targetCPUUtilizationPercentage: 70 targetMemoryUtilizationPercentage: 80 service: type: ClusterIP port: 80 targetPort: 3000 annotations: service.beta.kubernetes.io/aws-load-balancer-type: \"nlb\" ingress: enabled: true className: nginx annotations: cert-manager.io/cluster-issuer: \"letsencrypt-prod\" nginx.ingress.kubernetes.io/ssl-redirect: \"true\" hosts: - host: api.cloudflow.io paths: - path: / pathType: Prefix tls:",
              "score": null
            },
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_0",
              "content": "# CloudFlow Platform - Deployment and Operations Guide **Version:** 2.4.0 **Last Updated:** January 2026 **Target Environment:** Production (AWS EKS) ## Table of Contents 1. [Overview](#overview) 2. [Prerequisites](#prerequisites) 3. [Infrastructure Setup](#infrastructure-setup) 4. [Kubernetes Deployment](#kubernetes-deployment) 5. [Database Configuration](#database-configuration) 6. [Monitoring and Observability](#monitoring-and-observability) 7. [Backup and Disaster Recovery](#backup-and-disaster-recovery) 8. [Scaling and Performance](#scaling-and-performance) 9. [Security and Compliance](#security-and-compliance) 10. [Troubleshooting](#troubleshooting) --- ## Overview CloudFlow is a cloud-native workflow orchestration platform designed for high-availability production environments. This guide provides comprehensive instructions for deploying and operating CloudFlow on Amazon EKS (Elastic Kubernetes Service). ### Architecture Summary CloudFlow consists of the following components: - **API Server**: REST API for workflow management (Node.js/Express) - **Worker Service**: Background job processor (Node.js) - **Scheduler**: Cron-based task scheduler (Node.js) - **PostgreSQL**: Primary data store (version 14) - **Redis**: Cache and message queue (version 7.0) - **PgBouncer**: Database connection pooler ### Deployment Model - **Namespace**: `cloudflow-prod` - **Cluster**: AWS EKS 1.28 - **Region**: us-east-1 (primary), us-west-2 (disaster recovery) - **High Availability**: Multi-AZ deployment across 3 availability zones --- ## Prerequisites Before beginning the deployment process, ensure you have the following: ### Required Tools - `kubectl` (v1.28 or later) - `helm` (v3.12 or later) - `aws-cli` (v2.13 or later) - `eksctl` (v0.165 or later) - `terraform` (v1.6 or later) - for infrastructure provisioning ### Access Requirements - AWS account with appropriate IAM permissions - EKS cluster admin access - Container registry access (ECR) - Domain name and SSL certificates - Secrets management access (AWS Secrets Manager or Vault) ### Network Requirements - VPC with at least 3 public and 3 private subnets - NAT Gateway configured - Security groups allowing: - Ingress: HTTPS (443), HTTP (80) - Internal: PostgreSQL (5432), Redis (6379) - Monitoring: Prometheus (9090), Grafana (3000) --- ## Infrastructure Setup ### EKS Cluster Creation Use the following `eksctl` configuration to create the EKS cluster: ```yaml # cluster-config.yaml apiVersion: eksctl.io/v1alpha5 kind: ClusterConfig metadata: name: cloudflow-production region: us-east-1 version: \"1.28\" availabilityZones: - us-east-1a - us-east-1b - us-east-1c vpc: cidr: 10.0.0.0/16 nat: gateway: HighlyAvailable managedNodeGroups: - name: cloudflow-workers instanceType: m5.xlarge minSize: 3 maxSize: 10 desiredCapacity: 5 volumeSize: 100 privateNetworking: true labels: role: worker environment: production tags: Environment: production Application: cloudflow iam: withAddonPolicies: ebs: true efs: true albIngress: true cloudWatch: true cloudWatch: clusterLogging: enableTypes: - api - audit - authenticator - controllerManager - scheduler ``` Create the cluster: ```bash eksctl",
              "score": null
            },
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_7",
              "content": "podSelector: matchLabels: app: cloudflow policyTypes: - Ingress - Egress ingress: - from: - namespaceSelector: matchLabels: name: cloudflow-prod - podSelector: matchLabels: app: nginx-ingress ports: - protocol: TCP port: 3000 egress: - to: - podSelector: matchLabels: app: postgresql ports: - protocol: TCP port: 5432 - to: - podSelector: matchLabels: app: redis ports: - protocol: TCP port: 6379 ``` ### Pod Security Standards Enforce Pod Security Standards at the namespace level: ```bash kubectl label namespace cloudflow-prod \\ pod-security.kubernetes.io/enforce=restricted \\ pod-security.kubernetes.io/audit=restricted \\ pod-security.kubernetes.io/warn=restricted ``` ### Secrets Management Use AWS Secrets Manager for sensitive credentials: ```bash # Install External Secrets Operator helm install external-secrets external-secrets/external-secrets \\ --namespace external-secrets-system \\ --create-namespace # Create SecretStore kubectl apply -f - <<EOF apiVersion: external-secrets.io/v1beta1 kind: SecretStore metadata: name: aws-secrets-manager namespace: cloudflow-prod spec: provider: aws: service: SecretsManager region: us-east-1 auth: jwt: serviceAccountRef: name: external-secrets-sa EOF ``` --- ## Troubleshooting ### Common Issues #### Pods in CrashLoopBackOff **Symptoms**: Pods continuously restart **Diagnosis**: ```bash kubectl logs -n cloudflow-prod <pod-name> --previous kubectl describe pod -n cloudflow-prod <pod-name> ``` **Common Causes**: - Database connection failure - Invalid environment variables - Insufficient resources #### High Memory Usage **Symptoms**: Pods being OOMKilled **Diagnosis**: ```bash kubectl top pods -n cloudflow-prod ``` **Resolution**: - Increase memory limits in deployment - Check for memory leaks in application logs - Review heap dump for Node.js processes #### Database Connection Pool Exhausted **Symptoms**: \"Too many connections\" errors **Diagnosis**: ```bash # Check PgBouncer stats kubectl exec -it pgbouncer-xxx -n cloudflow-prod -- \\ psql -p 5432 -U pgbouncer pgbouncer -c \"SHOW POOLS;\" ``` **Resolution**: - Increase `max_db_connections` in PgBouncer - Optimize application connection usage - Add more PostgreSQL replicas ### Support Contacts - **On-call Engineer**: pagerduty.com/cloudflow-oncall - **Slack Channel**: #cloudflow-operations - **Documentation**: https://docs.cloudflow.io - **Status Page**: https://status.cloudflow.io --- ## Appendix ### Useful Commands ```bash # View all resources in namespace kubectl get all -n cloudflow-prod # Check resource usage kubectl top pods -n cloudflow-prod kubectl top nodes # View logs kubectl logs -f deployment/cloudflow-api -n cloudflow-prod # Execute command in pod kubectl exec -it <pod-name> -n cloudflow-prod -- /bin/sh # Port forward for local testing kubectl port-forward svc/cloudflow-api 8080:80 -n cloudflow-prod # Rollback deployment helm rollback cloudflow -n cloudflow-prod # Update deployment helm upgrade cloudflow cloudflow/cloudflow \\ --namespace cloudflow-prod \\ --values values-production.yaml ``` ### Maintenance Windows Scheduled maintenance occurs during the following windows: - **Primary**: Sunday 02:00-06:00",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_8",
              "content": "Leverage caching:** ```bash # Enable client-side caching export CLOUDFLOW_CACHE_ENABLED=true export CLOUDFLOW_CACHE_TTL=300 # Cache workflow metadata cloudflow workflows list --use-cache --cache-ttl 600 ``` --- ## Log Analysis & Debugging ### Accessing CloudFlow Logs #### Kubernetes Deployments ```bash # List all CloudFlow pods kubectl get pods -n cloudflow # Tail logs from API server kubectl logs -f -n cloudflow deployment/cloudflow-api --tail=100 # Get logs from specific pod kubectl logs -n cloudflow cloudflow-api-7d4f6b8c9d-x7k2m # Get logs from previous crashed pod kubectl logs -n cloudflow cloudflow-api-7d4f6b8c9d-x7k2m --previous # Get logs from all pods in deployment kubectl logs -n cloudflow deployment/cloudflow-api --all-containers=true # Stream logs from multiple pods kubectl logs -n cloudflow -l app=cloudflow-api -f --max-log-requests=10 ``` #### Log Levels CloudFlow supports the following log levels: - `TRACE` - Very detailed debugging information - `DEBUG` - Detailed debugging information - `INFO` - Informational messages (default) - `WARN` - Warning messages - `ERROR` - Error messages - `FATAL` - Fatal errors causing shutdown **Changing log levels:** ```bash # Set global log level cloudflow config set logging.level DEBUG # Set log level for specific component cloudflow config set logging.components.database DEBUG cloudflow config set logging.components.auth INFO cloudflow config set logging.components.workflows TRACE # Temporary log level increase (resets after 1 hour) cloudflow debug set-log-level DEBUG --duration 3600 # View current log configuration cloudflow config get logging --format json ``` ### Grep Patterns for Common Issues #### Authentication Failures ```bash # Find all authentication errors kubectl logs -n cloudflow deployment/cloudflow-api | \\ grep -i \"authentication\\|401\\|unauthorized\" | \\ tail -n 50 # Find JWT validation failures kubectl logs -n cloudflow deployment/cloudflow-api | \\ grep -E \"JWT|token.*invalid|signature.*failed\" # Find clock skew issues kubectl logs -n cloudflow deployment/cloudflow-api | \\ grep -i \"clock skew\\|time.*sync\\|nbf\\|exp\" ``` #### Database Errors ```bash # Find database connection errors kubectl logs -n cloudflow deployment/cloudflow-api | \\ grep -E \"connection.*pool|could not connect|database.*timeout\" # Find slow queries kubectl logs -n cloudflow deployment/cloudflow-api | \\ grep \"slow query\" | \\ awk '{print $NF}' | \\ sort -n | \\ tail -n 20 # Find deadlock errors kubectl logs -n cloudflow deployment/cloudflow-api | \\ grep -i \"deadlock detected\" ``` #### Workflow Execution Errors ```bash # Find workflow timeout errors kubectl logs -n cloudflow deployment/cloudflow-workflow-engine | \\ grep -E \"timeout|exceeded.*3600\" # Find workflow retry attempts kubectl logs -n cloudflow deployment/cloudflow-workflow-engine | \\ grep -E \"retry attempt [0-9]|retrying",
              "score": null
            },
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_2",
              "content": "- secretName: cloudflow-tls hosts: - api.cloudflow.io healthCheck: enabled: true livenessProbe: path: /health initialDelaySeconds: 30 periodSeconds: 10 timeoutSeconds: 5 failureThreshold: 3 readinessProbe: path: /ready initialDelaySeconds: 10 periodSeconds: 5 timeoutSeconds: 3 failureThreshold: 3 env: - name: NODE_ENV value: \"production\" - name: LOG_LEVEL value: \"info\" - name: API_PORT value: \"3000\" - name: WORKER_CONCURRENCY value: \"10\" - name: AWS_REGION value: \"us-east-1\" - name: METRICS_ENABLED value: \"true\" envFrom: - secretRef: name: cloudflow-secrets podAnnotations: prometheus.io/scrape: \"true\" prometheus.io/port: \"9090\" prometheus.io/path: \"/metrics\" podSecurityContext: runAsNonRoot: true runAsUser: 1000 fsGroup: 1000 securityContext: allowPrivilegeEscalation: false capabilities: drop: - ALL readOnlyRootFilesystem: true persistence: enabled: true storageClass: gp3 accessMode: ReadWriteOnce size: 50Gi redis: enabled: true architecture: replication auth: enabled: true master: persistence: size: 20Gi replica: replicaCount: 2 persistence: size: 20Gi ``` Deploy CloudFlow using Helm: ```bash helm install cloudflow cloudflow/cloudflow \\ --namespace cloudflow-prod \\ --values values-production.yaml \\ --wait \\ --timeout 10m ``` ### Deployment Verification Verify the deployment status: ```bash # Check pod status kubectl get pods -n cloudflow-prod # Expected output: # NAME READY STATUS RESTARTS AGE # cloudflow-api-7d8f9c5b6d-4xk2p 1/1 Running 0 2m # cloudflow-api-7d8f9c5b6d-9hj5m 1/1 Running 0 2m # cloudflow-api-7d8f9c5b6d-tn8wq 1/1 Running 0 2m # Check service endpoints kubectl get svc -n cloudflow-prod # Check ingress kubectl get ingress -n cloudflow-prod ``` Test health endpoints: ```bash # Port forward for testing kubectl port-forward -n cloudflow-prod svc/cloudflow-api 8080:80 # Test health endpoint curl http://localhost:8080/health # Expected: {\"status\":\"healthy\",\"timestamp\":\"2026-01-24T10:30:00Z\"} # Test readiness endpoint curl http://localhost:8080/ready # Expected: {\"status\":\"ready\",\"dependencies\":{\"database\":\"connected\",\"redis\":\"connected\"}} ``` --- ## Database Configuration ### PostgreSQL Setup CloudFlow uses PostgreSQL 14 as its primary data store.",
              "score": null
            }
          ],
          "latency_ms": 941.7458349998924,
          "query_id": "realistic_005",
          "dimension": "casual",
          "query": "prod namespace"
        },
        {
          "key_facts": [
            "cloudflow-prod"
          ],
          "found_facts": [
            "cloudflow-prod"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "deployment_guide"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_0",
              "content": "# CloudFlow Platform - Deployment and Operations Guide **Version:** 2.4.0 **Last Updated:** January 2026 **Target Environment:** Production (AWS EKS) ## Table of Contents 1. [Overview](#overview) 2. [Prerequisites](#prerequisites) 3. [Infrastructure Setup](#infrastructure-setup) 4. [Kubernetes Deployment](#kubernetes-deployment) 5. [Database Configuration](#database-configuration) 6. [Monitoring and Observability](#monitoring-and-observability) 7. [Backup and Disaster Recovery](#backup-and-disaster-recovery) 8. [Scaling and Performance](#scaling-and-performance) 9. [Security and Compliance](#security-and-compliance) 10. [Troubleshooting](#troubleshooting) --- ## Overview CloudFlow is a cloud-native workflow orchestration platform designed for high-availability production environments. This guide provides comprehensive instructions for deploying and operating CloudFlow on Amazon EKS (Elastic Kubernetes Service). ### Architecture Summary CloudFlow consists of the following components: - **API Server**: REST API for workflow management (Node.js/Express) - **Worker Service**: Background job processor (Node.js) - **Scheduler**: Cron-based task scheduler (Node.js) - **PostgreSQL**: Primary data store (version 14) - **Redis**: Cache and message queue (version 7.0) - **PgBouncer**: Database connection pooler ### Deployment Model - **Namespace**: `cloudflow-prod` - **Cluster**: AWS EKS 1.28 - **Region**: us-east-1 (primary), us-west-2 (disaster recovery) - **High Availability**: Multi-AZ deployment across 3 availability zones --- ## Prerequisites Before beginning the deployment process, ensure you have the following: ### Required Tools - `kubectl` (v1.28 or later) - `helm` (v3.12 or later) - `aws-cli` (v2.13 or later) - `eksctl` (v0.165 or later) - `terraform` (v1.6 or later) - for infrastructure provisioning ### Access Requirements - AWS account with appropriate IAM permissions - EKS cluster admin access - Container registry access (ECR) - Domain name and SSL certificates - Secrets management access (AWS Secrets Manager or Vault) ### Network Requirements - VPC with at least 3 public and 3 private subnets - NAT Gateway configured - Security groups allowing: - Ingress: HTTPS (443), HTTP (80) - Internal: PostgreSQL (5432), Redis (6379) - Monitoring: Prometheus (9090), Grafana (3000) --- ## Infrastructure Setup ### EKS Cluster Creation Use the following `eksctl` configuration to create the EKS cluster: ```yaml # cluster-config.yaml apiVersion: eksctl.io/v1alpha5 kind: ClusterConfig metadata: name: cloudflow-production region: us-east-1 version: \"1.28\" availabilityZones: - us-east-1a - us-east-1b - us-east-1c vpc: cidr: 10.0.0.0/16 nat: gateway: HighlyAvailable managedNodeGroups: - name: cloudflow-workers instanceType: m5.xlarge minSize: 3 maxSize: 10 desiredCapacity: 5 volumeSize: 100 privateNetworking: true labels: role: worker environment: production tags: Environment: production Application: cloudflow iam: withAddonPolicies: ebs: true efs: true albIngress: true cloudWatch: true cloudWatch: clusterLogging: enableTypes: - api - audit - authenticator - controllerManager - scheduler ``` Create the cluster: ```bash eksctl",
              "score": null
            },
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_1",
              "content": "create cluster -f cluster-config.yaml ``` ### Storage Configuration Install the EBS CSI driver for persistent volumes: ```bash eksctl create addon --name aws-ebs-csi-driver \\ --cluster cloudflow-production \\ --service-account-role-arn arn:aws:iam::ACCOUNT_ID:role/AmazonEKS_EBS_CSI_DriverRole \\ --force ``` --- ## Kubernetes Deployment ### Namespace Setup Create the CloudFlow namespace and configure resource quotas: ```yaml # namespace.yaml apiVersion: v1 kind: Namespace metadata: name: cloudflow-prod labels: name: cloudflow-prod environment: production --- apiVersion: v1 kind: ResourceQuota metadata: name: cloudflow-quota namespace: cloudflow-prod spec: hard: requests.cpu: \"50\" requests.memory: 100Gi persistentvolumeclaims: \"10\" services.loadbalancers: \"2\" ``` Apply the namespace configuration: ```bash kubectl apply -f namespace.yaml ``` ### Environment Configuration CloudFlow requires the following environment variables: | Variable | Description | Example | Required | |----------|-------------|---------|----------| | `DATABASE_URL` | PostgreSQL connection string | `postgresql://user:pass@host:5432/cloudflow` | Yes | | `REDIS_URL` | Redis connection string | `redis://redis-master.cloudflow-prod.svc.cluster.local:6379` | Yes | | `JWT_SECRET` | Secret key for JWT token signing | `<generated-secret-256-bit>` | Yes | | `LOG_LEVEL` | Application log verbosity | `info`, `debug`, `warn`, `error` | No (default: `info`) | | `API_PORT` | API server port | `3000` | No (default: `3000`) | | `WORKER_CONCURRENCY` | Number of concurrent workers | `10` | No (default: `5`) | | `SESSION_SECRET` | Session encryption key | `<generated-secret-256-bit>` | Yes | | `AWS_REGION` | AWS region for services | `us-east-1` | Yes | | `S3_BUCKET` | S3 bucket for file storage | `cloudflow-prod-storage` | Yes | | `SMTP_HOST` | Email server hostname | `smtp.sendgrid.net` | No | | `SMTP_PORT` | Email server port | `587` | No | | `METRICS_ENABLED` | Enable Prometheus metrics | `true` | No (default: `false`) | Create a Kubernetes secret for sensitive variables: ```bash kubectl create secret generic cloudflow-secrets \\ --namespace cloudflow-prod \\ --from-literal=DATABASE_URL=\"postgresql://cloudflow:$(cat db-password.txt)@pgbouncer.cloudflow-prod.svc.cluster.local:5432/cloudflow\" \\ --from-literal=REDIS_URL=\"redis://redis-master.cloudflow-prod.svc.cluster.local:6379\" \\ --from-literal=JWT_SECRET=\"$(openssl rand -base64 32)\" \\ --from-literal=SESSION_SECRET=\"$(openssl rand -base64 32)\" ``` ### Helm Chart Deployment Add the CloudFlow Helm repository: ```bash helm repo add cloudflow https://charts.cloudflow.io helm repo update ``` Create a values file for production configuration: ```yaml # values-production.yaml replicaCount: 3 image: repository: 123456789012.dkr.ecr.us-east-1.amazonaws.com/cloudflow tag: \"2.4.0\" pullPolicy: IfNotPresent resources: limits: cpu: 2000m memory: 4Gi requests: cpu: 2000m memory: 4Gi autoscaling: enabled: true minReplicas: 3 maxReplicas: 10 targetCPUUtilizationPercentage: 70 targetMemoryUtilizationPercentage: 80 service: type: ClusterIP port: 80 targetPort: 3000 annotations: service.beta.kubernetes.io/aws-load-balancer-type: \"nlb\" ingress: enabled: true className: nginx annotations: cert-manager.io/cluster-issuer: \"letsencrypt-prod\" nginx.ingress.kubernetes.io/ssl-redirect: \"true\" hosts: - host: api.cloudflow.io paths: - path: / pathType: Prefix tls:",
              "score": null
            },
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_7",
              "content": "podSelector: matchLabels: app: cloudflow policyTypes: - Ingress - Egress ingress: - from: - namespaceSelector: matchLabels: name: cloudflow-prod - podSelector: matchLabels: app: nginx-ingress ports: - protocol: TCP port: 3000 egress: - to: - podSelector: matchLabels: app: postgresql ports: - protocol: TCP port: 5432 - to: - podSelector: matchLabels: app: redis ports: - protocol: TCP port: 6379 ``` ### Pod Security Standards Enforce Pod Security Standards at the namespace level: ```bash kubectl label namespace cloudflow-prod \\ pod-security.kubernetes.io/enforce=restricted \\ pod-security.kubernetes.io/audit=restricted \\ pod-security.kubernetes.io/warn=restricted ``` ### Secrets Management Use AWS Secrets Manager for sensitive credentials: ```bash # Install External Secrets Operator helm install external-secrets external-secrets/external-secrets \\ --namespace external-secrets-system \\ --create-namespace # Create SecretStore kubectl apply -f - <<EOF apiVersion: external-secrets.io/v1beta1 kind: SecretStore metadata: name: aws-secrets-manager namespace: cloudflow-prod spec: provider: aws: service: SecretsManager region: us-east-1 auth: jwt: serviceAccountRef: name: external-secrets-sa EOF ``` --- ## Troubleshooting ### Common Issues #### Pods in CrashLoopBackOff **Symptoms**: Pods continuously restart **Diagnosis**: ```bash kubectl logs -n cloudflow-prod <pod-name> --previous kubectl describe pod -n cloudflow-prod <pod-name> ``` **Common Causes**: - Database connection failure - Invalid environment variables - Insufficient resources #### High Memory Usage **Symptoms**: Pods being OOMKilled **Diagnosis**: ```bash kubectl top pods -n cloudflow-prod ``` **Resolution**: - Increase memory limits in deployment - Check for memory leaks in application logs - Review heap dump for Node.js processes #### Database Connection Pool Exhausted **Symptoms**: \"Too many connections\" errors **Diagnosis**: ```bash # Check PgBouncer stats kubectl exec -it pgbouncer-xxx -n cloudflow-prod -- \\ psql -p 5432 -U pgbouncer pgbouncer -c \"SHOW POOLS;\" ``` **Resolution**: - Increase `max_db_connections` in PgBouncer - Optimize application connection usage - Add more PostgreSQL replicas ### Support Contacts - **On-call Engineer**: pagerduty.com/cloudflow-oncall - **Slack Channel**: #cloudflow-operations - **Documentation**: https://docs.cloudflow.io - **Status Page**: https://status.cloudflow.io --- ## Appendix ### Useful Commands ```bash # View all resources in namespace kubectl get all -n cloudflow-prod # Check resource usage kubectl top pods -n cloudflow-prod kubectl top nodes # View logs kubectl logs -f deployment/cloudflow-api -n cloudflow-prod # Execute command in pod kubectl exec -it <pod-name> -n cloudflow-prod -- /bin/sh # Port forward for local testing kubectl port-forward svc/cloudflow-api 8080:80 -n cloudflow-prod # Rollback deployment helm rollback cloudflow -n cloudflow-prod # Update deployment helm upgrade cloudflow cloudflow/cloudflow \\ --namespace cloudflow-prod \\ --values values-production.yaml ``` ### Maintenance Windows Scheduled maintenance occurs during the following windows: - **Primary**: Sunday 02:00-06:00",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_8",
              "content": "Leverage caching:** ```bash # Enable client-side caching export CLOUDFLOW_CACHE_ENABLED=true export CLOUDFLOW_CACHE_TTL=300 # Cache workflow metadata cloudflow workflows list --use-cache --cache-ttl 600 ``` --- ## Log Analysis & Debugging ### Accessing CloudFlow Logs #### Kubernetes Deployments ```bash # List all CloudFlow pods kubectl get pods -n cloudflow # Tail logs from API server kubectl logs -f -n cloudflow deployment/cloudflow-api --tail=100 # Get logs from specific pod kubectl logs -n cloudflow cloudflow-api-7d4f6b8c9d-x7k2m # Get logs from previous crashed pod kubectl logs -n cloudflow cloudflow-api-7d4f6b8c9d-x7k2m --previous # Get logs from all pods in deployment kubectl logs -n cloudflow deployment/cloudflow-api --all-containers=true # Stream logs from multiple pods kubectl logs -n cloudflow -l app=cloudflow-api -f --max-log-requests=10 ``` #### Log Levels CloudFlow supports the following log levels: - `TRACE` - Very detailed debugging information - `DEBUG` - Detailed debugging information - `INFO` - Informational messages (default) - `WARN` - Warning messages - `ERROR` - Error messages - `FATAL` - Fatal errors causing shutdown **Changing log levels:** ```bash # Set global log level cloudflow config set logging.level DEBUG # Set log level for specific component cloudflow config set logging.components.database DEBUG cloudflow config set logging.components.auth INFO cloudflow config set logging.components.workflows TRACE # Temporary log level increase (resets after 1 hour) cloudflow debug set-log-level DEBUG --duration 3600 # View current log configuration cloudflow config get logging --format json ``` ### Grep Patterns for Common Issues #### Authentication Failures ```bash # Find all authentication errors kubectl logs -n cloudflow deployment/cloudflow-api | \\ grep -i \"authentication\\|401\\|unauthorized\" | \\ tail -n 50 # Find JWT validation failures kubectl logs -n cloudflow deployment/cloudflow-api | \\ grep -E \"JWT|token.*invalid|signature.*failed\" # Find clock skew issues kubectl logs -n cloudflow deployment/cloudflow-api | \\ grep -i \"clock skew\\|time.*sync\\|nbf\\|exp\" ``` #### Database Errors ```bash # Find database connection errors kubectl logs -n cloudflow deployment/cloudflow-api | \\ grep -E \"connection.*pool|could not connect|database.*timeout\" # Find slow queries kubectl logs -n cloudflow deployment/cloudflow-api | \\ grep \"slow query\" | \\ awk '{print $NF}' | \\ sort -n | \\ tail -n 20 # Find deadlock errors kubectl logs -n cloudflow deployment/cloudflow-api | \\ grep -i \"deadlock detected\" ``` #### Workflow Execution Errors ```bash # Find workflow timeout errors kubectl logs -n cloudflow deployment/cloudflow-workflow-engine | \\ grep -E \"timeout|exceeded.*3600\" # Find workflow retry attempts kubectl logs -n cloudflow deployment/cloudflow-workflow-engine | \\ grep -E \"retry attempt [0-9]|retrying",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_11",
              "content": "Service (2) - PostgreSQL Primary - PostgreSQL Replica - PostgreSQL Replica - Redis Primary (2) - Redis Replica (2) - Redis Replica (2) - Kafka Broker (2) - Kafka Broker (2) - Kafka Broker (1) ``` **Automatic Failover**: - Database: 30-60 seconds (automatic promotion of replica) - Redis: < 10 seconds (Sentinel-based failover) - Kafka: < 30 seconds (controller election) - Services: Kubernetes health checks with 10-second liveness probes ### Backup Strategy **Database Backups**: - Automated snapshots: Daily at 02:00 UTC - Retention: 30 days for daily, 90 days for monthly - Cross-region replication: Async replication to us-west-2 (15-minute lag) - Backup verification: Weekly automated restore test in staging environment **Backup Schedule**: ``` Daily: Full snapshot \u2192 S3 (encrypted) Hourly: WAL archives \u2192 S3 (point-in-time recovery) Weekly: Backup validation test Monthly: Long-term archive to Glacier ``` **Configuration Backups**: - Kubernetes manifests: Stored in Git (GitOps with ArgoCD) - Vault secrets: Automated snapshot every 6 hours - Kafka topic configurations: Exported daily to S3 ### Disaster Recovery Procedures **Scenario 1: Single AZ Failure** - Detection: Health checks fail for entire AZ (< 30 seconds) - Action: Traffic automatically routed to healthy AZs by ALB - Recovery time: < 5 minutes (no manual intervention) - Data loss: None (multi-AZ replication) **Scenario 2: Database Primary Failure** - Detection: Health check fails for primary database (< 30 seconds) - Action: Automatic promotion of read replica to primary - Recovery time: 30-60 seconds - Data loss: Minimal (< 1 second due to synchronous replication) **Scenario 3: Full Region Failure** - Detection: Multiple health check failures across all AZs (< 2 minutes) - Action: Manual failover to DR region (us-west-2) - Procedure: 1. Update DNS to point to DR region (TTL: 60 seconds) 2. Promote DR database replica to primary 3. Scale up DR region services to production capacity 4. Verify data consistency and integrity 5. Update monitoring dashboards - Recovery time: 2-4 hours (includes verification) - Data loss: < 1 hour (last cross-region replication) **Scenario 4: Data Corruption** - Detection: Data validation checks or user report - Action: Point-in-time recovery from WAL archives - Procedure: 1. Identify corruption time window 2. Restore from snapshot prior to corruption 3. Replay WAL logs up to corruption point 4. Verify data integrity 5.",
              "score": null
            }
          ],
          "latency_ms": 908.5759279987542,
          "query_id": "realistic_005",
          "dimension": "contextual",
          "query": "I need to deploy to production, what namespace should I use?"
        },
        {
          "key_facts": [
            "cloudflow-prod"
          ],
          "found_facts": [
            "cloudflow-prod"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "deployment_guide"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_1",
              "content": "create cluster -f cluster-config.yaml ``` ### Storage Configuration Install the EBS CSI driver for persistent volumes: ```bash eksctl create addon --name aws-ebs-csi-driver \\ --cluster cloudflow-production \\ --service-account-role-arn arn:aws:iam::ACCOUNT_ID:role/AmazonEKS_EBS_CSI_DriverRole \\ --force ``` --- ## Kubernetes Deployment ### Namespace Setup Create the CloudFlow namespace and configure resource quotas: ```yaml # namespace.yaml apiVersion: v1 kind: Namespace metadata: name: cloudflow-prod labels: name: cloudflow-prod environment: production --- apiVersion: v1 kind: ResourceQuota metadata: name: cloudflow-quota namespace: cloudflow-prod spec: hard: requests.cpu: \"50\" requests.memory: 100Gi persistentvolumeclaims: \"10\" services.loadbalancers: \"2\" ``` Apply the namespace configuration: ```bash kubectl apply -f namespace.yaml ``` ### Environment Configuration CloudFlow requires the following environment variables: | Variable | Description | Example | Required | |----------|-------------|---------|----------| | `DATABASE_URL` | PostgreSQL connection string | `postgresql://user:pass@host:5432/cloudflow` | Yes | | `REDIS_URL` | Redis connection string | `redis://redis-master.cloudflow-prod.svc.cluster.local:6379` | Yes | | `JWT_SECRET` | Secret key for JWT token signing | `<generated-secret-256-bit>` | Yes | | `LOG_LEVEL` | Application log verbosity | `info`, `debug`, `warn`, `error` | No (default: `info`) | | `API_PORT` | API server port | `3000` | No (default: `3000`) | | `WORKER_CONCURRENCY` | Number of concurrent workers | `10` | No (default: `5`) | | `SESSION_SECRET` | Session encryption key | `<generated-secret-256-bit>` | Yes | | `AWS_REGION` | AWS region for services | `us-east-1` | Yes | | `S3_BUCKET` | S3 bucket for file storage | `cloudflow-prod-storage` | Yes | | `SMTP_HOST` | Email server hostname | `smtp.sendgrid.net` | No | | `SMTP_PORT` | Email server port | `587` | No | | `METRICS_ENABLED` | Enable Prometheus metrics | `true` | No (default: `false`) | Create a Kubernetes secret for sensitive variables: ```bash kubectl create secret generic cloudflow-secrets \\ --namespace cloudflow-prod \\ --from-literal=DATABASE_URL=\"postgresql://cloudflow:$(cat db-password.txt)@pgbouncer.cloudflow-prod.svc.cluster.local:5432/cloudflow\" \\ --from-literal=REDIS_URL=\"redis://redis-master.cloudflow-prod.svc.cluster.local:6379\" \\ --from-literal=JWT_SECRET=\"$(openssl rand -base64 32)\" \\ --from-literal=SESSION_SECRET=\"$(openssl rand -base64 32)\" ``` ### Helm Chart Deployment Add the CloudFlow Helm repository: ```bash helm repo add cloudflow https://charts.cloudflow.io helm repo update ``` Create a values file for production configuration: ```yaml # values-production.yaml replicaCount: 3 image: repository: 123456789012.dkr.ecr.us-east-1.amazonaws.com/cloudflow tag: \"2.4.0\" pullPolicy: IfNotPresent resources: limits: cpu: 2000m memory: 4Gi requests: cpu: 2000m memory: 4Gi autoscaling: enabled: true minReplicas: 3 maxReplicas: 10 targetCPUUtilizationPercentage: 70 targetMemoryUtilizationPercentage: 80 service: type: ClusterIP port: 80 targetPort: 3000 annotations: service.beta.kubernetes.io/aws-load-balancer-type: \"nlb\" ingress: enabled: true className: nginx annotations: cert-manager.io/cluster-issuer: \"letsencrypt-prod\" nginx.ingress.kubernetes.io/ssl-redirect: \"true\" hosts: - host: api.cloudflow.io paths: - path: / pathType: Prefix tls:",
              "score": null
            },
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_7",
              "content": "podSelector: matchLabels: app: cloudflow policyTypes: - Ingress - Egress ingress: - from: - namespaceSelector: matchLabels: name: cloudflow-prod - podSelector: matchLabels: app: nginx-ingress ports: - protocol: TCP port: 3000 egress: - to: - podSelector: matchLabels: app: postgresql ports: - protocol: TCP port: 5432 - to: - podSelector: matchLabels: app: redis ports: - protocol: TCP port: 6379 ``` ### Pod Security Standards Enforce Pod Security Standards at the namespace level: ```bash kubectl label namespace cloudflow-prod \\ pod-security.kubernetes.io/enforce=restricted \\ pod-security.kubernetes.io/audit=restricted \\ pod-security.kubernetes.io/warn=restricted ``` ### Secrets Management Use AWS Secrets Manager for sensitive credentials: ```bash # Install External Secrets Operator helm install external-secrets external-secrets/external-secrets \\ --namespace external-secrets-system \\ --create-namespace # Create SecretStore kubectl apply -f - <<EOF apiVersion: external-secrets.io/v1beta1 kind: SecretStore metadata: name: aws-secrets-manager namespace: cloudflow-prod spec: provider: aws: service: SecretsManager region: us-east-1 auth: jwt: serviceAccountRef: name: external-secrets-sa EOF ``` --- ## Troubleshooting ### Common Issues #### Pods in CrashLoopBackOff **Symptoms**: Pods continuously restart **Diagnosis**: ```bash kubectl logs -n cloudflow-prod <pod-name> --previous kubectl describe pod -n cloudflow-prod <pod-name> ``` **Common Causes**: - Database connection failure - Invalid environment variables - Insufficient resources #### High Memory Usage **Symptoms**: Pods being OOMKilled **Diagnosis**: ```bash kubectl top pods -n cloudflow-prod ``` **Resolution**: - Increase memory limits in deployment - Check for memory leaks in application logs - Review heap dump for Node.js processes #### Database Connection Pool Exhausted **Symptoms**: \"Too many connections\" errors **Diagnosis**: ```bash # Check PgBouncer stats kubectl exec -it pgbouncer-xxx -n cloudflow-prod -- \\ psql -p 5432 -U pgbouncer pgbouncer -c \"SHOW POOLS;\" ``` **Resolution**: - Increase `max_db_connections` in PgBouncer - Optimize application connection usage - Add more PostgreSQL replicas ### Support Contacts - **On-call Engineer**: pagerduty.com/cloudflow-oncall - **Slack Channel**: #cloudflow-operations - **Documentation**: https://docs.cloudflow.io - **Status Page**: https://status.cloudflow.io --- ## Appendix ### Useful Commands ```bash # View all resources in namespace kubectl get all -n cloudflow-prod # Check resource usage kubectl top pods -n cloudflow-prod kubectl top nodes # View logs kubectl logs -f deployment/cloudflow-api -n cloudflow-prod # Execute command in pod kubectl exec -it <pod-name> -n cloudflow-prod -- /bin/sh # Port forward for local testing kubectl port-forward svc/cloudflow-api 8080:80 -n cloudflow-prod # Rollback deployment helm rollback cloudflow -n cloudflow-prod # Update deployment helm upgrade cloudflow cloudflow/cloudflow \\ --namespace cloudflow-prod \\ --values values-production.yaml ``` ### Maintenance Windows Scheduled maintenance occurs during the following windows: - **Primary**: Sunday 02:00-06:00",
              "score": null
            },
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_6",
              "content": "**Verify Service Health**: ```bash kubectl get pods -n cloudflow-prod curl https://api.cloudflow.io/health ``` **Recovery Time Objective (RTO)**: 4 hours **Recovery Point Objective (RPO)**: 24 hours --- ## Scaling and Performance ### Horizontal Pod Autoscaling CloudFlow is configured with Horizontal Pod Autoscaler (HPA) to automatically scale based on resource utilization: ```yaml # hpa.yaml apiVersion: autoscaling/v2 kind: HorizontalPodAutoscaler metadata: name: cloudflow-api-hpa namespace: cloudflow-prod spec: scaleTargetRef: apiVersion: apps/v1 kind: Deployment name: cloudflow-api minReplicas: 3 maxReplicas: 10 metrics: - type: Resource resource: name: cpu target: type: Utilization averageUtilization: 70 - type: Resource resource: name: memory target: type: Utilization averageUtilization: 80 behavior: scaleDown: stabilizationWindowSeconds: 300 policies: - type: Percent value: 50 periodSeconds: 60 scaleUp: stabilizationWindowSeconds: 60 policies: - type: Percent value: 100 periodSeconds: 30 - type: Pods value: 2 periodSeconds: 30 selectPolicy: Max ``` Monitor autoscaling events: ```bash kubectl get hpa -n cloudflow-prod -w kubectl describe hpa cloudflow-api-hpa -n cloudflow-prod ``` ### Performance Optimization #### Connection Pooling Ensure optimal database connection pooling settings in PgBouncer: - **Pool Mode**: Transaction (optimal for microservices) - **Default Pool Size**: 25 connections per user - **Max DB Connections**: 100 (matches PostgreSQL `max_connections`) #### Redis Caching Implement Redis caching for frequently accessed data: ```javascript // Example caching strategy const CACHE_TTL = { workflows: 300, // 5 minutes userSessions: 3600, // 1 hour apiResults: 60 // 1 minute }; ``` #### Load Testing Perform regular load testing to validate scaling configuration: ```bash # Install k6 for load testing kubectl apply -f - <<EOF apiVersion: v1 kind: ConfigMap metadata: name: loadtest-script namespace: cloudflow-prod data: script.js: | import http from 'k6/http'; import { check, sleep } from 'k6'; export let options = { stages: [ { duration: '2m', target: 100 }, { duration: '5m', target: 100 }, { duration: '2m', target: 200 }, { duration: '5m', target: 200 }, { duration: '2m', target: 0 }, ], thresholds: { http_req_duration: ['p(95)<500'], }, }; export default function () { let res = http.get('https://api.cloudflow.io/api/workflows'); check(res, { 'status is 200': (r) => r.status === 200 }); sleep(1); } EOF ``` **Performance Targets**: - P95 API latency: < 500ms - P99 API latency: < 1000ms - Error rate: < 0.1% - Throughput: > 1000 requests/second --- ## Security and Compliance ### Network Policies Implement network policies to restrict pod-to-pod communication: ```yaml # network-policy.yaml apiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: cloudflow-network-policy namespace: cloudflow-prod spec:",
              "score": null
            },
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_0",
              "content": "# CloudFlow Platform - Deployment and Operations Guide **Version:** 2.4.0 **Last Updated:** January 2026 **Target Environment:** Production (AWS EKS) ## Table of Contents 1. [Overview](#overview) 2. [Prerequisites](#prerequisites) 3. [Infrastructure Setup](#infrastructure-setup) 4. [Kubernetes Deployment](#kubernetes-deployment) 5. [Database Configuration](#database-configuration) 6. [Monitoring and Observability](#monitoring-and-observability) 7. [Backup and Disaster Recovery](#backup-and-disaster-recovery) 8. [Scaling and Performance](#scaling-and-performance) 9. [Security and Compliance](#security-and-compliance) 10. [Troubleshooting](#troubleshooting) --- ## Overview CloudFlow is a cloud-native workflow orchestration platform designed for high-availability production environments. This guide provides comprehensive instructions for deploying and operating CloudFlow on Amazon EKS (Elastic Kubernetes Service). ### Architecture Summary CloudFlow consists of the following components: - **API Server**: REST API for workflow management (Node.js/Express) - **Worker Service**: Background job processor (Node.js) - **Scheduler**: Cron-based task scheduler (Node.js) - **PostgreSQL**: Primary data store (version 14) - **Redis**: Cache and message queue (version 7.0) - **PgBouncer**: Database connection pooler ### Deployment Model - **Namespace**: `cloudflow-prod` - **Cluster**: AWS EKS 1.28 - **Region**: us-east-1 (primary), us-west-2 (disaster recovery) - **High Availability**: Multi-AZ deployment across 3 availability zones --- ## Prerequisites Before beginning the deployment process, ensure you have the following: ### Required Tools - `kubectl` (v1.28 or later) - `helm` (v3.12 or later) - `aws-cli` (v2.13 or later) - `eksctl` (v0.165 or later) - `terraform` (v1.6 or later) - for infrastructure provisioning ### Access Requirements - AWS account with appropriate IAM permissions - EKS cluster admin access - Container registry access (ECR) - Domain name and SSL certificates - Secrets management access (AWS Secrets Manager or Vault) ### Network Requirements - VPC with at least 3 public and 3 private subnets - NAT Gateway configured - Security groups allowing: - Ingress: HTTPS (443), HTTP (80) - Internal: PostgreSQL (5432), Redis (6379) - Monitoring: Prometheus (9090), Grafana (3000) --- ## Infrastructure Setup ### EKS Cluster Creation Use the following `eksctl` configuration to create the EKS cluster: ```yaml # cluster-config.yaml apiVersion: eksctl.io/v1alpha5 kind: ClusterConfig metadata: name: cloudflow-production region: us-east-1 version: \"1.28\" availabilityZones: - us-east-1a - us-east-1b - us-east-1c vpc: cidr: 10.0.0.0/16 nat: gateway: HighlyAvailable managedNodeGroups: - name: cloudflow-workers instanceType: m5.xlarge minSize: 3 maxSize: 10 desiredCapacity: 5 volumeSize: 100 privateNetworking: true labels: role: worker environment: production tags: Environment: production Application: cloudflow iam: withAddonPolicies: ebs: true efs: true albIngress: true cloudWatch: true cloudWatch: clusterLogging: enableTypes: - api - audit - authenticator - controllerManager - scheduler ``` Create the cluster: ```bash eksctl",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_1",
              "content": "**Diagnosis:** ```bash # Check system time timedatectl status # Compare with NTP server ntpdate -q pool.ntp.org # Check JWT issued time vs current time jwt_iat=$(echo $CF_ACCESS_TOKEN | cut -d'.' -f2 | base64 -d | jq '.iat') current_time=$(date +%s) skew=$((current_time - jwt_iat)) echo \"Clock skew: $skew seconds\" ``` **Resolution:** ```bash # Sync with NTP server sudo ntpdate -s pool.ntp.org # Enable automatic time sync sudo timedatectl set-ntp true # Restart CloudFlow client cloudflow restart ``` **4. Insufficient Permissions** Error: `User does not have required permissions for this operation` **Check user roles:** ```bash cloudflow auth whoami --verbose # Expected output: # User: john.doe@company.com # Roles: developer, workflow-admin # Scopes: workflow:*, data:read, metrics:read ``` **Request permission elevation:** ```bash # Submit access request cloudflow auth request-access \\ --resource \"workflow:production:deploy\" \\ --justification \"Deploy critical hotfix for TICKET-1234\" # Check pending approvals cloudflow auth list-requests --status pending ``` ### 403 Forbidden Errors #### RBAC Policy Violations CloudFlow uses role-based access control (RBAC) with the following hierarchy: - `viewer` - Read-only access - `developer` - Create and modify workflows (non-production) - `operator` - Execute workflows, view logs - `admin` - Full access to workspace - `platform-admin` - Cross-workspace administration **Verify resource permissions:** ```bash # Check effective permissions cloudflow rbac check \\ --user john.doe@company.com \\ --resource workflow:prod-pipeline \\ --action execute # List all policies affecting user cloudflow rbac policies --user john.doe@company.com --verbose ``` --- ## Performance Problems ### Slow Query Performance #### Symptoms - API response times > 5000ms - Database query latency warnings in logs - CloudFlow UI becomes unresponsive - Timeout errors: `Request exceeded maximum duration of 30000ms` #### Diagnosis Steps **1. Identify Slow Queries** ```bash # Query CloudFlow metrics cloudflow metrics query --metric api_request_duration_ms \\ --filter \"p95 > 5000\" \\ --last 1h # Check database slow query log kubectl logs -n cloudflow deploy/cloudflow-db-primary | \\ grep \"slow query\" | \\ tail -n 50 # Analyze query patterns cloudflow db analyze-queries --min-duration 5000 --limit 20 ``` **2.",
              "score": null
            }
          ],
          "latency_ms": 887.6406730014423,
          "query_id": "realistic_005",
          "dimension": "negation",
          "query": "Why can't I see resources in the default namespace?"
        },
        {
          "key_facts": [
            "2 vCPU, 4GB RAM per pod"
          ],
          "found_facts": [
            "2 vCPU, 4GB RAM per pod"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "architecture_overview"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_10",
              "content": "- Execution start rate: 500 per second - Completion rate: 450 per second (average 2-second execution time) **Database**: - Read throughput: 50,000 queries per second (across replicas) - Write throughput: 15,000 transactions per second - Connection capacity: 2,000 concurrent connections **Kafka**: - Message ingestion: 100,000 messages per second - Consumer throughput: 80,000 messages per second (aggregated) - End-to-end latency: < 100ms (P99) ### Resource Utilization **CPU Utilization** (Target: 60-70% average): - API Gateway: 55% average, 80% peak - Workflow Engine: 65% average, 85% peak - Auth Service: 40% average, 60% peak **Memory Utilization** (Target: < 80%): - API Gateway: 2.5GB average per pod (4GB allocated) - Workflow Engine: 6GB average per pod (8GB allocated) - Notification Service: 2.8GB average per pod (4GB allocated) **Network Throughput**: - Ingress: 2 Gbps average, 5 Gbps peak - Egress: 1.5 Gbps average, 4 Gbps peak - Internal (service mesh): 8 Gbps average ### Monitoring & Alerting **Key Metrics** (Monitored via Prometheus): - Request rate (per service, per endpoint) - Error rate (4xx, 5xx responses) - Latency percentiles (P50, P95, P99) - Resource utilization (CPU, memory, disk) - Cache hit/miss ratios - Database connection pool saturation - Kafka consumer lag **Alerts** (via PagerDuty): - P1 (Immediate): API error rate > 1%, database connection failure - P2 (< 15 min): P99 latency > 500ms, cache hit rate < 80% - P3 (< 1 hour): Resource utilization > 85%, replica count < minimum **SLI/SLO/SLA**: - SLI: API success rate (non-5xx responses) - SLO: 99.9% uptime per month (43 minutes downtime allowance) - SLA: 99.5% uptime guarantee (customer-facing SLA with credits) --- ## Disaster Recovery ### Recovery Objectives **RPO (Recovery Point Objective): 1 hour** - Maximum acceptable data loss: 1 hour of transactions - Achieved through: Continuous database replication + hourly snapshots - Kafka retention: 7 days allows event replay **RTO (Recovery Time Objective): 4 hours** - Maximum acceptable downtime: 4 hours for full system recovery - Includes: Failover, data verification, and service restoration - Automated runbooks reduce RTO to < 2 hours for common scenarios ### High Availability Architecture **Multi-AZ Deployment**: ``` Region: us-east-1 AZ-1a: AZ-1b: AZ-1c: - API Gateway (4) - API Gateway (4) - API Gateway (4) - Workflow Engine (6) - Workflow Engine (5) - Workflow Engine (5) - Auth Service (3) - Auth Service (3) - Auth",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_0",
              "content": "# CloudFlow Platform - System Architecture Overview **Document Version:** 2.3.1 **Last Updated:** January 15, 2026 **Owner:** Platform Architecture Team **Status:** Production ## Executive Summary CloudFlow is a distributed, cloud-native workflow automation platform designed to orchestrate complex business processes at scale. The platform processes over 2.5 million workflow executions daily with an average P99 latency of 180ms for API operations and 4.2 seconds for workflow execution. This document provides a comprehensive overview of the system architecture, including microservices design, data flow patterns, infrastructure decisions, and operational considerations. ## Table of Contents 1. [High-Level Architecture](#high-level-architecture) 2. [Microservices Breakdown](#microservices-breakdown) 3. [Data Flow Architecture](#data-flow-architecture) 4. [Database Architecture](#database-architecture) 5. [Message Queue Patterns](#message-queue-patterns) 6. [Caching Strategy](#caching-strategy) 7. [Security Architecture](#security-architecture) 8. [Performance Characteristics](#performance-characteristics) 9. [Disaster Recovery](#disaster-recovery) --- ## High-Level Architecture CloudFlow follows a microservices architecture pattern deployed across multiple availability zones in AWS. The platform is designed with the following core principles: - **Scalability**: Horizontal scaling for all services with auto-scaling groups - **Resilience**: Circuit breakers, bulkheads, and graceful degradation - **Observability**: Distributed tracing, centralized logging, and comprehensive metrics - **Security**: Zero-trust network architecture with mTLS encryption ### Architecture Diagram (Conceptual) ``` \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 Load Balancer (ALB) \u2502 \u2502 (TLS Termination - 443) \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u25bc \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 API Gateway Layer \u2502 \u2502 (Rate Limiting, Auth, Request Routing) \u2502 \u2514\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u2502 \u2502 \u2502 \u2502 \u25bc \u25bc \u25bc \u25bc \u25bc \u250c\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502Auth\u2502 \u2502Workflow\u2502 \u2502Scheduler\u2502 \u2502Notification\u2502 \u2502User Service\u2502 \u2502Svc \u2502 \u2502 Engine \u2502 \u2502 Service \u2502 \u2502 Service \u2502 \u2502 \u2502 \u2514\u2500\u252c\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u2502 \u2502 \u2502 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 \u2502 \u2502 \u25bc \u25bc \u25bc \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502PostgreSQL\u2502 \u2502 Redis \u2502 \u2502 Kafka \u2502 \u2502 Cluster \u2502 \u2502 Cluster \u2502 \u2502 Cluster \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 ``` ### Technology Stack - **Runtime**: Node.js 20.x (API Gateway, Workflow Engine), Go 1.21 (Auth Service, Scheduler) - **Container Orchestration**: Kubernetes 1.28 (EKS) - **Service Mesh**: Istio 1.20 for service-to-service communication - **Databases**: PostgreSQL 15.4, Redis 7.2 - **Message Broker**: Apache Kafka 3.6 - **Monitoring**: Prometheus, Grafana, Jaeger for distributed tracing - **Secrets Management**: HashiCorp Vault 1.15 --- ## Microservices Breakdown ### API Gateway **Purpose**: Single entry point for all client requests, providing authentication, rate limiting, request routing, and protocol translation.",
              "score": null
            },
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_6",
              "content": "**Verify Service Health**: ```bash kubectl get pods -n cloudflow-prod curl https://api.cloudflow.io/health ``` **Recovery Time Objective (RTO)**: 4 hours **Recovery Point Objective (RPO)**: 24 hours --- ## Scaling and Performance ### Horizontal Pod Autoscaling CloudFlow is configured with Horizontal Pod Autoscaler (HPA) to automatically scale based on resource utilization: ```yaml # hpa.yaml apiVersion: autoscaling/v2 kind: HorizontalPodAutoscaler metadata: name: cloudflow-api-hpa namespace: cloudflow-prod spec: scaleTargetRef: apiVersion: apps/v1 kind: Deployment name: cloudflow-api minReplicas: 3 maxReplicas: 10 metrics: - type: Resource resource: name: cpu target: type: Utilization averageUtilization: 70 - type: Resource resource: name: memory target: type: Utilization averageUtilization: 80 behavior: scaleDown: stabilizationWindowSeconds: 300 policies: - type: Percent value: 50 periodSeconds: 60 scaleUp: stabilizationWindowSeconds: 60 policies: - type: Percent value: 100 periodSeconds: 30 - type: Pods value: 2 periodSeconds: 30 selectPolicy: Max ``` Monitor autoscaling events: ```bash kubectl get hpa -n cloudflow-prod -w kubectl describe hpa cloudflow-api-hpa -n cloudflow-prod ``` ### Performance Optimization #### Connection Pooling Ensure optimal database connection pooling settings in PgBouncer: - **Pool Mode**: Transaction (optimal for microservices) - **Default Pool Size**: 25 connections per user - **Max DB Connections**: 100 (matches PostgreSQL `max_connections`) #### Redis Caching Implement Redis caching for frequently accessed data: ```javascript // Example caching strategy const CACHE_TTL = { workflows: 300, // 5 minutes userSessions: 3600, // 1 hour apiResults: 60 // 1 minute }; ``` #### Load Testing Perform regular load testing to validate scaling configuration: ```bash # Install k6 for load testing kubectl apply -f - <<EOF apiVersion: v1 kind: ConfigMap metadata: name: loadtest-script namespace: cloudflow-prod data: script.js: | import http from 'k6/http'; import { check, sleep } from 'k6'; export let options = { stages: [ { duration: '2m', target: 100 }, { duration: '5m', target: 100 }, { duration: '2m', target: 200 }, { duration: '5m', target: 200 }, { duration: '2m', target: 0 }, ], thresholds: { http_req_duration: ['p(95)<500'], }, }; export default function () { let res = http.get('https://api.cloudflow.io/api/workflows'); check(res, { 'status is 200': (r) => r.status === 200 }); sleep(1); } EOF ``` **Performance Targets**: - P95 API latency: < 500ms - P99 API latency: < 1000ms - Error rate: < 0.1% - Throughput: > 1000 requests/second --- ## Security and Compliance ### Network Policies Implement network policies to restrict pod-to-pod communication: ```yaml # network-policy.yaml apiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: cloudflow-network-policy namespace: cloudflow-prod spec:",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_1",
              "content": "**Technology**: Node.js with Express.js framework **Replicas**: 12 pods (production), auto-scaling 8-20 based on CPU **Resource Allocation**: 2 vCPU, 4GB RAM per pod **Key Responsibilities**: - JWT token validation (delegated to Auth Service for initial validation) - Rate limiting: 1000 requests per minute per API key (sliding window) - Request/response transformation and validation using JSON Schema - Routing to downstream services based on URL path patterns - CORS handling for web clients - Request/response logging and correlation ID injection **Critical Endpoints**: - `POST /api/v1/workflows` - Create new workflow - `GET /api/v1/workflows/:id` - Retrieve workflow status - `POST /api/v1/workflows/:id/execute` - Trigger workflow execution - `GET /api/v1/workflows/:id/history` - Get execution history **Dependencies**: - Auth Service (for token validation) - Redis (for rate limiting counters) - All downstream microservices **Performance Targets**: - P50 latency: < 50ms - P99 latency: < 200ms - Throughput: 10,000 RPS sustained --- ### Auth Service **Purpose**: Centralized authentication and authorization service handling user identity, token generation, and permission validation. **Technology**: Go with gRPC for internal communication, REST for external **Replicas**: 8 pods (production), auto-scaling 6-12 **Resource Allocation**: 1 vCPU, 2GB RAM per pod **Key Responsibilities**: - User authentication via multiple providers (OAuth2, SAML, local credentials) - JWT token generation and validation (RS256 algorithm) - Role-based access control (RBAC) with fine-grained permissions - Session management with Redis-backed storage - API key generation and validation for service accounts - MFA enforcement for administrative operations **Authentication Flow**: ``` Client Request \u2192 API Gateway \u2192 Auth Service \u2502 \u251c\u2500 Validate credentials \u251c\u2500 Check MFA if required \u251c\u2500 Generate JWT (15min expiry) \u251c\u2500 Generate refresh token (7 days) \u2514\u2500 Store session in Redis ``` **Token Structure**: - Access Token: JWT with 15-minute expiry - Refresh Token: Opaque token with 7-day expiry, stored in PostgreSQL - Claims: user_id, email, roles[], permissions[], tenant_id **Security Features**: - Password hashing: Argon2id with 64MB memory, 4 iterations - Token rotation on refresh to prevent replay attacks - Brute force protection: 5 failed attempts \u2192 15-minute lockout - Secrets stored in HashiCorp Vault, rotated every 90 days **Performance Targets**: - Token validation: < 10ms (P99) - Token generation: < 50ms (P99) - Throughput: 5,000 RPS for validation operations --- ### Workflow Engine **Purpose**: Core orchestration service that executes workflow definitions, manages state transitions, and coordinates task execution across distributed systems.",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_9",
              "content": "per API key (default: 1000 RPM) ### Secrets Management **HashiCorp Vault Integration**: - Dynamic database credentials: Generated on-demand, 1-hour TTL - Encryption keys: Transit secrets engine for encryption-as-a-service - API keys for external services: Stored with versioning - Rotation policy: Automated rotation every 90 days with notification **Secret Access Pattern**: ``` Service starts \u2192 Vault authentication (Kubernetes service account) \u2502 \u25bc Request secret lease (1-hour TTL) \u2502 \u25bc Vault returns dynamic credentials \u2502 \u25bc Service connects to PostgreSQL \u2502 Renew lease every 30 minutes ``` ### Data Protection **Encryption at Rest**: - PostgreSQL: AES-256 encryption enabled at volume level - Redis: Encryption enabled using AWS KMS - Kafka: Encryption at rest for all data on brokers - S3 backups: Server-side encryption with KMS (SSE-KMS) **Encryption in Transit**: - External traffic: TLS 1.3 only (TLS 1.2 deprecated) - Internal traffic: mTLS via Istio service mesh - Database connections: SSL/TLS enforced (sslmode=require) **Data Masking**: - Sensitive fields (email, phone) masked in logs - PII redacted from error messages and stack traces - Audit logs: Full data retention with access controls **Compliance**: - GDPR: User data export and deletion workflows - SOC 2 Type II: Audit logging for all data access - HIPAA: PHI isolation in dedicated tenants with enhanced encryption --- ## Performance Characteristics ### Latency Targets **API Operations** (P99 latency): - `GET /workflows/{id}`: < 50ms (cache hit), < 150ms (cache miss) - `POST /workflows`: < 200ms (includes validation and database write) - `POST /workflows/{id}/execute`: < 100ms (async, returns execution ID) - `GET /workflows/{id}/history`: < 300ms (paginated, 50 records per page) **Workflow Execution** (P99 latency): - Simple workflow (< 5 steps): < 2 seconds - Medium workflow (5-15 steps): < 5 seconds - Complex workflow (> 15 steps): < 15 seconds - Parallel execution: Linear scaling up to 10 concurrent branches **Database Operations** (P99 latency): - Simple SELECT: < 5ms - JOIN query: < 20ms - INSERT/UPDATE: < 10ms - Batch operations: < 100ms (batch size: 100 records) **Cache Operations** (P99 latency): - Redis GET: < 2ms - Redis SET: < 3ms - Redis DEL: < 2ms ### Throughput Capacity **API Gateway**: - Sustained: 10,000 requests per second - Peak: 25,000 requests per second (5-minute burst) - Rate limiting: 1,000 requests per minute per API key **Workflow Engine**: - Concurrent executions: 8,000 workflows (across 16 pods)",
              "score": null
            }
          ],
          "latency_ms": 1016.3856710023538,
          "query_id": "realistic_006",
          "dimension": "original",
          "query": "What are the resource requirements for the API Gateway?"
        },
        {
          "key_facts": [
            "2 vCPU, 4GB RAM per pod"
          ],
          "found_facts": [
            "2 vCPU, 4GB RAM per pod"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "architecture_overview"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_6",
              "content": "**Verify Service Health**: ```bash kubectl get pods -n cloudflow-prod curl https://api.cloudflow.io/health ``` **Recovery Time Objective (RTO)**: 4 hours **Recovery Point Objective (RPO)**: 24 hours --- ## Scaling and Performance ### Horizontal Pod Autoscaling CloudFlow is configured with Horizontal Pod Autoscaler (HPA) to automatically scale based on resource utilization: ```yaml # hpa.yaml apiVersion: autoscaling/v2 kind: HorizontalPodAutoscaler metadata: name: cloudflow-api-hpa namespace: cloudflow-prod spec: scaleTargetRef: apiVersion: apps/v1 kind: Deployment name: cloudflow-api minReplicas: 3 maxReplicas: 10 metrics: - type: Resource resource: name: cpu target: type: Utilization averageUtilization: 70 - type: Resource resource: name: memory target: type: Utilization averageUtilization: 80 behavior: scaleDown: stabilizationWindowSeconds: 300 policies: - type: Percent value: 50 periodSeconds: 60 scaleUp: stabilizationWindowSeconds: 60 policies: - type: Percent value: 100 periodSeconds: 30 - type: Pods value: 2 periodSeconds: 30 selectPolicy: Max ``` Monitor autoscaling events: ```bash kubectl get hpa -n cloudflow-prod -w kubectl describe hpa cloudflow-api-hpa -n cloudflow-prod ``` ### Performance Optimization #### Connection Pooling Ensure optimal database connection pooling settings in PgBouncer: - **Pool Mode**: Transaction (optimal for microservices) - **Default Pool Size**: 25 connections per user - **Max DB Connections**: 100 (matches PostgreSQL `max_connections`) #### Redis Caching Implement Redis caching for frequently accessed data: ```javascript // Example caching strategy const CACHE_TTL = { workflows: 300, // 5 minutes userSessions: 3600, // 1 hour apiResults: 60 // 1 minute }; ``` #### Load Testing Perform regular load testing to validate scaling configuration: ```bash # Install k6 for load testing kubectl apply -f - <<EOF apiVersion: v1 kind: ConfigMap metadata: name: loadtest-script namespace: cloudflow-prod data: script.js: | import http from 'k6/http'; import { check, sleep } from 'k6'; export let options = { stages: [ { duration: '2m', target: 100 }, { duration: '5m', target: 100 }, { duration: '2m', target: 200 }, { duration: '5m', target: 200 }, { duration: '2m', target: 0 }, ], thresholds: { http_req_duration: ['p(95)<500'], }, }; export default function () { let res = http.get('https://api.cloudflow.io/api/workflows'); check(res, { 'status is 200': (r) => r.status === 200 }); sleep(1); } EOF ``` **Performance Targets**: - P95 API latency: < 500ms - P99 API latency: < 1000ms - Error rate: < 0.1% - Throughput: > 1000 requests/second --- ## Security and Compliance ### Network Policies Implement network policies to restrict pod-to-pod communication: ```yaml # network-policy.yaml apiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: cloudflow-network-policy namespace: cloudflow-prod spec:",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_10",
              "content": "- Execution start rate: 500 per second - Completion rate: 450 per second (average 2-second execution time) **Database**: - Read throughput: 50,000 queries per second (across replicas) - Write throughput: 15,000 transactions per second - Connection capacity: 2,000 concurrent connections **Kafka**: - Message ingestion: 100,000 messages per second - Consumer throughput: 80,000 messages per second (aggregated) - End-to-end latency: < 100ms (P99) ### Resource Utilization **CPU Utilization** (Target: 60-70% average): - API Gateway: 55% average, 80% peak - Workflow Engine: 65% average, 85% peak - Auth Service: 40% average, 60% peak **Memory Utilization** (Target: < 80%): - API Gateway: 2.5GB average per pod (4GB allocated) - Workflow Engine: 6GB average per pod (8GB allocated) - Notification Service: 2.8GB average per pod (4GB allocated) **Network Throughput**: - Ingress: 2 Gbps average, 5 Gbps peak - Egress: 1.5 Gbps average, 4 Gbps peak - Internal (service mesh): 8 Gbps average ### Monitoring & Alerting **Key Metrics** (Monitored via Prometheus): - Request rate (per service, per endpoint) - Error rate (4xx, 5xx responses) - Latency percentiles (P50, P95, P99) - Resource utilization (CPU, memory, disk) - Cache hit/miss ratios - Database connection pool saturation - Kafka consumer lag **Alerts** (via PagerDuty): - P1 (Immediate): API error rate > 1%, database connection failure - P2 (< 15 min): P99 latency > 500ms, cache hit rate < 80% - P3 (< 1 hour): Resource utilization > 85%, replica count < minimum **SLI/SLO/SLA**: - SLI: API success rate (non-5xx responses) - SLO: 99.9% uptime per month (43 minutes downtime allowance) - SLA: 99.5% uptime guarantee (customer-facing SLA with credits) --- ## Disaster Recovery ### Recovery Objectives **RPO (Recovery Point Objective): 1 hour** - Maximum acceptable data loss: 1 hour of transactions - Achieved through: Continuous database replication + hourly snapshots - Kafka retention: 7 days allows event replay **RTO (Recovery Time Objective): 4 hours** - Maximum acceptable downtime: 4 hours for full system recovery - Includes: Failover, data verification, and service restoration - Automated runbooks reduce RTO to < 2 hours for common scenarios ### High Availability Architecture **Multi-AZ Deployment**: ``` Region: us-east-1 AZ-1a: AZ-1b: AZ-1c: - API Gateway (4) - API Gateway (4) - API Gateway (4) - Workflow Engine (6) - Workflow Engine (5) - Workflow Engine (5) - Auth Service (3) - Auth Service (3) - Auth",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_1",
              "content": "**Technology**: Node.js with Express.js framework **Replicas**: 12 pods (production), auto-scaling 8-20 based on CPU **Resource Allocation**: 2 vCPU, 4GB RAM per pod **Key Responsibilities**: - JWT token validation (delegated to Auth Service for initial validation) - Rate limiting: 1000 requests per minute per API key (sliding window) - Request/response transformation and validation using JSON Schema - Routing to downstream services based on URL path patterns - CORS handling for web clients - Request/response logging and correlation ID injection **Critical Endpoints**: - `POST /api/v1/workflows` - Create new workflow - `GET /api/v1/workflows/:id` - Retrieve workflow status - `POST /api/v1/workflows/:id/execute` - Trigger workflow execution - `GET /api/v1/workflows/:id/history` - Get execution history **Dependencies**: - Auth Service (for token validation) - Redis (for rate limiting counters) - All downstream microservices **Performance Targets**: - P50 latency: < 50ms - P99 latency: < 200ms - Throughput: 10,000 RPS sustained --- ### Auth Service **Purpose**: Centralized authentication and authorization service handling user identity, token generation, and permission validation. **Technology**: Go with gRPC for internal communication, REST for external **Replicas**: 8 pods (production), auto-scaling 6-12 **Resource Allocation**: 1 vCPU, 2GB RAM per pod **Key Responsibilities**: - User authentication via multiple providers (OAuth2, SAML, local credentials) - JWT token generation and validation (RS256 algorithm) - Role-based access control (RBAC) with fine-grained permissions - Session management with Redis-backed storage - API key generation and validation for service accounts - MFA enforcement for administrative operations **Authentication Flow**: ``` Client Request \u2192 API Gateway \u2192 Auth Service \u2502 \u251c\u2500 Validate credentials \u251c\u2500 Check MFA if required \u251c\u2500 Generate JWT (15min expiry) \u251c\u2500 Generate refresh token (7 days) \u2514\u2500 Store session in Redis ``` **Token Structure**: - Access Token: JWT with 15-minute expiry - Refresh Token: Opaque token with 7-day expiry, stored in PostgreSQL - Claims: user_id, email, roles[], permissions[], tenant_id **Security Features**: - Password hashing: Argon2id with 64MB memory, 4 iterations - Token rotation on refresh to prevent replay attacks - Brute force protection: 5 failed attempts \u2192 15-minute lockout - Secrets stored in HashiCorp Vault, rotated every 90 days **Performance Targets**: - Token validation: < 10ms (P99) - Token generation: < 50ms (P99) - Throughput: 5,000 RPS for validation operations --- ### Workflow Engine **Purpose**: Core orchestration service that executes workflow definitions, manages state transitions, and coordinates task execution across distributed systems.",
              "score": null
            },
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_4",
              "content": "--set prometheus.prometheusSpec.retention=30d \\ --set prometheus.prometheusSpec.storageSpec.volumeClaimTemplate.spec.resources.requests.storage=100Gi ``` CloudFlow exposes Prometheus metrics on port 9090 at the `/metrics` endpoint. The metrics include: - **HTTP Metrics**: Request rate, latency, error rate - **Database Metrics**: Connection pool status, query duration - **Worker Metrics**: Job queue size, processing time, failure rate - **System Metrics**: CPU usage, memory usage, heap statistics Example metrics exposed: ``` # HELP cloudflow_http_requests_total Total number of HTTP requests # TYPE cloudflow_http_requests_total counter cloudflow_http_requests_total{method=\"GET\",route=\"/api/workflows\",status=\"200\"} 15420 # HELP cloudflow_http_request_duration_seconds HTTP request duration in seconds # TYPE cloudflow_http_request_duration_seconds histogram cloudflow_http_request_duration_seconds_bucket{method=\"POST\",route=\"/api/workflows\",le=\"0.1\"} 8234 # HELP cloudflow_worker_jobs_processed_total Total number of jobs processed # TYPE cloudflow_worker_jobs_processed_total counter cloudflow_worker_jobs_processed_total{status=\"success\"} 45230 # HELP cloudflow_database_connections Current database connections # TYPE cloudflow_database_connections gauge cloudflow_database_connections{state=\"active\"} 23 ``` ### Grafana Dashboards Access Grafana to view CloudFlow dashboards: ```bash kubectl port-forward -n monitoring svc/prometheus-grafana 3000:80 ``` Import the CloudFlow dashboard (ID: 15847) or use the provided JSON template. Key dashboard panels include: 1. **API Performance**: Request rate, P95/P99 latency, error rate 2. **Resource Usage**: CPU, memory, disk I/O per pod 3. **Database Health**: Connection pool utilization, query performance 4. **Worker Status**: Queue depth, processing rate, job success/failure ratio 5. **System Overview**: Pod status, replica count, autoscaling events ### Logging with CloudWatch CloudFlow logs are automatically shipped to CloudWatch Logs.",
              "score": null
            },
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_4",
              "content": "### Error Response Format ```json { \"error\": { \"code\": \"invalid_parameter\", \"message\": \"The 'limit' parameter must be between 1 and 100\", \"field\": \"limit\", \"request_id\": \"req_8k3m9x2p\" } } ``` ### HTTP Status Codes | Status Code | Description | Common Causes | |------------|-------------|---------------| | 400 | Bad Request | Invalid parameters, malformed JSON, validation errors | | 401 | Unauthorized | Missing or invalid authentication credentials | | 403 | Forbidden | Insufficient permissions for requested resource | | 404 | Not Found | Resource does not exist or has been deleted | | 429 | Too Many Requests | Rate limit exceeded, retry after specified period | | 500 | Internal Server Error | Unexpected server error, contact support if persists | | 502 | Bad Gateway | Temporary service issue, retry with exponential backoff | | 503 | Service Unavailable | Scheduled maintenance or temporary outage | ### Error Codes CloudFlow returns specific error codes to help you identify and resolve issues: - `invalid_parameter`: One or more request parameters are invalid - `missing_required_field`: Required field is missing from request body - `authentication_failed`: Invalid API key or token - `insufficient_permissions`: User lacks required scope or permission - `resource_not_found`: Requested resource does not exist - `rate_limit_exceeded`: Too many requests, see rate limiting section - `workflow_execution_failed`: Workflow execution encountered an error - `invalid_json`: Request body contains malformed JSON - `duplicate_resource`: Resource with same identifier already exists - `quota_exceeded`: Account quota limit reached ### Error Handling Best Practices **Python Example:** ```python import requests import time def make_api_request(url, headers, max_retries=3): for attempt in range(max_retries): try: response = requests.get(url, headers=headers) if response.status_code == 200: return response.json() elif response.status_code == 429: # Rate limit exceeded retry_after = int(response.headers.get('X-RateLimit-Reset', 60)) print(f\"Rate limited. Waiting {retry_after} seconds...\") time.sleep(retry_after) continue elif response.status_code >= 500: # Server error - retry with exponential backoff wait_time = 2 ** attempt print(f\"Server error. Retrying in {wait_time} seconds...\") time.sleep(wait_time) continue else: # Client error - don't retry error_data = response.json() raise Exception(f\"API Error: {error_data['error']['message']}\") except requests.exceptions.RequestException as e: print(f\"Request failed: {e}\") if attempt < max_retries - 1: time.sleep(2 ** attempt) else: raise raise Exception(\"Max retries exceeded\") ``` ## Webhooks CloudFlow can send webhook notifications when specific events occur in your workflows.",
              "score": null
            }
          ],
          "latency_ms": 1047.9235140010132,
          "query_id": "realistic_006",
          "dimension": "synonym",
          "query": "How much CPU and memory does the API Gateway need?"
        },
        {
          "key_facts": [
            "2 vCPU, 4GB RAM per pod"
          ],
          "found_facts": [
            "2 vCPU, 4GB RAM per pod"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "architecture_overview"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_10",
              "content": "- Execution start rate: 500 per second - Completion rate: 450 per second (average 2-second execution time) **Database**: - Read throughput: 50,000 queries per second (across replicas) - Write throughput: 15,000 transactions per second - Connection capacity: 2,000 concurrent connections **Kafka**: - Message ingestion: 100,000 messages per second - Consumer throughput: 80,000 messages per second (aggregated) - End-to-end latency: < 100ms (P99) ### Resource Utilization **CPU Utilization** (Target: 60-70% average): - API Gateway: 55% average, 80% peak - Workflow Engine: 65% average, 85% peak - Auth Service: 40% average, 60% peak **Memory Utilization** (Target: < 80%): - API Gateway: 2.5GB average per pod (4GB allocated) - Workflow Engine: 6GB average per pod (8GB allocated) - Notification Service: 2.8GB average per pod (4GB allocated) **Network Throughput**: - Ingress: 2 Gbps average, 5 Gbps peak - Egress: 1.5 Gbps average, 4 Gbps peak - Internal (service mesh): 8 Gbps average ### Monitoring & Alerting **Key Metrics** (Monitored via Prometheus): - Request rate (per service, per endpoint) - Error rate (4xx, 5xx responses) - Latency percentiles (P50, P95, P99) - Resource utilization (CPU, memory, disk) - Cache hit/miss ratios - Database connection pool saturation - Kafka consumer lag **Alerts** (via PagerDuty): - P1 (Immediate): API error rate > 1%, database connection failure - P2 (< 15 min): P99 latency > 500ms, cache hit rate < 80% - P3 (< 1 hour): Resource utilization > 85%, replica count < minimum **SLI/SLO/SLA**: - SLI: API success rate (non-5xx responses) - SLO: 99.9% uptime per month (43 minutes downtime allowance) - SLA: 99.5% uptime guarantee (customer-facing SLA with credits) --- ## Disaster Recovery ### Recovery Objectives **RPO (Recovery Point Objective): 1 hour** - Maximum acceptable data loss: 1 hour of transactions - Achieved through: Continuous database replication + hourly snapshots - Kafka retention: 7 days allows event replay **RTO (Recovery Time Objective): 4 hours** - Maximum acceptable downtime: 4 hours for full system recovery - Includes: Failover, data verification, and service restoration - Automated runbooks reduce RTO to < 2 hours for common scenarios ### High Availability Architecture **Multi-AZ Deployment**: ``` Region: us-east-1 AZ-1a: AZ-1b: AZ-1c: - API Gateway (4) - API Gateway (4) - API Gateway (4) - Workflow Engine (6) - Workflow Engine (5) - Workflow Engine (5) - Auth Service (3) - Auth Service (3) - Auth",
              "score": null
            },
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_6",
              "content": "**Verify Service Health**: ```bash kubectl get pods -n cloudflow-prod curl https://api.cloudflow.io/health ``` **Recovery Time Objective (RTO)**: 4 hours **Recovery Point Objective (RPO)**: 24 hours --- ## Scaling and Performance ### Horizontal Pod Autoscaling CloudFlow is configured with Horizontal Pod Autoscaler (HPA) to automatically scale based on resource utilization: ```yaml # hpa.yaml apiVersion: autoscaling/v2 kind: HorizontalPodAutoscaler metadata: name: cloudflow-api-hpa namespace: cloudflow-prod spec: scaleTargetRef: apiVersion: apps/v1 kind: Deployment name: cloudflow-api minReplicas: 3 maxReplicas: 10 metrics: - type: Resource resource: name: cpu target: type: Utilization averageUtilization: 70 - type: Resource resource: name: memory target: type: Utilization averageUtilization: 80 behavior: scaleDown: stabilizationWindowSeconds: 300 policies: - type: Percent value: 50 periodSeconds: 60 scaleUp: stabilizationWindowSeconds: 60 policies: - type: Percent value: 100 periodSeconds: 30 - type: Pods value: 2 periodSeconds: 30 selectPolicy: Max ``` Monitor autoscaling events: ```bash kubectl get hpa -n cloudflow-prod -w kubectl describe hpa cloudflow-api-hpa -n cloudflow-prod ``` ### Performance Optimization #### Connection Pooling Ensure optimal database connection pooling settings in PgBouncer: - **Pool Mode**: Transaction (optimal for microservices) - **Default Pool Size**: 25 connections per user - **Max DB Connections**: 100 (matches PostgreSQL `max_connections`) #### Redis Caching Implement Redis caching for frequently accessed data: ```javascript // Example caching strategy const CACHE_TTL = { workflows: 300, // 5 minutes userSessions: 3600, // 1 hour apiResults: 60 // 1 minute }; ``` #### Load Testing Perform regular load testing to validate scaling configuration: ```bash # Install k6 for load testing kubectl apply -f - <<EOF apiVersion: v1 kind: ConfigMap metadata: name: loadtest-script namespace: cloudflow-prod data: script.js: | import http from 'k6/http'; import { check, sleep } from 'k6'; export let options = { stages: [ { duration: '2m', target: 100 }, { duration: '5m', target: 100 }, { duration: '2m', target: 200 }, { duration: '5m', target: 200 }, { duration: '2m', target: 0 }, ], thresholds: { http_req_duration: ['p(95)<500'], }, }; export default function () { let res = http.get('https://api.cloudflow.io/api/workflows'); check(res, { 'status is 200': (r) => r.status === 200 }); sleep(1); } EOF ``` **Performance Targets**: - P95 API latency: < 500ms - P99 API latency: < 1000ms - Error rate: < 0.1% - Throughput: > 1000 requests/second --- ## Security and Compliance ### Network Policies Implement network policies to restrict pod-to-pod communication: ```yaml # network-policy.yaml apiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: cloudflow-network-policy namespace: cloudflow-prod spec:",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_2",
              "content": "Review Query Execution Plans** ```sql -- Connect to CloudFlow database cloudflow db connect --readonly -- Explain slow query EXPLAIN ANALYZE SELECT w.*, e.status, e.error_message FROM workflows w LEFT JOIN executions e ON w.id = e.workflow_id WHERE w.workspace_id = 'ws_abc123' AND e.created_at > NOW() - INTERVAL '7 days' ORDER BY e.created_at DESC; -- Check for missing indexes SELECT schemaname, tablename, indexname, indexdef FROM pg_indexes WHERE tablename IN ('workflows', 'executions', 'workflow_steps') ORDER BY tablename, indexname; ``` **3. Optimize Queries** Common optimization techniques: **Add missing indexes:** ```sql -- Index for workflow lookup by workspace CREATE INDEX CONCURRENTLY idx_workflows_workspace_created ON workflows(workspace_id, created_at DESC); -- Index for execution status queries CREATE INDEX CONCURRENTLY idx_executions_status_created ON executions(workflow_id, status, created_at DESC) WHERE status IN ('running', 'pending'); -- Composite index for common filter combinations CREATE INDEX CONCURRENTLY idx_executions_workspace_status ON executions(workspace_id, status, created_at) INCLUDE (error_message, retry_count); ``` **Use query result caching:** ```bash # Enable query cache for workspace metadata cloudflow config set cache.workspace.ttl 3600 # Configure Redis cache backend cloudflow config set cache.backend redis cloudflow config set cache.redis.host redis.cloudflow.svc.cluster.local cloudflow config set cache.redis.port 6379 ``` ### High API Latency #### Latency Breakdown Analysis ```bash # Generate latency report cloudflow metrics latency-report --endpoint \"/api/v1/workflows\" --last 24h # Sample output: # Endpoint: POST /api/v1/workflows/execute # P50: 245ms | P95: 1823ms | P99: 4521ms # Breakdown: # - Auth: 45ms (18%) # - DB Query: 156ms (64%) # - Business Logic: 32ms (13%) # - Response Serialization: 12ms (5%) ``` **Network latency issues:** ```bash # Test connectivity to CloudFlow API time curl -w \"@curl-format.txt\" -o /dev/null -s https://api.cloudflow.io/health # Create curl-format.txt: cat > curl-format.txt << EOF time_namelookup: %{time_namelookup}s\\n time_connect: %{time_connect}s\\n time_appconnect: %{time_appconnect}s\\n time_pretransfer: %{time_pretransfer}s\\n time_redirect: %{time_redirect}s\\n time_starttransfer: %{time_starttransfer}s\\n ----------\\n time_total: %{time_total}s\\n EOF # Trace route to API endpoint traceroute api.cloudflow.io # Check DNS resolution time dig api.cloudflow.io | grep \"Query time\" ``` ### Memory Leaks #### Detection ```bash # Monitor CloudFlow service memory usage kubectl top pods -n cloudflow --sort-by=memory # Get detailed memory metrics for specific pod kubectl exec -n cloudflow deploy/cloudflow-api -- \\ curl localhost:9090/metrics | grep memory # Check for OOMKilled pods kubectl get pods -n cloudflow --field-selector=status.phase=Failed | \\ grep OOMKilled # Review memory limits and requests kubectl describe deployment cloudflow-api -n cloudflow | \\ grep -A 5 \"Limits\\|Requests\" ``` #### Common Causes **1.",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_3",
              "content": "Workflow Context Accumulation** Large workflow executions may accumulate state in memory. **Solution:** ```bash # Configure context cleanup cloudflow config set workflow.context.max_size_mb 100 cloudflow config set workflow.context.cleanup_threshold 0.8 # Enable context persistence to disk cloudflow config set workflow.context.persistence.enabled true cloudflow config set workflow.context.persistence.backend s3 ``` **2. Connection Pool Leaks** **Diagnosis:** ```bash # Check active connections cloudflow db connections --verbose # Expected output: # Active: 45/100 # Idle: 23 # Waiting: 2 # Average age: 245s ``` **Resolution:** ```bash # Adjust connection pool settings cloudflow config set db.pool.max_connections 100 cloudflow config set db.pool.min_connections 10 cloudflow config set db.pool.idle_timeout 300 cloudflow config set db.pool.max_lifetime 1800 # Force connection pool reset cloudflow db pool reset --confirm ``` **3. Event Stream Buffers** Unbounded event buffers can cause memory exhaustion. ```bash # Configure event buffer limits cloudflow config set events.buffer.max_size 10000 cloudflow config set events.buffer.overflow_strategy drop_oldest # Enable event streaming to external sink cloudflow config set events.sink.type kafka cloudflow config set events.sink.kafka.brokers \"kafka-1:9092,kafka-2:9092\" cloudflow config set events.sink.kafka.topic cloudflow-events ``` --- ## Database Connection Issues ### Connection Pool Exhaustion #### Symptoms - Error: `could not obtain connection from pool within 5000ms` - Error: `connection pool exhausted (100/100 connections in use)` - API requests fail with `503 Service Unavailable` - Database CPU usage normal, but connection count at maximum #### Investigation ```bash # Check current connection pool status cloudflow db pool status --detailed # Output example: # Pool Statistics: # Total Connections: 100/100 (100%) # Active: 87 # Idle: 13 # Waiting Requests: 45 # Average Wait Time: 3420ms # Max Wait Time: 8234ms # Identify long-running queries cloudflow db queries --status running --min-duration 30000 # Check connection distribution by client SELECT application_name, state, COUNT(*) as conn_count, AVG(EXTRACT(EPOCH FROM (NOW() - state_change))) as avg_duration_sec FROM pg_stat_activity WHERE datname = 'cloudflow_production' GROUP BY application_name, state ORDER BY conn_count DESC; ``` #### Resolution **Immediate mitigation:** ```bash # Temporarily increase connection limit (requires database restart) cloudflow db config set max_connections 150 # Kill idle connections older than 5 minutes cloudflow db connections kill --idle-timeout 300 # Restart connection pool without downtime kubectl rollout restart deployment/cloudflow-api -n cloudflow kubectl rollout status deployment/cloudflow-api -n cloudflow ``` **Long-term solutions:** 1.",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_4",
              "content": "**Technology**: Node.js with worker pool pattern **Replicas**: 8 pods (production), auto-scaling 6-16 **Resource Allocation**: 2 vCPU, 4GB RAM per pod **Key Responsibilities**: - Consume notification events from Kafka - Template rendering with Handlebars (cached in Redis) - Multi-channel delivery (Email via SendGrid, SMS via Twilio, Webhooks) - Retry logic with dead letter queue for failed deliveries - Delivery status tracking and analytics - User preference management (opt-in/opt-out) **Notification Flow**: ``` Event Source \u2192 Kafka (notifications.email topic) \u2502 \u25bc Notification Service Consumer \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u25bc \u25bc \u25bc [Email Queue] [SMS Queue] [Webhook Queue] \u2502 \u2502 \u2502 \u25bc \u25bc \u25bc [SendGrid] [Twilio] [HTTP POST] \u2502 \u2502 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u25bc Delivery Status \u2192 PostgreSQL ``` **Channel Configuration**: - Email: SendGrid with IP warming, DKIM/SPF configured - SMS: Twilio with fallback to AWS SNS - Webhook: Signed payloads (HMAC-SHA256), retry 3 times with exponential backoff - In-app: WebSocket connections via Socket.io **Template Management**: - Templates stored in PostgreSQL with versioning - Compiled templates cached in Redis (1-hour TTL) - Support for localization (i18n) with 12 languages - A/B testing capability for email campaigns **Performance Targets**: - Email delivery: < 5 seconds (P99) from event to SendGrid - SMS delivery: < 3 seconds (P99) - Throughput: 10,000 notifications per minute --- ## Data Flow Architecture ### Synchronous Request Flow Client requests follow a synchronous path through the API Gateway to backend services: ``` 1. Client \u2192 Load Balancer (TLS termination) 2. Load Balancer \u2192 API Gateway (HTTP/2) 3. API Gateway \u2192 Auth Service (JWT validation via gRPC) 4. API Gateway \u2192 Target Service (HTTP/REST or gRPC) 5. Target Service \u2192 Database/Cache (data retrieval) 6. Response propagates back through the chain ``` **Timeout Configuration**: - Client \u2192 Load Balancer: 60 seconds - Load Balancer \u2192 API Gateway: 55 seconds - API Gateway \u2192 Services: 45 seconds - Service \u2192 Database: 10 seconds ### Asynchronous Event Flow Long-running operations and inter-service communication use event-driven patterns: ``` 1. Service publishes event \u2192 Kafka topic 2. Kafka persists event (replication factor: 3) 3. Consumer groups subscribe to topics 4. Consumers process events with at-least-once delivery 5. State updates written to PostgreSQL 6.",
              "score": null
            }
          ],
          "latency_ms": 1151.5308639973227,
          "query_id": "realistic_006",
          "dimension": "problem",
          "query": "The API Gateway pods keep getting OOMKilled"
        },
        {
          "key_facts": [
            "2 vCPU, 4GB RAM per pod"
          ],
          "found_facts": [
            "2 vCPU, 4GB RAM per pod"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "architecture_overview"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_1",
              "content": "**Technology**: Node.js with Express.js framework **Replicas**: 12 pods (production), auto-scaling 8-20 based on CPU **Resource Allocation**: 2 vCPU, 4GB RAM per pod **Key Responsibilities**: - JWT token validation (delegated to Auth Service for initial validation) - Rate limiting: 1000 requests per minute per API key (sliding window) - Request/response transformation and validation using JSON Schema - Routing to downstream services based on URL path patterns - CORS handling for web clients - Request/response logging and correlation ID injection **Critical Endpoints**: - `POST /api/v1/workflows` - Create new workflow - `GET /api/v1/workflows/:id` - Retrieve workflow status - `POST /api/v1/workflows/:id/execute` - Trigger workflow execution - `GET /api/v1/workflows/:id/history` - Get execution history **Dependencies**: - Auth Service (for token validation) - Redis (for rate limiting counters) - All downstream microservices **Performance Targets**: - P50 latency: < 50ms - P99 latency: < 200ms - Throughput: 10,000 RPS sustained --- ### Auth Service **Purpose**: Centralized authentication and authorization service handling user identity, token generation, and permission validation. **Technology**: Go with gRPC for internal communication, REST for external **Replicas**: 8 pods (production), auto-scaling 6-12 **Resource Allocation**: 1 vCPU, 2GB RAM per pod **Key Responsibilities**: - User authentication via multiple providers (OAuth2, SAML, local credentials) - JWT token generation and validation (RS256 algorithm) - Role-based access control (RBAC) with fine-grained permissions - Session management with Redis-backed storage - API key generation and validation for service accounts - MFA enforcement for administrative operations **Authentication Flow**: ``` Client Request \u2192 API Gateway \u2192 Auth Service \u2502 \u251c\u2500 Validate credentials \u251c\u2500 Check MFA if required \u251c\u2500 Generate JWT (15min expiry) \u251c\u2500 Generate refresh token (7 days) \u2514\u2500 Store session in Redis ``` **Token Structure**: - Access Token: JWT with 15-minute expiry - Refresh Token: Opaque token with 7-day expiry, stored in PostgreSQL - Claims: user_id, email, roles[], permissions[], tenant_id **Security Features**: - Password hashing: Argon2id with 64MB memory, 4 iterations - Token rotation on refresh to prevent replay attacks - Brute force protection: 5 failed attempts \u2192 15-minute lockout - Secrets stored in HashiCorp Vault, rotated every 90 days **Performance Targets**: - Token validation: < 10ms (P99) - Token generation: < 50ms (P99) - Throughput: 5,000 RPS for validation operations --- ### Workflow Engine **Purpose**: Core orchestration service that executes workflow definitions, manages state transitions, and coordinates task execution across distributed systems.",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_10",
              "content": "- Execution start rate: 500 per second - Completion rate: 450 per second (average 2-second execution time) **Database**: - Read throughput: 50,000 queries per second (across replicas) - Write throughput: 15,000 transactions per second - Connection capacity: 2,000 concurrent connections **Kafka**: - Message ingestion: 100,000 messages per second - Consumer throughput: 80,000 messages per second (aggregated) - End-to-end latency: < 100ms (P99) ### Resource Utilization **CPU Utilization** (Target: 60-70% average): - API Gateway: 55% average, 80% peak - Workflow Engine: 65% average, 85% peak - Auth Service: 40% average, 60% peak **Memory Utilization** (Target: < 80%): - API Gateway: 2.5GB average per pod (4GB allocated) - Workflow Engine: 6GB average per pod (8GB allocated) - Notification Service: 2.8GB average per pod (4GB allocated) **Network Throughput**: - Ingress: 2 Gbps average, 5 Gbps peak - Egress: 1.5 Gbps average, 4 Gbps peak - Internal (service mesh): 8 Gbps average ### Monitoring & Alerting **Key Metrics** (Monitored via Prometheus): - Request rate (per service, per endpoint) - Error rate (4xx, 5xx responses) - Latency percentiles (P50, P95, P99) - Resource utilization (CPU, memory, disk) - Cache hit/miss ratios - Database connection pool saturation - Kafka consumer lag **Alerts** (via PagerDuty): - P1 (Immediate): API error rate > 1%, database connection failure - P2 (< 15 min): P99 latency > 500ms, cache hit rate < 80% - P3 (< 1 hour): Resource utilization > 85%, replica count < minimum **SLI/SLO/SLA**: - SLI: API success rate (non-5xx responses) - SLO: 99.9% uptime per month (43 minutes downtime allowance) - SLA: 99.5% uptime guarantee (customer-facing SLA with credits) --- ## Disaster Recovery ### Recovery Objectives **RPO (Recovery Point Objective): 1 hour** - Maximum acceptable data loss: 1 hour of transactions - Achieved through: Continuous database replication + hourly snapshots - Kafka retention: 7 days allows event replay **RTO (Recovery Time Objective): 4 hours** - Maximum acceptable downtime: 4 hours for full system recovery - Includes: Failover, data verification, and service restoration - Automated runbooks reduce RTO to < 2 hours for common scenarios ### High Availability Architecture **Multi-AZ Deployment**: ``` Region: us-east-1 AZ-1a: AZ-1b: AZ-1c: - API Gateway (4) - API Gateway (4) - API Gateway (4) - Workflow Engine (6) - Workflow Engine (5) - Workflow Engine (5) - Auth Service (3) - Auth Service (3) - Auth",
              "score": null
            },
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_6",
              "content": "**Verify Service Health**: ```bash kubectl get pods -n cloudflow-prod curl https://api.cloudflow.io/health ``` **Recovery Time Objective (RTO)**: 4 hours **Recovery Point Objective (RPO)**: 24 hours --- ## Scaling and Performance ### Horizontal Pod Autoscaling CloudFlow is configured with Horizontal Pod Autoscaler (HPA) to automatically scale based on resource utilization: ```yaml # hpa.yaml apiVersion: autoscaling/v2 kind: HorizontalPodAutoscaler metadata: name: cloudflow-api-hpa namespace: cloudflow-prod spec: scaleTargetRef: apiVersion: apps/v1 kind: Deployment name: cloudflow-api minReplicas: 3 maxReplicas: 10 metrics: - type: Resource resource: name: cpu target: type: Utilization averageUtilization: 70 - type: Resource resource: name: memory target: type: Utilization averageUtilization: 80 behavior: scaleDown: stabilizationWindowSeconds: 300 policies: - type: Percent value: 50 periodSeconds: 60 scaleUp: stabilizationWindowSeconds: 60 policies: - type: Percent value: 100 periodSeconds: 30 - type: Pods value: 2 periodSeconds: 30 selectPolicy: Max ``` Monitor autoscaling events: ```bash kubectl get hpa -n cloudflow-prod -w kubectl describe hpa cloudflow-api-hpa -n cloudflow-prod ``` ### Performance Optimization #### Connection Pooling Ensure optimal database connection pooling settings in PgBouncer: - **Pool Mode**: Transaction (optimal for microservices) - **Default Pool Size**: 25 connections per user - **Max DB Connections**: 100 (matches PostgreSQL `max_connections`) #### Redis Caching Implement Redis caching for frequently accessed data: ```javascript // Example caching strategy const CACHE_TTL = { workflows: 300, // 5 minutes userSessions: 3600, // 1 hour apiResults: 60 // 1 minute }; ``` #### Load Testing Perform regular load testing to validate scaling configuration: ```bash # Install k6 for load testing kubectl apply -f - <<EOF apiVersion: v1 kind: ConfigMap metadata: name: loadtest-script namespace: cloudflow-prod data: script.js: | import http from 'k6/http'; import { check, sleep } from 'k6'; export let options = { stages: [ { duration: '2m', target: 100 }, { duration: '5m', target: 100 }, { duration: '2m', target: 200 }, { duration: '5m', target: 200 }, { duration: '2m', target: 0 }, ], thresholds: { http_req_duration: ['p(95)<500'], }, }; export default function () { let res = http.get('https://api.cloudflow.io/api/workflows'); check(res, { 'status is 200': (r) => r.status === 200 }); sleep(1); } EOF ``` **Performance Targets**: - P95 API latency: < 500ms - P99 API latency: < 1000ms - Error rate: < 0.1% - Throughput: > 1000 requests/second --- ## Security and Compliance ### Network Policies Implement network policies to restrict pod-to-pod communication: ```yaml # network-policy.yaml apiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: cloudflow-network-policy namespace: cloudflow-prod spec:",
              "score": null
            },
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_1",
              "content": "create cluster -f cluster-config.yaml ``` ### Storage Configuration Install the EBS CSI driver for persistent volumes: ```bash eksctl create addon --name aws-ebs-csi-driver \\ --cluster cloudflow-production \\ --service-account-role-arn arn:aws:iam::ACCOUNT_ID:role/AmazonEKS_EBS_CSI_DriverRole \\ --force ``` --- ## Kubernetes Deployment ### Namespace Setup Create the CloudFlow namespace and configure resource quotas: ```yaml # namespace.yaml apiVersion: v1 kind: Namespace metadata: name: cloudflow-prod labels: name: cloudflow-prod environment: production --- apiVersion: v1 kind: ResourceQuota metadata: name: cloudflow-quota namespace: cloudflow-prod spec: hard: requests.cpu: \"50\" requests.memory: 100Gi persistentvolumeclaims: \"10\" services.loadbalancers: \"2\" ``` Apply the namespace configuration: ```bash kubectl apply -f namespace.yaml ``` ### Environment Configuration CloudFlow requires the following environment variables: | Variable | Description | Example | Required | |----------|-------------|---------|----------| | `DATABASE_URL` | PostgreSQL connection string | `postgresql://user:pass@host:5432/cloudflow` | Yes | | `REDIS_URL` | Redis connection string | `redis://redis-master.cloudflow-prod.svc.cluster.local:6379` | Yes | | `JWT_SECRET` | Secret key for JWT token signing | `<generated-secret-256-bit>` | Yes | | `LOG_LEVEL` | Application log verbosity | `info`, `debug`, `warn`, `error` | No (default: `info`) | | `API_PORT` | API server port | `3000` | No (default: `3000`) | | `WORKER_CONCURRENCY` | Number of concurrent workers | `10` | No (default: `5`) | | `SESSION_SECRET` | Session encryption key | `<generated-secret-256-bit>` | Yes | | `AWS_REGION` | AWS region for services | `us-east-1` | Yes | | `S3_BUCKET` | S3 bucket for file storage | `cloudflow-prod-storage` | Yes | | `SMTP_HOST` | Email server hostname | `smtp.sendgrid.net` | No | | `SMTP_PORT` | Email server port | `587` | No | | `METRICS_ENABLED` | Enable Prometheus metrics | `true` | No (default: `false`) | Create a Kubernetes secret for sensitive variables: ```bash kubectl create secret generic cloudflow-secrets \\ --namespace cloudflow-prod \\ --from-literal=DATABASE_URL=\"postgresql://cloudflow:$(cat db-password.txt)@pgbouncer.cloudflow-prod.svc.cluster.local:5432/cloudflow\" \\ --from-literal=REDIS_URL=\"redis://redis-master.cloudflow-prod.svc.cluster.local:6379\" \\ --from-literal=JWT_SECRET=\"$(openssl rand -base64 32)\" \\ --from-literal=SESSION_SECRET=\"$(openssl rand -base64 32)\" ``` ### Helm Chart Deployment Add the CloudFlow Helm repository: ```bash helm repo add cloudflow https://charts.cloudflow.io helm repo update ``` Create a values file for production configuration: ```yaml # values-production.yaml replicaCount: 3 image: repository: 123456789012.dkr.ecr.us-east-1.amazonaws.com/cloudflow tag: \"2.4.0\" pullPolicy: IfNotPresent resources: limits: cpu: 2000m memory: 4Gi requests: cpu: 2000m memory: 4Gi autoscaling: enabled: true minReplicas: 3 maxReplicas: 10 targetCPUUtilizationPercentage: 70 targetMemoryUtilizationPercentage: 80 service: type: ClusterIP port: 80 targetPort: 3000 annotations: service.beta.kubernetes.io/aws-load-balancer-type: \"nlb\" ingress: enabled: true className: nginx annotations: cert-manager.io/cluster-issuer: \"letsencrypt-prod\" nginx.ingress.kubernetes.io/ssl-redirect: \"true\" hosts: - host: api.cloudflow.io paths: - path: / pathType: Prefix tls:",
              "score": null
            },
            {
              "doc_id": "user_guide",
              "chunk_id": "user_guide_fix_2",
              "content": "### HTTP Requests Make HTTP requests to any API endpoint: **Configuration:** - **Method**: GET, POST, PUT, PATCH, DELETE, HEAD - **URL**: Full endpoint URL (supports variable interpolation) - **Headers**: Custom headers as key-value pairs - **Query Parameters**: URL parameters - **Body**: JSON, form data, or raw text - **Authentication**: Basic Auth, Bearer Token, API Key, OAuth 2.0 **Example:** ```yaml - id: fetch_user action: http_request config: method: GET url: \"https://api.example.com/users/{{user_id}}\" headers: Authorization: \"Bearer {{secrets.API_TOKEN}}\" Content-Type: \"application/json\" timeout: 30 ``` **Response Handling:** Access the response in subsequent steps: - `{{steps.fetch_user.status}}` - HTTP status code - `{{steps.fetch_user.body}}` - Response body - `{{steps.fetch_user.headers}}` - Response headers ### Database Queries Execute queries against supported databases (PostgreSQL, MySQL, MongoDB, Redis): **SQL Databases (PostgreSQL, MySQL):** ```yaml - id: get_orders action: database_query config: connection: \"{{secrets.DB_CONNECTION_STRING}}\" query: | SELECT * FROM orders WHERE customer_id = $1 AND status = $2 ORDER BY created_at DESC LIMIT 10 parameters: - \"{{trigger.customer_id}}\" - \"pending\" ``` **MongoDB:** ```yaml - id: find_documents action: mongodb_query config: connection: \"{{secrets.MONGO_URI}}\" database: \"production\" collection: \"users\" operation: find filter: email: \"{{trigger.email}}\" options: limit: 1 ``` **Important Notes:** - Always use parameterized queries to prevent SQL injection - Connection strings should be stored in secrets - Query timeout is 30 seconds by default - Maximum result set size is 10MB ### Email Notifications Send emails via SMTP or integrated email providers: **Configuration:** ```yaml - id: send_notification action: email config: provider: \"smtp\" # or \"sendgrid\", \"mailgun\", \"ses\" from: \"notifications@cloudflow.io\" to: \"{{trigger.recipient}}\" cc: \"manager@company.com\" subject: \"Alert: {{alert_type}}\" body: | Hello {{user.name}}, This is an automated notification about {{event_description}}. Details: - Time: {{timestamp}} - Type: {{alert_type}} - Priority: {{priority}} Best regards, CloudFlow Automation html: true attachments: - name: \"report.pdf\" content: \"{{steps.generate_report.output}}\" encoding: \"base64\" ``` **Email Templates:** CloudFlow supports dynamic templates with conditional content: ``` {{#if priority == 'high'}} \u26a0\ufe0f URGENT: Immediate attention required {{else}} \u2139\ufe0f Informational notification {{/if}} ``` ### Slack Messages Send messages to Slack channels or users: **Channel Messages:** ```yaml - id: notify_team action: slack_message config: channel: \"#deployments\" text: \"Deployment completed successfully!\" blocks: - type: \"section\" text: type: \"mrkdwn\" text: \"*Deployment Summary*\\nEnvironment: {{environment}}\\nVersion: {{version}}\\nDuration: {{duration}}s\" - type: \"actions\" elements: - type: \"button\" text: \"View Logs\" url: \"{{logs_url}}\" thread_ts: \"{{trigger.thread_ts}}\" # Reply to thread ``` **Direct Messages:** ```yaml - id: dm_user action: slack_message config: user: \"{{user_slack_id}}\" text: \"Your report is ready for review.\"",
              "score": null
            }
          ],
          "latency_ms": 874.0942949989403,
          "query_id": "realistic_006",
          "dimension": "casual",
          "query": "api gateway resources"
        },
        {
          "key_facts": [
            "2 vCPU, 4GB RAM per pod"
          ],
          "found_facts": [],
          "missed_facts": [
            "2 vCPU, 4GB RAM per pod"
          ],
          "coverage": 0.0,
          "expected_docs": [
            "architecture_overview"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_0",
              "content": "# CloudFlow Platform - System Architecture Overview **Document Version:** 2.3.1 **Last Updated:** January 15, 2026 **Owner:** Platform Architecture Team **Status:** Production ## Executive Summary CloudFlow is a distributed, cloud-native workflow automation platform designed to orchestrate complex business processes at scale. The platform processes over 2.5 million workflow executions daily with an average P99 latency of 180ms for API operations and 4.2 seconds for workflow execution. This document provides a comprehensive overview of the system architecture, including microservices design, data flow patterns, infrastructure decisions, and operational considerations. ## Table of Contents 1. [High-Level Architecture](#high-level-architecture) 2. [Microservices Breakdown](#microservices-breakdown) 3. [Data Flow Architecture](#data-flow-architecture) 4. [Database Architecture](#database-architecture) 5. [Message Queue Patterns](#message-queue-patterns) 6. [Caching Strategy](#caching-strategy) 7. [Security Architecture](#security-architecture) 8. [Performance Characteristics](#performance-characteristics) 9. [Disaster Recovery](#disaster-recovery) --- ## High-Level Architecture CloudFlow follows a microservices architecture pattern deployed across multiple availability zones in AWS. The platform is designed with the following core principles: - **Scalability**: Horizontal scaling for all services with auto-scaling groups - **Resilience**: Circuit breakers, bulkheads, and graceful degradation - **Observability**: Distributed tracing, centralized logging, and comprehensive metrics - **Security**: Zero-trust network architecture with mTLS encryption ### Architecture Diagram (Conceptual) ``` \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 Load Balancer (ALB) \u2502 \u2502 (TLS Termination - 443) \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u25bc \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 API Gateway Layer \u2502 \u2502 (Rate Limiting, Auth, Request Routing) \u2502 \u2514\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u2502 \u2502 \u2502 \u2502 \u25bc \u25bc \u25bc \u25bc \u25bc \u250c\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502Auth\u2502 \u2502Workflow\u2502 \u2502Scheduler\u2502 \u2502Notification\u2502 \u2502User Service\u2502 \u2502Svc \u2502 \u2502 Engine \u2502 \u2502 Service \u2502 \u2502 Service \u2502 \u2502 \u2502 \u2514\u2500\u252c\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u2502 \u2502 \u2502 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 \u2502 \u2502 \u25bc \u25bc \u25bc \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502PostgreSQL\u2502 \u2502 Redis \u2502 \u2502 Kafka \u2502 \u2502 Cluster \u2502 \u2502 Cluster \u2502 \u2502 Cluster \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 ``` ### Technology Stack - **Runtime**: Node.js 20.x (API Gateway, Workflow Engine), Go 1.21 (Auth Service, Scheduler) - **Container Orchestration**: Kubernetes 1.28 (EKS) - **Service Mesh**: Istio 1.20 for service-to-service communication - **Databases**: PostgreSQL 15.4, Redis 7.2 - **Message Broker**: Apache Kafka 3.6 - **Monitoring**: Prometheus, Grafana, Jaeger for distributed tracing - **Secrets Management**: HashiCorp Vault 1.15 --- ## Microservices Breakdown ### API Gateway **Purpose**: Single entry point for all client requests, providing authentication, rate limiting, request routing, and protocol translation.",
              "score": null
            },
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_6",
              "content": "**Verify Service Health**: ```bash kubectl get pods -n cloudflow-prod curl https://api.cloudflow.io/health ``` **Recovery Time Objective (RTO)**: 4 hours **Recovery Point Objective (RPO)**: 24 hours --- ## Scaling and Performance ### Horizontal Pod Autoscaling CloudFlow is configured with Horizontal Pod Autoscaler (HPA) to automatically scale based on resource utilization: ```yaml # hpa.yaml apiVersion: autoscaling/v2 kind: HorizontalPodAutoscaler metadata: name: cloudflow-api-hpa namespace: cloudflow-prod spec: scaleTargetRef: apiVersion: apps/v1 kind: Deployment name: cloudflow-api minReplicas: 3 maxReplicas: 10 metrics: - type: Resource resource: name: cpu target: type: Utilization averageUtilization: 70 - type: Resource resource: name: memory target: type: Utilization averageUtilization: 80 behavior: scaleDown: stabilizationWindowSeconds: 300 policies: - type: Percent value: 50 periodSeconds: 60 scaleUp: stabilizationWindowSeconds: 60 policies: - type: Percent value: 100 periodSeconds: 30 - type: Pods value: 2 periodSeconds: 30 selectPolicy: Max ``` Monitor autoscaling events: ```bash kubectl get hpa -n cloudflow-prod -w kubectl describe hpa cloudflow-api-hpa -n cloudflow-prod ``` ### Performance Optimization #### Connection Pooling Ensure optimal database connection pooling settings in PgBouncer: - **Pool Mode**: Transaction (optimal for microservices) - **Default Pool Size**: 25 connections per user - **Max DB Connections**: 100 (matches PostgreSQL `max_connections`) #### Redis Caching Implement Redis caching for frequently accessed data: ```javascript // Example caching strategy const CACHE_TTL = { workflows: 300, // 5 minutes userSessions: 3600, // 1 hour apiResults: 60 // 1 minute }; ``` #### Load Testing Perform regular load testing to validate scaling configuration: ```bash # Install k6 for load testing kubectl apply -f - <<EOF apiVersion: v1 kind: ConfigMap metadata: name: loadtest-script namespace: cloudflow-prod data: script.js: | import http from 'k6/http'; import { check, sleep } from 'k6'; export let options = { stages: [ { duration: '2m', target: 100 }, { duration: '5m', target: 100 }, { duration: '2m', target: 200 }, { duration: '5m', target: 200 }, { duration: '2m', target: 0 }, ], thresholds: { http_req_duration: ['p(95)<500'], }, }; export default function () { let res = http.get('https://api.cloudflow.io/api/workflows'); check(res, { 'status is 200': (r) => r.status === 200 }); sleep(1); } EOF ``` **Performance Targets**: - P95 API latency: < 500ms - P99 API latency: < 1000ms - Error rate: < 0.1% - Throughput: > 1000 requests/second --- ## Security and Compliance ### Network Policies Implement network policies to restrict pod-to-pod communication: ```yaml # network-policy.yaml apiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: cloudflow-network-policy namespace: cloudflow-prod spec:",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_10",
              "content": "- Execution start rate: 500 per second - Completion rate: 450 per second (average 2-second execution time) **Database**: - Read throughput: 50,000 queries per second (across replicas) - Write throughput: 15,000 transactions per second - Connection capacity: 2,000 concurrent connections **Kafka**: - Message ingestion: 100,000 messages per second - Consumer throughput: 80,000 messages per second (aggregated) - End-to-end latency: < 100ms (P99) ### Resource Utilization **CPU Utilization** (Target: 60-70% average): - API Gateway: 55% average, 80% peak - Workflow Engine: 65% average, 85% peak - Auth Service: 40% average, 60% peak **Memory Utilization** (Target: < 80%): - API Gateway: 2.5GB average per pod (4GB allocated) - Workflow Engine: 6GB average per pod (8GB allocated) - Notification Service: 2.8GB average per pod (4GB allocated) **Network Throughput**: - Ingress: 2 Gbps average, 5 Gbps peak - Egress: 1.5 Gbps average, 4 Gbps peak - Internal (service mesh): 8 Gbps average ### Monitoring & Alerting **Key Metrics** (Monitored via Prometheus): - Request rate (per service, per endpoint) - Error rate (4xx, 5xx responses) - Latency percentiles (P50, P95, P99) - Resource utilization (CPU, memory, disk) - Cache hit/miss ratios - Database connection pool saturation - Kafka consumer lag **Alerts** (via PagerDuty): - P1 (Immediate): API error rate > 1%, database connection failure - P2 (< 15 min): P99 latency > 500ms, cache hit rate < 80% - P3 (< 1 hour): Resource utilization > 85%, replica count < minimum **SLI/SLO/SLA**: - SLI: API success rate (non-5xx responses) - SLO: 99.9% uptime per month (43 minutes downtime allowance) - SLA: 99.5% uptime guarantee (customer-facing SLA with credits) --- ## Disaster Recovery ### Recovery Objectives **RPO (Recovery Point Objective): 1 hour** - Maximum acceptable data loss: 1 hour of transactions - Achieved through: Continuous database replication + hourly snapshots - Kafka retention: 7 days allows event replay **RTO (Recovery Time Objective): 4 hours** - Maximum acceptable downtime: 4 hours for full system recovery - Includes: Failover, data verification, and service restoration - Automated runbooks reduce RTO to < 2 hours for common scenarios ### High Availability Architecture **Multi-AZ Deployment**: ``` Region: us-east-1 AZ-1a: AZ-1b: AZ-1c: - API Gateway (4) - API Gateway (4) - API Gateway (4) - Workflow Engine (6) - Workflow Engine (5) - Workflow Engine (5) - Auth Service (3) - Auth Service (3) - Auth",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_5",
              "content": "Success/failure events published back to Kafka ``` **Event Schema**: ```json { \"event_id\": \"uuid-v4\", \"event_type\": \"workflow.execution.completed\", \"timestamp\": \"2026-01-15T10:30:00.000Z\", \"correlation_id\": \"request-trace-id\", \"payload\": { \"workflow_id\": \"wf-12345\", \"execution_id\": \"exec-67890\", \"status\": \"COMPLETED\", \"duration_ms\": 4230 }, \"metadata\": { \"source_service\": \"workflow-engine\", \"schema_version\": \"1.0\" } } ``` ### Inter-Service Communication Services communicate using two primary patterns: **Synchronous (gRPC)**: - Auth Service validation calls from API Gateway - User Service profile lookups - Low latency requirement (< 50ms) - Request-response pattern with circuit breaker **Asynchronous (Kafka)**: - Workflow execution events - Notification triggers - Audit log events - Eventual consistency acceptable --- ## Database Architecture ### PostgreSQL Primary Database **Cluster Configuration**: - Primary-replica setup with 1 primary + 2 read replicas - Instance type: db.r6g.2xlarge (8 vCPU, 64GB RAM) - Storage: 2TB gp3 SSD with 12,000 IOPS - Multi-AZ deployment for high availability - Automated backups: Daily snapshots, 30-day retention - Point-in-time recovery: 5-minute granularity **Database Schema Design**: ``` Core Tables: - users (5M rows): User accounts and profiles - workflows (2M rows): Workflow definitions - workflow_executions (500M rows, partitioned): Execution history - workflow_steps (2B rows, partitioned): Individual step records - schedules (100K rows): Scheduled workflow triggers - notifications (1B rows, partitioned): Notification delivery log Partitioning Strategy: - workflow_executions: Monthly partitions by created_at - workflow_steps: Monthly partitions by created_at - notifications: Weekly partitions by created_at - Automatic partition creation via cron job - Retention policy: Drop partitions older than 12 months ``` **Indexing Strategy**: - Primary keys: UUIDs with B-tree indexes - Frequently queried columns: Compound indexes (e.g., user_id + status + created_at) - JSONB columns: GIN indexes for workflow definitions - Full-text search: GiST indexes on description fields **Connection Pooling**: - PgBouncer in transaction mode - Pool size: 100 connections per service - Max client connections: 2000 - Connection timeout: 30 seconds **Performance Characteristics**: - Read query P99: < 20ms - Write query P99: < 50ms - Transaction throughput: 15,000 TPS - Replication lag: < 500ms --- ### Redis Caching Layer **Cluster Configuration**: - Redis Cluster with 6 nodes (3 primary + 3 replica) - Instance type: cache.r6g.xlarge (4 vCPU, 26GB RAM) - Total memory: 78GB usable cache space - Persistence: RDB snapshots every 5 minutes + AOF - Eviction policy: allkeys-lru **Cache Usage Patterns**: 1.",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_12",
              "content": "Resume normal operations - Recovery time: 1-3 hours depending on data volume - Data loss: None if corruption detected quickly ### Testing & Validation **DR Drill Schedule**: - Monthly: Automated failover test (single AZ failure simulation) - Quarterly: Full DR region failover (non-production hours) - Annually: Complete disaster simulation with all stakeholders **Last DR Test Results** (Dec 15, 2025): - Scenario: Full region failover - Actual RTO: 2 hours 23 minutes (target: 4 hours) - Actual RPO: 42 minutes (target: 1 hour) - Issues identified: DNS propagation slower than expected (resolved) - Success criteria: Met all recovery objectives ### Business Continuity **Communication Plan**: - Status page: status.cloudflow.com (updated every 15 minutes during incident) - Customer notifications: Email + SMS for all P1 incidents - Internal escalation: PagerDuty \u2192 Incident Commander \u2192 Engineering Manager \u2192 CTO **Data Retention Policy**: - Active data: 12 months in hot storage (PostgreSQL) - Archived data: 7 years in cold storage (S3 Glacier) - Audit logs: 7 years (compliance requirement) - Backup retention: 30 days standard, 90 days monthly snapshots --- ## Appendix ### Service Dependency Matrix | Service | Depends On | Critical Path | |---------|-----------|---------------| | API Gateway | Auth Service, Redis | Yes | | Auth Service | PostgreSQL, Redis, Vault | Yes | | Workflow Engine | PostgreSQL, Kafka, Redis | Yes | | Scheduler | PostgreSQL, Redis, Kafka | No | | Notification Service | Kafka, PostgreSQL, SendGrid, Twilio | No | ### Capacity Planning **Current Capacity** (Jan 2026): - Daily workflow executions: 2.5M - Active users: 150,000 - API requests per day: 50M - Database size: 1.2TB - Kafka throughput: 100K msg/sec peak **6-Month Projection** (Jul 2026): - Daily workflow executions: 4M (+60%) - Active users: 225,000 (+50%) - API requests per day: 80M (+60%) - Database size: 2TB (+67%) - Required scaling: +30% compute, +50% storage ### Contact Information - **Architecture Team**: architecture@cloudflow.internal - **On-Call Engineer**: PagerDuty escalation - **Security Team**: security@cloudflow.internal - **Documentation**: https://wiki.cloudflow.internal/architecture --- **Document Revision History**: - v2.3.1 (Jan 15, 2026): Updated performance metrics, added DR test results - v2.3.0 (Dec 1, 2025): Added multi-AZ deployment details - v2.2.0 (Oct 15, 2025): Security architecture overhaul - v2.1.0 (Sep 1, 2025): Initial Kafka migration documentation - v2.0.0 (Jul 1, 2025): Complete microservices rewrite",
              "score": null
            }
          ],
          "latency_ms": 1065.8749049980543,
          "query_id": "realistic_006",
          "dimension": "contextual",
          "query": "I'm provisioning infrastructure, what are the compute specs for API Gateway?"
        },
        {
          "key_facts": [
            "2 vCPU, 4GB RAM per pod"
          ],
          "found_facts": [
            "2 vCPU, 4GB RAM per pod"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "architecture_overview"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_10",
              "content": "- Execution start rate: 500 per second - Completion rate: 450 per second (average 2-second execution time) **Database**: - Read throughput: 50,000 queries per second (across replicas) - Write throughput: 15,000 transactions per second - Connection capacity: 2,000 concurrent connections **Kafka**: - Message ingestion: 100,000 messages per second - Consumer throughput: 80,000 messages per second (aggregated) - End-to-end latency: < 100ms (P99) ### Resource Utilization **CPU Utilization** (Target: 60-70% average): - API Gateway: 55% average, 80% peak - Workflow Engine: 65% average, 85% peak - Auth Service: 40% average, 60% peak **Memory Utilization** (Target: < 80%): - API Gateway: 2.5GB average per pod (4GB allocated) - Workflow Engine: 6GB average per pod (8GB allocated) - Notification Service: 2.8GB average per pod (4GB allocated) **Network Throughput**: - Ingress: 2 Gbps average, 5 Gbps peak - Egress: 1.5 Gbps average, 4 Gbps peak - Internal (service mesh): 8 Gbps average ### Monitoring & Alerting **Key Metrics** (Monitored via Prometheus): - Request rate (per service, per endpoint) - Error rate (4xx, 5xx responses) - Latency percentiles (P50, P95, P99) - Resource utilization (CPU, memory, disk) - Cache hit/miss ratios - Database connection pool saturation - Kafka consumer lag **Alerts** (via PagerDuty): - P1 (Immediate): API error rate > 1%, database connection failure - P2 (< 15 min): P99 latency > 500ms, cache hit rate < 80% - P3 (< 1 hour): Resource utilization > 85%, replica count < minimum **SLI/SLO/SLA**: - SLI: API success rate (non-5xx responses) - SLO: 99.9% uptime per month (43 minutes downtime allowance) - SLA: 99.5% uptime guarantee (customer-facing SLA with credits) --- ## Disaster Recovery ### Recovery Objectives **RPO (Recovery Point Objective): 1 hour** - Maximum acceptable data loss: 1 hour of transactions - Achieved through: Continuous database replication + hourly snapshots - Kafka retention: 7 days allows event replay **RTO (Recovery Time Objective): 4 hours** - Maximum acceptable downtime: 4 hours for full system recovery - Includes: Failover, data verification, and service restoration - Automated runbooks reduce RTO to < 2 hours for common scenarios ### High Availability Architecture **Multi-AZ Deployment**: ``` Region: us-east-1 AZ-1a: AZ-1b: AZ-1c: - API Gateway (4) - API Gateway (4) - API Gateway (4) - Workflow Engine (6) - Workflow Engine (5) - Workflow Engine (5) - Auth Service (3) - Auth Service (3) - Auth",
              "score": null
            },
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_6",
              "content": "**Verify Service Health**: ```bash kubectl get pods -n cloudflow-prod curl https://api.cloudflow.io/health ``` **Recovery Time Objective (RTO)**: 4 hours **Recovery Point Objective (RPO)**: 24 hours --- ## Scaling and Performance ### Horizontal Pod Autoscaling CloudFlow is configured with Horizontal Pod Autoscaler (HPA) to automatically scale based on resource utilization: ```yaml # hpa.yaml apiVersion: autoscaling/v2 kind: HorizontalPodAutoscaler metadata: name: cloudflow-api-hpa namespace: cloudflow-prod spec: scaleTargetRef: apiVersion: apps/v1 kind: Deployment name: cloudflow-api minReplicas: 3 maxReplicas: 10 metrics: - type: Resource resource: name: cpu target: type: Utilization averageUtilization: 70 - type: Resource resource: name: memory target: type: Utilization averageUtilization: 80 behavior: scaleDown: stabilizationWindowSeconds: 300 policies: - type: Percent value: 50 periodSeconds: 60 scaleUp: stabilizationWindowSeconds: 60 policies: - type: Percent value: 100 periodSeconds: 30 - type: Pods value: 2 periodSeconds: 30 selectPolicy: Max ``` Monitor autoscaling events: ```bash kubectl get hpa -n cloudflow-prod -w kubectl describe hpa cloudflow-api-hpa -n cloudflow-prod ``` ### Performance Optimization #### Connection Pooling Ensure optimal database connection pooling settings in PgBouncer: - **Pool Mode**: Transaction (optimal for microservices) - **Default Pool Size**: 25 connections per user - **Max DB Connections**: 100 (matches PostgreSQL `max_connections`) #### Redis Caching Implement Redis caching for frequently accessed data: ```javascript // Example caching strategy const CACHE_TTL = { workflows: 300, // 5 minutes userSessions: 3600, // 1 hour apiResults: 60 // 1 minute }; ``` #### Load Testing Perform regular load testing to validate scaling configuration: ```bash # Install k6 for load testing kubectl apply -f - <<EOF apiVersion: v1 kind: ConfigMap metadata: name: loadtest-script namespace: cloudflow-prod data: script.js: | import http from 'k6/http'; import { check, sleep } from 'k6'; export let options = { stages: [ { duration: '2m', target: 100 }, { duration: '5m', target: 100 }, { duration: '2m', target: 200 }, { duration: '5m', target: 200 }, { duration: '2m', target: 0 }, ], thresholds: { http_req_duration: ['p(95)<500'], }, }; export default function () { let res = http.get('https://api.cloudflow.io/api/workflows'); check(res, { 'status is 200': (r) => r.status === 200 }); sleep(1); } EOF ``` **Performance Targets**: - P95 API latency: < 500ms - P99 API latency: < 1000ms - Error rate: < 0.1% - Throughput: > 1000 requests/second --- ## Security and Compliance ### Network Policies Implement network policies to restrict pod-to-pod communication: ```yaml # network-policy.yaml apiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: cloudflow-network-policy namespace: cloudflow-prod spec:",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_2",
              "content": "Review Query Execution Plans** ```sql -- Connect to CloudFlow database cloudflow db connect --readonly -- Explain slow query EXPLAIN ANALYZE SELECT w.*, e.status, e.error_message FROM workflows w LEFT JOIN executions e ON w.id = e.workflow_id WHERE w.workspace_id = 'ws_abc123' AND e.created_at > NOW() - INTERVAL '7 days' ORDER BY e.created_at DESC; -- Check for missing indexes SELECT schemaname, tablename, indexname, indexdef FROM pg_indexes WHERE tablename IN ('workflows', 'executions', 'workflow_steps') ORDER BY tablename, indexname; ``` **3. Optimize Queries** Common optimization techniques: **Add missing indexes:** ```sql -- Index for workflow lookup by workspace CREATE INDEX CONCURRENTLY idx_workflows_workspace_created ON workflows(workspace_id, created_at DESC); -- Index for execution status queries CREATE INDEX CONCURRENTLY idx_executions_status_created ON executions(workflow_id, status, created_at DESC) WHERE status IN ('running', 'pending'); -- Composite index for common filter combinations CREATE INDEX CONCURRENTLY idx_executions_workspace_status ON executions(workspace_id, status, created_at) INCLUDE (error_message, retry_count); ``` **Use query result caching:** ```bash # Enable query cache for workspace metadata cloudflow config set cache.workspace.ttl 3600 # Configure Redis cache backend cloudflow config set cache.backend redis cloudflow config set cache.redis.host redis.cloudflow.svc.cluster.local cloudflow config set cache.redis.port 6379 ``` ### High API Latency #### Latency Breakdown Analysis ```bash # Generate latency report cloudflow metrics latency-report --endpoint \"/api/v1/workflows\" --last 24h # Sample output: # Endpoint: POST /api/v1/workflows/execute # P50: 245ms | P95: 1823ms | P99: 4521ms # Breakdown: # - Auth: 45ms (18%) # - DB Query: 156ms (64%) # - Business Logic: 32ms (13%) # - Response Serialization: 12ms (5%) ``` **Network latency issues:** ```bash # Test connectivity to CloudFlow API time curl -w \"@curl-format.txt\" -o /dev/null -s https://api.cloudflow.io/health # Create curl-format.txt: cat > curl-format.txt << EOF time_namelookup: %{time_namelookup}s\\n time_connect: %{time_connect}s\\n time_appconnect: %{time_appconnect}s\\n time_pretransfer: %{time_pretransfer}s\\n time_redirect: %{time_redirect}s\\n time_starttransfer: %{time_starttransfer}s\\n ----------\\n time_total: %{time_total}s\\n EOF # Trace route to API endpoint traceroute api.cloudflow.io # Check DNS resolution time dig api.cloudflow.io | grep \"Query time\" ``` ### Memory Leaks #### Detection ```bash # Monitor CloudFlow service memory usage kubectl top pods -n cloudflow --sort-by=memory # Get detailed memory metrics for specific pod kubectl exec -n cloudflow deploy/cloudflow-api -- \\ curl localhost:9090/metrics | grep memory # Check for OOMKilled pods kubectl get pods -n cloudflow --field-selector=status.phase=Failed | \\ grep OOMKilled # Review memory limits and requests kubectl describe deployment cloudflow-api -n cloudflow | \\ grep -A 5 \"Limits\\|Requests\" ``` #### Common Causes **1.",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_1",
              "content": "**Technology**: Node.js with Express.js framework **Replicas**: 12 pods (production), auto-scaling 8-20 based on CPU **Resource Allocation**: 2 vCPU, 4GB RAM per pod **Key Responsibilities**: - JWT token validation (delegated to Auth Service for initial validation) - Rate limiting: 1000 requests per minute per API key (sliding window) - Request/response transformation and validation using JSON Schema - Routing to downstream services based on URL path patterns - CORS handling for web clients - Request/response logging and correlation ID injection **Critical Endpoints**: - `POST /api/v1/workflows` - Create new workflow - `GET /api/v1/workflows/:id` - Retrieve workflow status - `POST /api/v1/workflows/:id/execute` - Trigger workflow execution - `GET /api/v1/workflows/:id/history` - Get execution history **Dependencies**: - Auth Service (for token validation) - Redis (for rate limiting counters) - All downstream microservices **Performance Targets**: - P50 latency: < 50ms - P99 latency: < 200ms - Throughput: 10,000 RPS sustained --- ### Auth Service **Purpose**: Centralized authentication and authorization service handling user identity, token generation, and permission validation. **Technology**: Go with gRPC for internal communication, REST for external **Replicas**: 8 pods (production), auto-scaling 6-12 **Resource Allocation**: 1 vCPU, 2GB RAM per pod **Key Responsibilities**: - User authentication via multiple providers (OAuth2, SAML, local credentials) - JWT token generation and validation (RS256 algorithm) - Role-based access control (RBAC) with fine-grained permissions - Session management with Redis-backed storage - API key generation and validation for service accounts - MFA enforcement for administrative operations **Authentication Flow**: ``` Client Request \u2192 API Gateway \u2192 Auth Service \u2502 \u251c\u2500 Validate credentials \u251c\u2500 Check MFA if required \u251c\u2500 Generate JWT (15min expiry) \u251c\u2500 Generate refresh token (7 days) \u2514\u2500 Store session in Redis ``` **Token Structure**: - Access Token: JWT with 15-minute expiry - Refresh Token: Opaque token with 7-day expiry, stored in PostgreSQL - Claims: user_id, email, roles[], permissions[], tenant_id **Security Features**: - Password hashing: Argon2id with 64MB memory, 4 iterations - Token rotation on refresh to prevent replay attacks - Brute force protection: 5 failed attempts \u2192 15-minute lockout - Secrets stored in HashiCorp Vault, rotated every 90 days **Performance Targets**: - Token validation: < 10ms (P99) - Token generation: < 50ms (P99) - Throughput: 5,000 RPS for validation operations --- ### Workflow Engine **Purpose**: Core orchestration service that executes workflow definitions, manages state transitions, and coordinates task execution across distributed systems.",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_0",
              "content": "# CloudFlow Platform - System Architecture Overview **Document Version:** 2.3.1 **Last Updated:** January 15, 2026 **Owner:** Platform Architecture Team **Status:** Production ## Executive Summary CloudFlow is a distributed, cloud-native workflow automation platform designed to orchestrate complex business processes at scale. The platform processes over 2.5 million workflow executions daily with an average P99 latency of 180ms for API operations and 4.2 seconds for workflow execution. This document provides a comprehensive overview of the system architecture, including microservices design, data flow patterns, infrastructure decisions, and operational considerations. ## Table of Contents 1. [High-Level Architecture](#high-level-architecture) 2. [Microservices Breakdown](#microservices-breakdown) 3. [Data Flow Architecture](#data-flow-architecture) 4. [Database Architecture](#database-architecture) 5. [Message Queue Patterns](#message-queue-patterns) 6. [Caching Strategy](#caching-strategy) 7. [Security Architecture](#security-architecture) 8. [Performance Characteristics](#performance-characteristics) 9. [Disaster Recovery](#disaster-recovery) --- ## High-Level Architecture CloudFlow follows a microservices architecture pattern deployed across multiple availability zones in AWS. The platform is designed with the following core principles: - **Scalability**: Horizontal scaling for all services with auto-scaling groups - **Resilience**: Circuit breakers, bulkheads, and graceful degradation - **Observability**: Distributed tracing, centralized logging, and comprehensive metrics - **Security**: Zero-trust network architecture with mTLS encryption ### Architecture Diagram (Conceptual) ``` \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 Load Balancer (ALB) \u2502 \u2502 (TLS Termination - 443) \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u25bc \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 API Gateway Layer \u2502 \u2502 (Rate Limiting, Auth, Request Routing) \u2502 \u2514\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u2502 \u2502 \u2502 \u2502 \u25bc \u25bc \u25bc \u25bc \u25bc \u250c\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502Auth\u2502 \u2502Workflow\u2502 \u2502Scheduler\u2502 \u2502Notification\u2502 \u2502User Service\u2502 \u2502Svc \u2502 \u2502 Engine \u2502 \u2502 Service \u2502 \u2502 Service \u2502 \u2502 \u2502 \u2514\u2500\u252c\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u2502 \u2502 \u2502 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 \u2502 \u2502 \u25bc \u25bc \u25bc \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502PostgreSQL\u2502 \u2502 Redis \u2502 \u2502 Kafka \u2502 \u2502 Cluster \u2502 \u2502 Cluster \u2502 \u2502 Cluster \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 ``` ### Technology Stack - **Runtime**: Node.js 20.x (API Gateway, Workflow Engine), Go 1.21 (Auth Service, Scheduler) - **Container Orchestration**: Kubernetes 1.28 (EKS) - **Service Mesh**: Istio 1.20 for service-to-service communication - **Databases**: PostgreSQL 15.4, Redis 7.2 - **Message Broker**: Apache Kafka 3.6 - **Monitoring**: Prometheus, Grafana, Jaeger for distributed tracing - **Secrets Management**: HashiCorp Vault 1.15 --- ## Microservices Breakdown ### API Gateway **Purpose**: Single entry point for all client requests, providing authentication, rate limiting, request routing, and protocol translation.",
              "score": null
            }
          ],
          "latency_ms": 875.8070430012594,
          "query_id": "realistic_006",
          "dimension": "negation",
          "query": "Why is 1GB RAM not enough for API Gateway?"
        },
        {
          "key_facts": [
            "/health",
            "/ready"
          ],
          "found_facts": [
            "/health",
            "/ready"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "deployment_guide"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_4",
              "content": "--set prometheus.prometheusSpec.retention=30d \\ --set prometheus.prometheusSpec.storageSpec.volumeClaimTemplate.spec.resources.requests.storage=100Gi ``` CloudFlow exposes Prometheus metrics on port 9090 at the `/metrics` endpoint. The metrics include: - **HTTP Metrics**: Request rate, latency, error rate - **Database Metrics**: Connection pool status, query duration - **Worker Metrics**: Job queue size, processing time, failure rate - **System Metrics**: CPU usage, memory usage, heap statistics Example metrics exposed: ``` # HELP cloudflow_http_requests_total Total number of HTTP requests # TYPE cloudflow_http_requests_total counter cloudflow_http_requests_total{method=\"GET\",route=\"/api/workflows\",status=\"200\"} 15420 # HELP cloudflow_http_request_duration_seconds HTTP request duration in seconds # TYPE cloudflow_http_request_duration_seconds histogram cloudflow_http_request_duration_seconds_bucket{method=\"POST\",route=\"/api/workflows\",le=\"0.1\"} 8234 # HELP cloudflow_worker_jobs_processed_total Total number of jobs processed # TYPE cloudflow_worker_jobs_processed_total counter cloudflow_worker_jobs_processed_total{status=\"success\"} 45230 # HELP cloudflow_database_connections Current database connections # TYPE cloudflow_database_connections gauge cloudflow_database_connections{state=\"active\"} 23 ``` ### Grafana Dashboards Access Grafana to view CloudFlow dashboards: ```bash kubectl port-forward -n monitoring svc/prometheus-grafana 3000:80 ``` Import the CloudFlow dashboard (ID: 15847) or use the provided JSON template. Key dashboard panels include: 1. **API Performance**: Request rate, P95/P99 latency, error rate 2. **Resource Usage**: CPU, memory, disk I/O per pod 3. **Database Health**: Connection pool utilization, query performance 4. **Worker Status**: Queue depth, processing rate, job success/failure ratio 5. **System Overview**: Pod status, replica count, autoscaling events ### Logging with CloudWatch CloudFlow logs are automatically shipped to CloudWatch Logs.",
              "score": null
            },
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_3",
              "content": "Deploy PostgreSQL using the Bitnami Helm chart: ```yaml # postgres-values.yaml global: postgresql: auth: username: cloudflow database: cloudflow existingSecret: postgres-credentials image: tag: \"14.10.0\" primary: resources: limits: cpu: 4000m memory: 8Gi requests: cpu: 2000m memory: 4Gi persistence: enabled: true size: 100Gi storageClass: gp3 extendedConfiguration: | max_connections = 100 shared_buffers = 2GB effective_cache_size = 6GB maintenance_work_mem = 512MB checkpoint_completion_target = 0.9 wal_buffers = 16MB default_statistics_target = 100 random_page_cost = 1.1 effective_io_concurrency = 200 work_mem = 10MB min_wal_size = 1GB max_wal_size = 4GB readReplicas: replicaCount: 2 resources: limits: cpu: 2000m memory: 4Gi requests: cpu: 1000m memory: 2Gi metrics: enabled: true serviceMonitor: enabled: true ``` Install PostgreSQL: ```bash # Create password secret kubectl create secret generic postgres-credentials \\ --namespace cloudflow-prod \\ --from-literal=postgres-password=\"$(openssl rand -base64 32)\" \\ --from-literal=password=\"$(openssl rand -base64 32)\" # Install PostgreSQL helm install postgresql bitnami/postgresql \\ --namespace cloudflow-prod \\ --values postgres-values.yaml \\ --wait ``` ### PgBouncer Configuration Deploy PgBouncer for connection pooling to handle up to 100 connections efficiently: ```yaml # pgbouncer.yaml apiVersion: v1 kind: ConfigMap metadata: name: pgbouncer-config namespace: cloudflow-prod data: pgbouncer.ini: | [databases] cloudflow = host=postgresql.cloudflow-prod.svc.cluster.local port=5432 dbname=cloudflow [pgbouncer] listen_addr = 0.0.0.0 listen_port = 5432 auth_type = md5 auth_file = /etc/pgbouncer/userlist.txt pool_mode = transaction max_client_conn = 1000 default_pool_size = 25 reserve_pool_size = 5 reserve_pool_timeout = 3 max_db_connections = 100 max_user_connections = 100 server_lifetime = 3600 server_idle_timeout = 600 log_connections = 1 log_disconnections = 1 log_pooler_errors = 1 --- apiVersion: apps/v1 kind: Deployment metadata: name: pgbouncer namespace: cloudflow-prod spec: replicas: 2 selector: matchLabels: app: pgbouncer template: metadata: labels: app: pgbouncer spec: containers: - name: pgbouncer image: pgbouncer/pgbouncer:1.21.0 ports: - containerPort: 5432 resources: requests: cpu: 500m memory: 512Mi limits: cpu: 1000m memory: 1Gi volumeMounts: - name: config mountPath: /etc/pgbouncer volumes: - name: config configMap: name: pgbouncer-config --- apiVersion: v1 kind: Service metadata: name: pgbouncer namespace: cloudflow-prod spec: selector: app: pgbouncer ports: - port: 5432 targetPort: 5432 type: ClusterIP ``` Apply the PgBouncer configuration: ```bash kubectl apply -f pgbouncer.yaml ``` ### Database Migrations Run database migrations before deploying new versions: ```bash # Create migration job kubectl create job cloudflow-migrate-$(date +%s) \\ --namespace cloudflow-prod \\ --image=123456789012.dkr.ecr.us-east-1.amazonaws.com/cloudflow:2.4.0 \\ -- npm run migrate # Monitor migration progress kubectl logs -f job/cloudflow-migrate-<timestamp> -n cloudflow-prod ``` --- ## Monitoring and Observability ### Prometheus Setup Install Prometheus using the kube-prometheus-stack: ```bash helm install prometheus prometheus-community/kube-prometheus-stack \\ --namespace monitoring \\ --create-namespace \\ --set prometheus.prometheusSpec.serviceMonitorSelectorNilUsesHelmValues=false \\",
              "score": null
            },
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_2",
              "content": "- secretName: cloudflow-tls hosts: - api.cloudflow.io healthCheck: enabled: true livenessProbe: path: /health initialDelaySeconds: 30 periodSeconds: 10 timeoutSeconds: 5 failureThreshold: 3 readinessProbe: path: /ready initialDelaySeconds: 10 periodSeconds: 5 timeoutSeconds: 3 failureThreshold: 3 env: - name: NODE_ENV value: \"production\" - name: LOG_LEVEL value: \"info\" - name: API_PORT value: \"3000\" - name: WORKER_CONCURRENCY value: \"10\" - name: AWS_REGION value: \"us-east-1\" - name: METRICS_ENABLED value: \"true\" envFrom: - secretRef: name: cloudflow-secrets podAnnotations: prometheus.io/scrape: \"true\" prometheus.io/port: \"9090\" prometheus.io/path: \"/metrics\" podSecurityContext: runAsNonRoot: true runAsUser: 1000 fsGroup: 1000 securityContext: allowPrivilegeEscalation: false capabilities: drop: - ALL readOnlyRootFilesystem: true persistence: enabled: true storageClass: gp3 accessMode: ReadWriteOnce size: 50Gi redis: enabled: true architecture: replication auth: enabled: true master: persistence: size: 20Gi replica: replicaCount: 2 persistence: size: 20Gi ``` Deploy CloudFlow using Helm: ```bash helm install cloudflow cloudflow/cloudflow \\ --namespace cloudflow-prod \\ --values values-production.yaml \\ --wait \\ --timeout 10m ``` ### Deployment Verification Verify the deployment status: ```bash # Check pod status kubectl get pods -n cloudflow-prod # Expected output: # NAME READY STATUS RESTARTS AGE # cloudflow-api-7d8f9c5b6d-4xk2p 1/1 Running 0 2m # cloudflow-api-7d8f9c5b6d-9hj5m 1/1 Running 0 2m # cloudflow-api-7d8f9c5b6d-tn8wq 1/1 Running 0 2m # Check service endpoints kubectl get svc -n cloudflow-prod # Check ingress kubectl get ingress -n cloudflow-prod ``` Test health endpoints: ```bash # Port forward for testing kubectl port-forward -n cloudflow-prod svc/cloudflow-api 8080:80 # Test health endpoint curl http://localhost:8080/health # Expected: {\"status\":\"healthy\",\"timestamp\":\"2026-01-24T10:30:00Z\"} # Test readiness endpoint curl http://localhost:8080/ready # Expected: {\"status\":\"ready\",\"dependencies\":{\"database\":\"connected\",\"redis\":\"connected\"}} ``` --- ## Database Configuration ### PostgreSQL Setup CloudFlow uses PostgreSQL 14 as its primary data store.",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_11",
              "content": "Service (2) - PostgreSQL Primary - PostgreSQL Replica - PostgreSQL Replica - Redis Primary (2) - Redis Replica (2) - Redis Replica (2) - Kafka Broker (2) - Kafka Broker (2) - Kafka Broker (1) ``` **Automatic Failover**: - Database: 30-60 seconds (automatic promotion of replica) - Redis: < 10 seconds (Sentinel-based failover) - Kafka: < 30 seconds (controller election) - Services: Kubernetes health checks with 10-second liveness probes ### Backup Strategy **Database Backups**: - Automated snapshots: Daily at 02:00 UTC - Retention: 30 days for daily, 90 days for monthly - Cross-region replication: Async replication to us-west-2 (15-minute lag) - Backup verification: Weekly automated restore test in staging environment **Backup Schedule**: ``` Daily: Full snapshot \u2192 S3 (encrypted) Hourly: WAL archives \u2192 S3 (point-in-time recovery) Weekly: Backup validation test Monthly: Long-term archive to Glacier ``` **Configuration Backups**: - Kubernetes manifests: Stored in Git (GitOps with ArgoCD) - Vault secrets: Automated snapshot every 6 hours - Kafka topic configurations: Exported daily to S3 ### Disaster Recovery Procedures **Scenario 1: Single AZ Failure** - Detection: Health checks fail for entire AZ (< 30 seconds) - Action: Traffic automatically routed to healthy AZs by ALB - Recovery time: < 5 minutes (no manual intervention) - Data loss: None (multi-AZ replication) **Scenario 2: Database Primary Failure** - Detection: Health check fails for primary database (< 30 seconds) - Action: Automatic promotion of read replica to primary - Recovery time: 30-60 seconds - Data loss: Minimal (< 1 second due to synchronous replication) **Scenario 3: Full Region Failure** - Detection: Multiple health check failures across all AZs (< 2 minutes) - Action: Manual failover to DR region (us-west-2) - Procedure: 1. Update DNS to point to DR region (TTL: 60 seconds) 2. Promote DR database replica to primary 3. Scale up DR region services to production capacity 4. Verify data consistency and integrity 5. Update monitoring dashboards - Recovery time: 2-4 hours (includes verification) - Data loss: < 1 hour (last cross-region replication) **Scenario 4: Data Corruption** - Detection: Data validation checks or user report - Action: Point-in-time recovery from WAL archives - Procedure: 1. Identify corruption time window 2. Restore from snapshot prior to corruption 3. Replay WAL logs up to corruption point 4. Verify data integrity 5.",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_10",
              "content": "database nc -zv cloudflow-db.internal.company.com 5432 # Trace route traceroute api.cloudflow.io # Capture packets tcpdump -i any -w /tmp/capture.pcap port 5432 ``` --- ## Escalation Procedures ### Severity Levels CloudFlow incidents are classified into four severity levels: #### SEV-1: Critical (P1) - **Definition:** Complete service outage or severe degradation affecting all users - **Examples:** - API returns 5xx errors for > 5 minutes - Database completely unavailable - Authentication system down - Data loss or corruption - **Response Time:** Immediate (< 15 minutes) - **Escalation:** Page on-call engineer immediately #### SEV-2: High (P2) - **Definition:** Major functionality impaired affecting multiple users - **Examples:** - Workflow execution success rate < 90% - Significant performance degradation (p95 latency > 10s) - Rate limiting affecting large customer segment - **Response Time:** < 1 hour - **Escalation:** Create incident ticket and notify on-call #### SEV-3: Medium (P3) - **Definition:** Partial functionality degraded affecting some users - **Examples:** - Intermittent failures for specific workflow types - Minor performance issues - Non-critical feature unavailable - **Response Time:** < 4 hours - **Escalation:** Create ticket, normal business hours support #### SEV-4: Low (P4) - **Definition:** Minor issues with minimal user impact - **Examples:** - Cosmetic issues - Documentation errors - Feature requests - **Response Time:** < 2 business days - **Escalation:** Standard support ticket ### Escalation Steps #### Step 1: Initial Assessment (0-5 minutes) ```bash # Run health check cloudflow health check --comprehensive # Check status page curl https://status.cloudflow.io/api/v1/status.json # Review recent changes cloudflow audit log --last 2h --event-type \"deployment,configuration\" # Check metrics dashboard cloudflow metrics dashboard --incident-mode ``` #### Step 2: Gather Information (5-15 minutes) Create incident document with: - Incident timestamp and duration - Affected services and endpoints - Error rates and user impact - Recent changes or deployments - Relevant log excerpts - Correlation IDs for failed requests ```bash # Generate incident report cloudflow debug incident-report \\ --start \"2026-01-24T10:30:00Z\" \\ --end \"2026-01-24T11:00:00Z\" \\ --output incident-report.md # Capture system snapshot cloudflow debug snapshot --output snapshot-$(date +%Y%m%d-%H%M%S).tar.gz ``` #### Step 3: Escalate Based on Severity **For SEV-1 (Critical):** ```bash # Page on-call engineer cloudflow incident create \\ --severity SEV-1 \\ --title \"Complete API outage\" \\ --description \"All API requests returning 503\" \\ --page-oncall # Notify status page cloudflow status update \\ --status \"major_outage\" \\ --message \"We are investigating a complete service",
              "score": null
            }
          ],
          "latency_ms": 920.3708630011533,
          "query_id": "realistic_007",
          "dimension": "original",
          "query": "What are the health check endpoints?"
        },
        {
          "key_facts": [
            "/health",
            "/ready"
          ],
          "found_facts": [
            "/health",
            "/ready"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "deployment_guide"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_4",
              "content": "--set prometheus.prometheusSpec.retention=30d \\ --set prometheus.prometheusSpec.storageSpec.volumeClaimTemplate.spec.resources.requests.storage=100Gi ``` CloudFlow exposes Prometheus metrics on port 9090 at the `/metrics` endpoint. The metrics include: - **HTTP Metrics**: Request rate, latency, error rate - **Database Metrics**: Connection pool status, query duration - **Worker Metrics**: Job queue size, processing time, failure rate - **System Metrics**: CPU usage, memory usage, heap statistics Example metrics exposed: ``` # HELP cloudflow_http_requests_total Total number of HTTP requests # TYPE cloudflow_http_requests_total counter cloudflow_http_requests_total{method=\"GET\",route=\"/api/workflows\",status=\"200\"} 15420 # HELP cloudflow_http_request_duration_seconds HTTP request duration in seconds # TYPE cloudflow_http_request_duration_seconds histogram cloudflow_http_request_duration_seconds_bucket{method=\"POST\",route=\"/api/workflows\",le=\"0.1\"} 8234 # HELP cloudflow_worker_jobs_processed_total Total number of jobs processed # TYPE cloudflow_worker_jobs_processed_total counter cloudflow_worker_jobs_processed_total{status=\"success\"} 45230 # HELP cloudflow_database_connections Current database connections # TYPE cloudflow_database_connections gauge cloudflow_database_connections{state=\"active\"} 23 ``` ### Grafana Dashboards Access Grafana to view CloudFlow dashboards: ```bash kubectl port-forward -n monitoring svc/prometheus-grafana 3000:80 ``` Import the CloudFlow dashboard (ID: 15847) or use the provided JSON template. Key dashboard panels include: 1. **API Performance**: Request rate, P95/P99 latency, error rate 2. **Resource Usage**: CPU, memory, disk I/O per pod 3. **Database Health**: Connection pool utilization, query performance 4. **Worker Status**: Queue depth, processing rate, job success/failure ratio 5. **System Overview**: Pod status, replica count, autoscaling events ### Logging with CloudWatch CloudFlow logs are automatically shipped to CloudWatch Logs.",
              "score": null
            },
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_3",
              "content": "Deploy PostgreSQL using the Bitnami Helm chart: ```yaml # postgres-values.yaml global: postgresql: auth: username: cloudflow database: cloudflow existingSecret: postgres-credentials image: tag: \"14.10.0\" primary: resources: limits: cpu: 4000m memory: 8Gi requests: cpu: 2000m memory: 4Gi persistence: enabled: true size: 100Gi storageClass: gp3 extendedConfiguration: | max_connections = 100 shared_buffers = 2GB effective_cache_size = 6GB maintenance_work_mem = 512MB checkpoint_completion_target = 0.9 wal_buffers = 16MB default_statistics_target = 100 random_page_cost = 1.1 effective_io_concurrency = 200 work_mem = 10MB min_wal_size = 1GB max_wal_size = 4GB readReplicas: replicaCount: 2 resources: limits: cpu: 2000m memory: 4Gi requests: cpu: 1000m memory: 2Gi metrics: enabled: true serviceMonitor: enabled: true ``` Install PostgreSQL: ```bash # Create password secret kubectl create secret generic postgres-credentials \\ --namespace cloudflow-prod \\ --from-literal=postgres-password=\"$(openssl rand -base64 32)\" \\ --from-literal=password=\"$(openssl rand -base64 32)\" # Install PostgreSQL helm install postgresql bitnami/postgresql \\ --namespace cloudflow-prod \\ --values postgres-values.yaml \\ --wait ``` ### PgBouncer Configuration Deploy PgBouncer for connection pooling to handle up to 100 connections efficiently: ```yaml # pgbouncer.yaml apiVersion: v1 kind: ConfigMap metadata: name: pgbouncer-config namespace: cloudflow-prod data: pgbouncer.ini: | [databases] cloudflow = host=postgresql.cloudflow-prod.svc.cluster.local port=5432 dbname=cloudflow [pgbouncer] listen_addr = 0.0.0.0 listen_port = 5432 auth_type = md5 auth_file = /etc/pgbouncer/userlist.txt pool_mode = transaction max_client_conn = 1000 default_pool_size = 25 reserve_pool_size = 5 reserve_pool_timeout = 3 max_db_connections = 100 max_user_connections = 100 server_lifetime = 3600 server_idle_timeout = 600 log_connections = 1 log_disconnections = 1 log_pooler_errors = 1 --- apiVersion: apps/v1 kind: Deployment metadata: name: pgbouncer namespace: cloudflow-prod spec: replicas: 2 selector: matchLabels: app: pgbouncer template: metadata: labels: app: pgbouncer spec: containers: - name: pgbouncer image: pgbouncer/pgbouncer:1.21.0 ports: - containerPort: 5432 resources: requests: cpu: 500m memory: 512Mi limits: cpu: 1000m memory: 1Gi volumeMounts: - name: config mountPath: /etc/pgbouncer volumes: - name: config configMap: name: pgbouncer-config --- apiVersion: v1 kind: Service metadata: name: pgbouncer namespace: cloudflow-prod spec: selector: app: pgbouncer ports: - port: 5432 targetPort: 5432 type: ClusterIP ``` Apply the PgBouncer configuration: ```bash kubectl apply -f pgbouncer.yaml ``` ### Database Migrations Run database migrations before deploying new versions: ```bash # Create migration job kubectl create job cloudflow-migrate-$(date +%s) \\ --namespace cloudflow-prod \\ --image=123456789012.dkr.ecr.us-east-1.amazonaws.com/cloudflow:2.4.0 \\ -- npm run migrate # Monitor migration progress kubectl logs -f job/cloudflow-migrate-<timestamp> -n cloudflow-prod ``` --- ## Monitoring and Observability ### Prometheus Setup Install Prometheus using the kube-prometheus-stack: ```bash helm install prometheus prometheus-community/kube-prometheus-stack \\ --namespace monitoring \\ --create-namespace \\ --set prometheus.prometheusSpec.serviceMonitorSelectorNilUsesHelmValues=false \\",
              "score": null
            },
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_0",
              "content": "# CloudFlow Platform - Deployment and Operations Guide **Version:** 2.4.0 **Last Updated:** January 2026 **Target Environment:** Production (AWS EKS) ## Table of Contents 1. [Overview](#overview) 2. [Prerequisites](#prerequisites) 3. [Infrastructure Setup](#infrastructure-setup) 4. [Kubernetes Deployment](#kubernetes-deployment) 5. [Database Configuration](#database-configuration) 6. [Monitoring and Observability](#monitoring-and-observability) 7. [Backup and Disaster Recovery](#backup-and-disaster-recovery) 8. [Scaling and Performance](#scaling-and-performance) 9. [Security and Compliance](#security-and-compliance) 10. [Troubleshooting](#troubleshooting) --- ## Overview CloudFlow is a cloud-native workflow orchestration platform designed for high-availability production environments. This guide provides comprehensive instructions for deploying and operating CloudFlow on Amazon EKS (Elastic Kubernetes Service). ### Architecture Summary CloudFlow consists of the following components: - **API Server**: REST API for workflow management (Node.js/Express) - **Worker Service**: Background job processor (Node.js) - **Scheduler**: Cron-based task scheduler (Node.js) - **PostgreSQL**: Primary data store (version 14) - **Redis**: Cache and message queue (version 7.0) - **PgBouncer**: Database connection pooler ### Deployment Model - **Namespace**: `cloudflow-prod` - **Cluster**: AWS EKS 1.28 - **Region**: us-east-1 (primary), us-west-2 (disaster recovery) - **High Availability**: Multi-AZ deployment across 3 availability zones --- ## Prerequisites Before beginning the deployment process, ensure you have the following: ### Required Tools - `kubectl` (v1.28 or later) - `helm` (v3.12 or later) - `aws-cli` (v2.13 or later) - `eksctl` (v0.165 or later) - `terraform` (v1.6 or later) - for infrastructure provisioning ### Access Requirements - AWS account with appropriate IAM permissions - EKS cluster admin access - Container registry access (ECR) - Domain name and SSL certificates - Secrets management access (AWS Secrets Manager or Vault) ### Network Requirements - VPC with at least 3 public and 3 private subnets - NAT Gateway configured - Security groups allowing: - Ingress: HTTPS (443), HTTP (80) - Internal: PostgreSQL (5432), Redis (6379) - Monitoring: Prometheus (9090), Grafana (3000) --- ## Infrastructure Setup ### EKS Cluster Creation Use the following `eksctl` configuration to create the EKS cluster: ```yaml # cluster-config.yaml apiVersion: eksctl.io/v1alpha5 kind: ClusterConfig metadata: name: cloudflow-production region: us-east-1 version: \"1.28\" availabilityZones: - us-east-1a - us-east-1b - us-east-1c vpc: cidr: 10.0.0.0/16 nat: gateway: HighlyAvailable managedNodeGroups: - name: cloudflow-workers instanceType: m5.xlarge minSize: 3 maxSize: 10 desiredCapacity: 5 volumeSize: 100 privateNetworking: true labels: role: worker environment: production tags: Environment: production Application: cloudflow iam: withAddonPolicies: ebs: true efs: true albIngress: true cloudWatch: true cloudWatch: clusterLogging: enableTypes: - api - audit - authenticator - controllerManager - scheduler ``` Create the cluster: ```bash eksctl",
              "score": null
            },
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_2",
              "content": "- secretName: cloudflow-tls hosts: - api.cloudflow.io healthCheck: enabled: true livenessProbe: path: /health initialDelaySeconds: 30 periodSeconds: 10 timeoutSeconds: 5 failureThreshold: 3 readinessProbe: path: /ready initialDelaySeconds: 10 periodSeconds: 5 timeoutSeconds: 3 failureThreshold: 3 env: - name: NODE_ENV value: \"production\" - name: LOG_LEVEL value: \"info\" - name: API_PORT value: \"3000\" - name: WORKER_CONCURRENCY value: \"10\" - name: AWS_REGION value: \"us-east-1\" - name: METRICS_ENABLED value: \"true\" envFrom: - secretRef: name: cloudflow-secrets podAnnotations: prometheus.io/scrape: \"true\" prometheus.io/port: \"9090\" prometheus.io/path: \"/metrics\" podSecurityContext: runAsNonRoot: true runAsUser: 1000 fsGroup: 1000 securityContext: allowPrivilegeEscalation: false capabilities: drop: - ALL readOnlyRootFilesystem: true persistence: enabled: true storageClass: gp3 accessMode: ReadWriteOnce size: 50Gi redis: enabled: true architecture: replication auth: enabled: true master: persistence: size: 20Gi replica: replicaCount: 2 persistence: size: 20Gi ``` Deploy CloudFlow using Helm: ```bash helm install cloudflow cloudflow/cloudflow \\ --namespace cloudflow-prod \\ --values values-production.yaml \\ --wait \\ --timeout 10m ``` ### Deployment Verification Verify the deployment status: ```bash # Check pod status kubectl get pods -n cloudflow-prod # Expected output: # NAME READY STATUS RESTARTS AGE # cloudflow-api-7d8f9c5b6d-4xk2p 1/1 Running 0 2m # cloudflow-api-7d8f9c5b6d-9hj5m 1/1 Running 0 2m # cloudflow-api-7d8f9c5b6d-tn8wq 1/1 Running 0 2m # Check service endpoints kubectl get svc -n cloudflow-prod # Check ingress kubectl get ingress -n cloudflow-prod ``` Test health endpoints: ```bash # Port forward for testing kubectl port-forward -n cloudflow-prod svc/cloudflow-api 8080:80 # Test health endpoint curl http://localhost:8080/health # Expected: {\"status\":\"healthy\",\"timestamp\":\"2026-01-24T10:30:00Z\"} # Test readiness endpoint curl http://localhost:8080/ready # Expected: {\"status\":\"ready\",\"dependencies\":{\"database\":\"connected\",\"redis\":\"connected\"}} ``` --- ## Database Configuration ### PostgreSQL Setup CloudFlow uses PostgreSQL 14 as its primary data store.",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_0",
              "content": "# CloudFlow Platform - System Architecture Overview **Document Version:** 2.3.1 **Last Updated:** January 15, 2026 **Owner:** Platform Architecture Team **Status:** Production ## Executive Summary CloudFlow is a distributed, cloud-native workflow automation platform designed to orchestrate complex business processes at scale. The platform processes over 2.5 million workflow executions daily with an average P99 latency of 180ms for API operations and 4.2 seconds for workflow execution. This document provides a comprehensive overview of the system architecture, including microservices design, data flow patterns, infrastructure decisions, and operational considerations. ## Table of Contents 1. [High-Level Architecture](#high-level-architecture) 2. [Microservices Breakdown](#microservices-breakdown) 3. [Data Flow Architecture](#data-flow-architecture) 4. [Database Architecture](#database-architecture) 5. [Message Queue Patterns](#message-queue-patterns) 6. [Caching Strategy](#caching-strategy) 7. [Security Architecture](#security-architecture) 8. [Performance Characteristics](#performance-characteristics) 9. [Disaster Recovery](#disaster-recovery) --- ## High-Level Architecture CloudFlow follows a microservices architecture pattern deployed across multiple availability zones in AWS. The platform is designed with the following core principles: - **Scalability**: Horizontal scaling for all services with auto-scaling groups - **Resilience**: Circuit breakers, bulkheads, and graceful degradation - **Observability**: Distributed tracing, centralized logging, and comprehensive metrics - **Security**: Zero-trust network architecture with mTLS encryption ### Architecture Diagram (Conceptual) ``` \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 Load Balancer (ALB) \u2502 \u2502 (TLS Termination - 443) \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u25bc \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 API Gateway Layer \u2502 \u2502 (Rate Limiting, Auth, Request Routing) \u2502 \u2514\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u2502 \u2502 \u2502 \u2502 \u25bc \u25bc \u25bc \u25bc \u25bc \u250c\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502Auth\u2502 \u2502Workflow\u2502 \u2502Scheduler\u2502 \u2502Notification\u2502 \u2502User Service\u2502 \u2502Svc \u2502 \u2502 Engine \u2502 \u2502 Service \u2502 \u2502 Service \u2502 \u2502 \u2502 \u2514\u2500\u252c\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u2502 \u2502 \u2502 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 \u2502 \u2502 \u25bc \u25bc \u25bc \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502PostgreSQL\u2502 \u2502 Redis \u2502 \u2502 Kafka \u2502 \u2502 Cluster \u2502 \u2502 Cluster \u2502 \u2502 Cluster \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 ``` ### Technology Stack - **Runtime**: Node.js 20.x (API Gateway, Workflow Engine), Go 1.21 (Auth Service, Scheduler) - **Container Orchestration**: Kubernetes 1.28 (EKS) - **Service Mesh**: Istio 1.20 for service-to-service communication - **Databases**: PostgreSQL 15.4, Redis 7.2 - **Message Broker**: Apache Kafka 3.6 - **Monitoring**: Prometheus, Grafana, Jaeger for distributed tracing - **Secrets Management**: HashiCorp Vault 1.15 --- ## Microservices Breakdown ### API Gateway **Purpose**: Single entry point for all client requests, providing authentication, rate limiting, request routing, and protocol translation.",
              "score": null
            }
          ],
          "latency_ms": 830.6066490004014,
          "query_id": "realistic_007",
          "dimension": "synonym",
          "query": "Which URLs should I use for health monitoring?"
        },
        {
          "key_facts": [
            "/health",
            "/ready"
          ],
          "found_facts": [
            "/health",
            "/ready"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "deployment_guide"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_2",
              "content": "- secretName: cloudflow-tls hosts: - api.cloudflow.io healthCheck: enabled: true livenessProbe: path: /health initialDelaySeconds: 30 periodSeconds: 10 timeoutSeconds: 5 failureThreshold: 3 readinessProbe: path: /ready initialDelaySeconds: 10 periodSeconds: 5 timeoutSeconds: 3 failureThreshold: 3 env: - name: NODE_ENV value: \"production\" - name: LOG_LEVEL value: \"info\" - name: API_PORT value: \"3000\" - name: WORKER_CONCURRENCY value: \"10\" - name: AWS_REGION value: \"us-east-1\" - name: METRICS_ENABLED value: \"true\" envFrom: - secretRef: name: cloudflow-secrets podAnnotations: prometheus.io/scrape: \"true\" prometheus.io/port: \"9090\" prometheus.io/path: \"/metrics\" podSecurityContext: runAsNonRoot: true runAsUser: 1000 fsGroup: 1000 securityContext: allowPrivilegeEscalation: false capabilities: drop: - ALL readOnlyRootFilesystem: true persistence: enabled: true storageClass: gp3 accessMode: ReadWriteOnce size: 50Gi redis: enabled: true architecture: replication auth: enabled: true master: persistence: size: 20Gi replica: replicaCount: 2 persistence: size: 20Gi ``` Deploy CloudFlow using Helm: ```bash helm install cloudflow cloudflow/cloudflow \\ --namespace cloudflow-prod \\ --values values-production.yaml \\ --wait \\ --timeout 10m ``` ### Deployment Verification Verify the deployment status: ```bash # Check pod status kubectl get pods -n cloudflow-prod # Expected output: # NAME READY STATUS RESTARTS AGE # cloudflow-api-7d8f9c5b6d-4xk2p 1/1 Running 0 2m # cloudflow-api-7d8f9c5b6d-9hj5m 1/1 Running 0 2m # cloudflow-api-7d8f9c5b6d-tn8wq 1/1 Running 0 2m # Check service endpoints kubectl get svc -n cloudflow-prod # Check ingress kubectl get ingress -n cloudflow-prod ``` Test health endpoints: ```bash # Port forward for testing kubectl port-forward -n cloudflow-prod svc/cloudflow-api 8080:80 # Test health endpoint curl http://localhost:8080/health # Expected: {\"status\":\"healthy\",\"timestamp\":\"2026-01-24T10:30:00Z\"} # Test readiness endpoint curl http://localhost:8080/ready # Expected: {\"status\":\"ready\",\"dependencies\":{\"database\":\"connected\",\"redis\":\"connected\"}} ``` --- ## Database Configuration ### PostgreSQL Setup CloudFlow uses PostgreSQL 14 as its primary data store.",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_10",
              "content": "database nc -zv cloudflow-db.internal.company.com 5432 # Trace route traceroute api.cloudflow.io # Capture packets tcpdump -i any -w /tmp/capture.pcap port 5432 ``` --- ## Escalation Procedures ### Severity Levels CloudFlow incidents are classified into four severity levels: #### SEV-1: Critical (P1) - **Definition:** Complete service outage or severe degradation affecting all users - **Examples:** - API returns 5xx errors for > 5 minutes - Database completely unavailable - Authentication system down - Data loss or corruption - **Response Time:** Immediate (< 15 minutes) - **Escalation:** Page on-call engineer immediately #### SEV-2: High (P2) - **Definition:** Major functionality impaired affecting multiple users - **Examples:** - Workflow execution success rate < 90% - Significant performance degradation (p95 latency > 10s) - Rate limiting affecting large customer segment - **Response Time:** < 1 hour - **Escalation:** Create incident ticket and notify on-call #### SEV-3: Medium (P3) - **Definition:** Partial functionality degraded affecting some users - **Examples:** - Intermittent failures for specific workflow types - Minor performance issues - Non-critical feature unavailable - **Response Time:** < 4 hours - **Escalation:** Create ticket, normal business hours support #### SEV-4: Low (P4) - **Definition:** Minor issues with minimal user impact - **Examples:** - Cosmetic issues - Documentation errors - Feature requests - **Response Time:** < 2 business days - **Escalation:** Standard support ticket ### Escalation Steps #### Step 1: Initial Assessment (0-5 minutes) ```bash # Run health check cloudflow health check --comprehensive # Check status page curl https://status.cloudflow.io/api/v1/status.json # Review recent changes cloudflow audit log --last 2h --event-type \"deployment,configuration\" # Check metrics dashboard cloudflow metrics dashboard --incident-mode ``` #### Step 2: Gather Information (5-15 minutes) Create incident document with: - Incident timestamp and duration - Affected services and endpoints - Error rates and user impact - Recent changes or deployments - Relevant log excerpts - Correlation IDs for failed requests ```bash # Generate incident report cloudflow debug incident-report \\ --start \"2026-01-24T10:30:00Z\" \\ --end \"2026-01-24T11:00:00Z\" \\ --output incident-report.md # Capture system snapshot cloudflow debug snapshot --output snapshot-$(date +%Y%m%d-%H%M%S).tar.gz ``` #### Step 3: Escalate Based on Severity **For SEV-1 (Critical):** ```bash # Page on-call engineer cloudflow incident create \\ --severity SEV-1 \\ --title \"Complete API outage\" \\ --description \"All API requests returning 503\" \\ --page-oncall # Notify status page cloudflow status update \\ --status \"major_outage\" \\ --message \"We are investigating a complete service",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_0",
              "content": "# CloudFlow Platform Troubleshooting Guide **Version:** 3.2.1 **Last Updated:** January 2026 **Audience:** Platform Engineers, DevOps, Support Teams ## Table of Contents 1. [Overview](#overview) 2. [Authentication & Authorization Issues](#authentication--authorization-issues) 3. [Performance Problems](#performance-problems) 4. [Database Connection Issues](#database-connection-issues) 5. [Workflow Execution Failures](#workflow-execution-failures) 6. [Rate Limiting & Throttling](#rate-limiting--throttling) 7. [Log Analysis & Debugging](#log-analysis--debugging) 8. [Escalation Procedures](#escalation-procedures) --- ## Overview This guide provides comprehensive troubleshooting steps for common CloudFlow platform issues encountered in production environments. Each section includes error symptoms, root cause analysis, resolution steps, and preventive measures. ### Quick Diagnostic Checklist Before diving into specific issues, perform these initial checks: - Verify service health: `cloudflow status --all` - Check API connectivity: `curl -I https://api.cloudflow.io/health` - Review recent deployments: `kubectl get deployments -n cloudflow --sort-by=.metadata.creationTimestamp` - Inspect platform metrics: `cloudflow metrics --last 1h` --- ## Authentication & Authorization Issues ### 401 Unauthorized Errors #### Symptoms - API requests return `401 Unauthorized` - Error message: `Authentication credentials were not provided or are invalid` - Frontend displays \"Session expired\" message #### Common Causes **1. Token Expiration** CloudFlow access tokens expire after 3600 seconds (1 hour) by default. Refresh tokens are valid for 30 days. **Verification:** ```bash # Decode JWT to check expiration echo $CF_ACCESS_TOKEN | cut -d'.' -f2 | base64 -d | jq '.exp' # Compare with current time date +%s ``` **Resolution:** ```bash # Refresh the access token curl -X POST https://api.cloudflow.io/auth/refresh \\ -H \"Content-Type: application/json\" \\ -d '{\"refresh_token\": \"'$CF_REFRESH_TOKEN'\"}' # Update environment variable export CF_ACCESS_TOKEN=\"<new_token>\" ``` **2. Invalid JWT Signature** Error: `JWT signature verification failed` **Causes:** - Token was modified or corrupted - Using wrong signing key - Token generated with different secret **Resolution:** ```bash # Validate token structure cloudflow auth validate --token $CF_ACCESS_TOKEN # Generate new token cloudflow auth login --username <user> --password <pass> # For service accounts cloudflow auth service-account --create --name <sa-name> --scopes \"workflow:read,workflow:write\" ``` **3. Clock Skew Issues** JWT validation fails when system clocks are out of sync (tolerance: \u00b1300 seconds).",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_0",
              "content": "# CloudFlow Platform - System Architecture Overview **Document Version:** 2.3.1 **Last Updated:** January 15, 2026 **Owner:** Platform Architecture Team **Status:** Production ## Executive Summary CloudFlow is a distributed, cloud-native workflow automation platform designed to orchestrate complex business processes at scale. The platform processes over 2.5 million workflow executions daily with an average P99 latency of 180ms for API operations and 4.2 seconds for workflow execution. This document provides a comprehensive overview of the system architecture, including microservices design, data flow patterns, infrastructure decisions, and operational considerations. ## Table of Contents 1. [High-Level Architecture](#high-level-architecture) 2. [Microservices Breakdown](#microservices-breakdown) 3. [Data Flow Architecture](#data-flow-architecture) 4. [Database Architecture](#database-architecture) 5. [Message Queue Patterns](#message-queue-patterns) 6. [Caching Strategy](#caching-strategy) 7. [Security Architecture](#security-architecture) 8. [Performance Characteristics](#performance-characteristics) 9. [Disaster Recovery](#disaster-recovery) --- ## High-Level Architecture CloudFlow follows a microservices architecture pattern deployed across multiple availability zones in AWS. The platform is designed with the following core principles: - **Scalability**: Horizontal scaling for all services with auto-scaling groups - **Resilience**: Circuit breakers, bulkheads, and graceful degradation - **Observability**: Distributed tracing, centralized logging, and comprehensive metrics - **Security**: Zero-trust network architecture with mTLS encryption ### Architecture Diagram (Conceptual) ``` \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 Load Balancer (ALB) \u2502 \u2502 (TLS Termination - 443) \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u25bc \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 API Gateway Layer \u2502 \u2502 (Rate Limiting, Auth, Request Routing) \u2502 \u2514\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u2502 \u2502 \u2502 \u2502 \u25bc \u25bc \u25bc \u25bc \u25bc \u250c\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502Auth\u2502 \u2502Workflow\u2502 \u2502Scheduler\u2502 \u2502Notification\u2502 \u2502User Service\u2502 \u2502Svc \u2502 \u2502 Engine \u2502 \u2502 Service \u2502 \u2502 Service \u2502 \u2502 \u2502 \u2514\u2500\u252c\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u2502 \u2502 \u2502 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 \u2502 \u2502 \u25bc \u25bc \u25bc \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502PostgreSQL\u2502 \u2502 Redis \u2502 \u2502 Kafka \u2502 \u2502 Cluster \u2502 \u2502 Cluster \u2502 \u2502 Cluster \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 ``` ### Technology Stack - **Runtime**: Node.js 20.x (API Gateway, Workflow Engine), Go 1.21 (Auth Service, Scheduler) - **Container Orchestration**: Kubernetes 1.28 (EKS) - **Service Mesh**: Istio 1.20 for service-to-service communication - **Databases**: PostgreSQL 15.4, Redis 7.2 - **Message Broker**: Apache Kafka 3.6 - **Monitoring**: Prometheus, Grafana, Jaeger for distributed tracing - **Secrets Management**: HashiCorp Vault 1.15 --- ## Microservices Breakdown ### API Gateway **Purpose**: Single entry point for all client requests, providing authentication, rate limiting, request routing, and protocol translation.",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_6",
              "content": "Data Validation Errors** ``` ValidationError: Field 'customer_id' is required but missing in 234 records ``` **Resolution:** ```bash # Add data quality checks cloudflow workflows update wf_9k2n4m8p1q \\ --step data_ingestion \\ --add-validator required_fields \\ --validator-config '{\"fields\": [\"customer_id\", \"timestamp\", \"amount\"]}' # Configure error handling cloudflow workflows update wf_9k2n4m8p1q \\ --step data_validation \\ --on-error continue \\ --error-threshold 5% # Fail if > 5% of records invalid ``` **2. External API Failures** ``` ExternalAPIError: API request to https://partner-api.example.com failed with status 502 ``` **Resolution:** ```bash # Add circuit breaker cloudflow workflows update wf_9k2n4m8p1q \\ --step external_api_call \\ --circuit-breaker-enabled true \\ --circuit-breaker-threshold 5 \\ --circuit-breaker-timeout 30000 # Configure fallback behavior cloudflow workflows update wf_9k2n4m8p1q \\ --step external_api_call \\ --fallback-action use_cached_data \\ --cache-ttl 3600 ``` --- ## Rate Limiting & Throttling ### 429 Too Many Requests #### Error Response ```json { \"error\": \"rate_limit_exceeded\", \"message\": \"API rate limit exceeded. Retry after 45 seconds.\", \"status\": 429, \"headers\": { \"X-RateLimit-Limit\": \"1000\", \"X-RateLimit-Remaining\": \"0\", \"X-RateLimit-Reset\": \"1706112345\", \"Retry-After\": \"45\" } } ``` #### Rate Limit Tiers CloudFlow enforces the following rate limits per workspace: | Tier | Requests/Minute | Requests/Hour | Concurrent Workflows | |------|-----------------|---------------|----------------------| | Free | 60 | 1,000 | 5 | | Standard | 1,000 | 50,000 | 50 | | Premium | 5,000 | 250,000 | 200 | | Enterprise | Custom | Custom | Unlimited | #### Checking Rate Limit Status ```bash # Check current rate limit status curl -I https://api.cloudflow.io/api/v1/workflows \\ -H \"Authorization: Bearer $CF_ACCESS_TOKEN\" # Extract rate limit headers curl -s -I https://api.cloudflow.io/api/v1/workflows \\ -H \"Authorization: Bearer $CF_ACCESS_TOKEN\" | \\ grep -i \"x-ratelimit\" # Output: # X-RateLimit-Limit: 1000 # X-RateLimit-Remaining: 247 # X-RateLimit-Reset: 1706112400 # Monitor rate limit usage cloudflow metrics query \\ --metric rate_limit_remaining \\ --workspace ws_abc123 \\ --last 1h \\ --interval 1m ``` #### Handling Rate Limits in Code **Python example with retry logic:** ```python import time import requests def cloudflow_api_call_with_retry(url, headers, max_retries=3): for attempt in range(max_retries): response = requests.get(url, headers=headers) if response.status_code == 429: retry_after = int(response.headers.get('Retry-After', 60)) print(f\"Rate limited.",
              "score": null
            }
          ],
          "latency_ms": 988.5230290019535,
          "query_id": "realistic_007",
          "dimension": "problem",
          "query": "My load balancer health checks are failing"
        },
        {
          "key_facts": [
            "/health",
            "/ready"
          ],
          "found_facts": [
            "/health",
            "/ready"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "deployment_guide"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_4",
              "content": "--set prometheus.prometheusSpec.retention=30d \\ --set prometheus.prometheusSpec.storageSpec.volumeClaimTemplate.spec.resources.requests.storage=100Gi ``` CloudFlow exposes Prometheus metrics on port 9090 at the `/metrics` endpoint. The metrics include: - **HTTP Metrics**: Request rate, latency, error rate - **Database Metrics**: Connection pool status, query duration - **Worker Metrics**: Job queue size, processing time, failure rate - **System Metrics**: CPU usage, memory usage, heap statistics Example metrics exposed: ``` # HELP cloudflow_http_requests_total Total number of HTTP requests # TYPE cloudflow_http_requests_total counter cloudflow_http_requests_total{method=\"GET\",route=\"/api/workflows\",status=\"200\"} 15420 # HELP cloudflow_http_request_duration_seconds HTTP request duration in seconds # TYPE cloudflow_http_request_duration_seconds histogram cloudflow_http_request_duration_seconds_bucket{method=\"POST\",route=\"/api/workflows\",le=\"0.1\"} 8234 # HELP cloudflow_worker_jobs_processed_total Total number of jobs processed # TYPE cloudflow_worker_jobs_processed_total counter cloudflow_worker_jobs_processed_total{status=\"success\"} 45230 # HELP cloudflow_database_connections Current database connections # TYPE cloudflow_database_connections gauge cloudflow_database_connections{state=\"active\"} 23 ``` ### Grafana Dashboards Access Grafana to view CloudFlow dashboards: ```bash kubectl port-forward -n monitoring svc/prometheus-grafana 3000:80 ``` Import the CloudFlow dashboard (ID: 15847) or use the provided JSON template. Key dashboard panels include: 1. **API Performance**: Request rate, P95/P99 latency, error rate 2. **Resource Usage**: CPU, memory, disk I/O per pod 3. **Database Health**: Connection pool utilization, query performance 4. **Worker Status**: Queue depth, processing rate, job success/failure ratio 5. **System Overview**: Pod status, replica count, autoscaling events ### Logging with CloudWatch CloudFlow logs are automatically shipped to CloudWatch Logs.",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_0",
              "content": "# CloudFlow Platform - System Architecture Overview **Document Version:** 2.3.1 **Last Updated:** January 15, 2026 **Owner:** Platform Architecture Team **Status:** Production ## Executive Summary CloudFlow is a distributed, cloud-native workflow automation platform designed to orchestrate complex business processes at scale. The platform processes over 2.5 million workflow executions daily with an average P99 latency of 180ms for API operations and 4.2 seconds for workflow execution. This document provides a comprehensive overview of the system architecture, including microservices design, data flow patterns, infrastructure decisions, and operational considerations. ## Table of Contents 1. [High-Level Architecture](#high-level-architecture) 2. [Microservices Breakdown](#microservices-breakdown) 3. [Data Flow Architecture](#data-flow-architecture) 4. [Database Architecture](#database-architecture) 5. [Message Queue Patterns](#message-queue-patterns) 6. [Caching Strategy](#caching-strategy) 7. [Security Architecture](#security-architecture) 8. [Performance Characteristics](#performance-characteristics) 9. [Disaster Recovery](#disaster-recovery) --- ## High-Level Architecture CloudFlow follows a microservices architecture pattern deployed across multiple availability zones in AWS. The platform is designed with the following core principles: - **Scalability**: Horizontal scaling for all services with auto-scaling groups - **Resilience**: Circuit breakers, bulkheads, and graceful degradation - **Observability**: Distributed tracing, centralized logging, and comprehensive metrics - **Security**: Zero-trust network architecture with mTLS encryption ### Architecture Diagram (Conceptual) ``` \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 Load Balancer (ALB) \u2502 \u2502 (TLS Termination - 443) \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u25bc \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 API Gateway Layer \u2502 \u2502 (Rate Limiting, Auth, Request Routing) \u2502 \u2514\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u2502 \u2502 \u2502 \u2502 \u25bc \u25bc \u25bc \u25bc \u25bc \u250c\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502Auth\u2502 \u2502Workflow\u2502 \u2502Scheduler\u2502 \u2502Notification\u2502 \u2502User Service\u2502 \u2502Svc \u2502 \u2502 Engine \u2502 \u2502 Service \u2502 \u2502 Service \u2502 \u2502 \u2502 \u2514\u2500\u252c\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u2502 \u2502 \u2502 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 \u2502 \u2502 \u25bc \u25bc \u25bc \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502PostgreSQL\u2502 \u2502 Redis \u2502 \u2502 Kafka \u2502 \u2502 Cluster \u2502 \u2502 Cluster \u2502 \u2502 Cluster \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 ``` ### Technology Stack - **Runtime**: Node.js 20.x (API Gateway, Workflow Engine), Go 1.21 (Auth Service, Scheduler) - **Container Orchestration**: Kubernetes 1.28 (EKS) - **Service Mesh**: Istio 1.20 for service-to-service communication - **Databases**: PostgreSQL 15.4, Redis 7.2 - **Message Broker**: Apache Kafka 3.6 - **Monitoring**: Prometheus, Grafana, Jaeger for distributed tracing - **Secrets Management**: HashiCorp Vault 1.15 --- ## Microservices Breakdown ### API Gateway **Purpose**: Single entry point for all client requests, providing authentication, rate limiting, request routing, and protocol translation.",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_10",
              "content": "database nc -zv cloudflow-db.internal.company.com 5432 # Trace route traceroute api.cloudflow.io # Capture packets tcpdump -i any -w /tmp/capture.pcap port 5432 ``` --- ## Escalation Procedures ### Severity Levels CloudFlow incidents are classified into four severity levels: #### SEV-1: Critical (P1) - **Definition:** Complete service outage or severe degradation affecting all users - **Examples:** - API returns 5xx errors for > 5 minutes - Database completely unavailable - Authentication system down - Data loss or corruption - **Response Time:** Immediate (< 15 minutes) - **Escalation:** Page on-call engineer immediately #### SEV-2: High (P2) - **Definition:** Major functionality impaired affecting multiple users - **Examples:** - Workflow execution success rate < 90% - Significant performance degradation (p95 latency > 10s) - Rate limiting affecting large customer segment - **Response Time:** < 1 hour - **Escalation:** Create incident ticket and notify on-call #### SEV-3: Medium (P3) - **Definition:** Partial functionality degraded affecting some users - **Examples:** - Intermittent failures for specific workflow types - Minor performance issues - Non-critical feature unavailable - **Response Time:** < 4 hours - **Escalation:** Create ticket, normal business hours support #### SEV-4: Low (P4) - **Definition:** Minor issues with minimal user impact - **Examples:** - Cosmetic issues - Documentation errors - Feature requests - **Response Time:** < 2 business days - **Escalation:** Standard support ticket ### Escalation Steps #### Step 1: Initial Assessment (0-5 minutes) ```bash # Run health check cloudflow health check --comprehensive # Check status page curl https://status.cloudflow.io/api/v1/status.json # Review recent changes cloudflow audit log --last 2h --event-type \"deployment,configuration\" # Check metrics dashboard cloudflow metrics dashboard --incident-mode ``` #### Step 2: Gather Information (5-15 minutes) Create incident document with: - Incident timestamp and duration - Affected services and endpoints - Error rates and user impact - Recent changes or deployments - Relevant log excerpts - Correlation IDs for failed requests ```bash # Generate incident report cloudflow debug incident-report \\ --start \"2026-01-24T10:30:00Z\" \\ --end \"2026-01-24T11:00:00Z\" \\ --output incident-report.md # Capture system snapshot cloudflow debug snapshot --output snapshot-$(date +%Y%m%d-%H%M%S).tar.gz ``` #### Step 3: Escalate Based on Severity **For SEV-1 (Critical):** ```bash # Page on-call engineer cloudflow incident create \\ --severity SEV-1 \\ --title \"Complete API outage\" \\ --description \"All API requests returning 503\" \\ --page-oncall # Notify status page cloudflow status update \\ --status \"major_outage\" \\ --message \"We are investigating a complete service",
              "score": null
            },
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_2",
              "content": "- secretName: cloudflow-tls hosts: - api.cloudflow.io healthCheck: enabled: true livenessProbe: path: /health initialDelaySeconds: 30 periodSeconds: 10 timeoutSeconds: 5 failureThreshold: 3 readinessProbe: path: /ready initialDelaySeconds: 10 periodSeconds: 5 timeoutSeconds: 3 failureThreshold: 3 env: - name: NODE_ENV value: \"production\" - name: LOG_LEVEL value: \"info\" - name: API_PORT value: \"3000\" - name: WORKER_CONCURRENCY value: \"10\" - name: AWS_REGION value: \"us-east-1\" - name: METRICS_ENABLED value: \"true\" envFrom: - secretRef: name: cloudflow-secrets podAnnotations: prometheus.io/scrape: \"true\" prometheus.io/port: \"9090\" prometheus.io/path: \"/metrics\" podSecurityContext: runAsNonRoot: true runAsUser: 1000 fsGroup: 1000 securityContext: allowPrivilegeEscalation: false capabilities: drop: - ALL readOnlyRootFilesystem: true persistence: enabled: true storageClass: gp3 accessMode: ReadWriteOnce size: 50Gi redis: enabled: true architecture: replication auth: enabled: true master: persistence: size: 20Gi replica: replicaCount: 2 persistence: size: 20Gi ``` Deploy CloudFlow using Helm: ```bash helm install cloudflow cloudflow/cloudflow \\ --namespace cloudflow-prod \\ --values values-production.yaml \\ --wait \\ --timeout 10m ``` ### Deployment Verification Verify the deployment status: ```bash # Check pod status kubectl get pods -n cloudflow-prod # Expected output: # NAME READY STATUS RESTARTS AGE # cloudflow-api-7d8f9c5b6d-4xk2p 1/1 Running 0 2m # cloudflow-api-7d8f9c5b6d-9hj5m 1/1 Running 0 2m # cloudflow-api-7d8f9c5b6d-tn8wq 1/1 Running 0 2m # Check service endpoints kubectl get svc -n cloudflow-prod # Check ingress kubectl get ingress -n cloudflow-prod ``` Test health endpoints: ```bash # Port forward for testing kubectl port-forward -n cloudflow-prod svc/cloudflow-api 8080:80 # Test health endpoint curl http://localhost:8080/health # Expected: {\"status\":\"healthy\",\"timestamp\":\"2026-01-24T10:30:00Z\"} # Test readiness endpoint curl http://localhost:8080/ready # Expected: {\"status\":\"ready\",\"dependencies\":{\"database\":\"connected\",\"redis\":\"connected\"}} ``` --- ## Database Configuration ### PostgreSQL Setup CloudFlow uses PostgreSQL 14 as its primary data store.",
              "score": null
            },
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_0",
              "content": "# CloudFlow Platform - Deployment and Operations Guide **Version:** 2.4.0 **Last Updated:** January 2026 **Target Environment:** Production (AWS EKS) ## Table of Contents 1. [Overview](#overview) 2. [Prerequisites](#prerequisites) 3. [Infrastructure Setup](#infrastructure-setup) 4. [Kubernetes Deployment](#kubernetes-deployment) 5. [Database Configuration](#database-configuration) 6. [Monitoring and Observability](#monitoring-and-observability) 7. [Backup and Disaster Recovery](#backup-and-disaster-recovery) 8. [Scaling and Performance](#scaling-and-performance) 9. [Security and Compliance](#security-and-compliance) 10. [Troubleshooting](#troubleshooting) --- ## Overview CloudFlow is a cloud-native workflow orchestration platform designed for high-availability production environments. This guide provides comprehensive instructions for deploying and operating CloudFlow on Amazon EKS (Elastic Kubernetes Service). ### Architecture Summary CloudFlow consists of the following components: - **API Server**: REST API for workflow management (Node.js/Express) - **Worker Service**: Background job processor (Node.js) - **Scheduler**: Cron-based task scheduler (Node.js) - **PostgreSQL**: Primary data store (version 14) - **Redis**: Cache and message queue (version 7.0) - **PgBouncer**: Database connection pooler ### Deployment Model - **Namespace**: `cloudflow-prod` - **Cluster**: AWS EKS 1.28 - **Region**: us-east-1 (primary), us-west-2 (disaster recovery) - **High Availability**: Multi-AZ deployment across 3 availability zones --- ## Prerequisites Before beginning the deployment process, ensure you have the following: ### Required Tools - `kubectl` (v1.28 or later) - `helm` (v3.12 or later) - `aws-cli` (v2.13 or later) - `eksctl` (v0.165 or later) - `terraform` (v1.6 or later) - for infrastructure provisioning ### Access Requirements - AWS account with appropriate IAM permissions - EKS cluster admin access - Container registry access (ECR) - Domain name and SSL certificates - Secrets management access (AWS Secrets Manager or Vault) ### Network Requirements - VPC with at least 3 public and 3 private subnets - NAT Gateway configured - Security groups allowing: - Ingress: HTTPS (443), HTTP (80) - Internal: PostgreSQL (5432), Redis (6379) - Monitoring: Prometheus (9090), Grafana (3000) --- ## Infrastructure Setup ### EKS Cluster Creation Use the following `eksctl` configuration to create the EKS cluster: ```yaml # cluster-config.yaml apiVersion: eksctl.io/v1alpha5 kind: ClusterConfig metadata: name: cloudflow-production region: us-east-1 version: \"1.28\" availabilityZones: - us-east-1a - us-east-1b - us-east-1c vpc: cidr: 10.0.0.0/16 nat: gateway: HighlyAvailable managedNodeGroups: - name: cloudflow-workers instanceType: m5.xlarge minSize: 3 maxSize: 10 desiredCapacity: 5 volumeSize: 100 privateNetworking: true labels: role: worker environment: production tags: Environment: production Application: cloudflow iam: withAddonPolicies: ebs: true efs: true albIngress: true cloudWatch: true cloudWatch: clusterLogging: enableTypes: - api - audit - authenticator - controllerManager - scheduler ``` Create the cluster: ```bash eksctl",
              "score": null
            }
          ],
          "latency_ms": 1107.4912430012773,
          "query_id": "realistic_007",
          "dimension": "casual",
          "query": "health endpoints"
        },
        {
          "key_facts": [
            "/health",
            "/ready"
          ],
          "found_facts": [],
          "missed_facts": [
            "/health",
            "/ready"
          ],
          "coverage": 0.0,
          "expected_docs": [
            "deployment_guide"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_11",
              "content": "Service (2) - PostgreSQL Primary - PostgreSQL Replica - PostgreSQL Replica - Redis Primary (2) - Redis Replica (2) - Redis Replica (2) - Kafka Broker (2) - Kafka Broker (2) - Kafka Broker (1) ``` **Automatic Failover**: - Database: 30-60 seconds (automatic promotion of replica) - Redis: < 10 seconds (Sentinel-based failover) - Kafka: < 30 seconds (controller election) - Services: Kubernetes health checks with 10-second liveness probes ### Backup Strategy **Database Backups**: - Automated snapshots: Daily at 02:00 UTC - Retention: 30 days for daily, 90 days for monthly - Cross-region replication: Async replication to us-west-2 (15-minute lag) - Backup verification: Weekly automated restore test in staging environment **Backup Schedule**: ``` Daily: Full snapshot \u2192 S3 (encrypted) Hourly: WAL archives \u2192 S3 (point-in-time recovery) Weekly: Backup validation test Monthly: Long-term archive to Glacier ``` **Configuration Backups**: - Kubernetes manifests: Stored in Git (GitOps with ArgoCD) - Vault secrets: Automated snapshot every 6 hours - Kafka topic configurations: Exported daily to S3 ### Disaster Recovery Procedures **Scenario 1: Single AZ Failure** - Detection: Health checks fail for entire AZ (< 30 seconds) - Action: Traffic automatically routed to healthy AZs by ALB - Recovery time: < 5 minutes (no manual intervention) - Data loss: None (multi-AZ replication) **Scenario 2: Database Primary Failure** - Detection: Health check fails for primary database (< 30 seconds) - Action: Automatic promotion of read replica to primary - Recovery time: 30-60 seconds - Data loss: Minimal (< 1 second due to synchronous replication) **Scenario 3: Full Region Failure** - Detection: Multiple health check failures across all AZs (< 2 minutes) - Action: Manual failover to DR region (us-west-2) - Procedure: 1. Update DNS to point to DR region (TTL: 60 seconds) 2. Promote DR database replica to primary 3. Scale up DR region services to production capacity 4. Verify data consistency and integrity 5. Update monitoring dashboards - Recovery time: 2-4 hours (includes verification) - Data loss: < 1 hour (last cross-region replication) **Scenario 4: Data Corruption** - Detection: Data validation checks or user report - Action: Point-in-time recovery from WAL archives - Procedure: 1. Identify corruption time window 2. Restore from snapshot prior to corruption 3. Replay WAL logs up to corruption point 4. Verify data integrity 5.",
              "score": null
            },
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_4",
              "content": "--set prometheus.prometheusSpec.retention=30d \\ --set prometheus.prometheusSpec.storageSpec.volumeClaimTemplate.spec.resources.requests.storage=100Gi ``` CloudFlow exposes Prometheus metrics on port 9090 at the `/metrics` endpoint. The metrics include: - **HTTP Metrics**: Request rate, latency, error rate - **Database Metrics**: Connection pool status, query duration - **Worker Metrics**: Job queue size, processing time, failure rate - **System Metrics**: CPU usage, memory usage, heap statistics Example metrics exposed: ``` # HELP cloudflow_http_requests_total Total number of HTTP requests # TYPE cloudflow_http_requests_total counter cloudflow_http_requests_total{method=\"GET\",route=\"/api/workflows\",status=\"200\"} 15420 # HELP cloudflow_http_request_duration_seconds HTTP request duration in seconds # TYPE cloudflow_http_request_duration_seconds histogram cloudflow_http_request_duration_seconds_bucket{method=\"POST\",route=\"/api/workflows\",le=\"0.1\"} 8234 # HELP cloudflow_worker_jobs_processed_total Total number of jobs processed # TYPE cloudflow_worker_jobs_processed_total counter cloudflow_worker_jobs_processed_total{status=\"success\"} 45230 # HELP cloudflow_database_connections Current database connections # TYPE cloudflow_database_connections gauge cloudflow_database_connections{state=\"active\"} 23 ``` ### Grafana Dashboards Access Grafana to view CloudFlow dashboards: ```bash kubectl port-forward -n monitoring svc/prometheus-grafana 3000:80 ``` Import the CloudFlow dashboard (ID: 15847) or use the provided JSON template. Key dashboard panels include: 1. **API Performance**: Request rate, P95/P99 latency, error rate 2. **Resource Usage**: CPU, memory, disk I/O per pod 3. **Database Health**: Connection pool utilization, query performance 4. **Worker Status**: Queue depth, processing rate, job success/failure ratio 5. **System Overview**: Pod status, replica count, autoscaling events ### Logging with CloudWatch CloudFlow logs are automatically shipped to CloudWatch Logs.",
              "score": null
            },
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_3",
              "content": "Deploy PostgreSQL using the Bitnami Helm chart: ```yaml # postgres-values.yaml global: postgresql: auth: username: cloudflow database: cloudflow existingSecret: postgres-credentials image: tag: \"14.10.0\" primary: resources: limits: cpu: 4000m memory: 8Gi requests: cpu: 2000m memory: 4Gi persistence: enabled: true size: 100Gi storageClass: gp3 extendedConfiguration: | max_connections = 100 shared_buffers = 2GB effective_cache_size = 6GB maintenance_work_mem = 512MB checkpoint_completion_target = 0.9 wal_buffers = 16MB default_statistics_target = 100 random_page_cost = 1.1 effective_io_concurrency = 200 work_mem = 10MB min_wal_size = 1GB max_wal_size = 4GB readReplicas: replicaCount: 2 resources: limits: cpu: 2000m memory: 4Gi requests: cpu: 1000m memory: 2Gi metrics: enabled: true serviceMonitor: enabled: true ``` Install PostgreSQL: ```bash # Create password secret kubectl create secret generic postgres-credentials \\ --namespace cloudflow-prod \\ --from-literal=postgres-password=\"$(openssl rand -base64 32)\" \\ --from-literal=password=\"$(openssl rand -base64 32)\" # Install PostgreSQL helm install postgresql bitnami/postgresql \\ --namespace cloudflow-prod \\ --values postgres-values.yaml \\ --wait ``` ### PgBouncer Configuration Deploy PgBouncer for connection pooling to handle up to 100 connections efficiently: ```yaml # pgbouncer.yaml apiVersion: v1 kind: ConfigMap metadata: name: pgbouncer-config namespace: cloudflow-prod data: pgbouncer.ini: | [databases] cloudflow = host=postgresql.cloudflow-prod.svc.cluster.local port=5432 dbname=cloudflow [pgbouncer] listen_addr = 0.0.0.0 listen_port = 5432 auth_type = md5 auth_file = /etc/pgbouncer/userlist.txt pool_mode = transaction max_client_conn = 1000 default_pool_size = 25 reserve_pool_size = 5 reserve_pool_timeout = 3 max_db_connections = 100 max_user_connections = 100 server_lifetime = 3600 server_idle_timeout = 600 log_connections = 1 log_disconnections = 1 log_pooler_errors = 1 --- apiVersion: apps/v1 kind: Deployment metadata: name: pgbouncer namespace: cloudflow-prod spec: replicas: 2 selector: matchLabels: app: pgbouncer template: metadata: labels: app: pgbouncer spec: containers: - name: pgbouncer image: pgbouncer/pgbouncer:1.21.0 ports: - containerPort: 5432 resources: requests: cpu: 500m memory: 512Mi limits: cpu: 1000m memory: 1Gi volumeMounts: - name: config mountPath: /etc/pgbouncer volumes: - name: config configMap: name: pgbouncer-config --- apiVersion: v1 kind: Service metadata: name: pgbouncer namespace: cloudflow-prod spec: selector: app: pgbouncer ports: - port: 5432 targetPort: 5432 type: ClusterIP ``` Apply the PgBouncer configuration: ```bash kubectl apply -f pgbouncer.yaml ``` ### Database Migrations Run database migrations before deploying new versions: ```bash # Create migration job kubectl create job cloudflow-migrate-$(date +%s) \\ --namespace cloudflow-prod \\ --image=123456789012.dkr.ecr.us-east-1.amazonaws.com/cloudflow:2.4.0 \\ -- npm run migrate # Monitor migration progress kubectl logs -f job/cloudflow-migrate-<timestamp> -n cloudflow-prod ``` --- ## Monitoring and Observability ### Prometheus Setup Install Prometheus using the kube-prometheus-stack: ```bash helm install prometheus prometheus-community/kube-prometheus-stack \\ --namespace monitoring \\ --create-namespace \\ --set prometheus.prometheusSpec.serviceMonitorSelectorNilUsesHelmValues=false \\",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_10",
              "content": "database nc -zv cloudflow-db.internal.company.com 5432 # Trace route traceroute api.cloudflow.io # Capture packets tcpdump -i any -w /tmp/capture.pcap port 5432 ``` --- ## Escalation Procedures ### Severity Levels CloudFlow incidents are classified into four severity levels: #### SEV-1: Critical (P1) - **Definition:** Complete service outage or severe degradation affecting all users - **Examples:** - API returns 5xx errors for > 5 minutes - Database completely unavailable - Authentication system down - Data loss or corruption - **Response Time:** Immediate (< 15 minutes) - **Escalation:** Page on-call engineer immediately #### SEV-2: High (P2) - **Definition:** Major functionality impaired affecting multiple users - **Examples:** - Workflow execution success rate < 90% - Significant performance degradation (p95 latency > 10s) - Rate limiting affecting large customer segment - **Response Time:** < 1 hour - **Escalation:** Create incident ticket and notify on-call #### SEV-3: Medium (P3) - **Definition:** Partial functionality degraded affecting some users - **Examples:** - Intermittent failures for specific workflow types - Minor performance issues - Non-critical feature unavailable - **Response Time:** < 4 hours - **Escalation:** Create ticket, normal business hours support #### SEV-4: Low (P4) - **Definition:** Minor issues with minimal user impact - **Examples:** - Cosmetic issues - Documentation errors - Feature requests - **Response Time:** < 2 business days - **Escalation:** Standard support ticket ### Escalation Steps #### Step 1: Initial Assessment (0-5 minutes) ```bash # Run health check cloudflow health check --comprehensive # Check status page curl https://status.cloudflow.io/api/v1/status.json # Review recent changes cloudflow audit log --last 2h --event-type \"deployment,configuration\" # Check metrics dashboard cloudflow metrics dashboard --incident-mode ``` #### Step 2: Gather Information (5-15 minutes) Create incident document with: - Incident timestamp and duration - Affected services and endpoints - Error rates and user impact - Recent changes or deployments - Relevant log excerpts - Correlation IDs for failed requests ```bash # Generate incident report cloudflow debug incident-report \\ --start \"2026-01-24T10:30:00Z\" \\ --end \"2026-01-24T11:00:00Z\" \\ --output incident-report.md # Capture system snapshot cloudflow debug snapshot --output snapshot-$(date +%Y%m%d-%H%M%S).tar.gz ``` #### Step 3: Escalate Based on Severity **For SEV-1 (Critical):** ```bash # Page on-call engineer cloudflow incident create \\ --severity SEV-1 \\ --title \"Complete API outage\" \\ --description \"All API requests returning 503\" \\ --page-oncall # Notify status page cloudflow status update \\ --status \"major_outage\" \\ --message \"We are investigating a complete service",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_0",
              "content": "# CloudFlow Platform - System Architecture Overview **Document Version:** 2.3.1 **Last Updated:** January 15, 2026 **Owner:** Platform Architecture Team **Status:** Production ## Executive Summary CloudFlow is a distributed, cloud-native workflow automation platform designed to orchestrate complex business processes at scale. The platform processes over 2.5 million workflow executions daily with an average P99 latency of 180ms for API operations and 4.2 seconds for workflow execution. This document provides a comprehensive overview of the system architecture, including microservices design, data flow patterns, infrastructure decisions, and operational considerations. ## Table of Contents 1. [High-Level Architecture](#high-level-architecture) 2. [Microservices Breakdown](#microservices-breakdown) 3. [Data Flow Architecture](#data-flow-architecture) 4. [Database Architecture](#database-architecture) 5. [Message Queue Patterns](#message-queue-patterns) 6. [Caching Strategy](#caching-strategy) 7. [Security Architecture](#security-architecture) 8. [Performance Characteristics](#performance-characteristics) 9. [Disaster Recovery](#disaster-recovery) --- ## High-Level Architecture CloudFlow follows a microservices architecture pattern deployed across multiple availability zones in AWS. The platform is designed with the following core principles: - **Scalability**: Horizontal scaling for all services with auto-scaling groups - **Resilience**: Circuit breakers, bulkheads, and graceful degradation - **Observability**: Distributed tracing, centralized logging, and comprehensive metrics - **Security**: Zero-trust network architecture with mTLS encryption ### Architecture Diagram (Conceptual) ``` \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 Load Balancer (ALB) \u2502 \u2502 (TLS Termination - 443) \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u25bc \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 API Gateway Layer \u2502 \u2502 (Rate Limiting, Auth, Request Routing) \u2502 \u2514\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u2502 \u2502 \u2502 \u2502 \u25bc \u25bc \u25bc \u25bc \u25bc \u250c\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502Auth\u2502 \u2502Workflow\u2502 \u2502Scheduler\u2502 \u2502Notification\u2502 \u2502User Service\u2502 \u2502Svc \u2502 \u2502 Engine \u2502 \u2502 Service \u2502 \u2502 Service \u2502 \u2502 \u2502 \u2514\u2500\u252c\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u2502 \u2502 \u2502 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 \u2502 \u2502 \u25bc \u25bc \u25bc \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502PostgreSQL\u2502 \u2502 Redis \u2502 \u2502 Kafka \u2502 \u2502 Cluster \u2502 \u2502 Cluster \u2502 \u2502 Cluster \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 ``` ### Technology Stack - **Runtime**: Node.js 20.x (API Gateway, Workflow Engine), Go 1.21 (Auth Service, Scheduler) - **Container Orchestration**: Kubernetes 1.28 (EKS) - **Service Mesh**: Istio 1.20 for service-to-service communication - **Databases**: PostgreSQL 15.4, Redis 7.2 - **Message Broker**: Apache Kafka 3.6 - **Monitoring**: Prometheus, Grafana, Jaeger for distributed tracing - **Secrets Management**: HashiCorp Vault 1.15 --- ## Microservices Breakdown ### API Gateway **Purpose**: Single entry point for all client requests, providing authentication, rate limiting, request routing, and protocol translation.",
              "score": null
            }
          ],
          "latency_ms": 903.283735999139,
          "query_id": "realistic_007",
          "dimension": "contextual",
          "query": "Setting up monitoring, what endpoints indicate service health?"
        },
        {
          "key_facts": [
            "/health",
            "/ready"
          ],
          "found_facts": [
            "/health",
            "/ready"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "deployment_guide"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_10",
              "content": "database nc -zv cloudflow-db.internal.company.com 5432 # Trace route traceroute api.cloudflow.io # Capture packets tcpdump -i any -w /tmp/capture.pcap port 5432 ``` --- ## Escalation Procedures ### Severity Levels CloudFlow incidents are classified into four severity levels: #### SEV-1: Critical (P1) - **Definition:** Complete service outage or severe degradation affecting all users - **Examples:** - API returns 5xx errors for > 5 minutes - Database completely unavailable - Authentication system down - Data loss or corruption - **Response Time:** Immediate (< 15 minutes) - **Escalation:** Page on-call engineer immediately #### SEV-2: High (P2) - **Definition:** Major functionality impaired affecting multiple users - **Examples:** - Workflow execution success rate < 90% - Significant performance degradation (p95 latency > 10s) - Rate limiting affecting large customer segment - **Response Time:** < 1 hour - **Escalation:** Create incident ticket and notify on-call #### SEV-3: Medium (P3) - **Definition:** Partial functionality degraded affecting some users - **Examples:** - Intermittent failures for specific workflow types - Minor performance issues - Non-critical feature unavailable - **Response Time:** < 4 hours - **Escalation:** Create ticket, normal business hours support #### SEV-4: Low (P4) - **Definition:** Minor issues with minimal user impact - **Examples:** - Cosmetic issues - Documentation errors - Feature requests - **Response Time:** < 2 business days - **Escalation:** Standard support ticket ### Escalation Steps #### Step 1: Initial Assessment (0-5 minutes) ```bash # Run health check cloudflow health check --comprehensive # Check status page curl https://status.cloudflow.io/api/v1/status.json # Review recent changes cloudflow audit log --last 2h --event-type \"deployment,configuration\" # Check metrics dashboard cloudflow metrics dashboard --incident-mode ``` #### Step 2: Gather Information (5-15 minutes) Create incident document with: - Incident timestamp and duration - Affected services and endpoints - Error rates and user impact - Recent changes or deployments - Relevant log excerpts - Correlation IDs for failed requests ```bash # Generate incident report cloudflow debug incident-report \\ --start \"2026-01-24T10:30:00Z\" \\ --end \"2026-01-24T11:00:00Z\" \\ --output incident-report.md # Capture system snapshot cloudflow debug snapshot --output snapshot-$(date +%Y%m%d-%H%M%S).tar.gz ``` #### Step 3: Escalate Based on Severity **For SEV-1 (Critical):** ```bash # Page on-call engineer cloudflow incident create \\ --severity SEV-1 \\ --title \"Complete API outage\" \\ --description \"All API requests returning 503\" \\ --page-oncall # Notify status page cloudflow status update \\ --status \"major_outage\" \\ --message \"We are investigating a complete service",
              "score": null
            },
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_2",
              "content": "- secretName: cloudflow-tls hosts: - api.cloudflow.io healthCheck: enabled: true livenessProbe: path: /health initialDelaySeconds: 30 periodSeconds: 10 timeoutSeconds: 5 failureThreshold: 3 readinessProbe: path: /ready initialDelaySeconds: 10 periodSeconds: 5 timeoutSeconds: 3 failureThreshold: 3 env: - name: NODE_ENV value: \"production\" - name: LOG_LEVEL value: \"info\" - name: API_PORT value: \"3000\" - name: WORKER_CONCURRENCY value: \"10\" - name: AWS_REGION value: \"us-east-1\" - name: METRICS_ENABLED value: \"true\" envFrom: - secretRef: name: cloudflow-secrets podAnnotations: prometheus.io/scrape: \"true\" prometheus.io/port: \"9090\" prometheus.io/path: \"/metrics\" podSecurityContext: runAsNonRoot: true runAsUser: 1000 fsGroup: 1000 securityContext: allowPrivilegeEscalation: false capabilities: drop: - ALL readOnlyRootFilesystem: true persistence: enabled: true storageClass: gp3 accessMode: ReadWriteOnce size: 50Gi redis: enabled: true architecture: replication auth: enabled: true master: persistence: size: 20Gi replica: replicaCount: 2 persistence: size: 20Gi ``` Deploy CloudFlow using Helm: ```bash helm install cloudflow cloudflow/cloudflow \\ --namespace cloudflow-prod \\ --values values-production.yaml \\ --wait \\ --timeout 10m ``` ### Deployment Verification Verify the deployment status: ```bash # Check pod status kubectl get pods -n cloudflow-prod # Expected output: # NAME READY STATUS RESTARTS AGE # cloudflow-api-7d8f9c5b6d-4xk2p 1/1 Running 0 2m # cloudflow-api-7d8f9c5b6d-9hj5m 1/1 Running 0 2m # cloudflow-api-7d8f9c5b6d-tn8wq 1/1 Running 0 2m # Check service endpoints kubectl get svc -n cloudflow-prod # Check ingress kubectl get ingress -n cloudflow-prod ``` Test health endpoints: ```bash # Port forward for testing kubectl port-forward -n cloudflow-prod svc/cloudflow-api 8080:80 # Test health endpoint curl http://localhost:8080/health # Expected: {\"status\":\"healthy\",\"timestamp\":\"2026-01-24T10:30:00Z\"} # Test readiness endpoint curl http://localhost:8080/ready # Expected: {\"status\":\"ready\",\"dependencies\":{\"database\":\"connected\",\"redis\":\"connected\"}} ``` --- ## Database Configuration ### PostgreSQL Setup CloudFlow uses PostgreSQL 14 as its primary data store.",
              "score": null
            },
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_2",
              "content": "**Endpoint:** `POST /workflows` **Request Body:** ```json { \"name\": \"Email Campaign Automation\", \"description\": \"Sends personalized emails based on user behavior\", \"trigger\": { \"type\": \"webhook\", \"url\": \"https://your-app.com/webhook\" }, \"steps\": [ { \"type\": \"fetch_data\", \"source\": \"users_table\", \"filters\": {\"active\": true} }, { \"type\": \"transform\", \"script\": \"data.map(user => ({ ...user, segment: calculateSegment(user) }))\" }, { \"type\": \"send_email\", \"template_id\": \"tpl_welcome_v2\" } ] } ``` **Example Request:** ```bash curl -X POST https://api.cloudflow.io/v2/workflows \\ -H \"X-API-Key: YOUR_API_KEY\" \\ -H \"Content-Type: application/json\" \\ -d '{ \"name\": \"Email Campaign Automation\", \"trigger\": {\"type\": \"webhook\"}, \"steps\": [{\"type\": \"fetch_data\", \"source\": \"users_table\"}] }' ``` **Example Response (201 Created):** ```json { \"id\": \"wf_9k3m7n2q\", \"name\": \"Email Campaign Automation\", \"status\": \"active\", \"created_at\": \"2026-01-24T10:15:00Z\", \"webhook_url\": \"https://api.cloudflow.io/v2/webhooks/wf_9k3m7n2q/trigger\" } ``` #### Get Workflow Details Retrieve detailed information about a specific workflow. **Endpoint:** `GET /workflows/{workflow_id}` **Example Request:** ```bash curl -H \"X-API-Key: YOUR_API_KEY\" \\ https://api.cloudflow.io/v2/workflows/wf_8x7k2m9p ``` **Example Response (200 OK):** ```json { \"id\": \"wf_8x7k2m9p\", \"name\": \"Data Processing Pipeline\", \"description\": \"Processes customer data every hour\", \"status\": \"active\", \"trigger\": { \"type\": \"schedule\", \"cron\": \"0 * * * *\", \"timezone\": \"UTC\" }, \"steps\": [ { \"id\": \"step_1\", \"type\": \"fetch_data\", \"source\": \"customer_db\", \"query\": \"SELECT * FROM customers WHERE updated_at > :last_run\" }, { \"id\": \"step_2\", \"type\": \"transform\", \"script_id\": \"scr_transform_v3\" } ], \"metrics\": { \"total_runs\": 1247, \"success_rate\": 98.7, \"avg_duration_ms\": 3420, \"last_error\": null }, \"created_at\": \"2026-01-15T10:30:00Z\", \"updated_at\": \"2026-01-23T14:22:00Z\" } ``` #### Update Workflow Modify an existing workflow's configuration. **Endpoint:** `PATCH /workflows/{workflow_id}` **Example Request:** ```bash curl -X PATCH https://api.cloudflow.io/v2/workflows/wf_8x7k2m9p \\ -H \"X-API-Key: YOUR_API_KEY\" \\ -H \"Content-Type: application/json\" \\ -d '{\"status\": \"paused\"}' ``` **Example Response (200 OK):** ```json { \"id\": \"wf_8x7k2m9p\", \"status\": \"paused\", \"updated_at\": \"2026-01-24T10:30:00Z\" } ``` #### Delete Workflow Permanently delete a workflow. **Endpoint:** `DELETE /workflows/{workflow_id}` **Example Request:** ```bash curl -X DELETE https://api.cloudflow.io/v2/workflows/wf_8x7k2m9p \\ -H \"X-API-Key: YOUR_API_KEY\" ``` **Example Response (204 No Content)** ### Pipelines #### List Pipeline Executions Retrieve execution history for data pipelines.",
              "score": null
            },
            {
              "doc_id": "user_guide",
              "chunk_id": "user_guide_fix_2",
              "content": "### HTTP Requests Make HTTP requests to any API endpoint: **Configuration:** - **Method**: GET, POST, PUT, PATCH, DELETE, HEAD - **URL**: Full endpoint URL (supports variable interpolation) - **Headers**: Custom headers as key-value pairs - **Query Parameters**: URL parameters - **Body**: JSON, form data, or raw text - **Authentication**: Basic Auth, Bearer Token, API Key, OAuth 2.0 **Example:** ```yaml - id: fetch_user action: http_request config: method: GET url: \"https://api.example.com/users/{{user_id}}\" headers: Authorization: \"Bearer {{secrets.API_TOKEN}}\" Content-Type: \"application/json\" timeout: 30 ``` **Response Handling:** Access the response in subsequent steps: - `{{steps.fetch_user.status}}` - HTTP status code - `{{steps.fetch_user.body}}` - Response body - `{{steps.fetch_user.headers}}` - Response headers ### Database Queries Execute queries against supported databases (PostgreSQL, MySQL, MongoDB, Redis): **SQL Databases (PostgreSQL, MySQL):** ```yaml - id: get_orders action: database_query config: connection: \"{{secrets.DB_CONNECTION_STRING}}\" query: | SELECT * FROM orders WHERE customer_id = $1 AND status = $2 ORDER BY created_at DESC LIMIT 10 parameters: - \"{{trigger.customer_id}}\" - \"pending\" ``` **MongoDB:** ```yaml - id: find_documents action: mongodb_query config: connection: \"{{secrets.MONGO_URI}}\" database: \"production\" collection: \"users\" operation: find filter: email: \"{{trigger.email}}\" options: limit: 1 ``` **Important Notes:** - Always use parameterized queries to prevent SQL injection - Connection strings should be stored in secrets - Query timeout is 30 seconds by default - Maximum result set size is 10MB ### Email Notifications Send emails via SMTP or integrated email providers: **Configuration:** ```yaml - id: send_notification action: email config: provider: \"smtp\" # or \"sendgrid\", \"mailgun\", \"ses\" from: \"notifications@cloudflow.io\" to: \"{{trigger.recipient}}\" cc: \"manager@company.com\" subject: \"Alert: {{alert_type}}\" body: | Hello {{user.name}}, This is an automated notification about {{event_description}}. Details: - Time: {{timestamp}} - Type: {{alert_type}} - Priority: {{priority}} Best regards, CloudFlow Automation html: true attachments: - name: \"report.pdf\" content: \"{{steps.generate_report.output}}\" encoding: \"base64\" ``` **Email Templates:** CloudFlow supports dynamic templates with conditional content: ``` {{#if priority == 'high'}} \u26a0\ufe0f URGENT: Immediate attention required {{else}} \u2139\ufe0f Informational notification {{/if}} ``` ### Slack Messages Send messages to Slack channels or users: **Channel Messages:** ```yaml - id: notify_team action: slack_message config: channel: \"#deployments\" text: \"Deployment completed successfully!\" blocks: - type: \"section\" text: type: \"mrkdwn\" text: \"*Deployment Summary*\\nEnvironment: {{environment}}\\nVersion: {{version}}\\nDuration: {{duration}}s\" - type: \"actions\" elements: - type: \"button\" text: \"View Logs\" url: \"{{logs_url}}\" thread_ts: \"{{trigger.thread_ts}}\" # Reply to thread ``` **Direct Messages:** ```yaml - id: dm_user action: slack_message config: user: \"{{user_slack_id}}\" text: \"Your report is ready for review.\"",
              "score": null
            },
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_3",
              "content": "**Endpoint:** `GET /pipelines/{pipeline_id}/executions` **Query Parameters:** - `status` (optional): Filter by status (`running`, `completed`, `failed`) - `start_date` (optional): ISO 8601 timestamp - `end_date` (optional): ISO 8601 timestamp - `limit` (optional): Items per page (max 100) - `offset` (optional): Pagination offset **Example Request:** ```python import requests api_key = \"YOUR_API_KEY\" pipeline_id = \"pipe_4x9k2m\" url = f\"https://api.cloudflow.io/v2/pipelines/{pipeline_id}/executions\" headers = {\"X-API-Key\": api_key} params = { \"status\": \"completed\", \"start_date\": \"2026-01-20T00:00:00Z\", \"limit\": 50 } response = requests.get(url, headers=headers, params=params) executions = response.json() for execution in executions['data']: print(f\"Execution {execution['id']}: {execution['status']} - {execution['duration_ms']}ms\") ``` **Example Response (200 OK):** ```json { \"data\": [ { \"id\": \"exec_7m3k9x2p\", \"pipeline_id\": \"pipe_4x9k2m\", \"status\": \"completed\", \"started_at\": \"2026-01-23T15:00:00Z\", \"completed_at\": \"2026-01-23T15:03:42Z\", \"duration_ms\": 222000, \"records_processed\": 15420, \"records_failed\": 3, \"error_rate\": 0.02 } ], \"pagination\": { \"total\": 156, \"limit\": 50, \"offset\": 0, \"has_more\": true } } ``` ### Analytics #### Get Workflow Metrics Retrieve performance metrics and analytics for workflows. **Endpoint:** `GET /analytics/workflows/{workflow_id}` **Query Parameters:** - `period` (required): Time period (`1h`, `24h`, `7d`, `30d`) - `metrics` (optional): Comma-separated list of metrics **Available Metrics:** - `execution_count`: Total number of executions - `success_rate`: Percentage of successful executions - `avg_duration`: Average execution duration in milliseconds - `error_count`: Total number of errors - `throughput`: Records processed per hour **Example Request:** ```bash curl -H \"X-API-Key: YOUR_API_KEY\" \\ \"https://api.cloudflow.io/v2/analytics/workflows/wf_8x7k2m9p?period=7d&metrics=execution_count,success_rate,avg_duration\" ``` **Example Response (200 OK):** ```json { \"workflow_id\": \"wf_8x7k2m9p\", \"period\": \"7d\", \"start_date\": \"2026-01-17T10:30:00Z\", \"end_date\": \"2026-01-24T10:30:00Z\", \"metrics\": { \"execution_count\": 168, \"success_rate\": 97.6, \"avg_duration\": 3285, \"error_count\": 4, \"throughput\": 2847 }, \"timeseries\": [ { \"timestamp\": \"2026-01-17T00:00:00Z\", \"execution_count\": 24, \"success_rate\": 100.0 }, { \"timestamp\": \"2026-01-18T00:00:00Z\", \"execution_count\": 24, \"success_rate\": 95.8 } ] } ``` ## Error Handling CloudFlow uses standard HTTP status codes and returns detailed error information in JSON format.",
              "score": null
            }
          ],
          "latency_ms": 985.555844999908,
          "query_id": "realistic_007",
          "dimension": "negation",
          "query": "Why doesn't /status return health information?"
        },
        {
          "key_facts": [
            "minReplicas: 3",
            "maxReplicas: 10",
            "targetCPUUtilizationPercentage: 70"
          ],
          "found_facts": [
            "minReplicas: 3",
            "maxReplicas: 10",
            "targetCPUUtilizationPercentage: 70"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "deployment_guide"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_6",
              "content": "**Verify Service Health**: ```bash kubectl get pods -n cloudflow-prod curl https://api.cloudflow.io/health ``` **Recovery Time Objective (RTO)**: 4 hours **Recovery Point Objective (RPO)**: 24 hours --- ## Scaling and Performance ### Horizontal Pod Autoscaling CloudFlow is configured with Horizontal Pod Autoscaler (HPA) to automatically scale based on resource utilization: ```yaml # hpa.yaml apiVersion: autoscaling/v2 kind: HorizontalPodAutoscaler metadata: name: cloudflow-api-hpa namespace: cloudflow-prod spec: scaleTargetRef: apiVersion: apps/v1 kind: Deployment name: cloudflow-api minReplicas: 3 maxReplicas: 10 metrics: - type: Resource resource: name: cpu target: type: Utilization averageUtilization: 70 - type: Resource resource: name: memory target: type: Utilization averageUtilization: 80 behavior: scaleDown: stabilizationWindowSeconds: 300 policies: - type: Percent value: 50 periodSeconds: 60 scaleUp: stabilizationWindowSeconds: 60 policies: - type: Percent value: 100 periodSeconds: 30 - type: Pods value: 2 periodSeconds: 30 selectPolicy: Max ``` Monitor autoscaling events: ```bash kubectl get hpa -n cloudflow-prod -w kubectl describe hpa cloudflow-api-hpa -n cloudflow-prod ``` ### Performance Optimization #### Connection Pooling Ensure optimal database connection pooling settings in PgBouncer: - **Pool Mode**: Transaction (optimal for microservices) - **Default Pool Size**: 25 connections per user - **Max DB Connections**: 100 (matches PostgreSQL `max_connections`) #### Redis Caching Implement Redis caching for frequently accessed data: ```javascript // Example caching strategy const CACHE_TTL = { workflows: 300, // 5 minutes userSessions: 3600, // 1 hour apiResults: 60 // 1 minute }; ``` #### Load Testing Perform regular load testing to validate scaling configuration: ```bash # Install k6 for load testing kubectl apply -f - <<EOF apiVersion: v1 kind: ConfigMap metadata: name: loadtest-script namespace: cloudflow-prod data: script.js: | import http from 'k6/http'; import { check, sleep } from 'k6'; export let options = { stages: [ { duration: '2m', target: 100 }, { duration: '5m', target: 100 }, { duration: '2m', target: 200 }, { duration: '5m', target: 200 }, { duration: '2m', target: 0 }, ], thresholds: { http_req_duration: ['p(95)<500'], }, }; export default function () { let res = http.get('https://api.cloudflow.io/api/workflows'); check(res, { 'status is 200': (r) => r.status === 200 }); sleep(1); } EOF ``` **Performance Targets**: - P95 API latency: < 500ms - P99 API latency: < 1000ms - Error rate: < 0.1% - Throughput: > 1000 requests/second --- ## Security and Compliance ### Network Policies Implement network policies to restrict pod-to-pod communication: ```yaml # network-policy.yaml apiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: cloudflow-network-policy namespace: cloudflow-prod spec:",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_0",
              "content": "# CloudFlow Platform - System Architecture Overview **Document Version:** 2.3.1 **Last Updated:** January 15, 2026 **Owner:** Platform Architecture Team **Status:** Production ## Executive Summary CloudFlow is a distributed, cloud-native workflow automation platform designed to orchestrate complex business processes at scale. The platform processes over 2.5 million workflow executions daily with an average P99 latency of 180ms for API operations and 4.2 seconds for workflow execution. This document provides a comprehensive overview of the system architecture, including microservices design, data flow patterns, infrastructure decisions, and operational considerations. ## Table of Contents 1. [High-Level Architecture](#high-level-architecture) 2. [Microservices Breakdown](#microservices-breakdown) 3. [Data Flow Architecture](#data-flow-architecture) 4. [Database Architecture](#database-architecture) 5. [Message Queue Patterns](#message-queue-patterns) 6. [Caching Strategy](#caching-strategy) 7. [Security Architecture](#security-architecture) 8. [Performance Characteristics](#performance-characteristics) 9. [Disaster Recovery](#disaster-recovery) --- ## High-Level Architecture CloudFlow follows a microservices architecture pattern deployed across multiple availability zones in AWS. The platform is designed with the following core principles: - **Scalability**: Horizontal scaling for all services with auto-scaling groups - **Resilience**: Circuit breakers, bulkheads, and graceful degradation - **Observability**: Distributed tracing, centralized logging, and comprehensive metrics - **Security**: Zero-trust network architecture with mTLS encryption ### Architecture Diagram (Conceptual) ``` \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 Load Balancer (ALB) \u2502 \u2502 (TLS Termination - 443) \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u25bc \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 API Gateway Layer \u2502 \u2502 (Rate Limiting, Auth, Request Routing) \u2502 \u2514\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u2502 \u2502 \u2502 \u2502 \u25bc \u25bc \u25bc \u25bc \u25bc \u250c\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502Auth\u2502 \u2502Workflow\u2502 \u2502Scheduler\u2502 \u2502Notification\u2502 \u2502User Service\u2502 \u2502Svc \u2502 \u2502 Engine \u2502 \u2502 Service \u2502 \u2502 Service \u2502 \u2502 \u2502 \u2514\u2500\u252c\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u2502 \u2502 \u2502 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 \u2502 \u2502 \u25bc \u25bc \u25bc \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502PostgreSQL\u2502 \u2502 Redis \u2502 \u2502 Kafka \u2502 \u2502 Cluster \u2502 \u2502 Cluster \u2502 \u2502 Cluster \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 ``` ### Technology Stack - **Runtime**: Node.js 20.x (API Gateway, Workflow Engine), Go 1.21 (Auth Service, Scheduler) - **Container Orchestration**: Kubernetes 1.28 (EKS) - **Service Mesh**: Istio 1.20 for service-to-service communication - **Databases**: PostgreSQL 15.4, Redis 7.2 - **Message Broker**: Apache Kafka 3.6 - **Monitoring**: Prometheus, Grafana, Jaeger for distributed tracing - **Secrets Management**: HashiCorp Vault 1.15 --- ## Microservices Breakdown ### API Gateway **Purpose**: Single entry point for all client requests, providing authentication, rate limiting, request routing, and protocol translation.",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_8",
              "content": "Leverage caching:** ```bash # Enable client-side caching export CLOUDFLOW_CACHE_ENABLED=true export CLOUDFLOW_CACHE_TTL=300 # Cache workflow metadata cloudflow workflows list --use-cache --cache-ttl 600 ``` --- ## Log Analysis & Debugging ### Accessing CloudFlow Logs #### Kubernetes Deployments ```bash # List all CloudFlow pods kubectl get pods -n cloudflow # Tail logs from API server kubectl logs -f -n cloudflow deployment/cloudflow-api --tail=100 # Get logs from specific pod kubectl logs -n cloudflow cloudflow-api-7d4f6b8c9d-x7k2m # Get logs from previous crashed pod kubectl logs -n cloudflow cloudflow-api-7d4f6b8c9d-x7k2m --previous # Get logs from all pods in deployment kubectl logs -n cloudflow deployment/cloudflow-api --all-containers=true # Stream logs from multiple pods kubectl logs -n cloudflow -l app=cloudflow-api -f --max-log-requests=10 ``` #### Log Levels CloudFlow supports the following log levels: - `TRACE` - Very detailed debugging information - `DEBUG` - Detailed debugging information - `INFO` - Informational messages (default) - `WARN` - Warning messages - `ERROR` - Error messages - `FATAL` - Fatal errors causing shutdown **Changing log levels:** ```bash # Set global log level cloudflow config set logging.level DEBUG # Set log level for specific component cloudflow config set logging.components.database DEBUG cloudflow config set logging.components.auth INFO cloudflow config set logging.components.workflows TRACE # Temporary log level increase (resets after 1 hour) cloudflow debug set-log-level DEBUG --duration 3600 # View current log configuration cloudflow config get logging --format json ``` ### Grep Patterns for Common Issues #### Authentication Failures ```bash # Find all authentication errors kubectl logs -n cloudflow deployment/cloudflow-api | \\ grep -i \"authentication\\|401\\|unauthorized\" | \\ tail -n 50 # Find JWT validation failures kubectl logs -n cloudflow deployment/cloudflow-api | \\ grep -E \"JWT|token.*invalid|signature.*failed\" # Find clock skew issues kubectl logs -n cloudflow deployment/cloudflow-api | \\ grep -i \"clock skew\\|time.*sync\\|nbf\\|exp\" ``` #### Database Errors ```bash # Find database connection errors kubectl logs -n cloudflow deployment/cloudflow-api | \\ grep -E \"connection.*pool|could not connect|database.*timeout\" # Find slow queries kubectl logs -n cloudflow deployment/cloudflow-api | \\ grep \"slow query\" | \\ awk '{print $NF}' | \\ sort -n | \\ tail -n 20 # Find deadlock errors kubectl logs -n cloudflow deployment/cloudflow-api | \\ grep -i \"deadlock detected\" ``` #### Workflow Execution Errors ```bash # Find workflow timeout errors kubectl logs -n cloudflow deployment/cloudflow-workflow-engine | \\ grep -E \"timeout|exceeded.*3600\" # Find workflow retry attempts kubectl logs -n cloudflow deployment/cloudflow-workflow-engine | \\ grep -E \"retry attempt [0-9]|retrying",
              "score": null
            },
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_1",
              "content": "create cluster -f cluster-config.yaml ``` ### Storage Configuration Install the EBS CSI driver for persistent volumes: ```bash eksctl create addon --name aws-ebs-csi-driver \\ --cluster cloudflow-production \\ --service-account-role-arn arn:aws:iam::ACCOUNT_ID:role/AmazonEKS_EBS_CSI_DriverRole \\ --force ``` --- ## Kubernetes Deployment ### Namespace Setup Create the CloudFlow namespace and configure resource quotas: ```yaml # namespace.yaml apiVersion: v1 kind: Namespace metadata: name: cloudflow-prod labels: name: cloudflow-prod environment: production --- apiVersion: v1 kind: ResourceQuota metadata: name: cloudflow-quota namespace: cloudflow-prod spec: hard: requests.cpu: \"50\" requests.memory: 100Gi persistentvolumeclaims: \"10\" services.loadbalancers: \"2\" ``` Apply the namespace configuration: ```bash kubectl apply -f namespace.yaml ``` ### Environment Configuration CloudFlow requires the following environment variables: | Variable | Description | Example | Required | |----------|-------------|---------|----------| | `DATABASE_URL` | PostgreSQL connection string | `postgresql://user:pass@host:5432/cloudflow` | Yes | | `REDIS_URL` | Redis connection string | `redis://redis-master.cloudflow-prod.svc.cluster.local:6379` | Yes | | `JWT_SECRET` | Secret key for JWT token signing | `<generated-secret-256-bit>` | Yes | | `LOG_LEVEL` | Application log verbosity | `info`, `debug`, `warn`, `error` | No (default: `info`) | | `API_PORT` | API server port | `3000` | No (default: `3000`) | | `WORKER_CONCURRENCY` | Number of concurrent workers | `10` | No (default: `5`) | | `SESSION_SECRET` | Session encryption key | `<generated-secret-256-bit>` | Yes | | `AWS_REGION` | AWS region for services | `us-east-1` | Yes | | `S3_BUCKET` | S3 bucket for file storage | `cloudflow-prod-storage` | Yes | | `SMTP_HOST` | Email server hostname | `smtp.sendgrid.net` | No | | `SMTP_PORT` | Email server port | `587` | No | | `METRICS_ENABLED` | Enable Prometheus metrics | `true` | No (default: `false`) | Create a Kubernetes secret for sensitive variables: ```bash kubectl create secret generic cloudflow-secrets \\ --namespace cloudflow-prod \\ --from-literal=DATABASE_URL=\"postgresql://cloudflow:$(cat db-password.txt)@pgbouncer.cloudflow-prod.svc.cluster.local:5432/cloudflow\" \\ --from-literal=REDIS_URL=\"redis://redis-master.cloudflow-prod.svc.cluster.local:6379\" \\ --from-literal=JWT_SECRET=\"$(openssl rand -base64 32)\" \\ --from-literal=SESSION_SECRET=\"$(openssl rand -base64 32)\" ``` ### Helm Chart Deployment Add the CloudFlow Helm repository: ```bash helm repo add cloudflow https://charts.cloudflow.io helm repo update ``` Create a values file for production configuration: ```yaml # values-production.yaml replicaCount: 3 image: repository: 123456789012.dkr.ecr.us-east-1.amazonaws.com/cloudflow tag: \"2.4.0\" pullPolicy: IfNotPresent resources: limits: cpu: 2000m memory: 4Gi requests: cpu: 2000m memory: 4Gi autoscaling: enabled: true minReplicas: 3 maxReplicas: 10 targetCPUUtilizationPercentage: 70 targetMemoryUtilizationPercentage: 80 service: type: ClusterIP port: 80 targetPort: 3000 annotations: service.beta.kubernetes.io/aws-load-balancer-type: \"nlb\" ingress: enabled: true className: nginx annotations: cert-manager.io/cluster-issuer: \"letsencrypt-prod\" nginx.ingress.kubernetes.io/ssl-redirect: \"true\" hosts: - host: api.cloudflow.io paths: - path: / pathType: Prefix tls:",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_4",
              "content": "**Implement connection pooling optimization:** ```bash # Use PgBouncer for connection pooling kubectl apply -f cloudflow-pgbouncer.yaml # Configure CloudFlow to use PgBouncer cloudflow config set db.host pgbouncer.cloudflow.svc.cluster.local cloudflow config set db.port 6432 cloudflow config set db.pool.mode transaction ``` 2. **Add read replicas:** ```bash # Route read-only queries to replicas cloudflow db replicas add --count 2 cloudflow config set db.read_replicas \"replica-1:5432,replica-2:5432\" cloudflow config set db.read_write_split true ``` ### Connection Timeout Errors #### Error Messages - `connection timeout after 30000ms` - `could not connect to database server at 10.0.2.45:5432` - `database server unreachable` #### Troubleshooting Steps ```bash # Test network connectivity telnet cloudflow-db.internal.company.com 5432 # Check DNS resolution nslookup cloudflow-db.internal.company.com # Verify database is accepting connections pg_isready -h cloudflow-db.internal.company.com -p 5432 -U cloudflow # Check firewall rules sudo iptables -L -n | grep 5432 # Test from CloudFlow pod network kubectl run -n cloudflow debug-pod --rm -i --tty \\ --image=postgres:14 -- \\ psql -h cloudflow-db.internal.company.com -U cloudflow -d cloudflow_production # Review database logs for connection rejections kubectl logs -n cloudflow statefulset/cloudflow-db --tail=100 | \\ grep -i \"connection\\|reject\\|authentication\" ``` ### Maximum Connection Limit (100) Reached This is a hard limit in CloudFlow's database tier. #### Permanent Solutions **Option 1: Upgrade database tier** ```bash # Check available tiers cloudflow db tiers list # Upgrade to higher tier (supports 200 connections) cloudflow db upgrade --tier standard-plus --confirm # Monitor migration progress cloudflow db migration status ``` **Option 2: Implement aggressive connection reuse** ```bash # Reduce connection lifetime cloudflow config set db.pool.max_lifetime 600 # 10 minutes # Enable prepared statement caching cloudflow config set db.prepared_statements.cache true cloudflow config set db.prepared_statements.max_size 250 # Reduce idle connection timeout cloudflow config set db.pool.idle_timeout 120 # 2 minutes ``` --- ## Workflow Execution Failures ### Timeout Errors (3600 second limit) #### Error Message ``` WorkflowExecutionError: Workflow exceeded maximum execution time of 3600 seconds Status: TIMEOUT Workflow ID: wf_9k2n4m8p1q Execution ID: exec_7h3j6k9m2n ``` #### Analysis ```bash # Get workflow execution details cloudflow workflows executions get exec_7h3j6k9m2n --verbose # Check step-by-step breakdown cloudflow workflows executions steps exec_7h3j6k9m2n # Sample output: # Step 1: data_ingestion - Duration: 245s - Status: SUCCESS # Step 2: data_validation - Duration: 123s - Status: SUCCESS # Step 3: data_transformation - Duration: 3189s - Status: TIMEOUT # Step 4: data_export - Duration: 0s - Status: SKIPPED # Identify bottleneck step cloudflow workflows analyze",
              "score": null
            }
          ],
          "latency_ms": 808.4600720030721,
          "query_id": "realistic_008",
          "dimension": "original",
          "query": "What are the HPA scaling parameters?"
        },
        {
          "key_facts": [
            "minReplicas: 3",
            "maxReplicas: 10",
            "targetCPUUtilizationPercentage: 70"
          ],
          "found_facts": [
            "minReplicas: 3",
            "maxReplicas: 10",
            "targetCPUUtilizationPercentage: 70"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "deployment_guide"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_6",
              "content": "**Verify Service Health**: ```bash kubectl get pods -n cloudflow-prod curl https://api.cloudflow.io/health ``` **Recovery Time Objective (RTO)**: 4 hours **Recovery Point Objective (RPO)**: 24 hours --- ## Scaling and Performance ### Horizontal Pod Autoscaling CloudFlow is configured with Horizontal Pod Autoscaler (HPA) to automatically scale based on resource utilization: ```yaml # hpa.yaml apiVersion: autoscaling/v2 kind: HorizontalPodAutoscaler metadata: name: cloudflow-api-hpa namespace: cloudflow-prod spec: scaleTargetRef: apiVersion: apps/v1 kind: Deployment name: cloudflow-api minReplicas: 3 maxReplicas: 10 metrics: - type: Resource resource: name: cpu target: type: Utilization averageUtilization: 70 - type: Resource resource: name: memory target: type: Utilization averageUtilization: 80 behavior: scaleDown: stabilizationWindowSeconds: 300 policies: - type: Percent value: 50 periodSeconds: 60 scaleUp: stabilizationWindowSeconds: 60 policies: - type: Percent value: 100 periodSeconds: 30 - type: Pods value: 2 periodSeconds: 30 selectPolicy: Max ``` Monitor autoscaling events: ```bash kubectl get hpa -n cloudflow-prod -w kubectl describe hpa cloudflow-api-hpa -n cloudflow-prod ``` ### Performance Optimization #### Connection Pooling Ensure optimal database connection pooling settings in PgBouncer: - **Pool Mode**: Transaction (optimal for microservices) - **Default Pool Size**: 25 connections per user - **Max DB Connections**: 100 (matches PostgreSQL `max_connections`) #### Redis Caching Implement Redis caching for frequently accessed data: ```javascript // Example caching strategy const CACHE_TTL = { workflows: 300, // 5 minutes userSessions: 3600, // 1 hour apiResults: 60 // 1 minute }; ``` #### Load Testing Perform regular load testing to validate scaling configuration: ```bash # Install k6 for load testing kubectl apply -f - <<EOF apiVersion: v1 kind: ConfigMap metadata: name: loadtest-script namespace: cloudflow-prod data: script.js: | import http from 'k6/http'; import { check, sleep } from 'k6'; export let options = { stages: [ { duration: '2m', target: 100 }, { duration: '5m', target: 100 }, { duration: '2m', target: 200 }, { duration: '5m', target: 200 }, { duration: '2m', target: 0 }, ], thresholds: { http_req_duration: ['p(95)<500'], }, }; export default function () { let res = http.get('https://api.cloudflow.io/api/workflows'); check(res, { 'status is 200': (r) => r.status === 200 }); sleep(1); } EOF ``` **Performance Targets**: - P95 API latency: < 500ms - P99 API latency: < 1000ms - Error rate: < 0.1% - Throughput: > 1000 requests/second --- ## Security and Compliance ### Network Policies Implement network policies to restrict pod-to-pod communication: ```yaml # network-policy.yaml apiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: cloudflow-network-policy namespace: cloudflow-prod spec:",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_0",
              "content": "# CloudFlow Platform - System Architecture Overview **Document Version:** 2.3.1 **Last Updated:** January 15, 2026 **Owner:** Platform Architecture Team **Status:** Production ## Executive Summary CloudFlow is a distributed, cloud-native workflow automation platform designed to orchestrate complex business processes at scale. The platform processes over 2.5 million workflow executions daily with an average P99 latency of 180ms for API operations and 4.2 seconds for workflow execution. This document provides a comprehensive overview of the system architecture, including microservices design, data flow patterns, infrastructure decisions, and operational considerations. ## Table of Contents 1. [High-Level Architecture](#high-level-architecture) 2. [Microservices Breakdown](#microservices-breakdown) 3. [Data Flow Architecture](#data-flow-architecture) 4. [Database Architecture](#database-architecture) 5. [Message Queue Patterns](#message-queue-patterns) 6. [Caching Strategy](#caching-strategy) 7. [Security Architecture](#security-architecture) 8. [Performance Characteristics](#performance-characteristics) 9. [Disaster Recovery](#disaster-recovery) --- ## High-Level Architecture CloudFlow follows a microservices architecture pattern deployed across multiple availability zones in AWS. The platform is designed with the following core principles: - **Scalability**: Horizontal scaling for all services with auto-scaling groups - **Resilience**: Circuit breakers, bulkheads, and graceful degradation - **Observability**: Distributed tracing, centralized logging, and comprehensive metrics - **Security**: Zero-trust network architecture with mTLS encryption ### Architecture Diagram (Conceptual) ``` \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 Load Balancer (ALB) \u2502 \u2502 (TLS Termination - 443) \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u25bc \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 API Gateway Layer \u2502 \u2502 (Rate Limiting, Auth, Request Routing) \u2502 \u2514\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u2502 \u2502 \u2502 \u2502 \u25bc \u25bc \u25bc \u25bc \u25bc \u250c\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502Auth\u2502 \u2502Workflow\u2502 \u2502Scheduler\u2502 \u2502Notification\u2502 \u2502User Service\u2502 \u2502Svc \u2502 \u2502 Engine \u2502 \u2502 Service \u2502 \u2502 Service \u2502 \u2502 \u2502 \u2514\u2500\u252c\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u2502 \u2502 \u2502 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 \u2502 \u2502 \u25bc \u25bc \u25bc \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502PostgreSQL\u2502 \u2502 Redis \u2502 \u2502 Kafka \u2502 \u2502 Cluster \u2502 \u2502 Cluster \u2502 \u2502 Cluster \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 ``` ### Technology Stack - **Runtime**: Node.js 20.x (API Gateway, Workflow Engine), Go 1.21 (Auth Service, Scheduler) - **Container Orchestration**: Kubernetes 1.28 (EKS) - **Service Mesh**: Istio 1.20 for service-to-service communication - **Databases**: PostgreSQL 15.4, Redis 7.2 - **Message Broker**: Apache Kafka 3.6 - **Monitoring**: Prometheus, Grafana, Jaeger for distributed tracing - **Secrets Management**: HashiCorp Vault 1.15 --- ## Microservices Breakdown ### API Gateway **Purpose**: Single entry point for all client requests, providing authentication, rate limiting, request routing, and protocol translation.",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_8",
              "content": "Leverage caching:** ```bash # Enable client-side caching export CLOUDFLOW_CACHE_ENABLED=true export CLOUDFLOW_CACHE_TTL=300 # Cache workflow metadata cloudflow workflows list --use-cache --cache-ttl 600 ``` --- ## Log Analysis & Debugging ### Accessing CloudFlow Logs #### Kubernetes Deployments ```bash # List all CloudFlow pods kubectl get pods -n cloudflow # Tail logs from API server kubectl logs -f -n cloudflow deployment/cloudflow-api --tail=100 # Get logs from specific pod kubectl logs -n cloudflow cloudflow-api-7d4f6b8c9d-x7k2m # Get logs from previous crashed pod kubectl logs -n cloudflow cloudflow-api-7d4f6b8c9d-x7k2m --previous # Get logs from all pods in deployment kubectl logs -n cloudflow deployment/cloudflow-api --all-containers=true # Stream logs from multiple pods kubectl logs -n cloudflow -l app=cloudflow-api -f --max-log-requests=10 ``` #### Log Levels CloudFlow supports the following log levels: - `TRACE` - Very detailed debugging information - `DEBUG` - Detailed debugging information - `INFO` - Informational messages (default) - `WARN` - Warning messages - `ERROR` - Error messages - `FATAL` - Fatal errors causing shutdown **Changing log levels:** ```bash # Set global log level cloudflow config set logging.level DEBUG # Set log level for specific component cloudflow config set logging.components.database DEBUG cloudflow config set logging.components.auth INFO cloudflow config set logging.components.workflows TRACE # Temporary log level increase (resets after 1 hour) cloudflow debug set-log-level DEBUG --duration 3600 # View current log configuration cloudflow config get logging --format json ``` ### Grep Patterns for Common Issues #### Authentication Failures ```bash # Find all authentication errors kubectl logs -n cloudflow deployment/cloudflow-api | \\ grep -i \"authentication\\|401\\|unauthorized\" | \\ tail -n 50 # Find JWT validation failures kubectl logs -n cloudflow deployment/cloudflow-api | \\ grep -E \"JWT|token.*invalid|signature.*failed\" # Find clock skew issues kubectl logs -n cloudflow deployment/cloudflow-api | \\ grep -i \"clock skew\\|time.*sync\\|nbf\\|exp\" ``` #### Database Errors ```bash # Find database connection errors kubectl logs -n cloudflow deployment/cloudflow-api | \\ grep -E \"connection.*pool|could not connect|database.*timeout\" # Find slow queries kubectl logs -n cloudflow deployment/cloudflow-api | \\ grep \"slow query\" | \\ awk '{print $NF}' | \\ sort -n | \\ tail -n 20 # Find deadlock errors kubectl logs -n cloudflow deployment/cloudflow-api | \\ grep -i \"deadlock detected\" ``` #### Workflow Execution Errors ```bash # Find workflow timeout errors kubectl logs -n cloudflow deployment/cloudflow-workflow-engine | \\ grep -E \"timeout|exceeded.*3600\" # Find workflow retry attempts kubectl logs -n cloudflow deployment/cloudflow-workflow-engine | \\ grep -E \"retry attempt [0-9]|retrying",
              "score": null
            },
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_1",
              "content": "create cluster -f cluster-config.yaml ``` ### Storage Configuration Install the EBS CSI driver for persistent volumes: ```bash eksctl create addon --name aws-ebs-csi-driver \\ --cluster cloudflow-production \\ --service-account-role-arn arn:aws:iam::ACCOUNT_ID:role/AmazonEKS_EBS_CSI_DriverRole \\ --force ``` --- ## Kubernetes Deployment ### Namespace Setup Create the CloudFlow namespace and configure resource quotas: ```yaml # namespace.yaml apiVersion: v1 kind: Namespace metadata: name: cloudflow-prod labels: name: cloudflow-prod environment: production --- apiVersion: v1 kind: ResourceQuota metadata: name: cloudflow-quota namespace: cloudflow-prod spec: hard: requests.cpu: \"50\" requests.memory: 100Gi persistentvolumeclaims: \"10\" services.loadbalancers: \"2\" ``` Apply the namespace configuration: ```bash kubectl apply -f namespace.yaml ``` ### Environment Configuration CloudFlow requires the following environment variables: | Variable | Description | Example | Required | |----------|-------------|---------|----------| | `DATABASE_URL` | PostgreSQL connection string | `postgresql://user:pass@host:5432/cloudflow` | Yes | | `REDIS_URL` | Redis connection string | `redis://redis-master.cloudflow-prod.svc.cluster.local:6379` | Yes | | `JWT_SECRET` | Secret key for JWT token signing | `<generated-secret-256-bit>` | Yes | | `LOG_LEVEL` | Application log verbosity | `info`, `debug`, `warn`, `error` | No (default: `info`) | | `API_PORT` | API server port | `3000` | No (default: `3000`) | | `WORKER_CONCURRENCY` | Number of concurrent workers | `10` | No (default: `5`) | | `SESSION_SECRET` | Session encryption key | `<generated-secret-256-bit>` | Yes | | `AWS_REGION` | AWS region for services | `us-east-1` | Yes | | `S3_BUCKET` | S3 bucket for file storage | `cloudflow-prod-storage` | Yes | | `SMTP_HOST` | Email server hostname | `smtp.sendgrid.net` | No | | `SMTP_PORT` | Email server port | `587` | No | | `METRICS_ENABLED` | Enable Prometheus metrics | `true` | No (default: `false`) | Create a Kubernetes secret for sensitive variables: ```bash kubectl create secret generic cloudflow-secrets \\ --namespace cloudflow-prod \\ --from-literal=DATABASE_URL=\"postgresql://cloudflow:$(cat db-password.txt)@pgbouncer.cloudflow-prod.svc.cluster.local:5432/cloudflow\" \\ --from-literal=REDIS_URL=\"redis://redis-master.cloudflow-prod.svc.cluster.local:6379\" \\ --from-literal=JWT_SECRET=\"$(openssl rand -base64 32)\" \\ --from-literal=SESSION_SECRET=\"$(openssl rand -base64 32)\" ``` ### Helm Chart Deployment Add the CloudFlow Helm repository: ```bash helm repo add cloudflow https://charts.cloudflow.io helm repo update ``` Create a values file for production configuration: ```yaml # values-production.yaml replicaCount: 3 image: repository: 123456789012.dkr.ecr.us-east-1.amazonaws.com/cloudflow tag: \"2.4.0\" pullPolicy: IfNotPresent resources: limits: cpu: 2000m memory: 4Gi requests: cpu: 2000m memory: 4Gi autoscaling: enabled: true minReplicas: 3 maxReplicas: 10 targetCPUUtilizationPercentage: 70 targetMemoryUtilizationPercentage: 80 service: type: ClusterIP port: 80 targetPort: 3000 annotations: service.beta.kubernetes.io/aws-load-balancer-type: \"nlb\" ingress: enabled: true className: nginx annotations: cert-manager.io/cluster-issuer: \"letsencrypt-prod\" nginx.ingress.kubernetes.io/ssl-redirect: \"true\" hosts: - host: api.cloudflow.io paths: - path: / pathType: Prefix tls:",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_3",
              "content": "**Technology**: Go with distributed locking via Redis **Replicas**: 4 pods (production), active-passive with leader election **Resource Allocation**: 2 vCPU, 4GB RAM per pod **Key Responsibilities**: - Parse and validate cron expressions (extended format supporting seconds) - Maintain schedule registry in PostgreSQL - Distributed scheduling with leader election (one active scheduler) - Missed execution handling with configurable catch-up policy - Schedule conflict detection and resolution - Time zone support for international schedules **Scheduling Architecture**: ``` PostgreSQL Schedules Table \u2502 \u25bc Leader Scheduler (elected via Redis) \u2502 \u251c\u2500 Scan for due schedules (every 10 seconds) \u251c\u2500 Acquire lock per schedule (prevents duplicates) \u251c\u2500 Publish to Kafka \u2192 workflow.events topic \u2514\u2500 Update last_run timestamp ``` **Schedule Types**: - **Cron-based**: `0 */5 * * * *` (every 5 minutes) - **One-time**: Specific timestamp for delayed execution - **Interval-based**: Every N seconds/minutes/hours **Reliability Features**: - Leader election using Redis with 30-second lease - Heartbeat mechanism to detect leader failure (5-second interval) - Automatic failover to standby scheduler (< 10 seconds) - Schedule versioning to handle updates during execution - Missed execution policy: SKIP, RUN_ONCE, or RUN_ALL **Performance Targets**: - Schedule evaluation: < 100ms per cycle - Accuracy: \u00b1 1 second for schedule triggers - Capacity: 100,000 active schedules --- ### Notification Service **Purpose**: Multi-channel notification delivery system supporting email, SMS, webhooks, and in-app notifications.",
              "score": null
            }
          ],
          "latency_ms": 925.9595509975043,
          "query_id": "realistic_008",
          "dimension": "synonym",
          "query": "How does horizontal pod autoscaling work?"
        },
        {
          "key_facts": [
            "minReplicas: 3",
            "maxReplicas: 10",
            "targetCPUUtilizationPercentage: 70"
          ],
          "found_facts": [
            "minReplicas: 3",
            "maxReplicas: 10"
          ],
          "missed_facts": [
            "targetCPUUtilizationPercentage: 70"
          ],
          "coverage": 0.6666666666666666,
          "expected_docs": [
            "deployment_guide"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_6",
              "content": "**Verify Service Health**: ```bash kubectl get pods -n cloudflow-prod curl https://api.cloudflow.io/health ``` **Recovery Time Objective (RTO)**: 4 hours **Recovery Point Objective (RPO)**: 24 hours --- ## Scaling and Performance ### Horizontal Pod Autoscaling CloudFlow is configured with Horizontal Pod Autoscaler (HPA) to automatically scale based on resource utilization: ```yaml # hpa.yaml apiVersion: autoscaling/v2 kind: HorizontalPodAutoscaler metadata: name: cloudflow-api-hpa namespace: cloudflow-prod spec: scaleTargetRef: apiVersion: apps/v1 kind: Deployment name: cloudflow-api minReplicas: 3 maxReplicas: 10 metrics: - type: Resource resource: name: cpu target: type: Utilization averageUtilization: 70 - type: Resource resource: name: memory target: type: Utilization averageUtilization: 80 behavior: scaleDown: stabilizationWindowSeconds: 300 policies: - type: Percent value: 50 periodSeconds: 60 scaleUp: stabilizationWindowSeconds: 60 policies: - type: Percent value: 100 periodSeconds: 30 - type: Pods value: 2 periodSeconds: 30 selectPolicy: Max ``` Monitor autoscaling events: ```bash kubectl get hpa -n cloudflow-prod -w kubectl describe hpa cloudflow-api-hpa -n cloudflow-prod ``` ### Performance Optimization #### Connection Pooling Ensure optimal database connection pooling settings in PgBouncer: - **Pool Mode**: Transaction (optimal for microservices) - **Default Pool Size**: 25 connections per user - **Max DB Connections**: 100 (matches PostgreSQL `max_connections`) #### Redis Caching Implement Redis caching for frequently accessed data: ```javascript // Example caching strategy const CACHE_TTL = { workflows: 300, // 5 minutes userSessions: 3600, // 1 hour apiResults: 60 // 1 minute }; ``` #### Load Testing Perform regular load testing to validate scaling configuration: ```bash # Install k6 for load testing kubectl apply -f - <<EOF apiVersion: v1 kind: ConfigMap metadata: name: loadtest-script namespace: cloudflow-prod data: script.js: | import http from 'k6/http'; import { check, sleep } from 'k6'; export let options = { stages: [ { duration: '2m', target: 100 }, { duration: '5m', target: 100 }, { duration: '2m', target: 200 }, { duration: '5m', target: 200 }, { duration: '2m', target: 0 }, ], thresholds: { http_req_duration: ['p(95)<500'], }, }; export default function () { let res = http.get('https://api.cloudflow.io/api/workflows'); check(res, { 'status is 200': (r) => r.status === 200 }); sleep(1); } EOF ``` **Performance Targets**: - P95 API latency: < 500ms - P99 API latency: < 1000ms - Error rate: < 0.1% - Throughput: > 1000 requests/second --- ## Security and Compliance ### Network Policies Implement network policies to restrict pod-to-pod communication: ```yaml # network-policy.yaml apiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: cloudflow-network-policy namespace: cloudflow-prod spec:",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_0",
              "content": "# CloudFlow Platform - System Architecture Overview **Document Version:** 2.3.1 **Last Updated:** January 15, 2026 **Owner:** Platform Architecture Team **Status:** Production ## Executive Summary CloudFlow is a distributed, cloud-native workflow automation platform designed to orchestrate complex business processes at scale. The platform processes over 2.5 million workflow executions daily with an average P99 latency of 180ms for API operations and 4.2 seconds for workflow execution. This document provides a comprehensive overview of the system architecture, including microservices design, data flow patterns, infrastructure decisions, and operational considerations. ## Table of Contents 1. [High-Level Architecture](#high-level-architecture) 2. [Microservices Breakdown](#microservices-breakdown) 3. [Data Flow Architecture](#data-flow-architecture) 4. [Database Architecture](#database-architecture) 5. [Message Queue Patterns](#message-queue-patterns) 6. [Caching Strategy](#caching-strategy) 7. [Security Architecture](#security-architecture) 8. [Performance Characteristics](#performance-characteristics) 9. [Disaster Recovery](#disaster-recovery) --- ## High-Level Architecture CloudFlow follows a microservices architecture pattern deployed across multiple availability zones in AWS. The platform is designed with the following core principles: - **Scalability**: Horizontal scaling for all services with auto-scaling groups - **Resilience**: Circuit breakers, bulkheads, and graceful degradation - **Observability**: Distributed tracing, centralized logging, and comprehensive metrics - **Security**: Zero-trust network architecture with mTLS encryption ### Architecture Diagram (Conceptual) ``` \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 Load Balancer (ALB) \u2502 \u2502 (TLS Termination - 443) \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u25bc \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 API Gateway Layer \u2502 \u2502 (Rate Limiting, Auth, Request Routing) \u2502 \u2514\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u2502 \u2502 \u2502 \u2502 \u25bc \u25bc \u25bc \u25bc \u25bc \u250c\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502Auth\u2502 \u2502Workflow\u2502 \u2502Scheduler\u2502 \u2502Notification\u2502 \u2502User Service\u2502 \u2502Svc \u2502 \u2502 Engine \u2502 \u2502 Service \u2502 \u2502 Service \u2502 \u2502 \u2502 \u2514\u2500\u252c\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u2502 \u2502 \u2502 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 \u2502 \u2502 \u25bc \u25bc \u25bc \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502PostgreSQL\u2502 \u2502 Redis \u2502 \u2502 Kafka \u2502 \u2502 Cluster \u2502 \u2502 Cluster \u2502 \u2502 Cluster \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 ``` ### Technology Stack - **Runtime**: Node.js 20.x (API Gateway, Workflow Engine), Go 1.21 (Auth Service, Scheduler) - **Container Orchestration**: Kubernetes 1.28 (EKS) - **Service Mesh**: Istio 1.20 for service-to-service communication - **Databases**: PostgreSQL 15.4, Redis 7.2 - **Message Broker**: Apache Kafka 3.6 - **Monitoring**: Prometheus, Grafana, Jaeger for distributed tracing - **Secrets Management**: HashiCorp Vault 1.15 --- ## Microservices Breakdown ### API Gateway **Purpose**: Single entry point for all client requests, providing authentication, rate limiting, request routing, and protocol translation.",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_11",
              "content": "Service (2) - PostgreSQL Primary - PostgreSQL Replica - PostgreSQL Replica - Redis Primary (2) - Redis Replica (2) - Redis Replica (2) - Kafka Broker (2) - Kafka Broker (2) - Kafka Broker (1) ``` **Automatic Failover**: - Database: 30-60 seconds (automatic promotion of replica) - Redis: < 10 seconds (Sentinel-based failover) - Kafka: < 30 seconds (controller election) - Services: Kubernetes health checks with 10-second liveness probes ### Backup Strategy **Database Backups**: - Automated snapshots: Daily at 02:00 UTC - Retention: 30 days for daily, 90 days for monthly - Cross-region replication: Async replication to us-west-2 (15-minute lag) - Backup verification: Weekly automated restore test in staging environment **Backup Schedule**: ``` Daily: Full snapshot \u2192 S3 (encrypted) Hourly: WAL archives \u2192 S3 (point-in-time recovery) Weekly: Backup validation test Monthly: Long-term archive to Glacier ``` **Configuration Backups**: - Kubernetes manifests: Stored in Git (GitOps with ArgoCD) - Vault secrets: Automated snapshot every 6 hours - Kafka topic configurations: Exported daily to S3 ### Disaster Recovery Procedures **Scenario 1: Single AZ Failure** - Detection: Health checks fail for entire AZ (< 30 seconds) - Action: Traffic automatically routed to healthy AZs by ALB - Recovery time: < 5 minutes (no manual intervention) - Data loss: None (multi-AZ replication) **Scenario 2: Database Primary Failure** - Detection: Health check fails for primary database (< 30 seconds) - Action: Automatic promotion of read replica to primary - Recovery time: 30-60 seconds - Data loss: Minimal (< 1 second due to synchronous replication) **Scenario 3: Full Region Failure** - Detection: Multiple health check failures across all AZs (< 2 minutes) - Action: Manual failover to DR region (us-west-2) - Procedure: 1. Update DNS to point to DR region (TTL: 60 seconds) 2. Promote DR database replica to primary 3. Scale up DR region services to production capacity 4. Verify data consistency and integrity 5. Update monitoring dashboards - Recovery time: 2-4 hours (includes verification) - Data loss: < 1 hour (last cross-region replication) **Scenario 4: Data Corruption** - Detection: Data validation checks or user report - Action: Point-in-time recovery from WAL archives - Procedure: 1. Identify corruption time window 2. Restore from snapshot prior to corruption 3. Replay WAL logs up to corruption point 4. Verify data integrity 5.",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_8",
              "content": "Leverage caching:** ```bash # Enable client-side caching export CLOUDFLOW_CACHE_ENABLED=true export CLOUDFLOW_CACHE_TTL=300 # Cache workflow metadata cloudflow workflows list --use-cache --cache-ttl 600 ``` --- ## Log Analysis & Debugging ### Accessing CloudFlow Logs #### Kubernetes Deployments ```bash # List all CloudFlow pods kubectl get pods -n cloudflow # Tail logs from API server kubectl logs -f -n cloudflow deployment/cloudflow-api --tail=100 # Get logs from specific pod kubectl logs -n cloudflow cloudflow-api-7d4f6b8c9d-x7k2m # Get logs from previous crashed pod kubectl logs -n cloudflow cloudflow-api-7d4f6b8c9d-x7k2m --previous # Get logs from all pods in deployment kubectl logs -n cloudflow deployment/cloudflow-api --all-containers=true # Stream logs from multiple pods kubectl logs -n cloudflow -l app=cloudflow-api -f --max-log-requests=10 ``` #### Log Levels CloudFlow supports the following log levels: - `TRACE` - Very detailed debugging information - `DEBUG` - Detailed debugging information - `INFO` - Informational messages (default) - `WARN` - Warning messages - `ERROR` - Error messages - `FATAL` - Fatal errors causing shutdown **Changing log levels:** ```bash # Set global log level cloudflow config set logging.level DEBUG # Set log level for specific component cloudflow config set logging.components.database DEBUG cloudflow config set logging.components.auth INFO cloudflow config set logging.components.workflows TRACE # Temporary log level increase (resets after 1 hour) cloudflow debug set-log-level DEBUG --duration 3600 # View current log configuration cloudflow config get logging --format json ``` ### Grep Patterns for Common Issues #### Authentication Failures ```bash # Find all authentication errors kubectl logs -n cloudflow deployment/cloudflow-api | \\ grep -i \"authentication\\|401\\|unauthorized\" | \\ tail -n 50 # Find JWT validation failures kubectl logs -n cloudflow deployment/cloudflow-api | \\ grep -E \"JWT|token.*invalid|signature.*failed\" # Find clock skew issues kubectl logs -n cloudflow deployment/cloudflow-api | \\ grep -i \"clock skew\\|time.*sync\\|nbf\\|exp\" ``` #### Database Errors ```bash # Find database connection errors kubectl logs -n cloudflow deployment/cloudflow-api | \\ grep -E \"connection.*pool|could not connect|database.*timeout\" # Find slow queries kubectl logs -n cloudflow deployment/cloudflow-api | \\ grep \"slow query\" | \\ awk '{print $NF}' | \\ sort -n | \\ tail -n 20 # Find deadlock errors kubectl logs -n cloudflow deployment/cloudflow-api | \\ grep -i \"deadlock detected\" ``` #### Workflow Execution Errors ```bash # Find workflow timeout errors kubectl logs -n cloudflow deployment/cloudflow-workflow-engine | \\ grep -E \"timeout|exceeded.*3600\" # Find workflow retry attempts kubectl logs -n cloudflow deployment/cloudflow-workflow-engine | \\ grep -E \"retry attempt [0-9]|retrying",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_5",
              "content": "exec_7h3j6k9m2n --show-bottlenecks ``` #### Solutions **1. Increase workflow timeout (if justified):** ```bash # Update workflow configuration cloudflow workflows update wf_9k2n4m8p1q \\ --timeout 7200 \\ --reason \"Large dataset processing requires extended time\" # Verify update cloudflow workflows get wf_9k2n4m8p1q | grep timeout ``` **2. Optimize slow steps:** ```bash # Enable parallel processing cloudflow workflows update wf_9k2n4m8p1q \\ --step data_transformation \\ --parallel-workers 8 \\ --batch-size 1000 # Add checkpointing for long operations cloudflow workflows update wf_9k2n4m8p1q \\ --step data_transformation \\ --enable-checkpointing \\ --checkpoint-interval 300 ``` **3. Split workflow into smaller workflows:** ```bash # Create sub-workflows cloudflow workflows create data-pipeline-part1 \\ --steps \"data_ingestion,data_validation\" \\ --timeout 1800 cloudflow workflows create data-pipeline-part2 \\ --steps \"data_transformation,data_export\" \\ --timeout 3600 \\ --trigger workflow_completed \\ --trigger-workflow data-pipeline-part1 ``` ### Retry Logic and Exponential Backoff CloudFlow implements automatic retry with exponential backoff for transient failures: - Max retries: 3 - Initial delay: 1 second - Backoff multiplier: 2 - Max delay: 60 seconds #### Retry Sequence ``` Attempt 1: Immediate Attempt 2: Wait 1s (2^0 * 1s) Attempt 3: Wait 2s (2^1 * 1s) Attempt 4: Wait 4s (2^2 * 1s) ``` #### Configuration ```bash # View current retry settings cloudflow workflows get wf_9k2n4m8p1q --format json | jq '.retry_policy' # Customize retry behavior cloudflow workflows update wf_9k2n4m8p1q \\ --retry-max-attempts 5 \\ --retry-initial-delay 2000 \\ --retry-backoff-multiplier 2 \\ --retry-max-delay 120000 \\ --retry-on-errors \"NETWORK_ERROR,TIMEOUT,RATE_LIMIT\" # Disable retry for specific step cloudflow workflows update wf_9k2n4m8p1q \\ --step data_export \\ --retry-enabled false ``` #### Monitoring Retries ```bash # List failed executions with retry information cloudflow workflows executions list \\ --status FAILED \\ --show-retries \\ --last 7d # Get retry history for specific execution cloudflow workflows executions retries exec_7h3j6k9m2n # Output: # Execution: exec_7h3j6k9m2n # Attempt 1: FAILED - NetworkError: Connection refused (delay: 0ms) # Attempt 2: FAILED - NetworkError: Connection timeout (delay: 1000ms) # Attempt 3: FAILED - NetworkError: Connection timeout (delay: 2000ms) # Attempt 4: FAILED - NetworkError: Connection timeout (delay: 4000ms) # Final Status: FAILED_AFTER_RETRIES ``` ### Workflow Step Failures #### Common Error Patterns **1.",
              "score": null
            }
          ],
          "latency_ms": 939.6619250001095,
          "query_id": "realistic_008",
          "dimension": "problem",
          "query": "Pods aren't scaling up during traffic spikes"
        },
        {
          "key_facts": [
            "minReplicas: 3",
            "maxReplicas: 10",
            "targetCPUUtilizationPercentage: 70"
          ],
          "found_facts": [
            "minReplicas: 3",
            "maxReplicas: 10"
          ],
          "missed_facts": [
            "targetCPUUtilizationPercentage: 70"
          ],
          "coverage": 0.6666666666666666,
          "expected_docs": [
            "deployment_guide"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_6",
              "content": "**Verify Service Health**: ```bash kubectl get pods -n cloudflow-prod curl https://api.cloudflow.io/health ``` **Recovery Time Objective (RTO)**: 4 hours **Recovery Point Objective (RPO)**: 24 hours --- ## Scaling and Performance ### Horizontal Pod Autoscaling CloudFlow is configured with Horizontal Pod Autoscaler (HPA) to automatically scale based on resource utilization: ```yaml # hpa.yaml apiVersion: autoscaling/v2 kind: HorizontalPodAutoscaler metadata: name: cloudflow-api-hpa namespace: cloudflow-prod spec: scaleTargetRef: apiVersion: apps/v1 kind: Deployment name: cloudflow-api minReplicas: 3 maxReplicas: 10 metrics: - type: Resource resource: name: cpu target: type: Utilization averageUtilization: 70 - type: Resource resource: name: memory target: type: Utilization averageUtilization: 80 behavior: scaleDown: stabilizationWindowSeconds: 300 policies: - type: Percent value: 50 periodSeconds: 60 scaleUp: stabilizationWindowSeconds: 60 policies: - type: Percent value: 100 periodSeconds: 30 - type: Pods value: 2 periodSeconds: 30 selectPolicy: Max ``` Monitor autoscaling events: ```bash kubectl get hpa -n cloudflow-prod -w kubectl describe hpa cloudflow-api-hpa -n cloudflow-prod ``` ### Performance Optimization #### Connection Pooling Ensure optimal database connection pooling settings in PgBouncer: - **Pool Mode**: Transaction (optimal for microservices) - **Default Pool Size**: 25 connections per user - **Max DB Connections**: 100 (matches PostgreSQL `max_connections`) #### Redis Caching Implement Redis caching for frequently accessed data: ```javascript // Example caching strategy const CACHE_TTL = { workflows: 300, // 5 minutes userSessions: 3600, // 1 hour apiResults: 60 // 1 minute }; ``` #### Load Testing Perform regular load testing to validate scaling configuration: ```bash # Install k6 for load testing kubectl apply -f - <<EOF apiVersion: v1 kind: ConfigMap metadata: name: loadtest-script namespace: cloudflow-prod data: script.js: | import http from 'k6/http'; import { check, sleep } from 'k6'; export let options = { stages: [ { duration: '2m', target: 100 }, { duration: '5m', target: 100 }, { duration: '2m', target: 200 }, { duration: '5m', target: 200 }, { duration: '2m', target: 0 }, ], thresholds: { http_req_duration: ['p(95)<500'], }, }; export default function () { let res = http.get('https://api.cloudflow.io/api/workflows'); check(res, { 'status is 200': (r) => r.status === 200 }); sleep(1); } EOF ``` **Performance Targets**: - P95 API latency: < 500ms - P99 API latency: < 1000ms - Error rate: < 0.1% - Throughput: > 1000 requests/second --- ## Security and Compliance ### Network Policies Implement network policies to restrict pod-to-pod communication: ```yaml # network-policy.yaml apiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: cloudflow-network-policy namespace: cloudflow-prod spec:",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_10",
              "content": "- Execution start rate: 500 per second - Completion rate: 450 per second (average 2-second execution time) **Database**: - Read throughput: 50,000 queries per second (across replicas) - Write throughput: 15,000 transactions per second - Connection capacity: 2,000 concurrent connections **Kafka**: - Message ingestion: 100,000 messages per second - Consumer throughput: 80,000 messages per second (aggregated) - End-to-end latency: < 100ms (P99) ### Resource Utilization **CPU Utilization** (Target: 60-70% average): - API Gateway: 55% average, 80% peak - Workflow Engine: 65% average, 85% peak - Auth Service: 40% average, 60% peak **Memory Utilization** (Target: < 80%): - API Gateway: 2.5GB average per pod (4GB allocated) - Workflow Engine: 6GB average per pod (8GB allocated) - Notification Service: 2.8GB average per pod (4GB allocated) **Network Throughput**: - Ingress: 2 Gbps average, 5 Gbps peak - Egress: 1.5 Gbps average, 4 Gbps peak - Internal (service mesh): 8 Gbps average ### Monitoring & Alerting **Key Metrics** (Monitored via Prometheus): - Request rate (per service, per endpoint) - Error rate (4xx, 5xx responses) - Latency percentiles (P50, P95, P99) - Resource utilization (CPU, memory, disk) - Cache hit/miss ratios - Database connection pool saturation - Kafka consumer lag **Alerts** (via PagerDuty): - P1 (Immediate): API error rate > 1%, database connection failure - P2 (< 15 min): P99 latency > 500ms, cache hit rate < 80% - P3 (< 1 hour): Resource utilization > 85%, replica count < minimum **SLI/SLO/SLA**: - SLI: API success rate (non-5xx responses) - SLO: 99.9% uptime per month (43 minutes downtime allowance) - SLA: 99.5% uptime guarantee (customer-facing SLA with credits) --- ## Disaster Recovery ### Recovery Objectives **RPO (Recovery Point Objective): 1 hour** - Maximum acceptable data loss: 1 hour of transactions - Achieved through: Continuous database replication + hourly snapshots - Kafka retention: 7 days allows event replay **RTO (Recovery Time Objective): 4 hours** - Maximum acceptable downtime: 4 hours for full system recovery - Includes: Failover, data verification, and service restoration - Automated runbooks reduce RTO to < 2 hours for common scenarios ### High Availability Architecture **Multi-AZ Deployment**: ``` Region: us-east-1 AZ-1a: AZ-1b: AZ-1c: - API Gateway (4) - API Gateway (4) - API Gateway (4) - Workflow Engine (6) - Workflow Engine (5) - Workflow Engine (5) - Auth Service (3) - Auth Service (3) - Auth",
              "score": null
            },
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_4",
              "content": "--set prometheus.prometheusSpec.retention=30d \\ --set prometheus.prometheusSpec.storageSpec.volumeClaimTemplate.spec.resources.requests.storage=100Gi ``` CloudFlow exposes Prometheus metrics on port 9090 at the `/metrics` endpoint. The metrics include: - **HTTP Metrics**: Request rate, latency, error rate - **Database Metrics**: Connection pool status, query duration - **Worker Metrics**: Job queue size, processing time, failure rate - **System Metrics**: CPU usage, memory usage, heap statistics Example metrics exposed: ``` # HELP cloudflow_http_requests_total Total number of HTTP requests # TYPE cloudflow_http_requests_total counter cloudflow_http_requests_total{method=\"GET\",route=\"/api/workflows\",status=\"200\"} 15420 # HELP cloudflow_http_request_duration_seconds HTTP request duration in seconds # TYPE cloudflow_http_request_duration_seconds histogram cloudflow_http_request_duration_seconds_bucket{method=\"POST\",route=\"/api/workflows\",le=\"0.1\"} 8234 # HELP cloudflow_worker_jobs_processed_total Total number of jobs processed # TYPE cloudflow_worker_jobs_processed_total counter cloudflow_worker_jobs_processed_total{status=\"success\"} 45230 # HELP cloudflow_database_connections Current database connections # TYPE cloudflow_database_connections gauge cloudflow_database_connections{state=\"active\"} 23 ``` ### Grafana Dashboards Access Grafana to view CloudFlow dashboards: ```bash kubectl port-forward -n monitoring svc/prometheus-grafana 3000:80 ``` Import the CloudFlow dashboard (ID: 15847) or use the provided JSON template. Key dashboard panels include: 1. **API Performance**: Request rate, P95/P99 latency, error rate 2. **Resource Usage**: CPU, memory, disk I/O per pod 3. **Database Health**: Connection pool utilization, query performance 4. **Worker Status**: Queue depth, processing rate, job success/failure ratio 5. **System Overview**: Pod status, replica count, autoscaling events ### Logging with CloudWatch CloudFlow logs are automatically shipped to CloudWatch Logs.",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_9",
              "content": "per API key (default: 1000 RPM) ### Secrets Management **HashiCorp Vault Integration**: - Dynamic database credentials: Generated on-demand, 1-hour TTL - Encryption keys: Transit secrets engine for encryption-as-a-service - API keys for external services: Stored with versioning - Rotation policy: Automated rotation every 90 days with notification **Secret Access Pattern**: ``` Service starts \u2192 Vault authentication (Kubernetes service account) \u2502 \u25bc Request secret lease (1-hour TTL) \u2502 \u25bc Vault returns dynamic credentials \u2502 \u25bc Service connects to PostgreSQL \u2502 Renew lease every 30 minutes ``` ### Data Protection **Encryption at Rest**: - PostgreSQL: AES-256 encryption enabled at volume level - Redis: Encryption enabled using AWS KMS - Kafka: Encryption at rest for all data on brokers - S3 backups: Server-side encryption with KMS (SSE-KMS) **Encryption in Transit**: - External traffic: TLS 1.3 only (TLS 1.2 deprecated) - Internal traffic: mTLS via Istio service mesh - Database connections: SSL/TLS enforced (sslmode=require) **Data Masking**: - Sensitive fields (email, phone) masked in logs - PII redacted from error messages and stack traces - Audit logs: Full data retention with access controls **Compliance**: - GDPR: User data export and deletion workflows - SOC 2 Type II: Audit logging for all data access - HIPAA: PHI isolation in dedicated tenants with enhanced encryption --- ## Performance Characteristics ### Latency Targets **API Operations** (P99 latency): - `GET /workflows/{id}`: < 50ms (cache hit), < 150ms (cache miss) - `POST /workflows`: < 200ms (includes validation and database write) - `POST /workflows/{id}/execute`: < 100ms (async, returns execution ID) - `GET /workflows/{id}/history`: < 300ms (paginated, 50 records per page) **Workflow Execution** (P99 latency): - Simple workflow (< 5 steps): < 2 seconds - Medium workflow (5-15 steps): < 5 seconds - Complex workflow (> 15 steps): < 15 seconds - Parallel execution: Linear scaling up to 10 concurrent branches **Database Operations** (P99 latency): - Simple SELECT: < 5ms - JOIN query: < 20ms - INSERT/UPDATE: < 10ms - Batch operations: < 100ms (batch size: 100 records) **Cache Operations** (P99 latency): - Redis GET: < 2ms - Redis SET: < 3ms - Redis DEL: < 2ms ### Throughput Capacity **API Gateway**: - Sustained: 10,000 requests per second - Peak: 25,000 requests per second (5-minute burst) - Rate limiting: 1,000 requests per minute per API key **Workflow Engine**: - Concurrent executions: 8,000 workflows (across 16 pods)",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_0",
              "content": "# CloudFlow Platform - System Architecture Overview **Document Version:** 2.3.1 **Last Updated:** January 15, 2026 **Owner:** Platform Architecture Team **Status:** Production ## Executive Summary CloudFlow is a distributed, cloud-native workflow automation platform designed to orchestrate complex business processes at scale. The platform processes over 2.5 million workflow executions daily with an average P99 latency of 180ms for API operations and 4.2 seconds for workflow execution. This document provides a comprehensive overview of the system architecture, including microservices design, data flow patterns, infrastructure decisions, and operational considerations. ## Table of Contents 1. [High-Level Architecture](#high-level-architecture) 2. [Microservices Breakdown](#microservices-breakdown) 3. [Data Flow Architecture](#data-flow-architecture) 4. [Database Architecture](#database-architecture) 5. [Message Queue Patterns](#message-queue-patterns) 6. [Caching Strategy](#caching-strategy) 7. [Security Architecture](#security-architecture) 8. [Performance Characteristics](#performance-characteristics) 9. [Disaster Recovery](#disaster-recovery) --- ## High-Level Architecture CloudFlow follows a microservices architecture pattern deployed across multiple availability zones in AWS. The platform is designed with the following core principles: - **Scalability**: Horizontal scaling for all services with auto-scaling groups - **Resilience**: Circuit breakers, bulkheads, and graceful degradation - **Observability**: Distributed tracing, centralized logging, and comprehensive metrics - **Security**: Zero-trust network architecture with mTLS encryption ### Architecture Diagram (Conceptual) ``` \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 Load Balancer (ALB) \u2502 \u2502 (TLS Termination - 443) \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u25bc \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 API Gateway Layer \u2502 \u2502 (Rate Limiting, Auth, Request Routing) \u2502 \u2514\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u2502 \u2502 \u2502 \u2502 \u25bc \u25bc \u25bc \u25bc \u25bc \u250c\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502Auth\u2502 \u2502Workflow\u2502 \u2502Scheduler\u2502 \u2502Notification\u2502 \u2502User Service\u2502 \u2502Svc \u2502 \u2502 Engine \u2502 \u2502 Service \u2502 \u2502 Service \u2502 \u2502 \u2502 \u2514\u2500\u252c\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u2502 \u2502 \u2502 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 \u2502 \u2502 \u25bc \u25bc \u25bc \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502PostgreSQL\u2502 \u2502 Redis \u2502 \u2502 Kafka \u2502 \u2502 Cluster \u2502 \u2502 Cluster \u2502 \u2502 Cluster \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 ``` ### Technology Stack - **Runtime**: Node.js 20.x (API Gateway, Workflow Engine), Go 1.21 (Auth Service, Scheduler) - **Container Orchestration**: Kubernetes 1.28 (EKS) - **Service Mesh**: Istio 1.20 for service-to-service communication - **Databases**: PostgreSQL 15.4, Redis 7.2 - **Message Broker**: Apache Kafka 3.6 - **Monitoring**: Prometheus, Grafana, Jaeger for distributed tracing - **Secrets Management**: HashiCorp Vault 1.15 --- ## Microservices Breakdown ### API Gateway **Purpose**: Single entry point for all client requests, providing authentication, rate limiting, request routing, and protocol translation.",
              "score": null
            }
          ],
          "latency_ms": 889.7887330022058,
          "query_id": "realistic_008",
          "dimension": "casual",
          "query": "autoscaling config"
        },
        {
          "key_facts": [
            "minReplicas: 3",
            "maxReplicas: 10",
            "targetCPUUtilizationPercentage: 70"
          ],
          "found_facts": [
            "minReplicas: 3",
            "maxReplicas: 10"
          ],
          "missed_facts": [
            "targetCPUUtilizationPercentage: 70"
          ],
          "coverage": 0.6666666666666666,
          "expected_docs": [
            "deployment_guide"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_6",
              "content": "**Verify Service Health**: ```bash kubectl get pods -n cloudflow-prod curl https://api.cloudflow.io/health ``` **Recovery Time Objective (RTO)**: 4 hours **Recovery Point Objective (RPO)**: 24 hours --- ## Scaling and Performance ### Horizontal Pod Autoscaling CloudFlow is configured with Horizontal Pod Autoscaler (HPA) to automatically scale based on resource utilization: ```yaml # hpa.yaml apiVersion: autoscaling/v2 kind: HorizontalPodAutoscaler metadata: name: cloudflow-api-hpa namespace: cloudflow-prod spec: scaleTargetRef: apiVersion: apps/v1 kind: Deployment name: cloudflow-api minReplicas: 3 maxReplicas: 10 metrics: - type: Resource resource: name: cpu target: type: Utilization averageUtilization: 70 - type: Resource resource: name: memory target: type: Utilization averageUtilization: 80 behavior: scaleDown: stabilizationWindowSeconds: 300 policies: - type: Percent value: 50 periodSeconds: 60 scaleUp: stabilizationWindowSeconds: 60 policies: - type: Percent value: 100 periodSeconds: 30 - type: Pods value: 2 periodSeconds: 30 selectPolicy: Max ``` Monitor autoscaling events: ```bash kubectl get hpa -n cloudflow-prod -w kubectl describe hpa cloudflow-api-hpa -n cloudflow-prod ``` ### Performance Optimization #### Connection Pooling Ensure optimal database connection pooling settings in PgBouncer: - **Pool Mode**: Transaction (optimal for microservices) - **Default Pool Size**: 25 connections per user - **Max DB Connections**: 100 (matches PostgreSQL `max_connections`) #### Redis Caching Implement Redis caching for frequently accessed data: ```javascript // Example caching strategy const CACHE_TTL = { workflows: 300, // 5 minutes userSessions: 3600, // 1 hour apiResults: 60 // 1 minute }; ``` #### Load Testing Perform regular load testing to validate scaling configuration: ```bash # Install k6 for load testing kubectl apply -f - <<EOF apiVersion: v1 kind: ConfigMap metadata: name: loadtest-script namespace: cloudflow-prod data: script.js: | import http from 'k6/http'; import { check, sleep } from 'k6'; export let options = { stages: [ { duration: '2m', target: 100 }, { duration: '5m', target: 100 }, { duration: '2m', target: 200 }, { duration: '5m', target: 200 }, { duration: '2m', target: 0 }, ], thresholds: { http_req_duration: ['p(95)<500'], }, }; export default function () { let res = http.get('https://api.cloudflow.io/api/workflows'); check(res, { 'status is 200': (r) => r.status === 200 }); sleep(1); } EOF ``` **Performance Targets**: - P95 API latency: < 500ms - P99 API latency: < 1000ms - Error rate: < 0.1% - Throughput: > 1000 requests/second --- ## Security and Compliance ### Network Policies Implement network policies to restrict pod-to-pod communication: ```yaml # network-policy.yaml apiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: cloudflow-network-policy namespace: cloudflow-prod spec:",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_10",
              "content": "- Execution start rate: 500 per second - Completion rate: 450 per second (average 2-second execution time) **Database**: - Read throughput: 50,000 queries per second (across replicas) - Write throughput: 15,000 transactions per second - Connection capacity: 2,000 concurrent connections **Kafka**: - Message ingestion: 100,000 messages per second - Consumer throughput: 80,000 messages per second (aggregated) - End-to-end latency: < 100ms (P99) ### Resource Utilization **CPU Utilization** (Target: 60-70% average): - API Gateway: 55% average, 80% peak - Workflow Engine: 65% average, 85% peak - Auth Service: 40% average, 60% peak **Memory Utilization** (Target: < 80%): - API Gateway: 2.5GB average per pod (4GB allocated) - Workflow Engine: 6GB average per pod (8GB allocated) - Notification Service: 2.8GB average per pod (4GB allocated) **Network Throughput**: - Ingress: 2 Gbps average, 5 Gbps peak - Egress: 1.5 Gbps average, 4 Gbps peak - Internal (service mesh): 8 Gbps average ### Monitoring & Alerting **Key Metrics** (Monitored via Prometheus): - Request rate (per service, per endpoint) - Error rate (4xx, 5xx responses) - Latency percentiles (P50, P95, P99) - Resource utilization (CPU, memory, disk) - Cache hit/miss ratios - Database connection pool saturation - Kafka consumer lag **Alerts** (via PagerDuty): - P1 (Immediate): API error rate > 1%, database connection failure - P2 (< 15 min): P99 latency > 500ms, cache hit rate < 80% - P3 (< 1 hour): Resource utilization > 85%, replica count < minimum **SLI/SLO/SLA**: - SLI: API success rate (non-5xx responses) - SLO: 99.9% uptime per month (43 minutes downtime allowance) - SLA: 99.5% uptime guarantee (customer-facing SLA with credits) --- ## Disaster Recovery ### Recovery Objectives **RPO (Recovery Point Objective): 1 hour** - Maximum acceptable data loss: 1 hour of transactions - Achieved through: Continuous database replication + hourly snapshots - Kafka retention: 7 days allows event replay **RTO (Recovery Time Objective): 4 hours** - Maximum acceptable downtime: 4 hours for full system recovery - Includes: Failover, data verification, and service restoration - Automated runbooks reduce RTO to < 2 hours for common scenarios ### High Availability Architecture **Multi-AZ Deployment**: ``` Region: us-east-1 AZ-1a: AZ-1b: AZ-1c: - API Gateway (4) - API Gateway (4) - API Gateway (4) - Workflow Engine (6) - Workflow Engine (5) - Workflow Engine (5) - Auth Service (3) - Auth Service (3) - Auth",
              "score": null
            },
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_4",
              "content": "--set prometheus.prometheusSpec.retention=30d \\ --set prometheus.prometheusSpec.storageSpec.volumeClaimTemplate.spec.resources.requests.storage=100Gi ``` CloudFlow exposes Prometheus metrics on port 9090 at the `/metrics` endpoint. The metrics include: - **HTTP Metrics**: Request rate, latency, error rate - **Database Metrics**: Connection pool status, query duration - **Worker Metrics**: Job queue size, processing time, failure rate - **System Metrics**: CPU usage, memory usage, heap statistics Example metrics exposed: ``` # HELP cloudflow_http_requests_total Total number of HTTP requests # TYPE cloudflow_http_requests_total counter cloudflow_http_requests_total{method=\"GET\",route=\"/api/workflows\",status=\"200\"} 15420 # HELP cloudflow_http_request_duration_seconds HTTP request duration in seconds # TYPE cloudflow_http_request_duration_seconds histogram cloudflow_http_request_duration_seconds_bucket{method=\"POST\",route=\"/api/workflows\",le=\"0.1\"} 8234 # HELP cloudflow_worker_jobs_processed_total Total number of jobs processed # TYPE cloudflow_worker_jobs_processed_total counter cloudflow_worker_jobs_processed_total{status=\"success\"} 45230 # HELP cloudflow_database_connections Current database connections # TYPE cloudflow_database_connections gauge cloudflow_database_connections{state=\"active\"} 23 ``` ### Grafana Dashboards Access Grafana to view CloudFlow dashboards: ```bash kubectl port-forward -n monitoring svc/prometheus-grafana 3000:80 ``` Import the CloudFlow dashboard (ID: 15847) or use the provided JSON template. Key dashboard panels include: 1. **API Performance**: Request rate, P95/P99 latency, error rate 2. **Resource Usage**: CPU, memory, disk I/O per pod 3. **Database Health**: Connection pool utilization, query performance 4. **Worker Status**: Queue depth, processing rate, job success/failure ratio 5. **System Overview**: Pod status, replica count, autoscaling events ### Logging with CloudWatch CloudFlow logs are automatically shipped to CloudWatch Logs.",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_4",
              "content": "**Implement connection pooling optimization:** ```bash # Use PgBouncer for connection pooling kubectl apply -f cloudflow-pgbouncer.yaml # Configure CloudFlow to use PgBouncer cloudflow config set db.host pgbouncer.cloudflow.svc.cluster.local cloudflow config set db.port 6432 cloudflow config set db.pool.mode transaction ``` 2. **Add read replicas:** ```bash # Route read-only queries to replicas cloudflow db replicas add --count 2 cloudflow config set db.read_replicas \"replica-1:5432,replica-2:5432\" cloudflow config set db.read_write_split true ``` ### Connection Timeout Errors #### Error Messages - `connection timeout after 30000ms` - `could not connect to database server at 10.0.2.45:5432` - `database server unreachable` #### Troubleshooting Steps ```bash # Test network connectivity telnet cloudflow-db.internal.company.com 5432 # Check DNS resolution nslookup cloudflow-db.internal.company.com # Verify database is accepting connections pg_isready -h cloudflow-db.internal.company.com -p 5432 -U cloudflow # Check firewall rules sudo iptables -L -n | grep 5432 # Test from CloudFlow pod network kubectl run -n cloudflow debug-pod --rm -i --tty \\ --image=postgres:14 -- \\ psql -h cloudflow-db.internal.company.com -U cloudflow -d cloudflow_production # Review database logs for connection rejections kubectl logs -n cloudflow statefulset/cloudflow-db --tail=100 | \\ grep -i \"connection\\|reject\\|authentication\" ``` ### Maximum Connection Limit (100) Reached This is a hard limit in CloudFlow's database tier. #### Permanent Solutions **Option 1: Upgrade database tier** ```bash # Check available tiers cloudflow db tiers list # Upgrade to higher tier (supports 200 connections) cloudflow db upgrade --tier standard-plus --confirm # Monitor migration progress cloudflow db migration status ``` **Option 2: Implement aggressive connection reuse** ```bash # Reduce connection lifetime cloudflow config set db.pool.max_lifetime 600 # 10 minutes # Enable prepared statement caching cloudflow config set db.prepared_statements.cache true cloudflow config set db.prepared_statements.max_size 250 # Reduce idle connection timeout cloudflow config set db.pool.idle_timeout 120 # 2 minutes ``` --- ## Workflow Execution Failures ### Timeout Errors (3600 second limit) #### Error Message ``` WorkflowExecutionError: Workflow exceeded maximum execution time of 3600 seconds Status: TIMEOUT Workflow ID: wf_9k2n4m8p1q Execution ID: exec_7h3j6k9m2n ``` #### Analysis ```bash # Get workflow execution details cloudflow workflows executions get exec_7h3j6k9m2n --verbose # Check step-by-step breakdown cloudflow workflows executions steps exec_7h3j6k9m2n # Sample output: # Step 1: data_ingestion - Duration: 245s - Status: SUCCESS # Step 2: data_validation - Duration: 123s - Status: SUCCESS # Step 3: data_transformation - Duration: 3189s - Status: TIMEOUT # Step 4: data_export - Duration: 0s - Status: SKIPPED # Identify bottleneck step cloudflow workflows analyze",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_3",
              "content": "Workflow Context Accumulation** Large workflow executions may accumulate state in memory. **Solution:** ```bash # Configure context cleanup cloudflow config set workflow.context.max_size_mb 100 cloudflow config set workflow.context.cleanup_threshold 0.8 # Enable context persistence to disk cloudflow config set workflow.context.persistence.enabled true cloudflow config set workflow.context.persistence.backend s3 ``` **2. Connection Pool Leaks** **Diagnosis:** ```bash # Check active connections cloudflow db connections --verbose # Expected output: # Active: 45/100 # Idle: 23 # Waiting: 2 # Average age: 245s ``` **Resolution:** ```bash # Adjust connection pool settings cloudflow config set db.pool.max_connections 100 cloudflow config set db.pool.min_connections 10 cloudflow config set db.pool.idle_timeout 300 cloudflow config set db.pool.max_lifetime 1800 # Force connection pool reset cloudflow db pool reset --confirm ``` **3. Event Stream Buffers** Unbounded event buffers can cause memory exhaustion. ```bash # Configure event buffer limits cloudflow config set events.buffer.max_size 10000 cloudflow config set events.buffer.overflow_strategy drop_oldest # Enable event streaming to external sink cloudflow config set events.sink.type kafka cloudflow config set events.sink.kafka.brokers \"kafka-1:9092,kafka-2:9092\" cloudflow config set events.sink.kafka.topic cloudflow-events ``` --- ## Database Connection Issues ### Connection Pool Exhaustion #### Symptoms - Error: `could not obtain connection from pool within 5000ms` - Error: `connection pool exhausted (100/100 connections in use)` - API requests fail with `503 Service Unavailable` - Database CPU usage normal, but connection count at maximum #### Investigation ```bash # Check current connection pool status cloudflow db pool status --detailed # Output example: # Pool Statistics: # Total Connections: 100/100 (100%) # Active: 87 # Idle: 13 # Waiting Requests: 45 # Average Wait Time: 3420ms # Max Wait Time: 8234ms # Identify long-running queries cloudflow db queries --status running --min-duration 30000 # Check connection distribution by client SELECT application_name, state, COUNT(*) as conn_count, AVG(EXTRACT(EPOCH FROM (NOW() - state_change))) as avg_duration_sec FROM pg_stat_activity WHERE datname = 'cloudflow_production' GROUP BY application_name, state ORDER BY conn_count DESC; ``` #### Resolution **Immediate mitigation:** ```bash # Temporarily increase connection limit (requires database restart) cloudflow db config set max_connections 150 # Kill idle connections older than 5 minutes cloudflow db connections kill --idle-timeout 300 # Restart connection pool without downtime kubectl rollout restart deployment/cloudflow-api -n cloudflow kubectl rollout status deployment/cloudflow-api -n cloudflow ``` **Long-term solutions:** 1.",
              "score": null
            }
          ],
          "latency_ms": 1086.6603279973788,
          "query_id": "realistic_008",
          "dimension": "contextual",
          "query": "I need to configure autoscaling, what are the min/max replicas and thresholds?"
        },
        {
          "key_facts": [
            "minReplicas: 3",
            "maxReplicas: 10",
            "targetCPUUtilizationPercentage: 70"
          ],
          "found_facts": [
            "minReplicas: 3",
            "maxReplicas: 10",
            "targetCPUUtilizationPercentage: 70"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "deployment_guide"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_5",
              "content": "Success/failure events published back to Kafka ``` **Event Schema**: ```json { \"event_id\": \"uuid-v4\", \"event_type\": \"workflow.execution.completed\", \"timestamp\": \"2026-01-15T10:30:00.000Z\", \"correlation_id\": \"request-trace-id\", \"payload\": { \"workflow_id\": \"wf-12345\", \"execution_id\": \"exec-67890\", \"status\": \"COMPLETED\", \"duration_ms\": 4230 }, \"metadata\": { \"source_service\": \"workflow-engine\", \"schema_version\": \"1.0\" } } ``` ### Inter-Service Communication Services communicate using two primary patterns: **Synchronous (gRPC)**: - Auth Service validation calls from API Gateway - User Service profile lookups - Low latency requirement (< 50ms) - Request-response pattern with circuit breaker **Asynchronous (Kafka)**: - Workflow execution events - Notification triggers - Audit log events - Eventual consistency acceptable --- ## Database Architecture ### PostgreSQL Primary Database **Cluster Configuration**: - Primary-replica setup with 1 primary + 2 read replicas - Instance type: db.r6g.2xlarge (8 vCPU, 64GB RAM) - Storage: 2TB gp3 SSD with 12,000 IOPS - Multi-AZ deployment for high availability - Automated backups: Daily snapshots, 30-day retention - Point-in-time recovery: 5-minute granularity **Database Schema Design**: ``` Core Tables: - users (5M rows): User accounts and profiles - workflows (2M rows): Workflow definitions - workflow_executions (500M rows, partitioned): Execution history - workflow_steps (2B rows, partitioned): Individual step records - schedules (100K rows): Scheduled workflow triggers - notifications (1B rows, partitioned): Notification delivery log Partitioning Strategy: - workflow_executions: Monthly partitions by created_at - workflow_steps: Monthly partitions by created_at - notifications: Weekly partitions by created_at - Automatic partition creation via cron job - Retention policy: Drop partitions older than 12 months ``` **Indexing Strategy**: - Primary keys: UUIDs with B-tree indexes - Frequently queried columns: Compound indexes (e.g., user_id + status + created_at) - JSONB columns: GIN indexes for workflow definitions - Full-text search: GiST indexes on description fields **Connection Pooling**: - PgBouncer in transaction mode - Pool size: 100 connections per service - Max client connections: 2000 - Connection timeout: 30 seconds **Performance Characteristics**: - Read query P99: < 20ms - Write query P99: < 50ms - Transaction throughput: 15,000 TPS - Replication lag: < 500ms --- ### Redis Caching Layer **Cluster Configuration**: - Redis Cluster with 6 nodes (3 primary + 3 replica) - Instance type: cache.r6g.xlarge (4 vCPU, 26GB RAM) - Total memory: 78GB usable cache space - Persistence: RDB snapshots every 5 minutes + AOF - Eviction policy: allkeys-lru **Cache Usage Patterns**: 1.",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_0",
              "content": "# CloudFlow Platform - System Architecture Overview **Document Version:** 2.3.1 **Last Updated:** January 15, 2026 **Owner:** Platform Architecture Team **Status:** Production ## Executive Summary CloudFlow is a distributed, cloud-native workflow automation platform designed to orchestrate complex business processes at scale. The platform processes over 2.5 million workflow executions daily with an average P99 latency of 180ms for API operations and 4.2 seconds for workflow execution. This document provides a comprehensive overview of the system architecture, including microservices design, data flow patterns, infrastructure decisions, and operational considerations. ## Table of Contents 1. [High-Level Architecture](#high-level-architecture) 2. [Microservices Breakdown](#microservices-breakdown) 3. [Data Flow Architecture](#data-flow-architecture) 4. [Database Architecture](#database-architecture) 5. [Message Queue Patterns](#message-queue-patterns) 6. [Caching Strategy](#caching-strategy) 7. [Security Architecture](#security-architecture) 8. [Performance Characteristics](#performance-characteristics) 9. [Disaster Recovery](#disaster-recovery) --- ## High-Level Architecture CloudFlow follows a microservices architecture pattern deployed across multiple availability zones in AWS. The platform is designed with the following core principles: - **Scalability**: Horizontal scaling for all services with auto-scaling groups - **Resilience**: Circuit breakers, bulkheads, and graceful degradation - **Observability**: Distributed tracing, centralized logging, and comprehensive metrics - **Security**: Zero-trust network architecture with mTLS encryption ### Architecture Diagram (Conceptual) ``` \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 Load Balancer (ALB) \u2502 \u2502 (TLS Termination - 443) \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u25bc \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 API Gateway Layer \u2502 \u2502 (Rate Limiting, Auth, Request Routing) \u2502 \u2514\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u2502 \u2502 \u2502 \u2502 \u25bc \u25bc \u25bc \u25bc \u25bc \u250c\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502Auth\u2502 \u2502Workflow\u2502 \u2502Scheduler\u2502 \u2502Notification\u2502 \u2502User Service\u2502 \u2502Svc \u2502 \u2502 Engine \u2502 \u2502 Service \u2502 \u2502 Service \u2502 \u2502 \u2502 \u2514\u2500\u252c\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u2502 \u2502 \u2502 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 \u2502 \u2502 \u25bc \u25bc \u25bc \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502PostgreSQL\u2502 \u2502 Redis \u2502 \u2502 Kafka \u2502 \u2502 Cluster \u2502 \u2502 Cluster \u2502 \u2502 Cluster \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 ``` ### Technology Stack - **Runtime**: Node.js 20.x (API Gateway, Workflow Engine), Go 1.21 (Auth Service, Scheduler) - **Container Orchestration**: Kubernetes 1.28 (EKS) - **Service Mesh**: Istio 1.20 for service-to-service communication - **Databases**: PostgreSQL 15.4, Redis 7.2 - **Message Broker**: Apache Kafka 3.6 - **Monitoring**: Prometheus, Grafana, Jaeger for distributed tracing - **Secrets Management**: HashiCorp Vault 1.15 --- ## Microservices Breakdown ### API Gateway **Purpose**: Single entry point for all client requests, providing authentication, rate limiting, request routing, and protocol translation.",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_10",
              "content": "- Execution start rate: 500 per second - Completion rate: 450 per second (average 2-second execution time) **Database**: - Read throughput: 50,000 queries per second (across replicas) - Write throughput: 15,000 transactions per second - Connection capacity: 2,000 concurrent connections **Kafka**: - Message ingestion: 100,000 messages per second - Consumer throughput: 80,000 messages per second (aggregated) - End-to-end latency: < 100ms (P99) ### Resource Utilization **CPU Utilization** (Target: 60-70% average): - API Gateway: 55% average, 80% peak - Workflow Engine: 65% average, 85% peak - Auth Service: 40% average, 60% peak **Memory Utilization** (Target: < 80%): - API Gateway: 2.5GB average per pod (4GB allocated) - Workflow Engine: 6GB average per pod (8GB allocated) - Notification Service: 2.8GB average per pod (4GB allocated) **Network Throughput**: - Ingress: 2 Gbps average, 5 Gbps peak - Egress: 1.5 Gbps average, 4 Gbps peak - Internal (service mesh): 8 Gbps average ### Monitoring & Alerting **Key Metrics** (Monitored via Prometheus): - Request rate (per service, per endpoint) - Error rate (4xx, 5xx responses) - Latency percentiles (P50, P95, P99) - Resource utilization (CPU, memory, disk) - Cache hit/miss ratios - Database connection pool saturation - Kafka consumer lag **Alerts** (via PagerDuty): - P1 (Immediate): API error rate > 1%, database connection failure - P2 (< 15 min): P99 latency > 500ms, cache hit rate < 80% - P3 (< 1 hour): Resource utilization > 85%, replica count < minimum **SLI/SLO/SLA**: - SLI: API success rate (non-5xx responses) - SLO: 99.9% uptime per month (43 minutes downtime allowance) - SLA: 99.5% uptime guarantee (customer-facing SLA with credits) --- ## Disaster Recovery ### Recovery Objectives **RPO (Recovery Point Objective): 1 hour** - Maximum acceptable data loss: 1 hour of transactions - Achieved through: Continuous database replication + hourly snapshots - Kafka retention: 7 days allows event replay **RTO (Recovery Time Objective): 4 hours** - Maximum acceptable downtime: 4 hours for full system recovery - Includes: Failover, data verification, and service restoration - Automated runbooks reduce RTO to < 2 hours for common scenarios ### High Availability Architecture **Multi-AZ Deployment**: ``` Region: us-east-1 AZ-1a: AZ-1b: AZ-1c: - API Gateway (4) - API Gateway (4) - API Gateway (4) - Workflow Engine (6) - Workflow Engine (5) - Workflow Engine (5) - Auth Service (3) - Auth Service (3) - Auth",
              "score": null
            },
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_1",
              "content": "create cluster -f cluster-config.yaml ``` ### Storage Configuration Install the EBS CSI driver for persistent volumes: ```bash eksctl create addon --name aws-ebs-csi-driver \\ --cluster cloudflow-production \\ --service-account-role-arn arn:aws:iam::ACCOUNT_ID:role/AmazonEKS_EBS_CSI_DriverRole \\ --force ``` --- ## Kubernetes Deployment ### Namespace Setup Create the CloudFlow namespace and configure resource quotas: ```yaml # namespace.yaml apiVersion: v1 kind: Namespace metadata: name: cloudflow-prod labels: name: cloudflow-prod environment: production --- apiVersion: v1 kind: ResourceQuota metadata: name: cloudflow-quota namespace: cloudflow-prod spec: hard: requests.cpu: \"50\" requests.memory: 100Gi persistentvolumeclaims: \"10\" services.loadbalancers: \"2\" ``` Apply the namespace configuration: ```bash kubectl apply -f namespace.yaml ``` ### Environment Configuration CloudFlow requires the following environment variables: | Variable | Description | Example | Required | |----------|-------------|---------|----------| | `DATABASE_URL` | PostgreSQL connection string | `postgresql://user:pass@host:5432/cloudflow` | Yes | | `REDIS_URL` | Redis connection string | `redis://redis-master.cloudflow-prod.svc.cluster.local:6379` | Yes | | `JWT_SECRET` | Secret key for JWT token signing | `<generated-secret-256-bit>` | Yes | | `LOG_LEVEL` | Application log verbosity | `info`, `debug`, `warn`, `error` | No (default: `info`) | | `API_PORT` | API server port | `3000` | No (default: `3000`) | | `WORKER_CONCURRENCY` | Number of concurrent workers | `10` | No (default: `5`) | | `SESSION_SECRET` | Session encryption key | `<generated-secret-256-bit>` | Yes | | `AWS_REGION` | AWS region for services | `us-east-1` | Yes | | `S3_BUCKET` | S3 bucket for file storage | `cloudflow-prod-storage` | Yes | | `SMTP_HOST` | Email server hostname | `smtp.sendgrid.net` | No | | `SMTP_PORT` | Email server port | `587` | No | | `METRICS_ENABLED` | Enable Prometheus metrics | `true` | No (default: `false`) | Create a Kubernetes secret for sensitive variables: ```bash kubectl create secret generic cloudflow-secrets \\ --namespace cloudflow-prod \\ --from-literal=DATABASE_URL=\"postgresql://cloudflow:$(cat db-password.txt)@pgbouncer.cloudflow-prod.svc.cluster.local:5432/cloudflow\" \\ --from-literal=REDIS_URL=\"redis://redis-master.cloudflow-prod.svc.cluster.local:6379\" \\ --from-literal=JWT_SECRET=\"$(openssl rand -base64 32)\" \\ --from-literal=SESSION_SECRET=\"$(openssl rand -base64 32)\" ``` ### Helm Chart Deployment Add the CloudFlow Helm repository: ```bash helm repo add cloudflow https://charts.cloudflow.io helm repo update ``` Create a values file for production configuration: ```yaml # values-production.yaml replicaCount: 3 image: repository: 123456789012.dkr.ecr.us-east-1.amazonaws.com/cloudflow tag: \"2.4.0\" pullPolicy: IfNotPresent resources: limits: cpu: 2000m memory: 4Gi requests: cpu: 2000m memory: 4Gi autoscaling: enabled: true minReplicas: 3 maxReplicas: 10 targetCPUUtilizationPercentage: 70 targetMemoryUtilizationPercentage: 80 service: type: ClusterIP port: 80 targetPort: 3000 annotations: service.beta.kubernetes.io/aws-load-balancer-type: \"nlb\" ingress: enabled: true className: nginx annotations: cert-manager.io/cluster-issuer: \"letsencrypt-prod\" nginx.ingress.kubernetes.io/ssl-redirect: \"true\" hosts: - host: api.cloudflow.io paths: - path: / pathType: Prefix tls:",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_11",
              "content": "Service (2) - PostgreSQL Primary - PostgreSQL Replica - PostgreSQL Replica - Redis Primary (2) - Redis Replica (2) - Redis Replica (2) - Kafka Broker (2) - Kafka Broker (2) - Kafka Broker (1) ``` **Automatic Failover**: - Database: 30-60 seconds (automatic promotion of replica) - Redis: < 10 seconds (Sentinel-based failover) - Kafka: < 30 seconds (controller election) - Services: Kubernetes health checks with 10-second liveness probes ### Backup Strategy **Database Backups**: - Automated snapshots: Daily at 02:00 UTC - Retention: 30 days for daily, 90 days for monthly - Cross-region replication: Async replication to us-west-2 (15-minute lag) - Backup verification: Weekly automated restore test in staging environment **Backup Schedule**: ``` Daily: Full snapshot \u2192 S3 (encrypted) Hourly: WAL archives \u2192 S3 (point-in-time recovery) Weekly: Backup validation test Monthly: Long-term archive to Glacier ``` **Configuration Backups**: - Kubernetes manifests: Stored in Git (GitOps with ArgoCD) - Vault secrets: Automated snapshot every 6 hours - Kafka topic configurations: Exported daily to S3 ### Disaster Recovery Procedures **Scenario 1: Single AZ Failure** - Detection: Health checks fail for entire AZ (< 30 seconds) - Action: Traffic automatically routed to healthy AZs by ALB - Recovery time: < 5 minutes (no manual intervention) - Data loss: None (multi-AZ replication) **Scenario 2: Database Primary Failure** - Detection: Health check fails for primary database (< 30 seconds) - Action: Automatic promotion of read replica to primary - Recovery time: 30-60 seconds - Data loss: Minimal (< 1 second due to synchronous replication) **Scenario 3: Full Region Failure** - Detection: Multiple health check failures across all AZs (< 2 minutes) - Action: Manual failover to DR region (us-west-2) - Procedure: 1. Update DNS to point to DR region (TTL: 60 seconds) 2. Promote DR database replica to primary 3. Scale up DR region services to production capacity 4. Verify data consistency and integrity 5. Update monitoring dashboards - Recovery time: 2-4 hours (includes verification) - Data loss: < 1 hour (last cross-region replication) **Scenario 4: Data Corruption** - Detection: Data validation checks or user report - Action: Point-in-time recovery from WAL archives - Procedure: 1. Identify corruption time window 2. Restore from snapshot prior to corruption 3. Replay WAL logs up to corruption point 4. Verify data integrity 5.",
              "score": null
            }
          ],
          "latency_ms": 1244.199963999563,
          "query_id": "realistic_008",
          "dimension": "negation",
          "query": "Why do we have 3 replicas even with low traffic?"
        },
        {
          "key_facts": [
            "P99 latency: < 200ms",
            "average P99 latency of 180ms for API operations"
          ],
          "found_facts": [
            "P99 latency: < 200ms",
            "average P99 latency of 180ms for API operations"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "architecture_overview"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_6",
              "content": "**Verify Service Health**: ```bash kubectl get pods -n cloudflow-prod curl https://api.cloudflow.io/health ``` **Recovery Time Objective (RTO)**: 4 hours **Recovery Point Objective (RPO)**: 24 hours --- ## Scaling and Performance ### Horizontal Pod Autoscaling CloudFlow is configured with Horizontal Pod Autoscaler (HPA) to automatically scale based on resource utilization: ```yaml # hpa.yaml apiVersion: autoscaling/v2 kind: HorizontalPodAutoscaler metadata: name: cloudflow-api-hpa namespace: cloudflow-prod spec: scaleTargetRef: apiVersion: apps/v1 kind: Deployment name: cloudflow-api minReplicas: 3 maxReplicas: 10 metrics: - type: Resource resource: name: cpu target: type: Utilization averageUtilization: 70 - type: Resource resource: name: memory target: type: Utilization averageUtilization: 80 behavior: scaleDown: stabilizationWindowSeconds: 300 policies: - type: Percent value: 50 periodSeconds: 60 scaleUp: stabilizationWindowSeconds: 60 policies: - type: Percent value: 100 periodSeconds: 30 - type: Pods value: 2 periodSeconds: 30 selectPolicy: Max ``` Monitor autoscaling events: ```bash kubectl get hpa -n cloudflow-prod -w kubectl describe hpa cloudflow-api-hpa -n cloudflow-prod ``` ### Performance Optimization #### Connection Pooling Ensure optimal database connection pooling settings in PgBouncer: - **Pool Mode**: Transaction (optimal for microservices) - **Default Pool Size**: 25 connections per user - **Max DB Connections**: 100 (matches PostgreSQL `max_connections`) #### Redis Caching Implement Redis caching for frequently accessed data: ```javascript // Example caching strategy const CACHE_TTL = { workflows: 300, // 5 minutes userSessions: 3600, // 1 hour apiResults: 60 // 1 minute }; ``` #### Load Testing Perform regular load testing to validate scaling configuration: ```bash # Install k6 for load testing kubectl apply -f - <<EOF apiVersion: v1 kind: ConfigMap metadata: name: loadtest-script namespace: cloudflow-prod data: script.js: | import http from 'k6/http'; import { check, sleep } from 'k6'; export let options = { stages: [ { duration: '2m', target: 100 }, { duration: '5m', target: 100 }, { duration: '2m', target: 200 }, { duration: '5m', target: 200 }, { duration: '2m', target: 0 }, ], thresholds: { http_req_duration: ['p(95)<500'], }, }; export default function () { let res = http.get('https://api.cloudflow.io/api/workflows'); check(res, { 'status is 200': (r) => r.status === 200 }); sleep(1); } EOF ``` **Performance Targets**: - P95 API latency: < 500ms - P99 API latency: < 1000ms - Error rate: < 0.1% - Throughput: > 1000 requests/second --- ## Security and Compliance ### Network Policies Implement network policies to restrict pod-to-pod communication: ```yaml # network-policy.yaml apiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: cloudflow-network-policy namespace: cloudflow-prod spec:",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_10",
              "content": "- Execution start rate: 500 per second - Completion rate: 450 per second (average 2-second execution time) **Database**: - Read throughput: 50,000 queries per second (across replicas) - Write throughput: 15,000 transactions per second - Connection capacity: 2,000 concurrent connections **Kafka**: - Message ingestion: 100,000 messages per second - Consumer throughput: 80,000 messages per second (aggregated) - End-to-end latency: < 100ms (P99) ### Resource Utilization **CPU Utilization** (Target: 60-70% average): - API Gateway: 55% average, 80% peak - Workflow Engine: 65% average, 85% peak - Auth Service: 40% average, 60% peak **Memory Utilization** (Target: < 80%): - API Gateway: 2.5GB average per pod (4GB allocated) - Workflow Engine: 6GB average per pod (8GB allocated) - Notification Service: 2.8GB average per pod (4GB allocated) **Network Throughput**: - Ingress: 2 Gbps average, 5 Gbps peak - Egress: 1.5 Gbps average, 4 Gbps peak - Internal (service mesh): 8 Gbps average ### Monitoring & Alerting **Key Metrics** (Monitored via Prometheus): - Request rate (per service, per endpoint) - Error rate (4xx, 5xx responses) - Latency percentiles (P50, P95, P99) - Resource utilization (CPU, memory, disk) - Cache hit/miss ratios - Database connection pool saturation - Kafka consumer lag **Alerts** (via PagerDuty): - P1 (Immediate): API error rate > 1%, database connection failure - P2 (< 15 min): P99 latency > 500ms, cache hit rate < 80% - P3 (< 1 hour): Resource utilization > 85%, replica count < minimum **SLI/SLO/SLA**: - SLI: API success rate (non-5xx responses) - SLO: 99.9% uptime per month (43 minutes downtime allowance) - SLA: 99.5% uptime guarantee (customer-facing SLA with credits) --- ## Disaster Recovery ### Recovery Objectives **RPO (Recovery Point Objective): 1 hour** - Maximum acceptable data loss: 1 hour of transactions - Achieved through: Continuous database replication + hourly snapshots - Kafka retention: 7 days allows event replay **RTO (Recovery Time Objective): 4 hours** - Maximum acceptable downtime: 4 hours for full system recovery - Includes: Failover, data verification, and service restoration - Automated runbooks reduce RTO to < 2 hours for common scenarios ### High Availability Architecture **Multi-AZ Deployment**: ``` Region: us-east-1 AZ-1a: AZ-1b: AZ-1c: - API Gateway (4) - API Gateway (4) - API Gateway (4) - Workflow Engine (6) - Workflow Engine (5) - Workflow Engine (5) - Auth Service (3) - Auth Service (3) - Auth",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_9",
              "content": "per API key (default: 1000 RPM) ### Secrets Management **HashiCorp Vault Integration**: - Dynamic database credentials: Generated on-demand, 1-hour TTL - Encryption keys: Transit secrets engine for encryption-as-a-service - API keys for external services: Stored with versioning - Rotation policy: Automated rotation every 90 days with notification **Secret Access Pattern**: ``` Service starts \u2192 Vault authentication (Kubernetes service account) \u2502 \u25bc Request secret lease (1-hour TTL) \u2502 \u25bc Vault returns dynamic credentials \u2502 \u25bc Service connects to PostgreSQL \u2502 Renew lease every 30 minutes ``` ### Data Protection **Encryption at Rest**: - PostgreSQL: AES-256 encryption enabled at volume level - Redis: Encryption enabled using AWS KMS - Kafka: Encryption at rest for all data on brokers - S3 backups: Server-side encryption with KMS (SSE-KMS) **Encryption in Transit**: - External traffic: TLS 1.3 only (TLS 1.2 deprecated) - Internal traffic: mTLS via Istio service mesh - Database connections: SSL/TLS enforced (sslmode=require) **Data Masking**: - Sensitive fields (email, phone) masked in logs - PII redacted from error messages and stack traces - Audit logs: Full data retention with access controls **Compliance**: - GDPR: User data export and deletion workflows - SOC 2 Type II: Audit logging for all data access - HIPAA: PHI isolation in dedicated tenants with enhanced encryption --- ## Performance Characteristics ### Latency Targets **API Operations** (P99 latency): - `GET /workflows/{id}`: < 50ms (cache hit), < 150ms (cache miss) - `POST /workflows`: < 200ms (includes validation and database write) - `POST /workflows/{id}/execute`: < 100ms (async, returns execution ID) - `GET /workflows/{id}/history`: < 300ms (paginated, 50 records per page) **Workflow Execution** (P99 latency): - Simple workflow (< 5 steps): < 2 seconds - Medium workflow (5-15 steps): < 5 seconds - Complex workflow (> 15 steps): < 15 seconds - Parallel execution: Linear scaling up to 10 concurrent branches **Database Operations** (P99 latency): - Simple SELECT: < 5ms - JOIN query: < 20ms - INSERT/UPDATE: < 10ms - Batch operations: < 100ms (batch size: 100 records) **Cache Operations** (P99 latency): - Redis GET: < 2ms - Redis SET: < 3ms - Redis DEL: < 2ms ### Throughput Capacity **API Gateway**: - Sustained: 10,000 requests per second - Peak: 25,000 requests per second (5-minute burst) - Rate limiting: 1,000 requests per minute per API key **Workflow Engine**: - Concurrent executions: 8,000 workflows (across 16 pods)",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_0",
              "content": "# CloudFlow Platform - System Architecture Overview **Document Version:** 2.3.1 **Last Updated:** January 15, 2026 **Owner:** Platform Architecture Team **Status:** Production ## Executive Summary CloudFlow is a distributed, cloud-native workflow automation platform designed to orchestrate complex business processes at scale. The platform processes over 2.5 million workflow executions daily with an average P99 latency of 180ms for API operations and 4.2 seconds for workflow execution. This document provides a comprehensive overview of the system architecture, including microservices design, data flow patterns, infrastructure decisions, and operational considerations. ## Table of Contents 1. [High-Level Architecture](#high-level-architecture) 2. [Microservices Breakdown](#microservices-breakdown) 3. [Data Flow Architecture](#data-flow-architecture) 4. [Database Architecture](#database-architecture) 5. [Message Queue Patterns](#message-queue-patterns) 6. [Caching Strategy](#caching-strategy) 7. [Security Architecture](#security-architecture) 8. [Performance Characteristics](#performance-characteristics) 9. [Disaster Recovery](#disaster-recovery) --- ## High-Level Architecture CloudFlow follows a microservices architecture pattern deployed across multiple availability zones in AWS. The platform is designed with the following core principles: - **Scalability**: Horizontal scaling for all services with auto-scaling groups - **Resilience**: Circuit breakers, bulkheads, and graceful degradation - **Observability**: Distributed tracing, centralized logging, and comprehensive metrics - **Security**: Zero-trust network architecture with mTLS encryption ### Architecture Diagram (Conceptual) ``` \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 Load Balancer (ALB) \u2502 \u2502 (TLS Termination - 443) \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u25bc \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 API Gateway Layer \u2502 \u2502 (Rate Limiting, Auth, Request Routing) \u2502 \u2514\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u2502 \u2502 \u2502 \u2502 \u25bc \u25bc \u25bc \u25bc \u25bc \u250c\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502Auth\u2502 \u2502Workflow\u2502 \u2502Scheduler\u2502 \u2502Notification\u2502 \u2502User Service\u2502 \u2502Svc \u2502 \u2502 Engine \u2502 \u2502 Service \u2502 \u2502 Service \u2502 \u2502 \u2502 \u2514\u2500\u252c\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u2502 \u2502 \u2502 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 \u2502 \u2502 \u25bc \u25bc \u25bc \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502PostgreSQL\u2502 \u2502 Redis \u2502 \u2502 Kafka \u2502 \u2502 Cluster \u2502 \u2502 Cluster \u2502 \u2502 Cluster \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 ``` ### Technology Stack - **Runtime**: Node.js 20.x (API Gateway, Workflow Engine), Go 1.21 (Auth Service, Scheduler) - **Container Orchestration**: Kubernetes 1.28 (EKS) - **Service Mesh**: Istio 1.20 for service-to-service communication - **Databases**: PostgreSQL 15.4, Redis 7.2 - **Message Broker**: Apache Kafka 3.6 - **Monitoring**: Prometheus, Grafana, Jaeger for distributed tracing - **Secrets Management**: HashiCorp Vault 1.15 --- ## Microservices Breakdown ### API Gateway **Purpose**: Single entry point for all client requests, providing authentication, rate limiting, request routing, and protocol translation.",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_1",
              "content": "**Technology**: Node.js with Express.js framework **Replicas**: 12 pods (production), auto-scaling 8-20 based on CPU **Resource Allocation**: 2 vCPU, 4GB RAM per pod **Key Responsibilities**: - JWT token validation (delegated to Auth Service for initial validation) - Rate limiting: 1000 requests per minute per API key (sliding window) - Request/response transformation and validation using JSON Schema - Routing to downstream services based on URL path patterns - CORS handling for web clients - Request/response logging and correlation ID injection **Critical Endpoints**: - `POST /api/v1/workflows` - Create new workflow - `GET /api/v1/workflows/:id` - Retrieve workflow status - `POST /api/v1/workflows/:id/execute` - Trigger workflow execution - `GET /api/v1/workflows/:id/history` - Get execution history **Dependencies**: - Auth Service (for token validation) - Redis (for rate limiting counters) - All downstream microservices **Performance Targets**: - P50 latency: < 50ms - P99 latency: < 200ms - Throughput: 10,000 RPS sustained --- ### Auth Service **Purpose**: Centralized authentication and authorization service handling user identity, token generation, and permission validation. **Technology**: Go with gRPC for internal communication, REST for external **Replicas**: 8 pods (production), auto-scaling 6-12 **Resource Allocation**: 1 vCPU, 2GB RAM per pod **Key Responsibilities**: - User authentication via multiple providers (OAuth2, SAML, local credentials) - JWT token generation and validation (RS256 algorithm) - Role-based access control (RBAC) with fine-grained permissions - Session management with Redis-backed storage - API key generation and validation for service accounts - MFA enforcement for administrative operations **Authentication Flow**: ``` Client Request \u2192 API Gateway \u2192 Auth Service \u2502 \u251c\u2500 Validate credentials \u251c\u2500 Check MFA if required \u251c\u2500 Generate JWT (15min expiry) \u251c\u2500 Generate refresh token (7 days) \u2514\u2500 Store session in Redis ``` **Token Structure**: - Access Token: JWT with 15-minute expiry - Refresh Token: Opaque token with 7-day expiry, stored in PostgreSQL - Claims: user_id, email, roles[], permissions[], tenant_id **Security Features**: - Password hashing: Argon2id with 64MB memory, 4 iterations - Token rotation on refresh to prevent replay attacks - Brute force protection: 5 failed attempts \u2192 15-minute lockout - Secrets stored in HashiCorp Vault, rotated every 90 days **Performance Targets**: - Token validation: < 10ms (P99) - Token generation: < 50ms (P99) - Throughput: 5,000 RPS for validation operations --- ### Workflow Engine **Purpose**: Core orchestration service that executes workflow definitions, manages state transitions, and coordinates task execution across distributed systems.",
              "score": null
            }
          ],
          "latency_ms": 1144.8850919987308,
          "query_id": "realistic_009",
          "dimension": "original",
          "query": "What is the P99 latency target for API operations?"
        },
        {
          "key_facts": [
            "P99 latency: < 200ms",
            "average P99 latency of 180ms for API operations"
          ],
          "found_facts": [],
          "missed_facts": [
            "P99 latency: < 200ms",
            "average P99 latency of 180ms for API operations"
          ],
          "coverage": 0.0,
          "expected_docs": [
            "architecture_overview"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_6",
              "content": "**Verify Service Health**: ```bash kubectl get pods -n cloudflow-prod curl https://api.cloudflow.io/health ``` **Recovery Time Objective (RTO)**: 4 hours **Recovery Point Objective (RPO)**: 24 hours --- ## Scaling and Performance ### Horizontal Pod Autoscaling CloudFlow is configured with Horizontal Pod Autoscaler (HPA) to automatically scale based on resource utilization: ```yaml # hpa.yaml apiVersion: autoscaling/v2 kind: HorizontalPodAutoscaler metadata: name: cloudflow-api-hpa namespace: cloudflow-prod spec: scaleTargetRef: apiVersion: apps/v1 kind: Deployment name: cloudflow-api minReplicas: 3 maxReplicas: 10 metrics: - type: Resource resource: name: cpu target: type: Utilization averageUtilization: 70 - type: Resource resource: name: memory target: type: Utilization averageUtilization: 80 behavior: scaleDown: stabilizationWindowSeconds: 300 policies: - type: Percent value: 50 periodSeconds: 60 scaleUp: stabilizationWindowSeconds: 60 policies: - type: Percent value: 100 periodSeconds: 30 - type: Pods value: 2 periodSeconds: 30 selectPolicy: Max ``` Monitor autoscaling events: ```bash kubectl get hpa -n cloudflow-prod -w kubectl describe hpa cloudflow-api-hpa -n cloudflow-prod ``` ### Performance Optimization #### Connection Pooling Ensure optimal database connection pooling settings in PgBouncer: - **Pool Mode**: Transaction (optimal for microservices) - **Default Pool Size**: 25 connections per user - **Max DB Connections**: 100 (matches PostgreSQL `max_connections`) #### Redis Caching Implement Redis caching for frequently accessed data: ```javascript // Example caching strategy const CACHE_TTL = { workflows: 300, // 5 minutes userSessions: 3600, // 1 hour apiResults: 60 // 1 minute }; ``` #### Load Testing Perform regular load testing to validate scaling configuration: ```bash # Install k6 for load testing kubectl apply -f - <<EOF apiVersion: v1 kind: ConfigMap metadata: name: loadtest-script namespace: cloudflow-prod data: script.js: | import http from 'k6/http'; import { check, sleep } from 'k6'; export let options = { stages: [ { duration: '2m', target: 100 }, { duration: '5m', target: 100 }, { duration: '2m', target: 200 }, { duration: '5m', target: 200 }, { duration: '2m', target: 0 }, ], thresholds: { http_req_duration: ['p(95)<500'], }, }; export default function () { let res = http.get('https://api.cloudflow.io/api/workflows'); check(res, { 'status is 200': (r) => r.status === 200 }); sleep(1); } EOF ``` **Performance Targets**: - P95 API latency: < 500ms - P99 API latency: < 1000ms - Error rate: < 0.1% - Throughput: > 1000 requests/second --- ## Security and Compliance ### Network Policies Implement network policies to restrict pod-to-pod communication: ```yaml # network-policy.yaml apiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: cloudflow-network-policy namespace: cloudflow-prod spec:",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_10",
              "content": "- Execution start rate: 500 per second - Completion rate: 450 per second (average 2-second execution time) **Database**: - Read throughput: 50,000 queries per second (across replicas) - Write throughput: 15,000 transactions per second - Connection capacity: 2,000 concurrent connections **Kafka**: - Message ingestion: 100,000 messages per second - Consumer throughput: 80,000 messages per second (aggregated) - End-to-end latency: < 100ms (P99) ### Resource Utilization **CPU Utilization** (Target: 60-70% average): - API Gateway: 55% average, 80% peak - Workflow Engine: 65% average, 85% peak - Auth Service: 40% average, 60% peak **Memory Utilization** (Target: < 80%): - API Gateway: 2.5GB average per pod (4GB allocated) - Workflow Engine: 6GB average per pod (8GB allocated) - Notification Service: 2.8GB average per pod (4GB allocated) **Network Throughput**: - Ingress: 2 Gbps average, 5 Gbps peak - Egress: 1.5 Gbps average, 4 Gbps peak - Internal (service mesh): 8 Gbps average ### Monitoring & Alerting **Key Metrics** (Monitored via Prometheus): - Request rate (per service, per endpoint) - Error rate (4xx, 5xx responses) - Latency percentiles (P50, P95, P99) - Resource utilization (CPU, memory, disk) - Cache hit/miss ratios - Database connection pool saturation - Kafka consumer lag **Alerts** (via PagerDuty): - P1 (Immediate): API error rate > 1%, database connection failure - P2 (< 15 min): P99 latency > 500ms, cache hit rate < 80% - P3 (< 1 hour): Resource utilization > 85%, replica count < minimum **SLI/SLO/SLA**: - SLI: API success rate (non-5xx responses) - SLO: 99.9% uptime per month (43 minutes downtime allowance) - SLA: 99.5% uptime guarantee (customer-facing SLA with credits) --- ## Disaster Recovery ### Recovery Objectives **RPO (Recovery Point Objective): 1 hour** - Maximum acceptable data loss: 1 hour of transactions - Achieved through: Continuous database replication + hourly snapshots - Kafka retention: 7 days allows event replay **RTO (Recovery Time Objective): 4 hours** - Maximum acceptable downtime: 4 hours for full system recovery - Includes: Failover, data verification, and service restoration - Automated runbooks reduce RTO to < 2 hours for common scenarios ### High Availability Architecture **Multi-AZ Deployment**: ``` Region: us-east-1 AZ-1a: AZ-1b: AZ-1c: - API Gateway (4) - API Gateway (4) - API Gateway (4) - Workflow Engine (6) - Workflow Engine (5) - Workflow Engine (5) - Auth Service (3) - Auth Service (3) - Auth",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_9",
              "content": "per API key (default: 1000 RPM) ### Secrets Management **HashiCorp Vault Integration**: - Dynamic database credentials: Generated on-demand, 1-hour TTL - Encryption keys: Transit secrets engine for encryption-as-a-service - API keys for external services: Stored with versioning - Rotation policy: Automated rotation every 90 days with notification **Secret Access Pattern**: ``` Service starts \u2192 Vault authentication (Kubernetes service account) \u2502 \u25bc Request secret lease (1-hour TTL) \u2502 \u25bc Vault returns dynamic credentials \u2502 \u25bc Service connects to PostgreSQL \u2502 Renew lease every 30 minutes ``` ### Data Protection **Encryption at Rest**: - PostgreSQL: AES-256 encryption enabled at volume level - Redis: Encryption enabled using AWS KMS - Kafka: Encryption at rest for all data on brokers - S3 backups: Server-side encryption with KMS (SSE-KMS) **Encryption in Transit**: - External traffic: TLS 1.3 only (TLS 1.2 deprecated) - Internal traffic: mTLS via Istio service mesh - Database connections: SSL/TLS enforced (sslmode=require) **Data Masking**: - Sensitive fields (email, phone) masked in logs - PII redacted from error messages and stack traces - Audit logs: Full data retention with access controls **Compliance**: - GDPR: User data export and deletion workflows - SOC 2 Type II: Audit logging for all data access - HIPAA: PHI isolation in dedicated tenants with enhanced encryption --- ## Performance Characteristics ### Latency Targets **API Operations** (P99 latency): - `GET /workflows/{id}`: < 50ms (cache hit), < 150ms (cache miss) - `POST /workflows`: < 200ms (includes validation and database write) - `POST /workflows/{id}/execute`: < 100ms (async, returns execution ID) - `GET /workflows/{id}/history`: < 300ms (paginated, 50 records per page) **Workflow Execution** (P99 latency): - Simple workflow (< 5 steps): < 2 seconds - Medium workflow (5-15 steps): < 5 seconds - Complex workflow (> 15 steps): < 15 seconds - Parallel execution: Linear scaling up to 10 concurrent branches **Database Operations** (P99 latency): - Simple SELECT: < 5ms - JOIN query: < 20ms - INSERT/UPDATE: < 10ms - Batch operations: < 100ms (batch size: 100 records) **Cache Operations** (P99 latency): - Redis GET: < 2ms - Redis SET: < 3ms - Redis DEL: < 2ms ### Throughput Capacity **API Gateway**: - Sustained: 10,000 requests per second - Peak: 25,000 requests per second (5-minute burst) - Rate limiting: 1,000 requests per minute per API key **Workflow Engine**: - Concurrent executions: 8,000 workflows (across 16 pods)",
              "score": null
            },
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_3",
              "content": "**Endpoint:** `GET /pipelines/{pipeline_id}/executions` **Query Parameters:** - `status` (optional): Filter by status (`running`, `completed`, `failed`) - `start_date` (optional): ISO 8601 timestamp - `end_date` (optional): ISO 8601 timestamp - `limit` (optional): Items per page (max 100) - `offset` (optional): Pagination offset **Example Request:** ```python import requests api_key = \"YOUR_API_KEY\" pipeline_id = \"pipe_4x9k2m\" url = f\"https://api.cloudflow.io/v2/pipelines/{pipeline_id}/executions\" headers = {\"X-API-Key\": api_key} params = { \"status\": \"completed\", \"start_date\": \"2026-01-20T00:00:00Z\", \"limit\": 50 } response = requests.get(url, headers=headers, params=params) executions = response.json() for execution in executions['data']: print(f\"Execution {execution['id']}: {execution['status']} - {execution['duration_ms']}ms\") ``` **Example Response (200 OK):** ```json { \"data\": [ { \"id\": \"exec_7m3k9x2p\", \"pipeline_id\": \"pipe_4x9k2m\", \"status\": \"completed\", \"started_at\": \"2026-01-23T15:00:00Z\", \"completed_at\": \"2026-01-23T15:03:42Z\", \"duration_ms\": 222000, \"records_processed\": 15420, \"records_failed\": 3, \"error_rate\": 0.02 } ], \"pagination\": { \"total\": 156, \"limit\": 50, \"offset\": 0, \"has_more\": true } } ``` ### Analytics #### Get Workflow Metrics Retrieve performance metrics and analytics for workflows. **Endpoint:** `GET /analytics/workflows/{workflow_id}` **Query Parameters:** - `period` (required): Time period (`1h`, `24h`, `7d`, `30d`) - `metrics` (optional): Comma-separated list of metrics **Available Metrics:** - `execution_count`: Total number of executions - `success_rate`: Percentage of successful executions - `avg_duration`: Average execution duration in milliseconds - `error_count`: Total number of errors - `throughput`: Records processed per hour **Example Request:** ```bash curl -H \"X-API-Key: YOUR_API_KEY\" \\ \"https://api.cloudflow.io/v2/analytics/workflows/wf_8x7k2m9p?period=7d&metrics=execution_count,success_rate,avg_duration\" ``` **Example Response (200 OK):** ```json { \"workflow_id\": \"wf_8x7k2m9p\", \"period\": \"7d\", \"start_date\": \"2026-01-17T10:30:00Z\", \"end_date\": \"2026-01-24T10:30:00Z\", \"metrics\": { \"execution_count\": 168, \"success_rate\": 97.6, \"avg_duration\": 3285, \"error_count\": 4, \"throughput\": 2847 }, \"timeseries\": [ { \"timestamp\": \"2026-01-17T00:00:00Z\", \"execution_count\": 24, \"success_rate\": 100.0 }, { \"timestamp\": \"2026-01-18T00:00:00Z\", \"execution_count\": 24, \"success_rate\": 95.8 } ] } ``` ## Error Handling CloudFlow uses standard HTTP status codes and returns detailed error information in JSON format.",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_4",
              "content": "**Technology**: Node.js with worker pool pattern **Replicas**: 8 pods (production), auto-scaling 6-16 **Resource Allocation**: 2 vCPU, 4GB RAM per pod **Key Responsibilities**: - Consume notification events from Kafka - Template rendering with Handlebars (cached in Redis) - Multi-channel delivery (Email via SendGrid, SMS via Twilio, Webhooks) - Retry logic with dead letter queue for failed deliveries - Delivery status tracking and analytics - User preference management (opt-in/opt-out) **Notification Flow**: ``` Event Source \u2192 Kafka (notifications.email topic) \u2502 \u25bc Notification Service Consumer \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u25bc \u25bc \u25bc [Email Queue] [SMS Queue] [Webhook Queue] \u2502 \u2502 \u2502 \u25bc \u25bc \u25bc [SendGrid] [Twilio] [HTTP POST] \u2502 \u2502 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u25bc Delivery Status \u2192 PostgreSQL ``` **Channel Configuration**: - Email: SendGrid with IP warming, DKIM/SPF configured - SMS: Twilio with fallback to AWS SNS - Webhook: Signed payloads (HMAC-SHA256), retry 3 times with exponential backoff - In-app: WebSocket connections via Socket.io **Template Management**: - Templates stored in PostgreSQL with versioning - Compiled templates cached in Redis (1-hour TTL) - Support for localization (i18n) with 12 languages - A/B testing capability for email campaigns **Performance Targets**: - Email delivery: < 5 seconds (P99) from event to SendGrid - SMS delivery: < 3 seconds (P99) - Throughput: 10,000 notifications per minute --- ## Data Flow Architecture ### Synchronous Request Flow Client requests follow a synchronous path through the API Gateway to backend services: ``` 1. Client \u2192 Load Balancer (TLS termination) 2. Load Balancer \u2192 API Gateway (HTTP/2) 3. API Gateway \u2192 Auth Service (JWT validation via gRPC) 4. API Gateway \u2192 Target Service (HTTP/REST or gRPC) 5. Target Service \u2192 Database/Cache (data retrieval) 6. Response propagates back through the chain ``` **Timeout Configuration**: - Client \u2192 Load Balancer: 60 seconds - Load Balancer \u2192 API Gateway: 55 seconds - API Gateway \u2192 Services: 45 seconds - Service \u2192 Database: 10 seconds ### Asynchronous Event Flow Long-running operations and inter-service communication use event-driven patterns: ``` 1. Service publishes event \u2192 Kafka topic 2. Kafka persists event (replication factor: 3) 3. Consumer groups subscribe to topics 4. Consumers process events with at-least-once delivery 5. State updates written to PostgreSQL 6.",
              "score": null
            }
          ],
          "latency_ms": 1085.9973600017838,
          "query_id": "realistic_009",
          "dimension": "synonym",
          "query": "What's the 99th percentile response time target?"
        },
        {
          "key_facts": [
            "P99 latency: < 200ms",
            "average P99 latency of 180ms for API operations"
          ],
          "found_facts": [],
          "missed_facts": [
            "P99 latency: < 200ms",
            "average P99 latency of 180ms for API operations"
          ],
          "coverage": 0.0,
          "expected_docs": [
            "architecture_overview"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_10",
              "content": "- Execution start rate: 500 per second - Completion rate: 450 per second (average 2-second execution time) **Database**: - Read throughput: 50,000 queries per second (across replicas) - Write throughput: 15,000 transactions per second - Connection capacity: 2,000 concurrent connections **Kafka**: - Message ingestion: 100,000 messages per second - Consumer throughput: 80,000 messages per second (aggregated) - End-to-end latency: < 100ms (P99) ### Resource Utilization **CPU Utilization** (Target: 60-70% average): - API Gateway: 55% average, 80% peak - Workflow Engine: 65% average, 85% peak - Auth Service: 40% average, 60% peak **Memory Utilization** (Target: < 80%): - API Gateway: 2.5GB average per pod (4GB allocated) - Workflow Engine: 6GB average per pod (8GB allocated) - Notification Service: 2.8GB average per pod (4GB allocated) **Network Throughput**: - Ingress: 2 Gbps average, 5 Gbps peak - Egress: 1.5 Gbps average, 4 Gbps peak - Internal (service mesh): 8 Gbps average ### Monitoring & Alerting **Key Metrics** (Monitored via Prometheus): - Request rate (per service, per endpoint) - Error rate (4xx, 5xx responses) - Latency percentiles (P50, P95, P99) - Resource utilization (CPU, memory, disk) - Cache hit/miss ratios - Database connection pool saturation - Kafka consumer lag **Alerts** (via PagerDuty): - P1 (Immediate): API error rate > 1%, database connection failure - P2 (< 15 min): P99 latency > 500ms, cache hit rate < 80% - P3 (< 1 hour): Resource utilization > 85%, replica count < minimum **SLI/SLO/SLA**: - SLI: API success rate (non-5xx responses) - SLO: 99.9% uptime per month (43 minutes downtime allowance) - SLA: 99.5% uptime guarantee (customer-facing SLA with credits) --- ## Disaster Recovery ### Recovery Objectives **RPO (Recovery Point Objective): 1 hour** - Maximum acceptable data loss: 1 hour of transactions - Achieved through: Continuous database replication + hourly snapshots - Kafka retention: 7 days allows event replay **RTO (Recovery Time Objective): 4 hours** - Maximum acceptable downtime: 4 hours for full system recovery - Includes: Failover, data verification, and service restoration - Automated runbooks reduce RTO to < 2 hours for common scenarios ### High Availability Architecture **Multi-AZ Deployment**: ``` Region: us-east-1 AZ-1a: AZ-1b: AZ-1c: - API Gateway (4) - API Gateway (4) - API Gateway (4) - Workflow Engine (6) - Workflow Engine (5) - Workflow Engine (5) - Auth Service (3) - Auth Service (3) - Auth",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_9",
              "content": "per API key (default: 1000 RPM) ### Secrets Management **HashiCorp Vault Integration**: - Dynamic database credentials: Generated on-demand, 1-hour TTL - Encryption keys: Transit secrets engine for encryption-as-a-service - API keys for external services: Stored with versioning - Rotation policy: Automated rotation every 90 days with notification **Secret Access Pattern**: ``` Service starts \u2192 Vault authentication (Kubernetes service account) \u2502 \u25bc Request secret lease (1-hour TTL) \u2502 \u25bc Vault returns dynamic credentials \u2502 \u25bc Service connects to PostgreSQL \u2502 Renew lease every 30 minutes ``` ### Data Protection **Encryption at Rest**: - PostgreSQL: AES-256 encryption enabled at volume level - Redis: Encryption enabled using AWS KMS - Kafka: Encryption at rest for all data on brokers - S3 backups: Server-side encryption with KMS (SSE-KMS) **Encryption in Transit**: - External traffic: TLS 1.3 only (TLS 1.2 deprecated) - Internal traffic: mTLS via Istio service mesh - Database connections: SSL/TLS enforced (sslmode=require) **Data Masking**: - Sensitive fields (email, phone) masked in logs - PII redacted from error messages and stack traces - Audit logs: Full data retention with access controls **Compliance**: - GDPR: User data export and deletion workflows - SOC 2 Type II: Audit logging for all data access - HIPAA: PHI isolation in dedicated tenants with enhanced encryption --- ## Performance Characteristics ### Latency Targets **API Operations** (P99 latency): - `GET /workflows/{id}`: < 50ms (cache hit), < 150ms (cache miss) - `POST /workflows`: < 200ms (includes validation and database write) - `POST /workflows/{id}/execute`: < 100ms (async, returns execution ID) - `GET /workflows/{id}/history`: < 300ms (paginated, 50 records per page) **Workflow Execution** (P99 latency): - Simple workflow (< 5 steps): < 2 seconds - Medium workflow (5-15 steps): < 5 seconds - Complex workflow (> 15 steps): < 15 seconds - Parallel execution: Linear scaling up to 10 concurrent branches **Database Operations** (P99 latency): - Simple SELECT: < 5ms - JOIN query: < 20ms - INSERT/UPDATE: < 10ms - Batch operations: < 100ms (batch size: 100 records) **Cache Operations** (P99 latency): - Redis GET: < 2ms - Redis SET: < 3ms - Redis DEL: < 2ms ### Throughput Capacity **API Gateway**: - Sustained: 10,000 requests per second - Peak: 25,000 requests per second (5-minute burst) - Rate limiting: 1,000 requests per minute per API key **Workflow Engine**: - Concurrent executions: 8,000 workflows (across 16 pods)",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_2",
              "content": "Review Query Execution Plans** ```sql -- Connect to CloudFlow database cloudflow db connect --readonly -- Explain slow query EXPLAIN ANALYZE SELECT w.*, e.status, e.error_message FROM workflows w LEFT JOIN executions e ON w.id = e.workflow_id WHERE w.workspace_id = 'ws_abc123' AND e.created_at > NOW() - INTERVAL '7 days' ORDER BY e.created_at DESC; -- Check for missing indexes SELECT schemaname, tablename, indexname, indexdef FROM pg_indexes WHERE tablename IN ('workflows', 'executions', 'workflow_steps') ORDER BY tablename, indexname; ``` **3. Optimize Queries** Common optimization techniques: **Add missing indexes:** ```sql -- Index for workflow lookup by workspace CREATE INDEX CONCURRENTLY idx_workflows_workspace_created ON workflows(workspace_id, created_at DESC); -- Index for execution status queries CREATE INDEX CONCURRENTLY idx_executions_status_created ON executions(workflow_id, status, created_at DESC) WHERE status IN ('running', 'pending'); -- Composite index for common filter combinations CREATE INDEX CONCURRENTLY idx_executions_workspace_status ON executions(workspace_id, status, created_at) INCLUDE (error_message, retry_count); ``` **Use query result caching:** ```bash # Enable query cache for workspace metadata cloudflow config set cache.workspace.ttl 3600 # Configure Redis cache backend cloudflow config set cache.backend redis cloudflow config set cache.redis.host redis.cloudflow.svc.cluster.local cloudflow config set cache.redis.port 6379 ``` ### High API Latency #### Latency Breakdown Analysis ```bash # Generate latency report cloudflow metrics latency-report --endpoint \"/api/v1/workflows\" --last 24h # Sample output: # Endpoint: POST /api/v1/workflows/execute # P50: 245ms | P95: 1823ms | P99: 4521ms # Breakdown: # - Auth: 45ms (18%) # - DB Query: 156ms (64%) # - Business Logic: 32ms (13%) # - Response Serialization: 12ms (5%) ``` **Network latency issues:** ```bash # Test connectivity to CloudFlow API time curl -w \"@curl-format.txt\" -o /dev/null -s https://api.cloudflow.io/health # Create curl-format.txt: cat > curl-format.txt << EOF time_namelookup: %{time_namelookup}s\\n time_connect: %{time_connect}s\\n time_appconnect: %{time_appconnect}s\\n time_pretransfer: %{time_pretransfer}s\\n time_redirect: %{time_redirect}s\\n time_starttransfer: %{time_starttransfer}s\\n ----------\\n time_total: %{time_total}s\\n EOF # Trace route to API endpoint traceroute api.cloudflow.io # Check DNS resolution time dig api.cloudflow.io | grep \"Query time\" ``` ### Memory Leaks #### Detection ```bash # Monitor CloudFlow service memory usage kubectl top pods -n cloudflow --sort-by=memory # Get detailed memory metrics for specific pod kubectl exec -n cloudflow deploy/cloudflow-api -- \\ curl localhost:9090/metrics | grep memory # Check for OOMKilled pods kubectl get pods -n cloudflow --field-selector=status.phase=Failed | \\ grep OOMKilled # Review memory limits and requests kubectl describe deployment cloudflow-api -n cloudflow | \\ grep -A 5 \"Limits\\|Requests\" ``` #### Common Causes **1.",
              "score": null
            },
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_6",
              "content": "**Verify Service Health**: ```bash kubectl get pods -n cloudflow-prod curl https://api.cloudflow.io/health ``` **Recovery Time Objective (RTO)**: 4 hours **Recovery Point Objective (RPO)**: 24 hours --- ## Scaling and Performance ### Horizontal Pod Autoscaling CloudFlow is configured with Horizontal Pod Autoscaler (HPA) to automatically scale based on resource utilization: ```yaml # hpa.yaml apiVersion: autoscaling/v2 kind: HorizontalPodAutoscaler metadata: name: cloudflow-api-hpa namespace: cloudflow-prod spec: scaleTargetRef: apiVersion: apps/v1 kind: Deployment name: cloudflow-api minReplicas: 3 maxReplicas: 10 metrics: - type: Resource resource: name: cpu target: type: Utilization averageUtilization: 70 - type: Resource resource: name: memory target: type: Utilization averageUtilization: 80 behavior: scaleDown: stabilizationWindowSeconds: 300 policies: - type: Percent value: 50 periodSeconds: 60 scaleUp: stabilizationWindowSeconds: 60 policies: - type: Percent value: 100 periodSeconds: 30 - type: Pods value: 2 periodSeconds: 30 selectPolicy: Max ``` Monitor autoscaling events: ```bash kubectl get hpa -n cloudflow-prod -w kubectl describe hpa cloudflow-api-hpa -n cloudflow-prod ``` ### Performance Optimization #### Connection Pooling Ensure optimal database connection pooling settings in PgBouncer: - **Pool Mode**: Transaction (optimal for microservices) - **Default Pool Size**: 25 connections per user - **Max DB Connections**: 100 (matches PostgreSQL `max_connections`) #### Redis Caching Implement Redis caching for frequently accessed data: ```javascript // Example caching strategy const CACHE_TTL = { workflows: 300, // 5 minutes userSessions: 3600, // 1 hour apiResults: 60 // 1 minute }; ``` #### Load Testing Perform regular load testing to validate scaling configuration: ```bash # Install k6 for load testing kubectl apply -f - <<EOF apiVersion: v1 kind: ConfigMap metadata: name: loadtest-script namespace: cloudflow-prod data: script.js: | import http from 'k6/http'; import { check, sleep } from 'k6'; export let options = { stages: [ { duration: '2m', target: 100 }, { duration: '5m', target: 100 }, { duration: '2m', target: 200 }, { duration: '5m', target: 200 }, { duration: '2m', target: 0 }, ], thresholds: { http_req_duration: ['p(95)<500'], }, }; export default function () { let res = http.get('https://api.cloudflow.io/api/workflows'); check(res, { 'status is 200': (r) => r.status === 200 }); sleep(1); } EOF ``` **Performance Targets**: - P95 API latency: < 500ms - P99 API latency: < 1000ms - Error rate: < 0.1% - Throughput: > 1000 requests/second --- ## Security and Compliance ### Network Policies Implement network policies to restrict pod-to-pod communication: ```yaml # network-policy.yaml apiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: cloudflow-network-policy namespace: cloudflow-prod spec:",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_5",
              "content": "Success/failure events published back to Kafka ``` **Event Schema**: ```json { \"event_id\": \"uuid-v4\", \"event_type\": \"workflow.execution.completed\", \"timestamp\": \"2026-01-15T10:30:00.000Z\", \"correlation_id\": \"request-trace-id\", \"payload\": { \"workflow_id\": \"wf-12345\", \"execution_id\": \"exec-67890\", \"status\": \"COMPLETED\", \"duration_ms\": 4230 }, \"metadata\": { \"source_service\": \"workflow-engine\", \"schema_version\": \"1.0\" } } ``` ### Inter-Service Communication Services communicate using two primary patterns: **Synchronous (gRPC)**: - Auth Service validation calls from API Gateway - User Service profile lookups - Low latency requirement (< 50ms) - Request-response pattern with circuit breaker **Asynchronous (Kafka)**: - Workflow execution events - Notification triggers - Audit log events - Eventual consistency acceptable --- ## Database Architecture ### PostgreSQL Primary Database **Cluster Configuration**: - Primary-replica setup with 1 primary + 2 read replicas - Instance type: db.r6g.2xlarge (8 vCPU, 64GB RAM) - Storage: 2TB gp3 SSD with 12,000 IOPS - Multi-AZ deployment for high availability - Automated backups: Daily snapshots, 30-day retention - Point-in-time recovery: 5-minute granularity **Database Schema Design**: ``` Core Tables: - users (5M rows): User accounts and profiles - workflows (2M rows): Workflow definitions - workflow_executions (500M rows, partitioned): Execution history - workflow_steps (2B rows, partitioned): Individual step records - schedules (100K rows): Scheduled workflow triggers - notifications (1B rows, partitioned): Notification delivery log Partitioning Strategy: - workflow_executions: Monthly partitions by created_at - workflow_steps: Monthly partitions by created_at - notifications: Weekly partitions by created_at - Automatic partition creation via cron job - Retention policy: Drop partitions older than 12 months ``` **Indexing Strategy**: - Primary keys: UUIDs with B-tree indexes - Frequently queried columns: Compound indexes (e.g., user_id + status + created_at) - JSONB columns: GIN indexes for workflow definitions - Full-text search: GiST indexes on description fields **Connection Pooling**: - PgBouncer in transaction mode - Pool size: 100 connections per service - Max client connections: 2000 - Connection timeout: 30 seconds **Performance Characteristics**: - Read query P99: < 20ms - Write query P99: < 50ms - Transaction throughput: 15,000 TPS - Replication lag: < 500ms --- ### Redis Caching Layer **Cluster Configuration**: - Redis Cluster with 6 nodes (3 primary + 3 replica) - Instance type: cache.r6g.xlarge (4 vCPU, 26GB RAM) - Total memory: 78GB usable cache space - Persistence: RDB snapshots every 5 minutes + AOF - Eviction policy: allkeys-lru **Cache Usage Patterns**: 1.",
              "score": null
            }
          ],
          "latency_ms": 947.6907429998391,
          "query_id": "realistic_009",
          "dimension": "problem",
          "query": "Our API latency is 500ms, is that acceptable?"
        },
        {
          "key_facts": [
            "P99 latency: < 200ms",
            "average P99 latency of 180ms for API operations"
          ],
          "found_facts": [],
          "missed_facts": [
            "P99 latency: < 200ms",
            "average P99 latency of 180ms for API operations"
          ],
          "coverage": 0.0,
          "expected_docs": [
            "architecture_overview"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_6",
              "content": "**Verify Service Health**: ```bash kubectl get pods -n cloudflow-prod curl https://api.cloudflow.io/health ``` **Recovery Time Objective (RTO)**: 4 hours **Recovery Point Objective (RPO)**: 24 hours --- ## Scaling and Performance ### Horizontal Pod Autoscaling CloudFlow is configured with Horizontal Pod Autoscaler (HPA) to automatically scale based on resource utilization: ```yaml # hpa.yaml apiVersion: autoscaling/v2 kind: HorizontalPodAutoscaler metadata: name: cloudflow-api-hpa namespace: cloudflow-prod spec: scaleTargetRef: apiVersion: apps/v1 kind: Deployment name: cloudflow-api minReplicas: 3 maxReplicas: 10 metrics: - type: Resource resource: name: cpu target: type: Utilization averageUtilization: 70 - type: Resource resource: name: memory target: type: Utilization averageUtilization: 80 behavior: scaleDown: stabilizationWindowSeconds: 300 policies: - type: Percent value: 50 periodSeconds: 60 scaleUp: stabilizationWindowSeconds: 60 policies: - type: Percent value: 100 periodSeconds: 30 - type: Pods value: 2 periodSeconds: 30 selectPolicy: Max ``` Monitor autoscaling events: ```bash kubectl get hpa -n cloudflow-prod -w kubectl describe hpa cloudflow-api-hpa -n cloudflow-prod ``` ### Performance Optimization #### Connection Pooling Ensure optimal database connection pooling settings in PgBouncer: - **Pool Mode**: Transaction (optimal for microservices) - **Default Pool Size**: 25 connections per user - **Max DB Connections**: 100 (matches PostgreSQL `max_connections`) #### Redis Caching Implement Redis caching for frequently accessed data: ```javascript // Example caching strategy const CACHE_TTL = { workflows: 300, // 5 minutes userSessions: 3600, // 1 hour apiResults: 60 // 1 minute }; ``` #### Load Testing Perform regular load testing to validate scaling configuration: ```bash # Install k6 for load testing kubectl apply -f - <<EOF apiVersion: v1 kind: ConfigMap metadata: name: loadtest-script namespace: cloudflow-prod data: script.js: | import http from 'k6/http'; import { check, sleep } from 'k6'; export let options = { stages: [ { duration: '2m', target: 100 }, { duration: '5m', target: 100 }, { duration: '2m', target: 200 }, { duration: '5m', target: 200 }, { duration: '2m', target: 0 }, ], thresholds: { http_req_duration: ['p(95)<500'], }, }; export default function () { let res = http.get('https://api.cloudflow.io/api/workflows'); check(res, { 'status is 200': (r) => r.status === 200 }); sleep(1); } EOF ``` **Performance Targets**: - P95 API latency: < 500ms - P99 API latency: < 1000ms - Error rate: < 0.1% - Throughput: > 1000 requests/second --- ## Security and Compliance ### Network Policies Implement network policies to restrict pod-to-pod communication: ```yaml # network-policy.yaml apiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: cloudflow-network-policy namespace: cloudflow-prod spec:",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_10",
              "content": "- Execution start rate: 500 per second - Completion rate: 450 per second (average 2-second execution time) **Database**: - Read throughput: 50,000 queries per second (across replicas) - Write throughput: 15,000 transactions per second - Connection capacity: 2,000 concurrent connections **Kafka**: - Message ingestion: 100,000 messages per second - Consumer throughput: 80,000 messages per second (aggregated) - End-to-end latency: < 100ms (P99) ### Resource Utilization **CPU Utilization** (Target: 60-70% average): - API Gateway: 55% average, 80% peak - Workflow Engine: 65% average, 85% peak - Auth Service: 40% average, 60% peak **Memory Utilization** (Target: < 80%): - API Gateway: 2.5GB average per pod (4GB allocated) - Workflow Engine: 6GB average per pod (8GB allocated) - Notification Service: 2.8GB average per pod (4GB allocated) **Network Throughput**: - Ingress: 2 Gbps average, 5 Gbps peak - Egress: 1.5 Gbps average, 4 Gbps peak - Internal (service mesh): 8 Gbps average ### Monitoring & Alerting **Key Metrics** (Monitored via Prometheus): - Request rate (per service, per endpoint) - Error rate (4xx, 5xx responses) - Latency percentiles (P50, P95, P99) - Resource utilization (CPU, memory, disk) - Cache hit/miss ratios - Database connection pool saturation - Kafka consumer lag **Alerts** (via PagerDuty): - P1 (Immediate): API error rate > 1%, database connection failure - P2 (< 15 min): P99 latency > 500ms, cache hit rate < 80% - P3 (< 1 hour): Resource utilization > 85%, replica count < minimum **SLI/SLO/SLA**: - SLI: API success rate (non-5xx responses) - SLO: 99.9% uptime per month (43 minutes downtime allowance) - SLA: 99.5% uptime guarantee (customer-facing SLA with credits) --- ## Disaster Recovery ### Recovery Objectives **RPO (Recovery Point Objective): 1 hour** - Maximum acceptable data loss: 1 hour of transactions - Achieved through: Continuous database replication + hourly snapshots - Kafka retention: 7 days allows event replay **RTO (Recovery Time Objective): 4 hours** - Maximum acceptable downtime: 4 hours for full system recovery - Includes: Failover, data verification, and service restoration - Automated runbooks reduce RTO to < 2 hours for common scenarios ### High Availability Architecture **Multi-AZ Deployment**: ``` Region: us-east-1 AZ-1a: AZ-1b: AZ-1c: - API Gateway (4) - API Gateway (4) - API Gateway (4) - Workflow Engine (6) - Workflow Engine (5) - Workflow Engine (5) - Auth Service (3) - Auth Service (3) - Auth",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_9",
              "content": "per API key (default: 1000 RPM) ### Secrets Management **HashiCorp Vault Integration**: - Dynamic database credentials: Generated on-demand, 1-hour TTL - Encryption keys: Transit secrets engine for encryption-as-a-service - API keys for external services: Stored with versioning - Rotation policy: Automated rotation every 90 days with notification **Secret Access Pattern**: ``` Service starts \u2192 Vault authentication (Kubernetes service account) \u2502 \u25bc Request secret lease (1-hour TTL) \u2502 \u25bc Vault returns dynamic credentials \u2502 \u25bc Service connects to PostgreSQL \u2502 Renew lease every 30 minutes ``` ### Data Protection **Encryption at Rest**: - PostgreSQL: AES-256 encryption enabled at volume level - Redis: Encryption enabled using AWS KMS - Kafka: Encryption at rest for all data on brokers - S3 backups: Server-side encryption with KMS (SSE-KMS) **Encryption in Transit**: - External traffic: TLS 1.3 only (TLS 1.2 deprecated) - Internal traffic: mTLS via Istio service mesh - Database connections: SSL/TLS enforced (sslmode=require) **Data Masking**: - Sensitive fields (email, phone) masked in logs - PII redacted from error messages and stack traces - Audit logs: Full data retention with access controls **Compliance**: - GDPR: User data export and deletion workflows - SOC 2 Type II: Audit logging for all data access - HIPAA: PHI isolation in dedicated tenants with enhanced encryption --- ## Performance Characteristics ### Latency Targets **API Operations** (P99 latency): - `GET /workflows/{id}`: < 50ms (cache hit), < 150ms (cache miss) - `POST /workflows`: < 200ms (includes validation and database write) - `POST /workflows/{id}/execute`: < 100ms (async, returns execution ID) - `GET /workflows/{id}/history`: < 300ms (paginated, 50 records per page) **Workflow Execution** (P99 latency): - Simple workflow (< 5 steps): < 2 seconds - Medium workflow (5-15 steps): < 5 seconds - Complex workflow (> 15 steps): < 15 seconds - Parallel execution: Linear scaling up to 10 concurrent branches **Database Operations** (P99 latency): - Simple SELECT: < 5ms - JOIN query: < 20ms - INSERT/UPDATE: < 10ms - Batch operations: < 100ms (batch size: 100 records) **Cache Operations** (P99 latency): - Redis GET: < 2ms - Redis SET: < 3ms - Redis DEL: < 2ms ### Throughput Capacity **API Gateway**: - Sustained: 10,000 requests per second - Peak: 25,000 requests per second (5-minute burst) - Rate limiting: 1,000 requests per minute per API key **Workflow Engine**: - Concurrent executions: 8,000 workflows (across 16 pods)",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_2",
              "content": "Review Query Execution Plans** ```sql -- Connect to CloudFlow database cloudflow db connect --readonly -- Explain slow query EXPLAIN ANALYZE SELECT w.*, e.status, e.error_message FROM workflows w LEFT JOIN executions e ON w.id = e.workflow_id WHERE w.workspace_id = 'ws_abc123' AND e.created_at > NOW() - INTERVAL '7 days' ORDER BY e.created_at DESC; -- Check for missing indexes SELECT schemaname, tablename, indexname, indexdef FROM pg_indexes WHERE tablename IN ('workflows', 'executions', 'workflow_steps') ORDER BY tablename, indexname; ``` **3. Optimize Queries** Common optimization techniques: **Add missing indexes:** ```sql -- Index for workflow lookup by workspace CREATE INDEX CONCURRENTLY idx_workflows_workspace_created ON workflows(workspace_id, created_at DESC); -- Index for execution status queries CREATE INDEX CONCURRENTLY idx_executions_status_created ON executions(workflow_id, status, created_at DESC) WHERE status IN ('running', 'pending'); -- Composite index for common filter combinations CREATE INDEX CONCURRENTLY idx_executions_workspace_status ON executions(workspace_id, status, created_at) INCLUDE (error_message, retry_count); ``` **Use query result caching:** ```bash # Enable query cache for workspace metadata cloudflow config set cache.workspace.ttl 3600 # Configure Redis cache backend cloudflow config set cache.backend redis cloudflow config set cache.redis.host redis.cloudflow.svc.cluster.local cloudflow config set cache.redis.port 6379 ``` ### High API Latency #### Latency Breakdown Analysis ```bash # Generate latency report cloudflow metrics latency-report --endpoint \"/api/v1/workflows\" --last 24h # Sample output: # Endpoint: POST /api/v1/workflows/execute # P50: 245ms | P95: 1823ms | P99: 4521ms # Breakdown: # - Auth: 45ms (18%) # - DB Query: 156ms (64%) # - Business Logic: 32ms (13%) # - Response Serialization: 12ms (5%) ``` **Network latency issues:** ```bash # Test connectivity to CloudFlow API time curl -w \"@curl-format.txt\" -o /dev/null -s https://api.cloudflow.io/health # Create curl-format.txt: cat > curl-format.txt << EOF time_namelookup: %{time_namelookup}s\\n time_connect: %{time_connect}s\\n time_appconnect: %{time_appconnect}s\\n time_pretransfer: %{time_pretransfer}s\\n time_redirect: %{time_redirect}s\\n time_starttransfer: %{time_starttransfer}s\\n ----------\\n time_total: %{time_total}s\\n EOF # Trace route to API endpoint traceroute api.cloudflow.io # Check DNS resolution time dig api.cloudflow.io | grep \"Query time\" ``` ### Memory Leaks #### Detection ```bash # Monitor CloudFlow service memory usage kubectl top pods -n cloudflow --sort-by=memory # Get detailed memory metrics for specific pod kubectl exec -n cloudflow deploy/cloudflow-api -- \\ curl localhost:9090/metrics | grep memory # Check for OOMKilled pods kubectl get pods -n cloudflow --field-selector=status.phase=Failed | \\ grep OOMKilled # Review memory limits and requests kubectl describe deployment cloudflow-api -n cloudflow | \\ grep -A 5 \"Limits\\|Requests\" ``` #### Common Causes **1.",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_10",
              "content": "database nc -zv cloudflow-db.internal.company.com 5432 # Trace route traceroute api.cloudflow.io # Capture packets tcpdump -i any -w /tmp/capture.pcap port 5432 ``` --- ## Escalation Procedures ### Severity Levels CloudFlow incidents are classified into four severity levels: #### SEV-1: Critical (P1) - **Definition:** Complete service outage or severe degradation affecting all users - **Examples:** - API returns 5xx errors for > 5 minutes - Database completely unavailable - Authentication system down - Data loss or corruption - **Response Time:** Immediate (< 15 minutes) - **Escalation:** Page on-call engineer immediately #### SEV-2: High (P2) - **Definition:** Major functionality impaired affecting multiple users - **Examples:** - Workflow execution success rate < 90% - Significant performance degradation (p95 latency > 10s) - Rate limiting affecting large customer segment - **Response Time:** < 1 hour - **Escalation:** Create incident ticket and notify on-call #### SEV-3: Medium (P3) - **Definition:** Partial functionality degraded affecting some users - **Examples:** - Intermittent failures for specific workflow types - Minor performance issues - Non-critical feature unavailable - **Response Time:** < 4 hours - **Escalation:** Create ticket, normal business hours support #### SEV-4: Low (P4) - **Definition:** Minor issues with minimal user impact - **Examples:** - Cosmetic issues - Documentation errors - Feature requests - **Response Time:** < 2 business days - **Escalation:** Standard support ticket ### Escalation Steps #### Step 1: Initial Assessment (0-5 minutes) ```bash # Run health check cloudflow health check --comprehensive # Check status page curl https://status.cloudflow.io/api/v1/status.json # Review recent changes cloudflow audit log --last 2h --event-type \"deployment,configuration\" # Check metrics dashboard cloudflow metrics dashboard --incident-mode ``` #### Step 2: Gather Information (5-15 minutes) Create incident document with: - Incident timestamp and duration - Affected services and endpoints - Error rates and user impact - Recent changes or deployments - Relevant log excerpts - Correlation IDs for failed requests ```bash # Generate incident report cloudflow debug incident-report \\ --start \"2026-01-24T10:30:00Z\" \\ --end \"2026-01-24T11:00:00Z\" \\ --output incident-report.md # Capture system snapshot cloudflow debug snapshot --output snapshot-$(date +%Y%m%d-%H%M%S).tar.gz ``` #### Step 3: Escalate Based on Severity **For SEV-1 (Critical):** ```bash # Page on-call engineer cloudflow incident create \\ --severity SEV-1 \\ --title \"Complete API outage\" \\ --description \"All API requests returning 503\" \\ --page-oncall # Notify status page cloudflow status update \\ --status \"major_outage\" \\ --message \"We are investigating a complete service",
              "score": null
            }
          ],
          "latency_ms": 944.9530490019242,
          "query_id": "realistic_009",
          "dimension": "casual",
          "query": "api latency target"
        },
        {
          "key_facts": [
            "P99 latency: < 200ms",
            "average P99 latency of 180ms for API operations"
          ],
          "found_facts": [
            "average P99 latency of 180ms for API operations"
          ],
          "missed_facts": [
            "P99 latency: < 200ms"
          ],
          "coverage": 0.5,
          "expected_docs": [
            "architecture_overview"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_6",
              "content": "**Verify Service Health**: ```bash kubectl get pods -n cloudflow-prod curl https://api.cloudflow.io/health ``` **Recovery Time Objective (RTO)**: 4 hours **Recovery Point Objective (RPO)**: 24 hours --- ## Scaling and Performance ### Horizontal Pod Autoscaling CloudFlow is configured with Horizontal Pod Autoscaler (HPA) to automatically scale based on resource utilization: ```yaml # hpa.yaml apiVersion: autoscaling/v2 kind: HorizontalPodAutoscaler metadata: name: cloudflow-api-hpa namespace: cloudflow-prod spec: scaleTargetRef: apiVersion: apps/v1 kind: Deployment name: cloudflow-api minReplicas: 3 maxReplicas: 10 metrics: - type: Resource resource: name: cpu target: type: Utilization averageUtilization: 70 - type: Resource resource: name: memory target: type: Utilization averageUtilization: 80 behavior: scaleDown: stabilizationWindowSeconds: 300 policies: - type: Percent value: 50 periodSeconds: 60 scaleUp: stabilizationWindowSeconds: 60 policies: - type: Percent value: 100 periodSeconds: 30 - type: Pods value: 2 periodSeconds: 30 selectPolicy: Max ``` Monitor autoscaling events: ```bash kubectl get hpa -n cloudflow-prod -w kubectl describe hpa cloudflow-api-hpa -n cloudflow-prod ``` ### Performance Optimization #### Connection Pooling Ensure optimal database connection pooling settings in PgBouncer: - **Pool Mode**: Transaction (optimal for microservices) - **Default Pool Size**: 25 connections per user - **Max DB Connections**: 100 (matches PostgreSQL `max_connections`) #### Redis Caching Implement Redis caching for frequently accessed data: ```javascript // Example caching strategy const CACHE_TTL = { workflows: 300, // 5 minutes userSessions: 3600, // 1 hour apiResults: 60 // 1 minute }; ``` #### Load Testing Perform regular load testing to validate scaling configuration: ```bash # Install k6 for load testing kubectl apply -f - <<EOF apiVersion: v1 kind: ConfigMap metadata: name: loadtest-script namespace: cloudflow-prod data: script.js: | import http from 'k6/http'; import { check, sleep } from 'k6'; export let options = { stages: [ { duration: '2m', target: 100 }, { duration: '5m', target: 100 }, { duration: '2m', target: 200 }, { duration: '5m', target: 200 }, { duration: '2m', target: 0 }, ], thresholds: { http_req_duration: ['p(95)<500'], }, }; export default function () { let res = http.get('https://api.cloudflow.io/api/workflows'); check(res, { 'status is 200': (r) => r.status === 200 }); sleep(1); } EOF ``` **Performance Targets**: - P95 API latency: < 500ms - P99 API latency: < 1000ms - Error rate: < 0.1% - Throughput: > 1000 requests/second --- ## Security and Compliance ### Network Policies Implement network policies to restrict pod-to-pod communication: ```yaml # network-policy.yaml apiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: cloudflow-network-policy namespace: cloudflow-prod spec:",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_0",
              "content": "# CloudFlow Platform - System Architecture Overview **Document Version:** 2.3.1 **Last Updated:** January 15, 2026 **Owner:** Platform Architecture Team **Status:** Production ## Executive Summary CloudFlow is a distributed, cloud-native workflow automation platform designed to orchestrate complex business processes at scale. The platform processes over 2.5 million workflow executions daily with an average P99 latency of 180ms for API operations and 4.2 seconds for workflow execution. This document provides a comprehensive overview of the system architecture, including microservices design, data flow patterns, infrastructure decisions, and operational considerations. ## Table of Contents 1. [High-Level Architecture](#high-level-architecture) 2. [Microservices Breakdown](#microservices-breakdown) 3. [Data Flow Architecture](#data-flow-architecture) 4. [Database Architecture](#database-architecture) 5. [Message Queue Patterns](#message-queue-patterns) 6. [Caching Strategy](#caching-strategy) 7. [Security Architecture](#security-architecture) 8. [Performance Characteristics](#performance-characteristics) 9. [Disaster Recovery](#disaster-recovery) --- ## High-Level Architecture CloudFlow follows a microservices architecture pattern deployed across multiple availability zones in AWS. The platform is designed with the following core principles: - **Scalability**: Horizontal scaling for all services with auto-scaling groups - **Resilience**: Circuit breakers, bulkheads, and graceful degradation - **Observability**: Distributed tracing, centralized logging, and comprehensive metrics - **Security**: Zero-trust network architecture with mTLS encryption ### Architecture Diagram (Conceptual) ``` \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 Load Balancer (ALB) \u2502 \u2502 (TLS Termination - 443) \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u25bc \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 API Gateway Layer \u2502 \u2502 (Rate Limiting, Auth, Request Routing) \u2502 \u2514\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u2502 \u2502 \u2502 \u2502 \u25bc \u25bc \u25bc \u25bc \u25bc \u250c\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502Auth\u2502 \u2502Workflow\u2502 \u2502Scheduler\u2502 \u2502Notification\u2502 \u2502User Service\u2502 \u2502Svc \u2502 \u2502 Engine \u2502 \u2502 Service \u2502 \u2502 Service \u2502 \u2502 \u2502 \u2514\u2500\u252c\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u2502 \u2502 \u2502 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 \u2502 \u2502 \u25bc \u25bc \u25bc \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502PostgreSQL\u2502 \u2502 Redis \u2502 \u2502 Kafka \u2502 \u2502 Cluster \u2502 \u2502 Cluster \u2502 \u2502 Cluster \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 ``` ### Technology Stack - **Runtime**: Node.js 20.x (API Gateway, Workflow Engine), Go 1.21 (Auth Service, Scheduler) - **Container Orchestration**: Kubernetes 1.28 (EKS) - **Service Mesh**: Istio 1.20 for service-to-service communication - **Databases**: PostgreSQL 15.4, Redis 7.2 - **Message Broker**: Apache Kafka 3.6 - **Monitoring**: Prometheus, Grafana, Jaeger for distributed tracing - **Secrets Management**: HashiCorp Vault 1.15 --- ## Microservices Breakdown ### API Gateway **Purpose**: Single entry point for all client requests, providing authentication, rate limiting, request routing, and protocol translation.",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_2",
              "content": "Review Query Execution Plans** ```sql -- Connect to CloudFlow database cloudflow db connect --readonly -- Explain slow query EXPLAIN ANALYZE SELECT w.*, e.status, e.error_message FROM workflows w LEFT JOIN executions e ON w.id = e.workflow_id WHERE w.workspace_id = 'ws_abc123' AND e.created_at > NOW() - INTERVAL '7 days' ORDER BY e.created_at DESC; -- Check for missing indexes SELECT schemaname, tablename, indexname, indexdef FROM pg_indexes WHERE tablename IN ('workflows', 'executions', 'workflow_steps') ORDER BY tablename, indexname; ``` **3. Optimize Queries** Common optimization techniques: **Add missing indexes:** ```sql -- Index for workflow lookup by workspace CREATE INDEX CONCURRENTLY idx_workflows_workspace_created ON workflows(workspace_id, created_at DESC); -- Index for execution status queries CREATE INDEX CONCURRENTLY idx_executions_status_created ON executions(workflow_id, status, created_at DESC) WHERE status IN ('running', 'pending'); -- Composite index for common filter combinations CREATE INDEX CONCURRENTLY idx_executions_workspace_status ON executions(workspace_id, status, created_at) INCLUDE (error_message, retry_count); ``` **Use query result caching:** ```bash # Enable query cache for workspace metadata cloudflow config set cache.workspace.ttl 3600 # Configure Redis cache backend cloudflow config set cache.backend redis cloudflow config set cache.redis.host redis.cloudflow.svc.cluster.local cloudflow config set cache.redis.port 6379 ``` ### High API Latency #### Latency Breakdown Analysis ```bash # Generate latency report cloudflow metrics latency-report --endpoint \"/api/v1/workflows\" --last 24h # Sample output: # Endpoint: POST /api/v1/workflows/execute # P50: 245ms | P95: 1823ms | P99: 4521ms # Breakdown: # - Auth: 45ms (18%) # - DB Query: 156ms (64%) # - Business Logic: 32ms (13%) # - Response Serialization: 12ms (5%) ``` **Network latency issues:** ```bash # Test connectivity to CloudFlow API time curl -w \"@curl-format.txt\" -o /dev/null -s https://api.cloudflow.io/health # Create curl-format.txt: cat > curl-format.txt << EOF time_namelookup: %{time_namelookup}s\\n time_connect: %{time_connect}s\\n time_appconnect: %{time_appconnect}s\\n time_pretransfer: %{time_pretransfer}s\\n time_redirect: %{time_redirect}s\\n time_starttransfer: %{time_starttransfer}s\\n ----------\\n time_total: %{time_total}s\\n EOF # Trace route to API endpoint traceroute api.cloudflow.io # Check DNS resolution time dig api.cloudflow.io | grep \"Query time\" ``` ### Memory Leaks #### Detection ```bash # Monitor CloudFlow service memory usage kubectl top pods -n cloudflow --sort-by=memory # Get detailed memory metrics for specific pod kubectl exec -n cloudflow deploy/cloudflow-api -- \\ curl localhost:9090/metrics | grep memory # Check for OOMKilled pods kubectl get pods -n cloudflow --field-selector=status.phase=Failed | \\ grep OOMKilled # Review memory limits and requests kubectl describe deployment cloudflow-api -n cloudflow | \\ grep -A 5 \"Limits\\|Requests\" ``` #### Common Causes **1.",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_10",
              "content": "database nc -zv cloudflow-db.internal.company.com 5432 # Trace route traceroute api.cloudflow.io # Capture packets tcpdump -i any -w /tmp/capture.pcap port 5432 ``` --- ## Escalation Procedures ### Severity Levels CloudFlow incidents are classified into four severity levels: #### SEV-1: Critical (P1) - **Definition:** Complete service outage or severe degradation affecting all users - **Examples:** - API returns 5xx errors for > 5 minutes - Database completely unavailable - Authentication system down - Data loss or corruption - **Response Time:** Immediate (< 15 minutes) - **Escalation:** Page on-call engineer immediately #### SEV-2: High (P2) - **Definition:** Major functionality impaired affecting multiple users - **Examples:** - Workflow execution success rate < 90% - Significant performance degradation (p95 latency > 10s) - Rate limiting affecting large customer segment - **Response Time:** < 1 hour - **Escalation:** Create incident ticket and notify on-call #### SEV-3: Medium (P3) - **Definition:** Partial functionality degraded affecting some users - **Examples:** - Intermittent failures for specific workflow types - Minor performance issues - Non-critical feature unavailable - **Response Time:** < 4 hours - **Escalation:** Create ticket, normal business hours support #### SEV-4: Low (P4) - **Definition:** Minor issues with minimal user impact - **Examples:** - Cosmetic issues - Documentation errors - Feature requests - **Response Time:** < 2 business days - **Escalation:** Standard support ticket ### Escalation Steps #### Step 1: Initial Assessment (0-5 minutes) ```bash # Run health check cloudflow health check --comprehensive # Check status page curl https://status.cloudflow.io/api/v1/status.json # Review recent changes cloudflow audit log --last 2h --event-type \"deployment,configuration\" # Check metrics dashboard cloudflow metrics dashboard --incident-mode ``` #### Step 2: Gather Information (5-15 minutes) Create incident document with: - Incident timestamp and duration - Affected services and endpoints - Error rates and user impact - Recent changes or deployments - Relevant log excerpts - Correlation IDs for failed requests ```bash # Generate incident report cloudflow debug incident-report \\ --start \"2026-01-24T10:30:00Z\" \\ --end \"2026-01-24T11:00:00Z\" \\ --output incident-report.md # Capture system snapshot cloudflow debug snapshot --output snapshot-$(date +%Y%m%d-%H%M%S).tar.gz ``` #### Step 3: Escalate Based on Severity **For SEV-1 (Critical):** ```bash # Page on-call engineer cloudflow incident create \\ --severity SEV-1 \\ --title \"Complete API outage\" \\ --description \"All API requests returning 503\" \\ --page-oncall # Notify status page cloudflow status update \\ --status \"major_outage\" \\ --message \"We are investigating a complete service",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_8",
              "content": "Leverage caching:** ```bash # Enable client-side caching export CLOUDFLOW_CACHE_ENABLED=true export CLOUDFLOW_CACHE_TTL=300 # Cache workflow metadata cloudflow workflows list --use-cache --cache-ttl 600 ``` --- ## Log Analysis & Debugging ### Accessing CloudFlow Logs #### Kubernetes Deployments ```bash # List all CloudFlow pods kubectl get pods -n cloudflow # Tail logs from API server kubectl logs -f -n cloudflow deployment/cloudflow-api --tail=100 # Get logs from specific pod kubectl logs -n cloudflow cloudflow-api-7d4f6b8c9d-x7k2m # Get logs from previous crashed pod kubectl logs -n cloudflow cloudflow-api-7d4f6b8c9d-x7k2m --previous # Get logs from all pods in deployment kubectl logs -n cloudflow deployment/cloudflow-api --all-containers=true # Stream logs from multiple pods kubectl logs -n cloudflow -l app=cloudflow-api -f --max-log-requests=10 ``` #### Log Levels CloudFlow supports the following log levels: - `TRACE` - Very detailed debugging information - `DEBUG` - Detailed debugging information - `INFO` - Informational messages (default) - `WARN` - Warning messages - `ERROR` - Error messages - `FATAL` - Fatal errors causing shutdown **Changing log levels:** ```bash # Set global log level cloudflow config set logging.level DEBUG # Set log level for specific component cloudflow config set logging.components.database DEBUG cloudflow config set logging.components.auth INFO cloudflow config set logging.components.workflows TRACE # Temporary log level increase (resets after 1 hour) cloudflow debug set-log-level DEBUG --duration 3600 # View current log configuration cloudflow config get logging --format json ``` ### Grep Patterns for Common Issues #### Authentication Failures ```bash # Find all authentication errors kubectl logs -n cloudflow deployment/cloudflow-api | \\ grep -i \"authentication\\|401\\|unauthorized\" | \\ tail -n 50 # Find JWT validation failures kubectl logs -n cloudflow deployment/cloudflow-api | \\ grep -E \"JWT|token.*invalid|signature.*failed\" # Find clock skew issues kubectl logs -n cloudflow deployment/cloudflow-api | \\ grep -i \"clock skew\\|time.*sync\\|nbf\\|exp\" ``` #### Database Errors ```bash # Find database connection errors kubectl logs -n cloudflow deployment/cloudflow-api | \\ grep -E \"connection.*pool|could not connect|database.*timeout\" # Find slow queries kubectl logs -n cloudflow deployment/cloudflow-api | \\ grep \"slow query\" | \\ awk '{print $NF}' | \\ sort -n | \\ tail -n 20 # Find deadlock errors kubectl logs -n cloudflow deployment/cloudflow-api | \\ grep -i \"deadlock detected\" ``` #### Workflow Execution Errors ```bash # Find workflow timeout errors kubectl logs -n cloudflow deployment/cloudflow-workflow-engine | \\ grep -E \"timeout|exceeded.*3600\" # Find workflow retry attempts kubectl logs -n cloudflow deployment/cloudflow-workflow-engine | \\ grep -E \"retry attempt [0-9]|retrying",
              "score": null
            }
          ],
          "latency_ms": 1379.7181269983412,
          "query_id": "realistic_009",
          "dimension": "contextual",
          "query": "Setting SLOs for our service, what P99 latency does CloudFlow guarantee?"
        },
        {
          "key_facts": [
            "P99 latency: < 200ms",
            "average P99 latency of 180ms for API operations"
          ],
          "found_facts": [
            "average P99 latency of 180ms for API operations"
          ],
          "missed_facts": [
            "P99 latency: < 200ms"
          ],
          "coverage": 0.5,
          "expected_docs": [
            "architecture_overview"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_4",
              "content": "--set prometheus.prometheusSpec.retention=30d \\ --set prometheus.prometheusSpec.storageSpec.volumeClaimTemplate.spec.resources.requests.storage=100Gi ``` CloudFlow exposes Prometheus metrics on port 9090 at the `/metrics` endpoint. The metrics include: - **HTTP Metrics**: Request rate, latency, error rate - **Database Metrics**: Connection pool status, query duration - **Worker Metrics**: Job queue size, processing time, failure rate - **System Metrics**: CPU usage, memory usage, heap statistics Example metrics exposed: ``` # HELP cloudflow_http_requests_total Total number of HTTP requests # TYPE cloudflow_http_requests_total counter cloudflow_http_requests_total{method=\"GET\",route=\"/api/workflows\",status=\"200\"} 15420 # HELP cloudflow_http_request_duration_seconds HTTP request duration in seconds # TYPE cloudflow_http_request_duration_seconds histogram cloudflow_http_request_duration_seconds_bucket{method=\"POST\",route=\"/api/workflows\",le=\"0.1\"} 8234 # HELP cloudflow_worker_jobs_processed_total Total number of jobs processed # TYPE cloudflow_worker_jobs_processed_total counter cloudflow_worker_jobs_processed_total{status=\"success\"} 45230 # HELP cloudflow_database_connections Current database connections # TYPE cloudflow_database_connections gauge cloudflow_database_connections{state=\"active\"} 23 ``` ### Grafana Dashboards Access Grafana to view CloudFlow dashboards: ```bash kubectl port-forward -n monitoring svc/prometheus-grafana 3000:80 ``` Import the CloudFlow dashboard (ID: 15847) or use the provided JSON template. Key dashboard panels include: 1. **API Performance**: Request rate, P95/P99 latency, error rate 2. **Resource Usage**: CPU, memory, disk I/O per pod 3. **Database Health**: Connection pool utilization, query performance 4. **Worker Status**: Queue depth, processing rate, job success/failure ratio 5. **System Overview**: Pod status, replica count, autoscaling events ### Logging with CloudWatch CloudFlow logs are automatically shipped to CloudWatch Logs.",
              "score": null
            },
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_3",
              "content": "Deploy PostgreSQL using the Bitnami Helm chart: ```yaml # postgres-values.yaml global: postgresql: auth: username: cloudflow database: cloudflow existingSecret: postgres-credentials image: tag: \"14.10.0\" primary: resources: limits: cpu: 4000m memory: 8Gi requests: cpu: 2000m memory: 4Gi persistence: enabled: true size: 100Gi storageClass: gp3 extendedConfiguration: | max_connections = 100 shared_buffers = 2GB effective_cache_size = 6GB maintenance_work_mem = 512MB checkpoint_completion_target = 0.9 wal_buffers = 16MB default_statistics_target = 100 random_page_cost = 1.1 effective_io_concurrency = 200 work_mem = 10MB min_wal_size = 1GB max_wal_size = 4GB readReplicas: replicaCount: 2 resources: limits: cpu: 2000m memory: 4Gi requests: cpu: 1000m memory: 2Gi metrics: enabled: true serviceMonitor: enabled: true ``` Install PostgreSQL: ```bash # Create password secret kubectl create secret generic postgres-credentials \\ --namespace cloudflow-prod \\ --from-literal=postgres-password=\"$(openssl rand -base64 32)\" \\ --from-literal=password=\"$(openssl rand -base64 32)\" # Install PostgreSQL helm install postgresql bitnami/postgresql \\ --namespace cloudflow-prod \\ --values postgres-values.yaml \\ --wait ``` ### PgBouncer Configuration Deploy PgBouncer for connection pooling to handle up to 100 connections efficiently: ```yaml # pgbouncer.yaml apiVersion: v1 kind: ConfigMap metadata: name: pgbouncer-config namespace: cloudflow-prod data: pgbouncer.ini: | [databases] cloudflow = host=postgresql.cloudflow-prod.svc.cluster.local port=5432 dbname=cloudflow [pgbouncer] listen_addr = 0.0.0.0 listen_port = 5432 auth_type = md5 auth_file = /etc/pgbouncer/userlist.txt pool_mode = transaction max_client_conn = 1000 default_pool_size = 25 reserve_pool_size = 5 reserve_pool_timeout = 3 max_db_connections = 100 max_user_connections = 100 server_lifetime = 3600 server_idle_timeout = 600 log_connections = 1 log_disconnections = 1 log_pooler_errors = 1 --- apiVersion: apps/v1 kind: Deployment metadata: name: pgbouncer namespace: cloudflow-prod spec: replicas: 2 selector: matchLabels: app: pgbouncer template: metadata: labels: app: pgbouncer spec: containers: - name: pgbouncer image: pgbouncer/pgbouncer:1.21.0 ports: - containerPort: 5432 resources: requests: cpu: 500m memory: 512Mi limits: cpu: 1000m memory: 1Gi volumeMounts: - name: config mountPath: /etc/pgbouncer volumes: - name: config configMap: name: pgbouncer-config --- apiVersion: v1 kind: Service metadata: name: pgbouncer namespace: cloudflow-prod spec: selector: app: pgbouncer ports: - port: 5432 targetPort: 5432 type: ClusterIP ``` Apply the PgBouncer configuration: ```bash kubectl apply -f pgbouncer.yaml ``` ### Database Migrations Run database migrations before deploying new versions: ```bash # Create migration job kubectl create job cloudflow-migrate-$(date +%s) \\ --namespace cloudflow-prod \\ --image=123456789012.dkr.ecr.us-east-1.amazonaws.com/cloudflow:2.4.0 \\ -- npm run migrate # Monitor migration progress kubectl logs -f job/cloudflow-migrate-<timestamp> -n cloudflow-prod ``` --- ## Monitoring and Observability ### Prometheus Setup Install Prometheus using the kube-prometheus-stack: ```bash helm install prometheus prometheus-community/kube-prometheus-stack \\ --namespace monitoring \\ --create-namespace \\ --set prometheus.prometheusSpec.serviceMonitorSelectorNilUsesHelmValues=false \\",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_0",
              "content": "# CloudFlow Platform - System Architecture Overview **Document Version:** 2.3.1 **Last Updated:** January 15, 2026 **Owner:** Platform Architecture Team **Status:** Production ## Executive Summary CloudFlow is a distributed, cloud-native workflow automation platform designed to orchestrate complex business processes at scale. The platform processes over 2.5 million workflow executions daily with an average P99 latency of 180ms for API operations and 4.2 seconds for workflow execution. This document provides a comprehensive overview of the system architecture, including microservices design, data flow patterns, infrastructure decisions, and operational considerations. ## Table of Contents 1. [High-Level Architecture](#high-level-architecture) 2. [Microservices Breakdown](#microservices-breakdown) 3. [Data Flow Architecture](#data-flow-architecture) 4. [Database Architecture](#database-architecture) 5. [Message Queue Patterns](#message-queue-patterns) 6. [Caching Strategy](#caching-strategy) 7. [Security Architecture](#security-architecture) 8. [Performance Characteristics](#performance-characteristics) 9. [Disaster Recovery](#disaster-recovery) --- ## High-Level Architecture CloudFlow follows a microservices architecture pattern deployed across multiple availability zones in AWS. The platform is designed with the following core principles: - **Scalability**: Horizontal scaling for all services with auto-scaling groups - **Resilience**: Circuit breakers, bulkheads, and graceful degradation - **Observability**: Distributed tracing, centralized logging, and comprehensive metrics - **Security**: Zero-trust network architecture with mTLS encryption ### Architecture Diagram (Conceptual) ``` \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 Load Balancer (ALB) \u2502 \u2502 (TLS Termination - 443) \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u25bc \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 API Gateway Layer \u2502 \u2502 (Rate Limiting, Auth, Request Routing) \u2502 \u2514\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u2502 \u2502 \u2502 \u2502 \u25bc \u25bc \u25bc \u25bc \u25bc \u250c\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502Auth\u2502 \u2502Workflow\u2502 \u2502Scheduler\u2502 \u2502Notification\u2502 \u2502User Service\u2502 \u2502Svc \u2502 \u2502 Engine \u2502 \u2502 Service \u2502 \u2502 Service \u2502 \u2502 \u2502 \u2514\u2500\u252c\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u2502 \u2502 \u2502 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 \u2502 \u2502 \u25bc \u25bc \u25bc \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502PostgreSQL\u2502 \u2502 Redis \u2502 \u2502 Kafka \u2502 \u2502 Cluster \u2502 \u2502 Cluster \u2502 \u2502 Cluster \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 ``` ### Technology Stack - **Runtime**: Node.js 20.x (API Gateway, Workflow Engine), Go 1.21 (Auth Service, Scheduler) - **Container Orchestration**: Kubernetes 1.28 (EKS) - **Service Mesh**: Istio 1.20 for service-to-service communication - **Databases**: PostgreSQL 15.4, Redis 7.2 - **Message Broker**: Apache Kafka 3.6 - **Monitoring**: Prometheus, Grafana, Jaeger for distributed tracing - **Secrets Management**: HashiCorp Vault 1.15 --- ## Microservices Breakdown ### API Gateway **Purpose**: Single entry point for all client requests, providing authentication, rate limiting, request routing, and protocol translation.",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_5",
              "content": "exec_7h3j6k9m2n --show-bottlenecks ``` #### Solutions **1. Increase workflow timeout (if justified):** ```bash # Update workflow configuration cloudflow workflows update wf_9k2n4m8p1q \\ --timeout 7200 \\ --reason \"Large dataset processing requires extended time\" # Verify update cloudflow workflows get wf_9k2n4m8p1q | grep timeout ``` **2. Optimize slow steps:** ```bash # Enable parallel processing cloudflow workflows update wf_9k2n4m8p1q \\ --step data_transformation \\ --parallel-workers 8 \\ --batch-size 1000 # Add checkpointing for long operations cloudflow workflows update wf_9k2n4m8p1q \\ --step data_transformation \\ --enable-checkpointing \\ --checkpoint-interval 300 ``` **3. Split workflow into smaller workflows:** ```bash # Create sub-workflows cloudflow workflows create data-pipeline-part1 \\ --steps \"data_ingestion,data_validation\" \\ --timeout 1800 cloudflow workflows create data-pipeline-part2 \\ --steps \"data_transformation,data_export\" \\ --timeout 3600 \\ --trigger workflow_completed \\ --trigger-workflow data-pipeline-part1 ``` ### Retry Logic and Exponential Backoff CloudFlow implements automatic retry with exponential backoff for transient failures: - Max retries: 3 - Initial delay: 1 second - Backoff multiplier: 2 - Max delay: 60 seconds #### Retry Sequence ``` Attempt 1: Immediate Attempt 2: Wait 1s (2^0 * 1s) Attempt 3: Wait 2s (2^1 * 1s) Attempt 4: Wait 4s (2^2 * 1s) ``` #### Configuration ```bash # View current retry settings cloudflow workflows get wf_9k2n4m8p1q --format json | jq '.retry_policy' # Customize retry behavior cloudflow workflows update wf_9k2n4m8p1q \\ --retry-max-attempts 5 \\ --retry-initial-delay 2000 \\ --retry-backoff-multiplier 2 \\ --retry-max-delay 120000 \\ --retry-on-errors \"NETWORK_ERROR,TIMEOUT,RATE_LIMIT\" # Disable retry for specific step cloudflow workflows update wf_9k2n4m8p1q \\ --step data_export \\ --retry-enabled false ``` #### Monitoring Retries ```bash # List failed executions with retry information cloudflow workflows executions list \\ --status FAILED \\ --show-retries \\ --last 7d # Get retry history for specific execution cloudflow workflows executions retries exec_7h3j6k9m2n # Output: # Execution: exec_7h3j6k9m2n # Attempt 1: FAILED - NetworkError: Connection refused (delay: 0ms) # Attempt 2: FAILED - NetworkError: Connection timeout (delay: 1000ms) # Attempt 3: FAILED - NetworkError: Connection timeout (delay: 2000ms) # Attempt 4: FAILED - NetworkError: Connection timeout (delay: 4000ms) # Final Status: FAILED_AFTER_RETRIES ``` ### Workflow Step Failures #### Common Error Patterns **1.",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_10",
              "content": "- Execution start rate: 500 per second - Completion rate: 450 per second (average 2-second execution time) **Database**: - Read throughput: 50,000 queries per second (across replicas) - Write throughput: 15,000 transactions per second - Connection capacity: 2,000 concurrent connections **Kafka**: - Message ingestion: 100,000 messages per second - Consumer throughput: 80,000 messages per second (aggregated) - End-to-end latency: < 100ms (P99) ### Resource Utilization **CPU Utilization** (Target: 60-70% average): - API Gateway: 55% average, 80% peak - Workflow Engine: 65% average, 85% peak - Auth Service: 40% average, 60% peak **Memory Utilization** (Target: < 80%): - API Gateway: 2.5GB average per pod (4GB allocated) - Workflow Engine: 6GB average per pod (8GB allocated) - Notification Service: 2.8GB average per pod (4GB allocated) **Network Throughput**: - Ingress: 2 Gbps average, 5 Gbps peak - Egress: 1.5 Gbps average, 4 Gbps peak - Internal (service mesh): 8 Gbps average ### Monitoring & Alerting **Key Metrics** (Monitored via Prometheus): - Request rate (per service, per endpoint) - Error rate (4xx, 5xx responses) - Latency percentiles (P50, P95, P99) - Resource utilization (CPU, memory, disk) - Cache hit/miss ratios - Database connection pool saturation - Kafka consumer lag **Alerts** (via PagerDuty): - P1 (Immediate): API error rate > 1%, database connection failure - P2 (< 15 min): P99 latency > 500ms, cache hit rate < 80% - P3 (< 1 hour): Resource utilization > 85%, replica count < minimum **SLI/SLO/SLA**: - SLI: API success rate (non-5xx responses) - SLO: 99.9% uptime per month (43 minutes downtime allowance) - SLA: 99.5% uptime guarantee (customer-facing SLA with credits) --- ## Disaster Recovery ### Recovery Objectives **RPO (Recovery Point Objective): 1 hour** - Maximum acceptable data loss: 1 hour of transactions - Achieved through: Continuous database replication + hourly snapshots - Kafka retention: 7 days allows event replay **RTO (Recovery Time Objective): 4 hours** - Maximum acceptable downtime: 4 hours for full system recovery - Includes: Failover, data verification, and service restoration - Automated runbooks reduce RTO to < 2 hours for common scenarios ### High Availability Architecture **Multi-AZ Deployment**: ``` Region: us-east-1 AZ-1a: AZ-1b: AZ-1c: - API Gateway (4) - API Gateway (4) - API Gateway (4) - Workflow Engine (6) - Workflow Engine (5) - Workflow Engine (5) - Auth Service (3) - Auth Service (3) - Auth",
              "score": null
            }
          ],
          "latency_ms": 956.5919770029723,
          "query_id": "realistic_009",
          "dimension": "negation",
          "query": "Why are we getting alerts at 200ms latency?"
        },
        {
          "key_facts": [
            "RPO (Recovery Point Objective): 1 hour",
            "RTO (Recovery Time Objective): 4 hours"
          ],
          "found_facts": [
            "RPO (Recovery Point Objective): 1 hour",
            "RTO (Recovery Time Objective): 4 hours"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "architecture_overview",
            "deployment_guide"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_11",
              "content": "Service (2) - PostgreSQL Primary - PostgreSQL Replica - PostgreSQL Replica - Redis Primary (2) - Redis Replica (2) - Redis Replica (2) - Kafka Broker (2) - Kafka Broker (2) - Kafka Broker (1) ``` **Automatic Failover**: - Database: 30-60 seconds (automatic promotion of replica) - Redis: < 10 seconds (Sentinel-based failover) - Kafka: < 30 seconds (controller election) - Services: Kubernetes health checks with 10-second liveness probes ### Backup Strategy **Database Backups**: - Automated snapshots: Daily at 02:00 UTC - Retention: 30 days for daily, 90 days for monthly - Cross-region replication: Async replication to us-west-2 (15-minute lag) - Backup verification: Weekly automated restore test in staging environment **Backup Schedule**: ``` Daily: Full snapshot \u2192 S3 (encrypted) Hourly: WAL archives \u2192 S3 (point-in-time recovery) Weekly: Backup validation test Monthly: Long-term archive to Glacier ``` **Configuration Backups**: - Kubernetes manifests: Stored in Git (GitOps with ArgoCD) - Vault secrets: Automated snapshot every 6 hours - Kafka topic configurations: Exported daily to S3 ### Disaster Recovery Procedures **Scenario 1: Single AZ Failure** - Detection: Health checks fail for entire AZ (< 30 seconds) - Action: Traffic automatically routed to healthy AZs by ALB - Recovery time: < 5 minutes (no manual intervention) - Data loss: None (multi-AZ replication) **Scenario 2: Database Primary Failure** - Detection: Health check fails for primary database (< 30 seconds) - Action: Automatic promotion of read replica to primary - Recovery time: 30-60 seconds - Data loss: Minimal (< 1 second due to synchronous replication) **Scenario 3: Full Region Failure** - Detection: Multiple health check failures across all AZs (< 2 minutes) - Action: Manual failover to DR region (us-west-2) - Procedure: 1. Update DNS to point to DR region (TTL: 60 seconds) 2. Promote DR database replica to primary 3. Scale up DR region services to production capacity 4. Verify data consistency and integrity 5. Update monitoring dashboards - Recovery time: 2-4 hours (includes verification) - Data loss: < 1 hour (last cross-region replication) **Scenario 4: Data Corruption** - Detection: Data validation checks or user report - Action: Point-in-time recovery from WAL archives - Procedure: 1. Identify corruption time window 2. Restore from snapshot prior to corruption 3. Replay WAL logs up to corruption point 4. Verify data integrity 5.",
              "score": null
            },
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_5",
              "content": "Configure log aggregation: ```bash # Install Fluent Bit for log forwarding helm install fluent-bit fluent/fluent-bit \\ --namespace logging \\ --create-namespace \\ --set cloudWatch.enabled=true \\ --set cloudWatch.region=us-east-1 \\ --set cloudWatch.logGroupName=/aws/eks/cloudflow-production/application ``` Query logs using CloudWatch Insights: ```sql fields @timestamp, @message, level, requestId, userId | filter namespace = \"cloudflow-prod\" | filter level = \"error\" | sort @timestamp desc | limit 100 ``` --- ## Backup and Disaster Recovery ### Database Backup Strategy CloudFlow implements a comprehensive backup strategy with the following retention policy: - **Daily snapshots**: Retained for 30 days - **Weekly backups**: Retained for 90 days - **Monthly backups**: Retained for 1 year #### Automated Backup with Velero Install Velero for cluster-wide backups: ```bash velero install \\ --provider aws \\ --plugins velero/velero-plugin-for-aws:v1.8.0 \\ --bucket cloudflow-velero-backups \\ --backup-location-config region=us-east-1 \\ --snapshot-location-config region=us-east-1 \\ --secret-file ./credentials-velero ``` Create a daily backup schedule: ```yaml # backup-schedule.yaml apiVersion: velero.io/v1 kind: Schedule metadata: name: cloudflow-daily-backup namespace: velero spec: schedule: \"0 2 * * *\" # 2 AM UTC daily template: includedNamespaces: - cloudflow-prod ttl: 720h0m0s # 30 days storageLocation: default snapshotVolumes: true ``` Apply the backup schedule: ```bash kubectl apply -f backup-schedule.yaml ``` #### PostgreSQL Backup Configure PostgreSQL continuous archiving with WAL-G: ```bash # Create backup cronjob kubectl apply -f - <<EOF apiVersion: batch/v1 kind: CronJob metadata: name: postgres-backup namespace: cloudflow-prod spec: schedule: \"0 1 * * *\" # 1 AM UTC daily jobTemplate: spec: template: spec: containers: - name: backup image: postgres:14 env: - name: PGPASSWORD valueFrom: secretKeyRef: name: postgres-credentials key: password command: - /bin/bash - -c - | TIMESTAMP=\\$(date +%Y%m%d_%H%M%S) pg_dump -h postgresql.cloudflow-prod.svc.cluster.local \\ -U cloudflow -d cloudflow \\ | gzip > /backups/cloudflow_\\${TIMESTAMP}.sql.gz aws s3 cp /backups/cloudflow_\\${TIMESTAMP}.sql.gz \\ s3://cloudflow-db-backups/daily/ volumeMounts: - name: backup-storage mountPath: /backups volumes: - name: backup-storage emptyDir: {} restartPolicy: OnFailure EOF ``` ### Disaster Recovery Procedure In case of catastrophic failure, follow these steps to restore CloudFlow: 1. **Restore EKS Cluster** (if necessary): ```bash eksctl create cluster -f cluster-config.yaml ``` 2. **Restore Velero Backup**: ```bash # List available backups velero backup get # Restore from backup velero restore create --from-backup cloudflow-daily-backup-20260124 ``` 3. **Restore Database**: ```bash # Download backup from S3 aws s3 cp s3://cloudflow-db-backups/daily/cloudflow_20260124_010000.sql.gz . # Restore database gunzip -c cloudflow_20260124_010000.sql.gz | \\ psql -h postgresql.cloudflow-prod.svc.cluster.local -U cloudflow -d cloudflow ``` 4.",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_10",
              "content": "- Execution start rate: 500 per second - Completion rate: 450 per second (average 2-second execution time) **Database**: - Read throughput: 50,000 queries per second (across replicas) - Write throughput: 15,000 transactions per second - Connection capacity: 2,000 concurrent connections **Kafka**: - Message ingestion: 100,000 messages per second - Consumer throughput: 80,000 messages per second (aggregated) - End-to-end latency: < 100ms (P99) ### Resource Utilization **CPU Utilization** (Target: 60-70% average): - API Gateway: 55% average, 80% peak - Workflow Engine: 65% average, 85% peak - Auth Service: 40% average, 60% peak **Memory Utilization** (Target: < 80%): - API Gateway: 2.5GB average per pod (4GB allocated) - Workflow Engine: 6GB average per pod (8GB allocated) - Notification Service: 2.8GB average per pod (4GB allocated) **Network Throughput**: - Ingress: 2 Gbps average, 5 Gbps peak - Egress: 1.5 Gbps average, 4 Gbps peak - Internal (service mesh): 8 Gbps average ### Monitoring & Alerting **Key Metrics** (Monitored via Prometheus): - Request rate (per service, per endpoint) - Error rate (4xx, 5xx responses) - Latency percentiles (P50, P95, P99) - Resource utilization (CPU, memory, disk) - Cache hit/miss ratios - Database connection pool saturation - Kafka consumer lag **Alerts** (via PagerDuty): - P1 (Immediate): API error rate > 1%, database connection failure - P2 (< 15 min): P99 latency > 500ms, cache hit rate < 80% - P3 (< 1 hour): Resource utilization > 85%, replica count < minimum **SLI/SLO/SLA**: - SLI: API success rate (non-5xx responses) - SLO: 99.9% uptime per month (43 minutes downtime allowance) - SLA: 99.5% uptime guarantee (customer-facing SLA with credits) --- ## Disaster Recovery ### Recovery Objectives **RPO (Recovery Point Objective): 1 hour** - Maximum acceptable data loss: 1 hour of transactions - Achieved through: Continuous database replication + hourly snapshots - Kafka retention: 7 days allows event replay **RTO (Recovery Time Objective): 4 hours** - Maximum acceptable downtime: 4 hours for full system recovery - Includes: Failover, data verification, and service restoration - Automated runbooks reduce RTO to < 2 hours for common scenarios ### High Availability Architecture **Multi-AZ Deployment**: ``` Region: us-east-1 AZ-1a: AZ-1b: AZ-1c: - API Gateway (4) - API Gateway (4) - API Gateway (4) - Workflow Engine (6) - Workflow Engine (5) - Workflow Engine (5) - Auth Service (3) - Auth Service (3) - Auth",
              "score": null
            },
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_6",
              "content": "**Verify Service Health**: ```bash kubectl get pods -n cloudflow-prod curl https://api.cloudflow.io/health ``` **Recovery Time Objective (RTO)**: 4 hours **Recovery Point Objective (RPO)**: 24 hours --- ## Scaling and Performance ### Horizontal Pod Autoscaling CloudFlow is configured with Horizontal Pod Autoscaler (HPA) to automatically scale based on resource utilization: ```yaml # hpa.yaml apiVersion: autoscaling/v2 kind: HorizontalPodAutoscaler metadata: name: cloudflow-api-hpa namespace: cloudflow-prod spec: scaleTargetRef: apiVersion: apps/v1 kind: Deployment name: cloudflow-api minReplicas: 3 maxReplicas: 10 metrics: - type: Resource resource: name: cpu target: type: Utilization averageUtilization: 70 - type: Resource resource: name: memory target: type: Utilization averageUtilization: 80 behavior: scaleDown: stabilizationWindowSeconds: 300 policies: - type: Percent value: 50 periodSeconds: 60 scaleUp: stabilizationWindowSeconds: 60 policies: - type: Percent value: 100 periodSeconds: 30 - type: Pods value: 2 periodSeconds: 30 selectPolicy: Max ``` Monitor autoscaling events: ```bash kubectl get hpa -n cloudflow-prod -w kubectl describe hpa cloudflow-api-hpa -n cloudflow-prod ``` ### Performance Optimization #### Connection Pooling Ensure optimal database connection pooling settings in PgBouncer: - **Pool Mode**: Transaction (optimal for microservices) - **Default Pool Size**: 25 connections per user - **Max DB Connections**: 100 (matches PostgreSQL `max_connections`) #### Redis Caching Implement Redis caching for frequently accessed data: ```javascript // Example caching strategy const CACHE_TTL = { workflows: 300, // 5 minutes userSessions: 3600, // 1 hour apiResults: 60 // 1 minute }; ``` #### Load Testing Perform regular load testing to validate scaling configuration: ```bash # Install k6 for load testing kubectl apply -f - <<EOF apiVersion: v1 kind: ConfigMap metadata: name: loadtest-script namespace: cloudflow-prod data: script.js: | import http from 'k6/http'; import { check, sleep } from 'k6'; export let options = { stages: [ { duration: '2m', target: 100 }, { duration: '5m', target: 100 }, { duration: '2m', target: 200 }, { duration: '5m', target: 200 }, { duration: '2m', target: 0 }, ], thresholds: { http_req_duration: ['p(95)<500'], }, }; export default function () { let res = http.get('https://api.cloudflow.io/api/workflows'); check(res, { 'status is 200': (r) => r.status === 200 }); sleep(1); } EOF ``` **Performance Targets**: - P95 API latency: < 500ms - P99 API latency: < 1000ms - Error rate: < 0.1% - Throughput: > 1000 requests/second --- ## Security and Compliance ### Network Policies Implement network policies to restrict pod-to-pod communication: ```yaml # network-policy.yaml apiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: cloudflow-network-policy namespace: cloudflow-prod spec:",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_12",
              "content": "Resume normal operations - Recovery time: 1-3 hours depending on data volume - Data loss: None if corruption detected quickly ### Testing & Validation **DR Drill Schedule**: - Monthly: Automated failover test (single AZ failure simulation) - Quarterly: Full DR region failover (non-production hours) - Annually: Complete disaster simulation with all stakeholders **Last DR Test Results** (Dec 15, 2025): - Scenario: Full region failover - Actual RTO: 2 hours 23 minutes (target: 4 hours) - Actual RPO: 42 minutes (target: 1 hour) - Issues identified: DNS propagation slower than expected (resolved) - Success criteria: Met all recovery objectives ### Business Continuity **Communication Plan**: - Status page: status.cloudflow.com (updated every 15 minutes during incident) - Customer notifications: Email + SMS for all P1 incidents - Internal escalation: PagerDuty \u2192 Incident Commander \u2192 Engineering Manager \u2192 CTO **Data Retention Policy**: - Active data: 12 months in hot storage (PostgreSQL) - Archived data: 7 years in cold storage (S3 Glacier) - Audit logs: 7 years (compliance requirement) - Backup retention: 30 days standard, 90 days monthly snapshots --- ## Appendix ### Service Dependency Matrix | Service | Depends On | Critical Path | |---------|-----------|---------------| | API Gateway | Auth Service, Redis | Yes | | Auth Service | PostgreSQL, Redis, Vault | Yes | | Workflow Engine | PostgreSQL, Kafka, Redis | Yes | | Scheduler | PostgreSQL, Redis, Kafka | No | | Notification Service | Kafka, PostgreSQL, SendGrid, Twilio | No | ### Capacity Planning **Current Capacity** (Jan 2026): - Daily workflow executions: 2.5M - Active users: 150,000 - API requests per day: 50M - Database size: 1.2TB - Kafka throughput: 100K msg/sec peak **6-Month Projection** (Jul 2026): - Daily workflow executions: 4M (+60%) - Active users: 225,000 (+50%) - API requests per day: 80M (+60%) - Database size: 2TB (+67%) - Required scaling: +30% compute, +50% storage ### Contact Information - **Architecture Team**: architecture@cloudflow.internal - **On-Call Engineer**: PagerDuty escalation - **Security Team**: security@cloudflow.internal - **Documentation**: https://wiki.cloudflow.internal/architecture --- **Document Revision History**: - v2.3.1 (Jan 15, 2026): Updated performance metrics, added DR test results - v2.3.0 (Dec 1, 2025): Added multi-AZ deployment details - v2.2.0 (Oct 15, 2025): Security architecture overhaul - v2.1.0 (Sep 1, 2025): Initial Kafka migration documentation - v2.0.0 (Jul 1, 2025): Complete microservices rewrite",
              "score": null
            }
          ],
          "latency_ms": 1010.1801439996052,
          "query_id": "realistic_010",
          "dimension": "original",
          "query": "What are the disaster recovery RPO and RTO values?"
        },
        {
          "key_facts": [
            "RPO (Recovery Point Objective): 1 hour",
            "RTO (Recovery Time Objective): 4 hours"
          ],
          "found_facts": [
            "RPO (Recovery Point Objective): 1 hour",
            "RTO (Recovery Time Objective): 4 hours"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "architecture_overview",
            "deployment_guide"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_10",
              "content": "- Execution start rate: 500 per second - Completion rate: 450 per second (average 2-second execution time) **Database**: - Read throughput: 50,000 queries per second (across replicas) - Write throughput: 15,000 transactions per second - Connection capacity: 2,000 concurrent connections **Kafka**: - Message ingestion: 100,000 messages per second - Consumer throughput: 80,000 messages per second (aggregated) - End-to-end latency: < 100ms (P99) ### Resource Utilization **CPU Utilization** (Target: 60-70% average): - API Gateway: 55% average, 80% peak - Workflow Engine: 65% average, 85% peak - Auth Service: 40% average, 60% peak **Memory Utilization** (Target: < 80%): - API Gateway: 2.5GB average per pod (4GB allocated) - Workflow Engine: 6GB average per pod (8GB allocated) - Notification Service: 2.8GB average per pod (4GB allocated) **Network Throughput**: - Ingress: 2 Gbps average, 5 Gbps peak - Egress: 1.5 Gbps average, 4 Gbps peak - Internal (service mesh): 8 Gbps average ### Monitoring & Alerting **Key Metrics** (Monitored via Prometheus): - Request rate (per service, per endpoint) - Error rate (4xx, 5xx responses) - Latency percentiles (P50, P95, P99) - Resource utilization (CPU, memory, disk) - Cache hit/miss ratios - Database connection pool saturation - Kafka consumer lag **Alerts** (via PagerDuty): - P1 (Immediate): API error rate > 1%, database connection failure - P2 (< 15 min): P99 latency > 500ms, cache hit rate < 80% - P3 (< 1 hour): Resource utilization > 85%, replica count < minimum **SLI/SLO/SLA**: - SLI: API success rate (non-5xx responses) - SLO: 99.9% uptime per month (43 minutes downtime allowance) - SLA: 99.5% uptime guarantee (customer-facing SLA with credits) --- ## Disaster Recovery ### Recovery Objectives **RPO (Recovery Point Objective): 1 hour** - Maximum acceptable data loss: 1 hour of transactions - Achieved through: Continuous database replication + hourly snapshots - Kafka retention: 7 days allows event replay **RTO (Recovery Time Objective): 4 hours** - Maximum acceptable downtime: 4 hours for full system recovery - Includes: Failover, data verification, and service restoration - Automated runbooks reduce RTO to < 2 hours for common scenarios ### High Availability Architecture **Multi-AZ Deployment**: ``` Region: us-east-1 AZ-1a: AZ-1b: AZ-1c: - API Gateway (4) - API Gateway (4) - API Gateway (4) - Workflow Engine (6) - Workflow Engine (5) - Workflow Engine (5) - Auth Service (3) - Auth Service (3) - Auth",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_11",
              "content": "Service (2) - PostgreSQL Primary - PostgreSQL Replica - PostgreSQL Replica - Redis Primary (2) - Redis Replica (2) - Redis Replica (2) - Kafka Broker (2) - Kafka Broker (2) - Kafka Broker (1) ``` **Automatic Failover**: - Database: 30-60 seconds (automatic promotion of replica) - Redis: < 10 seconds (Sentinel-based failover) - Kafka: < 30 seconds (controller election) - Services: Kubernetes health checks with 10-second liveness probes ### Backup Strategy **Database Backups**: - Automated snapshots: Daily at 02:00 UTC - Retention: 30 days for daily, 90 days for monthly - Cross-region replication: Async replication to us-west-2 (15-minute lag) - Backup verification: Weekly automated restore test in staging environment **Backup Schedule**: ``` Daily: Full snapshot \u2192 S3 (encrypted) Hourly: WAL archives \u2192 S3 (point-in-time recovery) Weekly: Backup validation test Monthly: Long-term archive to Glacier ``` **Configuration Backups**: - Kubernetes manifests: Stored in Git (GitOps with ArgoCD) - Vault secrets: Automated snapshot every 6 hours - Kafka topic configurations: Exported daily to S3 ### Disaster Recovery Procedures **Scenario 1: Single AZ Failure** - Detection: Health checks fail for entire AZ (< 30 seconds) - Action: Traffic automatically routed to healthy AZs by ALB - Recovery time: < 5 minutes (no manual intervention) - Data loss: None (multi-AZ replication) **Scenario 2: Database Primary Failure** - Detection: Health check fails for primary database (< 30 seconds) - Action: Automatic promotion of read replica to primary - Recovery time: 30-60 seconds - Data loss: Minimal (< 1 second due to synchronous replication) **Scenario 3: Full Region Failure** - Detection: Multiple health check failures across all AZs (< 2 minutes) - Action: Manual failover to DR region (us-west-2) - Procedure: 1. Update DNS to point to DR region (TTL: 60 seconds) 2. Promote DR database replica to primary 3. Scale up DR region services to production capacity 4. Verify data consistency and integrity 5. Update monitoring dashboards - Recovery time: 2-4 hours (includes verification) - Data loss: < 1 hour (last cross-region replication) **Scenario 4: Data Corruption** - Detection: Data validation checks or user report - Action: Point-in-time recovery from WAL archives - Procedure: 1. Identify corruption time window 2. Restore from snapshot prior to corruption 3. Replay WAL logs up to corruption point 4. Verify data integrity 5.",
              "score": null
            },
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_5",
              "content": "Configure log aggregation: ```bash # Install Fluent Bit for log forwarding helm install fluent-bit fluent/fluent-bit \\ --namespace logging \\ --create-namespace \\ --set cloudWatch.enabled=true \\ --set cloudWatch.region=us-east-1 \\ --set cloudWatch.logGroupName=/aws/eks/cloudflow-production/application ``` Query logs using CloudWatch Insights: ```sql fields @timestamp, @message, level, requestId, userId | filter namespace = \"cloudflow-prod\" | filter level = \"error\" | sort @timestamp desc | limit 100 ``` --- ## Backup and Disaster Recovery ### Database Backup Strategy CloudFlow implements a comprehensive backup strategy with the following retention policy: - **Daily snapshots**: Retained for 30 days - **Weekly backups**: Retained for 90 days - **Monthly backups**: Retained for 1 year #### Automated Backup with Velero Install Velero for cluster-wide backups: ```bash velero install \\ --provider aws \\ --plugins velero/velero-plugin-for-aws:v1.8.0 \\ --bucket cloudflow-velero-backups \\ --backup-location-config region=us-east-1 \\ --snapshot-location-config region=us-east-1 \\ --secret-file ./credentials-velero ``` Create a daily backup schedule: ```yaml # backup-schedule.yaml apiVersion: velero.io/v1 kind: Schedule metadata: name: cloudflow-daily-backup namespace: velero spec: schedule: \"0 2 * * *\" # 2 AM UTC daily template: includedNamespaces: - cloudflow-prod ttl: 720h0m0s # 30 days storageLocation: default snapshotVolumes: true ``` Apply the backup schedule: ```bash kubectl apply -f backup-schedule.yaml ``` #### PostgreSQL Backup Configure PostgreSQL continuous archiving with WAL-G: ```bash # Create backup cronjob kubectl apply -f - <<EOF apiVersion: batch/v1 kind: CronJob metadata: name: postgres-backup namespace: cloudflow-prod spec: schedule: \"0 1 * * *\" # 1 AM UTC daily jobTemplate: spec: template: spec: containers: - name: backup image: postgres:14 env: - name: PGPASSWORD valueFrom: secretKeyRef: name: postgres-credentials key: password command: - /bin/bash - -c - | TIMESTAMP=\\$(date +%Y%m%d_%H%M%S) pg_dump -h postgresql.cloudflow-prod.svc.cluster.local \\ -U cloudflow -d cloudflow \\ | gzip > /backups/cloudflow_\\${TIMESTAMP}.sql.gz aws s3 cp /backups/cloudflow_\\${TIMESTAMP}.sql.gz \\ s3://cloudflow-db-backups/daily/ volumeMounts: - name: backup-storage mountPath: /backups volumes: - name: backup-storage emptyDir: {} restartPolicy: OnFailure EOF ``` ### Disaster Recovery Procedure In case of catastrophic failure, follow these steps to restore CloudFlow: 1. **Restore EKS Cluster** (if necessary): ```bash eksctl create cluster -f cluster-config.yaml ``` 2. **Restore Velero Backup**: ```bash # List available backups velero backup get # Restore from backup velero restore create --from-backup cloudflow-daily-backup-20260124 ``` 3. **Restore Database**: ```bash # Download backup from S3 aws s3 cp s3://cloudflow-db-backups/daily/cloudflow_20260124_010000.sql.gz . # Restore database gunzip -c cloudflow_20260124_010000.sql.gz | \\ psql -h postgresql.cloudflow-prod.svc.cluster.local -U cloudflow -d cloudflow ``` 4.",
              "score": null
            },
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_6",
              "content": "**Verify Service Health**: ```bash kubectl get pods -n cloudflow-prod curl https://api.cloudflow.io/health ``` **Recovery Time Objective (RTO)**: 4 hours **Recovery Point Objective (RPO)**: 24 hours --- ## Scaling and Performance ### Horizontal Pod Autoscaling CloudFlow is configured with Horizontal Pod Autoscaler (HPA) to automatically scale based on resource utilization: ```yaml # hpa.yaml apiVersion: autoscaling/v2 kind: HorizontalPodAutoscaler metadata: name: cloudflow-api-hpa namespace: cloudflow-prod spec: scaleTargetRef: apiVersion: apps/v1 kind: Deployment name: cloudflow-api minReplicas: 3 maxReplicas: 10 metrics: - type: Resource resource: name: cpu target: type: Utilization averageUtilization: 70 - type: Resource resource: name: memory target: type: Utilization averageUtilization: 80 behavior: scaleDown: stabilizationWindowSeconds: 300 policies: - type: Percent value: 50 periodSeconds: 60 scaleUp: stabilizationWindowSeconds: 60 policies: - type: Percent value: 100 periodSeconds: 30 - type: Pods value: 2 periodSeconds: 30 selectPolicy: Max ``` Monitor autoscaling events: ```bash kubectl get hpa -n cloudflow-prod -w kubectl describe hpa cloudflow-api-hpa -n cloudflow-prod ``` ### Performance Optimization #### Connection Pooling Ensure optimal database connection pooling settings in PgBouncer: - **Pool Mode**: Transaction (optimal for microservices) - **Default Pool Size**: 25 connections per user - **Max DB Connections**: 100 (matches PostgreSQL `max_connections`) #### Redis Caching Implement Redis caching for frequently accessed data: ```javascript // Example caching strategy const CACHE_TTL = { workflows: 300, // 5 minutes userSessions: 3600, // 1 hour apiResults: 60 // 1 minute }; ``` #### Load Testing Perform regular load testing to validate scaling configuration: ```bash # Install k6 for load testing kubectl apply -f - <<EOF apiVersion: v1 kind: ConfigMap metadata: name: loadtest-script namespace: cloudflow-prod data: script.js: | import http from 'k6/http'; import { check, sleep } from 'k6'; export let options = { stages: [ { duration: '2m', target: 100 }, { duration: '5m', target: 100 }, { duration: '2m', target: 200 }, { duration: '5m', target: 200 }, { duration: '2m', target: 0 }, ], thresholds: { http_req_duration: ['p(95)<500'], }, }; export default function () { let res = http.get('https://api.cloudflow.io/api/workflows'); check(res, { 'status is 200': (r) => r.status === 200 }); sleep(1); } EOF ``` **Performance Targets**: - P95 API latency: < 500ms - P99 API latency: < 1000ms - Error rate: < 0.1% - Throughput: > 1000 requests/second --- ## Security and Compliance ### Network Policies Implement network policies to restrict pod-to-pod communication: ```yaml # network-policy.yaml apiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: cloudflow-network-policy namespace: cloudflow-prod spec:",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_12",
              "content": "Resume normal operations - Recovery time: 1-3 hours depending on data volume - Data loss: None if corruption detected quickly ### Testing & Validation **DR Drill Schedule**: - Monthly: Automated failover test (single AZ failure simulation) - Quarterly: Full DR region failover (non-production hours) - Annually: Complete disaster simulation with all stakeholders **Last DR Test Results** (Dec 15, 2025): - Scenario: Full region failover - Actual RTO: 2 hours 23 minutes (target: 4 hours) - Actual RPO: 42 minutes (target: 1 hour) - Issues identified: DNS propagation slower than expected (resolved) - Success criteria: Met all recovery objectives ### Business Continuity **Communication Plan**: - Status page: status.cloudflow.com (updated every 15 minutes during incident) - Customer notifications: Email + SMS for all P1 incidents - Internal escalation: PagerDuty \u2192 Incident Commander \u2192 Engineering Manager \u2192 CTO **Data Retention Policy**: - Active data: 12 months in hot storage (PostgreSQL) - Archived data: 7 years in cold storage (S3 Glacier) - Audit logs: 7 years (compliance requirement) - Backup retention: 30 days standard, 90 days monthly snapshots --- ## Appendix ### Service Dependency Matrix | Service | Depends On | Critical Path | |---------|-----------|---------------| | API Gateway | Auth Service, Redis | Yes | | Auth Service | PostgreSQL, Redis, Vault | Yes | | Workflow Engine | PostgreSQL, Kafka, Redis | Yes | | Scheduler | PostgreSQL, Redis, Kafka | No | | Notification Service | Kafka, PostgreSQL, SendGrid, Twilio | No | ### Capacity Planning **Current Capacity** (Jan 2026): - Daily workflow executions: 2.5M - Active users: 150,000 - API requests per day: 50M - Database size: 1.2TB - Kafka throughput: 100K msg/sec peak **6-Month Projection** (Jul 2026): - Daily workflow executions: 4M (+60%) - Active users: 225,000 (+50%) - API requests per day: 80M (+60%) - Database size: 2TB (+67%) - Required scaling: +30% compute, +50% storage ### Contact Information - **Architecture Team**: architecture@cloudflow.internal - **On-Call Engineer**: PagerDuty escalation - **Security Team**: security@cloudflow.internal - **Documentation**: https://wiki.cloudflow.internal/architecture --- **Document Revision History**: - v2.3.1 (Jan 15, 2026): Updated performance metrics, added DR test results - v2.3.0 (Dec 1, 2025): Added multi-AZ deployment details - v2.2.0 (Oct 15, 2025): Security architecture overhaul - v2.1.0 (Sep 1, 2025): Initial Kafka migration documentation - v2.0.0 (Jul 1, 2025): Complete microservices rewrite",
              "score": null
            }
          ],
          "latency_ms": 1053.6103369995544,
          "query_id": "realistic_010",
          "dimension": "synonym",
          "query": "What's the maximum data loss and recovery time?"
        },
        {
          "key_facts": [
            "RPO (Recovery Point Objective): 1 hour",
            "RTO (Recovery Time Objective): 4 hours"
          ],
          "found_facts": [
            "RPO (Recovery Point Objective): 1 hour",
            "RTO (Recovery Time Objective): 4 hours"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "architecture_overview",
            "deployment_guide"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_11",
              "content": "Service (2) - PostgreSQL Primary - PostgreSQL Replica - PostgreSQL Replica - Redis Primary (2) - Redis Replica (2) - Redis Replica (2) - Kafka Broker (2) - Kafka Broker (2) - Kafka Broker (1) ``` **Automatic Failover**: - Database: 30-60 seconds (automatic promotion of replica) - Redis: < 10 seconds (Sentinel-based failover) - Kafka: < 30 seconds (controller election) - Services: Kubernetes health checks with 10-second liveness probes ### Backup Strategy **Database Backups**: - Automated snapshots: Daily at 02:00 UTC - Retention: 30 days for daily, 90 days for monthly - Cross-region replication: Async replication to us-west-2 (15-minute lag) - Backup verification: Weekly automated restore test in staging environment **Backup Schedule**: ``` Daily: Full snapshot \u2192 S3 (encrypted) Hourly: WAL archives \u2192 S3 (point-in-time recovery) Weekly: Backup validation test Monthly: Long-term archive to Glacier ``` **Configuration Backups**: - Kubernetes manifests: Stored in Git (GitOps with ArgoCD) - Vault secrets: Automated snapshot every 6 hours - Kafka topic configurations: Exported daily to S3 ### Disaster Recovery Procedures **Scenario 1: Single AZ Failure** - Detection: Health checks fail for entire AZ (< 30 seconds) - Action: Traffic automatically routed to healthy AZs by ALB - Recovery time: < 5 minutes (no manual intervention) - Data loss: None (multi-AZ replication) **Scenario 2: Database Primary Failure** - Detection: Health check fails for primary database (< 30 seconds) - Action: Automatic promotion of read replica to primary - Recovery time: 30-60 seconds - Data loss: Minimal (< 1 second due to synchronous replication) **Scenario 3: Full Region Failure** - Detection: Multiple health check failures across all AZs (< 2 minutes) - Action: Manual failover to DR region (us-west-2) - Procedure: 1. Update DNS to point to DR region (TTL: 60 seconds) 2. Promote DR database replica to primary 3. Scale up DR region services to production capacity 4. Verify data consistency and integrity 5. Update monitoring dashboards - Recovery time: 2-4 hours (includes verification) - Data loss: < 1 hour (last cross-region replication) **Scenario 4: Data Corruption** - Detection: Data validation checks or user report - Action: Point-in-time recovery from WAL archives - Procedure: 1. Identify corruption time window 2. Restore from snapshot prior to corruption 3. Replay WAL logs up to corruption point 4. Verify data integrity 5.",
              "score": null
            },
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_5",
              "content": "Configure log aggregation: ```bash # Install Fluent Bit for log forwarding helm install fluent-bit fluent/fluent-bit \\ --namespace logging \\ --create-namespace \\ --set cloudWatch.enabled=true \\ --set cloudWatch.region=us-east-1 \\ --set cloudWatch.logGroupName=/aws/eks/cloudflow-production/application ``` Query logs using CloudWatch Insights: ```sql fields @timestamp, @message, level, requestId, userId | filter namespace = \"cloudflow-prod\" | filter level = \"error\" | sort @timestamp desc | limit 100 ``` --- ## Backup and Disaster Recovery ### Database Backup Strategy CloudFlow implements a comprehensive backup strategy with the following retention policy: - **Daily snapshots**: Retained for 30 days - **Weekly backups**: Retained for 90 days - **Monthly backups**: Retained for 1 year #### Automated Backup with Velero Install Velero for cluster-wide backups: ```bash velero install \\ --provider aws \\ --plugins velero/velero-plugin-for-aws:v1.8.0 \\ --bucket cloudflow-velero-backups \\ --backup-location-config region=us-east-1 \\ --snapshot-location-config region=us-east-1 \\ --secret-file ./credentials-velero ``` Create a daily backup schedule: ```yaml # backup-schedule.yaml apiVersion: velero.io/v1 kind: Schedule metadata: name: cloudflow-daily-backup namespace: velero spec: schedule: \"0 2 * * *\" # 2 AM UTC daily template: includedNamespaces: - cloudflow-prod ttl: 720h0m0s # 30 days storageLocation: default snapshotVolumes: true ``` Apply the backup schedule: ```bash kubectl apply -f backup-schedule.yaml ``` #### PostgreSQL Backup Configure PostgreSQL continuous archiving with WAL-G: ```bash # Create backup cronjob kubectl apply -f - <<EOF apiVersion: batch/v1 kind: CronJob metadata: name: postgres-backup namespace: cloudflow-prod spec: schedule: \"0 1 * * *\" # 1 AM UTC daily jobTemplate: spec: template: spec: containers: - name: backup image: postgres:14 env: - name: PGPASSWORD valueFrom: secretKeyRef: name: postgres-credentials key: password command: - /bin/bash - -c - | TIMESTAMP=\\$(date +%Y%m%d_%H%M%S) pg_dump -h postgresql.cloudflow-prod.svc.cluster.local \\ -U cloudflow -d cloudflow \\ | gzip > /backups/cloudflow_\\${TIMESTAMP}.sql.gz aws s3 cp /backups/cloudflow_\\${TIMESTAMP}.sql.gz \\ s3://cloudflow-db-backups/daily/ volumeMounts: - name: backup-storage mountPath: /backups volumes: - name: backup-storage emptyDir: {} restartPolicy: OnFailure EOF ``` ### Disaster Recovery Procedure In case of catastrophic failure, follow these steps to restore CloudFlow: 1. **Restore EKS Cluster** (if necessary): ```bash eksctl create cluster -f cluster-config.yaml ``` 2. **Restore Velero Backup**: ```bash # List available backups velero backup get # Restore from backup velero restore create --from-backup cloudflow-daily-backup-20260124 ``` 3. **Restore Database**: ```bash # Download backup from S3 aws s3 cp s3://cloudflow-db-backups/daily/cloudflow_20260124_010000.sql.gz . # Restore database gunzip -c cloudflow_20260124_010000.sql.gz | \\ psql -h postgresql.cloudflow-prod.svc.cluster.local -U cloudflow -d cloudflow ``` 4.",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_10",
              "content": "- Execution start rate: 500 per second - Completion rate: 450 per second (average 2-second execution time) **Database**: - Read throughput: 50,000 queries per second (across replicas) - Write throughput: 15,000 transactions per second - Connection capacity: 2,000 concurrent connections **Kafka**: - Message ingestion: 100,000 messages per second - Consumer throughput: 80,000 messages per second (aggregated) - End-to-end latency: < 100ms (P99) ### Resource Utilization **CPU Utilization** (Target: 60-70% average): - API Gateway: 55% average, 80% peak - Workflow Engine: 65% average, 85% peak - Auth Service: 40% average, 60% peak **Memory Utilization** (Target: < 80%): - API Gateway: 2.5GB average per pod (4GB allocated) - Workflow Engine: 6GB average per pod (8GB allocated) - Notification Service: 2.8GB average per pod (4GB allocated) **Network Throughput**: - Ingress: 2 Gbps average, 5 Gbps peak - Egress: 1.5 Gbps average, 4 Gbps peak - Internal (service mesh): 8 Gbps average ### Monitoring & Alerting **Key Metrics** (Monitored via Prometheus): - Request rate (per service, per endpoint) - Error rate (4xx, 5xx responses) - Latency percentiles (P50, P95, P99) - Resource utilization (CPU, memory, disk) - Cache hit/miss ratios - Database connection pool saturation - Kafka consumer lag **Alerts** (via PagerDuty): - P1 (Immediate): API error rate > 1%, database connection failure - P2 (< 15 min): P99 latency > 500ms, cache hit rate < 80% - P3 (< 1 hour): Resource utilization > 85%, replica count < minimum **SLI/SLO/SLA**: - SLI: API success rate (non-5xx responses) - SLO: 99.9% uptime per month (43 minutes downtime allowance) - SLA: 99.5% uptime guarantee (customer-facing SLA with credits) --- ## Disaster Recovery ### Recovery Objectives **RPO (Recovery Point Objective): 1 hour** - Maximum acceptable data loss: 1 hour of transactions - Achieved through: Continuous database replication + hourly snapshots - Kafka retention: 7 days allows event replay **RTO (Recovery Time Objective): 4 hours** - Maximum acceptable downtime: 4 hours for full system recovery - Includes: Failover, data verification, and service restoration - Automated runbooks reduce RTO to < 2 hours for common scenarios ### High Availability Architecture **Multi-AZ Deployment**: ``` Region: us-east-1 AZ-1a: AZ-1b: AZ-1c: - API Gateway (4) - API Gateway (4) - API Gateway (4) - Workflow Engine (6) - Workflow Engine (5) - Workflow Engine (5) - Auth Service (3) - Auth Service (3) - Auth",
              "score": null
            },
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_6",
              "content": "**Verify Service Health**: ```bash kubectl get pods -n cloudflow-prod curl https://api.cloudflow.io/health ``` **Recovery Time Objective (RTO)**: 4 hours **Recovery Point Objective (RPO)**: 24 hours --- ## Scaling and Performance ### Horizontal Pod Autoscaling CloudFlow is configured with Horizontal Pod Autoscaler (HPA) to automatically scale based on resource utilization: ```yaml # hpa.yaml apiVersion: autoscaling/v2 kind: HorizontalPodAutoscaler metadata: name: cloudflow-api-hpa namespace: cloudflow-prod spec: scaleTargetRef: apiVersion: apps/v1 kind: Deployment name: cloudflow-api minReplicas: 3 maxReplicas: 10 metrics: - type: Resource resource: name: cpu target: type: Utilization averageUtilization: 70 - type: Resource resource: name: memory target: type: Utilization averageUtilization: 80 behavior: scaleDown: stabilizationWindowSeconds: 300 policies: - type: Percent value: 50 periodSeconds: 60 scaleUp: stabilizationWindowSeconds: 60 policies: - type: Percent value: 100 periodSeconds: 30 - type: Pods value: 2 periodSeconds: 30 selectPolicy: Max ``` Monitor autoscaling events: ```bash kubectl get hpa -n cloudflow-prod -w kubectl describe hpa cloudflow-api-hpa -n cloudflow-prod ``` ### Performance Optimization #### Connection Pooling Ensure optimal database connection pooling settings in PgBouncer: - **Pool Mode**: Transaction (optimal for microservices) - **Default Pool Size**: 25 connections per user - **Max DB Connections**: 100 (matches PostgreSQL `max_connections`) #### Redis Caching Implement Redis caching for frequently accessed data: ```javascript // Example caching strategy const CACHE_TTL = { workflows: 300, // 5 minutes userSessions: 3600, // 1 hour apiResults: 60 // 1 minute }; ``` #### Load Testing Perform regular load testing to validate scaling configuration: ```bash # Install k6 for load testing kubectl apply -f - <<EOF apiVersion: v1 kind: ConfigMap metadata: name: loadtest-script namespace: cloudflow-prod data: script.js: | import http from 'k6/http'; import { check, sleep } from 'k6'; export let options = { stages: [ { duration: '2m', target: 100 }, { duration: '5m', target: 100 }, { duration: '2m', target: 200 }, { duration: '5m', target: 200 }, { duration: '2m', target: 0 }, ], thresholds: { http_req_duration: ['p(95)<500'], }, }; export default function () { let res = http.get('https://api.cloudflow.io/api/workflows'); check(res, { 'status is 200': (r) => r.status === 200 }); sleep(1); } EOF ``` **Performance Targets**: - P95 API latency: < 500ms - P99 API latency: < 1000ms - Error rate: < 0.1% - Throughput: > 1000 requests/second --- ## Security and Compliance ### Network Policies Implement network policies to restrict pod-to-pod communication: ```yaml # network-policy.yaml apiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: cloudflow-network-policy namespace: cloudflow-prod spec:",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_12",
              "content": "Resume normal operations - Recovery time: 1-3 hours depending on data volume - Data loss: None if corruption detected quickly ### Testing & Validation **DR Drill Schedule**: - Monthly: Automated failover test (single AZ failure simulation) - Quarterly: Full DR region failover (non-production hours) - Annually: Complete disaster simulation with all stakeholders **Last DR Test Results** (Dec 15, 2025): - Scenario: Full region failover - Actual RTO: 2 hours 23 minutes (target: 4 hours) - Actual RPO: 42 minutes (target: 1 hour) - Issues identified: DNS propagation slower than expected (resolved) - Success criteria: Met all recovery objectives ### Business Continuity **Communication Plan**: - Status page: status.cloudflow.com (updated every 15 minutes during incident) - Customer notifications: Email + SMS for all P1 incidents - Internal escalation: PagerDuty \u2192 Incident Commander \u2192 Engineering Manager \u2192 CTO **Data Retention Policy**: - Active data: 12 months in hot storage (PostgreSQL) - Archived data: 7 years in cold storage (S3 Glacier) - Audit logs: 7 years (compliance requirement) - Backup retention: 30 days standard, 90 days monthly snapshots --- ## Appendix ### Service Dependency Matrix | Service | Depends On | Critical Path | |---------|-----------|---------------| | API Gateway | Auth Service, Redis | Yes | | Auth Service | PostgreSQL, Redis, Vault | Yes | | Workflow Engine | PostgreSQL, Kafka, Redis | Yes | | Scheduler | PostgreSQL, Redis, Kafka | No | | Notification Service | Kafka, PostgreSQL, SendGrid, Twilio | No | ### Capacity Planning **Current Capacity** (Jan 2026): - Daily workflow executions: 2.5M - Active users: 150,000 - API requests per day: 50M - Database size: 1.2TB - Kafka throughput: 100K msg/sec peak **6-Month Projection** (Jul 2026): - Daily workflow executions: 4M (+60%) - Active users: 225,000 (+50%) - API requests per day: 80M (+60%) - Database size: 2TB (+67%) - Required scaling: +30% compute, +50% storage ### Contact Information - **Architecture Team**: architecture@cloudflow.internal - **On-Call Engineer**: PagerDuty escalation - **Security Team**: security@cloudflow.internal - **Documentation**: https://wiki.cloudflow.internal/architecture --- **Document Revision History**: - v2.3.1 (Jan 15, 2026): Updated performance metrics, added DR test results - v2.3.0 (Dec 1, 2025): Added multi-AZ deployment details - v2.2.0 (Oct 15, 2025): Security architecture overhaul - v2.1.0 (Sep 1, 2025): Initial Kafka migration documentation - v2.0.0 (Jul 1, 2025): Complete microservices rewrite",
              "score": null
            }
          ],
          "latency_ms": 1113.9285509998444,
          "query_id": "realistic_010",
          "dimension": "problem",
          "query": "How long will it take to recover from a disaster?"
        },
        {
          "key_facts": [
            "RPO (Recovery Point Objective): 1 hour",
            "RTO (Recovery Time Objective): 4 hours"
          ],
          "found_facts": [
            "RPO (Recovery Point Objective): 1 hour",
            "RTO (Recovery Time Objective): 4 hours"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "architecture_overview",
            "deployment_guide"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_11",
              "content": "Service (2) - PostgreSQL Primary - PostgreSQL Replica - PostgreSQL Replica - Redis Primary (2) - Redis Replica (2) - Redis Replica (2) - Kafka Broker (2) - Kafka Broker (2) - Kafka Broker (1) ``` **Automatic Failover**: - Database: 30-60 seconds (automatic promotion of replica) - Redis: < 10 seconds (Sentinel-based failover) - Kafka: < 30 seconds (controller election) - Services: Kubernetes health checks with 10-second liveness probes ### Backup Strategy **Database Backups**: - Automated snapshots: Daily at 02:00 UTC - Retention: 30 days for daily, 90 days for monthly - Cross-region replication: Async replication to us-west-2 (15-minute lag) - Backup verification: Weekly automated restore test in staging environment **Backup Schedule**: ``` Daily: Full snapshot \u2192 S3 (encrypted) Hourly: WAL archives \u2192 S3 (point-in-time recovery) Weekly: Backup validation test Monthly: Long-term archive to Glacier ``` **Configuration Backups**: - Kubernetes manifests: Stored in Git (GitOps with ArgoCD) - Vault secrets: Automated snapshot every 6 hours - Kafka topic configurations: Exported daily to S3 ### Disaster Recovery Procedures **Scenario 1: Single AZ Failure** - Detection: Health checks fail for entire AZ (< 30 seconds) - Action: Traffic automatically routed to healthy AZs by ALB - Recovery time: < 5 minutes (no manual intervention) - Data loss: None (multi-AZ replication) **Scenario 2: Database Primary Failure** - Detection: Health check fails for primary database (< 30 seconds) - Action: Automatic promotion of read replica to primary - Recovery time: 30-60 seconds - Data loss: Minimal (< 1 second due to synchronous replication) **Scenario 3: Full Region Failure** - Detection: Multiple health check failures across all AZs (< 2 minutes) - Action: Manual failover to DR region (us-west-2) - Procedure: 1. Update DNS to point to DR region (TTL: 60 seconds) 2. Promote DR database replica to primary 3. Scale up DR region services to production capacity 4. Verify data consistency and integrity 5. Update monitoring dashboards - Recovery time: 2-4 hours (includes verification) - Data loss: < 1 hour (last cross-region replication) **Scenario 4: Data Corruption** - Detection: Data validation checks or user report - Action: Point-in-time recovery from WAL archives - Procedure: 1. Identify corruption time window 2. Restore from snapshot prior to corruption 3. Replay WAL logs up to corruption point 4. Verify data integrity 5.",
              "score": null
            },
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_5",
              "content": "Configure log aggregation: ```bash # Install Fluent Bit for log forwarding helm install fluent-bit fluent/fluent-bit \\ --namespace logging \\ --create-namespace \\ --set cloudWatch.enabled=true \\ --set cloudWatch.region=us-east-1 \\ --set cloudWatch.logGroupName=/aws/eks/cloudflow-production/application ``` Query logs using CloudWatch Insights: ```sql fields @timestamp, @message, level, requestId, userId | filter namespace = \"cloudflow-prod\" | filter level = \"error\" | sort @timestamp desc | limit 100 ``` --- ## Backup and Disaster Recovery ### Database Backup Strategy CloudFlow implements a comprehensive backup strategy with the following retention policy: - **Daily snapshots**: Retained for 30 days - **Weekly backups**: Retained for 90 days - **Monthly backups**: Retained for 1 year #### Automated Backup with Velero Install Velero for cluster-wide backups: ```bash velero install \\ --provider aws \\ --plugins velero/velero-plugin-for-aws:v1.8.0 \\ --bucket cloudflow-velero-backups \\ --backup-location-config region=us-east-1 \\ --snapshot-location-config region=us-east-1 \\ --secret-file ./credentials-velero ``` Create a daily backup schedule: ```yaml # backup-schedule.yaml apiVersion: velero.io/v1 kind: Schedule metadata: name: cloudflow-daily-backup namespace: velero spec: schedule: \"0 2 * * *\" # 2 AM UTC daily template: includedNamespaces: - cloudflow-prod ttl: 720h0m0s # 30 days storageLocation: default snapshotVolumes: true ``` Apply the backup schedule: ```bash kubectl apply -f backup-schedule.yaml ``` #### PostgreSQL Backup Configure PostgreSQL continuous archiving with WAL-G: ```bash # Create backup cronjob kubectl apply -f - <<EOF apiVersion: batch/v1 kind: CronJob metadata: name: postgres-backup namespace: cloudflow-prod spec: schedule: \"0 1 * * *\" # 1 AM UTC daily jobTemplate: spec: template: spec: containers: - name: backup image: postgres:14 env: - name: PGPASSWORD valueFrom: secretKeyRef: name: postgres-credentials key: password command: - /bin/bash - -c - | TIMESTAMP=\\$(date +%Y%m%d_%H%M%S) pg_dump -h postgresql.cloudflow-prod.svc.cluster.local \\ -U cloudflow -d cloudflow \\ | gzip > /backups/cloudflow_\\${TIMESTAMP}.sql.gz aws s3 cp /backups/cloudflow_\\${TIMESTAMP}.sql.gz \\ s3://cloudflow-db-backups/daily/ volumeMounts: - name: backup-storage mountPath: /backups volumes: - name: backup-storage emptyDir: {} restartPolicy: OnFailure EOF ``` ### Disaster Recovery Procedure In case of catastrophic failure, follow these steps to restore CloudFlow: 1. **Restore EKS Cluster** (if necessary): ```bash eksctl create cluster -f cluster-config.yaml ``` 2. **Restore Velero Backup**: ```bash # List available backups velero backup get # Restore from backup velero restore create --from-backup cloudflow-daily-backup-20260124 ``` 3. **Restore Database**: ```bash # Download backup from S3 aws s3 cp s3://cloudflow-db-backups/daily/cloudflow_20260124_010000.sql.gz . # Restore database gunzip -c cloudflow_20260124_010000.sql.gz | \\ psql -h postgresql.cloudflow-prod.svc.cluster.local -U cloudflow -d cloudflow ``` 4.",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_12",
              "content": "Resume normal operations - Recovery time: 1-3 hours depending on data volume - Data loss: None if corruption detected quickly ### Testing & Validation **DR Drill Schedule**: - Monthly: Automated failover test (single AZ failure simulation) - Quarterly: Full DR region failover (non-production hours) - Annually: Complete disaster simulation with all stakeholders **Last DR Test Results** (Dec 15, 2025): - Scenario: Full region failover - Actual RTO: 2 hours 23 minutes (target: 4 hours) - Actual RPO: 42 minutes (target: 1 hour) - Issues identified: DNS propagation slower than expected (resolved) - Success criteria: Met all recovery objectives ### Business Continuity **Communication Plan**: - Status page: status.cloudflow.com (updated every 15 minutes during incident) - Customer notifications: Email + SMS for all P1 incidents - Internal escalation: PagerDuty \u2192 Incident Commander \u2192 Engineering Manager \u2192 CTO **Data Retention Policy**: - Active data: 12 months in hot storage (PostgreSQL) - Archived data: 7 years in cold storage (S3 Glacier) - Audit logs: 7 years (compliance requirement) - Backup retention: 30 days standard, 90 days monthly snapshots --- ## Appendix ### Service Dependency Matrix | Service | Depends On | Critical Path | |---------|-----------|---------------| | API Gateway | Auth Service, Redis | Yes | | Auth Service | PostgreSQL, Redis, Vault | Yes | | Workflow Engine | PostgreSQL, Kafka, Redis | Yes | | Scheduler | PostgreSQL, Redis, Kafka | No | | Notification Service | Kafka, PostgreSQL, SendGrid, Twilio | No | ### Capacity Planning **Current Capacity** (Jan 2026): - Daily workflow executions: 2.5M - Active users: 150,000 - API requests per day: 50M - Database size: 1.2TB - Kafka throughput: 100K msg/sec peak **6-Month Projection** (Jul 2026): - Daily workflow executions: 4M (+60%) - Active users: 225,000 (+50%) - API requests per day: 80M (+60%) - Database size: 2TB (+67%) - Required scaling: +30% compute, +50% storage ### Contact Information - **Architecture Team**: architecture@cloudflow.internal - **On-Call Engineer**: PagerDuty escalation - **Security Team**: security@cloudflow.internal - **Documentation**: https://wiki.cloudflow.internal/architecture --- **Document Revision History**: - v2.3.1 (Jan 15, 2026): Updated performance metrics, added DR test results - v2.3.0 (Dec 1, 2025): Added multi-AZ deployment details - v2.2.0 (Oct 15, 2025): Security architecture overhaul - v2.1.0 (Sep 1, 2025): Initial Kafka migration documentation - v2.0.0 (Jul 1, 2025): Complete microservices rewrite",
              "score": null
            },
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_6",
              "content": "**Verify Service Health**: ```bash kubectl get pods -n cloudflow-prod curl https://api.cloudflow.io/health ``` **Recovery Time Objective (RTO)**: 4 hours **Recovery Point Objective (RPO)**: 24 hours --- ## Scaling and Performance ### Horizontal Pod Autoscaling CloudFlow is configured with Horizontal Pod Autoscaler (HPA) to automatically scale based on resource utilization: ```yaml # hpa.yaml apiVersion: autoscaling/v2 kind: HorizontalPodAutoscaler metadata: name: cloudflow-api-hpa namespace: cloudflow-prod spec: scaleTargetRef: apiVersion: apps/v1 kind: Deployment name: cloudflow-api minReplicas: 3 maxReplicas: 10 metrics: - type: Resource resource: name: cpu target: type: Utilization averageUtilization: 70 - type: Resource resource: name: memory target: type: Utilization averageUtilization: 80 behavior: scaleDown: stabilizationWindowSeconds: 300 policies: - type: Percent value: 50 periodSeconds: 60 scaleUp: stabilizationWindowSeconds: 60 policies: - type: Percent value: 100 periodSeconds: 30 - type: Pods value: 2 periodSeconds: 30 selectPolicy: Max ``` Monitor autoscaling events: ```bash kubectl get hpa -n cloudflow-prod -w kubectl describe hpa cloudflow-api-hpa -n cloudflow-prod ``` ### Performance Optimization #### Connection Pooling Ensure optimal database connection pooling settings in PgBouncer: - **Pool Mode**: Transaction (optimal for microservices) - **Default Pool Size**: 25 connections per user - **Max DB Connections**: 100 (matches PostgreSQL `max_connections`) #### Redis Caching Implement Redis caching for frequently accessed data: ```javascript // Example caching strategy const CACHE_TTL = { workflows: 300, // 5 minutes userSessions: 3600, // 1 hour apiResults: 60 // 1 minute }; ``` #### Load Testing Perform regular load testing to validate scaling configuration: ```bash # Install k6 for load testing kubectl apply -f - <<EOF apiVersion: v1 kind: ConfigMap metadata: name: loadtest-script namespace: cloudflow-prod data: script.js: | import http from 'k6/http'; import { check, sleep } from 'k6'; export let options = { stages: [ { duration: '2m', target: 100 }, { duration: '5m', target: 100 }, { duration: '2m', target: 200 }, { duration: '5m', target: 200 }, { duration: '2m', target: 0 }, ], thresholds: { http_req_duration: ['p(95)<500'], }, }; export default function () { let res = http.get('https://api.cloudflow.io/api/workflows'); check(res, { 'status is 200': (r) => r.status === 200 }); sleep(1); } EOF ``` **Performance Targets**: - P95 API latency: < 500ms - P99 API latency: < 1000ms - Error rate: < 0.1% - Throughput: > 1000 requests/second --- ## Security and Compliance ### Network Policies Implement network policies to restrict pod-to-pod communication: ```yaml # network-policy.yaml apiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: cloudflow-network-policy namespace: cloudflow-prod spec:",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_10",
              "content": "- Execution start rate: 500 per second - Completion rate: 450 per second (average 2-second execution time) **Database**: - Read throughput: 50,000 queries per second (across replicas) - Write throughput: 15,000 transactions per second - Connection capacity: 2,000 concurrent connections **Kafka**: - Message ingestion: 100,000 messages per second - Consumer throughput: 80,000 messages per second (aggregated) - End-to-end latency: < 100ms (P99) ### Resource Utilization **CPU Utilization** (Target: 60-70% average): - API Gateway: 55% average, 80% peak - Workflow Engine: 65% average, 85% peak - Auth Service: 40% average, 60% peak **Memory Utilization** (Target: < 80%): - API Gateway: 2.5GB average per pod (4GB allocated) - Workflow Engine: 6GB average per pod (8GB allocated) - Notification Service: 2.8GB average per pod (4GB allocated) **Network Throughput**: - Ingress: 2 Gbps average, 5 Gbps peak - Egress: 1.5 Gbps average, 4 Gbps peak - Internal (service mesh): 8 Gbps average ### Monitoring & Alerting **Key Metrics** (Monitored via Prometheus): - Request rate (per service, per endpoint) - Error rate (4xx, 5xx responses) - Latency percentiles (P50, P95, P99) - Resource utilization (CPU, memory, disk) - Cache hit/miss ratios - Database connection pool saturation - Kafka consumer lag **Alerts** (via PagerDuty): - P1 (Immediate): API error rate > 1%, database connection failure - P2 (< 15 min): P99 latency > 500ms, cache hit rate < 80% - P3 (< 1 hour): Resource utilization > 85%, replica count < minimum **SLI/SLO/SLA**: - SLI: API success rate (non-5xx responses) - SLO: 99.9% uptime per month (43 minutes downtime allowance) - SLA: 99.5% uptime guarantee (customer-facing SLA with credits) --- ## Disaster Recovery ### Recovery Objectives **RPO (Recovery Point Objective): 1 hour** - Maximum acceptable data loss: 1 hour of transactions - Achieved through: Continuous database replication + hourly snapshots - Kafka retention: 7 days allows event replay **RTO (Recovery Time Objective): 4 hours** - Maximum acceptable downtime: 4 hours for full system recovery - Includes: Failover, data verification, and service restoration - Automated runbooks reduce RTO to < 2 hours for common scenarios ### High Availability Architecture **Multi-AZ Deployment**: ``` Region: us-east-1 AZ-1a: AZ-1b: AZ-1c: - API Gateway (4) - API Gateway (4) - API Gateway (4) - Workflow Engine (6) - Workflow Engine (5) - Workflow Engine (5) - Auth Service (3) - Auth Service (3) - Auth",
              "score": null
            }
          ],
          "latency_ms": 938.5056829996756,
          "query_id": "realistic_010",
          "dimension": "casual",
          "query": "RPO RTO"
        },
        {
          "key_facts": [
            "RPO (Recovery Point Objective): 1 hour",
            "RTO (Recovery Time Objective): 4 hours"
          ],
          "found_facts": [
            "RPO (Recovery Point Objective): 1 hour",
            "RTO (Recovery Time Objective): 4 hours"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "architecture_overview",
            "deployment_guide"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_11",
              "content": "Service (2) - PostgreSQL Primary - PostgreSQL Replica - PostgreSQL Replica - Redis Primary (2) - Redis Replica (2) - Redis Replica (2) - Kafka Broker (2) - Kafka Broker (2) - Kafka Broker (1) ``` **Automatic Failover**: - Database: 30-60 seconds (automatic promotion of replica) - Redis: < 10 seconds (Sentinel-based failover) - Kafka: < 30 seconds (controller election) - Services: Kubernetes health checks with 10-second liveness probes ### Backup Strategy **Database Backups**: - Automated snapshots: Daily at 02:00 UTC - Retention: 30 days for daily, 90 days for monthly - Cross-region replication: Async replication to us-west-2 (15-minute lag) - Backup verification: Weekly automated restore test in staging environment **Backup Schedule**: ``` Daily: Full snapshot \u2192 S3 (encrypted) Hourly: WAL archives \u2192 S3 (point-in-time recovery) Weekly: Backup validation test Monthly: Long-term archive to Glacier ``` **Configuration Backups**: - Kubernetes manifests: Stored in Git (GitOps with ArgoCD) - Vault secrets: Automated snapshot every 6 hours - Kafka topic configurations: Exported daily to S3 ### Disaster Recovery Procedures **Scenario 1: Single AZ Failure** - Detection: Health checks fail for entire AZ (< 30 seconds) - Action: Traffic automatically routed to healthy AZs by ALB - Recovery time: < 5 minutes (no manual intervention) - Data loss: None (multi-AZ replication) **Scenario 2: Database Primary Failure** - Detection: Health check fails for primary database (< 30 seconds) - Action: Automatic promotion of read replica to primary - Recovery time: 30-60 seconds - Data loss: Minimal (< 1 second due to synchronous replication) **Scenario 3: Full Region Failure** - Detection: Multiple health check failures across all AZs (< 2 minutes) - Action: Manual failover to DR region (us-west-2) - Procedure: 1. Update DNS to point to DR region (TTL: 60 seconds) 2. Promote DR database replica to primary 3. Scale up DR region services to production capacity 4. Verify data consistency and integrity 5. Update monitoring dashboards - Recovery time: 2-4 hours (includes verification) - Data loss: < 1 hour (last cross-region replication) **Scenario 4: Data Corruption** - Detection: Data validation checks or user report - Action: Point-in-time recovery from WAL archives - Procedure: 1. Identify corruption time window 2. Restore from snapshot prior to corruption 3. Replay WAL logs up to corruption point 4. Verify data integrity 5.",
              "score": null
            },
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_5",
              "content": "Configure log aggregation: ```bash # Install Fluent Bit for log forwarding helm install fluent-bit fluent/fluent-bit \\ --namespace logging \\ --create-namespace \\ --set cloudWatch.enabled=true \\ --set cloudWatch.region=us-east-1 \\ --set cloudWatch.logGroupName=/aws/eks/cloudflow-production/application ``` Query logs using CloudWatch Insights: ```sql fields @timestamp, @message, level, requestId, userId | filter namespace = \"cloudflow-prod\" | filter level = \"error\" | sort @timestamp desc | limit 100 ``` --- ## Backup and Disaster Recovery ### Database Backup Strategy CloudFlow implements a comprehensive backup strategy with the following retention policy: - **Daily snapshots**: Retained for 30 days - **Weekly backups**: Retained for 90 days - **Monthly backups**: Retained for 1 year #### Automated Backup with Velero Install Velero for cluster-wide backups: ```bash velero install \\ --provider aws \\ --plugins velero/velero-plugin-for-aws:v1.8.0 \\ --bucket cloudflow-velero-backups \\ --backup-location-config region=us-east-1 \\ --snapshot-location-config region=us-east-1 \\ --secret-file ./credentials-velero ``` Create a daily backup schedule: ```yaml # backup-schedule.yaml apiVersion: velero.io/v1 kind: Schedule metadata: name: cloudflow-daily-backup namespace: velero spec: schedule: \"0 2 * * *\" # 2 AM UTC daily template: includedNamespaces: - cloudflow-prod ttl: 720h0m0s # 30 days storageLocation: default snapshotVolumes: true ``` Apply the backup schedule: ```bash kubectl apply -f backup-schedule.yaml ``` #### PostgreSQL Backup Configure PostgreSQL continuous archiving with WAL-G: ```bash # Create backup cronjob kubectl apply -f - <<EOF apiVersion: batch/v1 kind: CronJob metadata: name: postgres-backup namespace: cloudflow-prod spec: schedule: \"0 1 * * *\" # 1 AM UTC daily jobTemplate: spec: template: spec: containers: - name: backup image: postgres:14 env: - name: PGPASSWORD valueFrom: secretKeyRef: name: postgres-credentials key: password command: - /bin/bash - -c - | TIMESTAMP=\\$(date +%Y%m%d_%H%M%S) pg_dump -h postgresql.cloudflow-prod.svc.cluster.local \\ -U cloudflow -d cloudflow \\ | gzip > /backups/cloudflow_\\${TIMESTAMP}.sql.gz aws s3 cp /backups/cloudflow_\\${TIMESTAMP}.sql.gz \\ s3://cloudflow-db-backups/daily/ volumeMounts: - name: backup-storage mountPath: /backups volumes: - name: backup-storage emptyDir: {} restartPolicy: OnFailure EOF ``` ### Disaster Recovery Procedure In case of catastrophic failure, follow these steps to restore CloudFlow: 1. **Restore EKS Cluster** (if necessary): ```bash eksctl create cluster -f cluster-config.yaml ``` 2. **Restore Velero Backup**: ```bash # List available backups velero backup get # Restore from backup velero restore create --from-backup cloudflow-daily-backup-20260124 ``` 3. **Restore Database**: ```bash # Download backup from S3 aws s3 cp s3://cloudflow-db-backups/daily/cloudflow_20260124_010000.sql.gz . # Restore database gunzip -c cloudflow_20260124_010000.sql.gz | \\ psql -h postgresql.cloudflow-prod.svc.cluster.local -U cloudflow -d cloudflow ``` 4.",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_12",
              "content": "Resume normal operations - Recovery time: 1-3 hours depending on data volume - Data loss: None if corruption detected quickly ### Testing & Validation **DR Drill Schedule**: - Monthly: Automated failover test (single AZ failure simulation) - Quarterly: Full DR region failover (non-production hours) - Annually: Complete disaster simulation with all stakeholders **Last DR Test Results** (Dec 15, 2025): - Scenario: Full region failover - Actual RTO: 2 hours 23 minutes (target: 4 hours) - Actual RPO: 42 minutes (target: 1 hour) - Issues identified: DNS propagation slower than expected (resolved) - Success criteria: Met all recovery objectives ### Business Continuity **Communication Plan**: - Status page: status.cloudflow.com (updated every 15 minutes during incident) - Customer notifications: Email + SMS for all P1 incidents - Internal escalation: PagerDuty \u2192 Incident Commander \u2192 Engineering Manager \u2192 CTO **Data Retention Policy**: - Active data: 12 months in hot storage (PostgreSQL) - Archived data: 7 years in cold storage (S3 Glacier) - Audit logs: 7 years (compliance requirement) - Backup retention: 30 days standard, 90 days monthly snapshots --- ## Appendix ### Service Dependency Matrix | Service | Depends On | Critical Path | |---------|-----------|---------------| | API Gateway | Auth Service, Redis | Yes | | Auth Service | PostgreSQL, Redis, Vault | Yes | | Workflow Engine | PostgreSQL, Kafka, Redis | Yes | | Scheduler | PostgreSQL, Redis, Kafka | No | | Notification Service | Kafka, PostgreSQL, SendGrid, Twilio | No | ### Capacity Planning **Current Capacity** (Jan 2026): - Daily workflow executions: 2.5M - Active users: 150,000 - API requests per day: 50M - Database size: 1.2TB - Kafka throughput: 100K msg/sec peak **6-Month Projection** (Jul 2026): - Daily workflow executions: 4M (+60%) - Active users: 225,000 (+50%) - API requests per day: 80M (+60%) - Database size: 2TB (+67%) - Required scaling: +30% compute, +50% storage ### Contact Information - **Architecture Team**: architecture@cloudflow.internal - **On-Call Engineer**: PagerDuty escalation - **Security Team**: security@cloudflow.internal - **Documentation**: https://wiki.cloudflow.internal/architecture --- **Document Revision History**: - v2.3.1 (Jan 15, 2026): Updated performance metrics, added DR test results - v2.3.0 (Dec 1, 2025): Added multi-AZ deployment details - v2.2.0 (Oct 15, 2025): Security architecture overhaul - v2.1.0 (Sep 1, 2025): Initial Kafka migration documentation - v2.0.0 (Jul 1, 2025): Complete microservices rewrite",
              "score": null
            },
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_6",
              "content": "**Verify Service Health**: ```bash kubectl get pods -n cloudflow-prod curl https://api.cloudflow.io/health ``` **Recovery Time Objective (RTO)**: 4 hours **Recovery Point Objective (RPO)**: 24 hours --- ## Scaling and Performance ### Horizontal Pod Autoscaling CloudFlow is configured with Horizontal Pod Autoscaler (HPA) to automatically scale based on resource utilization: ```yaml # hpa.yaml apiVersion: autoscaling/v2 kind: HorizontalPodAutoscaler metadata: name: cloudflow-api-hpa namespace: cloudflow-prod spec: scaleTargetRef: apiVersion: apps/v1 kind: Deployment name: cloudflow-api minReplicas: 3 maxReplicas: 10 metrics: - type: Resource resource: name: cpu target: type: Utilization averageUtilization: 70 - type: Resource resource: name: memory target: type: Utilization averageUtilization: 80 behavior: scaleDown: stabilizationWindowSeconds: 300 policies: - type: Percent value: 50 periodSeconds: 60 scaleUp: stabilizationWindowSeconds: 60 policies: - type: Percent value: 100 periodSeconds: 30 - type: Pods value: 2 periodSeconds: 30 selectPolicy: Max ``` Monitor autoscaling events: ```bash kubectl get hpa -n cloudflow-prod -w kubectl describe hpa cloudflow-api-hpa -n cloudflow-prod ``` ### Performance Optimization #### Connection Pooling Ensure optimal database connection pooling settings in PgBouncer: - **Pool Mode**: Transaction (optimal for microservices) - **Default Pool Size**: 25 connections per user - **Max DB Connections**: 100 (matches PostgreSQL `max_connections`) #### Redis Caching Implement Redis caching for frequently accessed data: ```javascript // Example caching strategy const CACHE_TTL = { workflows: 300, // 5 minutes userSessions: 3600, // 1 hour apiResults: 60 // 1 minute }; ``` #### Load Testing Perform regular load testing to validate scaling configuration: ```bash # Install k6 for load testing kubectl apply -f - <<EOF apiVersion: v1 kind: ConfigMap metadata: name: loadtest-script namespace: cloudflow-prod data: script.js: | import http from 'k6/http'; import { check, sleep } from 'k6'; export let options = { stages: [ { duration: '2m', target: 100 }, { duration: '5m', target: 100 }, { duration: '2m', target: 200 }, { duration: '5m', target: 200 }, { duration: '2m', target: 0 }, ], thresholds: { http_req_duration: ['p(95)<500'], }, }; export default function () { let res = http.get('https://api.cloudflow.io/api/workflows'); check(res, { 'status is 200': (r) => r.status === 200 }); sleep(1); } EOF ``` **Performance Targets**: - P95 API latency: < 500ms - P99 API latency: < 1000ms - Error rate: < 0.1% - Throughput: > 1000 requests/second --- ## Security and Compliance ### Network Policies Implement network policies to restrict pod-to-pod communication: ```yaml # network-policy.yaml apiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: cloudflow-network-policy namespace: cloudflow-prod spec:",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_10",
              "content": "- Execution start rate: 500 per second - Completion rate: 450 per second (average 2-second execution time) **Database**: - Read throughput: 50,000 queries per second (across replicas) - Write throughput: 15,000 transactions per second - Connection capacity: 2,000 concurrent connections **Kafka**: - Message ingestion: 100,000 messages per second - Consumer throughput: 80,000 messages per second (aggregated) - End-to-end latency: < 100ms (P99) ### Resource Utilization **CPU Utilization** (Target: 60-70% average): - API Gateway: 55% average, 80% peak - Workflow Engine: 65% average, 85% peak - Auth Service: 40% average, 60% peak **Memory Utilization** (Target: < 80%): - API Gateway: 2.5GB average per pod (4GB allocated) - Workflow Engine: 6GB average per pod (8GB allocated) - Notification Service: 2.8GB average per pod (4GB allocated) **Network Throughput**: - Ingress: 2 Gbps average, 5 Gbps peak - Egress: 1.5 Gbps average, 4 Gbps peak - Internal (service mesh): 8 Gbps average ### Monitoring & Alerting **Key Metrics** (Monitored via Prometheus): - Request rate (per service, per endpoint) - Error rate (4xx, 5xx responses) - Latency percentiles (P50, P95, P99) - Resource utilization (CPU, memory, disk) - Cache hit/miss ratios - Database connection pool saturation - Kafka consumer lag **Alerts** (via PagerDuty): - P1 (Immediate): API error rate > 1%, database connection failure - P2 (< 15 min): P99 latency > 500ms, cache hit rate < 80% - P3 (< 1 hour): Resource utilization > 85%, replica count < minimum **SLI/SLO/SLA**: - SLI: API success rate (non-5xx responses) - SLO: 99.9% uptime per month (43 minutes downtime allowance) - SLA: 99.5% uptime guarantee (customer-facing SLA with credits) --- ## Disaster Recovery ### Recovery Objectives **RPO (Recovery Point Objective): 1 hour** - Maximum acceptable data loss: 1 hour of transactions - Achieved through: Continuous database replication + hourly snapshots - Kafka retention: 7 days allows event replay **RTO (Recovery Time Objective): 4 hours** - Maximum acceptable downtime: 4 hours for full system recovery - Includes: Failover, data verification, and service restoration - Automated runbooks reduce RTO to < 2 hours for common scenarios ### High Availability Architecture **Multi-AZ Deployment**: ``` Region: us-east-1 AZ-1a: AZ-1b: AZ-1c: - API Gateway (4) - API Gateway (4) - API Gateway (4) - Workflow Engine (6) - Workflow Engine (5) - Workflow Engine (5) - Auth Service (3) - Auth Service (3) - Auth",
              "score": null
            }
          ],
          "latency_ms": 906.5753639988543,
          "query_id": "realistic_010",
          "dimension": "contextual",
          "query": "For our business continuity plan, what are CloudFlow's recovery objectives?"
        },
        {
          "key_facts": [
            "RPO (Recovery Point Objective): 1 hour",
            "RTO (Recovery Time Objective): 4 hours"
          ],
          "found_facts": [
            "RPO (Recovery Point Objective): 1 hour",
            "RTO (Recovery Time Objective): 4 hours"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "architecture_overview",
            "deployment_guide"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_11",
              "content": "Service (2) - PostgreSQL Primary - PostgreSQL Replica - PostgreSQL Replica - Redis Primary (2) - Redis Replica (2) - Redis Replica (2) - Kafka Broker (2) - Kafka Broker (2) - Kafka Broker (1) ``` **Automatic Failover**: - Database: 30-60 seconds (automatic promotion of replica) - Redis: < 10 seconds (Sentinel-based failover) - Kafka: < 30 seconds (controller election) - Services: Kubernetes health checks with 10-second liveness probes ### Backup Strategy **Database Backups**: - Automated snapshots: Daily at 02:00 UTC - Retention: 30 days for daily, 90 days for monthly - Cross-region replication: Async replication to us-west-2 (15-minute lag) - Backup verification: Weekly automated restore test in staging environment **Backup Schedule**: ``` Daily: Full snapshot \u2192 S3 (encrypted) Hourly: WAL archives \u2192 S3 (point-in-time recovery) Weekly: Backup validation test Monthly: Long-term archive to Glacier ``` **Configuration Backups**: - Kubernetes manifests: Stored in Git (GitOps with ArgoCD) - Vault secrets: Automated snapshot every 6 hours - Kafka topic configurations: Exported daily to S3 ### Disaster Recovery Procedures **Scenario 1: Single AZ Failure** - Detection: Health checks fail for entire AZ (< 30 seconds) - Action: Traffic automatically routed to healthy AZs by ALB - Recovery time: < 5 minutes (no manual intervention) - Data loss: None (multi-AZ replication) **Scenario 2: Database Primary Failure** - Detection: Health check fails for primary database (< 30 seconds) - Action: Automatic promotion of read replica to primary - Recovery time: 30-60 seconds - Data loss: Minimal (< 1 second due to synchronous replication) **Scenario 3: Full Region Failure** - Detection: Multiple health check failures across all AZs (< 2 minutes) - Action: Manual failover to DR region (us-west-2) - Procedure: 1. Update DNS to point to DR region (TTL: 60 seconds) 2. Promote DR database replica to primary 3. Scale up DR region services to production capacity 4. Verify data consistency and integrity 5. Update monitoring dashboards - Recovery time: 2-4 hours (includes verification) - Data loss: < 1 hour (last cross-region replication) **Scenario 4: Data Corruption** - Detection: Data validation checks or user report - Action: Point-in-time recovery from WAL archives - Procedure: 1. Identify corruption time window 2. Restore from snapshot prior to corruption 3. Replay WAL logs up to corruption point 4. Verify data integrity 5.",
              "score": null
            },
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_5",
              "content": "Configure log aggregation: ```bash # Install Fluent Bit for log forwarding helm install fluent-bit fluent/fluent-bit \\ --namespace logging \\ --create-namespace \\ --set cloudWatch.enabled=true \\ --set cloudWatch.region=us-east-1 \\ --set cloudWatch.logGroupName=/aws/eks/cloudflow-production/application ``` Query logs using CloudWatch Insights: ```sql fields @timestamp, @message, level, requestId, userId | filter namespace = \"cloudflow-prod\" | filter level = \"error\" | sort @timestamp desc | limit 100 ``` --- ## Backup and Disaster Recovery ### Database Backup Strategy CloudFlow implements a comprehensive backup strategy with the following retention policy: - **Daily snapshots**: Retained for 30 days - **Weekly backups**: Retained for 90 days - **Monthly backups**: Retained for 1 year #### Automated Backup with Velero Install Velero for cluster-wide backups: ```bash velero install \\ --provider aws \\ --plugins velero/velero-plugin-for-aws:v1.8.0 \\ --bucket cloudflow-velero-backups \\ --backup-location-config region=us-east-1 \\ --snapshot-location-config region=us-east-1 \\ --secret-file ./credentials-velero ``` Create a daily backup schedule: ```yaml # backup-schedule.yaml apiVersion: velero.io/v1 kind: Schedule metadata: name: cloudflow-daily-backup namespace: velero spec: schedule: \"0 2 * * *\" # 2 AM UTC daily template: includedNamespaces: - cloudflow-prod ttl: 720h0m0s # 30 days storageLocation: default snapshotVolumes: true ``` Apply the backup schedule: ```bash kubectl apply -f backup-schedule.yaml ``` #### PostgreSQL Backup Configure PostgreSQL continuous archiving with WAL-G: ```bash # Create backup cronjob kubectl apply -f - <<EOF apiVersion: batch/v1 kind: CronJob metadata: name: postgres-backup namespace: cloudflow-prod spec: schedule: \"0 1 * * *\" # 1 AM UTC daily jobTemplate: spec: template: spec: containers: - name: backup image: postgres:14 env: - name: PGPASSWORD valueFrom: secretKeyRef: name: postgres-credentials key: password command: - /bin/bash - -c - | TIMESTAMP=\\$(date +%Y%m%d_%H%M%S) pg_dump -h postgresql.cloudflow-prod.svc.cluster.local \\ -U cloudflow -d cloudflow \\ | gzip > /backups/cloudflow_\\${TIMESTAMP}.sql.gz aws s3 cp /backups/cloudflow_\\${TIMESTAMP}.sql.gz \\ s3://cloudflow-db-backups/daily/ volumeMounts: - name: backup-storage mountPath: /backups volumes: - name: backup-storage emptyDir: {} restartPolicy: OnFailure EOF ``` ### Disaster Recovery Procedure In case of catastrophic failure, follow these steps to restore CloudFlow: 1. **Restore EKS Cluster** (if necessary): ```bash eksctl create cluster -f cluster-config.yaml ``` 2. **Restore Velero Backup**: ```bash # List available backups velero backup get # Restore from backup velero restore create --from-backup cloudflow-daily-backup-20260124 ``` 3. **Restore Database**: ```bash # Download backup from S3 aws s3 cp s3://cloudflow-db-backups/daily/cloudflow_20260124_010000.sql.gz . # Restore database gunzip -c cloudflow_20260124_010000.sql.gz | \\ psql -h postgresql.cloudflow-prod.svc.cluster.local -U cloudflow -d cloudflow ``` 4.",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_10",
              "content": "- Execution start rate: 500 per second - Completion rate: 450 per second (average 2-second execution time) **Database**: - Read throughput: 50,000 queries per second (across replicas) - Write throughput: 15,000 transactions per second - Connection capacity: 2,000 concurrent connections **Kafka**: - Message ingestion: 100,000 messages per second - Consumer throughput: 80,000 messages per second (aggregated) - End-to-end latency: < 100ms (P99) ### Resource Utilization **CPU Utilization** (Target: 60-70% average): - API Gateway: 55% average, 80% peak - Workflow Engine: 65% average, 85% peak - Auth Service: 40% average, 60% peak **Memory Utilization** (Target: < 80%): - API Gateway: 2.5GB average per pod (4GB allocated) - Workflow Engine: 6GB average per pod (8GB allocated) - Notification Service: 2.8GB average per pod (4GB allocated) **Network Throughput**: - Ingress: 2 Gbps average, 5 Gbps peak - Egress: 1.5 Gbps average, 4 Gbps peak - Internal (service mesh): 8 Gbps average ### Monitoring & Alerting **Key Metrics** (Monitored via Prometheus): - Request rate (per service, per endpoint) - Error rate (4xx, 5xx responses) - Latency percentiles (P50, P95, P99) - Resource utilization (CPU, memory, disk) - Cache hit/miss ratios - Database connection pool saturation - Kafka consumer lag **Alerts** (via PagerDuty): - P1 (Immediate): API error rate > 1%, database connection failure - P2 (< 15 min): P99 latency > 500ms, cache hit rate < 80% - P3 (< 1 hour): Resource utilization > 85%, replica count < minimum **SLI/SLO/SLA**: - SLI: API success rate (non-5xx responses) - SLO: 99.9% uptime per month (43 minutes downtime allowance) - SLA: 99.5% uptime guarantee (customer-facing SLA with credits) --- ## Disaster Recovery ### Recovery Objectives **RPO (Recovery Point Objective): 1 hour** - Maximum acceptable data loss: 1 hour of transactions - Achieved through: Continuous database replication + hourly snapshots - Kafka retention: 7 days allows event replay **RTO (Recovery Time Objective): 4 hours** - Maximum acceptable downtime: 4 hours for full system recovery - Includes: Failover, data verification, and service restoration - Automated runbooks reduce RTO to < 2 hours for common scenarios ### High Availability Architecture **Multi-AZ Deployment**: ``` Region: us-east-1 AZ-1a: AZ-1b: AZ-1c: - API Gateway (4) - API Gateway (4) - API Gateway (4) - Workflow Engine (6) - Workflow Engine (5) - Workflow Engine (5) - Auth Service (3) - Auth Service (3) - Auth",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_12",
              "content": "Resume normal operations - Recovery time: 1-3 hours depending on data volume - Data loss: None if corruption detected quickly ### Testing & Validation **DR Drill Schedule**: - Monthly: Automated failover test (single AZ failure simulation) - Quarterly: Full DR region failover (non-production hours) - Annually: Complete disaster simulation with all stakeholders **Last DR Test Results** (Dec 15, 2025): - Scenario: Full region failover - Actual RTO: 2 hours 23 minutes (target: 4 hours) - Actual RPO: 42 minutes (target: 1 hour) - Issues identified: DNS propagation slower than expected (resolved) - Success criteria: Met all recovery objectives ### Business Continuity **Communication Plan**: - Status page: status.cloudflow.com (updated every 15 minutes during incident) - Customer notifications: Email + SMS for all P1 incidents - Internal escalation: PagerDuty \u2192 Incident Commander \u2192 Engineering Manager \u2192 CTO **Data Retention Policy**: - Active data: 12 months in hot storage (PostgreSQL) - Archived data: 7 years in cold storage (S3 Glacier) - Audit logs: 7 years (compliance requirement) - Backup retention: 30 days standard, 90 days monthly snapshots --- ## Appendix ### Service Dependency Matrix | Service | Depends On | Critical Path | |---------|-----------|---------------| | API Gateway | Auth Service, Redis | Yes | | Auth Service | PostgreSQL, Redis, Vault | Yes | | Workflow Engine | PostgreSQL, Kafka, Redis | Yes | | Scheduler | PostgreSQL, Redis, Kafka | No | | Notification Service | Kafka, PostgreSQL, SendGrid, Twilio | No | ### Capacity Planning **Current Capacity** (Jan 2026): - Daily workflow executions: 2.5M - Active users: 150,000 - API requests per day: 50M - Database size: 1.2TB - Kafka throughput: 100K msg/sec peak **6-Month Projection** (Jul 2026): - Daily workflow executions: 4M (+60%) - Active users: 225,000 (+50%) - API requests per day: 80M (+60%) - Database size: 2TB (+67%) - Required scaling: +30% compute, +50% storage ### Contact Information - **Architecture Team**: architecture@cloudflow.internal - **On-Call Engineer**: PagerDuty escalation - **Security Team**: security@cloudflow.internal - **Documentation**: https://wiki.cloudflow.internal/architecture --- **Document Revision History**: - v2.3.1 (Jan 15, 2026): Updated performance metrics, added DR test results - v2.3.0 (Dec 1, 2025): Added multi-AZ deployment details - v2.2.0 (Oct 15, 2025): Security architecture overhaul - v2.1.0 (Sep 1, 2025): Initial Kafka migration documentation - v2.0.0 (Jul 1, 2025): Complete microservices rewrite",
              "score": null
            },
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_6",
              "content": "**Verify Service Health**: ```bash kubectl get pods -n cloudflow-prod curl https://api.cloudflow.io/health ``` **Recovery Time Objective (RTO)**: 4 hours **Recovery Point Objective (RPO)**: 24 hours --- ## Scaling and Performance ### Horizontal Pod Autoscaling CloudFlow is configured with Horizontal Pod Autoscaler (HPA) to automatically scale based on resource utilization: ```yaml # hpa.yaml apiVersion: autoscaling/v2 kind: HorizontalPodAutoscaler metadata: name: cloudflow-api-hpa namespace: cloudflow-prod spec: scaleTargetRef: apiVersion: apps/v1 kind: Deployment name: cloudflow-api minReplicas: 3 maxReplicas: 10 metrics: - type: Resource resource: name: cpu target: type: Utilization averageUtilization: 70 - type: Resource resource: name: memory target: type: Utilization averageUtilization: 80 behavior: scaleDown: stabilizationWindowSeconds: 300 policies: - type: Percent value: 50 periodSeconds: 60 scaleUp: stabilizationWindowSeconds: 60 policies: - type: Percent value: 100 periodSeconds: 30 - type: Pods value: 2 periodSeconds: 30 selectPolicy: Max ``` Monitor autoscaling events: ```bash kubectl get hpa -n cloudflow-prod -w kubectl describe hpa cloudflow-api-hpa -n cloudflow-prod ``` ### Performance Optimization #### Connection Pooling Ensure optimal database connection pooling settings in PgBouncer: - **Pool Mode**: Transaction (optimal for microservices) - **Default Pool Size**: 25 connections per user - **Max DB Connections**: 100 (matches PostgreSQL `max_connections`) #### Redis Caching Implement Redis caching for frequently accessed data: ```javascript // Example caching strategy const CACHE_TTL = { workflows: 300, // 5 minutes userSessions: 3600, // 1 hour apiResults: 60 // 1 minute }; ``` #### Load Testing Perform regular load testing to validate scaling configuration: ```bash # Install k6 for load testing kubectl apply -f - <<EOF apiVersion: v1 kind: ConfigMap metadata: name: loadtest-script namespace: cloudflow-prod data: script.js: | import http from 'k6/http'; import { check, sleep } from 'k6'; export let options = { stages: [ { duration: '2m', target: 100 }, { duration: '5m', target: 100 }, { duration: '2m', target: 200 }, { duration: '5m', target: 200 }, { duration: '2m', target: 0 }, ], thresholds: { http_req_duration: ['p(95)<500'], }, }; export default function () { let res = http.get('https://api.cloudflow.io/api/workflows'); check(res, { 'status is 200': (r) => r.status === 200 }); sleep(1); } EOF ``` **Performance Targets**: - P95 API latency: < 500ms - P99 API latency: < 1000ms - Error rate: < 0.1% - Throughput: > 1000 requests/second --- ## Security and Compliance ### Network Policies Implement network policies to restrict pod-to-pod communication: ```yaml # network-policy.yaml apiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: cloudflow-network-policy namespace: cloudflow-prod spec:",
              "score": null
            }
          ],
          "latency_ms": 980.6488350004656,
          "query_id": "realistic_010",
          "dimension": "negation",
          "query": "Why can't we guarantee zero data loss?"
        },
        {
          "key_facts": [
            "3600 seconds",
            "exceeded maximum execution time of 3600 seconds"
          ],
          "found_facts": [
            "3600 seconds",
            "exceeded maximum execution time of 3600 seconds"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "troubleshooting_guide",
            "user_guide"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "user_guide",
              "chunk_id": "user_guide_fix_6",
              "content": "Filter by error type, date range, or execution ID 3. Click an execution to view full details 4. Click **\"Retry\"** to reprocess with the same input data ### Error Notifications Get notified when workflows fail: ```yaml workflow: notifications: on_failure: - type: email to: \"ops-team@company.com\" - type: slack channel: \"#alerts\" message: \"Workflow {{workflow.name}} failed: {{error.message}}\" on_success_after_retry: - type: slack channel: \"#monitoring\" message: \"Workflow recovered after {{error.attempts}} attempts\" ``` ## Workflow Limits CloudFlow enforces the following limits to ensure platform stability and performance: ### Steps Per Workflow - **Maximum**: 50 steps per workflow - **Recommendation**: Keep workflows focused and modular. If you need more steps, consider splitting into multiple workflows connected via webhooks. ### Execution Timeout - **Default**: 3600 seconds (60 minutes) - **Behavior**: Workflows exceeding this timeout are automatically terminated - **Custom Timeouts**: Enterprise plans can request custom timeout limits **Setting Step-Level Timeouts:** ```yaml - id: long_running_task action: http_request config: url: \"https://api.example.com/process\" timeout: 300 # 5 minutes for this specific step ``` ### Execution Limits - **Maximum**: 1000 executions per day (per workflow) - **Rate Limiting**: 100 concurrent executions per workflow - **Burst Limit**: 10 executions per second **What happens when limits are reached:** - New executions are queued automatically - Webhook triggers return HTTP 429 (Too Many Requests) - Scheduled executions are skipped (logged in audit trail) - Email notifications sent to workflow owner **Monitoring Usage:** View real-time metrics in your workflow dashboard: - Executions today: 847 / 1000 - Average duration: 12.3s - Success rate: 98.2% - Current queue depth: 3 ### Data Limits - **Maximum request/response size**: 10MB per action - **Maximum execution payload**: 50MB total - **Variable value size**: 1MB per variable ### Enterprise Plan Limits Enterprise customers can request increased limits: - Up to 100 steps per workflow - Up to 10,000 executions per day - Up to 7200 second timeout (2 hours) - Priority execution queue - Dedicated capacity allocation Contact sales@cloudflow.io for Enterprise pricing and custom limits. ## Best Practices Follow these best practices to build reliable, maintainable workflows: ### 1. Use Descriptive Names **Good:** - Workflow: \"Sync Customer Data from Salesforce to Database\" - Step: \"validate_customer_email\" **Bad:** - Workflow: \"Workflow 1\" - Step: \"step3\" ### 2.",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_5",
              "content": "exec_7h3j6k9m2n --show-bottlenecks ``` #### Solutions **1. Increase workflow timeout (if justified):** ```bash # Update workflow configuration cloudflow workflows update wf_9k2n4m8p1q \\ --timeout 7200 \\ --reason \"Large dataset processing requires extended time\" # Verify update cloudflow workflows get wf_9k2n4m8p1q | grep timeout ``` **2. Optimize slow steps:** ```bash # Enable parallel processing cloudflow workflows update wf_9k2n4m8p1q \\ --step data_transformation \\ --parallel-workers 8 \\ --batch-size 1000 # Add checkpointing for long operations cloudflow workflows update wf_9k2n4m8p1q \\ --step data_transformation \\ --enable-checkpointing \\ --checkpoint-interval 300 ``` **3. Split workflow into smaller workflows:** ```bash # Create sub-workflows cloudflow workflows create data-pipeline-part1 \\ --steps \"data_ingestion,data_validation\" \\ --timeout 1800 cloudflow workflows create data-pipeline-part2 \\ --steps \"data_transformation,data_export\" \\ --timeout 3600 \\ --trigger workflow_completed \\ --trigger-workflow data-pipeline-part1 ``` ### Retry Logic and Exponential Backoff CloudFlow implements automatic retry with exponential backoff for transient failures: - Max retries: 3 - Initial delay: 1 second - Backoff multiplier: 2 - Max delay: 60 seconds #### Retry Sequence ``` Attempt 1: Immediate Attempt 2: Wait 1s (2^0 * 1s) Attempt 3: Wait 2s (2^1 * 1s) Attempt 4: Wait 4s (2^2 * 1s) ``` #### Configuration ```bash # View current retry settings cloudflow workflows get wf_9k2n4m8p1q --format json | jq '.retry_policy' # Customize retry behavior cloudflow workflows update wf_9k2n4m8p1q \\ --retry-max-attempts 5 \\ --retry-initial-delay 2000 \\ --retry-backoff-multiplier 2 \\ --retry-max-delay 120000 \\ --retry-on-errors \"NETWORK_ERROR,TIMEOUT,RATE_LIMIT\" # Disable retry for specific step cloudflow workflows update wf_9k2n4m8p1q \\ --step data_export \\ --retry-enabled false ``` #### Monitoring Retries ```bash # List failed executions with retry information cloudflow workflows executions list \\ --status FAILED \\ --show-retries \\ --last 7d # Get retry history for specific execution cloudflow workflows executions retries exec_7h3j6k9m2n # Output: # Execution: exec_7h3j6k9m2n # Attempt 1: FAILED - NetworkError: Connection refused (delay: 0ms) # Attempt 2: FAILED - NetworkError: Connection timeout (delay: 1000ms) # Attempt 3: FAILED - NetworkError: Connection timeout (delay: 2000ms) # Attempt 4: FAILED - NetworkError: Connection timeout (delay: 4000ms) # Final Status: FAILED_AFTER_RETRIES ``` ### Workflow Step Failures #### Common Error Patterns **1.",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_4",
              "content": "**Implement connection pooling optimization:** ```bash # Use PgBouncer for connection pooling kubectl apply -f cloudflow-pgbouncer.yaml # Configure CloudFlow to use PgBouncer cloudflow config set db.host pgbouncer.cloudflow.svc.cluster.local cloudflow config set db.port 6432 cloudflow config set db.pool.mode transaction ``` 2. **Add read replicas:** ```bash # Route read-only queries to replicas cloudflow db replicas add --count 2 cloudflow config set db.read_replicas \"replica-1:5432,replica-2:5432\" cloudflow config set db.read_write_split true ``` ### Connection Timeout Errors #### Error Messages - `connection timeout after 30000ms` - `could not connect to database server at 10.0.2.45:5432` - `database server unreachable` #### Troubleshooting Steps ```bash # Test network connectivity telnet cloudflow-db.internal.company.com 5432 # Check DNS resolution nslookup cloudflow-db.internal.company.com # Verify database is accepting connections pg_isready -h cloudflow-db.internal.company.com -p 5432 -U cloudflow # Check firewall rules sudo iptables -L -n | grep 5432 # Test from CloudFlow pod network kubectl run -n cloudflow debug-pod --rm -i --tty \\ --image=postgres:14 -- \\ psql -h cloudflow-db.internal.company.com -U cloudflow -d cloudflow_production # Review database logs for connection rejections kubectl logs -n cloudflow statefulset/cloudflow-db --tail=100 | \\ grep -i \"connection\\|reject\\|authentication\" ``` ### Maximum Connection Limit (100) Reached This is a hard limit in CloudFlow's database tier. #### Permanent Solutions **Option 1: Upgrade database tier** ```bash # Check available tiers cloudflow db tiers list # Upgrade to higher tier (supports 200 connections) cloudflow db upgrade --tier standard-plus --confirm # Monitor migration progress cloudflow db migration status ``` **Option 2: Implement aggressive connection reuse** ```bash # Reduce connection lifetime cloudflow config set db.pool.max_lifetime 600 # 10 minutes # Enable prepared statement caching cloudflow config set db.prepared_statements.cache true cloudflow config set db.prepared_statements.max_size 250 # Reduce idle connection timeout cloudflow config set db.pool.idle_timeout 120 # 2 minutes ``` --- ## Workflow Execution Failures ### Timeout Errors (3600 second limit) #### Error Message ``` WorkflowExecutionError: Workflow exceeded maximum execution time of 3600 seconds Status: TIMEOUT Workflow ID: wf_9k2n4m8p1q Execution ID: exec_7h3j6k9m2n ``` #### Analysis ```bash # Get workflow execution details cloudflow workflows executions get exec_7h3j6k9m2n --verbose # Check step-by-step breakdown cloudflow workflows executions steps exec_7h3j6k9m2n # Sample output: # Step 1: data_ingestion - Duration: 245s - Status: SUCCESS # Step 2: data_validation - Duration: 123s - Status: SUCCESS # Step 3: data_transformation - Duration: 3189s - Status: TIMEOUT # Step 4: data_export - Duration: 0s - Status: SKIPPED # Identify bottleneck step cloudflow workflows analyze",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_3",
              "content": "Workflow Context Accumulation** Large workflow executions may accumulate state in memory. **Solution:** ```bash # Configure context cleanup cloudflow config set workflow.context.max_size_mb 100 cloudflow config set workflow.context.cleanup_threshold 0.8 # Enable context persistence to disk cloudflow config set workflow.context.persistence.enabled true cloudflow config set workflow.context.persistence.backend s3 ``` **2. Connection Pool Leaks** **Diagnosis:** ```bash # Check active connections cloudflow db connections --verbose # Expected output: # Active: 45/100 # Idle: 23 # Waiting: 2 # Average age: 245s ``` **Resolution:** ```bash # Adjust connection pool settings cloudflow config set db.pool.max_connections 100 cloudflow config set db.pool.min_connections 10 cloudflow config set db.pool.idle_timeout 300 cloudflow config set db.pool.max_lifetime 1800 # Force connection pool reset cloudflow db pool reset --confirm ``` **3. Event Stream Buffers** Unbounded event buffers can cause memory exhaustion. ```bash # Configure event buffer limits cloudflow config set events.buffer.max_size 10000 cloudflow config set events.buffer.overflow_strategy drop_oldest # Enable event streaming to external sink cloudflow config set events.sink.type kafka cloudflow config set events.sink.kafka.brokers \"kafka-1:9092,kafka-2:9092\" cloudflow config set events.sink.kafka.topic cloudflow-events ``` --- ## Database Connection Issues ### Connection Pool Exhaustion #### Symptoms - Error: `could not obtain connection from pool within 5000ms` - Error: `connection pool exhausted (100/100 connections in use)` - API requests fail with `503 Service Unavailable` - Database CPU usage normal, but connection count at maximum #### Investigation ```bash # Check current connection pool status cloudflow db pool status --detailed # Output example: # Pool Statistics: # Total Connections: 100/100 (100%) # Active: 87 # Idle: 13 # Waiting Requests: 45 # Average Wait Time: 3420ms # Max Wait Time: 8234ms # Identify long-running queries cloudflow db queries --status running --min-duration 30000 # Check connection distribution by client SELECT application_name, state, COUNT(*) as conn_count, AVG(EXTRACT(EPOCH FROM (NOW() - state_change))) as avg_duration_sec FROM pg_stat_activity WHERE datname = 'cloudflow_production' GROUP BY application_name, state ORDER BY conn_count DESC; ``` #### Resolution **Immediate mitigation:** ```bash # Temporarily increase connection limit (requires database restart) cloudflow db config set max_connections 150 # Kill idle connections older than 5 minutes cloudflow db connections kill --idle-timeout 300 # Restart connection pool without downtime kubectl rollout restart deployment/cloudflow-api -n cloudflow kubectl rollout status deployment/cloudflow-api -n cloudflow ``` **Long-term solutions:** 1.",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_1",
              "content": "**Diagnosis:** ```bash # Check system time timedatectl status # Compare with NTP server ntpdate -q pool.ntp.org # Check JWT issued time vs current time jwt_iat=$(echo $CF_ACCESS_TOKEN | cut -d'.' -f2 | base64 -d | jq '.iat') current_time=$(date +%s) skew=$((current_time - jwt_iat)) echo \"Clock skew: $skew seconds\" ``` **Resolution:** ```bash # Sync with NTP server sudo ntpdate -s pool.ntp.org # Enable automatic time sync sudo timedatectl set-ntp true # Restart CloudFlow client cloudflow restart ``` **4. Insufficient Permissions** Error: `User does not have required permissions for this operation` **Check user roles:** ```bash cloudflow auth whoami --verbose # Expected output: # User: john.doe@company.com # Roles: developer, workflow-admin # Scopes: workflow:*, data:read, metrics:read ``` **Request permission elevation:** ```bash # Submit access request cloudflow auth request-access \\ --resource \"workflow:production:deploy\" \\ --justification \"Deploy critical hotfix for TICKET-1234\" # Check pending approvals cloudflow auth list-requests --status pending ``` ### 403 Forbidden Errors #### RBAC Policy Violations CloudFlow uses role-based access control (RBAC) with the following hierarchy: - `viewer` - Read-only access - `developer` - Create and modify workflows (non-production) - `operator` - Execute workflows, view logs - `admin` - Full access to workspace - `platform-admin` - Cross-workspace administration **Verify resource permissions:** ```bash # Check effective permissions cloudflow rbac check \\ --user john.doe@company.com \\ --resource workflow:prod-pipeline \\ --action execute # List all policies affecting user cloudflow rbac policies --user john.doe@company.com --verbose ``` --- ## Performance Problems ### Slow Query Performance #### Symptoms - API response times > 5000ms - Database query latency warnings in logs - CloudFlow UI becomes unresponsive - Timeout errors: `Request exceeded maximum duration of 30000ms` #### Diagnosis Steps **1. Identify Slow Queries** ```bash # Query CloudFlow metrics cloudflow metrics query --metric api_request_duration_ms \\ --filter \"p95 > 5000\" \\ --last 1h # Check database slow query log kubectl logs -n cloudflow deploy/cloudflow-db-primary | \\ grep \"slow query\" | \\ tail -n 50 # Analyze query patterns cloudflow db analyze-queries --min-duration 5000 --limit 20 ``` **2.",
              "score": null
            }
          ],
          "latency_ms": 866.2418489984702,
          "query_id": "realistic_011",
          "dimension": "original",
          "query": "What is the maximum workflow execution timeout?"
        },
        {
          "key_facts": [
            "3600 seconds",
            "exceeded maximum execution time of 3600 seconds"
          ],
          "found_facts": [
            "3600 seconds",
            "exceeded maximum execution time of 3600 seconds"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "troubleshooting_guide",
            "user_guide"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "user_guide",
              "chunk_id": "user_guide_fix_6",
              "content": "Filter by error type, date range, or execution ID 3. Click an execution to view full details 4. Click **\"Retry\"** to reprocess with the same input data ### Error Notifications Get notified when workflows fail: ```yaml workflow: notifications: on_failure: - type: email to: \"ops-team@company.com\" - type: slack channel: \"#alerts\" message: \"Workflow {{workflow.name}} failed: {{error.message}}\" on_success_after_retry: - type: slack channel: \"#monitoring\" message: \"Workflow recovered after {{error.attempts}} attempts\" ``` ## Workflow Limits CloudFlow enforces the following limits to ensure platform stability and performance: ### Steps Per Workflow - **Maximum**: 50 steps per workflow - **Recommendation**: Keep workflows focused and modular. If you need more steps, consider splitting into multiple workflows connected via webhooks. ### Execution Timeout - **Default**: 3600 seconds (60 minutes) - **Behavior**: Workflows exceeding this timeout are automatically terminated - **Custom Timeouts**: Enterprise plans can request custom timeout limits **Setting Step-Level Timeouts:** ```yaml - id: long_running_task action: http_request config: url: \"https://api.example.com/process\" timeout: 300 # 5 minutes for this specific step ``` ### Execution Limits - **Maximum**: 1000 executions per day (per workflow) - **Rate Limiting**: 100 concurrent executions per workflow - **Burst Limit**: 10 executions per second **What happens when limits are reached:** - New executions are queued automatically - Webhook triggers return HTTP 429 (Too Many Requests) - Scheduled executions are skipped (logged in audit trail) - Email notifications sent to workflow owner **Monitoring Usage:** View real-time metrics in your workflow dashboard: - Executions today: 847 / 1000 - Average duration: 12.3s - Success rate: 98.2% - Current queue depth: 3 ### Data Limits - **Maximum request/response size**: 10MB per action - **Maximum execution payload**: 50MB total - **Variable value size**: 1MB per variable ### Enterprise Plan Limits Enterprise customers can request increased limits: - Up to 100 steps per workflow - Up to 10,000 executions per day - Up to 7200 second timeout (2 hours) - Priority execution queue - Dedicated capacity allocation Contact sales@cloudflow.io for Enterprise pricing and custom limits. ## Best Practices Follow these best practices to build reliable, maintainable workflows: ### 1. Use Descriptive Names **Good:** - Workflow: \"Sync Customer Data from Salesforce to Database\" - Step: \"validate_customer_email\" **Bad:** - Workflow: \"Workflow 1\" - Step: \"step3\" ### 2.",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_4",
              "content": "**Implement connection pooling optimization:** ```bash # Use PgBouncer for connection pooling kubectl apply -f cloudflow-pgbouncer.yaml # Configure CloudFlow to use PgBouncer cloudflow config set db.host pgbouncer.cloudflow.svc.cluster.local cloudflow config set db.port 6432 cloudflow config set db.pool.mode transaction ``` 2. **Add read replicas:** ```bash # Route read-only queries to replicas cloudflow db replicas add --count 2 cloudflow config set db.read_replicas \"replica-1:5432,replica-2:5432\" cloudflow config set db.read_write_split true ``` ### Connection Timeout Errors #### Error Messages - `connection timeout after 30000ms` - `could not connect to database server at 10.0.2.45:5432` - `database server unreachable` #### Troubleshooting Steps ```bash # Test network connectivity telnet cloudflow-db.internal.company.com 5432 # Check DNS resolution nslookup cloudflow-db.internal.company.com # Verify database is accepting connections pg_isready -h cloudflow-db.internal.company.com -p 5432 -U cloudflow # Check firewall rules sudo iptables -L -n | grep 5432 # Test from CloudFlow pod network kubectl run -n cloudflow debug-pod --rm -i --tty \\ --image=postgres:14 -- \\ psql -h cloudflow-db.internal.company.com -U cloudflow -d cloudflow_production # Review database logs for connection rejections kubectl logs -n cloudflow statefulset/cloudflow-db --tail=100 | \\ grep -i \"connection\\|reject\\|authentication\" ``` ### Maximum Connection Limit (100) Reached This is a hard limit in CloudFlow's database tier. #### Permanent Solutions **Option 1: Upgrade database tier** ```bash # Check available tiers cloudflow db tiers list # Upgrade to higher tier (supports 200 connections) cloudflow db upgrade --tier standard-plus --confirm # Monitor migration progress cloudflow db migration status ``` **Option 2: Implement aggressive connection reuse** ```bash # Reduce connection lifetime cloudflow config set db.pool.max_lifetime 600 # 10 minutes # Enable prepared statement caching cloudflow config set db.prepared_statements.cache true cloudflow config set db.prepared_statements.max_size 250 # Reduce idle connection timeout cloudflow config set db.pool.idle_timeout 120 # 2 minutes ``` --- ## Workflow Execution Failures ### Timeout Errors (3600 second limit) #### Error Message ``` WorkflowExecutionError: Workflow exceeded maximum execution time of 3600 seconds Status: TIMEOUT Workflow ID: wf_9k2n4m8p1q Execution ID: exec_7h3j6k9m2n ``` #### Analysis ```bash # Get workflow execution details cloudflow workflows executions get exec_7h3j6k9m2n --verbose # Check step-by-step breakdown cloudflow workflows executions steps exec_7h3j6k9m2n # Sample output: # Step 1: data_ingestion - Duration: 245s - Status: SUCCESS # Step 2: data_validation - Duration: 123s - Status: SUCCESS # Step 3: data_transformation - Duration: 3189s - Status: TIMEOUT # Step 4: data_export - Duration: 0s - Status: SKIPPED # Identify bottleneck step cloudflow workflows analyze",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_5",
              "content": "exec_7h3j6k9m2n --show-bottlenecks ``` #### Solutions **1. Increase workflow timeout (if justified):** ```bash # Update workflow configuration cloudflow workflows update wf_9k2n4m8p1q \\ --timeout 7200 \\ --reason \"Large dataset processing requires extended time\" # Verify update cloudflow workflows get wf_9k2n4m8p1q | grep timeout ``` **2. Optimize slow steps:** ```bash # Enable parallel processing cloudflow workflows update wf_9k2n4m8p1q \\ --step data_transformation \\ --parallel-workers 8 \\ --batch-size 1000 # Add checkpointing for long operations cloudflow workflows update wf_9k2n4m8p1q \\ --step data_transformation \\ --enable-checkpointing \\ --checkpoint-interval 300 ``` **3. Split workflow into smaller workflows:** ```bash # Create sub-workflows cloudflow workflows create data-pipeline-part1 \\ --steps \"data_ingestion,data_validation\" \\ --timeout 1800 cloudflow workflows create data-pipeline-part2 \\ --steps \"data_transformation,data_export\" \\ --timeout 3600 \\ --trigger workflow_completed \\ --trigger-workflow data-pipeline-part1 ``` ### Retry Logic and Exponential Backoff CloudFlow implements automatic retry with exponential backoff for transient failures: - Max retries: 3 - Initial delay: 1 second - Backoff multiplier: 2 - Max delay: 60 seconds #### Retry Sequence ``` Attempt 1: Immediate Attempt 2: Wait 1s (2^0 * 1s) Attempt 3: Wait 2s (2^1 * 1s) Attempt 4: Wait 4s (2^2 * 1s) ``` #### Configuration ```bash # View current retry settings cloudflow workflows get wf_9k2n4m8p1q --format json | jq '.retry_policy' # Customize retry behavior cloudflow workflows update wf_9k2n4m8p1q \\ --retry-max-attempts 5 \\ --retry-initial-delay 2000 \\ --retry-backoff-multiplier 2 \\ --retry-max-delay 120000 \\ --retry-on-errors \"NETWORK_ERROR,TIMEOUT,RATE_LIMIT\" # Disable retry for specific step cloudflow workflows update wf_9k2n4m8p1q \\ --step data_export \\ --retry-enabled false ``` #### Monitoring Retries ```bash # List failed executions with retry information cloudflow workflows executions list \\ --status FAILED \\ --show-retries \\ --last 7d # Get retry history for specific execution cloudflow workflows executions retries exec_7h3j6k9m2n # Output: # Execution: exec_7h3j6k9m2n # Attempt 1: FAILED - NetworkError: Connection refused (delay: 0ms) # Attempt 2: FAILED - NetworkError: Connection timeout (delay: 1000ms) # Attempt 3: FAILED - NetworkError: Connection timeout (delay: 2000ms) # Attempt 4: FAILED - NetworkError: Connection timeout (delay: 4000ms) # Final Status: FAILED_AFTER_RETRIES ``` ### Workflow Step Failures #### Common Error Patterns **1.",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_1",
              "content": "**Diagnosis:** ```bash # Check system time timedatectl status # Compare with NTP server ntpdate -q pool.ntp.org # Check JWT issued time vs current time jwt_iat=$(echo $CF_ACCESS_TOKEN | cut -d'.' -f2 | base64 -d | jq '.iat') current_time=$(date +%s) skew=$((current_time - jwt_iat)) echo \"Clock skew: $skew seconds\" ``` **Resolution:** ```bash # Sync with NTP server sudo ntpdate -s pool.ntp.org # Enable automatic time sync sudo timedatectl set-ntp true # Restart CloudFlow client cloudflow restart ``` **4. Insufficient Permissions** Error: `User does not have required permissions for this operation` **Check user roles:** ```bash cloudflow auth whoami --verbose # Expected output: # User: john.doe@company.com # Roles: developer, workflow-admin # Scopes: workflow:*, data:read, metrics:read ``` **Request permission elevation:** ```bash # Submit access request cloudflow auth request-access \\ --resource \"workflow:production:deploy\" \\ --justification \"Deploy critical hotfix for TICKET-1234\" # Check pending approvals cloudflow auth list-requests --status pending ``` ### 403 Forbidden Errors #### RBAC Policy Violations CloudFlow uses role-based access control (RBAC) with the following hierarchy: - `viewer` - Read-only access - `developer` - Create and modify workflows (non-production) - `operator` - Execute workflows, view logs - `admin` - Full access to workspace - `platform-admin` - Cross-workspace administration **Verify resource permissions:** ```bash # Check effective permissions cloudflow rbac check \\ --user john.doe@company.com \\ --resource workflow:prod-pipeline \\ --action execute # List all policies affecting user cloudflow rbac policies --user john.doe@company.com --verbose ``` --- ## Performance Problems ### Slow Query Performance #### Symptoms - API response times > 5000ms - Database query latency warnings in logs - CloudFlow UI becomes unresponsive - Timeout errors: `Request exceeded maximum duration of 30000ms` #### Diagnosis Steps **1. Identify Slow Queries** ```bash # Query CloudFlow metrics cloudflow metrics query --metric api_request_duration_ms \\ --filter \"p95 > 5000\" \\ --last 1h # Check database slow query log kubectl logs -n cloudflow deploy/cloudflow-db-primary | \\ grep \"slow query\" | \\ tail -n 50 # Analyze query patterns cloudflow db analyze-queries --min-duration 5000 --limit 20 ``` **2.",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_3",
              "content": "Workflow Context Accumulation** Large workflow executions may accumulate state in memory. **Solution:** ```bash # Configure context cleanup cloudflow config set workflow.context.max_size_mb 100 cloudflow config set workflow.context.cleanup_threshold 0.8 # Enable context persistence to disk cloudflow config set workflow.context.persistence.enabled true cloudflow config set workflow.context.persistence.backend s3 ``` **2. Connection Pool Leaks** **Diagnosis:** ```bash # Check active connections cloudflow db connections --verbose # Expected output: # Active: 45/100 # Idle: 23 # Waiting: 2 # Average age: 245s ``` **Resolution:** ```bash # Adjust connection pool settings cloudflow config set db.pool.max_connections 100 cloudflow config set db.pool.min_connections 10 cloudflow config set db.pool.idle_timeout 300 cloudflow config set db.pool.max_lifetime 1800 # Force connection pool reset cloudflow db pool reset --confirm ``` **3. Event Stream Buffers** Unbounded event buffers can cause memory exhaustion. ```bash # Configure event buffer limits cloudflow config set events.buffer.max_size 10000 cloudflow config set events.buffer.overflow_strategy drop_oldest # Enable event streaming to external sink cloudflow config set events.sink.type kafka cloudflow config set events.sink.kafka.brokers \"kafka-1:9092,kafka-2:9092\" cloudflow config set events.sink.kafka.topic cloudflow-events ``` --- ## Database Connection Issues ### Connection Pool Exhaustion #### Symptoms - Error: `could not obtain connection from pool within 5000ms` - Error: `connection pool exhausted (100/100 connections in use)` - API requests fail with `503 Service Unavailable` - Database CPU usage normal, but connection count at maximum #### Investigation ```bash # Check current connection pool status cloudflow db pool status --detailed # Output example: # Pool Statistics: # Total Connections: 100/100 (100%) # Active: 87 # Idle: 13 # Waiting Requests: 45 # Average Wait Time: 3420ms # Max Wait Time: 8234ms # Identify long-running queries cloudflow db queries --status running --min-duration 30000 # Check connection distribution by client SELECT application_name, state, COUNT(*) as conn_count, AVG(EXTRACT(EPOCH FROM (NOW() - state_change))) as avg_duration_sec FROM pg_stat_activity WHERE datname = 'cloudflow_production' GROUP BY application_name, state ORDER BY conn_count DESC; ``` #### Resolution **Immediate mitigation:** ```bash # Temporarily increase connection limit (requires database restart) cloudflow db config set max_connections 150 # Kill idle connections older than 5 minutes cloudflow db connections kill --idle-timeout 300 # Restart connection pool without downtime kubectl rollout restart deployment/cloudflow-api -n cloudflow kubectl rollout status deployment/cloudflow-api -n cloudflow ``` **Long-term solutions:** 1.",
              "score": null
            }
          ],
          "latency_ms": 939.5277699986764,
          "query_id": "realistic_011",
          "dimension": "synonym",
          "query": "How long can a workflow run before timing out?"
        },
        {
          "key_facts": [
            "3600 seconds",
            "exceeded maximum execution time of 3600 seconds"
          ],
          "found_facts": [
            "3600 seconds",
            "exceeded maximum execution time of 3600 seconds"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "troubleshooting_guide",
            "user_guide"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "user_guide",
              "chunk_id": "user_guide_fix_6",
              "content": "Filter by error type, date range, or execution ID 3. Click an execution to view full details 4. Click **\"Retry\"** to reprocess with the same input data ### Error Notifications Get notified when workflows fail: ```yaml workflow: notifications: on_failure: - type: email to: \"ops-team@company.com\" - type: slack channel: \"#alerts\" message: \"Workflow {{workflow.name}} failed: {{error.message}}\" on_success_after_retry: - type: slack channel: \"#monitoring\" message: \"Workflow recovered after {{error.attempts}} attempts\" ``` ## Workflow Limits CloudFlow enforces the following limits to ensure platform stability and performance: ### Steps Per Workflow - **Maximum**: 50 steps per workflow - **Recommendation**: Keep workflows focused and modular. If you need more steps, consider splitting into multiple workflows connected via webhooks. ### Execution Timeout - **Default**: 3600 seconds (60 minutes) - **Behavior**: Workflows exceeding this timeout are automatically terminated - **Custom Timeouts**: Enterprise plans can request custom timeout limits **Setting Step-Level Timeouts:** ```yaml - id: long_running_task action: http_request config: url: \"https://api.example.com/process\" timeout: 300 # 5 minutes for this specific step ``` ### Execution Limits - **Maximum**: 1000 executions per day (per workflow) - **Rate Limiting**: 100 concurrent executions per workflow - **Burst Limit**: 10 executions per second **What happens when limits are reached:** - New executions are queued automatically - Webhook triggers return HTTP 429 (Too Many Requests) - Scheduled executions are skipped (logged in audit trail) - Email notifications sent to workflow owner **Monitoring Usage:** View real-time metrics in your workflow dashboard: - Executions today: 847 / 1000 - Average duration: 12.3s - Success rate: 98.2% - Current queue depth: 3 ### Data Limits - **Maximum request/response size**: 10MB per action - **Maximum execution payload**: 50MB total - **Variable value size**: 1MB per variable ### Enterprise Plan Limits Enterprise customers can request increased limits: - Up to 100 steps per workflow - Up to 10,000 executions per day - Up to 7200 second timeout (2 hours) - Priority execution queue - Dedicated capacity allocation Contact sales@cloudflow.io for Enterprise pricing and custom limits. ## Best Practices Follow these best practices to build reliable, maintainable workflows: ### 1. Use Descriptive Names **Good:** - Workflow: \"Sync Customer Data from Salesforce to Database\" - Step: \"validate_customer_email\" **Bad:** - Workflow: \"Workflow 1\" - Step: \"step3\" ### 2.",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_4",
              "content": "**Implement connection pooling optimization:** ```bash # Use PgBouncer for connection pooling kubectl apply -f cloudflow-pgbouncer.yaml # Configure CloudFlow to use PgBouncer cloudflow config set db.host pgbouncer.cloudflow.svc.cluster.local cloudflow config set db.port 6432 cloudflow config set db.pool.mode transaction ``` 2. **Add read replicas:** ```bash # Route read-only queries to replicas cloudflow db replicas add --count 2 cloudflow config set db.read_replicas \"replica-1:5432,replica-2:5432\" cloudflow config set db.read_write_split true ``` ### Connection Timeout Errors #### Error Messages - `connection timeout after 30000ms` - `could not connect to database server at 10.0.2.45:5432` - `database server unreachable` #### Troubleshooting Steps ```bash # Test network connectivity telnet cloudflow-db.internal.company.com 5432 # Check DNS resolution nslookup cloudflow-db.internal.company.com # Verify database is accepting connections pg_isready -h cloudflow-db.internal.company.com -p 5432 -U cloudflow # Check firewall rules sudo iptables -L -n | grep 5432 # Test from CloudFlow pod network kubectl run -n cloudflow debug-pod --rm -i --tty \\ --image=postgres:14 -- \\ psql -h cloudflow-db.internal.company.com -U cloudflow -d cloudflow_production # Review database logs for connection rejections kubectl logs -n cloudflow statefulset/cloudflow-db --tail=100 | \\ grep -i \"connection\\|reject\\|authentication\" ``` ### Maximum Connection Limit (100) Reached This is a hard limit in CloudFlow's database tier. #### Permanent Solutions **Option 1: Upgrade database tier** ```bash # Check available tiers cloudflow db tiers list # Upgrade to higher tier (supports 200 connections) cloudflow db upgrade --tier standard-plus --confirm # Monitor migration progress cloudflow db migration status ``` **Option 2: Implement aggressive connection reuse** ```bash # Reduce connection lifetime cloudflow config set db.pool.max_lifetime 600 # 10 minutes # Enable prepared statement caching cloudflow config set db.prepared_statements.cache true cloudflow config set db.prepared_statements.max_size 250 # Reduce idle connection timeout cloudflow config set db.pool.idle_timeout 120 # 2 minutes ``` --- ## Workflow Execution Failures ### Timeout Errors (3600 second limit) #### Error Message ``` WorkflowExecutionError: Workflow exceeded maximum execution time of 3600 seconds Status: TIMEOUT Workflow ID: wf_9k2n4m8p1q Execution ID: exec_7h3j6k9m2n ``` #### Analysis ```bash # Get workflow execution details cloudflow workflows executions get exec_7h3j6k9m2n --verbose # Check step-by-step breakdown cloudflow workflows executions steps exec_7h3j6k9m2n # Sample output: # Step 1: data_ingestion - Duration: 245s - Status: SUCCESS # Step 2: data_validation - Duration: 123s - Status: SUCCESS # Step 3: data_transformation - Duration: 3189s - Status: TIMEOUT # Step 4: data_export - Duration: 0s - Status: SKIPPED # Identify bottleneck step cloudflow workflows analyze",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_5",
              "content": "exec_7h3j6k9m2n --show-bottlenecks ``` #### Solutions **1. Increase workflow timeout (if justified):** ```bash # Update workflow configuration cloudflow workflows update wf_9k2n4m8p1q \\ --timeout 7200 \\ --reason \"Large dataset processing requires extended time\" # Verify update cloudflow workflows get wf_9k2n4m8p1q | grep timeout ``` **2. Optimize slow steps:** ```bash # Enable parallel processing cloudflow workflows update wf_9k2n4m8p1q \\ --step data_transformation \\ --parallel-workers 8 \\ --batch-size 1000 # Add checkpointing for long operations cloudflow workflows update wf_9k2n4m8p1q \\ --step data_transformation \\ --enable-checkpointing \\ --checkpoint-interval 300 ``` **3. Split workflow into smaller workflows:** ```bash # Create sub-workflows cloudflow workflows create data-pipeline-part1 \\ --steps \"data_ingestion,data_validation\" \\ --timeout 1800 cloudflow workflows create data-pipeline-part2 \\ --steps \"data_transformation,data_export\" \\ --timeout 3600 \\ --trigger workflow_completed \\ --trigger-workflow data-pipeline-part1 ``` ### Retry Logic and Exponential Backoff CloudFlow implements automatic retry with exponential backoff for transient failures: - Max retries: 3 - Initial delay: 1 second - Backoff multiplier: 2 - Max delay: 60 seconds #### Retry Sequence ``` Attempt 1: Immediate Attempt 2: Wait 1s (2^0 * 1s) Attempt 3: Wait 2s (2^1 * 1s) Attempt 4: Wait 4s (2^2 * 1s) ``` #### Configuration ```bash # View current retry settings cloudflow workflows get wf_9k2n4m8p1q --format json | jq '.retry_policy' # Customize retry behavior cloudflow workflows update wf_9k2n4m8p1q \\ --retry-max-attempts 5 \\ --retry-initial-delay 2000 \\ --retry-backoff-multiplier 2 \\ --retry-max-delay 120000 \\ --retry-on-errors \"NETWORK_ERROR,TIMEOUT,RATE_LIMIT\" # Disable retry for specific step cloudflow workflows update wf_9k2n4m8p1q \\ --step data_export \\ --retry-enabled false ``` #### Monitoring Retries ```bash # List failed executions with retry information cloudflow workflows executions list \\ --status FAILED \\ --show-retries \\ --last 7d # Get retry history for specific execution cloudflow workflows executions retries exec_7h3j6k9m2n # Output: # Execution: exec_7h3j6k9m2n # Attempt 1: FAILED - NetworkError: Connection refused (delay: 0ms) # Attempt 2: FAILED - NetworkError: Connection timeout (delay: 1000ms) # Attempt 3: FAILED - NetworkError: Connection timeout (delay: 2000ms) # Attempt 4: FAILED - NetworkError: Connection timeout (delay: 4000ms) # Final Status: FAILED_AFTER_RETRIES ``` ### Workflow Step Failures #### Common Error Patterns **1.",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_3",
              "content": "Workflow Context Accumulation** Large workflow executions may accumulate state in memory. **Solution:** ```bash # Configure context cleanup cloudflow config set workflow.context.max_size_mb 100 cloudflow config set workflow.context.cleanup_threshold 0.8 # Enable context persistence to disk cloudflow config set workflow.context.persistence.enabled true cloudflow config set workflow.context.persistence.backend s3 ``` **2. Connection Pool Leaks** **Diagnosis:** ```bash # Check active connections cloudflow db connections --verbose # Expected output: # Active: 45/100 # Idle: 23 # Waiting: 2 # Average age: 245s ``` **Resolution:** ```bash # Adjust connection pool settings cloudflow config set db.pool.max_connections 100 cloudflow config set db.pool.min_connections 10 cloudflow config set db.pool.idle_timeout 300 cloudflow config set db.pool.max_lifetime 1800 # Force connection pool reset cloudflow db pool reset --confirm ``` **3. Event Stream Buffers** Unbounded event buffers can cause memory exhaustion. ```bash # Configure event buffer limits cloudflow config set events.buffer.max_size 10000 cloudflow config set events.buffer.overflow_strategy drop_oldest # Enable event streaming to external sink cloudflow config set events.sink.type kafka cloudflow config set events.sink.kafka.brokers \"kafka-1:9092,kafka-2:9092\" cloudflow config set events.sink.kafka.topic cloudflow-events ``` --- ## Database Connection Issues ### Connection Pool Exhaustion #### Symptoms - Error: `could not obtain connection from pool within 5000ms` - Error: `connection pool exhausted (100/100 connections in use)` - API requests fail with `503 Service Unavailable` - Database CPU usage normal, but connection count at maximum #### Investigation ```bash # Check current connection pool status cloudflow db pool status --detailed # Output example: # Pool Statistics: # Total Connections: 100/100 (100%) # Active: 87 # Idle: 13 # Waiting Requests: 45 # Average Wait Time: 3420ms # Max Wait Time: 8234ms # Identify long-running queries cloudflow db queries --status running --min-duration 30000 # Check connection distribution by client SELECT application_name, state, COUNT(*) as conn_count, AVG(EXTRACT(EPOCH FROM (NOW() - state_change))) as avg_duration_sec FROM pg_stat_activity WHERE datname = 'cloudflow_production' GROUP BY application_name, state ORDER BY conn_count DESC; ``` #### Resolution **Immediate mitigation:** ```bash # Temporarily increase connection limit (requires database restart) cloudflow db config set max_connections 150 # Kill idle connections older than 5 minutes cloudflow db connections kill --idle-timeout 300 # Restart connection pool without downtime kubectl rollout restart deployment/cloudflow-api -n cloudflow kubectl rollout status deployment/cloudflow-api -n cloudflow ``` **Long-term solutions:** 1.",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_1",
              "content": "**Diagnosis:** ```bash # Check system time timedatectl status # Compare with NTP server ntpdate -q pool.ntp.org # Check JWT issued time vs current time jwt_iat=$(echo $CF_ACCESS_TOKEN | cut -d'.' -f2 | base64 -d | jq '.iat') current_time=$(date +%s) skew=$((current_time - jwt_iat)) echo \"Clock skew: $skew seconds\" ``` **Resolution:** ```bash # Sync with NTP server sudo ntpdate -s pool.ntp.org # Enable automatic time sync sudo timedatectl set-ntp true # Restart CloudFlow client cloudflow restart ``` **4. Insufficient Permissions** Error: `User does not have required permissions for this operation` **Check user roles:** ```bash cloudflow auth whoami --verbose # Expected output: # User: john.doe@company.com # Roles: developer, workflow-admin # Scopes: workflow:*, data:read, metrics:read ``` **Request permission elevation:** ```bash # Submit access request cloudflow auth request-access \\ --resource \"workflow:production:deploy\" \\ --justification \"Deploy critical hotfix for TICKET-1234\" # Check pending approvals cloudflow auth list-requests --status pending ``` ### 403 Forbidden Errors #### RBAC Policy Violations CloudFlow uses role-based access control (RBAC) with the following hierarchy: - `viewer` - Read-only access - `developer` - Create and modify workflows (non-production) - `operator` - Execute workflows, view logs - `admin` - Full access to workspace - `platform-admin` - Cross-workspace administration **Verify resource permissions:** ```bash # Check effective permissions cloudflow rbac check \\ --user john.doe@company.com \\ --resource workflow:prod-pipeline \\ --action execute # List all policies affecting user cloudflow rbac policies --user john.doe@company.com --verbose ``` --- ## Performance Problems ### Slow Query Performance #### Symptoms - API response times > 5000ms - Database query latency warnings in logs - CloudFlow UI becomes unresponsive - Timeout errors: `Request exceeded maximum duration of 30000ms` #### Diagnosis Steps **1. Identify Slow Queries** ```bash # Query CloudFlow metrics cloudflow metrics query --metric api_request_duration_ms \\ --filter \"p95 > 5000\" \\ --last 1h # Check database slow query log kubectl logs -n cloudflow deploy/cloudflow-db-primary | \\ grep \"slow query\" | \\ tail -n 50 # Analyze query patterns cloudflow db analyze-queries --min-duration 5000 --limit 20 ``` **2.",
              "score": null
            }
          ],
          "latency_ms": 984.81890399853,
          "query_id": "realistic_011",
          "dimension": "problem",
          "query": "My workflow is being killed after an hour"
        },
        {
          "key_facts": [
            "3600 seconds",
            "exceeded maximum execution time of 3600 seconds"
          ],
          "found_facts": [
            "3600 seconds",
            "exceeded maximum execution time of 3600 seconds"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "troubleshooting_guide",
            "user_guide"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "user_guide",
              "chunk_id": "user_guide_fix_6",
              "content": "Filter by error type, date range, or execution ID 3. Click an execution to view full details 4. Click **\"Retry\"** to reprocess with the same input data ### Error Notifications Get notified when workflows fail: ```yaml workflow: notifications: on_failure: - type: email to: \"ops-team@company.com\" - type: slack channel: \"#alerts\" message: \"Workflow {{workflow.name}} failed: {{error.message}}\" on_success_after_retry: - type: slack channel: \"#monitoring\" message: \"Workflow recovered after {{error.attempts}} attempts\" ``` ## Workflow Limits CloudFlow enforces the following limits to ensure platform stability and performance: ### Steps Per Workflow - **Maximum**: 50 steps per workflow - **Recommendation**: Keep workflows focused and modular. If you need more steps, consider splitting into multiple workflows connected via webhooks. ### Execution Timeout - **Default**: 3600 seconds (60 minutes) - **Behavior**: Workflows exceeding this timeout are automatically terminated - **Custom Timeouts**: Enterprise plans can request custom timeout limits **Setting Step-Level Timeouts:** ```yaml - id: long_running_task action: http_request config: url: \"https://api.example.com/process\" timeout: 300 # 5 minutes for this specific step ``` ### Execution Limits - **Maximum**: 1000 executions per day (per workflow) - **Rate Limiting**: 100 concurrent executions per workflow - **Burst Limit**: 10 executions per second **What happens when limits are reached:** - New executions are queued automatically - Webhook triggers return HTTP 429 (Too Many Requests) - Scheduled executions are skipped (logged in audit trail) - Email notifications sent to workflow owner **Monitoring Usage:** View real-time metrics in your workflow dashboard: - Executions today: 847 / 1000 - Average duration: 12.3s - Success rate: 98.2% - Current queue depth: 3 ### Data Limits - **Maximum request/response size**: 10MB per action - **Maximum execution payload**: 50MB total - **Variable value size**: 1MB per variable ### Enterprise Plan Limits Enterprise customers can request increased limits: - Up to 100 steps per workflow - Up to 10,000 executions per day - Up to 7200 second timeout (2 hours) - Priority execution queue - Dedicated capacity allocation Contact sales@cloudflow.io for Enterprise pricing and custom limits. ## Best Practices Follow these best practices to build reliable, maintainable workflows: ### 1. Use Descriptive Names **Good:** - Workflow: \"Sync Customer Data from Salesforce to Database\" - Step: \"validate_customer_email\" **Bad:** - Workflow: \"Workflow 1\" - Step: \"step3\" ### 2.",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_5",
              "content": "exec_7h3j6k9m2n --show-bottlenecks ``` #### Solutions **1. Increase workflow timeout (if justified):** ```bash # Update workflow configuration cloudflow workflows update wf_9k2n4m8p1q \\ --timeout 7200 \\ --reason \"Large dataset processing requires extended time\" # Verify update cloudflow workflows get wf_9k2n4m8p1q | grep timeout ``` **2. Optimize slow steps:** ```bash # Enable parallel processing cloudflow workflows update wf_9k2n4m8p1q \\ --step data_transformation \\ --parallel-workers 8 \\ --batch-size 1000 # Add checkpointing for long operations cloudflow workflows update wf_9k2n4m8p1q \\ --step data_transformation \\ --enable-checkpointing \\ --checkpoint-interval 300 ``` **3. Split workflow into smaller workflows:** ```bash # Create sub-workflows cloudflow workflows create data-pipeline-part1 \\ --steps \"data_ingestion,data_validation\" \\ --timeout 1800 cloudflow workflows create data-pipeline-part2 \\ --steps \"data_transformation,data_export\" \\ --timeout 3600 \\ --trigger workflow_completed \\ --trigger-workflow data-pipeline-part1 ``` ### Retry Logic and Exponential Backoff CloudFlow implements automatic retry with exponential backoff for transient failures: - Max retries: 3 - Initial delay: 1 second - Backoff multiplier: 2 - Max delay: 60 seconds #### Retry Sequence ``` Attempt 1: Immediate Attempt 2: Wait 1s (2^0 * 1s) Attempt 3: Wait 2s (2^1 * 1s) Attempt 4: Wait 4s (2^2 * 1s) ``` #### Configuration ```bash # View current retry settings cloudflow workflows get wf_9k2n4m8p1q --format json | jq '.retry_policy' # Customize retry behavior cloudflow workflows update wf_9k2n4m8p1q \\ --retry-max-attempts 5 \\ --retry-initial-delay 2000 \\ --retry-backoff-multiplier 2 \\ --retry-max-delay 120000 \\ --retry-on-errors \"NETWORK_ERROR,TIMEOUT,RATE_LIMIT\" # Disable retry for specific step cloudflow workflows update wf_9k2n4m8p1q \\ --step data_export \\ --retry-enabled false ``` #### Monitoring Retries ```bash # List failed executions with retry information cloudflow workflows executions list \\ --status FAILED \\ --show-retries \\ --last 7d # Get retry history for specific execution cloudflow workflows executions retries exec_7h3j6k9m2n # Output: # Execution: exec_7h3j6k9m2n # Attempt 1: FAILED - NetworkError: Connection refused (delay: 0ms) # Attempt 2: FAILED - NetworkError: Connection timeout (delay: 1000ms) # Attempt 3: FAILED - NetworkError: Connection timeout (delay: 2000ms) # Attempt 4: FAILED - NetworkError: Connection timeout (delay: 4000ms) # Final Status: FAILED_AFTER_RETRIES ``` ### Workflow Step Failures #### Common Error Patterns **1.",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_4",
              "content": "**Implement connection pooling optimization:** ```bash # Use PgBouncer for connection pooling kubectl apply -f cloudflow-pgbouncer.yaml # Configure CloudFlow to use PgBouncer cloudflow config set db.host pgbouncer.cloudflow.svc.cluster.local cloudflow config set db.port 6432 cloudflow config set db.pool.mode transaction ``` 2. **Add read replicas:** ```bash # Route read-only queries to replicas cloudflow db replicas add --count 2 cloudflow config set db.read_replicas \"replica-1:5432,replica-2:5432\" cloudflow config set db.read_write_split true ``` ### Connection Timeout Errors #### Error Messages - `connection timeout after 30000ms` - `could not connect to database server at 10.0.2.45:5432` - `database server unreachable` #### Troubleshooting Steps ```bash # Test network connectivity telnet cloudflow-db.internal.company.com 5432 # Check DNS resolution nslookup cloudflow-db.internal.company.com # Verify database is accepting connections pg_isready -h cloudflow-db.internal.company.com -p 5432 -U cloudflow # Check firewall rules sudo iptables -L -n | grep 5432 # Test from CloudFlow pod network kubectl run -n cloudflow debug-pod --rm -i --tty \\ --image=postgres:14 -- \\ psql -h cloudflow-db.internal.company.com -U cloudflow -d cloudflow_production # Review database logs for connection rejections kubectl logs -n cloudflow statefulset/cloudflow-db --tail=100 | \\ grep -i \"connection\\|reject\\|authentication\" ``` ### Maximum Connection Limit (100) Reached This is a hard limit in CloudFlow's database tier. #### Permanent Solutions **Option 1: Upgrade database tier** ```bash # Check available tiers cloudflow db tiers list # Upgrade to higher tier (supports 200 connections) cloudflow db upgrade --tier standard-plus --confirm # Monitor migration progress cloudflow db migration status ``` **Option 2: Implement aggressive connection reuse** ```bash # Reduce connection lifetime cloudflow config set db.pool.max_lifetime 600 # 10 minutes # Enable prepared statement caching cloudflow config set db.prepared_statements.cache true cloudflow config set db.prepared_statements.max_size 250 # Reduce idle connection timeout cloudflow config set db.pool.idle_timeout 120 # 2 minutes ``` --- ## Workflow Execution Failures ### Timeout Errors (3600 second limit) #### Error Message ``` WorkflowExecutionError: Workflow exceeded maximum execution time of 3600 seconds Status: TIMEOUT Workflow ID: wf_9k2n4m8p1q Execution ID: exec_7h3j6k9m2n ``` #### Analysis ```bash # Get workflow execution details cloudflow workflows executions get exec_7h3j6k9m2n --verbose # Check step-by-step breakdown cloudflow workflows executions steps exec_7h3j6k9m2n # Sample output: # Step 1: data_ingestion - Duration: 245s - Status: SUCCESS # Step 2: data_validation - Duration: 123s - Status: SUCCESS # Step 3: data_transformation - Duration: 3189s - Status: TIMEOUT # Step 4: data_export - Duration: 0s - Status: SKIPPED # Identify bottleneck step cloudflow workflows analyze",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_3",
              "content": "Workflow Context Accumulation** Large workflow executions may accumulate state in memory. **Solution:** ```bash # Configure context cleanup cloudflow config set workflow.context.max_size_mb 100 cloudflow config set workflow.context.cleanup_threshold 0.8 # Enable context persistence to disk cloudflow config set workflow.context.persistence.enabled true cloudflow config set workflow.context.persistence.backend s3 ``` **2. Connection Pool Leaks** **Diagnosis:** ```bash # Check active connections cloudflow db connections --verbose # Expected output: # Active: 45/100 # Idle: 23 # Waiting: 2 # Average age: 245s ``` **Resolution:** ```bash # Adjust connection pool settings cloudflow config set db.pool.max_connections 100 cloudflow config set db.pool.min_connections 10 cloudflow config set db.pool.idle_timeout 300 cloudflow config set db.pool.max_lifetime 1800 # Force connection pool reset cloudflow db pool reset --confirm ``` **3. Event Stream Buffers** Unbounded event buffers can cause memory exhaustion. ```bash # Configure event buffer limits cloudflow config set events.buffer.max_size 10000 cloudflow config set events.buffer.overflow_strategy drop_oldest # Enable event streaming to external sink cloudflow config set events.sink.type kafka cloudflow config set events.sink.kafka.brokers \"kafka-1:9092,kafka-2:9092\" cloudflow config set events.sink.kafka.topic cloudflow-events ``` --- ## Database Connection Issues ### Connection Pool Exhaustion #### Symptoms - Error: `could not obtain connection from pool within 5000ms` - Error: `connection pool exhausted (100/100 connections in use)` - API requests fail with `503 Service Unavailable` - Database CPU usage normal, but connection count at maximum #### Investigation ```bash # Check current connection pool status cloudflow db pool status --detailed # Output example: # Pool Statistics: # Total Connections: 100/100 (100%) # Active: 87 # Idle: 13 # Waiting Requests: 45 # Average Wait Time: 3420ms # Max Wait Time: 8234ms # Identify long-running queries cloudflow db queries --status running --min-duration 30000 # Check connection distribution by client SELECT application_name, state, COUNT(*) as conn_count, AVG(EXTRACT(EPOCH FROM (NOW() - state_change))) as avg_duration_sec FROM pg_stat_activity WHERE datname = 'cloudflow_production' GROUP BY application_name, state ORDER BY conn_count DESC; ``` #### Resolution **Immediate mitigation:** ```bash # Temporarily increase connection limit (requires database restart) cloudflow db config set max_connections 150 # Kill idle connections older than 5 minutes cloudflow db connections kill --idle-timeout 300 # Restart connection pool without downtime kubectl rollout restart deployment/cloudflow-api -n cloudflow kubectl rollout status deployment/cloudflow-api -n cloudflow ``` **Long-term solutions:** 1.",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_1",
              "content": "**Diagnosis:** ```bash # Check system time timedatectl status # Compare with NTP server ntpdate -q pool.ntp.org # Check JWT issued time vs current time jwt_iat=$(echo $CF_ACCESS_TOKEN | cut -d'.' -f2 | base64 -d | jq '.iat') current_time=$(date +%s) skew=$((current_time - jwt_iat)) echo \"Clock skew: $skew seconds\" ``` **Resolution:** ```bash # Sync with NTP server sudo ntpdate -s pool.ntp.org # Enable automatic time sync sudo timedatectl set-ntp true # Restart CloudFlow client cloudflow restart ``` **4. Insufficient Permissions** Error: `User does not have required permissions for this operation` **Check user roles:** ```bash cloudflow auth whoami --verbose # Expected output: # User: john.doe@company.com # Roles: developer, workflow-admin # Scopes: workflow:*, data:read, metrics:read ``` **Request permission elevation:** ```bash # Submit access request cloudflow auth request-access \\ --resource \"workflow:production:deploy\" \\ --justification \"Deploy critical hotfix for TICKET-1234\" # Check pending approvals cloudflow auth list-requests --status pending ``` ### 403 Forbidden Errors #### RBAC Policy Violations CloudFlow uses role-based access control (RBAC) with the following hierarchy: - `viewer` - Read-only access - `developer` - Create and modify workflows (non-production) - `operator` - Execute workflows, view logs - `admin` - Full access to workspace - `platform-admin` - Cross-workspace administration **Verify resource permissions:** ```bash # Check effective permissions cloudflow rbac check \\ --user john.doe@company.com \\ --resource workflow:prod-pipeline \\ --action execute # List all policies affecting user cloudflow rbac policies --user john.doe@company.com --verbose ``` --- ## Performance Problems ### Slow Query Performance #### Symptoms - API response times > 5000ms - Database query latency warnings in logs - CloudFlow UI becomes unresponsive - Timeout errors: `Request exceeded maximum duration of 30000ms` #### Diagnosis Steps **1. Identify Slow Queries** ```bash # Query CloudFlow metrics cloudflow metrics query --metric api_request_duration_ms \\ --filter \"p95 > 5000\" \\ --last 1h # Check database slow query log kubectl logs -n cloudflow deploy/cloudflow-db-primary | \\ grep \"slow query\" | \\ tail -n 50 # Analyze query patterns cloudflow db analyze-queries --min-duration 5000 --limit 20 ``` **2.",
              "score": null
            }
          ],
          "latency_ms": 809.1930330010655,
          "query_id": "realistic_011",
          "dimension": "casual",
          "query": "workflow timeout"
        },
        {
          "key_facts": [
            "3600 seconds",
            "exceeded maximum execution time of 3600 seconds"
          ],
          "found_facts": [
            "3600 seconds",
            "exceeded maximum execution time of 3600 seconds"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "troubleshooting_guide",
            "user_guide"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_10",
              "content": "- Execution start rate: 500 per second - Completion rate: 450 per second (average 2-second execution time) **Database**: - Read throughput: 50,000 queries per second (across replicas) - Write throughput: 15,000 transactions per second - Connection capacity: 2,000 concurrent connections **Kafka**: - Message ingestion: 100,000 messages per second - Consumer throughput: 80,000 messages per second (aggregated) - End-to-end latency: < 100ms (P99) ### Resource Utilization **CPU Utilization** (Target: 60-70% average): - API Gateway: 55% average, 80% peak - Workflow Engine: 65% average, 85% peak - Auth Service: 40% average, 60% peak **Memory Utilization** (Target: < 80%): - API Gateway: 2.5GB average per pod (4GB allocated) - Workflow Engine: 6GB average per pod (8GB allocated) - Notification Service: 2.8GB average per pod (4GB allocated) **Network Throughput**: - Ingress: 2 Gbps average, 5 Gbps peak - Egress: 1.5 Gbps average, 4 Gbps peak - Internal (service mesh): 8 Gbps average ### Monitoring & Alerting **Key Metrics** (Monitored via Prometheus): - Request rate (per service, per endpoint) - Error rate (4xx, 5xx responses) - Latency percentiles (P50, P95, P99) - Resource utilization (CPU, memory, disk) - Cache hit/miss ratios - Database connection pool saturation - Kafka consumer lag **Alerts** (via PagerDuty): - P1 (Immediate): API error rate > 1%, database connection failure - P2 (< 15 min): P99 latency > 500ms, cache hit rate < 80% - P3 (< 1 hour): Resource utilization > 85%, replica count < minimum **SLI/SLO/SLA**: - SLI: API success rate (non-5xx responses) - SLO: 99.9% uptime per month (43 minutes downtime allowance) - SLA: 99.5% uptime guarantee (customer-facing SLA with credits) --- ## Disaster Recovery ### Recovery Objectives **RPO (Recovery Point Objective): 1 hour** - Maximum acceptable data loss: 1 hour of transactions - Achieved through: Continuous database replication + hourly snapshots - Kafka retention: 7 days allows event replay **RTO (Recovery Time Objective): 4 hours** - Maximum acceptable downtime: 4 hours for full system recovery - Includes: Failover, data verification, and service restoration - Automated runbooks reduce RTO to < 2 hours for common scenarios ### High Availability Architecture **Multi-AZ Deployment**: ``` Region: us-east-1 AZ-1a: AZ-1b: AZ-1c: - API Gateway (4) - API Gateway (4) - API Gateway (4) - Workflow Engine (6) - Workflow Engine (5) - Workflow Engine (5) - Auth Service (3) - Auth Service (3) - Auth",
              "score": null
            },
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_1",
              "content": "## Rate Limiting To ensure fair usage and system stability, CloudFlow enforces rate limits on all API endpoints. **Default Limits:** - 100 requests per minute per authenticated user - 20 requests per minute for unauthenticated requests - Burst allowance: 150 requests in a 10-second window ### Rate Limit Headers Every API response includes rate limit information: ``` X-RateLimit-Limit: 100 X-RateLimit-Remaining: 87 X-RateLimit-Reset: 1640995200 ``` When you exceed the rate limit, you'll receive a `429 Too Many Requests` response: ```json { \"error\": { \"code\": \"rate_limit_exceeded\", \"message\": \"Rate limit exceeded. Please retry after 42 seconds.\", \"retry_after\": 42 } } ``` **Best Practices:** - Monitor `X-RateLimit-Remaining` header values - Implement exponential backoff when receiving 429 responses - Cache responses when appropriate to reduce API calls - Consider upgrading to Enterprise tier for higher limits (1000 req/min) ## Pagination List endpoints return paginated results to optimize performance. Use `limit` and `offset` parameters to navigate through result sets. **Parameters:** - `limit`: Number of items per page (default: 20, max: 100) - `offset`: Number of items to skip (default: 0) **Example Request:** ```bash curl -H \"X-API-Key: YOUR_API_KEY\" \\ \"https://api.cloudflow.io/v2/workflows?limit=50&offset=100\" ``` **Response Structure:** ```json { \"data\": [...], \"pagination\": { \"total\": 347, \"limit\": 50, \"offset\": 100, \"has_more\": true } } ``` **Python Example:** ```python import requests def fetch_all_workflows(api_key): base_url = \"https://api.cloudflow.io/v2/workflows\" headers = {\"X-API-Key\": api_key} all_workflows = [] offset = 0 limit = 100 while True: response = requests.get( base_url, headers=headers, params={\"limit\": limit, \"offset\": offset} ) data = response.json() all_workflows.extend(data['data']) if not data['pagination']['has_more']: break offset += limit return all_workflows ``` ## Endpoints ### Workflows #### List Workflows Retrieve a paginated list of all workflows in your account. **Endpoint:** `GET /workflows` **Query Parameters:** - `status` (optional): Filter by status (`active`, `paused`, `archived`) - `created_after` (optional): ISO 8601 timestamp - `limit` (optional): Items per page (max 100) - `offset` (optional): Pagination offset **Example Request:** ```bash curl -H \"X-API-Key: YOUR_API_KEY\" \\ \"https://api.cloudflow.io/v2/workflows?status=active&limit=25\" ``` **Example Response (200 OK):** ```json { \"data\": [ { \"id\": \"wf_8x7k2m9p\", \"name\": \"Data Processing Pipeline\", \"description\": \"Processes customer data every hour\", \"status\": \"active\", \"trigger\": { \"type\": \"schedule\", \"cron\": \"0 * * * *\" }, \"steps\": 12, \"created_at\": \"2026-01-15T10:30:00Z\", \"updated_at\": \"2026-01-23T14:22:00Z\", \"last_run\": \"2026-01-24T09:00:00Z\" } ], \"pagination\": { \"total\": 1, \"limit\": 25, \"offset\": 0, \"has_more\": false } } ``` #### Create Workflow Create a new workflow with specified configuration.",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_5",
              "content": "exec_7h3j6k9m2n --show-bottlenecks ``` #### Solutions **1. Increase workflow timeout (if justified):** ```bash # Update workflow configuration cloudflow workflows update wf_9k2n4m8p1q \\ --timeout 7200 \\ --reason \"Large dataset processing requires extended time\" # Verify update cloudflow workflows get wf_9k2n4m8p1q | grep timeout ``` **2. Optimize slow steps:** ```bash # Enable parallel processing cloudflow workflows update wf_9k2n4m8p1q \\ --step data_transformation \\ --parallel-workers 8 \\ --batch-size 1000 # Add checkpointing for long operations cloudflow workflows update wf_9k2n4m8p1q \\ --step data_transformation \\ --enable-checkpointing \\ --checkpoint-interval 300 ``` **3. Split workflow into smaller workflows:** ```bash # Create sub-workflows cloudflow workflows create data-pipeline-part1 \\ --steps \"data_ingestion,data_validation\" \\ --timeout 1800 cloudflow workflows create data-pipeline-part2 \\ --steps \"data_transformation,data_export\" \\ --timeout 3600 \\ --trigger workflow_completed \\ --trigger-workflow data-pipeline-part1 ``` ### Retry Logic and Exponential Backoff CloudFlow implements automatic retry with exponential backoff for transient failures: - Max retries: 3 - Initial delay: 1 second - Backoff multiplier: 2 - Max delay: 60 seconds #### Retry Sequence ``` Attempt 1: Immediate Attempt 2: Wait 1s (2^0 * 1s) Attempt 3: Wait 2s (2^1 * 1s) Attempt 4: Wait 4s (2^2 * 1s) ``` #### Configuration ```bash # View current retry settings cloudflow workflows get wf_9k2n4m8p1q --format json | jq '.retry_policy' # Customize retry behavior cloudflow workflows update wf_9k2n4m8p1q \\ --retry-max-attempts 5 \\ --retry-initial-delay 2000 \\ --retry-backoff-multiplier 2 \\ --retry-max-delay 120000 \\ --retry-on-errors \"NETWORK_ERROR,TIMEOUT,RATE_LIMIT\" # Disable retry for specific step cloudflow workflows update wf_9k2n4m8p1q \\ --step data_export \\ --retry-enabled false ``` #### Monitoring Retries ```bash # List failed executions with retry information cloudflow workflows executions list \\ --status FAILED \\ --show-retries \\ --last 7d # Get retry history for specific execution cloudflow workflows executions retries exec_7h3j6k9m2n # Output: # Execution: exec_7h3j6k9m2n # Attempt 1: FAILED - NetworkError: Connection refused (delay: 0ms) # Attempt 2: FAILED - NetworkError: Connection timeout (delay: 1000ms) # Attempt 3: FAILED - NetworkError: Connection timeout (delay: 2000ms) # Attempt 4: FAILED - NetworkError: Connection timeout (delay: 4000ms) # Final Status: FAILED_AFTER_RETRIES ``` ### Workflow Step Failures #### Common Error Patterns **1.",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_4",
              "content": "**Implement connection pooling optimization:** ```bash # Use PgBouncer for connection pooling kubectl apply -f cloudflow-pgbouncer.yaml # Configure CloudFlow to use PgBouncer cloudflow config set db.host pgbouncer.cloudflow.svc.cluster.local cloudflow config set db.port 6432 cloudflow config set db.pool.mode transaction ``` 2. **Add read replicas:** ```bash # Route read-only queries to replicas cloudflow db replicas add --count 2 cloudflow config set db.read_replicas \"replica-1:5432,replica-2:5432\" cloudflow config set db.read_write_split true ``` ### Connection Timeout Errors #### Error Messages - `connection timeout after 30000ms` - `could not connect to database server at 10.0.2.45:5432` - `database server unreachable` #### Troubleshooting Steps ```bash # Test network connectivity telnet cloudflow-db.internal.company.com 5432 # Check DNS resolution nslookup cloudflow-db.internal.company.com # Verify database is accepting connections pg_isready -h cloudflow-db.internal.company.com -p 5432 -U cloudflow # Check firewall rules sudo iptables -L -n | grep 5432 # Test from CloudFlow pod network kubectl run -n cloudflow debug-pod --rm -i --tty \\ --image=postgres:14 -- \\ psql -h cloudflow-db.internal.company.com -U cloudflow -d cloudflow_production # Review database logs for connection rejections kubectl logs -n cloudflow statefulset/cloudflow-db --tail=100 | \\ grep -i \"connection\\|reject\\|authentication\" ``` ### Maximum Connection Limit (100) Reached This is a hard limit in CloudFlow's database tier. #### Permanent Solutions **Option 1: Upgrade database tier** ```bash # Check available tiers cloudflow db tiers list # Upgrade to higher tier (supports 200 connections) cloudflow db upgrade --tier standard-plus --confirm # Monitor migration progress cloudflow db migration status ``` **Option 2: Implement aggressive connection reuse** ```bash # Reduce connection lifetime cloudflow config set db.pool.max_lifetime 600 # 10 minutes # Enable prepared statement caching cloudflow config set db.prepared_statements.cache true cloudflow config set db.prepared_statements.max_size 250 # Reduce idle connection timeout cloudflow config set db.pool.idle_timeout 120 # 2 minutes ``` --- ## Workflow Execution Failures ### Timeout Errors (3600 second limit) #### Error Message ``` WorkflowExecutionError: Workflow exceeded maximum execution time of 3600 seconds Status: TIMEOUT Workflow ID: wf_9k2n4m8p1q Execution ID: exec_7h3j6k9m2n ``` #### Analysis ```bash # Get workflow execution details cloudflow workflows executions get exec_7h3j6k9m2n --verbose # Check step-by-step breakdown cloudflow workflows executions steps exec_7h3j6k9m2n # Sample output: # Step 1: data_ingestion - Duration: 245s - Status: SUCCESS # Step 2: data_validation - Duration: 123s - Status: SUCCESS # Step 3: data_transformation - Duration: 3189s - Status: TIMEOUT # Step 4: data_export - Duration: 0s - Status: SKIPPED # Identify bottleneck step cloudflow workflows analyze",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_1",
              "content": "**Diagnosis:** ```bash # Check system time timedatectl status # Compare with NTP server ntpdate -q pool.ntp.org # Check JWT issued time vs current time jwt_iat=$(echo $CF_ACCESS_TOKEN | cut -d'.' -f2 | base64 -d | jq '.iat') current_time=$(date +%s) skew=$((current_time - jwt_iat)) echo \"Clock skew: $skew seconds\" ``` **Resolution:** ```bash # Sync with NTP server sudo ntpdate -s pool.ntp.org # Enable automatic time sync sudo timedatectl set-ntp true # Restart CloudFlow client cloudflow restart ``` **4. Insufficient Permissions** Error: `User does not have required permissions for this operation` **Check user roles:** ```bash cloudflow auth whoami --verbose # Expected output: # User: john.doe@company.com # Roles: developer, workflow-admin # Scopes: workflow:*, data:read, metrics:read ``` **Request permission elevation:** ```bash # Submit access request cloudflow auth request-access \\ --resource \"workflow:production:deploy\" \\ --justification \"Deploy critical hotfix for TICKET-1234\" # Check pending approvals cloudflow auth list-requests --status pending ``` ### 403 Forbidden Errors #### RBAC Policy Violations CloudFlow uses role-based access control (RBAC) with the following hierarchy: - `viewer` - Read-only access - `developer` - Create and modify workflows (non-production) - `operator` - Execute workflows, view logs - `admin` - Full access to workspace - `platform-admin` - Cross-workspace administration **Verify resource permissions:** ```bash # Check effective permissions cloudflow rbac check \\ --user john.doe@company.com \\ --resource workflow:prod-pipeline \\ --action execute # List all policies affecting user cloudflow rbac policies --user john.doe@company.com --verbose ``` --- ## Performance Problems ### Slow Query Performance #### Symptoms - API response times > 5000ms - Database query latency warnings in logs - CloudFlow UI becomes unresponsive - Timeout errors: `Request exceeded maximum duration of 30000ms` #### Diagnosis Steps **1. Identify Slow Queries** ```bash # Query CloudFlow metrics cloudflow metrics query --metric api_request_duration_ms \\ --filter \"p95 > 5000\" \\ --last 1h # Check database slow query log kubectl logs -n cloudflow deploy/cloudflow-db-primary | \\ grep \"slow query\" | \\ tail -n 50 # Analyze query patterns cloudflow db analyze-queries --min-duration 5000 --limit 20 ``` **2.",
              "score": null
            }
          ],
          "latency_ms": 980.232555000839,
          "query_id": "realistic_011",
          "dimension": "contextual",
          "query": "I have a long-running data processing workflow, what's the time limit?"
        },
        {
          "key_facts": [
            "3600 seconds",
            "exceeded maximum execution time of 3600 seconds"
          ],
          "found_facts": [
            "3600 seconds",
            "exceeded maximum execution time of 3600 seconds"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "troubleshooting_guide",
            "user_guide"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "user_guide",
              "chunk_id": "user_guide_fix_6",
              "content": "Filter by error type, date range, or execution ID 3. Click an execution to view full details 4. Click **\"Retry\"** to reprocess with the same input data ### Error Notifications Get notified when workflows fail: ```yaml workflow: notifications: on_failure: - type: email to: \"ops-team@company.com\" - type: slack channel: \"#alerts\" message: \"Workflow {{workflow.name}} failed: {{error.message}}\" on_success_after_retry: - type: slack channel: \"#monitoring\" message: \"Workflow recovered after {{error.attempts}} attempts\" ``` ## Workflow Limits CloudFlow enforces the following limits to ensure platform stability and performance: ### Steps Per Workflow - **Maximum**: 50 steps per workflow - **Recommendation**: Keep workflows focused and modular. If you need more steps, consider splitting into multiple workflows connected via webhooks. ### Execution Timeout - **Default**: 3600 seconds (60 minutes) - **Behavior**: Workflows exceeding this timeout are automatically terminated - **Custom Timeouts**: Enterprise plans can request custom timeout limits **Setting Step-Level Timeouts:** ```yaml - id: long_running_task action: http_request config: url: \"https://api.example.com/process\" timeout: 300 # 5 minutes for this specific step ``` ### Execution Limits - **Maximum**: 1000 executions per day (per workflow) - **Rate Limiting**: 100 concurrent executions per workflow - **Burst Limit**: 10 executions per second **What happens when limits are reached:** - New executions are queued automatically - Webhook triggers return HTTP 429 (Too Many Requests) - Scheduled executions are skipped (logged in audit trail) - Email notifications sent to workflow owner **Monitoring Usage:** View real-time metrics in your workflow dashboard: - Executions today: 847 / 1000 - Average duration: 12.3s - Success rate: 98.2% - Current queue depth: 3 ### Data Limits - **Maximum request/response size**: 10MB per action - **Maximum execution payload**: 50MB total - **Variable value size**: 1MB per variable ### Enterprise Plan Limits Enterprise customers can request increased limits: - Up to 100 steps per workflow - Up to 10,000 executions per day - Up to 7200 second timeout (2 hours) - Priority execution queue - Dedicated capacity allocation Contact sales@cloudflow.io for Enterprise pricing and custom limits. ## Best Practices Follow these best practices to build reliable, maintainable workflows: ### 1. Use Descriptive Names **Good:** - Workflow: \"Sync Customer Data from Salesforce to Database\" - Step: \"validate_customer_email\" **Bad:** - Workflow: \"Workflow 1\" - Step: \"step3\" ### 2.",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_4",
              "content": "**Implement connection pooling optimization:** ```bash # Use PgBouncer for connection pooling kubectl apply -f cloudflow-pgbouncer.yaml # Configure CloudFlow to use PgBouncer cloudflow config set db.host pgbouncer.cloudflow.svc.cluster.local cloudflow config set db.port 6432 cloudflow config set db.pool.mode transaction ``` 2. **Add read replicas:** ```bash # Route read-only queries to replicas cloudflow db replicas add --count 2 cloudflow config set db.read_replicas \"replica-1:5432,replica-2:5432\" cloudflow config set db.read_write_split true ``` ### Connection Timeout Errors #### Error Messages - `connection timeout after 30000ms` - `could not connect to database server at 10.0.2.45:5432` - `database server unreachable` #### Troubleshooting Steps ```bash # Test network connectivity telnet cloudflow-db.internal.company.com 5432 # Check DNS resolution nslookup cloudflow-db.internal.company.com # Verify database is accepting connections pg_isready -h cloudflow-db.internal.company.com -p 5432 -U cloudflow # Check firewall rules sudo iptables -L -n | grep 5432 # Test from CloudFlow pod network kubectl run -n cloudflow debug-pod --rm -i --tty \\ --image=postgres:14 -- \\ psql -h cloudflow-db.internal.company.com -U cloudflow -d cloudflow_production # Review database logs for connection rejections kubectl logs -n cloudflow statefulset/cloudflow-db --tail=100 | \\ grep -i \"connection\\|reject\\|authentication\" ``` ### Maximum Connection Limit (100) Reached This is a hard limit in CloudFlow's database tier. #### Permanent Solutions **Option 1: Upgrade database tier** ```bash # Check available tiers cloudflow db tiers list # Upgrade to higher tier (supports 200 connections) cloudflow db upgrade --tier standard-plus --confirm # Monitor migration progress cloudflow db migration status ``` **Option 2: Implement aggressive connection reuse** ```bash # Reduce connection lifetime cloudflow config set db.pool.max_lifetime 600 # 10 minutes # Enable prepared statement caching cloudflow config set db.prepared_statements.cache true cloudflow config set db.prepared_statements.max_size 250 # Reduce idle connection timeout cloudflow config set db.pool.idle_timeout 120 # 2 minutes ``` --- ## Workflow Execution Failures ### Timeout Errors (3600 second limit) #### Error Message ``` WorkflowExecutionError: Workflow exceeded maximum execution time of 3600 seconds Status: TIMEOUT Workflow ID: wf_9k2n4m8p1q Execution ID: exec_7h3j6k9m2n ``` #### Analysis ```bash # Get workflow execution details cloudflow workflows executions get exec_7h3j6k9m2n --verbose # Check step-by-step breakdown cloudflow workflows executions steps exec_7h3j6k9m2n # Sample output: # Step 1: data_ingestion - Duration: 245s - Status: SUCCESS # Step 2: data_validation - Duration: 123s - Status: SUCCESS # Step 3: data_transformation - Duration: 3189s - Status: TIMEOUT # Step 4: data_export - Duration: 0s - Status: SKIPPED # Identify bottleneck step cloudflow workflows analyze",
              "score": null
            },
            {
              "doc_id": "user_guide",
              "chunk_id": "user_guide_fix_5",
              "content": "Save and activate **Testing Schedules:** Use the built-in schedule calculator to preview upcoming executions: ``` Next 5 executions: 1. 2026-01-24 14:00:00 UTC 2. 2026-01-25 14:00:00 UTC 3. 2026-01-26 14:00:00 UTC 4. 2026-01-27 14:00:00 UTC 5. 2026-01-28 14:00:00 UTC ``` ## Error Handling Robust error handling ensures your workflows are resilient and reliable. ### Retry Policies Configure automatic retries for failed actions: ```yaml - id: api_call action: http_request config: url: \"https://api.example.com/data\" retry: max_attempts: 3 backoff_type: \"exponential\" # or \"fixed\", \"linear\" initial_interval: 1000 # milliseconds max_interval: 30000 multiplier: 2.0 retry_on: - timeout - network_error - status: [500, 502, 503, 504] ``` **Backoff Strategies:** - **Fixed**: Wait the same amount of time between retries - Attempt 1: Wait 1s - Attempt 2: Wait 1s - Attempt 3: Wait 1s - **Linear**: Increase wait time by a fixed amount - Attempt 1: Wait 1s - Attempt 2: Wait 2s - Attempt 3: Wait 3s - **Exponential**: Double the wait time with each retry (recommended) - Attempt 1: Wait 1s - Attempt 2: Wait 2s - Attempt 3: Wait 4s **Retry Conditions:** Control which errors trigger retries: - `timeout`: Request timeout - `network_error`: Connection failures - `status`: Specific HTTP status codes - `error_code`: Application-specific error codes ### Fallback Actions Execute alternative actions when the primary action fails: ```yaml - id: primary_payment action: http_request config: url: \"https://primary-payment-gateway.com/charge\" method: POST body: amount: \"{{amount}}\" on_error: - id: fallback_payment action: http_request config: url: \"https://backup-payment-gateway.com/charge\" method: POST body: amount: \"{{amount}}\" - id: notify_admin action: email config: to: \"admin@company.com\" subject: \"Payment Gateway Failure\" body: \"Primary gateway failed, switched to backup\" ``` **Error Object:** Access error details in fallback actions: ``` {{error.message}} # Error message {{error.code}} # Error code {{error.step_id}} # Failed step ID {{error.timestamp}} # When the error occurred {{error.attempts}} # Number of retry attempts ``` ### Dead Letter Queue When all retries and fallbacks fail, CloudFlow can route failed executions to a Dead Letter Queue (DLQ) for manual review: **Enable DLQ:** ```yaml workflow: error_handling: dead_letter_queue: enabled: true retain_days: 30 ``` **DLQ Features:** - View failed executions in the dashboard - Inspect complete execution context and error details - Retry individual executions after fixing issues - Export failed executions for analysis - Set up alerts for DLQ threshold breaches **Accessing the DLQ:** 1. Navigate to **Workflows** > **[Your Workflow]** > **Dead Letter Queue** 2.",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_6",
              "content": "Data Validation Errors** ``` ValidationError: Field 'customer_id' is required but missing in 234 records ``` **Resolution:** ```bash # Add data quality checks cloudflow workflows update wf_9k2n4m8p1q \\ --step data_ingestion \\ --add-validator required_fields \\ --validator-config '{\"fields\": [\"customer_id\", \"timestamp\", \"amount\"]}' # Configure error handling cloudflow workflows update wf_9k2n4m8p1q \\ --step data_validation \\ --on-error continue \\ --error-threshold 5% # Fail if > 5% of records invalid ``` **2. External API Failures** ``` ExternalAPIError: API request to https://partner-api.example.com failed with status 502 ``` **Resolution:** ```bash # Add circuit breaker cloudflow workflows update wf_9k2n4m8p1q \\ --step external_api_call \\ --circuit-breaker-enabled true \\ --circuit-breaker-threshold 5 \\ --circuit-breaker-timeout 30000 # Configure fallback behavior cloudflow workflows update wf_9k2n4m8p1q \\ --step external_api_call \\ --fallback-action use_cached_data \\ --cache-ttl 3600 ``` --- ## Rate Limiting & Throttling ### 429 Too Many Requests #### Error Response ```json { \"error\": \"rate_limit_exceeded\", \"message\": \"API rate limit exceeded. Retry after 45 seconds.\", \"status\": 429, \"headers\": { \"X-RateLimit-Limit\": \"1000\", \"X-RateLimit-Remaining\": \"0\", \"X-RateLimit-Reset\": \"1706112345\", \"Retry-After\": \"45\" } } ``` #### Rate Limit Tiers CloudFlow enforces the following rate limits per workspace: | Tier | Requests/Minute | Requests/Hour | Concurrent Workflows | |------|-----------------|---------------|----------------------| | Free | 60 | 1,000 | 5 | | Standard | 1,000 | 50,000 | 50 | | Premium | 5,000 | 250,000 | 200 | | Enterprise | Custom | Custom | Unlimited | #### Checking Rate Limit Status ```bash # Check current rate limit status curl -I https://api.cloudflow.io/api/v1/workflows \\ -H \"Authorization: Bearer $CF_ACCESS_TOKEN\" # Extract rate limit headers curl -s -I https://api.cloudflow.io/api/v1/workflows \\ -H \"Authorization: Bearer $CF_ACCESS_TOKEN\" | \\ grep -i \"x-ratelimit\" # Output: # X-RateLimit-Limit: 1000 # X-RateLimit-Remaining: 247 # X-RateLimit-Reset: 1706112400 # Monitor rate limit usage cloudflow metrics query \\ --metric rate_limit_remaining \\ --workspace ws_abc123 \\ --last 1h \\ --interval 1m ``` #### Handling Rate Limits in Code **Python example with retry logic:** ```python import time import requests def cloudflow_api_call_with_retry(url, headers, max_retries=3): for attempt in range(max_retries): response = requests.get(url, headers=headers) if response.status_code == 429: retry_after = int(response.headers.get('Retry-After', 60)) print(f\"Rate limited.",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_5",
              "content": "exec_7h3j6k9m2n --show-bottlenecks ``` #### Solutions **1. Increase workflow timeout (if justified):** ```bash # Update workflow configuration cloudflow workflows update wf_9k2n4m8p1q \\ --timeout 7200 \\ --reason \"Large dataset processing requires extended time\" # Verify update cloudflow workflows get wf_9k2n4m8p1q | grep timeout ``` **2. Optimize slow steps:** ```bash # Enable parallel processing cloudflow workflows update wf_9k2n4m8p1q \\ --step data_transformation \\ --parallel-workers 8 \\ --batch-size 1000 # Add checkpointing for long operations cloudflow workflows update wf_9k2n4m8p1q \\ --step data_transformation \\ --enable-checkpointing \\ --checkpoint-interval 300 ``` **3. Split workflow into smaller workflows:** ```bash # Create sub-workflows cloudflow workflows create data-pipeline-part1 \\ --steps \"data_ingestion,data_validation\" \\ --timeout 1800 cloudflow workflows create data-pipeline-part2 \\ --steps \"data_transformation,data_export\" \\ --timeout 3600 \\ --trigger workflow_completed \\ --trigger-workflow data-pipeline-part1 ``` ### Retry Logic and Exponential Backoff CloudFlow implements automatic retry with exponential backoff for transient failures: - Max retries: 3 - Initial delay: 1 second - Backoff multiplier: 2 - Max delay: 60 seconds #### Retry Sequence ``` Attempt 1: Immediate Attempt 2: Wait 1s (2^0 * 1s) Attempt 3: Wait 2s (2^1 * 1s) Attempt 4: Wait 4s (2^2 * 1s) ``` #### Configuration ```bash # View current retry settings cloudflow workflows get wf_9k2n4m8p1q --format json | jq '.retry_policy' # Customize retry behavior cloudflow workflows update wf_9k2n4m8p1q \\ --retry-max-attempts 5 \\ --retry-initial-delay 2000 \\ --retry-backoff-multiplier 2 \\ --retry-max-delay 120000 \\ --retry-on-errors \"NETWORK_ERROR,TIMEOUT,RATE_LIMIT\" # Disable retry for specific step cloudflow workflows update wf_9k2n4m8p1q \\ --step data_export \\ --retry-enabled false ``` #### Monitoring Retries ```bash # List failed executions with retry information cloudflow workflows executions list \\ --status FAILED \\ --show-retries \\ --last 7d # Get retry history for specific execution cloudflow workflows executions retries exec_7h3j6k9m2n # Output: # Execution: exec_7h3j6k9m2n # Attempt 1: FAILED - NetworkError: Connection refused (delay: 0ms) # Attempt 2: FAILED - NetworkError: Connection timeout (delay: 1000ms) # Attempt 3: FAILED - NetworkError: Connection timeout (delay: 2000ms) # Attempt 4: FAILED - NetworkError: Connection timeout (delay: 4000ms) # Final Status: FAILED_AFTER_RETRIES ``` ### Workflow Step Failures #### Common Error Patterns **1.",
              "score": null
            }
          ],
          "latency_ms": 976.4128860006167,
          "query_id": "realistic_011",
          "dimension": "negation",
          "query": "Why did my workflow fail after 3600 seconds?"
        },
        {
          "key_facts": [
            "RS256",
            "RS256 algorithm",
            "RS256 signing algorithm"
          ],
          "found_facts": [
            "RS256",
            "RS256 algorithm",
            "RS256 signing algorithm"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "api_reference"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_0",
              "content": "# CloudFlow Platform Troubleshooting Guide **Version:** 3.2.1 **Last Updated:** January 2026 **Audience:** Platform Engineers, DevOps, Support Teams ## Table of Contents 1. [Overview](#overview) 2. [Authentication & Authorization Issues](#authentication--authorization-issues) 3. [Performance Problems](#performance-problems) 4. [Database Connection Issues](#database-connection-issues) 5. [Workflow Execution Failures](#workflow-execution-failures) 6. [Rate Limiting & Throttling](#rate-limiting--throttling) 7. [Log Analysis & Debugging](#log-analysis--debugging) 8. [Escalation Procedures](#escalation-procedures) --- ## Overview This guide provides comprehensive troubleshooting steps for common CloudFlow platform issues encountered in production environments. Each section includes error symptoms, root cause analysis, resolution steps, and preventive measures. ### Quick Diagnostic Checklist Before diving into specific issues, perform these initial checks: - Verify service health: `cloudflow status --all` - Check API connectivity: `curl -I https://api.cloudflow.io/health` - Review recent deployments: `kubectl get deployments -n cloudflow --sort-by=.metadata.creationTimestamp` - Inspect platform metrics: `cloudflow metrics --last 1h` --- ## Authentication & Authorization Issues ### 401 Unauthorized Errors #### Symptoms - API requests return `401 Unauthorized` - Error message: `Authentication credentials were not provided or are invalid` - Frontend displays \"Session expired\" message #### Common Causes **1. Token Expiration** CloudFlow access tokens expire after 3600 seconds (1 hour) by default. Refresh tokens are valid for 30 days. **Verification:** ```bash # Decode JWT to check expiration echo $CF_ACCESS_TOKEN | cut -d'.' -f2 | base64 -d | jq '.exp' # Compare with current time date +%s ``` **Resolution:** ```bash # Refresh the access token curl -X POST https://api.cloudflow.io/auth/refresh \\ -H \"Content-Type: application/json\" \\ -d '{\"refresh_token\": \"'$CF_REFRESH_TOKEN'\"}' # Update environment variable export CF_ACCESS_TOKEN=\"<new_token>\" ``` **2. Invalid JWT Signature** Error: `JWT signature verification failed` **Causes:** - Token was modified or corrupted - Using wrong signing key - Token generated with different secret **Resolution:** ```bash # Validate token structure cloudflow auth validate --token $CF_ACCESS_TOKEN # Generate new token cloudflow auth login --username <user> --password <pass> # For service accounts cloudflow auth service-account --create --name <sa-name> --scopes \"workflow:read,workflow:write\" ``` **3. Clock Skew Issues** JWT validation fails when system clocks are out of sync (tolerance: \u00b1300 seconds).",
              "score": null
            },
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_0",
              "content": "# CloudFlow API Reference Version 2.1.0 | Last Updated: January 2026 ## Overview The CloudFlow API is a RESTful service that enables developers to programmatically manage cloud workflows, data pipelines, and automation tasks. This documentation provides comprehensive details on authentication, endpoints, request/response formats, error handling, and best practices. **Base URL:** `https://api.cloudflow.io/v2` **API Status:** https://status.cloudflow.io ## Authentication CloudFlow supports three authentication methods to suit different use cases and security requirements. ### API Keys API keys provide simple authentication for server-to-server communication. Include your API key in the request header: ```bash curl -H \"X-API-Key: cf_live_a1b2c3d4e5f6g7h8i9j0\" \\ https://api.cloudflow.io/v2/workflows ``` **Security Notes:** - Never expose API keys in client-side code - Rotate keys every 90 days - Use separate keys for development and production environments ### OAuth 2.0 OAuth 2.0 is recommended for applications that access CloudFlow on behalf of users. We support the Authorization Code flow with PKCE. **Authorization Endpoint:** `https://auth.cloudflow.io/oauth/authorize` **Token Endpoint:** `https://auth.cloudflow.io/oauth/token` **Supported Scopes:** - `workflows:read` - Read workflow configurations - `workflows:write` - Create and modify workflows - `pipelines:read` - Read pipeline data - `pipelines:write` - Create and manage pipelines - `analytics:read` - Access analytics and metrics - `admin:full` - Full administrative access Example authorization request: ```bash curl -X POST https://auth.cloudflow.io/oauth/token \\ -H \"Content-Type: application/x-www-form-urlencoded\" \\ -d \"grant_type=authorization_code\" \\ -d \"code=AUTH_CODE_HERE\" \\ -d \"client_id=YOUR_CLIENT_ID\" \\ -d \"client_secret=YOUR_CLIENT_SECRET\" \\ -d \"redirect_uri=https://yourapp.com/callback\" ``` ### JWT Tokens For advanced use cases, CloudFlow supports JSON Web Tokens (JWT) with RS256 signing algorithm. JWTs must include the following claims: - `iss` (issuer): Your application identifier - `sub` (subject): User or service account ID - `aud` (audience): `https://api.cloudflow.io` - `exp` (expiration): Unix timestamp (max 3600 seconds from `iat`) - `iat` (issued at): Unix timestamp - `scope`: Space-separated list of requested scopes Example JWT header: ```python import jwt import time from cryptography.hazmat.primitives import serialization from cryptography.hazmat.backends import default_backend # Load your private key with open('private_key.pem', 'rb') as key_file: private_key = serialization.load_pem_private_key( key_file.read(), password=None, backend=default_backend() ) # Create JWT payload payload = { 'iss': 'your-app-id', 'sub': 'user-12345', 'aud': 'https://api.cloudflow.io', 'exp': int(time.time()) + 3600, 'iat': int(time.time()), 'scope': 'workflows:read workflows:write' } # Generate token token = jwt.encode(payload, private_key, algorithm='RS256') # Use in API request headers = {'Authorization': f'Bearer {token}'} ``` **Token Expiration:** All tokens expire after 3600 seconds (1 hour). Implement token refresh logic in your application.",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_1",
              "content": "**Technology**: Node.js with Express.js framework **Replicas**: 12 pods (production), auto-scaling 8-20 based on CPU **Resource Allocation**: 2 vCPU, 4GB RAM per pod **Key Responsibilities**: - JWT token validation (delegated to Auth Service for initial validation) - Rate limiting: 1000 requests per minute per API key (sliding window) - Request/response transformation and validation using JSON Schema - Routing to downstream services based on URL path patterns - CORS handling for web clients - Request/response logging and correlation ID injection **Critical Endpoints**: - `POST /api/v1/workflows` - Create new workflow - `GET /api/v1/workflows/:id` - Retrieve workflow status - `POST /api/v1/workflows/:id/execute` - Trigger workflow execution - `GET /api/v1/workflows/:id/history` - Get execution history **Dependencies**: - Auth Service (for token validation) - Redis (for rate limiting counters) - All downstream microservices **Performance Targets**: - P50 latency: < 50ms - P99 latency: < 200ms - Throughput: 10,000 RPS sustained --- ### Auth Service **Purpose**: Centralized authentication and authorization service handling user identity, token generation, and permission validation. **Technology**: Go with gRPC for internal communication, REST for external **Replicas**: 8 pods (production), auto-scaling 6-12 **Resource Allocation**: 1 vCPU, 2GB RAM per pod **Key Responsibilities**: - User authentication via multiple providers (OAuth2, SAML, local credentials) - JWT token generation and validation (RS256 algorithm) - Role-based access control (RBAC) with fine-grained permissions - Session management with Redis-backed storage - API key generation and validation for service accounts - MFA enforcement for administrative operations **Authentication Flow**: ``` Client Request \u2192 API Gateway \u2192 Auth Service \u2502 \u251c\u2500 Validate credentials \u251c\u2500 Check MFA if required \u251c\u2500 Generate JWT (15min expiry) \u251c\u2500 Generate refresh token (7 days) \u2514\u2500 Store session in Redis ``` **Token Structure**: - Access Token: JWT with 15-minute expiry - Refresh Token: Opaque token with 7-day expiry, stored in PostgreSQL - Claims: user_id, email, roles[], permissions[], tenant_id **Security Features**: - Password hashing: Argon2id with 64MB memory, 4 iterations - Token rotation on refresh to prevent replay attacks - Brute force protection: 5 failed attempts \u2192 15-minute lockout - Secrets stored in HashiCorp Vault, rotated every 90 days **Performance Targets**: - Token validation: < 10ms (P99) - Token generation: < 50ms (P99) - Throughput: 5,000 RPS for validation operations --- ### Workflow Engine **Purpose**: Core orchestration service that executes workflow definitions, manages state transitions, and coordinates task execution across distributed systems.",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_8",
              "content": "### Cache Invalidation Strategies **Time-Based Expiration (TTL)**: - Short-lived data: 5-15 minutes (session tokens, rate limit counters) - Medium-lived data: 1 hour (workflow definitions, templates) - Long-lived data: 24 hours (static configuration) **Event-Based Invalidation**: ``` Database Update Event \u2192 Kafka (cache.invalidation topic) \u2502 \u25bc All service instances consume event \u2502 \u25bc Redis DEL for affected keys ``` **Pattern-Based Invalidation**: - Use Redis SCAN + DEL for wildcard patterns - Example: Invalidate all user caches: `user:*:{user_id}` **Cache Warming**: - Scheduled job runs every hour to pre-populate frequently accessed data - Targets: Top 1000 workflows, system configuration, active user profiles - Reduces cache miss rate during traffic spikes ### Multi-Level Caching Application implements L1 (in-memory) and L2 (Redis) caching: ``` Request \u2192 L1 Cache (Node.js Map, 1000 entry LRU) \u2502 MISS \u2502 \u25bc L2 Cache (Redis, distributed) \u2502 MISS \u2502 \u25bc Database (PostgreSQL) ``` L1 cache reduces Redis network calls by 40% for hot data. --- ## Security Architecture ### Network Security **Zero-Trust Network Model**: - All service-to-service communication encrypted with mTLS - Certificate rotation: Every 90 days (automated via cert-manager) - Certificate authority: Internal PKI with HashiCorp Vault - Network segmentation: Private subnets for services, public subnet for ALB only **Service Mesh (Istio)**: ``` Service A \u2192 Envoy Sidecar (mTLS client cert) \u2502 Encrypted channel \u2502 Envoy Sidecar (mTLS server cert) \u2192 Service B ``` **Firewall Rules**: - Security groups: Deny all by default, explicit allow rules - Ingress: Only ALB can reach API Gateway (port 8080) - Egress: Services can only reach specific dependencies - Database access: Limited to application subnets only ### Authentication & Authorization **JWT Token Validation**: - Algorithm: RS256 (asymmetric signing) - Key rotation: Every 30 days with 7-day overlap period - Public key distribution: JWKS endpoint cached in Redis - Validation: Signature, expiry, issuer, audience claims - Token revocation: Blacklist in Redis for compromised tokens **Permission Model**: ``` User \u2192 Roles \u2192 Permissions \u2198 \u2197 Tenants (Multi-tenancy isolation) ``` Example permissions: - `workflow:read` - View workflows - `workflow:write` - Create/update workflows - `workflow:execute` - Trigger workflow execution - `workflow:delete` - Delete workflows - `admin:*` - All administrative operations **API Key Management**: - Format: `cfk_live_<32-char-random>` (production), `cfk_test_<32-char-random>` (sandbox) - Hashing: SHA-256 before storage in PostgreSQL - Scoping: API keys can be scoped to specific workflows or operations - Rate limits: Configurable",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_6",
              "content": "**Session Storage**: - Key pattern: `session:{user_id}` - TTL: 15 minutes (aligned with JWT expiry) - Data: User session state, preferences - Invalidation: On logout or password change 2. **Rate Limiting Counters**: - Key pattern: `ratelimit:{api_key}:{window}` - TTL: 60 seconds (sliding window) - Data: Request count per window - Algorithm: Token bucket with Redis INCR 3. **Compiled Templates**: - Key pattern: `template:{template_id}:{version}` - TTL: 1 hour - Data: Compiled Handlebars template - Invalidation: On template update 4. **Workflow Definitions**: - Key pattern: `workflow:def:{workflow_id}` - TTL: 1 hour - Data: Parsed workflow JSON - Invalidation: On workflow update or manual flush 5. **User Profiles**: - Key pattern: `user:profile:{user_id}` - TTL: 30 minutes - Data: User metadata (name, email, roles) - Invalidation: On profile update **Cache Hit Rates** (Production Metrics): - Session lookups: 98.5% - Workflow definitions: 94.2% - Template cache: 99.1% - User profiles: 91.8% **Performance Characteristics**: - GET operation P99: < 2ms - SET operation P99: < 3ms - Throughput: 100,000 operations per second - Network latency: < 1ms (same AZ) --- ### Kafka Event Streaming **Cluster Configuration**: - 5 broker nodes (distributed across 3 AZs) - Instance type: kafka.m5.2xlarge (8 vCPU, 32GB RAM) - Storage: 10TB per broker (gp3 SSD) - ZooKeeper ensemble: 3 nodes for cluster coordination - Replication factor: 3 (min in-sync replicas: 2) **Topic Architecture**: ``` workflow.events (32 partitions): - Workflow lifecycle events (created, started, completed, failed) - Retention: 7 days - Message rate: 5,000/sec peak - Consumer groups: workflow-engine, analytics-pipeline notifications.email (16 partitions): - Email notification triggers - Retention: 3 days - Message rate: 2,000/sec peak - Consumer groups: notification-service notifications.sms (8 partitions): - SMS notification triggers - Retention: 3 days - Message rate: 500/sec peak - Consumer groups: notification-service audit.logs (24 partitions): - Security and compliance audit events - Retention: 90 days (compliance requirement) - Message rate: 3,000/sec peak - Consumer groups: audit-processor, security-monitor dead-letter-queue (8 partitions): - Failed message processing - Retention: 30 days - Manual intervention required - Consumer groups: ops-team-alerts ``` **Producer Configuration**: - Acknowledgment: `acks=all` (wait for all in-sync replicas) - Compression: LZ4 (reduces network bandwidth by 60%) - Batching: 100ms linger time, 100KB batch size - Idempotence: Enabled to prevent duplicates **Consumer Configuration**: - Auto-commit: Disabled (manual commit after processing) - Offset reset: Earliest (replay from beginning on new consumer group) - Max poll",
              "score": null
            }
          ],
          "latency_ms": 903.3729699985997,
          "query_id": "realistic_012",
          "dimension": "original",
          "query": "What JWT algorithm is used for token signing?"
        },
        {
          "key_facts": [
            "RS256",
            "RS256 algorithm",
            "RS256 signing algorithm"
          ],
          "found_facts": [
            "RS256",
            "RS256 algorithm",
            "RS256 signing algorithm"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "api_reference"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_0",
              "content": "# CloudFlow API Reference Version 2.1.0 | Last Updated: January 2026 ## Overview The CloudFlow API is a RESTful service that enables developers to programmatically manage cloud workflows, data pipelines, and automation tasks. This documentation provides comprehensive details on authentication, endpoints, request/response formats, error handling, and best practices. **Base URL:** `https://api.cloudflow.io/v2` **API Status:** https://status.cloudflow.io ## Authentication CloudFlow supports three authentication methods to suit different use cases and security requirements. ### API Keys API keys provide simple authentication for server-to-server communication. Include your API key in the request header: ```bash curl -H \"X-API-Key: cf_live_a1b2c3d4e5f6g7h8i9j0\" \\ https://api.cloudflow.io/v2/workflows ``` **Security Notes:** - Never expose API keys in client-side code - Rotate keys every 90 days - Use separate keys for development and production environments ### OAuth 2.0 OAuth 2.0 is recommended for applications that access CloudFlow on behalf of users. We support the Authorization Code flow with PKCE. **Authorization Endpoint:** `https://auth.cloudflow.io/oauth/authorize` **Token Endpoint:** `https://auth.cloudflow.io/oauth/token` **Supported Scopes:** - `workflows:read` - Read workflow configurations - `workflows:write` - Create and modify workflows - `pipelines:read` - Read pipeline data - `pipelines:write` - Create and manage pipelines - `analytics:read` - Access analytics and metrics - `admin:full` - Full administrative access Example authorization request: ```bash curl -X POST https://auth.cloudflow.io/oauth/token \\ -H \"Content-Type: application/x-www-form-urlencoded\" \\ -d \"grant_type=authorization_code\" \\ -d \"code=AUTH_CODE_HERE\" \\ -d \"client_id=YOUR_CLIENT_ID\" \\ -d \"client_secret=YOUR_CLIENT_SECRET\" \\ -d \"redirect_uri=https://yourapp.com/callback\" ``` ### JWT Tokens For advanced use cases, CloudFlow supports JSON Web Tokens (JWT) with RS256 signing algorithm. JWTs must include the following claims: - `iss` (issuer): Your application identifier - `sub` (subject): User or service account ID - `aud` (audience): `https://api.cloudflow.io` - `exp` (expiration): Unix timestamp (max 3600 seconds from `iat`) - `iat` (issued at): Unix timestamp - `scope`: Space-separated list of requested scopes Example JWT header: ```python import jwt import time from cryptography.hazmat.primitives import serialization from cryptography.hazmat.backends import default_backend # Load your private key with open('private_key.pem', 'rb') as key_file: private_key = serialization.load_pem_private_key( key_file.read(), password=None, backend=default_backend() ) # Create JWT payload payload = { 'iss': 'your-app-id', 'sub': 'user-12345', 'aud': 'https://api.cloudflow.io', 'exp': int(time.time()) + 3600, 'iat': int(time.time()), 'scope': 'workflows:read workflows:write' } # Generate token token = jwt.encode(payload, private_key, algorithm='RS256') # Use in API request headers = {'Authorization': f'Bearer {token}'} ``` **Token Expiration:** All tokens expire after 3600 seconds (1 hour). Implement token refresh logic in your application.",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_1",
              "content": "**Technology**: Node.js with Express.js framework **Replicas**: 12 pods (production), auto-scaling 8-20 based on CPU **Resource Allocation**: 2 vCPU, 4GB RAM per pod **Key Responsibilities**: - JWT token validation (delegated to Auth Service for initial validation) - Rate limiting: 1000 requests per minute per API key (sliding window) - Request/response transformation and validation using JSON Schema - Routing to downstream services based on URL path patterns - CORS handling for web clients - Request/response logging and correlation ID injection **Critical Endpoints**: - `POST /api/v1/workflows` - Create new workflow - `GET /api/v1/workflows/:id` - Retrieve workflow status - `POST /api/v1/workflows/:id/execute` - Trigger workflow execution - `GET /api/v1/workflows/:id/history` - Get execution history **Dependencies**: - Auth Service (for token validation) - Redis (for rate limiting counters) - All downstream microservices **Performance Targets**: - P50 latency: < 50ms - P99 latency: < 200ms - Throughput: 10,000 RPS sustained --- ### Auth Service **Purpose**: Centralized authentication and authorization service handling user identity, token generation, and permission validation. **Technology**: Go with gRPC for internal communication, REST for external **Replicas**: 8 pods (production), auto-scaling 6-12 **Resource Allocation**: 1 vCPU, 2GB RAM per pod **Key Responsibilities**: - User authentication via multiple providers (OAuth2, SAML, local credentials) - JWT token generation and validation (RS256 algorithm) - Role-based access control (RBAC) with fine-grained permissions - Session management with Redis-backed storage - API key generation and validation for service accounts - MFA enforcement for administrative operations **Authentication Flow**: ``` Client Request \u2192 API Gateway \u2192 Auth Service \u2502 \u251c\u2500 Validate credentials \u251c\u2500 Check MFA if required \u251c\u2500 Generate JWT (15min expiry) \u251c\u2500 Generate refresh token (7 days) \u2514\u2500 Store session in Redis ``` **Token Structure**: - Access Token: JWT with 15-minute expiry - Refresh Token: Opaque token with 7-day expiry, stored in PostgreSQL - Claims: user_id, email, roles[], permissions[], tenant_id **Security Features**: - Password hashing: Argon2id with 64MB memory, 4 iterations - Token rotation on refresh to prevent replay attacks - Brute force protection: 5 failed attempts \u2192 15-minute lockout - Secrets stored in HashiCorp Vault, rotated every 90 days **Performance Targets**: - Token validation: < 10ms (P99) - Token generation: < 50ms (P99) - Throughput: 5,000 RPS for validation operations --- ### Workflow Engine **Purpose**: Core orchestration service that executes workflow definitions, manages state transitions, and coordinates task execution across distributed systems.",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_0",
              "content": "# CloudFlow Platform Troubleshooting Guide **Version:** 3.2.1 **Last Updated:** January 2026 **Audience:** Platform Engineers, DevOps, Support Teams ## Table of Contents 1. [Overview](#overview) 2. [Authentication & Authorization Issues](#authentication--authorization-issues) 3. [Performance Problems](#performance-problems) 4. [Database Connection Issues](#database-connection-issues) 5. [Workflow Execution Failures](#workflow-execution-failures) 6. [Rate Limiting & Throttling](#rate-limiting--throttling) 7. [Log Analysis & Debugging](#log-analysis--debugging) 8. [Escalation Procedures](#escalation-procedures) --- ## Overview This guide provides comprehensive troubleshooting steps for common CloudFlow platform issues encountered in production environments. Each section includes error symptoms, root cause analysis, resolution steps, and preventive measures. ### Quick Diagnostic Checklist Before diving into specific issues, perform these initial checks: - Verify service health: `cloudflow status --all` - Check API connectivity: `curl -I https://api.cloudflow.io/health` - Review recent deployments: `kubectl get deployments -n cloudflow --sort-by=.metadata.creationTimestamp` - Inspect platform metrics: `cloudflow metrics --last 1h` --- ## Authentication & Authorization Issues ### 401 Unauthorized Errors #### Symptoms - API requests return `401 Unauthorized` - Error message: `Authentication credentials were not provided or are invalid` - Frontend displays \"Session expired\" message #### Common Causes **1. Token Expiration** CloudFlow access tokens expire after 3600 seconds (1 hour) by default. Refresh tokens are valid for 30 days. **Verification:** ```bash # Decode JWT to check expiration echo $CF_ACCESS_TOKEN | cut -d'.' -f2 | base64 -d | jq '.exp' # Compare with current time date +%s ``` **Resolution:** ```bash # Refresh the access token curl -X POST https://api.cloudflow.io/auth/refresh \\ -H \"Content-Type: application/json\" \\ -d '{\"refresh_token\": \"'$CF_REFRESH_TOKEN'\"}' # Update environment variable export CF_ACCESS_TOKEN=\"<new_token>\" ``` **2. Invalid JWT Signature** Error: `JWT signature verification failed` **Causes:** - Token was modified or corrupted - Using wrong signing key - Token generated with different secret **Resolution:** ```bash # Validate token structure cloudflow auth validate --token $CF_ACCESS_TOKEN # Generate new token cloudflow auth login --username <user> --password <pass> # For service accounts cloudflow auth service-account --create --name <sa-name> --scopes \"workflow:read,workflow:write\" ``` **3. Clock Skew Issues** JWT validation fails when system clocks are out of sync (tolerance: \u00b1300 seconds).",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_1",
              "content": "**Diagnosis:** ```bash # Check system time timedatectl status # Compare with NTP server ntpdate -q pool.ntp.org # Check JWT issued time vs current time jwt_iat=$(echo $CF_ACCESS_TOKEN | cut -d'.' -f2 | base64 -d | jq '.iat') current_time=$(date +%s) skew=$((current_time - jwt_iat)) echo \"Clock skew: $skew seconds\" ``` **Resolution:** ```bash # Sync with NTP server sudo ntpdate -s pool.ntp.org # Enable automatic time sync sudo timedatectl set-ntp true # Restart CloudFlow client cloudflow restart ``` **4. Insufficient Permissions** Error: `User does not have required permissions for this operation` **Check user roles:** ```bash cloudflow auth whoami --verbose # Expected output: # User: john.doe@company.com # Roles: developer, workflow-admin # Scopes: workflow:*, data:read, metrics:read ``` **Request permission elevation:** ```bash # Submit access request cloudflow auth request-access \\ --resource \"workflow:production:deploy\" \\ --justification \"Deploy critical hotfix for TICKET-1234\" # Check pending approvals cloudflow auth list-requests --status pending ``` ### 403 Forbidden Errors #### RBAC Policy Violations CloudFlow uses role-based access control (RBAC) with the following hierarchy: - `viewer` - Read-only access - `developer` - Create and modify workflows (non-production) - `operator` - Execute workflows, view logs - `admin` - Full access to workspace - `platform-admin` - Cross-workspace administration **Verify resource permissions:** ```bash # Check effective permissions cloudflow rbac check \\ --user john.doe@company.com \\ --resource workflow:prod-pipeline \\ --action execute # List all policies affecting user cloudflow rbac policies --user john.doe@company.com --verbose ``` --- ## Performance Problems ### Slow Query Performance #### Symptoms - API response times > 5000ms - Database query latency warnings in logs - CloudFlow UI becomes unresponsive - Timeout errors: `Request exceeded maximum duration of 30000ms` #### Diagnosis Steps **1. Identify Slow Queries** ```bash # Query CloudFlow metrics cloudflow metrics query --metric api_request_duration_ms \\ --filter \"p95 > 5000\" \\ --last 1h # Check database slow query log kubectl logs -n cloudflow deploy/cloudflow-db-primary | \\ grep \"slow query\" | \\ tail -n 50 # Analyze query patterns cloudflow db analyze-queries --min-duration 5000 --limit 20 ``` **2.",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_8",
              "content": "Leverage caching:** ```bash # Enable client-side caching export CLOUDFLOW_CACHE_ENABLED=true export CLOUDFLOW_CACHE_TTL=300 # Cache workflow metadata cloudflow workflows list --use-cache --cache-ttl 600 ``` --- ## Log Analysis & Debugging ### Accessing CloudFlow Logs #### Kubernetes Deployments ```bash # List all CloudFlow pods kubectl get pods -n cloudflow # Tail logs from API server kubectl logs -f -n cloudflow deployment/cloudflow-api --tail=100 # Get logs from specific pod kubectl logs -n cloudflow cloudflow-api-7d4f6b8c9d-x7k2m # Get logs from previous crashed pod kubectl logs -n cloudflow cloudflow-api-7d4f6b8c9d-x7k2m --previous # Get logs from all pods in deployment kubectl logs -n cloudflow deployment/cloudflow-api --all-containers=true # Stream logs from multiple pods kubectl logs -n cloudflow -l app=cloudflow-api -f --max-log-requests=10 ``` #### Log Levels CloudFlow supports the following log levels: - `TRACE` - Very detailed debugging information - `DEBUG` - Detailed debugging information - `INFO` - Informational messages (default) - `WARN` - Warning messages - `ERROR` - Error messages - `FATAL` - Fatal errors causing shutdown **Changing log levels:** ```bash # Set global log level cloudflow config set logging.level DEBUG # Set log level for specific component cloudflow config set logging.components.database DEBUG cloudflow config set logging.components.auth INFO cloudflow config set logging.components.workflows TRACE # Temporary log level increase (resets after 1 hour) cloudflow debug set-log-level DEBUG --duration 3600 # View current log configuration cloudflow config get logging --format json ``` ### Grep Patterns for Common Issues #### Authentication Failures ```bash # Find all authentication errors kubectl logs -n cloudflow deployment/cloudflow-api | \\ grep -i \"authentication\\|401\\|unauthorized\" | \\ tail -n 50 # Find JWT validation failures kubectl logs -n cloudflow deployment/cloudflow-api | \\ grep -E \"JWT|token.*invalid|signature.*failed\" # Find clock skew issues kubectl logs -n cloudflow deployment/cloudflow-api | \\ grep -i \"clock skew\\|time.*sync\\|nbf\\|exp\" ``` #### Database Errors ```bash # Find database connection errors kubectl logs -n cloudflow deployment/cloudflow-api | \\ grep -E \"connection.*pool|could not connect|database.*timeout\" # Find slow queries kubectl logs -n cloudflow deployment/cloudflow-api | \\ grep \"slow query\" | \\ awk '{print $NF}' | \\ sort -n | \\ tail -n 20 # Find deadlock errors kubectl logs -n cloudflow deployment/cloudflow-api | \\ grep -i \"deadlock detected\" ``` #### Workflow Execution Errors ```bash # Find workflow timeout errors kubectl logs -n cloudflow deployment/cloudflow-workflow-engine | \\ grep -E \"timeout|exceeded.*3600\" # Find workflow retry attempts kubectl logs -n cloudflow deployment/cloudflow-workflow-engine | \\ grep -E \"retry attempt [0-9]|retrying",
              "score": null
            }
          ],
          "latency_ms": 842.2874930001853,
          "query_id": "realistic_012",
          "dimension": "synonym",
          "query": "Which signing algorithm does CloudFlow use for JWTs?"
        },
        {
          "key_facts": [
            "RS256",
            "RS256 algorithm",
            "RS256 signing algorithm"
          ],
          "found_facts": [
            "RS256",
            "RS256 algorithm",
            "RS256 signing algorithm"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "api_reference"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_1",
              "content": "**Technology**: Node.js with Express.js framework **Replicas**: 12 pods (production), auto-scaling 8-20 based on CPU **Resource Allocation**: 2 vCPU, 4GB RAM per pod **Key Responsibilities**: - JWT token validation (delegated to Auth Service for initial validation) - Rate limiting: 1000 requests per minute per API key (sliding window) - Request/response transformation and validation using JSON Schema - Routing to downstream services based on URL path patterns - CORS handling for web clients - Request/response logging and correlation ID injection **Critical Endpoints**: - `POST /api/v1/workflows` - Create new workflow - `GET /api/v1/workflows/:id` - Retrieve workflow status - `POST /api/v1/workflows/:id/execute` - Trigger workflow execution - `GET /api/v1/workflows/:id/history` - Get execution history **Dependencies**: - Auth Service (for token validation) - Redis (for rate limiting counters) - All downstream microservices **Performance Targets**: - P50 latency: < 50ms - P99 latency: < 200ms - Throughput: 10,000 RPS sustained --- ### Auth Service **Purpose**: Centralized authentication and authorization service handling user identity, token generation, and permission validation. **Technology**: Go with gRPC for internal communication, REST for external **Replicas**: 8 pods (production), auto-scaling 6-12 **Resource Allocation**: 1 vCPU, 2GB RAM per pod **Key Responsibilities**: - User authentication via multiple providers (OAuth2, SAML, local credentials) - JWT token generation and validation (RS256 algorithm) - Role-based access control (RBAC) with fine-grained permissions - Session management with Redis-backed storage - API key generation and validation for service accounts - MFA enforcement for administrative operations **Authentication Flow**: ``` Client Request \u2192 API Gateway \u2192 Auth Service \u2502 \u251c\u2500 Validate credentials \u251c\u2500 Check MFA if required \u251c\u2500 Generate JWT (15min expiry) \u251c\u2500 Generate refresh token (7 days) \u2514\u2500 Store session in Redis ``` **Token Structure**: - Access Token: JWT with 15-minute expiry - Refresh Token: Opaque token with 7-day expiry, stored in PostgreSQL - Claims: user_id, email, roles[], permissions[], tenant_id **Security Features**: - Password hashing: Argon2id with 64MB memory, 4 iterations - Token rotation on refresh to prevent replay attacks - Brute force protection: 5 failed attempts \u2192 15-minute lockout - Secrets stored in HashiCorp Vault, rotated every 90 days **Performance Targets**: - Token validation: < 10ms (P99) - Token generation: < 50ms (P99) - Throughput: 5,000 RPS for validation operations --- ### Workflow Engine **Purpose**: Core orchestration service that executes workflow definitions, manages state transitions, and coordinates task execution across distributed systems.",
              "score": null
            },
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_0",
              "content": "# CloudFlow API Reference Version 2.1.0 | Last Updated: January 2026 ## Overview The CloudFlow API is a RESTful service that enables developers to programmatically manage cloud workflows, data pipelines, and automation tasks. This documentation provides comprehensive details on authentication, endpoints, request/response formats, error handling, and best practices. **Base URL:** `https://api.cloudflow.io/v2` **API Status:** https://status.cloudflow.io ## Authentication CloudFlow supports three authentication methods to suit different use cases and security requirements. ### API Keys API keys provide simple authentication for server-to-server communication. Include your API key in the request header: ```bash curl -H \"X-API-Key: cf_live_a1b2c3d4e5f6g7h8i9j0\" \\ https://api.cloudflow.io/v2/workflows ``` **Security Notes:** - Never expose API keys in client-side code - Rotate keys every 90 days - Use separate keys for development and production environments ### OAuth 2.0 OAuth 2.0 is recommended for applications that access CloudFlow on behalf of users. We support the Authorization Code flow with PKCE. **Authorization Endpoint:** `https://auth.cloudflow.io/oauth/authorize` **Token Endpoint:** `https://auth.cloudflow.io/oauth/token` **Supported Scopes:** - `workflows:read` - Read workflow configurations - `workflows:write` - Create and modify workflows - `pipelines:read` - Read pipeline data - `pipelines:write` - Create and manage pipelines - `analytics:read` - Access analytics and metrics - `admin:full` - Full administrative access Example authorization request: ```bash curl -X POST https://auth.cloudflow.io/oauth/token \\ -H \"Content-Type: application/x-www-form-urlencoded\" \\ -d \"grant_type=authorization_code\" \\ -d \"code=AUTH_CODE_HERE\" \\ -d \"client_id=YOUR_CLIENT_ID\" \\ -d \"client_secret=YOUR_CLIENT_SECRET\" \\ -d \"redirect_uri=https://yourapp.com/callback\" ``` ### JWT Tokens For advanced use cases, CloudFlow supports JSON Web Tokens (JWT) with RS256 signing algorithm. JWTs must include the following claims: - `iss` (issuer): Your application identifier - `sub` (subject): User or service account ID - `aud` (audience): `https://api.cloudflow.io` - `exp` (expiration): Unix timestamp (max 3600 seconds from `iat`) - `iat` (issued at): Unix timestamp - `scope`: Space-separated list of requested scopes Example JWT header: ```python import jwt import time from cryptography.hazmat.primitives import serialization from cryptography.hazmat.backends import default_backend # Load your private key with open('private_key.pem', 'rb') as key_file: private_key = serialization.load_pem_private_key( key_file.read(), password=None, backend=default_backend() ) # Create JWT payload payload = { 'iss': 'your-app-id', 'sub': 'user-12345', 'aud': 'https://api.cloudflow.io', 'exp': int(time.time()) + 3600, 'iat': int(time.time()), 'scope': 'workflows:read workflows:write' } # Generate token token = jwt.encode(payload, private_key, algorithm='RS256') # Use in API request headers = {'Authorization': f'Bearer {token}'} ``` **Token Expiration:** All tokens expire after 3600 seconds (1 hour). Implement token refresh logic in your application.",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_0",
              "content": "# CloudFlow Platform Troubleshooting Guide **Version:** 3.2.1 **Last Updated:** January 2026 **Audience:** Platform Engineers, DevOps, Support Teams ## Table of Contents 1. [Overview](#overview) 2. [Authentication & Authorization Issues](#authentication--authorization-issues) 3. [Performance Problems](#performance-problems) 4. [Database Connection Issues](#database-connection-issues) 5. [Workflow Execution Failures](#workflow-execution-failures) 6. [Rate Limiting & Throttling](#rate-limiting--throttling) 7. [Log Analysis & Debugging](#log-analysis--debugging) 8. [Escalation Procedures](#escalation-procedures) --- ## Overview This guide provides comprehensive troubleshooting steps for common CloudFlow platform issues encountered in production environments. Each section includes error symptoms, root cause analysis, resolution steps, and preventive measures. ### Quick Diagnostic Checklist Before diving into specific issues, perform these initial checks: - Verify service health: `cloudflow status --all` - Check API connectivity: `curl -I https://api.cloudflow.io/health` - Review recent deployments: `kubectl get deployments -n cloudflow --sort-by=.metadata.creationTimestamp` - Inspect platform metrics: `cloudflow metrics --last 1h` --- ## Authentication & Authorization Issues ### 401 Unauthorized Errors #### Symptoms - API requests return `401 Unauthorized` - Error message: `Authentication credentials were not provided or are invalid` - Frontend displays \"Session expired\" message #### Common Causes **1. Token Expiration** CloudFlow access tokens expire after 3600 seconds (1 hour) by default. Refresh tokens are valid for 30 days. **Verification:** ```bash # Decode JWT to check expiration echo $CF_ACCESS_TOKEN | cut -d'.' -f2 | base64 -d | jq '.exp' # Compare with current time date +%s ``` **Resolution:** ```bash # Refresh the access token curl -X POST https://api.cloudflow.io/auth/refresh \\ -H \"Content-Type: application/json\" \\ -d '{\"refresh_token\": \"'$CF_REFRESH_TOKEN'\"}' # Update environment variable export CF_ACCESS_TOKEN=\"<new_token>\" ``` **2. Invalid JWT Signature** Error: `JWT signature verification failed` **Causes:** - Token was modified or corrupted - Using wrong signing key - Token generated with different secret **Resolution:** ```bash # Validate token structure cloudflow auth validate --token $CF_ACCESS_TOKEN # Generate new token cloudflow auth login --username <user> --password <pass> # For service accounts cloudflow auth service-account --create --name <sa-name> --scopes \"workflow:read,workflow:write\" ``` **3. Clock Skew Issues** JWT validation fails when system clocks are out of sync (tolerance: \u00b1300 seconds).",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_8",
              "content": "Leverage caching:** ```bash # Enable client-side caching export CLOUDFLOW_CACHE_ENABLED=true export CLOUDFLOW_CACHE_TTL=300 # Cache workflow metadata cloudflow workflows list --use-cache --cache-ttl 600 ``` --- ## Log Analysis & Debugging ### Accessing CloudFlow Logs #### Kubernetes Deployments ```bash # List all CloudFlow pods kubectl get pods -n cloudflow # Tail logs from API server kubectl logs -f -n cloudflow deployment/cloudflow-api --tail=100 # Get logs from specific pod kubectl logs -n cloudflow cloudflow-api-7d4f6b8c9d-x7k2m # Get logs from previous crashed pod kubectl logs -n cloudflow cloudflow-api-7d4f6b8c9d-x7k2m --previous # Get logs from all pods in deployment kubectl logs -n cloudflow deployment/cloudflow-api --all-containers=true # Stream logs from multiple pods kubectl logs -n cloudflow -l app=cloudflow-api -f --max-log-requests=10 ``` #### Log Levels CloudFlow supports the following log levels: - `TRACE` - Very detailed debugging information - `DEBUG` - Detailed debugging information - `INFO` - Informational messages (default) - `WARN` - Warning messages - `ERROR` - Error messages - `FATAL` - Fatal errors causing shutdown **Changing log levels:** ```bash # Set global log level cloudflow config set logging.level DEBUG # Set log level for specific component cloudflow config set logging.components.database DEBUG cloudflow config set logging.components.auth INFO cloudflow config set logging.components.workflows TRACE # Temporary log level increase (resets after 1 hour) cloudflow debug set-log-level DEBUG --duration 3600 # View current log configuration cloudflow config get logging --format json ``` ### Grep Patterns for Common Issues #### Authentication Failures ```bash # Find all authentication errors kubectl logs -n cloudflow deployment/cloudflow-api | \\ grep -i \"authentication\\|401\\|unauthorized\" | \\ tail -n 50 # Find JWT validation failures kubectl logs -n cloudflow deployment/cloudflow-api | \\ grep -E \"JWT|token.*invalid|signature.*failed\" # Find clock skew issues kubectl logs -n cloudflow deployment/cloudflow-api | \\ grep -i \"clock skew\\|time.*sync\\|nbf\\|exp\" ``` #### Database Errors ```bash # Find database connection errors kubectl logs -n cloudflow deployment/cloudflow-api | \\ grep -E \"connection.*pool|could not connect|database.*timeout\" # Find slow queries kubectl logs -n cloudflow deployment/cloudflow-api | \\ grep \"slow query\" | \\ awk '{print $NF}' | \\ sort -n | \\ tail -n 20 # Find deadlock errors kubectl logs -n cloudflow deployment/cloudflow-api | \\ grep -i \"deadlock detected\" ``` #### Workflow Execution Errors ```bash # Find workflow timeout errors kubectl logs -n cloudflow deployment/cloudflow-workflow-engine | \\ grep -E \"timeout|exceeded.*3600\" # Find workflow retry attempts kubectl logs -n cloudflow deployment/cloudflow-workflow-engine | \\ grep -E \"retry attempt [0-9]|retrying",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_8",
              "content": "### Cache Invalidation Strategies **Time-Based Expiration (TTL)**: - Short-lived data: 5-15 minutes (session tokens, rate limit counters) - Medium-lived data: 1 hour (workflow definitions, templates) - Long-lived data: 24 hours (static configuration) **Event-Based Invalidation**: ``` Database Update Event \u2192 Kafka (cache.invalidation topic) \u2502 \u25bc All service instances consume event \u2502 \u25bc Redis DEL for affected keys ``` **Pattern-Based Invalidation**: - Use Redis SCAN + DEL for wildcard patterns - Example: Invalidate all user caches: `user:*:{user_id}` **Cache Warming**: - Scheduled job runs every hour to pre-populate frequently accessed data - Targets: Top 1000 workflows, system configuration, active user profiles - Reduces cache miss rate during traffic spikes ### Multi-Level Caching Application implements L1 (in-memory) and L2 (Redis) caching: ``` Request \u2192 L1 Cache (Node.js Map, 1000 entry LRU) \u2502 MISS \u2502 \u25bc L2 Cache (Redis, distributed) \u2502 MISS \u2502 \u25bc Database (PostgreSQL) ``` L1 cache reduces Redis network calls by 40% for hot data. --- ## Security Architecture ### Network Security **Zero-Trust Network Model**: - All service-to-service communication encrypted with mTLS - Certificate rotation: Every 90 days (automated via cert-manager) - Certificate authority: Internal PKI with HashiCorp Vault - Network segmentation: Private subnets for services, public subnet for ALB only **Service Mesh (Istio)**: ``` Service A \u2192 Envoy Sidecar (mTLS client cert) \u2502 Encrypted channel \u2502 Envoy Sidecar (mTLS server cert) \u2192 Service B ``` **Firewall Rules**: - Security groups: Deny all by default, explicit allow rules - Ingress: Only ALB can reach API Gateway (port 8080) - Egress: Services can only reach specific dependencies - Database access: Limited to application subnets only ### Authentication & Authorization **JWT Token Validation**: - Algorithm: RS256 (asymmetric signing) - Key rotation: Every 30 days with 7-day overlap period - Public key distribution: JWKS endpoint cached in Redis - Validation: Signature, expiry, issuer, audience claims - Token revocation: Blacklist in Redis for compromised tokens **Permission Model**: ``` User \u2192 Roles \u2192 Permissions \u2198 \u2197 Tenants (Multi-tenancy isolation) ``` Example permissions: - `workflow:read` - View workflows - `workflow:write` - Create/update workflows - `workflow:execute` - Trigger workflow execution - `workflow:delete` - Delete workflows - `admin:*` - All administrative operations **API Key Management**: - Format: `cfk_live_<32-char-random>` (production), `cfk_test_<32-char-random>` (sandbox) - Hashing: SHA-256 before storage in PostgreSQL - Scoping: API keys can be scoped to specific workflows or operations - Rate limits: Configurable",
              "score": null
            }
          ],
          "latency_ms": 895.7512149972899,
          "query_id": "realistic_012",
          "dimension": "problem",
          "query": "My JWT validation is failing with algorithm mismatch"
        },
        {
          "key_facts": [
            "RS256",
            "RS256 algorithm",
            "RS256 signing algorithm"
          ],
          "found_facts": [
            "RS256",
            "RS256 algorithm",
            "RS256 signing algorithm"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "api_reference"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_0",
              "content": "# CloudFlow API Reference Version 2.1.0 | Last Updated: January 2026 ## Overview The CloudFlow API is a RESTful service that enables developers to programmatically manage cloud workflows, data pipelines, and automation tasks. This documentation provides comprehensive details on authentication, endpoints, request/response formats, error handling, and best practices. **Base URL:** `https://api.cloudflow.io/v2` **API Status:** https://status.cloudflow.io ## Authentication CloudFlow supports three authentication methods to suit different use cases and security requirements. ### API Keys API keys provide simple authentication for server-to-server communication. Include your API key in the request header: ```bash curl -H \"X-API-Key: cf_live_a1b2c3d4e5f6g7h8i9j0\" \\ https://api.cloudflow.io/v2/workflows ``` **Security Notes:** - Never expose API keys in client-side code - Rotate keys every 90 days - Use separate keys for development and production environments ### OAuth 2.0 OAuth 2.0 is recommended for applications that access CloudFlow on behalf of users. We support the Authorization Code flow with PKCE. **Authorization Endpoint:** `https://auth.cloudflow.io/oauth/authorize` **Token Endpoint:** `https://auth.cloudflow.io/oauth/token` **Supported Scopes:** - `workflows:read` - Read workflow configurations - `workflows:write` - Create and modify workflows - `pipelines:read` - Read pipeline data - `pipelines:write` - Create and manage pipelines - `analytics:read` - Access analytics and metrics - `admin:full` - Full administrative access Example authorization request: ```bash curl -X POST https://auth.cloudflow.io/oauth/token \\ -H \"Content-Type: application/x-www-form-urlencoded\" \\ -d \"grant_type=authorization_code\" \\ -d \"code=AUTH_CODE_HERE\" \\ -d \"client_id=YOUR_CLIENT_ID\" \\ -d \"client_secret=YOUR_CLIENT_SECRET\" \\ -d \"redirect_uri=https://yourapp.com/callback\" ``` ### JWT Tokens For advanced use cases, CloudFlow supports JSON Web Tokens (JWT) with RS256 signing algorithm. JWTs must include the following claims: - `iss` (issuer): Your application identifier - `sub` (subject): User or service account ID - `aud` (audience): `https://api.cloudflow.io` - `exp` (expiration): Unix timestamp (max 3600 seconds from `iat`) - `iat` (issued at): Unix timestamp - `scope`: Space-separated list of requested scopes Example JWT header: ```python import jwt import time from cryptography.hazmat.primitives import serialization from cryptography.hazmat.backends import default_backend # Load your private key with open('private_key.pem', 'rb') as key_file: private_key = serialization.load_pem_private_key( key_file.read(), password=None, backend=default_backend() ) # Create JWT payload payload = { 'iss': 'your-app-id', 'sub': 'user-12345', 'aud': 'https://api.cloudflow.io', 'exp': int(time.time()) + 3600, 'iat': int(time.time()), 'scope': 'workflows:read workflows:write' } # Generate token token = jwt.encode(payload, private_key, algorithm='RS256') # Use in API request headers = {'Authorization': f'Bearer {token}'} ``` **Token Expiration:** All tokens expire after 3600 seconds (1 hour). Implement token refresh logic in your application.",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_1",
              "content": "**Technology**: Node.js with Express.js framework **Replicas**: 12 pods (production), auto-scaling 8-20 based on CPU **Resource Allocation**: 2 vCPU, 4GB RAM per pod **Key Responsibilities**: - JWT token validation (delegated to Auth Service for initial validation) - Rate limiting: 1000 requests per minute per API key (sliding window) - Request/response transformation and validation using JSON Schema - Routing to downstream services based on URL path patterns - CORS handling for web clients - Request/response logging and correlation ID injection **Critical Endpoints**: - `POST /api/v1/workflows` - Create new workflow - `GET /api/v1/workflows/:id` - Retrieve workflow status - `POST /api/v1/workflows/:id/execute` - Trigger workflow execution - `GET /api/v1/workflows/:id/history` - Get execution history **Dependencies**: - Auth Service (for token validation) - Redis (for rate limiting counters) - All downstream microservices **Performance Targets**: - P50 latency: < 50ms - P99 latency: < 200ms - Throughput: 10,000 RPS sustained --- ### Auth Service **Purpose**: Centralized authentication and authorization service handling user identity, token generation, and permission validation. **Technology**: Go with gRPC for internal communication, REST for external **Replicas**: 8 pods (production), auto-scaling 6-12 **Resource Allocation**: 1 vCPU, 2GB RAM per pod **Key Responsibilities**: - User authentication via multiple providers (OAuth2, SAML, local credentials) - JWT token generation and validation (RS256 algorithm) - Role-based access control (RBAC) with fine-grained permissions - Session management with Redis-backed storage - API key generation and validation for service accounts - MFA enforcement for administrative operations **Authentication Flow**: ``` Client Request \u2192 API Gateway \u2192 Auth Service \u2502 \u251c\u2500 Validate credentials \u251c\u2500 Check MFA if required \u251c\u2500 Generate JWT (15min expiry) \u251c\u2500 Generate refresh token (7 days) \u2514\u2500 Store session in Redis ``` **Token Structure**: - Access Token: JWT with 15-minute expiry - Refresh Token: Opaque token with 7-day expiry, stored in PostgreSQL - Claims: user_id, email, roles[], permissions[], tenant_id **Security Features**: - Password hashing: Argon2id with 64MB memory, 4 iterations - Token rotation on refresh to prevent replay attacks - Brute force protection: 5 failed attempts \u2192 15-minute lockout - Secrets stored in HashiCorp Vault, rotated every 90 days **Performance Targets**: - Token validation: < 10ms (P99) - Token generation: < 50ms (P99) - Throughput: 5,000 RPS for validation operations --- ### Workflow Engine **Purpose**: Core orchestration service that executes workflow definitions, manages state transitions, and coordinates task execution across distributed systems.",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_0",
              "content": "# CloudFlow Platform Troubleshooting Guide **Version:** 3.2.1 **Last Updated:** January 2026 **Audience:** Platform Engineers, DevOps, Support Teams ## Table of Contents 1. [Overview](#overview) 2. [Authentication & Authorization Issues](#authentication--authorization-issues) 3. [Performance Problems](#performance-problems) 4. [Database Connection Issues](#database-connection-issues) 5. [Workflow Execution Failures](#workflow-execution-failures) 6. [Rate Limiting & Throttling](#rate-limiting--throttling) 7. [Log Analysis & Debugging](#log-analysis--debugging) 8. [Escalation Procedures](#escalation-procedures) --- ## Overview This guide provides comprehensive troubleshooting steps for common CloudFlow platform issues encountered in production environments. Each section includes error symptoms, root cause analysis, resolution steps, and preventive measures. ### Quick Diagnostic Checklist Before diving into specific issues, perform these initial checks: - Verify service health: `cloudflow status --all` - Check API connectivity: `curl -I https://api.cloudflow.io/health` - Review recent deployments: `kubectl get deployments -n cloudflow --sort-by=.metadata.creationTimestamp` - Inspect platform metrics: `cloudflow metrics --last 1h` --- ## Authentication & Authorization Issues ### 401 Unauthorized Errors #### Symptoms - API requests return `401 Unauthorized` - Error message: `Authentication credentials were not provided or are invalid` - Frontend displays \"Session expired\" message #### Common Causes **1. Token Expiration** CloudFlow access tokens expire after 3600 seconds (1 hour) by default. Refresh tokens are valid for 30 days. **Verification:** ```bash # Decode JWT to check expiration echo $CF_ACCESS_TOKEN | cut -d'.' -f2 | base64 -d | jq '.exp' # Compare with current time date +%s ``` **Resolution:** ```bash # Refresh the access token curl -X POST https://api.cloudflow.io/auth/refresh \\ -H \"Content-Type: application/json\" \\ -d '{\"refresh_token\": \"'$CF_REFRESH_TOKEN'\"}' # Update environment variable export CF_ACCESS_TOKEN=\"<new_token>\" ``` **2. Invalid JWT Signature** Error: `JWT signature verification failed` **Causes:** - Token was modified or corrupted - Using wrong signing key - Token generated with different secret **Resolution:** ```bash # Validate token structure cloudflow auth validate --token $CF_ACCESS_TOKEN # Generate new token cloudflow auth login --username <user> --password <pass> # For service accounts cloudflow auth service-account --create --name <sa-name> --scopes \"workflow:read,workflow:write\" ``` **3. Clock Skew Issues** JWT validation fails when system clocks are out of sync (tolerance: \u00b1300 seconds).",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_6",
              "content": "**Session Storage**: - Key pattern: `session:{user_id}` - TTL: 15 minutes (aligned with JWT expiry) - Data: User session state, preferences - Invalidation: On logout or password change 2. **Rate Limiting Counters**: - Key pattern: `ratelimit:{api_key}:{window}` - TTL: 60 seconds (sliding window) - Data: Request count per window - Algorithm: Token bucket with Redis INCR 3. **Compiled Templates**: - Key pattern: `template:{template_id}:{version}` - TTL: 1 hour - Data: Compiled Handlebars template - Invalidation: On template update 4. **Workflow Definitions**: - Key pattern: `workflow:def:{workflow_id}` - TTL: 1 hour - Data: Parsed workflow JSON - Invalidation: On workflow update or manual flush 5. **User Profiles**: - Key pattern: `user:profile:{user_id}` - TTL: 30 minutes - Data: User metadata (name, email, roles) - Invalidation: On profile update **Cache Hit Rates** (Production Metrics): - Session lookups: 98.5% - Workflow definitions: 94.2% - Template cache: 99.1% - User profiles: 91.8% **Performance Characteristics**: - GET operation P99: < 2ms - SET operation P99: < 3ms - Throughput: 100,000 operations per second - Network latency: < 1ms (same AZ) --- ### Kafka Event Streaming **Cluster Configuration**: - 5 broker nodes (distributed across 3 AZs) - Instance type: kafka.m5.2xlarge (8 vCPU, 32GB RAM) - Storage: 10TB per broker (gp3 SSD) - ZooKeeper ensemble: 3 nodes for cluster coordination - Replication factor: 3 (min in-sync replicas: 2) **Topic Architecture**: ``` workflow.events (32 partitions): - Workflow lifecycle events (created, started, completed, failed) - Retention: 7 days - Message rate: 5,000/sec peak - Consumer groups: workflow-engine, analytics-pipeline notifications.email (16 partitions): - Email notification triggers - Retention: 3 days - Message rate: 2,000/sec peak - Consumer groups: notification-service notifications.sms (8 partitions): - SMS notification triggers - Retention: 3 days - Message rate: 500/sec peak - Consumer groups: notification-service audit.logs (24 partitions): - Security and compliance audit events - Retention: 90 days (compliance requirement) - Message rate: 3,000/sec peak - Consumer groups: audit-processor, security-monitor dead-letter-queue (8 partitions): - Failed message processing - Retention: 30 days - Manual intervention required - Consumer groups: ops-team-alerts ``` **Producer Configuration**: - Acknowledgment: `acks=all` (wait for all in-sync replicas) - Compression: LZ4 (reduces network bandwidth by 60%) - Batching: 100ms linger time, 100KB batch size - Idempotence: Enabled to prevent duplicates **Consumer Configuration**: - Auto-commit: Disabled (manual commit after processing) - Offset reset: Earliest (replay from beginning on new consumer group) - Max poll",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_1",
              "content": "**Diagnosis:** ```bash # Check system time timedatectl status # Compare with NTP server ntpdate -q pool.ntp.org # Check JWT issued time vs current time jwt_iat=$(echo $CF_ACCESS_TOKEN | cut -d'.' -f2 | base64 -d | jq '.iat') current_time=$(date +%s) skew=$((current_time - jwt_iat)) echo \"Clock skew: $skew seconds\" ``` **Resolution:** ```bash # Sync with NTP server sudo ntpdate -s pool.ntp.org # Enable automatic time sync sudo timedatectl set-ntp true # Restart CloudFlow client cloudflow restart ``` **4. Insufficient Permissions** Error: `User does not have required permissions for this operation` **Check user roles:** ```bash cloudflow auth whoami --verbose # Expected output: # User: john.doe@company.com # Roles: developer, workflow-admin # Scopes: workflow:*, data:read, metrics:read ``` **Request permission elevation:** ```bash # Submit access request cloudflow auth request-access \\ --resource \"workflow:production:deploy\" \\ --justification \"Deploy critical hotfix for TICKET-1234\" # Check pending approvals cloudflow auth list-requests --status pending ``` ### 403 Forbidden Errors #### RBAC Policy Violations CloudFlow uses role-based access control (RBAC) with the following hierarchy: - `viewer` - Read-only access - `developer` - Create and modify workflows (non-production) - `operator` - Execute workflows, view logs - `admin` - Full access to workspace - `platform-admin` - Cross-workspace administration **Verify resource permissions:** ```bash # Check effective permissions cloudflow rbac check \\ --user john.doe@company.com \\ --resource workflow:prod-pipeline \\ --action execute # List all policies affecting user cloudflow rbac policies --user john.doe@company.com --verbose ``` --- ## Performance Problems ### Slow Query Performance #### Symptoms - API response times > 5000ms - Database query latency warnings in logs - CloudFlow UI becomes unresponsive - Timeout errors: `Request exceeded maximum duration of 30000ms` #### Diagnosis Steps **1. Identify Slow Queries** ```bash # Query CloudFlow metrics cloudflow metrics query --metric api_request_duration_ms \\ --filter \"p95 > 5000\" \\ --last 1h # Check database slow query log kubectl logs -n cloudflow deploy/cloudflow-db-primary | \\ grep \"slow query\" | \\ tail -n 50 # Analyze query patterns cloudflow db analyze-queries --min-duration 5000 --limit 20 ``` **2.",
              "score": null
            }
          ],
          "latency_ms": 846.9921889991383,
          "query_id": "realistic_012",
          "dimension": "casual",
          "query": "jwt algorithm"
        },
        {
          "key_facts": [
            "RS256",
            "RS256 algorithm",
            "RS256 signing algorithm"
          ],
          "found_facts": [
            "RS256",
            "RS256 algorithm",
            "RS256 signing algorithm"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "api_reference"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_0",
              "content": "# CloudFlow Platform Troubleshooting Guide **Version:** 3.2.1 **Last Updated:** January 2026 **Audience:** Platform Engineers, DevOps, Support Teams ## Table of Contents 1. [Overview](#overview) 2. [Authentication & Authorization Issues](#authentication--authorization-issues) 3. [Performance Problems](#performance-problems) 4. [Database Connection Issues](#database-connection-issues) 5. [Workflow Execution Failures](#workflow-execution-failures) 6. [Rate Limiting & Throttling](#rate-limiting--throttling) 7. [Log Analysis & Debugging](#log-analysis--debugging) 8. [Escalation Procedures](#escalation-procedures) --- ## Overview This guide provides comprehensive troubleshooting steps for common CloudFlow platform issues encountered in production environments. Each section includes error symptoms, root cause analysis, resolution steps, and preventive measures. ### Quick Diagnostic Checklist Before diving into specific issues, perform these initial checks: - Verify service health: `cloudflow status --all` - Check API connectivity: `curl -I https://api.cloudflow.io/health` - Review recent deployments: `kubectl get deployments -n cloudflow --sort-by=.metadata.creationTimestamp` - Inspect platform metrics: `cloudflow metrics --last 1h` --- ## Authentication & Authorization Issues ### 401 Unauthorized Errors #### Symptoms - API requests return `401 Unauthorized` - Error message: `Authentication credentials were not provided or are invalid` - Frontend displays \"Session expired\" message #### Common Causes **1. Token Expiration** CloudFlow access tokens expire after 3600 seconds (1 hour) by default. Refresh tokens are valid for 30 days. **Verification:** ```bash # Decode JWT to check expiration echo $CF_ACCESS_TOKEN | cut -d'.' -f2 | base64 -d | jq '.exp' # Compare with current time date +%s ``` **Resolution:** ```bash # Refresh the access token curl -X POST https://api.cloudflow.io/auth/refresh \\ -H \"Content-Type: application/json\" \\ -d '{\"refresh_token\": \"'$CF_REFRESH_TOKEN'\"}' # Update environment variable export CF_ACCESS_TOKEN=\"<new_token>\" ``` **2. Invalid JWT Signature** Error: `JWT signature verification failed` **Causes:** - Token was modified or corrupted - Using wrong signing key - Token generated with different secret **Resolution:** ```bash # Validate token structure cloudflow auth validate --token $CF_ACCESS_TOKEN # Generate new token cloudflow auth login --username <user> --password <pass> # For service accounts cloudflow auth service-account --create --name <sa-name> --scopes \"workflow:read,workflow:write\" ``` **3. Clock Skew Issues** JWT validation fails when system clocks are out of sync (tolerance: \u00b1300 seconds).",
              "score": null
            },
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_0",
              "content": "# CloudFlow API Reference Version 2.1.0 | Last Updated: January 2026 ## Overview The CloudFlow API is a RESTful service that enables developers to programmatically manage cloud workflows, data pipelines, and automation tasks. This documentation provides comprehensive details on authentication, endpoints, request/response formats, error handling, and best practices. **Base URL:** `https://api.cloudflow.io/v2` **API Status:** https://status.cloudflow.io ## Authentication CloudFlow supports three authentication methods to suit different use cases and security requirements. ### API Keys API keys provide simple authentication for server-to-server communication. Include your API key in the request header: ```bash curl -H \"X-API-Key: cf_live_a1b2c3d4e5f6g7h8i9j0\" \\ https://api.cloudflow.io/v2/workflows ``` **Security Notes:** - Never expose API keys in client-side code - Rotate keys every 90 days - Use separate keys for development and production environments ### OAuth 2.0 OAuth 2.0 is recommended for applications that access CloudFlow on behalf of users. We support the Authorization Code flow with PKCE. **Authorization Endpoint:** `https://auth.cloudflow.io/oauth/authorize` **Token Endpoint:** `https://auth.cloudflow.io/oauth/token` **Supported Scopes:** - `workflows:read` - Read workflow configurations - `workflows:write` - Create and modify workflows - `pipelines:read` - Read pipeline data - `pipelines:write` - Create and manage pipelines - `analytics:read` - Access analytics and metrics - `admin:full` - Full administrative access Example authorization request: ```bash curl -X POST https://auth.cloudflow.io/oauth/token \\ -H \"Content-Type: application/x-www-form-urlencoded\" \\ -d \"grant_type=authorization_code\" \\ -d \"code=AUTH_CODE_HERE\" \\ -d \"client_id=YOUR_CLIENT_ID\" \\ -d \"client_secret=YOUR_CLIENT_SECRET\" \\ -d \"redirect_uri=https://yourapp.com/callback\" ``` ### JWT Tokens For advanced use cases, CloudFlow supports JSON Web Tokens (JWT) with RS256 signing algorithm. JWTs must include the following claims: - `iss` (issuer): Your application identifier - `sub` (subject): User or service account ID - `aud` (audience): `https://api.cloudflow.io` - `exp` (expiration): Unix timestamp (max 3600 seconds from `iat`) - `iat` (issued at): Unix timestamp - `scope`: Space-separated list of requested scopes Example JWT header: ```python import jwt import time from cryptography.hazmat.primitives import serialization from cryptography.hazmat.backends import default_backend # Load your private key with open('private_key.pem', 'rb') as key_file: private_key = serialization.load_pem_private_key( key_file.read(), password=None, backend=default_backend() ) # Create JWT payload payload = { 'iss': 'your-app-id', 'sub': 'user-12345', 'aud': 'https://api.cloudflow.io', 'exp': int(time.time()) + 3600, 'iat': int(time.time()), 'scope': 'workflows:read workflows:write' } # Generate token token = jwt.encode(payload, private_key, algorithm='RS256') # Use in API request headers = {'Authorization': f'Bearer {token}'} ``` **Token Expiration:** All tokens expire after 3600 seconds (1 hour). Implement token refresh logic in your application.",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_1",
              "content": "**Technology**: Node.js with Express.js framework **Replicas**: 12 pods (production), auto-scaling 8-20 based on CPU **Resource Allocation**: 2 vCPU, 4GB RAM per pod **Key Responsibilities**: - JWT token validation (delegated to Auth Service for initial validation) - Rate limiting: 1000 requests per minute per API key (sliding window) - Request/response transformation and validation using JSON Schema - Routing to downstream services based on URL path patterns - CORS handling for web clients - Request/response logging and correlation ID injection **Critical Endpoints**: - `POST /api/v1/workflows` - Create new workflow - `GET /api/v1/workflows/:id` - Retrieve workflow status - `POST /api/v1/workflows/:id/execute` - Trigger workflow execution - `GET /api/v1/workflows/:id/history` - Get execution history **Dependencies**: - Auth Service (for token validation) - Redis (for rate limiting counters) - All downstream microservices **Performance Targets**: - P50 latency: < 50ms - P99 latency: < 200ms - Throughput: 10,000 RPS sustained --- ### Auth Service **Purpose**: Centralized authentication and authorization service handling user identity, token generation, and permission validation. **Technology**: Go with gRPC for internal communication, REST for external **Replicas**: 8 pods (production), auto-scaling 6-12 **Resource Allocation**: 1 vCPU, 2GB RAM per pod **Key Responsibilities**: - User authentication via multiple providers (OAuth2, SAML, local credentials) - JWT token generation and validation (RS256 algorithm) - Role-based access control (RBAC) with fine-grained permissions - Session management with Redis-backed storage - API key generation and validation for service accounts - MFA enforcement for administrative operations **Authentication Flow**: ``` Client Request \u2192 API Gateway \u2192 Auth Service \u2502 \u251c\u2500 Validate credentials \u251c\u2500 Check MFA if required \u251c\u2500 Generate JWT (15min expiry) \u251c\u2500 Generate refresh token (7 days) \u2514\u2500 Store session in Redis ``` **Token Structure**: - Access Token: JWT with 15-minute expiry - Refresh Token: Opaque token with 7-day expiry, stored in PostgreSQL - Claims: user_id, email, roles[], permissions[], tenant_id **Security Features**: - Password hashing: Argon2id with 64MB memory, 4 iterations - Token rotation on refresh to prevent replay attacks - Brute force protection: 5 failed attempts \u2192 15-minute lockout - Secrets stored in HashiCorp Vault, rotated every 90 days **Performance Targets**: - Token validation: < 10ms (P99) - Token generation: < 50ms (P99) - Throughput: 5,000 RPS for validation operations --- ### Workflow Engine **Purpose**: Core orchestration service that executes workflow definitions, manages state transitions, and coordinates task execution across distributed systems.",
              "score": null
            },
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_5",
              "content": "**Supported Events:** - `workflow.started` - `workflow.completed` - `workflow.failed` - `pipeline.completed` - `pipeline.failed` **Webhook Payload Example:** ```json { \"event\": \"workflow.completed\", \"timestamp\": \"2026-01-24T10:45:00Z\", \"data\": { \"workflow_id\": \"wf_8x7k2m9p\", \"execution_id\": \"exec_9k3m7n2q\", \"status\": \"completed\", \"duration_ms\": 3420, \"records_processed\": 1247 } } ``` Configure webhooks in your account settings or via the API: ```bash curl -X POST https://api.cloudflow.io/v2/webhooks \\ -H \"X-API-Key: YOUR_API_KEY\" \\ -H \"Content-Type: application/json\" \\ -d '{ \"url\": \"https://your-app.com/cloudflow-webhook\", \"events\": [\"workflow.completed\", \"workflow.failed\"], \"secret\": \"whsec_your_webhook_secret\" }' ``` ## Support For additional help and resources: - **Documentation:** https://docs.cloudflow.io - **API Status:** https://status.cloudflow.io - **Support Email:** support@cloudflow.io - **Community Forum:** https://community.cloudflow.io Enterprise customers have access to 24/7 priority support via phone and dedicated Slack channels.",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_6",
              "content": "**Session Storage**: - Key pattern: `session:{user_id}` - TTL: 15 minutes (aligned with JWT expiry) - Data: User session state, preferences - Invalidation: On logout or password change 2. **Rate Limiting Counters**: - Key pattern: `ratelimit:{api_key}:{window}` - TTL: 60 seconds (sliding window) - Data: Request count per window - Algorithm: Token bucket with Redis INCR 3. **Compiled Templates**: - Key pattern: `template:{template_id}:{version}` - TTL: 1 hour - Data: Compiled Handlebars template - Invalidation: On template update 4. **Workflow Definitions**: - Key pattern: `workflow:def:{workflow_id}` - TTL: 1 hour - Data: Parsed workflow JSON - Invalidation: On workflow update or manual flush 5. **User Profiles**: - Key pattern: `user:profile:{user_id}` - TTL: 30 minutes - Data: User metadata (name, email, roles) - Invalidation: On profile update **Cache Hit Rates** (Production Metrics): - Session lookups: 98.5% - Workflow definitions: 94.2% - Template cache: 99.1% - User profiles: 91.8% **Performance Characteristics**: - GET operation P99: < 2ms - SET operation P99: < 3ms - Throughput: 100,000 operations per second - Network latency: < 1ms (same AZ) --- ### Kafka Event Streaming **Cluster Configuration**: - 5 broker nodes (distributed across 3 AZs) - Instance type: kafka.m5.2xlarge (8 vCPU, 32GB RAM) - Storage: 10TB per broker (gp3 SSD) - ZooKeeper ensemble: 3 nodes for cluster coordination - Replication factor: 3 (min in-sync replicas: 2) **Topic Architecture**: ``` workflow.events (32 partitions): - Workflow lifecycle events (created, started, completed, failed) - Retention: 7 days - Message rate: 5,000/sec peak - Consumer groups: workflow-engine, analytics-pipeline notifications.email (16 partitions): - Email notification triggers - Retention: 3 days - Message rate: 2,000/sec peak - Consumer groups: notification-service notifications.sms (8 partitions): - SMS notification triggers - Retention: 3 days - Message rate: 500/sec peak - Consumer groups: notification-service audit.logs (24 partitions): - Security and compliance audit events - Retention: 90 days (compliance requirement) - Message rate: 3,000/sec peak - Consumer groups: audit-processor, security-monitor dead-letter-queue (8 partitions): - Failed message processing - Retention: 30 days - Manual intervention required - Consumer groups: ops-team-alerts ``` **Producer Configuration**: - Acknowledgment: `acks=all` (wait for all in-sync replicas) - Compression: LZ4 (reduces network bandwidth by 60%) - Batching: 100ms linger time, 100KB batch size - Idempotence: Enabled to prevent duplicates **Consumer Configuration**: - Auto-commit: Disabled (manual commit after processing) - Offset reset: Earliest (replay from beginning on new consumer group) - Max poll",
              "score": null
            }
          ],
          "latency_ms": 1015.7283980006468,
          "query_id": "realistic_012",
          "dimension": "contextual",
          "query": "I'm implementing JWT verification, what algorithm should I expect?"
        },
        {
          "key_facts": [
            "RS256",
            "RS256 algorithm",
            "RS256 signing algorithm"
          ],
          "found_facts": [
            "RS256",
            "RS256 algorithm",
            "RS256 signing algorithm"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "api_reference"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_0",
              "content": "# CloudFlow Platform Troubleshooting Guide **Version:** 3.2.1 **Last Updated:** January 2026 **Audience:** Platform Engineers, DevOps, Support Teams ## Table of Contents 1. [Overview](#overview) 2. [Authentication & Authorization Issues](#authentication--authorization-issues) 3. [Performance Problems](#performance-problems) 4. [Database Connection Issues](#database-connection-issues) 5. [Workflow Execution Failures](#workflow-execution-failures) 6. [Rate Limiting & Throttling](#rate-limiting--throttling) 7. [Log Analysis & Debugging](#log-analysis--debugging) 8. [Escalation Procedures](#escalation-procedures) --- ## Overview This guide provides comprehensive troubleshooting steps for common CloudFlow platform issues encountered in production environments. Each section includes error symptoms, root cause analysis, resolution steps, and preventive measures. ### Quick Diagnostic Checklist Before diving into specific issues, perform these initial checks: - Verify service health: `cloudflow status --all` - Check API connectivity: `curl -I https://api.cloudflow.io/health` - Review recent deployments: `kubectl get deployments -n cloudflow --sort-by=.metadata.creationTimestamp` - Inspect platform metrics: `cloudflow metrics --last 1h` --- ## Authentication & Authorization Issues ### 401 Unauthorized Errors #### Symptoms - API requests return `401 Unauthorized` - Error message: `Authentication credentials were not provided or are invalid` - Frontend displays \"Session expired\" message #### Common Causes **1. Token Expiration** CloudFlow access tokens expire after 3600 seconds (1 hour) by default. Refresh tokens are valid for 30 days. **Verification:** ```bash # Decode JWT to check expiration echo $CF_ACCESS_TOKEN | cut -d'.' -f2 | base64 -d | jq '.exp' # Compare with current time date +%s ``` **Resolution:** ```bash # Refresh the access token curl -X POST https://api.cloudflow.io/auth/refresh \\ -H \"Content-Type: application/json\" \\ -d '{\"refresh_token\": \"'$CF_REFRESH_TOKEN'\"}' # Update environment variable export CF_ACCESS_TOKEN=\"<new_token>\" ``` **2. Invalid JWT Signature** Error: `JWT signature verification failed` **Causes:** - Token was modified or corrupted - Using wrong signing key - Token generated with different secret **Resolution:** ```bash # Validate token structure cloudflow auth validate --token $CF_ACCESS_TOKEN # Generate new token cloudflow auth login --username <user> --password <pass> # For service accounts cloudflow auth service-account --create --name <sa-name> --scopes \"workflow:read,workflow:write\" ``` **3. Clock Skew Issues** JWT validation fails when system clocks are out of sync (tolerance: \u00b1300 seconds).",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_1",
              "content": "**Technology**: Node.js with Express.js framework **Replicas**: 12 pods (production), auto-scaling 8-20 based on CPU **Resource Allocation**: 2 vCPU, 4GB RAM per pod **Key Responsibilities**: - JWT token validation (delegated to Auth Service for initial validation) - Rate limiting: 1000 requests per minute per API key (sliding window) - Request/response transformation and validation using JSON Schema - Routing to downstream services based on URL path patterns - CORS handling for web clients - Request/response logging and correlation ID injection **Critical Endpoints**: - `POST /api/v1/workflows` - Create new workflow - `GET /api/v1/workflows/:id` - Retrieve workflow status - `POST /api/v1/workflows/:id/execute` - Trigger workflow execution - `GET /api/v1/workflows/:id/history` - Get execution history **Dependencies**: - Auth Service (for token validation) - Redis (for rate limiting counters) - All downstream microservices **Performance Targets**: - P50 latency: < 50ms - P99 latency: < 200ms - Throughput: 10,000 RPS sustained --- ### Auth Service **Purpose**: Centralized authentication and authorization service handling user identity, token generation, and permission validation. **Technology**: Go with gRPC for internal communication, REST for external **Replicas**: 8 pods (production), auto-scaling 6-12 **Resource Allocation**: 1 vCPU, 2GB RAM per pod **Key Responsibilities**: - User authentication via multiple providers (OAuth2, SAML, local credentials) - JWT token generation and validation (RS256 algorithm) - Role-based access control (RBAC) with fine-grained permissions - Session management with Redis-backed storage - API key generation and validation for service accounts - MFA enforcement for administrative operations **Authentication Flow**: ``` Client Request \u2192 API Gateway \u2192 Auth Service \u2502 \u251c\u2500 Validate credentials \u251c\u2500 Check MFA if required \u251c\u2500 Generate JWT (15min expiry) \u251c\u2500 Generate refresh token (7 days) \u2514\u2500 Store session in Redis ``` **Token Structure**: - Access Token: JWT with 15-minute expiry - Refresh Token: Opaque token with 7-day expiry, stored in PostgreSQL - Claims: user_id, email, roles[], permissions[], tenant_id **Security Features**: - Password hashing: Argon2id with 64MB memory, 4 iterations - Token rotation on refresh to prevent replay attacks - Brute force protection: 5 failed attempts \u2192 15-minute lockout - Secrets stored in HashiCorp Vault, rotated every 90 days **Performance Targets**: - Token validation: < 10ms (P99) - Token generation: < 50ms (P99) - Throughput: 5,000 RPS for validation operations --- ### Workflow Engine **Purpose**: Core orchestration service that executes workflow definitions, manages state transitions, and coordinates task execution across distributed systems.",
              "score": null
            },
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_0",
              "content": "# CloudFlow API Reference Version 2.1.0 | Last Updated: January 2026 ## Overview The CloudFlow API is a RESTful service that enables developers to programmatically manage cloud workflows, data pipelines, and automation tasks. This documentation provides comprehensive details on authentication, endpoints, request/response formats, error handling, and best practices. **Base URL:** `https://api.cloudflow.io/v2` **API Status:** https://status.cloudflow.io ## Authentication CloudFlow supports three authentication methods to suit different use cases and security requirements. ### API Keys API keys provide simple authentication for server-to-server communication. Include your API key in the request header: ```bash curl -H \"X-API-Key: cf_live_a1b2c3d4e5f6g7h8i9j0\" \\ https://api.cloudflow.io/v2/workflows ``` **Security Notes:** - Never expose API keys in client-side code - Rotate keys every 90 days - Use separate keys for development and production environments ### OAuth 2.0 OAuth 2.0 is recommended for applications that access CloudFlow on behalf of users. We support the Authorization Code flow with PKCE. **Authorization Endpoint:** `https://auth.cloudflow.io/oauth/authorize` **Token Endpoint:** `https://auth.cloudflow.io/oauth/token` **Supported Scopes:** - `workflows:read` - Read workflow configurations - `workflows:write` - Create and modify workflows - `pipelines:read` - Read pipeline data - `pipelines:write` - Create and manage pipelines - `analytics:read` - Access analytics and metrics - `admin:full` - Full administrative access Example authorization request: ```bash curl -X POST https://auth.cloudflow.io/oauth/token \\ -H \"Content-Type: application/x-www-form-urlencoded\" \\ -d \"grant_type=authorization_code\" \\ -d \"code=AUTH_CODE_HERE\" \\ -d \"client_id=YOUR_CLIENT_ID\" \\ -d \"client_secret=YOUR_CLIENT_SECRET\" \\ -d \"redirect_uri=https://yourapp.com/callback\" ``` ### JWT Tokens For advanced use cases, CloudFlow supports JSON Web Tokens (JWT) with RS256 signing algorithm. JWTs must include the following claims: - `iss` (issuer): Your application identifier - `sub` (subject): User or service account ID - `aud` (audience): `https://api.cloudflow.io` - `exp` (expiration): Unix timestamp (max 3600 seconds from `iat`) - `iat` (issued at): Unix timestamp - `scope`: Space-separated list of requested scopes Example JWT header: ```python import jwt import time from cryptography.hazmat.primitives import serialization from cryptography.hazmat.backends import default_backend # Load your private key with open('private_key.pem', 'rb') as key_file: private_key = serialization.load_pem_private_key( key_file.read(), password=None, backend=default_backend() ) # Create JWT payload payload = { 'iss': 'your-app-id', 'sub': 'user-12345', 'aud': 'https://api.cloudflow.io', 'exp': int(time.time()) + 3600, 'iat': int(time.time()), 'scope': 'workflows:read workflows:write' } # Generate token token = jwt.encode(payload, private_key, algorithm='RS256') # Use in API request headers = {'Authorization': f'Bearer {token}'} ``` **Token Expiration:** All tokens expire after 3600 seconds (1 hour). Implement token refresh logic in your application.",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_8",
              "content": "### Cache Invalidation Strategies **Time-Based Expiration (TTL)**: - Short-lived data: 5-15 minutes (session tokens, rate limit counters) - Medium-lived data: 1 hour (workflow definitions, templates) - Long-lived data: 24 hours (static configuration) **Event-Based Invalidation**: ``` Database Update Event \u2192 Kafka (cache.invalidation topic) \u2502 \u25bc All service instances consume event \u2502 \u25bc Redis DEL for affected keys ``` **Pattern-Based Invalidation**: - Use Redis SCAN + DEL for wildcard patterns - Example: Invalidate all user caches: `user:*:{user_id}` **Cache Warming**: - Scheduled job runs every hour to pre-populate frequently accessed data - Targets: Top 1000 workflows, system configuration, active user profiles - Reduces cache miss rate during traffic spikes ### Multi-Level Caching Application implements L1 (in-memory) and L2 (Redis) caching: ``` Request \u2192 L1 Cache (Node.js Map, 1000 entry LRU) \u2502 MISS \u2502 \u25bc L2 Cache (Redis, distributed) \u2502 MISS \u2502 \u25bc Database (PostgreSQL) ``` L1 cache reduces Redis network calls by 40% for hot data. --- ## Security Architecture ### Network Security **Zero-Trust Network Model**: - All service-to-service communication encrypted with mTLS - Certificate rotation: Every 90 days (automated via cert-manager) - Certificate authority: Internal PKI with HashiCorp Vault - Network segmentation: Private subnets for services, public subnet for ALB only **Service Mesh (Istio)**: ``` Service A \u2192 Envoy Sidecar (mTLS client cert) \u2502 Encrypted channel \u2502 Envoy Sidecar (mTLS server cert) \u2192 Service B ``` **Firewall Rules**: - Security groups: Deny all by default, explicit allow rules - Ingress: Only ALB can reach API Gateway (port 8080) - Egress: Services can only reach specific dependencies - Database access: Limited to application subnets only ### Authentication & Authorization **JWT Token Validation**: - Algorithm: RS256 (asymmetric signing) - Key rotation: Every 30 days with 7-day overlap period - Public key distribution: JWKS endpoint cached in Redis - Validation: Signature, expiry, issuer, audience claims - Token revocation: Blacklist in Redis for compromised tokens **Permission Model**: ``` User \u2192 Roles \u2192 Permissions \u2198 \u2197 Tenants (Multi-tenancy isolation) ``` Example permissions: - `workflow:read` - View workflows - `workflow:write` - Create/update workflows - `workflow:execute` - Trigger workflow execution - `workflow:delete` - Delete workflows - `admin:*` - All administrative operations **API Key Management**: - Format: `cfk_live_<32-char-random>` (production), `cfk_test_<32-char-random>` (sandbox) - Hashing: SHA-256 before storage in PostgreSQL - Scoping: API keys can be scoped to specific workflows or operations - Rate limits: Configurable",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_6",
              "content": "**Session Storage**: - Key pattern: `session:{user_id}` - TTL: 15 minutes (aligned with JWT expiry) - Data: User session state, preferences - Invalidation: On logout or password change 2. **Rate Limiting Counters**: - Key pattern: `ratelimit:{api_key}:{window}` - TTL: 60 seconds (sliding window) - Data: Request count per window - Algorithm: Token bucket with Redis INCR 3. **Compiled Templates**: - Key pattern: `template:{template_id}:{version}` - TTL: 1 hour - Data: Compiled Handlebars template - Invalidation: On template update 4. **Workflow Definitions**: - Key pattern: `workflow:def:{workflow_id}` - TTL: 1 hour - Data: Parsed workflow JSON - Invalidation: On workflow update or manual flush 5. **User Profiles**: - Key pattern: `user:profile:{user_id}` - TTL: 30 minutes - Data: User metadata (name, email, roles) - Invalidation: On profile update **Cache Hit Rates** (Production Metrics): - Session lookups: 98.5% - Workflow definitions: 94.2% - Template cache: 99.1% - User profiles: 91.8% **Performance Characteristics**: - GET operation P99: < 2ms - SET operation P99: < 3ms - Throughput: 100,000 operations per second - Network latency: < 1ms (same AZ) --- ### Kafka Event Streaming **Cluster Configuration**: - 5 broker nodes (distributed across 3 AZs) - Instance type: kafka.m5.2xlarge (8 vCPU, 32GB RAM) - Storage: 10TB per broker (gp3 SSD) - ZooKeeper ensemble: 3 nodes for cluster coordination - Replication factor: 3 (min in-sync replicas: 2) **Topic Architecture**: ``` workflow.events (32 partitions): - Workflow lifecycle events (created, started, completed, failed) - Retention: 7 days - Message rate: 5,000/sec peak - Consumer groups: workflow-engine, analytics-pipeline notifications.email (16 partitions): - Email notification triggers - Retention: 3 days - Message rate: 2,000/sec peak - Consumer groups: notification-service notifications.sms (8 partitions): - SMS notification triggers - Retention: 3 days - Message rate: 500/sec peak - Consumer groups: notification-service audit.logs (24 partitions): - Security and compliance audit events - Retention: 90 days (compliance requirement) - Message rate: 3,000/sec peak - Consumer groups: audit-processor, security-monitor dead-letter-queue (8 partitions): - Failed message processing - Retention: 30 days - Manual intervention required - Consumer groups: ops-team-alerts ``` **Producer Configuration**: - Acknowledgment: `acks=all` (wait for all in-sync replicas) - Compression: LZ4 (reduces network bandwidth by 60%) - Batching: 100ms linger time, 100KB batch size - Idempotence: Enabled to prevent duplicates **Consumer Configuration**: - Auto-commit: Disabled (manual commit after processing) - Offset reset: Earliest (replay from beginning on new consumer group) - Max poll",
              "score": null
            }
          ],
          "latency_ms": 1031.5169610003068,
          "query_id": "realistic_012",
          "dimension": "negation",
          "query": "Why doesn't HS256 work for token validation?"
        },
        {
          "key_facts": [
            "TTL: 1 hour",
            "Workflow Definitions: TTL: 1 hour"
          ],
          "found_facts": [],
          "missed_facts": [
            "TTL: 1 hour",
            "Workflow Definitions: TTL: 1 hour"
          ],
          "coverage": 0.0,
          "expected_docs": [
            "architecture_overview"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_8",
              "content": "### Cache Invalidation Strategies **Time-Based Expiration (TTL)**: - Short-lived data: 5-15 minutes (session tokens, rate limit counters) - Medium-lived data: 1 hour (workflow definitions, templates) - Long-lived data: 24 hours (static configuration) **Event-Based Invalidation**: ``` Database Update Event \u2192 Kafka (cache.invalidation topic) \u2502 \u25bc All service instances consume event \u2502 \u25bc Redis DEL for affected keys ``` **Pattern-Based Invalidation**: - Use Redis SCAN + DEL for wildcard patterns - Example: Invalidate all user caches: `user:*:{user_id}` **Cache Warming**: - Scheduled job runs every hour to pre-populate frequently accessed data - Targets: Top 1000 workflows, system configuration, active user profiles - Reduces cache miss rate during traffic spikes ### Multi-Level Caching Application implements L1 (in-memory) and L2 (Redis) caching: ``` Request \u2192 L1 Cache (Node.js Map, 1000 entry LRU) \u2502 MISS \u2502 \u25bc L2 Cache (Redis, distributed) \u2502 MISS \u2502 \u25bc Database (PostgreSQL) ``` L1 cache reduces Redis network calls by 40% for hot data. --- ## Security Architecture ### Network Security **Zero-Trust Network Model**: - All service-to-service communication encrypted with mTLS - Certificate rotation: Every 90 days (automated via cert-manager) - Certificate authority: Internal PKI with HashiCorp Vault - Network segmentation: Private subnets for services, public subnet for ALB only **Service Mesh (Istio)**: ``` Service A \u2192 Envoy Sidecar (mTLS client cert) \u2502 Encrypted channel \u2502 Envoy Sidecar (mTLS server cert) \u2192 Service B ``` **Firewall Rules**: - Security groups: Deny all by default, explicit allow rules - Ingress: Only ALB can reach API Gateway (port 8080) - Egress: Services can only reach specific dependencies - Database access: Limited to application subnets only ### Authentication & Authorization **JWT Token Validation**: - Algorithm: RS256 (asymmetric signing) - Key rotation: Every 30 days with 7-day overlap period - Public key distribution: JWKS endpoint cached in Redis - Validation: Signature, expiry, issuer, audience claims - Token revocation: Blacklist in Redis for compromised tokens **Permission Model**: ``` User \u2192 Roles \u2192 Permissions \u2198 \u2197 Tenants (Multi-tenancy isolation) ``` Example permissions: - `workflow:read` - View workflows - `workflow:write` - Create/update workflows - `workflow:execute` - Trigger workflow execution - `workflow:delete` - Delete workflows - `admin:*` - All administrative operations **API Key Management**: - Format: `cfk_live_<32-char-random>` (production), `cfk_test_<32-char-random>` (sandbox) - Hashing: SHA-256 before storage in PostgreSQL - Scoping: API keys can be scoped to specific workflows or operations - Rate limits: Configurable",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_2",
              "content": "Review Query Execution Plans** ```sql -- Connect to CloudFlow database cloudflow db connect --readonly -- Explain slow query EXPLAIN ANALYZE SELECT w.*, e.status, e.error_message FROM workflows w LEFT JOIN executions e ON w.id = e.workflow_id WHERE w.workspace_id = 'ws_abc123' AND e.created_at > NOW() - INTERVAL '7 days' ORDER BY e.created_at DESC; -- Check for missing indexes SELECT schemaname, tablename, indexname, indexdef FROM pg_indexes WHERE tablename IN ('workflows', 'executions', 'workflow_steps') ORDER BY tablename, indexname; ``` **3. Optimize Queries** Common optimization techniques: **Add missing indexes:** ```sql -- Index for workflow lookup by workspace CREATE INDEX CONCURRENTLY idx_workflows_workspace_created ON workflows(workspace_id, created_at DESC); -- Index for execution status queries CREATE INDEX CONCURRENTLY idx_executions_status_created ON executions(workflow_id, status, created_at DESC) WHERE status IN ('running', 'pending'); -- Composite index for common filter combinations CREATE INDEX CONCURRENTLY idx_executions_workspace_status ON executions(workspace_id, status, created_at) INCLUDE (error_message, retry_count); ``` **Use query result caching:** ```bash # Enable query cache for workspace metadata cloudflow config set cache.workspace.ttl 3600 # Configure Redis cache backend cloudflow config set cache.backend redis cloudflow config set cache.redis.host redis.cloudflow.svc.cluster.local cloudflow config set cache.redis.port 6379 ``` ### High API Latency #### Latency Breakdown Analysis ```bash # Generate latency report cloudflow metrics latency-report --endpoint \"/api/v1/workflows\" --last 24h # Sample output: # Endpoint: POST /api/v1/workflows/execute # P50: 245ms | P95: 1823ms | P99: 4521ms # Breakdown: # - Auth: 45ms (18%) # - DB Query: 156ms (64%) # - Business Logic: 32ms (13%) # - Response Serialization: 12ms (5%) ``` **Network latency issues:** ```bash # Test connectivity to CloudFlow API time curl -w \"@curl-format.txt\" -o /dev/null -s https://api.cloudflow.io/health # Create curl-format.txt: cat > curl-format.txt << EOF time_namelookup: %{time_namelookup}s\\n time_connect: %{time_connect}s\\n time_appconnect: %{time_appconnect}s\\n time_pretransfer: %{time_pretransfer}s\\n time_redirect: %{time_redirect}s\\n time_starttransfer: %{time_starttransfer}s\\n ----------\\n time_total: %{time_total}s\\n EOF # Trace route to API endpoint traceroute api.cloudflow.io # Check DNS resolution time dig api.cloudflow.io | grep \"Query time\" ``` ### Memory Leaks #### Detection ```bash # Monitor CloudFlow service memory usage kubectl top pods -n cloudflow --sort-by=memory # Get detailed memory metrics for specific pod kubectl exec -n cloudflow deploy/cloudflow-api -- \\ curl localhost:9090/metrics | grep memory # Check for OOMKilled pods kubectl get pods -n cloudflow --field-selector=status.phase=Failed | \\ grep OOMKilled # Review memory limits and requests kubectl describe deployment cloudflow-api -n cloudflow | \\ grep -A 5 \"Limits\\|Requests\" ``` #### Common Causes **1.",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_5",
              "content": "Success/failure events published back to Kafka ``` **Event Schema**: ```json { \"event_id\": \"uuid-v4\", \"event_type\": \"workflow.execution.completed\", \"timestamp\": \"2026-01-15T10:30:00.000Z\", \"correlation_id\": \"request-trace-id\", \"payload\": { \"workflow_id\": \"wf-12345\", \"execution_id\": \"exec-67890\", \"status\": \"COMPLETED\", \"duration_ms\": 4230 }, \"metadata\": { \"source_service\": \"workflow-engine\", \"schema_version\": \"1.0\" } } ``` ### Inter-Service Communication Services communicate using two primary patterns: **Synchronous (gRPC)**: - Auth Service validation calls from API Gateway - User Service profile lookups - Low latency requirement (< 50ms) - Request-response pattern with circuit breaker **Asynchronous (Kafka)**: - Workflow execution events - Notification triggers - Audit log events - Eventual consistency acceptable --- ## Database Architecture ### PostgreSQL Primary Database **Cluster Configuration**: - Primary-replica setup with 1 primary + 2 read replicas - Instance type: db.r6g.2xlarge (8 vCPU, 64GB RAM) - Storage: 2TB gp3 SSD with 12,000 IOPS - Multi-AZ deployment for high availability - Automated backups: Daily snapshots, 30-day retention - Point-in-time recovery: 5-minute granularity **Database Schema Design**: ``` Core Tables: - users (5M rows): User accounts and profiles - workflows (2M rows): Workflow definitions - workflow_executions (500M rows, partitioned): Execution history - workflow_steps (2B rows, partitioned): Individual step records - schedules (100K rows): Scheduled workflow triggers - notifications (1B rows, partitioned): Notification delivery log Partitioning Strategy: - workflow_executions: Monthly partitions by created_at - workflow_steps: Monthly partitions by created_at - notifications: Weekly partitions by created_at - Automatic partition creation via cron job - Retention policy: Drop partitions older than 12 months ``` **Indexing Strategy**: - Primary keys: UUIDs with B-tree indexes - Frequently queried columns: Compound indexes (e.g., user_id + status + created_at) - JSONB columns: GIN indexes for workflow definitions - Full-text search: GiST indexes on description fields **Connection Pooling**: - PgBouncer in transaction mode - Pool size: 100 connections per service - Max client connections: 2000 - Connection timeout: 30 seconds **Performance Characteristics**: - Read query P99: < 20ms - Write query P99: < 50ms - Transaction throughput: 15,000 TPS - Replication lag: < 500ms --- ### Redis Caching Layer **Cluster Configuration**: - Redis Cluster with 6 nodes (3 primary + 3 replica) - Instance type: cache.r6g.xlarge (4 vCPU, 26GB RAM) - Total memory: 78GB usable cache space - Persistence: RDB snapshots every 5 minutes + AOF - Eviction policy: allkeys-lru **Cache Usage Patterns**: 1.",
              "score": null
            },
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_6",
              "content": "**Verify Service Health**: ```bash kubectl get pods -n cloudflow-prod curl https://api.cloudflow.io/health ``` **Recovery Time Objective (RTO)**: 4 hours **Recovery Point Objective (RPO)**: 24 hours --- ## Scaling and Performance ### Horizontal Pod Autoscaling CloudFlow is configured with Horizontal Pod Autoscaler (HPA) to automatically scale based on resource utilization: ```yaml # hpa.yaml apiVersion: autoscaling/v2 kind: HorizontalPodAutoscaler metadata: name: cloudflow-api-hpa namespace: cloudflow-prod spec: scaleTargetRef: apiVersion: apps/v1 kind: Deployment name: cloudflow-api minReplicas: 3 maxReplicas: 10 metrics: - type: Resource resource: name: cpu target: type: Utilization averageUtilization: 70 - type: Resource resource: name: memory target: type: Utilization averageUtilization: 80 behavior: scaleDown: stabilizationWindowSeconds: 300 policies: - type: Percent value: 50 periodSeconds: 60 scaleUp: stabilizationWindowSeconds: 60 policies: - type: Percent value: 100 periodSeconds: 30 - type: Pods value: 2 periodSeconds: 30 selectPolicy: Max ``` Monitor autoscaling events: ```bash kubectl get hpa -n cloudflow-prod -w kubectl describe hpa cloudflow-api-hpa -n cloudflow-prod ``` ### Performance Optimization #### Connection Pooling Ensure optimal database connection pooling settings in PgBouncer: - **Pool Mode**: Transaction (optimal for microservices) - **Default Pool Size**: 25 connections per user - **Max DB Connections**: 100 (matches PostgreSQL `max_connections`) #### Redis Caching Implement Redis caching for frequently accessed data: ```javascript // Example caching strategy const CACHE_TTL = { workflows: 300, // 5 minutes userSessions: 3600, // 1 hour apiResults: 60 // 1 minute }; ``` #### Load Testing Perform regular load testing to validate scaling configuration: ```bash # Install k6 for load testing kubectl apply -f - <<EOF apiVersion: v1 kind: ConfigMap metadata: name: loadtest-script namespace: cloudflow-prod data: script.js: | import http from 'k6/http'; import { check, sleep } from 'k6'; export let options = { stages: [ { duration: '2m', target: 100 }, { duration: '5m', target: 100 }, { duration: '2m', target: 200 }, { duration: '5m', target: 200 }, { duration: '2m', target: 0 }, ], thresholds: { http_req_duration: ['p(95)<500'], }, }; export default function () { let res = http.get('https://api.cloudflow.io/api/workflows'); check(res, { 'status is 200': (r) => r.status === 200 }); sleep(1); } EOF ``` **Performance Targets**: - P95 API latency: < 500ms - P99 API latency: < 1000ms - Error rate: < 0.1% - Throughput: > 1000 requests/second --- ## Security and Compliance ### Network Policies Implement network policies to restrict pod-to-pod communication: ```yaml # network-policy.yaml apiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: cloudflow-network-policy namespace: cloudflow-prod spec:",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_9",
              "content": "per API key (default: 1000 RPM) ### Secrets Management **HashiCorp Vault Integration**: - Dynamic database credentials: Generated on-demand, 1-hour TTL - Encryption keys: Transit secrets engine for encryption-as-a-service - API keys for external services: Stored with versioning - Rotation policy: Automated rotation every 90 days with notification **Secret Access Pattern**: ``` Service starts \u2192 Vault authentication (Kubernetes service account) \u2502 \u25bc Request secret lease (1-hour TTL) \u2502 \u25bc Vault returns dynamic credentials \u2502 \u25bc Service connects to PostgreSQL \u2502 Renew lease every 30 minutes ``` ### Data Protection **Encryption at Rest**: - PostgreSQL: AES-256 encryption enabled at volume level - Redis: Encryption enabled using AWS KMS - Kafka: Encryption at rest for all data on brokers - S3 backups: Server-side encryption with KMS (SSE-KMS) **Encryption in Transit**: - External traffic: TLS 1.3 only (TLS 1.2 deprecated) - Internal traffic: mTLS via Istio service mesh - Database connections: SSL/TLS enforced (sslmode=require) **Data Masking**: - Sensitive fields (email, phone) masked in logs - PII redacted from error messages and stack traces - Audit logs: Full data retention with access controls **Compliance**: - GDPR: User data export and deletion workflows - SOC 2 Type II: Audit logging for all data access - HIPAA: PHI isolation in dedicated tenants with enhanced encryption --- ## Performance Characteristics ### Latency Targets **API Operations** (P99 latency): - `GET /workflows/{id}`: < 50ms (cache hit), < 150ms (cache miss) - `POST /workflows`: < 200ms (includes validation and database write) - `POST /workflows/{id}/execute`: < 100ms (async, returns execution ID) - `GET /workflows/{id}/history`: < 300ms (paginated, 50 records per page) **Workflow Execution** (P99 latency): - Simple workflow (< 5 steps): < 2 seconds - Medium workflow (5-15 steps): < 5 seconds - Complex workflow (> 15 steps): < 15 seconds - Parallel execution: Linear scaling up to 10 concurrent branches **Database Operations** (P99 latency): - Simple SELECT: < 5ms - JOIN query: < 20ms - INSERT/UPDATE: < 10ms - Batch operations: < 100ms (batch size: 100 records) **Cache Operations** (P99 latency): - Redis GET: < 2ms - Redis SET: < 3ms - Redis DEL: < 2ms ### Throughput Capacity **API Gateway**: - Sustained: 10,000 requests per second - Peak: 25,000 requests per second (5-minute burst) - Rate limiting: 1,000 requests per minute per API key **Workflow Engine**: - Concurrent executions: 8,000 workflows (across 16 pods)",
              "score": null
            }
          ],
          "latency_ms": 897.2048309988168,
          "query_id": "realistic_013",
          "dimension": "original",
          "query": "What is the Redis cache TTL for workflow definitions?"
        },
        {
          "key_facts": [
            "TTL: 1 hour",
            "Workflow Definitions: TTL: 1 hour"
          ],
          "found_facts": [],
          "missed_facts": [
            "TTL: 1 hour",
            "Workflow Definitions: TTL: 1 hour"
          ],
          "coverage": 0.0,
          "expected_docs": [
            "architecture_overview"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_5",
              "content": "Success/failure events published back to Kafka ``` **Event Schema**: ```json { \"event_id\": \"uuid-v4\", \"event_type\": \"workflow.execution.completed\", \"timestamp\": \"2026-01-15T10:30:00.000Z\", \"correlation_id\": \"request-trace-id\", \"payload\": { \"workflow_id\": \"wf-12345\", \"execution_id\": \"exec-67890\", \"status\": \"COMPLETED\", \"duration_ms\": 4230 }, \"metadata\": { \"source_service\": \"workflow-engine\", \"schema_version\": \"1.0\" } } ``` ### Inter-Service Communication Services communicate using two primary patterns: **Synchronous (gRPC)**: - Auth Service validation calls from API Gateway - User Service profile lookups - Low latency requirement (< 50ms) - Request-response pattern with circuit breaker **Asynchronous (Kafka)**: - Workflow execution events - Notification triggers - Audit log events - Eventual consistency acceptable --- ## Database Architecture ### PostgreSQL Primary Database **Cluster Configuration**: - Primary-replica setup with 1 primary + 2 read replicas - Instance type: db.r6g.2xlarge (8 vCPU, 64GB RAM) - Storage: 2TB gp3 SSD with 12,000 IOPS - Multi-AZ deployment for high availability - Automated backups: Daily snapshots, 30-day retention - Point-in-time recovery: 5-minute granularity **Database Schema Design**: ``` Core Tables: - users (5M rows): User accounts and profiles - workflows (2M rows): Workflow definitions - workflow_executions (500M rows, partitioned): Execution history - workflow_steps (2B rows, partitioned): Individual step records - schedules (100K rows): Scheduled workflow triggers - notifications (1B rows, partitioned): Notification delivery log Partitioning Strategy: - workflow_executions: Monthly partitions by created_at - workflow_steps: Monthly partitions by created_at - notifications: Weekly partitions by created_at - Automatic partition creation via cron job - Retention policy: Drop partitions older than 12 months ``` **Indexing Strategy**: - Primary keys: UUIDs with B-tree indexes - Frequently queried columns: Compound indexes (e.g., user_id + status + created_at) - JSONB columns: GIN indexes for workflow definitions - Full-text search: GiST indexes on description fields **Connection Pooling**: - PgBouncer in transaction mode - Pool size: 100 connections per service - Max client connections: 2000 - Connection timeout: 30 seconds **Performance Characteristics**: - Read query P99: < 20ms - Write query P99: < 50ms - Transaction throughput: 15,000 TPS - Replication lag: < 500ms --- ### Redis Caching Layer **Cluster Configuration**: - Redis Cluster with 6 nodes (3 primary + 3 replica) - Instance type: cache.r6g.xlarge (4 vCPU, 26GB RAM) - Total memory: 78GB usable cache space - Persistence: RDB snapshots every 5 minutes + AOF - Eviction policy: allkeys-lru **Cache Usage Patterns**: 1.",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_7",
              "content": "records: 500 - Session timeout: 30 seconds --- ## Message Queue Patterns ### Pub/Sub Pattern Used for broadcasting events to multiple interested consumers: ``` Workflow Engine publishes workflow.execution.completed \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u25bc \u25bc \u25bc \u25bc Analytics Notification Audit Logger Billing Service Service Service Service ``` Each service maintains its own consumer group and processes events independently. Failures in one consumer don't affect others. ### Request-Reply Pattern Synchronous communication over async messaging (used sparingly): ``` API Gateway publishes request \u2192 Kafka (reply-to: temp-queue-123) \u2502 \u25bc Service processes request \u2502 \u25bc Service publishes response \u2192 temp-queue-123 \u2502 \u25bc API Gateway receives response ``` Timeout: 5 seconds, fallback to direct HTTP call if no response. ### Saga Pattern Distributed transaction management for multi-service workflows: ``` Step 1: Create Order \u2192 SUCCESS \u2192 Step 2: Reserve Inventory \u2502 FAILURE \u2502 \u25bc Compensating Transaction \u2502 \u25bc Cancel Order (Rollback) ``` Orchestrated by Workflow Engine with compensation logic defined in workflow DSL. ### Dead Letter Queue (DLQ) Failed messages after all retry attempts are routed to DLQ: ``` Message processing fails (3 retries with exponential backoff) \u2502 \u25bc Publish to dead-letter-queue topic \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u25bc \u25bc Alert to PagerDuty Store in PostgreSQL for analysis \u2502 \u2502 \u25bc \u25bc Manual investigation Automated pattern detection ``` DLQ Processing SLA: < 4 hours for critical events, < 24 hours for non-critical. --- ## Caching Strategy ### Cache-Aside Pattern (Lazy Loading) Primary caching pattern used across all services: ``` 1. Application checks cache (Redis GET) 2. If HIT \u2192 Return cached data 3. If MISS: a. Query database (PostgreSQL) b. Store result in cache with TTL c. Return data to application ``` **Implementation Example** (Workflow Definition): ```javascript async function getWorkflowDefinition(workflowId) { const cacheKey = `workflow:def:${workflowId}`; // Step 1: Check cache let workflow = await redis.get(cacheKey); if (workflow) { metrics.increment('cache.hit.workflow_def'); return JSON.parse(workflow); } // Step 2: Cache miss - query database metrics.increment('cache.miss.workflow_def'); workflow = await db.query( 'SELECT * FROM workflows WHERE id = $1', [workflowId] ); // Step 3: Store in cache (1 hour TTL) await redis.setex(cacheKey, 3600, JSON.stringify(workflow)); return workflow; } ``` ### Write-Through Pattern Used for critical data where cache consistency is paramount: ``` 1. Application writes to database 2. Database transaction commits 3. Application updates cache 4. Return success to client ``` Applied to: User profiles, authentication sessions, system configuration.",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_8",
              "content": "Leverage caching:** ```bash # Enable client-side caching export CLOUDFLOW_CACHE_ENABLED=true export CLOUDFLOW_CACHE_TTL=300 # Cache workflow metadata cloudflow workflows list --use-cache --cache-ttl 600 ``` --- ## Log Analysis & Debugging ### Accessing CloudFlow Logs #### Kubernetes Deployments ```bash # List all CloudFlow pods kubectl get pods -n cloudflow # Tail logs from API server kubectl logs -f -n cloudflow deployment/cloudflow-api --tail=100 # Get logs from specific pod kubectl logs -n cloudflow cloudflow-api-7d4f6b8c9d-x7k2m # Get logs from previous crashed pod kubectl logs -n cloudflow cloudflow-api-7d4f6b8c9d-x7k2m --previous # Get logs from all pods in deployment kubectl logs -n cloudflow deployment/cloudflow-api --all-containers=true # Stream logs from multiple pods kubectl logs -n cloudflow -l app=cloudflow-api -f --max-log-requests=10 ``` #### Log Levels CloudFlow supports the following log levels: - `TRACE` - Very detailed debugging information - `DEBUG` - Detailed debugging information - `INFO` - Informational messages (default) - `WARN` - Warning messages - `ERROR` - Error messages - `FATAL` - Fatal errors causing shutdown **Changing log levels:** ```bash # Set global log level cloudflow config set logging.level DEBUG # Set log level for specific component cloudflow config set logging.components.database DEBUG cloudflow config set logging.components.auth INFO cloudflow config set logging.components.workflows TRACE # Temporary log level increase (resets after 1 hour) cloudflow debug set-log-level DEBUG --duration 3600 # View current log configuration cloudflow config get logging --format json ``` ### Grep Patterns for Common Issues #### Authentication Failures ```bash # Find all authentication errors kubectl logs -n cloudflow deployment/cloudflow-api | \\ grep -i \"authentication\\|401\\|unauthorized\" | \\ tail -n 50 # Find JWT validation failures kubectl logs -n cloudflow deployment/cloudflow-api | \\ grep -E \"JWT|token.*invalid|signature.*failed\" # Find clock skew issues kubectl logs -n cloudflow deployment/cloudflow-api | \\ grep -i \"clock skew\\|time.*sync\\|nbf\\|exp\" ``` #### Database Errors ```bash # Find database connection errors kubectl logs -n cloudflow deployment/cloudflow-api | \\ grep -E \"connection.*pool|could not connect|database.*timeout\" # Find slow queries kubectl logs -n cloudflow deployment/cloudflow-api | \\ grep \"slow query\" | \\ awk '{print $NF}' | \\ sort -n | \\ tail -n 20 # Find deadlock errors kubectl logs -n cloudflow deployment/cloudflow-api | \\ grep -i \"deadlock detected\" ``` #### Workflow Execution Errors ```bash # Find workflow timeout errors kubectl logs -n cloudflow deployment/cloudflow-workflow-engine | \\ grep -E \"timeout|exceeded.*3600\" # Find workflow retry attempts kubectl logs -n cloudflow deployment/cloudflow-workflow-engine | \\ grep -E \"retry attempt [0-9]|retrying",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_12",
              "content": "Resume normal operations - Recovery time: 1-3 hours depending on data volume - Data loss: None if corruption detected quickly ### Testing & Validation **DR Drill Schedule**: - Monthly: Automated failover test (single AZ failure simulation) - Quarterly: Full DR region failover (non-production hours) - Annually: Complete disaster simulation with all stakeholders **Last DR Test Results** (Dec 15, 2025): - Scenario: Full region failover - Actual RTO: 2 hours 23 minutes (target: 4 hours) - Actual RPO: 42 minutes (target: 1 hour) - Issues identified: DNS propagation slower than expected (resolved) - Success criteria: Met all recovery objectives ### Business Continuity **Communication Plan**: - Status page: status.cloudflow.com (updated every 15 minutes during incident) - Customer notifications: Email + SMS for all P1 incidents - Internal escalation: PagerDuty \u2192 Incident Commander \u2192 Engineering Manager \u2192 CTO **Data Retention Policy**: - Active data: 12 months in hot storage (PostgreSQL) - Archived data: 7 years in cold storage (S3 Glacier) - Audit logs: 7 years (compliance requirement) - Backup retention: 30 days standard, 90 days monthly snapshots --- ## Appendix ### Service Dependency Matrix | Service | Depends On | Critical Path | |---------|-----------|---------------| | API Gateway | Auth Service, Redis | Yes | | Auth Service | PostgreSQL, Redis, Vault | Yes | | Workflow Engine | PostgreSQL, Kafka, Redis | Yes | | Scheduler | PostgreSQL, Redis, Kafka | No | | Notification Service | Kafka, PostgreSQL, SendGrid, Twilio | No | ### Capacity Planning **Current Capacity** (Jan 2026): - Daily workflow executions: 2.5M - Active users: 150,000 - API requests per day: 50M - Database size: 1.2TB - Kafka throughput: 100K msg/sec peak **6-Month Projection** (Jul 2026): - Daily workflow executions: 4M (+60%) - Active users: 225,000 (+50%) - API requests per day: 80M (+60%) - Database size: 2TB (+67%) - Required scaling: +30% compute, +50% storage ### Contact Information - **Architecture Team**: architecture@cloudflow.internal - **On-Call Engineer**: PagerDuty escalation - **Security Team**: security@cloudflow.internal - **Documentation**: https://wiki.cloudflow.internal/architecture --- **Document Revision History**: - v2.3.1 (Jan 15, 2026): Updated performance metrics, added DR test results - v2.3.0 (Dec 1, 2025): Added multi-AZ deployment details - v2.2.0 (Oct 15, 2025): Security architecture overhaul - v2.1.0 (Sep 1, 2025): Initial Kafka migration documentation - v2.0.0 (Jul 1, 2025): Complete microservices rewrite",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_5",
              "content": "exec_7h3j6k9m2n --show-bottlenecks ``` #### Solutions **1. Increase workflow timeout (if justified):** ```bash # Update workflow configuration cloudflow workflows update wf_9k2n4m8p1q \\ --timeout 7200 \\ --reason \"Large dataset processing requires extended time\" # Verify update cloudflow workflows get wf_9k2n4m8p1q | grep timeout ``` **2. Optimize slow steps:** ```bash # Enable parallel processing cloudflow workflows update wf_9k2n4m8p1q \\ --step data_transformation \\ --parallel-workers 8 \\ --batch-size 1000 # Add checkpointing for long operations cloudflow workflows update wf_9k2n4m8p1q \\ --step data_transformation \\ --enable-checkpointing \\ --checkpoint-interval 300 ``` **3. Split workflow into smaller workflows:** ```bash # Create sub-workflows cloudflow workflows create data-pipeline-part1 \\ --steps \"data_ingestion,data_validation\" \\ --timeout 1800 cloudflow workflows create data-pipeline-part2 \\ --steps \"data_transformation,data_export\" \\ --timeout 3600 \\ --trigger workflow_completed \\ --trigger-workflow data-pipeline-part1 ``` ### Retry Logic and Exponential Backoff CloudFlow implements automatic retry with exponential backoff for transient failures: - Max retries: 3 - Initial delay: 1 second - Backoff multiplier: 2 - Max delay: 60 seconds #### Retry Sequence ``` Attempt 1: Immediate Attempt 2: Wait 1s (2^0 * 1s) Attempt 3: Wait 2s (2^1 * 1s) Attempt 4: Wait 4s (2^2 * 1s) ``` #### Configuration ```bash # View current retry settings cloudflow workflows get wf_9k2n4m8p1q --format json | jq '.retry_policy' # Customize retry behavior cloudflow workflows update wf_9k2n4m8p1q \\ --retry-max-attempts 5 \\ --retry-initial-delay 2000 \\ --retry-backoff-multiplier 2 \\ --retry-max-delay 120000 \\ --retry-on-errors \"NETWORK_ERROR,TIMEOUT,RATE_LIMIT\" # Disable retry for specific step cloudflow workflows update wf_9k2n4m8p1q \\ --step data_export \\ --retry-enabled false ``` #### Monitoring Retries ```bash # List failed executions with retry information cloudflow workflows executions list \\ --status FAILED \\ --show-retries \\ --last 7d # Get retry history for specific execution cloudflow workflows executions retries exec_7h3j6k9m2n # Output: # Execution: exec_7h3j6k9m2n # Attempt 1: FAILED - NetworkError: Connection refused (delay: 0ms) # Attempt 2: FAILED - NetworkError: Connection timeout (delay: 1000ms) # Attempt 3: FAILED - NetworkError: Connection timeout (delay: 2000ms) # Attempt 4: FAILED - NetworkError: Connection timeout (delay: 4000ms) # Final Status: FAILED_AFTER_RETRIES ``` ### Workflow Step Failures #### Common Error Patterns **1.",
              "score": null
            }
          ],
          "latency_ms": 1564.7570920009457,
          "query_id": "realistic_013",
          "dimension": "synonym",
          "query": "How long are workflow definitions cached?"
        },
        {
          "key_facts": [
            "TTL: 1 hour",
            "Workflow Definitions: TTL: 1 hour"
          ],
          "found_facts": [],
          "missed_facts": [
            "TTL: 1 hour",
            "Workflow Definitions: TTL: 1 hour"
          ],
          "coverage": 0.0,
          "expected_docs": [
            "architecture_overview"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_5",
              "content": "exec_7h3j6k9m2n --show-bottlenecks ``` #### Solutions **1. Increase workflow timeout (if justified):** ```bash # Update workflow configuration cloudflow workflows update wf_9k2n4m8p1q \\ --timeout 7200 \\ --reason \"Large dataset processing requires extended time\" # Verify update cloudflow workflows get wf_9k2n4m8p1q | grep timeout ``` **2. Optimize slow steps:** ```bash # Enable parallel processing cloudflow workflows update wf_9k2n4m8p1q \\ --step data_transformation \\ --parallel-workers 8 \\ --batch-size 1000 # Add checkpointing for long operations cloudflow workflows update wf_9k2n4m8p1q \\ --step data_transformation \\ --enable-checkpointing \\ --checkpoint-interval 300 ``` **3. Split workflow into smaller workflows:** ```bash # Create sub-workflows cloudflow workflows create data-pipeline-part1 \\ --steps \"data_ingestion,data_validation\" \\ --timeout 1800 cloudflow workflows create data-pipeline-part2 \\ --steps \"data_transformation,data_export\" \\ --timeout 3600 \\ --trigger workflow_completed \\ --trigger-workflow data-pipeline-part1 ``` ### Retry Logic and Exponential Backoff CloudFlow implements automatic retry with exponential backoff for transient failures: - Max retries: 3 - Initial delay: 1 second - Backoff multiplier: 2 - Max delay: 60 seconds #### Retry Sequence ``` Attempt 1: Immediate Attempt 2: Wait 1s (2^0 * 1s) Attempt 3: Wait 2s (2^1 * 1s) Attempt 4: Wait 4s (2^2 * 1s) ``` #### Configuration ```bash # View current retry settings cloudflow workflows get wf_9k2n4m8p1q --format json | jq '.retry_policy' # Customize retry behavior cloudflow workflows update wf_9k2n4m8p1q \\ --retry-max-attempts 5 \\ --retry-initial-delay 2000 \\ --retry-backoff-multiplier 2 \\ --retry-max-delay 120000 \\ --retry-on-errors \"NETWORK_ERROR,TIMEOUT,RATE_LIMIT\" # Disable retry for specific step cloudflow workflows update wf_9k2n4m8p1q \\ --step data_export \\ --retry-enabled false ``` #### Monitoring Retries ```bash # List failed executions with retry information cloudflow workflows executions list \\ --status FAILED \\ --show-retries \\ --last 7d # Get retry history for specific execution cloudflow workflows executions retries exec_7h3j6k9m2n # Output: # Execution: exec_7h3j6k9m2n # Attempt 1: FAILED - NetworkError: Connection refused (delay: 0ms) # Attempt 2: FAILED - NetworkError: Connection timeout (delay: 1000ms) # Attempt 3: FAILED - NetworkError: Connection timeout (delay: 2000ms) # Attempt 4: FAILED - NetworkError: Connection timeout (delay: 4000ms) # Final Status: FAILED_AFTER_RETRIES ``` ### Workflow Step Failures #### Common Error Patterns **1.",
              "score": null
            },
            {
              "doc_id": "user_guide",
              "chunk_id": "user_guide_fix_6",
              "content": "Filter by error type, date range, or execution ID 3. Click an execution to view full details 4. Click **\"Retry\"** to reprocess with the same input data ### Error Notifications Get notified when workflows fail: ```yaml workflow: notifications: on_failure: - type: email to: \"ops-team@company.com\" - type: slack channel: \"#alerts\" message: \"Workflow {{workflow.name}} failed: {{error.message}}\" on_success_after_retry: - type: slack channel: \"#monitoring\" message: \"Workflow recovered after {{error.attempts}} attempts\" ``` ## Workflow Limits CloudFlow enforces the following limits to ensure platform stability and performance: ### Steps Per Workflow - **Maximum**: 50 steps per workflow - **Recommendation**: Keep workflows focused and modular. If you need more steps, consider splitting into multiple workflows connected via webhooks. ### Execution Timeout - **Default**: 3600 seconds (60 minutes) - **Behavior**: Workflows exceeding this timeout are automatically terminated - **Custom Timeouts**: Enterprise plans can request custom timeout limits **Setting Step-Level Timeouts:** ```yaml - id: long_running_task action: http_request config: url: \"https://api.example.com/process\" timeout: 300 # 5 minutes for this specific step ``` ### Execution Limits - **Maximum**: 1000 executions per day (per workflow) - **Rate Limiting**: 100 concurrent executions per workflow - **Burst Limit**: 10 executions per second **What happens when limits are reached:** - New executions are queued automatically - Webhook triggers return HTTP 429 (Too Many Requests) - Scheduled executions are skipped (logged in audit trail) - Email notifications sent to workflow owner **Monitoring Usage:** View real-time metrics in your workflow dashboard: - Executions today: 847 / 1000 - Average duration: 12.3s - Success rate: 98.2% - Current queue depth: 3 ### Data Limits - **Maximum request/response size**: 10MB per action - **Maximum execution payload**: 50MB total - **Variable value size**: 1MB per variable ### Enterprise Plan Limits Enterprise customers can request increased limits: - Up to 100 steps per workflow - Up to 10,000 executions per day - Up to 7200 second timeout (2 hours) - Priority execution queue - Dedicated capacity allocation Contact sales@cloudflow.io for Enterprise pricing and custom limits. ## Best Practices Follow these best practices to build reliable, maintainable workflows: ### 1. Use Descriptive Names **Good:** - Workflow: \"Sync Customer Data from Salesforce to Database\" - Step: \"validate_customer_email\" **Bad:** - Workflow: \"Workflow 1\" - Step: \"step3\" ### 2.",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_12",
              "content": "Resume normal operations - Recovery time: 1-3 hours depending on data volume - Data loss: None if corruption detected quickly ### Testing & Validation **DR Drill Schedule**: - Monthly: Automated failover test (single AZ failure simulation) - Quarterly: Full DR region failover (non-production hours) - Annually: Complete disaster simulation with all stakeholders **Last DR Test Results** (Dec 15, 2025): - Scenario: Full region failover - Actual RTO: 2 hours 23 minutes (target: 4 hours) - Actual RPO: 42 minutes (target: 1 hour) - Issues identified: DNS propagation slower than expected (resolved) - Success criteria: Met all recovery objectives ### Business Continuity **Communication Plan**: - Status page: status.cloudflow.com (updated every 15 minutes during incident) - Customer notifications: Email + SMS for all P1 incidents - Internal escalation: PagerDuty \u2192 Incident Commander \u2192 Engineering Manager \u2192 CTO **Data Retention Policy**: - Active data: 12 months in hot storage (PostgreSQL) - Archived data: 7 years in cold storage (S3 Glacier) - Audit logs: 7 years (compliance requirement) - Backup retention: 30 days standard, 90 days monthly snapshots --- ## Appendix ### Service Dependency Matrix | Service | Depends On | Critical Path | |---------|-----------|---------------| | API Gateway | Auth Service, Redis | Yes | | Auth Service | PostgreSQL, Redis, Vault | Yes | | Workflow Engine | PostgreSQL, Kafka, Redis | Yes | | Scheduler | PostgreSQL, Redis, Kafka | No | | Notification Service | Kafka, PostgreSQL, SendGrid, Twilio | No | ### Capacity Planning **Current Capacity** (Jan 2026): - Daily workflow executions: 2.5M - Active users: 150,000 - API requests per day: 50M - Database size: 1.2TB - Kafka throughput: 100K msg/sec peak **6-Month Projection** (Jul 2026): - Daily workflow executions: 4M (+60%) - Active users: 225,000 (+50%) - API requests per day: 80M (+60%) - Database size: 2TB (+67%) - Required scaling: +30% compute, +50% storage ### Contact Information - **Architecture Team**: architecture@cloudflow.internal - **On-Call Engineer**: PagerDuty escalation - **Security Team**: security@cloudflow.internal - **Documentation**: https://wiki.cloudflow.internal/architecture --- **Document Revision History**: - v2.3.1 (Jan 15, 2026): Updated performance metrics, added DR test results - v2.3.0 (Dec 1, 2025): Added multi-AZ deployment details - v2.2.0 (Oct 15, 2025): Security architecture overhaul - v2.1.0 (Sep 1, 2025): Initial Kafka migration documentation - v2.0.0 (Jul 1, 2025): Complete microservices rewrite",
              "score": null
            },
            {
              "doc_id": "user_guide",
              "chunk_id": "user_guide_fix_0",
              "content": "# CloudFlow User Guide Welcome to CloudFlow, the modern workflow automation platform that helps you connect your apps, automate repetitive tasks, and build powerful integrations without writing code. ## Table of Contents 1. [Getting Started](#getting-started) 2. [Workflow Creation](#workflow-creation) 3. [Available Actions](#available-actions) 4. [Variables and Expressions](#variables-and-expressions) 5. [Scheduling](#scheduling) 6. [Error Handling](#error-handling) 7. [Workflow Limits](#workflow-limits) 8. [Best Practices](#best-practices) 9. [Common Workflow Patterns](#common-workflow-patterns) ## Getting Started ### Account Setup Creating your CloudFlow account takes just a few minutes: 1. Navigate to https://app.cloudflow.io/signup 2. Enter your email address and create a strong password 3. Verify your email address by clicking the link sent to your inbox 4. Complete your profile by providing your organization name and use case 5. Choose your plan (Free, Professional, or Enterprise) Once your account is created, you'll be redirected to your CloudFlow dashboard where you can start building workflows immediately. ### Your First Workflow Let's create a simple workflow that sends you a Slack notification when a new file is uploaded to your Google Drive: 1. Click the **\"Create Workflow\"** button in the top-right corner 2. Give your workflow a name: \"Google Drive to Slack Notifier\" 3. Select **\"Google Drive - New File\"** as your trigger 4. Authenticate your Google Drive account when prompted 5. Choose the folder you want to monitor 6. Click **\"Add Action\"** and select **\"Slack - Send Message\"** 7. Authenticate your Slack account 8. Select the channel where you want to receive notifications 9. Customize your message using variables: `New file uploaded: {{trigger.file.name}}` 10. Click **\"Save & Activate\"** to enable your workflow Congratulations! Your first workflow is now live. Every time a file is added to the specified Google Drive folder, you'll receive a Slack notification. ## Workflow Creation CloudFlow offers multiple ways to create and manage workflows, giving you the flexibility to work in the way that suits you best. ### Visual Editor The Visual Editor is CloudFlow's drag-and-drop interface for building workflows without code. It's perfect for users who prefer a graphical approach: **Key Features:** - Drag-and-drop action blocks from the sidebar - Visual connections between steps show your workflow logic - Inline configuration for each action - Real-time validation and error checking - Test mode to run your workflow with sample data **Using the Visual Editor:** 1.",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_10",
              "content": "- Execution start rate: 500 per second - Completion rate: 450 per second (average 2-second execution time) **Database**: - Read throughput: 50,000 queries per second (across replicas) - Write throughput: 15,000 transactions per second - Connection capacity: 2,000 concurrent connections **Kafka**: - Message ingestion: 100,000 messages per second - Consumer throughput: 80,000 messages per second (aggregated) - End-to-end latency: < 100ms (P99) ### Resource Utilization **CPU Utilization** (Target: 60-70% average): - API Gateway: 55% average, 80% peak - Workflow Engine: 65% average, 85% peak - Auth Service: 40% average, 60% peak **Memory Utilization** (Target: < 80%): - API Gateway: 2.5GB average per pod (4GB allocated) - Workflow Engine: 6GB average per pod (8GB allocated) - Notification Service: 2.8GB average per pod (4GB allocated) **Network Throughput**: - Ingress: 2 Gbps average, 5 Gbps peak - Egress: 1.5 Gbps average, 4 Gbps peak - Internal (service mesh): 8 Gbps average ### Monitoring & Alerting **Key Metrics** (Monitored via Prometheus): - Request rate (per service, per endpoint) - Error rate (4xx, 5xx responses) - Latency percentiles (P50, P95, P99) - Resource utilization (CPU, memory, disk) - Cache hit/miss ratios - Database connection pool saturation - Kafka consumer lag **Alerts** (via PagerDuty): - P1 (Immediate): API error rate > 1%, database connection failure - P2 (< 15 min): P99 latency > 500ms, cache hit rate < 80% - P3 (< 1 hour): Resource utilization > 85%, replica count < minimum **SLI/SLO/SLA**: - SLI: API success rate (non-5xx responses) - SLO: 99.9% uptime per month (43 minutes downtime allowance) - SLA: 99.5% uptime guarantee (customer-facing SLA with credits) --- ## Disaster Recovery ### Recovery Objectives **RPO (Recovery Point Objective): 1 hour** - Maximum acceptable data loss: 1 hour of transactions - Achieved through: Continuous database replication + hourly snapshots - Kafka retention: 7 days allows event replay **RTO (Recovery Time Objective): 4 hours** - Maximum acceptable downtime: 4 hours for full system recovery - Includes: Failover, data verification, and service restoration - Automated runbooks reduce RTO to < 2 hours for common scenarios ### High Availability Architecture **Multi-AZ Deployment**: ``` Region: us-east-1 AZ-1a: AZ-1b: AZ-1c: - API Gateway (4) - API Gateway (4) - API Gateway (4) - Workflow Engine (6) - Workflow Engine (5) - Workflow Engine (5) - Auth Service (3) - Auth Service (3) - Auth",
              "score": null
            }
          ],
          "latency_ms": 890.9116020004149,
          "query_id": "realistic_013",
          "dimension": "problem",
          "query": "My workflow updates aren't reflecting immediately"
        },
        {
          "key_facts": [
            "TTL: 1 hour",
            "Workflow Definitions: TTL: 1 hour"
          ],
          "found_facts": [],
          "missed_facts": [
            "TTL: 1 hour",
            "Workflow Definitions: TTL: 1 hour"
          ],
          "coverage": 0.0,
          "expected_docs": [
            "architecture_overview"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_5",
              "content": "exec_7h3j6k9m2n --show-bottlenecks ``` #### Solutions **1. Increase workflow timeout (if justified):** ```bash # Update workflow configuration cloudflow workflows update wf_9k2n4m8p1q \\ --timeout 7200 \\ --reason \"Large dataset processing requires extended time\" # Verify update cloudflow workflows get wf_9k2n4m8p1q | grep timeout ``` **2. Optimize slow steps:** ```bash # Enable parallel processing cloudflow workflows update wf_9k2n4m8p1q \\ --step data_transformation \\ --parallel-workers 8 \\ --batch-size 1000 # Add checkpointing for long operations cloudflow workflows update wf_9k2n4m8p1q \\ --step data_transformation \\ --enable-checkpointing \\ --checkpoint-interval 300 ``` **3. Split workflow into smaller workflows:** ```bash # Create sub-workflows cloudflow workflows create data-pipeline-part1 \\ --steps \"data_ingestion,data_validation\" \\ --timeout 1800 cloudflow workflows create data-pipeline-part2 \\ --steps \"data_transformation,data_export\" \\ --timeout 3600 \\ --trigger workflow_completed \\ --trigger-workflow data-pipeline-part1 ``` ### Retry Logic and Exponential Backoff CloudFlow implements automatic retry with exponential backoff for transient failures: - Max retries: 3 - Initial delay: 1 second - Backoff multiplier: 2 - Max delay: 60 seconds #### Retry Sequence ``` Attempt 1: Immediate Attempt 2: Wait 1s (2^0 * 1s) Attempt 3: Wait 2s (2^1 * 1s) Attempt 4: Wait 4s (2^2 * 1s) ``` #### Configuration ```bash # View current retry settings cloudflow workflows get wf_9k2n4m8p1q --format json | jq '.retry_policy' # Customize retry behavior cloudflow workflows update wf_9k2n4m8p1q \\ --retry-max-attempts 5 \\ --retry-initial-delay 2000 \\ --retry-backoff-multiplier 2 \\ --retry-max-delay 120000 \\ --retry-on-errors \"NETWORK_ERROR,TIMEOUT,RATE_LIMIT\" # Disable retry for specific step cloudflow workflows update wf_9k2n4m8p1q \\ --step data_export \\ --retry-enabled false ``` #### Monitoring Retries ```bash # List failed executions with retry information cloudflow workflows executions list \\ --status FAILED \\ --show-retries \\ --last 7d # Get retry history for specific execution cloudflow workflows executions retries exec_7h3j6k9m2n # Output: # Execution: exec_7h3j6k9m2n # Attempt 1: FAILED - NetworkError: Connection refused (delay: 0ms) # Attempt 2: FAILED - NetworkError: Connection timeout (delay: 1000ms) # Attempt 3: FAILED - NetworkError: Connection timeout (delay: 2000ms) # Attempt 4: FAILED - NetworkError: Connection timeout (delay: 4000ms) # Final Status: FAILED_AFTER_RETRIES ``` ### Workflow Step Failures #### Common Error Patterns **1.",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_8",
              "content": "Leverage caching:** ```bash # Enable client-side caching export CLOUDFLOW_CACHE_ENABLED=true export CLOUDFLOW_CACHE_TTL=300 # Cache workflow metadata cloudflow workflows list --use-cache --cache-ttl 600 ``` --- ## Log Analysis & Debugging ### Accessing CloudFlow Logs #### Kubernetes Deployments ```bash # List all CloudFlow pods kubectl get pods -n cloudflow # Tail logs from API server kubectl logs -f -n cloudflow deployment/cloudflow-api --tail=100 # Get logs from specific pod kubectl logs -n cloudflow cloudflow-api-7d4f6b8c9d-x7k2m # Get logs from previous crashed pod kubectl logs -n cloudflow cloudflow-api-7d4f6b8c9d-x7k2m --previous # Get logs from all pods in deployment kubectl logs -n cloudflow deployment/cloudflow-api --all-containers=true # Stream logs from multiple pods kubectl logs -n cloudflow -l app=cloudflow-api -f --max-log-requests=10 ``` #### Log Levels CloudFlow supports the following log levels: - `TRACE` - Very detailed debugging information - `DEBUG` - Detailed debugging information - `INFO` - Informational messages (default) - `WARN` - Warning messages - `ERROR` - Error messages - `FATAL` - Fatal errors causing shutdown **Changing log levels:** ```bash # Set global log level cloudflow config set logging.level DEBUG # Set log level for specific component cloudflow config set logging.components.database DEBUG cloudflow config set logging.components.auth INFO cloudflow config set logging.components.workflows TRACE # Temporary log level increase (resets after 1 hour) cloudflow debug set-log-level DEBUG --duration 3600 # View current log configuration cloudflow config get logging --format json ``` ### Grep Patterns for Common Issues #### Authentication Failures ```bash # Find all authentication errors kubectl logs -n cloudflow deployment/cloudflow-api | \\ grep -i \"authentication\\|401\\|unauthorized\" | \\ tail -n 50 # Find JWT validation failures kubectl logs -n cloudflow deployment/cloudflow-api | \\ grep -E \"JWT|token.*invalid|signature.*failed\" # Find clock skew issues kubectl logs -n cloudflow deployment/cloudflow-api | \\ grep -i \"clock skew\\|time.*sync\\|nbf\\|exp\" ``` #### Database Errors ```bash # Find database connection errors kubectl logs -n cloudflow deployment/cloudflow-api | \\ grep -E \"connection.*pool|could not connect|database.*timeout\" # Find slow queries kubectl logs -n cloudflow deployment/cloudflow-api | \\ grep \"slow query\" | \\ awk '{print $NF}' | \\ sort -n | \\ tail -n 20 # Find deadlock errors kubectl logs -n cloudflow deployment/cloudflow-api | \\ grep -i \"deadlock detected\" ``` #### Workflow Execution Errors ```bash # Find workflow timeout errors kubectl logs -n cloudflow deployment/cloudflow-workflow-engine | \\ grep -E \"timeout|exceeded.*3600\" # Find workflow retry attempts kubectl logs -n cloudflow deployment/cloudflow-workflow-engine | \\ grep -E \"retry attempt [0-9]|retrying",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_3",
              "content": "Workflow Context Accumulation** Large workflow executions may accumulate state in memory. **Solution:** ```bash # Configure context cleanup cloudflow config set workflow.context.max_size_mb 100 cloudflow config set workflow.context.cleanup_threshold 0.8 # Enable context persistence to disk cloudflow config set workflow.context.persistence.enabled true cloudflow config set workflow.context.persistence.backend s3 ``` **2. Connection Pool Leaks** **Diagnosis:** ```bash # Check active connections cloudflow db connections --verbose # Expected output: # Active: 45/100 # Idle: 23 # Waiting: 2 # Average age: 245s ``` **Resolution:** ```bash # Adjust connection pool settings cloudflow config set db.pool.max_connections 100 cloudflow config set db.pool.min_connections 10 cloudflow config set db.pool.idle_timeout 300 cloudflow config set db.pool.max_lifetime 1800 # Force connection pool reset cloudflow db pool reset --confirm ``` **3. Event Stream Buffers** Unbounded event buffers can cause memory exhaustion. ```bash # Configure event buffer limits cloudflow config set events.buffer.max_size 10000 cloudflow config set events.buffer.overflow_strategy drop_oldest # Enable event streaming to external sink cloudflow config set events.sink.type kafka cloudflow config set events.sink.kafka.brokers \"kafka-1:9092,kafka-2:9092\" cloudflow config set events.sink.kafka.topic cloudflow-events ``` --- ## Database Connection Issues ### Connection Pool Exhaustion #### Symptoms - Error: `could not obtain connection from pool within 5000ms` - Error: `connection pool exhausted (100/100 connections in use)` - API requests fail with `503 Service Unavailable` - Database CPU usage normal, but connection count at maximum #### Investigation ```bash # Check current connection pool status cloudflow db pool status --detailed # Output example: # Pool Statistics: # Total Connections: 100/100 (100%) # Active: 87 # Idle: 13 # Waiting Requests: 45 # Average Wait Time: 3420ms # Max Wait Time: 8234ms # Identify long-running queries cloudflow db queries --status running --min-duration 30000 # Check connection distribution by client SELECT application_name, state, COUNT(*) as conn_count, AVG(EXTRACT(EPOCH FROM (NOW() - state_change))) as avg_duration_sec FROM pg_stat_activity WHERE datname = 'cloudflow_production' GROUP BY application_name, state ORDER BY conn_count DESC; ``` #### Resolution **Immediate mitigation:** ```bash # Temporarily increase connection limit (requires database restart) cloudflow db config set max_connections 150 # Kill idle connections older than 5 minutes cloudflow db connections kill --idle-timeout 300 # Restart connection pool without downtime kubectl rollout restart deployment/cloudflow-api -n cloudflow kubectl rollout status deployment/cloudflow-api -n cloudflow ``` **Long-term solutions:** 1.",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_7",
              "content": "records: 500 - Session timeout: 30 seconds --- ## Message Queue Patterns ### Pub/Sub Pattern Used for broadcasting events to multiple interested consumers: ``` Workflow Engine publishes workflow.execution.completed \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u25bc \u25bc \u25bc \u25bc Analytics Notification Audit Logger Billing Service Service Service Service ``` Each service maintains its own consumer group and processes events independently. Failures in one consumer don't affect others. ### Request-Reply Pattern Synchronous communication over async messaging (used sparingly): ``` API Gateway publishes request \u2192 Kafka (reply-to: temp-queue-123) \u2502 \u25bc Service processes request \u2502 \u25bc Service publishes response \u2192 temp-queue-123 \u2502 \u25bc API Gateway receives response ``` Timeout: 5 seconds, fallback to direct HTTP call if no response. ### Saga Pattern Distributed transaction management for multi-service workflows: ``` Step 1: Create Order \u2192 SUCCESS \u2192 Step 2: Reserve Inventory \u2502 FAILURE \u2502 \u25bc Compensating Transaction \u2502 \u25bc Cancel Order (Rollback) ``` Orchestrated by Workflow Engine with compensation logic defined in workflow DSL. ### Dead Letter Queue (DLQ) Failed messages after all retry attempts are routed to DLQ: ``` Message processing fails (3 retries with exponential backoff) \u2502 \u25bc Publish to dead-letter-queue topic \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u25bc \u25bc Alert to PagerDuty Store in PostgreSQL for analysis \u2502 \u2502 \u25bc \u25bc Manual investigation Automated pattern detection ``` DLQ Processing SLA: < 4 hours for critical events, < 24 hours for non-critical. --- ## Caching Strategy ### Cache-Aside Pattern (Lazy Loading) Primary caching pattern used across all services: ``` 1. Application checks cache (Redis GET) 2. If HIT \u2192 Return cached data 3. If MISS: a. Query database (PostgreSQL) b. Store result in cache with TTL c. Return data to application ``` **Implementation Example** (Workflow Definition): ```javascript async function getWorkflowDefinition(workflowId) { const cacheKey = `workflow:def:${workflowId}`; // Step 1: Check cache let workflow = await redis.get(cacheKey); if (workflow) { metrics.increment('cache.hit.workflow_def'); return JSON.parse(workflow); } // Step 2: Cache miss - query database metrics.increment('cache.miss.workflow_def'); workflow = await db.query( 'SELECT * FROM workflows WHERE id = $1', [workflowId] ); // Step 3: Store in cache (1 hour TTL) await redis.setex(cacheKey, 3600, JSON.stringify(workflow)); return workflow; } ``` ### Write-Through Pattern Used for critical data where cache consistency is paramount: ``` 1. Application writes to database 2. Database transaction commits 3. Application updates cache 4. Return success to client ``` Applied to: User profiles, authentication sessions, system configuration.",
              "score": null
            },
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_6",
              "content": "**Verify Service Health**: ```bash kubectl get pods -n cloudflow-prod curl https://api.cloudflow.io/health ``` **Recovery Time Objective (RTO)**: 4 hours **Recovery Point Objective (RPO)**: 24 hours --- ## Scaling and Performance ### Horizontal Pod Autoscaling CloudFlow is configured with Horizontal Pod Autoscaler (HPA) to automatically scale based on resource utilization: ```yaml # hpa.yaml apiVersion: autoscaling/v2 kind: HorizontalPodAutoscaler metadata: name: cloudflow-api-hpa namespace: cloudflow-prod spec: scaleTargetRef: apiVersion: apps/v1 kind: Deployment name: cloudflow-api minReplicas: 3 maxReplicas: 10 metrics: - type: Resource resource: name: cpu target: type: Utilization averageUtilization: 70 - type: Resource resource: name: memory target: type: Utilization averageUtilization: 80 behavior: scaleDown: stabilizationWindowSeconds: 300 policies: - type: Percent value: 50 periodSeconds: 60 scaleUp: stabilizationWindowSeconds: 60 policies: - type: Percent value: 100 periodSeconds: 30 - type: Pods value: 2 periodSeconds: 30 selectPolicy: Max ``` Monitor autoscaling events: ```bash kubectl get hpa -n cloudflow-prod -w kubectl describe hpa cloudflow-api-hpa -n cloudflow-prod ``` ### Performance Optimization #### Connection Pooling Ensure optimal database connection pooling settings in PgBouncer: - **Pool Mode**: Transaction (optimal for microservices) - **Default Pool Size**: 25 connections per user - **Max DB Connections**: 100 (matches PostgreSQL `max_connections`) #### Redis Caching Implement Redis caching for frequently accessed data: ```javascript // Example caching strategy const CACHE_TTL = { workflows: 300, // 5 minutes userSessions: 3600, // 1 hour apiResults: 60 // 1 minute }; ``` #### Load Testing Perform regular load testing to validate scaling configuration: ```bash # Install k6 for load testing kubectl apply -f - <<EOF apiVersion: v1 kind: ConfigMap metadata: name: loadtest-script namespace: cloudflow-prod data: script.js: | import http from 'k6/http'; import { check, sleep } from 'k6'; export let options = { stages: [ { duration: '2m', target: 100 }, { duration: '5m', target: 100 }, { duration: '2m', target: 200 }, { duration: '5m', target: 200 }, { duration: '2m', target: 0 }, ], thresholds: { http_req_duration: ['p(95)<500'], }, }; export default function () { let res = http.get('https://api.cloudflow.io/api/workflows'); check(res, { 'status is 200': (r) => r.status === 200 }); sleep(1); } EOF ``` **Performance Targets**: - P95 API latency: < 500ms - P99 API latency: < 1000ms - Error rate: < 0.1% - Throughput: > 1000 requests/second --- ## Security and Compliance ### Network Policies Implement network policies to restrict pod-to-pod communication: ```yaml # network-policy.yaml apiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: cloudflow-network-policy namespace: cloudflow-prod spec:",
              "score": null
            }
          ],
          "latency_ms": 905.5676739990304,
          "query_id": "realistic_013",
          "dimension": "casual",
          "query": "cache ttl workflows"
        },
        {
          "key_facts": [
            "TTL: 1 hour",
            "Workflow Definitions: TTL: 1 hour"
          ],
          "found_facts": [],
          "missed_facts": [
            "TTL: 1 hour",
            "Workflow Definitions: TTL: 1 hour"
          ],
          "coverage": 0.0,
          "expected_docs": [
            "architecture_overview"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_8",
              "content": "### Cache Invalidation Strategies **Time-Based Expiration (TTL)**: - Short-lived data: 5-15 minutes (session tokens, rate limit counters) - Medium-lived data: 1 hour (workflow definitions, templates) - Long-lived data: 24 hours (static configuration) **Event-Based Invalidation**: ``` Database Update Event \u2192 Kafka (cache.invalidation topic) \u2502 \u25bc All service instances consume event \u2502 \u25bc Redis DEL for affected keys ``` **Pattern-Based Invalidation**: - Use Redis SCAN + DEL for wildcard patterns - Example: Invalidate all user caches: `user:*:{user_id}` **Cache Warming**: - Scheduled job runs every hour to pre-populate frequently accessed data - Targets: Top 1000 workflows, system configuration, active user profiles - Reduces cache miss rate during traffic spikes ### Multi-Level Caching Application implements L1 (in-memory) and L2 (Redis) caching: ``` Request \u2192 L1 Cache (Node.js Map, 1000 entry LRU) \u2502 MISS \u2502 \u25bc L2 Cache (Redis, distributed) \u2502 MISS \u2502 \u25bc Database (PostgreSQL) ``` L1 cache reduces Redis network calls by 40% for hot data. --- ## Security Architecture ### Network Security **Zero-Trust Network Model**: - All service-to-service communication encrypted with mTLS - Certificate rotation: Every 90 days (automated via cert-manager) - Certificate authority: Internal PKI with HashiCorp Vault - Network segmentation: Private subnets for services, public subnet for ALB only **Service Mesh (Istio)**: ``` Service A \u2192 Envoy Sidecar (mTLS client cert) \u2502 Encrypted channel \u2502 Envoy Sidecar (mTLS server cert) \u2192 Service B ``` **Firewall Rules**: - Security groups: Deny all by default, explicit allow rules - Ingress: Only ALB can reach API Gateway (port 8080) - Egress: Services can only reach specific dependencies - Database access: Limited to application subnets only ### Authentication & Authorization **JWT Token Validation**: - Algorithm: RS256 (asymmetric signing) - Key rotation: Every 30 days with 7-day overlap period - Public key distribution: JWKS endpoint cached in Redis - Validation: Signature, expiry, issuer, audience claims - Token revocation: Blacklist in Redis for compromised tokens **Permission Model**: ``` User \u2192 Roles \u2192 Permissions \u2198 \u2197 Tenants (Multi-tenancy isolation) ``` Example permissions: - `workflow:read` - View workflows - `workflow:write` - Create/update workflows - `workflow:execute` - Trigger workflow execution - `workflow:delete` - Delete workflows - `admin:*` - All administrative operations **API Key Management**: - Format: `cfk_live_<32-char-random>` (production), `cfk_test_<32-char-random>` (sandbox) - Hashing: SHA-256 before storage in PostgreSQL - Scoping: API keys can be scoped to specific workflows or operations - Rate limits: Configurable",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_5",
              "content": "exec_7h3j6k9m2n --show-bottlenecks ``` #### Solutions **1. Increase workflow timeout (if justified):** ```bash # Update workflow configuration cloudflow workflows update wf_9k2n4m8p1q \\ --timeout 7200 \\ --reason \"Large dataset processing requires extended time\" # Verify update cloudflow workflows get wf_9k2n4m8p1q | grep timeout ``` **2. Optimize slow steps:** ```bash # Enable parallel processing cloudflow workflows update wf_9k2n4m8p1q \\ --step data_transformation \\ --parallel-workers 8 \\ --batch-size 1000 # Add checkpointing for long operations cloudflow workflows update wf_9k2n4m8p1q \\ --step data_transformation \\ --enable-checkpointing \\ --checkpoint-interval 300 ``` **3. Split workflow into smaller workflows:** ```bash # Create sub-workflows cloudflow workflows create data-pipeline-part1 \\ --steps \"data_ingestion,data_validation\" \\ --timeout 1800 cloudflow workflows create data-pipeline-part2 \\ --steps \"data_transformation,data_export\" \\ --timeout 3600 \\ --trigger workflow_completed \\ --trigger-workflow data-pipeline-part1 ``` ### Retry Logic and Exponential Backoff CloudFlow implements automatic retry with exponential backoff for transient failures: - Max retries: 3 - Initial delay: 1 second - Backoff multiplier: 2 - Max delay: 60 seconds #### Retry Sequence ``` Attempt 1: Immediate Attempt 2: Wait 1s (2^0 * 1s) Attempt 3: Wait 2s (2^1 * 1s) Attempt 4: Wait 4s (2^2 * 1s) ``` #### Configuration ```bash # View current retry settings cloudflow workflows get wf_9k2n4m8p1q --format json | jq '.retry_policy' # Customize retry behavior cloudflow workflows update wf_9k2n4m8p1q \\ --retry-max-attempts 5 \\ --retry-initial-delay 2000 \\ --retry-backoff-multiplier 2 \\ --retry-max-delay 120000 \\ --retry-on-errors \"NETWORK_ERROR,TIMEOUT,RATE_LIMIT\" # Disable retry for specific step cloudflow workflows update wf_9k2n4m8p1q \\ --step data_export \\ --retry-enabled false ``` #### Monitoring Retries ```bash # List failed executions with retry information cloudflow workflows executions list \\ --status FAILED \\ --show-retries \\ --last 7d # Get retry history for specific execution cloudflow workflows executions retries exec_7h3j6k9m2n # Output: # Execution: exec_7h3j6k9m2n # Attempt 1: FAILED - NetworkError: Connection refused (delay: 0ms) # Attempt 2: FAILED - NetworkError: Connection timeout (delay: 1000ms) # Attempt 3: FAILED - NetworkError: Connection timeout (delay: 2000ms) # Attempt 4: FAILED - NetworkError: Connection timeout (delay: 4000ms) # Final Status: FAILED_AFTER_RETRIES ``` ### Workflow Step Failures #### Common Error Patterns **1.",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_2",
              "content": "Review Query Execution Plans** ```sql -- Connect to CloudFlow database cloudflow db connect --readonly -- Explain slow query EXPLAIN ANALYZE SELECT w.*, e.status, e.error_message FROM workflows w LEFT JOIN executions e ON w.id = e.workflow_id WHERE w.workspace_id = 'ws_abc123' AND e.created_at > NOW() - INTERVAL '7 days' ORDER BY e.created_at DESC; -- Check for missing indexes SELECT schemaname, tablename, indexname, indexdef FROM pg_indexes WHERE tablename IN ('workflows', 'executions', 'workflow_steps') ORDER BY tablename, indexname; ``` **3. Optimize Queries** Common optimization techniques: **Add missing indexes:** ```sql -- Index for workflow lookup by workspace CREATE INDEX CONCURRENTLY idx_workflows_workspace_created ON workflows(workspace_id, created_at DESC); -- Index for execution status queries CREATE INDEX CONCURRENTLY idx_executions_status_created ON executions(workflow_id, status, created_at DESC) WHERE status IN ('running', 'pending'); -- Composite index for common filter combinations CREATE INDEX CONCURRENTLY idx_executions_workspace_status ON executions(workspace_id, status, created_at) INCLUDE (error_message, retry_count); ``` **Use query result caching:** ```bash # Enable query cache for workspace metadata cloudflow config set cache.workspace.ttl 3600 # Configure Redis cache backend cloudflow config set cache.backend redis cloudflow config set cache.redis.host redis.cloudflow.svc.cluster.local cloudflow config set cache.redis.port 6379 ``` ### High API Latency #### Latency Breakdown Analysis ```bash # Generate latency report cloudflow metrics latency-report --endpoint \"/api/v1/workflows\" --last 24h # Sample output: # Endpoint: POST /api/v1/workflows/execute # P50: 245ms | P95: 1823ms | P99: 4521ms # Breakdown: # - Auth: 45ms (18%) # - DB Query: 156ms (64%) # - Business Logic: 32ms (13%) # - Response Serialization: 12ms (5%) ``` **Network latency issues:** ```bash # Test connectivity to CloudFlow API time curl -w \"@curl-format.txt\" -o /dev/null -s https://api.cloudflow.io/health # Create curl-format.txt: cat > curl-format.txt << EOF time_namelookup: %{time_namelookup}s\\n time_connect: %{time_connect}s\\n time_appconnect: %{time_appconnect}s\\n time_pretransfer: %{time_pretransfer}s\\n time_redirect: %{time_redirect}s\\n time_starttransfer: %{time_starttransfer}s\\n ----------\\n time_total: %{time_total}s\\n EOF # Trace route to API endpoint traceroute api.cloudflow.io # Check DNS resolution time dig api.cloudflow.io | grep \"Query time\" ``` ### Memory Leaks #### Detection ```bash # Monitor CloudFlow service memory usage kubectl top pods -n cloudflow --sort-by=memory # Get detailed memory metrics for specific pod kubectl exec -n cloudflow deploy/cloudflow-api -- \\ curl localhost:9090/metrics | grep memory # Check for OOMKilled pods kubectl get pods -n cloudflow --field-selector=status.phase=Failed | \\ grep OOMKilled # Review memory limits and requests kubectl describe deployment cloudflow-api -n cloudflow | \\ grep -A 5 \"Limits\\|Requests\" ``` #### Common Causes **1.",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_0",
              "content": "# CloudFlow Platform Troubleshooting Guide **Version:** 3.2.1 **Last Updated:** January 2026 **Audience:** Platform Engineers, DevOps, Support Teams ## Table of Contents 1. [Overview](#overview) 2. [Authentication & Authorization Issues](#authentication--authorization-issues) 3. [Performance Problems](#performance-problems) 4. [Database Connection Issues](#database-connection-issues) 5. [Workflow Execution Failures](#workflow-execution-failures) 6. [Rate Limiting & Throttling](#rate-limiting--throttling) 7. [Log Analysis & Debugging](#log-analysis--debugging) 8. [Escalation Procedures](#escalation-procedures) --- ## Overview This guide provides comprehensive troubleshooting steps for common CloudFlow platform issues encountered in production environments. Each section includes error symptoms, root cause analysis, resolution steps, and preventive measures. ### Quick Diagnostic Checklist Before diving into specific issues, perform these initial checks: - Verify service health: `cloudflow status --all` - Check API connectivity: `curl -I https://api.cloudflow.io/health` - Review recent deployments: `kubectl get deployments -n cloudflow --sort-by=.metadata.creationTimestamp` - Inspect platform metrics: `cloudflow metrics --last 1h` --- ## Authentication & Authorization Issues ### 401 Unauthorized Errors #### Symptoms - API requests return `401 Unauthorized` - Error message: `Authentication credentials were not provided or are invalid` - Frontend displays \"Session expired\" message #### Common Causes **1. Token Expiration** CloudFlow access tokens expire after 3600 seconds (1 hour) by default. Refresh tokens are valid for 30 days. **Verification:** ```bash # Decode JWT to check expiration echo $CF_ACCESS_TOKEN | cut -d'.' -f2 | base64 -d | jq '.exp' # Compare with current time date +%s ``` **Resolution:** ```bash # Refresh the access token curl -X POST https://api.cloudflow.io/auth/refresh \\ -H \"Content-Type: application/json\" \\ -d '{\"refresh_token\": \"'$CF_REFRESH_TOKEN'\"}' # Update environment variable export CF_ACCESS_TOKEN=\"<new_token>\" ``` **2. Invalid JWT Signature** Error: `JWT signature verification failed` **Causes:** - Token was modified or corrupted - Using wrong signing key - Token generated with different secret **Resolution:** ```bash # Validate token structure cloudflow auth validate --token $CF_ACCESS_TOKEN # Generate new token cloudflow auth login --username <user> --password <pass> # For service accounts cloudflow auth service-account --create --name <sa-name> --scopes \"workflow:read,workflow:write\" ``` **3. Clock Skew Issues** JWT validation fails when system clocks are out of sync (tolerance: \u00b1300 seconds).",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_7",
              "content": "records: 500 - Session timeout: 30 seconds --- ## Message Queue Patterns ### Pub/Sub Pattern Used for broadcasting events to multiple interested consumers: ``` Workflow Engine publishes workflow.execution.completed \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u25bc \u25bc \u25bc \u25bc Analytics Notification Audit Logger Billing Service Service Service Service ``` Each service maintains its own consumer group and processes events independently. Failures in one consumer don't affect others. ### Request-Reply Pattern Synchronous communication over async messaging (used sparingly): ``` API Gateway publishes request \u2192 Kafka (reply-to: temp-queue-123) \u2502 \u25bc Service processes request \u2502 \u25bc Service publishes response \u2192 temp-queue-123 \u2502 \u25bc API Gateway receives response ``` Timeout: 5 seconds, fallback to direct HTTP call if no response. ### Saga Pattern Distributed transaction management for multi-service workflows: ``` Step 1: Create Order \u2192 SUCCESS \u2192 Step 2: Reserve Inventory \u2502 FAILURE \u2502 \u25bc Compensating Transaction \u2502 \u25bc Cancel Order (Rollback) ``` Orchestrated by Workflow Engine with compensation logic defined in workflow DSL. ### Dead Letter Queue (DLQ) Failed messages after all retry attempts are routed to DLQ: ``` Message processing fails (3 retries with exponential backoff) \u2502 \u25bc Publish to dead-letter-queue topic \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u25bc \u25bc Alert to PagerDuty Store in PostgreSQL for analysis \u2502 \u2502 \u25bc \u25bc Manual investigation Automated pattern detection ``` DLQ Processing SLA: < 4 hours for critical events, < 24 hours for non-critical. --- ## Caching Strategy ### Cache-Aside Pattern (Lazy Loading) Primary caching pattern used across all services: ``` 1. Application checks cache (Redis GET) 2. If HIT \u2192 Return cached data 3. If MISS: a. Query database (PostgreSQL) b. Store result in cache with TTL c. Return data to application ``` **Implementation Example** (Workflow Definition): ```javascript async function getWorkflowDefinition(workflowId) { const cacheKey = `workflow:def:${workflowId}`; // Step 1: Check cache let workflow = await redis.get(cacheKey); if (workflow) { metrics.increment('cache.hit.workflow_def'); return JSON.parse(workflow); } // Step 2: Cache miss - query database metrics.increment('cache.miss.workflow_def'); workflow = await db.query( 'SELECT * FROM workflows WHERE id = $1', [workflowId] ); // Step 3: Store in cache (1 hour TTL) await redis.setex(cacheKey, 3600, JSON.stringify(workflow)); return workflow; } ``` ### Write-Through Pattern Used for critical data where cache consistency is paramount: ``` 1. Application writes to database 2. Database transaction commits 3. Application updates cache 4. Return success to client ``` Applied to: User profiles, authentication sessions, system configuration.",
              "score": null
            }
          ],
          "latency_ms": 1007.1944400006032,
          "query_id": "realistic_013",
          "dimension": "contextual",
          "query": "After updating a workflow, how long until the cache expires?"
        },
        {
          "key_facts": [
            "TTL: 1 hour",
            "Workflow Definitions: TTL: 1 hour"
          ],
          "found_facts": [],
          "missed_facts": [
            "TTL: 1 hour",
            "Workflow Definitions: TTL: 1 hour"
          ],
          "coverage": 0.0,
          "expected_docs": [
            "architecture_overview"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_11",
              "content": "Service (2) - PostgreSQL Primary - PostgreSQL Replica - PostgreSQL Replica - Redis Primary (2) - Redis Replica (2) - Redis Replica (2) - Kafka Broker (2) - Kafka Broker (2) - Kafka Broker (1) ``` **Automatic Failover**: - Database: 30-60 seconds (automatic promotion of replica) - Redis: < 10 seconds (Sentinel-based failover) - Kafka: < 30 seconds (controller election) - Services: Kubernetes health checks with 10-second liveness probes ### Backup Strategy **Database Backups**: - Automated snapshots: Daily at 02:00 UTC - Retention: 30 days for daily, 90 days for monthly - Cross-region replication: Async replication to us-west-2 (15-minute lag) - Backup verification: Weekly automated restore test in staging environment **Backup Schedule**: ``` Daily: Full snapshot \u2192 S3 (encrypted) Hourly: WAL archives \u2192 S3 (point-in-time recovery) Weekly: Backup validation test Monthly: Long-term archive to Glacier ``` **Configuration Backups**: - Kubernetes manifests: Stored in Git (GitOps with ArgoCD) - Vault secrets: Automated snapshot every 6 hours - Kafka topic configurations: Exported daily to S3 ### Disaster Recovery Procedures **Scenario 1: Single AZ Failure** - Detection: Health checks fail for entire AZ (< 30 seconds) - Action: Traffic automatically routed to healthy AZs by ALB - Recovery time: < 5 minutes (no manual intervention) - Data loss: None (multi-AZ replication) **Scenario 2: Database Primary Failure** - Detection: Health check fails for primary database (< 30 seconds) - Action: Automatic promotion of read replica to primary - Recovery time: 30-60 seconds - Data loss: Minimal (< 1 second due to synchronous replication) **Scenario 3: Full Region Failure** - Detection: Multiple health check failures across all AZs (< 2 minutes) - Action: Manual failover to DR region (us-west-2) - Procedure: 1. Update DNS to point to DR region (TTL: 60 seconds) 2. Promote DR database replica to primary 3. Scale up DR region services to production capacity 4. Verify data consistency and integrity 5. Update monitoring dashboards - Recovery time: 2-4 hours (includes verification) - Data loss: < 1 hour (last cross-region replication) **Scenario 4: Data Corruption** - Detection: Data validation checks or user report - Action: Point-in-time recovery from WAL archives - Procedure: 1. Identify corruption time window 2. Restore from snapshot prior to corruption 3. Replay WAL logs up to corruption point 4. Verify data integrity 5.",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_12",
              "content": "Resume normal operations - Recovery time: 1-3 hours depending on data volume - Data loss: None if corruption detected quickly ### Testing & Validation **DR Drill Schedule**: - Monthly: Automated failover test (single AZ failure simulation) - Quarterly: Full DR region failover (non-production hours) - Annually: Complete disaster simulation with all stakeholders **Last DR Test Results** (Dec 15, 2025): - Scenario: Full region failover - Actual RTO: 2 hours 23 minutes (target: 4 hours) - Actual RPO: 42 minutes (target: 1 hour) - Issues identified: DNS propagation slower than expected (resolved) - Success criteria: Met all recovery objectives ### Business Continuity **Communication Plan**: - Status page: status.cloudflow.com (updated every 15 minutes during incident) - Customer notifications: Email + SMS for all P1 incidents - Internal escalation: PagerDuty \u2192 Incident Commander \u2192 Engineering Manager \u2192 CTO **Data Retention Policy**: - Active data: 12 months in hot storage (PostgreSQL) - Archived data: 7 years in cold storage (S3 Glacier) - Audit logs: 7 years (compliance requirement) - Backup retention: 30 days standard, 90 days monthly snapshots --- ## Appendix ### Service Dependency Matrix | Service | Depends On | Critical Path | |---------|-----------|---------------| | API Gateway | Auth Service, Redis | Yes | | Auth Service | PostgreSQL, Redis, Vault | Yes | | Workflow Engine | PostgreSQL, Kafka, Redis | Yes | | Scheduler | PostgreSQL, Redis, Kafka | No | | Notification Service | Kafka, PostgreSQL, SendGrid, Twilio | No | ### Capacity Planning **Current Capacity** (Jan 2026): - Daily workflow executions: 2.5M - Active users: 150,000 - API requests per day: 50M - Database size: 1.2TB - Kafka throughput: 100K msg/sec peak **6-Month Projection** (Jul 2026): - Daily workflow executions: 4M (+60%) - Active users: 225,000 (+50%) - API requests per day: 80M (+60%) - Database size: 2TB (+67%) - Required scaling: +30% compute, +50% storage ### Contact Information - **Architecture Team**: architecture@cloudflow.internal - **On-Call Engineer**: PagerDuty escalation - **Security Team**: security@cloudflow.internal - **Documentation**: https://wiki.cloudflow.internal/architecture --- **Document Revision History**: - v2.3.1 (Jan 15, 2026): Updated performance metrics, added DR test results - v2.3.0 (Dec 1, 2025): Added multi-AZ deployment details - v2.2.0 (Oct 15, 2025): Security architecture overhaul - v2.1.0 (Sep 1, 2025): Initial Kafka migration documentation - v2.0.0 (Jul 1, 2025): Complete microservices rewrite",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_10",
              "content": "- Execution start rate: 500 per second - Completion rate: 450 per second (average 2-second execution time) **Database**: - Read throughput: 50,000 queries per second (across replicas) - Write throughput: 15,000 transactions per second - Connection capacity: 2,000 concurrent connections **Kafka**: - Message ingestion: 100,000 messages per second - Consumer throughput: 80,000 messages per second (aggregated) - End-to-end latency: < 100ms (P99) ### Resource Utilization **CPU Utilization** (Target: 60-70% average): - API Gateway: 55% average, 80% peak - Workflow Engine: 65% average, 85% peak - Auth Service: 40% average, 60% peak **Memory Utilization** (Target: < 80%): - API Gateway: 2.5GB average per pod (4GB allocated) - Workflow Engine: 6GB average per pod (8GB allocated) - Notification Service: 2.8GB average per pod (4GB allocated) **Network Throughput**: - Ingress: 2 Gbps average, 5 Gbps peak - Egress: 1.5 Gbps average, 4 Gbps peak - Internal (service mesh): 8 Gbps average ### Monitoring & Alerting **Key Metrics** (Monitored via Prometheus): - Request rate (per service, per endpoint) - Error rate (4xx, 5xx responses) - Latency percentiles (P50, P95, P99) - Resource utilization (CPU, memory, disk) - Cache hit/miss ratios - Database connection pool saturation - Kafka consumer lag **Alerts** (via PagerDuty): - P1 (Immediate): API error rate > 1%, database connection failure - P2 (< 15 min): P99 latency > 500ms, cache hit rate < 80% - P3 (< 1 hour): Resource utilization > 85%, replica count < minimum **SLI/SLO/SLA**: - SLI: API success rate (non-5xx responses) - SLO: 99.9% uptime per month (43 minutes downtime allowance) - SLA: 99.5% uptime guarantee (customer-facing SLA with credits) --- ## Disaster Recovery ### Recovery Objectives **RPO (Recovery Point Objective): 1 hour** - Maximum acceptable data loss: 1 hour of transactions - Achieved through: Continuous database replication + hourly snapshots - Kafka retention: 7 days allows event replay **RTO (Recovery Time Objective): 4 hours** - Maximum acceptable downtime: 4 hours for full system recovery - Includes: Failover, data verification, and service restoration - Automated runbooks reduce RTO to < 2 hours for common scenarios ### High Availability Architecture **Multi-AZ Deployment**: ``` Region: us-east-1 AZ-1a: AZ-1b: AZ-1c: - API Gateway (4) - API Gateway (4) - API Gateway (4) - Workflow Engine (6) - Workflow Engine (5) - Workflow Engine (5) - Auth Service (3) - Auth Service (3) - Auth",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_10",
              "content": "database nc -zv cloudflow-db.internal.company.com 5432 # Trace route traceroute api.cloudflow.io # Capture packets tcpdump -i any -w /tmp/capture.pcap port 5432 ``` --- ## Escalation Procedures ### Severity Levels CloudFlow incidents are classified into four severity levels: #### SEV-1: Critical (P1) - **Definition:** Complete service outage or severe degradation affecting all users - **Examples:** - API returns 5xx errors for > 5 minutes - Database completely unavailable - Authentication system down - Data loss or corruption - **Response Time:** Immediate (< 15 minutes) - **Escalation:** Page on-call engineer immediately #### SEV-2: High (P2) - **Definition:** Major functionality impaired affecting multiple users - **Examples:** - Workflow execution success rate < 90% - Significant performance degradation (p95 latency > 10s) - Rate limiting affecting large customer segment - **Response Time:** < 1 hour - **Escalation:** Create incident ticket and notify on-call #### SEV-3: Medium (P3) - **Definition:** Partial functionality degraded affecting some users - **Examples:** - Intermittent failures for specific workflow types - Minor performance issues - Non-critical feature unavailable - **Response Time:** < 4 hours - **Escalation:** Create ticket, normal business hours support #### SEV-4: Low (P4) - **Definition:** Minor issues with minimal user impact - **Examples:** - Cosmetic issues - Documentation errors - Feature requests - **Response Time:** < 2 business days - **Escalation:** Standard support ticket ### Escalation Steps #### Step 1: Initial Assessment (0-5 minutes) ```bash # Run health check cloudflow health check --comprehensive # Check status page curl https://status.cloudflow.io/api/v1/status.json # Review recent changes cloudflow audit log --last 2h --event-type \"deployment,configuration\" # Check metrics dashboard cloudflow metrics dashboard --incident-mode ``` #### Step 2: Gather Information (5-15 minutes) Create incident document with: - Incident timestamp and duration - Affected services and endpoints - Error rates and user impact - Recent changes or deployments - Relevant log excerpts - Correlation IDs for failed requests ```bash # Generate incident report cloudflow debug incident-report \\ --start \"2026-01-24T10:30:00Z\" \\ --end \"2026-01-24T11:00:00Z\" \\ --output incident-report.md # Capture system snapshot cloudflow debug snapshot --output snapshot-$(date +%Y%m%d-%H%M%S).tar.gz ``` #### Step 3: Escalate Based on Severity **For SEV-1 (Critical):** ```bash # Page on-call engineer cloudflow incident create \\ --severity SEV-1 \\ --title \"Complete API outage\" \\ --description \"All API requests returning 503\" \\ --page-oncall # Notify status page cloudflow status update \\ --status \"major_outage\" \\ --message \"We are investigating a complete service",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_5",
              "content": "Success/failure events published back to Kafka ``` **Event Schema**: ```json { \"event_id\": \"uuid-v4\", \"event_type\": \"workflow.execution.completed\", \"timestamp\": \"2026-01-15T10:30:00.000Z\", \"correlation_id\": \"request-trace-id\", \"payload\": { \"workflow_id\": \"wf-12345\", \"execution_id\": \"exec-67890\", \"status\": \"COMPLETED\", \"duration_ms\": 4230 }, \"metadata\": { \"source_service\": \"workflow-engine\", \"schema_version\": \"1.0\" } } ``` ### Inter-Service Communication Services communicate using two primary patterns: **Synchronous (gRPC)**: - Auth Service validation calls from API Gateway - User Service profile lookups - Low latency requirement (< 50ms) - Request-response pattern with circuit breaker **Asynchronous (Kafka)**: - Workflow execution events - Notification triggers - Audit log events - Eventual consistency acceptable --- ## Database Architecture ### PostgreSQL Primary Database **Cluster Configuration**: - Primary-replica setup with 1 primary + 2 read replicas - Instance type: db.r6g.2xlarge (8 vCPU, 64GB RAM) - Storage: 2TB gp3 SSD with 12,000 IOPS - Multi-AZ deployment for high availability - Automated backups: Daily snapshots, 30-day retention - Point-in-time recovery: 5-minute granularity **Database Schema Design**: ``` Core Tables: - users (5M rows): User accounts and profiles - workflows (2M rows): Workflow definitions - workflow_executions (500M rows, partitioned): Execution history - workflow_steps (2B rows, partitioned): Individual step records - schedules (100K rows): Scheduled workflow triggers - notifications (1B rows, partitioned): Notification delivery log Partitioning Strategy: - workflow_executions: Monthly partitions by created_at - workflow_steps: Monthly partitions by created_at - notifications: Weekly partitions by created_at - Automatic partition creation via cron job - Retention policy: Drop partitions older than 12 months ``` **Indexing Strategy**: - Primary keys: UUIDs with B-tree indexes - Frequently queried columns: Compound indexes (e.g., user_id + status + created_at) - JSONB columns: GIN indexes for workflow definitions - Full-text search: GiST indexes on description fields **Connection Pooling**: - PgBouncer in transaction mode - Pool size: 100 connections per service - Max client connections: 2000 - Connection timeout: 30 seconds **Performance Characteristics**: - Read query P99: < 20ms - Write query P99: < 50ms - Transaction throughput: 15,000 TPS - Replication lag: < 500ms --- ### Redis Caching Layer **Cluster Configuration**: - Redis Cluster with 6 nodes (3 primary + 3 replica) - Instance type: cache.r6g.xlarge (4 vCPU, 26GB RAM) - Total memory: 78GB usable cache space - Persistence: RDB snapshots every 5 minutes + AOF - Eviction policy: allkeys-lru **Cache Usage Patterns**: 1.",
              "score": null
            }
          ],
          "latency_ms": 1219.520829999965,
          "query_id": "realistic_013",
          "dimension": "negation",
          "query": "Why are changes taking an hour to appear?"
        },
        {
          "key_facts": [
            "Prometheus",
            "Grafana",
            "Jaeger for distributed tracing"
          ],
          "found_facts": [
            "Prometheus",
            "Grafana",
            "Jaeger for distributed tracing"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "architecture_overview",
            "deployment_guide"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_4",
              "content": "--set prometheus.prometheusSpec.retention=30d \\ --set prometheus.prometheusSpec.storageSpec.volumeClaimTemplate.spec.resources.requests.storage=100Gi ``` CloudFlow exposes Prometheus metrics on port 9090 at the `/metrics` endpoint. The metrics include: - **HTTP Metrics**: Request rate, latency, error rate - **Database Metrics**: Connection pool status, query duration - **Worker Metrics**: Job queue size, processing time, failure rate - **System Metrics**: CPU usage, memory usage, heap statistics Example metrics exposed: ``` # HELP cloudflow_http_requests_total Total number of HTTP requests # TYPE cloudflow_http_requests_total counter cloudflow_http_requests_total{method=\"GET\",route=\"/api/workflows\",status=\"200\"} 15420 # HELP cloudflow_http_request_duration_seconds HTTP request duration in seconds # TYPE cloudflow_http_request_duration_seconds histogram cloudflow_http_request_duration_seconds_bucket{method=\"POST\",route=\"/api/workflows\",le=\"0.1\"} 8234 # HELP cloudflow_worker_jobs_processed_total Total number of jobs processed # TYPE cloudflow_worker_jobs_processed_total counter cloudflow_worker_jobs_processed_total{status=\"success\"} 45230 # HELP cloudflow_database_connections Current database connections # TYPE cloudflow_database_connections gauge cloudflow_database_connections{state=\"active\"} 23 ``` ### Grafana Dashboards Access Grafana to view CloudFlow dashboards: ```bash kubectl port-forward -n monitoring svc/prometheus-grafana 3000:80 ``` Import the CloudFlow dashboard (ID: 15847) or use the provided JSON template. Key dashboard panels include: 1. **API Performance**: Request rate, P95/P99 latency, error rate 2. **Resource Usage**: CPU, memory, disk I/O per pod 3. **Database Health**: Connection pool utilization, query performance 4. **Worker Status**: Queue depth, processing rate, job success/failure ratio 5. **System Overview**: Pod status, replica count, autoscaling events ### Logging with CloudWatch CloudFlow logs are automatically shipped to CloudWatch Logs.",
              "score": null
            },
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_3",
              "content": "Deploy PostgreSQL using the Bitnami Helm chart: ```yaml # postgres-values.yaml global: postgresql: auth: username: cloudflow database: cloudflow existingSecret: postgres-credentials image: tag: \"14.10.0\" primary: resources: limits: cpu: 4000m memory: 8Gi requests: cpu: 2000m memory: 4Gi persistence: enabled: true size: 100Gi storageClass: gp3 extendedConfiguration: | max_connections = 100 shared_buffers = 2GB effective_cache_size = 6GB maintenance_work_mem = 512MB checkpoint_completion_target = 0.9 wal_buffers = 16MB default_statistics_target = 100 random_page_cost = 1.1 effective_io_concurrency = 200 work_mem = 10MB min_wal_size = 1GB max_wal_size = 4GB readReplicas: replicaCount: 2 resources: limits: cpu: 2000m memory: 4Gi requests: cpu: 1000m memory: 2Gi metrics: enabled: true serviceMonitor: enabled: true ``` Install PostgreSQL: ```bash # Create password secret kubectl create secret generic postgres-credentials \\ --namespace cloudflow-prod \\ --from-literal=postgres-password=\"$(openssl rand -base64 32)\" \\ --from-literal=password=\"$(openssl rand -base64 32)\" # Install PostgreSQL helm install postgresql bitnami/postgresql \\ --namespace cloudflow-prod \\ --values postgres-values.yaml \\ --wait ``` ### PgBouncer Configuration Deploy PgBouncer for connection pooling to handle up to 100 connections efficiently: ```yaml # pgbouncer.yaml apiVersion: v1 kind: ConfigMap metadata: name: pgbouncer-config namespace: cloudflow-prod data: pgbouncer.ini: | [databases] cloudflow = host=postgresql.cloudflow-prod.svc.cluster.local port=5432 dbname=cloudflow [pgbouncer] listen_addr = 0.0.0.0 listen_port = 5432 auth_type = md5 auth_file = /etc/pgbouncer/userlist.txt pool_mode = transaction max_client_conn = 1000 default_pool_size = 25 reserve_pool_size = 5 reserve_pool_timeout = 3 max_db_connections = 100 max_user_connections = 100 server_lifetime = 3600 server_idle_timeout = 600 log_connections = 1 log_disconnections = 1 log_pooler_errors = 1 --- apiVersion: apps/v1 kind: Deployment metadata: name: pgbouncer namespace: cloudflow-prod spec: replicas: 2 selector: matchLabels: app: pgbouncer template: metadata: labels: app: pgbouncer spec: containers: - name: pgbouncer image: pgbouncer/pgbouncer:1.21.0 ports: - containerPort: 5432 resources: requests: cpu: 500m memory: 512Mi limits: cpu: 1000m memory: 1Gi volumeMounts: - name: config mountPath: /etc/pgbouncer volumes: - name: config configMap: name: pgbouncer-config --- apiVersion: v1 kind: Service metadata: name: pgbouncer namespace: cloudflow-prod spec: selector: app: pgbouncer ports: - port: 5432 targetPort: 5432 type: ClusterIP ``` Apply the PgBouncer configuration: ```bash kubectl apply -f pgbouncer.yaml ``` ### Database Migrations Run database migrations before deploying new versions: ```bash # Create migration job kubectl create job cloudflow-migrate-$(date +%s) \\ --namespace cloudflow-prod \\ --image=123456789012.dkr.ecr.us-east-1.amazonaws.com/cloudflow:2.4.0 \\ -- npm run migrate # Monitor migration progress kubectl logs -f job/cloudflow-migrate-<timestamp> -n cloudflow-prod ``` --- ## Monitoring and Observability ### Prometheus Setup Install Prometheus using the kube-prometheus-stack: ```bash helm install prometheus prometheus-community/kube-prometheus-stack \\ --namespace monitoring \\ --create-namespace \\ --set prometheus.prometheusSpec.serviceMonitorSelectorNilUsesHelmValues=false \\",
              "score": null
            },
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_0",
              "content": "# CloudFlow Platform - Deployment and Operations Guide **Version:** 2.4.0 **Last Updated:** January 2026 **Target Environment:** Production (AWS EKS) ## Table of Contents 1. [Overview](#overview) 2. [Prerequisites](#prerequisites) 3. [Infrastructure Setup](#infrastructure-setup) 4. [Kubernetes Deployment](#kubernetes-deployment) 5. [Database Configuration](#database-configuration) 6. [Monitoring and Observability](#monitoring-and-observability) 7. [Backup and Disaster Recovery](#backup-and-disaster-recovery) 8. [Scaling and Performance](#scaling-and-performance) 9. [Security and Compliance](#security-and-compliance) 10. [Troubleshooting](#troubleshooting) --- ## Overview CloudFlow is a cloud-native workflow orchestration platform designed for high-availability production environments. This guide provides comprehensive instructions for deploying and operating CloudFlow on Amazon EKS (Elastic Kubernetes Service). ### Architecture Summary CloudFlow consists of the following components: - **API Server**: REST API for workflow management (Node.js/Express) - **Worker Service**: Background job processor (Node.js) - **Scheduler**: Cron-based task scheduler (Node.js) - **PostgreSQL**: Primary data store (version 14) - **Redis**: Cache and message queue (version 7.0) - **PgBouncer**: Database connection pooler ### Deployment Model - **Namespace**: `cloudflow-prod` - **Cluster**: AWS EKS 1.28 - **Region**: us-east-1 (primary), us-west-2 (disaster recovery) - **High Availability**: Multi-AZ deployment across 3 availability zones --- ## Prerequisites Before beginning the deployment process, ensure you have the following: ### Required Tools - `kubectl` (v1.28 or later) - `helm` (v3.12 or later) - `aws-cli` (v2.13 or later) - `eksctl` (v0.165 or later) - `terraform` (v1.6 or later) - for infrastructure provisioning ### Access Requirements - AWS account with appropriate IAM permissions - EKS cluster admin access - Container registry access (ECR) - Domain name and SSL certificates - Secrets management access (AWS Secrets Manager or Vault) ### Network Requirements - VPC with at least 3 public and 3 private subnets - NAT Gateway configured - Security groups allowing: - Ingress: HTTPS (443), HTTP (80) - Internal: PostgreSQL (5432), Redis (6379) - Monitoring: Prometheus (9090), Grafana (3000) --- ## Infrastructure Setup ### EKS Cluster Creation Use the following `eksctl` configuration to create the EKS cluster: ```yaml # cluster-config.yaml apiVersion: eksctl.io/v1alpha5 kind: ClusterConfig metadata: name: cloudflow-production region: us-east-1 version: \"1.28\" availabilityZones: - us-east-1a - us-east-1b - us-east-1c vpc: cidr: 10.0.0.0/16 nat: gateway: HighlyAvailable managedNodeGroups: - name: cloudflow-workers instanceType: m5.xlarge minSize: 3 maxSize: 10 desiredCapacity: 5 volumeSize: 100 privateNetworking: true labels: role: worker environment: production tags: Environment: production Application: cloudflow iam: withAddonPolicies: ebs: true efs: true albIngress: true cloudWatch: true cloudWatch: clusterLogging: enableTypes: - api - audit - authenticator - controllerManager - scheduler ``` Create the cluster: ```bash eksctl",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_0",
              "content": "# CloudFlow Platform - System Architecture Overview **Document Version:** 2.3.1 **Last Updated:** January 15, 2026 **Owner:** Platform Architecture Team **Status:** Production ## Executive Summary CloudFlow is a distributed, cloud-native workflow automation platform designed to orchestrate complex business processes at scale. The platform processes over 2.5 million workflow executions daily with an average P99 latency of 180ms for API operations and 4.2 seconds for workflow execution. This document provides a comprehensive overview of the system architecture, including microservices design, data flow patterns, infrastructure decisions, and operational considerations. ## Table of Contents 1. [High-Level Architecture](#high-level-architecture) 2. [Microservices Breakdown](#microservices-breakdown) 3. [Data Flow Architecture](#data-flow-architecture) 4. [Database Architecture](#database-architecture) 5. [Message Queue Patterns](#message-queue-patterns) 6. [Caching Strategy](#caching-strategy) 7. [Security Architecture](#security-architecture) 8. [Performance Characteristics](#performance-characteristics) 9. [Disaster Recovery](#disaster-recovery) --- ## High-Level Architecture CloudFlow follows a microservices architecture pattern deployed across multiple availability zones in AWS. The platform is designed with the following core principles: - **Scalability**: Horizontal scaling for all services with auto-scaling groups - **Resilience**: Circuit breakers, bulkheads, and graceful degradation - **Observability**: Distributed tracing, centralized logging, and comprehensive metrics - **Security**: Zero-trust network architecture with mTLS encryption ### Architecture Diagram (Conceptual) ``` \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 Load Balancer (ALB) \u2502 \u2502 (TLS Termination - 443) \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u25bc \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 API Gateway Layer \u2502 \u2502 (Rate Limiting, Auth, Request Routing) \u2502 \u2514\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u2502 \u2502 \u2502 \u2502 \u25bc \u25bc \u25bc \u25bc \u25bc \u250c\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502Auth\u2502 \u2502Workflow\u2502 \u2502Scheduler\u2502 \u2502Notification\u2502 \u2502User Service\u2502 \u2502Svc \u2502 \u2502 Engine \u2502 \u2502 Service \u2502 \u2502 Service \u2502 \u2502 \u2502 \u2514\u2500\u252c\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u2502 \u2502 \u2502 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 \u2502 \u2502 \u25bc \u25bc \u25bc \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502PostgreSQL\u2502 \u2502 Redis \u2502 \u2502 Kafka \u2502 \u2502 Cluster \u2502 \u2502 Cluster \u2502 \u2502 Cluster \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 ``` ### Technology Stack - **Runtime**: Node.js 20.x (API Gateway, Workflow Engine), Go 1.21 (Auth Service, Scheduler) - **Container Orchestration**: Kubernetes 1.28 (EKS) - **Service Mesh**: Istio 1.20 for service-to-service communication - **Databases**: PostgreSQL 15.4, Redis 7.2 - **Message Broker**: Apache Kafka 3.6 - **Monitoring**: Prometheus, Grafana, Jaeger for distributed tracing - **Secrets Management**: HashiCorp Vault 1.15 --- ## Microservices Breakdown ### API Gateway **Purpose**: Single entry point for all client requests, providing authentication, rate limiting, request routing, and protocol translation.",
              "score": null
            },
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_1",
              "content": "create cluster -f cluster-config.yaml ``` ### Storage Configuration Install the EBS CSI driver for persistent volumes: ```bash eksctl create addon --name aws-ebs-csi-driver \\ --cluster cloudflow-production \\ --service-account-role-arn arn:aws:iam::ACCOUNT_ID:role/AmazonEKS_EBS_CSI_DriverRole \\ --force ``` --- ## Kubernetes Deployment ### Namespace Setup Create the CloudFlow namespace and configure resource quotas: ```yaml # namespace.yaml apiVersion: v1 kind: Namespace metadata: name: cloudflow-prod labels: name: cloudflow-prod environment: production --- apiVersion: v1 kind: ResourceQuota metadata: name: cloudflow-quota namespace: cloudflow-prod spec: hard: requests.cpu: \"50\" requests.memory: 100Gi persistentvolumeclaims: \"10\" services.loadbalancers: \"2\" ``` Apply the namespace configuration: ```bash kubectl apply -f namespace.yaml ``` ### Environment Configuration CloudFlow requires the following environment variables: | Variable | Description | Example | Required | |----------|-------------|---------|----------| | `DATABASE_URL` | PostgreSQL connection string | `postgresql://user:pass@host:5432/cloudflow` | Yes | | `REDIS_URL` | Redis connection string | `redis://redis-master.cloudflow-prod.svc.cluster.local:6379` | Yes | | `JWT_SECRET` | Secret key for JWT token signing | `<generated-secret-256-bit>` | Yes | | `LOG_LEVEL` | Application log verbosity | `info`, `debug`, `warn`, `error` | No (default: `info`) | | `API_PORT` | API server port | `3000` | No (default: `3000`) | | `WORKER_CONCURRENCY` | Number of concurrent workers | `10` | No (default: `5`) | | `SESSION_SECRET` | Session encryption key | `<generated-secret-256-bit>` | Yes | | `AWS_REGION` | AWS region for services | `us-east-1` | Yes | | `S3_BUCKET` | S3 bucket for file storage | `cloudflow-prod-storage` | Yes | | `SMTP_HOST` | Email server hostname | `smtp.sendgrid.net` | No | | `SMTP_PORT` | Email server port | `587` | No | | `METRICS_ENABLED` | Enable Prometheus metrics | `true` | No (default: `false`) | Create a Kubernetes secret for sensitive variables: ```bash kubectl create secret generic cloudflow-secrets \\ --namespace cloudflow-prod \\ --from-literal=DATABASE_URL=\"postgresql://cloudflow:$(cat db-password.txt)@pgbouncer.cloudflow-prod.svc.cluster.local:5432/cloudflow\" \\ --from-literal=REDIS_URL=\"redis://redis-master.cloudflow-prod.svc.cluster.local:6379\" \\ --from-literal=JWT_SECRET=\"$(openssl rand -base64 32)\" \\ --from-literal=SESSION_SECRET=\"$(openssl rand -base64 32)\" ``` ### Helm Chart Deployment Add the CloudFlow Helm repository: ```bash helm repo add cloudflow https://charts.cloudflow.io helm repo update ``` Create a values file for production configuration: ```yaml # values-production.yaml replicaCount: 3 image: repository: 123456789012.dkr.ecr.us-east-1.amazonaws.com/cloudflow tag: \"2.4.0\" pullPolicy: IfNotPresent resources: limits: cpu: 2000m memory: 4Gi requests: cpu: 2000m memory: 4Gi autoscaling: enabled: true minReplicas: 3 maxReplicas: 10 targetCPUUtilizationPercentage: 70 targetMemoryUtilizationPercentage: 80 service: type: ClusterIP port: 80 targetPort: 3000 annotations: service.beta.kubernetes.io/aws-load-balancer-type: \"nlb\" ingress: enabled: true className: nginx annotations: cert-manager.io/cluster-issuer: \"letsencrypt-prod\" nginx.ingress.kubernetes.io/ssl-redirect: \"true\" hosts: - host: api.cloudflow.io paths: - path: / pathType: Prefix tls:",
              "score": null
            }
          ],
          "latency_ms": 1046.228698996856,
          "query_id": "realistic_014",
          "dimension": "original",
          "query": "What monitoring tools does CloudFlow use?"
        },
        {
          "key_facts": [
            "Prometheus",
            "Grafana",
            "Jaeger for distributed tracing"
          ],
          "found_facts": [
            "Prometheus",
            "Grafana",
            "Jaeger for distributed tracing"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "architecture_overview",
            "deployment_guide"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_4",
              "content": "--set prometheus.prometheusSpec.retention=30d \\ --set prometheus.prometheusSpec.storageSpec.volumeClaimTemplate.spec.resources.requests.storage=100Gi ``` CloudFlow exposes Prometheus metrics on port 9090 at the `/metrics` endpoint. The metrics include: - **HTTP Metrics**: Request rate, latency, error rate - **Database Metrics**: Connection pool status, query duration - **Worker Metrics**: Job queue size, processing time, failure rate - **System Metrics**: CPU usage, memory usage, heap statistics Example metrics exposed: ``` # HELP cloudflow_http_requests_total Total number of HTTP requests # TYPE cloudflow_http_requests_total counter cloudflow_http_requests_total{method=\"GET\",route=\"/api/workflows\",status=\"200\"} 15420 # HELP cloudflow_http_request_duration_seconds HTTP request duration in seconds # TYPE cloudflow_http_request_duration_seconds histogram cloudflow_http_request_duration_seconds_bucket{method=\"POST\",route=\"/api/workflows\",le=\"0.1\"} 8234 # HELP cloudflow_worker_jobs_processed_total Total number of jobs processed # TYPE cloudflow_worker_jobs_processed_total counter cloudflow_worker_jobs_processed_total{status=\"success\"} 45230 # HELP cloudflow_database_connections Current database connections # TYPE cloudflow_database_connections gauge cloudflow_database_connections{state=\"active\"} 23 ``` ### Grafana Dashboards Access Grafana to view CloudFlow dashboards: ```bash kubectl port-forward -n monitoring svc/prometheus-grafana 3000:80 ``` Import the CloudFlow dashboard (ID: 15847) or use the provided JSON template. Key dashboard panels include: 1. **API Performance**: Request rate, P95/P99 latency, error rate 2. **Resource Usage**: CPU, memory, disk I/O per pod 3. **Database Health**: Connection pool utilization, query performance 4. **Worker Status**: Queue depth, processing rate, job success/failure ratio 5. **System Overview**: Pod status, replica count, autoscaling events ### Logging with CloudWatch CloudFlow logs are automatically shipped to CloudWatch Logs.",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_0",
              "content": "# CloudFlow Platform - System Architecture Overview **Document Version:** 2.3.1 **Last Updated:** January 15, 2026 **Owner:** Platform Architecture Team **Status:** Production ## Executive Summary CloudFlow is a distributed, cloud-native workflow automation platform designed to orchestrate complex business processes at scale. The platform processes over 2.5 million workflow executions daily with an average P99 latency of 180ms for API operations and 4.2 seconds for workflow execution. This document provides a comprehensive overview of the system architecture, including microservices design, data flow patterns, infrastructure decisions, and operational considerations. ## Table of Contents 1. [High-Level Architecture](#high-level-architecture) 2. [Microservices Breakdown](#microservices-breakdown) 3. [Data Flow Architecture](#data-flow-architecture) 4. [Database Architecture](#database-architecture) 5. [Message Queue Patterns](#message-queue-patterns) 6. [Caching Strategy](#caching-strategy) 7. [Security Architecture](#security-architecture) 8. [Performance Characteristics](#performance-characteristics) 9. [Disaster Recovery](#disaster-recovery) --- ## High-Level Architecture CloudFlow follows a microservices architecture pattern deployed across multiple availability zones in AWS. The platform is designed with the following core principles: - **Scalability**: Horizontal scaling for all services with auto-scaling groups - **Resilience**: Circuit breakers, bulkheads, and graceful degradation - **Observability**: Distributed tracing, centralized logging, and comprehensive metrics - **Security**: Zero-trust network architecture with mTLS encryption ### Architecture Diagram (Conceptual) ``` \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 Load Balancer (ALB) \u2502 \u2502 (TLS Termination - 443) \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u25bc \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 API Gateway Layer \u2502 \u2502 (Rate Limiting, Auth, Request Routing) \u2502 \u2514\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u2502 \u2502 \u2502 \u2502 \u25bc \u25bc \u25bc \u25bc \u25bc \u250c\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502Auth\u2502 \u2502Workflow\u2502 \u2502Scheduler\u2502 \u2502Notification\u2502 \u2502User Service\u2502 \u2502Svc \u2502 \u2502 Engine \u2502 \u2502 Service \u2502 \u2502 Service \u2502 \u2502 \u2502 \u2514\u2500\u252c\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u2502 \u2502 \u2502 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 \u2502 \u2502 \u25bc \u25bc \u25bc \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502PostgreSQL\u2502 \u2502 Redis \u2502 \u2502 Kafka \u2502 \u2502 Cluster \u2502 \u2502 Cluster \u2502 \u2502 Cluster \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 ``` ### Technology Stack - **Runtime**: Node.js 20.x (API Gateway, Workflow Engine), Go 1.21 (Auth Service, Scheduler) - **Container Orchestration**: Kubernetes 1.28 (EKS) - **Service Mesh**: Istio 1.20 for service-to-service communication - **Databases**: PostgreSQL 15.4, Redis 7.2 - **Message Broker**: Apache Kafka 3.6 - **Monitoring**: Prometheus, Grafana, Jaeger for distributed tracing - **Secrets Management**: HashiCorp Vault 1.15 --- ## Microservices Breakdown ### API Gateway **Purpose**: Single entry point for all client requests, providing authentication, rate limiting, request routing, and protocol translation.",
              "score": null
            },
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_0",
              "content": "# CloudFlow Platform - Deployment and Operations Guide **Version:** 2.4.0 **Last Updated:** January 2026 **Target Environment:** Production (AWS EKS) ## Table of Contents 1. [Overview](#overview) 2. [Prerequisites](#prerequisites) 3. [Infrastructure Setup](#infrastructure-setup) 4. [Kubernetes Deployment](#kubernetes-deployment) 5. [Database Configuration](#database-configuration) 6. [Monitoring and Observability](#monitoring-and-observability) 7. [Backup and Disaster Recovery](#backup-and-disaster-recovery) 8. [Scaling and Performance](#scaling-and-performance) 9. [Security and Compliance](#security-and-compliance) 10. [Troubleshooting](#troubleshooting) --- ## Overview CloudFlow is a cloud-native workflow orchestration platform designed for high-availability production environments. This guide provides comprehensive instructions for deploying and operating CloudFlow on Amazon EKS (Elastic Kubernetes Service). ### Architecture Summary CloudFlow consists of the following components: - **API Server**: REST API for workflow management (Node.js/Express) - **Worker Service**: Background job processor (Node.js) - **Scheduler**: Cron-based task scheduler (Node.js) - **PostgreSQL**: Primary data store (version 14) - **Redis**: Cache and message queue (version 7.0) - **PgBouncer**: Database connection pooler ### Deployment Model - **Namespace**: `cloudflow-prod` - **Cluster**: AWS EKS 1.28 - **Region**: us-east-1 (primary), us-west-2 (disaster recovery) - **High Availability**: Multi-AZ deployment across 3 availability zones --- ## Prerequisites Before beginning the deployment process, ensure you have the following: ### Required Tools - `kubectl` (v1.28 or later) - `helm` (v3.12 or later) - `aws-cli` (v2.13 or later) - `eksctl` (v0.165 or later) - `terraform` (v1.6 or later) - for infrastructure provisioning ### Access Requirements - AWS account with appropriate IAM permissions - EKS cluster admin access - Container registry access (ECR) - Domain name and SSL certificates - Secrets management access (AWS Secrets Manager or Vault) ### Network Requirements - VPC with at least 3 public and 3 private subnets - NAT Gateway configured - Security groups allowing: - Ingress: HTTPS (443), HTTP (80) - Internal: PostgreSQL (5432), Redis (6379) - Monitoring: Prometheus (9090), Grafana (3000) --- ## Infrastructure Setup ### EKS Cluster Creation Use the following `eksctl` configuration to create the EKS cluster: ```yaml # cluster-config.yaml apiVersion: eksctl.io/v1alpha5 kind: ClusterConfig metadata: name: cloudflow-production region: us-east-1 version: \"1.28\" availabilityZones: - us-east-1a - us-east-1b - us-east-1c vpc: cidr: 10.0.0.0/16 nat: gateway: HighlyAvailable managedNodeGroups: - name: cloudflow-workers instanceType: m5.xlarge minSize: 3 maxSize: 10 desiredCapacity: 5 volumeSize: 100 privateNetworking: true labels: role: worker environment: production tags: Environment: production Application: cloudflow iam: withAddonPolicies: ebs: true efs: true albIngress: true cloudWatch: true cloudWatch: clusterLogging: enableTypes: - api - audit - authenticator - controllerManager - scheduler ``` Create the cluster: ```bash eksctl",
              "score": null
            },
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_3",
              "content": "Deploy PostgreSQL using the Bitnami Helm chart: ```yaml # postgres-values.yaml global: postgresql: auth: username: cloudflow database: cloudflow existingSecret: postgres-credentials image: tag: \"14.10.0\" primary: resources: limits: cpu: 4000m memory: 8Gi requests: cpu: 2000m memory: 4Gi persistence: enabled: true size: 100Gi storageClass: gp3 extendedConfiguration: | max_connections = 100 shared_buffers = 2GB effective_cache_size = 6GB maintenance_work_mem = 512MB checkpoint_completion_target = 0.9 wal_buffers = 16MB default_statistics_target = 100 random_page_cost = 1.1 effective_io_concurrency = 200 work_mem = 10MB min_wal_size = 1GB max_wal_size = 4GB readReplicas: replicaCount: 2 resources: limits: cpu: 2000m memory: 4Gi requests: cpu: 1000m memory: 2Gi metrics: enabled: true serviceMonitor: enabled: true ``` Install PostgreSQL: ```bash # Create password secret kubectl create secret generic postgres-credentials \\ --namespace cloudflow-prod \\ --from-literal=postgres-password=\"$(openssl rand -base64 32)\" \\ --from-literal=password=\"$(openssl rand -base64 32)\" # Install PostgreSQL helm install postgresql bitnami/postgresql \\ --namespace cloudflow-prod \\ --values postgres-values.yaml \\ --wait ``` ### PgBouncer Configuration Deploy PgBouncer for connection pooling to handle up to 100 connections efficiently: ```yaml # pgbouncer.yaml apiVersion: v1 kind: ConfigMap metadata: name: pgbouncer-config namespace: cloudflow-prod data: pgbouncer.ini: | [databases] cloudflow = host=postgresql.cloudflow-prod.svc.cluster.local port=5432 dbname=cloudflow [pgbouncer] listen_addr = 0.0.0.0 listen_port = 5432 auth_type = md5 auth_file = /etc/pgbouncer/userlist.txt pool_mode = transaction max_client_conn = 1000 default_pool_size = 25 reserve_pool_size = 5 reserve_pool_timeout = 3 max_db_connections = 100 max_user_connections = 100 server_lifetime = 3600 server_idle_timeout = 600 log_connections = 1 log_disconnections = 1 log_pooler_errors = 1 --- apiVersion: apps/v1 kind: Deployment metadata: name: pgbouncer namespace: cloudflow-prod spec: replicas: 2 selector: matchLabels: app: pgbouncer template: metadata: labels: app: pgbouncer spec: containers: - name: pgbouncer image: pgbouncer/pgbouncer:1.21.0 ports: - containerPort: 5432 resources: requests: cpu: 500m memory: 512Mi limits: cpu: 1000m memory: 1Gi volumeMounts: - name: config mountPath: /etc/pgbouncer volumes: - name: config configMap: name: pgbouncer-config --- apiVersion: v1 kind: Service metadata: name: pgbouncer namespace: cloudflow-prod spec: selector: app: pgbouncer ports: - port: 5432 targetPort: 5432 type: ClusterIP ``` Apply the PgBouncer configuration: ```bash kubectl apply -f pgbouncer.yaml ``` ### Database Migrations Run database migrations before deploying new versions: ```bash # Create migration job kubectl create job cloudflow-migrate-$(date +%s) \\ --namespace cloudflow-prod \\ --image=123456789012.dkr.ecr.us-east-1.amazonaws.com/cloudflow:2.4.0 \\ -- npm run migrate # Monitor migration progress kubectl logs -f job/cloudflow-migrate-<timestamp> -n cloudflow-prod ``` --- ## Monitoring and Observability ### Prometheus Setup Install Prometheus using the kube-prometheus-stack: ```bash helm install prometheus prometheus-community/kube-prometheus-stack \\ --namespace monitoring \\ --create-namespace \\ --set prometheus.prometheusSpec.serviceMonitorSelectorNilUsesHelmValues=false \\",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_0",
              "content": "# CloudFlow Platform Troubleshooting Guide **Version:** 3.2.1 **Last Updated:** January 2026 **Audience:** Platform Engineers, DevOps, Support Teams ## Table of Contents 1. [Overview](#overview) 2. [Authentication & Authorization Issues](#authentication--authorization-issues) 3. [Performance Problems](#performance-problems) 4. [Database Connection Issues](#database-connection-issues) 5. [Workflow Execution Failures](#workflow-execution-failures) 6. [Rate Limiting & Throttling](#rate-limiting--throttling) 7. [Log Analysis & Debugging](#log-analysis--debugging) 8. [Escalation Procedures](#escalation-procedures) --- ## Overview This guide provides comprehensive troubleshooting steps for common CloudFlow platform issues encountered in production environments. Each section includes error symptoms, root cause analysis, resolution steps, and preventive measures. ### Quick Diagnostic Checklist Before diving into specific issues, perform these initial checks: - Verify service health: `cloudflow status --all` - Check API connectivity: `curl -I https://api.cloudflow.io/health` - Review recent deployments: `kubectl get deployments -n cloudflow --sort-by=.metadata.creationTimestamp` - Inspect platform metrics: `cloudflow metrics --last 1h` --- ## Authentication & Authorization Issues ### 401 Unauthorized Errors #### Symptoms - API requests return `401 Unauthorized` - Error message: `Authentication credentials were not provided or are invalid` - Frontend displays \"Session expired\" message #### Common Causes **1. Token Expiration** CloudFlow access tokens expire after 3600 seconds (1 hour) by default. Refresh tokens are valid for 30 days. **Verification:** ```bash # Decode JWT to check expiration echo $CF_ACCESS_TOKEN | cut -d'.' -f2 | base64 -d | jq '.exp' # Compare with current time date +%s ``` **Resolution:** ```bash # Refresh the access token curl -X POST https://api.cloudflow.io/auth/refresh \\ -H \"Content-Type: application/json\" \\ -d '{\"refresh_token\": \"'$CF_REFRESH_TOKEN'\"}' # Update environment variable export CF_ACCESS_TOKEN=\"<new_token>\" ``` **2. Invalid JWT Signature** Error: `JWT signature verification failed` **Causes:** - Token was modified or corrupted - Using wrong signing key - Token generated with different secret **Resolution:** ```bash # Validate token structure cloudflow auth validate --token $CF_ACCESS_TOKEN # Generate new token cloudflow auth login --username <user> --password <pass> # For service accounts cloudflow auth service-account --create --name <sa-name> --scopes \"workflow:read,workflow:write\" ``` **3. Clock Skew Issues** JWT validation fails when system clocks are out of sync (tolerance: \u00b1300 seconds).",
              "score": null
            }
          ],
          "latency_ms": 835.0907630010624,
          "query_id": "realistic_014",
          "dimension": "synonym",
          "query": "Which observability platform is integrated?"
        },
        {
          "key_facts": [
            "Prometheus",
            "Grafana",
            "Jaeger for distributed tracing"
          ],
          "found_facts": [
            "Prometheus",
            "Grafana",
            "Jaeger for distributed tracing"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "architecture_overview",
            "deployment_guide"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_4",
              "content": "--set prometheus.prometheusSpec.retention=30d \\ --set prometheus.prometheusSpec.storageSpec.volumeClaimTemplate.spec.resources.requests.storage=100Gi ``` CloudFlow exposes Prometheus metrics on port 9090 at the `/metrics` endpoint. The metrics include: - **HTTP Metrics**: Request rate, latency, error rate - **Database Metrics**: Connection pool status, query duration - **Worker Metrics**: Job queue size, processing time, failure rate - **System Metrics**: CPU usage, memory usage, heap statistics Example metrics exposed: ``` # HELP cloudflow_http_requests_total Total number of HTTP requests # TYPE cloudflow_http_requests_total counter cloudflow_http_requests_total{method=\"GET\",route=\"/api/workflows\",status=\"200\"} 15420 # HELP cloudflow_http_request_duration_seconds HTTP request duration in seconds # TYPE cloudflow_http_request_duration_seconds histogram cloudflow_http_request_duration_seconds_bucket{method=\"POST\",route=\"/api/workflows\",le=\"0.1\"} 8234 # HELP cloudflow_worker_jobs_processed_total Total number of jobs processed # TYPE cloudflow_worker_jobs_processed_total counter cloudflow_worker_jobs_processed_total{status=\"success\"} 45230 # HELP cloudflow_database_connections Current database connections # TYPE cloudflow_database_connections gauge cloudflow_database_connections{state=\"active\"} 23 ``` ### Grafana Dashboards Access Grafana to view CloudFlow dashboards: ```bash kubectl port-forward -n monitoring svc/prometheus-grafana 3000:80 ``` Import the CloudFlow dashboard (ID: 15847) or use the provided JSON template. Key dashboard panels include: 1. **API Performance**: Request rate, P95/P99 latency, error rate 2. **Resource Usage**: CPU, memory, disk I/O per pod 3. **Database Health**: Connection pool utilization, query performance 4. **Worker Status**: Queue depth, processing rate, job success/failure ratio 5. **System Overview**: Pod status, replica count, autoscaling events ### Logging with CloudWatch CloudFlow logs are automatically shipped to CloudWatch Logs.",
              "score": null
            },
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_3",
              "content": "Deploy PostgreSQL using the Bitnami Helm chart: ```yaml # postgres-values.yaml global: postgresql: auth: username: cloudflow database: cloudflow existingSecret: postgres-credentials image: tag: \"14.10.0\" primary: resources: limits: cpu: 4000m memory: 8Gi requests: cpu: 2000m memory: 4Gi persistence: enabled: true size: 100Gi storageClass: gp3 extendedConfiguration: | max_connections = 100 shared_buffers = 2GB effective_cache_size = 6GB maintenance_work_mem = 512MB checkpoint_completion_target = 0.9 wal_buffers = 16MB default_statistics_target = 100 random_page_cost = 1.1 effective_io_concurrency = 200 work_mem = 10MB min_wal_size = 1GB max_wal_size = 4GB readReplicas: replicaCount: 2 resources: limits: cpu: 2000m memory: 4Gi requests: cpu: 1000m memory: 2Gi metrics: enabled: true serviceMonitor: enabled: true ``` Install PostgreSQL: ```bash # Create password secret kubectl create secret generic postgres-credentials \\ --namespace cloudflow-prod \\ --from-literal=postgres-password=\"$(openssl rand -base64 32)\" \\ --from-literal=password=\"$(openssl rand -base64 32)\" # Install PostgreSQL helm install postgresql bitnami/postgresql \\ --namespace cloudflow-prod \\ --values postgres-values.yaml \\ --wait ``` ### PgBouncer Configuration Deploy PgBouncer for connection pooling to handle up to 100 connections efficiently: ```yaml # pgbouncer.yaml apiVersion: v1 kind: ConfigMap metadata: name: pgbouncer-config namespace: cloudflow-prod data: pgbouncer.ini: | [databases] cloudflow = host=postgresql.cloudflow-prod.svc.cluster.local port=5432 dbname=cloudflow [pgbouncer] listen_addr = 0.0.0.0 listen_port = 5432 auth_type = md5 auth_file = /etc/pgbouncer/userlist.txt pool_mode = transaction max_client_conn = 1000 default_pool_size = 25 reserve_pool_size = 5 reserve_pool_timeout = 3 max_db_connections = 100 max_user_connections = 100 server_lifetime = 3600 server_idle_timeout = 600 log_connections = 1 log_disconnections = 1 log_pooler_errors = 1 --- apiVersion: apps/v1 kind: Deployment metadata: name: pgbouncer namespace: cloudflow-prod spec: replicas: 2 selector: matchLabels: app: pgbouncer template: metadata: labels: app: pgbouncer spec: containers: - name: pgbouncer image: pgbouncer/pgbouncer:1.21.0 ports: - containerPort: 5432 resources: requests: cpu: 500m memory: 512Mi limits: cpu: 1000m memory: 1Gi volumeMounts: - name: config mountPath: /etc/pgbouncer volumes: - name: config configMap: name: pgbouncer-config --- apiVersion: v1 kind: Service metadata: name: pgbouncer namespace: cloudflow-prod spec: selector: app: pgbouncer ports: - port: 5432 targetPort: 5432 type: ClusterIP ``` Apply the PgBouncer configuration: ```bash kubectl apply -f pgbouncer.yaml ``` ### Database Migrations Run database migrations before deploying new versions: ```bash # Create migration job kubectl create job cloudflow-migrate-$(date +%s) \\ --namespace cloudflow-prod \\ --image=123456789012.dkr.ecr.us-east-1.amazonaws.com/cloudflow:2.4.0 \\ -- npm run migrate # Monitor migration progress kubectl logs -f job/cloudflow-migrate-<timestamp> -n cloudflow-prod ``` --- ## Monitoring and Observability ### Prometheus Setup Install Prometheus using the kube-prometheus-stack: ```bash helm install prometheus prometheus-community/kube-prometheus-stack \\ --namespace monitoring \\ --create-namespace \\ --set prometheus.prometheusSpec.serviceMonitorSelectorNilUsesHelmValues=false \\",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_0",
              "content": "# CloudFlow Platform - System Architecture Overview **Document Version:** 2.3.1 **Last Updated:** January 15, 2026 **Owner:** Platform Architecture Team **Status:** Production ## Executive Summary CloudFlow is a distributed, cloud-native workflow automation platform designed to orchestrate complex business processes at scale. The platform processes over 2.5 million workflow executions daily with an average P99 latency of 180ms for API operations and 4.2 seconds for workflow execution. This document provides a comprehensive overview of the system architecture, including microservices design, data flow patterns, infrastructure decisions, and operational considerations. ## Table of Contents 1. [High-Level Architecture](#high-level-architecture) 2. [Microservices Breakdown](#microservices-breakdown) 3. [Data Flow Architecture](#data-flow-architecture) 4. [Database Architecture](#database-architecture) 5. [Message Queue Patterns](#message-queue-patterns) 6. [Caching Strategy](#caching-strategy) 7. [Security Architecture](#security-architecture) 8. [Performance Characteristics](#performance-characteristics) 9. [Disaster Recovery](#disaster-recovery) --- ## High-Level Architecture CloudFlow follows a microservices architecture pattern deployed across multiple availability zones in AWS. The platform is designed with the following core principles: - **Scalability**: Horizontal scaling for all services with auto-scaling groups - **Resilience**: Circuit breakers, bulkheads, and graceful degradation - **Observability**: Distributed tracing, centralized logging, and comprehensive metrics - **Security**: Zero-trust network architecture with mTLS encryption ### Architecture Diagram (Conceptual) ``` \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 Load Balancer (ALB) \u2502 \u2502 (TLS Termination - 443) \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u25bc \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 API Gateway Layer \u2502 \u2502 (Rate Limiting, Auth, Request Routing) \u2502 \u2514\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u2502 \u2502 \u2502 \u2502 \u25bc \u25bc \u25bc \u25bc \u25bc \u250c\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502Auth\u2502 \u2502Workflow\u2502 \u2502Scheduler\u2502 \u2502Notification\u2502 \u2502User Service\u2502 \u2502Svc \u2502 \u2502 Engine \u2502 \u2502 Service \u2502 \u2502 Service \u2502 \u2502 \u2502 \u2514\u2500\u252c\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u2502 \u2502 \u2502 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 \u2502 \u2502 \u25bc \u25bc \u25bc \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502PostgreSQL\u2502 \u2502 Redis \u2502 \u2502 Kafka \u2502 \u2502 Cluster \u2502 \u2502 Cluster \u2502 \u2502 Cluster \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 ``` ### Technology Stack - **Runtime**: Node.js 20.x (API Gateway, Workflow Engine), Go 1.21 (Auth Service, Scheduler) - **Container Orchestration**: Kubernetes 1.28 (EKS) - **Service Mesh**: Istio 1.20 for service-to-service communication - **Databases**: PostgreSQL 15.4, Redis 7.2 - **Message Broker**: Apache Kafka 3.6 - **Monitoring**: Prometheus, Grafana, Jaeger for distributed tracing - **Secrets Management**: HashiCorp Vault 1.15 --- ## Microservices Breakdown ### API Gateway **Purpose**: Single entry point for all client requests, providing authentication, rate limiting, request routing, and protocol translation.",
              "score": null
            },
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_0",
              "content": "# CloudFlow Platform - Deployment and Operations Guide **Version:** 2.4.0 **Last Updated:** January 2026 **Target Environment:** Production (AWS EKS) ## Table of Contents 1. [Overview](#overview) 2. [Prerequisites](#prerequisites) 3. [Infrastructure Setup](#infrastructure-setup) 4. [Kubernetes Deployment](#kubernetes-deployment) 5. [Database Configuration](#database-configuration) 6. [Monitoring and Observability](#monitoring-and-observability) 7. [Backup and Disaster Recovery](#backup-and-disaster-recovery) 8. [Scaling and Performance](#scaling-and-performance) 9. [Security and Compliance](#security-and-compliance) 10. [Troubleshooting](#troubleshooting) --- ## Overview CloudFlow is a cloud-native workflow orchestration platform designed for high-availability production environments. This guide provides comprehensive instructions for deploying and operating CloudFlow on Amazon EKS (Elastic Kubernetes Service). ### Architecture Summary CloudFlow consists of the following components: - **API Server**: REST API for workflow management (Node.js/Express) - **Worker Service**: Background job processor (Node.js) - **Scheduler**: Cron-based task scheduler (Node.js) - **PostgreSQL**: Primary data store (version 14) - **Redis**: Cache and message queue (version 7.0) - **PgBouncer**: Database connection pooler ### Deployment Model - **Namespace**: `cloudflow-prod` - **Cluster**: AWS EKS 1.28 - **Region**: us-east-1 (primary), us-west-2 (disaster recovery) - **High Availability**: Multi-AZ deployment across 3 availability zones --- ## Prerequisites Before beginning the deployment process, ensure you have the following: ### Required Tools - `kubectl` (v1.28 or later) - `helm` (v3.12 or later) - `aws-cli` (v2.13 or later) - `eksctl` (v0.165 or later) - `terraform` (v1.6 or later) - for infrastructure provisioning ### Access Requirements - AWS account with appropriate IAM permissions - EKS cluster admin access - Container registry access (ECR) - Domain name and SSL certificates - Secrets management access (AWS Secrets Manager or Vault) ### Network Requirements - VPC with at least 3 public and 3 private subnets - NAT Gateway configured - Security groups allowing: - Ingress: HTTPS (443), HTTP (80) - Internal: PostgreSQL (5432), Redis (6379) - Monitoring: Prometheus (9090), Grafana (3000) --- ## Infrastructure Setup ### EKS Cluster Creation Use the following `eksctl` configuration to create the EKS cluster: ```yaml # cluster-config.yaml apiVersion: eksctl.io/v1alpha5 kind: ClusterConfig metadata: name: cloudflow-production region: us-east-1 version: \"1.28\" availabilityZones: - us-east-1a - us-east-1b - us-east-1c vpc: cidr: 10.0.0.0/16 nat: gateway: HighlyAvailable managedNodeGroups: - name: cloudflow-workers instanceType: m5.xlarge minSize: 3 maxSize: 10 desiredCapacity: 5 volumeSize: 100 privateNetworking: true labels: role: worker environment: production tags: Environment: production Application: cloudflow iam: withAddonPolicies: ebs: true efs: true albIngress: true cloudWatch: true cloudWatch: clusterLogging: enableTypes: - api - audit - authenticator - controllerManager - scheduler ``` Create the cluster: ```bash eksctl",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_10",
              "content": "database nc -zv cloudflow-db.internal.company.com 5432 # Trace route traceroute api.cloudflow.io # Capture packets tcpdump -i any -w /tmp/capture.pcap port 5432 ``` --- ## Escalation Procedures ### Severity Levels CloudFlow incidents are classified into four severity levels: #### SEV-1: Critical (P1) - **Definition:** Complete service outage or severe degradation affecting all users - **Examples:** - API returns 5xx errors for > 5 minutes - Database completely unavailable - Authentication system down - Data loss or corruption - **Response Time:** Immediate (< 15 minutes) - **Escalation:** Page on-call engineer immediately #### SEV-2: High (P2) - **Definition:** Major functionality impaired affecting multiple users - **Examples:** - Workflow execution success rate < 90% - Significant performance degradation (p95 latency > 10s) - Rate limiting affecting large customer segment - **Response Time:** < 1 hour - **Escalation:** Create incident ticket and notify on-call #### SEV-3: Medium (P3) - **Definition:** Partial functionality degraded affecting some users - **Examples:** - Intermittent failures for specific workflow types - Minor performance issues - Non-critical feature unavailable - **Response Time:** < 4 hours - **Escalation:** Create ticket, normal business hours support #### SEV-4: Low (P4) - **Definition:** Minor issues with minimal user impact - **Examples:** - Cosmetic issues - Documentation errors - Feature requests - **Response Time:** < 2 business days - **Escalation:** Standard support ticket ### Escalation Steps #### Step 1: Initial Assessment (0-5 minutes) ```bash # Run health check cloudflow health check --comprehensive # Check status page curl https://status.cloudflow.io/api/v1/status.json # Review recent changes cloudflow audit log --last 2h --event-type \"deployment,configuration\" # Check metrics dashboard cloudflow metrics dashboard --incident-mode ``` #### Step 2: Gather Information (5-15 minutes) Create incident document with: - Incident timestamp and duration - Affected services and endpoints - Error rates and user impact - Recent changes or deployments - Relevant log excerpts - Correlation IDs for failed requests ```bash # Generate incident report cloudflow debug incident-report \\ --start \"2026-01-24T10:30:00Z\" \\ --end \"2026-01-24T11:00:00Z\" \\ --output incident-report.md # Capture system snapshot cloudflow debug snapshot --output snapshot-$(date +%Y%m%d-%H%M%S).tar.gz ``` #### Step 3: Escalate Based on Severity **For SEV-1 (Critical):** ```bash # Page on-call engineer cloudflow incident create \\ --severity SEV-1 \\ --title \"Complete API outage\" \\ --description \"All API requests returning 503\" \\ --page-oncall # Notify status page cloudflow status update \\ --status \"major_outage\" \\ --message \"We are investigating a complete service",
              "score": null
            }
          ],
          "latency_ms": 958.179844001279,
          "query_id": "realistic_014",
          "dimension": "problem",
          "query": "Where can I view CloudFlow metrics and logs?"
        },
        {
          "key_facts": [
            "Prometheus",
            "Grafana",
            "Jaeger for distributed tracing"
          ],
          "found_facts": [
            "Prometheus",
            "Grafana",
            "Jaeger for distributed tracing"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "architecture_overview",
            "deployment_guide"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_4",
              "content": "--set prometheus.prometheusSpec.retention=30d \\ --set prometheus.prometheusSpec.storageSpec.volumeClaimTemplate.spec.resources.requests.storage=100Gi ``` CloudFlow exposes Prometheus metrics on port 9090 at the `/metrics` endpoint. The metrics include: - **HTTP Metrics**: Request rate, latency, error rate - **Database Metrics**: Connection pool status, query duration - **Worker Metrics**: Job queue size, processing time, failure rate - **System Metrics**: CPU usage, memory usage, heap statistics Example metrics exposed: ``` # HELP cloudflow_http_requests_total Total number of HTTP requests # TYPE cloudflow_http_requests_total counter cloudflow_http_requests_total{method=\"GET\",route=\"/api/workflows\",status=\"200\"} 15420 # HELP cloudflow_http_request_duration_seconds HTTP request duration in seconds # TYPE cloudflow_http_request_duration_seconds histogram cloudflow_http_request_duration_seconds_bucket{method=\"POST\",route=\"/api/workflows\",le=\"0.1\"} 8234 # HELP cloudflow_worker_jobs_processed_total Total number of jobs processed # TYPE cloudflow_worker_jobs_processed_total counter cloudflow_worker_jobs_processed_total{status=\"success\"} 45230 # HELP cloudflow_database_connections Current database connections # TYPE cloudflow_database_connections gauge cloudflow_database_connections{state=\"active\"} 23 ``` ### Grafana Dashboards Access Grafana to view CloudFlow dashboards: ```bash kubectl port-forward -n monitoring svc/prometheus-grafana 3000:80 ``` Import the CloudFlow dashboard (ID: 15847) or use the provided JSON template. Key dashboard panels include: 1. **API Performance**: Request rate, P95/P99 latency, error rate 2. **Resource Usage**: CPU, memory, disk I/O per pod 3. **Database Health**: Connection pool utilization, query performance 4. **Worker Status**: Queue depth, processing rate, job success/failure ratio 5. **System Overview**: Pod status, replica count, autoscaling events ### Logging with CloudWatch CloudFlow logs are automatically shipped to CloudWatch Logs.",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_0",
              "content": "# CloudFlow Platform - System Architecture Overview **Document Version:** 2.3.1 **Last Updated:** January 15, 2026 **Owner:** Platform Architecture Team **Status:** Production ## Executive Summary CloudFlow is a distributed, cloud-native workflow automation platform designed to orchestrate complex business processes at scale. The platform processes over 2.5 million workflow executions daily with an average P99 latency of 180ms for API operations and 4.2 seconds for workflow execution. This document provides a comprehensive overview of the system architecture, including microservices design, data flow patterns, infrastructure decisions, and operational considerations. ## Table of Contents 1. [High-Level Architecture](#high-level-architecture) 2. [Microservices Breakdown](#microservices-breakdown) 3. [Data Flow Architecture](#data-flow-architecture) 4. [Database Architecture](#database-architecture) 5. [Message Queue Patterns](#message-queue-patterns) 6. [Caching Strategy](#caching-strategy) 7. [Security Architecture](#security-architecture) 8. [Performance Characteristics](#performance-characteristics) 9. [Disaster Recovery](#disaster-recovery) --- ## High-Level Architecture CloudFlow follows a microservices architecture pattern deployed across multiple availability zones in AWS. The platform is designed with the following core principles: - **Scalability**: Horizontal scaling for all services with auto-scaling groups - **Resilience**: Circuit breakers, bulkheads, and graceful degradation - **Observability**: Distributed tracing, centralized logging, and comprehensive metrics - **Security**: Zero-trust network architecture with mTLS encryption ### Architecture Diagram (Conceptual) ``` \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 Load Balancer (ALB) \u2502 \u2502 (TLS Termination - 443) \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u25bc \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 API Gateway Layer \u2502 \u2502 (Rate Limiting, Auth, Request Routing) \u2502 \u2514\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u2502 \u2502 \u2502 \u2502 \u25bc \u25bc \u25bc \u25bc \u25bc \u250c\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502Auth\u2502 \u2502Workflow\u2502 \u2502Scheduler\u2502 \u2502Notification\u2502 \u2502User Service\u2502 \u2502Svc \u2502 \u2502 Engine \u2502 \u2502 Service \u2502 \u2502 Service \u2502 \u2502 \u2502 \u2514\u2500\u252c\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u2502 \u2502 \u2502 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 \u2502 \u2502 \u25bc \u25bc \u25bc \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502PostgreSQL\u2502 \u2502 Redis \u2502 \u2502 Kafka \u2502 \u2502 Cluster \u2502 \u2502 Cluster \u2502 \u2502 Cluster \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 ``` ### Technology Stack - **Runtime**: Node.js 20.x (API Gateway, Workflow Engine), Go 1.21 (Auth Service, Scheduler) - **Container Orchestration**: Kubernetes 1.28 (EKS) - **Service Mesh**: Istio 1.20 for service-to-service communication - **Databases**: PostgreSQL 15.4, Redis 7.2 - **Message Broker**: Apache Kafka 3.6 - **Monitoring**: Prometheus, Grafana, Jaeger for distributed tracing - **Secrets Management**: HashiCorp Vault 1.15 --- ## Microservices Breakdown ### API Gateway **Purpose**: Single entry point for all client requests, providing authentication, rate limiting, request routing, and protocol translation.",
              "score": null
            },
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_3",
              "content": "Deploy PostgreSQL using the Bitnami Helm chart: ```yaml # postgres-values.yaml global: postgresql: auth: username: cloudflow database: cloudflow existingSecret: postgres-credentials image: tag: \"14.10.0\" primary: resources: limits: cpu: 4000m memory: 8Gi requests: cpu: 2000m memory: 4Gi persistence: enabled: true size: 100Gi storageClass: gp3 extendedConfiguration: | max_connections = 100 shared_buffers = 2GB effective_cache_size = 6GB maintenance_work_mem = 512MB checkpoint_completion_target = 0.9 wal_buffers = 16MB default_statistics_target = 100 random_page_cost = 1.1 effective_io_concurrency = 200 work_mem = 10MB min_wal_size = 1GB max_wal_size = 4GB readReplicas: replicaCount: 2 resources: limits: cpu: 2000m memory: 4Gi requests: cpu: 1000m memory: 2Gi metrics: enabled: true serviceMonitor: enabled: true ``` Install PostgreSQL: ```bash # Create password secret kubectl create secret generic postgres-credentials \\ --namespace cloudflow-prod \\ --from-literal=postgres-password=\"$(openssl rand -base64 32)\" \\ --from-literal=password=\"$(openssl rand -base64 32)\" # Install PostgreSQL helm install postgresql bitnami/postgresql \\ --namespace cloudflow-prod \\ --values postgres-values.yaml \\ --wait ``` ### PgBouncer Configuration Deploy PgBouncer for connection pooling to handle up to 100 connections efficiently: ```yaml # pgbouncer.yaml apiVersion: v1 kind: ConfigMap metadata: name: pgbouncer-config namespace: cloudflow-prod data: pgbouncer.ini: | [databases] cloudflow = host=postgresql.cloudflow-prod.svc.cluster.local port=5432 dbname=cloudflow [pgbouncer] listen_addr = 0.0.0.0 listen_port = 5432 auth_type = md5 auth_file = /etc/pgbouncer/userlist.txt pool_mode = transaction max_client_conn = 1000 default_pool_size = 25 reserve_pool_size = 5 reserve_pool_timeout = 3 max_db_connections = 100 max_user_connections = 100 server_lifetime = 3600 server_idle_timeout = 600 log_connections = 1 log_disconnections = 1 log_pooler_errors = 1 --- apiVersion: apps/v1 kind: Deployment metadata: name: pgbouncer namespace: cloudflow-prod spec: replicas: 2 selector: matchLabels: app: pgbouncer template: metadata: labels: app: pgbouncer spec: containers: - name: pgbouncer image: pgbouncer/pgbouncer:1.21.0 ports: - containerPort: 5432 resources: requests: cpu: 500m memory: 512Mi limits: cpu: 1000m memory: 1Gi volumeMounts: - name: config mountPath: /etc/pgbouncer volumes: - name: config configMap: name: pgbouncer-config --- apiVersion: v1 kind: Service metadata: name: pgbouncer namespace: cloudflow-prod spec: selector: app: pgbouncer ports: - port: 5432 targetPort: 5432 type: ClusterIP ``` Apply the PgBouncer configuration: ```bash kubectl apply -f pgbouncer.yaml ``` ### Database Migrations Run database migrations before deploying new versions: ```bash # Create migration job kubectl create job cloudflow-migrate-$(date +%s) \\ --namespace cloudflow-prod \\ --image=123456789012.dkr.ecr.us-east-1.amazonaws.com/cloudflow:2.4.0 \\ -- npm run migrate # Monitor migration progress kubectl logs -f job/cloudflow-migrate-<timestamp> -n cloudflow-prod ``` --- ## Monitoring and Observability ### Prometheus Setup Install Prometheus using the kube-prometheus-stack: ```bash helm install prometheus prometheus-community/kube-prometheus-stack \\ --namespace monitoring \\ --create-namespace \\ --set prometheus.prometheusSpec.serviceMonitorSelectorNilUsesHelmValues=false \\",
              "score": null
            },
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_0",
              "content": "# CloudFlow Platform - Deployment and Operations Guide **Version:** 2.4.0 **Last Updated:** January 2026 **Target Environment:** Production (AWS EKS) ## Table of Contents 1. [Overview](#overview) 2. [Prerequisites](#prerequisites) 3. [Infrastructure Setup](#infrastructure-setup) 4. [Kubernetes Deployment](#kubernetes-deployment) 5. [Database Configuration](#database-configuration) 6. [Monitoring and Observability](#monitoring-and-observability) 7. [Backup and Disaster Recovery](#backup-and-disaster-recovery) 8. [Scaling and Performance](#scaling-and-performance) 9. [Security and Compliance](#security-and-compliance) 10. [Troubleshooting](#troubleshooting) --- ## Overview CloudFlow is a cloud-native workflow orchestration platform designed for high-availability production environments. This guide provides comprehensive instructions for deploying and operating CloudFlow on Amazon EKS (Elastic Kubernetes Service). ### Architecture Summary CloudFlow consists of the following components: - **API Server**: REST API for workflow management (Node.js/Express) - **Worker Service**: Background job processor (Node.js) - **Scheduler**: Cron-based task scheduler (Node.js) - **PostgreSQL**: Primary data store (version 14) - **Redis**: Cache and message queue (version 7.0) - **PgBouncer**: Database connection pooler ### Deployment Model - **Namespace**: `cloudflow-prod` - **Cluster**: AWS EKS 1.28 - **Region**: us-east-1 (primary), us-west-2 (disaster recovery) - **High Availability**: Multi-AZ deployment across 3 availability zones --- ## Prerequisites Before beginning the deployment process, ensure you have the following: ### Required Tools - `kubectl` (v1.28 or later) - `helm` (v3.12 or later) - `aws-cli` (v2.13 or later) - `eksctl` (v0.165 or later) - `terraform` (v1.6 or later) - for infrastructure provisioning ### Access Requirements - AWS account with appropriate IAM permissions - EKS cluster admin access - Container registry access (ECR) - Domain name and SSL certificates - Secrets management access (AWS Secrets Manager or Vault) ### Network Requirements - VPC with at least 3 public and 3 private subnets - NAT Gateway configured - Security groups allowing: - Ingress: HTTPS (443), HTTP (80) - Internal: PostgreSQL (5432), Redis (6379) - Monitoring: Prometheus (9090), Grafana (3000) --- ## Infrastructure Setup ### EKS Cluster Creation Use the following `eksctl` configuration to create the EKS cluster: ```yaml # cluster-config.yaml apiVersion: eksctl.io/v1alpha5 kind: ClusterConfig metadata: name: cloudflow-production region: us-east-1 version: \"1.28\" availabilityZones: - us-east-1a - us-east-1b - us-east-1c vpc: cidr: 10.0.0.0/16 nat: gateway: HighlyAvailable managedNodeGroups: - name: cloudflow-workers instanceType: m5.xlarge minSize: 3 maxSize: 10 desiredCapacity: 5 volumeSize: 100 privateNetworking: true labels: role: worker environment: production tags: Environment: production Application: cloudflow iam: withAddonPolicies: ebs: true efs: true albIngress: true cloudWatch: true cloudWatch: clusterLogging: enableTypes: - api - audit - authenticator - controllerManager - scheduler ``` Create the cluster: ```bash eksctl",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_10",
              "content": "- Execution start rate: 500 per second - Completion rate: 450 per second (average 2-second execution time) **Database**: - Read throughput: 50,000 queries per second (across replicas) - Write throughput: 15,000 transactions per second - Connection capacity: 2,000 concurrent connections **Kafka**: - Message ingestion: 100,000 messages per second - Consumer throughput: 80,000 messages per second (aggregated) - End-to-end latency: < 100ms (P99) ### Resource Utilization **CPU Utilization** (Target: 60-70% average): - API Gateway: 55% average, 80% peak - Workflow Engine: 65% average, 85% peak - Auth Service: 40% average, 60% peak **Memory Utilization** (Target: < 80%): - API Gateway: 2.5GB average per pod (4GB allocated) - Workflow Engine: 6GB average per pod (8GB allocated) - Notification Service: 2.8GB average per pod (4GB allocated) **Network Throughput**: - Ingress: 2 Gbps average, 5 Gbps peak - Egress: 1.5 Gbps average, 4 Gbps peak - Internal (service mesh): 8 Gbps average ### Monitoring & Alerting **Key Metrics** (Monitored via Prometheus): - Request rate (per service, per endpoint) - Error rate (4xx, 5xx responses) - Latency percentiles (P50, P95, P99) - Resource utilization (CPU, memory, disk) - Cache hit/miss ratios - Database connection pool saturation - Kafka consumer lag **Alerts** (via PagerDuty): - P1 (Immediate): API error rate > 1%, database connection failure - P2 (< 15 min): P99 latency > 500ms, cache hit rate < 80% - P3 (< 1 hour): Resource utilization > 85%, replica count < minimum **SLI/SLO/SLA**: - SLI: API success rate (non-5xx responses) - SLO: 99.9% uptime per month (43 minutes downtime allowance) - SLA: 99.5% uptime guarantee (customer-facing SLA with credits) --- ## Disaster Recovery ### Recovery Objectives **RPO (Recovery Point Objective): 1 hour** - Maximum acceptable data loss: 1 hour of transactions - Achieved through: Continuous database replication + hourly snapshots - Kafka retention: 7 days allows event replay **RTO (Recovery Time Objective): 4 hours** - Maximum acceptable downtime: 4 hours for full system recovery - Includes: Failover, data verification, and service restoration - Automated runbooks reduce RTO to < 2 hours for common scenarios ### High Availability Architecture **Multi-AZ Deployment**: ``` Region: us-east-1 AZ-1a: AZ-1b: AZ-1c: - API Gateway (4) - API Gateway (4) - API Gateway (4) - Workflow Engine (6) - Workflow Engine (5) - Workflow Engine (5) - Auth Service (3) - Auth Service (3) - Auth",
              "score": null
            }
          ],
          "latency_ms": 965.7825009999215,
          "query_id": "realistic_014",
          "dimension": "casual",
          "query": "monitoring stack"
        },
        {
          "key_facts": [
            "Prometheus",
            "Grafana",
            "Jaeger for distributed tracing"
          ],
          "found_facts": [
            "Prometheus",
            "Grafana",
            "Jaeger for distributed tracing"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "architecture_overview",
            "deployment_guide"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_4",
              "content": "--set prometheus.prometheusSpec.retention=30d \\ --set prometheus.prometheusSpec.storageSpec.volumeClaimTemplate.spec.resources.requests.storage=100Gi ``` CloudFlow exposes Prometheus metrics on port 9090 at the `/metrics` endpoint. The metrics include: - **HTTP Metrics**: Request rate, latency, error rate - **Database Metrics**: Connection pool status, query duration - **Worker Metrics**: Job queue size, processing time, failure rate - **System Metrics**: CPU usage, memory usage, heap statistics Example metrics exposed: ``` # HELP cloudflow_http_requests_total Total number of HTTP requests # TYPE cloudflow_http_requests_total counter cloudflow_http_requests_total{method=\"GET\",route=\"/api/workflows\",status=\"200\"} 15420 # HELP cloudflow_http_request_duration_seconds HTTP request duration in seconds # TYPE cloudflow_http_request_duration_seconds histogram cloudflow_http_request_duration_seconds_bucket{method=\"POST\",route=\"/api/workflows\",le=\"0.1\"} 8234 # HELP cloudflow_worker_jobs_processed_total Total number of jobs processed # TYPE cloudflow_worker_jobs_processed_total counter cloudflow_worker_jobs_processed_total{status=\"success\"} 45230 # HELP cloudflow_database_connections Current database connections # TYPE cloudflow_database_connections gauge cloudflow_database_connections{state=\"active\"} 23 ``` ### Grafana Dashboards Access Grafana to view CloudFlow dashboards: ```bash kubectl port-forward -n monitoring svc/prometheus-grafana 3000:80 ``` Import the CloudFlow dashboard (ID: 15847) or use the provided JSON template. Key dashboard panels include: 1. **API Performance**: Request rate, P95/P99 latency, error rate 2. **Resource Usage**: CPU, memory, disk I/O per pod 3. **Database Health**: Connection pool utilization, query performance 4. **Worker Status**: Queue depth, processing rate, job success/failure ratio 5. **System Overview**: Pod status, replica count, autoscaling events ### Logging with CloudWatch CloudFlow logs are automatically shipped to CloudWatch Logs.",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_0",
              "content": "# CloudFlow Platform - System Architecture Overview **Document Version:** 2.3.1 **Last Updated:** January 15, 2026 **Owner:** Platform Architecture Team **Status:** Production ## Executive Summary CloudFlow is a distributed, cloud-native workflow automation platform designed to orchestrate complex business processes at scale. The platform processes over 2.5 million workflow executions daily with an average P99 latency of 180ms for API operations and 4.2 seconds for workflow execution. This document provides a comprehensive overview of the system architecture, including microservices design, data flow patterns, infrastructure decisions, and operational considerations. ## Table of Contents 1. [High-Level Architecture](#high-level-architecture) 2. [Microservices Breakdown](#microservices-breakdown) 3. [Data Flow Architecture](#data-flow-architecture) 4. [Database Architecture](#database-architecture) 5. [Message Queue Patterns](#message-queue-patterns) 6. [Caching Strategy](#caching-strategy) 7. [Security Architecture](#security-architecture) 8. [Performance Characteristics](#performance-characteristics) 9. [Disaster Recovery](#disaster-recovery) --- ## High-Level Architecture CloudFlow follows a microservices architecture pattern deployed across multiple availability zones in AWS. The platform is designed with the following core principles: - **Scalability**: Horizontal scaling for all services with auto-scaling groups - **Resilience**: Circuit breakers, bulkheads, and graceful degradation - **Observability**: Distributed tracing, centralized logging, and comprehensive metrics - **Security**: Zero-trust network architecture with mTLS encryption ### Architecture Diagram (Conceptual) ``` \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 Load Balancer (ALB) \u2502 \u2502 (TLS Termination - 443) \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u25bc \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 API Gateway Layer \u2502 \u2502 (Rate Limiting, Auth, Request Routing) \u2502 \u2514\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u2502 \u2502 \u2502 \u2502 \u25bc \u25bc \u25bc \u25bc \u25bc \u250c\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502Auth\u2502 \u2502Workflow\u2502 \u2502Scheduler\u2502 \u2502Notification\u2502 \u2502User Service\u2502 \u2502Svc \u2502 \u2502 Engine \u2502 \u2502 Service \u2502 \u2502 Service \u2502 \u2502 \u2502 \u2514\u2500\u252c\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u2502 \u2502 \u2502 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 \u2502 \u2502 \u25bc \u25bc \u25bc \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502PostgreSQL\u2502 \u2502 Redis \u2502 \u2502 Kafka \u2502 \u2502 Cluster \u2502 \u2502 Cluster \u2502 \u2502 Cluster \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 ``` ### Technology Stack - **Runtime**: Node.js 20.x (API Gateway, Workflow Engine), Go 1.21 (Auth Service, Scheduler) - **Container Orchestration**: Kubernetes 1.28 (EKS) - **Service Mesh**: Istio 1.20 for service-to-service communication - **Databases**: PostgreSQL 15.4, Redis 7.2 - **Message Broker**: Apache Kafka 3.6 - **Monitoring**: Prometheus, Grafana, Jaeger for distributed tracing - **Secrets Management**: HashiCorp Vault 1.15 --- ## Microservices Breakdown ### API Gateway **Purpose**: Single entry point for all client requests, providing authentication, rate limiting, request routing, and protocol translation.",
              "score": null
            },
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_3",
              "content": "Deploy PostgreSQL using the Bitnami Helm chart: ```yaml # postgres-values.yaml global: postgresql: auth: username: cloudflow database: cloudflow existingSecret: postgres-credentials image: tag: \"14.10.0\" primary: resources: limits: cpu: 4000m memory: 8Gi requests: cpu: 2000m memory: 4Gi persistence: enabled: true size: 100Gi storageClass: gp3 extendedConfiguration: | max_connections = 100 shared_buffers = 2GB effective_cache_size = 6GB maintenance_work_mem = 512MB checkpoint_completion_target = 0.9 wal_buffers = 16MB default_statistics_target = 100 random_page_cost = 1.1 effective_io_concurrency = 200 work_mem = 10MB min_wal_size = 1GB max_wal_size = 4GB readReplicas: replicaCount: 2 resources: limits: cpu: 2000m memory: 4Gi requests: cpu: 1000m memory: 2Gi metrics: enabled: true serviceMonitor: enabled: true ``` Install PostgreSQL: ```bash # Create password secret kubectl create secret generic postgres-credentials \\ --namespace cloudflow-prod \\ --from-literal=postgres-password=\"$(openssl rand -base64 32)\" \\ --from-literal=password=\"$(openssl rand -base64 32)\" # Install PostgreSQL helm install postgresql bitnami/postgresql \\ --namespace cloudflow-prod \\ --values postgres-values.yaml \\ --wait ``` ### PgBouncer Configuration Deploy PgBouncer for connection pooling to handle up to 100 connections efficiently: ```yaml # pgbouncer.yaml apiVersion: v1 kind: ConfigMap metadata: name: pgbouncer-config namespace: cloudflow-prod data: pgbouncer.ini: | [databases] cloudflow = host=postgresql.cloudflow-prod.svc.cluster.local port=5432 dbname=cloudflow [pgbouncer] listen_addr = 0.0.0.0 listen_port = 5432 auth_type = md5 auth_file = /etc/pgbouncer/userlist.txt pool_mode = transaction max_client_conn = 1000 default_pool_size = 25 reserve_pool_size = 5 reserve_pool_timeout = 3 max_db_connections = 100 max_user_connections = 100 server_lifetime = 3600 server_idle_timeout = 600 log_connections = 1 log_disconnections = 1 log_pooler_errors = 1 --- apiVersion: apps/v1 kind: Deployment metadata: name: pgbouncer namespace: cloudflow-prod spec: replicas: 2 selector: matchLabels: app: pgbouncer template: metadata: labels: app: pgbouncer spec: containers: - name: pgbouncer image: pgbouncer/pgbouncer:1.21.0 ports: - containerPort: 5432 resources: requests: cpu: 500m memory: 512Mi limits: cpu: 1000m memory: 1Gi volumeMounts: - name: config mountPath: /etc/pgbouncer volumes: - name: config configMap: name: pgbouncer-config --- apiVersion: v1 kind: Service metadata: name: pgbouncer namespace: cloudflow-prod spec: selector: app: pgbouncer ports: - port: 5432 targetPort: 5432 type: ClusterIP ``` Apply the PgBouncer configuration: ```bash kubectl apply -f pgbouncer.yaml ``` ### Database Migrations Run database migrations before deploying new versions: ```bash # Create migration job kubectl create job cloudflow-migrate-$(date +%s) \\ --namespace cloudflow-prod \\ --image=123456789012.dkr.ecr.us-east-1.amazonaws.com/cloudflow:2.4.0 \\ -- npm run migrate # Monitor migration progress kubectl logs -f job/cloudflow-migrate-<timestamp> -n cloudflow-prod ``` --- ## Monitoring and Observability ### Prometheus Setup Install Prometheus using the kube-prometheus-stack: ```bash helm install prometheus prometheus-community/kube-prometheus-stack \\ --namespace monitoring \\ --create-namespace \\ --set prometheus.prometheusSpec.serviceMonitorSelectorNilUsesHelmValues=false \\",
              "score": null
            },
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_5",
              "content": "**Supported Events:** - `workflow.started` - `workflow.completed` - `workflow.failed` - `pipeline.completed` - `pipeline.failed` **Webhook Payload Example:** ```json { \"event\": \"workflow.completed\", \"timestamp\": \"2026-01-24T10:45:00Z\", \"data\": { \"workflow_id\": \"wf_8x7k2m9p\", \"execution_id\": \"exec_9k3m7n2q\", \"status\": \"completed\", \"duration_ms\": 3420, \"records_processed\": 1247 } } ``` Configure webhooks in your account settings or via the API: ```bash curl -X POST https://api.cloudflow.io/v2/webhooks \\ -H \"X-API-Key: YOUR_API_KEY\" \\ -H \"Content-Type: application/json\" \\ -d '{ \"url\": \"https://your-app.com/cloudflow-webhook\", \"events\": [\"workflow.completed\", \"workflow.failed\"], \"secret\": \"whsec_your_webhook_secret\" }' ``` ## Support For additional help and resources: - **Documentation:** https://docs.cloudflow.io - **API Status:** https://status.cloudflow.io - **Support Email:** support@cloudflow.io - **Community Forum:** https://community.cloudflow.io Enterprise customers have access to 24/7 priority support via phone and dedicated Slack channels.",
              "score": null
            },
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_0",
              "content": "# CloudFlow Platform - Deployment and Operations Guide **Version:** 2.4.0 **Last Updated:** January 2026 **Target Environment:** Production (AWS EKS) ## Table of Contents 1. [Overview](#overview) 2. [Prerequisites](#prerequisites) 3. [Infrastructure Setup](#infrastructure-setup) 4. [Kubernetes Deployment](#kubernetes-deployment) 5. [Database Configuration](#database-configuration) 6. [Monitoring and Observability](#monitoring-and-observability) 7. [Backup and Disaster Recovery](#backup-and-disaster-recovery) 8. [Scaling and Performance](#scaling-and-performance) 9. [Security and Compliance](#security-and-compliance) 10. [Troubleshooting](#troubleshooting) --- ## Overview CloudFlow is a cloud-native workflow orchestration platform designed for high-availability production environments. This guide provides comprehensive instructions for deploying and operating CloudFlow on Amazon EKS (Elastic Kubernetes Service). ### Architecture Summary CloudFlow consists of the following components: - **API Server**: REST API for workflow management (Node.js/Express) - **Worker Service**: Background job processor (Node.js) - **Scheduler**: Cron-based task scheduler (Node.js) - **PostgreSQL**: Primary data store (version 14) - **Redis**: Cache and message queue (version 7.0) - **PgBouncer**: Database connection pooler ### Deployment Model - **Namespace**: `cloudflow-prod` - **Cluster**: AWS EKS 1.28 - **Region**: us-east-1 (primary), us-west-2 (disaster recovery) - **High Availability**: Multi-AZ deployment across 3 availability zones --- ## Prerequisites Before beginning the deployment process, ensure you have the following: ### Required Tools - `kubectl` (v1.28 or later) - `helm` (v3.12 or later) - `aws-cli` (v2.13 or later) - `eksctl` (v0.165 or later) - `terraform` (v1.6 or later) - for infrastructure provisioning ### Access Requirements - AWS account with appropriate IAM permissions - EKS cluster admin access - Container registry access (ECR) - Domain name and SSL certificates - Secrets management access (AWS Secrets Manager or Vault) ### Network Requirements - VPC with at least 3 public and 3 private subnets - NAT Gateway configured - Security groups allowing: - Ingress: HTTPS (443), HTTP (80) - Internal: PostgreSQL (5432), Redis (6379) - Monitoring: Prometheus (9090), Grafana (3000) --- ## Infrastructure Setup ### EKS Cluster Creation Use the following `eksctl` configuration to create the EKS cluster: ```yaml # cluster-config.yaml apiVersion: eksctl.io/v1alpha5 kind: ClusterConfig metadata: name: cloudflow-production region: us-east-1 version: \"1.28\" availabilityZones: - us-east-1a - us-east-1b - us-east-1c vpc: cidr: 10.0.0.0/16 nat: gateway: HighlyAvailable managedNodeGroups: - name: cloudflow-workers instanceType: m5.xlarge minSize: 3 maxSize: 10 desiredCapacity: 5 volumeSize: 100 privateNetworking: true labels: role: worker environment: production tags: Environment: production Application: cloudflow iam: withAddonPolicies: ebs: true efs: true albIngress: true cloudWatch: true cloudWatch: clusterLogging: enableTypes: - api - audit - authenticator - controllerManager - scheduler ``` Create the cluster: ```bash eksctl",
              "score": null
            }
          ],
          "latency_ms": 1015.5813029996352,
          "query_id": "realistic_014",
          "dimension": "contextual",
          "query": "I need to set up dashboards, what monitoring systems are available?"
        },
        {
          "key_facts": [
            "Prometheus",
            "Grafana",
            "Jaeger for distributed tracing"
          ],
          "found_facts": [
            "Prometheus",
            "Grafana",
            "Jaeger for distributed tracing"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "architecture_overview",
            "deployment_guide"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_4",
              "content": "--set prometheus.prometheusSpec.retention=30d \\ --set prometheus.prometheusSpec.storageSpec.volumeClaimTemplate.spec.resources.requests.storage=100Gi ``` CloudFlow exposes Prometheus metrics on port 9090 at the `/metrics` endpoint. The metrics include: - **HTTP Metrics**: Request rate, latency, error rate - **Database Metrics**: Connection pool status, query duration - **Worker Metrics**: Job queue size, processing time, failure rate - **System Metrics**: CPU usage, memory usage, heap statistics Example metrics exposed: ``` # HELP cloudflow_http_requests_total Total number of HTTP requests # TYPE cloudflow_http_requests_total counter cloudflow_http_requests_total{method=\"GET\",route=\"/api/workflows\",status=\"200\"} 15420 # HELP cloudflow_http_request_duration_seconds HTTP request duration in seconds # TYPE cloudflow_http_request_duration_seconds histogram cloudflow_http_request_duration_seconds_bucket{method=\"POST\",route=\"/api/workflows\",le=\"0.1\"} 8234 # HELP cloudflow_worker_jobs_processed_total Total number of jobs processed # TYPE cloudflow_worker_jobs_processed_total counter cloudflow_worker_jobs_processed_total{status=\"success\"} 45230 # HELP cloudflow_database_connections Current database connections # TYPE cloudflow_database_connections gauge cloudflow_database_connections{state=\"active\"} 23 ``` ### Grafana Dashboards Access Grafana to view CloudFlow dashboards: ```bash kubectl port-forward -n monitoring svc/prometheus-grafana 3000:80 ``` Import the CloudFlow dashboard (ID: 15847) or use the provided JSON template. Key dashboard panels include: 1. **API Performance**: Request rate, P95/P99 latency, error rate 2. **Resource Usage**: CPU, memory, disk I/O per pod 3. **Database Health**: Connection pool utilization, query performance 4. **Worker Status**: Queue depth, processing rate, job success/failure ratio 5. **System Overview**: Pod status, replica count, autoscaling events ### Logging with CloudWatch CloudFlow logs are automatically shipped to CloudWatch Logs.",
              "score": null
            },
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_3",
              "content": "Deploy PostgreSQL using the Bitnami Helm chart: ```yaml # postgres-values.yaml global: postgresql: auth: username: cloudflow database: cloudflow existingSecret: postgres-credentials image: tag: \"14.10.0\" primary: resources: limits: cpu: 4000m memory: 8Gi requests: cpu: 2000m memory: 4Gi persistence: enabled: true size: 100Gi storageClass: gp3 extendedConfiguration: | max_connections = 100 shared_buffers = 2GB effective_cache_size = 6GB maintenance_work_mem = 512MB checkpoint_completion_target = 0.9 wal_buffers = 16MB default_statistics_target = 100 random_page_cost = 1.1 effective_io_concurrency = 200 work_mem = 10MB min_wal_size = 1GB max_wal_size = 4GB readReplicas: replicaCount: 2 resources: limits: cpu: 2000m memory: 4Gi requests: cpu: 1000m memory: 2Gi metrics: enabled: true serviceMonitor: enabled: true ``` Install PostgreSQL: ```bash # Create password secret kubectl create secret generic postgres-credentials \\ --namespace cloudflow-prod \\ --from-literal=postgres-password=\"$(openssl rand -base64 32)\" \\ --from-literal=password=\"$(openssl rand -base64 32)\" # Install PostgreSQL helm install postgresql bitnami/postgresql \\ --namespace cloudflow-prod \\ --values postgres-values.yaml \\ --wait ``` ### PgBouncer Configuration Deploy PgBouncer for connection pooling to handle up to 100 connections efficiently: ```yaml # pgbouncer.yaml apiVersion: v1 kind: ConfigMap metadata: name: pgbouncer-config namespace: cloudflow-prod data: pgbouncer.ini: | [databases] cloudflow = host=postgresql.cloudflow-prod.svc.cluster.local port=5432 dbname=cloudflow [pgbouncer] listen_addr = 0.0.0.0 listen_port = 5432 auth_type = md5 auth_file = /etc/pgbouncer/userlist.txt pool_mode = transaction max_client_conn = 1000 default_pool_size = 25 reserve_pool_size = 5 reserve_pool_timeout = 3 max_db_connections = 100 max_user_connections = 100 server_lifetime = 3600 server_idle_timeout = 600 log_connections = 1 log_disconnections = 1 log_pooler_errors = 1 --- apiVersion: apps/v1 kind: Deployment metadata: name: pgbouncer namespace: cloudflow-prod spec: replicas: 2 selector: matchLabels: app: pgbouncer template: metadata: labels: app: pgbouncer spec: containers: - name: pgbouncer image: pgbouncer/pgbouncer:1.21.0 ports: - containerPort: 5432 resources: requests: cpu: 500m memory: 512Mi limits: cpu: 1000m memory: 1Gi volumeMounts: - name: config mountPath: /etc/pgbouncer volumes: - name: config configMap: name: pgbouncer-config --- apiVersion: v1 kind: Service metadata: name: pgbouncer namespace: cloudflow-prod spec: selector: app: pgbouncer ports: - port: 5432 targetPort: 5432 type: ClusterIP ``` Apply the PgBouncer configuration: ```bash kubectl apply -f pgbouncer.yaml ``` ### Database Migrations Run database migrations before deploying new versions: ```bash # Create migration job kubectl create job cloudflow-migrate-$(date +%s) \\ --namespace cloudflow-prod \\ --image=123456789012.dkr.ecr.us-east-1.amazonaws.com/cloudflow:2.4.0 \\ -- npm run migrate # Monitor migration progress kubectl logs -f job/cloudflow-migrate-<timestamp> -n cloudflow-prod ``` --- ## Monitoring and Observability ### Prometheus Setup Install Prometheus using the kube-prometheus-stack: ```bash helm install prometheus prometheus-community/kube-prometheus-stack \\ --namespace monitoring \\ --create-namespace \\ --set prometheus.prometheusSpec.serviceMonitorSelectorNilUsesHelmValues=false \\",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_0",
              "content": "# CloudFlow Platform - System Architecture Overview **Document Version:** 2.3.1 **Last Updated:** January 15, 2026 **Owner:** Platform Architecture Team **Status:** Production ## Executive Summary CloudFlow is a distributed, cloud-native workflow automation platform designed to orchestrate complex business processes at scale. The platform processes over 2.5 million workflow executions daily with an average P99 latency of 180ms for API operations and 4.2 seconds for workflow execution. This document provides a comprehensive overview of the system architecture, including microservices design, data flow patterns, infrastructure decisions, and operational considerations. ## Table of Contents 1. [High-Level Architecture](#high-level-architecture) 2. [Microservices Breakdown](#microservices-breakdown) 3. [Data Flow Architecture](#data-flow-architecture) 4. [Database Architecture](#database-architecture) 5. [Message Queue Patterns](#message-queue-patterns) 6. [Caching Strategy](#caching-strategy) 7. [Security Architecture](#security-architecture) 8. [Performance Characteristics](#performance-characteristics) 9. [Disaster Recovery](#disaster-recovery) --- ## High-Level Architecture CloudFlow follows a microservices architecture pattern deployed across multiple availability zones in AWS. The platform is designed with the following core principles: - **Scalability**: Horizontal scaling for all services with auto-scaling groups - **Resilience**: Circuit breakers, bulkheads, and graceful degradation - **Observability**: Distributed tracing, centralized logging, and comprehensive metrics - **Security**: Zero-trust network architecture with mTLS encryption ### Architecture Diagram (Conceptual) ``` \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 Load Balancer (ALB) \u2502 \u2502 (TLS Termination - 443) \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u25bc \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 API Gateway Layer \u2502 \u2502 (Rate Limiting, Auth, Request Routing) \u2502 \u2514\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u2502 \u2502 \u2502 \u2502 \u25bc \u25bc \u25bc \u25bc \u25bc \u250c\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502Auth\u2502 \u2502Workflow\u2502 \u2502Scheduler\u2502 \u2502Notification\u2502 \u2502User Service\u2502 \u2502Svc \u2502 \u2502 Engine \u2502 \u2502 Service \u2502 \u2502 Service \u2502 \u2502 \u2502 \u2514\u2500\u252c\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u2502 \u2502 \u2502 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 \u2502 \u2502 \u25bc \u25bc \u25bc \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502PostgreSQL\u2502 \u2502 Redis \u2502 \u2502 Kafka \u2502 \u2502 Cluster \u2502 \u2502 Cluster \u2502 \u2502 Cluster \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 ``` ### Technology Stack - **Runtime**: Node.js 20.x (API Gateway, Workflow Engine), Go 1.21 (Auth Service, Scheduler) - **Container Orchestration**: Kubernetes 1.28 (EKS) - **Service Mesh**: Istio 1.20 for service-to-service communication - **Databases**: PostgreSQL 15.4, Redis 7.2 - **Message Broker**: Apache Kafka 3.6 - **Monitoring**: Prometheus, Grafana, Jaeger for distributed tracing - **Secrets Management**: HashiCorp Vault 1.15 --- ## Microservices Breakdown ### API Gateway **Purpose**: Single entry point for all client requests, providing authentication, rate limiting, request routing, and protocol translation.",
              "score": null
            },
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_0",
              "content": "# CloudFlow Platform - Deployment and Operations Guide **Version:** 2.4.0 **Last Updated:** January 2026 **Target Environment:** Production (AWS EKS) ## Table of Contents 1. [Overview](#overview) 2. [Prerequisites](#prerequisites) 3. [Infrastructure Setup](#infrastructure-setup) 4. [Kubernetes Deployment](#kubernetes-deployment) 5. [Database Configuration](#database-configuration) 6. [Monitoring and Observability](#monitoring-and-observability) 7. [Backup and Disaster Recovery](#backup-and-disaster-recovery) 8. [Scaling and Performance](#scaling-and-performance) 9. [Security and Compliance](#security-and-compliance) 10. [Troubleshooting](#troubleshooting) --- ## Overview CloudFlow is a cloud-native workflow orchestration platform designed for high-availability production environments. This guide provides comprehensive instructions for deploying and operating CloudFlow on Amazon EKS (Elastic Kubernetes Service). ### Architecture Summary CloudFlow consists of the following components: - **API Server**: REST API for workflow management (Node.js/Express) - **Worker Service**: Background job processor (Node.js) - **Scheduler**: Cron-based task scheduler (Node.js) - **PostgreSQL**: Primary data store (version 14) - **Redis**: Cache and message queue (version 7.0) - **PgBouncer**: Database connection pooler ### Deployment Model - **Namespace**: `cloudflow-prod` - **Cluster**: AWS EKS 1.28 - **Region**: us-east-1 (primary), us-west-2 (disaster recovery) - **High Availability**: Multi-AZ deployment across 3 availability zones --- ## Prerequisites Before beginning the deployment process, ensure you have the following: ### Required Tools - `kubectl` (v1.28 or later) - `helm` (v3.12 or later) - `aws-cli` (v2.13 or later) - `eksctl` (v0.165 or later) - `terraform` (v1.6 or later) - for infrastructure provisioning ### Access Requirements - AWS account with appropriate IAM permissions - EKS cluster admin access - Container registry access (ECR) - Domain name and SSL certificates - Secrets management access (AWS Secrets Manager or Vault) ### Network Requirements - VPC with at least 3 public and 3 private subnets - NAT Gateway configured - Security groups allowing: - Ingress: HTTPS (443), HTTP (80) - Internal: PostgreSQL (5432), Redis (6379) - Monitoring: Prometheus (9090), Grafana (3000) --- ## Infrastructure Setup ### EKS Cluster Creation Use the following `eksctl` configuration to create the EKS cluster: ```yaml # cluster-config.yaml apiVersion: eksctl.io/v1alpha5 kind: ClusterConfig metadata: name: cloudflow-production region: us-east-1 version: \"1.28\" availabilityZones: - us-east-1a - us-east-1b - us-east-1c vpc: cidr: 10.0.0.0/16 nat: gateway: HighlyAvailable managedNodeGroups: - name: cloudflow-workers instanceType: m5.xlarge minSize: 3 maxSize: 10 desiredCapacity: 5 volumeSize: 100 privateNetworking: true labels: role: worker environment: production tags: Environment: production Application: cloudflow iam: withAddonPolicies: ebs: true efs: true albIngress: true cloudWatch: true cloudWatch: clusterLogging: enableTypes: - api - audit - authenticator - controllerManager - scheduler ``` Create the cluster: ```bash eksctl",
              "score": null
            },
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_1",
              "content": "create cluster -f cluster-config.yaml ``` ### Storage Configuration Install the EBS CSI driver for persistent volumes: ```bash eksctl create addon --name aws-ebs-csi-driver \\ --cluster cloudflow-production \\ --service-account-role-arn arn:aws:iam::ACCOUNT_ID:role/AmazonEKS_EBS_CSI_DriverRole \\ --force ``` --- ## Kubernetes Deployment ### Namespace Setup Create the CloudFlow namespace and configure resource quotas: ```yaml # namespace.yaml apiVersion: v1 kind: Namespace metadata: name: cloudflow-prod labels: name: cloudflow-prod environment: production --- apiVersion: v1 kind: ResourceQuota metadata: name: cloudflow-quota namespace: cloudflow-prod spec: hard: requests.cpu: \"50\" requests.memory: 100Gi persistentvolumeclaims: \"10\" services.loadbalancers: \"2\" ``` Apply the namespace configuration: ```bash kubectl apply -f namespace.yaml ``` ### Environment Configuration CloudFlow requires the following environment variables: | Variable | Description | Example | Required | |----------|-------------|---------|----------| | `DATABASE_URL` | PostgreSQL connection string | `postgresql://user:pass@host:5432/cloudflow` | Yes | | `REDIS_URL` | Redis connection string | `redis://redis-master.cloudflow-prod.svc.cluster.local:6379` | Yes | | `JWT_SECRET` | Secret key for JWT token signing | `<generated-secret-256-bit>` | Yes | | `LOG_LEVEL` | Application log verbosity | `info`, `debug`, `warn`, `error` | No (default: `info`) | | `API_PORT` | API server port | `3000` | No (default: `3000`) | | `WORKER_CONCURRENCY` | Number of concurrent workers | `10` | No (default: `5`) | | `SESSION_SECRET` | Session encryption key | `<generated-secret-256-bit>` | Yes | | `AWS_REGION` | AWS region for services | `us-east-1` | Yes | | `S3_BUCKET` | S3 bucket for file storage | `cloudflow-prod-storage` | Yes | | `SMTP_HOST` | Email server hostname | `smtp.sendgrid.net` | No | | `SMTP_PORT` | Email server port | `587` | No | | `METRICS_ENABLED` | Enable Prometheus metrics | `true` | No (default: `false`) | Create a Kubernetes secret for sensitive variables: ```bash kubectl create secret generic cloudflow-secrets \\ --namespace cloudflow-prod \\ --from-literal=DATABASE_URL=\"postgresql://cloudflow:$(cat db-password.txt)@pgbouncer.cloudflow-prod.svc.cluster.local:5432/cloudflow\" \\ --from-literal=REDIS_URL=\"redis://redis-master.cloudflow-prod.svc.cluster.local:6379\" \\ --from-literal=JWT_SECRET=\"$(openssl rand -base64 32)\" \\ --from-literal=SESSION_SECRET=\"$(openssl rand -base64 32)\" ``` ### Helm Chart Deployment Add the CloudFlow Helm repository: ```bash helm repo add cloudflow https://charts.cloudflow.io helm repo update ``` Create a values file for production configuration: ```yaml # values-production.yaml replicaCount: 3 image: repository: 123456789012.dkr.ecr.us-east-1.amazonaws.com/cloudflow tag: \"2.4.0\" pullPolicy: IfNotPresent resources: limits: cpu: 2000m memory: 4Gi requests: cpu: 2000m memory: 4Gi autoscaling: enabled: true minReplicas: 3 maxReplicas: 10 targetCPUUtilizationPercentage: 70 targetMemoryUtilizationPercentage: 80 service: type: ClusterIP port: 80 targetPort: 3000 annotations: service.beta.kubernetes.io/aws-load-balancer-type: \"nlb\" ingress: enabled: true className: nginx annotations: cert-manager.io/cluster-issuer: \"letsencrypt-prod\" nginx.ingress.kubernetes.io/ssl-redirect: \"true\" hosts: - host: api.cloudflow.io paths: - path: / pathType: Prefix tls:",
              "score": null
            }
          ],
          "latency_ms": 1001.0312560007151,
          "query_id": "realistic_014",
          "dimension": "negation",
          "query": "Why can't I see metrics in Datadog?"
        },
        {
          "key_facts": [
            "could not obtain connection from pool within 5000ms",
            "connection pool exhausted (100/100 connections in use)",
            "PgBouncer",
            "max_db_connections = 100"
          ],
          "found_facts": [
            "could not obtain connection from pool within 5000ms",
            "connection pool exhausted (100/100 connections in use)",
            "PgBouncer",
            "max_db_connections = 100"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "troubleshooting_guide",
            "deployment_guide"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_4",
              "content": "--set prometheus.prometheusSpec.retention=30d \\ --set prometheus.prometheusSpec.storageSpec.volumeClaimTemplate.spec.resources.requests.storage=100Gi ``` CloudFlow exposes Prometheus metrics on port 9090 at the `/metrics` endpoint. The metrics include: - **HTTP Metrics**: Request rate, latency, error rate - **Database Metrics**: Connection pool status, query duration - **Worker Metrics**: Job queue size, processing time, failure rate - **System Metrics**: CPU usage, memory usage, heap statistics Example metrics exposed: ``` # HELP cloudflow_http_requests_total Total number of HTTP requests # TYPE cloudflow_http_requests_total counter cloudflow_http_requests_total{method=\"GET\",route=\"/api/workflows\",status=\"200\"} 15420 # HELP cloudflow_http_request_duration_seconds HTTP request duration in seconds # TYPE cloudflow_http_request_duration_seconds histogram cloudflow_http_request_duration_seconds_bucket{method=\"POST\",route=\"/api/workflows\",le=\"0.1\"} 8234 # HELP cloudflow_worker_jobs_processed_total Total number of jobs processed # TYPE cloudflow_worker_jobs_processed_total counter cloudflow_worker_jobs_processed_total{status=\"success\"} 45230 # HELP cloudflow_database_connections Current database connections # TYPE cloudflow_database_connections gauge cloudflow_database_connections{state=\"active\"} 23 ``` ### Grafana Dashboards Access Grafana to view CloudFlow dashboards: ```bash kubectl port-forward -n monitoring svc/prometheus-grafana 3000:80 ``` Import the CloudFlow dashboard (ID: 15847) or use the provided JSON template. Key dashboard panels include: 1. **API Performance**: Request rate, P95/P99 latency, error rate 2. **Resource Usage**: CPU, memory, disk I/O per pod 3. **Database Health**: Connection pool utilization, query performance 4. **Worker Status**: Queue depth, processing rate, job success/failure ratio 5. **System Overview**: Pod status, replica count, autoscaling events ### Logging with CloudWatch CloudFlow logs are automatically shipped to CloudWatch Logs.",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_3",
              "content": "Workflow Context Accumulation** Large workflow executions may accumulate state in memory. **Solution:** ```bash # Configure context cleanup cloudflow config set workflow.context.max_size_mb 100 cloudflow config set workflow.context.cleanup_threshold 0.8 # Enable context persistence to disk cloudflow config set workflow.context.persistence.enabled true cloudflow config set workflow.context.persistence.backend s3 ``` **2. Connection Pool Leaks** **Diagnosis:** ```bash # Check active connections cloudflow db connections --verbose # Expected output: # Active: 45/100 # Idle: 23 # Waiting: 2 # Average age: 245s ``` **Resolution:** ```bash # Adjust connection pool settings cloudflow config set db.pool.max_connections 100 cloudflow config set db.pool.min_connections 10 cloudflow config set db.pool.idle_timeout 300 cloudflow config set db.pool.max_lifetime 1800 # Force connection pool reset cloudflow db pool reset --confirm ``` **3. Event Stream Buffers** Unbounded event buffers can cause memory exhaustion. ```bash # Configure event buffer limits cloudflow config set events.buffer.max_size 10000 cloudflow config set events.buffer.overflow_strategy drop_oldest # Enable event streaming to external sink cloudflow config set events.sink.type kafka cloudflow config set events.sink.kafka.brokers \"kafka-1:9092,kafka-2:9092\" cloudflow config set events.sink.kafka.topic cloudflow-events ``` --- ## Database Connection Issues ### Connection Pool Exhaustion #### Symptoms - Error: `could not obtain connection from pool within 5000ms` - Error: `connection pool exhausted (100/100 connections in use)` - API requests fail with `503 Service Unavailable` - Database CPU usage normal, but connection count at maximum #### Investigation ```bash # Check current connection pool status cloudflow db pool status --detailed # Output example: # Pool Statistics: # Total Connections: 100/100 (100%) # Active: 87 # Idle: 13 # Waiting Requests: 45 # Average Wait Time: 3420ms # Max Wait Time: 8234ms # Identify long-running queries cloudflow db queries --status running --min-duration 30000 # Check connection distribution by client SELECT application_name, state, COUNT(*) as conn_count, AVG(EXTRACT(EPOCH FROM (NOW() - state_change))) as avg_duration_sec FROM pg_stat_activity WHERE datname = 'cloudflow_production' GROUP BY application_name, state ORDER BY conn_count DESC; ``` #### Resolution **Immediate mitigation:** ```bash # Temporarily increase connection limit (requires database restart) cloudflow db config set max_connections 150 # Kill idle connections older than 5 minutes cloudflow db connections kill --idle-timeout 300 # Restart connection pool without downtime kubectl rollout restart deployment/cloudflow-api -n cloudflow kubectl rollout status deployment/cloudflow-api -n cloudflow ``` **Long-term solutions:** 1.",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_0",
              "content": "# CloudFlow Platform - System Architecture Overview **Document Version:** 2.3.1 **Last Updated:** January 15, 2026 **Owner:** Platform Architecture Team **Status:** Production ## Executive Summary CloudFlow is a distributed, cloud-native workflow automation platform designed to orchestrate complex business processes at scale. The platform processes over 2.5 million workflow executions daily with an average P99 latency of 180ms for API operations and 4.2 seconds for workflow execution. This document provides a comprehensive overview of the system architecture, including microservices design, data flow patterns, infrastructure decisions, and operational considerations. ## Table of Contents 1. [High-Level Architecture](#high-level-architecture) 2. [Microservices Breakdown](#microservices-breakdown) 3. [Data Flow Architecture](#data-flow-architecture) 4. [Database Architecture](#database-architecture) 5. [Message Queue Patterns](#message-queue-patterns) 6. [Caching Strategy](#caching-strategy) 7. [Security Architecture](#security-architecture) 8. [Performance Characteristics](#performance-characteristics) 9. [Disaster Recovery](#disaster-recovery) --- ## High-Level Architecture CloudFlow follows a microservices architecture pattern deployed across multiple availability zones in AWS. The platform is designed with the following core principles: - **Scalability**: Horizontal scaling for all services with auto-scaling groups - **Resilience**: Circuit breakers, bulkheads, and graceful degradation - **Observability**: Distributed tracing, centralized logging, and comprehensive metrics - **Security**: Zero-trust network architecture with mTLS encryption ### Architecture Diagram (Conceptual) ``` \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 Load Balancer (ALB) \u2502 \u2502 (TLS Termination - 443) \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u25bc \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 API Gateway Layer \u2502 \u2502 (Rate Limiting, Auth, Request Routing) \u2502 \u2514\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u2502 \u2502 \u2502 \u2502 \u25bc \u25bc \u25bc \u25bc \u25bc \u250c\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502Auth\u2502 \u2502Workflow\u2502 \u2502Scheduler\u2502 \u2502Notification\u2502 \u2502User Service\u2502 \u2502Svc \u2502 \u2502 Engine \u2502 \u2502 Service \u2502 \u2502 Service \u2502 \u2502 \u2502 \u2514\u2500\u252c\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u2502 \u2502 \u2502 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 \u2502 \u2502 \u25bc \u25bc \u25bc \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502PostgreSQL\u2502 \u2502 Redis \u2502 \u2502 Kafka \u2502 \u2502 Cluster \u2502 \u2502 Cluster \u2502 \u2502 Cluster \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 ``` ### Technology Stack - **Runtime**: Node.js 20.x (API Gateway, Workflow Engine), Go 1.21 (Auth Service, Scheduler) - **Container Orchestration**: Kubernetes 1.28 (EKS) - **Service Mesh**: Istio 1.20 for service-to-service communication - **Databases**: PostgreSQL 15.4, Redis 7.2 - **Message Broker**: Apache Kafka 3.6 - **Monitoring**: Prometheus, Grafana, Jaeger for distributed tracing - **Secrets Management**: HashiCorp Vault 1.15 --- ## Microservices Breakdown ### API Gateway **Purpose**: Single entry point for all client requests, providing authentication, rate limiting, request routing, and protocol translation.",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_5",
              "content": "Success/failure events published back to Kafka ``` **Event Schema**: ```json { \"event_id\": \"uuid-v4\", \"event_type\": \"workflow.execution.completed\", \"timestamp\": \"2026-01-15T10:30:00.000Z\", \"correlation_id\": \"request-trace-id\", \"payload\": { \"workflow_id\": \"wf-12345\", \"execution_id\": \"exec-67890\", \"status\": \"COMPLETED\", \"duration_ms\": 4230 }, \"metadata\": { \"source_service\": \"workflow-engine\", \"schema_version\": \"1.0\" } } ``` ### Inter-Service Communication Services communicate using two primary patterns: **Synchronous (gRPC)**: - Auth Service validation calls from API Gateway - User Service profile lookups - Low latency requirement (< 50ms) - Request-response pattern with circuit breaker **Asynchronous (Kafka)**: - Workflow execution events - Notification triggers - Audit log events - Eventual consistency acceptable --- ## Database Architecture ### PostgreSQL Primary Database **Cluster Configuration**: - Primary-replica setup with 1 primary + 2 read replicas - Instance type: db.r6g.2xlarge (8 vCPU, 64GB RAM) - Storage: 2TB gp3 SSD with 12,000 IOPS - Multi-AZ deployment for high availability - Automated backups: Daily snapshots, 30-day retention - Point-in-time recovery: 5-minute granularity **Database Schema Design**: ``` Core Tables: - users (5M rows): User accounts and profiles - workflows (2M rows): Workflow definitions - workflow_executions (500M rows, partitioned): Execution history - workflow_steps (2B rows, partitioned): Individual step records - schedules (100K rows): Scheduled workflow triggers - notifications (1B rows, partitioned): Notification delivery log Partitioning Strategy: - workflow_executions: Monthly partitions by created_at - workflow_steps: Monthly partitions by created_at - notifications: Weekly partitions by created_at - Automatic partition creation via cron job - Retention policy: Drop partitions older than 12 months ``` **Indexing Strategy**: - Primary keys: UUIDs with B-tree indexes - Frequently queried columns: Compound indexes (e.g., user_id + status + created_at) - JSONB columns: GIN indexes for workflow definitions - Full-text search: GiST indexes on description fields **Connection Pooling**: - PgBouncer in transaction mode - Pool size: 100 connections per service - Max client connections: 2000 - Connection timeout: 30 seconds **Performance Characteristics**: - Read query P99: < 20ms - Write query P99: < 50ms - Transaction throughput: 15,000 TPS - Replication lag: < 500ms --- ### Redis Caching Layer **Cluster Configuration**: - Redis Cluster with 6 nodes (3 primary + 3 replica) - Instance type: cache.r6g.xlarge (4 vCPU, 26GB RAM) - Total memory: 78GB usable cache space - Persistence: RDB snapshots every 5 minutes + AOF - Eviction policy: allkeys-lru **Cache Usage Patterns**: 1.",
              "score": null
            },
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_3",
              "content": "Deploy PostgreSQL using the Bitnami Helm chart: ```yaml # postgres-values.yaml global: postgresql: auth: username: cloudflow database: cloudflow existingSecret: postgres-credentials image: tag: \"14.10.0\" primary: resources: limits: cpu: 4000m memory: 8Gi requests: cpu: 2000m memory: 4Gi persistence: enabled: true size: 100Gi storageClass: gp3 extendedConfiguration: | max_connections = 100 shared_buffers = 2GB effective_cache_size = 6GB maintenance_work_mem = 512MB checkpoint_completion_target = 0.9 wal_buffers = 16MB default_statistics_target = 100 random_page_cost = 1.1 effective_io_concurrency = 200 work_mem = 10MB min_wal_size = 1GB max_wal_size = 4GB readReplicas: replicaCount: 2 resources: limits: cpu: 2000m memory: 4Gi requests: cpu: 1000m memory: 2Gi metrics: enabled: true serviceMonitor: enabled: true ``` Install PostgreSQL: ```bash # Create password secret kubectl create secret generic postgres-credentials \\ --namespace cloudflow-prod \\ --from-literal=postgres-password=\"$(openssl rand -base64 32)\" \\ --from-literal=password=\"$(openssl rand -base64 32)\" # Install PostgreSQL helm install postgresql bitnami/postgresql \\ --namespace cloudflow-prod \\ --values postgres-values.yaml \\ --wait ``` ### PgBouncer Configuration Deploy PgBouncer for connection pooling to handle up to 100 connections efficiently: ```yaml # pgbouncer.yaml apiVersion: v1 kind: ConfigMap metadata: name: pgbouncer-config namespace: cloudflow-prod data: pgbouncer.ini: | [databases] cloudflow = host=postgresql.cloudflow-prod.svc.cluster.local port=5432 dbname=cloudflow [pgbouncer] listen_addr = 0.0.0.0 listen_port = 5432 auth_type = md5 auth_file = /etc/pgbouncer/userlist.txt pool_mode = transaction max_client_conn = 1000 default_pool_size = 25 reserve_pool_size = 5 reserve_pool_timeout = 3 max_db_connections = 100 max_user_connections = 100 server_lifetime = 3600 server_idle_timeout = 600 log_connections = 1 log_disconnections = 1 log_pooler_errors = 1 --- apiVersion: apps/v1 kind: Deployment metadata: name: pgbouncer namespace: cloudflow-prod spec: replicas: 2 selector: matchLabels: app: pgbouncer template: metadata: labels: app: pgbouncer spec: containers: - name: pgbouncer image: pgbouncer/pgbouncer:1.21.0 ports: - containerPort: 5432 resources: requests: cpu: 500m memory: 512Mi limits: cpu: 1000m memory: 1Gi volumeMounts: - name: config mountPath: /etc/pgbouncer volumes: - name: config configMap: name: pgbouncer-config --- apiVersion: v1 kind: Service metadata: name: pgbouncer namespace: cloudflow-prod spec: selector: app: pgbouncer ports: - port: 5432 targetPort: 5432 type: ClusterIP ``` Apply the PgBouncer configuration: ```bash kubectl apply -f pgbouncer.yaml ``` ### Database Migrations Run database migrations before deploying new versions: ```bash # Create migration job kubectl create job cloudflow-migrate-$(date +%s) \\ --namespace cloudflow-prod \\ --image=123456789012.dkr.ecr.us-east-1.amazonaws.com/cloudflow:2.4.0 \\ -- npm run migrate # Monitor migration progress kubectl logs -f job/cloudflow-migrate-<timestamp> -n cloudflow-prod ``` --- ## Monitoring and Observability ### Prometheus Setup Install Prometheus using the kube-prometheus-stack: ```bash helm install prometheus prometheus-community/kube-prometheus-stack \\ --namespace monitoring \\ --create-namespace \\ --set prometheus.prometheusSpec.serviceMonitorSelectorNilUsesHelmValues=false \\",
              "score": null
            }
          ],
          "latency_ms": 1435.7772559997102,
          "query_id": "realistic_015",
          "dimension": "original",
          "query": "How do I diagnose database connection pool exhaustion?"
        },
        {
          "key_facts": [
            "could not obtain connection from pool within 5000ms",
            "connection pool exhausted (100/100 connections in use)",
            "PgBouncer",
            "max_db_connections = 100"
          ],
          "found_facts": [
            "could not obtain connection from pool within 5000ms",
            "connection pool exhausted (100/100 connections in use)",
            "PgBouncer"
          ],
          "missed_facts": [
            "max_db_connections = 100"
          ],
          "coverage": 0.75,
          "expected_docs": [
            "troubleshooting_guide",
            "deployment_guide"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_3",
              "content": "Workflow Context Accumulation** Large workflow executions may accumulate state in memory. **Solution:** ```bash # Configure context cleanup cloudflow config set workflow.context.max_size_mb 100 cloudflow config set workflow.context.cleanup_threshold 0.8 # Enable context persistence to disk cloudflow config set workflow.context.persistence.enabled true cloudflow config set workflow.context.persistence.backend s3 ``` **2. Connection Pool Leaks** **Diagnosis:** ```bash # Check active connections cloudflow db connections --verbose # Expected output: # Active: 45/100 # Idle: 23 # Waiting: 2 # Average age: 245s ``` **Resolution:** ```bash # Adjust connection pool settings cloudflow config set db.pool.max_connections 100 cloudflow config set db.pool.min_connections 10 cloudflow config set db.pool.idle_timeout 300 cloudflow config set db.pool.max_lifetime 1800 # Force connection pool reset cloudflow db pool reset --confirm ``` **3. Event Stream Buffers** Unbounded event buffers can cause memory exhaustion. ```bash # Configure event buffer limits cloudflow config set events.buffer.max_size 10000 cloudflow config set events.buffer.overflow_strategy drop_oldest # Enable event streaming to external sink cloudflow config set events.sink.type kafka cloudflow config set events.sink.kafka.brokers \"kafka-1:9092,kafka-2:9092\" cloudflow config set events.sink.kafka.topic cloudflow-events ``` --- ## Database Connection Issues ### Connection Pool Exhaustion #### Symptoms - Error: `could not obtain connection from pool within 5000ms` - Error: `connection pool exhausted (100/100 connections in use)` - API requests fail with `503 Service Unavailable` - Database CPU usage normal, but connection count at maximum #### Investigation ```bash # Check current connection pool status cloudflow db pool status --detailed # Output example: # Pool Statistics: # Total Connections: 100/100 (100%) # Active: 87 # Idle: 13 # Waiting Requests: 45 # Average Wait Time: 3420ms # Max Wait Time: 8234ms # Identify long-running queries cloudflow db queries --status running --min-duration 30000 # Check connection distribution by client SELECT application_name, state, COUNT(*) as conn_count, AVG(EXTRACT(EPOCH FROM (NOW() - state_change))) as avg_duration_sec FROM pg_stat_activity WHERE datname = 'cloudflow_production' GROUP BY application_name, state ORDER BY conn_count DESC; ``` #### Resolution **Immediate mitigation:** ```bash # Temporarily increase connection limit (requires database restart) cloudflow db config set max_connections 150 # Kill idle connections older than 5 minutes cloudflow db connections kill --idle-timeout 300 # Restart connection pool without downtime kubectl rollout restart deployment/cloudflow-api -n cloudflow kubectl rollout status deployment/cloudflow-api -n cloudflow ``` **Long-term solutions:** 1.",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_4",
              "content": "**Implement connection pooling optimization:** ```bash # Use PgBouncer for connection pooling kubectl apply -f cloudflow-pgbouncer.yaml # Configure CloudFlow to use PgBouncer cloudflow config set db.host pgbouncer.cloudflow.svc.cluster.local cloudflow config set db.port 6432 cloudflow config set db.pool.mode transaction ``` 2. **Add read replicas:** ```bash # Route read-only queries to replicas cloudflow db replicas add --count 2 cloudflow config set db.read_replicas \"replica-1:5432,replica-2:5432\" cloudflow config set db.read_write_split true ``` ### Connection Timeout Errors #### Error Messages - `connection timeout after 30000ms` - `could not connect to database server at 10.0.2.45:5432` - `database server unreachable` #### Troubleshooting Steps ```bash # Test network connectivity telnet cloudflow-db.internal.company.com 5432 # Check DNS resolution nslookup cloudflow-db.internal.company.com # Verify database is accepting connections pg_isready -h cloudflow-db.internal.company.com -p 5432 -U cloudflow # Check firewall rules sudo iptables -L -n | grep 5432 # Test from CloudFlow pod network kubectl run -n cloudflow debug-pod --rm -i --tty \\ --image=postgres:14 -- \\ psql -h cloudflow-db.internal.company.com -U cloudflow -d cloudflow_production # Review database logs for connection rejections kubectl logs -n cloudflow statefulset/cloudflow-db --tail=100 | \\ grep -i \"connection\\|reject\\|authentication\" ``` ### Maximum Connection Limit (100) Reached This is a hard limit in CloudFlow's database tier. #### Permanent Solutions **Option 1: Upgrade database tier** ```bash # Check available tiers cloudflow db tiers list # Upgrade to higher tier (supports 200 connections) cloudflow db upgrade --tier standard-plus --confirm # Monitor migration progress cloudflow db migration status ``` **Option 2: Implement aggressive connection reuse** ```bash # Reduce connection lifetime cloudflow config set db.pool.max_lifetime 600 # 10 minutes # Enable prepared statement caching cloudflow config set db.prepared_statements.cache true cloudflow config set db.prepared_statements.max_size 250 # Reduce idle connection timeout cloudflow config set db.pool.idle_timeout 120 # 2 minutes ``` --- ## Workflow Execution Failures ### Timeout Errors (3600 second limit) #### Error Message ``` WorkflowExecutionError: Workflow exceeded maximum execution time of 3600 seconds Status: TIMEOUT Workflow ID: wf_9k2n4m8p1q Execution ID: exec_7h3j6k9m2n ``` #### Analysis ```bash # Get workflow execution details cloudflow workflows executions get exec_7h3j6k9m2n --verbose # Check step-by-step breakdown cloudflow workflows executions steps exec_7h3j6k9m2n # Sample output: # Step 1: data_ingestion - Duration: 245s - Status: SUCCESS # Step 2: data_validation - Duration: 123s - Status: SUCCESS # Step 3: data_transformation - Duration: 3189s - Status: TIMEOUT # Step 4: data_export - Duration: 0s - Status: SKIPPED # Identify bottleneck step cloudflow workflows analyze",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_5",
              "content": "Success/failure events published back to Kafka ``` **Event Schema**: ```json { \"event_id\": \"uuid-v4\", \"event_type\": \"workflow.execution.completed\", \"timestamp\": \"2026-01-15T10:30:00.000Z\", \"correlation_id\": \"request-trace-id\", \"payload\": { \"workflow_id\": \"wf-12345\", \"execution_id\": \"exec-67890\", \"status\": \"COMPLETED\", \"duration_ms\": 4230 }, \"metadata\": { \"source_service\": \"workflow-engine\", \"schema_version\": \"1.0\" } } ``` ### Inter-Service Communication Services communicate using two primary patterns: **Synchronous (gRPC)**: - Auth Service validation calls from API Gateway - User Service profile lookups - Low latency requirement (< 50ms) - Request-response pattern with circuit breaker **Asynchronous (Kafka)**: - Workflow execution events - Notification triggers - Audit log events - Eventual consistency acceptable --- ## Database Architecture ### PostgreSQL Primary Database **Cluster Configuration**: - Primary-replica setup with 1 primary + 2 read replicas - Instance type: db.r6g.2xlarge (8 vCPU, 64GB RAM) - Storage: 2TB gp3 SSD with 12,000 IOPS - Multi-AZ deployment for high availability - Automated backups: Daily snapshots, 30-day retention - Point-in-time recovery: 5-minute granularity **Database Schema Design**: ``` Core Tables: - users (5M rows): User accounts and profiles - workflows (2M rows): Workflow definitions - workflow_executions (500M rows, partitioned): Execution history - workflow_steps (2B rows, partitioned): Individual step records - schedules (100K rows): Scheduled workflow triggers - notifications (1B rows, partitioned): Notification delivery log Partitioning Strategy: - workflow_executions: Monthly partitions by created_at - workflow_steps: Monthly partitions by created_at - notifications: Weekly partitions by created_at - Automatic partition creation via cron job - Retention policy: Drop partitions older than 12 months ``` **Indexing Strategy**: - Primary keys: UUIDs with B-tree indexes - Frequently queried columns: Compound indexes (e.g., user_id + status + created_at) - JSONB columns: GIN indexes for workflow definitions - Full-text search: GiST indexes on description fields **Connection Pooling**: - PgBouncer in transaction mode - Pool size: 100 connections per service - Max client connections: 2000 - Connection timeout: 30 seconds **Performance Characteristics**: - Read query P99: < 20ms - Write query P99: < 50ms - Transaction throughput: 15,000 TPS - Replication lag: < 500ms --- ### Redis Caching Layer **Cluster Configuration**: - Redis Cluster with 6 nodes (3 primary + 3 replica) - Instance type: cache.r6g.xlarge (4 vCPU, 26GB RAM) - Total memory: 78GB usable cache space - Persistence: RDB snapshots every 5 minutes + AOF - Eviction policy: allkeys-lru **Cache Usage Patterns**: 1.",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_10",
              "content": "- Execution start rate: 500 per second - Completion rate: 450 per second (average 2-second execution time) **Database**: - Read throughput: 50,000 queries per second (across replicas) - Write throughput: 15,000 transactions per second - Connection capacity: 2,000 concurrent connections **Kafka**: - Message ingestion: 100,000 messages per second - Consumer throughput: 80,000 messages per second (aggregated) - End-to-end latency: < 100ms (P99) ### Resource Utilization **CPU Utilization** (Target: 60-70% average): - API Gateway: 55% average, 80% peak - Workflow Engine: 65% average, 85% peak - Auth Service: 40% average, 60% peak **Memory Utilization** (Target: < 80%): - API Gateway: 2.5GB average per pod (4GB allocated) - Workflow Engine: 6GB average per pod (8GB allocated) - Notification Service: 2.8GB average per pod (4GB allocated) **Network Throughput**: - Ingress: 2 Gbps average, 5 Gbps peak - Egress: 1.5 Gbps average, 4 Gbps peak - Internal (service mesh): 8 Gbps average ### Monitoring & Alerting **Key Metrics** (Monitored via Prometheus): - Request rate (per service, per endpoint) - Error rate (4xx, 5xx responses) - Latency percentiles (P50, P95, P99) - Resource utilization (CPU, memory, disk) - Cache hit/miss ratios - Database connection pool saturation - Kafka consumer lag **Alerts** (via PagerDuty): - P1 (Immediate): API error rate > 1%, database connection failure - P2 (< 15 min): P99 latency > 500ms, cache hit rate < 80% - P3 (< 1 hour): Resource utilization > 85%, replica count < minimum **SLI/SLO/SLA**: - SLI: API success rate (non-5xx responses) - SLO: 99.9% uptime per month (43 minutes downtime allowance) - SLA: 99.5% uptime guarantee (customer-facing SLA with credits) --- ## Disaster Recovery ### Recovery Objectives **RPO (Recovery Point Objective): 1 hour** - Maximum acceptable data loss: 1 hour of transactions - Achieved through: Continuous database replication + hourly snapshots - Kafka retention: 7 days allows event replay **RTO (Recovery Time Objective): 4 hours** - Maximum acceptable downtime: 4 hours for full system recovery - Includes: Failover, data verification, and service restoration - Automated runbooks reduce RTO to < 2 hours for common scenarios ### High Availability Architecture **Multi-AZ Deployment**: ``` Region: us-east-1 AZ-1a: AZ-1b: AZ-1c: - API Gateway (4) - API Gateway (4) - API Gateway (4) - Workflow Engine (6) - Workflow Engine (5) - Workflow Engine (5) - Auth Service (3) - Auth Service (3) - Auth",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_4",
              "content": "**Technology**: Node.js with worker pool pattern **Replicas**: 8 pods (production), auto-scaling 6-16 **Resource Allocation**: 2 vCPU, 4GB RAM per pod **Key Responsibilities**: - Consume notification events from Kafka - Template rendering with Handlebars (cached in Redis) - Multi-channel delivery (Email via SendGrid, SMS via Twilio, Webhooks) - Retry logic with dead letter queue for failed deliveries - Delivery status tracking and analytics - User preference management (opt-in/opt-out) **Notification Flow**: ``` Event Source \u2192 Kafka (notifications.email topic) \u2502 \u25bc Notification Service Consumer \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u25bc \u25bc \u25bc [Email Queue] [SMS Queue] [Webhook Queue] \u2502 \u2502 \u2502 \u25bc \u25bc \u25bc [SendGrid] [Twilio] [HTTP POST] \u2502 \u2502 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u25bc Delivery Status \u2192 PostgreSQL ``` **Channel Configuration**: - Email: SendGrid with IP warming, DKIM/SPF configured - SMS: Twilio with fallback to AWS SNS - Webhook: Signed payloads (HMAC-SHA256), retry 3 times with exponential backoff - In-app: WebSocket connections via Socket.io **Template Management**: - Templates stored in PostgreSQL with versioning - Compiled templates cached in Redis (1-hour TTL) - Support for localization (i18n) with 12 languages - A/B testing capability for email campaigns **Performance Targets**: - Email delivery: < 5 seconds (P99) from event to SendGrid - SMS delivery: < 3 seconds (P99) - Throughput: 10,000 notifications per minute --- ## Data Flow Architecture ### Synchronous Request Flow Client requests follow a synchronous path through the API Gateway to backend services: ``` 1. Client \u2192 Load Balancer (TLS termination) 2. Load Balancer \u2192 API Gateway (HTTP/2) 3. API Gateway \u2192 Auth Service (JWT validation via gRPC) 4. API Gateway \u2192 Target Service (HTTP/REST or gRPC) 5. Target Service \u2192 Database/Cache (data retrieval) 6. Response propagates back through the chain ``` **Timeout Configuration**: - Client \u2192 Load Balancer: 60 seconds - Load Balancer \u2192 API Gateway: 55 seconds - API Gateway \u2192 Services: 45 seconds - Service \u2192 Database: 10 seconds ### Asynchronous Event Flow Long-running operations and inter-service communication use event-driven patterns: ``` 1. Service publishes event \u2192 Kafka topic 2. Kafka persists event (replication factor: 3) 3. Consumer groups subscribe to topics 4. Consumers process events with at-least-once delivery 5. State updates written to PostgreSQL 6.",
              "score": null
            }
          ],
          "latency_ms": 1074.5002280018525,
          "query_id": "realistic_015",
          "dimension": "synonym",
          "query": "What should I check when I run out of database connections?"
        },
        {
          "key_facts": [
            "could not obtain connection from pool within 5000ms",
            "connection pool exhausted (100/100 connections in use)",
            "PgBouncer",
            "max_db_connections = 100"
          ],
          "found_facts": [
            "could not obtain connection from pool within 5000ms",
            "connection pool exhausted (100/100 connections in use)",
            "PgBouncer"
          ],
          "missed_facts": [
            "max_db_connections = 100"
          ],
          "coverage": 0.75,
          "expected_docs": [
            "troubleshooting_guide",
            "deployment_guide"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_3",
              "content": "Workflow Context Accumulation** Large workflow executions may accumulate state in memory. **Solution:** ```bash # Configure context cleanup cloudflow config set workflow.context.max_size_mb 100 cloudflow config set workflow.context.cleanup_threshold 0.8 # Enable context persistence to disk cloudflow config set workflow.context.persistence.enabled true cloudflow config set workflow.context.persistence.backend s3 ``` **2. Connection Pool Leaks** **Diagnosis:** ```bash # Check active connections cloudflow db connections --verbose # Expected output: # Active: 45/100 # Idle: 23 # Waiting: 2 # Average age: 245s ``` **Resolution:** ```bash # Adjust connection pool settings cloudflow config set db.pool.max_connections 100 cloudflow config set db.pool.min_connections 10 cloudflow config set db.pool.idle_timeout 300 cloudflow config set db.pool.max_lifetime 1800 # Force connection pool reset cloudflow db pool reset --confirm ``` **3. Event Stream Buffers** Unbounded event buffers can cause memory exhaustion. ```bash # Configure event buffer limits cloudflow config set events.buffer.max_size 10000 cloudflow config set events.buffer.overflow_strategy drop_oldest # Enable event streaming to external sink cloudflow config set events.sink.type kafka cloudflow config set events.sink.kafka.brokers \"kafka-1:9092,kafka-2:9092\" cloudflow config set events.sink.kafka.topic cloudflow-events ``` --- ## Database Connection Issues ### Connection Pool Exhaustion #### Symptoms - Error: `could not obtain connection from pool within 5000ms` - Error: `connection pool exhausted (100/100 connections in use)` - API requests fail with `503 Service Unavailable` - Database CPU usage normal, but connection count at maximum #### Investigation ```bash # Check current connection pool status cloudflow db pool status --detailed # Output example: # Pool Statistics: # Total Connections: 100/100 (100%) # Active: 87 # Idle: 13 # Waiting Requests: 45 # Average Wait Time: 3420ms # Max Wait Time: 8234ms # Identify long-running queries cloudflow db queries --status running --min-duration 30000 # Check connection distribution by client SELECT application_name, state, COUNT(*) as conn_count, AVG(EXTRACT(EPOCH FROM (NOW() - state_change))) as avg_duration_sec FROM pg_stat_activity WHERE datname = 'cloudflow_production' GROUP BY application_name, state ORDER BY conn_count DESC; ``` #### Resolution **Immediate mitigation:** ```bash # Temporarily increase connection limit (requires database restart) cloudflow db config set max_connections 150 # Kill idle connections older than 5 minutes cloudflow db connections kill --idle-timeout 300 # Restart connection pool without downtime kubectl rollout restart deployment/cloudflow-api -n cloudflow kubectl rollout status deployment/cloudflow-api -n cloudflow ``` **Long-term solutions:** 1.",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_5",
              "content": "Success/failure events published back to Kafka ``` **Event Schema**: ```json { \"event_id\": \"uuid-v4\", \"event_type\": \"workflow.execution.completed\", \"timestamp\": \"2026-01-15T10:30:00.000Z\", \"correlation_id\": \"request-trace-id\", \"payload\": { \"workflow_id\": \"wf-12345\", \"execution_id\": \"exec-67890\", \"status\": \"COMPLETED\", \"duration_ms\": 4230 }, \"metadata\": { \"source_service\": \"workflow-engine\", \"schema_version\": \"1.0\" } } ``` ### Inter-Service Communication Services communicate using two primary patterns: **Synchronous (gRPC)**: - Auth Service validation calls from API Gateway - User Service profile lookups - Low latency requirement (< 50ms) - Request-response pattern with circuit breaker **Asynchronous (Kafka)**: - Workflow execution events - Notification triggers - Audit log events - Eventual consistency acceptable --- ## Database Architecture ### PostgreSQL Primary Database **Cluster Configuration**: - Primary-replica setup with 1 primary + 2 read replicas - Instance type: db.r6g.2xlarge (8 vCPU, 64GB RAM) - Storage: 2TB gp3 SSD with 12,000 IOPS - Multi-AZ deployment for high availability - Automated backups: Daily snapshots, 30-day retention - Point-in-time recovery: 5-minute granularity **Database Schema Design**: ``` Core Tables: - users (5M rows): User accounts and profiles - workflows (2M rows): Workflow definitions - workflow_executions (500M rows, partitioned): Execution history - workflow_steps (2B rows, partitioned): Individual step records - schedules (100K rows): Scheduled workflow triggers - notifications (1B rows, partitioned): Notification delivery log Partitioning Strategy: - workflow_executions: Monthly partitions by created_at - workflow_steps: Monthly partitions by created_at - notifications: Weekly partitions by created_at - Automatic partition creation via cron job - Retention policy: Drop partitions older than 12 months ``` **Indexing Strategy**: - Primary keys: UUIDs with B-tree indexes - Frequently queried columns: Compound indexes (e.g., user_id + status + created_at) - JSONB columns: GIN indexes for workflow definitions - Full-text search: GiST indexes on description fields **Connection Pooling**: - PgBouncer in transaction mode - Pool size: 100 connections per service - Max client connections: 2000 - Connection timeout: 30 seconds **Performance Characteristics**: - Read query P99: < 20ms - Write query P99: < 50ms - Transaction throughput: 15,000 TPS - Replication lag: < 500ms --- ### Redis Caching Layer **Cluster Configuration**: - Redis Cluster with 6 nodes (3 primary + 3 replica) - Instance type: cache.r6g.xlarge (4 vCPU, 26GB RAM) - Total memory: 78GB usable cache space - Persistence: RDB snapshots every 5 minutes + AOF - Eviction policy: allkeys-lru **Cache Usage Patterns**: 1.",
              "score": null
            },
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_7",
              "content": "podSelector: matchLabels: app: cloudflow policyTypes: - Ingress - Egress ingress: - from: - namespaceSelector: matchLabels: name: cloudflow-prod - podSelector: matchLabels: app: nginx-ingress ports: - protocol: TCP port: 3000 egress: - to: - podSelector: matchLabels: app: postgresql ports: - protocol: TCP port: 5432 - to: - podSelector: matchLabels: app: redis ports: - protocol: TCP port: 6379 ``` ### Pod Security Standards Enforce Pod Security Standards at the namespace level: ```bash kubectl label namespace cloudflow-prod \\ pod-security.kubernetes.io/enforce=restricted \\ pod-security.kubernetes.io/audit=restricted \\ pod-security.kubernetes.io/warn=restricted ``` ### Secrets Management Use AWS Secrets Manager for sensitive credentials: ```bash # Install External Secrets Operator helm install external-secrets external-secrets/external-secrets \\ --namespace external-secrets-system \\ --create-namespace # Create SecretStore kubectl apply -f - <<EOF apiVersion: external-secrets.io/v1beta1 kind: SecretStore metadata: name: aws-secrets-manager namespace: cloudflow-prod spec: provider: aws: service: SecretsManager region: us-east-1 auth: jwt: serviceAccountRef: name: external-secrets-sa EOF ``` --- ## Troubleshooting ### Common Issues #### Pods in CrashLoopBackOff **Symptoms**: Pods continuously restart **Diagnosis**: ```bash kubectl logs -n cloudflow-prod <pod-name> --previous kubectl describe pod -n cloudflow-prod <pod-name> ``` **Common Causes**: - Database connection failure - Invalid environment variables - Insufficient resources #### High Memory Usage **Symptoms**: Pods being OOMKilled **Diagnosis**: ```bash kubectl top pods -n cloudflow-prod ``` **Resolution**: - Increase memory limits in deployment - Check for memory leaks in application logs - Review heap dump for Node.js processes #### Database Connection Pool Exhausted **Symptoms**: \"Too many connections\" errors **Diagnosis**: ```bash # Check PgBouncer stats kubectl exec -it pgbouncer-xxx -n cloudflow-prod -- \\ psql -p 5432 -U pgbouncer pgbouncer -c \"SHOW POOLS;\" ``` **Resolution**: - Increase `max_db_connections` in PgBouncer - Optimize application connection usage - Add more PostgreSQL replicas ### Support Contacts - **On-call Engineer**: pagerduty.com/cloudflow-oncall - **Slack Channel**: #cloudflow-operations - **Documentation**: https://docs.cloudflow.io - **Status Page**: https://status.cloudflow.io --- ## Appendix ### Useful Commands ```bash # View all resources in namespace kubectl get all -n cloudflow-prod # Check resource usage kubectl top pods -n cloudflow-prod kubectl top nodes # View logs kubectl logs -f deployment/cloudflow-api -n cloudflow-prod # Execute command in pod kubectl exec -it <pod-name> -n cloudflow-prod -- /bin/sh # Port forward for local testing kubectl port-forward svc/cloudflow-api 8080:80 -n cloudflow-prod # Rollback deployment helm rollback cloudflow -n cloudflow-prod # Update deployment helm upgrade cloudflow cloudflow/cloudflow \\ --namespace cloudflow-prod \\ --values values-production.yaml ``` ### Maintenance Windows Scheduled maintenance occurs during the following windows: - **Primary**: Sunday 02:00-06:00",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_10",
              "content": "- Execution start rate: 500 per second - Completion rate: 450 per second (average 2-second execution time) **Database**: - Read throughput: 50,000 queries per second (across replicas) - Write throughput: 15,000 transactions per second - Connection capacity: 2,000 concurrent connections **Kafka**: - Message ingestion: 100,000 messages per second - Consumer throughput: 80,000 messages per second (aggregated) - End-to-end latency: < 100ms (P99) ### Resource Utilization **CPU Utilization** (Target: 60-70% average): - API Gateway: 55% average, 80% peak - Workflow Engine: 65% average, 85% peak - Auth Service: 40% average, 60% peak **Memory Utilization** (Target: < 80%): - API Gateway: 2.5GB average per pod (4GB allocated) - Workflow Engine: 6GB average per pod (8GB allocated) - Notification Service: 2.8GB average per pod (4GB allocated) **Network Throughput**: - Ingress: 2 Gbps average, 5 Gbps peak - Egress: 1.5 Gbps average, 4 Gbps peak - Internal (service mesh): 8 Gbps average ### Monitoring & Alerting **Key Metrics** (Monitored via Prometheus): - Request rate (per service, per endpoint) - Error rate (4xx, 5xx responses) - Latency percentiles (P50, P95, P99) - Resource utilization (CPU, memory, disk) - Cache hit/miss ratios - Database connection pool saturation - Kafka consumer lag **Alerts** (via PagerDuty): - P1 (Immediate): API error rate > 1%, database connection failure - P2 (< 15 min): P99 latency > 500ms, cache hit rate < 80% - P3 (< 1 hour): Resource utilization > 85%, replica count < minimum **SLI/SLO/SLA**: - SLI: API success rate (non-5xx responses) - SLO: 99.9% uptime per month (43 minutes downtime allowance) - SLA: 99.5% uptime guarantee (customer-facing SLA with credits) --- ## Disaster Recovery ### Recovery Objectives **RPO (Recovery Point Objective): 1 hour** - Maximum acceptable data loss: 1 hour of transactions - Achieved through: Continuous database replication + hourly snapshots - Kafka retention: 7 days allows event replay **RTO (Recovery Time Objective): 4 hours** - Maximum acceptable downtime: 4 hours for full system recovery - Includes: Failover, data verification, and service restoration - Automated runbooks reduce RTO to < 2 hours for common scenarios ### High Availability Architecture **Multi-AZ Deployment**: ``` Region: us-east-1 AZ-1a: AZ-1b: AZ-1c: - API Gateway (4) - API Gateway (4) - API Gateway (4) - Workflow Engine (6) - Workflow Engine (5) - Workflow Engine (5) - Auth Service (3) - Auth Service (3) - Auth",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_0",
              "content": "# CloudFlow Platform - System Architecture Overview **Document Version:** 2.3.1 **Last Updated:** January 15, 2026 **Owner:** Platform Architecture Team **Status:** Production ## Executive Summary CloudFlow is a distributed, cloud-native workflow automation platform designed to orchestrate complex business processes at scale. The platform processes over 2.5 million workflow executions daily with an average P99 latency of 180ms for API operations and 4.2 seconds for workflow execution. This document provides a comprehensive overview of the system architecture, including microservices design, data flow patterns, infrastructure decisions, and operational considerations. ## Table of Contents 1. [High-Level Architecture](#high-level-architecture) 2. [Microservices Breakdown](#microservices-breakdown) 3. [Data Flow Architecture](#data-flow-architecture) 4. [Database Architecture](#database-architecture) 5. [Message Queue Patterns](#message-queue-patterns) 6. [Caching Strategy](#caching-strategy) 7. [Security Architecture](#security-architecture) 8. [Performance Characteristics](#performance-characteristics) 9. [Disaster Recovery](#disaster-recovery) --- ## High-Level Architecture CloudFlow follows a microservices architecture pattern deployed across multiple availability zones in AWS. The platform is designed with the following core principles: - **Scalability**: Horizontal scaling for all services with auto-scaling groups - **Resilience**: Circuit breakers, bulkheads, and graceful degradation - **Observability**: Distributed tracing, centralized logging, and comprehensive metrics - **Security**: Zero-trust network architecture with mTLS encryption ### Architecture Diagram (Conceptual) ``` \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 Load Balancer (ALB) \u2502 \u2502 (TLS Termination - 443) \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u25bc \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 API Gateway Layer \u2502 \u2502 (Rate Limiting, Auth, Request Routing) \u2502 \u2514\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u2502 \u2502 \u2502 \u2502 \u25bc \u25bc \u25bc \u25bc \u25bc \u250c\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502Auth\u2502 \u2502Workflow\u2502 \u2502Scheduler\u2502 \u2502Notification\u2502 \u2502User Service\u2502 \u2502Svc \u2502 \u2502 Engine \u2502 \u2502 Service \u2502 \u2502 Service \u2502 \u2502 \u2502 \u2514\u2500\u252c\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u2502 \u2502 \u2502 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 \u2502 \u2502 \u25bc \u25bc \u25bc \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502PostgreSQL\u2502 \u2502 Redis \u2502 \u2502 Kafka \u2502 \u2502 Cluster \u2502 \u2502 Cluster \u2502 \u2502 Cluster \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 ``` ### Technology Stack - **Runtime**: Node.js 20.x (API Gateway, Workflow Engine), Go 1.21 (Auth Service, Scheduler) - **Container Orchestration**: Kubernetes 1.28 (EKS) - **Service Mesh**: Istio 1.20 for service-to-service communication - **Databases**: PostgreSQL 15.4, Redis 7.2 - **Message Broker**: Apache Kafka 3.6 - **Monitoring**: Prometheus, Grafana, Jaeger for distributed tracing - **Secrets Management**: HashiCorp Vault 1.15 --- ## Microservices Breakdown ### API Gateway **Purpose**: Single entry point for all client requests, providing authentication, rate limiting, request routing, and protocol translation.",
              "score": null
            }
          ],
          "latency_ms": 892.042188999767,
          "query_id": "realistic_015",
          "dimension": "problem",
          "query": "Getting 'could not obtain connection from pool' errors"
        },
        {
          "key_facts": [
            "could not obtain connection from pool within 5000ms",
            "connection pool exhausted (100/100 connections in use)",
            "PgBouncer",
            "max_db_connections = 100"
          ],
          "found_facts": [
            "could not obtain connection from pool within 5000ms",
            "connection pool exhausted (100/100 connections in use)",
            "PgBouncer"
          ],
          "missed_facts": [
            "max_db_connections = 100"
          ],
          "coverage": 0.75,
          "expected_docs": [
            "troubleshooting_guide",
            "deployment_guide"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_3",
              "content": "Workflow Context Accumulation** Large workflow executions may accumulate state in memory. **Solution:** ```bash # Configure context cleanup cloudflow config set workflow.context.max_size_mb 100 cloudflow config set workflow.context.cleanup_threshold 0.8 # Enable context persistence to disk cloudflow config set workflow.context.persistence.enabled true cloudflow config set workflow.context.persistence.backend s3 ``` **2. Connection Pool Leaks** **Diagnosis:** ```bash # Check active connections cloudflow db connections --verbose # Expected output: # Active: 45/100 # Idle: 23 # Waiting: 2 # Average age: 245s ``` **Resolution:** ```bash # Adjust connection pool settings cloudflow config set db.pool.max_connections 100 cloudflow config set db.pool.min_connections 10 cloudflow config set db.pool.idle_timeout 300 cloudflow config set db.pool.max_lifetime 1800 # Force connection pool reset cloudflow db pool reset --confirm ``` **3. Event Stream Buffers** Unbounded event buffers can cause memory exhaustion. ```bash # Configure event buffer limits cloudflow config set events.buffer.max_size 10000 cloudflow config set events.buffer.overflow_strategy drop_oldest # Enable event streaming to external sink cloudflow config set events.sink.type kafka cloudflow config set events.sink.kafka.brokers \"kafka-1:9092,kafka-2:9092\" cloudflow config set events.sink.kafka.topic cloudflow-events ``` --- ## Database Connection Issues ### Connection Pool Exhaustion #### Symptoms - Error: `could not obtain connection from pool within 5000ms` - Error: `connection pool exhausted (100/100 connections in use)` - API requests fail with `503 Service Unavailable` - Database CPU usage normal, but connection count at maximum #### Investigation ```bash # Check current connection pool status cloudflow db pool status --detailed # Output example: # Pool Statistics: # Total Connections: 100/100 (100%) # Active: 87 # Idle: 13 # Waiting Requests: 45 # Average Wait Time: 3420ms # Max Wait Time: 8234ms # Identify long-running queries cloudflow db queries --status running --min-duration 30000 # Check connection distribution by client SELECT application_name, state, COUNT(*) as conn_count, AVG(EXTRACT(EPOCH FROM (NOW() - state_change))) as avg_duration_sec FROM pg_stat_activity WHERE datname = 'cloudflow_production' GROUP BY application_name, state ORDER BY conn_count DESC; ``` #### Resolution **Immediate mitigation:** ```bash # Temporarily increase connection limit (requires database restart) cloudflow db config set max_connections 150 # Kill idle connections older than 5 minutes cloudflow db connections kill --idle-timeout 300 # Restart connection pool without downtime kubectl rollout restart deployment/cloudflow-api -n cloudflow kubectl rollout status deployment/cloudflow-api -n cloudflow ``` **Long-term solutions:** 1.",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_5",
              "content": "Success/failure events published back to Kafka ``` **Event Schema**: ```json { \"event_id\": \"uuid-v4\", \"event_type\": \"workflow.execution.completed\", \"timestamp\": \"2026-01-15T10:30:00.000Z\", \"correlation_id\": \"request-trace-id\", \"payload\": { \"workflow_id\": \"wf-12345\", \"execution_id\": \"exec-67890\", \"status\": \"COMPLETED\", \"duration_ms\": 4230 }, \"metadata\": { \"source_service\": \"workflow-engine\", \"schema_version\": \"1.0\" } } ``` ### Inter-Service Communication Services communicate using two primary patterns: **Synchronous (gRPC)**: - Auth Service validation calls from API Gateway - User Service profile lookups - Low latency requirement (< 50ms) - Request-response pattern with circuit breaker **Asynchronous (Kafka)**: - Workflow execution events - Notification triggers - Audit log events - Eventual consistency acceptable --- ## Database Architecture ### PostgreSQL Primary Database **Cluster Configuration**: - Primary-replica setup with 1 primary + 2 read replicas - Instance type: db.r6g.2xlarge (8 vCPU, 64GB RAM) - Storage: 2TB gp3 SSD with 12,000 IOPS - Multi-AZ deployment for high availability - Automated backups: Daily snapshots, 30-day retention - Point-in-time recovery: 5-minute granularity **Database Schema Design**: ``` Core Tables: - users (5M rows): User accounts and profiles - workflows (2M rows): Workflow definitions - workflow_executions (500M rows, partitioned): Execution history - workflow_steps (2B rows, partitioned): Individual step records - schedules (100K rows): Scheduled workflow triggers - notifications (1B rows, partitioned): Notification delivery log Partitioning Strategy: - workflow_executions: Monthly partitions by created_at - workflow_steps: Monthly partitions by created_at - notifications: Weekly partitions by created_at - Automatic partition creation via cron job - Retention policy: Drop partitions older than 12 months ``` **Indexing Strategy**: - Primary keys: UUIDs with B-tree indexes - Frequently queried columns: Compound indexes (e.g., user_id + status + created_at) - JSONB columns: GIN indexes for workflow definitions - Full-text search: GiST indexes on description fields **Connection Pooling**: - PgBouncer in transaction mode - Pool size: 100 connections per service - Max client connections: 2000 - Connection timeout: 30 seconds **Performance Characteristics**: - Read query P99: < 20ms - Write query P99: < 50ms - Transaction throughput: 15,000 TPS - Replication lag: < 500ms --- ### Redis Caching Layer **Cluster Configuration**: - Redis Cluster with 6 nodes (3 primary + 3 replica) - Instance type: cache.r6g.xlarge (4 vCPU, 26GB RAM) - Total memory: 78GB usable cache space - Persistence: RDB snapshots every 5 minutes + AOF - Eviction policy: allkeys-lru **Cache Usage Patterns**: 1.",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_10",
              "content": "- Execution start rate: 500 per second - Completion rate: 450 per second (average 2-second execution time) **Database**: - Read throughput: 50,000 queries per second (across replicas) - Write throughput: 15,000 transactions per second - Connection capacity: 2,000 concurrent connections **Kafka**: - Message ingestion: 100,000 messages per second - Consumer throughput: 80,000 messages per second (aggregated) - End-to-end latency: < 100ms (P99) ### Resource Utilization **CPU Utilization** (Target: 60-70% average): - API Gateway: 55% average, 80% peak - Workflow Engine: 65% average, 85% peak - Auth Service: 40% average, 60% peak **Memory Utilization** (Target: < 80%): - API Gateway: 2.5GB average per pod (4GB allocated) - Workflow Engine: 6GB average per pod (8GB allocated) - Notification Service: 2.8GB average per pod (4GB allocated) **Network Throughput**: - Ingress: 2 Gbps average, 5 Gbps peak - Egress: 1.5 Gbps average, 4 Gbps peak - Internal (service mesh): 8 Gbps average ### Monitoring & Alerting **Key Metrics** (Monitored via Prometheus): - Request rate (per service, per endpoint) - Error rate (4xx, 5xx responses) - Latency percentiles (P50, P95, P99) - Resource utilization (CPU, memory, disk) - Cache hit/miss ratios - Database connection pool saturation - Kafka consumer lag **Alerts** (via PagerDuty): - P1 (Immediate): API error rate > 1%, database connection failure - P2 (< 15 min): P99 latency > 500ms, cache hit rate < 80% - P3 (< 1 hour): Resource utilization > 85%, replica count < minimum **SLI/SLO/SLA**: - SLI: API success rate (non-5xx responses) - SLO: 99.9% uptime per month (43 minutes downtime allowance) - SLA: 99.5% uptime guarantee (customer-facing SLA with credits) --- ## Disaster Recovery ### Recovery Objectives **RPO (Recovery Point Objective): 1 hour** - Maximum acceptable data loss: 1 hour of transactions - Achieved through: Continuous database replication + hourly snapshots - Kafka retention: 7 days allows event replay **RTO (Recovery Time Objective): 4 hours** - Maximum acceptable downtime: 4 hours for full system recovery - Includes: Failover, data verification, and service restoration - Automated runbooks reduce RTO to < 2 hours for common scenarios ### High Availability Architecture **Multi-AZ Deployment**: ``` Region: us-east-1 AZ-1a: AZ-1b: AZ-1c: - API Gateway (4) - API Gateway (4) - API Gateway (4) - Workflow Engine (6) - Workflow Engine (5) - Workflow Engine (5) - Auth Service (3) - Auth Service (3) - Auth",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_12",
              "content": "Resume normal operations - Recovery time: 1-3 hours depending on data volume - Data loss: None if corruption detected quickly ### Testing & Validation **DR Drill Schedule**: - Monthly: Automated failover test (single AZ failure simulation) - Quarterly: Full DR region failover (non-production hours) - Annually: Complete disaster simulation with all stakeholders **Last DR Test Results** (Dec 15, 2025): - Scenario: Full region failover - Actual RTO: 2 hours 23 minutes (target: 4 hours) - Actual RPO: 42 minutes (target: 1 hour) - Issues identified: DNS propagation slower than expected (resolved) - Success criteria: Met all recovery objectives ### Business Continuity **Communication Plan**: - Status page: status.cloudflow.com (updated every 15 minutes during incident) - Customer notifications: Email + SMS for all P1 incidents - Internal escalation: PagerDuty \u2192 Incident Commander \u2192 Engineering Manager \u2192 CTO **Data Retention Policy**: - Active data: 12 months in hot storage (PostgreSQL) - Archived data: 7 years in cold storage (S3 Glacier) - Audit logs: 7 years (compliance requirement) - Backup retention: 30 days standard, 90 days monthly snapshots --- ## Appendix ### Service Dependency Matrix | Service | Depends On | Critical Path | |---------|-----------|---------------| | API Gateway | Auth Service, Redis | Yes | | Auth Service | PostgreSQL, Redis, Vault | Yes | | Workflow Engine | PostgreSQL, Kafka, Redis | Yes | | Scheduler | PostgreSQL, Redis, Kafka | No | | Notification Service | Kafka, PostgreSQL, SendGrid, Twilio | No | ### Capacity Planning **Current Capacity** (Jan 2026): - Daily workflow executions: 2.5M - Active users: 150,000 - API requests per day: 50M - Database size: 1.2TB - Kafka throughput: 100K msg/sec peak **6-Month Projection** (Jul 2026): - Daily workflow executions: 4M (+60%) - Active users: 225,000 (+50%) - API requests per day: 80M (+60%) - Database size: 2TB (+67%) - Required scaling: +30% compute, +50% storage ### Contact Information - **Architecture Team**: architecture@cloudflow.internal - **On-Call Engineer**: PagerDuty escalation - **Security Team**: security@cloudflow.internal - **Documentation**: https://wiki.cloudflow.internal/architecture --- **Document Revision History**: - v2.3.1 (Jan 15, 2026): Updated performance metrics, added DR test results - v2.3.0 (Dec 1, 2025): Added multi-AZ deployment details - v2.2.0 (Oct 15, 2025): Security architecture overhaul - v2.1.0 (Sep 1, 2025): Initial Kafka migration documentation - v2.0.0 (Jul 1, 2025): Complete microservices rewrite",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_8",
              "content": "### Cache Invalidation Strategies **Time-Based Expiration (TTL)**: - Short-lived data: 5-15 minutes (session tokens, rate limit counters) - Medium-lived data: 1 hour (workflow definitions, templates) - Long-lived data: 24 hours (static configuration) **Event-Based Invalidation**: ``` Database Update Event \u2192 Kafka (cache.invalidation topic) \u2502 \u25bc All service instances consume event \u2502 \u25bc Redis DEL for affected keys ``` **Pattern-Based Invalidation**: - Use Redis SCAN + DEL for wildcard patterns - Example: Invalidate all user caches: `user:*:{user_id}` **Cache Warming**: - Scheduled job runs every hour to pre-populate frequently accessed data - Targets: Top 1000 workflows, system configuration, active user profiles - Reduces cache miss rate during traffic spikes ### Multi-Level Caching Application implements L1 (in-memory) and L2 (Redis) caching: ``` Request \u2192 L1 Cache (Node.js Map, 1000 entry LRU) \u2502 MISS \u2502 \u25bc L2 Cache (Redis, distributed) \u2502 MISS \u2502 \u25bc Database (PostgreSQL) ``` L1 cache reduces Redis network calls by 40% for hot data. --- ## Security Architecture ### Network Security **Zero-Trust Network Model**: - All service-to-service communication encrypted with mTLS - Certificate rotation: Every 90 days (automated via cert-manager) - Certificate authority: Internal PKI with HashiCorp Vault - Network segmentation: Private subnets for services, public subnet for ALB only **Service Mesh (Istio)**: ``` Service A \u2192 Envoy Sidecar (mTLS client cert) \u2502 Encrypted channel \u2502 Envoy Sidecar (mTLS server cert) \u2192 Service B ``` **Firewall Rules**: - Security groups: Deny all by default, explicit allow rules - Ingress: Only ALB can reach API Gateway (port 8080) - Egress: Services can only reach specific dependencies - Database access: Limited to application subnets only ### Authentication & Authorization **JWT Token Validation**: - Algorithm: RS256 (asymmetric signing) - Key rotation: Every 30 days with 7-day overlap period - Public key distribution: JWKS endpoint cached in Redis - Validation: Signature, expiry, issuer, audience claims - Token revocation: Blacklist in Redis for compromised tokens **Permission Model**: ``` User \u2192 Roles \u2192 Permissions \u2198 \u2197 Tenants (Multi-tenancy isolation) ``` Example permissions: - `workflow:read` - View workflows - `workflow:write` - Create/update workflows - `workflow:execute` - Trigger workflow execution - `workflow:delete` - Delete workflows - `admin:*` - All administrative operations **API Key Management**: - Format: `cfk_live_<32-char-random>` (production), `cfk_test_<32-char-random>` (sandbox) - Hashing: SHA-256 before storage in PostgreSQL - Scoping: API keys can be scoped to specific workflows or operations - Rate limits: Configurable",
              "score": null
            }
          ],
          "latency_ms": 883.6888609985181,
          "query_id": "realistic_015",
          "dimension": "casual",
          "query": "connection pool full"
        },
        {
          "key_facts": [
            "could not obtain connection from pool within 5000ms",
            "connection pool exhausted (100/100 connections in use)",
            "PgBouncer",
            "max_db_connections = 100"
          ],
          "found_facts": [
            "could not obtain connection from pool within 5000ms",
            "connection pool exhausted (100/100 connections in use)",
            "PgBouncer"
          ],
          "missed_facts": [
            "max_db_connections = 100"
          ],
          "coverage": 0.75,
          "expected_docs": [
            "troubleshooting_guide",
            "deployment_guide"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_3",
              "content": "Workflow Context Accumulation** Large workflow executions may accumulate state in memory. **Solution:** ```bash # Configure context cleanup cloudflow config set workflow.context.max_size_mb 100 cloudflow config set workflow.context.cleanup_threshold 0.8 # Enable context persistence to disk cloudflow config set workflow.context.persistence.enabled true cloudflow config set workflow.context.persistence.backend s3 ``` **2. Connection Pool Leaks** **Diagnosis:** ```bash # Check active connections cloudflow db connections --verbose # Expected output: # Active: 45/100 # Idle: 23 # Waiting: 2 # Average age: 245s ``` **Resolution:** ```bash # Adjust connection pool settings cloudflow config set db.pool.max_connections 100 cloudflow config set db.pool.min_connections 10 cloudflow config set db.pool.idle_timeout 300 cloudflow config set db.pool.max_lifetime 1800 # Force connection pool reset cloudflow db pool reset --confirm ``` **3. Event Stream Buffers** Unbounded event buffers can cause memory exhaustion. ```bash # Configure event buffer limits cloudflow config set events.buffer.max_size 10000 cloudflow config set events.buffer.overflow_strategy drop_oldest # Enable event streaming to external sink cloudflow config set events.sink.type kafka cloudflow config set events.sink.kafka.brokers \"kafka-1:9092,kafka-2:9092\" cloudflow config set events.sink.kafka.topic cloudflow-events ``` --- ## Database Connection Issues ### Connection Pool Exhaustion #### Symptoms - Error: `could not obtain connection from pool within 5000ms` - Error: `connection pool exhausted (100/100 connections in use)` - API requests fail with `503 Service Unavailable` - Database CPU usage normal, but connection count at maximum #### Investigation ```bash # Check current connection pool status cloudflow db pool status --detailed # Output example: # Pool Statistics: # Total Connections: 100/100 (100%) # Active: 87 # Idle: 13 # Waiting Requests: 45 # Average Wait Time: 3420ms # Max Wait Time: 8234ms # Identify long-running queries cloudflow db queries --status running --min-duration 30000 # Check connection distribution by client SELECT application_name, state, COUNT(*) as conn_count, AVG(EXTRACT(EPOCH FROM (NOW() - state_change))) as avg_duration_sec FROM pg_stat_activity WHERE datname = 'cloudflow_production' GROUP BY application_name, state ORDER BY conn_count DESC; ``` #### Resolution **Immediate mitigation:** ```bash # Temporarily increase connection limit (requires database restart) cloudflow db config set max_connections 150 # Kill idle connections older than 5 minutes cloudflow db connections kill --idle-timeout 300 # Restart connection pool without downtime kubectl rollout restart deployment/cloudflow-api -n cloudflow kubectl rollout status deployment/cloudflow-api -n cloudflow ``` **Long-term solutions:** 1.",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_10",
              "content": "- Execution start rate: 500 per second - Completion rate: 450 per second (average 2-second execution time) **Database**: - Read throughput: 50,000 queries per second (across replicas) - Write throughput: 15,000 transactions per second - Connection capacity: 2,000 concurrent connections **Kafka**: - Message ingestion: 100,000 messages per second - Consumer throughput: 80,000 messages per second (aggregated) - End-to-end latency: < 100ms (P99) ### Resource Utilization **CPU Utilization** (Target: 60-70% average): - API Gateway: 55% average, 80% peak - Workflow Engine: 65% average, 85% peak - Auth Service: 40% average, 60% peak **Memory Utilization** (Target: < 80%): - API Gateway: 2.5GB average per pod (4GB allocated) - Workflow Engine: 6GB average per pod (8GB allocated) - Notification Service: 2.8GB average per pod (4GB allocated) **Network Throughput**: - Ingress: 2 Gbps average, 5 Gbps peak - Egress: 1.5 Gbps average, 4 Gbps peak - Internal (service mesh): 8 Gbps average ### Monitoring & Alerting **Key Metrics** (Monitored via Prometheus): - Request rate (per service, per endpoint) - Error rate (4xx, 5xx responses) - Latency percentiles (P50, P95, P99) - Resource utilization (CPU, memory, disk) - Cache hit/miss ratios - Database connection pool saturation - Kafka consumer lag **Alerts** (via PagerDuty): - P1 (Immediate): API error rate > 1%, database connection failure - P2 (< 15 min): P99 latency > 500ms, cache hit rate < 80% - P3 (< 1 hour): Resource utilization > 85%, replica count < minimum **SLI/SLO/SLA**: - SLI: API success rate (non-5xx responses) - SLO: 99.9% uptime per month (43 minutes downtime allowance) - SLA: 99.5% uptime guarantee (customer-facing SLA with credits) --- ## Disaster Recovery ### Recovery Objectives **RPO (Recovery Point Objective): 1 hour** - Maximum acceptable data loss: 1 hour of transactions - Achieved through: Continuous database replication + hourly snapshots - Kafka retention: 7 days allows event replay **RTO (Recovery Time Objective): 4 hours** - Maximum acceptable downtime: 4 hours for full system recovery - Includes: Failover, data verification, and service restoration - Automated runbooks reduce RTO to < 2 hours for common scenarios ### High Availability Architecture **Multi-AZ Deployment**: ``` Region: us-east-1 AZ-1a: AZ-1b: AZ-1c: - API Gateway (4) - API Gateway (4) - API Gateway (4) - Workflow Engine (6) - Workflow Engine (5) - Workflow Engine (5) - Auth Service (3) - Auth Service (3) - Auth",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_5",
              "content": "Success/failure events published back to Kafka ``` **Event Schema**: ```json { \"event_id\": \"uuid-v4\", \"event_type\": \"workflow.execution.completed\", \"timestamp\": \"2026-01-15T10:30:00.000Z\", \"correlation_id\": \"request-trace-id\", \"payload\": { \"workflow_id\": \"wf-12345\", \"execution_id\": \"exec-67890\", \"status\": \"COMPLETED\", \"duration_ms\": 4230 }, \"metadata\": { \"source_service\": \"workflow-engine\", \"schema_version\": \"1.0\" } } ``` ### Inter-Service Communication Services communicate using two primary patterns: **Synchronous (gRPC)**: - Auth Service validation calls from API Gateway - User Service profile lookups - Low latency requirement (< 50ms) - Request-response pattern with circuit breaker **Asynchronous (Kafka)**: - Workflow execution events - Notification triggers - Audit log events - Eventual consistency acceptable --- ## Database Architecture ### PostgreSQL Primary Database **Cluster Configuration**: - Primary-replica setup with 1 primary + 2 read replicas - Instance type: db.r6g.2xlarge (8 vCPU, 64GB RAM) - Storage: 2TB gp3 SSD with 12,000 IOPS - Multi-AZ deployment for high availability - Automated backups: Daily snapshots, 30-day retention - Point-in-time recovery: 5-minute granularity **Database Schema Design**: ``` Core Tables: - users (5M rows): User accounts and profiles - workflows (2M rows): Workflow definitions - workflow_executions (500M rows, partitioned): Execution history - workflow_steps (2B rows, partitioned): Individual step records - schedules (100K rows): Scheduled workflow triggers - notifications (1B rows, partitioned): Notification delivery log Partitioning Strategy: - workflow_executions: Monthly partitions by created_at - workflow_steps: Monthly partitions by created_at - notifications: Weekly partitions by created_at - Automatic partition creation via cron job - Retention policy: Drop partitions older than 12 months ``` **Indexing Strategy**: - Primary keys: UUIDs with B-tree indexes - Frequently queried columns: Compound indexes (e.g., user_id + status + created_at) - JSONB columns: GIN indexes for workflow definitions - Full-text search: GiST indexes on description fields **Connection Pooling**: - PgBouncer in transaction mode - Pool size: 100 connections per service - Max client connections: 2000 - Connection timeout: 30 seconds **Performance Characteristics**: - Read query P99: < 20ms - Write query P99: < 50ms - Transaction throughput: 15,000 TPS - Replication lag: < 500ms --- ### Redis Caching Layer **Cluster Configuration**: - Redis Cluster with 6 nodes (3 primary + 3 replica) - Instance type: cache.r6g.xlarge (4 vCPU, 26GB RAM) - Total memory: 78GB usable cache space - Persistence: RDB snapshots every 5 minutes + AOF - Eviction policy: allkeys-lru **Cache Usage Patterns**: 1.",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_4",
              "content": "**Implement connection pooling optimization:** ```bash # Use PgBouncer for connection pooling kubectl apply -f cloudflow-pgbouncer.yaml # Configure CloudFlow to use PgBouncer cloudflow config set db.host pgbouncer.cloudflow.svc.cluster.local cloudflow config set db.port 6432 cloudflow config set db.pool.mode transaction ``` 2. **Add read replicas:** ```bash # Route read-only queries to replicas cloudflow db replicas add --count 2 cloudflow config set db.read_replicas \"replica-1:5432,replica-2:5432\" cloudflow config set db.read_write_split true ``` ### Connection Timeout Errors #### Error Messages - `connection timeout after 30000ms` - `could not connect to database server at 10.0.2.45:5432` - `database server unreachable` #### Troubleshooting Steps ```bash # Test network connectivity telnet cloudflow-db.internal.company.com 5432 # Check DNS resolution nslookup cloudflow-db.internal.company.com # Verify database is accepting connections pg_isready -h cloudflow-db.internal.company.com -p 5432 -U cloudflow # Check firewall rules sudo iptables -L -n | grep 5432 # Test from CloudFlow pod network kubectl run -n cloudflow debug-pod --rm -i --tty \\ --image=postgres:14 -- \\ psql -h cloudflow-db.internal.company.com -U cloudflow -d cloudflow_production # Review database logs for connection rejections kubectl logs -n cloudflow statefulset/cloudflow-db --tail=100 | \\ grep -i \"connection\\|reject\\|authentication\" ``` ### Maximum Connection Limit (100) Reached This is a hard limit in CloudFlow's database tier. #### Permanent Solutions **Option 1: Upgrade database tier** ```bash # Check available tiers cloudflow db tiers list # Upgrade to higher tier (supports 200 connections) cloudflow db upgrade --tier standard-plus --confirm # Monitor migration progress cloudflow db migration status ``` **Option 2: Implement aggressive connection reuse** ```bash # Reduce connection lifetime cloudflow config set db.pool.max_lifetime 600 # 10 minutes # Enable prepared statement caching cloudflow config set db.prepared_statements.cache true cloudflow config set db.prepared_statements.max_size 250 # Reduce idle connection timeout cloudflow config set db.pool.idle_timeout 120 # 2 minutes ``` --- ## Workflow Execution Failures ### Timeout Errors (3600 second limit) #### Error Message ``` WorkflowExecutionError: Workflow exceeded maximum execution time of 3600 seconds Status: TIMEOUT Workflow ID: wf_9k2n4m8p1q Execution ID: exec_7h3j6k9m2n ``` #### Analysis ```bash # Get workflow execution details cloudflow workflows executions get exec_7h3j6k9m2n --verbose # Check step-by-step breakdown cloudflow workflows executions steps exec_7h3j6k9m2n # Sample output: # Step 1: data_ingestion - Duration: 245s - Status: SUCCESS # Step 2: data_validation - Duration: 123s - Status: SUCCESS # Step 3: data_transformation - Duration: 3189s - Status: TIMEOUT # Step 4: data_export - Duration: 0s - Status: SKIPPED # Identify bottleneck step cloudflow workflows analyze",
              "score": null
            },
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_1",
              "content": "create cluster -f cluster-config.yaml ``` ### Storage Configuration Install the EBS CSI driver for persistent volumes: ```bash eksctl create addon --name aws-ebs-csi-driver \\ --cluster cloudflow-production \\ --service-account-role-arn arn:aws:iam::ACCOUNT_ID:role/AmazonEKS_EBS_CSI_DriverRole \\ --force ``` --- ## Kubernetes Deployment ### Namespace Setup Create the CloudFlow namespace and configure resource quotas: ```yaml # namespace.yaml apiVersion: v1 kind: Namespace metadata: name: cloudflow-prod labels: name: cloudflow-prod environment: production --- apiVersion: v1 kind: ResourceQuota metadata: name: cloudflow-quota namespace: cloudflow-prod spec: hard: requests.cpu: \"50\" requests.memory: 100Gi persistentvolumeclaims: \"10\" services.loadbalancers: \"2\" ``` Apply the namespace configuration: ```bash kubectl apply -f namespace.yaml ``` ### Environment Configuration CloudFlow requires the following environment variables: | Variable | Description | Example | Required | |----------|-------------|---------|----------| | `DATABASE_URL` | PostgreSQL connection string | `postgresql://user:pass@host:5432/cloudflow` | Yes | | `REDIS_URL` | Redis connection string | `redis://redis-master.cloudflow-prod.svc.cluster.local:6379` | Yes | | `JWT_SECRET` | Secret key for JWT token signing | `<generated-secret-256-bit>` | Yes | | `LOG_LEVEL` | Application log verbosity | `info`, `debug`, `warn`, `error` | No (default: `info`) | | `API_PORT` | API server port | `3000` | No (default: `3000`) | | `WORKER_CONCURRENCY` | Number of concurrent workers | `10` | No (default: `5`) | | `SESSION_SECRET` | Session encryption key | `<generated-secret-256-bit>` | Yes | | `AWS_REGION` | AWS region for services | `us-east-1` | Yes | | `S3_BUCKET` | S3 bucket for file storage | `cloudflow-prod-storage` | Yes | | `SMTP_HOST` | Email server hostname | `smtp.sendgrid.net` | No | | `SMTP_PORT` | Email server port | `587` | No | | `METRICS_ENABLED` | Enable Prometheus metrics | `true` | No (default: `false`) | Create a Kubernetes secret for sensitive variables: ```bash kubectl create secret generic cloudflow-secrets \\ --namespace cloudflow-prod \\ --from-literal=DATABASE_URL=\"postgresql://cloudflow:$(cat db-password.txt)@pgbouncer.cloudflow-prod.svc.cluster.local:5432/cloudflow\" \\ --from-literal=REDIS_URL=\"redis://redis-master.cloudflow-prod.svc.cluster.local:6379\" \\ --from-literal=JWT_SECRET=\"$(openssl rand -base64 32)\" \\ --from-literal=SESSION_SECRET=\"$(openssl rand -base64 32)\" ``` ### Helm Chart Deployment Add the CloudFlow Helm repository: ```bash helm repo add cloudflow https://charts.cloudflow.io helm repo update ``` Create a values file for production configuration: ```yaml # values-production.yaml replicaCount: 3 image: repository: 123456789012.dkr.ecr.us-east-1.amazonaws.com/cloudflow tag: \"2.4.0\" pullPolicy: IfNotPresent resources: limits: cpu: 2000m memory: 4Gi requests: cpu: 2000m memory: 4Gi autoscaling: enabled: true minReplicas: 3 maxReplicas: 10 targetCPUUtilizationPercentage: 70 targetMemoryUtilizationPercentage: 80 service: type: ClusterIP port: 80 targetPort: 3000 annotations: service.beta.kubernetes.io/aws-load-balancer-type: \"nlb\" ingress: enabled: true className: nginx annotations: cert-manager.io/cluster-issuer: \"letsencrypt-prod\" nginx.ingress.kubernetes.io/ssl-redirect: \"true\" hosts: - host: api.cloudflow.io paths: - path: / pathType: Prefix tls:",
              "score": null
            }
          ],
          "latency_ms": 1060.731918001693,
          "query_id": "realistic_015",
          "dimension": "contextual",
          "query": "My app is failing with connection pool errors, how do I troubleshoot?"
        },
        {
          "key_facts": [
            "could not obtain connection from pool within 5000ms",
            "connection pool exhausted (100/100 connections in use)",
            "PgBouncer",
            "max_db_connections = 100"
          ],
          "found_facts": [
            "could not obtain connection from pool within 5000ms",
            "connection pool exhausted (100/100 connections in use)",
            "PgBouncer"
          ],
          "missed_facts": [
            "max_db_connections = 100"
          ],
          "coverage": 0.75,
          "expected_docs": [
            "troubleshooting_guide",
            "deployment_guide"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_3",
              "content": "Workflow Context Accumulation** Large workflow executions may accumulate state in memory. **Solution:** ```bash # Configure context cleanup cloudflow config set workflow.context.max_size_mb 100 cloudflow config set workflow.context.cleanup_threshold 0.8 # Enable context persistence to disk cloudflow config set workflow.context.persistence.enabled true cloudflow config set workflow.context.persistence.backend s3 ``` **2. Connection Pool Leaks** **Diagnosis:** ```bash # Check active connections cloudflow db connections --verbose # Expected output: # Active: 45/100 # Idle: 23 # Waiting: 2 # Average age: 245s ``` **Resolution:** ```bash # Adjust connection pool settings cloudflow config set db.pool.max_connections 100 cloudflow config set db.pool.min_connections 10 cloudflow config set db.pool.idle_timeout 300 cloudflow config set db.pool.max_lifetime 1800 # Force connection pool reset cloudflow db pool reset --confirm ``` **3. Event Stream Buffers** Unbounded event buffers can cause memory exhaustion. ```bash # Configure event buffer limits cloudflow config set events.buffer.max_size 10000 cloudflow config set events.buffer.overflow_strategy drop_oldest # Enable event streaming to external sink cloudflow config set events.sink.type kafka cloudflow config set events.sink.kafka.brokers \"kafka-1:9092,kafka-2:9092\" cloudflow config set events.sink.kafka.topic cloudflow-events ``` --- ## Database Connection Issues ### Connection Pool Exhaustion #### Symptoms - Error: `could not obtain connection from pool within 5000ms` - Error: `connection pool exhausted (100/100 connections in use)` - API requests fail with `503 Service Unavailable` - Database CPU usage normal, but connection count at maximum #### Investigation ```bash # Check current connection pool status cloudflow db pool status --detailed # Output example: # Pool Statistics: # Total Connections: 100/100 (100%) # Active: 87 # Idle: 13 # Waiting Requests: 45 # Average Wait Time: 3420ms # Max Wait Time: 8234ms # Identify long-running queries cloudflow db queries --status running --min-duration 30000 # Check connection distribution by client SELECT application_name, state, COUNT(*) as conn_count, AVG(EXTRACT(EPOCH FROM (NOW() - state_change))) as avg_duration_sec FROM pg_stat_activity WHERE datname = 'cloudflow_production' GROUP BY application_name, state ORDER BY conn_count DESC; ``` #### Resolution **Immediate mitigation:** ```bash # Temporarily increase connection limit (requires database restart) cloudflow db config set max_connections 150 # Kill idle connections older than 5 minutes cloudflow db connections kill --idle-timeout 300 # Restart connection pool without downtime kubectl rollout restart deployment/cloudflow-api -n cloudflow kubectl rollout status deployment/cloudflow-api -n cloudflow ``` **Long-term solutions:** 1.",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_5",
              "content": "Success/failure events published back to Kafka ``` **Event Schema**: ```json { \"event_id\": \"uuid-v4\", \"event_type\": \"workflow.execution.completed\", \"timestamp\": \"2026-01-15T10:30:00.000Z\", \"correlation_id\": \"request-trace-id\", \"payload\": { \"workflow_id\": \"wf-12345\", \"execution_id\": \"exec-67890\", \"status\": \"COMPLETED\", \"duration_ms\": 4230 }, \"metadata\": { \"source_service\": \"workflow-engine\", \"schema_version\": \"1.0\" } } ``` ### Inter-Service Communication Services communicate using two primary patterns: **Synchronous (gRPC)**: - Auth Service validation calls from API Gateway - User Service profile lookups - Low latency requirement (< 50ms) - Request-response pattern with circuit breaker **Asynchronous (Kafka)**: - Workflow execution events - Notification triggers - Audit log events - Eventual consistency acceptable --- ## Database Architecture ### PostgreSQL Primary Database **Cluster Configuration**: - Primary-replica setup with 1 primary + 2 read replicas - Instance type: db.r6g.2xlarge (8 vCPU, 64GB RAM) - Storage: 2TB gp3 SSD with 12,000 IOPS - Multi-AZ deployment for high availability - Automated backups: Daily snapshots, 30-day retention - Point-in-time recovery: 5-minute granularity **Database Schema Design**: ``` Core Tables: - users (5M rows): User accounts and profiles - workflows (2M rows): Workflow definitions - workflow_executions (500M rows, partitioned): Execution history - workflow_steps (2B rows, partitioned): Individual step records - schedules (100K rows): Scheduled workflow triggers - notifications (1B rows, partitioned): Notification delivery log Partitioning Strategy: - workflow_executions: Monthly partitions by created_at - workflow_steps: Monthly partitions by created_at - notifications: Weekly partitions by created_at - Automatic partition creation via cron job - Retention policy: Drop partitions older than 12 months ``` **Indexing Strategy**: - Primary keys: UUIDs with B-tree indexes - Frequently queried columns: Compound indexes (e.g., user_id + status + created_at) - JSONB columns: GIN indexes for workflow definitions - Full-text search: GiST indexes on description fields **Connection Pooling**: - PgBouncer in transaction mode - Pool size: 100 connections per service - Max client connections: 2000 - Connection timeout: 30 seconds **Performance Characteristics**: - Read query P99: < 20ms - Write query P99: < 50ms - Transaction throughput: 15,000 TPS - Replication lag: < 500ms --- ### Redis Caching Layer **Cluster Configuration**: - Redis Cluster with 6 nodes (3 primary + 3 replica) - Instance type: cache.r6g.xlarge (4 vCPU, 26GB RAM) - Total memory: 78GB usable cache space - Persistence: RDB snapshots every 5 minutes + AOF - Eviction policy: allkeys-lru **Cache Usage Patterns**: 1.",
              "score": null
            },
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_7",
              "content": "podSelector: matchLabels: app: cloudflow policyTypes: - Ingress - Egress ingress: - from: - namespaceSelector: matchLabels: name: cloudflow-prod - podSelector: matchLabels: app: nginx-ingress ports: - protocol: TCP port: 3000 egress: - to: - podSelector: matchLabels: app: postgresql ports: - protocol: TCP port: 5432 - to: - podSelector: matchLabels: app: redis ports: - protocol: TCP port: 6379 ``` ### Pod Security Standards Enforce Pod Security Standards at the namespace level: ```bash kubectl label namespace cloudflow-prod \\ pod-security.kubernetes.io/enforce=restricted \\ pod-security.kubernetes.io/audit=restricted \\ pod-security.kubernetes.io/warn=restricted ``` ### Secrets Management Use AWS Secrets Manager for sensitive credentials: ```bash # Install External Secrets Operator helm install external-secrets external-secrets/external-secrets \\ --namespace external-secrets-system \\ --create-namespace # Create SecretStore kubectl apply -f - <<EOF apiVersion: external-secrets.io/v1beta1 kind: SecretStore metadata: name: aws-secrets-manager namespace: cloudflow-prod spec: provider: aws: service: SecretsManager region: us-east-1 auth: jwt: serviceAccountRef: name: external-secrets-sa EOF ``` --- ## Troubleshooting ### Common Issues #### Pods in CrashLoopBackOff **Symptoms**: Pods continuously restart **Diagnosis**: ```bash kubectl logs -n cloudflow-prod <pod-name> --previous kubectl describe pod -n cloudflow-prod <pod-name> ``` **Common Causes**: - Database connection failure - Invalid environment variables - Insufficient resources #### High Memory Usage **Symptoms**: Pods being OOMKilled **Diagnosis**: ```bash kubectl top pods -n cloudflow-prod ``` **Resolution**: - Increase memory limits in deployment - Check for memory leaks in application logs - Review heap dump for Node.js processes #### Database Connection Pool Exhausted **Symptoms**: \"Too many connections\" errors **Diagnosis**: ```bash # Check PgBouncer stats kubectl exec -it pgbouncer-xxx -n cloudflow-prod -- \\ psql -p 5432 -U pgbouncer pgbouncer -c \"SHOW POOLS;\" ``` **Resolution**: - Increase `max_db_connections` in PgBouncer - Optimize application connection usage - Add more PostgreSQL replicas ### Support Contacts - **On-call Engineer**: pagerduty.com/cloudflow-oncall - **Slack Channel**: #cloudflow-operations - **Documentation**: https://docs.cloudflow.io - **Status Page**: https://status.cloudflow.io --- ## Appendix ### Useful Commands ```bash # View all resources in namespace kubectl get all -n cloudflow-prod # Check resource usage kubectl top pods -n cloudflow-prod kubectl top nodes # View logs kubectl logs -f deployment/cloudflow-api -n cloudflow-prod # Execute command in pod kubectl exec -it <pod-name> -n cloudflow-prod -- /bin/sh # Port forward for local testing kubectl port-forward svc/cloudflow-api 8080:80 -n cloudflow-prod # Rollback deployment helm rollback cloudflow -n cloudflow-prod # Update deployment helm upgrade cloudflow cloudflow/cloudflow \\ --namespace cloudflow-prod \\ --values values-production.yaml ``` ### Maintenance Windows Scheduled maintenance occurs during the following windows: - **Primary**: Sunday 02:00-06:00",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_4",
              "content": "**Technology**: Node.js with worker pool pattern **Replicas**: 8 pods (production), auto-scaling 6-16 **Resource Allocation**: 2 vCPU, 4GB RAM per pod **Key Responsibilities**: - Consume notification events from Kafka - Template rendering with Handlebars (cached in Redis) - Multi-channel delivery (Email via SendGrid, SMS via Twilio, Webhooks) - Retry logic with dead letter queue for failed deliveries - Delivery status tracking and analytics - User preference management (opt-in/opt-out) **Notification Flow**: ``` Event Source \u2192 Kafka (notifications.email topic) \u2502 \u25bc Notification Service Consumer \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u25bc \u25bc \u25bc [Email Queue] [SMS Queue] [Webhook Queue] \u2502 \u2502 \u2502 \u25bc \u25bc \u25bc [SendGrid] [Twilio] [HTTP POST] \u2502 \u2502 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u25bc Delivery Status \u2192 PostgreSQL ``` **Channel Configuration**: - Email: SendGrid with IP warming, DKIM/SPF configured - SMS: Twilio with fallback to AWS SNS - Webhook: Signed payloads (HMAC-SHA256), retry 3 times with exponential backoff - In-app: WebSocket connections via Socket.io **Template Management**: - Templates stored in PostgreSQL with versioning - Compiled templates cached in Redis (1-hour TTL) - Support for localization (i18n) with 12 languages - A/B testing capability for email campaigns **Performance Targets**: - Email delivery: < 5 seconds (P99) from event to SendGrid - SMS delivery: < 3 seconds (P99) - Throughput: 10,000 notifications per minute --- ## Data Flow Architecture ### Synchronous Request Flow Client requests follow a synchronous path through the API Gateway to backend services: ``` 1. Client \u2192 Load Balancer (TLS termination) 2. Load Balancer \u2192 API Gateway (HTTP/2) 3. API Gateway \u2192 Auth Service (JWT validation via gRPC) 4. API Gateway \u2192 Target Service (HTTP/REST or gRPC) 5. Target Service \u2192 Database/Cache (data retrieval) 6. Response propagates back through the chain ``` **Timeout Configuration**: - Client \u2192 Load Balancer: 60 seconds - Load Balancer \u2192 API Gateway: 55 seconds - API Gateway \u2192 Services: 45 seconds - Service \u2192 Database: 10 seconds ### Asynchronous Event Flow Long-running operations and inter-service communication use event-driven patterns: ``` 1. Service publishes event \u2192 Kafka topic 2. Kafka persists event (replication factor: 3) 3. Consumer groups subscribe to topics 4. Consumers process events with at-least-once delivery 5. State updates written to PostgreSQL 6.",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_10",
              "content": "- Execution start rate: 500 per second - Completion rate: 450 per second (average 2-second execution time) **Database**: - Read throughput: 50,000 queries per second (across replicas) - Write throughput: 15,000 transactions per second - Connection capacity: 2,000 concurrent connections **Kafka**: - Message ingestion: 100,000 messages per second - Consumer throughput: 80,000 messages per second (aggregated) - End-to-end latency: < 100ms (P99) ### Resource Utilization **CPU Utilization** (Target: 60-70% average): - API Gateway: 55% average, 80% peak - Workflow Engine: 65% average, 85% peak - Auth Service: 40% average, 60% peak **Memory Utilization** (Target: < 80%): - API Gateway: 2.5GB average per pod (4GB allocated) - Workflow Engine: 6GB average per pod (8GB allocated) - Notification Service: 2.8GB average per pod (4GB allocated) **Network Throughput**: - Ingress: 2 Gbps average, 5 Gbps peak - Egress: 1.5 Gbps average, 4 Gbps peak - Internal (service mesh): 8 Gbps average ### Monitoring & Alerting **Key Metrics** (Monitored via Prometheus): - Request rate (per service, per endpoint) - Error rate (4xx, 5xx responses) - Latency percentiles (P50, P95, P99) - Resource utilization (CPU, memory, disk) - Cache hit/miss ratios - Database connection pool saturation - Kafka consumer lag **Alerts** (via PagerDuty): - P1 (Immediate): API error rate > 1%, database connection failure - P2 (< 15 min): P99 latency > 500ms, cache hit rate < 80% - P3 (< 1 hour): Resource utilization > 85%, replica count < minimum **SLI/SLO/SLA**: - SLI: API success rate (non-5xx responses) - SLO: 99.9% uptime per month (43 minutes downtime allowance) - SLA: 99.5% uptime guarantee (customer-facing SLA with credits) --- ## Disaster Recovery ### Recovery Objectives **RPO (Recovery Point Objective): 1 hour** - Maximum acceptable data loss: 1 hour of transactions - Achieved through: Continuous database replication + hourly snapshots - Kafka retention: 7 days allows event replay **RTO (Recovery Time Objective): 4 hours** - Maximum acceptable downtime: 4 hours for full system recovery - Includes: Failover, data verification, and service restoration - Automated runbooks reduce RTO to < 2 hours for common scenarios ### High Availability Architecture **Multi-AZ Deployment**: ``` Region: us-east-1 AZ-1a: AZ-1b: AZ-1c: - API Gateway (4) - API Gateway (4) - API Gateway (4) - Workflow Engine (6) - Workflow Engine (5) - Workflow Engine (5) - Auth Service (3) - Auth Service (3) - Auth",
              "score": null
            }
          ],
          "latency_ms": 1025.0361760008673,
          "query_id": "realistic_015",
          "dimension": "negation",
          "query": "Why can't I get a database connection even though CPU is low?"
        },
        {
          "key_facts": [
            "OAuth 2.0",
            "API keys",
            "JWT tokens"
          ],
          "found_facts": [
            "OAuth 2.0",
            "API keys",
            "JWT tokens"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "api_reference"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_0",
              "content": "# CloudFlow API Reference Version 2.1.0 | Last Updated: January 2026 ## Overview The CloudFlow API is a RESTful service that enables developers to programmatically manage cloud workflows, data pipelines, and automation tasks. This documentation provides comprehensive details on authentication, endpoints, request/response formats, error handling, and best practices. **Base URL:** `https://api.cloudflow.io/v2` **API Status:** https://status.cloudflow.io ## Authentication CloudFlow supports three authentication methods to suit different use cases and security requirements. ### API Keys API keys provide simple authentication for server-to-server communication. Include your API key in the request header: ```bash curl -H \"X-API-Key: cf_live_a1b2c3d4e5f6g7h8i9j0\" \\ https://api.cloudflow.io/v2/workflows ``` **Security Notes:** - Never expose API keys in client-side code - Rotate keys every 90 days - Use separate keys for development and production environments ### OAuth 2.0 OAuth 2.0 is recommended for applications that access CloudFlow on behalf of users. We support the Authorization Code flow with PKCE. **Authorization Endpoint:** `https://auth.cloudflow.io/oauth/authorize` **Token Endpoint:** `https://auth.cloudflow.io/oauth/token` **Supported Scopes:** - `workflows:read` - Read workflow configurations - `workflows:write` - Create and modify workflows - `pipelines:read` - Read pipeline data - `pipelines:write` - Create and manage pipelines - `analytics:read` - Access analytics and metrics - `admin:full` - Full administrative access Example authorization request: ```bash curl -X POST https://auth.cloudflow.io/oauth/token \\ -H \"Content-Type: application/x-www-form-urlencoded\" \\ -d \"grant_type=authorization_code\" \\ -d \"code=AUTH_CODE_HERE\" \\ -d \"client_id=YOUR_CLIENT_ID\" \\ -d \"client_secret=YOUR_CLIENT_SECRET\" \\ -d \"redirect_uri=https://yourapp.com/callback\" ``` ### JWT Tokens For advanced use cases, CloudFlow supports JSON Web Tokens (JWT) with RS256 signing algorithm. JWTs must include the following claims: - `iss` (issuer): Your application identifier - `sub` (subject): User or service account ID - `aud` (audience): `https://api.cloudflow.io` - `exp` (expiration): Unix timestamp (max 3600 seconds from `iat`) - `iat` (issued at): Unix timestamp - `scope`: Space-separated list of requested scopes Example JWT header: ```python import jwt import time from cryptography.hazmat.primitives import serialization from cryptography.hazmat.backends import default_backend # Load your private key with open('private_key.pem', 'rb') as key_file: private_key = serialization.load_pem_private_key( key_file.read(), password=None, backend=default_backend() ) # Create JWT payload payload = { 'iss': 'your-app-id', 'sub': 'user-12345', 'aud': 'https://api.cloudflow.io', 'exp': int(time.time()) + 3600, 'iat': int(time.time()), 'scope': 'workflows:read workflows:write' } # Generate token token = jwt.encode(payload, private_key, algorithm='RS256') # Use in API request headers = {'Authorization': f'Bearer {token}'} ``` **Token Expiration:** All tokens expire after 3600 seconds (1 hour). Implement token refresh logic in your application.",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_0",
              "content": "# CloudFlow Platform Troubleshooting Guide **Version:** 3.2.1 **Last Updated:** January 2026 **Audience:** Platform Engineers, DevOps, Support Teams ## Table of Contents 1. [Overview](#overview) 2. [Authentication & Authorization Issues](#authentication--authorization-issues) 3. [Performance Problems](#performance-problems) 4. [Database Connection Issues](#database-connection-issues) 5. [Workflow Execution Failures](#workflow-execution-failures) 6. [Rate Limiting & Throttling](#rate-limiting--throttling) 7. [Log Analysis & Debugging](#log-analysis--debugging) 8. [Escalation Procedures](#escalation-procedures) --- ## Overview This guide provides comprehensive troubleshooting steps for common CloudFlow platform issues encountered in production environments. Each section includes error symptoms, root cause analysis, resolution steps, and preventive measures. ### Quick Diagnostic Checklist Before diving into specific issues, perform these initial checks: - Verify service health: `cloudflow status --all` - Check API connectivity: `curl -I https://api.cloudflow.io/health` - Review recent deployments: `kubectl get deployments -n cloudflow --sort-by=.metadata.creationTimestamp` - Inspect platform metrics: `cloudflow metrics --last 1h` --- ## Authentication & Authorization Issues ### 401 Unauthorized Errors #### Symptoms - API requests return `401 Unauthorized` - Error message: `Authentication credentials were not provided or are invalid` - Frontend displays \"Session expired\" message #### Common Causes **1. Token Expiration** CloudFlow access tokens expire after 3600 seconds (1 hour) by default. Refresh tokens are valid for 30 days. **Verification:** ```bash # Decode JWT to check expiration echo $CF_ACCESS_TOKEN | cut -d'.' -f2 | base64 -d | jq '.exp' # Compare with current time date +%s ``` **Resolution:** ```bash # Refresh the access token curl -X POST https://api.cloudflow.io/auth/refresh \\ -H \"Content-Type: application/json\" \\ -d '{\"refresh_token\": \"'$CF_REFRESH_TOKEN'\"}' # Update environment variable export CF_ACCESS_TOKEN=\"<new_token>\" ``` **2. Invalid JWT Signature** Error: `JWT signature verification failed` **Causes:** - Token was modified or corrupted - Using wrong signing key - Token generated with different secret **Resolution:** ```bash # Validate token structure cloudflow auth validate --token $CF_ACCESS_TOKEN # Generate new token cloudflow auth login --username <user> --password <pass> # For service accounts cloudflow auth service-account --create --name <sa-name> --scopes \"workflow:read,workflow:write\" ``` **3. Clock Skew Issues** JWT validation fails when system clocks are out of sync (tolerance: \u00b1300 seconds).",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_8",
              "content": "### Cache Invalidation Strategies **Time-Based Expiration (TTL)**: - Short-lived data: 5-15 minutes (session tokens, rate limit counters) - Medium-lived data: 1 hour (workflow definitions, templates) - Long-lived data: 24 hours (static configuration) **Event-Based Invalidation**: ``` Database Update Event \u2192 Kafka (cache.invalidation topic) \u2502 \u25bc All service instances consume event \u2502 \u25bc Redis DEL for affected keys ``` **Pattern-Based Invalidation**: - Use Redis SCAN + DEL for wildcard patterns - Example: Invalidate all user caches: `user:*:{user_id}` **Cache Warming**: - Scheduled job runs every hour to pre-populate frequently accessed data - Targets: Top 1000 workflows, system configuration, active user profiles - Reduces cache miss rate during traffic spikes ### Multi-Level Caching Application implements L1 (in-memory) and L2 (Redis) caching: ``` Request \u2192 L1 Cache (Node.js Map, 1000 entry LRU) \u2502 MISS \u2502 \u25bc L2 Cache (Redis, distributed) \u2502 MISS \u2502 \u25bc Database (PostgreSQL) ``` L1 cache reduces Redis network calls by 40% for hot data. --- ## Security Architecture ### Network Security **Zero-Trust Network Model**: - All service-to-service communication encrypted with mTLS - Certificate rotation: Every 90 days (automated via cert-manager) - Certificate authority: Internal PKI with HashiCorp Vault - Network segmentation: Private subnets for services, public subnet for ALB only **Service Mesh (Istio)**: ``` Service A \u2192 Envoy Sidecar (mTLS client cert) \u2502 Encrypted channel \u2502 Envoy Sidecar (mTLS server cert) \u2192 Service B ``` **Firewall Rules**: - Security groups: Deny all by default, explicit allow rules - Ingress: Only ALB can reach API Gateway (port 8080) - Egress: Services can only reach specific dependencies - Database access: Limited to application subnets only ### Authentication & Authorization **JWT Token Validation**: - Algorithm: RS256 (asymmetric signing) - Key rotation: Every 30 days with 7-day overlap period - Public key distribution: JWKS endpoint cached in Redis - Validation: Signature, expiry, issuer, audience claims - Token revocation: Blacklist in Redis for compromised tokens **Permission Model**: ``` User \u2192 Roles \u2192 Permissions \u2198 \u2197 Tenants (Multi-tenancy isolation) ``` Example permissions: - `workflow:read` - View workflows - `workflow:write` - Create/update workflows - `workflow:execute` - Trigger workflow execution - `workflow:delete` - Delete workflows - `admin:*` - All administrative operations **API Key Management**: - Format: `cfk_live_<32-char-random>` (production), `cfk_test_<32-char-random>` (sandbox) - Hashing: SHA-256 before storage in PostgreSQL - Scoping: API keys can be scoped to specific workflows or operations - Rate limits: Configurable",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_1",
              "content": "**Technology**: Node.js with Express.js framework **Replicas**: 12 pods (production), auto-scaling 8-20 based on CPU **Resource Allocation**: 2 vCPU, 4GB RAM per pod **Key Responsibilities**: - JWT token validation (delegated to Auth Service for initial validation) - Rate limiting: 1000 requests per minute per API key (sliding window) - Request/response transformation and validation using JSON Schema - Routing to downstream services based on URL path patterns - CORS handling for web clients - Request/response logging and correlation ID injection **Critical Endpoints**: - `POST /api/v1/workflows` - Create new workflow - `GET /api/v1/workflows/:id` - Retrieve workflow status - `POST /api/v1/workflows/:id/execute` - Trigger workflow execution - `GET /api/v1/workflows/:id/history` - Get execution history **Dependencies**: - Auth Service (for token validation) - Redis (for rate limiting counters) - All downstream microservices **Performance Targets**: - P50 latency: < 50ms - P99 latency: < 200ms - Throughput: 10,000 RPS sustained --- ### Auth Service **Purpose**: Centralized authentication and authorization service handling user identity, token generation, and permission validation. **Technology**: Go with gRPC for internal communication, REST for external **Replicas**: 8 pods (production), auto-scaling 6-12 **Resource Allocation**: 1 vCPU, 2GB RAM per pod **Key Responsibilities**: - User authentication via multiple providers (OAuth2, SAML, local credentials) - JWT token generation and validation (RS256 algorithm) - Role-based access control (RBAC) with fine-grained permissions - Session management with Redis-backed storage - API key generation and validation for service accounts - MFA enforcement for administrative operations **Authentication Flow**: ``` Client Request \u2192 API Gateway \u2192 Auth Service \u2502 \u251c\u2500 Validate credentials \u251c\u2500 Check MFA if required \u251c\u2500 Generate JWT (15min expiry) \u251c\u2500 Generate refresh token (7 days) \u2514\u2500 Store session in Redis ``` **Token Structure**: - Access Token: JWT with 15-minute expiry - Refresh Token: Opaque token with 7-day expiry, stored in PostgreSQL - Claims: user_id, email, roles[], permissions[], tenant_id **Security Features**: - Password hashing: Argon2id with 64MB memory, 4 iterations - Token rotation on refresh to prevent replay attacks - Brute force protection: 5 failed attempts \u2192 15-minute lockout - Secrets stored in HashiCorp Vault, rotated every 90 days **Performance Targets**: - Token validation: < 10ms (P99) - Token generation: < 50ms (P99) - Throughput: 5,000 RPS for validation operations --- ### Workflow Engine **Purpose**: Core orchestration service that executes workflow definitions, manages state transitions, and coordinates task execution across distributed systems.",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_8",
              "content": "Leverage caching:** ```bash # Enable client-side caching export CLOUDFLOW_CACHE_ENABLED=true export CLOUDFLOW_CACHE_TTL=300 # Cache workflow metadata cloudflow workflows list --use-cache --cache-ttl 600 ``` --- ## Log Analysis & Debugging ### Accessing CloudFlow Logs #### Kubernetes Deployments ```bash # List all CloudFlow pods kubectl get pods -n cloudflow # Tail logs from API server kubectl logs -f -n cloudflow deployment/cloudflow-api --tail=100 # Get logs from specific pod kubectl logs -n cloudflow cloudflow-api-7d4f6b8c9d-x7k2m # Get logs from previous crashed pod kubectl logs -n cloudflow cloudflow-api-7d4f6b8c9d-x7k2m --previous # Get logs from all pods in deployment kubectl logs -n cloudflow deployment/cloudflow-api --all-containers=true # Stream logs from multiple pods kubectl logs -n cloudflow -l app=cloudflow-api -f --max-log-requests=10 ``` #### Log Levels CloudFlow supports the following log levels: - `TRACE` - Very detailed debugging information - `DEBUG` - Detailed debugging information - `INFO` - Informational messages (default) - `WARN` - Warning messages - `ERROR` - Error messages - `FATAL` - Fatal errors causing shutdown **Changing log levels:** ```bash # Set global log level cloudflow config set logging.level DEBUG # Set log level for specific component cloudflow config set logging.components.database DEBUG cloudflow config set logging.components.auth INFO cloudflow config set logging.components.workflows TRACE # Temporary log level increase (resets after 1 hour) cloudflow debug set-log-level DEBUG --duration 3600 # View current log configuration cloudflow config get logging --format json ``` ### Grep Patterns for Common Issues #### Authentication Failures ```bash # Find all authentication errors kubectl logs -n cloudflow deployment/cloudflow-api | \\ grep -i \"authentication\\|401\\|unauthorized\" | \\ tail -n 50 # Find JWT validation failures kubectl logs -n cloudflow deployment/cloudflow-api | \\ grep -E \"JWT|token.*invalid|signature.*failed\" # Find clock skew issues kubectl logs -n cloudflow deployment/cloudflow-api | \\ grep -i \"clock skew\\|time.*sync\\|nbf\\|exp\" ``` #### Database Errors ```bash # Find database connection errors kubectl logs -n cloudflow deployment/cloudflow-api | \\ grep -E \"connection.*pool|could not connect|database.*timeout\" # Find slow queries kubectl logs -n cloudflow deployment/cloudflow-api | \\ grep \"slow query\" | \\ awk '{print $NF}' | \\ sort -n | \\ tail -n 20 # Find deadlock errors kubectl logs -n cloudflow deployment/cloudflow-api | \\ grep -i \"deadlock detected\" ``` #### Workflow Execution Errors ```bash # Find workflow timeout errors kubectl logs -n cloudflow deployment/cloudflow-workflow-engine | \\ grep -E \"timeout|exceeded.*3600\" # Find workflow retry attempts kubectl logs -n cloudflow deployment/cloudflow-workflow-engine | \\ grep -E \"retry attempt [0-9]|retrying",
              "score": null
            }
          ],
          "latency_ms": 886.0136199982662,
          "query_id": "realistic_016",
          "dimension": "original",
          "query": "How do I handle API authentication?"
        },
        {
          "key_facts": [
            "OAuth 2.0",
            "API keys",
            "JWT tokens"
          ],
          "found_facts": [
            "OAuth 2.0",
            "API keys",
            "JWT tokens"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "api_reference"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_0",
              "content": "# CloudFlow API Reference Version 2.1.0 | Last Updated: January 2026 ## Overview The CloudFlow API is a RESTful service that enables developers to programmatically manage cloud workflows, data pipelines, and automation tasks. This documentation provides comprehensive details on authentication, endpoints, request/response formats, error handling, and best practices. **Base URL:** `https://api.cloudflow.io/v2` **API Status:** https://status.cloudflow.io ## Authentication CloudFlow supports three authentication methods to suit different use cases and security requirements. ### API Keys API keys provide simple authentication for server-to-server communication. Include your API key in the request header: ```bash curl -H \"X-API-Key: cf_live_a1b2c3d4e5f6g7h8i9j0\" \\ https://api.cloudflow.io/v2/workflows ``` **Security Notes:** - Never expose API keys in client-side code - Rotate keys every 90 days - Use separate keys for development and production environments ### OAuth 2.0 OAuth 2.0 is recommended for applications that access CloudFlow on behalf of users. We support the Authorization Code flow with PKCE. **Authorization Endpoint:** `https://auth.cloudflow.io/oauth/authorize` **Token Endpoint:** `https://auth.cloudflow.io/oauth/token` **Supported Scopes:** - `workflows:read` - Read workflow configurations - `workflows:write` - Create and modify workflows - `pipelines:read` - Read pipeline data - `pipelines:write` - Create and manage pipelines - `analytics:read` - Access analytics and metrics - `admin:full` - Full administrative access Example authorization request: ```bash curl -X POST https://auth.cloudflow.io/oauth/token \\ -H \"Content-Type: application/x-www-form-urlencoded\" \\ -d \"grant_type=authorization_code\" \\ -d \"code=AUTH_CODE_HERE\" \\ -d \"client_id=YOUR_CLIENT_ID\" \\ -d \"client_secret=YOUR_CLIENT_SECRET\" \\ -d \"redirect_uri=https://yourapp.com/callback\" ``` ### JWT Tokens For advanced use cases, CloudFlow supports JSON Web Tokens (JWT) with RS256 signing algorithm. JWTs must include the following claims: - `iss` (issuer): Your application identifier - `sub` (subject): User or service account ID - `aud` (audience): `https://api.cloudflow.io` - `exp` (expiration): Unix timestamp (max 3600 seconds from `iat`) - `iat` (issued at): Unix timestamp - `scope`: Space-separated list of requested scopes Example JWT header: ```python import jwt import time from cryptography.hazmat.primitives import serialization from cryptography.hazmat.backends import default_backend # Load your private key with open('private_key.pem', 'rb') as key_file: private_key = serialization.load_pem_private_key( key_file.read(), password=None, backend=default_backend() ) # Create JWT payload payload = { 'iss': 'your-app-id', 'sub': 'user-12345', 'aud': 'https://api.cloudflow.io', 'exp': int(time.time()) + 3600, 'iat': int(time.time()), 'scope': 'workflows:read workflows:write' } # Generate token token = jwt.encode(payload, private_key, algorithm='RS256') # Use in API request headers = {'Authorization': f'Bearer {token}'} ``` **Token Expiration:** All tokens expire after 3600 seconds (1 hour). Implement token refresh logic in your application.",
              "score": null
            },
            {
              "doc_id": "user_guide",
              "chunk_id": "user_guide_fix_2",
              "content": "### HTTP Requests Make HTTP requests to any API endpoint: **Configuration:** - **Method**: GET, POST, PUT, PATCH, DELETE, HEAD - **URL**: Full endpoint URL (supports variable interpolation) - **Headers**: Custom headers as key-value pairs - **Query Parameters**: URL parameters - **Body**: JSON, form data, or raw text - **Authentication**: Basic Auth, Bearer Token, API Key, OAuth 2.0 **Example:** ```yaml - id: fetch_user action: http_request config: method: GET url: \"https://api.example.com/users/{{user_id}}\" headers: Authorization: \"Bearer {{secrets.API_TOKEN}}\" Content-Type: \"application/json\" timeout: 30 ``` **Response Handling:** Access the response in subsequent steps: - `{{steps.fetch_user.status}}` - HTTP status code - `{{steps.fetch_user.body}}` - Response body - `{{steps.fetch_user.headers}}` - Response headers ### Database Queries Execute queries against supported databases (PostgreSQL, MySQL, MongoDB, Redis): **SQL Databases (PostgreSQL, MySQL):** ```yaml - id: get_orders action: database_query config: connection: \"{{secrets.DB_CONNECTION_STRING}}\" query: | SELECT * FROM orders WHERE customer_id = $1 AND status = $2 ORDER BY created_at DESC LIMIT 10 parameters: - \"{{trigger.customer_id}}\" - \"pending\" ``` **MongoDB:** ```yaml - id: find_documents action: mongodb_query config: connection: \"{{secrets.MONGO_URI}}\" database: \"production\" collection: \"users\" operation: find filter: email: \"{{trigger.email}}\" options: limit: 1 ``` **Important Notes:** - Always use parameterized queries to prevent SQL injection - Connection strings should be stored in secrets - Query timeout is 30 seconds by default - Maximum result set size is 10MB ### Email Notifications Send emails via SMTP or integrated email providers: **Configuration:** ```yaml - id: send_notification action: email config: provider: \"smtp\" # or \"sendgrid\", \"mailgun\", \"ses\" from: \"notifications@cloudflow.io\" to: \"{{trigger.recipient}}\" cc: \"manager@company.com\" subject: \"Alert: {{alert_type}}\" body: | Hello {{user.name}}, This is an automated notification about {{event_description}}. Details: - Time: {{timestamp}} - Type: {{alert_type}} - Priority: {{priority}} Best regards, CloudFlow Automation html: true attachments: - name: \"report.pdf\" content: \"{{steps.generate_report.output}}\" encoding: \"base64\" ``` **Email Templates:** CloudFlow supports dynamic templates with conditional content: ``` {{#if priority == 'high'}} \u26a0\ufe0f URGENT: Immediate attention required {{else}} \u2139\ufe0f Informational notification {{/if}} ``` ### Slack Messages Send messages to Slack channels or users: **Channel Messages:** ```yaml - id: notify_team action: slack_message config: channel: \"#deployments\" text: \"Deployment completed successfully!\" blocks: - type: \"section\" text: type: \"mrkdwn\" text: \"*Deployment Summary*\\nEnvironment: {{environment}}\\nVersion: {{version}}\\nDuration: {{duration}}s\" - type: \"actions\" elements: - type: \"button\" text: \"View Logs\" url: \"{{logs_url}}\" thread_ts: \"{{trigger.thread_ts}}\" # Reply to thread ``` **Direct Messages:** ```yaml - id: dm_user action: slack_message config: user: \"{{user_slack_id}}\" text: \"Your report is ready for review.\"",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_1",
              "content": "**Technology**: Node.js with Express.js framework **Replicas**: 12 pods (production), auto-scaling 8-20 based on CPU **Resource Allocation**: 2 vCPU, 4GB RAM per pod **Key Responsibilities**: - JWT token validation (delegated to Auth Service for initial validation) - Rate limiting: 1000 requests per minute per API key (sliding window) - Request/response transformation and validation using JSON Schema - Routing to downstream services based on URL path patterns - CORS handling for web clients - Request/response logging and correlation ID injection **Critical Endpoints**: - `POST /api/v1/workflows` - Create new workflow - `GET /api/v1/workflows/:id` - Retrieve workflow status - `POST /api/v1/workflows/:id/execute` - Trigger workflow execution - `GET /api/v1/workflows/:id/history` - Get execution history **Dependencies**: - Auth Service (for token validation) - Redis (for rate limiting counters) - All downstream microservices **Performance Targets**: - P50 latency: < 50ms - P99 latency: < 200ms - Throughput: 10,000 RPS sustained --- ### Auth Service **Purpose**: Centralized authentication and authorization service handling user identity, token generation, and permission validation. **Technology**: Go with gRPC for internal communication, REST for external **Replicas**: 8 pods (production), auto-scaling 6-12 **Resource Allocation**: 1 vCPU, 2GB RAM per pod **Key Responsibilities**: - User authentication via multiple providers (OAuth2, SAML, local credentials) - JWT token generation and validation (RS256 algorithm) - Role-based access control (RBAC) with fine-grained permissions - Session management with Redis-backed storage - API key generation and validation for service accounts - MFA enforcement for administrative operations **Authentication Flow**: ``` Client Request \u2192 API Gateway \u2192 Auth Service \u2502 \u251c\u2500 Validate credentials \u251c\u2500 Check MFA if required \u251c\u2500 Generate JWT (15min expiry) \u251c\u2500 Generate refresh token (7 days) \u2514\u2500 Store session in Redis ``` **Token Structure**: - Access Token: JWT with 15-minute expiry - Refresh Token: Opaque token with 7-day expiry, stored in PostgreSQL - Claims: user_id, email, roles[], permissions[], tenant_id **Security Features**: - Password hashing: Argon2id with 64MB memory, 4 iterations - Token rotation on refresh to prevent replay attacks - Brute force protection: 5 failed attempts \u2192 15-minute lockout - Secrets stored in HashiCorp Vault, rotated every 90 days **Performance Targets**: - Token validation: < 10ms (P99) - Token generation: < 50ms (P99) - Throughput: 5,000 RPS for validation operations --- ### Workflow Engine **Purpose**: Core orchestration service that executes workflow definitions, manages state transitions, and coordinates task execution across distributed systems.",
              "score": null
            },
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_5",
              "content": "**Supported Events:** - `workflow.started` - `workflow.completed` - `workflow.failed` - `pipeline.completed` - `pipeline.failed` **Webhook Payload Example:** ```json { \"event\": \"workflow.completed\", \"timestamp\": \"2026-01-24T10:45:00Z\", \"data\": { \"workflow_id\": \"wf_8x7k2m9p\", \"execution_id\": \"exec_9k3m7n2q\", \"status\": \"completed\", \"duration_ms\": 3420, \"records_processed\": 1247 } } ``` Configure webhooks in your account settings or via the API: ```bash curl -X POST https://api.cloudflow.io/v2/webhooks \\ -H \"X-API-Key: YOUR_API_KEY\" \\ -H \"Content-Type: application/json\" \\ -d '{ \"url\": \"https://your-app.com/cloudflow-webhook\", \"events\": [\"workflow.completed\", \"workflow.failed\"], \"secret\": \"whsec_your_webhook_secret\" }' ``` ## Support For additional help and resources: - **Documentation:** https://docs.cloudflow.io - **API Status:** https://status.cloudflow.io - **Support Email:** support@cloudflow.io - **Community Forum:** https://community.cloudflow.io Enterprise customers have access to 24/7 priority support via phone and dedicated Slack channels.",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_9",
              "content": "per API key (default: 1000 RPM) ### Secrets Management **HashiCorp Vault Integration**: - Dynamic database credentials: Generated on-demand, 1-hour TTL - Encryption keys: Transit secrets engine for encryption-as-a-service - API keys for external services: Stored with versioning - Rotation policy: Automated rotation every 90 days with notification **Secret Access Pattern**: ``` Service starts \u2192 Vault authentication (Kubernetes service account) \u2502 \u25bc Request secret lease (1-hour TTL) \u2502 \u25bc Vault returns dynamic credentials \u2502 \u25bc Service connects to PostgreSQL \u2502 Renew lease every 30 minutes ``` ### Data Protection **Encryption at Rest**: - PostgreSQL: AES-256 encryption enabled at volume level - Redis: Encryption enabled using AWS KMS - Kafka: Encryption at rest for all data on brokers - S3 backups: Server-side encryption with KMS (SSE-KMS) **Encryption in Transit**: - External traffic: TLS 1.3 only (TLS 1.2 deprecated) - Internal traffic: mTLS via Istio service mesh - Database connections: SSL/TLS enforced (sslmode=require) **Data Masking**: - Sensitive fields (email, phone) masked in logs - PII redacted from error messages and stack traces - Audit logs: Full data retention with access controls **Compliance**: - GDPR: User data export and deletion workflows - SOC 2 Type II: Audit logging for all data access - HIPAA: PHI isolation in dedicated tenants with enhanced encryption --- ## Performance Characteristics ### Latency Targets **API Operations** (P99 latency): - `GET /workflows/{id}`: < 50ms (cache hit), < 150ms (cache miss) - `POST /workflows`: < 200ms (includes validation and database write) - `POST /workflows/{id}/execute`: < 100ms (async, returns execution ID) - `GET /workflows/{id}/history`: < 300ms (paginated, 50 records per page) **Workflow Execution** (P99 latency): - Simple workflow (< 5 steps): < 2 seconds - Medium workflow (5-15 steps): < 5 seconds - Complex workflow (> 15 steps): < 15 seconds - Parallel execution: Linear scaling up to 10 concurrent branches **Database Operations** (P99 latency): - Simple SELECT: < 5ms - JOIN query: < 20ms - INSERT/UPDATE: < 10ms - Batch operations: < 100ms (batch size: 100 records) **Cache Operations** (P99 latency): - Redis GET: < 2ms - Redis SET: < 3ms - Redis DEL: < 2ms ### Throughput Capacity **API Gateway**: - Sustained: 10,000 requests per second - Peak: 25,000 requests per second (5-minute burst) - Rate limiting: 1,000 requests per minute per API key **Workflow Engine**: - Concurrent executions: 8,000 workflows (across 16 pods)",
              "score": null
            }
          ],
          "latency_ms": 1057.0293349992426,
          "query_id": "realistic_016",
          "dimension": "synonym",
          "query": "What authentication methods are supported?"
        },
        {
          "key_facts": [
            "OAuth 2.0",
            "API keys",
            "JWT tokens"
          ],
          "found_facts": [
            "OAuth 2.0",
            "API keys",
            "JWT tokens"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "api_reference"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_0",
              "content": "# CloudFlow Platform Troubleshooting Guide **Version:** 3.2.1 **Last Updated:** January 2026 **Audience:** Platform Engineers, DevOps, Support Teams ## Table of Contents 1. [Overview](#overview) 2. [Authentication & Authorization Issues](#authentication--authorization-issues) 3. [Performance Problems](#performance-problems) 4. [Database Connection Issues](#database-connection-issues) 5. [Workflow Execution Failures](#workflow-execution-failures) 6. [Rate Limiting & Throttling](#rate-limiting--throttling) 7. [Log Analysis & Debugging](#log-analysis--debugging) 8. [Escalation Procedures](#escalation-procedures) --- ## Overview This guide provides comprehensive troubleshooting steps for common CloudFlow platform issues encountered in production environments. Each section includes error symptoms, root cause analysis, resolution steps, and preventive measures. ### Quick Diagnostic Checklist Before diving into specific issues, perform these initial checks: - Verify service health: `cloudflow status --all` - Check API connectivity: `curl -I https://api.cloudflow.io/health` - Review recent deployments: `kubectl get deployments -n cloudflow --sort-by=.metadata.creationTimestamp` - Inspect platform metrics: `cloudflow metrics --last 1h` --- ## Authentication & Authorization Issues ### 401 Unauthorized Errors #### Symptoms - API requests return `401 Unauthorized` - Error message: `Authentication credentials were not provided or are invalid` - Frontend displays \"Session expired\" message #### Common Causes **1. Token Expiration** CloudFlow access tokens expire after 3600 seconds (1 hour) by default. Refresh tokens are valid for 30 days. **Verification:** ```bash # Decode JWT to check expiration echo $CF_ACCESS_TOKEN | cut -d'.' -f2 | base64 -d | jq '.exp' # Compare with current time date +%s ``` **Resolution:** ```bash # Refresh the access token curl -X POST https://api.cloudflow.io/auth/refresh \\ -H \"Content-Type: application/json\" \\ -d '{\"refresh_token\": \"'$CF_REFRESH_TOKEN'\"}' # Update environment variable export CF_ACCESS_TOKEN=\"<new_token>\" ``` **2. Invalid JWT Signature** Error: `JWT signature verification failed` **Causes:** - Token was modified or corrupted - Using wrong signing key - Token generated with different secret **Resolution:** ```bash # Validate token structure cloudflow auth validate --token $CF_ACCESS_TOKEN # Generate new token cloudflow auth login --username <user> --password <pass> # For service accounts cloudflow auth service-account --create --name <sa-name> --scopes \"workflow:read,workflow:write\" ``` **3. Clock Skew Issues** JWT validation fails when system clocks are out of sync (tolerance: \u00b1300 seconds).",
              "score": null
            },
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_4",
              "content": "### Error Response Format ```json { \"error\": { \"code\": \"invalid_parameter\", \"message\": \"The 'limit' parameter must be between 1 and 100\", \"field\": \"limit\", \"request_id\": \"req_8k3m9x2p\" } } ``` ### HTTP Status Codes | Status Code | Description | Common Causes | |------------|-------------|---------------| | 400 | Bad Request | Invalid parameters, malformed JSON, validation errors | | 401 | Unauthorized | Missing or invalid authentication credentials | | 403 | Forbidden | Insufficient permissions for requested resource | | 404 | Not Found | Resource does not exist or has been deleted | | 429 | Too Many Requests | Rate limit exceeded, retry after specified period | | 500 | Internal Server Error | Unexpected server error, contact support if persists | | 502 | Bad Gateway | Temporary service issue, retry with exponential backoff | | 503 | Service Unavailable | Scheduled maintenance or temporary outage | ### Error Codes CloudFlow returns specific error codes to help you identify and resolve issues: - `invalid_parameter`: One or more request parameters are invalid - `missing_required_field`: Required field is missing from request body - `authentication_failed`: Invalid API key or token - `insufficient_permissions`: User lacks required scope or permission - `resource_not_found`: Requested resource does not exist - `rate_limit_exceeded`: Too many requests, see rate limiting section - `workflow_execution_failed`: Workflow execution encountered an error - `invalid_json`: Request body contains malformed JSON - `duplicate_resource`: Resource with same identifier already exists - `quota_exceeded`: Account quota limit reached ### Error Handling Best Practices **Python Example:** ```python import requests import time def make_api_request(url, headers, max_retries=3): for attempt in range(max_retries): try: response = requests.get(url, headers=headers) if response.status_code == 200: return response.json() elif response.status_code == 429: # Rate limit exceeded retry_after = int(response.headers.get('X-RateLimit-Reset', 60)) print(f\"Rate limited. Waiting {retry_after} seconds...\") time.sleep(retry_after) continue elif response.status_code >= 500: # Server error - retry with exponential backoff wait_time = 2 ** attempt print(f\"Server error. Retrying in {wait_time} seconds...\") time.sleep(wait_time) continue else: # Client error - don't retry error_data = response.json() raise Exception(f\"API Error: {error_data['error']['message']}\") except requests.exceptions.RequestException as e: print(f\"Request failed: {e}\") if attempt < max_retries - 1: time.sleep(2 ** attempt) else: raise raise Exception(\"Max retries exceeded\") ``` ## Webhooks CloudFlow can send webhook notifications when specific events occur in your workflows.",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_1",
              "content": "**Technology**: Node.js with Express.js framework **Replicas**: 12 pods (production), auto-scaling 8-20 based on CPU **Resource Allocation**: 2 vCPU, 4GB RAM per pod **Key Responsibilities**: - JWT token validation (delegated to Auth Service for initial validation) - Rate limiting: 1000 requests per minute per API key (sliding window) - Request/response transformation and validation using JSON Schema - Routing to downstream services based on URL path patterns - CORS handling for web clients - Request/response logging and correlation ID injection **Critical Endpoints**: - `POST /api/v1/workflows` - Create new workflow - `GET /api/v1/workflows/:id` - Retrieve workflow status - `POST /api/v1/workflows/:id/execute` - Trigger workflow execution - `GET /api/v1/workflows/:id/history` - Get execution history **Dependencies**: - Auth Service (for token validation) - Redis (for rate limiting counters) - All downstream microservices **Performance Targets**: - P50 latency: < 50ms - P99 latency: < 200ms - Throughput: 10,000 RPS sustained --- ### Auth Service **Purpose**: Centralized authentication and authorization service handling user identity, token generation, and permission validation. **Technology**: Go with gRPC for internal communication, REST for external **Replicas**: 8 pods (production), auto-scaling 6-12 **Resource Allocation**: 1 vCPU, 2GB RAM per pod **Key Responsibilities**: - User authentication via multiple providers (OAuth2, SAML, local credentials) - JWT token generation and validation (RS256 algorithm) - Role-based access control (RBAC) with fine-grained permissions - Session management with Redis-backed storage - API key generation and validation for service accounts - MFA enforcement for administrative operations **Authentication Flow**: ``` Client Request \u2192 API Gateway \u2192 Auth Service \u2502 \u251c\u2500 Validate credentials \u251c\u2500 Check MFA if required \u251c\u2500 Generate JWT (15min expiry) \u251c\u2500 Generate refresh token (7 days) \u2514\u2500 Store session in Redis ``` **Token Structure**: - Access Token: JWT with 15-minute expiry - Refresh Token: Opaque token with 7-day expiry, stored in PostgreSQL - Claims: user_id, email, roles[], permissions[], tenant_id **Security Features**: - Password hashing: Argon2id with 64MB memory, 4 iterations - Token rotation on refresh to prevent replay attacks - Brute force protection: 5 failed attempts \u2192 15-minute lockout - Secrets stored in HashiCorp Vault, rotated every 90 days **Performance Targets**: - Token validation: < 10ms (P99) - Token generation: < 50ms (P99) - Throughput: 5,000 RPS for validation operations --- ### Workflow Engine **Purpose**: Core orchestration service that executes workflow definitions, manages state transitions, and coordinates task execution across distributed systems.",
              "score": null
            },
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_0",
              "content": "# CloudFlow API Reference Version 2.1.0 | Last Updated: January 2026 ## Overview The CloudFlow API is a RESTful service that enables developers to programmatically manage cloud workflows, data pipelines, and automation tasks. This documentation provides comprehensive details on authentication, endpoints, request/response formats, error handling, and best practices. **Base URL:** `https://api.cloudflow.io/v2` **API Status:** https://status.cloudflow.io ## Authentication CloudFlow supports three authentication methods to suit different use cases and security requirements. ### API Keys API keys provide simple authentication for server-to-server communication. Include your API key in the request header: ```bash curl -H \"X-API-Key: cf_live_a1b2c3d4e5f6g7h8i9j0\" \\ https://api.cloudflow.io/v2/workflows ``` **Security Notes:** - Never expose API keys in client-side code - Rotate keys every 90 days - Use separate keys for development and production environments ### OAuth 2.0 OAuth 2.0 is recommended for applications that access CloudFlow on behalf of users. We support the Authorization Code flow with PKCE. **Authorization Endpoint:** `https://auth.cloudflow.io/oauth/authorize` **Token Endpoint:** `https://auth.cloudflow.io/oauth/token` **Supported Scopes:** - `workflows:read` - Read workflow configurations - `workflows:write` - Create and modify workflows - `pipelines:read` - Read pipeline data - `pipelines:write` - Create and manage pipelines - `analytics:read` - Access analytics and metrics - `admin:full` - Full administrative access Example authorization request: ```bash curl -X POST https://auth.cloudflow.io/oauth/token \\ -H \"Content-Type: application/x-www-form-urlencoded\" \\ -d \"grant_type=authorization_code\" \\ -d \"code=AUTH_CODE_HERE\" \\ -d \"client_id=YOUR_CLIENT_ID\" \\ -d \"client_secret=YOUR_CLIENT_SECRET\" \\ -d \"redirect_uri=https://yourapp.com/callback\" ``` ### JWT Tokens For advanced use cases, CloudFlow supports JSON Web Tokens (JWT) with RS256 signing algorithm. JWTs must include the following claims: - `iss` (issuer): Your application identifier - `sub` (subject): User or service account ID - `aud` (audience): `https://api.cloudflow.io` - `exp` (expiration): Unix timestamp (max 3600 seconds from `iat`) - `iat` (issued at): Unix timestamp - `scope`: Space-separated list of requested scopes Example JWT header: ```python import jwt import time from cryptography.hazmat.primitives import serialization from cryptography.hazmat.backends import default_backend # Load your private key with open('private_key.pem', 'rb') as key_file: private_key = serialization.load_pem_private_key( key_file.read(), password=None, backend=default_backend() ) # Create JWT payload payload = { 'iss': 'your-app-id', 'sub': 'user-12345', 'aud': 'https://api.cloudflow.io', 'exp': int(time.time()) + 3600, 'iat': int(time.time()), 'scope': 'workflows:read workflows:write' } # Generate token token = jwt.encode(payload, private_key, algorithm='RS256') # Use in API request headers = {'Authorization': f'Bearer {token}'} ``` **Token Expiration:** All tokens expire after 3600 seconds (1 hour). Implement token refresh logic in your application.",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_8",
              "content": "### Cache Invalidation Strategies **Time-Based Expiration (TTL)**: - Short-lived data: 5-15 minutes (session tokens, rate limit counters) - Medium-lived data: 1 hour (workflow definitions, templates) - Long-lived data: 24 hours (static configuration) **Event-Based Invalidation**: ``` Database Update Event \u2192 Kafka (cache.invalidation topic) \u2502 \u25bc All service instances consume event \u2502 \u25bc Redis DEL for affected keys ``` **Pattern-Based Invalidation**: - Use Redis SCAN + DEL for wildcard patterns - Example: Invalidate all user caches: `user:*:{user_id}` **Cache Warming**: - Scheduled job runs every hour to pre-populate frequently accessed data - Targets: Top 1000 workflows, system configuration, active user profiles - Reduces cache miss rate during traffic spikes ### Multi-Level Caching Application implements L1 (in-memory) and L2 (Redis) caching: ``` Request \u2192 L1 Cache (Node.js Map, 1000 entry LRU) \u2502 MISS \u2502 \u25bc L2 Cache (Redis, distributed) \u2502 MISS \u2502 \u25bc Database (PostgreSQL) ``` L1 cache reduces Redis network calls by 40% for hot data. --- ## Security Architecture ### Network Security **Zero-Trust Network Model**: - All service-to-service communication encrypted with mTLS - Certificate rotation: Every 90 days (automated via cert-manager) - Certificate authority: Internal PKI with HashiCorp Vault - Network segmentation: Private subnets for services, public subnet for ALB only **Service Mesh (Istio)**: ``` Service A \u2192 Envoy Sidecar (mTLS client cert) \u2502 Encrypted channel \u2502 Envoy Sidecar (mTLS server cert) \u2192 Service B ``` **Firewall Rules**: - Security groups: Deny all by default, explicit allow rules - Ingress: Only ALB can reach API Gateway (port 8080) - Egress: Services can only reach specific dependencies - Database access: Limited to application subnets only ### Authentication & Authorization **JWT Token Validation**: - Algorithm: RS256 (asymmetric signing) - Key rotation: Every 30 days with 7-day overlap period - Public key distribution: JWKS endpoint cached in Redis - Validation: Signature, expiry, issuer, audience claims - Token revocation: Blacklist in Redis for compromised tokens **Permission Model**: ``` User \u2192 Roles \u2192 Permissions \u2198 \u2197 Tenants (Multi-tenancy isolation) ``` Example permissions: - `workflow:read` - View workflows - `workflow:write` - Create/update workflows - `workflow:execute` - Trigger workflow execution - `workflow:delete` - Delete workflows - `admin:*` - All administrative operations **API Key Management**: - Format: `cfk_live_<32-char-random>` (production), `cfk_test_<32-char-random>` (sandbox) - Hashing: SHA-256 before storage in PostgreSQL - Scoping: API keys can be scoped to specific workflows or operations - Rate limits: Configurable",
              "score": null
            }
          ],
          "latency_ms": 1048.5185550023743,
          "query_id": "realistic_016",
          "dimension": "problem",
          "query": "My API requests are getting 401 errors"
        },
        {
          "key_facts": [
            "OAuth 2.0",
            "API keys",
            "JWT tokens"
          ],
          "found_facts": [
            "OAuth 2.0",
            "API keys",
            "JWT tokens"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "api_reference"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_0",
              "content": "# CloudFlow API Reference Version 2.1.0 | Last Updated: January 2026 ## Overview The CloudFlow API is a RESTful service that enables developers to programmatically manage cloud workflows, data pipelines, and automation tasks. This documentation provides comprehensive details on authentication, endpoints, request/response formats, error handling, and best practices. **Base URL:** `https://api.cloudflow.io/v2` **API Status:** https://status.cloudflow.io ## Authentication CloudFlow supports three authentication methods to suit different use cases and security requirements. ### API Keys API keys provide simple authentication for server-to-server communication. Include your API key in the request header: ```bash curl -H \"X-API-Key: cf_live_a1b2c3d4e5f6g7h8i9j0\" \\ https://api.cloudflow.io/v2/workflows ``` **Security Notes:** - Never expose API keys in client-side code - Rotate keys every 90 days - Use separate keys for development and production environments ### OAuth 2.0 OAuth 2.0 is recommended for applications that access CloudFlow on behalf of users. We support the Authorization Code flow with PKCE. **Authorization Endpoint:** `https://auth.cloudflow.io/oauth/authorize` **Token Endpoint:** `https://auth.cloudflow.io/oauth/token` **Supported Scopes:** - `workflows:read` - Read workflow configurations - `workflows:write` - Create and modify workflows - `pipelines:read` - Read pipeline data - `pipelines:write` - Create and manage pipelines - `analytics:read` - Access analytics and metrics - `admin:full` - Full administrative access Example authorization request: ```bash curl -X POST https://auth.cloudflow.io/oauth/token \\ -H \"Content-Type: application/x-www-form-urlencoded\" \\ -d \"grant_type=authorization_code\" \\ -d \"code=AUTH_CODE_HERE\" \\ -d \"client_id=YOUR_CLIENT_ID\" \\ -d \"client_secret=YOUR_CLIENT_SECRET\" \\ -d \"redirect_uri=https://yourapp.com/callback\" ``` ### JWT Tokens For advanced use cases, CloudFlow supports JSON Web Tokens (JWT) with RS256 signing algorithm. JWTs must include the following claims: - `iss` (issuer): Your application identifier - `sub` (subject): User or service account ID - `aud` (audience): `https://api.cloudflow.io` - `exp` (expiration): Unix timestamp (max 3600 seconds from `iat`) - `iat` (issued at): Unix timestamp - `scope`: Space-separated list of requested scopes Example JWT header: ```python import jwt import time from cryptography.hazmat.primitives import serialization from cryptography.hazmat.backends import default_backend # Load your private key with open('private_key.pem', 'rb') as key_file: private_key = serialization.load_pem_private_key( key_file.read(), password=None, backend=default_backend() ) # Create JWT payload payload = { 'iss': 'your-app-id', 'sub': 'user-12345', 'aud': 'https://api.cloudflow.io', 'exp': int(time.time()) + 3600, 'iat': int(time.time()), 'scope': 'workflows:read workflows:write' } # Generate token token = jwt.encode(payload, private_key, algorithm='RS256') # Use in API request headers = {'Authorization': f'Bearer {token}'} ``` **Token Expiration:** All tokens expire after 3600 seconds (1 hour). Implement token refresh logic in your application.",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_9",
              "content": "per API key (default: 1000 RPM) ### Secrets Management **HashiCorp Vault Integration**: - Dynamic database credentials: Generated on-demand, 1-hour TTL - Encryption keys: Transit secrets engine for encryption-as-a-service - API keys for external services: Stored with versioning - Rotation policy: Automated rotation every 90 days with notification **Secret Access Pattern**: ``` Service starts \u2192 Vault authentication (Kubernetes service account) \u2502 \u25bc Request secret lease (1-hour TTL) \u2502 \u25bc Vault returns dynamic credentials \u2502 \u25bc Service connects to PostgreSQL \u2502 Renew lease every 30 minutes ``` ### Data Protection **Encryption at Rest**: - PostgreSQL: AES-256 encryption enabled at volume level - Redis: Encryption enabled using AWS KMS - Kafka: Encryption at rest for all data on brokers - S3 backups: Server-side encryption with KMS (SSE-KMS) **Encryption in Transit**: - External traffic: TLS 1.3 only (TLS 1.2 deprecated) - Internal traffic: mTLS via Istio service mesh - Database connections: SSL/TLS enforced (sslmode=require) **Data Masking**: - Sensitive fields (email, phone) masked in logs - PII redacted from error messages and stack traces - Audit logs: Full data retention with access controls **Compliance**: - GDPR: User data export and deletion workflows - SOC 2 Type II: Audit logging for all data access - HIPAA: PHI isolation in dedicated tenants with enhanced encryption --- ## Performance Characteristics ### Latency Targets **API Operations** (P99 latency): - `GET /workflows/{id}`: < 50ms (cache hit), < 150ms (cache miss) - `POST /workflows`: < 200ms (includes validation and database write) - `POST /workflows/{id}/execute`: < 100ms (async, returns execution ID) - `GET /workflows/{id}/history`: < 300ms (paginated, 50 records per page) **Workflow Execution** (P99 latency): - Simple workflow (< 5 steps): < 2 seconds - Medium workflow (5-15 steps): < 5 seconds - Complex workflow (> 15 steps): < 15 seconds - Parallel execution: Linear scaling up to 10 concurrent branches **Database Operations** (P99 latency): - Simple SELECT: < 5ms - JOIN query: < 20ms - INSERT/UPDATE: < 10ms - Batch operations: < 100ms (batch size: 100 records) **Cache Operations** (P99 latency): - Redis GET: < 2ms - Redis SET: < 3ms - Redis DEL: < 2ms ### Throughput Capacity **API Gateway**: - Sustained: 10,000 requests per second - Peak: 25,000 requests per second (5-minute burst) - Rate limiting: 1,000 requests per minute per API key **Workflow Engine**: - Concurrent executions: 8,000 workflows (across 16 pods)",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_1",
              "content": "**Technology**: Node.js with Express.js framework **Replicas**: 12 pods (production), auto-scaling 8-20 based on CPU **Resource Allocation**: 2 vCPU, 4GB RAM per pod **Key Responsibilities**: - JWT token validation (delegated to Auth Service for initial validation) - Rate limiting: 1000 requests per minute per API key (sliding window) - Request/response transformation and validation using JSON Schema - Routing to downstream services based on URL path patterns - CORS handling for web clients - Request/response logging and correlation ID injection **Critical Endpoints**: - `POST /api/v1/workflows` - Create new workflow - `GET /api/v1/workflows/:id` - Retrieve workflow status - `POST /api/v1/workflows/:id/execute` - Trigger workflow execution - `GET /api/v1/workflows/:id/history` - Get execution history **Dependencies**: - Auth Service (for token validation) - Redis (for rate limiting counters) - All downstream microservices **Performance Targets**: - P50 latency: < 50ms - P99 latency: < 200ms - Throughput: 10,000 RPS sustained --- ### Auth Service **Purpose**: Centralized authentication and authorization service handling user identity, token generation, and permission validation. **Technology**: Go with gRPC for internal communication, REST for external **Replicas**: 8 pods (production), auto-scaling 6-12 **Resource Allocation**: 1 vCPU, 2GB RAM per pod **Key Responsibilities**: - User authentication via multiple providers (OAuth2, SAML, local credentials) - JWT token generation and validation (RS256 algorithm) - Role-based access control (RBAC) with fine-grained permissions - Session management with Redis-backed storage - API key generation and validation for service accounts - MFA enforcement for administrative operations **Authentication Flow**: ``` Client Request \u2192 API Gateway \u2192 Auth Service \u2502 \u251c\u2500 Validate credentials \u251c\u2500 Check MFA if required \u251c\u2500 Generate JWT (15min expiry) \u251c\u2500 Generate refresh token (7 days) \u2514\u2500 Store session in Redis ``` **Token Structure**: - Access Token: JWT with 15-minute expiry - Refresh Token: Opaque token with 7-day expiry, stored in PostgreSQL - Claims: user_id, email, roles[], permissions[], tenant_id **Security Features**: - Password hashing: Argon2id with 64MB memory, 4 iterations - Token rotation on refresh to prevent replay attacks - Brute force protection: 5 failed attempts \u2192 15-minute lockout - Secrets stored in HashiCorp Vault, rotated every 90 days **Performance Targets**: - Token validation: < 10ms (P99) - Token generation: < 50ms (P99) - Throughput: 5,000 RPS for validation operations --- ### Workflow Engine **Purpose**: Core orchestration service that executes workflow definitions, manages state transitions, and coordinates task execution across distributed systems.",
              "score": null
            },
            {
              "doc_id": "user_guide",
              "chunk_id": "user_guide_fix_2",
              "content": "### HTTP Requests Make HTTP requests to any API endpoint: **Configuration:** - **Method**: GET, POST, PUT, PATCH, DELETE, HEAD - **URL**: Full endpoint URL (supports variable interpolation) - **Headers**: Custom headers as key-value pairs - **Query Parameters**: URL parameters - **Body**: JSON, form data, or raw text - **Authentication**: Basic Auth, Bearer Token, API Key, OAuth 2.0 **Example:** ```yaml - id: fetch_user action: http_request config: method: GET url: \"https://api.example.com/users/{{user_id}}\" headers: Authorization: \"Bearer {{secrets.API_TOKEN}}\" Content-Type: \"application/json\" timeout: 30 ``` **Response Handling:** Access the response in subsequent steps: - `{{steps.fetch_user.status}}` - HTTP status code - `{{steps.fetch_user.body}}` - Response body - `{{steps.fetch_user.headers}}` - Response headers ### Database Queries Execute queries against supported databases (PostgreSQL, MySQL, MongoDB, Redis): **SQL Databases (PostgreSQL, MySQL):** ```yaml - id: get_orders action: database_query config: connection: \"{{secrets.DB_CONNECTION_STRING}}\" query: | SELECT * FROM orders WHERE customer_id = $1 AND status = $2 ORDER BY created_at DESC LIMIT 10 parameters: - \"{{trigger.customer_id}}\" - \"pending\" ``` **MongoDB:** ```yaml - id: find_documents action: mongodb_query config: connection: \"{{secrets.MONGO_URI}}\" database: \"production\" collection: \"users\" operation: find filter: email: \"{{trigger.email}}\" options: limit: 1 ``` **Important Notes:** - Always use parameterized queries to prevent SQL injection - Connection strings should be stored in secrets - Query timeout is 30 seconds by default - Maximum result set size is 10MB ### Email Notifications Send emails via SMTP or integrated email providers: **Configuration:** ```yaml - id: send_notification action: email config: provider: \"smtp\" # or \"sendgrid\", \"mailgun\", \"ses\" from: \"notifications@cloudflow.io\" to: \"{{trigger.recipient}}\" cc: \"manager@company.com\" subject: \"Alert: {{alert_type}}\" body: | Hello {{user.name}}, This is an automated notification about {{event_description}}. Details: - Time: {{timestamp}} - Type: {{alert_type}} - Priority: {{priority}} Best regards, CloudFlow Automation html: true attachments: - name: \"report.pdf\" content: \"{{steps.generate_report.output}}\" encoding: \"base64\" ``` **Email Templates:** CloudFlow supports dynamic templates with conditional content: ``` {{#if priority == 'high'}} \u26a0\ufe0f URGENT: Immediate attention required {{else}} \u2139\ufe0f Informational notification {{/if}} ``` ### Slack Messages Send messages to Slack channels or users: **Channel Messages:** ```yaml - id: notify_team action: slack_message config: channel: \"#deployments\" text: \"Deployment completed successfully!\" blocks: - type: \"section\" text: type: \"mrkdwn\" text: \"*Deployment Summary*\\nEnvironment: {{environment}}\\nVersion: {{version}}\\nDuration: {{duration}}s\" - type: \"actions\" elements: - type: \"button\" text: \"View Logs\" url: \"{{logs_url}}\" thread_ts: \"{{trigger.thread_ts}}\" # Reply to thread ``` **Direct Messages:** ```yaml - id: dm_user action: slack_message config: user: \"{{user_slack_id}}\" text: \"Your report is ready for review.\"",
              "score": null
            },
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_5",
              "content": "**Supported Events:** - `workflow.started` - `workflow.completed` - `workflow.failed` - `pipeline.completed` - `pipeline.failed` **Webhook Payload Example:** ```json { \"event\": \"workflow.completed\", \"timestamp\": \"2026-01-24T10:45:00Z\", \"data\": { \"workflow_id\": \"wf_8x7k2m9p\", \"execution_id\": \"exec_9k3m7n2q\", \"status\": \"completed\", \"duration_ms\": 3420, \"records_processed\": 1247 } } ``` Configure webhooks in your account settings or via the API: ```bash curl -X POST https://api.cloudflow.io/v2/webhooks \\ -H \"X-API-Key: YOUR_API_KEY\" \\ -H \"Content-Type: application/json\" \\ -d '{ \"url\": \"https://your-app.com/cloudflow-webhook\", \"events\": [\"workflow.completed\", \"workflow.failed\"], \"secret\": \"whsec_your_webhook_secret\" }' ``` ## Support For additional help and resources: - **Documentation:** https://docs.cloudflow.io - **API Status:** https://status.cloudflow.io - **Support Email:** support@cloudflow.io - **Community Forum:** https://community.cloudflow.io Enterprise customers have access to 24/7 priority support via phone and dedicated Slack channels.",
              "score": null
            }
          ],
          "latency_ms": 922.2776589995192,
          "query_id": "realistic_016",
          "dimension": "casual",
          "query": "auth methods"
        },
        {
          "key_facts": [
            "OAuth 2.0",
            "API keys",
            "JWT tokens"
          ],
          "found_facts": [
            "OAuth 2.0",
            "API keys",
            "JWT tokens"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "api_reference"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_0",
              "content": "# CloudFlow API Reference Version 2.1.0 | Last Updated: January 2026 ## Overview The CloudFlow API is a RESTful service that enables developers to programmatically manage cloud workflows, data pipelines, and automation tasks. This documentation provides comprehensive details on authentication, endpoints, request/response formats, error handling, and best practices. **Base URL:** `https://api.cloudflow.io/v2` **API Status:** https://status.cloudflow.io ## Authentication CloudFlow supports three authentication methods to suit different use cases and security requirements. ### API Keys API keys provide simple authentication for server-to-server communication. Include your API key in the request header: ```bash curl -H \"X-API-Key: cf_live_a1b2c3d4e5f6g7h8i9j0\" \\ https://api.cloudflow.io/v2/workflows ``` **Security Notes:** - Never expose API keys in client-side code - Rotate keys every 90 days - Use separate keys for development and production environments ### OAuth 2.0 OAuth 2.0 is recommended for applications that access CloudFlow on behalf of users. We support the Authorization Code flow with PKCE. **Authorization Endpoint:** `https://auth.cloudflow.io/oauth/authorize` **Token Endpoint:** `https://auth.cloudflow.io/oauth/token` **Supported Scopes:** - `workflows:read` - Read workflow configurations - `workflows:write` - Create and modify workflows - `pipelines:read` - Read pipeline data - `pipelines:write` - Create and manage pipelines - `analytics:read` - Access analytics and metrics - `admin:full` - Full administrative access Example authorization request: ```bash curl -X POST https://auth.cloudflow.io/oauth/token \\ -H \"Content-Type: application/x-www-form-urlencoded\" \\ -d \"grant_type=authorization_code\" \\ -d \"code=AUTH_CODE_HERE\" \\ -d \"client_id=YOUR_CLIENT_ID\" \\ -d \"client_secret=YOUR_CLIENT_SECRET\" \\ -d \"redirect_uri=https://yourapp.com/callback\" ``` ### JWT Tokens For advanced use cases, CloudFlow supports JSON Web Tokens (JWT) with RS256 signing algorithm. JWTs must include the following claims: - `iss` (issuer): Your application identifier - `sub` (subject): User or service account ID - `aud` (audience): `https://api.cloudflow.io` - `exp` (expiration): Unix timestamp (max 3600 seconds from `iat`) - `iat` (issued at): Unix timestamp - `scope`: Space-separated list of requested scopes Example JWT header: ```python import jwt import time from cryptography.hazmat.primitives import serialization from cryptography.hazmat.backends import default_backend # Load your private key with open('private_key.pem', 'rb') as key_file: private_key = serialization.load_pem_private_key( key_file.read(), password=None, backend=default_backend() ) # Create JWT payload payload = { 'iss': 'your-app-id', 'sub': 'user-12345', 'aud': 'https://api.cloudflow.io', 'exp': int(time.time()) + 3600, 'iat': int(time.time()), 'scope': 'workflows:read workflows:write' } # Generate token token = jwt.encode(payload, private_key, algorithm='RS256') # Use in API request headers = {'Authorization': f'Bearer {token}'} ``` **Token Expiration:** All tokens expire after 3600 seconds (1 hour). Implement token refresh logic in your application.",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_0",
              "content": "# CloudFlow Platform Troubleshooting Guide **Version:** 3.2.1 **Last Updated:** January 2026 **Audience:** Platform Engineers, DevOps, Support Teams ## Table of Contents 1. [Overview](#overview) 2. [Authentication & Authorization Issues](#authentication--authorization-issues) 3. [Performance Problems](#performance-problems) 4. [Database Connection Issues](#database-connection-issues) 5. [Workflow Execution Failures](#workflow-execution-failures) 6. [Rate Limiting & Throttling](#rate-limiting--throttling) 7. [Log Analysis & Debugging](#log-analysis--debugging) 8. [Escalation Procedures](#escalation-procedures) --- ## Overview This guide provides comprehensive troubleshooting steps for common CloudFlow platform issues encountered in production environments. Each section includes error symptoms, root cause analysis, resolution steps, and preventive measures. ### Quick Diagnostic Checklist Before diving into specific issues, perform these initial checks: - Verify service health: `cloudflow status --all` - Check API connectivity: `curl -I https://api.cloudflow.io/health` - Review recent deployments: `kubectl get deployments -n cloudflow --sort-by=.metadata.creationTimestamp` - Inspect platform metrics: `cloudflow metrics --last 1h` --- ## Authentication & Authorization Issues ### 401 Unauthorized Errors #### Symptoms - API requests return `401 Unauthorized` - Error message: `Authentication credentials were not provided or are invalid` - Frontend displays \"Session expired\" message #### Common Causes **1. Token Expiration** CloudFlow access tokens expire after 3600 seconds (1 hour) by default. Refresh tokens are valid for 30 days. **Verification:** ```bash # Decode JWT to check expiration echo $CF_ACCESS_TOKEN | cut -d'.' -f2 | base64 -d | jq '.exp' # Compare with current time date +%s ``` **Resolution:** ```bash # Refresh the access token curl -X POST https://api.cloudflow.io/auth/refresh \\ -H \"Content-Type: application/json\" \\ -d '{\"refresh_token\": \"'$CF_REFRESH_TOKEN'\"}' # Update environment variable export CF_ACCESS_TOKEN=\"<new_token>\" ``` **2. Invalid JWT Signature** Error: `JWT signature verification failed` **Causes:** - Token was modified or corrupted - Using wrong signing key - Token generated with different secret **Resolution:** ```bash # Validate token structure cloudflow auth validate --token $CF_ACCESS_TOKEN # Generate new token cloudflow auth login --username <user> --password <pass> # For service accounts cloudflow auth service-account --create --name <sa-name> --scopes \"workflow:read,workflow:write\" ``` **3. Clock Skew Issues** JWT validation fails when system clocks are out of sync (tolerance: \u00b1300 seconds).",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_8",
              "content": "Leverage caching:** ```bash # Enable client-side caching export CLOUDFLOW_CACHE_ENABLED=true export CLOUDFLOW_CACHE_TTL=300 # Cache workflow metadata cloudflow workflows list --use-cache --cache-ttl 600 ``` --- ## Log Analysis & Debugging ### Accessing CloudFlow Logs #### Kubernetes Deployments ```bash # List all CloudFlow pods kubectl get pods -n cloudflow # Tail logs from API server kubectl logs -f -n cloudflow deployment/cloudflow-api --tail=100 # Get logs from specific pod kubectl logs -n cloudflow cloudflow-api-7d4f6b8c9d-x7k2m # Get logs from previous crashed pod kubectl logs -n cloudflow cloudflow-api-7d4f6b8c9d-x7k2m --previous # Get logs from all pods in deployment kubectl logs -n cloudflow deployment/cloudflow-api --all-containers=true # Stream logs from multiple pods kubectl logs -n cloudflow -l app=cloudflow-api -f --max-log-requests=10 ``` #### Log Levels CloudFlow supports the following log levels: - `TRACE` - Very detailed debugging information - `DEBUG` - Detailed debugging information - `INFO` - Informational messages (default) - `WARN` - Warning messages - `ERROR` - Error messages - `FATAL` - Fatal errors causing shutdown **Changing log levels:** ```bash # Set global log level cloudflow config set logging.level DEBUG # Set log level for specific component cloudflow config set logging.components.database DEBUG cloudflow config set logging.components.auth INFO cloudflow config set logging.components.workflows TRACE # Temporary log level increase (resets after 1 hour) cloudflow debug set-log-level DEBUG --duration 3600 # View current log configuration cloudflow config get logging --format json ``` ### Grep Patterns for Common Issues #### Authentication Failures ```bash # Find all authentication errors kubectl logs -n cloudflow deployment/cloudflow-api | \\ grep -i \"authentication\\|401\\|unauthorized\" | \\ tail -n 50 # Find JWT validation failures kubectl logs -n cloudflow deployment/cloudflow-api | \\ grep -E \"JWT|token.*invalid|signature.*failed\" # Find clock skew issues kubectl logs -n cloudflow deployment/cloudflow-api | \\ grep -i \"clock skew\\|time.*sync\\|nbf\\|exp\" ``` #### Database Errors ```bash # Find database connection errors kubectl logs -n cloudflow deployment/cloudflow-api | \\ grep -E \"connection.*pool|could not connect|database.*timeout\" # Find slow queries kubectl logs -n cloudflow deployment/cloudflow-api | \\ grep \"slow query\" | \\ awk '{print $NF}' | \\ sort -n | \\ tail -n 20 # Find deadlock errors kubectl logs -n cloudflow deployment/cloudflow-api | \\ grep -i \"deadlock detected\" ``` #### Workflow Execution Errors ```bash # Find workflow timeout errors kubectl logs -n cloudflow deployment/cloudflow-workflow-engine | \\ grep -E \"timeout|exceeded.*3600\" # Find workflow retry attempts kubectl logs -n cloudflow deployment/cloudflow-workflow-engine | \\ grep -E \"retry attempt [0-9]|retrying",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_2",
              "content": "Review Query Execution Plans** ```sql -- Connect to CloudFlow database cloudflow db connect --readonly -- Explain slow query EXPLAIN ANALYZE SELECT w.*, e.status, e.error_message FROM workflows w LEFT JOIN executions e ON w.id = e.workflow_id WHERE w.workspace_id = 'ws_abc123' AND e.created_at > NOW() - INTERVAL '7 days' ORDER BY e.created_at DESC; -- Check for missing indexes SELECT schemaname, tablename, indexname, indexdef FROM pg_indexes WHERE tablename IN ('workflows', 'executions', 'workflow_steps') ORDER BY tablename, indexname; ``` **3. Optimize Queries** Common optimization techniques: **Add missing indexes:** ```sql -- Index for workflow lookup by workspace CREATE INDEX CONCURRENTLY idx_workflows_workspace_created ON workflows(workspace_id, created_at DESC); -- Index for execution status queries CREATE INDEX CONCURRENTLY idx_executions_status_created ON executions(workflow_id, status, created_at DESC) WHERE status IN ('running', 'pending'); -- Composite index for common filter combinations CREATE INDEX CONCURRENTLY idx_executions_workspace_status ON executions(workspace_id, status, created_at) INCLUDE (error_message, retry_count); ``` **Use query result caching:** ```bash # Enable query cache for workspace metadata cloudflow config set cache.workspace.ttl 3600 # Configure Redis cache backend cloudflow config set cache.backend redis cloudflow config set cache.redis.host redis.cloudflow.svc.cluster.local cloudflow config set cache.redis.port 6379 ``` ### High API Latency #### Latency Breakdown Analysis ```bash # Generate latency report cloudflow metrics latency-report --endpoint \"/api/v1/workflows\" --last 24h # Sample output: # Endpoint: POST /api/v1/workflows/execute # P50: 245ms | P95: 1823ms | P99: 4521ms # Breakdown: # - Auth: 45ms (18%) # - DB Query: 156ms (64%) # - Business Logic: 32ms (13%) # - Response Serialization: 12ms (5%) ``` **Network latency issues:** ```bash # Test connectivity to CloudFlow API time curl -w \"@curl-format.txt\" -o /dev/null -s https://api.cloudflow.io/health # Create curl-format.txt: cat > curl-format.txt << EOF time_namelookup: %{time_namelookup}s\\n time_connect: %{time_connect}s\\n time_appconnect: %{time_appconnect}s\\n time_pretransfer: %{time_pretransfer}s\\n time_redirect: %{time_redirect}s\\n time_starttransfer: %{time_starttransfer}s\\n ----------\\n time_total: %{time_total}s\\n EOF # Trace route to API endpoint traceroute api.cloudflow.io # Check DNS resolution time dig api.cloudflow.io | grep \"Query time\" ``` ### Memory Leaks #### Detection ```bash # Monitor CloudFlow service memory usage kubectl top pods -n cloudflow --sort-by=memory # Get detailed memory metrics for specific pod kubectl exec -n cloudflow deploy/cloudflow-api -- \\ curl localhost:9090/metrics | grep memory # Check for OOMKilled pods kubectl get pods -n cloudflow --field-selector=status.phase=Failed | \\ grep OOMKilled # Review memory limits and requests kubectl describe deployment cloudflow-api -n cloudflow | \\ grep -A 5 \"Limits\\|Requests\" ``` #### Common Causes **1.",
              "score": null
            },
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_1",
              "content": "## Rate Limiting To ensure fair usage and system stability, CloudFlow enforces rate limits on all API endpoints. **Default Limits:** - 100 requests per minute per authenticated user - 20 requests per minute for unauthenticated requests - Burst allowance: 150 requests in a 10-second window ### Rate Limit Headers Every API response includes rate limit information: ``` X-RateLimit-Limit: 100 X-RateLimit-Remaining: 87 X-RateLimit-Reset: 1640995200 ``` When you exceed the rate limit, you'll receive a `429 Too Many Requests` response: ```json { \"error\": { \"code\": \"rate_limit_exceeded\", \"message\": \"Rate limit exceeded. Please retry after 42 seconds.\", \"retry_after\": 42 } } ``` **Best Practices:** - Monitor `X-RateLimit-Remaining` header values - Implement exponential backoff when receiving 429 responses - Cache responses when appropriate to reduce API calls - Consider upgrading to Enterprise tier for higher limits (1000 req/min) ## Pagination List endpoints return paginated results to optimize performance. Use `limit` and `offset` parameters to navigate through result sets. **Parameters:** - `limit`: Number of items per page (default: 20, max: 100) - `offset`: Number of items to skip (default: 0) **Example Request:** ```bash curl -H \"X-API-Key: YOUR_API_KEY\" \\ \"https://api.cloudflow.io/v2/workflows?limit=50&offset=100\" ``` **Response Structure:** ```json { \"data\": [...], \"pagination\": { \"total\": 347, \"limit\": 50, \"offset\": 100, \"has_more\": true } } ``` **Python Example:** ```python import requests def fetch_all_workflows(api_key): base_url = \"https://api.cloudflow.io/v2/workflows\" headers = {\"X-API-Key\": api_key} all_workflows = [] offset = 0 limit = 100 while True: response = requests.get( base_url, headers=headers, params={\"limit\": limit, \"offset\": offset} ) data = response.json() all_workflows.extend(data['data']) if not data['pagination']['has_more']: break offset += limit return all_workflows ``` ## Endpoints ### Workflows #### List Workflows Retrieve a paginated list of all workflows in your account. **Endpoint:** `GET /workflows` **Query Parameters:** - `status` (optional): Filter by status (`active`, `paused`, `archived`) - `created_after` (optional): ISO 8601 timestamp - `limit` (optional): Items per page (max 100) - `offset` (optional): Pagination offset **Example Request:** ```bash curl -H \"X-API-Key: YOUR_API_KEY\" \\ \"https://api.cloudflow.io/v2/workflows?status=active&limit=25\" ``` **Example Response (200 OK):** ```json { \"data\": [ { \"id\": \"wf_8x7k2m9p\", \"name\": \"Data Processing Pipeline\", \"description\": \"Processes customer data every hour\", \"status\": \"active\", \"trigger\": { \"type\": \"schedule\", \"cron\": \"0 * * * *\" }, \"steps\": 12, \"created_at\": \"2026-01-15T10:30:00Z\", \"updated_at\": \"2026-01-23T14:22:00Z\", \"last_run\": \"2026-01-24T09:00:00Z\" } ], \"pagination\": { \"total\": 1, \"limit\": 25, \"offset\": 0, \"has_more\": false } } ``` #### Create Workflow Create a new workflow with specified configuration.",
              "score": null
            }
          ],
          "latency_ms": 949.6408199993311,
          "query_id": "realistic_016",
          "dimension": "contextual",
          "query": "I'm integrating with CloudFlow API, what authentication options do I have?"
        },
        {
          "key_facts": [
            "OAuth 2.0",
            "API keys",
            "JWT tokens"
          ],
          "found_facts": [
            "OAuth 2.0",
            "API keys",
            "JWT tokens"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "api_reference"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "user_guide",
              "chunk_id": "user_guide_fix_2",
              "content": "### HTTP Requests Make HTTP requests to any API endpoint: **Configuration:** - **Method**: GET, POST, PUT, PATCH, DELETE, HEAD - **URL**: Full endpoint URL (supports variable interpolation) - **Headers**: Custom headers as key-value pairs - **Query Parameters**: URL parameters - **Body**: JSON, form data, or raw text - **Authentication**: Basic Auth, Bearer Token, API Key, OAuth 2.0 **Example:** ```yaml - id: fetch_user action: http_request config: method: GET url: \"https://api.example.com/users/{{user_id}}\" headers: Authorization: \"Bearer {{secrets.API_TOKEN}}\" Content-Type: \"application/json\" timeout: 30 ``` **Response Handling:** Access the response in subsequent steps: - `{{steps.fetch_user.status}}` - HTTP status code - `{{steps.fetch_user.body}}` - Response body - `{{steps.fetch_user.headers}}` - Response headers ### Database Queries Execute queries against supported databases (PostgreSQL, MySQL, MongoDB, Redis): **SQL Databases (PostgreSQL, MySQL):** ```yaml - id: get_orders action: database_query config: connection: \"{{secrets.DB_CONNECTION_STRING}}\" query: | SELECT * FROM orders WHERE customer_id = $1 AND status = $2 ORDER BY created_at DESC LIMIT 10 parameters: - \"{{trigger.customer_id}}\" - \"pending\" ``` **MongoDB:** ```yaml - id: find_documents action: mongodb_query config: connection: \"{{secrets.MONGO_URI}}\" database: \"production\" collection: \"users\" operation: find filter: email: \"{{trigger.email}}\" options: limit: 1 ``` **Important Notes:** - Always use parameterized queries to prevent SQL injection - Connection strings should be stored in secrets - Query timeout is 30 seconds by default - Maximum result set size is 10MB ### Email Notifications Send emails via SMTP or integrated email providers: **Configuration:** ```yaml - id: send_notification action: email config: provider: \"smtp\" # or \"sendgrid\", \"mailgun\", \"ses\" from: \"notifications@cloudflow.io\" to: \"{{trigger.recipient}}\" cc: \"manager@company.com\" subject: \"Alert: {{alert_type}}\" body: | Hello {{user.name}}, This is an automated notification about {{event_description}}. Details: - Time: {{timestamp}} - Type: {{alert_type}} - Priority: {{priority}} Best regards, CloudFlow Automation html: true attachments: - name: \"report.pdf\" content: \"{{steps.generate_report.output}}\" encoding: \"base64\" ``` **Email Templates:** CloudFlow supports dynamic templates with conditional content: ``` {{#if priority == 'high'}} \u26a0\ufe0f URGENT: Immediate attention required {{else}} \u2139\ufe0f Informational notification {{/if}} ``` ### Slack Messages Send messages to Slack channels or users: **Channel Messages:** ```yaml - id: notify_team action: slack_message config: channel: \"#deployments\" text: \"Deployment completed successfully!\" blocks: - type: \"section\" text: type: \"mrkdwn\" text: \"*Deployment Summary*\\nEnvironment: {{environment}}\\nVersion: {{version}}\\nDuration: {{duration}}s\" - type: \"actions\" elements: - type: \"button\" text: \"View Logs\" url: \"{{logs_url}}\" thread_ts: \"{{trigger.thread_ts}}\" # Reply to thread ``` **Direct Messages:** ```yaml - id: dm_user action: slack_message config: user: \"{{user_slack_id}}\" text: \"Your report is ready for review.\"",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_0",
              "content": "# CloudFlow Platform Troubleshooting Guide **Version:** 3.2.1 **Last Updated:** January 2026 **Audience:** Platform Engineers, DevOps, Support Teams ## Table of Contents 1. [Overview](#overview) 2. [Authentication & Authorization Issues](#authentication--authorization-issues) 3. [Performance Problems](#performance-problems) 4. [Database Connection Issues](#database-connection-issues) 5. [Workflow Execution Failures](#workflow-execution-failures) 6. [Rate Limiting & Throttling](#rate-limiting--throttling) 7. [Log Analysis & Debugging](#log-analysis--debugging) 8. [Escalation Procedures](#escalation-procedures) --- ## Overview This guide provides comprehensive troubleshooting steps for common CloudFlow platform issues encountered in production environments. Each section includes error symptoms, root cause analysis, resolution steps, and preventive measures. ### Quick Diagnostic Checklist Before diving into specific issues, perform these initial checks: - Verify service health: `cloudflow status --all` - Check API connectivity: `curl -I https://api.cloudflow.io/health` - Review recent deployments: `kubectl get deployments -n cloudflow --sort-by=.metadata.creationTimestamp` - Inspect platform metrics: `cloudflow metrics --last 1h` --- ## Authentication & Authorization Issues ### 401 Unauthorized Errors #### Symptoms - API requests return `401 Unauthorized` - Error message: `Authentication credentials were not provided or are invalid` - Frontend displays \"Session expired\" message #### Common Causes **1. Token Expiration** CloudFlow access tokens expire after 3600 seconds (1 hour) by default. Refresh tokens are valid for 30 days. **Verification:** ```bash # Decode JWT to check expiration echo $CF_ACCESS_TOKEN | cut -d'.' -f2 | base64 -d | jq '.exp' # Compare with current time date +%s ``` **Resolution:** ```bash # Refresh the access token curl -X POST https://api.cloudflow.io/auth/refresh \\ -H \"Content-Type: application/json\" \\ -d '{\"refresh_token\": \"'$CF_REFRESH_TOKEN'\"}' # Update environment variable export CF_ACCESS_TOKEN=\"<new_token>\" ``` **2. Invalid JWT Signature** Error: `JWT signature verification failed` **Causes:** - Token was modified or corrupted - Using wrong signing key - Token generated with different secret **Resolution:** ```bash # Validate token structure cloudflow auth validate --token $CF_ACCESS_TOKEN # Generate new token cloudflow auth login --username <user> --password <pass> # For service accounts cloudflow auth service-account --create --name <sa-name> --scopes \"workflow:read,workflow:write\" ``` **3. Clock Skew Issues** JWT validation fails when system clocks are out of sync (tolerance: \u00b1300 seconds).",
              "score": null
            },
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_0",
              "content": "# CloudFlow API Reference Version 2.1.0 | Last Updated: January 2026 ## Overview The CloudFlow API is a RESTful service that enables developers to programmatically manage cloud workflows, data pipelines, and automation tasks. This documentation provides comprehensive details on authentication, endpoints, request/response formats, error handling, and best practices. **Base URL:** `https://api.cloudflow.io/v2` **API Status:** https://status.cloudflow.io ## Authentication CloudFlow supports three authentication methods to suit different use cases and security requirements. ### API Keys API keys provide simple authentication for server-to-server communication. Include your API key in the request header: ```bash curl -H \"X-API-Key: cf_live_a1b2c3d4e5f6g7h8i9j0\" \\ https://api.cloudflow.io/v2/workflows ``` **Security Notes:** - Never expose API keys in client-side code - Rotate keys every 90 days - Use separate keys for development and production environments ### OAuth 2.0 OAuth 2.0 is recommended for applications that access CloudFlow on behalf of users. We support the Authorization Code flow with PKCE. **Authorization Endpoint:** `https://auth.cloudflow.io/oauth/authorize` **Token Endpoint:** `https://auth.cloudflow.io/oauth/token` **Supported Scopes:** - `workflows:read` - Read workflow configurations - `workflows:write` - Create and modify workflows - `pipelines:read` - Read pipeline data - `pipelines:write` - Create and manage pipelines - `analytics:read` - Access analytics and metrics - `admin:full` - Full administrative access Example authorization request: ```bash curl -X POST https://auth.cloudflow.io/oauth/token \\ -H \"Content-Type: application/x-www-form-urlencoded\" \\ -d \"grant_type=authorization_code\" \\ -d \"code=AUTH_CODE_HERE\" \\ -d \"client_id=YOUR_CLIENT_ID\" \\ -d \"client_secret=YOUR_CLIENT_SECRET\" \\ -d \"redirect_uri=https://yourapp.com/callback\" ``` ### JWT Tokens For advanced use cases, CloudFlow supports JSON Web Tokens (JWT) with RS256 signing algorithm. JWTs must include the following claims: - `iss` (issuer): Your application identifier - `sub` (subject): User or service account ID - `aud` (audience): `https://api.cloudflow.io` - `exp` (expiration): Unix timestamp (max 3600 seconds from `iat`) - `iat` (issued at): Unix timestamp - `scope`: Space-separated list of requested scopes Example JWT header: ```python import jwt import time from cryptography.hazmat.primitives import serialization from cryptography.hazmat.backends import default_backend # Load your private key with open('private_key.pem', 'rb') as key_file: private_key = serialization.load_pem_private_key( key_file.read(), password=None, backend=default_backend() ) # Create JWT payload payload = { 'iss': 'your-app-id', 'sub': 'user-12345', 'aud': 'https://api.cloudflow.io', 'exp': int(time.time()) + 3600, 'iat': int(time.time()), 'scope': 'workflows:read workflows:write' } # Generate token token = jwt.encode(payload, private_key, algorithm='RS256') # Use in API request headers = {'Authorization': f'Bearer {token}'} ``` **Token Expiration:** All tokens expire after 3600 seconds (1 hour). Implement token refresh logic in your application.",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_1",
              "content": "**Technology**: Node.js with Express.js framework **Replicas**: 12 pods (production), auto-scaling 8-20 based on CPU **Resource Allocation**: 2 vCPU, 4GB RAM per pod **Key Responsibilities**: - JWT token validation (delegated to Auth Service for initial validation) - Rate limiting: 1000 requests per minute per API key (sliding window) - Request/response transformation and validation using JSON Schema - Routing to downstream services based on URL path patterns - CORS handling for web clients - Request/response logging and correlation ID injection **Critical Endpoints**: - `POST /api/v1/workflows` - Create new workflow - `GET /api/v1/workflows/:id` - Retrieve workflow status - `POST /api/v1/workflows/:id/execute` - Trigger workflow execution - `GET /api/v1/workflows/:id/history` - Get execution history **Dependencies**: - Auth Service (for token validation) - Redis (for rate limiting counters) - All downstream microservices **Performance Targets**: - P50 latency: < 50ms - P99 latency: < 200ms - Throughput: 10,000 RPS sustained --- ### Auth Service **Purpose**: Centralized authentication and authorization service handling user identity, token generation, and permission validation. **Technology**: Go with gRPC for internal communication, REST for external **Replicas**: 8 pods (production), auto-scaling 6-12 **Resource Allocation**: 1 vCPU, 2GB RAM per pod **Key Responsibilities**: - User authentication via multiple providers (OAuth2, SAML, local credentials) - JWT token generation and validation (RS256 algorithm) - Role-based access control (RBAC) with fine-grained permissions - Session management with Redis-backed storage - API key generation and validation for service accounts - MFA enforcement for administrative operations **Authentication Flow**: ``` Client Request \u2192 API Gateway \u2192 Auth Service \u2502 \u251c\u2500 Validate credentials \u251c\u2500 Check MFA if required \u251c\u2500 Generate JWT (15min expiry) \u251c\u2500 Generate refresh token (7 days) \u2514\u2500 Store session in Redis ``` **Token Structure**: - Access Token: JWT with 15-minute expiry - Refresh Token: Opaque token with 7-day expiry, stored in PostgreSQL - Claims: user_id, email, roles[], permissions[], tenant_id **Security Features**: - Password hashing: Argon2id with 64MB memory, 4 iterations - Token rotation on refresh to prevent replay attacks - Brute force protection: 5 failed attempts \u2192 15-minute lockout - Secrets stored in HashiCorp Vault, rotated every 90 days **Performance Targets**: - Token validation: < 10ms (P99) - Token generation: < 50ms (P99) - Throughput: 5,000 RPS for validation operations --- ### Workflow Engine **Purpose**: Core orchestration service that executes workflow definitions, manages state transitions, and coordinates task execution across distributed systems.",
              "score": null
            },
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_1",
              "content": "create cluster -f cluster-config.yaml ``` ### Storage Configuration Install the EBS CSI driver for persistent volumes: ```bash eksctl create addon --name aws-ebs-csi-driver \\ --cluster cloudflow-production \\ --service-account-role-arn arn:aws:iam::ACCOUNT_ID:role/AmazonEKS_EBS_CSI_DriverRole \\ --force ``` --- ## Kubernetes Deployment ### Namespace Setup Create the CloudFlow namespace and configure resource quotas: ```yaml # namespace.yaml apiVersion: v1 kind: Namespace metadata: name: cloudflow-prod labels: name: cloudflow-prod environment: production --- apiVersion: v1 kind: ResourceQuota metadata: name: cloudflow-quota namespace: cloudflow-prod spec: hard: requests.cpu: \"50\" requests.memory: 100Gi persistentvolumeclaims: \"10\" services.loadbalancers: \"2\" ``` Apply the namespace configuration: ```bash kubectl apply -f namespace.yaml ``` ### Environment Configuration CloudFlow requires the following environment variables: | Variable | Description | Example | Required | |----------|-------------|---------|----------| | `DATABASE_URL` | PostgreSQL connection string | `postgresql://user:pass@host:5432/cloudflow` | Yes | | `REDIS_URL` | Redis connection string | `redis://redis-master.cloudflow-prod.svc.cluster.local:6379` | Yes | | `JWT_SECRET` | Secret key for JWT token signing | `<generated-secret-256-bit>` | Yes | | `LOG_LEVEL` | Application log verbosity | `info`, `debug`, `warn`, `error` | No (default: `info`) | | `API_PORT` | API server port | `3000` | No (default: `3000`) | | `WORKER_CONCURRENCY` | Number of concurrent workers | `10` | No (default: `5`) | | `SESSION_SECRET` | Session encryption key | `<generated-secret-256-bit>` | Yes | | `AWS_REGION` | AWS region for services | `us-east-1` | Yes | | `S3_BUCKET` | S3 bucket for file storage | `cloudflow-prod-storage` | Yes | | `SMTP_HOST` | Email server hostname | `smtp.sendgrid.net` | No | | `SMTP_PORT` | Email server port | `587` | No | | `METRICS_ENABLED` | Enable Prometheus metrics | `true` | No (default: `false`) | Create a Kubernetes secret for sensitive variables: ```bash kubectl create secret generic cloudflow-secrets \\ --namespace cloudflow-prod \\ --from-literal=DATABASE_URL=\"postgresql://cloudflow:$(cat db-password.txt)@pgbouncer.cloudflow-prod.svc.cluster.local:5432/cloudflow\" \\ --from-literal=REDIS_URL=\"redis://redis-master.cloudflow-prod.svc.cluster.local:6379\" \\ --from-literal=JWT_SECRET=\"$(openssl rand -base64 32)\" \\ --from-literal=SESSION_SECRET=\"$(openssl rand -base64 32)\" ``` ### Helm Chart Deployment Add the CloudFlow Helm repository: ```bash helm repo add cloudflow https://charts.cloudflow.io helm repo update ``` Create a values file for production configuration: ```yaml # values-production.yaml replicaCount: 3 image: repository: 123456789012.dkr.ecr.us-east-1.amazonaws.com/cloudflow tag: \"2.4.0\" pullPolicy: IfNotPresent resources: limits: cpu: 2000m memory: 4Gi requests: cpu: 2000m memory: 4Gi autoscaling: enabled: true minReplicas: 3 maxReplicas: 10 targetCPUUtilizationPercentage: 70 targetMemoryUtilizationPercentage: 80 service: type: ClusterIP port: 80 targetPort: 3000 annotations: service.beta.kubernetes.io/aws-load-balancer-type: \"nlb\" ingress: enabled: true className: nginx annotations: cert-manager.io/cluster-issuer: \"letsencrypt-prod\" nginx.ingress.kubernetes.io/ssl-redirect: \"true\" hosts: - host: api.cloudflow.io paths: - path: / pathType: Prefix tls:",
              "score": null
            }
          ],
          "latency_ms": 1078.8655130018014,
          "query_id": "realistic_016",
          "dimension": "negation",
          "query": "Why isn't basic auth working?"
        },
        {
          "key_facts": [
            "PgBouncer",
            "connection pooling",
            "max_db_connections = 100",
            "default_pool_size = 25",
            "pool_mode = transaction"
          ],
          "found_facts": [
            "PgBouncer",
            "connection pooling",
            "max_db_connections = 100",
            "default_pool_size = 25",
            "pool_mode = transaction"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "deployment_guide",
            "architecture_overview"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_4",
              "content": "**Implement connection pooling optimization:** ```bash # Use PgBouncer for connection pooling kubectl apply -f cloudflow-pgbouncer.yaml # Configure CloudFlow to use PgBouncer cloudflow config set db.host pgbouncer.cloudflow.svc.cluster.local cloudflow config set db.port 6432 cloudflow config set db.pool.mode transaction ``` 2. **Add read replicas:** ```bash # Route read-only queries to replicas cloudflow db replicas add --count 2 cloudflow config set db.read_replicas \"replica-1:5432,replica-2:5432\" cloudflow config set db.read_write_split true ``` ### Connection Timeout Errors #### Error Messages - `connection timeout after 30000ms` - `could not connect to database server at 10.0.2.45:5432` - `database server unreachable` #### Troubleshooting Steps ```bash # Test network connectivity telnet cloudflow-db.internal.company.com 5432 # Check DNS resolution nslookup cloudflow-db.internal.company.com # Verify database is accepting connections pg_isready -h cloudflow-db.internal.company.com -p 5432 -U cloudflow # Check firewall rules sudo iptables -L -n | grep 5432 # Test from CloudFlow pod network kubectl run -n cloudflow debug-pod --rm -i --tty \\ --image=postgres:14 -- \\ psql -h cloudflow-db.internal.company.com -U cloudflow -d cloudflow_production # Review database logs for connection rejections kubectl logs -n cloudflow statefulset/cloudflow-db --tail=100 | \\ grep -i \"connection\\|reject\\|authentication\" ``` ### Maximum Connection Limit (100) Reached This is a hard limit in CloudFlow's database tier. #### Permanent Solutions **Option 1: Upgrade database tier** ```bash # Check available tiers cloudflow db tiers list # Upgrade to higher tier (supports 200 connections) cloudflow db upgrade --tier standard-plus --confirm # Monitor migration progress cloudflow db migration status ``` **Option 2: Implement aggressive connection reuse** ```bash # Reduce connection lifetime cloudflow config set db.pool.max_lifetime 600 # 10 minutes # Enable prepared statement caching cloudflow config set db.prepared_statements.cache true cloudflow config set db.prepared_statements.max_size 250 # Reduce idle connection timeout cloudflow config set db.pool.idle_timeout 120 # 2 minutes ``` --- ## Workflow Execution Failures ### Timeout Errors (3600 second limit) #### Error Message ``` WorkflowExecutionError: Workflow exceeded maximum execution time of 3600 seconds Status: TIMEOUT Workflow ID: wf_9k2n4m8p1q Execution ID: exec_7h3j6k9m2n ``` #### Analysis ```bash # Get workflow execution details cloudflow workflows executions get exec_7h3j6k9m2n --verbose # Check step-by-step breakdown cloudflow workflows executions steps exec_7h3j6k9m2n # Sample output: # Step 1: data_ingestion - Duration: 245s - Status: SUCCESS # Step 2: data_validation - Duration: 123s - Status: SUCCESS # Step 3: data_transformation - Duration: 3189s - Status: TIMEOUT # Step 4: data_export - Duration: 0s - Status: SKIPPED # Identify bottleneck step cloudflow workflows analyze",
              "score": null
            },
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_6",
              "content": "**Verify Service Health**: ```bash kubectl get pods -n cloudflow-prod curl https://api.cloudflow.io/health ``` **Recovery Time Objective (RTO)**: 4 hours **Recovery Point Objective (RPO)**: 24 hours --- ## Scaling and Performance ### Horizontal Pod Autoscaling CloudFlow is configured with Horizontal Pod Autoscaler (HPA) to automatically scale based on resource utilization: ```yaml # hpa.yaml apiVersion: autoscaling/v2 kind: HorizontalPodAutoscaler metadata: name: cloudflow-api-hpa namespace: cloudflow-prod spec: scaleTargetRef: apiVersion: apps/v1 kind: Deployment name: cloudflow-api minReplicas: 3 maxReplicas: 10 metrics: - type: Resource resource: name: cpu target: type: Utilization averageUtilization: 70 - type: Resource resource: name: memory target: type: Utilization averageUtilization: 80 behavior: scaleDown: stabilizationWindowSeconds: 300 policies: - type: Percent value: 50 periodSeconds: 60 scaleUp: stabilizationWindowSeconds: 60 policies: - type: Percent value: 100 periodSeconds: 30 - type: Pods value: 2 periodSeconds: 30 selectPolicy: Max ``` Monitor autoscaling events: ```bash kubectl get hpa -n cloudflow-prod -w kubectl describe hpa cloudflow-api-hpa -n cloudflow-prod ``` ### Performance Optimization #### Connection Pooling Ensure optimal database connection pooling settings in PgBouncer: - **Pool Mode**: Transaction (optimal for microservices) - **Default Pool Size**: 25 connections per user - **Max DB Connections**: 100 (matches PostgreSQL `max_connections`) #### Redis Caching Implement Redis caching for frequently accessed data: ```javascript // Example caching strategy const CACHE_TTL = { workflows: 300, // 5 minutes userSessions: 3600, // 1 hour apiResults: 60 // 1 minute }; ``` #### Load Testing Perform regular load testing to validate scaling configuration: ```bash # Install k6 for load testing kubectl apply -f - <<EOF apiVersion: v1 kind: ConfigMap metadata: name: loadtest-script namespace: cloudflow-prod data: script.js: | import http from 'k6/http'; import { check, sleep } from 'k6'; export let options = { stages: [ { duration: '2m', target: 100 }, { duration: '5m', target: 100 }, { duration: '2m', target: 200 }, { duration: '5m', target: 200 }, { duration: '2m', target: 0 }, ], thresholds: { http_req_duration: ['p(95)<500'], }, }; export default function () { let res = http.get('https://api.cloudflow.io/api/workflows'); check(res, { 'status is 200': (r) => r.status === 200 }); sleep(1); } EOF ``` **Performance Targets**: - P95 API latency: < 500ms - P99 API latency: < 1000ms - Error rate: < 0.1% - Throughput: > 1000 requests/second --- ## Security and Compliance ### Network Policies Implement network policies to restrict pod-to-pod communication: ```yaml # network-policy.yaml apiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: cloudflow-network-policy namespace: cloudflow-prod spec:",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_0",
              "content": "# CloudFlow Platform - System Architecture Overview **Document Version:** 2.3.1 **Last Updated:** January 15, 2026 **Owner:** Platform Architecture Team **Status:** Production ## Executive Summary CloudFlow is a distributed, cloud-native workflow automation platform designed to orchestrate complex business processes at scale. The platform processes over 2.5 million workflow executions daily with an average P99 latency of 180ms for API operations and 4.2 seconds for workflow execution. This document provides a comprehensive overview of the system architecture, including microservices design, data flow patterns, infrastructure decisions, and operational considerations. ## Table of Contents 1. [High-Level Architecture](#high-level-architecture) 2. [Microservices Breakdown](#microservices-breakdown) 3. [Data Flow Architecture](#data-flow-architecture) 4. [Database Architecture](#database-architecture) 5. [Message Queue Patterns](#message-queue-patterns) 6. [Caching Strategy](#caching-strategy) 7. [Security Architecture](#security-architecture) 8. [Performance Characteristics](#performance-characteristics) 9. [Disaster Recovery](#disaster-recovery) --- ## High-Level Architecture CloudFlow follows a microservices architecture pattern deployed across multiple availability zones in AWS. The platform is designed with the following core principles: - **Scalability**: Horizontal scaling for all services with auto-scaling groups - **Resilience**: Circuit breakers, bulkheads, and graceful degradation - **Observability**: Distributed tracing, centralized logging, and comprehensive metrics - **Security**: Zero-trust network architecture with mTLS encryption ### Architecture Diagram (Conceptual) ``` \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 Load Balancer (ALB) \u2502 \u2502 (TLS Termination - 443) \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u25bc \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 API Gateway Layer \u2502 \u2502 (Rate Limiting, Auth, Request Routing) \u2502 \u2514\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u2502 \u2502 \u2502 \u2502 \u25bc \u25bc \u25bc \u25bc \u25bc \u250c\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502Auth\u2502 \u2502Workflow\u2502 \u2502Scheduler\u2502 \u2502Notification\u2502 \u2502User Service\u2502 \u2502Svc \u2502 \u2502 Engine \u2502 \u2502 Service \u2502 \u2502 Service \u2502 \u2502 \u2502 \u2514\u2500\u252c\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u2502 \u2502 \u2502 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 \u2502 \u2502 \u25bc \u25bc \u25bc \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502PostgreSQL\u2502 \u2502 Redis \u2502 \u2502 Kafka \u2502 \u2502 Cluster \u2502 \u2502 Cluster \u2502 \u2502 Cluster \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 ``` ### Technology Stack - **Runtime**: Node.js 20.x (API Gateway, Workflow Engine), Go 1.21 (Auth Service, Scheduler) - **Container Orchestration**: Kubernetes 1.28 (EKS) - **Service Mesh**: Istio 1.20 for service-to-service communication - **Databases**: PostgreSQL 15.4, Redis 7.2 - **Message Broker**: Apache Kafka 3.6 - **Monitoring**: Prometheus, Grafana, Jaeger for distributed tracing - **Secrets Management**: HashiCorp Vault 1.15 --- ## Microservices Breakdown ### API Gateway **Purpose**: Single entry point for all client requests, providing authentication, rate limiting, request routing, and protocol translation.",
              "score": null
            },
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_3",
              "content": "Deploy PostgreSQL using the Bitnami Helm chart: ```yaml # postgres-values.yaml global: postgresql: auth: username: cloudflow database: cloudflow existingSecret: postgres-credentials image: tag: \"14.10.0\" primary: resources: limits: cpu: 4000m memory: 8Gi requests: cpu: 2000m memory: 4Gi persistence: enabled: true size: 100Gi storageClass: gp3 extendedConfiguration: | max_connections = 100 shared_buffers = 2GB effective_cache_size = 6GB maintenance_work_mem = 512MB checkpoint_completion_target = 0.9 wal_buffers = 16MB default_statistics_target = 100 random_page_cost = 1.1 effective_io_concurrency = 200 work_mem = 10MB min_wal_size = 1GB max_wal_size = 4GB readReplicas: replicaCount: 2 resources: limits: cpu: 2000m memory: 4Gi requests: cpu: 1000m memory: 2Gi metrics: enabled: true serviceMonitor: enabled: true ``` Install PostgreSQL: ```bash # Create password secret kubectl create secret generic postgres-credentials \\ --namespace cloudflow-prod \\ --from-literal=postgres-password=\"$(openssl rand -base64 32)\" \\ --from-literal=password=\"$(openssl rand -base64 32)\" # Install PostgreSQL helm install postgresql bitnami/postgresql \\ --namespace cloudflow-prod \\ --values postgres-values.yaml \\ --wait ``` ### PgBouncer Configuration Deploy PgBouncer for connection pooling to handle up to 100 connections efficiently: ```yaml # pgbouncer.yaml apiVersion: v1 kind: ConfigMap metadata: name: pgbouncer-config namespace: cloudflow-prod data: pgbouncer.ini: | [databases] cloudflow = host=postgresql.cloudflow-prod.svc.cluster.local port=5432 dbname=cloudflow [pgbouncer] listen_addr = 0.0.0.0 listen_port = 5432 auth_type = md5 auth_file = /etc/pgbouncer/userlist.txt pool_mode = transaction max_client_conn = 1000 default_pool_size = 25 reserve_pool_size = 5 reserve_pool_timeout = 3 max_db_connections = 100 max_user_connections = 100 server_lifetime = 3600 server_idle_timeout = 600 log_connections = 1 log_disconnections = 1 log_pooler_errors = 1 --- apiVersion: apps/v1 kind: Deployment metadata: name: pgbouncer namespace: cloudflow-prod spec: replicas: 2 selector: matchLabels: app: pgbouncer template: metadata: labels: app: pgbouncer spec: containers: - name: pgbouncer image: pgbouncer/pgbouncer:1.21.0 ports: - containerPort: 5432 resources: requests: cpu: 500m memory: 512Mi limits: cpu: 1000m memory: 1Gi volumeMounts: - name: config mountPath: /etc/pgbouncer volumes: - name: config configMap: name: pgbouncer-config --- apiVersion: v1 kind: Service metadata: name: pgbouncer namespace: cloudflow-prod spec: selector: app: pgbouncer ports: - port: 5432 targetPort: 5432 type: ClusterIP ``` Apply the PgBouncer configuration: ```bash kubectl apply -f pgbouncer.yaml ``` ### Database Migrations Run database migrations before deploying new versions: ```bash # Create migration job kubectl create job cloudflow-migrate-$(date +%s) \\ --namespace cloudflow-prod \\ --image=123456789012.dkr.ecr.us-east-1.amazonaws.com/cloudflow:2.4.0 \\ -- npm run migrate # Monitor migration progress kubectl logs -f job/cloudflow-migrate-<timestamp> -n cloudflow-prod ``` --- ## Monitoring and Observability ### Prometheus Setup Install Prometheus using the kube-prometheus-stack: ```bash helm install prometheus prometheus-community/kube-prometheus-stack \\ --namespace monitoring \\ --create-namespace \\ --set prometheus.prometheusSpec.serviceMonitorSelectorNilUsesHelmValues=false \\",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_3",
              "content": "Workflow Context Accumulation** Large workflow executions may accumulate state in memory. **Solution:** ```bash # Configure context cleanup cloudflow config set workflow.context.max_size_mb 100 cloudflow config set workflow.context.cleanup_threshold 0.8 # Enable context persistence to disk cloudflow config set workflow.context.persistence.enabled true cloudflow config set workflow.context.persistence.backend s3 ``` **2. Connection Pool Leaks** **Diagnosis:** ```bash # Check active connections cloudflow db connections --verbose # Expected output: # Active: 45/100 # Idle: 23 # Waiting: 2 # Average age: 245s ``` **Resolution:** ```bash # Adjust connection pool settings cloudflow config set db.pool.max_connections 100 cloudflow config set db.pool.min_connections 10 cloudflow config set db.pool.idle_timeout 300 cloudflow config set db.pool.max_lifetime 1800 # Force connection pool reset cloudflow db pool reset --confirm ``` **3. Event Stream Buffers** Unbounded event buffers can cause memory exhaustion. ```bash # Configure event buffer limits cloudflow config set events.buffer.max_size 10000 cloudflow config set events.buffer.overflow_strategy drop_oldest # Enable event streaming to external sink cloudflow config set events.sink.type kafka cloudflow config set events.sink.kafka.brokers \"kafka-1:9092,kafka-2:9092\" cloudflow config set events.sink.kafka.topic cloudflow-events ``` --- ## Database Connection Issues ### Connection Pool Exhaustion #### Symptoms - Error: `could not obtain connection from pool within 5000ms` - Error: `connection pool exhausted (100/100 connections in use)` - API requests fail with `503 Service Unavailable` - Database CPU usage normal, but connection count at maximum #### Investigation ```bash # Check current connection pool status cloudflow db pool status --detailed # Output example: # Pool Statistics: # Total Connections: 100/100 (100%) # Active: 87 # Idle: 13 # Waiting Requests: 45 # Average Wait Time: 3420ms # Max Wait Time: 8234ms # Identify long-running queries cloudflow db queries --status running --min-duration 30000 # Check connection distribution by client SELECT application_name, state, COUNT(*) as conn_count, AVG(EXTRACT(EPOCH FROM (NOW() - state_change))) as avg_duration_sec FROM pg_stat_activity WHERE datname = 'cloudflow_production' GROUP BY application_name, state ORDER BY conn_count DESC; ``` #### Resolution **Immediate mitigation:** ```bash # Temporarily increase connection limit (requires database restart) cloudflow db config set max_connections 150 # Kill idle connections older than 5 minutes cloudflow db connections kill --idle-timeout 300 # Restart connection pool without downtime kubectl rollout restart deployment/cloudflow-api -n cloudflow kubectl rollout status deployment/cloudflow-api -n cloudflow ``` **Long-term solutions:** 1.",
              "score": null
            }
          ],
          "latency_ms": 968.7114249973092,
          "query_id": "realistic_017",
          "dimension": "original",
          "query": "What is PgBouncer and why is it used in CloudFlow?"
        },
        {
          "key_facts": [
            "PgBouncer",
            "connection pooling",
            "max_db_connections = 100",
            "default_pool_size = 25",
            "pool_mode = transaction"
          ],
          "found_facts": [
            "PgBouncer",
            "connection pooling"
          ],
          "missed_facts": [
            "max_db_connections = 100",
            "default_pool_size = 25",
            "pool_mode = transaction"
          ],
          "coverage": 0.4,
          "expected_docs": [
            "deployment_guide",
            "architecture_overview"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_6",
              "content": "**Verify Service Health**: ```bash kubectl get pods -n cloudflow-prod curl https://api.cloudflow.io/health ``` **Recovery Time Objective (RTO)**: 4 hours **Recovery Point Objective (RPO)**: 24 hours --- ## Scaling and Performance ### Horizontal Pod Autoscaling CloudFlow is configured with Horizontal Pod Autoscaler (HPA) to automatically scale based on resource utilization: ```yaml # hpa.yaml apiVersion: autoscaling/v2 kind: HorizontalPodAutoscaler metadata: name: cloudflow-api-hpa namespace: cloudflow-prod spec: scaleTargetRef: apiVersion: apps/v1 kind: Deployment name: cloudflow-api minReplicas: 3 maxReplicas: 10 metrics: - type: Resource resource: name: cpu target: type: Utilization averageUtilization: 70 - type: Resource resource: name: memory target: type: Utilization averageUtilization: 80 behavior: scaleDown: stabilizationWindowSeconds: 300 policies: - type: Percent value: 50 periodSeconds: 60 scaleUp: stabilizationWindowSeconds: 60 policies: - type: Percent value: 100 periodSeconds: 30 - type: Pods value: 2 periodSeconds: 30 selectPolicy: Max ``` Monitor autoscaling events: ```bash kubectl get hpa -n cloudflow-prod -w kubectl describe hpa cloudflow-api-hpa -n cloudflow-prod ``` ### Performance Optimization #### Connection Pooling Ensure optimal database connection pooling settings in PgBouncer: - **Pool Mode**: Transaction (optimal for microservices) - **Default Pool Size**: 25 connections per user - **Max DB Connections**: 100 (matches PostgreSQL `max_connections`) #### Redis Caching Implement Redis caching for frequently accessed data: ```javascript // Example caching strategy const CACHE_TTL = { workflows: 300, // 5 minutes userSessions: 3600, // 1 hour apiResults: 60 // 1 minute }; ``` #### Load Testing Perform regular load testing to validate scaling configuration: ```bash # Install k6 for load testing kubectl apply -f - <<EOF apiVersion: v1 kind: ConfigMap metadata: name: loadtest-script namespace: cloudflow-prod data: script.js: | import http from 'k6/http'; import { check, sleep } from 'k6'; export let options = { stages: [ { duration: '2m', target: 100 }, { duration: '5m', target: 100 }, { duration: '2m', target: 200 }, { duration: '5m', target: 200 }, { duration: '2m', target: 0 }, ], thresholds: { http_req_duration: ['p(95)<500'], }, }; export default function () { let res = http.get('https://api.cloudflow.io/api/workflows'); check(res, { 'status is 200': (r) => r.status === 200 }); sleep(1); } EOF ``` **Performance Targets**: - P95 API latency: < 500ms - P99 API latency: < 1000ms - Error rate: < 0.1% - Throughput: > 1000 requests/second --- ## Security and Compliance ### Network Policies Implement network policies to restrict pod-to-pod communication: ```yaml # network-policy.yaml apiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: cloudflow-network-policy namespace: cloudflow-prod spec:",
              "score": null
            },
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_0",
              "content": "# CloudFlow Platform - Deployment and Operations Guide **Version:** 2.4.0 **Last Updated:** January 2026 **Target Environment:** Production (AWS EKS) ## Table of Contents 1. [Overview](#overview) 2. [Prerequisites](#prerequisites) 3. [Infrastructure Setup](#infrastructure-setup) 4. [Kubernetes Deployment](#kubernetes-deployment) 5. [Database Configuration](#database-configuration) 6. [Monitoring and Observability](#monitoring-and-observability) 7. [Backup and Disaster Recovery](#backup-and-disaster-recovery) 8. [Scaling and Performance](#scaling-and-performance) 9. [Security and Compliance](#security-and-compliance) 10. [Troubleshooting](#troubleshooting) --- ## Overview CloudFlow is a cloud-native workflow orchestration platform designed for high-availability production environments. This guide provides comprehensive instructions for deploying and operating CloudFlow on Amazon EKS (Elastic Kubernetes Service). ### Architecture Summary CloudFlow consists of the following components: - **API Server**: REST API for workflow management (Node.js/Express) - **Worker Service**: Background job processor (Node.js) - **Scheduler**: Cron-based task scheduler (Node.js) - **PostgreSQL**: Primary data store (version 14) - **Redis**: Cache and message queue (version 7.0) - **PgBouncer**: Database connection pooler ### Deployment Model - **Namespace**: `cloudflow-prod` - **Cluster**: AWS EKS 1.28 - **Region**: us-east-1 (primary), us-west-2 (disaster recovery) - **High Availability**: Multi-AZ deployment across 3 availability zones --- ## Prerequisites Before beginning the deployment process, ensure you have the following: ### Required Tools - `kubectl` (v1.28 or later) - `helm` (v3.12 or later) - `aws-cli` (v2.13 or later) - `eksctl` (v0.165 or later) - `terraform` (v1.6 or later) - for infrastructure provisioning ### Access Requirements - AWS account with appropriate IAM permissions - EKS cluster admin access - Container registry access (ECR) - Domain name and SSL certificates - Secrets management access (AWS Secrets Manager or Vault) ### Network Requirements - VPC with at least 3 public and 3 private subnets - NAT Gateway configured - Security groups allowing: - Ingress: HTTPS (443), HTTP (80) - Internal: PostgreSQL (5432), Redis (6379) - Monitoring: Prometheus (9090), Grafana (3000) --- ## Infrastructure Setup ### EKS Cluster Creation Use the following `eksctl` configuration to create the EKS cluster: ```yaml # cluster-config.yaml apiVersion: eksctl.io/v1alpha5 kind: ClusterConfig metadata: name: cloudflow-production region: us-east-1 version: \"1.28\" availabilityZones: - us-east-1a - us-east-1b - us-east-1c vpc: cidr: 10.0.0.0/16 nat: gateway: HighlyAvailable managedNodeGroups: - name: cloudflow-workers instanceType: m5.xlarge minSize: 3 maxSize: 10 desiredCapacity: 5 volumeSize: 100 privateNetworking: true labels: role: worker environment: production tags: Environment: production Application: cloudflow iam: withAddonPolicies: ebs: true efs: true albIngress: true cloudWatch: true cloudWatch: clusterLogging: enableTypes: - api - audit - authenticator - controllerManager - scheduler ``` Create the cluster: ```bash eksctl",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_5",
              "content": "Success/failure events published back to Kafka ``` **Event Schema**: ```json { \"event_id\": \"uuid-v4\", \"event_type\": \"workflow.execution.completed\", \"timestamp\": \"2026-01-15T10:30:00.000Z\", \"correlation_id\": \"request-trace-id\", \"payload\": { \"workflow_id\": \"wf-12345\", \"execution_id\": \"exec-67890\", \"status\": \"COMPLETED\", \"duration_ms\": 4230 }, \"metadata\": { \"source_service\": \"workflow-engine\", \"schema_version\": \"1.0\" } } ``` ### Inter-Service Communication Services communicate using two primary patterns: **Synchronous (gRPC)**: - Auth Service validation calls from API Gateway - User Service profile lookups - Low latency requirement (< 50ms) - Request-response pattern with circuit breaker **Asynchronous (Kafka)**: - Workflow execution events - Notification triggers - Audit log events - Eventual consistency acceptable --- ## Database Architecture ### PostgreSQL Primary Database **Cluster Configuration**: - Primary-replica setup with 1 primary + 2 read replicas - Instance type: db.r6g.2xlarge (8 vCPU, 64GB RAM) - Storage: 2TB gp3 SSD with 12,000 IOPS - Multi-AZ deployment for high availability - Automated backups: Daily snapshots, 30-day retention - Point-in-time recovery: 5-minute granularity **Database Schema Design**: ``` Core Tables: - users (5M rows): User accounts and profiles - workflows (2M rows): Workflow definitions - workflow_executions (500M rows, partitioned): Execution history - workflow_steps (2B rows, partitioned): Individual step records - schedules (100K rows): Scheduled workflow triggers - notifications (1B rows, partitioned): Notification delivery log Partitioning Strategy: - workflow_executions: Monthly partitions by created_at - workflow_steps: Monthly partitions by created_at - notifications: Weekly partitions by created_at - Automatic partition creation via cron job - Retention policy: Drop partitions older than 12 months ``` **Indexing Strategy**: - Primary keys: UUIDs with B-tree indexes - Frequently queried columns: Compound indexes (e.g., user_id + status + created_at) - JSONB columns: GIN indexes for workflow definitions - Full-text search: GiST indexes on description fields **Connection Pooling**: - PgBouncer in transaction mode - Pool size: 100 connections per service - Max client connections: 2000 - Connection timeout: 30 seconds **Performance Characteristics**: - Read query P99: < 20ms - Write query P99: < 50ms - Transaction throughput: 15,000 TPS - Replication lag: < 500ms --- ### Redis Caching Layer **Cluster Configuration**: - Redis Cluster with 6 nodes (3 primary + 3 replica) - Instance type: cache.r6g.xlarge (4 vCPU, 26GB RAM) - Total memory: 78GB usable cache space - Persistence: RDB snapshots every 5 minutes + AOF - Eviction policy: allkeys-lru **Cache Usage Patterns**: 1.",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_12",
              "content": "Resume normal operations - Recovery time: 1-3 hours depending on data volume - Data loss: None if corruption detected quickly ### Testing & Validation **DR Drill Schedule**: - Monthly: Automated failover test (single AZ failure simulation) - Quarterly: Full DR region failover (non-production hours) - Annually: Complete disaster simulation with all stakeholders **Last DR Test Results** (Dec 15, 2025): - Scenario: Full region failover - Actual RTO: 2 hours 23 minutes (target: 4 hours) - Actual RPO: 42 minutes (target: 1 hour) - Issues identified: DNS propagation slower than expected (resolved) - Success criteria: Met all recovery objectives ### Business Continuity **Communication Plan**: - Status page: status.cloudflow.com (updated every 15 minutes during incident) - Customer notifications: Email + SMS for all P1 incidents - Internal escalation: PagerDuty \u2192 Incident Commander \u2192 Engineering Manager \u2192 CTO **Data Retention Policy**: - Active data: 12 months in hot storage (PostgreSQL) - Archived data: 7 years in cold storage (S3 Glacier) - Audit logs: 7 years (compliance requirement) - Backup retention: 30 days standard, 90 days monthly snapshots --- ## Appendix ### Service Dependency Matrix | Service | Depends On | Critical Path | |---------|-----------|---------------| | API Gateway | Auth Service, Redis | Yes | | Auth Service | PostgreSQL, Redis, Vault | Yes | | Workflow Engine | PostgreSQL, Kafka, Redis | Yes | | Scheduler | PostgreSQL, Redis, Kafka | No | | Notification Service | Kafka, PostgreSQL, SendGrid, Twilio | No | ### Capacity Planning **Current Capacity** (Jan 2026): - Daily workflow executions: 2.5M - Active users: 150,000 - API requests per day: 50M - Database size: 1.2TB - Kafka throughput: 100K msg/sec peak **6-Month Projection** (Jul 2026): - Daily workflow executions: 4M (+60%) - Active users: 225,000 (+50%) - API requests per day: 80M (+60%) - Database size: 2TB (+67%) - Required scaling: +30% compute, +50% storage ### Contact Information - **Architecture Team**: architecture@cloudflow.internal - **On-Call Engineer**: PagerDuty escalation - **Security Team**: security@cloudflow.internal - **Documentation**: https://wiki.cloudflow.internal/architecture --- **Document Revision History**: - v2.3.1 (Jan 15, 2026): Updated performance metrics, added DR test results - v2.3.0 (Dec 1, 2025): Added multi-AZ deployment details - v2.2.0 (Oct 15, 2025): Security architecture overhaul - v2.1.0 (Sep 1, 2025): Initial Kafka migration documentation - v2.0.0 (Jul 1, 2025): Complete microservices rewrite",
              "score": null
            },
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_1",
              "content": "create cluster -f cluster-config.yaml ``` ### Storage Configuration Install the EBS CSI driver for persistent volumes: ```bash eksctl create addon --name aws-ebs-csi-driver \\ --cluster cloudflow-production \\ --service-account-role-arn arn:aws:iam::ACCOUNT_ID:role/AmazonEKS_EBS_CSI_DriverRole \\ --force ``` --- ## Kubernetes Deployment ### Namespace Setup Create the CloudFlow namespace and configure resource quotas: ```yaml # namespace.yaml apiVersion: v1 kind: Namespace metadata: name: cloudflow-prod labels: name: cloudflow-prod environment: production --- apiVersion: v1 kind: ResourceQuota metadata: name: cloudflow-quota namespace: cloudflow-prod spec: hard: requests.cpu: \"50\" requests.memory: 100Gi persistentvolumeclaims: \"10\" services.loadbalancers: \"2\" ``` Apply the namespace configuration: ```bash kubectl apply -f namespace.yaml ``` ### Environment Configuration CloudFlow requires the following environment variables: | Variable | Description | Example | Required | |----------|-------------|---------|----------| | `DATABASE_URL` | PostgreSQL connection string | `postgresql://user:pass@host:5432/cloudflow` | Yes | | `REDIS_URL` | Redis connection string | `redis://redis-master.cloudflow-prod.svc.cluster.local:6379` | Yes | | `JWT_SECRET` | Secret key for JWT token signing | `<generated-secret-256-bit>` | Yes | | `LOG_LEVEL` | Application log verbosity | `info`, `debug`, `warn`, `error` | No (default: `info`) | | `API_PORT` | API server port | `3000` | No (default: `3000`) | | `WORKER_CONCURRENCY` | Number of concurrent workers | `10` | No (default: `5`) | | `SESSION_SECRET` | Session encryption key | `<generated-secret-256-bit>` | Yes | | `AWS_REGION` | AWS region for services | `us-east-1` | Yes | | `S3_BUCKET` | S3 bucket for file storage | `cloudflow-prod-storage` | Yes | | `SMTP_HOST` | Email server hostname | `smtp.sendgrid.net` | No | | `SMTP_PORT` | Email server port | `587` | No | | `METRICS_ENABLED` | Enable Prometheus metrics | `true` | No (default: `false`) | Create a Kubernetes secret for sensitive variables: ```bash kubectl create secret generic cloudflow-secrets \\ --namespace cloudflow-prod \\ --from-literal=DATABASE_URL=\"postgresql://cloudflow:$(cat db-password.txt)@pgbouncer.cloudflow-prod.svc.cluster.local:5432/cloudflow\" \\ --from-literal=REDIS_URL=\"redis://redis-master.cloudflow-prod.svc.cluster.local:6379\" \\ --from-literal=JWT_SECRET=\"$(openssl rand -base64 32)\" \\ --from-literal=SESSION_SECRET=\"$(openssl rand -base64 32)\" ``` ### Helm Chart Deployment Add the CloudFlow Helm repository: ```bash helm repo add cloudflow https://charts.cloudflow.io helm repo update ``` Create a values file for production configuration: ```yaml # values-production.yaml replicaCount: 3 image: repository: 123456789012.dkr.ecr.us-east-1.amazonaws.com/cloudflow tag: \"2.4.0\" pullPolicy: IfNotPresent resources: limits: cpu: 2000m memory: 4Gi requests: cpu: 2000m memory: 4Gi autoscaling: enabled: true minReplicas: 3 maxReplicas: 10 targetCPUUtilizationPercentage: 70 targetMemoryUtilizationPercentage: 80 service: type: ClusterIP port: 80 targetPort: 3000 annotations: service.beta.kubernetes.io/aws-load-balancer-type: \"nlb\" ingress: enabled: true className: nginx annotations: cert-manager.io/cluster-issuer: \"letsencrypt-prod\" nginx.ingress.kubernetes.io/ssl-redirect: \"true\" hosts: - host: api.cloudflow.io paths: - path: / pathType: Prefix tls:",
              "score": null
            }
          ],
          "latency_ms": 855.4036819987232,
          "query_id": "realistic_017",
          "dimension": "synonym",
          "query": "What's the purpose of the connection pooler?"
        },
        {
          "key_facts": [
            "PgBouncer",
            "connection pooling",
            "max_db_connections = 100",
            "default_pool_size = 25",
            "pool_mode = transaction"
          ],
          "found_facts": [
            "PgBouncer",
            "connection pooling",
            "max_db_connections = 100",
            "default_pool_size = 25",
            "pool_mode = transaction"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "deployment_guide",
            "architecture_overview"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_4",
              "content": "**Implement connection pooling optimization:** ```bash # Use PgBouncer for connection pooling kubectl apply -f cloudflow-pgbouncer.yaml # Configure CloudFlow to use PgBouncer cloudflow config set db.host pgbouncer.cloudflow.svc.cluster.local cloudflow config set db.port 6432 cloudflow config set db.pool.mode transaction ``` 2. **Add read replicas:** ```bash # Route read-only queries to replicas cloudflow db replicas add --count 2 cloudflow config set db.read_replicas \"replica-1:5432,replica-2:5432\" cloudflow config set db.read_write_split true ``` ### Connection Timeout Errors #### Error Messages - `connection timeout after 30000ms` - `could not connect to database server at 10.0.2.45:5432` - `database server unreachable` #### Troubleshooting Steps ```bash # Test network connectivity telnet cloudflow-db.internal.company.com 5432 # Check DNS resolution nslookup cloudflow-db.internal.company.com # Verify database is accepting connections pg_isready -h cloudflow-db.internal.company.com -p 5432 -U cloudflow # Check firewall rules sudo iptables -L -n | grep 5432 # Test from CloudFlow pod network kubectl run -n cloudflow debug-pod --rm -i --tty \\ --image=postgres:14 -- \\ psql -h cloudflow-db.internal.company.com -U cloudflow -d cloudflow_production # Review database logs for connection rejections kubectl logs -n cloudflow statefulset/cloudflow-db --tail=100 | \\ grep -i \"connection\\|reject\\|authentication\" ``` ### Maximum Connection Limit (100) Reached This is a hard limit in CloudFlow's database tier. #### Permanent Solutions **Option 1: Upgrade database tier** ```bash # Check available tiers cloudflow db tiers list # Upgrade to higher tier (supports 200 connections) cloudflow db upgrade --tier standard-plus --confirm # Monitor migration progress cloudflow db migration status ``` **Option 2: Implement aggressive connection reuse** ```bash # Reduce connection lifetime cloudflow config set db.pool.max_lifetime 600 # 10 minutes # Enable prepared statement caching cloudflow config set db.prepared_statements.cache true cloudflow config set db.prepared_statements.max_size 250 # Reduce idle connection timeout cloudflow config set db.pool.idle_timeout 120 # 2 minutes ``` --- ## Workflow Execution Failures ### Timeout Errors (3600 second limit) #### Error Message ``` WorkflowExecutionError: Workflow exceeded maximum execution time of 3600 seconds Status: TIMEOUT Workflow ID: wf_9k2n4m8p1q Execution ID: exec_7h3j6k9m2n ``` #### Analysis ```bash # Get workflow execution details cloudflow workflows executions get exec_7h3j6k9m2n --verbose # Check step-by-step breakdown cloudflow workflows executions steps exec_7h3j6k9m2n # Sample output: # Step 1: data_ingestion - Duration: 245s - Status: SUCCESS # Step 2: data_validation - Duration: 123s - Status: SUCCESS # Step 3: data_transformation - Duration: 3189s - Status: TIMEOUT # Step 4: data_export - Duration: 0s - Status: SKIPPED # Identify bottleneck step cloudflow workflows analyze",
              "score": null
            },
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_3",
              "content": "Deploy PostgreSQL using the Bitnami Helm chart: ```yaml # postgres-values.yaml global: postgresql: auth: username: cloudflow database: cloudflow existingSecret: postgres-credentials image: tag: \"14.10.0\" primary: resources: limits: cpu: 4000m memory: 8Gi requests: cpu: 2000m memory: 4Gi persistence: enabled: true size: 100Gi storageClass: gp3 extendedConfiguration: | max_connections = 100 shared_buffers = 2GB effective_cache_size = 6GB maintenance_work_mem = 512MB checkpoint_completion_target = 0.9 wal_buffers = 16MB default_statistics_target = 100 random_page_cost = 1.1 effective_io_concurrency = 200 work_mem = 10MB min_wal_size = 1GB max_wal_size = 4GB readReplicas: replicaCount: 2 resources: limits: cpu: 2000m memory: 4Gi requests: cpu: 1000m memory: 2Gi metrics: enabled: true serviceMonitor: enabled: true ``` Install PostgreSQL: ```bash # Create password secret kubectl create secret generic postgres-credentials \\ --namespace cloudflow-prod \\ --from-literal=postgres-password=\"$(openssl rand -base64 32)\" \\ --from-literal=password=\"$(openssl rand -base64 32)\" # Install PostgreSQL helm install postgresql bitnami/postgresql \\ --namespace cloudflow-prod \\ --values postgres-values.yaml \\ --wait ``` ### PgBouncer Configuration Deploy PgBouncer for connection pooling to handle up to 100 connections efficiently: ```yaml # pgbouncer.yaml apiVersion: v1 kind: ConfigMap metadata: name: pgbouncer-config namespace: cloudflow-prod data: pgbouncer.ini: | [databases] cloudflow = host=postgresql.cloudflow-prod.svc.cluster.local port=5432 dbname=cloudflow [pgbouncer] listen_addr = 0.0.0.0 listen_port = 5432 auth_type = md5 auth_file = /etc/pgbouncer/userlist.txt pool_mode = transaction max_client_conn = 1000 default_pool_size = 25 reserve_pool_size = 5 reserve_pool_timeout = 3 max_db_connections = 100 max_user_connections = 100 server_lifetime = 3600 server_idle_timeout = 600 log_connections = 1 log_disconnections = 1 log_pooler_errors = 1 --- apiVersion: apps/v1 kind: Deployment metadata: name: pgbouncer namespace: cloudflow-prod spec: replicas: 2 selector: matchLabels: app: pgbouncer template: metadata: labels: app: pgbouncer spec: containers: - name: pgbouncer image: pgbouncer/pgbouncer:1.21.0 ports: - containerPort: 5432 resources: requests: cpu: 500m memory: 512Mi limits: cpu: 1000m memory: 1Gi volumeMounts: - name: config mountPath: /etc/pgbouncer volumes: - name: config configMap: name: pgbouncer-config --- apiVersion: v1 kind: Service metadata: name: pgbouncer namespace: cloudflow-prod spec: selector: app: pgbouncer ports: - port: 5432 targetPort: 5432 type: ClusterIP ``` Apply the PgBouncer configuration: ```bash kubectl apply -f pgbouncer.yaml ``` ### Database Migrations Run database migrations before deploying new versions: ```bash # Create migration job kubectl create job cloudflow-migrate-$(date +%s) \\ --namespace cloudflow-prod \\ --image=123456789012.dkr.ecr.us-east-1.amazonaws.com/cloudflow:2.4.0 \\ -- npm run migrate # Monitor migration progress kubectl logs -f job/cloudflow-migrate-<timestamp> -n cloudflow-prod ``` --- ## Monitoring and Observability ### Prometheus Setup Install Prometheus using the kube-prometheus-stack: ```bash helm install prometheus prometheus-community/kube-prometheus-stack \\ --namespace monitoring \\ --create-namespace \\ --set prometheus.prometheusSpec.serviceMonitorSelectorNilUsesHelmValues=false \\",
              "score": null
            },
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_6",
              "content": "**Verify Service Health**: ```bash kubectl get pods -n cloudflow-prod curl https://api.cloudflow.io/health ``` **Recovery Time Objective (RTO)**: 4 hours **Recovery Point Objective (RPO)**: 24 hours --- ## Scaling and Performance ### Horizontal Pod Autoscaling CloudFlow is configured with Horizontal Pod Autoscaler (HPA) to automatically scale based on resource utilization: ```yaml # hpa.yaml apiVersion: autoscaling/v2 kind: HorizontalPodAutoscaler metadata: name: cloudflow-api-hpa namespace: cloudflow-prod spec: scaleTargetRef: apiVersion: apps/v1 kind: Deployment name: cloudflow-api minReplicas: 3 maxReplicas: 10 metrics: - type: Resource resource: name: cpu target: type: Utilization averageUtilization: 70 - type: Resource resource: name: memory target: type: Utilization averageUtilization: 80 behavior: scaleDown: stabilizationWindowSeconds: 300 policies: - type: Percent value: 50 periodSeconds: 60 scaleUp: stabilizationWindowSeconds: 60 policies: - type: Percent value: 100 periodSeconds: 30 - type: Pods value: 2 periodSeconds: 30 selectPolicy: Max ``` Monitor autoscaling events: ```bash kubectl get hpa -n cloudflow-prod -w kubectl describe hpa cloudflow-api-hpa -n cloudflow-prod ``` ### Performance Optimization #### Connection Pooling Ensure optimal database connection pooling settings in PgBouncer: - **Pool Mode**: Transaction (optimal for microservices) - **Default Pool Size**: 25 connections per user - **Max DB Connections**: 100 (matches PostgreSQL `max_connections`) #### Redis Caching Implement Redis caching for frequently accessed data: ```javascript // Example caching strategy const CACHE_TTL = { workflows: 300, // 5 minutes userSessions: 3600, // 1 hour apiResults: 60 // 1 minute }; ``` #### Load Testing Perform regular load testing to validate scaling configuration: ```bash # Install k6 for load testing kubectl apply -f - <<EOF apiVersion: v1 kind: ConfigMap metadata: name: loadtest-script namespace: cloudflow-prod data: script.js: | import http from 'k6/http'; import { check, sleep } from 'k6'; export let options = { stages: [ { duration: '2m', target: 100 }, { duration: '5m', target: 100 }, { duration: '2m', target: 200 }, { duration: '5m', target: 200 }, { duration: '2m', target: 0 }, ], thresholds: { http_req_duration: ['p(95)<500'], }, }; export default function () { let res = http.get('https://api.cloudflow.io/api/workflows'); check(res, { 'status is 200': (r) => r.status === 200 }); sleep(1); } EOF ``` **Performance Targets**: - P95 API latency: < 500ms - P99 API latency: < 1000ms - Error rate: < 0.1% - Throughput: > 1000 requests/second --- ## Security and Compliance ### Network Policies Implement network policies to restrict pod-to-pod communication: ```yaml # network-policy.yaml apiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: cloudflow-network-policy namespace: cloudflow-prod spec:",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_5",
              "content": "Success/failure events published back to Kafka ``` **Event Schema**: ```json { \"event_id\": \"uuid-v4\", \"event_type\": \"workflow.execution.completed\", \"timestamp\": \"2026-01-15T10:30:00.000Z\", \"correlation_id\": \"request-trace-id\", \"payload\": { \"workflow_id\": \"wf-12345\", \"execution_id\": \"exec-67890\", \"status\": \"COMPLETED\", \"duration_ms\": 4230 }, \"metadata\": { \"source_service\": \"workflow-engine\", \"schema_version\": \"1.0\" } } ``` ### Inter-Service Communication Services communicate using two primary patterns: **Synchronous (gRPC)**: - Auth Service validation calls from API Gateway - User Service profile lookups - Low latency requirement (< 50ms) - Request-response pattern with circuit breaker **Asynchronous (Kafka)**: - Workflow execution events - Notification triggers - Audit log events - Eventual consistency acceptable --- ## Database Architecture ### PostgreSQL Primary Database **Cluster Configuration**: - Primary-replica setup with 1 primary + 2 read replicas - Instance type: db.r6g.2xlarge (8 vCPU, 64GB RAM) - Storage: 2TB gp3 SSD with 12,000 IOPS - Multi-AZ deployment for high availability - Automated backups: Daily snapshots, 30-day retention - Point-in-time recovery: 5-minute granularity **Database Schema Design**: ``` Core Tables: - users (5M rows): User accounts and profiles - workflows (2M rows): Workflow definitions - workflow_executions (500M rows, partitioned): Execution history - workflow_steps (2B rows, partitioned): Individual step records - schedules (100K rows): Scheduled workflow triggers - notifications (1B rows, partitioned): Notification delivery log Partitioning Strategy: - workflow_executions: Monthly partitions by created_at - workflow_steps: Monthly partitions by created_at - notifications: Weekly partitions by created_at - Automatic partition creation via cron job - Retention policy: Drop partitions older than 12 months ``` **Indexing Strategy**: - Primary keys: UUIDs with B-tree indexes - Frequently queried columns: Compound indexes (e.g., user_id + status + created_at) - JSONB columns: GIN indexes for workflow definitions - Full-text search: GiST indexes on description fields **Connection Pooling**: - PgBouncer in transaction mode - Pool size: 100 connections per service - Max client connections: 2000 - Connection timeout: 30 seconds **Performance Characteristics**: - Read query P99: < 20ms - Write query P99: < 50ms - Transaction throughput: 15,000 TPS - Replication lag: < 500ms --- ### Redis Caching Layer **Cluster Configuration**: - Redis Cluster with 6 nodes (3 primary + 3 replica) - Instance type: cache.r6g.xlarge (4 vCPU, 26GB RAM) - Total memory: 78GB usable cache space - Persistence: RDB snapshots every 5 minutes + AOF - Eviction policy: allkeys-lru **Cache Usage Patterns**: 1.",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_3",
              "content": "Workflow Context Accumulation** Large workflow executions may accumulate state in memory. **Solution:** ```bash # Configure context cleanup cloudflow config set workflow.context.max_size_mb 100 cloudflow config set workflow.context.cleanup_threshold 0.8 # Enable context persistence to disk cloudflow config set workflow.context.persistence.enabled true cloudflow config set workflow.context.persistence.backend s3 ``` **2. Connection Pool Leaks** **Diagnosis:** ```bash # Check active connections cloudflow db connections --verbose # Expected output: # Active: 45/100 # Idle: 23 # Waiting: 2 # Average age: 245s ``` **Resolution:** ```bash # Adjust connection pool settings cloudflow config set db.pool.max_connections 100 cloudflow config set db.pool.min_connections 10 cloudflow config set db.pool.idle_timeout 300 cloudflow config set db.pool.max_lifetime 1800 # Force connection pool reset cloudflow db pool reset --confirm ``` **3. Event Stream Buffers** Unbounded event buffers can cause memory exhaustion. ```bash # Configure event buffer limits cloudflow config set events.buffer.max_size 10000 cloudflow config set events.buffer.overflow_strategy drop_oldest # Enable event streaming to external sink cloudflow config set events.sink.type kafka cloudflow config set events.sink.kafka.brokers \"kafka-1:9092,kafka-2:9092\" cloudflow config set events.sink.kafka.topic cloudflow-events ``` --- ## Database Connection Issues ### Connection Pool Exhaustion #### Symptoms - Error: `could not obtain connection from pool within 5000ms` - Error: `connection pool exhausted (100/100 connections in use)` - API requests fail with `503 Service Unavailable` - Database CPU usage normal, but connection count at maximum #### Investigation ```bash # Check current connection pool status cloudflow db pool status --detailed # Output example: # Pool Statistics: # Total Connections: 100/100 (100%) # Active: 87 # Idle: 13 # Waiting Requests: 45 # Average Wait Time: 3420ms # Max Wait Time: 8234ms # Identify long-running queries cloudflow db queries --status running --min-duration 30000 # Check connection distribution by client SELECT application_name, state, COUNT(*) as conn_count, AVG(EXTRACT(EPOCH FROM (NOW() - state_change))) as avg_duration_sec FROM pg_stat_activity WHERE datname = 'cloudflow_production' GROUP BY application_name, state ORDER BY conn_count DESC; ``` #### Resolution **Immediate mitigation:** ```bash # Temporarily increase connection limit (requires database restart) cloudflow db config set max_connections 150 # Kill idle connections older than 5 minutes cloudflow db connections kill --idle-timeout 300 # Restart connection pool without downtime kubectl rollout restart deployment/cloudflow-api -n cloudflow kubectl rollout status deployment/cloudflow-api -n cloudflow ``` **Long-term solutions:** 1.",
              "score": null
            }
          ],
          "latency_ms": 1105.3950499990606,
          "query_id": "realistic_017",
          "dimension": "problem",
          "query": "Should I connect directly to PostgreSQL or through PgBouncer?"
        },
        {
          "key_facts": [
            "PgBouncer",
            "connection pooling",
            "max_db_connections = 100",
            "default_pool_size = 25",
            "pool_mode = transaction"
          ],
          "found_facts": [
            "PgBouncer",
            "connection pooling",
            "max_db_connections = 100",
            "default_pool_size = 25",
            "pool_mode = transaction"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "deployment_guide",
            "architecture_overview"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_3",
              "content": "Deploy PostgreSQL using the Bitnami Helm chart: ```yaml # postgres-values.yaml global: postgresql: auth: username: cloudflow database: cloudflow existingSecret: postgres-credentials image: tag: \"14.10.0\" primary: resources: limits: cpu: 4000m memory: 8Gi requests: cpu: 2000m memory: 4Gi persistence: enabled: true size: 100Gi storageClass: gp3 extendedConfiguration: | max_connections = 100 shared_buffers = 2GB effective_cache_size = 6GB maintenance_work_mem = 512MB checkpoint_completion_target = 0.9 wal_buffers = 16MB default_statistics_target = 100 random_page_cost = 1.1 effective_io_concurrency = 200 work_mem = 10MB min_wal_size = 1GB max_wal_size = 4GB readReplicas: replicaCount: 2 resources: limits: cpu: 2000m memory: 4Gi requests: cpu: 1000m memory: 2Gi metrics: enabled: true serviceMonitor: enabled: true ``` Install PostgreSQL: ```bash # Create password secret kubectl create secret generic postgres-credentials \\ --namespace cloudflow-prod \\ --from-literal=postgres-password=\"$(openssl rand -base64 32)\" \\ --from-literal=password=\"$(openssl rand -base64 32)\" # Install PostgreSQL helm install postgresql bitnami/postgresql \\ --namespace cloudflow-prod \\ --values postgres-values.yaml \\ --wait ``` ### PgBouncer Configuration Deploy PgBouncer for connection pooling to handle up to 100 connections efficiently: ```yaml # pgbouncer.yaml apiVersion: v1 kind: ConfigMap metadata: name: pgbouncer-config namespace: cloudflow-prod data: pgbouncer.ini: | [databases] cloudflow = host=postgresql.cloudflow-prod.svc.cluster.local port=5432 dbname=cloudflow [pgbouncer] listen_addr = 0.0.0.0 listen_port = 5432 auth_type = md5 auth_file = /etc/pgbouncer/userlist.txt pool_mode = transaction max_client_conn = 1000 default_pool_size = 25 reserve_pool_size = 5 reserve_pool_timeout = 3 max_db_connections = 100 max_user_connections = 100 server_lifetime = 3600 server_idle_timeout = 600 log_connections = 1 log_disconnections = 1 log_pooler_errors = 1 --- apiVersion: apps/v1 kind: Deployment metadata: name: pgbouncer namespace: cloudflow-prod spec: replicas: 2 selector: matchLabels: app: pgbouncer template: metadata: labels: app: pgbouncer spec: containers: - name: pgbouncer image: pgbouncer/pgbouncer:1.21.0 ports: - containerPort: 5432 resources: requests: cpu: 500m memory: 512Mi limits: cpu: 1000m memory: 1Gi volumeMounts: - name: config mountPath: /etc/pgbouncer volumes: - name: config configMap: name: pgbouncer-config --- apiVersion: v1 kind: Service metadata: name: pgbouncer namespace: cloudflow-prod spec: selector: app: pgbouncer ports: - port: 5432 targetPort: 5432 type: ClusterIP ``` Apply the PgBouncer configuration: ```bash kubectl apply -f pgbouncer.yaml ``` ### Database Migrations Run database migrations before deploying new versions: ```bash # Create migration job kubectl create job cloudflow-migrate-$(date +%s) \\ --namespace cloudflow-prod \\ --image=123456789012.dkr.ecr.us-east-1.amazonaws.com/cloudflow:2.4.0 \\ -- npm run migrate # Monitor migration progress kubectl logs -f job/cloudflow-migrate-<timestamp> -n cloudflow-prod ``` --- ## Monitoring and Observability ### Prometheus Setup Install Prometheus using the kube-prometheus-stack: ```bash helm install prometheus prometheus-community/kube-prometheus-stack \\ --namespace monitoring \\ --create-namespace \\ --set prometheus.prometheusSpec.serviceMonitorSelectorNilUsesHelmValues=false \\",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_4",
              "content": "**Implement connection pooling optimization:** ```bash # Use PgBouncer for connection pooling kubectl apply -f cloudflow-pgbouncer.yaml # Configure CloudFlow to use PgBouncer cloudflow config set db.host pgbouncer.cloudflow.svc.cluster.local cloudflow config set db.port 6432 cloudflow config set db.pool.mode transaction ``` 2. **Add read replicas:** ```bash # Route read-only queries to replicas cloudflow db replicas add --count 2 cloudflow config set db.read_replicas \"replica-1:5432,replica-2:5432\" cloudflow config set db.read_write_split true ``` ### Connection Timeout Errors #### Error Messages - `connection timeout after 30000ms` - `could not connect to database server at 10.0.2.45:5432` - `database server unreachable` #### Troubleshooting Steps ```bash # Test network connectivity telnet cloudflow-db.internal.company.com 5432 # Check DNS resolution nslookup cloudflow-db.internal.company.com # Verify database is accepting connections pg_isready -h cloudflow-db.internal.company.com -p 5432 -U cloudflow # Check firewall rules sudo iptables -L -n | grep 5432 # Test from CloudFlow pod network kubectl run -n cloudflow debug-pod --rm -i --tty \\ --image=postgres:14 -- \\ psql -h cloudflow-db.internal.company.com -U cloudflow -d cloudflow_production # Review database logs for connection rejections kubectl logs -n cloudflow statefulset/cloudflow-db --tail=100 | \\ grep -i \"connection\\|reject\\|authentication\" ``` ### Maximum Connection Limit (100) Reached This is a hard limit in CloudFlow's database tier. #### Permanent Solutions **Option 1: Upgrade database tier** ```bash # Check available tiers cloudflow db tiers list # Upgrade to higher tier (supports 200 connections) cloudflow db upgrade --tier standard-plus --confirm # Monitor migration progress cloudflow db migration status ``` **Option 2: Implement aggressive connection reuse** ```bash # Reduce connection lifetime cloudflow config set db.pool.max_lifetime 600 # 10 minutes # Enable prepared statement caching cloudflow config set db.prepared_statements.cache true cloudflow config set db.prepared_statements.max_size 250 # Reduce idle connection timeout cloudflow config set db.pool.idle_timeout 120 # 2 minutes ``` --- ## Workflow Execution Failures ### Timeout Errors (3600 second limit) #### Error Message ``` WorkflowExecutionError: Workflow exceeded maximum execution time of 3600 seconds Status: TIMEOUT Workflow ID: wf_9k2n4m8p1q Execution ID: exec_7h3j6k9m2n ``` #### Analysis ```bash # Get workflow execution details cloudflow workflows executions get exec_7h3j6k9m2n --verbose # Check step-by-step breakdown cloudflow workflows executions steps exec_7h3j6k9m2n # Sample output: # Step 1: data_ingestion - Duration: 245s - Status: SUCCESS # Step 2: data_validation - Duration: 123s - Status: SUCCESS # Step 3: data_transformation - Duration: 3189s - Status: TIMEOUT # Step 4: data_export - Duration: 0s - Status: SKIPPED # Identify bottleneck step cloudflow workflows analyze",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_5",
              "content": "Success/failure events published back to Kafka ``` **Event Schema**: ```json { \"event_id\": \"uuid-v4\", \"event_type\": \"workflow.execution.completed\", \"timestamp\": \"2026-01-15T10:30:00.000Z\", \"correlation_id\": \"request-trace-id\", \"payload\": { \"workflow_id\": \"wf-12345\", \"execution_id\": \"exec-67890\", \"status\": \"COMPLETED\", \"duration_ms\": 4230 }, \"metadata\": { \"source_service\": \"workflow-engine\", \"schema_version\": \"1.0\" } } ``` ### Inter-Service Communication Services communicate using two primary patterns: **Synchronous (gRPC)**: - Auth Service validation calls from API Gateway - User Service profile lookups - Low latency requirement (< 50ms) - Request-response pattern with circuit breaker **Asynchronous (Kafka)**: - Workflow execution events - Notification triggers - Audit log events - Eventual consistency acceptable --- ## Database Architecture ### PostgreSQL Primary Database **Cluster Configuration**: - Primary-replica setup with 1 primary + 2 read replicas - Instance type: db.r6g.2xlarge (8 vCPU, 64GB RAM) - Storage: 2TB gp3 SSD with 12,000 IOPS - Multi-AZ deployment for high availability - Automated backups: Daily snapshots, 30-day retention - Point-in-time recovery: 5-minute granularity **Database Schema Design**: ``` Core Tables: - users (5M rows): User accounts and profiles - workflows (2M rows): Workflow definitions - workflow_executions (500M rows, partitioned): Execution history - workflow_steps (2B rows, partitioned): Individual step records - schedules (100K rows): Scheduled workflow triggers - notifications (1B rows, partitioned): Notification delivery log Partitioning Strategy: - workflow_executions: Monthly partitions by created_at - workflow_steps: Monthly partitions by created_at - notifications: Weekly partitions by created_at - Automatic partition creation via cron job - Retention policy: Drop partitions older than 12 months ``` **Indexing Strategy**: - Primary keys: UUIDs with B-tree indexes - Frequently queried columns: Compound indexes (e.g., user_id + status + created_at) - JSONB columns: GIN indexes for workflow definitions - Full-text search: GiST indexes on description fields **Connection Pooling**: - PgBouncer in transaction mode - Pool size: 100 connections per service - Max client connections: 2000 - Connection timeout: 30 seconds **Performance Characteristics**: - Read query P99: < 20ms - Write query P99: < 50ms - Transaction throughput: 15,000 TPS - Replication lag: < 500ms --- ### Redis Caching Layer **Cluster Configuration**: - Redis Cluster with 6 nodes (3 primary + 3 replica) - Instance type: cache.r6g.xlarge (4 vCPU, 26GB RAM) - Total memory: 78GB usable cache space - Persistence: RDB snapshots every 5 minutes + AOF - Eviction policy: allkeys-lru **Cache Usage Patterns**: 1.",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_12",
              "content": "Resume normal operations - Recovery time: 1-3 hours depending on data volume - Data loss: None if corruption detected quickly ### Testing & Validation **DR Drill Schedule**: - Monthly: Automated failover test (single AZ failure simulation) - Quarterly: Full DR region failover (non-production hours) - Annually: Complete disaster simulation with all stakeholders **Last DR Test Results** (Dec 15, 2025): - Scenario: Full region failover - Actual RTO: 2 hours 23 minutes (target: 4 hours) - Actual RPO: 42 minutes (target: 1 hour) - Issues identified: DNS propagation slower than expected (resolved) - Success criteria: Met all recovery objectives ### Business Continuity **Communication Plan**: - Status page: status.cloudflow.com (updated every 15 minutes during incident) - Customer notifications: Email + SMS for all P1 incidents - Internal escalation: PagerDuty \u2192 Incident Commander \u2192 Engineering Manager \u2192 CTO **Data Retention Policy**: - Active data: 12 months in hot storage (PostgreSQL) - Archived data: 7 years in cold storage (S3 Glacier) - Audit logs: 7 years (compliance requirement) - Backup retention: 30 days standard, 90 days monthly snapshots --- ## Appendix ### Service Dependency Matrix | Service | Depends On | Critical Path | |---------|-----------|---------------| | API Gateway | Auth Service, Redis | Yes | | Auth Service | PostgreSQL, Redis, Vault | Yes | | Workflow Engine | PostgreSQL, Kafka, Redis | Yes | | Scheduler | PostgreSQL, Redis, Kafka | No | | Notification Service | Kafka, PostgreSQL, SendGrid, Twilio | No | ### Capacity Planning **Current Capacity** (Jan 2026): - Daily workflow executions: 2.5M - Active users: 150,000 - API requests per day: 50M - Database size: 1.2TB - Kafka throughput: 100K msg/sec peak **6-Month Projection** (Jul 2026): - Daily workflow executions: 4M (+60%) - Active users: 225,000 (+50%) - API requests per day: 80M (+60%) - Database size: 2TB (+67%) - Required scaling: +30% compute, +50% storage ### Contact Information - **Architecture Team**: architecture@cloudflow.internal - **On-Call Engineer**: PagerDuty escalation - **Security Team**: security@cloudflow.internal - **Documentation**: https://wiki.cloudflow.internal/architecture --- **Document Revision History**: - v2.3.1 (Jan 15, 2026): Updated performance metrics, added DR test results - v2.3.0 (Dec 1, 2025): Added multi-AZ deployment details - v2.2.0 (Oct 15, 2025): Security architecture overhaul - v2.1.0 (Sep 1, 2025): Initial Kafka migration documentation - v2.0.0 (Jul 1, 2025): Complete microservices rewrite",
              "score": null
            },
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_7",
              "content": "podSelector: matchLabels: app: cloudflow policyTypes: - Ingress - Egress ingress: - from: - namespaceSelector: matchLabels: name: cloudflow-prod - podSelector: matchLabels: app: nginx-ingress ports: - protocol: TCP port: 3000 egress: - to: - podSelector: matchLabels: app: postgresql ports: - protocol: TCP port: 5432 - to: - podSelector: matchLabels: app: redis ports: - protocol: TCP port: 6379 ``` ### Pod Security Standards Enforce Pod Security Standards at the namespace level: ```bash kubectl label namespace cloudflow-prod \\ pod-security.kubernetes.io/enforce=restricted \\ pod-security.kubernetes.io/audit=restricted \\ pod-security.kubernetes.io/warn=restricted ``` ### Secrets Management Use AWS Secrets Manager for sensitive credentials: ```bash # Install External Secrets Operator helm install external-secrets external-secrets/external-secrets \\ --namespace external-secrets-system \\ --create-namespace # Create SecretStore kubectl apply -f - <<EOF apiVersion: external-secrets.io/v1beta1 kind: SecretStore metadata: name: aws-secrets-manager namespace: cloudflow-prod spec: provider: aws: service: SecretsManager region: us-east-1 auth: jwt: serviceAccountRef: name: external-secrets-sa EOF ``` --- ## Troubleshooting ### Common Issues #### Pods in CrashLoopBackOff **Symptoms**: Pods continuously restart **Diagnosis**: ```bash kubectl logs -n cloudflow-prod <pod-name> --previous kubectl describe pod -n cloudflow-prod <pod-name> ``` **Common Causes**: - Database connection failure - Invalid environment variables - Insufficient resources #### High Memory Usage **Symptoms**: Pods being OOMKilled **Diagnosis**: ```bash kubectl top pods -n cloudflow-prod ``` **Resolution**: - Increase memory limits in deployment - Check for memory leaks in application logs - Review heap dump for Node.js processes #### Database Connection Pool Exhausted **Symptoms**: \"Too many connections\" errors **Diagnosis**: ```bash # Check PgBouncer stats kubectl exec -it pgbouncer-xxx -n cloudflow-prod -- \\ psql -p 5432 -U pgbouncer pgbouncer -c \"SHOW POOLS;\" ``` **Resolution**: - Increase `max_db_connections` in PgBouncer - Optimize application connection usage - Add more PostgreSQL replicas ### Support Contacts - **On-call Engineer**: pagerduty.com/cloudflow-oncall - **Slack Channel**: #cloudflow-operations - **Documentation**: https://docs.cloudflow.io - **Status Page**: https://status.cloudflow.io --- ## Appendix ### Useful Commands ```bash # View all resources in namespace kubectl get all -n cloudflow-prod # Check resource usage kubectl top pods -n cloudflow-prod kubectl top nodes # View logs kubectl logs -f deployment/cloudflow-api -n cloudflow-prod # Execute command in pod kubectl exec -it <pod-name> -n cloudflow-prod -- /bin/sh # Port forward for local testing kubectl port-forward svc/cloudflow-api 8080:80 -n cloudflow-prod # Rollback deployment helm rollback cloudflow -n cloudflow-prod # Update deployment helm upgrade cloudflow cloudflow/cloudflow \\ --namespace cloudflow-prod \\ --values values-production.yaml ``` ### Maintenance Windows Scheduled maintenance occurs during the following windows: - **Primary**: Sunday 02:00-06:00",
              "score": null
            }
          ],
          "latency_ms": 793.2933880001656,
          "query_id": "realistic_017",
          "dimension": "casual",
          "query": "pgbouncer purpose"
        },
        {
          "key_facts": [
            "PgBouncer",
            "connection pooling",
            "max_db_connections = 100",
            "default_pool_size = 25",
            "pool_mode = transaction"
          ],
          "found_facts": [
            "PgBouncer",
            "connection pooling"
          ],
          "missed_facts": [
            "max_db_connections = 100",
            "default_pool_size = 25",
            "pool_mode = transaction"
          ],
          "coverage": 0.4,
          "expected_docs": [
            "deployment_guide",
            "architecture_overview"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_6",
              "content": "**Verify Service Health**: ```bash kubectl get pods -n cloudflow-prod curl https://api.cloudflow.io/health ``` **Recovery Time Objective (RTO)**: 4 hours **Recovery Point Objective (RPO)**: 24 hours --- ## Scaling and Performance ### Horizontal Pod Autoscaling CloudFlow is configured with Horizontal Pod Autoscaler (HPA) to automatically scale based on resource utilization: ```yaml # hpa.yaml apiVersion: autoscaling/v2 kind: HorizontalPodAutoscaler metadata: name: cloudflow-api-hpa namespace: cloudflow-prod spec: scaleTargetRef: apiVersion: apps/v1 kind: Deployment name: cloudflow-api minReplicas: 3 maxReplicas: 10 metrics: - type: Resource resource: name: cpu target: type: Utilization averageUtilization: 70 - type: Resource resource: name: memory target: type: Utilization averageUtilization: 80 behavior: scaleDown: stabilizationWindowSeconds: 300 policies: - type: Percent value: 50 periodSeconds: 60 scaleUp: stabilizationWindowSeconds: 60 policies: - type: Percent value: 100 periodSeconds: 30 - type: Pods value: 2 periodSeconds: 30 selectPolicy: Max ``` Monitor autoscaling events: ```bash kubectl get hpa -n cloudflow-prod -w kubectl describe hpa cloudflow-api-hpa -n cloudflow-prod ``` ### Performance Optimization #### Connection Pooling Ensure optimal database connection pooling settings in PgBouncer: - **Pool Mode**: Transaction (optimal for microservices) - **Default Pool Size**: 25 connections per user - **Max DB Connections**: 100 (matches PostgreSQL `max_connections`) #### Redis Caching Implement Redis caching for frequently accessed data: ```javascript // Example caching strategy const CACHE_TTL = { workflows: 300, // 5 minutes userSessions: 3600, // 1 hour apiResults: 60 // 1 minute }; ``` #### Load Testing Perform regular load testing to validate scaling configuration: ```bash # Install k6 for load testing kubectl apply -f - <<EOF apiVersion: v1 kind: ConfigMap metadata: name: loadtest-script namespace: cloudflow-prod data: script.js: | import http from 'k6/http'; import { check, sleep } from 'k6'; export let options = { stages: [ { duration: '2m', target: 100 }, { duration: '5m', target: 100 }, { duration: '2m', target: 200 }, { duration: '5m', target: 200 }, { duration: '2m', target: 0 }, ], thresholds: { http_req_duration: ['p(95)<500'], }, }; export default function () { let res = http.get('https://api.cloudflow.io/api/workflows'); check(res, { 'status is 200': (r) => r.status === 200 }); sleep(1); } EOF ``` **Performance Targets**: - P95 API latency: < 500ms - P99 API latency: < 1000ms - Error rate: < 0.1% - Throughput: > 1000 requests/second --- ## Security and Compliance ### Network Policies Implement network policies to restrict pod-to-pod communication: ```yaml # network-policy.yaml apiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: cloudflow-network-policy namespace: cloudflow-prod spec:",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_12",
              "content": "Resume normal operations - Recovery time: 1-3 hours depending on data volume - Data loss: None if corruption detected quickly ### Testing & Validation **DR Drill Schedule**: - Monthly: Automated failover test (single AZ failure simulation) - Quarterly: Full DR region failover (non-production hours) - Annually: Complete disaster simulation with all stakeholders **Last DR Test Results** (Dec 15, 2025): - Scenario: Full region failover - Actual RTO: 2 hours 23 minutes (target: 4 hours) - Actual RPO: 42 minutes (target: 1 hour) - Issues identified: DNS propagation slower than expected (resolved) - Success criteria: Met all recovery objectives ### Business Continuity **Communication Plan**: - Status page: status.cloudflow.com (updated every 15 minutes during incident) - Customer notifications: Email + SMS for all P1 incidents - Internal escalation: PagerDuty \u2192 Incident Commander \u2192 Engineering Manager \u2192 CTO **Data Retention Policy**: - Active data: 12 months in hot storage (PostgreSQL) - Archived data: 7 years in cold storage (S3 Glacier) - Audit logs: 7 years (compliance requirement) - Backup retention: 30 days standard, 90 days monthly snapshots --- ## Appendix ### Service Dependency Matrix | Service | Depends On | Critical Path | |---------|-----------|---------------| | API Gateway | Auth Service, Redis | Yes | | Auth Service | PostgreSQL, Redis, Vault | Yes | | Workflow Engine | PostgreSQL, Kafka, Redis | Yes | | Scheduler | PostgreSQL, Redis, Kafka | No | | Notification Service | Kafka, PostgreSQL, SendGrid, Twilio | No | ### Capacity Planning **Current Capacity** (Jan 2026): - Daily workflow executions: 2.5M - Active users: 150,000 - API requests per day: 50M - Database size: 1.2TB - Kafka throughput: 100K msg/sec peak **6-Month Projection** (Jul 2026): - Daily workflow executions: 4M (+60%) - Active users: 225,000 (+50%) - API requests per day: 80M (+60%) - Database size: 2TB (+67%) - Required scaling: +30% compute, +50% storage ### Contact Information - **Architecture Team**: architecture@cloudflow.internal - **On-Call Engineer**: PagerDuty escalation - **Security Team**: security@cloudflow.internal - **Documentation**: https://wiki.cloudflow.internal/architecture --- **Document Revision History**: - v2.3.1 (Jan 15, 2026): Updated performance metrics, added DR test results - v2.3.0 (Dec 1, 2025): Added multi-AZ deployment details - v2.2.0 (Oct 15, 2025): Security architecture overhaul - v2.1.0 (Sep 1, 2025): Initial Kafka migration documentation - v2.0.0 (Jul 1, 2025): Complete microservices rewrite",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_5",
              "content": "Success/failure events published back to Kafka ``` **Event Schema**: ```json { \"event_id\": \"uuid-v4\", \"event_type\": \"workflow.execution.completed\", \"timestamp\": \"2026-01-15T10:30:00.000Z\", \"correlation_id\": \"request-trace-id\", \"payload\": { \"workflow_id\": \"wf-12345\", \"execution_id\": \"exec-67890\", \"status\": \"COMPLETED\", \"duration_ms\": 4230 }, \"metadata\": { \"source_service\": \"workflow-engine\", \"schema_version\": \"1.0\" } } ``` ### Inter-Service Communication Services communicate using two primary patterns: **Synchronous (gRPC)**: - Auth Service validation calls from API Gateway - User Service profile lookups - Low latency requirement (< 50ms) - Request-response pattern with circuit breaker **Asynchronous (Kafka)**: - Workflow execution events - Notification triggers - Audit log events - Eventual consistency acceptable --- ## Database Architecture ### PostgreSQL Primary Database **Cluster Configuration**: - Primary-replica setup with 1 primary + 2 read replicas - Instance type: db.r6g.2xlarge (8 vCPU, 64GB RAM) - Storage: 2TB gp3 SSD with 12,000 IOPS - Multi-AZ deployment for high availability - Automated backups: Daily snapshots, 30-day retention - Point-in-time recovery: 5-minute granularity **Database Schema Design**: ``` Core Tables: - users (5M rows): User accounts and profiles - workflows (2M rows): Workflow definitions - workflow_executions (500M rows, partitioned): Execution history - workflow_steps (2B rows, partitioned): Individual step records - schedules (100K rows): Scheduled workflow triggers - notifications (1B rows, partitioned): Notification delivery log Partitioning Strategy: - workflow_executions: Monthly partitions by created_at - workflow_steps: Monthly partitions by created_at - notifications: Weekly partitions by created_at - Automatic partition creation via cron job - Retention policy: Drop partitions older than 12 months ``` **Indexing Strategy**: - Primary keys: UUIDs with B-tree indexes - Frequently queried columns: Compound indexes (e.g., user_id + status + created_at) - JSONB columns: GIN indexes for workflow definitions - Full-text search: GiST indexes on description fields **Connection Pooling**: - PgBouncer in transaction mode - Pool size: 100 connections per service - Max client connections: 2000 - Connection timeout: 30 seconds **Performance Characteristics**: - Read query P99: < 20ms - Write query P99: < 50ms - Transaction throughput: 15,000 TPS - Replication lag: < 500ms --- ### Redis Caching Layer **Cluster Configuration**: - Redis Cluster with 6 nodes (3 primary + 3 replica) - Instance type: cache.r6g.xlarge (4 vCPU, 26GB RAM) - Total memory: 78GB usable cache space - Persistence: RDB snapshots every 5 minutes + AOF - Eviction policy: allkeys-lru **Cache Usage Patterns**: 1.",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_0",
              "content": "# CloudFlow Platform - System Architecture Overview **Document Version:** 2.3.1 **Last Updated:** January 15, 2026 **Owner:** Platform Architecture Team **Status:** Production ## Executive Summary CloudFlow is a distributed, cloud-native workflow automation platform designed to orchestrate complex business processes at scale. The platform processes over 2.5 million workflow executions daily with an average P99 latency of 180ms for API operations and 4.2 seconds for workflow execution. This document provides a comprehensive overview of the system architecture, including microservices design, data flow patterns, infrastructure decisions, and operational considerations. ## Table of Contents 1. [High-Level Architecture](#high-level-architecture) 2. [Microservices Breakdown](#microservices-breakdown) 3. [Data Flow Architecture](#data-flow-architecture) 4. [Database Architecture](#database-architecture) 5. [Message Queue Patterns](#message-queue-patterns) 6. [Caching Strategy](#caching-strategy) 7. [Security Architecture](#security-architecture) 8. [Performance Characteristics](#performance-characteristics) 9. [Disaster Recovery](#disaster-recovery) --- ## High-Level Architecture CloudFlow follows a microservices architecture pattern deployed across multiple availability zones in AWS. The platform is designed with the following core principles: - **Scalability**: Horizontal scaling for all services with auto-scaling groups - **Resilience**: Circuit breakers, bulkheads, and graceful degradation - **Observability**: Distributed tracing, centralized logging, and comprehensive metrics - **Security**: Zero-trust network architecture with mTLS encryption ### Architecture Diagram (Conceptual) ``` \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 Load Balancer (ALB) \u2502 \u2502 (TLS Termination - 443) \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u25bc \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 API Gateway Layer \u2502 \u2502 (Rate Limiting, Auth, Request Routing) \u2502 \u2514\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u2502 \u2502 \u2502 \u2502 \u25bc \u25bc \u25bc \u25bc \u25bc \u250c\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502Auth\u2502 \u2502Workflow\u2502 \u2502Scheduler\u2502 \u2502Notification\u2502 \u2502User Service\u2502 \u2502Svc \u2502 \u2502 Engine \u2502 \u2502 Service \u2502 \u2502 Service \u2502 \u2502 \u2502 \u2514\u2500\u252c\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u2502 \u2502 \u2502 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 \u2502 \u2502 \u25bc \u25bc \u25bc \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502PostgreSQL\u2502 \u2502 Redis \u2502 \u2502 Kafka \u2502 \u2502 Cluster \u2502 \u2502 Cluster \u2502 \u2502 Cluster \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 ``` ### Technology Stack - **Runtime**: Node.js 20.x (API Gateway, Workflow Engine), Go 1.21 (Auth Service, Scheduler) - **Container Orchestration**: Kubernetes 1.28 (EKS) - **Service Mesh**: Istio 1.20 for service-to-service communication - **Databases**: PostgreSQL 15.4, Redis 7.2 - **Message Broker**: Apache Kafka 3.6 - **Monitoring**: Prometheus, Grafana, Jaeger for distributed tracing - **Secrets Management**: HashiCorp Vault 1.15 --- ## Microservices Breakdown ### API Gateway **Purpose**: Single entry point for all client requests, providing authentication, rate limiting, request routing, and protocol translation.",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_4",
              "content": "**Implement connection pooling optimization:** ```bash # Use PgBouncer for connection pooling kubectl apply -f cloudflow-pgbouncer.yaml # Configure CloudFlow to use PgBouncer cloudflow config set db.host pgbouncer.cloudflow.svc.cluster.local cloudflow config set db.port 6432 cloudflow config set db.pool.mode transaction ``` 2. **Add read replicas:** ```bash # Route read-only queries to replicas cloudflow db replicas add --count 2 cloudflow config set db.read_replicas \"replica-1:5432,replica-2:5432\" cloudflow config set db.read_write_split true ``` ### Connection Timeout Errors #### Error Messages - `connection timeout after 30000ms` - `could not connect to database server at 10.0.2.45:5432` - `database server unreachable` #### Troubleshooting Steps ```bash # Test network connectivity telnet cloudflow-db.internal.company.com 5432 # Check DNS resolution nslookup cloudflow-db.internal.company.com # Verify database is accepting connections pg_isready -h cloudflow-db.internal.company.com -p 5432 -U cloudflow # Check firewall rules sudo iptables -L -n | grep 5432 # Test from CloudFlow pod network kubectl run -n cloudflow debug-pod --rm -i --tty \\ --image=postgres:14 -- \\ psql -h cloudflow-db.internal.company.com -U cloudflow -d cloudflow_production # Review database logs for connection rejections kubectl logs -n cloudflow statefulset/cloudflow-db --tail=100 | \\ grep -i \"connection\\|reject\\|authentication\" ``` ### Maximum Connection Limit (100) Reached This is a hard limit in CloudFlow's database tier. #### Permanent Solutions **Option 1: Upgrade database tier** ```bash # Check available tiers cloudflow db tiers list # Upgrade to higher tier (supports 200 connections) cloudflow db upgrade --tier standard-plus --confirm # Monitor migration progress cloudflow db migration status ``` **Option 2: Implement aggressive connection reuse** ```bash # Reduce connection lifetime cloudflow config set db.pool.max_lifetime 600 # 10 minutes # Enable prepared statement caching cloudflow config set db.prepared_statements.cache true cloudflow config set db.prepared_statements.max_size 250 # Reduce idle connection timeout cloudflow config set db.pool.idle_timeout 120 # 2 minutes ``` --- ## Workflow Execution Failures ### Timeout Errors (3600 second limit) #### Error Message ``` WorkflowExecutionError: Workflow exceeded maximum execution time of 3600 seconds Status: TIMEOUT Workflow ID: wf_9k2n4m8p1q Execution ID: exec_7h3j6k9m2n ``` #### Analysis ```bash # Get workflow execution details cloudflow workflows executions get exec_7h3j6k9m2n --verbose # Check step-by-step breakdown cloudflow workflows executions steps exec_7h3j6k9m2n # Sample output: # Step 1: data_ingestion - Duration: 245s - Status: SUCCESS # Step 2: data_validation - Duration: 123s - Status: SUCCESS # Step 3: data_transformation - Duration: 3189s - Status: TIMEOUT # Step 4: data_export - Duration: 0s - Status: SKIPPED # Identify bottleneck step cloudflow workflows analyze",
              "score": null
            }
          ],
          "latency_ms": 886.1087810000754,
          "query_id": "realistic_017",
          "dimension": "contextual",
          "query": "Optimizing database connections, what role does PgBouncer play?"
        },
        {
          "key_facts": [
            "PgBouncer",
            "connection pooling",
            "max_db_connections = 100",
            "default_pool_size = 25",
            "pool_mode = transaction"
          ],
          "found_facts": [
            "PgBouncer"
          ],
          "missed_facts": [
            "connection pooling",
            "max_db_connections = 100",
            "default_pool_size = 25",
            "pool_mode = transaction"
          ],
          "coverage": 0.2,
          "expected_docs": [
            "deployment_guide",
            "architecture_overview"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_8",
              "content": "### Cache Invalidation Strategies **Time-Based Expiration (TTL)**: - Short-lived data: 5-15 minutes (session tokens, rate limit counters) - Medium-lived data: 1 hour (workflow definitions, templates) - Long-lived data: 24 hours (static configuration) **Event-Based Invalidation**: ``` Database Update Event \u2192 Kafka (cache.invalidation topic) \u2502 \u25bc All service instances consume event \u2502 \u25bc Redis DEL for affected keys ``` **Pattern-Based Invalidation**: - Use Redis SCAN + DEL for wildcard patterns - Example: Invalidate all user caches: `user:*:{user_id}` **Cache Warming**: - Scheduled job runs every hour to pre-populate frequently accessed data - Targets: Top 1000 workflows, system configuration, active user profiles - Reduces cache miss rate during traffic spikes ### Multi-Level Caching Application implements L1 (in-memory) and L2 (Redis) caching: ``` Request \u2192 L1 Cache (Node.js Map, 1000 entry LRU) \u2502 MISS \u2502 \u25bc L2 Cache (Redis, distributed) \u2502 MISS \u2502 \u25bc Database (PostgreSQL) ``` L1 cache reduces Redis network calls by 40% for hot data. --- ## Security Architecture ### Network Security **Zero-Trust Network Model**: - All service-to-service communication encrypted with mTLS - Certificate rotation: Every 90 days (automated via cert-manager) - Certificate authority: Internal PKI with HashiCorp Vault - Network segmentation: Private subnets for services, public subnet for ALB only **Service Mesh (Istio)**: ``` Service A \u2192 Envoy Sidecar (mTLS client cert) \u2502 Encrypted channel \u2502 Envoy Sidecar (mTLS server cert) \u2192 Service B ``` **Firewall Rules**: - Security groups: Deny all by default, explicit allow rules - Ingress: Only ALB can reach API Gateway (port 8080) - Egress: Services can only reach specific dependencies - Database access: Limited to application subnets only ### Authentication & Authorization **JWT Token Validation**: - Algorithm: RS256 (asymmetric signing) - Key rotation: Every 30 days with 7-day overlap period - Public key distribution: JWKS endpoint cached in Redis - Validation: Signature, expiry, issuer, audience claims - Token revocation: Blacklist in Redis for compromised tokens **Permission Model**: ``` User \u2192 Roles \u2192 Permissions \u2198 \u2197 Tenants (Multi-tenancy isolation) ``` Example permissions: - `workflow:read` - View workflows - `workflow:write` - Create/update workflows - `workflow:execute` - Trigger workflow execution - `workflow:delete` - Delete workflows - `admin:*` - All administrative operations **API Key Management**: - Format: `cfk_live_<32-char-random>` (production), `cfk_test_<32-char-random>` (sandbox) - Hashing: SHA-256 before storage in PostgreSQL - Scoping: API keys can be scoped to specific workflows or operations - Rate limits: Configurable",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_1",
              "content": "**Technology**: Node.js with Express.js framework **Replicas**: 12 pods (production), auto-scaling 8-20 based on CPU **Resource Allocation**: 2 vCPU, 4GB RAM per pod **Key Responsibilities**: - JWT token validation (delegated to Auth Service for initial validation) - Rate limiting: 1000 requests per minute per API key (sliding window) - Request/response transformation and validation using JSON Schema - Routing to downstream services based on URL path patterns - CORS handling for web clients - Request/response logging and correlation ID injection **Critical Endpoints**: - `POST /api/v1/workflows` - Create new workflow - `GET /api/v1/workflows/:id` - Retrieve workflow status - `POST /api/v1/workflows/:id/execute` - Trigger workflow execution - `GET /api/v1/workflows/:id/history` - Get execution history **Dependencies**: - Auth Service (for token validation) - Redis (for rate limiting counters) - All downstream microservices **Performance Targets**: - P50 latency: < 50ms - P99 latency: < 200ms - Throughput: 10,000 RPS sustained --- ### Auth Service **Purpose**: Centralized authentication and authorization service handling user identity, token generation, and permission validation. **Technology**: Go with gRPC for internal communication, REST for external **Replicas**: 8 pods (production), auto-scaling 6-12 **Resource Allocation**: 1 vCPU, 2GB RAM per pod **Key Responsibilities**: - User authentication via multiple providers (OAuth2, SAML, local credentials) - JWT token generation and validation (RS256 algorithm) - Role-based access control (RBAC) with fine-grained permissions - Session management with Redis-backed storage - API key generation and validation for service accounts - MFA enforcement for administrative operations **Authentication Flow**: ``` Client Request \u2192 API Gateway \u2192 Auth Service \u2502 \u251c\u2500 Validate credentials \u251c\u2500 Check MFA if required \u251c\u2500 Generate JWT (15min expiry) \u251c\u2500 Generate refresh token (7 days) \u2514\u2500 Store session in Redis ``` **Token Structure**: - Access Token: JWT with 15-minute expiry - Refresh Token: Opaque token with 7-day expiry, stored in PostgreSQL - Claims: user_id, email, roles[], permissions[], tenant_id **Security Features**: - Password hashing: Argon2id with 64MB memory, 4 iterations - Token rotation on refresh to prevent replay attacks - Brute force protection: 5 failed attempts \u2192 15-minute lockout - Secrets stored in HashiCorp Vault, rotated every 90 days **Performance Targets**: - Token validation: < 10ms (P99) - Token generation: < 50ms (P99) - Throughput: 5,000 RPS for validation operations --- ### Workflow Engine **Purpose**: Core orchestration service that executes workflow definitions, manages state transitions, and coordinates task execution across distributed systems.",
              "score": null
            },
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_0",
              "content": "# CloudFlow API Reference Version 2.1.0 | Last Updated: January 2026 ## Overview The CloudFlow API is a RESTful service that enables developers to programmatically manage cloud workflows, data pipelines, and automation tasks. This documentation provides comprehensive details on authentication, endpoints, request/response formats, error handling, and best practices. **Base URL:** `https://api.cloudflow.io/v2` **API Status:** https://status.cloudflow.io ## Authentication CloudFlow supports three authentication methods to suit different use cases and security requirements. ### API Keys API keys provide simple authentication for server-to-server communication. Include your API key in the request header: ```bash curl -H \"X-API-Key: cf_live_a1b2c3d4e5f6g7h8i9j0\" \\ https://api.cloudflow.io/v2/workflows ``` **Security Notes:** - Never expose API keys in client-side code - Rotate keys every 90 days - Use separate keys for development and production environments ### OAuth 2.0 OAuth 2.0 is recommended for applications that access CloudFlow on behalf of users. We support the Authorization Code flow with PKCE. **Authorization Endpoint:** `https://auth.cloudflow.io/oauth/authorize` **Token Endpoint:** `https://auth.cloudflow.io/oauth/token` **Supported Scopes:** - `workflows:read` - Read workflow configurations - `workflows:write` - Create and modify workflows - `pipelines:read` - Read pipeline data - `pipelines:write` - Create and manage pipelines - `analytics:read` - Access analytics and metrics - `admin:full` - Full administrative access Example authorization request: ```bash curl -X POST https://auth.cloudflow.io/oauth/token \\ -H \"Content-Type: application/x-www-form-urlencoded\" \\ -d \"grant_type=authorization_code\" \\ -d \"code=AUTH_CODE_HERE\" \\ -d \"client_id=YOUR_CLIENT_ID\" \\ -d \"client_secret=YOUR_CLIENT_SECRET\" \\ -d \"redirect_uri=https://yourapp.com/callback\" ``` ### JWT Tokens For advanced use cases, CloudFlow supports JSON Web Tokens (JWT) with RS256 signing algorithm. JWTs must include the following claims: - `iss` (issuer): Your application identifier - `sub` (subject): User or service account ID - `aud` (audience): `https://api.cloudflow.io` - `exp` (expiration): Unix timestamp (max 3600 seconds from `iat`) - `iat` (issued at): Unix timestamp - `scope`: Space-separated list of requested scopes Example JWT header: ```python import jwt import time from cryptography.hazmat.primitives import serialization from cryptography.hazmat.backends import default_backend # Load your private key with open('private_key.pem', 'rb') as key_file: private_key = serialization.load_pem_private_key( key_file.read(), password=None, backend=default_backend() ) # Create JWT payload payload = { 'iss': 'your-app-id', 'sub': 'user-12345', 'aud': 'https://api.cloudflow.io', 'exp': int(time.time()) + 3600, 'iat': int(time.time()), 'scope': 'workflows:read workflows:write' } # Generate token token = jwt.encode(payload, private_key, algorithm='RS256') # Use in API request headers = {'Authorization': f'Bearer {token}'} ``` **Token Expiration:** All tokens expire after 3600 seconds (1 hour). Implement token refresh logic in your application.",
              "score": null
            },
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_1",
              "content": "create cluster -f cluster-config.yaml ``` ### Storage Configuration Install the EBS CSI driver for persistent volumes: ```bash eksctl create addon --name aws-ebs-csi-driver \\ --cluster cloudflow-production \\ --service-account-role-arn arn:aws:iam::ACCOUNT_ID:role/AmazonEKS_EBS_CSI_DriverRole \\ --force ``` --- ## Kubernetes Deployment ### Namespace Setup Create the CloudFlow namespace and configure resource quotas: ```yaml # namespace.yaml apiVersion: v1 kind: Namespace metadata: name: cloudflow-prod labels: name: cloudflow-prod environment: production --- apiVersion: v1 kind: ResourceQuota metadata: name: cloudflow-quota namespace: cloudflow-prod spec: hard: requests.cpu: \"50\" requests.memory: 100Gi persistentvolumeclaims: \"10\" services.loadbalancers: \"2\" ``` Apply the namespace configuration: ```bash kubectl apply -f namespace.yaml ``` ### Environment Configuration CloudFlow requires the following environment variables: | Variable | Description | Example | Required | |----------|-------------|---------|----------| | `DATABASE_URL` | PostgreSQL connection string | `postgresql://user:pass@host:5432/cloudflow` | Yes | | `REDIS_URL` | Redis connection string | `redis://redis-master.cloudflow-prod.svc.cluster.local:6379` | Yes | | `JWT_SECRET` | Secret key for JWT token signing | `<generated-secret-256-bit>` | Yes | | `LOG_LEVEL` | Application log verbosity | `info`, `debug`, `warn`, `error` | No (default: `info`) | | `API_PORT` | API server port | `3000` | No (default: `3000`) | | `WORKER_CONCURRENCY` | Number of concurrent workers | `10` | No (default: `5`) | | `SESSION_SECRET` | Session encryption key | `<generated-secret-256-bit>` | Yes | | `AWS_REGION` | AWS region for services | `us-east-1` | Yes | | `S3_BUCKET` | S3 bucket for file storage | `cloudflow-prod-storage` | Yes | | `SMTP_HOST` | Email server hostname | `smtp.sendgrid.net` | No | | `SMTP_PORT` | Email server port | `587` | No | | `METRICS_ENABLED` | Enable Prometheus metrics | `true` | No (default: `false`) | Create a Kubernetes secret for sensitive variables: ```bash kubectl create secret generic cloudflow-secrets \\ --namespace cloudflow-prod \\ --from-literal=DATABASE_URL=\"postgresql://cloudflow:$(cat db-password.txt)@pgbouncer.cloudflow-prod.svc.cluster.local:5432/cloudflow\" \\ --from-literal=REDIS_URL=\"redis://redis-master.cloudflow-prod.svc.cluster.local:6379\" \\ --from-literal=JWT_SECRET=\"$(openssl rand -base64 32)\" \\ --from-literal=SESSION_SECRET=\"$(openssl rand -base64 32)\" ``` ### Helm Chart Deployment Add the CloudFlow Helm repository: ```bash helm repo add cloudflow https://charts.cloudflow.io helm repo update ``` Create a values file for production configuration: ```yaml # values-production.yaml replicaCount: 3 image: repository: 123456789012.dkr.ecr.us-east-1.amazonaws.com/cloudflow tag: \"2.4.0\" pullPolicy: IfNotPresent resources: limits: cpu: 2000m memory: 4Gi requests: cpu: 2000m memory: 4Gi autoscaling: enabled: true minReplicas: 3 maxReplicas: 10 targetCPUUtilizationPercentage: 70 targetMemoryUtilizationPercentage: 80 service: type: ClusterIP port: 80 targetPort: 3000 annotations: service.beta.kubernetes.io/aws-load-balancer-type: \"nlb\" ingress: enabled: true className: nginx annotations: cert-manager.io/cluster-issuer: \"letsencrypt-prod\" nginx.ingress.kubernetes.io/ssl-redirect: \"true\" hosts: - host: api.cloudflow.io paths: - path: / pathType: Prefix tls:",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_0",
              "content": "# CloudFlow Platform - System Architecture Overview **Document Version:** 2.3.1 **Last Updated:** January 15, 2026 **Owner:** Platform Architecture Team **Status:** Production ## Executive Summary CloudFlow is a distributed, cloud-native workflow automation platform designed to orchestrate complex business processes at scale. The platform processes over 2.5 million workflow executions daily with an average P99 latency of 180ms for API operations and 4.2 seconds for workflow execution. This document provides a comprehensive overview of the system architecture, including microservices design, data flow patterns, infrastructure decisions, and operational considerations. ## Table of Contents 1. [High-Level Architecture](#high-level-architecture) 2. [Microservices Breakdown](#microservices-breakdown) 3. [Data Flow Architecture](#data-flow-architecture) 4. [Database Architecture](#database-architecture) 5. [Message Queue Patterns](#message-queue-patterns) 6. [Caching Strategy](#caching-strategy) 7. [Security Architecture](#security-architecture) 8. [Performance Characteristics](#performance-characteristics) 9. [Disaster Recovery](#disaster-recovery) --- ## High-Level Architecture CloudFlow follows a microservices architecture pattern deployed across multiple availability zones in AWS. The platform is designed with the following core principles: - **Scalability**: Horizontal scaling for all services with auto-scaling groups - **Resilience**: Circuit breakers, bulkheads, and graceful degradation - **Observability**: Distributed tracing, centralized logging, and comprehensive metrics - **Security**: Zero-trust network architecture with mTLS encryption ### Architecture Diagram (Conceptual) ``` \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 Load Balancer (ALB) \u2502 \u2502 (TLS Termination - 443) \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u25bc \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 API Gateway Layer \u2502 \u2502 (Rate Limiting, Auth, Request Routing) \u2502 \u2514\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u2502 \u2502 \u2502 \u2502 \u25bc \u25bc \u25bc \u25bc \u25bc \u250c\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502Auth\u2502 \u2502Workflow\u2502 \u2502Scheduler\u2502 \u2502Notification\u2502 \u2502User Service\u2502 \u2502Svc \u2502 \u2502 Engine \u2502 \u2502 Service \u2502 \u2502 Service \u2502 \u2502 \u2502 \u2514\u2500\u252c\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u2502 \u2502 \u2502 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 \u2502 \u2502 \u25bc \u25bc \u25bc \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502PostgreSQL\u2502 \u2502 Redis \u2502 \u2502 Kafka \u2502 \u2502 Cluster \u2502 \u2502 Cluster \u2502 \u2502 Cluster \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 ``` ### Technology Stack - **Runtime**: Node.js 20.x (API Gateway, Workflow Engine), Go 1.21 (Auth Service, Scheduler) - **Container Orchestration**: Kubernetes 1.28 (EKS) - **Service Mesh**: Istio 1.20 for service-to-service communication - **Databases**: PostgreSQL 15.4, Redis 7.2 - **Message Broker**: Apache Kafka 3.6 - **Monitoring**: Prometheus, Grafana, Jaeger for distributed tracing - **Secrets Management**: HashiCorp Vault 1.15 --- ## Microservices Breakdown ### API Gateway **Purpose**: Single entry point for all client requests, providing authentication, rate limiting, request routing, and protocol translation.",
              "score": null
            }
          ],
          "latency_ms": 857.127704002778,
          "query_id": "realistic_017",
          "dimension": "negation",
          "query": "Why can't I connect directly to the database?"
        },
        {
          "key_facts": [
            "max_attempts: 3",
            "backoff_type: exponential",
            "initial_interval: 1000",
            "max retries: 3"
          ],
          "found_facts": [
            "max_attempts: 3",
            "backoff_type: exponential",
            "initial_interval: 1000",
            "max retries: 3"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "troubleshooting_guide",
            "user_guide"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_5",
              "content": "exec_7h3j6k9m2n --show-bottlenecks ``` #### Solutions **1. Increase workflow timeout (if justified):** ```bash # Update workflow configuration cloudflow workflows update wf_9k2n4m8p1q \\ --timeout 7200 \\ --reason \"Large dataset processing requires extended time\" # Verify update cloudflow workflows get wf_9k2n4m8p1q | grep timeout ``` **2. Optimize slow steps:** ```bash # Enable parallel processing cloudflow workflows update wf_9k2n4m8p1q \\ --step data_transformation \\ --parallel-workers 8 \\ --batch-size 1000 # Add checkpointing for long operations cloudflow workflows update wf_9k2n4m8p1q \\ --step data_transformation \\ --enable-checkpointing \\ --checkpoint-interval 300 ``` **3. Split workflow into smaller workflows:** ```bash # Create sub-workflows cloudflow workflows create data-pipeline-part1 \\ --steps \"data_ingestion,data_validation\" \\ --timeout 1800 cloudflow workflows create data-pipeline-part2 \\ --steps \"data_transformation,data_export\" \\ --timeout 3600 \\ --trigger workflow_completed \\ --trigger-workflow data-pipeline-part1 ``` ### Retry Logic and Exponential Backoff CloudFlow implements automatic retry with exponential backoff for transient failures: - Max retries: 3 - Initial delay: 1 second - Backoff multiplier: 2 - Max delay: 60 seconds #### Retry Sequence ``` Attempt 1: Immediate Attempt 2: Wait 1s (2^0 * 1s) Attempt 3: Wait 2s (2^1 * 1s) Attempt 4: Wait 4s (2^2 * 1s) ``` #### Configuration ```bash # View current retry settings cloudflow workflows get wf_9k2n4m8p1q --format json | jq '.retry_policy' # Customize retry behavior cloudflow workflows update wf_9k2n4m8p1q \\ --retry-max-attempts 5 \\ --retry-initial-delay 2000 \\ --retry-backoff-multiplier 2 \\ --retry-max-delay 120000 \\ --retry-on-errors \"NETWORK_ERROR,TIMEOUT,RATE_LIMIT\" # Disable retry for specific step cloudflow workflows update wf_9k2n4m8p1q \\ --step data_export \\ --retry-enabled false ``` #### Monitoring Retries ```bash # List failed executions with retry information cloudflow workflows executions list \\ --status FAILED \\ --show-retries \\ --last 7d # Get retry history for specific execution cloudflow workflows executions retries exec_7h3j6k9m2n # Output: # Execution: exec_7h3j6k9m2n # Attempt 1: FAILED - NetworkError: Connection refused (delay: 0ms) # Attempt 2: FAILED - NetworkError: Connection timeout (delay: 1000ms) # Attempt 3: FAILED - NetworkError: Connection timeout (delay: 2000ms) # Attempt 4: FAILED - NetworkError: Connection timeout (delay: 4000ms) # Final Status: FAILED_AFTER_RETRIES ``` ### Workflow Step Failures #### Common Error Patterns **1.",
              "score": null
            },
            {
              "doc_id": "user_guide",
              "chunk_id": "user_guide_fix_5",
              "content": "Save and activate **Testing Schedules:** Use the built-in schedule calculator to preview upcoming executions: ``` Next 5 executions: 1. 2026-01-24 14:00:00 UTC 2. 2026-01-25 14:00:00 UTC 3. 2026-01-26 14:00:00 UTC 4. 2026-01-27 14:00:00 UTC 5. 2026-01-28 14:00:00 UTC ``` ## Error Handling Robust error handling ensures your workflows are resilient and reliable. ### Retry Policies Configure automatic retries for failed actions: ```yaml - id: api_call action: http_request config: url: \"https://api.example.com/data\" retry: max_attempts: 3 backoff_type: \"exponential\" # or \"fixed\", \"linear\" initial_interval: 1000 # milliseconds max_interval: 30000 multiplier: 2.0 retry_on: - timeout - network_error - status: [500, 502, 503, 504] ``` **Backoff Strategies:** - **Fixed**: Wait the same amount of time between retries - Attempt 1: Wait 1s - Attempt 2: Wait 1s - Attempt 3: Wait 1s - **Linear**: Increase wait time by a fixed amount - Attempt 1: Wait 1s - Attempt 2: Wait 2s - Attempt 3: Wait 3s - **Exponential**: Double the wait time with each retry (recommended) - Attempt 1: Wait 1s - Attempt 2: Wait 2s - Attempt 3: Wait 4s **Retry Conditions:** Control which errors trigger retries: - `timeout`: Request timeout - `network_error`: Connection failures - `status`: Specific HTTP status codes - `error_code`: Application-specific error codes ### Fallback Actions Execute alternative actions when the primary action fails: ```yaml - id: primary_payment action: http_request config: url: \"https://primary-payment-gateway.com/charge\" method: POST body: amount: \"{{amount}}\" on_error: - id: fallback_payment action: http_request config: url: \"https://backup-payment-gateway.com/charge\" method: POST body: amount: \"{{amount}}\" - id: notify_admin action: email config: to: \"admin@company.com\" subject: \"Payment Gateway Failure\" body: \"Primary gateway failed, switched to backup\" ``` **Error Object:** Access error details in fallback actions: ``` {{error.message}} # Error message {{error.code}} # Error code {{error.step_id}} # Failed step ID {{error.timestamp}} # When the error occurred {{error.attempts}} # Number of retry attempts ``` ### Dead Letter Queue When all retries and fallbacks fail, CloudFlow can route failed executions to a Dead Letter Queue (DLQ) for manual review: **Enable DLQ:** ```yaml workflow: error_handling: dead_letter_queue: enabled: true retain_days: 30 ``` **DLQ Features:** - View failed executions in the dashboard - Inspect complete execution context and error details - Retry individual executions after fixing issues - Export failed executions for analysis - Set up alerts for DLQ threshold breaches **Accessing the DLQ:** 1. Navigate to **Workflows** > **[Your Workflow]** > **Dead Letter Queue** 2.",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_6",
              "content": "Data Validation Errors** ``` ValidationError: Field 'customer_id' is required but missing in 234 records ``` **Resolution:** ```bash # Add data quality checks cloudflow workflows update wf_9k2n4m8p1q \\ --step data_ingestion \\ --add-validator required_fields \\ --validator-config '{\"fields\": [\"customer_id\", \"timestamp\", \"amount\"]}' # Configure error handling cloudflow workflows update wf_9k2n4m8p1q \\ --step data_validation \\ --on-error continue \\ --error-threshold 5% # Fail if > 5% of records invalid ``` **2. External API Failures** ``` ExternalAPIError: API request to https://partner-api.example.com failed with status 502 ``` **Resolution:** ```bash # Add circuit breaker cloudflow workflows update wf_9k2n4m8p1q \\ --step external_api_call \\ --circuit-breaker-enabled true \\ --circuit-breaker-threshold 5 \\ --circuit-breaker-timeout 30000 # Configure fallback behavior cloudflow workflows update wf_9k2n4m8p1q \\ --step external_api_call \\ --fallback-action use_cached_data \\ --cache-ttl 3600 ``` --- ## Rate Limiting & Throttling ### 429 Too Many Requests #### Error Response ```json { \"error\": \"rate_limit_exceeded\", \"message\": \"API rate limit exceeded. Retry after 45 seconds.\", \"status\": 429, \"headers\": { \"X-RateLimit-Limit\": \"1000\", \"X-RateLimit-Remaining\": \"0\", \"X-RateLimit-Reset\": \"1706112345\", \"Retry-After\": \"45\" } } ``` #### Rate Limit Tiers CloudFlow enforces the following rate limits per workspace: | Tier | Requests/Minute | Requests/Hour | Concurrent Workflows | |------|-----------------|---------------|----------------------| | Free | 60 | 1,000 | 5 | | Standard | 1,000 | 50,000 | 50 | | Premium | 5,000 | 250,000 | 200 | | Enterprise | Custom | Custom | Unlimited | #### Checking Rate Limit Status ```bash # Check current rate limit status curl -I https://api.cloudflow.io/api/v1/workflows \\ -H \"Authorization: Bearer $CF_ACCESS_TOKEN\" # Extract rate limit headers curl -s -I https://api.cloudflow.io/api/v1/workflows \\ -H \"Authorization: Bearer $CF_ACCESS_TOKEN\" | \\ grep -i \"x-ratelimit\" # Output: # X-RateLimit-Limit: 1000 # X-RateLimit-Remaining: 247 # X-RateLimit-Reset: 1706112400 # Monitor rate limit usage cloudflow metrics query \\ --metric rate_limit_remaining \\ --workspace ws_abc123 \\ --last 1h \\ --interval 1m ``` #### Handling Rate Limits in Code **Python example with retry logic:** ```python import time import requests def cloudflow_api_call_with_retry(url, headers, max_retries=3): for attempt in range(max_retries): response = requests.get(url, headers=headers) if response.status_code == 429: retry_after = int(response.headers.get('Retry-After', 60)) print(f\"Rate limited.",
              "score": null
            },
            {
              "doc_id": "user_guide",
              "chunk_id": "user_guide_fix_6",
              "content": "Filter by error type, date range, or execution ID 3. Click an execution to view full details 4. Click **\"Retry\"** to reprocess with the same input data ### Error Notifications Get notified when workflows fail: ```yaml workflow: notifications: on_failure: - type: email to: \"ops-team@company.com\" - type: slack channel: \"#alerts\" message: \"Workflow {{workflow.name}} failed: {{error.message}}\" on_success_after_retry: - type: slack channel: \"#monitoring\" message: \"Workflow recovered after {{error.attempts}} attempts\" ``` ## Workflow Limits CloudFlow enforces the following limits to ensure platform stability and performance: ### Steps Per Workflow - **Maximum**: 50 steps per workflow - **Recommendation**: Keep workflows focused and modular. If you need more steps, consider splitting into multiple workflows connected via webhooks. ### Execution Timeout - **Default**: 3600 seconds (60 minutes) - **Behavior**: Workflows exceeding this timeout are automatically terminated - **Custom Timeouts**: Enterprise plans can request custom timeout limits **Setting Step-Level Timeouts:** ```yaml - id: long_running_task action: http_request config: url: \"https://api.example.com/process\" timeout: 300 # 5 minutes for this specific step ``` ### Execution Limits - **Maximum**: 1000 executions per day (per workflow) - **Rate Limiting**: 100 concurrent executions per workflow - **Burst Limit**: 10 executions per second **What happens when limits are reached:** - New executions are queued automatically - Webhook triggers return HTTP 429 (Too Many Requests) - Scheduled executions are skipped (logged in audit trail) - Email notifications sent to workflow owner **Monitoring Usage:** View real-time metrics in your workflow dashboard: - Executions today: 847 / 1000 - Average duration: 12.3s - Success rate: 98.2% - Current queue depth: 3 ### Data Limits - **Maximum request/response size**: 10MB per action - **Maximum execution payload**: 50MB total - **Variable value size**: 1MB per variable ### Enterprise Plan Limits Enterprise customers can request increased limits: - Up to 100 steps per workflow - Up to 10,000 executions per day - Up to 7200 second timeout (2 hours) - Priority execution queue - Dedicated capacity allocation Contact sales@cloudflow.io for Enterprise pricing and custom limits. ## Best Practices Follow these best practices to build reliable, maintainable workflows: ### 1. Use Descriptive Names **Good:** - Workflow: \"Sync Customer Data from Salesforce to Database\" - Step: \"validate_customer_email\" **Bad:** - Workflow: \"Workflow 1\" - Step: \"step3\" ### 2.",
              "score": null
            },
            {
              "doc_id": "user_guide",
              "chunk_id": "user_guide_fix_7",
              "content": "Handle Errors Gracefully Always implement error handling for external API calls and database operations: ```yaml - id: fetch_data action: http_request config: url: \"https://api.example.com/data\" retry: max_attempts: 3 backoff_type: exponential on_error: - id: log_error action: database_query config: query: \"INSERT INTO error_log (workflow_id, error) VALUES ($1, $2)\" parameters: - \"{{workflow.id}}\" - \"{{error.message}}\" ``` ### 3. Use Secrets for Sensitive Data Never hardcode API keys, passwords, or tokens in workflows: **Bad:** ```yaml headers: Authorization: \"Bearer sk_live_abc123xyz789\" ``` **Good:** ```yaml headers: Authorization: \"Bearer {{secrets.API_TOKEN}}\" ``` Store secrets in **Settings** > **Secrets** with encryption at rest. ### 4. Validate Input Data Always validate trigger data before processing: ```yaml - id: validate_input action: javascript code: | const required_fields = ['email', 'name', 'order_id']; for (const field of required_fields) { if (!input[field]) { throw new Error(`Missing required field: ${field}`); } } // Validate email format const email_regex = /^[^\\s@]+@[^\\s@]+\\.[^\\s@]+$/; if (!email_regex.test(input.email)) { throw new Error('Invalid email format'); } return { validated: true }; ``` ### 5. Use Idempotency Keys For operations that shouldn't be repeated (payments, record creation), use idempotency keys: ```yaml - id: create_charge action: http_request config: url: \"https://api.stripe.com/v1/charges\" method: POST headers: Idempotency-Key: \"{{workflow.id}}-{{execution.id}}\" body: amount: \"{{amount}}\" ``` ### 6. Monitor and Log Add logging steps for important workflow milestones: ```yaml - id: log_start action: database_query config: query: \"INSERT INTO workflow_audit (execution_id, step, timestamp) VALUES ($1, $2, $3)\" parameters: - \"{{execution.id}}\" - \"workflow_started\" - \"{{now()}}\" ``` ### 7. Keep Workflows Modular Break complex workflows into smaller, reusable components: - Use sub-workflows for repeated logic - Trigger child workflows via webhooks - Share common configurations via templates ### 8. Test Thoroughly Before activating a workflow: 1. Use test mode with sample data 2. Verify all actions execute correctly 3. Test error handling paths 4. Review execution logs 5. Start with a limited scope (e.g., test channel, small dataset) ### 9. Document Your Workflows Add descriptions to workflows and steps: ```yaml workflow: name: \"Daily Sales Report\" description: | Generates a daily sales report and distributes it to the sales team. Runs at 8:00 AM EST Monday-Friday. Data source: PostgreSQL sales database Recipients: sales-team@company.com steps: - id: fetch_sales name: \"Fetch Yesterday's Sales\" description: \"Query database for all completed orders from previous day\" ``` ### 10.",
              "score": null
            }
          ],
          "latency_ms": 956.3725679981872,
          "query_id": "realistic_018",
          "dimension": "original",
          "query": "How do I implement retry logic for failed workflow steps?"
        },
        {
          "key_facts": [
            "max_attempts: 3",
            "backoff_type: exponential",
            "initial_interval: 1000",
            "max retries: 3"
          ],
          "found_facts": [
            "max_attempts: 3",
            "backoff_type: exponential",
            "initial_interval: 1000",
            "max retries: 3"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "troubleshooting_guide",
            "user_guide"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "user_guide",
              "chunk_id": "user_guide_fix_5",
              "content": "Save and activate **Testing Schedules:** Use the built-in schedule calculator to preview upcoming executions: ``` Next 5 executions: 1. 2026-01-24 14:00:00 UTC 2. 2026-01-25 14:00:00 UTC 3. 2026-01-26 14:00:00 UTC 4. 2026-01-27 14:00:00 UTC 5. 2026-01-28 14:00:00 UTC ``` ## Error Handling Robust error handling ensures your workflows are resilient and reliable. ### Retry Policies Configure automatic retries for failed actions: ```yaml - id: api_call action: http_request config: url: \"https://api.example.com/data\" retry: max_attempts: 3 backoff_type: \"exponential\" # or \"fixed\", \"linear\" initial_interval: 1000 # milliseconds max_interval: 30000 multiplier: 2.0 retry_on: - timeout - network_error - status: [500, 502, 503, 504] ``` **Backoff Strategies:** - **Fixed**: Wait the same amount of time between retries - Attempt 1: Wait 1s - Attempt 2: Wait 1s - Attempt 3: Wait 1s - **Linear**: Increase wait time by a fixed amount - Attempt 1: Wait 1s - Attempt 2: Wait 2s - Attempt 3: Wait 3s - **Exponential**: Double the wait time with each retry (recommended) - Attempt 1: Wait 1s - Attempt 2: Wait 2s - Attempt 3: Wait 4s **Retry Conditions:** Control which errors trigger retries: - `timeout`: Request timeout - `network_error`: Connection failures - `status`: Specific HTTP status codes - `error_code`: Application-specific error codes ### Fallback Actions Execute alternative actions when the primary action fails: ```yaml - id: primary_payment action: http_request config: url: \"https://primary-payment-gateway.com/charge\" method: POST body: amount: \"{{amount}}\" on_error: - id: fallback_payment action: http_request config: url: \"https://backup-payment-gateway.com/charge\" method: POST body: amount: \"{{amount}}\" - id: notify_admin action: email config: to: \"admin@company.com\" subject: \"Payment Gateway Failure\" body: \"Primary gateway failed, switched to backup\" ``` **Error Object:** Access error details in fallback actions: ``` {{error.message}} # Error message {{error.code}} # Error code {{error.step_id}} # Failed step ID {{error.timestamp}} # When the error occurred {{error.attempts}} # Number of retry attempts ``` ### Dead Letter Queue When all retries and fallbacks fail, CloudFlow can route failed executions to a Dead Letter Queue (DLQ) for manual review: **Enable DLQ:** ```yaml workflow: error_handling: dead_letter_queue: enabled: true retain_days: 30 ``` **DLQ Features:** - View failed executions in the dashboard - Inspect complete execution context and error details - Retry individual executions after fixing issues - Export failed executions for analysis - Set up alerts for DLQ threshold breaches **Accessing the DLQ:** 1. Navigate to **Workflows** > **[Your Workflow]** > **Dead Letter Queue** 2.",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_3",
              "content": "**Technology**: Go with distributed locking via Redis **Replicas**: 4 pods (production), active-passive with leader election **Resource Allocation**: 2 vCPU, 4GB RAM per pod **Key Responsibilities**: - Parse and validate cron expressions (extended format supporting seconds) - Maintain schedule registry in PostgreSQL - Distributed scheduling with leader election (one active scheduler) - Missed execution handling with configurable catch-up policy - Schedule conflict detection and resolution - Time zone support for international schedules **Scheduling Architecture**: ``` PostgreSQL Schedules Table \u2502 \u25bc Leader Scheduler (elected via Redis) \u2502 \u251c\u2500 Scan for due schedules (every 10 seconds) \u251c\u2500 Acquire lock per schedule (prevents duplicates) \u251c\u2500 Publish to Kafka \u2192 workflow.events topic \u2514\u2500 Update last_run timestamp ``` **Schedule Types**: - **Cron-based**: `0 */5 * * * *` (every 5 minutes) - **One-time**: Specific timestamp for delayed execution - **Interval-based**: Every N seconds/minutes/hours **Reliability Features**: - Leader election using Redis with 30-second lease - Heartbeat mechanism to detect leader failure (5-second interval) - Automatic failover to standby scheduler (< 10 seconds) - Schedule versioning to handle updates during execution - Missed execution policy: SKIP, RUN_ONCE, or RUN_ALL **Performance Targets**: - Schedule evaluation: < 100ms per cycle - Accuracy: \u00b1 1 second for schedule triggers - Capacity: 100,000 active schedules --- ### Notification Service **Purpose**: Multi-channel notification delivery system supporting email, SMS, webhooks, and in-app notifications.",
              "score": null
            },
            {
              "doc_id": "user_guide",
              "chunk_id": "user_guide_fix_7",
              "content": "Handle Errors Gracefully Always implement error handling for external API calls and database operations: ```yaml - id: fetch_data action: http_request config: url: \"https://api.example.com/data\" retry: max_attempts: 3 backoff_type: exponential on_error: - id: log_error action: database_query config: query: \"INSERT INTO error_log (workflow_id, error) VALUES ($1, $2)\" parameters: - \"{{workflow.id}}\" - \"{{error.message}}\" ``` ### 3. Use Secrets for Sensitive Data Never hardcode API keys, passwords, or tokens in workflows: **Bad:** ```yaml headers: Authorization: \"Bearer sk_live_abc123xyz789\" ``` **Good:** ```yaml headers: Authorization: \"Bearer {{secrets.API_TOKEN}}\" ``` Store secrets in **Settings** > **Secrets** with encryption at rest. ### 4. Validate Input Data Always validate trigger data before processing: ```yaml - id: validate_input action: javascript code: | const required_fields = ['email', 'name', 'order_id']; for (const field of required_fields) { if (!input[field]) { throw new Error(`Missing required field: ${field}`); } } // Validate email format const email_regex = /^[^\\s@]+@[^\\s@]+\\.[^\\s@]+$/; if (!email_regex.test(input.email)) { throw new Error('Invalid email format'); } return { validated: true }; ``` ### 5. Use Idempotency Keys For operations that shouldn't be repeated (payments, record creation), use idempotency keys: ```yaml - id: create_charge action: http_request config: url: \"https://api.stripe.com/v1/charges\" method: POST headers: Idempotency-Key: \"{{workflow.id}}-{{execution.id}}\" body: amount: \"{{amount}}\" ``` ### 6. Monitor and Log Add logging steps for important workflow milestones: ```yaml - id: log_start action: database_query config: query: \"INSERT INTO workflow_audit (execution_id, step, timestamp) VALUES ($1, $2, $3)\" parameters: - \"{{execution.id}}\" - \"workflow_started\" - \"{{now()}}\" ``` ### 7. Keep Workflows Modular Break complex workflows into smaller, reusable components: - Use sub-workflows for repeated logic - Trigger child workflows via webhooks - Share common configurations via templates ### 8. Test Thoroughly Before activating a workflow: 1. Use test mode with sample data 2. Verify all actions execute correctly 3. Test error handling paths 4. Review execution logs 5. Start with a limited scope (e.g., test channel, small dataset) ### 9. Document Your Workflows Add descriptions to workflows and steps: ```yaml workflow: name: \"Daily Sales Report\" description: | Generates a daily sales report and distributes it to the sales team. Runs at 8:00 AM EST Monday-Friday. Data source: PostgreSQL sales database Recipients: sales-team@company.com steps: - id: fetch_sales name: \"Fetch Yesterday's Sales\" description: \"Query database for all completed orders from previous day\" ``` ### 10.",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_6",
              "content": "Data Validation Errors** ``` ValidationError: Field 'customer_id' is required but missing in 234 records ``` **Resolution:** ```bash # Add data quality checks cloudflow workflows update wf_9k2n4m8p1q \\ --step data_ingestion \\ --add-validator required_fields \\ --validator-config '{\"fields\": [\"customer_id\", \"timestamp\", \"amount\"]}' # Configure error handling cloudflow workflows update wf_9k2n4m8p1q \\ --step data_validation \\ --on-error continue \\ --error-threshold 5% # Fail if > 5% of records invalid ``` **2. External API Failures** ``` ExternalAPIError: API request to https://partner-api.example.com failed with status 502 ``` **Resolution:** ```bash # Add circuit breaker cloudflow workflows update wf_9k2n4m8p1q \\ --step external_api_call \\ --circuit-breaker-enabled true \\ --circuit-breaker-threshold 5 \\ --circuit-breaker-timeout 30000 # Configure fallback behavior cloudflow workflows update wf_9k2n4m8p1q \\ --step external_api_call \\ --fallback-action use_cached_data \\ --cache-ttl 3600 ``` --- ## Rate Limiting & Throttling ### 429 Too Many Requests #### Error Response ```json { \"error\": \"rate_limit_exceeded\", \"message\": \"API rate limit exceeded. Retry after 45 seconds.\", \"status\": 429, \"headers\": { \"X-RateLimit-Limit\": \"1000\", \"X-RateLimit-Remaining\": \"0\", \"X-RateLimit-Reset\": \"1706112345\", \"Retry-After\": \"45\" } } ``` #### Rate Limit Tiers CloudFlow enforces the following rate limits per workspace: | Tier | Requests/Minute | Requests/Hour | Concurrent Workflows | |------|-----------------|---------------|----------------------| | Free | 60 | 1,000 | 5 | | Standard | 1,000 | 50,000 | 50 | | Premium | 5,000 | 250,000 | 200 | | Enterprise | Custom | Custom | Unlimited | #### Checking Rate Limit Status ```bash # Check current rate limit status curl -I https://api.cloudflow.io/api/v1/workflows \\ -H \"Authorization: Bearer $CF_ACCESS_TOKEN\" # Extract rate limit headers curl -s -I https://api.cloudflow.io/api/v1/workflows \\ -H \"Authorization: Bearer $CF_ACCESS_TOKEN\" | \\ grep -i \"x-ratelimit\" # Output: # X-RateLimit-Limit: 1000 # X-RateLimit-Remaining: 247 # X-RateLimit-Reset: 1706112400 # Monitor rate limit usage cloudflow metrics query \\ --metric rate_limit_remaining \\ --workspace ws_abc123 \\ --last 1h \\ --interval 1m ``` #### Handling Rate Limits in Code **Python example with retry logic:** ```python import time import requests def cloudflow_api_call_with_retry(url, headers, max_retries=3): for attempt in range(max_retries): response = requests.get(url, headers=headers) if response.status_code == 429: retry_after = int(response.headers.get('Retry-After', 60)) print(f\"Rate limited.",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_5",
              "content": "exec_7h3j6k9m2n --show-bottlenecks ``` #### Solutions **1. Increase workflow timeout (if justified):** ```bash # Update workflow configuration cloudflow workflows update wf_9k2n4m8p1q \\ --timeout 7200 \\ --reason \"Large dataset processing requires extended time\" # Verify update cloudflow workflows get wf_9k2n4m8p1q | grep timeout ``` **2. Optimize slow steps:** ```bash # Enable parallel processing cloudflow workflows update wf_9k2n4m8p1q \\ --step data_transformation \\ --parallel-workers 8 \\ --batch-size 1000 # Add checkpointing for long operations cloudflow workflows update wf_9k2n4m8p1q \\ --step data_transformation \\ --enable-checkpointing \\ --checkpoint-interval 300 ``` **3. Split workflow into smaller workflows:** ```bash # Create sub-workflows cloudflow workflows create data-pipeline-part1 \\ --steps \"data_ingestion,data_validation\" \\ --timeout 1800 cloudflow workflows create data-pipeline-part2 \\ --steps \"data_transformation,data_export\" \\ --timeout 3600 \\ --trigger workflow_completed \\ --trigger-workflow data-pipeline-part1 ``` ### Retry Logic and Exponential Backoff CloudFlow implements automatic retry with exponential backoff for transient failures: - Max retries: 3 - Initial delay: 1 second - Backoff multiplier: 2 - Max delay: 60 seconds #### Retry Sequence ``` Attempt 1: Immediate Attempt 2: Wait 1s (2^0 * 1s) Attempt 3: Wait 2s (2^1 * 1s) Attempt 4: Wait 4s (2^2 * 1s) ``` #### Configuration ```bash # View current retry settings cloudflow workflows get wf_9k2n4m8p1q --format json | jq '.retry_policy' # Customize retry behavior cloudflow workflows update wf_9k2n4m8p1q \\ --retry-max-attempts 5 \\ --retry-initial-delay 2000 \\ --retry-backoff-multiplier 2 \\ --retry-max-delay 120000 \\ --retry-on-errors \"NETWORK_ERROR,TIMEOUT,RATE_LIMIT\" # Disable retry for specific step cloudflow workflows update wf_9k2n4m8p1q \\ --step data_export \\ --retry-enabled false ``` #### Monitoring Retries ```bash # List failed executions with retry information cloudflow workflows executions list \\ --status FAILED \\ --show-retries \\ --last 7d # Get retry history for specific execution cloudflow workflows executions retries exec_7h3j6k9m2n # Output: # Execution: exec_7h3j6k9m2n # Attempt 1: FAILED - NetworkError: Connection refused (delay: 0ms) # Attempt 2: FAILED - NetworkError: Connection timeout (delay: 1000ms) # Attempt 3: FAILED - NetworkError: Connection timeout (delay: 2000ms) # Attempt 4: FAILED - NetworkError: Connection timeout (delay: 4000ms) # Final Status: FAILED_AFTER_RETRIES ``` ### Workflow Step Failures #### Common Error Patterns **1.",
              "score": null
            }
          ],
          "latency_ms": 1069.3958109986852,
          "query_id": "realistic_018",
          "dimension": "synonym",
          "query": "What's the retry strategy for transient failures?"
        },
        {
          "key_facts": [
            "max_attempts: 3",
            "backoff_type: exponential",
            "initial_interval: 1000",
            "max retries: 3"
          ],
          "found_facts": [
            "max_attempts: 3",
            "initial_interval: 1000",
            "max retries: 3"
          ],
          "missed_facts": [
            "backoff_type: exponential"
          ],
          "coverage": 0.75,
          "expected_docs": [
            "troubleshooting_guide",
            "user_guide"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_5",
              "content": "exec_7h3j6k9m2n --show-bottlenecks ``` #### Solutions **1. Increase workflow timeout (if justified):** ```bash # Update workflow configuration cloudflow workflows update wf_9k2n4m8p1q \\ --timeout 7200 \\ --reason \"Large dataset processing requires extended time\" # Verify update cloudflow workflows get wf_9k2n4m8p1q | grep timeout ``` **2. Optimize slow steps:** ```bash # Enable parallel processing cloudflow workflows update wf_9k2n4m8p1q \\ --step data_transformation \\ --parallel-workers 8 \\ --batch-size 1000 # Add checkpointing for long operations cloudflow workflows update wf_9k2n4m8p1q \\ --step data_transformation \\ --enable-checkpointing \\ --checkpoint-interval 300 ``` **3. Split workflow into smaller workflows:** ```bash # Create sub-workflows cloudflow workflows create data-pipeline-part1 \\ --steps \"data_ingestion,data_validation\" \\ --timeout 1800 cloudflow workflows create data-pipeline-part2 \\ --steps \"data_transformation,data_export\" \\ --timeout 3600 \\ --trigger workflow_completed \\ --trigger-workflow data-pipeline-part1 ``` ### Retry Logic and Exponential Backoff CloudFlow implements automatic retry with exponential backoff for transient failures: - Max retries: 3 - Initial delay: 1 second - Backoff multiplier: 2 - Max delay: 60 seconds #### Retry Sequence ``` Attempt 1: Immediate Attempt 2: Wait 1s (2^0 * 1s) Attempt 3: Wait 2s (2^1 * 1s) Attempt 4: Wait 4s (2^2 * 1s) ``` #### Configuration ```bash # View current retry settings cloudflow workflows get wf_9k2n4m8p1q --format json | jq '.retry_policy' # Customize retry behavior cloudflow workflows update wf_9k2n4m8p1q \\ --retry-max-attempts 5 \\ --retry-initial-delay 2000 \\ --retry-backoff-multiplier 2 \\ --retry-max-delay 120000 \\ --retry-on-errors \"NETWORK_ERROR,TIMEOUT,RATE_LIMIT\" # Disable retry for specific step cloudflow workflows update wf_9k2n4m8p1q \\ --step data_export \\ --retry-enabled false ``` #### Monitoring Retries ```bash # List failed executions with retry information cloudflow workflows executions list \\ --status FAILED \\ --show-retries \\ --last 7d # Get retry history for specific execution cloudflow workflows executions retries exec_7h3j6k9m2n # Output: # Execution: exec_7h3j6k9m2n # Attempt 1: FAILED - NetworkError: Connection refused (delay: 0ms) # Attempt 2: FAILED - NetworkError: Connection timeout (delay: 1000ms) # Attempt 3: FAILED - NetworkError: Connection timeout (delay: 2000ms) # Attempt 4: FAILED - NetworkError: Connection timeout (delay: 4000ms) # Final Status: FAILED_AFTER_RETRIES ``` ### Workflow Step Failures #### Common Error Patterns **1.",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_4",
              "content": "**Implement connection pooling optimization:** ```bash # Use PgBouncer for connection pooling kubectl apply -f cloudflow-pgbouncer.yaml # Configure CloudFlow to use PgBouncer cloudflow config set db.host pgbouncer.cloudflow.svc.cluster.local cloudflow config set db.port 6432 cloudflow config set db.pool.mode transaction ``` 2. **Add read replicas:** ```bash # Route read-only queries to replicas cloudflow db replicas add --count 2 cloudflow config set db.read_replicas \"replica-1:5432,replica-2:5432\" cloudflow config set db.read_write_split true ``` ### Connection Timeout Errors #### Error Messages - `connection timeout after 30000ms` - `could not connect to database server at 10.0.2.45:5432` - `database server unreachable` #### Troubleshooting Steps ```bash # Test network connectivity telnet cloudflow-db.internal.company.com 5432 # Check DNS resolution nslookup cloudflow-db.internal.company.com # Verify database is accepting connections pg_isready -h cloudflow-db.internal.company.com -p 5432 -U cloudflow # Check firewall rules sudo iptables -L -n | grep 5432 # Test from CloudFlow pod network kubectl run -n cloudflow debug-pod --rm -i --tty \\ --image=postgres:14 -- \\ psql -h cloudflow-db.internal.company.com -U cloudflow -d cloudflow_production # Review database logs for connection rejections kubectl logs -n cloudflow statefulset/cloudflow-db --tail=100 | \\ grep -i \"connection\\|reject\\|authentication\" ``` ### Maximum Connection Limit (100) Reached This is a hard limit in CloudFlow's database tier. #### Permanent Solutions **Option 1: Upgrade database tier** ```bash # Check available tiers cloudflow db tiers list # Upgrade to higher tier (supports 200 connections) cloudflow db upgrade --tier standard-plus --confirm # Monitor migration progress cloudflow db migration status ``` **Option 2: Implement aggressive connection reuse** ```bash # Reduce connection lifetime cloudflow config set db.pool.max_lifetime 600 # 10 minutes # Enable prepared statement caching cloudflow config set db.prepared_statements.cache true cloudflow config set db.prepared_statements.max_size 250 # Reduce idle connection timeout cloudflow config set db.pool.idle_timeout 120 # 2 minutes ``` --- ## Workflow Execution Failures ### Timeout Errors (3600 second limit) #### Error Message ``` WorkflowExecutionError: Workflow exceeded maximum execution time of 3600 seconds Status: TIMEOUT Workflow ID: wf_9k2n4m8p1q Execution ID: exec_7h3j6k9m2n ``` #### Analysis ```bash # Get workflow execution details cloudflow workflows executions get exec_7h3j6k9m2n --verbose # Check step-by-step breakdown cloudflow workflows executions steps exec_7h3j6k9m2n # Sample output: # Step 1: data_ingestion - Duration: 245s - Status: SUCCESS # Step 2: data_validation - Duration: 123s - Status: SUCCESS # Step 3: data_transformation - Duration: 3189s - Status: TIMEOUT # Step 4: data_export - Duration: 0s - Status: SKIPPED # Identify bottleneck step cloudflow workflows analyze",
              "score": null
            },
            {
              "doc_id": "user_guide",
              "chunk_id": "user_guide_fix_5",
              "content": "Save and activate **Testing Schedules:** Use the built-in schedule calculator to preview upcoming executions: ``` Next 5 executions: 1. 2026-01-24 14:00:00 UTC 2. 2026-01-25 14:00:00 UTC 3. 2026-01-26 14:00:00 UTC 4. 2026-01-27 14:00:00 UTC 5. 2026-01-28 14:00:00 UTC ``` ## Error Handling Robust error handling ensures your workflows are resilient and reliable. ### Retry Policies Configure automatic retries for failed actions: ```yaml - id: api_call action: http_request config: url: \"https://api.example.com/data\" retry: max_attempts: 3 backoff_type: \"exponential\" # or \"fixed\", \"linear\" initial_interval: 1000 # milliseconds max_interval: 30000 multiplier: 2.0 retry_on: - timeout - network_error - status: [500, 502, 503, 504] ``` **Backoff Strategies:** - **Fixed**: Wait the same amount of time between retries - Attempt 1: Wait 1s - Attempt 2: Wait 1s - Attempt 3: Wait 1s - **Linear**: Increase wait time by a fixed amount - Attempt 1: Wait 1s - Attempt 2: Wait 2s - Attempt 3: Wait 3s - **Exponential**: Double the wait time with each retry (recommended) - Attempt 1: Wait 1s - Attempt 2: Wait 2s - Attempt 3: Wait 4s **Retry Conditions:** Control which errors trigger retries: - `timeout`: Request timeout - `network_error`: Connection failures - `status`: Specific HTTP status codes - `error_code`: Application-specific error codes ### Fallback Actions Execute alternative actions when the primary action fails: ```yaml - id: primary_payment action: http_request config: url: \"https://primary-payment-gateway.com/charge\" method: POST body: amount: \"{{amount}}\" on_error: - id: fallback_payment action: http_request config: url: \"https://backup-payment-gateway.com/charge\" method: POST body: amount: \"{{amount}}\" - id: notify_admin action: email config: to: \"admin@company.com\" subject: \"Payment Gateway Failure\" body: \"Primary gateway failed, switched to backup\" ``` **Error Object:** Access error details in fallback actions: ``` {{error.message}} # Error message {{error.code}} # Error code {{error.step_id}} # Failed step ID {{error.timestamp}} # When the error occurred {{error.attempts}} # Number of retry attempts ``` ### Dead Letter Queue When all retries and fallbacks fail, CloudFlow can route failed executions to a Dead Letter Queue (DLQ) for manual review: **Enable DLQ:** ```yaml workflow: error_handling: dead_letter_queue: enabled: true retain_days: 30 ``` **DLQ Features:** - View failed executions in the dashboard - Inspect complete execution context and error details - Retry individual executions after fixing issues - Export failed executions for analysis - Set up alerts for DLQ threshold breaches **Accessing the DLQ:** 1. Navigate to **Workflows** > **[Your Workflow]** > **Dead Letter Queue** 2.",
              "score": null
            },
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_0",
              "content": "# CloudFlow Platform - Deployment and Operations Guide **Version:** 2.4.0 **Last Updated:** January 2026 **Target Environment:** Production (AWS EKS) ## Table of Contents 1. [Overview](#overview) 2. [Prerequisites](#prerequisites) 3. [Infrastructure Setup](#infrastructure-setup) 4. [Kubernetes Deployment](#kubernetes-deployment) 5. [Database Configuration](#database-configuration) 6. [Monitoring and Observability](#monitoring-and-observability) 7. [Backup and Disaster Recovery](#backup-and-disaster-recovery) 8. [Scaling and Performance](#scaling-and-performance) 9. [Security and Compliance](#security-and-compliance) 10. [Troubleshooting](#troubleshooting) --- ## Overview CloudFlow is a cloud-native workflow orchestration platform designed for high-availability production environments. This guide provides comprehensive instructions for deploying and operating CloudFlow on Amazon EKS (Elastic Kubernetes Service). ### Architecture Summary CloudFlow consists of the following components: - **API Server**: REST API for workflow management (Node.js/Express) - **Worker Service**: Background job processor (Node.js) - **Scheduler**: Cron-based task scheduler (Node.js) - **PostgreSQL**: Primary data store (version 14) - **Redis**: Cache and message queue (version 7.0) - **PgBouncer**: Database connection pooler ### Deployment Model - **Namespace**: `cloudflow-prod` - **Cluster**: AWS EKS 1.28 - **Region**: us-east-1 (primary), us-west-2 (disaster recovery) - **High Availability**: Multi-AZ deployment across 3 availability zones --- ## Prerequisites Before beginning the deployment process, ensure you have the following: ### Required Tools - `kubectl` (v1.28 or later) - `helm` (v3.12 or later) - `aws-cli` (v2.13 or later) - `eksctl` (v0.165 or later) - `terraform` (v1.6 or later) - for infrastructure provisioning ### Access Requirements - AWS account with appropriate IAM permissions - EKS cluster admin access - Container registry access (ECR) - Domain name and SSL certificates - Secrets management access (AWS Secrets Manager or Vault) ### Network Requirements - VPC with at least 3 public and 3 private subnets - NAT Gateway configured - Security groups allowing: - Ingress: HTTPS (443), HTTP (80) - Internal: PostgreSQL (5432), Redis (6379) - Monitoring: Prometheus (9090), Grafana (3000) --- ## Infrastructure Setup ### EKS Cluster Creation Use the following `eksctl` configuration to create the EKS cluster: ```yaml # cluster-config.yaml apiVersion: eksctl.io/v1alpha5 kind: ClusterConfig metadata: name: cloudflow-production region: us-east-1 version: \"1.28\" availabilityZones: - us-east-1a - us-east-1b - us-east-1c vpc: cidr: 10.0.0.0/16 nat: gateway: HighlyAvailable managedNodeGroups: - name: cloudflow-workers instanceType: m5.xlarge minSize: 3 maxSize: 10 desiredCapacity: 5 volumeSize: 100 privateNetworking: true labels: role: worker environment: production tags: Environment: production Application: cloudflow iam: withAddonPolicies: ebs: true efs: true albIngress: true cloudWatch: true cloudWatch: clusterLogging: enableTypes: - api - audit - authenticator - controllerManager - scheduler ``` Create the cluster: ```bash eksctl",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_6",
              "content": "Data Validation Errors** ``` ValidationError: Field 'customer_id' is required but missing in 234 records ``` **Resolution:** ```bash # Add data quality checks cloudflow workflows update wf_9k2n4m8p1q \\ --step data_ingestion \\ --add-validator required_fields \\ --validator-config '{\"fields\": [\"customer_id\", \"timestamp\", \"amount\"]}' # Configure error handling cloudflow workflows update wf_9k2n4m8p1q \\ --step data_validation \\ --on-error continue \\ --error-threshold 5% # Fail if > 5% of records invalid ``` **2. External API Failures** ``` ExternalAPIError: API request to https://partner-api.example.com failed with status 502 ``` **Resolution:** ```bash # Add circuit breaker cloudflow workflows update wf_9k2n4m8p1q \\ --step external_api_call \\ --circuit-breaker-enabled true \\ --circuit-breaker-threshold 5 \\ --circuit-breaker-timeout 30000 # Configure fallback behavior cloudflow workflows update wf_9k2n4m8p1q \\ --step external_api_call \\ --fallback-action use_cached_data \\ --cache-ttl 3600 ``` --- ## Rate Limiting & Throttling ### 429 Too Many Requests #### Error Response ```json { \"error\": \"rate_limit_exceeded\", \"message\": \"API rate limit exceeded. Retry after 45 seconds.\", \"status\": 429, \"headers\": { \"X-RateLimit-Limit\": \"1000\", \"X-RateLimit-Remaining\": \"0\", \"X-RateLimit-Reset\": \"1706112345\", \"Retry-After\": \"45\" } } ``` #### Rate Limit Tiers CloudFlow enforces the following rate limits per workspace: | Tier | Requests/Minute | Requests/Hour | Concurrent Workflows | |------|-----------------|---------------|----------------------| | Free | 60 | 1,000 | 5 | | Standard | 1,000 | 50,000 | 50 | | Premium | 5,000 | 250,000 | 200 | | Enterprise | Custom | Custom | Unlimited | #### Checking Rate Limit Status ```bash # Check current rate limit status curl -I https://api.cloudflow.io/api/v1/workflows \\ -H \"Authorization: Bearer $CF_ACCESS_TOKEN\" # Extract rate limit headers curl -s -I https://api.cloudflow.io/api/v1/workflows \\ -H \"Authorization: Bearer $CF_ACCESS_TOKEN\" | \\ grep -i \"x-ratelimit\" # Output: # X-RateLimit-Limit: 1000 # X-RateLimit-Remaining: 247 # X-RateLimit-Reset: 1706112400 # Monitor rate limit usage cloudflow metrics query \\ --metric rate_limit_remaining \\ --workspace ws_abc123 \\ --last 1h \\ --interval 1m ``` #### Handling Rate Limits in Code **Python example with retry logic:** ```python import time import requests def cloudflow_api_call_with_retry(url, headers, max_retries=3): for attempt in range(max_retries): response = requests.get(url, headers=headers) if response.status_code == 429: retry_after = int(response.headers.get('Retry-After', 60)) print(f\"Rate limited.",
              "score": null
            }
          ],
          "latency_ms": 930.2981059990998,
          "query_id": "realistic_018",
          "dimension": "problem",
          "query": "My workflow fails on temporary network errors"
        },
        {
          "key_facts": [
            "max_attempts: 3",
            "backoff_type: exponential",
            "initial_interval: 1000",
            "max retries: 3"
          ],
          "found_facts": [
            "max_attempts: 3",
            "initial_interval: 1000",
            "max retries: 3"
          ],
          "missed_facts": [
            "backoff_type: exponential"
          ],
          "coverage": 0.75,
          "expected_docs": [
            "troubleshooting_guide",
            "user_guide"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "user_guide",
              "chunk_id": "user_guide_fix_5",
              "content": "Save and activate **Testing Schedules:** Use the built-in schedule calculator to preview upcoming executions: ``` Next 5 executions: 1. 2026-01-24 14:00:00 UTC 2. 2026-01-25 14:00:00 UTC 3. 2026-01-26 14:00:00 UTC 4. 2026-01-27 14:00:00 UTC 5. 2026-01-28 14:00:00 UTC ``` ## Error Handling Robust error handling ensures your workflows are resilient and reliable. ### Retry Policies Configure automatic retries for failed actions: ```yaml - id: api_call action: http_request config: url: \"https://api.example.com/data\" retry: max_attempts: 3 backoff_type: \"exponential\" # or \"fixed\", \"linear\" initial_interval: 1000 # milliseconds max_interval: 30000 multiplier: 2.0 retry_on: - timeout - network_error - status: [500, 502, 503, 504] ``` **Backoff Strategies:** - **Fixed**: Wait the same amount of time between retries - Attempt 1: Wait 1s - Attempt 2: Wait 1s - Attempt 3: Wait 1s - **Linear**: Increase wait time by a fixed amount - Attempt 1: Wait 1s - Attempt 2: Wait 2s - Attempt 3: Wait 3s - **Exponential**: Double the wait time with each retry (recommended) - Attempt 1: Wait 1s - Attempt 2: Wait 2s - Attempt 3: Wait 4s **Retry Conditions:** Control which errors trigger retries: - `timeout`: Request timeout - `network_error`: Connection failures - `status`: Specific HTTP status codes - `error_code`: Application-specific error codes ### Fallback Actions Execute alternative actions when the primary action fails: ```yaml - id: primary_payment action: http_request config: url: \"https://primary-payment-gateway.com/charge\" method: POST body: amount: \"{{amount}}\" on_error: - id: fallback_payment action: http_request config: url: \"https://backup-payment-gateway.com/charge\" method: POST body: amount: \"{{amount}}\" - id: notify_admin action: email config: to: \"admin@company.com\" subject: \"Payment Gateway Failure\" body: \"Primary gateway failed, switched to backup\" ``` **Error Object:** Access error details in fallback actions: ``` {{error.message}} # Error message {{error.code}} # Error code {{error.step_id}} # Failed step ID {{error.timestamp}} # When the error occurred {{error.attempts}} # Number of retry attempts ``` ### Dead Letter Queue When all retries and fallbacks fail, CloudFlow can route failed executions to a Dead Letter Queue (DLQ) for manual review: **Enable DLQ:** ```yaml workflow: error_handling: dead_letter_queue: enabled: true retain_days: 30 ``` **DLQ Features:** - View failed executions in the dashboard - Inspect complete execution context and error details - Retry individual executions after fixing issues - Export failed executions for analysis - Set up alerts for DLQ threshold breaches **Accessing the DLQ:** 1. Navigate to **Workflows** > **[Your Workflow]** > **Dead Letter Queue** 2.",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_5",
              "content": "exec_7h3j6k9m2n --show-bottlenecks ``` #### Solutions **1. Increase workflow timeout (if justified):** ```bash # Update workflow configuration cloudflow workflows update wf_9k2n4m8p1q \\ --timeout 7200 \\ --reason \"Large dataset processing requires extended time\" # Verify update cloudflow workflows get wf_9k2n4m8p1q | grep timeout ``` **2. Optimize slow steps:** ```bash # Enable parallel processing cloudflow workflows update wf_9k2n4m8p1q \\ --step data_transformation \\ --parallel-workers 8 \\ --batch-size 1000 # Add checkpointing for long operations cloudflow workflows update wf_9k2n4m8p1q \\ --step data_transformation \\ --enable-checkpointing \\ --checkpoint-interval 300 ``` **3. Split workflow into smaller workflows:** ```bash # Create sub-workflows cloudflow workflows create data-pipeline-part1 \\ --steps \"data_ingestion,data_validation\" \\ --timeout 1800 cloudflow workflows create data-pipeline-part2 \\ --steps \"data_transformation,data_export\" \\ --timeout 3600 \\ --trigger workflow_completed \\ --trigger-workflow data-pipeline-part1 ``` ### Retry Logic and Exponential Backoff CloudFlow implements automatic retry with exponential backoff for transient failures: - Max retries: 3 - Initial delay: 1 second - Backoff multiplier: 2 - Max delay: 60 seconds #### Retry Sequence ``` Attempt 1: Immediate Attempt 2: Wait 1s (2^0 * 1s) Attempt 3: Wait 2s (2^1 * 1s) Attempt 4: Wait 4s (2^2 * 1s) ``` #### Configuration ```bash # View current retry settings cloudflow workflows get wf_9k2n4m8p1q --format json | jq '.retry_policy' # Customize retry behavior cloudflow workflows update wf_9k2n4m8p1q \\ --retry-max-attempts 5 \\ --retry-initial-delay 2000 \\ --retry-backoff-multiplier 2 \\ --retry-max-delay 120000 \\ --retry-on-errors \"NETWORK_ERROR,TIMEOUT,RATE_LIMIT\" # Disable retry for specific step cloudflow workflows update wf_9k2n4m8p1q \\ --step data_export \\ --retry-enabled false ``` #### Monitoring Retries ```bash # List failed executions with retry information cloudflow workflows executions list \\ --status FAILED \\ --show-retries \\ --last 7d # Get retry history for specific execution cloudflow workflows executions retries exec_7h3j6k9m2n # Output: # Execution: exec_7h3j6k9m2n # Attempt 1: FAILED - NetworkError: Connection refused (delay: 0ms) # Attempt 2: FAILED - NetworkError: Connection timeout (delay: 1000ms) # Attempt 3: FAILED - NetworkError: Connection timeout (delay: 2000ms) # Attempt 4: FAILED - NetworkError: Connection timeout (delay: 4000ms) # Final Status: FAILED_AFTER_RETRIES ``` ### Workflow Step Failures #### Common Error Patterns **1.",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_3",
              "content": "**Technology**: Go with distributed locking via Redis **Replicas**: 4 pods (production), active-passive with leader election **Resource Allocation**: 2 vCPU, 4GB RAM per pod **Key Responsibilities**: - Parse and validate cron expressions (extended format supporting seconds) - Maintain schedule registry in PostgreSQL - Distributed scheduling with leader election (one active scheduler) - Missed execution handling with configurable catch-up policy - Schedule conflict detection and resolution - Time zone support for international schedules **Scheduling Architecture**: ``` PostgreSQL Schedules Table \u2502 \u25bc Leader Scheduler (elected via Redis) \u2502 \u251c\u2500 Scan for due schedules (every 10 seconds) \u251c\u2500 Acquire lock per schedule (prevents duplicates) \u251c\u2500 Publish to Kafka \u2192 workflow.events topic \u2514\u2500 Update last_run timestamp ``` **Schedule Types**: - **Cron-based**: `0 */5 * * * *` (every 5 minutes) - **One-time**: Specific timestamp for delayed execution - **Interval-based**: Every N seconds/minutes/hours **Reliability Features**: - Leader election using Redis with 30-second lease - Heartbeat mechanism to detect leader failure (5-second interval) - Automatic failover to standby scheduler (< 10 seconds) - Schedule versioning to handle updates during execution - Missed execution policy: SKIP, RUN_ONCE, or RUN_ALL **Performance Targets**: - Schedule evaluation: < 100ms per cycle - Accuracy: \u00b1 1 second for schedule triggers - Capacity: 100,000 active schedules --- ### Notification Service **Purpose**: Multi-channel notification delivery system supporting email, SMS, webhooks, and in-app notifications.",
              "score": null
            },
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_6",
              "content": "**Verify Service Health**: ```bash kubectl get pods -n cloudflow-prod curl https://api.cloudflow.io/health ``` **Recovery Time Objective (RTO)**: 4 hours **Recovery Point Objective (RPO)**: 24 hours --- ## Scaling and Performance ### Horizontal Pod Autoscaling CloudFlow is configured with Horizontal Pod Autoscaler (HPA) to automatically scale based on resource utilization: ```yaml # hpa.yaml apiVersion: autoscaling/v2 kind: HorizontalPodAutoscaler metadata: name: cloudflow-api-hpa namespace: cloudflow-prod spec: scaleTargetRef: apiVersion: apps/v1 kind: Deployment name: cloudflow-api minReplicas: 3 maxReplicas: 10 metrics: - type: Resource resource: name: cpu target: type: Utilization averageUtilization: 70 - type: Resource resource: name: memory target: type: Utilization averageUtilization: 80 behavior: scaleDown: stabilizationWindowSeconds: 300 policies: - type: Percent value: 50 periodSeconds: 60 scaleUp: stabilizationWindowSeconds: 60 policies: - type: Percent value: 100 periodSeconds: 30 - type: Pods value: 2 periodSeconds: 30 selectPolicy: Max ``` Monitor autoscaling events: ```bash kubectl get hpa -n cloudflow-prod -w kubectl describe hpa cloudflow-api-hpa -n cloudflow-prod ``` ### Performance Optimization #### Connection Pooling Ensure optimal database connection pooling settings in PgBouncer: - **Pool Mode**: Transaction (optimal for microservices) - **Default Pool Size**: 25 connections per user - **Max DB Connections**: 100 (matches PostgreSQL `max_connections`) #### Redis Caching Implement Redis caching for frequently accessed data: ```javascript // Example caching strategy const CACHE_TTL = { workflows: 300, // 5 minutes userSessions: 3600, // 1 hour apiResults: 60 // 1 minute }; ``` #### Load Testing Perform regular load testing to validate scaling configuration: ```bash # Install k6 for load testing kubectl apply -f - <<EOF apiVersion: v1 kind: ConfigMap metadata: name: loadtest-script namespace: cloudflow-prod data: script.js: | import http from 'k6/http'; import { check, sleep } from 'k6'; export let options = { stages: [ { duration: '2m', target: 100 }, { duration: '5m', target: 100 }, { duration: '2m', target: 200 }, { duration: '5m', target: 200 }, { duration: '2m', target: 0 }, ], thresholds: { http_req_duration: ['p(95)<500'], }, }; export default function () { let res = http.get('https://api.cloudflow.io/api/workflows'); check(res, { 'status is 200': (r) => r.status === 200 }); sleep(1); } EOF ``` **Performance Targets**: - P95 API latency: < 500ms - P99 API latency: < 1000ms - Error rate: < 0.1% - Throughput: > 1000 requests/second --- ## Security and Compliance ### Network Policies Implement network policies to restrict pod-to-pod communication: ```yaml # network-policy.yaml apiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: cloudflow-network-policy namespace: cloudflow-prod spec:",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_1",
              "content": "**Diagnosis:** ```bash # Check system time timedatectl status # Compare with NTP server ntpdate -q pool.ntp.org # Check JWT issued time vs current time jwt_iat=$(echo $CF_ACCESS_TOKEN | cut -d'.' -f2 | base64 -d | jq '.iat') current_time=$(date +%s) skew=$((current_time - jwt_iat)) echo \"Clock skew: $skew seconds\" ``` **Resolution:** ```bash # Sync with NTP server sudo ntpdate -s pool.ntp.org # Enable automatic time sync sudo timedatectl set-ntp true # Restart CloudFlow client cloudflow restart ``` **4. Insufficient Permissions** Error: `User does not have required permissions for this operation` **Check user roles:** ```bash cloudflow auth whoami --verbose # Expected output: # User: john.doe@company.com # Roles: developer, workflow-admin # Scopes: workflow:*, data:read, metrics:read ``` **Request permission elevation:** ```bash # Submit access request cloudflow auth request-access \\ --resource \"workflow:production:deploy\" \\ --justification \"Deploy critical hotfix for TICKET-1234\" # Check pending approvals cloudflow auth list-requests --status pending ``` ### 403 Forbidden Errors #### RBAC Policy Violations CloudFlow uses role-based access control (RBAC) with the following hierarchy: - `viewer` - Read-only access - `developer` - Create and modify workflows (non-production) - `operator` - Execute workflows, view logs - `admin` - Full access to workspace - `platform-admin` - Cross-workspace administration **Verify resource permissions:** ```bash # Check effective permissions cloudflow rbac check \\ --user john.doe@company.com \\ --resource workflow:prod-pipeline \\ --action execute # List all policies affecting user cloudflow rbac policies --user john.doe@company.com --verbose ``` --- ## Performance Problems ### Slow Query Performance #### Symptoms - API response times > 5000ms - Database query latency warnings in logs - CloudFlow UI becomes unresponsive - Timeout errors: `Request exceeded maximum duration of 30000ms` #### Diagnosis Steps **1. Identify Slow Queries** ```bash # Query CloudFlow metrics cloudflow metrics query --metric api_request_duration_ms \\ --filter \"p95 > 5000\" \\ --last 1h # Check database slow query log kubectl logs -n cloudflow deploy/cloudflow-db-primary | \\ grep \"slow query\" | \\ tail -n 50 # Analyze query patterns cloudflow db analyze-queries --min-duration 5000 --limit 20 ``` **2.",
              "score": null
            }
          ],
          "latency_ms": 943.1296269976883,
          "query_id": "realistic_018",
          "dimension": "casual",
          "query": "retry config"
        },
        {
          "key_facts": [
            "max_attempts: 3",
            "backoff_type: exponential",
            "initial_interval: 1000",
            "max retries: 3"
          ],
          "found_facts": [
            "max_attempts: 3",
            "backoff_type: exponential",
            "initial_interval: 1000",
            "max retries: 3"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "troubleshooting_guide",
            "user_guide"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_5",
              "content": "exec_7h3j6k9m2n --show-bottlenecks ``` #### Solutions **1. Increase workflow timeout (if justified):** ```bash # Update workflow configuration cloudflow workflows update wf_9k2n4m8p1q \\ --timeout 7200 \\ --reason \"Large dataset processing requires extended time\" # Verify update cloudflow workflows get wf_9k2n4m8p1q | grep timeout ``` **2. Optimize slow steps:** ```bash # Enable parallel processing cloudflow workflows update wf_9k2n4m8p1q \\ --step data_transformation \\ --parallel-workers 8 \\ --batch-size 1000 # Add checkpointing for long operations cloudflow workflows update wf_9k2n4m8p1q \\ --step data_transformation \\ --enable-checkpointing \\ --checkpoint-interval 300 ``` **3. Split workflow into smaller workflows:** ```bash # Create sub-workflows cloudflow workflows create data-pipeline-part1 \\ --steps \"data_ingestion,data_validation\" \\ --timeout 1800 cloudflow workflows create data-pipeline-part2 \\ --steps \"data_transformation,data_export\" \\ --timeout 3600 \\ --trigger workflow_completed \\ --trigger-workflow data-pipeline-part1 ``` ### Retry Logic and Exponential Backoff CloudFlow implements automatic retry with exponential backoff for transient failures: - Max retries: 3 - Initial delay: 1 second - Backoff multiplier: 2 - Max delay: 60 seconds #### Retry Sequence ``` Attempt 1: Immediate Attempt 2: Wait 1s (2^0 * 1s) Attempt 3: Wait 2s (2^1 * 1s) Attempt 4: Wait 4s (2^2 * 1s) ``` #### Configuration ```bash # View current retry settings cloudflow workflows get wf_9k2n4m8p1q --format json | jq '.retry_policy' # Customize retry behavior cloudflow workflows update wf_9k2n4m8p1q \\ --retry-max-attempts 5 \\ --retry-initial-delay 2000 \\ --retry-backoff-multiplier 2 \\ --retry-max-delay 120000 \\ --retry-on-errors \"NETWORK_ERROR,TIMEOUT,RATE_LIMIT\" # Disable retry for specific step cloudflow workflows update wf_9k2n4m8p1q \\ --step data_export \\ --retry-enabled false ``` #### Monitoring Retries ```bash # List failed executions with retry information cloudflow workflows executions list \\ --status FAILED \\ --show-retries \\ --last 7d # Get retry history for specific execution cloudflow workflows executions retries exec_7h3j6k9m2n # Output: # Execution: exec_7h3j6k9m2n # Attempt 1: FAILED - NetworkError: Connection refused (delay: 0ms) # Attempt 2: FAILED - NetworkError: Connection timeout (delay: 1000ms) # Attempt 3: FAILED - NetworkError: Connection timeout (delay: 2000ms) # Attempt 4: FAILED - NetworkError: Connection timeout (delay: 4000ms) # Final Status: FAILED_AFTER_RETRIES ``` ### Workflow Step Failures #### Common Error Patterns **1.",
              "score": null
            },
            {
              "doc_id": "user_guide",
              "chunk_id": "user_guide_fix_5",
              "content": "Save and activate **Testing Schedules:** Use the built-in schedule calculator to preview upcoming executions: ``` Next 5 executions: 1. 2026-01-24 14:00:00 UTC 2. 2026-01-25 14:00:00 UTC 3. 2026-01-26 14:00:00 UTC 4. 2026-01-27 14:00:00 UTC 5. 2026-01-28 14:00:00 UTC ``` ## Error Handling Robust error handling ensures your workflows are resilient and reliable. ### Retry Policies Configure automatic retries for failed actions: ```yaml - id: api_call action: http_request config: url: \"https://api.example.com/data\" retry: max_attempts: 3 backoff_type: \"exponential\" # or \"fixed\", \"linear\" initial_interval: 1000 # milliseconds max_interval: 30000 multiplier: 2.0 retry_on: - timeout - network_error - status: [500, 502, 503, 504] ``` **Backoff Strategies:** - **Fixed**: Wait the same amount of time between retries - Attempt 1: Wait 1s - Attempt 2: Wait 1s - Attempt 3: Wait 1s - **Linear**: Increase wait time by a fixed amount - Attempt 1: Wait 1s - Attempt 2: Wait 2s - Attempt 3: Wait 3s - **Exponential**: Double the wait time with each retry (recommended) - Attempt 1: Wait 1s - Attempt 2: Wait 2s - Attempt 3: Wait 4s **Retry Conditions:** Control which errors trigger retries: - `timeout`: Request timeout - `network_error`: Connection failures - `status`: Specific HTTP status codes - `error_code`: Application-specific error codes ### Fallback Actions Execute alternative actions when the primary action fails: ```yaml - id: primary_payment action: http_request config: url: \"https://primary-payment-gateway.com/charge\" method: POST body: amount: \"{{amount}}\" on_error: - id: fallback_payment action: http_request config: url: \"https://backup-payment-gateway.com/charge\" method: POST body: amount: \"{{amount}}\" - id: notify_admin action: email config: to: \"admin@company.com\" subject: \"Payment Gateway Failure\" body: \"Primary gateway failed, switched to backup\" ``` **Error Object:** Access error details in fallback actions: ``` {{error.message}} # Error message {{error.code}} # Error code {{error.step_id}} # Failed step ID {{error.timestamp}} # When the error occurred {{error.attempts}} # Number of retry attempts ``` ### Dead Letter Queue When all retries and fallbacks fail, CloudFlow can route failed executions to a Dead Letter Queue (DLQ) for manual review: **Enable DLQ:** ```yaml workflow: error_handling: dead_letter_queue: enabled: true retain_days: 30 ``` **DLQ Features:** - View failed executions in the dashboard - Inspect complete execution context and error details - Retry individual executions after fixing issues - Export failed executions for analysis - Set up alerts for DLQ threshold breaches **Accessing the DLQ:** 1. Navigate to **Workflows** > **[Your Workflow]** > **Dead Letter Queue** 2.",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_6",
              "content": "Data Validation Errors** ``` ValidationError: Field 'customer_id' is required but missing in 234 records ``` **Resolution:** ```bash # Add data quality checks cloudflow workflows update wf_9k2n4m8p1q \\ --step data_ingestion \\ --add-validator required_fields \\ --validator-config '{\"fields\": [\"customer_id\", \"timestamp\", \"amount\"]}' # Configure error handling cloudflow workflows update wf_9k2n4m8p1q \\ --step data_validation \\ --on-error continue \\ --error-threshold 5% # Fail if > 5% of records invalid ``` **2. External API Failures** ``` ExternalAPIError: API request to https://partner-api.example.com failed with status 502 ``` **Resolution:** ```bash # Add circuit breaker cloudflow workflows update wf_9k2n4m8p1q \\ --step external_api_call \\ --circuit-breaker-enabled true \\ --circuit-breaker-threshold 5 \\ --circuit-breaker-timeout 30000 # Configure fallback behavior cloudflow workflows update wf_9k2n4m8p1q \\ --step external_api_call \\ --fallback-action use_cached_data \\ --cache-ttl 3600 ``` --- ## Rate Limiting & Throttling ### 429 Too Many Requests #### Error Response ```json { \"error\": \"rate_limit_exceeded\", \"message\": \"API rate limit exceeded. Retry after 45 seconds.\", \"status\": 429, \"headers\": { \"X-RateLimit-Limit\": \"1000\", \"X-RateLimit-Remaining\": \"0\", \"X-RateLimit-Reset\": \"1706112345\", \"Retry-After\": \"45\" } } ``` #### Rate Limit Tiers CloudFlow enforces the following rate limits per workspace: | Tier | Requests/Minute | Requests/Hour | Concurrent Workflows | |------|-----------------|---------------|----------------------| | Free | 60 | 1,000 | 5 | | Standard | 1,000 | 50,000 | 50 | | Premium | 5,000 | 250,000 | 200 | | Enterprise | Custom | Custom | Unlimited | #### Checking Rate Limit Status ```bash # Check current rate limit status curl -I https://api.cloudflow.io/api/v1/workflows \\ -H \"Authorization: Bearer $CF_ACCESS_TOKEN\" # Extract rate limit headers curl -s -I https://api.cloudflow.io/api/v1/workflows \\ -H \"Authorization: Bearer $CF_ACCESS_TOKEN\" | \\ grep -i \"x-ratelimit\" # Output: # X-RateLimit-Limit: 1000 # X-RateLimit-Remaining: 247 # X-RateLimit-Reset: 1706112400 # Monitor rate limit usage cloudflow metrics query \\ --metric rate_limit_remaining \\ --workspace ws_abc123 \\ --last 1h \\ --interval 1m ``` #### Handling Rate Limits in Code **Python example with retry logic:** ```python import time import requests def cloudflow_api_call_with_retry(url, headers, max_retries=3): for attempt in range(max_retries): response = requests.get(url, headers=headers) if response.status_code == 429: retry_after = int(response.headers.get('Retry-After', 60)) print(f\"Rate limited.",
              "score": null
            },
            {
              "doc_id": "user_guide",
              "chunk_id": "user_guide_fix_7",
              "content": "Handle Errors Gracefully Always implement error handling for external API calls and database operations: ```yaml - id: fetch_data action: http_request config: url: \"https://api.example.com/data\" retry: max_attempts: 3 backoff_type: exponential on_error: - id: log_error action: database_query config: query: \"INSERT INTO error_log (workflow_id, error) VALUES ($1, $2)\" parameters: - \"{{workflow.id}}\" - \"{{error.message}}\" ``` ### 3. Use Secrets for Sensitive Data Never hardcode API keys, passwords, or tokens in workflows: **Bad:** ```yaml headers: Authorization: \"Bearer sk_live_abc123xyz789\" ``` **Good:** ```yaml headers: Authorization: \"Bearer {{secrets.API_TOKEN}}\" ``` Store secrets in **Settings** > **Secrets** with encryption at rest. ### 4. Validate Input Data Always validate trigger data before processing: ```yaml - id: validate_input action: javascript code: | const required_fields = ['email', 'name', 'order_id']; for (const field of required_fields) { if (!input[field]) { throw new Error(`Missing required field: ${field}`); } } // Validate email format const email_regex = /^[^\\s@]+@[^\\s@]+\\.[^\\s@]+$/; if (!email_regex.test(input.email)) { throw new Error('Invalid email format'); } return { validated: true }; ``` ### 5. Use Idempotency Keys For operations that shouldn't be repeated (payments, record creation), use idempotency keys: ```yaml - id: create_charge action: http_request config: url: \"https://api.stripe.com/v1/charges\" method: POST headers: Idempotency-Key: \"{{workflow.id}}-{{execution.id}}\" body: amount: \"{{amount}}\" ``` ### 6. Monitor and Log Add logging steps for important workflow milestones: ```yaml - id: log_start action: database_query config: query: \"INSERT INTO workflow_audit (execution_id, step, timestamp) VALUES ($1, $2, $3)\" parameters: - \"{{execution.id}}\" - \"workflow_started\" - \"{{now()}}\" ``` ### 7. Keep Workflows Modular Break complex workflows into smaller, reusable components: - Use sub-workflows for repeated logic - Trigger child workflows via webhooks - Share common configurations via templates ### 8. Test Thoroughly Before activating a workflow: 1. Use test mode with sample data 2. Verify all actions execute correctly 3. Test error handling paths 4. Review execution logs 5. Start with a limited scope (e.g., test channel, small dataset) ### 9. Document Your Workflows Add descriptions to workflows and steps: ```yaml workflow: name: \"Daily Sales Report\" description: | Generates a daily sales report and distributes it to the sales team. Runs at 8:00 AM EST Monday-Friday. Data source: PostgreSQL sales database Recipients: sales-team@company.com steps: - id: fetch_sales name: \"Fetch Yesterday's Sales\" description: \"Query database for all completed orders from previous day\" ``` ### 10.",
              "score": null
            },
            {
              "doc_id": "user_guide",
              "chunk_id": "user_guide_fix_0",
              "content": "# CloudFlow User Guide Welcome to CloudFlow, the modern workflow automation platform that helps you connect your apps, automate repetitive tasks, and build powerful integrations without writing code. ## Table of Contents 1. [Getting Started](#getting-started) 2. [Workflow Creation](#workflow-creation) 3. [Available Actions](#available-actions) 4. [Variables and Expressions](#variables-and-expressions) 5. [Scheduling](#scheduling) 6. [Error Handling](#error-handling) 7. [Workflow Limits](#workflow-limits) 8. [Best Practices](#best-practices) 9. [Common Workflow Patterns](#common-workflow-patterns) ## Getting Started ### Account Setup Creating your CloudFlow account takes just a few minutes: 1. Navigate to https://app.cloudflow.io/signup 2. Enter your email address and create a strong password 3. Verify your email address by clicking the link sent to your inbox 4. Complete your profile by providing your organization name and use case 5. Choose your plan (Free, Professional, or Enterprise) Once your account is created, you'll be redirected to your CloudFlow dashboard where you can start building workflows immediately. ### Your First Workflow Let's create a simple workflow that sends you a Slack notification when a new file is uploaded to your Google Drive: 1. Click the **\"Create Workflow\"** button in the top-right corner 2. Give your workflow a name: \"Google Drive to Slack Notifier\" 3. Select **\"Google Drive - New File\"** as your trigger 4. Authenticate your Google Drive account when prompted 5. Choose the folder you want to monitor 6. Click **\"Add Action\"** and select **\"Slack - Send Message\"** 7. Authenticate your Slack account 8. Select the channel where you want to receive notifications 9. Customize your message using variables: `New file uploaded: {{trigger.file.name}}` 10. Click **\"Save & Activate\"** to enable your workflow Congratulations! Your first workflow is now live. Every time a file is added to the specified Google Drive folder, you'll receive a Slack notification. ## Workflow Creation CloudFlow offers multiple ways to create and manage workflows, giving you the flexibility to work in the way that suits you best. ### Visual Editor The Visual Editor is CloudFlow's drag-and-drop interface for building workflows without code. It's perfect for users who prefer a graphical approach: **Key Features:** - Drag-and-drop action blocks from the sidebar - Visual connections between steps show your workflow logic - Inline configuration for each action - Real-time validation and error checking - Test mode to run your workflow with sample data **Using the Visual Editor:** 1.",
              "score": null
            }
          ],
          "latency_ms": 882.4994110000262,
          "query_id": "realistic_018",
          "dimension": "contextual",
          "query": "I want workflows to automatically retry on errors, what are the options?"
        },
        {
          "key_facts": [
            "max_attempts: 3",
            "backoff_type: exponential",
            "initial_interval: 1000",
            "max retries: 3"
          ],
          "found_facts": [
            "max_attempts: 3",
            "backoff_type: exponential",
            "initial_interval: 1000",
            "max retries: 3"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "troubleshooting_guide",
            "user_guide"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "user_guide",
              "chunk_id": "user_guide_fix_5",
              "content": "Save and activate **Testing Schedules:** Use the built-in schedule calculator to preview upcoming executions: ``` Next 5 executions: 1. 2026-01-24 14:00:00 UTC 2. 2026-01-25 14:00:00 UTC 3. 2026-01-26 14:00:00 UTC 4. 2026-01-27 14:00:00 UTC 5. 2026-01-28 14:00:00 UTC ``` ## Error Handling Robust error handling ensures your workflows are resilient and reliable. ### Retry Policies Configure automatic retries for failed actions: ```yaml - id: api_call action: http_request config: url: \"https://api.example.com/data\" retry: max_attempts: 3 backoff_type: \"exponential\" # or \"fixed\", \"linear\" initial_interval: 1000 # milliseconds max_interval: 30000 multiplier: 2.0 retry_on: - timeout - network_error - status: [500, 502, 503, 504] ``` **Backoff Strategies:** - **Fixed**: Wait the same amount of time between retries - Attempt 1: Wait 1s - Attempt 2: Wait 1s - Attempt 3: Wait 1s - **Linear**: Increase wait time by a fixed amount - Attempt 1: Wait 1s - Attempt 2: Wait 2s - Attempt 3: Wait 3s - **Exponential**: Double the wait time with each retry (recommended) - Attempt 1: Wait 1s - Attempt 2: Wait 2s - Attempt 3: Wait 4s **Retry Conditions:** Control which errors trigger retries: - `timeout`: Request timeout - `network_error`: Connection failures - `status`: Specific HTTP status codes - `error_code`: Application-specific error codes ### Fallback Actions Execute alternative actions when the primary action fails: ```yaml - id: primary_payment action: http_request config: url: \"https://primary-payment-gateway.com/charge\" method: POST body: amount: \"{{amount}}\" on_error: - id: fallback_payment action: http_request config: url: \"https://backup-payment-gateway.com/charge\" method: POST body: amount: \"{{amount}}\" - id: notify_admin action: email config: to: \"admin@company.com\" subject: \"Payment Gateway Failure\" body: \"Primary gateway failed, switched to backup\" ``` **Error Object:** Access error details in fallback actions: ``` {{error.message}} # Error message {{error.code}} # Error code {{error.step_id}} # Failed step ID {{error.timestamp}} # When the error occurred {{error.attempts}} # Number of retry attempts ``` ### Dead Letter Queue When all retries and fallbacks fail, CloudFlow can route failed executions to a Dead Letter Queue (DLQ) for manual review: **Enable DLQ:** ```yaml workflow: error_handling: dead_letter_queue: enabled: true retain_days: 30 ``` **DLQ Features:** - View failed executions in the dashboard - Inspect complete execution context and error details - Retry individual executions after fixing issues - Export failed executions for analysis - Set up alerts for DLQ threshold breaches **Accessing the DLQ:** 1. Navigate to **Workflows** > **[Your Workflow]** > **Dead Letter Queue** 2.",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_5",
              "content": "exec_7h3j6k9m2n --show-bottlenecks ``` #### Solutions **1. Increase workflow timeout (if justified):** ```bash # Update workflow configuration cloudflow workflows update wf_9k2n4m8p1q \\ --timeout 7200 \\ --reason \"Large dataset processing requires extended time\" # Verify update cloudflow workflows get wf_9k2n4m8p1q | grep timeout ``` **2. Optimize slow steps:** ```bash # Enable parallel processing cloudflow workflows update wf_9k2n4m8p1q \\ --step data_transformation \\ --parallel-workers 8 \\ --batch-size 1000 # Add checkpointing for long operations cloudflow workflows update wf_9k2n4m8p1q \\ --step data_transformation \\ --enable-checkpointing \\ --checkpoint-interval 300 ``` **3. Split workflow into smaller workflows:** ```bash # Create sub-workflows cloudflow workflows create data-pipeline-part1 \\ --steps \"data_ingestion,data_validation\" \\ --timeout 1800 cloudflow workflows create data-pipeline-part2 \\ --steps \"data_transformation,data_export\" \\ --timeout 3600 \\ --trigger workflow_completed \\ --trigger-workflow data-pipeline-part1 ``` ### Retry Logic and Exponential Backoff CloudFlow implements automatic retry with exponential backoff for transient failures: - Max retries: 3 - Initial delay: 1 second - Backoff multiplier: 2 - Max delay: 60 seconds #### Retry Sequence ``` Attempt 1: Immediate Attempt 2: Wait 1s (2^0 * 1s) Attempt 3: Wait 2s (2^1 * 1s) Attempt 4: Wait 4s (2^2 * 1s) ``` #### Configuration ```bash # View current retry settings cloudflow workflows get wf_9k2n4m8p1q --format json | jq '.retry_policy' # Customize retry behavior cloudflow workflows update wf_9k2n4m8p1q \\ --retry-max-attempts 5 \\ --retry-initial-delay 2000 \\ --retry-backoff-multiplier 2 \\ --retry-max-delay 120000 \\ --retry-on-errors \"NETWORK_ERROR,TIMEOUT,RATE_LIMIT\" # Disable retry for specific step cloudflow workflows update wf_9k2n4m8p1q \\ --step data_export \\ --retry-enabled false ``` #### Monitoring Retries ```bash # List failed executions with retry information cloudflow workflows executions list \\ --status FAILED \\ --show-retries \\ --last 7d # Get retry history for specific execution cloudflow workflows executions retries exec_7h3j6k9m2n # Output: # Execution: exec_7h3j6k9m2n # Attempt 1: FAILED - NetworkError: Connection refused (delay: 0ms) # Attempt 2: FAILED - NetworkError: Connection timeout (delay: 1000ms) # Attempt 3: FAILED - NetworkError: Connection timeout (delay: 2000ms) # Attempt 4: FAILED - NetworkError: Connection timeout (delay: 4000ms) # Final Status: FAILED_AFTER_RETRIES ``` ### Workflow Step Failures #### Common Error Patterns **1.",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_6",
              "content": "Data Validation Errors** ``` ValidationError: Field 'customer_id' is required but missing in 234 records ``` **Resolution:** ```bash # Add data quality checks cloudflow workflows update wf_9k2n4m8p1q \\ --step data_ingestion \\ --add-validator required_fields \\ --validator-config '{\"fields\": [\"customer_id\", \"timestamp\", \"amount\"]}' # Configure error handling cloudflow workflows update wf_9k2n4m8p1q \\ --step data_validation \\ --on-error continue \\ --error-threshold 5% # Fail if > 5% of records invalid ``` **2. External API Failures** ``` ExternalAPIError: API request to https://partner-api.example.com failed with status 502 ``` **Resolution:** ```bash # Add circuit breaker cloudflow workflows update wf_9k2n4m8p1q \\ --step external_api_call \\ --circuit-breaker-enabled true \\ --circuit-breaker-threshold 5 \\ --circuit-breaker-timeout 30000 # Configure fallback behavior cloudflow workflows update wf_9k2n4m8p1q \\ --step external_api_call \\ --fallback-action use_cached_data \\ --cache-ttl 3600 ``` --- ## Rate Limiting & Throttling ### 429 Too Many Requests #### Error Response ```json { \"error\": \"rate_limit_exceeded\", \"message\": \"API rate limit exceeded. Retry after 45 seconds.\", \"status\": 429, \"headers\": { \"X-RateLimit-Limit\": \"1000\", \"X-RateLimit-Remaining\": \"0\", \"X-RateLimit-Reset\": \"1706112345\", \"Retry-After\": \"45\" } } ``` #### Rate Limit Tiers CloudFlow enforces the following rate limits per workspace: | Tier | Requests/Minute | Requests/Hour | Concurrent Workflows | |------|-----------------|---------------|----------------------| | Free | 60 | 1,000 | 5 | | Standard | 1,000 | 50,000 | 50 | | Premium | 5,000 | 250,000 | 200 | | Enterprise | Custom | Custom | Unlimited | #### Checking Rate Limit Status ```bash # Check current rate limit status curl -I https://api.cloudflow.io/api/v1/workflows \\ -H \"Authorization: Bearer $CF_ACCESS_TOKEN\" # Extract rate limit headers curl -s -I https://api.cloudflow.io/api/v1/workflows \\ -H \"Authorization: Bearer $CF_ACCESS_TOKEN\" | \\ grep -i \"x-ratelimit\" # Output: # X-RateLimit-Limit: 1000 # X-RateLimit-Remaining: 247 # X-RateLimit-Reset: 1706112400 # Monitor rate limit usage cloudflow metrics query \\ --metric rate_limit_remaining \\ --workspace ws_abc123 \\ --last 1h \\ --interval 1m ``` #### Handling Rate Limits in Code **Python example with retry logic:** ```python import time import requests def cloudflow_api_call_with_retry(url, headers, max_retries=3): for attempt in range(max_retries): response = requests.get(url, headers=headers) if response.status_code == 429: retry_after = int(response.headers.get('Retry-After', 60)) print(f\"Rate limited.",
              "score": null
            },
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_4",
              "content": "### Error Response Format ```json { \"error\": { \"code\": \"invalid_parameter\", \"message\": \"The 'limit' parameter must be between 1 and 100\", \"field\": \"limit\", \"request_id\": \"req_8k3m9x2p\" } } ``` ### HTTP Status Codes | Status Code | Description | Common Causes | |------------|-------------|---------------| | 400 | Bad Request | Invalid parameters, malformed JSON, validation errors | | 401 | Unauthorized | Missing or invalid authentication credentials | | 403 | Forbidden | Insufficient permissions for requested resource | | 404 | Not Found | Resource does not exist or has been deleted | | 429 | Too Many Requests | Rate limit exceeded, retry after specified period | | 500 | Internal Server Error | Unexpected server error, contact support if persists | | 502 | Bad Gateway | Temporary service issue, retry with exponential backoff | | 503 | Service Unavailable | Scheduled maintenance or temporary outage | ### Error Codes CloudFlow returns specific error codes to help you identify and resolve issues: - `invalid_parameter`: One or more request parameters are invalid - `missing_required_field`: Required field is missing from request body - `authentication_failed`: Invalid API key or token - `insufficient_permissions`: User lacks required scope or permission - `resource_not_found`: Requested resource does not exist - `rate_limit_exceeded`: Too many requests, see rate limiting section - `workflow_execution_failed`: Workflow execution encountered an error - `invalid_json`: Request body contains malformed JSON - `duplicate_resource`: Resource with same identifier already exists - `quota_exceeded`: Account quota limit reached ### Error Handling Best Practices **Python Example:** ```python import requests import time def make_api_request(url, headers, max_retries=3): for attempt in range(max_retries): try: response = requests.get(url, headers=headers) if response.status_code == 200: return response.json() elif response.status_code == 429: # Rate limit exceeded retry_after = int(response.headers.get('X-RateLimit-Reset', 60)) print(f\"Rate limited. Waiting {retry_after} seconds...\") time.sleep(retry_after) continue elif response.status_code >= 500: # Server error - retry with exponential backoff wait_time = 2 ** attempt print(f\"Server error. Retrying in {wait_time} seconds...\") time.sleep(wait_time) continue else: # Client error - don't retry error_data = response.json() raise Exception(f\"API Error: {error_data['error']['message']}\") except requests.exceptions.RequestException as e: print(f\"Request failed: {e}\") if attempt < max_retries - 1: time.sleep(2 ** attempt) else: raise raise Exception(\"Max retries exceeded\") ``` ## Webhooks CloudFlow can send webhook notifications when specific events occur in your workflows.",
              "score": null
            },
            {
              "doc_id": "user_guide",
              "chunk_id": "user_guide_fix_7",
              "content": "Handle Errors Gracefully Always implement error handling for external API calls and database operations: ```yaml - id: fetch_data action: http_request config: url: \"https://api.example.com/data\" retry: max_attempts: 3 backoff_type: exponential on_error: - id: log_error action: database_query config: query: \"INSERT INTO error_log (workflow_id, error) VALUES ($1, $2)\" parameters: - \"{{workflow.id}}\" - \"{{error.message}}\" ``` ### 3. Use Secrets for Sensitive Data Never hardcode API keys, passwords, or tokens in workflows: **Bad:** ```yaml headers: Authorization: \"Bearer sk_live_abc123xyz789\" ``` **Good:** ```yaml headers: Authorization: \"Bearer {{secrets.API_TOKEN}}\" ``` Store secrets in **Settings** > **Secrets** with encryption at rest. ### 4. Validate Input Data Always validate trigger data before processing: ```yaml - id: validate_input action: javascript code: | const required_fields = ['email', 'name', 'order_id']; for (const field of required_fields) { if (!input[field]) { throw new Error(`Missing required field: ${field}`); } } // Validate email format const email_regex = /^[^\\s@]+@[^\\s@]+\\.[^\\s@]+$/; if (!email_regex.test(input.email)) { throw new Error('Invalid email format'); } return { validated: true }; ``` ### 5. Use Idempotency Keys For operations that shouldn't be repeated (payments, record creation), use idempotency keys: ```yaml - id: create_charge action: http_request config: url: \"https://api.stripe.com/v1/charges\" method: POST headers: Idempotency-Key: \"{{workflow.id}}-{{execution.id}}\" body: amount: \"{{amount}}\" ``` ### 6. Monitor and Log Add logging steps for important workflow milestones: ```yaml - id: log_start action: database_query config: query: \"INSERT INTO workflow_audit (execution_id, step, timestamp) VALUES ($1, $2, $3)\" parameters: - \"{{execution.id}}\" - \"workflow_started\" - \"{{now()}}\" ``` ### 7. Keep Workflows Modular Break complex workflows into smaller, reusable components: - Use sub-workflows for repeated logic - Trigger child workflows via webhooks - Share common configurations via templates ### 8. Test Thoroughly Before activating a workflow: 1. Use test mode with sample data 2. Verify all actions execute correctly 3. Test error handling paths 4. Review execution logs 5. Start with a limited scope (e.g., test channel, small dataset) ### 9. Document Your Workflows Add descriptions to workflows and steps: ```yaml workflow: name: \"Daily Sales Report\" description: | Generates a daily sales report and distributes it to the sales team. Runs at 8:00 AM EST Monday-Friday. Data source: PostgreSQL sales database Recipients: sales-team@company.com steps: - id: fetch_sales name: \"Fetch Yesterday's Sales\" description: \"Query database for all completed orders from previous day\" ``` ### 10.",
              "score": null
            }
          ],
          "latency_ms": 1004.1639560004114,
          "query_id": "realistic_018",
          "dimension": "negation",
          "query": "Why doesn't my workflow retry after failing?"
        },
        {
          "key_facts": [
            "helm repo add cloudflow https://charts.cloudflow.io"
          ],
          "found_facts": [
            "helm repo add cloudflow https://charts.cloudflow.io"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "deployment_guide"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_1",
              "content": "create cluster -f cluster-config.yaml ``` ### Storage Configuration Install the EBS CSI driver for persistent volumes: ```bash eksctl create addon --name aws-ebs-csi-driver \\ --cluster cloudflow-production \\ --service-account-role-arn arn:aws:iam::ACCOUNT_ID:role/AmazonEKS_EBS_CSI_DriverRole \\ --force ``` --- ## Kubernetes Deployment ### Namespace Setup Create the CloudFlow namespace and configure resource quotas: ```yaml # namespace.yaml apiVersion: v1 kind: Namespace metadata: name: cloudflow-prod labels: name: cloudflow-prod environment: production --- apiVersion: v1 kind: ResourceQuota metadata: name: cloudflow-quota namespace: cloudflow-prod spec: hard: requests.cpu: \"50\" requests.memory: 100Gi persistentvolumeclaims: \"10\" services.loadbalancers: \"2\" ``` Apply the namespace configuration: ```bash kubectl apply -f namespace.yaml ``` ### Environment Configuration CloudFlow requires the following environment variables: | Variable | Description | Example | Required | |----------|-------------|---------|----------| | `DATABASE_URL` | PostgreSQL connection string | `postgresql://user:pass@host:5432/cloudflow` | Yes | | `REDIS_URL` | Redis connection string | `redis://redis-master.cloudflow-prod.svc.cluster.local:6379` | Yes | | `JWT_SECRET` | Secret key for JWT token signing | `<generated-secret-256-bit>` | Yes | | `LOG_LEVEL` | Application log verbosity | `info`, `debug`, `warn`, `error` | No (default: `info`) | | `API_PORT` | API server port | `3000` | No (default: `3000`) | | `WORKER_CONCURRENCY` | Number of concurrent workers | `10` | No (default: `5`) | | `SESSION_SECRET` | Session encryption key | `<generated-secret-256-bit>` | Yes | | `AWS_REGION` | AWS region for services | `us-east-1` | Yes | | `S3_BUCKET` | S3 bucket for file storage | `cloudflow-prod-storage` | Yes | | `SMTP_HOST` | Email server hostname | `smtp.sendgrid.net` | No | | `SMTP_PORT` | Email server port | `587` | No | | `METRICS_ENABLED` | Enable Prometheus metrics | `true` | No (default: `false`) | Create a Kubernetes secret for sensitive variables: ```bash kubectl create secret generic cloudflow-secrets \\ --namespace cloudflow-prod \\ --from-literal=DATABASE_URL=\"postgresql://cloudflow:$(cat db-password.txt)@pgbouncer.cloudflow-prod.svc.cluster.local:5432/cloudflow\" \\ --from-literal=REDIS_URL=\"redis://redis-master.cloudflow-prod.svc.cluster.local:6379\" \\ --from-literal=JWT_SECRET=\"$(openssl rand -base64 32)\" \\ --from-literal=SESSION_SECRET=\"$(openssl rand -base64 32)\" ``` ### Helm Chart Deployment Add the CloudFlow Helm repository: ```bash helm repo add cloudflow https://charts.cloudflow.io helm repo update ``` Create a values file for production configuration: ```yaml # values-production.yaml replicaCount: 3 image: repository: 123456789012.dkr.ecr.us-east-1.amazonaws.com/cloudflow tag: \"2.4.0\" pullPolicy: IfNotPresent resources: limits: cpu: 2000m memory: 4Gi requests: cpu: 2000m memory: 4Gi autoscaling: enabled: true minReplicas: 3 maxReplicas: 10 targetCPUUtilizationPercentage: 70 targetMemoryUtilizationPercentage: 80 service: type: ClusterIP port: 80 targetPort: 3000 annotations: service.beta.kubernetes.io/aws-load-balancer-type: \"nlb\" ingress: enabled: true className: nginx annotations: cert-manager.io/cluster-issuer: \"letsencrypt-prod\" nginx.ingress.kubernetes.io/ssl-redirect: \"true\" hosts: - host: api.cloudflow.io paths: - path: / pathType: Prefix tls:",
              "score": null
            },
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_3",
              "content": "Deploy PostgreSQL using the Bitnami Helm chart: ```yaml # postgres-values.yaml global: postgresql: auth: username: cloudflow database: cloudflow existingSecret: postgres-credentials image: tag: \"14.10.0\" primary: resources: limits: cpu: 4000m memory: 8Gi requests: cpu: 2000m memory: 4Gi persistence: enabled: true size: 100Gi storageClass: gp3 extendedConfiguration: | max_connections = 100 shared_buffers = 2GB effective_cache_size = 6GB maintenance_work_mem = 512MB checkpoint_completion_target = 0.9 wal_buffers = 16MB default_statistics_target = 100 random_page_cost = 1.1 effective_io_concurrency = 200 work_mem = 10MB min_wal_size = 1GB max_wal_size = 4GB readReplicas: replicaCount: 2 resources: limits: cpu: 2000m memory: 4Gi requests: cpu: 1000m memory: 2Gi metrics: enabled: true serviceMonitor: enabled: true ``` Install PostgreSQL: ```bash # Create password secret kubectl create secret generic postgres-credentials \\ --namespace cloudflow-prod \\ --from-literal=postgres-password=\"$(openssl rand -base64 32)\" \\ --from-literal=password=\"$(openssl rand -base64 32)\" # Install PostgreSQL helm install postgresql bitnami/postgresql \\ --namespace cloudflow-prod \\ --values postgres-values.yaml \\ --wait ``` ### PgBouncer Configuration Deploy PgBouncer for connection pooling to handle up to 100 connections efficiently: ```yaml # pgbouncer.yaml apiVersion: v1 kind: ConfigMap metadata: name: pgbouncer-config namespace: cloudflow-prod data: pgbouncer.ini: | [databases] cloudflow = host=postgresql.cloudflow-prod.svc.cluster.local port=5432 dbname=cloudflow [pgbouncer] listen_addr = 0.0.0.0 listen_port = 5432 auth_type = md5 auth_file = /etc/pgbouncer/userlist.txt pool_mode = transaction max_client_conn = 1000 default_pool_size = 25 reserve_pool_size = 5 reserve_pool_timeout = 3 max_db_connections = 100 max_user_connections = 100 server_lifetime = 3600 server_idle_timeout = 600 log_connections = 1 log_disconnections = 1 log_pooler_errors = 1 --- apiVersion: apps/v1 kind: Deployment metadata: name: pgbouncer namespace: cloudflow-prod spec: replicas: 2 selector: matchLabels: app: pgbouncer template: metadata: labels: app: pgbouncer spec: containers: - name: pgbouncer image: pgbouncer/pgbouncer:1.21.0 ports: - containerPort: 5432 resources: requests: cpu: 500m memory: 512Mi limits: cpu: 1000m memory: 1Gi volumeMounts: - name: config mountPath: /etc/pgbouncer volumes: - name: config configMap: name: pgbouncer-config --- apiVersion: v1 kind: Service metadata: name: pgbouncer namespace: cloudflow-prod spec: selector: app: pgbouncer ports: - port: 5432 targetPort: 5432 type: ClusterIP ``` Apply the PgBouncer configuration: ```bash kubectl apply -f pgbouncer.yaml ``` ### Database Migrations Run database migrations before deploying new versions: ```bash # Create migration job kubectl create job cloudflow-migrate-$(date +%s) \\ --namespace cloudflow-prod \\ --image=123456789012.dkr.ecr.us-east-1.amazonaws.com/cloudflow:2.4.0 \\ -- npm run migrate # Monitor migration progress kubectl logs -f job/cloudflow-migrate-<timestamp> -n cloudflow-prod ``` --- ## Monitoring and Observability ### Prometheus Setup Install Prometheus using the kube-prometheus-stack: ```bash helm install prometheus prometheus-community/kube-prometheus-stack \\ --namespace monitoring \\ --create-namespace \\ --set prometheus.prometheusSpec.serviceMonitorSelectorNilUsesHelmValues=false \\",
              "score": null
            },
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_0",
              "content": "# CloudFlow Platform - Deployment and Operations Guide **Version:** 2.4.0 **Last Updated:** January 2026 **Target Environment:** Production (AWS EKS) ## Table of Contents 1. [Overview](#overview) 2. [Prerequisites](#prerequisites) 3. [Infrastructure Setup](#infrastructure-setup) 4. [Kubernetes Deployment](#kubernetes-deployment) 5. [Database Configuration](#database-configuration) 6. [Monitoring and Observability](#monitoring-and-observability) 7. [Backup and Disaster Recovery](#backup-and-disaster-recovery) 8. [Scaling and Performance](#scaling-and-performance) 9. [Security and Compliance](#security-and-compliance) 10. [Troubleshooting](#troubleshooting) --- ## Overview CloudFlow is a cloud-native workflow orchestration platform designed for high-availability production environments. This guide provides comprehensive instructions for deploying and operating CloudFlow on Amazon EKS (Elastic Kubernetes Service). ### Architecture Summary CloudFlow consists of the following components: - **API Server**: REST API for workflow management (Node.js/Express) - **Worker Service**: Background job processor (Node.js) - **Scheduler**: Cron-based task scheduler (Node.js) - **PostgreSQL**: Primary data store (version 14) - **Redis**: Cache and message queue (version 7.0) - **PgBouncer**: Database connection pooler ### Deployment Model - **Namespace**: `cloudflow-prod` - **Cluster**: AWS EKS 1.28 - **Region**: us-east-1 (primary), us-west-2 (disaster recovery) - **High Availability**: Multi-AZ deployment across 3 availability zones --- ## Prerequisites Before beginning the deployment process, ensure you have the following: ### Required Tools - `kubectl` (v1.28 or later) - `helm` (v3.12 or later) - `aws-cli` (v2.13 or later) - `eksctl` (v0.165 or later) - `terraform` (v1.6 or later) - for infrastructure provisioning ### Access Requirements - AWS account with appropriate IAM permissions - EKS cluster admin access - Container registry access (ECR) - Domain name and SSL certificates - Secrets management access (AWS Secrets Manager or Vault) ### Network Requirements - VPC with at least 3 public and 3 private subnets - NAT Gateway configured - Security groups allowing: - Ingress: HTTPS (443), HTTP (80) - Internal: PostgreSQL (5432), Redis (6379) - Monitoring: Prometheus (9090), Grafana (3000) --- ## Infrastructure Setup ### EKS Cluster Creation Use the following `eksctl` configuration to create the EKS cluster: ```yaml # cluster-config.yaml apiVersion: eksctl.io/v1alpha5 kind: ClusterConfig metadata: name: cloudflow-production region: us-east-1 version: \"1.28\" availabilityZones: - us-east-1a - us-east-1b - us-east-1c vpc: cidr: 10.0.0.0/16 nat: gateway: HighlyAvailable managedNodeGroups: - name: cloudflow-workers instanceType: m5.xlarge minSize: 3 maxSize: 10 desiredCapacity: 5 volumeSize: 100 privateNetworking: true labels: role: worker environment: production tags: Environment: production Application: cloudflow iam: withAddonPolicies: ebs: true efs: true albIngress: true cloudWatch: true cloudWatch: clusterLogging: enableTypes: - api - audit - authenticator - controllerManager - scheduler ``` Create the cluster: ```bash eksctl",
              "score": null
            },
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_2",
              "content": "- secretName: cloudflow-tls hosts: - api.cloudflow.io healthCheck: enabled: true livenessProbe: path: /health initialDelaySeconds: 30 periodSeconds: 10 timeoutSeconds: 5 failureThreshold: 3 readinessProbe: path: /ready initialDelaySeconds: 10 periodSeconds: 5 timeoutSeconds: 3 failureThreshold: 3 env: - name: NODE_ENV value: \"production\" - name: LOG_LEVEL value: \"info\" - name: API_PORT value: \"3000\" - name: WORKER_CONCURRENCY value: \"10\" - name: AWS_REGION value: \"us-east-1\" - name: METRICS_ENABLED value: \"true\" envFrom: - secretRef: name: cloudflow-secrets podAnnotations: prometheus.io/scrape: \"true\" prometheus.io/port: \"9090\" prometheus.io/path: \"/metrics\" podSecurityContext: runAsNonRoot: true runAsUser: 1000 fsGroup: 1000 securityContext: allowPrivilegeEscalation: false capabilities: drop: - ALL readOnlyRootFilesystem: true persistence: enabled: true storageClass: gp3 accessMode: ReadWriteOnce size: 50Gi redis: enabled: true architecture: replication auth: enabled: true master: persistence: size: 20Gi replica: replicaCount: 2 persistence: size: 20Gi ``` Deploy CloudFlow using Helm: ```bash helm install cloudflow cloudflow/cloudflow \\ --namespace cloudflow-prod \\ --values values-production.yaml \\ --wait \\ --timeout 10m ``` ### Deployment Verification Verify the deployment status: ```bash # Check pod status kubectl get pods -n cloudflow-prod # Expected output: # NAME READY STATUS RESTARTS AGE # cloudflow-api-7d8f9c5b6d-4xk2p 1/1 Running 0 2m # cloudflow-api-7d8f9c5b6d-9hj5m 1/1 Running 0 2m # cloudflow-api-7d8f9c5b6d-tn8wq 1/1 Running 0 2m # Check service endpoints kubectl get svc -n cloudflow-prod # Check ingress kubectl get ingress -n cloudflow-prod ``` Test health endpoints: ```bash # Port forward for testing kubectl port-forward -n cloudflow-prod svc/cloudflow-api 8080:80 # Test health endpoint curl http://localhost:8080/health # Expected: {\"status\":\"healthy\",\"timestamp\":\"2026-01-24T10:30:00Z\"} # Test readiness endpoint curl http://localhost:8080/ready # Expected: {\"status\":\"ready\",\"dependencies\":{\"database\":\"connected\",\"redis\":\"connected\"}} ``` --- ## Database Configuration ### PostgreSQL Setup CloudFlow uses PostgreSQL 14 as its primary data store.",
              "score": null
            },
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_5",
              "content": "Configure log aggregation: ```bash # Install Fluent Bit for log forwarding helm install fluent-bit fluent/fluent-bit \\ --namespace logging \\ --create-namespace \\ --set cloudWatch.enabled=true \\ --set cloudWatch.region=us-east-1 \\ --set cloudWatch.logGroupName=/aws/eks/cloudflow-production/application ``` Query logs using CloudWatch Insights: ```sql fields @timestamp, @message, level, requestId, userId | filter namespace = \"cloudflow-prod\" | filter level = \"error\" | sort @timestamp desc | limit 100 ``` --- ## Backup and Disaster Recovery ### Database Backup Strategy CloudFlow implements a comprehensive backup strategy with the following retention policy: - **Daily snapshots**: Retained for 30 days - **Weekly backups**: Retained for 90 days - **Monthly backups**: Retained for 1 year #### Automated Backup with Velero Install Velero for cluster-wide backups: ```bash velero install \\ --provider aws \\ --plugins velero/velero-plugin-for-aws:v1.8.0 \\ --bucket cloudflow-velero-backups \\ --backup-location-config region=us-east-1 \\ --snapshot-location-config region=us-east-1 \\ --secret-file ./credentials-velero ``` Create a daily backup schedule: ```yaml # backup-schedule.yaml apiVersion: velero.io/v1 kind: Schedule metadata: name: cloudflow-daily-backup namespace: velero spec: schedule: \"0 2 * * *\" # 2 AM UTC daily template: includedNamespaces: - cloudflow-prod ttl: 720h0m0s # 30 days storageLocation: default snapshotVolumes: true ``` Apply the backup schedule: ```bash kubectl apply -f backup-schedule.yaml ``` #### PostgreSQL Backup Configure PostgreSQL continuous archiving with WAL-G: ```bash # Create backup cronjob kubectl apply -f - <<EOF apiVersion: batch/v1 kind: CronJob metadata: name: postgres-backup namespace: cloudflow-prod spec: schedule: \"0 1 * * *\" # 1 AM UTC daily jobTemplate: spec: template: spec: containers: - name: backup image: postgres:14 env: - name: PGPASSWORD valueFrom: secretKeyRef: name: postgres-credentials key: password command: - /bin/bash - -c - | TIMESTAMP=\\$(date +%Y%m%d_%H%M%S) pg_dump -h postgresql.cloudflow-prod.svc.cluster.local \\ -U cloudflow -d cloudflow \\ | gzip > /backups/cloudflow_\\${TIMESTAMP}.sql.gz aws s3 cp /backups/cloudflow_\\${TIMESTAMP}.sql.gz \\ s3://cloudflow-db-backups/daily/ volumeMounts: - name: backup-storage mountPath: /backups volumes: - name: backup-storage emptyDir: {} restartPolicy: OnFailure EOF ``` ### Disaster Recovery Procedure In case of catastrophic failure, follow these steps to restore CloudFlow: 1. **Restore EKS Cluster** (if necessary): ```bash eksctl create cluster -f cluster-config.yaml ``` 2. **Restore Velero Backup**: ```bash # List available backups velero backup get # Restore from backup velero restore create --from-backup cloudflow-daily-backup-20260124 ``` 3. **Restore Database**: ```bash # Download backup from S3 aws s3 cp s3://cloudflow-db-backups/daily/cloudflow_20260124_010000.sql.gz . # Restore database gunzip -c cloudflow_20260124_010000.sql.gz | \\ psql -h postgresql.cloudflow-prod.svc.cluster.local -U cloudflow -d cloudflow ``` 4.",
              "score": null
            }
          ],
          "latency_ms": 831.3378889979504,
          "query_id": "realistic_019",
          "dimension": "original",
          "query": "What Helm chart repository should I use for deployment?"
        },
        {
          "key_facts": [
            "helm repo add cloudflow https://charts.cloudflow.io"
          ],
          "found_facts": [
            "helm repo add cloudflow https://charts.cloudflow.io"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "deployment_guide"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_1",
              "content": "create cluster -f cluster-config.yaml ``` ### Storage Configuration Install the EBS CSI driver for persistent volumes: ```bash eksctl create addon --name aws-ebs-csi-driver \\ --cluster cloudflow-production \\ --service-account-role-arn arn:aws:iam::ACCOUNT_ID:role/AmazonEKS_EBS_CSI_DriverRole \\ --force ``` --- ## Kubernetes Deployment ### Namespace Setup Create the CloudFlow namespace and configure resource quotas: ```yaml # namespace.yaml apiVersion: v1 kind: Namespace metadata: name: cloudflow-prod labels: name: cloudflow-prod environment: production --- apiVersion: v1 kind: ResourceQuota metadata: name: cloudflow-quota namespace: cloudflow-prod spec: hard: requests.cpu: \"50\" requests.memory: 100Gi persistentvolumeclaims: \"10\" services.loadbalancers: \"2\" ``` Apply the namespace configuration: ```bash kubectl apply -f namespace.yaml ``` ### Environment Configuration CloudFlow requires the following environment variables: | Variable | Description | Example | Required | |----------|-------------|---------|----------| | `DATABASE_URL` | PostgreSQL connection string | `postgresql://user:pass@host:5432/cloudflow` | Yes | | `REDIS_URL` | Redis connection string | `redis://redis-master.cloudflow-prod.svc.cluster.local:6379` | Yes | | `JWT_SECRET` | Secret key for JWT token signing | `<generated-secret-256-bit>` | Yes | | `LOG_LEVEL` | Application log verbosity | `info`, `debug`, `warn`, `error` | No (default: `info`) | | `API_PORT` | API server port | `3000` | No (default: `3000`) | | `WORKER_CONCURRENCY` | Number of concurrent workers | `10` | No (default: `5`) | | `SESSION_SECRET` | Session encryption key | `<generated-secret-256-bit>` | Yes | | `AWS_REGION` | AWS region for services | `us-east-1` | Yes | | `S3_BUCKET` | S3 bucket for file storage | `cloudflow-prod-storage` | Yes | | `SMTP_HOST` | Email server hostname | `smtp.sendgrid.net` | No | | `SMTP_PORT` | Email server port | `587` | No | | `METRICS_ENABLED` | Enable Prometheus metrics | `true` | No (default: `false`) | Create a Kubernetes secret for sensitive variables: ```bash kubectl create secret generic cloudflow-secrets \\ --namespace cloudflow-prod \\ --from-literal=DATABASE_URL=\"postgresql://cloudflow:$(cat db-password.txt)@pgbouncer.cloudflow-prod.svc.cluster.local:5432/cloudflow\" \\ --from-literal=REDIS_URL=\"redis://redis-master.cloudflow-prod.svc.cluster.local:6379\" \\ --from-literal=JWT_SECRET=\"$(openssl rand -base64 32)\" \\ --from-literal=SESSION_SECRET=\"$(openssl rand -base64 32)\" ``` ### Helm Chart Deployment Add the CloudFlow Helm repository: ```bash helm repo add cloudflow https://charts.cloudflow.io helm repo update ``` Create a values file for production configuration: ```yaml # values-production.yaml replicaCount: 3 image: repository: 123456789012.dkr.ecr.us-east-1.amazonaws.com/cloudflow tag: \"2.4.0\" pullPolicy: IfNotPresent resources: limits: cpu: 2000m memory: 4Gi requests: cpu: 2000m memory: 4Gi autoscaling: enabled: true minReplicas: 3 maxReplicas: 10 targetCPUUtilizationPercentage: 70 targetMemoryUtilizationPercentage: 80 service: type: ClusterIP port: 80 targetPort: 3000 annotations: service.beta.kubernetes.io/aws-load-balancer-type: \"nlb\" ingress: enabled: true className: nginx annotations: cert-manager.io/cluster-issuer: \"letsencrypt-prod\" nginx.ingress.kubernetes.io/ssl-redirect: \"true\" hosts: - host: api.cloudflow.io paths: - path: / pathType: Prefix tls:",
              "score": null
            },
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_2",
              "content": "- secretName: cloudflow-tls hosts: - api.cloudflow.io healthCheck: enabled: true livenessProbe: path: /health initialDelaySeconds: 30 periodSeconds: 10 timeoutSeconds: 5 failureThreshold: 3 readinessProbe: path: /ready initialDelaySeconds: 10 periodSeconds: 5 timeoutSeconds: 3 failureThreshold: 3 env: - name: NODE_ENV value: \"production\" - name: LOG_LEVEL value: \"info\" - name: API_PORT value: \"3000\" - name: WORKER_CONCURRENCY value: \"10\" - name: AWS_REGION value: \"us-east-1\" - name: METRICS_ENABLED value: \"true\" envFrom: - secretRef: name: cloudflow-secrets podAnnotations: prometheus.io/scrape: \"true\" prometheus.io/port: \"9090\" prometheus.io/path: \"/metrics\" podSecurityContext: runAsNonRoot: true runAsUser: 1000 fsGroup: 1000 securityContext: allowPrivilegeEscalation: false capabilities: drop: - ALL readOnlyRootFilesystem: true persistence: enabled: true storageClass: gp3 accessMode: ReadWriteOnce size: 50Gi redis: enabled: true architecture: replication auth: enabled: true master: persistence: size: 20Gi replica: replicaCount: 2 persistence: size: 20Gi ``` Deploy CloudFlow using Helm: ```bash helm install cloudflow cloudflow/cloudflow \\ --namespace cloudflow-prod \\ --values values-production.yaml \\ --wait \\ --timeout 10m ``` ### Deployment Verification Verify the deployment status: ```bash # Check pod status kubectl get pods -n cloudflow-prod # Expected output: # NAME READY STATUS RESTARTS AGE # cloudflow-api-7d8f9c5b6d-4xk2p 1/1 Running 0 2m # cloudflow-api-7d8f9c5b6d-9hj5m 1/1 Running 0 2m # cloudflow-api-7d8f9c5b6d-tn8wq 1/1 Running 0 2m # Check service endpoints kubectl get svc -n cloudflow-prod # Check ingress kubectl get ingress -n cloudflow-prod ``` Test health endpoints: ```bash # Port forward for testing kubectl port-forward -n cloudflow-prod svc/cloudflow-api 8080:80 # Test health endpoint curl http://localhost:8080/health # Expected: {\"status\":\"healthy\",\"timestamp\":\"2026-01-24T10:30:00Z\"} # Test readiness endpoint curl http://localhost:8080/ready # Expected: {\"status\":\"ready\",\"dependencies\":{\"database\":\"connected\",\"redis\":\"connected\"}} ``` --- ## Database Configuration ### PostgreSQL Setup CloudFlow uses PostgreSQL 14 as its primary data store.",
              "score": null
            },
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_3",
              "content": "Deploy PostgreSQL using the Bitnami Helm chart: ```yaml # postgres-values.yaml global: postgresql: auth: username: cloudflow database: cloudflow existingSecret: postgres-credentials image: tag: \"14.10.0\" primary: resources: limits: cpu: 4000m memory: 8Gi requests: cpu: 2000m memory: 4Gi persistence: enabled: true size: 100Gi storageClass: gp3 extendedConfiguration: | max_connections = 100 shared_buffers = 2GB effective_cache_size = 6GB maintenance_work_mem = 512MB checkpoint_completion_target = 0.9 wal_buffers = 16MB default_statistics_target = 100 random_page_cost = 1.1 effective_io_concurrency = 200 work_mem = 10MB min_wal_size = 1GB max_wal_size = 4GB readReplicas: replicaCount: 2 resources: limits: cpu: 2000m memory: 4Gi requests: cpu: 1000m memory: 2Gi metrics: enabled: true serviceMonitor: enabled: true ``` Install PostgreSQL: ```bash # Create password secret kubectl create secret generic postgres-credentials \\ --namespace cloudflow-prod \\ --from-literal=postgres-password=\"$(openssl rand -base64 32)\" \\ --from-literal=password=\"$(openssl rand -base64 32)\" # Install PostgreSQL helm install postgresql bitnami/postgresql \\ --namespace cloudflow-prod \\ --values postgres-values.yaml \\ --wait ``` ### PgBouncer Configuration Deploy PgBouncer for connection pooling to handle up to 100 connections efficiently: ```yaml # pgbouncer.yaml apiVersion: v1 kind: ConfigMap metadata: name: pgbouncer-config namespace: cloudflow-prod data: pgbouncer.ini: | [databases] cloudflow = host=postgresql.cloudflow-prod.svc.cluster.local port=5432 dbname=cloudflow [pgbouncer] listen_addr = 0.0.0.0 listen_port = 5432 auth_type = md5 auth_file = /etc/pgbouncer/userlist.txt pool_mode = transaction max_client_conn = 1000 default_pool_size = 25 reserve_pool_size = 5 reserve_pool_timeout = 3 max_db_connections = 100 max_user_connections = 100 server_lifetime = 3600 server_idle_timeout = 600 log_connections = 1 log_disconnections = 1 log_pooler_errors = 1 --- apiVersion: apps/v1 kind: Deployment metadata: name: pgbouncer namespace: cloudflow-prod spec: replicas: 2 selector: matchLabels: app: pgbouncer template: metadata: labels: app: pgbouncer spec: containers: - name: pgbouncer image: pgbouncer/pgbouncer:1.21.0 ports: - containerPort: 5432 resources: requests: cpu: 500m memory: 512Mi limits: cpu: 1000m memory: 1Gi volumeMounts: - name: config mountPath: /etc/pgbouncer volumes: - name: config configMap: name: pgbouncer-config --- apiVersion: v1 kind: Service metadata: name: pgbouncer namespace: cloudflow-prod spec: selector: app: pgbouncer ports: - port: 5432 targetPort: 5432 type: ClusterIP ``` Apply the PgBouncer configuration: ```bash kubectl apply -f pgbouncer.yaml ``` ### Database Migrations Run database migrations before deploying new versions: ```bash # Create migration job kubectl create job cloudflow-migrate-$(date +%s) \\ --namespace cloudflow-prod \\ --image=123456789012.dkr.ecr.us-east-1.amazonaws.com/cloudflow:2.4.0 \\ -- npm run migrate # Monitor migration progress kubectl logs -f job/cloudflow-migrate-<timestamp> -n cloudflow-prod ``` --- ## Monitoring and Observability ### Prometheus Setup Install Prometheus using the kube-prometheus-stack: ```bash helm install prometheus prometheus-community/kube-prometheus-stack \\ --namespace monitoring \\ --create-namespace \\ --set prometheus.prometheusSpec.serviceMonitorSelectorNilUsesHelmValues=false \\",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_8",
              "content": "Leverage caching:** ```bash # Enable client-side caching export CLOUDFLOW_CACHE_ENABLED=true export CLOUDFLOW_CACHE_TTL=300 # Cache workflow metadata cloudflow workflows list --use-cache --cache-ttl 600 ``` --- ## Log Analysis & Debugging ### Accessing CloudFlow Logs #### Kubernetes Deployments ```bash # List all CloudFlow pods kubectl get pods -n cloudflow # Tail logs from API server kubectl logs -f -n cloudflow deployment/cloudflow-api --tail=100 # Get logs from specific pod kubectl logs -n cloudflow cloudflow-api-7d4f6b8c9d-x7k2m # Get logs from previous crashed pod kubectl logs -n cloudflow cloudflow-api-7d4f6b8c9d-x7k2m --previous # Get logs from all pods in deployment kubectl logs -n cloudflow deployment/cloudflow-api --all-containers=true # Stream logs from multiple pods kubectl logs -n cloudflow -l app=cloudflow-api -f --max-log-requests=10 ``` #### Log Levels CloudFlow supports the following log levels: - `TRACE` - Very detailed debugging information - `DEBUG` - Detailed debugging information - `INFO` - Informational messages (default) - `WARN` - Warning messages - `ERROR` - Error messages - `FATAL` - Fatal errors causing shutdown **Changing log levels:** ```bash # Set global log level cloudflow config set logging.level DEBUG # Set log level for specific component cloudflow config set logging.components.database DEBUG cloudflow config set logging.components.auth INFO cloudflow config set logging.components.workflows TRACE # Temporary log level increase (resets after 1 hour) cloudflow debug set-log-level DEBUG --duration 3600 # View current log configuration cloudflow config get logging --format json ``` ### Grep Patterns for Common Issues #### Authentication Failures ```bash # Find all authentication errors kubectl logs -n cloudflow deployment/cloudflow-api | \\ grep -i \"authentication\\|401\\|unauthorized\" | \\ tail -n 50 # Find JWT validation failures kubectl logs -n cloudflow deployment/cloudflow-api | \\ grep -E \"JWT|token.*invalid|signature.*failed\" # Find clock skew issues kubectl logs -n cloudflow deployment/cloudflow-api | \\ grep -i \"clock skew\\|time.*sync\\|nbf\\|exp\" ``` #### Database Errors ```bash # Find database connection errors kubectl logs -n cloudflow deployment/cloudflow-api | \\ grep -E \"connection.*pool|could not connect|database.*timeout\" # Find slow queries kubectl logs -n cloudflow deployment/cloudflow-api | \\ grep \"slow query\" | \\ awk '{print $NF}' | \\ sort -n | \\ tail -n 20 # Find deadlock errors kubectl logs -n cloudflow deployment/cloudflow-api | \\ grep -i \"deadlock detected\" ``` #### Workflow Execution Errors ```bash # Find workflow timeout errors kubectl logs -n cloudflow deployment/cloudflow-workflow-engine | \\ grep -E \"timeout|exceeded.*3600\" # Find workflow retry attempts kubectl logs -n cloudflow deployment/cloudflow-workflow-engine | \\ grep -E \"retry attempt [0-9]|retrying",
              "score": null
            },
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_5",
              "content": "Configure log aggregation: ```bash # Install Fluent Bit for log forwarding helm install fluent-bit fluent/fluent-bit \\ --namespace logging \\ --create-namespace \\ --set cloudWatch.enabled=true \\ --set cloudWatch.region=us-east-1 \\ --set cloudWatch.logGroupName=/aws/eks/cloudflow-production/application ``` Query logs using CloudWatch Insights: ```sql fields @timestamp, @message, level, requestId, userId | filter namespace = \"cloudflow-prod\" | filter level = \"error\" | sort @timestamp desc | limit 100 ``` --- ## Backup and Disaster Recovery ### Database Backup Strategy CloudFlow implements a comprehensive backup strategy with the following retention policy: - **Daily snapshots**: Retained for 30 days - **Weekly backups**: Retained for 90 days - **Monthly backups**: Retained for 1 year #### Automated Backup with Velero Install Velero for cluster-wide backups: ```bash velero install \\ --provider aws \\ --plugins velero/velero-plugin-for-aws:v1.8.0 \\ --bucket cloudflow-velero-backups \\ --backup-location-config region=us-east-1 \\ --snapshot-location-config region=us-east-1 \\ --secret-file ./credentials-velero ``` Create a daily backup schedule: ```yaml # backup-schedule.yaml apiVersion: velero.io/v1 kind: Schedule metadata: name: cloudflow-daily-backup namespace: velero spec: schedule: \"0 2 * * *\" # 2 AM UTC daily template: includedNamespaces: - cloudflow-prod ttl: 720h0m0s # 30 days storageLocation: default snapshotVolumes: true ``` Apply the backup schedule: ```bash kubectl apply -f backup-schedule.yaml ``` #### PostgreSQL Backup Configure PostgreSQL continuous archiving with WAL-G: ```bash # Create backup cronjob kubectl apply -f - <<EOF apiVersion: batch/v1 kind: CronJob metadata: name: postgres-backup namespace: cloudflow-prod spec: schedule: \"0 1 * * *\" # 1 AM UTC daily jobTemplate: spec: template: spec: containers: - name: backup image: postgres:14 env: - name: PGPASSWORD valueFrom: secretKeyRef: name: postgres-credentials key: password command: - /bin/bash - -c - | TIMESTAMP=\\$(date +%Y%m%d_%H%M%S) pg_dump -h postgresql.cloudflow-prod.svc.cluster.local \\ -U cloudflow -d cloudflow \\ | gzip > /backups/cloudflow_\\${TIMESTAMP}.sql.gz aws s3 cp /backups/cloudflow_\\${TIMESTAMP}.sql.gz \\ s3://cloudflow-db-backups/daily/ volumeMounts: - name: backup-storage mountPath: /backups volumes: - name: backup-storage emptyDir: {} restartPolicy: OnFailure EOF ``` ### Disaster Recovery Procedure In case of catastrophic failure, follow these steps to restore CloudFlow: 1. **Restore EKS Cluster** (if necessary): ```bash eksctl create cluster -f cluster-config.yaml ``` 2. **Restore Velero Backup**: ```bash # List available backups velero backup get # Restore from backup velero restore create --from-backup cloudflow-daily-backup-20260124 ``` 3. **Restore Database**: ```bash # Download backup from S3 aws s3 cp s3://cloudflow-db-backups/daily/cloudflow_20260124_010000.sql.gz . # Restore database gunzip -c cloudflow_20260124_010000.sql.gz | \\ psql -h postgresql.cloudflow-prod.svc.cluster.local -U cloudflow -d cloudflow ``` 4.",
              "score": null
            }
          ],
          "latency_ms": 872.0130459987558,
          "query_id": "realistic_019",
          "dimension": "synonym",
          "query": "Where is the CloudFlow Helm chart hosted?"
        },
        {
          "key_facts": [
            "helm repo add cloudflow https://charts.cloudflow.io"
          ],
          "found_facts": [
            "helm repo add cloudflow https://charts.cloudflow.io"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "deployment_guide"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_1",
              "content": "create cluster -f cluster-config.yaml ``` ### Storage Configuration Install the EBS CSI driver for persistent volumes: ```bash eksctl create addon --name aws-ebs-csi-driver \\ --cluster cloudflow-production \\ --service-account-role-arn arn:aws:iam::ACCOUNT_ID:role/AmazonEKS_EBS_CSI_DriverRole \\ --force ``` --- ## Kubernetes Deployment ### Namespace Setup Create the CloudFlow namespace and configure resource quotas: ```yaml # namespace.yaml apiVersion: v1 kind: Namespace metadata: name: cloudflow-prod labels: name: cloudflow-prod environment: production --- apiVersion: v1 kind: ResourceQuota metadata: name: cloudflow-quota namespace: cloudflow-prod spec: hard: requests.cpu: \"50\" requests.memory: 100Gi persistentvolumeclaims: \"10\" services.loadbalancers: \"2\" ``` Apply the namespace configuration: ```bash kubectl apply -f namespace.yaml ``` ### Environment Configuration CloudFlow requires the following environment variables: | Variable | Description | Example | Required | |----------|-------------|---------|----------| | `DATABASE_URL` | PostgreSQL connection string | `postgresql://user:pass@host:5432/cloudflow` | Yes | | `REDIS_URL` | Redis connection string | `redis://redis-master.cloudflow-prod.svc.cluster.local:6379` | Yes | | `JWT_SECRET` | Secret key for JWT token signing | `<generated-secret-256-bit>` | Yes | | `LOG_LEVEL` | Application log verbosity | `info`, `debug`, `warn`, `error` | No (default: `info`) | | `API_PORT` | API server port | `3000` | No (default: `3000`) | | `WORKER_CONCURRENCY` | Number of concurrent workers | `10` | No (default: `5`) | | `SESSION_SECRET` | Session encryption key | `<generated-secret-256-bit>` | Yes | | `AWS_REGION` | AWS region for services | `us-east-1` | Yes | | `S3_BUCKET` | S3 bucket for file storage | `cloudflow-prod-storage` | Yes | | `SMTP_HOST` | Email server hostname | `smtp.sendgrid.net` | No | | `SMTP_PORT` | Email server port | `587` | No | | `METRICS_ENABLED` | Enable Prometheus metrics | `true` | No (default: `false`) | Create a Kubernetes secret for sensitive variables: ```bash kubectl create secret generic cloudflow-secrets \\ --namespace cloudflow-prod \\ --from-literal=DATABASE_URL=\"postgresql://cloudflow:$(cat db-password.txt)@pgbouncer.cloudflow-prod.svc.cluster.local:5432/cloudflow\" \\ --from-literal=REDIS_URL=\"redis://redis-master.cloudflow-prod.svc.cluster.local:6379\" \\ --from-literal=JWT_SECRET=\"$(openssl rand -base64 32)\" \\ --from-literal=SESSION_SECRET=\"$(openssl rand -base64 32)\" ``` ### Helm Chart Deployment Add the CloudFlow Helm repository: ```bash helm repo add cloudflow https://charts.cloudflow.io helm repo update ``` Create a values file for production configuration: ```yaml # values-production.yaml replicaCount: 3 image: repository: 123456789012.dkr.ecr.us-east-1.amazonaws.com/cloudflow tag: \"2.4.0\" pullPolicy: IfNotPresent resources: limits: cpu: 2000m memory: 4Gi requests: cpu: 2000m memory: 4Gi autoscaling: enabled: true minReplicas: 3 maxReplicas: 10 targetCPUUtilizationPercentage: 70 targetMemoryUtilizationPercentage: 80 service: type: ClusterIP port: 80 targetPort: 3000 annotations: service.beta.kubernetes.io/aws-load-balancer-type: \"nlb\" ingress: enabled: true className: nginx annotations: cert-manager.io/cluster-issuer: \"letsencrypt-prod\" nginx.ingress.kubernetes.io/ssl-redirect: \"true\" hosts: - host: api.cloudflow.io paths: - path: / pathType: Prefix tls:",
              "score": null
            },
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_2",
              "content": "- secretName: cloudflow-tls hosts: - api.cloudflow.io healthCheck: enabled: true livenessProbe: path: /health initialDelaySeconds: 30 periodSeconds: 10 timeoutSeconds: 5 failureThreshold: 3 readinessProbe: path: /ready initialDelaySeconds: 10 periodSeconds: 5 timeoutSeconds: 3 failureThreshold: 3 env: - name: NODE_ENV value: \"production\" - name: LOG_LEVEL value: \"info\" - name: API_PORT value: \"3000\" - name: WORKER_CONCURRENCY value: \"10\" - name: AWS_REGION value: \"us-east-1\" - name: METRICS_ENABLED value: \"true\" envFrom: - secretRef: name: cloudflow-secrets podAnnotations: prometheus.io/scrape: \"true\" prometheus.io/port: \"9090\" prometheus.io/path: \"/metrics\" podSecurityContext: runAsNonRoot: true runAsUser: 1000 fsGroup: 1000 securityContext: allowPrivilegeEscalation: false capabilities: drop: - ALL readOnlyRootFilesystem: true persistence: enabled: true storageClass: gp3 accessMode: ReadWriteOnce size: 50Gi redis: enabled: true architecture: replication auth: enabled: true master: persistence: size: 20Gi replica: replicaCount: 2 persistence: size: 20Gi ``` Deploy CloudFlow using Helm: ```bash helm install cloudflow cloudflow/cloudflow \\ --namespace cloudflow-prod \\ --values values-production.yaml \\ --wait \\ --timeout 10m ``` ### Deployment Verification Verify the deployment status: ```bash # Check pod status kubectl get pods -n cloudflow-prod # Expected output: # NAME READY STATUS RESTARTS AGE # cloudflow-api-7d8f9c5b6d-4xk2p 1/1 Running 0 2m # cloudflow-api-7d8f9c5b6d-9hj5m 1/1 Running 0 2m # cloudflow-api-7d8f9c5b6d-tn8wq 1/1 Running 0 2m # Check service endpoints kubectl get svc -n cloudflow-prod # Check ingress kubectl get ingress -n cloudflow-prod ``` Test health endpoints: ```bash # Port forward for testing kubectl port-forward -n cloudflow-prod svc/cloudflow-api 8080:80 # Test health endpoint curl http://localhost:8080/health # Expected: {\"status\":\"healthy\",\"timestamp\":\"2026-01-24T10:30:00Z\"} # Test readiness endpoint curl http://localhost:8080/ready # Expected: {\"status\":\"ready\",\"dependencies\":{\"database\":\"connected\",\"redis\":\"connected\"}} ``` --- ## Database Configuration ### PostgreSQL Setup CloudFlow uses PostgreSQL 14 as its primary data store.",
              "score": null
            },
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_0",
              "content": "# CloudFlow Platform - Deployment and Operations Guide **Version:** 2.4.0 **Last Updated:** January 2026 **Target Environment:** Production (AWS EKS) ## Table of Contents 1. [Overview](#overview) 2. [Prerequisites](#prerequisites) 3. [Infrastructure Setup](#infrastructure-setup) 4. [Kubernetes Deployment](#kubernetes-deployment) 5. [Database Configuration](#database-configuration) 6. [Monitoring and Observability](#monitoring-and-observability) 7. [Backup and Disaster Recovery](#backup-and-disaster-recovery) 8. [Scaling and Performance](#scaling-and-performance) 9. [Security and Compliance](#security-and-compliance) 10. [Troubleshooting](#troubleshooting) --- ## Overview CloudFlow is a cloud-native workflow orchestration platform designed for high-availability production environments. This guide provides comprehensive instructions for deploying and operating CloudFlow on Amazon EKS (Elastic Kubernetes Service). ### Architecture Summary CloudFlow consists of the following components: - **API Server**: REST API for workflow management (Node.js/Express) - **Worker Service**: Background job processor (Node.js) - **Scheduler**: Cron-based task scheduler (Node.js) - **PostgreSQL**: Primary data store (version 14) - **Redis**: Cache and message queue (version 7.0) - **PgBouncer**: Database connection pooler ### Deployment Model - **Namespace**: `cloudflow-prod` - **Cluster**: AWS EKS 1.28 - **Region**: us-east-1 (primary), us-west-2 (disaster recovery) - **High Availability**: Multi-AZ deployment across 3 availability zones --- ## Prerequisites Before beginning the deployment process, ensure you have the following: ### Required Tools - `kubectl` (v1.28 or later) - `helm` (v3.12 or later) - `aws-cli` (v2.13 or later) - `eksctl` (v0.165 or later) - `terraform` (v1.6 or later) - for infrastructure provisioning ### Access Requirements - AWS account with appropriate IAM permissions - EKS cluster admin access - Container registry access (ECR) - Domain name and SSL certificates - Secrets management access (AWS Secrets Manager or Vault) ### Network Requirements - VPC with at least 3 public and 3 private subnets - NAT Gateway configured - Security groups allowing: - Ingress: HTTPS (443), HTTP (80) - Internal: PostgreSQL (5432), Redis (6379) - Monitoring: Prometheus (9090), Grafana (3000) --- ## Infrastructure Setup ### EKS Cluster Creation Use the following `eksctl` configuration to create the EKS cluster: ```yaml # cluster-config.yaml apiVersion: eksctl.io/v1alpha5 kind: ClusterConfig metadata: name: cloudflow-production region: us-east-1 version: \"1.28\" availabilityZones: - us-east-1a - us-east-1b - us-east-1c vpc: cidr: 10.0.0.0/16 nat: gateway: HighlyAvailable managedNodeGroups: - name: cloudflow-workers instanceType: m5.xlarge minSize: 3 maxSize: 10 desiredCapacity: 5 volumeSize: 100 privateNetworking: true labels: role: worker environment: production tags: Environment: production Application: cloudflow iam: withAddonPolicies: ebs: true efs: true albIngress: true cloudWatch: true cloudWatch: clusterLogging: enableTypes: - api - audit - authenticator - controllerManager - scheduler ``` Create the cluster: ```bash eksctl",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_1",
              "content": "**Technology**: Node.js with Express.js framework **Replicas**: 12 pods (production), auto-scaling 8-20 based on CPU **Resource Allocation**: 2 vCPU, 4GB RAM per pod **Key Responsibilities**: - JWT token validation (delegated to Auth Service for initial validation) - Rate limiting: 1000 requests per minute per API key (sliding window) - Request/response transformation and validation using JSON Schema - Routing to downstream services based on URL path patterns - CORS handling for web clients - Request/response logging and correlation ID injection **Critical Endpoints**: - `POST /api/v1/workflows` - Create new workflow - `GET /api/v1/workflows/:id` - Retrieve workflow status - `POST /api/v1/workflows/:id/execute` - Trigger workflow execution - `GET /api/v1/workflows/:id/history` - Get execution history **Dependencies**: - Auth Service (for token validation) - Redis (for rate limiting counters) - All downstream microservices **Performance Targets**: - P50 latency: < 50ms - P99 latency: < 200ms - Throughput: 10,000 RPS sustained --- ### Auth Service **Purpose**: Centralized authentication and authorization service handling user identity, token generation, and permission validation. **Technology**: Go with gRPC for internal communication, REST for external **Replicas**: 8 pods (production), auto-scaling 6-12 **Resource Allocation**: 1 vCPU, 2GB RAM per pod **Key Responsibilities**: - User authentication via multiple providers (OAuth2, SAML, local credentials) - JWT token generation and validation (RS256 algorithm) - Role-based access control (RBAC) with fine-grained permissions - Session management with Redis-backed storage - API key generation and validation for service accounts - MFA enforcement for administrative operations **Authentication Flow**: ``` Client Request \u2192 API Gateway \u2192 Auth Service \u2502 \u251c\u2500 Validate credentials \u251c\u2500 Check MFA if required \u251c\u2500 Generate JWT (15min expiry) \u251c\u2500 Generate refresh token (7 days) \u2514\u2500 Store session in Redis ``` **Token Structure**: - Access Token: JWT with 15-minute expiry - Refresh Token: Opaque token with 7-day expiry, stored in PostgreSQL - Claims: user_id, email, roles[], permissions[], tenant_id **Security Features**: - Password hashing: Argon2id with 64MB memory, 4 iterations - Token rotation on refresh to prevent replay attacks - Brute force protection: 5 failed attempts \u2192 15-minute lockout - Secrets stored in HashiCorp Vault, rotated every 90 days **Performance Targets**: - Token validation: < 10ms (P99) - Token generation: < 50ms (P99) - Throughput: 5,000 RPS for validation operations --- ### Workflow Engine **Purpose**: Core orchestration service that executes workflow definitions, manages state transitions, and coordinates task execution across distributed systems.",
              "score": null
            },
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_3",
              "content": "Deploy PostgreSQL using the Bitnami Helm chart: ```yaml # postgres-values.yaml global: postgresql: auth: username: cloudflow database: cloudflow existingSecret: postgres-credentials image: tag: \"14.10.0\" primary: resources: limits: cpu: 4000m memory: 8Gi requests: cpu: 2000m memory: 4Gi persistence: enabled: true size: 100Gi storageClass: gp3 extendedConfiguration: | max_connections = 100 shared_buffers = 2GB effective_cache_size = 6GB maintenance_work_mem = 512MB checkpoint_completion_target = 0.9 wal_buffers = 16MB default_statistics_target = 100 random_page_cost = 1.1 effective_io_concurrency = 200 work_mem = 10MB min_wal_size = 1GB max_wal_size = 4GB readReplicas: replicaCount: 2 resources: limits: cpu: 2000m memory: 4Gi requests: cpu: 1000m memory: 2Gi metrics: enabled: true serviceMonitor: enabled: true ``` Install PostgreSQL: ```bash # Create password secret kubectl create secret generic postgres-credentials \\ --namespace cloudflow-prod \\ --from-literal=postgres-password=\"$(openssl rand -base64 32)\" \\ --from-literal=password=\"$(openssl rand -base64 32)\" # Install PostgreSQL helm install postgresql bitnami/postgresql \\ --namespace cloudflow-prod \\ --values postgres-values.yaml \\ --wait ``` ### PgBouncer Configuration Deploy PgBouncer for connection pooling to handle up to 100 connections efficiently: ```yaml # pgbouncer.yaml apiVersion: v1 kind: ConfigMap metadata: name: pgbouncer-config namespace: cloudflow-prod data: pgbouncer.ini: | [databases] cloudflow = host=postgresql.cloudflow-prod.svc.cluster.local port=5432 dbname=cloudflow [pgbouncer] listen_addr = 0.0.0.0 listen_port = 5432 auth_type = md5 auth_file = /etc/pgbouncer/userlist.txt pool_mode = transaction max_client_conn = 1000 default_pool_size = 25 reserve_pool_size = 5 reserve_pool_timeout = 3 max_db_connections = 100 max_user_connections = 100 server_lifetime = 3600 server_idle_timeout = 600 log_connections = 1 log_disconnections = 1 log_pooler_errors = 1 --- apiVersion: apps/v1 kind: Deployment metadata: name: pgbouncer namespace: cloudflow-prod spec: replicas: 2 selector: matchLabels: app: pgbouncer template: metadata: labels: app: pgbouncer spec: containers: - name: pgbouncer image: pgbouncer/pgbouncer:1.21.0 ports: - containerPort: 5432 resources: requests: cpu: 500m memory: 512Mi limits: cpu: 1000m memory: 1Gi volumeMounts: - name: config mountPath: /etc/pgbouncer volumes: - name: config configMap: name: pgbouncer-config --- apiVersion: v1 kind: Service metadata: name: pgbouncer namespace: cloudflow-prod spec: selector: app: pgbouncer ports: - port: 5432 targetPort: 5432 type: ClusterIP ``` Apply the PgBouncer configuration: ```bash kubectl apply -f pgbouncer.yaml ``` ### Database Migrations Run database migrations before deploying new versions: ```bash # Create migration job kubectl create job cloudflow-migrate-$(date +%s) \\ --namespace cloudflow-prod \\ --image=123456789012.dkr.ecr.us-east-1.amazonaws.com/cloudflow:2.4.0 \\ -- npm run migrate # Monitor migration progress kubectl logs -f job/cloudflow-migrate-<timestamp> -n cloudflow-prod ``` --- ## Monitoring and Observability ### Prometheus Setup Install Prometheus using the kube-prometheus-stack: ```bash helm install prometheus prometheus-community/kube-prometheus-stack \\ --namespace monitoring \\ --create-namespace \\ --set prometheus.prometheusSpec.serviceMonitorSelectorNilUsesHelmValues=false \\",
              "score": null
            }
          ],
          "latency_ms": 1106.7904690025898,
          "query_id": "realistic_019",
          "dimension": "problem",
          "query": "helm repo add is failing, what's the correct URL?"
        },
        {
          "key_facts": [
            "helm repo add cloudflow https://charts.cloudflow.io"
          ],
          "found_facts": [
            "helm repo add cloudflow https://charts.cloudflow.io"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "deployment_guide"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_1",
              "content": "create cluster -f cluster-config.yaml ``` ### Storage Configuration Install the EBS CSI driver for persistent volumes: ```bash eksctl create addon --name aws-ebs-csi-driver \\ --cluster cloudflow-production \\ --service-account-role-arn arn:aws:iam::ACCOUNT_ID:role/AmazonEKS_EBS_CSI_DriverRole \\ --force ``` --- ## Kubernetes Deployment ### Namespace Setup Create the CloudFlow namespace and configure resource quotas: ```yaml # namespace.yaml apiVersion: v1 kind: Namespace metadata: name: cloudflow-prod labels: name: cloudflow-prod environment: production --- apiVersion: v1 kind: ResourceQuota metadata: name: cloudflow-quota namespace: cloudflow-prod spec: hard: requests.cpu: \"50\" requests.memory: 100Gi persistentvolumeclaims: \"10\" services.loadbalancers: \"2\" ``` Apply the namespace configuration: ```bash kubectl apply -f namespace.yaml ``` ### Environment Configuration CloudFlow requires the following environment variables: | Variable | Description | Example | Required | |----------|-------------|---------|----------| | `DATABASE_URL` | PostgreSQL connection string | `postgresql://user:pass@host:5432/cloudflow` | Yes | | `REDIS_URL` | Redis connection string | `redis://redis-master.cloudflow-prod.svc.cluster.local:6379` | Yes | | `JWT_SECRET` | Secret key for JWT token signing | `<generated-secret-256-bit>` | Yes | | `LOG_LEVEL` | Application log verbosity | `info`, `debug`, `warn`, `error` | No (default: `info`) | | `API_PORT` | API server port | `3000` | No (default: `3000`) | | `WORKER_CONCURRENCY` | Number of concurrent workers | `10` | No (default: `5`) | | `SESSION_SECRET` | Session encryption key | `<generated-secret-256-bit>` | Yes | | `AWS_REGION` | AWS region for services | `us-east-1` | Yes | | `S3_BUCKET` | S3 bucket for file storage | `cloudflow-prod-storage` | Yes | | `SMTP_HOST` | Email server hostname | `smtp.sendgrid.net` | No | | `SMTP_PORT` | Email server port | `587` | No | | `METRICS_ENABLED` | Enable Prometheus metrics | `true` | No (default: `false`) | Create a Kubernetes secret for sensitive variables: ```bash kubectl create secret generic cloudflow-secrets \\ --namespace cloudflow-prod \\ --from-literal=DATABASE_URL=\"postgresql://cloudflow:$(cat db-password.txt)@pgbouncer.cloudflow-prod.svc.cluster.local:5432/cloudflow\" \\ --from-literal=REDIS_URL=\"redis://redis-master.cloudflow-prod.svc.cluster.local:6379\" \\ --from-literal=JWT_SECRET=\"$(openssl rand -base64 32)\" \\ --from-literal=SESSION_SECRET=\"$(openssl rand -base64 32)\" ``` ### Helm Chart Deployment Add the CloudFlow Helm repository: ```bash helm repo add cloudflow https://charts.cloudflow.io helm repo update ``` Create a values file for production configuration: ```yaml # values-production.yaml replicaCount: 3 image: repository: 123456789012.dkr.ecr.us-east-1.amazonaws.com/cloudflow tag: \"2.4.0\" pullPolicy: IfNotPresent resources: limits: cpu: 2000m memory: 4Gi requests: cpu: 2000m memory: 4Gi autoscaling: enabled: true minReplicas: 3 maxReplicas: 10 targetCPUUtilizationPercentage: 70 targetMemoryUtilizationPercentage: 80 service: type: ClusterIP port: 80 targetPort: 3000 annotations: service.beta.kubernetes.io/aws-load-balancer-type: \"nlb\" ingress: enabled: true className: nginx annotations: cert-manager.io/cluster-issuer: \"letsencrypt-prod\" nginx.ingress.kubernetes.io/ssl-redirect: \"true\" hosts: - host: api.cloudflow.io paths: - path: / pathType: Prefix tls:",
              "score": null
            },
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_3",
              "content": "Deploy PostgreSQL using the Bitnami Helm chart: ```yaml # postgres-values.yaml global: postgresql: auth: username: cloudflow database: cloudflow existingSecret: postgres-credentials image: tag: \"14.10.0\" primary: resources: limits: cpu: 4000m memory: 8Gi requests: cpu: 2000m memory: 4Gi persistence: enabled: true size: 100Gi storageClass: gp3 extendedConfiguration: | max_connections = 100 shared_buffers = 2GB effective_cache_size = 6GB maintenance_work_mem = 512MB checkpoint_completion_target = 0.9 wal_buffers = 16MB default_statistics_target = 100 random_page_cost = 1.1 effective_io_concurrency = 200 work_mem = 10MB min_wal_size = 1GB max_wal_size = 4GB readReplicas: replicaCount: 2 resources: limits: cpu: 2000m memory: 4Gi requests: cpu: 1000m memory: 2Gi metrics: enabled: true serviceMonitor: enabled: true ``` Install PostgreSQL: ```bash # Create password secret kubectl create secret generic postgres-credentials \\ --namespace cloudflow-prod \\ --from-literal=postgres-password=\"$(openssl rand -base64 32)\" \\ --from-literal=password=\"$(openssl rand -base64 32)\" # Install PostgreSQL helm install postgresql bitnami/postgresql \\ --namespace cloudflow-prod \\ --values postgres-values.yaml \\ --wait ``` ### PgBouncer Configuration Deploy PgBouncer for connection pooling to handle up to 100 connections efficiently: ```yaml # pgbouncer.yaml apiVersion: v1 kind: ConfigMap metadata: name: pgbouncer-config namespace: cloudflow-prod data: pgbouncer.ini: | [databases] cloudflow = host=postgresql.cloudflow-prod.svc.cluster.local port=5432 dbname=cloudflow [pgbouncer] listen_addr = 0.0.0.0 listen_port = 5432 auth_type = md5 auth_file = /etc/pgbouncer/userlist.txt pool_mode = transaction max_client_conn = 1000 default_pool_size = 25 reserve_pool_size = 5 reserve_pool_timeout = 3 max_db_connections = 100 max_user_connections = 100 server_lifetime = 3600 server_idle_timeout = 600 log_connections = 1 log_disconnections = 1 log_pooler_errors = 1 --- apiVersion: apps/v1 kind: Deployment metadata: name: pgbouncer namespace: cloudflow-prod spec: replicas: 2 selector: matchLabels: app: pgbouncer template: metadata: labels: app: pgbouncer spec: containers: - name: pgbouncer image: pgbouncer/pgbouncer:1.21.0 ports: - containerPort: 5432 resources: requests: cpu: 500m memory: 512Mi limits: cpu: 1000m memory: 1Gi volumeMounts: - name: config mountPath: /etc/pgbouncer volumes: - name: config configMap: name: pgbouncer-config --- apiVersion: v1 kind: Service metadata: name: pgbouncer namespace: cloudflow-prod spec: selector: app: pgbouncer ports: - port: 5432 targetPort: 5432 type: ClusterIP ``` Apply the PgBouncer configuration: ```bash kubectl apply -f pgbouncer.yaml ``` ### Database Migrations Run database migrations before deploying new versions: ```bash # Create migration job kubectl create job cloudflow-migrate-$(date +%s) \\ --namespace cloudflow-prod \\ --image=123456789012.dkr.ecr.us-east-1.amazonaws.com/cloudflow:2.4.0 \\ -- npm run migrate # Monitor migration progress kubectl logs -f job/cloudflow-migrate-<timestamp> -n cloudflow-prod ``` --- ## Monitoring and Observability ### Prometheus Setup Install Prometheus using the kube-prometheus-stack: ```bash helm install prometheus prometheus-community/kube-prometheus-stack \\ --namespace monitoring \\ --create-namespace \\ --set prometheus.prometheusSpec.serviceMonitorSelectorNilUsesHelmValues=false \\",
              "score": null
            },
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_7",
              "content": "podSelector: matchLabels: app: cloudflow policyTypes: - Ingress - Egress ingress: - from: - namespaceSelector: matchLabels: name: cloudflow-prod - podSelector: matchLabels: app: nginx-ingress ports: - protocol: TCP port: 3000 egress: - to: - podSelector: matchLabels: app: postgresql ports: - protocol: TCP port: 5432 - to: - podSelector: matchLabels: app: redis ports: - protocol: TCP port: 6379 ``` ### Pod Security Standards Enforce Pod Security Standards at the namespace level: ```bash kubectl label namespace cloudflow-prod \\ pod-security.kubernetes.io/enforce=restricted \\ pod-security.kubernetes.io/audit=restricted \\ pod-security.kubernetes.io/warn=restricted ``` ### Secrets Management Use AWS Secrets Manager for sensitive credentials: ```bash # Install External Secrets Operator helm install external-secrets external-secrets/external-secrets \\ --namespace external-secrets-system \\ --create-namespace # Create SecretStore kubectl apply -f - <<EOF apiVersion: external-secrets.io/v1beta1 kind: SecretStore metadata: name: aws-secrets-manager namespace: cloudflow-prod spec: provider: aws: service: SecretsManager region: us-east-1 auth: jwt: serviceAccountRef: name: external-secrets-sa EOF ``` --- ## Troubleshooting ### Common Issues #### Pods in CrashLoopBackOff **Symptoms**: Pods continuously restart **Diagnosis**: ```bash kubectl logs -n cloudflow-prod <pod-name> --previous kubectl describe pod -n cloudflow-prod <pod-name> ``` **Common Causes**: - Database connection failure - Invalid environment variables - Insufficient resources #### High Memory Usage **Symptoms**: Pods being OOMKilled **Diagnosis**: ```bash kubectl top pods -n cloudflow-prod ``` **Resolution**: - Increase memory limits in deployment - Check for memory leaks in application logs - Review heap dump for Node.js processes #### Database Connection Pool Exhausted **Symptoms**: \"Too many connections\" errors **Diagnosis**: ```bash # Check PgBouncer stats kubectl exec -it pgbouncer-xxx -n cloudflow-prod -- \\ psql -p 5432 -U pgbouncer pgbouncer -c \"SHOW POOLS;\" ``` **Resolution**: - Increase `max_db_connections` in PgBouncer - Optimize application connection usage - Add more PostgreSQL replicas ### Support Contacts - **On-call Engineer**: pagerduty.com/cloudflow-oncall - **Slack Channel**: #cloudflow-operations - **Documentation**: https://docs.cloudflow.io - **Status Page**: https://status.cloudflow.io --- ## Appendix ### Useful Commands ```bash # View all resources in namespace kubectl get all -n cloudflow-prod # Check resource usage kubectl top pods -n cloudflow-prod kubectl top nodes # View logs kubectl logs -f deployment/cloudflow-api -n cloudflow-prod # Execute command in pod kubectl exec -it <pod-name> -n cloudflow-prod -- /bin/sh # Port forward for local testing kubectl port-forward svc/cloudflow-api 8080:80 -n cloudflow-prod # Rollback deployment helm rollback cloudflow -n cloudflow-prod # Update deployment helm upgrade cloudflow cloudflow/cloudflow \\ --namespace cloudflow-prod \\ --values values-production.yaml ``` ### Maintenance Windows Scheduled maintenance occurs during the following windows: - **Primary**: Sunday 02:00-06:00",
              "score": null
            },
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_5",
              "content": "Configure log aggregation: ```bash # Install Fluent Bit for log forwarding helm install fluent-bit fluent/fluent-bit \\ --namespace logging \\ --create-namespace \\ --set cloudWatch.enabled=true \\ --set cloudWatch.region=us-east-1 \\ --set cloudWatch.logGroupName=/aws/eks/cloudflow-production/application ``` Query logs using CloudWatch Insights: ```sql fields @timestamp, @message, level, requestId, userId | filter namespace = \"cloudflow-prod\" | filter level = \"error\" | sort @timestamp desc | limit 100 ``` --- ## Backup and Disaster Recovery ### Database Backup Strategy CloudFlow implements a comprehensive backup strategy with the following retention policy: - **Daily snapshots**: Retained for 30 days - **Weekly backups**: Retained for 90 days - **Monthly backups**: Retained for 1 year #### Automated Backup with Velero Install Velero for cluster-wide backups: ```bash velero install \\ --provider aws \\ --plugins velero/velero-plugin-for-aws:v1.8.0 \\ --bucket cloudflow-velero-backups \\ --backup-location-config region=us-east-1 \\ --snapshot-location-config region=us-east-1 \\ --secret-file ./credentials-velero ``` Create a daily backup schedule: ```yaml # backup-schedule.yaml apiVersion: velero.io/v1 kind: Schedule metadata: name: cloudflow-daily-backup namespace: velero spec: schedule: \"0 2 * * *\" # 2 AM UTC daily template: includedNamespaces: - cloudflow-prod ttl: 720h0m0s # 30 days storageLocation: default snapshotVolumes: true ``` Apply the backup schedule: ```bash kubectl apply -f backup-schedule.yaml ``` #### PostgreSQL Backup Configure PostgreSQL continuous archiving with WAL-G: ```bash # Create backup cronjob kubectl apply -f - <<EOF apiVersion: batch/v1 kind: CronJob metadata: name: postgres-backup namespace: cloudflow-prod spec: schedule: \"0 1 * * *\" # 1 AM UTC daily jobTemplate: spec: template: spec: containers: - name: backup image: postgres:14 env: - name: PGPASSWORD valueFrom: secretKeyRef: name: postgres-credentials key: password command: - /bin/bash - -c - | TIMESTAMP=\\$(date +%Y%m%d_%H%M%S) pg_dump -h postgresql.cloudflow-prod.svc.cluster.local \\ -U cloudflow -d cloudflow \\ | gzip > /backups/cloudflow_\\${TIMESTAMP}.sql.gz aws s3 cp /backups/cloudflow_\\${TIMESTAMP}.sql.gz \\ s3://cloudflow-db-backups/daily/ volumeMounts: - name: backup-storage mountPath: /backups volumes: - name: backup-storage emptyDir: {} restartPolicy: OnFailure EOF ``` ### Disaster Recovery Procedure In case of catastrophic failure, follow these steps to restore CloudFlow: 1. **Restore EKS Cluster** (if necessary): ```bash eksctl create cluster -f cluster-config.yaml ``` 2. **Restore Velero Backup**: ```bash # List available backups velero backup get # Restore from backup velero restore create --from-backup cloudflow-daily-backup-20260124 ``` 3. **Restore Database**: ```bash # Download backup from S3 aws s3 cp s3://cloudflow-db-backups/daily/cloudflow_20260124_010000.sql.gz . # Restore database gunzip -c cloudflow_20260124_010000.sql.gz | \\ psql -h postgresql.cloudflow-prod.svc.cluster.local -U cloudflow -d cloudflow ``` 4.",
              "score": null
            },
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_0",
              "content": "# CloudFlow Platform - Deployment and Operations Guide **Version:** 2.4.0 **Last Updated:** January 2026 **Target Environment:** Production (AWS EKS) ## Table of Contents 1. [Overview](#overview) 2. [Prerequisites](#prerequisites) 3. [Infrastructure Setup](#infrastructure-setup) 4. [Kubernetes Deployment](#kubernetes-deployment) 5. [Database Configuration](#database-configuration) 6. [Monitoring and Observability](#monitoring-and-observability) 7. [Backup and Disaster Recovery](#backup-and-disaster-recovery) 8. [Scaling and Performance](#scaling-and-performance) 9. [Security and Compliance](#security-and-compliance) 10. [Troubleshooting](#troubleshooting) --- ## Overview CloudFlow is a cloud-native workflow orchestration platform designed for high-availability production environments. This guide provides comprehensive instructions for deploying and operating CloudFlow on Amazon EKS (Elastic Kubernetes Service). ### Architecture Summary CloudFlow consists of the following components: - **API Server**: REST API for workflow management (Node.js/Express) - **Worker Service**: Background job processor (Node.js) - **Scheduler**: Cron-based task scheduler (Node.js) - **PostgreSQL**: Primary data store (version 14) - **Redis**: Cache and message queue (version 7.0) - **PgBouncer**: Database connection pooler ### Deployment Model - **Namespace**: `cloudflow-prod` - **Cluster**: AWS EKS 1.28 - **Region**: us-east-1 (primary), us-west-2 (disaster recovery) - **High Availability**: Multi-AZ deployment across 3 availability zones --- ## Prerequisites Before beginning the deployment process, ensure you have the following: ### Required Tools - `kubectl` (v1.28 or later) - `helm` (v3.12 or later) - `aws-cli` (v2.13 or later) - `eksctl` (v0.165 or later) - `terraform` (v1.6 or later) - for infrastructure provisioning ### Access Requirements - AWS account with appropriate IAM permissions - EKS cluster admin access - Container registry access (ECR) - Domain name and SSL certificates - Secrets management access (AWS Secrets Manager or Vault) ### Network Requirements - VPC with at least 3 public and 3 private subnets - NAT Gateway configured - Security groups allowing: - Ingress: HTTPS (443), HTTP (80) - Internal: PostgreSQL (5432), Redis (6379) - Monitoring: Prometheus (9090), Grafana (3000) --- ## Infrastructure Setup ### EKS Cluster Creation Use the following `eksctl` configuration to create the EKS cluster: ```yaml # cluster-config.yaml apiVersion: eksctl.io/v1alpha5 kind: ClusterConfig metadata: name: cloudflow-production region: us-east-1 version: \"1.28\" availabilityZones: - us-east-1a - us-east-1b - us-east-1c vpc: cidr: 10.0.0.0/16 nat: gateway: HighlyAvailable managedNodeGroups: - name: cloudflow-workers instanceType: m5.xlarge minSize: 3 maxSize: 10 desiredCapacity: 5 volumeSize: 100 privateNetworking: true labels: role: worker environment: production tags: Environment: production Application: cloudflow iam: withAddonPolicies: ebs: true efs: true albIngress: true cloudWatch: true cloudWatch: clusterLogging: enableTypes: - api - audit - authenticator - controllerManager - scheduler ``` Create the cluster: ```bash eksctl",
              "score": null
            }
          ],
          "latency_ms": 984.0307839986053,
          "query_id": "realistic_019",
          "dimension": "casual",
          "query": "helm repo"
        },
        {
          "key_facts": [
            "helm repo add cloudflow https://charts.cloudflow.io"
          ],
          "found_facts": [
            "helm repo add cloudflow https://charts.cloudflow.io"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "deployment_guide"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_2",
              "content": "- secretName: cloudflow-tls hosts: - api.cloudflow.io healthCheck: enabled: true livenessProbe: path: /health initialDelaySeconds: 30 periodSeconds: 10 timeoutSeconds: 5 failureThreshold: 3 readinessProbe: path: /ready initialDelaySeconds: 10 periodSeconds: 5 timeoutSeconds: 3 failureThreshold: 3 env: - name: NODE_ENV value: \"production\" - name: LOG_LEVEL value: \"info\" - name: API_PORT value: \"3000\" - name: WORKER_CONCURRENCY value: \"10\" - name: AWS_REGION value: \"us-east-1\" - name: METRICS_ENABLED value: \"true\" envFrom: - secretRef: name: cloudflow-secrets podAnnotations: prometheus.io/scrape: \"true\" prometheus.io/port: \"9090\" prometheus.io/path: \"/metrics\" podSecurityContext: runAsNonRoot: true runAsUser: 1000 fsGroup: 1000 securityContext: allowPrivilegeEscalation: false capabilities: drop: - ALL readOnlyRootFilesystem: true persistence: enabled: true storageClass: gp3 accessMode: ReadWriteOnce size: 50Gi redis: enabled: true architecture: replication auth: enabled: true master: persistence: size: 20Gi replica: replicaCount: 2 persistence: size: 20Gi ``` Deploy CloudFlow using Helm: ```bash helm install cloudflow cloudflow/cloudflow \\ --namespace cloudflow-prod \\ --values values-production.yaml \\ --wait \\ --timeout 10m ``` ### Deployment Verification Verify the deployment status: ```bash # Check pod status kubectl get pods -n cloudflow-prod # Expected output: # NAME READY STATUS RESTARTS AGE # cloudflow-api-7d8f9c5b6d-4xk2p 1/1 Running 0 2m # cloudflow-api-7d8f9c5b6d-9hj5m 1/1 Running 0 2m # cloudflow-api-7d8f9c5b6d-tn8wq 1/1 Running 0 2m # Check service endpoints kubectl get svc -n cloudflow-prod # Check ingress kubectl get ingress -n cloudflow-prod ``` Test health endpoints: ```bash # Port forward for testing kubectl port-forward -n cloudflow-prod svc/cloudflow-api 8080:80 # Test health endpoint curl http://localhost:8080/health # Expected: {\"status\":\"healthy\",\"timestamp\":\"2026-01-24T10:30:00Z\"} # Test readiness endpoint curl http://localhost:8080/ready # Expected: {\"status\":\"ready\",\"dependencies\":{\"database\":\"connected\",\"redis\":\"connected\"}} ``` --- ## Database Configuration ### PostgreSQL Setup CloudFlow uses PostgreSQL 14 as its primary data store.",
              "score": null
            },
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_1",
              "content": "create cluster -f cluster-config.yaml ``` ### Storage Configuration Install the EBS CSI driver for persistent volumes: ```bash eksctl create addon --name aws-ebs-csi-driver \\ --cluster cloudflow-production \\ --service-account-role-arn arn:aws:iam::ACCOUNT_ID:role/AmazonEKS_EBS_CSI_DriverRole \\ --force ``` --- ## Kubernetes Deployment ### Namespace Setup Create the CloudFlow namespace and configure resource quotas: ```yaml # namespace.yaml apiVersion: v1 kind: Namespace metadata: name: cloudflow-prod labels: name: cloudflow-prod environment: production --- apiVersion: v1 kind: ResourceQuota metadata: name: cloudflow-quota namespace: cloudflow-prod spec: hard: requests.cpu: \"50\" requests.memory: 100Gi persistentvolumeclaims: \"10\" services.loadbalancers: \"2\" ``` Apply the namespace configuration: ```bash kubectl apply -f namespace.yaml ``` ### Environment Configuration CloudFlow requires the following environment variables: | Variable | Description | Example | Required | |----------|-------------|---------|----------| | `DATABASE_URL` | PostgreSQL connection string | `postgresql://user:pass@host:5432/cloudflow` | Yes | | `REDIS_URL` | Redis connection string | `redis://redis-master.cloudflow-prod.svc.cluster.local:6379` | Yes | | `JWT_SECRET` | Secret key for JWT token signing | `<generated-secret-256-bit>` | Yes | | `LOG_LEVEL` | Application log verbosity | `info`, `debug`, `warn`, `error` | No (default: `info`) | | `API_PORT` | API server port | `3000` | No (default: `3000`) | | `WORKER_CONCURRENCY` | Number of concurrent workers | `10` | No (default: `5`) | | `SESSION_SECRET` | Session encryption key | `<generated-secret-256-bit>` | Yes | | `AWS_REGION` | AWS region for services | `us-east-1` | Yes | | `S3_BUCKET` | S3 bucket for file storage | `cloudflow-prod-storage` | Yes | | `SMTP_HOST` | Email server hostname | `smtp.sendgrid.net` | No | | `SMTP_PORT` | Email server port | `587` | No | | `METRICS_ENABLED` | Enable Prometheus metrics | `true` | No (default: `false`) | Create a Kubernetes secret for sensitive variables: ```bash kubectl create secret generic cloudflow-secrets \\ --namespace cloudflow-prod \\ --from-literal=DATABASE_URL=\"postgresql://cloudflow:$(cat db-password.txt)@pgbouncer.cloudflow-prod.svc.cluster.local:5432/cloudflow\" \\ --from-literal=REDIS_URL=\"redis://redis-master.cloudflow-prod.svc.cluster.local:6379\" \\ --from-literal=JWT_SECRET=\"$(openssl rand -base64 32)\" \\ --from-literal=SESSION_SECRET=\"$(openssl rand -base64 32)\" ``` ### Helm Chart Deployment Add the CloudFlow Helm repository: ```bash helm repo add cloudflow https://charts.cloudflow.io helm repo update ``` Create a values file for production configuration: ```yaml # values-production.yaml replicaCount: 3 image: repository: 123456789012.dkr.ecr.us-east-1.amazonaws.com/cloudflow tag: \"2.4.0\" pullPolicy: IfNotPresent resources: limits: cpu: 2000m memory: 4Gi requests: cpu: 2000m memory: 4Gi autoscaling: enabled: true minReplicas: 3 maxReplicas: 10 targetCPUUtilizationPercentage: 70 targetMemoryUtilizationPercentage: 80 service: type: ClusterIP port: 80 targetPort: 3000 annotations: service.beta.kubernetes.io/aws-load-balancer-type: \"nlb\" ingress: enabled: true className: nginx annotations: cert-manager.io/cluster-issuer: \"letsencrypt-prod\" nginx.ingress.kubernetes.io/ssl-redirect: \"true\" hosts: - host: api.cloudflow.io paths: - path: / pathType: Prefix tls:",
              "score": null
            },
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_0",
              "content": "# CloudFlow Platform - Deployment and Operations Guide **Version:** 2.4.0 **Last Updated:** January 2026 **Target Environment:** Production (AWS EKS) ## Table of Contents 1. [Overview](#overview) 2. [Prerequisites](#prerequisites) 3. [Infrastructure Setup](#infrastructure-setup) 4. [Kubernetes Deployment](#kubernetes-deployment) 5. [Database Configuration](#database-configuration) 6. [Monitoring and Observability](#monitoring-and-observability) 7. [Backup and Disaster Recovery](#backup-and-disaster-recovery) 8. [Scaling and Performance](#scaling-and-performance) 9. [Security and Compliance](#security-and-compliance) 10. [Troubleshooting](#troubleshooting) --- ## Overview CloudFlow is a cloud-native workflow orchestration platform designed for high-availability production environments. This guide provides comprehensive instructions for deploying and operating CloudFlow on Amazon EKS (Elastic Kubernetes Service). ### Architecture Summary CloudFlow consists of the following components: - **API Server**: REST API for workflow management (Node.js/Express) - **Worker Service**: Background job processor (Node.js) - **Scheduler**: Cron-based task scheduler (Node.js) - **PostgreSQL**: Primary data store (version 14) - **Redis**: Cache and message queue (version 7.0) - **PgBouncer**: Database connection pooler ### Deployment Model - **Namespace**: `cloudflow-prod` - **Cluster**: AWS EKS 1.28 - **Region**: us-east-1 (primary), us-west-2 (disaster recovery) - **High Availability**: Multi-AZ deployment across 3 availability zones --- ## Prerequisites Before beginning the deployment process, ensure you have the following: ### Required Tools - `kubectl` (v1.28 or later) - `helm` (v3.12 or later) - `aws-cli` (v2.13 or later) - `eksctl` (v0.165 or later) - `terraform` (v1.6 or later) - for infrastructure provisioning ### Access Requirements - AWS account with appropriate IAM permissions - EKS cluster admin access - Container registry access (ECR) - Domain name and SSL certificates - Secrets management access (AWS Secrets Manager or Vault) ### Network Requirements - VPC with at least 3 public and 3 private subnets - NAT Gateway configured - Security groups allowing: - Ingress: HTTPS (443), HTTP (80) - Internal: PostgreSQL (5432), Redis (6379) - Monitoring: Prometheus (9090), Grafana (3000) --- ## Infrastructure Setup ### EKS Cluster Creation Use the following `eksctl` configuration to create the EKS cluster: ```yaml # cluster-config.yaml apiVersion: eksctl.io/v1alpha5 kind: ClusterConfig metadata: name: cloudflow-production region: us-east-1 version: \"1.28\" availabilityZones: - us-east-1a - us-east-1b - us-east-1c vpc: cidr: 10.0.0.0/16 nat: gateway: HighlyAvailable managedNodeGroups: - name: cloudflow-workers instanceType: m5.xlarge minSize: 3 maxSize: 10 desiredCapacity: 5 volumeSize: 100 privateNetworking: true labels: role: worker environment: production tags: Environment: production Application: cloudflow iam: withAddonPolicies: ebs: true efs: true albIngress: true cloudWatch: true cloudWatch: clusterLogging: enableTypes: - api - audit - authenticator - controllerManager - scheduler ``` Create the cluster: ```bash eksctl",
              "score": null
            },
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_0",
              "content": "# CloudFlow API Reference Version 2.1.0 | Last Updated: January 2026 ## Overview The CloudFlow API is a RESTful service that enables developers to programmatically manage cloud workflows, data pipelines, and automation tasks. This documentation provides comprehensive details on authentication, endpoints, request/response formats, error handling, and best practices. **Base URL:** `https://api.cloudflow.io/v2` **API Status:** https://status.cloudflow.io ## Authentication CloudFlow supports three authentication methods to suit different use cases and security requirements. ### API Keys API keys provide simple authentication for server-to-server communication. Include your API key in the request header: ```bash curl -H \"X-API-Key: cf_live_a1b2c3d4e5f6g7h8i9j0\" \\ https://api.cloudflow.io/v2/workflows ``` **Security Notes:** - Never expose API keys in client-side code - Rotate keys every 90 days - Use separate keys for development and production environments ### OAuth 2.0 OAuth 2.0 is recommended for applications that access CloudFlow on behalf of users. We support the Authorization Code flow with PKCE. **Authorization Endpoint:** `https://auth.cloudflow.io/oauth/authorize` **Token Endpoint:** `https://auth.cloudflow.io/oauth/token` **Supported Scopes:** - `workflows:read` - Read workflow configurations - `workflows:write` - Create and modify workflows - `pipelines:read` - Read pipeline data - `pipelines:write` - Create and manage pipelines - `analytics:read` - Access analytics and metrics - `admin:full` - Full administrative access Example authorization request: ```bash curl -X POST https://auth.cloudflow.io/oauth/token \\ -H \"Content-Type: application/x-www-form-urlencoded\" \\ -d \"grant_type=authorization_code\" \\ -d \"code=AUTH_CODE_HERE\" \\ -d \"client_id=YOUR_CLIENT_ID\" \\ -d \"client_secret=YOUR_CLIENT_SECRET\" \\ -d \"redirect_uri=https://yourapp.com/callback\" ``` ### JWT Tokens For advanced use cases, CloudFlow supports JSON Web Tokens (JWT) with RS256 signing algorithm. JWTs must include the following claims: - `iss` (issuer): Your application identifier - `sub` (subject): User or service account ID - `aud` (audience): `https://api.cloudflow.io` - `exp` (expiration): Unix timestamp (max 3600 seconds from `iat`) - `iat` (issued at): Unix timestamp - `scope`: Space-separated list of requested scopes Example JWT header: ```python import jwt import time from cryptography.hazmat.primitives import serialization from cryptography.hazmat.backends import default_backend # Load your private key with open('private_key.pem', 'rb') as key_file: private_key = serialization.load_pem_private_key( key_file.read(), password=None, backend=default_backend() ) # Create JWT payload payload = { 'iss': 'your-app-id', 'sub': 'user-12345', 'aud': 'https://api.cloudflow.io', 'exp': int(time.time()) + 3600, 'iat': int(time.time()), 'scope': 'workflows:read workflows:write' } # Generate token token = jwt.encode(payload, private_key, algorithm='RS256') # Use in API request headers = {'Authorization': f'Bearer {token}'} ``` **Token Expiration:** All tokens expire after 3600 seconds (1 hour). Implement token refresh logic in your application.",
              "score": null
            },
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_3",
              "content": "Deploy PostgreSQL using the Bitnami Helm chart: ```yaml # postgres-values.yaml global: postgresql: auth: username: cloudflow database: cloudflow existingSecret: postgres-credentials image: tag: \"14.10.0\" primary: resources: limits: cpu: 4000m memory: 8Gi requests: cpu: 2000m memory: 4Gi persistence: enabled: true size: 100Gi storageClass: gp3 extendedConfiguration: | max_connections = 100 shared_buffers = 2GB effective_cache_size = 6GB maintenance_work_mem = 512MB checkpoint_completion_target = 0.9 wal_buffers = 16MB default_statistics_target = 100 random_page_cost = 1.1 effective_io_concurrency = 200 work_mem = 10MB min_wal_size = 1GB max_wal_size = 4GB readReplicas: replicaCount: 2 resources: limits: cpu: 2000m memory: 4Gi requests: cpu: 1000m memory: 2Gi metrics: enabled: true serviceMonitor: enabled: true ``` Install PostgreSQL: ```bash # Create password secret kubectl create secret generic postgres-credentials \\ --namespace cloudflow-prod \\ --from-literal=postgres-password=\"$(openssl rand -base64 32)\" \\ --from-literal=password=\"$(openssl rand -base64 32)\" # Install PostgreSQL helm install postgresql bitnami/postgresql \\ --namespace cloudflow-prod \\ --values postgres-values.yaml \\ --wait ``` ### PgBouncer Configuration Deploy PgBouncer for connection pooling to handle up to 100 connections efficiently: ```yaml # pgbouncer.yaml apiVersion: v1 kind: ConfigMap metadata: name: pgbouncer-config namespace: cloudflow-prod data: pgbouncer.ini: | [databases] cloudflow = host=postgresql.cloudflow-prod.svc.cluster.local port=5432 dbname=cloudflow [pgbouncer] listen_addr = 0.0.0.0 listen_port = 5432 auth_type = md5 auth_file = /etc/pgbouncer/userlist.txt pool_mode = transaction max_client_conn = 1000 default_pool_size = 25 reserve_pool_size = 5 reserve_pool_timeout = 3 max_db_connections = 100 max_user_connections = 100 server_lifetime = 3600 server_idle_timeout = 600 log_connections = 1 log_disconnections = 1 log_pooler_errors = 1 --- apiVersion: apps/v1 kind: Deployment metadata: name: pgbouncer namespace: cloudflow-prod spec: replicas: 2 selector: matchLabels: app: pgbouncer template: metadata: labels: app: pgbouncer spec: containers: - name: pgbouncer image: pgbouncer/pgbouncer:1.21.0 ports: - containerPort: 5432 resources: requests: cpu: 500m memory: 512Mi limits: cpu: 1000m memory: 1Gi volumeMounts: - name: config mountPath: /etc/pgbouncer volumes: - name: config configMap: name: pgbouncer-config --- apiVersion: v1 kind: Service metadata: name: pgbouncer namespace: cloudflow-prod spec: selector: app: pgbouncer ports: - port: 5432 targetPort: 5432 type: ClusterIP ``` Apply the PgBouncer configuration: ```bash kubectl apply -f pgbouncer.yaml ``` ### Database Migrations Run database migrations before deploying new versions: ```bash # Create migration job kubectl create job cloudflow-migrate-$(date +%s) \\ --namespace cloudflow-prod \\ --image=123456789012.dkr.ecr.us-east-1.amazonaws.com/cloudflow:2.4.0 \\ -- npm run migrate # Monitor migration progress kubectl logs -f job/cloudflow-migrate-<timestamp> -n cloudflow-prod ``` --- ## Monitoring and Observability ### Prometheus Setup Install Prometheus using the kube-prometheus-stack: ```bash helm install prometheus prometheus-community/kube-prometheus-stack \\ --namespace monitoring \\ --create-namespace \\ --set prometheus.prometheusSpec.serviceMonitorSelectorNilUsesHelmValues=false \\",
              "score": null
            }
          ],
          "latency_ms": 897.2372410025855,
          "query_id": "realistic_019",
          "dimension": "contextual",
          "query": "Setting up deployment pipeline, which Helm repository has CloudFlow charts?"
        },
        {
          "key_facts": [
            "helm repo add cloudflow https://charts.cloudflow.io"
          ],
          "found_facts": [
            "helm repo add cloudflow https://charts.cloudflow.io"
          ],
          "missed_facts": [],
          "coverage": 1.0,
          "expected_docs": [
            "deployment_guide"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_0",
              "content": "# CloudFlow Platform - Deployment and Operations Guide **Version:** 2.4.0 **Last Updated:** January 2026 **Target Environment:** Production (AWS EKS) ## Table of Contents 1. [Overview](#overview) 2. [Prerequisites](#prerequisites) 3. [Infrastructure Setup](#infrastructure-setup) 4. [Kubernetes Deployment](#kubernetes-deployment) 5. [Database Configuration](#database-configuration) 6. [Monitoring and Observability](#monitoring-and-observability) 7. [Backup and Disaster Recovery](#backup-and-disaster-recovery) 8. [Scaling and Performance](#scaling-and-performance) 9. [Security and Compliance](#security-and-compliance) 10. [Troubleshooting](#troubleshooting) --- ## Overview CloudFlow is a cloud-native workflow orchestration platform designed for high-availability production environments. This guide provides comprehensive instructions for deploying and operating CloudFlow on Amazon EKS (Elastic Kubernetes Service). ### Architecture Summary CloudFlow consists of the following components: - **API Server**: REST API for workflow management (Node.js/Express) - **Worker Service**: Background job processor (Node.js) - **Scheduler**: Cron-based task scheduler (Node.js) - **PostgreSQL**: Primary data store (version 14) - **Redis**: Cache and message queue (version 7.0) - **PgBouncer**: Database connection pooler ### Deployment Model - **Namespace**: `cloudflow-prod` - **Cluster**: AWS EKS 1.28 - **Region**: us-east-1 (primary), us-west-2 (disaster recovery) - **High Availability**: Multi-AZ deployment across 3 availability zones --- ## Prerequisites Before beginning the deployment process, ensure you have the following: ### Required Tools - `kubectl` (v1.28 or later) - `helm` (v3.12 or later) - `aws-cli` (v2.13 or later) - `eksctl` (v0.165 or later) - `terraform` (v1.6 or later) - for infrastructure provisioning ### Access Requirements - AWS account with appropriate IAM permissions - EKS cluster admin access - Container registry access (ECR) - Domain name and SSL certificates - Secrets management access (AWS Secrets Manager or Vault) ### Network Requirements - VPC with at least 3 public and 3 private subnets - NAT Gateway configured - Security groups allowing: - Ingress: HTTPS (443), HTTP (80) - Internal: PostgreSQL (5432), Redis (6379) - Monitoring: Prometheus (9090), Grafana (3000) --- ## Infrastructure Setup ### EKS Cluster Creation Use the following `eksctl` configuration to create the EKS cluster: ```yaml # cluster-config.yaml apiVersion: eksctl.io/v1alpha5 kind: ClusterConfig metadata: name: cloudflow-production region: us-east-1 version: \"1.28\" availabilityZones: - us-east-1a - us-east-1b - us-east-1c vpc: cidr: 10.0.0.0/16 nat: gateway: HighlyAvailable managedNodeGroups: - name: cloudflow-workers instanceType: m5.xlarge minSize: 3 maxSize: 10 desiredCapacity: 5 volumeSize: 100 privateNetworking: true labels: role: worker environment: production tags: Environment: production Application: cloudflow iam: withAddonPolicies: ebs: true efs: true albIngress: true cloudWatch: true cloudWatch: clusterLogging: enableTypes: - api - audit - authenticator - controllerManager - scheduler ``` Create the cluster: ```bash eksctl",
              "score": null
            },
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_1",
              "content": "create cluster -f cluster-config.yaml ``` ### Storage Configuration Install the EBS CSI driver for persistent volumes: ```bash eksctl create addon --name aws-ebs-csi-driver \\ --cluster cloudflow-production \\ --service-account-role-arn arn:aws:iam::ACCOUNT_ID:role/AmazonEKS_EBS_CSI_DriverRole \\ --force ``` --- ## Kubernetes Deployment ### Namespace Setup Create the CloudFlow namespace and configure resource quotas: ```yaml # namespace.yaml apiVersion: v1 kind: Namespace metadata: name: cloudflow-prod labels: name: cloudflow-prod environment: production --- apiVersion: v1 kind: ResourceQuota metadata: name: cloudflow-quota namespace: cloudflow-prod spec: hard: requests.cpu: \"50\" requests.memory: 100Gi persistentvolumeclaims: \"10\" services.loadbalancers: \"2\" ``` Apply the namespace configuration: ```bash kubectl apply -f namespace.yaml ``` ### Environment Configuration CloudFlow requires the following environment variables: | Variable | Description | Example | Required | |----------|-------------|---------|----------| | `DATABASE_URL` | PostgreSQL connection string | `postgresql://user:pass@host:5432/cloudflow` | Yes | | `REDIS_URL` | Redis connection string | `redis://redis-master.cloudflow-prod.svc.cluster.local:6379` | Yes | | `JWT_SECRET` | Secret key for JWT token signing | `<generated-secret-256-bit>` | Yes | | `LOG_LEVEL` | Application log verbosity | `info`, `debug`, `warn`, `error` | No (default: `info`) | | `API_PORT` | API server port | `3000` | No (default: `3000`) | | `WORKER_CONCURRENCY` | Number of concurrent workers | `10` | No (default: `5`) | | `SESSION_SECRET` | Session encryption key | `<generated-secret-256-bit>` | Yes | | `AWS_REGION` | AWS region for services | `us-east-1` | Yes | | `S3_BUCKET` | S3 bucket for file storage | `cloudflow-prod-storage` | Yes | | `SMTP_HOST` | Email server hostname | `smtp.sendgrid.net` | No | | `SMTP_PORT` | Email server port | `587` | No | | `METRICS_ENABLED` | Enable Prometheus metrics | `true` | No (default: `false`) | Create a Kubernetes secret for sensitive variables: ```bash kubectl create secret generic cloudflow-secrets \\ --namespace cloudflow-prod \\ --from-literal=DATABASE_URL=\"postgresql://cloudflow:$(cat db-password.txt)@pgbouncer.cloudflow-prod.svc.cluster.local:5432/cloudflow\" \\ --from-literal=REDIS_URL=\"redis://redis-master.cloudflow-prod.svc.cluster.local:6379\" \\ --from-literal=JWT_SECRET=\"$(openssl rand -base64 32)\" \\ --from-literal=SESSION_SECRET=\"$(openssl rand -base64 32)\" ``` ### Helm Chart Deployment Add the CloudFlow Helm repository: ```bash helm repo add cloudflow https://charts.cloudflow.io helm repo update ``` Create a values file for production configuration: ```yaml # values-production.yaml replicaCount: 3 image: repository: 123456789012.dkr.ecr.us-east-1.amazonaws.com/cloudflow tag: \"2.4.0\" pullPolicy: IfNotPresent resources: limits: cpu: 2000m memory: 4Gi requests: cpu: 2000m memory: 4Gi autoscaling: enabled: true minReplicas: 3 maxReplicas: 10 targetCPUUtilizationPercentage: 70 targetMemoryUtilizationPercentage: 80 service: type: ClusterIP port: 80 targetPort: 3000 annotations: service.beta.kubernetes.io/aws-load-balancer-type: \"nlb\" ingress: enabled: true className: nginx annotations: cert-manager.io/cluster-issuer: \"letsencrypt-prod\" nginx.ingress.kubernetes.io/ssl-redirect: \"true\" hosts: - host: api.cloudflow.io paths: - path: / pathType: Prefix tls:",
              "score": null
            },
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_2",
              "content": "- secretName: cloudflow-tls hosts: - api.cloudflow.io healthCheck: enabled: true livenessProbe: path: /health initialDelaySeconds: 30 periodSeconds: 10 timeoutSeconds: 5 failureThreshold: 3 readinessProbe: path: /ready initialDelaySeconds: 10 periodSeconds: 5 timeoutSeconds: 3 failureThreshold: 3 env: - name: NODE_ENV value: \"production\" - name: LOG_LEVEL value: \"info\" - name: API_PORT value: \"3000\" - name: WORKER_CONCURRENCY value: \"10\" - name: AWS_REGION value: \"us-east-1\" - name: METRICS_ENABLED value: \"true\" envFrom: - secretRef: name: cloudflow-secrets podAnnotations: prometheus.io/scrape: \"true\" prometheus.io/port: \"9090\" prometheus.io/path: \"/metrics\" podSecurityContext: runAsNonRoot: true runAsUser: 1000 fsGroup: 1000 securityContext: allowPrivilegeEscalation: false capabilities: drop: - ALL readOnlyRootFilesystem: true persistence: enabled: true storageClass: gp3 accessMode: ReadWriteOnce size: 50Gi redis: enabled: true architecture: replication auth: enabled: true master: persistence: size: 20Gi replica: replicaCount: 2 persistence: size: 20Gi ``` Deploy CloudFlow using Helm: ```bash helm install cloudflow cloudflow/cloudflow \\ --namespace cloudflow-prod \\ --values values-production.yaml \\ --wait \\ --timeout 10m ``` ### Deployment Verification Verify the deployment status: ```bash # Check pod status kubectl get pods -n cloudflow-prod # Expected output: # NAME READY STATUS RESTARTS AGE # cloudflow-api-7d8f9c5b6d-4xk2p 1/1 Running 0 2m # cloudflow-api-7d8f9c5b6d-9hj5m 1/1 Running 0 2m # cloudflow-api-7d8f9c5b6d-tn8wq 1/1 Running 0 2m # Check service endpoints kubectl get svc -n cloudflow-prod # Check ingress kubectl get ingress -n cloudflow-prod ``` Test health endpoints: ```bash # Port forward for testing kubectl port-forward -n cloudflow-prod svc/cloudflow-api 8080:80 # Test health endpoint curl http://localhost:8080/health # Expected: {\"status\":\"healthy\",\"timestamp\":\"2026-01-24T10:30:00Z\"} # Test readiness endpoint curl http://localhost:8080/ready # Expected: {\"status\":\"ready\",\"dependencies\":{\"database\":\"connected\",\"redis\":\"connected\"}} ``` --- ## Database Configuration ### PostgreSQL Setup CloudFlow uses PostgreSQL 14 as its primary data store.",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_8",
              "content": "Leverage caching:** ```bash # Enable client-side caching export CLOUDFLOW_CACHE_ENABLED=true export CLOUDFLOW_CACHE_TTL=300 # Cache workflow metadata cloudflow workflows list --use-cache --cache-ttl 600 ``` --- ## Log Analysis & Debugging ### Accessing CloudFlow Logs #### Kubernetes Deployments ```bash # List all CloudFlow pods kubectl get pods -n cloudflow # Tail logs from API server kubectl logs -f -n cloudflow deployment/cloudflow-api --tail=100 # Get logs from specific pod kubectl logs -n cloudflow cloudflow-api-7d4f6b8c9d-x7k2m # Get logs from previous crashed pod kubectl logs -n cloudflow cloudflow-api-7d4f6b8c9d-x7k2m --previous # Get logs from all pods in deployment kubectl logs -n cloudflow deployment/cloudflow-api --all-containers=true # Stream logs from multiple pods kubectl logs -n cloudflow -l app=cloudflow-api -f --max-log-requests=10 ``` #### Log Levels CloudFlow supports the following log levels: - `TRACE` - Very detailed debugging information - `DEBUG` - Detailed debugging information - `INFO` - Informational messages (default) - `WARN` - Warning messages - `ERROR` - Error messages - `FATAL` - Fatal errors causing shutdown **Changing log levels:** ```bash # Set global log level cloudflow config set logging.level DEBUG # Set log level for specific component cloudflow config set logging.components.database DEBUG cloudflow config set logging.components.auth INFO cloudflow config set logging.components.workflows TRACE # Temporary log level increase (resets after 1 hour) cloudflow debug set-log-level DEBUG --duration 3600 # View current log configuration cloudflow config get logging --format json ``` ### Grep Patterns for Common Issues #### Authentication Failures ```bash # Find all authentication errors kubectl logs -n cloudflow deployment/cloudflow-api | \\ grep -i \"authentication\\|401\\|unauthorized\" | \\ tail -n 50 # Find JWT validation failures kubectl logs -n cloudflow deployment/cloudflow-api | \\ grep -E \"JWT|token.*invalid|signature.*failed\" # Find clock skew issues kubectl logs -n cloudflow deployment/cloudflow-api | \\ grep -i \"clock skew\\|time.*sync\\|nbf\\|exp\" ``` #### Database Errors ```bash # Find database connection errors kubectl logs -n cloudflow deployment/cloudflow-api | \\ grep -E \"connection.*pool|could not connect|database.*timeout\" # Find slow queries kubectl logs -n cloudflow deployment/cloudflow-api | \\ grep \"slow query\" | \\ awk '{print $NF}' | \\ sort -n | \\ tail -n 20 # Find deadlock errors kubectl logs -n cloudflow deployment/cloudflow-api | \\ grep -i \"deadlock detected\" ``` #### Workflow Execution Errors ```bash # Find workflow timeout errors kubectl logs -n cloudflow deployment/cloudflow-workflow-engine | \\ grep -E \"timeout|exceeded.*3600\" # Find workflow retry attempts kubectl logs -n cloudflow deployment/cloudflow-workflow-engine | \\ grep -E \"retry attempt [0-9]|retrying",
              "score": null
            },
            {
              "doc_id": "deployment_guide",
              "chunk_id": "deployment_guide_fix_7",
              "content": "podSelector: matchLabels: app: cloudflow policyTypes: - Ingress - Egress ingress: - from: - namespaceSelector: matchLabels: name: cloudflow-prod - podSelector: matchLabels: app: nginx-ingress ports: - protocol: TCP port: 3000 egress: - to: - podSelector: matchLabels: app: postgresql ports: - protocol: TCP port: 5432 - to: - podSelector: matchLabels: app: redis ports: - protocol: TCP port: 6379 ``` ### Pod Security Standards Enforce Pod Security Standards at the namespace level: ```bash kubectl label namespace cloudflow-prod \\ pod-security.kubernetes.io/enforce=restricted \\ pod-security.kubernetes.io/audit=restricted \\ pod-security.kubernetes.io/warn=restricted ``` ### Secrets Management Use AWS Secrets Manager for sensitive credentials: ```bash # Install External Secrets Operator helm install external-secrets external-secrets/external-secrets \\ --namespace external-secrets-system \\ --create-namespace # Create SecretStore kubectl apply -f - <<EOF apiVersion: external-secrets.io/v1beta1 kind: SecretStore metadata: name: aws-secrets-manager namespace: cloudflow-prod spec: provider: aws: service: SecretsManager region: us-east-1 auth: jwt: serviceAccountRef: name: external-secrets-sa EOF ``` --- ## Troubleshooting ### Common Issues #### Pods in CrashLoopBackOff **Symptoms**: Pods continuously restart **Diagnosis**: ```bash kubectl logs -n cloudflow-prod <pod-name> --previous kubectl describe pod -n cloudflow-prod <pod-name> ``` **Common Causes**: - Database connection failure - Invalid environment variables - Insufficient resources #### High Memory Usage **Symptoms**: Pods being OOMKilled **Diagnosis**: ```bash kubectl top pods -n cloudflow-prod ``` **Resolution**: - Increase memory limits in deployment - Check for memory leaks in application logs - Review heap dump for Node.js processes #### Database Connection Pool Exhausted **Symptoms**: \"Too many connections\" errors **Diagnosis**: ```bash # Check PgBouncer stats kubectl exec -it pgbouncer-xxx -n cloudflow-prod -- \\ psql -p 5432 -U pgbouncer pgbouncer -c \"SHOW POOLS;\" ``` **Resolution**: - Increase `max_db_connections` in PgBouncer - Optimize application connection usage - Add more PostgreSQL replicas ### Support Contacts - **On-call Engineer**: pagerduty.com/cloudflow-oncall - **Slack Channel**: #cloudflow-operations - **Documentation**: https://docs.cloudflow.io - **Status Page**: https://status.cloudflow.io --- ## Appendix ### Useful Commands ```bash # View all resources in namespace kubectl get all -n cloudflow-prod # Check resource usage kubectl top pods -n cloudflow-prod kubectl top nodes # View logs kubectl logs -f deployment/cloudflow-api -n cloudflow-prod # Execute command in pod kubectl exec -it <pod-name> -n cloudflow-prod -- /bin/sh # Port forward for local testing kubectl port-forward svc/cloudflow-api 8080:80 -n cloudflow-prod # Rollback deployment helm rollback cloudflow -n cloudflow-prod # Update deployment helm upgrade cloudflow cloudflow/cloudflow \\ --namespace cloudflow-prod \\ --values values-production.yaml ``` ### Maintenance Windows Scheduled maintenance occurs during the following windows: - **Primary**: Sunday 02:00-06:00",
              "score": null
            }
          ],
          "latency_ms": 928.7460480009031,
          "query_id": "realistic_019",
          "dimension": "negation",
          "query": "Why can't I find CloudFlow in the official Helm hub?"
        },
        {
          "key_facts": [
            "minimum scheduling interval is 1 minute",
            "The minimum scheduling interval is **1 minute**"
          ],
          "found_facts": [],
          "missed_facts": [
            "minimum scheduling interval is 1 minute",
            "The minimum scheduling interval is **1 minute**"
          ],
          "coverage": 0.0,
          "expected_docs": [
            "user_guide"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_1",
              "content": "## Rate Limiting To ensure fair usage and system stability, CloudFlow enforces rate limits on all API endpoints. **Default Limits:** - 100 requests per minute per authenticated user - 20 requests per minute for unauthenticated requests - Burst allowance: 150 requests in a 10-second window ### Rate Limit Headers Every API response includes rate limit information: ``` X-RateLimit-Limit: 100 X-RateLimit-Remaining: 87 X-RateLimit-Reset: 1640995200 ``` When you exceed the rate limit, you'll receive a `429 Too Many Requests` response: ```json { \"error\": { \"code\": \"rate_limit_exceeded\", \"message\": \"Rate limit exceeded. Please retry after 42 seconds.\", \"retry_after\": 42 } } ``` **Best Practices:** - Monitor `X-RateLimit-Remaining` header values - Implement exponential backoff when receiving 429 responses - Cache responses when appropriate to reduce API calls - Consider upgrading to Enterprise tier for higher limits (1000 req/min) ## Pagination List endpoints return paginated results to optimize performance. Use `limit` and `offset` parameters to navigate through result sets. **Parameters:** - `limit`: Number of items per page (default: 20, max: 100) - `offset`: Number of items to skip (default: 0) **Example Request:** ```bash curl -H \"X-API-Key: YOUR_API_KEY\" \\ \"https://api.cloudflow.io/v2/workflows?limit=50&offset=100\" ``` **Response Structure:** ```json { \"data\": [...], \"pagination\": { \"total\": 347, \"limit\": 50, \"offset\": 100, \"has_more\": true } } ``` **Python Example:** ```python import requests def fetch_all_workflows(api_key): base_url = \"https://api.cloudflow.io/v2/workflows\" headers = {\"X-API-Key\": api_key} all_workflows = [] offset = 0 limit = 100 while True: response = requests.get( base_url, headers=headers, params={\"limit\": limit, \"offset\": offset} ) data = response.json() all_workflows.extend(data['data']) if not data['pagination']['has_more']: break offset += limit return all_workflows ``` ## Endpoints ### Workflows #### List Workflows Retrieve a paginated list of all workflows in your account. **Endpoint:** `GET /workflows` **Query Parameters:** - `status` (optional): Filter by status (`active`, `paused`, `archived`) - `created_after` (optional): ISO 8601 timestamp - `limit` (optional): Items per page (max 100) - `offset` (optional): Pagination offset **Example Request:** ```bash curl -H \"X-API-Key: YOUR_API_KEY\" \\ \"https://api.cloudflow.io/v2/workflows?status=active&limit=25\" ``` **Example Response (200 OK):** ```json { \"data\": [ { \"id\": \"wf_8x7k2m9p\", \"name\": \"Data Processing Pipeline\", \"description\": \"Processes customer data every hour\", \"status\": \"active\", \"trigger\": { \"type\": \"schedule\", \"cron\": \"0 * * * *\" }, \"steps\": 12, \"created_at\": \"2026-01-15T10:30:00Z\", \"updated_at\": \"2026-01-23T14:22:00Z\", \"last_run\": \"2026-01-24T09:00:00Z\" } ], \"pagination\": { \"total\": 1, \"limit\": 25, \"offset\": 0, \"has_more\": false } } ``` #### Create Workflow Create a new workflow with specified configuration.",
              "score": null
            },
            {
              "doc_id": "user_guide",
              "chunk_id": "user_guide_fix_8",
              "content": "Version Control For critical workflows: - Export YAML definitions regularly - Store in version control (Git) - Use pull requests for changes - Tag releases with semantic versioning ## Common Workflow Patterns Here are proven patterns for common automation scenarios: ### Pattern 1: Form to Database Capture form submissions and store in database: ```yaml name: \"Contact Form to Database\" trigger: type: webhook method: POST steps: - id: validate_submission action: javascript code: | if (!input.email || !input.message) { throw new Error(\"Email and message required\"); } return input; - id: check_duplicate action: database_query config: connection: \"{{secrets.DB_CONNECTION}}\" query: \"SELECT id FROM contacts WHERE email = $1 AND created_at > NOW() - INTERVAL '1 hour'\" parameters: - \"{{trigger.body.email}}\" - id: insert_contact action: database_query condition: \"{{length(steps.check_duplicate.rows) == 0}}\" config: connection: \"{{secrets.DB_CONNECTION}}\" query: | INSERT INTO contacts (name, email, message, source, created_at) VALUES ($1, $2, $3, $4, NOW()) parameters: - \"{{trigger.body.name}}\" - \"{{trigger.body.email}}\" - \"{{trigger.body.message}}\" - \"website_form\" - id: notify_sales action: slack_message condition: \"{{steps.insert_contact.affected_rows > 0}}\" config: channel: \"#leads\" text: \"New contact form submission from {{trigger.body.email}}\" ``` ### Pattern 2: API Polling Periodically check an API and take action on new items: ```yaml name: \"Poll GitHub Issues\" schedule: cron: \"*/15 * * * *\" # Every 15 minutes timezone: \"UTC\" steps: - id: get_last_check action: database_query config: query: \"SELECT last_checked_at FROM workflow_state WHERE workflow_id = $1\" parameters: - \"{{workflow.id}}\" - id: fetch_issues action: http_request config: method: GET url: \"https://api.github.com/repos/company/project/issues\" params: since: \"{{steps.get_last_check.rows[0].last_checked_at}}\" state: \"open\" headers: Authorization: \"token {{secrets.GITHUB_TOKEN}}\" - id: process_new_issues action: javascript code: | const issues = input.body; return { count: issues.length, issues: issues.map(i => ({ number: i.number, title: i.title, url: i.html_url })) }; - id: notify_team action: slack_message condition: \"{{steps.process_new_issues.output.count > 0}}\" config: channel: \"#engineering\" text: \"{{steps.process_new_issues.output.count}} new GitHub issues\" - id: update_last_check action: database_query config: query: | UPDATE workflow_state SET last_checked_at = $1 WHERE workflow_id = $2 parameters: - \"{{now()}}\" - \"{{workflow.id}}\" ``` ### Pattern 3: Multi-Step Approval Implement approval workflows with timeouts: ```yaml name: \"Expense Approval Workflow\" trigger: type: webhook method: POST steps: - id: create_approval_request action: database_query config: query: | INSERT INTO approvals (expense_id, amount, requester, status, created_at) VALUES ($1, $2, $3, 'pending', NOW()) RETURNING id parameters: - \"{{trigger.body.expense_id}}\" - \"{{trigger.body.amount}}\" - \"{{trigger.body.requester}}\" - id: notify_manager action: email config: to: \"{{trigger.body.manager_email}}\" subject: \"Expense Approval Required: ${{trigger.body.amount}}\" body: | An expense requires your approval: Amount: ${{trigger.body.amount}}",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_10",
              "content": "- Execution start rate: 500 per second - Completion rate: 450 per second (average 2-second execution time) **Database**: - Read throughput: 50,000 queries per second (across replicas) - Write throughput: 15,000 transactions per second - Connection capacity: 2,000 concurrent connections **Kafka**: - Message ingestion: 100,000 messages per second - Consumer throughput: 80,000 messages per second (aggregated) - End-to-end latency: < 100ms (P99) ### Resource Utilization **CPU Utilization** (Target: 60-70% average): - API Gateway: 55% average, 80% peak - Workflow Engine: 65% average, 85% peak - Auth Service: 40% average, 60% peak **Memory Utilization** (Target: < 80%): - API Gateway: 2.5GB average per pod (4GB allocated) - Workflow Engine: 6GB average per pod (8GB allocated) - Notification Service: 2.8GB average per pod (4GB allocated) **Network Throughput**: - Ingress: 2 Gbps average, 5 Gbps peak - Egress: 1.5 Gbps average, 4 Gbps peak - Internal (service mesh): 8 Gbps average ### Monitoring & Alerting **Key Metrics** (Monitored via Prometheus): - Request rate (per service, per endpoint) - Error rate (4xx, 5xx responses) - Latency percentiles (P50, P95, P99) - Resource utilization (CPU, memory, disk) - Cache hit/miss ratios - Database connection pool saturation - Kafka consumer lag **Alerts** (via PagerDuty): - P1 (Immediate): API error rate > 1%, database connection failure - P2 (< 15 min): P99 latency > 500ms, cache hit rate < 80% - P3 (< 1 hour): Resource utilization > 85%, replica count < minimum **SLI/SLO/SLA**: - SLI: API success rate (non-5xx responses) - SLO: 99.9% uptime per month (43 minutes downtime allowance) - SLA: 99.5% uptime guarantee (customer-facing SLA with credits) --- ## Disaster Recovery ### Recovery Objectives **RPO (Recovery Point Objective): 1 hour** - Maximum acceptable data loss: 1 hour of transactions - Achieved through: Continuous database replication + hourly snapshots - Kafka retention: 7 days allows event replay **RTO (Recovery Time Objective): 4 hours** - Maximum acceptable downtime: 4 hours for full system recovery - Includes: Failover, data verification, and service restoration - Automated runbooks reduce RTO to < 2 hours for common scenarios ### High Availability Architecture **Multi-AZ Deployment**: ``` Region: us-east-1 AZ-1a: AZ-1b: AZ-1c: - API Gateway (4) - API Gateway (4) - API Gateway (4) - Workflow Engine (6) - Workflow Engine (5) - Workflow Engine (5) - Auth Service (3) - Auth Service (3) - Auth",
              "score": null
            },
            {
              "doc_id": "user_guide",
              "chunk_id": "user_guide_fix_6",
              "content": "Filter by error type, date range, or execution ID 3. Click an execution to view full details 4. Click **\"Retry\"** to reprocess with the same input data ### Error Notifications Get notified when workflows fail: ```yaml workflow: notifications: on_failure: - type: email to: \"ops-team@company.com\" - type: slack channel: \"#alerts\" message: \"Workflow {{workflow.name}} failed: {{error.message}}\" on_success_after_retry: - type: slack channel: \"#monitoring\" message: \"Workflow recovered after {{error.attempts}} attempts\" ``` ## Workflow Limits CloudFlow enforces the following limits to ensure platform stability and performance: ### Steps Per Workflow - **Maximum**: 50 steps per workflow - **Recommendation**: Keep workflows focused and modular. If you need more steps, consider splitting into multiple workflows connected via webhooks. ### Execution Timeout - **Default**: 3600 seconds (60 minutes) - **Behavior**: Workflows exceeding this timeout are automatically terminated - **Custom Timeouts**: Enterprise plans can request custom timeout limits **Setting Step-Level Timeouts:** ```yaml - id: long_running_task action: http_request config: url: \"https://api.example.com/process\" timeout: 300 # 5 minutes for this specific step ``` ### Execution Limits - **Maximum**: 1000 executions per day (per workflow) - **Rate Limiting**: 100 concurrent executions per workflow - **Burst Limit**: 10 executions per second **What happens when limits are reached:** - New executions are queued automatically - Webhook triggers return HTTP 429 (Too Many Requests) - Scheduled executions are skipped (logged in audit trail) - Email notifications sent to workflow owner **Monitoring Usage:** View real-time metrics in your workflow dashboard: - Executions today: 847 / 1000 - Average duration: 12.3s - Success rate: 98.2% - Current queue depth: 3 ### Data Limits - **Maximum request/response size**: 10MB per action - **Maximum execution payload**: 50MB total - **Variable value size**: 1MB per variable ### Enterprise Plan Limits Enterprise customers can request increased limits: - Up to 100 steps per workflow - Up to 10,000 executions per day - Up to 7200 second timeout (2 hours) - Priority execution queue - Dedicated capacity allocation Contact sales@cloudflow.io for Enterprise pricing and custom limits. ## Best Practices Follow these best practices to build reliable, maintainable workflows: ### 1. Use Descriptive Names **Good:** - Workflow: \"Sync Customer Data from Salesforce to Database\" - Step: \"validate_customer_email\" **Bad:** - Workflow: \"Workflow 1\" - Step: \"step3\" ### 2.",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_3",
              "content": "**Technology**: Go with distributed locking via Redis **Replicas**: 4 pods (production), active-passive with leader election **Resource Allocation**: 2 vCPU, 4GB RAM per pod **Key Responsibilities**: - Parse and validate cron expressions (extended format supporting seconds) - Maintain schedule registry in PostgreSQL - Distributed scheduling with leader election (one active scheduler) - Missed execution handling with configurable catch-up policy - Schedule conflict detection and resolution - Time zone support for international schedules **Scheduling Architecture**: ``` PostgreSQL Schedules Table \u2502 \u25bc Leader Scheduler (elected via Redis) \u2502 \u251c\u2500 Scan for due schedules (every 10 seconds) \u251c\u2500 Acquire lock per schedule (prevents duplicates) \u251c\u2500 Publish to Kafka \u2192 workflow.events topic \u2514\u2500 Update last_run timestamp ``` **Schedule Types**: - **Cron-based**: `0 */5 * * * *` (every 5 minutes) - **One-time**: Specific timestamp for delayed execution - **Interval-based**: Every N seconds/minutes/hours **Reliability Features**: - Leader election using Redis with 30-second lease - Heartbeat mechanism to detect leader failure (5-second interval) - Automatic failover to standby scheduler (< 10 seconds) - Schedule versioning to handle updates during execution - Missed execution policy: SKIP, RUN_ONCE, or RUN_ALL **Performance Targets**: - Schedule evaluation: < 100ms per cycle - Accuracy: \u00b1 1 second for schedule triggers - Capacity: 100,000 active schedules --- ### Notification Service **Purpose**: Multi-channel notification delivery system supporting email, SMS, webhooks, and in-app notifications.",
              "score": null
            }
          ],
          "latency_ms": 876.3757440028712,
          "query_id": "realistic_020",
          "dimension": "original",
          "query": "What is the minimum scheduling interval for workflows?"
        },
        {
          "key_facts": [
            "minimum scheduling interval is 1 minute",
            "The minimum scheduling interval is **1 minute**"
          ],
          "found_facts": [
            "The minimum scheduling interval is **1 minute**"
          ],
          "missed_facts": [
            "minimum scheduling interval is 1 minute"
          ],
          "coverage": 0.5,
          "expected_docs": [
            "user_guide"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "user_guide",
              "chunk_id": "user_guide_fix_0",
              "content": "# CloudFlow User Guide Welcome to CloudFlow, the modern workflow automation platform that helps you connect your apps, automate repetitive tasks, and build powerful integrations without writing code. ## Table of Contents 1. [Getting Started](#getting-started) 2. [Workflow Creation](#workflow-creation) 3. [Available Actions](#available-actions) 4. [Variables and Expressions](#variables-and-expressions) 5. [Scheduling](#scheduling) 6. [Error Handling](#error-handling) 7. [Workflow Limits](#workflow-limits) 8. [Best Practices](#best-practices) 9. [Common Workflow Patterns](#common-workflow-patterns) ## Getting Started ### Account Setup Creating your CloudFlow account takes just a few minutes: 1. Navigate to https://app.cloudflow.io/signup 2. Enter your email address and create a strong password 3. Verify your email address by clicking the link sent to your inbox 4. Complete your profile by providing your organization name and use case 5. Choose your plan (Free, Professional, or Enterprise) Once your account is created, you'll be redirected to your CloudFlow dashboard where you can start building workflows immediately. ### Your First Workflow Let's create a simple workflow that sends you a Slack notification when a new file is uploaded to your Google Drive: 1. Click the **\"Create Workflow\"** button in the top-right corner 2. Give your workflow a name: \"Google Drive to Slack Notifier\" 3. Select **\"Google Drive - New File\"** as your trigger 4. Authenticate your Google Drive account when prompted 5. Choose the folder you want to monitor 6. Click **\"Add Action\"** and select **\"Slack - Send Message\"** 7. Authenticate your Slack account 8. Select the channel where you want to receive notifications 9. Customize your message using variables: `New file uploaded: {{trigger.file.name}}` 10. Click **\"Save & Activate\"** to enable your workflow Congratulations! Your first workflow is now live. Every time a file is added to the specified Google Drive folder, you'll receive a Slack notification. ## Workflow Creation CloudFlow offers multiple ways to create and manage workflows, giving you the flexibility to work in the way that suits you best. ### Visual Editor The Visual Editor is CloudFlow's drag-and-drop interface for building workflows without code. It's perfect for users who prefer a graphical approach: **Key Features:** - Drag-and-drop action blocks from the sidebar - Visual connections between steps show your workflow logic - Inline configuration for each action - Real-time validation and error checking - Test mode to run your workflow with sample data **Using the Visual Editor:** 1.",
              "score": null
            },
            {
              "doc_id": "user_guide",
              "chunk_id": "user_guide_fix_4",
              "content": "### Cron Syntax Use standard cron expressions to define schedules: ``` * * * * * \u252c \u252c \u252c \u252c \u252c \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2514\u2500\u2500\u2500 Day of Week (0-6, Sunday=0) \u2502 \u2502 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Month (1-12) \u2502 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Day of Month (1-31) \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Hour (0-23) \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Minute (0-59) ``` **Common Cron Patterns:** | Pattern | Description | |---------|-------------| | `*/5 * * * *` | Every 5 minutes | | `0 * * * *` | Every hour at minute 0 | | `0 9 * * *` | Daily at 9:00 AM | | `0 9 * * 1` | Every Monday at 9:00 AM | | `0 0 1 * *` | First day of every month at midnight | | `0 0 * * 0` | Every Sunday at midnight | | `0 9-17 * * 1-5` | Every hour from 9 AM to 5 PM, Monday-Friday | | `*/15 9-17 * * 1-5` | Every 15 minutes during business hours | **Important:** The minimum scheduling interval is **1 minute**. Expressions that evaluate to more frequent executions will be rejected. ### Timezone Handling All scheduled workflows run in **UTC by default**. To account for your local timezone: **Option 1: Convert to UTC** If you want a workflow to run at 9:00 AM EST (UTC-5), schedule it for 14:00 UTC: ``` 0 14 * * * # 9:00 AM EST = 14:00 UTC ``` **Option 2: Use Timezone Configuration** Specify a timezone in your workflow configuration: ```yaml schedule: cron: \"0 9 * * *\" timezone: \"America/New_York\" # IANA timezone identifier ``` **Supported Timezones:** CloudFlow supports all IANA timezone identifiers, including: - `America/New_York` (Eastern Time) - `America/Chicago` (Central Time) - `America/Los_Angeles` (Pacific Time) - `Europe/London` (GMT/BST) - `Asia/Tokyo` (Japan Standard Time) - `Australia/Sydney` (Australian Eastern Time) **Daylight Saving Time:** When using timezone configuration, CloudFlow automatically handles DST transitions. A workflow scheduled for 9:00 AM will run at 9:00 AM local time regardless of DST changes. ### Schedule Management **Creating a Schedule:** 1. Open your workflow in the editor 2. Click the **\"Trigger\"** section 3. Select **\"Schedule\"** as the trigger type 4. Enter your cron expression or use the visual schedule builder 5. Select your timezone 6.",
              "score": null
            },
            {
              "doc_id": "user_guide",
              "chunk_id": "user_guide_fix_5",
              "content": "Save and activate **Testing Schedules:** Use the built-in schedule calculator to preview upcoming executions: ``` Next 5 executions: 1. 2026-01-24 14:00:00 UTC 2. 2026-01-25 14:00:00 UTC 3. 2026-01-26 14:00:00 UTC 4. 2026-01-27 14:00:00 UTC 5. 2026-01-28 14:00:00 UTC ``` ## Error Handling Robust error handling ensures your workflows are resilient and reliable. ### Retry Policies Configure automatic retries for failed actions: ```yaml - id: api_call action: http_request config: url: \"https://api.example.com/data\" retry: max_attempts: 3 backoff_type: \"exponential\" # or \"fixed\", \"linear\" initial_interval: 1000 # milliseconds max_interval: 30000 multiplier: 2.0 retry_on: - timeout - network_error - status: [500, 502, 503, 504] ``` **Backoff Strategies:** - **Fixed**: Wait the same amount of time between retries - Attempt 1: Wait 1s - Attempt 2: Wait 1s - Attempt 3: Wait 1s - **Linear**: Increase wait time by a fixed amount - Attempt 1: Wait 1s - Attempt 2: Wait 2s - Attempt 3: Wait 3s - **Exponential**: Double the wait time with each retry (recommended) - Attempt 1: Wait 1s - Attempt 2: Wait 2s - Attempt 3: Wait 4s **Retry Conditions:** Control which errors trigger retries: - `timeout`: Request timeout - `network_error`: Connection failures - `status`: Specific HTTP status codes - `error_code`: Application-specific error codes ### Fallback Actions Execute alternative actions when the primary action fails: ```yaml - id: primary_payment action: http_request config: url: \"https://primary-payment-gateway.com/charge\" method: POST body: amount: \"{{amount}}\" on_error: - id: fallback_payment action: http_request config: url: \"https://backup-payment-gateway.com/charge\" method: POST body: amount: \"{{amount}}\" - id: notify_admin action: email config: to: \"admin@company.com\" subject: \"Payment Gateway Failure\" body: \"Primary gateway failed, switched to backup\" ``` **Error Object:** Access error details in fallback actions: ``` {{error.message}} # Error message {{error.code}} # Error code {{error.step_id}} # Failed step ID {{error.timestamp}} # When the error occurred {{error.attempts}} # Number of retry attempts ``` ### Dead Letter Queue When all retries and fallbacks fail, CloudFlow can route failed executions to a Dead Letter Queue (DLQ) for manual review: **Enable DLQ:** ```yaml workflow: error_handling: dead_letter_queue: enabled: true retain_days: 30 ``` **DLQ Features:** - View failed executions in the dashboard - Inspect complete execution context and error details - Retry individual executions after fixing issues - Export failed executions for analysis - Set up alerts for DLQ threshold breaches **Accessing the DLQ:** 1. Navigate to **Workflows** > **[Your Workflow]** > **Dead Letter Queue** 2.",
              "score": null
            },
            {
              "doc_id": "user_guide",
              "chunk_id": "user_guide_fix_6",
              "content": "Filter by error type, date range, or execution ID 3. Click an execution to view full details 4. Click **\"Retry\"** to reprocess with the same input data ### Error Notifications Get notified when workflows fail: ```yaml workflow: notifications: on_failure: - type: email to: \"ops-team@company.com\" - type: slack channel: \"#alerts\" message: \"Workflow {{workflow.name}} failed: {{error.message}}\" on_success_after_retry: - type: slack channel: \"#monitoring\" message: \"Workflow recovered after {{error.attempts}} attempts\" ``` ## Workflow Limits CloudFlow enforces the following limits to ensure platform stability and performance: ### Steps Per Workflow - **Maximum**: 50 steps per workflow - **Recommendation**: Keep workflows focused and modular. If you need more steps, consider splitting into multiple workflows connected via webhooks. ### Execution Timeout - **Default**: 3600 seconds (60 minutes) - **Behavior**: Workflows exceeding this timeout are automatically terminated - **Custom Timeouts**: Enterprise plans can request custom timeout limits **Setting Step-Level Timeouts:** ```yaml - id: long_running_task action: http_request config: url: \"https://api.example.com/process\" timeout: 300 # 5 minutes for this specific step ``` ### Execution Limits - **Maximum**: 1000 executions per day (per workflow) - **Rate Limiting**: 100 concurrent executions per workflow - **Burst Limit**: 10 executions per second **What happens when limits are reached:** - New executions are queued automatically - Webhook triggers return HTTP 429 (Too Many Requests) - Scheduled executions are skipped (logged in audit trail) - Email notifications sent to workflow owner **Monitoring Usage:** View real-time metrics in your workflow dashboard: - Executions today: 847 / 1000 - Average duration: 12.3s - Success rate: 98.2% - Current queue depth: 3 ### Data Limits - **Maximum request/response size**: 10MB per action - **Maximum execution payload**: 50MB total - **Variable value size**: 1MB per variable ### Enterprise Plan Limits Enterprise customers can request increased limits: - Up to 100 steps per workflow - Up to 10,000 executions per day - Up to 7200 second timeout (2 hours) - Priority execution queue - Dedicated capacity allocation Contact sales@cloudflow.io for Enterprise pricing and custom limits. ## Best Practices Follow these best practices to build reliable, maintainable workflows: ### 1. Use Descriptive Names **Good:** - Workflow: \"Sync Customer Data from Salesforce to Database\" - Step: \"validate_customer_email\" **Bad:** - Workflow: \"Workflow 1\" - Step: \"step3\" ### 2.",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_3",
              "content": "**Technology**: Go with distributed locking via Redis **Replicas**: 4 pods (production), active-passive with leader election **Resource Allocation**: 2 vCPU, 4GB RAM per pod **Key Responsibilities**: - Parse and validate cron expressions (extended format supporting seconds) - Maintain schedule registry in PostgreSQL - Distributed scheduling with leader election (one active scheduler) - Missed execution handling with configurable catch-up policy - Schedule conflict detection and resolution - Time zone support for international schedules **Scheduling Architecture**: ``` PostgreSQL Schedules Table \u2502 \u25bc Leader Scheduler (elected via Redis) \u2502 \u251c\u2500 Scan for due schedules (every 10 seconds) \u251c\u2500 Acquire lock per schedule (prevents duplicates) \u251c\u2500 Publish to Kafka \u2192 workflow.events topic \u2514\u2500 Update last_run timestamp ``` **Schedule Types**: - **Cron-based**: `0 */5 * * * *` (every 5 minutes) - **One-time**: Specific timestamp for delayed execution - **Interval-based**: Every N seconds/minutes/hours **Reliability Features**: - Leader election using Redis with 30-second lease - Heartbeat mechanism to detect leader failure (5-second interval) - Automatic failover to standby scheduler (< 10 seconds) - Schedule versioning to handle updates during execution - Missed execution policy: SKIP, RUN_ONCE, or RUN_ALL **Performance Targets**: - Schedule evaluation: < 100ms per cycle - Accuracy: \u00b1 1 second for schedule triggers - Capacity: 100,000 active schedules --- ### Notification Service **Purpose**: Multi-channel notification delivery system supporting email, SMS, webhooks, and in-app notifications.",
              "score": null
            }
          ],
          "latency_ms": 5058.732574001624,
          "query_id": "realistic_020",
          "dimension": "synonym",
          "query": "How frequently can I schedule a workflow?"
        },
        {
          "key_facts": [
            "minimum scheduling interval is 1 minute",
            "The minimum scheduling interval is **1 minute**"
          ],
          "found_facts": [],
          "missed_facts": [
            "minimum scheduling interval is 1 minute",
            "The minimum scheduling interval is **1 minute**"
          ],
          "coverage": 0.0,
          "expected_docs": [
            "user_guide"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_2",
              "content": "**Technology**: Node.js with TypeScript, Bull queue library **Replicas**: 16 pods (production), auto-scaling 12-24 **Resource Allocation**: 4 vCPU, 8GB RAM per pod **Key Responsibilities**: - Parse and validate workflow definitions (JSON-based DSL) - Execute workflow steps with state machine pattern - Handle retries with exponential backoff (max 3 retries, backoff: 2^n seconds) - Coordinate parallel and sequential task execution - Manage workflow state persistence and recovery - Support for conditional branching and loops - Sub-workflow invocation and composition **Workflow Execution Model**: ``` Workflow Submitted \u2192 Validation \u2192 Queue in Kafka \u2502 \u25bc Workflow Engine picks up \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u25bc \u25bc \u25bc [Task 1 Exec] [Task 2 Exec] [Task 3 Exec] \u2502 \u2502 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u25bc State Update \u2192 PostgreSQL \u2502 \u25bc Publish Event \u2192 Kafka ``` **Workflow DSL Example**: ```json { \"workflow_id\": \"customer-onboarding-v2\", \"version\": \"2.1.0\", \"steps\": [ { \"id\": \"validate-customer\", \"type\": \"http_request\", \"config\": { \"url\": \"https://validation.internal/verify\", \"method\": \"POST\", \"timeout\": 5000, \"retry\": {\"max_attempts\": 3, \"backoff\": \"exponential\"} } }, { \"id\": \"create-account\", \"type\": \"database_operation\", \"depends_on\": [\"validate-customer\"] } ] } ``` **State Management**: - Workflow states: PENDING, RUNNING, COMPLETED, FAILED, CANCELLED - Step states: QUEUED, EXECUTING, SUCCESS, FAILED, SKIPPED - State transitions stored in PostgreSQL with event sourcing pattern - Checkpointing every 10 steps for long-running workflows **Performance Targets**: - Simple workflow (3-5 steps): < 2 seconds (P99) - Complex workflow (20+ steps): < 5 seconds (P99) - Throughput: 500 concurrent workflow executions per pod --- ### Scheduler Service **Purpose**: Time-based workflow triggering system supporting cron-like schedules and one-time delayed executions.",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_3",
              "content": "**Technology**: Go with distributed locking via Redis **Replicas**: 4 pods (production), active-passive with leader election **Resource Allocation**: 2 vCPU, 4GB RAM per pod **Key Responsibilities**: - Parse and validate cron expressions (extended format supporting seconds) - Maintain schedule registry in PostgreSQL - Distributed scheduling with leader election (one active scheduler) - Missed execution handling with configurable catch-up policy - Schedule conflict detection and resolution - Time zone support for international schedules **Scheduling Architecture**: ``` PostgreSQL Schedules Table \u2502 \u25bc Leader Scheduler (elected via Redis) \u2502 \u251c\u2500 Scan for due schedules (every 10 seconds) \u251c\u2500 Acquire lock per schedule (prevents duplicates) \u251c\u2500 Publish to Kafka \u2192 workflow.events topic \u2514\u2500 Update last_run timestamp ``` **Schedule Types**: - **Cron-based**: `0 */5 * * * *` (every 5 minutes) - **One-time**: Specific timestamp for delayed execution - **Interval-based**: Every N seconds/minutes/hours **Reliability Features**: - Leader election using Redis with 30-second lease - Heartbeat mechanism to detect leader failure (5-second interval) - Automatic failover to standby scheduler (< 10 seconds) - Schedule versioning to handle updates during execution - Missed execution policy: SKIP, RUN_ONCE, or RUN_ALL **Performance Targets**: - Schedule evaluation: < 100ms per cycle - Accuracy: \u00b1 1 second for schedule triggers - Capacity: 100,000 active schedules --- ### Notification Service **Purpose**: Multi-channel notification delivery system supporting email, SMS, webhooks, and in-app notifications.",
              "score": null
            },
            {
              "doc_id": "user_guide",
              "chunk_id": "user_guide_fix_8",
              "content": "Version Control For critical workflows: - Export YAML definitions regularly - Store in version control (Git) - Use pull requests for changes - Tag releases with semantic versioning ## Common Workflow Patterns Here are proven patterns for common automation scenarios: ### Pattern 1: Form to Database Capture form submissions and store in database: ```yaml name: \"Contact Form to Database\" trigger: type: webhook method: POST steps: - id: validate_submission action: javascript code: | if (!input.email || !input.message) { throw new Error(\"Email and message required\"); } return input; - id: check_duplicate action: database_query config: connection: \"{{secrets.DB_CONNECTION}}\" query: \"SELECT id FROM contacts WHERE email = $1 AND created_at > NOW() - INTERVAL '1 hour'\" parameters: - \"{{trigger.body.email}}\" - id: insert_contact action: database_query condition: \"{{length(steps.check_duplicate.rows) == 0}}\" config: connection: \"{{secrets.DB_CONNECTION}}\" query: | INSERT INTO contacts (name, email, message, source, created_at) VALUES ($1, $2, $3, $4, NOW()) parameters: - \"{{trigger.body.name}}\" - \"{{trigger.body.email}}\" - \"{{trigger.body.message}}\" - \"website_form\" - id: notify_sales action: slack_message condition: \"{{steps.insert_contact.affected_rows > 0}}\" config: channel: \"#leads\" text: \"New contact form submission from {{trigger.body.email}}\" ``` ### Pattern 2: API Polling Periodically check an API and take action on new items: ```yaml name: \"Poll GitHub Issues\" schedule: cron: \"*/15 * * * *\" # Every 15 minutes timezone: \"UTC\" steps: - id: get_last_check action: database_query config: query: \"SELECT last_checked_at FROM workflow_state WHERE workflow_id = $1\" parameters: - \"{{workflow.id}}\" - id: fetch_issues action: http_request config: method: GET url: \"https://api.github.com/repos/company/project/issues\" params: since: \"{{steps.get_last_check.rows[0].last_checked_at}}\" state: \"open\" headers: Authorization: \"token {{secrets.GITHUB_TOKEN}}\" - id: process_new_issues action: javascript code: | const issues = input.body; return { count: issues.length, issues: issues.map(i => ({ number: i.number, title: i.title, url: i.html_url })) }; - id: notify_team action: slack_message condition: \"{{steps.process_new_issues.output.count > 0}}\" config: channel: \"#engineering\" text: \"{{steps.process_new_issues.output.count}} new GitHub issues\" - id: update_last_check action: database_query config: query: | UPDATE workflow_state SET last_checked_at = $1 WHERE workflow_id = $2 parameters: - \"{{now()}}\" - \"{{workflow.id}}\" ``` ### Pattern 3: Multi-Step Approval Implement approval workflows with timeouts: ```yaml name: \"Expense Approval Workflow\" trigger: type: webhook method: POST steps: - id: create_approval_request action: database_query config: query: | INSERT INTO approvals (expense_id, amount, requester, status, created_at) VALUES ($1, $2, $3, 'pending', NOW()) RETURNING id parameters: - \"{{trigger.body.expense_id}}\" - \"{{trigger.body.amount}}\" - \"{{trigger.body.requester}}\" - id: notify_manager action: email config: to: \"{{trigger.body.manager_email}}\" subject: \"Expense Approval Required: ${{trigger.body.amount}}\" body: | An expense requires your approval: Amount: ${{trigger.body.amount}}",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_6",
              "content": "Data Validation Errors** ``` ValidationError: Field 'customer_id' is required but missing in 234 records ``` **Resolution:** ```bash # Add data quality checks cloudflow workflows update wf_9k2n4m8p1q \\ --step data_ingestion \\ --add-validator required_fields \\ --validator-config '{\"fields\": [\"customer_id\", \"timestamp\", \"amount\"]}' # Configure error handling cloudflow workflows update wf_9k2n4m8p1q \\ --step data_validation \\ --on-error continue \\ --error-threshold 5% # Fail if > 5% of records invalid ``` **2. External API Failures** ``` ExternalAPIError: API request to https://partner-api.example.com failed with status 502 ``` **Resolution:** ```bash # Add circuit breaker cloudflow workflows update wf_9k2n4m8p1q \\ --step external_api_call \\ --circuit-breaker-enabled true \\ --circuit-breaker-threshold 5 \\ --circuit-breaker-timeout 30000 # Configure fallback behavior cloudflow workflows update wf_9k2n4m8p1q \\ --step external_api_call \\ --fallback-action use_cached_data \\ --cache-ttl 3600 ``` --- ## Rate Limiting & Throttling ### 429 Too Many Requests #### Error Response ```json { \"error\": \"rate_limit_exceeded\", \"message\": \"API rate limit exceeded. Retry after 45 seconds.\", \"status\": 429, \"headers\": { \"X-RateLimit-Limit\": \"1000\", \"X-RateLimit-Remaining\": \"0\", \"X-RateLimit-Reset\": \"1706112345\", \"Retry-After\": \"45\" } } ``` #### Rate Limit Tiers CloudFlow enforces the following rate limits per workspace: | Tier | Requests/Minute | Requests/Hour | Concurrent Workflows | |------|-----------------|---------------|----------------------| | Free | 60 | 1,000 | 5 | | Standard | 1,000 | 50,000 | 50 | | Premium | 5,000 | 250,000 | 200 | | Enterprise | Custom | Custom | Unlimited | #### Checking Rate Limit Status ```bash # Check current rate limit status curl -I https://api.cloudflow.io/api/v1/workflows \\ -H \"Authorization: Bearer $CF_ACCESS_TOKEN\" # Extract rate limit headers curl -s -I https://api.cloudflow.io/api/v1/workflows \\ -H \"Authorization: Bearer $CF_ACCESS_TOKEN\" | \\ grep -i \"x-ratelimit\" # Output: # X-RateLimit-Limit: 1000 # X-RateLimit-Remaining: 247 # X-RateLimit-Reset: 1706112400 # Monitor rate limit usage cloudflow metrics query \\ --metric rate_limit_remaining \\ --workspace ws_abc123 \\ --last 1h \\ --interval 1m ``` #### Handling Rate Limits in Code **Python example with retry logic:** ```python import time import requests def cloudflow_api_call_with_retry(url, headers, max_retries=3): for attempt in range(max_retries): response = requests.get(url, headers=headers) if response.status_code == 429: retry_after = int(response.headers.get('Retry-After', 60)) print(f\"Rate limited.",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_2",
              "content": "Review Query Execution Plans** ```sql -- Connect to CloudFlow database cloudflow db connect --readonly -- Explain slow query EXPLAIN ANALYZE SELECT w.*, e.status, e.error_message FROM workflows w LEFT JOIN executions e ON w.id = e.workflow_id WHERE w.workspace_id = 'ws_abc123' AND e.created_at > NOW() - INTERVAL '7 days' ORDER BY e.created_at DESC; -- Check for missing indexes SELECT schemaname, tablename, indexname, indexdef FROM pg_indexes WHERE tablename IN ('workflows', 'executions', 'workflow_steps') ORDER BY tablename, indexname; ``` **3. Optimize Queries** Common optimization techniques: **Add missing indexes:** ```sql -- Index for workflow lookup by workspace CREATE INDEX CONCURRENTLY idx_workflows_workspace_created ON workflows(workspace_id, created_at DESC); -- Index for execution status queries CREATE INDEX CONCURRENTLY idx_executions_status_created ON executions(workflow_id, status, created_at DESC) WHERE status IN ('running', 'pending'); -- Composite index for common filter combinations CREATE INDEX CONCURRENTLY idx_executions_workspace_status ON executions(workspace_id, status, created_at) INCLUDE (error_message, retry_count); ``` **Use query result caching:** ```bash # Enable query cache for workspace metadata cloudflow config set cache.workspace.ttl 3600 # Configure Redis cache backend cloudflow config set cache.backend redis cloudflow config set cache.redis.host redis.cloudflow.svc.cluster.local cloudflow config set cache.redis.port 6379 ``` ### High API Latency #### Latency Breakdown Analysis ```bash # Generate latency report cloudflow metrics latency-report --endpoint \"/api/v1/workflows\" --last 24h # Sample output: # Endpoint: POST /api/v1/workflows/execute # P50: 245ms | P95: 1823ms | P99: 4521ms # Breakdown: # - Auth: 45ms (18%) # - DB Query: 156ms (64%) # - Business Logic: 32ms (13%) # - Response Serialization: 12ms (5%) ``` **Network latency issues:** ```bash # Test connectivity to CloudFlow API time curl -w \"@curl-format.txt\" -o /dev/null -s https://api.cloudflow.io/health # Create curl-format.txt: cat > curl-format.txt << EOF time_namelookup: %{time_namelookup}s\\n time_connect: %{time_connect}s\\n time_appconnect: %{time_appconnect}s\\n time_pretransfer: %{time_pretransfer}s\\n time_redirect: %{time_redirect}s\\n time_starttransfer: %{time_starttransfer}s\\n ----------\\n time_total: %{time_total}s\\n EOF # Trace route to API endpoint traceroute api.cloudflow.io # Check DNS resolution time dig api.cloudflow.io | grep \"Query time\" ``` ### Memory Leaks #### Detection ```bash # Monitor CloudFlow service memory usage kubectl top pods -n cloudflow --sort-by=memory # Get detailed memory metrics for specific pod kubectl exec -n cloudflow deploy/cloudflow-api -- \\ curl localhost:9090/metrics | grep memory # Check for OOMKilled pods kubectl get pods -n cloudflow --field-selector=status.phase=Failed | \\ grep OOMKilled # Review memory limits and requests kubectl describe deployment cloudflow-api -n cloudflow | \\ grep -A 5 \"Limits\\|Requests\" ``` #### Common Causes **1.",
              "score": null
            }
          ],
          "latency_ms": 760.3132349977386,
          "query_id": "realistic_020",
          "dimension": "problem",
          "query": "My every-30-seconds schedule is being rejected"
        },
        {
          "key_facts": [
            "minimum scheduling interval is 1 minute",
            "The minimum scheduling interval is **1 minute**"
          ],
          "found_facts": [
            "The minimum scheduling interval is **1 minute**"
          ],
          "missed_facts": [
            "minimum scheduling interval is 1 minute"
          ],
          "coverage": 0.5,
          "expected_docs": [
            "user_guide"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "user_guide",
              "chunk_id": "user_guide_fix_6",
              "content": "Filter by error type, date range, or execution ID 3. Click an execution to view full details 4. Click **\"Retry\"** to reprocess with the same input data ### Error Notifications Get notified when workflows fail: ```yaml workflow: notifications: on_failure: - type: email to: \"ops-team@company.com\" - type: slack channel: \"#alerts\" message: \"Workflow {{workflow.name}} failed: {{error.message}}\" on_success_after_retry: - type: slack channel: \"#monitoring\" message: \"Workflow recovered after {{error.attempts}} attempts\" ``` ## Workflow Limits CloudFlow enforces the following limits to ensure platform stability and performance: ### Steps Per Workflow - **Maximum**: 50 steps per workflow - **Recommendation**: Keep workflows focused and modular. If you need more steps, consider splitting into multiple workflows connected via webhooks. ### Execution Timeout - **Default**: 3600 seconds (60 minutes) - **Behavior**: Workflows exceeding this timeout are automatically terminated - **Custom Timeouts**: Enterprise plans can request custom timeout limits **Setting Step-Level Timeouts:** ```yaml - id: long_running_task action: http_request config: url: \"https://api.example.com/process\" timeout: 300 # 5 minutes for this specific step ``` ### Execution Limits - **Maximum**: 1000 executions per day (per workflow) - **Rate Limiting**: 100 concurrent executions per workflow - **Burst Limit**: 10 executions per second **What happens when limits are reached:** - New executions are queued automatically - Webhook triggers return HTTP 429 (Too Many Requests) - Scheduled executions are skipped (logged in audit trail) - Email notifications sent to workflow owner **Monitoring Usage:** View real-time metrics in your workflow dashboard: - Executions today: 847 / 1000 - Average duration: 12.3s - Success rate: 98.2% - Current queue depth: 3 ### Data Limits - **Maximum request/response size**: 10MB per action - **Maximum execution payload**: 50MB total - **Variable value size**: 1MB per variable ### Enterprise Plan Limits Enterprise customers can request increased limits: - Up to 100 steps per workflow - Up to 10,000 executions per day - Up to 7200 second timeout (2 hours) - Priority execution queue - Dedicated capacity allocation Contact sales@cloudflow.io for Enterprise pricing and custom limits. ## Best Practices Follow these best practices to build reliable, maintainable workflows: ### 1. Use Descriptive Names **Good:** - Workflow: \"Sync Customer Data from Salesforce to Database\" - Step: \"validate_customer_email\" **Bad:** - Workflow: \"Workflow 1\" - Step: \"step3\" ### 2.",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_5",
              "content": "exec_7h3j6k9m2n --show-bottlenecks ``` #### Solutions **1. Increase workflow timeout (if justified):** ```bash # Update workflow configuration cloudflow workflows update wf_9k2n4m8p1q \\ --timeout 7200 \\ --reason \"Large dataset processing requires extended time\" # Verify update cloudflow workflows get wf_9k2n4m8p1q | grep timeout ``` **2. Optimize slow steps:** ```bash # Enable parallel processing cloudflow workflows update wf_9k2n4m8p1q \\ --step data_transformation \\ --parallel-workers 8 \\ --batch-size 1000 # Add checkpointing for long operations cloudflow workflows update wf_9k2n4m8p1q \\ --step data_transformation \\ --enable-checkpointing \\ --checkpoint-interval 300 ``` **3. Split workflow into smaller workflows:** ```bash # Create sub-workflows cloudflow workflows create data-pipeline-part1 \\ --steps \"data_ingestion,data_validation\" \\ --timeout 1800 cloudflow workflows create data-pipeline-part2 \\ --steps \"data_transformation,data_export\" \\ --timeout 3600 \\ --trigger workflow_completed \\ --trigger-workflow data-pipeline-part1 ``` ### Retry Logic and Exponential Backoff CloudFlow implements automatic retry with exponential backoff for transient failures: - Max retries: 3 - Initial delay: 1 second - Backoff multiplier: 2 - Max delay: 60 seconds #### Retry Sequence ``` Attempt 1: Immediate Attempt 2: Wait 1s (2^0 * 1s) Attempt 3: Wait 2s (2^1 * 1s) Attempt 4: Wait 4s (2^2 * 1s) ``` #### Configuration ```bash # View current retry settings cloudflow workflows get wf_9k2n4m8p1q --format json | jq '.retry_policy' # Customize retry behavior cloudflow workflows update wf_9k2n4m8p1q \\ --retry-max-attempts 5 \\ --retry-initial-delay 2000 \\ --retry-backoff-multiplier 2 \\ --retry-max-delay 120000 \\ --retry-on-errors \"NETWORK_ERROR,TIMEOUT,RATE_LIMIT\" # Disable retry for specific step cloudflow workflows update wf_9k2n4m8p1q \\ --step data_export \\ --retry-enabled false ``` #### Monitoring Retries ```bash # List failed executions with retry information cloudflow workflows executions list \\ --status FAILED \\ --show-retries \\ --last 7d # Get retry history for specific execution cloudflow workflows executions retries exec_7h3j6k9m2n # Output: # Execution: exec_7h3j6k9m2n # Attempt 1: FAILED - NetworkError: Connection refused (delay: 0ms) # Attempt 2: FAILED - NetworkError: Connection timeout (delay: 1000ms) # Attempt 3: FAILED - NetworkError: Connection timeout (delay: 2000ms) # Attempt 4: FAILED - NetworkError: Connection timeout (delay: 4000ms) # Final Status: FAILED_AFTER_RETRIES ``` ### Workflow Step Failures #### Common Error Patterns **1.",
              "score": null
            },
            {
              "doc_id": "api_reference",
              "chunk_id": "api_reference_fix_1",
              "content": "## Rate Limiting To ensure fair usage and system stability, CloudFlow enforces rate limits on all API endpoints. **Default Limits:** - 100 requests per minute per authenticated user - 20 requests per minute for unauthenticated requests - Burst allowance: 150 requests in a 10-second window ### Rate Limit Headers Every API response includes rate limit information: ``` X-RateLimit-Limit: 100 X-RateLimit-Remaining: 87 X-RateLimit-Reset: 1640995200 ``` When you exceed the rate limit, you'll receive a `429 Too Many Requests` response: ```json { \"error\": { \"code\": \"rate_limit_exceeded\", \"message\": \"Rate limit exceeded. Please retry after 42 seconds.\", \"retry_after\": 42 } } ``` **Best Practices:** - Monitor `X-RateLimit-Remaining` header values - Implement exponential backoff when receiving 429 responses - Cache responses when appropriate to reduce API calls - Consider upgrading to Enterprise tier for higher limits (1000 req/min) ## Pagination List endpoints return paginated results to optimize performance. Use `limit` and `offset` parameters to navigate through result sets. **Parameters:** - `limit`: Number of items per page (default: 20, max: 100) - `offset`: Number of items to skip (default: 0) **Example Request:** ```bash curl -H \"X-API-Key: YOUR_API_KEY\" \\ \"https://api.cloudflow.io/v2/workflows?limit=50&offset=100\" ``` **Response Structure:** ```json { \"data\": [...], \"pagination\": { \"total\": 347, \"limit\": 50, \"offset\": 100, \"has_more\": true } } ``` **Python Example:** ```python import requests def fetch_all_workflows(api_key): base_url = \"https://api.cloudflow.io/v2/workflows\" headers = {\"X-API-Key\": api_key} all_workflows = [] offset = 0 limit = 100 while True: response = requests.get( base_url, headers=headers, params={\"limit\": limit, \"offset\": offset} ) data = response.json() all_workflows.extend(data['data']) if not data['pagination']['has_more']: break offset += limit return all_workflows ``` ## Endpoints ### Workflows #### List Workflows Retrieve a paginated list of all workflows in your account. **Endpoint:** `GET /workflows` **Query Parameters:** - `status` (optional): Filter by status (`active`, `paused`, `archived`) - `created_after` (optional): ISO 8601 timestamp - `limit` (optional): Items per page (max 100) - `offset` (optional): Pagination offset **Example Request:** ```bash curl -H \"X-API-Key: YOUR_API_KEY\" \\ \"https://api.cloudflow.io/v2/workflows?status=active&limit=25\" ``` **Example Response (200 OK):** ```json { \"data\": [ { \"id\": \"wf_8x7k2m9p\", \"name\": \"Data Processing Pipeline\", \"description\": \"Processes customer data every hour\", \"status\": \"active\", \"trigger\": { \"type\": \"schedule\", \"cron\": \"0 * * * *\" }, \"steps\": 12, \"created_at\": \"2026-01-15T10:30:00Z\", \"updated_at\": \"2026-01-23T14:22:00Z\", \"last_run\": \"2026-01-24T09:00:00Z\" } ], \"pagination\": { \"total\": 1, \"limit\": 25, \"offset\": 0, \"has_more\": false } } ``` #### Create Workflow Create a new workflow with specified configuration.",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_2",
              "content": "Review Query Execution Plans** ```sql -- Connect to CloudFlow database cloudflow db connect --readonly -- Explain slow query EXPLAIN ANALYZE SELECT w.*, e.status, e.error_message FROM workflows w LEFT JOIN executions e ON w.id = e.workflow_id WHERE w.workspace_id = 'ws_abc123' AND e.created_at > NOW() - INTERVAL '7 days' ORDER BY e.created_at DESC; -- Check for missing indexes SELECT schemaname, tablename, indexname, indexdef FROM pg_indexes WHERE tablename IN ('workflows', 'executions', 'workflow_steps') ORDER BY tablename, indexname; ``` **3. Optimize Queries** Common optimization techniques: **Add missing indexes:** ```sql -- Index for workflow lookup by workspace CREATE INDEX CONCURRENTLY idx_workflows_workspace_created ON workflows(workspace_id, created_at DESC); -- Index for execution status queries CREATE INDEX CONCURRENTLY idx_executions_status_created ON executions(workflow_id, status, created_at DESC) WHERE status IN ('running', 'pending'); -- Composite index for common filter combinations CREATE INDEX CONCURRENTLY idx_executions_workspace_status ON executions(workspace_id, status, created_at) INCLUDE (error_message, retry_count); ``` **Use query result caching:** ```bash # Enable query cache for workspace metadata cloudflow config set cache.workspace.ttl 3600 # Configure Redis cache backend cloudflow config set cache.backend redis cloudflow config set cache.redis.host redis.cloudflow.svc.cluster.local cloudflow config set cache.redis.port 6379 ``` ### High API Latency #### Latency Breakdown Analysis ```bash # Generate latency report cloudflow metrics latency-report --endpoint \"/api/v1/workflows\" --last 24h # Sample output: # Endpoint: POST /api/v1/workflows/execute # P50: 245ms | P95: 1823ms | P99: 4521ms # Breakdown: # - Auth: 45ms (18%) # - DB Query: 156ms (64%) # - Business Logic: 32ms (13%) # - Response Serialization: 12ms (5%) ``` **Network latency issues:** ```bash # Test connectivity to CloudFlow API time curl -w \"@curl-format.txt\" -o /dev/null -s https://api.cloudflow.io/health # Create curl-format.txt: cat > curl-format.txt << EOF time_namelookup: %{time_namelookup}s\\n time_connect: %{time_connect}s\\n time_appconnect: %{time_appconnect}s\\n time_pretransfer: %{time_pretransfer}s\\n time_redirect: %{time_redirect}s\\n time_starttransfer: %{time_starttransfer}s\\n ----------\\n time_total: %{time_total}s\\n EOF # Trace route to API endpoint traceroute api.cloudflow.io # Check DNS resolution time dig api.cloudflow.io | grep \"Query time\" ``` ### Memory Leaks #### Detection ```bash # Monitor CloudFlow service memory usage kubectl top pods -n cloudflow --sort-by=memory # Get detailed memory metrics for specific pod kubectl exec -n cloudflow deploy/cloudflow-api -- \\ curl localhost:9090/metrics | grep memory # Check for OOMKilled pods kubectl get pods -n cloudflow --field-selector=status.phase=Failed | \\ grep OOMKilled # Review memory limits and requests kubectl describe deployment cloudflow-api -n cloudflow | \\ grep -A 5 \"Limits\\|Requests\" ``` #### Common Causes **1.",
              "score": null
            },
            {
              "doc_id": "user_guide",
              "chunk_id": "user_guide_fix_4",
              "content": "### Cron Syntax Use standard cron expressions to define schedules: ``` * * * * * \u252c \u252c \u252c \u252c \u252c \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2514\u2500\u2500\u2500 Day of Week (0-6, Sunday=0) \u2502 \u2502 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Month (1-12) \u2502 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Day of Month (1-31) \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Hour (0-23) \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Minute (0-59) ``` **Common Cron Patterns:** | Pattern | Description | |---------|-------------| | `*/5 * * * *` | Every 5 minutes | | `0 * * * *` | Every hour at minute 0 | | `0 9 * * *` | Daily at 9:00 AM | | `0 9 * * 1` | Every Monday at 9:00 AM | | `0 0 1 * *` | First day of every month at midnight | | `0 0 * * 0` | Every Sunday at midnight | | `0 9-17 * * 1-5` | Every hour from 9 AM to 5 PM, Monday-Friday | | `*/15 9-17 * * 1-5` | Every 15 minutes during business hours | **Important:** The minimum scheduling interval is **1 minute**. Expressions that evaluate to more frequent executions will be rejected. ### Timezone Handling All scheduled workflows run in **UTC by default**. To account for your local timezone: **Option 1: Convert to UTC** If you want a workflow to run at 9:00 AM EST (UTC-5), schedule it for 14:00 UTC: ``` 0 14 * * * # 9:00 AM EST = 14:00 UTC ``` **Option 2: Use Timezone Configuration** Specify a timezone in your workflow configuration: ```yaml schedule: cron: \"0 9 * * *\" timezone: \"America/New_York\" # IANA timezone identifier ``` **Supported Timezones:** CloudFlow supports all IANA timezone identifiers, including: - `America/New_York` (Eastern Time) - `America/Chicago` (Central Time) - `America/Los_Angeles` (Pacific Time) - `Europe/London` (GMT/BST) - `Asia/Tokyo` (Japan Standard Time) - `Australia/Sydney` (Australian Eastern Time) **Daylight Saving Time:** When using timezone configuration, CloudFlow automatically handles DST transitions. A workflow scheduled for 9:00 AM will run at 9:00 AM local time regardless of DST changes. ### Schedule Management **Creating a Schedule:** 1. Open your workflow in the editor 2. Click the **\"Trigger\"** section 3. Select **\"Schedule\"** as the trigger type 4. Enter your cron expression or use the visual schedule builder 5. Select your timezone 6.",
              "score": null
            }
          ],
          "latency_ms": 823.8391849990876,
          "query_id": "realistic_020",
          "dimension": "casual",
          "query": "min schedule interval"
        },
        {
          "key_facts": [
            "minimum scheduling interval is 1 minute",
            "The minimum scheduling interval is **1 minute**"
          ],
          "found_facts": [],
          "missed_facts": [
            "minimum scheduling interval is 1 minute",
            "The minimum scheduling interval is **1 minute**"
          ],
          "coverage": 0.0,
          "expected_docs": [
            "user_guide"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "user_guide",
              "chunk_id": "user_guide_fix_6",
              "content": "Filter by error type, date range, or execution ID 3. Click an execution to view full details 4. Click **\"Retry\"** to reprocess with the same input data ### Error Notifications Get notified when workflows fail: ```yaml workflow: notifications: on_failure: - type: email to: \"ops-team@company.com\" - type: slack channel: \"#alerts\" message: \"Workflow {{workflow.name}} failed: {{error.message}}\" on_success_after_retry: - type: slack channel: \"#monitoring\" message: \"Workflow recovered after {{error.attempts}} attempts\" ``` ## Workflow Limits CloudFlow enforces the following limits to ensure platform stability and performance: ### Steps Per Workflow - **Maximum**: 50 steps per workflow - **Recommendation**: Keep workflows focused and modular. If you need more steps, consider splitting into multiple workflows connected via webhooks. ### Execution Timeout - **Default**: 3600 seconds (60 minutes) - **Behavior**: Workflows exceeding this timeout are automatically terminated - **Custom Timeouts**: Enterprise plans can request custom timeout limits **Setting Step-Level Timeouts:** ```yaml - id: long_running_task action: http_request config: url: \"https://api.example.com/process\" timeout: 300 # 5 minutes for this specific step ``` ### Execution Limits - **Maximum**: 1000 executions per day (per workflow) - **Rate Limiting**: 100 concurrent executions per workflow - **Burst Limit**: 10 executions per second **What happens when limits are reached:** - New executions are queued automatically - Webhook triggers return HTTP 429 (Too Many Requests) - Scheduled executions are skipped (logged in audit trail) - Email notifications sent to workflow owner **Monitoring Usage:** View real-time metrics in your workflow dashboard: - Executions today: 847 / 1000 - Average duration: 12.3s - Success rate: 98.2% - Current queue depth: 3 ### Data Limits - **Maximum request/response size**: 10MB per action - **Maximum execution payload**: 50MB total - **Variable value size**: 1MB per variable ### Enterprise Plan Limits Enterprise customers can request increased limits: - Up to 100 steps per workflow - Up to 10,000 executions per day - Up to 7200 second timeout (2 hours) - Priority execution queue - Dedicated capacity allocation Contact sales@cloudflow.io for Enterprise pricing and custom limits. ## Best Practices Follow these best practices to build reliable, maintainable workflows: ### 1. Use Descriptive Names **Good:** - Workflow: \"Sync Customer Data from Salesforce to Database\" - Step: \"validate_customer_email\" **Bad:** - Workflow: \"Workflow 1\" - Step: \"step3\" ### 2.",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_2",
              "content": "Review Query Execution Plans** ```sql -- Connect to CloudFlow database cloudflow db connect --readonly -- Explain slow query EXPLAIN ANALYZE SELECT w.*, e.status, e.error_message FROM workflows w LEFT JOIN executions e ON w.id = e.workflow_id WHERE w.workspace_id = 'ws_abc123' AND e.created_at > NOW() - INTERVAL '7 days' ORDER BY e.created_at DESC; -- Check for missing indexes SELECT schemaname, tablename, indexname, indexdef FROM pg_indexes WHERE tablename IN ('workflows', 'executions', 'workflow_steps') ORDER BY tablename, indexname; ``` **3. Optimize Queries** Common optimization techniques: **Add missing indexes:** ```sql -- Index for workflow lookup by workspace CREATE INDEX CONCURRENTLY idx_workflows_workspace_created ON workflows(workspace_id, created_at DESC); -- Index for execution status queries CREATE INDEX CONCURRENTLY idx_executions_status_created ON executions(workflow_id, status, created_at DESC) WHERE status IN ('running', 'pending'); -- Composite index for common filter combinations CREATE INDEX CONCURRENTLY idx_executions_workspace_status ON executions(workspace_id, status, created_at) INCLUDE (error_message, retry_count); ``` **Use query result caching:** ```bash # Enable query cache for workspace metadata cloudflow config set cache.workspace.ttl 3600 # Configure Redis cache backend cloudflow config set cache.backend redis cloudflow config set cache.redis.host redis.cloudflow.svc.cluster.local cloudflow config set cache.redis.port 6379 ``` ### High API Latency #### Latency Breakdown Analysis ```bash # Generate latency report cloudflow metrics latency-report --endpoint \"/api/v1/workflows\" --last 24h # Sample output: # Endpoint: POST /api/v1/workflows/execute # P50: 245ms | P95: 1823ms | P99: 4521ms # Breakdown: # - Auth: 45ms (18%) # - DB Query: 156ms (64%) # - Business Logic: 32ms (13%) # - Response Serialization: 12ms (5%) ``` **Network latency issues:** ```bash # Test connectivity to CloudFlow API time curl -w \"@curl-format.txt\" -o /dev/null -s https://api.cloudflow.io/health # Create curl-format.txt: cat > curl-format.txt << EOF time_namelookup: %{time_namelookup}s\\n time_connect: %{time_connect}s\\n time_appconnect: %{time_appconnect}s\\n time_pretransfer: %{time_pretransfer}s\\n time_redirect: %{time_redirect}s\\n time_starttransfer: %{time_starttransfer}s\\n ----------\\n time_total: %{time_total}s\\n EOF # Trace route to API endpoint traceroute api.cloudflow.io # Check DNS resolution time dig api.cloudflow.io | grep \"Query time\" ``` ### Memory Leaks #### Detection ```bash # Monitor CloudFlow service memory usage kubectl top pods -n cloudflow --sort-by=memory # Get detailed memory metrics for specific pod kubectl exec -n cloudflow deploy/cloudflow-api -- \\ curl localhost:9090/metrics | grep memory # Check for OOMKilled pods kubectl get pods -n cloudflow --field-selector=status.phase=Failed | \\ grep OOMKilled # Review memory limits and requests kubectl describe deployment cloudflow-api -n cloudflow | \\ grep -A 5 \"Limits\\|Requests\" ``` #### Common Causes **1.",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_2",
              "content": "**Technology**: Node.js with TypeScript, Bull queue library **Replicas**: 16 pods (production), auto-scaling 12-24 **Resource Allocation**: 4 vCPU, 8GB RAM per pod **Key Responsibilities**: - Parse and validate workflow definitions (JSON-based DSL) - Execute workflow steps with state machine pattern - Handle retries with exponential backoff (max 3 retries, backoff: 2^n seconds) - Coordinate parallel and sequential task execution - Manage workflow state persistence and recovery - Support for conditional branching and loops - Sub-workflow invocation and composition **Workflow Execution Model**: ``` Workflow Submitted \u2192 Validation \u2192 Queue in Kafka \u2502 \u25bc Workflow Engine picks up \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u25bc \u25bc \u25bc [Task 1 Exec] [Task 2 Exec] [Task 3 Exec] \u2502 \u2502 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u25bc State Update \u2192 PostgreSQL \u2502 \u25bc Publish Event \u2192 Kafka ``` **Workflow DSL Example**: ```json { \"workflow_id\": \"customer-onboarding-v2\", \"version\": \"2.1.0\", \"steps\": [ { \"id\": \"validate-customer\", \"type\": \"http_request\", \"config\": { \"url\": \"https://validation.internal/verify\", \"method\": \"POST\", \"timeout\": 5000, \"retry\": {\"max_attempts\": 3, \"backoff\": \"exponential\"} } }, { \"id\": \"create-account\", \"type\": \"database_operation\", \"depends_on\": [\"validate-customer\"] } ] } ``` **State Management**: - Workflow states: PENDING, RUNNING, COMPLETED, FAILED, CANCELLED - Step states: QUEUED, EXECUTING, SUCCESS, FAILED, SKIPPED - State transitions stored in PostgreSQL with event sourcing pattern - Checkpointing every 10 steps for long-running workflows **Performance Targets**: - Simple workflow (3-5 steps): < 2 seconds (P99) - Complex workflow (20+ steps): < 5 seconds (P99) - Throughput: 500 concurrent workflow executions per pod --- ### Scheduler Service **Purpose**: Time-based workflow triggering system supporting cron-like schedules and one-time delayed executions.",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_3",
              "content": "**Technology**: Go with distributed locking via Redis **Replicas**: 4 pods (production), active-passive with leader election **Resource Allocation**: 2 vCPU, 4GB RAM per pod **Key Responsibilities**: - Parse and validate cron expressions (extended format supporting seconds) - Maintain schedule registry in PostgreSQL - Distributed scheduling with leader election (one active scheduler) - Missed execution handling with configurable catch-up policy - Schedule conflict detection and resolution - Time zone support for international schedules **Scheduling Architecture**: ``` PostgreSQL Schedules Table \u2502 \u25bc Leader Scheduler (elected via Redis) \u2502 \u251c\u2500 Scan for due schedules (every 10 seconds) \u251c\u2500 Acquire lock per schedule (prevents duplicates) \u251c\u2500 Publish to Kafka \u2192 workflow.events topic \u2514\u2500 Update last_run timestamp ``` **Schedule Types**: - **Cron-based**: `0 */5 * * * *` (every 5 minutes) - **One-time**: Specific timestamp for delayed execution - **Interval-based**: Every N seconds/minutes/hours **Reliability Features**: - Leader election using Redis with 30-second lease - Heartbeat mechanism to detect leader failure (5-second interval) - Automatic failover to standby scheduler (< 10 seconds) - Schedule versioning to handle updates during execution - Missed execution policy: SKIP, RUN_ONCE, or RUN_ALL **Performance Targets**: - Schedule evaluation: < 100ms per cycle - Accuracy: \u00b1 1 second for schedule triggers - Capacity: 100,000 active schedules --- ### Notification Service **Purpose**: Multi-channel notification delivery system supporting email, SMS, webhooks, and in-app notifications.",
              "score": null
            },
            {
              "doc_id": "user_guide",
              "chunk_id": "user_guide_fix_3",
              "content": "``` **Slack App Integration:** - Install the CloudFlow Slack app in your workspace - Authenticate once per workspace - Use @mentions, emojis, and rich formatting - Send messages as a bot or as yourself ## Variables and Expressions CloudFlow uses a powerful templating system to work with dynamic data in your workflows. ### Variable Syntax Variables are enclosed in double curly braces: `{{variable_name}}` **Accessing Trigger Data:** ``` {{trigger.body.order_id}} {{trigger.headers.user_agent}} {{trigger.query.page}} ``` **Accessing Step Outputs:** ``` {{steps.step_id.output}} {{steps.fetch_user.body.email}} {{steps.query_db.rows[0].name}} ``` **System Variables:** ``` {{workflow.id}} # Current workflow ID {{workflow.name}} # Workflow name {{execution.id}} # Current execution ID {{execution.started_at}} # Execution start timestamp {{env.ENVIRONMENT}} # Environment variable {{secrets.API_KEY}} # Secret value (encrypted at rest) ``` ### Built-in Functions CloudFlow provides built-in functions for common data transformations: **String Functions:** ``` {{upper(user.name)}} # Convert to uppercase {{lower(email)}} # Convert to lowercase {{trim(input)}} # Remove whitespace {{substring(text, 0, 10)}} # Extract substring {{replace(text, \"old\", \"new\")}} # Replace text {{split(csv_string, \",\")}} # Split string into array {{join(array, \", \")}} # Join array into string ``` **Date/Time Functions:** ``` {{now()}} # Current timestamp (ISO 8601) {{now(\"America/New_York\")}} # Current time in timezone {{format_date(timestamp, \"YYYY-MM-DD\")}} # Format date {{add_days(date, 7)}} # Add days to date {{diff_hours(date1, date2)}} # Difference in hours ``` **Math Functions:** ``` {{add(num1, num2)}} # Addition {{subtract(num1, num2)}} # Subtraction {{multiply(num1, num2)}} # Multiplication {{divide(num1, num2)}} # Division {{round(number, 2)}} # Round to decimal places {{max(array)}} # Maximum value {{min(array)}} # Minimum value ``` **Array Functions:** ``` {{length(array)}} # Array length {{first(array)}} # First element {{last(array)}} # Last element {{filter(array, \"status\", \"active\")}} # Filter array {{map(array, \"id\")}} # Extract property from objects {{unique(array)}} # Remove duplicates ``` **Conditional Functions:** ``` {{if(condition, \"true_value\", \"false_value\")}} {{default(value, \"fallback\")}} # Use fallback if value is null/undefined ``` ### Expression Examples **Complex nested expressions:** ``` {{upper(trim(steps.get_user.body.name))}} ``` **Conditional formatting:** ``` Order Status: {{if(steps.check_inventory.in_stock, \"\u2713 Available\", \"\u2717 Out of Stock\")}} ``` **Date calculations:** ``` Due Date: {{format_date(add_days(now(), 7), \"MMMM DD, YYYY\")}} ``` **Dynamic URLs:** ``` https://api.example.com/v1/users/{{user_id}}/orders?status={{status}}&limit={{default(limit, 10)}} ``` ## Scheduling CloudFlow supports powerful scheduling options for recurring workflows.",
              "score": null
            }
          ],
          "latency_ms": 1012.0008260018949,
          "query_id": "realistic_020",
          "dimension": "contextual",
          "query": "I need near real-time execution, what's the fastest schedule I can set?"
        },
        {
          "key_facts": [
            "minimum scheduling interval is 1 minute",
            "The minimum scheduling interval is **1 minute**"
          ],
          "found_facts": [],
          "missed_facts": [
            "minimum scheduling interval is 1 minute",
            "The minimum scheduling interval is **1 minute**"
          ],
          "coverage": 0.0,
          "expected_docs": [
            "user_guide"
          ],
          "retrieved_chunks": [
            {
              "doc_id": "user_guide",
              "chunk_id": "user_guide_fix_8",
              "content": "Version Control For critical workflows: - Export YAML definitions regularly - Store in version control (Git) - Use pull requests for changes - Tag releases with semantic versioning ## Common Workflow Patterns Here are proven patterns for common automation scenarios: ### Pattern 1: Form to Database Capture form submissions and store in database: ```yaml name: \"Contact Form to Database\" trigger: type: webhook method: POST steps: - id: validate_submission action: javascript code: | if (!input.email || !input.message) { throw new Error(\"Email and message required\"); } return input; - id: check_duplicate action: database_query config: connection: \"{{secrets.DB_CONNECTION}}\" query: \"SELECT id FROM contacts WHERE email = $1 AND created_at > NOW() - INTERVAL '1 hour'\" parameters: - \"{{trigger.body.email}}\" - id: insert_contact action: database_query condition: \"{{length(steps.check_duplicate.rows) == 0}}\" config: connection: \"{{secrets.DB_CONNECTION}}\" query: | INSERT INTO contacts (name, email, message, source, created_at) VALUES ($1, $2, $3, $4, NOW()) parameters: - \"{{trigger.body.name}}\" - \"{{trigger.body.email}}\" - \"{{trigger.body.message}}\" - \"website_form\" - id: notify_sales action: slack_message condition: \"{{steps.insert_contact.affected_rows > 0}}\" config: channel: \"#leads\" text: \"New contact form submission from {{trigger.body.email}}\" ``` ### Pattern 2: API Polling Periodically check an API and take action on new items: ```yaml name: \"Poll GitHub Issues\" schedule: cron: \"*/15 * * * *\" # Every 15 minutes timezone: \"UTC\" steps: - id: get_last_check action: database_query config: query: \"SELECT last_checked_at FROM workflow_state WHERE workflow_id = $1\" parameters: - \"{{workflow.id}}\" - id: fetch_issues action: http_request config: method: GET url: \"https://api.github.com/repos/company/project/issues\" params: since: \"{{steps.get_last_check.rows[0].last_checked_at}}\" state: \"open\" headers: Authorization: \"token {{secrets.GITHUB_TOKEN}}\" - id: process_new_issues action: javascript code: | const issues = input.body; return { count: issues.length, issues: issues.map(i => ({ number: i.number, title: i.title, url: i.html_url })) }; - id: notify_team action: slack_message condition: \"{{steps.process_new_issues.output.count > 0}}\" config: channel: \"#engineering\" text: \"{{steps.process_new_issues.output.count}} new GitHub issues\" - id: update_last_check action: database_query config: query: | UPDATE workflow_state SET last_checked_at = $1 WHERE workflow_id = $2 parameters: - \"{{now()}}\" - \"{{workflow.id}}\" ``` ### Pattern 3: Multi-Step Approval Implement approval workflows with timeouts: ```yaml name: \"Expense Approval Workflow\" trigger: type: webhook method: POST steps: - id: create_approval_request action: database_query config: query: | INSERT INTO approvals (expense_id, amount, requester, status, created_at) VALUES ($1, $2, $3, 'pending', NOW()) RETURNING id parameters: - \"{{trigger.body.expense_id}}\" - \"{{trigger.body.amount}}\" - \"{{trigger.body.requester}}\" - id: notify_manager action: email config: to: \"{{trigger.body.manager_email}}\" subject: \"Expense Approval Required: ${{trigger.body.amount}}\" body: | An expense requires your approval: Amount: ${{trigger.body.amount}}",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_2",
              "content": "**Technology**: Node.js with TypeScript, Bull queue library **Replicas**: 16 pods (production), auto-scaling 12-24 **Resource Allocation**: 4 vCPU, 8GB RAM per pod **Key Responsibilities**: - Parse and validate workflow definitions (JSON-based DSL) - Execute workflow steps with state machine pattern - Handle retries with exponential backoff (max 3 retries, backoff: 2^n seconds) - Coordinate parallel and sequential task execution - Manage workflow state persistence and recovery - Support for conditional branching and loops - Sub-workflow invocation and composition **Workflow Execution Model**: ``` Workflow Submitted \u2192 Validation \u2192 Queue in Kafka \u2502 \u25bc Workflow Engine picks up \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u25bc \u25bc \u25bc [Task 1 Exec] [Task 2 Exec] [Task 3 Exec] \u2502 \u2502 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u25bc State Update \u2192 PostgreSQL \u2502 \u25bc Publish Event \u2192 Kafka ``` **Workflow DSL Example**: ```json { \"workflow_id\": \"customer-onboarding-v2\", \"version\": \"2.1.0\", \"steps\": [ { \"id\": \"validate-customer\", \"type\": \"http_request\", \"config\": { \"url\": \"https://validation.internal/verify\", \"method\": \"POST\", \"timeout\": 5000, \"retry\": {\"max_attempts\": 3, \"backoff\": \"exponential\"} } }, { \"id\": \"create-account\", \"type\": \"database_operation\", \"depends_on\": [\"validate-customer\"] } ] } ``` **State Management**: - Workflow states: PENDING, RUNNING, COMPLETED, FAILED, CANCELLED - Step states: QUEUED, EXECUTING, SUCCESS, FAILED, SKIPPED - State transitions stored in PostgreSQL with event sourcing pattern - Checkpointing every 10 steps for long-running workflows **Performance Targets**: - Simple workflow (3-5 steps): < 2 seconds (P99) - Complex workflow (20+ steps): < 5 seconds (P99) - Throughput: 500 concurrent workflow executions per pod --- ### Scheduler Service **Purpose**: Time-based workflow triggering system supporting cron-like schedules and one-time delayed executions.",
              "score": null
            },
            {
              "doc_id": "architecture_overview",
              "chunk_id": "architecture_overview_fix_3",
              "content": "**Technology**: Go with distributed locking via Redis **Replicas**: 4 pods (production), active-passive with leader election **Resource Allocation**: 2 vCPU, 4GB RAM per pod **Key Responsibilities**: - Parse and validate cron expressions (extended format supporting seconds) - Maintain schedule registry in PostgreSQL - Distributed scheduling with leader election (one active scheduler) - Missed execution handling with configurable catch-up policy - Schedule conflict detection and resolution - Time zone support for international schedules **Scheduling Architecture**: ``` PostgreSQL Schedules Table \u2502 \u25bc Leader Scheduler (elected via Redis) \u2502 \u251c\u2500 Scan for due schedules (every 10 seconds) \u251c\u2500 Acquire lock per schedule (prevents duplicates) \u251c\u2500 Publish to Kafka \u2192 workflow.events topic \u2514\u2500 Update last_run timestamp ``` **Schedule Types**: - **Cron-based**: `0 */5 * * * *` (every 5 minutes) - **One-time**: Specific timestamp for delayed execution - **Interval-based**: Every N seconds/minutes/hours **Reliability Features**: - Leader election using Redis with 30-second lease - Heartbeat mechanism to detect leader failure (5-second interval) - Automatic failover to standby scheduler (< 10 seconds) - Schedule versioning to handle updates during execution - Missed execution policy: SKIP, RUN_ONCE, or RUN_ALL **Performance Targets**: - Schedule evaluation: < 100ms per cycle - Accuracy: \u00b1 1 second for schedule triggers - Capacity: 100,000 active schedules --- ### Notification Service **Purpose**: Multi-channel notification delivery system supporting email, SMS, webhooks, and in-app notifications.",
              "score": null
            },
            {
              "doc_id": "troubleshooting_guide",
              "chunk_id": "troubleshooting_guide_fix_2",
              "content": "Review Query Execution Plans** ```sql -- Connect to CloudFlow database cloudflow db connect --readonly -- Explain slow query EXPLAIN ANALYZE SELECT w.*, e.status, e.error_message FROM workflows w LEFT JOIN executions e ON w.id = e.workflow_id WHERE w.workspace_id = 'ws_abc123' AND e.created_at > NOW() - INTERVAL '7 days' ORDER BY e.created_at DESC; -- Check for missing indexes SELECT schemaname, tablename, indexname, indexdef FROM pg_indexes WHERE tablename IN ('workflows', 'executions', 'workflow_steps') ORDER BY tablename, indexname; ``` **3. Optimize Queries** Common optimization techniques: **Add missing indexes:** ```sql -- Index for workflow lookup by workspace CREATE INDEX CONCURRENTLY idx_workflows_workspace_created ON workflows(workspace_id, created_at DESC); -- Index for execution status queries CREATE INDEX CONCURRENTLY idx_executions_status_created ON executions(workflow_id, status, created_at DESC) WHERE status IN ('running', 'pending'); -- Composite index for common filter combinations CREATE INDEX CONCURRENTLY idx_executions_workspace_status ON executions(workspace_id, status, created_at) INCLUDE (error_message, retry_count); ``` **Use query result caching:** ```bash # Enable query cache for workspace metadata cloudflow config set cache.workspace.ttl 3600 # Configure Redis cache backend cloudflow config set cache.backend redis cloudflow config set cache.redis.host redis.cloudflow.svc.cluster.local cloudflow config set cache.redis.port 6379 ``` ### High API Latency #### Latency Breakdown Analysis ```bash # Generate latency report cloudflow metrics latency-report --endpoint \"/api/v1/workflows\" --last 24h # Sample output: # Endpoint: POST /api/v1/workflows/execute # P50: 245ms | P95: 1823ms | P99: 4521ms # Breakdown: # - Auth: 45ms (18%) # - DB Query: 156ms (64%) # - Business Logic: 32ms (13%) # - Response Serialization: 12ms (5%) ``` **Network latency issues:** ```bash # Test connectivity to CloudFlow API time curl -w \"@curl-format.txt\" -o /dev/null -s https://api.cloudflow.io/health # Create curl-format.txt: cat > curl-format.txt << EOF time_namelookup: %{time_namelookup}s\\n time_connect: %{time_connect}s\\n time_appconnect: %{time_appconnect}s\\n time_pretransfer: %{time_pretransfer}s\\n time_redirect: %{time_redirect}s\\n time_starttransfer: %{time_starttransfer}s\\n ----------\\n time_total: %{time_total}s\\n EOF # Trace route to API endpoint traceroute api.cloudflow.io # Check DNS resolution time dig api.cloudflow.io | grep \"Query time\" ``` ### Memory Leaks #### Detection ```bash # Monitor CloudFlow service memory usage kubectl top pods -n cloudflow --sort-by=memory # Get detailed memory metrics for specific pod kubectl exec -n cloudflow deploy/cloudflow-api -- \\ curl localhost:9090/metrics | grep memory # Check for OOMKilled pods kubectl get pods -n cloudflow --field-selector=status.phase=Failed | \\ grep OOMKilled # Review memory limits and requests kubectl describe deployment cloudflow-api -n cloudflow | \\ grep -A 5 \"Limits\\|Requests\" ``` #### Common Causes **1.",
              "score": null
            },
            {
              "doc_id": "user_guide",
              "chunk_id": "user_guide_fix_6",
              "content": "Filter by error type, date range, or execution ID 3. Click an execution to view full details 4. Click **\"Retry\"** to reprocess with the same input data ### Error Notifications Get notified when workflows fail: ```yaml workflow: notifications: on_failure: - type: email to: \"ops-team@company.com\" - type: slack channel: \"#alerts\" message: \"Workflow {{workflow.name}} failed: {{error.message}}\" on_success_after_retry: - type: slack channel: \"#monitoring\" message: \"Workflow recovered after {{error.attempts}} attempts\" ``` ## Workflow Limits CloudFlow enforces the following limits to ensure platform stability and performance: ### Steps Per Workflow - **Maximum**: 50 steps per workflow - **Recommendation**: Keep workflows focused and modular. If you need more steps, consider splitting into multiple workflows connected via webhooks. ### Execution Timeout - **Default**: 3600 seconds (60 minutes) - **Behavior**: Workflows exceeding this timeout are automatically terminated - **Custom Timeouts**: Enterprise plans can request custom timeout limits **Setting Step-Level Timeouts:** ```yaml - id: long_running_task action: http_request config: url: \"https://api.example.com/process\" timeout: 300 # 5 minutes for this specific step ``` ### Execution Limits - **Maximum**: 1000 executions per day (per workflow) - **Rate Limiting**: 100 concurrent executions per workflow - **Burst Limit**: 10 executions per second **What happens when limits are reached:** - New executions are queued automatically - Webhook triggers return HTTP 429 (Too Many Requests) - Scheduled executions are skipped (logged in audit trail) - Email notifications sent to workflow owner **Monitoring Usage:** View real-time metrics in your workflow dashboard: - Executions today: 847 / 1000 - Average duration: 12.3s - Success rate: 98.2% - Current queue depth: 3 ### Data Limits - **Maximum request/response size**: 10MB per action - **Maximum execution payload**: 50MB total - **Variable value size**: 1MB per variable ### Enterprise Plan Limits Enterprise customers can request increased limits: - Up to 100 steps per workflow - Up to 10,000 executions per day - Up to 7200 second timeout (2 hours) - Priority execution queue - Dedicated capacity allocation Contact sales@cloudflow.io for Enterprise pricing and custom limits. ## Best Practices Follow these best practices to build reliable, maintainable workflows: ### 1. Use Descriptive Names **Good:** - Workflow: \"Sync Customer Data from Salesforce to Database\" - Step: \"validate_customer_email\" **Bad:** - Workflow: \"Workflow 1\" - Step: \"step3\" ### 2.",
              "score": null
            }
          ],
          "latency_ms": 962.3396019997017,
          "query_id": "realistic_020",
          "dimension": "negation",
          "query": "Why can't I schedule workflows every 30 seconds?"
        }
      ]
    }
  ]
}